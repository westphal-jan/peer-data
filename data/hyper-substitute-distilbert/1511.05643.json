{"id": "1511.05643", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "A New Smooth Approximation to the Zero One Loss with a Probabilistic Interpretation", "abstract": "proponents examine a new form residual smooth approximation termed estimate zero integral function estimate which learning method managed using a reformulation minus the lightly developed logistic function. more approach is relies on using direct posterior mean of given novel generalized beta - bernoulli formulation. estimation increases to a generalized population gradient that approximates the zero one loss, itself retains a probabilistic formulation conferring a number, useful conditions. the approach is slowly generalized to kernel logistic regression incorporating easily differentiated filtering methods for structured software. methods present strategies in enabling entrepreneurs learn such systems using greedy optimization method demonstrating first a combination of matching expressions onto convex descent using square grid, instructed as to escape from local minima. our approaches exploit observed optimization quality patterns improved when learning counter - parameters are ordered concurrently given a validation set. our tools show increases performance relative success continually used logistic and hinge loss methods on overly wide broad of levels ranging from standard uc problems and cluster decision parameters to product yield predictions and a balanced item support technique. we observe, better approach : 1 ) is more robust proper consistency relation to the logistic coefficients kernel losses ; 12 ) outperforms comparable variable and indifference margin models on larger error benchmark problems ; 3 ) when combined, gaussian - laplacian mixture prior on data the simulation simulation of our simulations permits other solutions than support vector machine classifiers ; and 4 ) when integrated into a probabilistic behavior prediction profile our paper provides exceptionally accurate measures incorporating fuzzy inference and increasing average retrieval performance.", "histories": [["v1", "Wed, 18 Nov 2015 02:31:16 GMT  (546kb,D)", "http://arxiv.org/abs/1511.05643v1", "32 pages, 7 figures, 15 tables"]], "COMMENTS": "32 pages, 7 figures, 15 tables", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.IR cs.LG", "authors": ["md kamrul hasan", "christopher j pal"], "accepted": false, "id": "1511.05643"}, "pdf": {"name": "1511.05643.pdf", "metadata": {"source": "CRF", "title": "A New Smooth Approximation to the Zero One Loss with a Probabilistic Interpretation", "authors": ["Md Kamrul Hasan", "Christopher J. Pal"], "emails": ["md-kamrul.hasan@polymtl.ca,", "christopher.pal@polymtl.ca"], "sections": [{"heading": "1 Introduction", "text": "Loss function minimization is a standard way of solving many important learning problems. In the classical statistical literature, this is known as Empirical Risk Minimization (ERM) [18], where learning is performed by minimizing the average risk or loss over\nar X\niv :1\n51 1.\n05 64\n3v 1\n[ cs\n.C V\n] 1\nthe training data. Formally, this is represented as\nf\u2217 = min f\u2208F\n1\nn n\u2211 i L(f(xi), ti) (1)\nwhere, f \u2208 F is a model, xi is the ith input feature vector with label ti, there are n pairs of features and labels, and L(f(xi), ti) is the loss for the model output f(xi). Let us focus for the moment on the standard binary linear classification task in which we encode the target class label as ti \u2208 {\u22121, 1} and the model parameter vector as w. Letting zi = tiwTxi, we can define the logistic, hinge, and 0-1 loss as\nLlog(zi) = log[1 + exp(\u2212zi)] (2) Lhinge(zi) = max(0, 1\u2212 zi) (3) L01(zi) = I[zi \u2264 0] (4)\nwhere I[x] is the indicator function which takes the value of 1 when its argument is true and 0 when its argument is false. Of course, loss functions can be more complex, for example defined and learned through a linear combination of simpler basis loss functions [20], but we focus on the widely used losses above for now.\nDifferent loss functions characterize the classification problem differently. The log logistic loss and the hinge loss are very similar in their shape, which can be verified from Figure 1. Logistic regression models involve optimizing the log logistic loss, while optimizing a hinge loss is the heart of Support Vector Machines (SVMs). While seemingly a sensible objective for a classification problem, empirical risk minimization with the 0-1 loss function is known to be an NP-hard problem [7].\nBoth the log logistic loss and the hinge loss are convex and therefore lead to optimization problems with a global minima. However, both the the log logistic loss and\nhinge loss penalize a model heavily when data points are classified incorrectly and are far away from the decision boundary. As can be seen in Figure 1 their penalties can be much more significant than the zero one loss. The zero-one loss captures the intuitive goal of simply minimizing classification errors and recent research has been directed to learning models using a smoothed zero-one loss approximation [24, 14]. Previous work has shown that both the hinge loss [24] and more recently the 0-1 loss [14] can be efficiently and effectively optimized directly using smooth approximations. The work in [14] also underscored the robustness advantages of the 0-1 loss to outliers. While the 0-1 loss is not convex, the current flurry of activity in the area of deep neural networks as well as the award winning work on 0-1 loss approximations in [2] have highlighted numerous other advantages to the use of non-convex loss functions. In our work here, we are interested in constructing a probabilistically formulated smooth approximation to the 0-1 loss.\nLet us first compare the widely used log logistic loss with the hinge loss and the 0-1 loss in a little more detail. The log logistic loss from the well known logistic regression model arises from the form of negative log likelihood defined by the model. More specifically, this logistic loss arises from a sigmoid function parametrizing probabilities and is easily recovered by re-arranging (2) to obtain a probability model of the form \u03c0(zi) = (1 + exp(\u2212zi))\u22121. In our work here, we will take this familiar logistic function and we shall transform it to create a new functional form. The sequence of curves starting with the blue curve in Figure 2 (top) give an intuitive visualization of the way in which we alter the traditional log logistic loss. We call our new loss function the generalized Beta-Bernoulli logistic loss and use the acronym BB\u03b3 when referring to it. We give it this name as it arises from the combined use of a Beta-Bernoulli distribution and a generalized logistic parametrization.\nWe give the Bayesian motivations for our Beta-Bernoulli construction in section 3. To gain some additional intuitions about the effect of our construction from a practical perspective, consider the following analysis. When viewing the negative log likelihood of the traditional logistic regression parametrization as a loss function, one might pose the following question: (1) what alternative functional form for the underlying probability \u03c0(zi) would lead to a loss function exhibiting a plateau similar to the 0-1 loss for incorrectly classified examples? One might also pose a second question: (2) is it possible to construct a simple parametrization in which a single parameter controls the sharpness of the smooth approximation to the 0-1 loss? The intuition for an answer to the first question is that the traditional logistic parametrization converges to zero probability for small values of its argument. This in turn leads to a loss function that increases with a linear behaviour for small values of zi as shown in Figure 1. In contrast, our new loss function is defined in such a way that for small values of zi, the function will converge to a non-zero probability. This effect manifests itself as the desired plateau, which can be seen clearly in the loss functions defined by our model in Figure 2 (top). The answer to our second question is indeed yes; and more specifically, to control the sharpness of our approximation, we use a \u03b3 factor reminiscent of a technique used in previous work which has created smooth approximations to the hinge loss [24] as well as smooth approximations of the 0-1 loss [14]. We show the intuitive effect of our construction for different increasing values of gamma in Figure 2 and define it more formally below.\nTo compare and contrast our loss function with other common loss functions such as those in equations (2-4) and others reviewed below, we express our loss here using zi and \u03b3 as arguments. For t = 1, the BB\u03b3 loss can be expressed as\nLBB\u03b3(zi, \u03b3) = \u2212 log ( a+ b[1 + exp(\u2212\u03b3zi)]\u22121 ) , (5)\nwhile for t = \u22121 it can be expressed as LBB\u03b3(zi, \u03b3) = \u2212 log [ 1\u2212 ( a+ b[1 + exp(\u03b3zi)] \u22121)]. (6) We show in section 3 that the constants a and b have well defined interpretations in terms of the standard \u03b1, \u03b2, and n parameters of the Beta distribution. Their impact on our proposed generalized Beta-Bernoulli loss arise from applying a fuller Bayesian analysis to the formulation of a logistic function.\nThe visualization of our proposed BB\u03b3 loss in Figure 2 corresponds to the use of a weak non-informative prior such as \u03b1 = 1 and \u03b2 = 1 and n = 100. In Figure 2, we show the probability given by the model as a function of wTx at the right and the negative log probability or the loss on the left as \u03b3 is varied over the integer powers in the interval [0, 10]. We see that the logistic function transition becomes more abrupt as \u03b3 increases. The loss function behaves like the usual logistic loss for \u03b3 close to 1, but provides an increasingly more accurate smooth approximation to the zero one loss with larger values of \u03b3. Intuitively, the location of the plateau of the smooth log logistic loss approximation on the y-axis is controlled by our choice of \u03b1, \u03b2 and n. The effect of the weak uniform prior is to add a small minimum probability to the model, which can be imperceptible in terms of the impact on the sigmoid function log space, but leads to the plateau in the negative log loss function. By contrast, the use of a strong prior for the losses in Figure 5 (left) leads to minimum and maximum probabilities that can be much further from zero and one.\nOur work makes a number of contributions which we enumerate here: (1) The primary contribution of our work is a new probabilistically formulated approximation to the 0-1 loss based on a generalized logistic function and the use of the Beta-Bernoulli distribution. The result is a generalized sigmoid function in both probability and negative log probability space. (2) A second key contribution of our work is that we present and explore an adapted version of the optimization algorithm proposed in [14] in which we optimize the meta parameters of learning using validation sets. We present a series of experiments in which we optimize the BB\u03b3 loss using the basic algorithm from [14] and our modified version. For linear models, we show that our complete approach outperforms the widely used techniques of logistic regression and linear support vector machines. As expected, our experiments indicate that the relative performance of the approach further increases when noisy outliers are present in the data. (3) We go on to present a number of experiments with larger scale data sets demonstrating that our method also outperforms widely used logistic regression and SVM techniques despite the fact that the underlying models involved are linear. (4) We apply our model in a structured prediction task formulated for mining faces in Wikipedia biography pages. Our proposed method is well adapted to this setting and we and find that the improved probabilistic modeling capabilities of our approach yields improved results for visual information extraction through improved probabilistic structured prediction. (5) We\nalso show how this approach is also easily adapted to create a novel form of kernel logistic regression based on our generalized Beta-Bernoulli Logistic Regression (BBLR) framework. We find that the kernelized version of our method, Kernel BBLR (KBBLR) outperforms non-linear support vector machines. As expected, the L2 regularized KBBLR does not yield sparse solutions; however, (6) since we have developed a robust method for optimizing a non-convex loss we propose and explore a novel non-convex sparsity encouraging prior based on a mixture of a Gaussian and a Laplacian. Sparse KBBLR typically yields sparser solutions than SVMs with comparable prediction performance, and the degree of sparsity scales much more favorably compared to SVMs .\nThe remainder of this paper is structured as follows. In section 2, we present a review of some relevant recent work in the area of 0-1 loss approximation. In section 3, we present the underlying Bayesian motivations for our proposed loss function. In section 4, we provide with the details of optimization and algorithms. In section 5, we present experimental results using protocols that both facilitate comparisons with prior work as well as evaluate our method on some large scale and structured prediction problems. We provide a final discussion and conclusions in section 6."}, {"heading": "2 Relevant Recent Work", "text": "It has been shown in [24] that it is possible to define a generalized logistic loss and produce a smooth approximation to the hinge loss using the following formulation\nLglog(ti,xi;w, \u03b3) = 1\n\u03b3 log[1 + exp(\u03b3(1\u2212 tiwTxi))], (7)\nLglog(zi, \u03b3) = \u03b3 \u22121 log[1 + exp(\u03b3(1\u2212 zi))], (8)\nsuch that lim\u03b3\u2192\u221e Lglog = Lhinge. We have achieved this approximation using a \u03b3 factor and a shifted version of the usual logistic loss. We illustrate the way in which this construction can be used to approximate the hinge loss in Figure 3 (left).\nThe maximum margin Bayesian network formulation in [16] also employs a smooth differentiable hinge loss inspired by the Huber loss, having a similar shape tomin[1, zi]. The sparse probabilistic classifier approach in [10] truncates the logistic loss leading to a sparse kernel logistic regression models. [15] proposed a technique for learning support vector classifiers based on arbitrary loss functions composed of using the combination of a hyperbolic tangent loss function and a polynomial loss function.\nOther recent work [14] has created a smooth approximation to the 0-1 loss by directly defining the loss as a modified sigmoid. They used the following function\nLsig(ti,xi;w, \u03b3) = 1\n1 + exp(\u03b3tiwTxi) , (9)\nLsig(zi, \u03b3) = [1 + exp(\u03b3zi)] \u22121. (10)\nIn a way similar to the smooth approximation to the hinge loss, here lim\u03b3\u2192\u221e Lsig = L01. We illustrate the way in which this construction approximates the 0-1 loss in Figure 3 (right).\nAnother important aspect of [14] is that they compared a variety of algorithms for directly optimizing the 0-1 loss with a novel algorithm for optimizing the sigmoid loss, Lsig(zi, \u03b3). They call their algorithm Smooth 0\u20131 Loss Approximation (SLA) for smooth loss approximation. The compared direct 0-1 loss optimization algorithms are: (1) a Branch and Bound (BnB) [11] technique, (2) a Prioritized Combinatorial Search (PCS) technique and (3) an algorithm referred to as a Combinatorial Search Approximation (CSA), both of which are presented in more detail in [14]. They compared these methods with the use of their SLA algorithm to optimize the sigmoidal approximation to the 0-1 loss.\nTo evaluate and compare the quality of the non-convex optimization results produced by the BnB, PCS and CSA, with their SLA algorithm for the sigmoid loss, [14] also presents training set errors for a number of standard evaluation datasets. We provide an excerpt of their results in Table 1 as we will perform similar comparisons in our experimental work. These results indicated that the SLA algorithm consistently yielded superior performance at finding a good minima to the underlying non-convex problem. Furthermore, in [14], they also provide an analysis of the run-time performance for each of the algorithms. Their experiments indicated that the SLA technique was significantly faster than the alternative algorithms for non-convex optimization. Based on these results we build upon the SLA approach in our work here.\nThe award winning work of [2] produced an approximation to the 0-1 loss by creating a ramp loss, Lramp, obtained by combining the traditional hinge loss with a shifted and inverted hinge loss as illustrated in Figure 4. They showed how to optimize the ramp loss using the Concave-Convex Procedure (CCCP) of [23] and that this yields faster training times compared to traditional SVMs. Other more recent work has proposed an alternative online SVM learning algorithm for the ramp loss [6]. [22] explored\na similar ramp loss which they refer to as a robust truncated hinge loss. More recent work [3] has explored a similar ramp like construction which they refer to as the slant loss. Interestingly, the ramp loss formulation has also been generalized to structured predictions [4, 8].\nAlthough the smoothed zero-one loss captured much attention recently, we can find older references to similar research. There has been the activity of using zero-one loss like functional losses in machine learning, specially by the boosting [13] and neural network [19] communities. Vincent [19] analyzes that the loss defined through a functional of the hyperbolic tangent, 1\u2212 tanh(zi), is more robust as it doesn\u2019t penalize the outliers too excessively compared to other { log logistic loss, hinge loss, and squared loss } loss functions. This loss has interesting properties of both being continuous and with zero-one loss like properties. A variant of this loss has been used in boosting algorithms [13]. Other work [19] has also shown that a hyperbolic tangent parametrized squared error loss, (0.65 \u2212 tanh(zi))2, transforms the squared error loss to behave more like the 1\u2212 tanh(z), hyperbolic tangent loss.\nWe shall see below how it is also possible to integrate our novel smooth loss formulation into models for structured prediction. In this way our work is similar to that of [8] which explored the use of the ramp loss of [2] in the context of structured prediction for machine translation."}, {"heading": "3 Our Approach: Generalized Beta-Bernoulli Logistic Classification", "text": "We now derive a novel form of logistic regression based on formulating a generalized sigmoid function arising from an underlying Bernoulli model with a Beta prior. We also use a \u03b3 scaling factor to increase the sharpness of our approximation. Consider first the traditional and widely used formulation of logistic regression which can be derived from a probabilistic model based on the Bernoulli distribution. The Bernoulli probabilistic model has the form:\nP (y|\u03b8) = \u03b8y(1\u2212 \u03b8)(1\u2212y), (11)\nwhere y \u2208 {0, 1} is the class label, and \u03b8 is the parameter of the model. The Bernoulli distribution can be re-expressed in standard exponential family form as\nP (y|\u03b8) = exp { log ( \u03b8 1\u2212 \u03b8 ) y + log(1\u2212 \u03b8) } , (12)\nwhere the natural parameter \u03b7 is given by\n\u03b7 = log ( \u03b8 1\u2212 \u03b8 ) (13)\nIn traditional logistic regression, we let the natural parameter \u03b7 = wTx, which leads to a model where \u03b8 = \u03b8ML in which the following parametrization is used\n\u03b8ML = \u00b5ML(w,x) = 1\n1+ exp(\u2212\u03b7) =\n1\n1+ exp(\u2212wTx) (14)\nThe conjugate distribution to the Bernoulli is the Beta distribution\nBeta(\u03b8|\u03b1, \u03b2) = 1 B(\u03b1, \u03b2) \u03b8\u03b1\u22121(1\u2212 \u03b8)\u03b2\u22121 (15)\nwhere \u03b1 and \u03b2 have the intuitive interpretation as the equivalent pseudo counts for observations for the two classes of the model and B(\u03b1, \u03b2) is the beta function. When we use the Beta distribution as the prior over the parameters of the Bernoulli distribution, the posterior mean of the Beta-Bernoulli model is easily computed due to the fact that the posterior is also a Beta distribution. This property also leads to an intuitive form for the posterior mean or expected value \u03b8BB in a Beta-Bernoulli model, which consists of a simple weighted average of the prior mean \u03b8B and the traditional maximum likelihood estimate, \u03b8ML, such that\n\u03b8BB = w\u03b8B + (1\u2212 w)\u03b8ML, (16)\nwhere w = \u03b1+ \u03b2\n\u03b1+ \u03b2 + n , and \u03b8B = ( \u03b1 \u03b1+ \u03b2 ) ,\nand where n is the number of examples used to estimate \u03b8ML. Consider now the task of making a prediction using a Beta posterior and the predictive distribution. It is\neasy to show that the mean or expected value of the posterior predictive distribution is equivalent to plugging the posterior mean parameters of the Beta distribution into the Bernoulli distribution, Ber(y|\u03b8), i.e.\np(y|D) = \u222b 1 0 p(y|\u03b8)p(\u03b8|D)d\u03b8 = Ber(y|\u03b8BB). (17)\nGiven these observations, we thus propose here to replace the traditional sigmoidal function used in logistic regression with the function given by the posterior mean of the Beta-Bernoulli model such that\n\u00b5BB(w,x) = w ( \u03b1 \u03b1+ \u03b2 ) + (1\u2212 w)\u00b5ML(w,x) (18)\nFurther, to increase our model\u2019s ability to approximate the zero one loss, we shall also use a generalized form of the Beta-Bernoulli model above where we set the natural parameter of \u00b5ML so that \u03b7 = \u03b3wTx. This leads to our complete model based on a generalized Beta-Bernoulli formulation\n\u00b5BB\u03b3(w,x) = w ( \u03b1 \u03b1+ \u03b2 ) + (1\u2212 w) 1 1 + exp(\u2212\u03b3wTx) . (19)\nIt is useful to remind the reader at this point that we have used the Beta-Bernoulli construction to define our function, not to define a prior over the parameter of a random variable as is frequently done with the Beta distribution. Furthermore, in traditional Bayesian approaches to logistic regression, a prior is placed on the parameters w and used for MAP parameter estimation or more fully Bayesian methods in which one integrates over the uncertainty in the parameters.\nIn our formulation here, we have placed a prior on the function \u00b5ML(w,x) as is commonly done with Gaussian processes. Our approach might be seen as a pragmatic alternative to working with the fully Bayesian posterior distributions over functions given data, p(f |D). The more fully Bayesian procedure would be to use the posterior predictive distribution to make predictions using\np(y\u2217|x\u2217,D) = \u222b p(y\u2217|f, x\u2217)p(f |D)df. (20)\nLet us consider again the negative log logistic loss function defined by our generalized Beta-Bernoulli formulation where we let z = wT x and we use our y \u2208 {0, 1} encoding for class labels. For y = 1 this leads to\n\u2212 log p(y = 1|z) = \u2212 log [ w\u03b8\u03b2 +\n(1\u2212 w) 1 + exp(\u2212\u03b3z)\n] , (21)\nwhile for the case when y = 0, the negative log probability is simply\n\u2212 log p(y = 0|z) = \u2212 log ( 1\u2212 [ w\u03b8\u03b2 +\n(1\u2212 w) 1 + exp(\u2212\u03b3z)\n]) (22)\nwhere w\u03b8\u03b2 = a and (1 \u2212 w) = b for the formulation of the corresponding loss given earlier in equations (5) and (6).\nIn Figure 2 we showed how setting this scalar parameter \u03b3 to larger values, i.e 1 allows our generalized Beta-Bernoulli model to more closely approximate the zero one loss. We show the BB\u03b3 loss with a = 1/4 and b = 1/2 in Figure 5 (left) which corresponds to a stronger Beta prior and as we can see, this leads to an approximation with a range of values that are even closer to the 0-1 loss. As one might imagine, with a little analysis of the form and asymptotics of this function, one can also see that for given a setting of \u03b1 = \u03b2 and n, a corresponding scaling factor s and linear translation c can be found so as to transform the range of the loss into the interval [0, 1] such that lim\u03b3\u2192\u221e s(LBB\u03b3 \u2212 c) = L01. However, when \u03b1 6= \u03b2 as shown in Figure 5 (right), the loss function is asymmetric and in the limit of large gamma this corresponds to different losses for true positives, false positives, true negatives and false negatives. For these and other reasons we believe that this formulation has many attractive and useful properties."}, {"heading": "3.1 Parameter Estimation and Gradients", "text": "We now turn to the problem of estimating the parameters w, given data in the form of D = {yi,xi}, i = 1, . . . , n, using our model. As we have defined a probabilistic model, as usual we shall simply write the probability defined by our model then optimize the parameters via maximizing the log probability or minimizing the negative log probability. As we shall discuss in more detail in section 4, we use a modified form of the SLA optimization algorithm in which we slowly increase \u03b3 and interleave gradient descent steps with coordinate descent implemented as a grid search. For the gradient\ndescent part of the optimization we shall need the gradients of our loss function and we therefore give them below.\nConsider first the usual formulation of the conditional probability used in logistic regression\nP ({yi}|{xi},w) = n\u220f i=1 \u00b5yii (1\u2212 \u00b5i) (1\u2212yi), (23)\nhere in place of the usual \u00b5i, in our generalized Beta-Bernoulli formulation we now have \u00b5i = \u00b5\u03b2B(\u03b7i) where \u03b7i = \u03b3wTxi. Given a data set D consisting of label and feature vector pairs, this yields a log-likelihood given by\nL = logP (D|w) = n\u2211 i=1 ( yi log\u00b5i + (1\u2212 yi) log(1\u2212 \u00b5i) ) (24)\nwhere the gradient of this function is given by\ndL dw = n\u2211 i=1 ( yi \u00b5i \u2212 1\u2212 yi 1\u2212 \u00b5i )d\u00b5i d\u03b7i d\u03b7i dw\n(25)\nwith d\u00b5i d\u03b7i = (1\u2212 w) exp(\u2212\u03b7i) (1 + exp(\u2212\u03b7i))2 and d\u03b7i dw = \u03b3x. (26)"}, {"heading": "3.2 Some Asymptotic Analysis", "text": "As we have stated at the beginning of our discussion on parameter estimation, at the end of our optimization we will have a model with a large \u03b3. With a sufficiently large \u03b3 all predictions will be given their maximum or minimum probabilities possible under the \u03b2B\u03b3 model. Defining the t = 1 class as the positive class, if we set the maximum probability under the model equal to the True Positive Rate (TPR) (e.g. on training and/or validation data) and the maximum probability for the negative class equal to the True Negative Rate (TNR) we have\nw\u03b8\u03b2 + (1\u2212 w) = TPR, (27) 1\u2212 w\u03b8B = TNR, (28)\nwhich allows us to conclude that this would equivalently correspond to setting\nw = 2\u2212 (TNR+ TPR), (29)\n\u03b8B = 1\u2212 TNR\n2\u2212 (TNR+ TPR) . (30)\nThis analysis gives us a good idea of the expected behavior of the model if we optimize w and \u03b8B on a training set. It also suggests that an even better strategy for tuning w and \u03b8B would be to use a validation set."}, {"heading": "3.3 Learning hyper-parameters", "text": "We have provided an asymptotic analysis of the expected values for w and \u03b8B in the previous section. In the experiment section, we provide BBLR results for using asymptotic values of these two parameters along with cross-validated values for other hyperparameters {\u03b3, \u03bb}, where \u03bb is the regularization parameter described in Section 4. It is however also possible to learn these hyper-parameters using the training set, validation set or both. Below, we provide partial-derivatives of likelihood function (24) for these hyper-parameters.\ndL dw = n\u2211 i=1 ( yi \u00b5i \u2212 1\u2212 yi 1\u2212 \u00b5i )d\u00b5i dw\n(31)\nwith d\u00b5i dw = \u03b8B \u2212 1 1 + exp(\u2212\u03b3wTx) (32)\nThe partial-derivatives with respect to \u03b8B and \u03b3 are as follows\ndL d\u03b8B = w n\u2211 i=1 ( yi \u00b5i \u2212 1\u2212 yi 1\u2212 \u00b5i ) (33)\ndL d\u03b3 = n\u2211 i=1 ( yi \u00b5i \u2212 1\u2212 yi 1\u2212 \u00b5i )d\u00b5i d\u03b7i d\u03b7i d\u03b3\n(34)\nwith d\u00b5i d\u03b7i = (1\u2212 w) exp(\u2212\u03b7i) (1 + exp(\u2212\u03b7i))2 and d\u03b7i d\u03b3 = wTx. (35)"}, {"heading": "3.4 Kernel Beta-Bernoulli Classification", "text": "It is possible to transform the traditional logistic regression technique discussed above into a kernel logistic regression (KLR) by replacing the linear discriminant function, \u03b7 = wT x, with\n\u03b7 = f(a, x) = N\u2211 j=1 ajK(x, xj), (36)\nwhere K(x, xj) is a kernel function and j is used as an index in the sum over all N training examples.\nTo create our generalized Beta-Bernoulli KLR model we take a similar path; however, in this case we let \u03b7 = \u03b3f(a, x). Thus, our Kernel Beta-Bernoulli model can be written as:\n\u00b5K\u03b2B(a,x) = w ( \u03b1 \u03b1+ \u03b2 ) + (1\u2212 w) 1 + exp ( \u2212 \u03b3f(a,x)\n) . (37) If we write f(a, x) = aTk(x), where k(x) is a vector of kernel values, then the gradient of the corresponding KBBLR log likelihood obtained by setting \u00b5i = \u00b5K\u03b2B(a,x) in\n(24) is dL\nda = \u03b3(1\u2212 w) n\u2211 i=1 k(xi) ( yi \u00b5i \u2212 1\u2212 yi 1\u2212 \u00b5i ) exp(\u2212\u03b7i) (1 + exp(\u2212\u03b7i))2 . (38)"}, {"heading": "4 Optimization and Algorithms", "text": "As we have discussed in the relevant recent work section above, the work of [14] has shown that their SLA algorithm applied to Lsig(zi, \u03b3) outperformed a number of other techniques in terms of both true 0-1 loss minimization performance and run time. As our generalized Beta-Bernoulli loss, LBB\u03b3(zi, \u03b3) is another type of smooth approximation to the 0-1 loss, we therefore use a variation of their SLA algorithm to optimize the BB\u03b3 loss. Recall that if one compares our generalized Beta-Bernoulli logistic loss with the directly defined sigmoidal loss used in the SLA work of [14], it becomes apparent that the BBLR formulation has three additional hyper-parameters, {\u03b1, \u03b2, w}. These additional parameters control the locations of the plateaus of our function and these plateaus have well defined interpretations in terms of probabilities. In contrast, the plateaus of the sigmoidal loss in [14] are located at zero and one. Additionally, in practise one is interested in optimizing the regularized loss, where some form of prior or regularization is used for parameters. In our experiments here, we follow the widely used practice of using a Gaussian prior for parameters. The corresponding regularized loss arising from the negative log likelihood with the additional L2 regularization term gives us our complete objective function\nL(D|w) = \u2212 n\u2211 i=1 ( yi log\u00b5i + (1\u2212 yi) log(1\u2212 \u00b5i) ) + \u03bb 2 \u2016w\u20162, (39)\nwhere the parameter \u03bb controls the strength of the regularization. With these additional hyper-parameters {\u03b1, \u03b2, w, \u03bb}, the original SLA algorithm is not directly applicable to our formulation. However, if we hold these hyper-parameters fixed, we are able to use the general idea of their approach and perform a Modified SLA optimization as given in Algorithms 1 and 2. In our experiments below, we use that strategy in the BBLR1,2,3 series of experiments. To deal with the issue of how to jointly learn weights w as well as hyper-parameters w, \u03b1, \u03b2, \u03bb and \u03b3; in our BBLR4 series of experiments we learn these hyper-parameters by gradient descent on the training set. More precisely, we learn w and \u03b8B (as opposed to learning \u03b1, \u03b2) as this permit the parameters to be easily re-parametrized so that they both lie within [0, 1].\nVery importantly, our initial experiments indicated that the basic SLA formulation required considerable hand tuning of learning parameters for each new data set. This was the case even using the simplest smooth loss function without the additional degrees of freedom afforded by our formulation. This led us to develop a metaoptimization procedure for learning algorithm parameters. The BBLR3,4 series of experiments below use this learning meta-parameter optimization procedure. Our initial and formal experiments here indicate that this meta-optimization of learning parameters is in fact essential in practice. We therefore present it in more detail below."}, {"heading": "4.1 Our SLA Algorithm Meta-optimization (SLAM)", "text": "Here we present our meta-optimization extension and various other modifications to the SLA approach of [14]. The SLA algorithm proposed in [14] can be decomposed into two different parts; an outer loop that initializes a model then enters a loop in which one slowly increases the \u03b3 factor of their sigmoidal loss, repeatedly calling an algorithm they refer to as Range Optimization for SLA or Gradient Descent in Range. The Range Optimization part consists of two stages. Stage 1 is a standard gradient descent optimization with a decreasing learning rate (using the new \u03b3 factor). Stage 2 probes each parameter wi in a radius R using a one dimensional grid search to determine if the loss can be further reduced, thus implementing a coordinate descent on a set of grid points. We provide a slightly modified form of the outer loop of their algorithm in Algorithm 1 where we have expressed the initial parameters given to the model, w0 as explicit parameters given to the algorithm. In their approach they hard code the initial parameter estimates as the result of an SVM run on their data. We provide a compressed version of their inner Range optimization technique in Algorithm 2.\nAlgorithm 1 Modified SLA - Initialization and outer loop Input: Training data X, t, and initial weights w0, and\nconstants: R0, S0 , \u03b3MIN , \u03b3MAX , r\u03b3 , rR, r Output: w\u2217, estimated weights minimizing 0-1 loss.\n1: function FIND-SLA-SOLUTION(X,t,w0) 2: w\u2190 w0 3: R\u2190 R0 4: S \u2190 S0 5: \u03b3 \u2190 \u03b3MIN 6: while \u03b3 \u2264 \u03b3MAX do 7: w\u2217 \u2190 GRAD-DESC-IN-RANGE(w\u2217,X, t, \u03b3, R, S) 8: \u03b3 \u2190 r\u03b3\u03b3 9: R\u2190 rRR\n10: S \u2190 r S 11: end while 12: return w\u2217 13: end function\nThe first minor difference between the SLA optimization algorithm of [14] and our extension to it are the selection of the initial w0 that the SLA algorithm starts optimizing. While the original SLA algorithm uses the SVM solution as its initial solution, w0, our modified SLA algorithm uses the \u03b3 and \u03bb obtained from experiments using a validation set defined within the training data to initialize w0 for the gradient based optimization technique which will start from w = 0. The idea here is to search for the best \u03b3 and \u03bb that produces a reasonable solution of w that the SLA algorithm will start with, where \u03bb is the weight associated with the Gaussian prior leading to L2 penalty added to (24).\nOur meta-optimization procedure consists of the following. We use the suggested values in the original SLA algorithm [14] for the parameters rR, R0, r , and S0 . For\nAlgorithm 2 Range Optimization for SLA Input: w,X, t, \u03b3, radius R, step size S Output: Updated estimate for w\u2217, minimizing 0-1 loss.\n1: function GRAD-DESC-IN-RANGE(w,X, t, \u03b3, R, S) 2: repeat B Stage 1: Find local minimum 3: w\u2217 \u2190 VANILLA-GRAD-DESC(w)\nB Stage 2: Probe each dimension in a radius R B to escape local minimum (if possible)\n4: for i = 1 . . . d do . For each dimension, wi 5: for step \u2208 { S ,\u2212 S , 2 S ,\u22122 S , . . . , R,\u2212R} do 6: w\u2190 w\u2217 7: wi \u2190 wi + step 8: if L\u03b3(w\u2217)\u2212 L\u03b3(w) \u2265 L then 9: break . Goto step 3\n10: end if 11: end for 12: end for 13: until L\u03b3(w\u2217)\u2212 L\u03b3(w) < L 14: return w\u2217 15: end function\nthe others, we use a cross validation run using the same modified SLA algorithm to fine-tune algorithm parameters.\nParameter r\u03b3 is chosen through a grid search, while \u03b3MIN and \u03b3MAX are chosen by a bracket search algorithm. In our experience, these model parameters change from problem (dataset) to problem, and hence must be fine-tuned for the best results."}, {"heading": "5 Experimental Setup and Results", "text": "Below, we present results for three different groups of benchmark problems: (1) a selection from the University of California Irvine (UCI) repository, (2) some larger and higher dimensionality text processing tasks from the LibSVM evaluation archive 1, and (3) the product review sentiment prediction datasets used in [5]. We then present results on a structured prediction problem formulated for the task of visual information extraction from Wikipedia biography pages. Finally we explore the kernelized version of our classifier.\nIn all experiments, unless otherwise stated, we use a Gaussian prior on parameters leading to an L2 penalty term. We explore four experimental configurations for our BBLR approach: (1) BBLR1, where we use our modified SLA algorithm with the following BBLR parameters held fixed : \u03b1 = \u03b2 = 1 and n = 100. This corresponds to a minor modification to the traditional negative log logistic loss, but yields a probabilistically well defined smooth sigmoid shaped loss (ex. as we have seen in Figure 2); (2)\n1http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/binary.html\nBBLR2, where we use values for \u03b1, \u03b2 and n corresponding to the empirical counts of positives, negatives and the total number of examples from the training set, which corresponds to a simplistic heuristic, partially justified by Bayesian reasoning; (3) BBLR3 in which an outer meta-optimization of learning parameters is performed on top of (2), ie SLAM, and (4) BBLR4 in which the outer meta-optimization of learning parameters is performed, and the hyper-parameters w, \u03b8B , \u03bb, and \u03b3 are optimized by gradient descent using the training set, with w and \u03b8B initialized using the values given by our asymptotic analysis using a hard threshold for classifications. At each iteration of this optimization step, as parameters {w, \u03b8B , \u03bb, \u03b3} get updated, the complementary SLAM hyper-parameters, \u03b3R, \u03b3MIN , \u03b3MAX are adjusted/redefined by using the same metaoptimization procedure (SLAM) and using a subset of the training data as a validation set.\nConsequently, models produced by the BBLR3 series of experiments explore the ability of our improved SLA learning parameter meta-optimization method (SLAM) to effectively minimize a smooth approximation to the zero one loss. While the BBLR4 series of experiments delve the deepest into the ability of our BBLR formulation and SLAM optimization to more accurately make probabilistic predictions."}, {"heading": "5.1 Binary Classification Tasks", "text": ""}, {"heading": "5.1.1 Experiments with UCI Benchmarks", "text": "We evaluate our technique on the following datasets from the University of California Irvine (UCI) Machine Learning Repository [1]: Breast, Heart, Liver and Pima. We use these datasets in part so as to compare directly with results in [14], to understand the behaviour of our novel logistic function formulation and to explore the behavior of our learning parameter optimization procedure. Table 2 shows some brief details of these databases.\nTo facilitate comparisons with previous results presented [14] such as those summarized in Table 3 of our literature review in Section 2, we provide a small set of initial experiments here following their experimental protocols. In our experiments here we compare our BBLRs with the following models: our own L2 Logistic Regression (LR) implementation, a linear SVM - using the same implementation (liblinear) that was used in [14], and the optimization of the sigmoid loss, Lsig(zi, \u03b3) of [14] using the SLA algorithm and the code distributed on the web site associated with [14] (indicated by SLA in our tables).\nDespite the fact that we used the code distributed on the website associated with [14] we found that the SLA algorithm applied to their sigmoid loss, Lsig(zi, \u03b3) gave errors that are slightly higher than those given in [14]. We use the term SLA in Table 3 and subsequent tables to denote experiments performed using both the sigmoidal loss explored in [14] and their algorithm for minimizing it. Applying the SLA algorithm to ourBB\u03b3 loss yielded slightly superior results to the sigmoidal loss when the empirical counts from the training set for \u03b1, \u03b2 and n are used and slightly worse results when we used \u03b1 = 1, \u03b2 = 1 and n = 100.\nAnalyzing the ability of different loss formulations and algorithms to minimize the 0-1 loss on different datasets using a common model class (i.e. linear models) can reveal differences in optimization performance across different models and algorithms. However, we are certainly more interested in evaluating the ability of different loss functions and optimization techniques to learn models that can be generalized to new data. We therefore provide the next set of experiments using traditional training, validation and testing splits, again following the protocols used in [14]; however, as we shall soon see, these experiments underscored the importance of extending the original SLA algorithm to automate the adjustment of learning parameters.\nIn Tables 4 and 5, we create 10 random splits of the data and perform a traditional 5 fold evaluation using cross validation within each training set to tune hyper-parameters. In Table 4, we present the sum of the 0-1 loss over each of the 10 splits as well as the total 0-1 loss across all experiments for each algorithm. This analysis allows us to make some intuitive comparisons with the results in Table 1, which represents an empirically derived lower bound on the 0-1 loss. In Table 5, we present the traditional mean accuracy across these same experiments. Examining columns SLA vs. BBLR2 in Table 4, we see that our re-formulated logistic loss is able to outperform the sigmoidal loss, but that only with the addition of the additional tuning of parameters during the optimization in column BBLR3 are we able to improve upon the overall zero-one loss yielded by the logistic regression and SVM baseline methods. However, it is important to remember that all of these methods are based on an underlying linear model, these are comparatively small datasets consisting of relatively low dimensional input feature vectors. As such, we do not necessarily expect there to be any statistically significant differences test set performance due to zero-one loss minimization performance. The same observation was made in [14] and it motivated their own exploration of learning with noisy feature vectors. We follow a similar path below, but then go on further to explore datasets that are much larger and of much higher dimensions in our subsequent experimental work.\nIn Table 6, we present the sum of the mean 0-1 loss over 10 repetitions of a 5 fold leave one out experiment where 10% noise has been added to the data following the protocol given in [14]. Here again, our BBLR2 achieved a moderate gain over the SLA algorithm, whereas the gain of BBLR3 over other models is noticeable. In this table, we also show the percentage of improvement for our best model over the linear SVM. In Table 7, we show the average errors (%) for these 10% noise added experiments. We see here that the advantages of more directly approximating the zero one loss are more pronounced. However, the fact that the SLA approach failed to outperform the LR and SVM baselines in our experiments here; whereas in a similar experiment in [14] the SLA algorithm and sigmoidal loss did outperform these methods leads us to believe\nthat the issue of per-dataset learning algorithm parameter tuning is a significant issue. However, we observe that our BBLR2 experiment which used the original SLA optimization algorithm outperformed the sigmoidal loss function optimized using the SLA algorithm. These results support the notion that our proposed Beta-Bernoulli logistic loss is in itself a superior approach to approximate the zero-one loss from an empirical\nperspective. However, our results in column BBLR4 indicate that the combined use of our novel logistic loss and learning parameter optimization yield the most substantial improvements to zero-one loss minimization, or correspondingly improvements to accuracy."}, {"heading": "5.1.2 Pooled McNemar Tests :", "text": "We performed McNemar tests for the four UCI benchmarks comparing BBLR3 with LR and linear SVMs. As we do not have significant number of test instances for any of these benchmarks, it became difficult to statistically justify and compare results. Therefore, we performed pooled McNemar tests by considering each split of our 5- fold leave one out experiments as independent tests and collectively performing the significance tests as a whole. The results of this pooled McNemar test is given in Table 8. Interestingly, for our noisy dataset experiments, our BBLR3 was found to be statistically significant over both the LR and SVM models with p \u2264 0.01."}, {"heading": "5.1.3 Experiments with LibSVM Benchmarks", "text": "In this section, we present classification results using two much larger datasets: the web8, and the webspam-unigrams. These datasets have predefined training and testing splits, which are distributed on the web site accompanying [25]2. These benchmarks are also distributed through the LibSVM binary data collection3. The webspam unigrams data originally came from the study in [21]4. Table 9 compiles some details of thsese databases.\nFor these experiments we do not add additional noise to the feature vectors. In Table 10, we present classification results, and one can see that for both cases our BBLR3 approach shows improved performance over the LR and the linear SVM baselines. As in our earlier small scale experiments, we used our own LR implementation and the liblinear SVM for these large scale experiments.\nWe performed McNemar\u2019s statistical tests comparing our BBLR3 with LR and linear SVM models for these two datasets. The results are found to be statistically significant with a p value \u2264 0.01 for all cases. Given that no noise has been added to\n2http://users.cecs.anu.edu.au/ xzhang/data/ 3http://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets/binary.html 4http://www.cc.gatech.edu/projects/doi/WebbSpamCorpus.html\nthese widely used benchmark problems and that each method compared here is fundamentally based on a linear model, the fact that these experiments show statistically significant improvements for BBLR3 over these two widely used methods is quite interesting."}, {"heading": "5.1.4 Experiments with Product Reviews", "text": "The goal of these tasks are to predict whether a product review is either positive or negative. For this set of experiments, we used the count based unigram features for four databases from the website associated with [5]. For each database, there are 1,000 positive and 1,000 negative product reviews. Table 11 compiles the feature dimension size of these sparse databases.\nWe present results in Table 12 using a ten fold cross validation setup as performed by [5]. Here again we do not add noise the the data.\nFor all four databases, our BBLR3 and BBLR4 models outperformed both the LR and linear SVM. To further analyze these results, we also performed a McNemer\u2019s test. For the Books and the DVDs database, the results of our BBLR3 and BBLR4 models are found statistically significant over both the LR and linear SVM with a p value \u2264 0.05. BBLR4 tended to outperform BBLR3, but not in a statistically significant way. However, since the primary advantage of the BBLR4 configuration is that it yields more accurate probabilities, we to not necessarily expect it to have dramatically superior performance compared to BBLR3 for classification. For this reason we explore the problem of using such models in the context of a structured prediction in the next set of experiments. When BBLR models are used to make structured predictions our\nhypothesis is that the benefits of providing a more accurate probabilistic prediction should be apparent through improved joint inference."}, {"heading": "5.2 Structured Prediction Experiments", "text": "One of the advantages of our Beta-Bernoulli logistic loss is that it allows a model to produce more accurate probabilistic estimates. Intuitively, the controllable nature of the plateaus in the log probability view of our formulation allow probabilistic predictions to take on values that are more representative of an appropriate confidence level for a classification. In simple terms, predictions for feature vectors far from a decision boundary need not take on values that are near probablity zero or probability one when the Beta-Bernoulli logistic model is used. If such models are used as components to larger systems which uses probabilistic inference for more complex reasoning tasks, the additional flexibility could be a significant advantage over the traditional logistic function formulation. The following experiments explore this hypothesis.\nIn [9], we performed a set of face mining experiments from Wikipedia biography pages using a technique that relies on probabilistic inference in a joint probability model. For a given identity, our mining technique dynamically creates probabilistic models to disambiguate the faces that correspond to the identity of interest. These models integrate uncertain information extracted throughout a document arising from three different modalities: text, meta data and images. Information from text and metadata is integrated into the larger model using multiple logistic regression based components.\nThe images, face detection results as bounding boxes, some text and meta information extracted from one of the Wikipedia identity, Mr. Richard Parks, are shown in the top panel of Figure 6. In the bottom panel, we show an instance of our mining model and give a summary of the variables used in our technique. The model is a dynamically instantiated Bayesian network. Using the Bayesian network illustrated in Figure 6, the processing of information is intuitive. Text and meta-data features are taken as input to the bottom layer of random variables {X}, which influence binary (target or not target) indicator variables {Y } for each detected face through logistic regression based sub-components. The result of visual comparisons between all faces detected in different images are encoded in the variables {D}.\nBoth text and meta data are transformed into feature vectors associated with each detected instance of a face. For text analysis, we use information such as: image file names and image captions. The location of an image in the page is an example of what we refer to as meta-data. We also treat other information about the image that is not directly involved in facial comparisons as meta-data, ex. the relative size of a face to other faces detected in an image. The bottom layer or set of random variables {X} in Figure 6 are used to encode these features, and we discuss the precise nature and definition of these features in more detail in [9]. Xmn = [X (mn) 1 , X (mn) 2 , \u00b7 \u00b7 \u00b7X (mn) K ] T is therefore the local feature vector for a face, xmn, where X (mn) k is the k\nth feature for face index n for image index m. These features are used as the input to the part of our model responsible for producing the probability that a given instance of a face belongs to the identity of interest, encoded by the random variables {Y } in Figure 6. {Y } = {{Ymn}Nmn=1}Mm=1 is therefore a set of binary target vs. not target indicator variables\ncorresponding to each face, xmn. Inferring these variables jointly corresponds to the goal of our mining model. The joint conditional distribution defined by the general case of our model is given by\np({{Ymn}Nmn=1}Mm=1, {Dl}Ll=1, {Sm}Mm=1|{{Xmn} Nm n=1}Mm=1)\n= M\u220f m=1 Nm\u220f n=1 p(Ymn|Xmn)p(Sm|{Ymn\u2032} N \u2032m n\u2032=1)\nL\u220f l=1 p(Dl|{Ym\u2032ln\u2032l , Ym\u2032\u2032l n\u2032\u2032l }). (40)\nApart from comparing cross images faces, p(Dl|{Ym\u2032ln\u2032l , Ym\u2032\u2032l n\u2032\u2032l }), the joint model uses predictive scores from per face local binary classifiers, p(Ymn|Xmn). As mentioned above and discussed in more detail in [9], we used Maximum Entropy Models (MEMs) or Logistic Regression models for these local binary predictions working on multimedia features in our previous work.\nHere, we compare the result of replacing the logistic regression components in the model discussed above with our BBLR formulation. We examine the impact of this change in terms of making predictions based solely on independent models taking text and meta-data features as input as well as the impact of this difference when LR vs BBLR models are used as sub-components in the joint structured prediction model. Our hypothesis here is that the BBLR method might improve results due to its robustness to outliers (which we have already seen in our binary classification experiments) and that the method is potentially able make more accurate probabilistic predictions, which could in turn lead to more precise joint inference.\nFor this particular experiment, we use the biographies with 2-7 faces. Table 13 shows results comparing the MaxEnt model with our BBLR model. The results are for a five-fold leave one out of the wikipedia dataset. One can see that we do indeed obtain superior performance with the independent BBLR models over the Maximum Entropy models. We also see improvement to performance when BBLR models are used in the coupled model where joint inference is used for predictions.\nIn the row labelled BBLR4, we optimized {w, \u03b8B , \u03b3, \u03bb} in addition to other model parameters using the technique, explained in Section 3.3. This produced statistically significant results compared to the maximum entropy model with p \u2264 0.05. For this significance test, we used the McNemar test like our earlier sets of experiments."}, {"heading": "5.3 Kernel Logistic Regression with the Generalized Beta-Bernoulli Loss", "text": "In Table 14 we compare Beta-Bernoulli logistic regression with an SVM and Kernel Beta-Bernoulli logistic regression (KBBLR). We see that our proposed approach compare favorably to the SVM result which is widely considered as a state of the art, strong baseline."}, {"heading": "5.4 Sparse Kernel BBLR", "text": "As shown in [2], one of the advantages of using the ramp loss for kernel based classification is that it can yield models that are even sparser than traditional SVMs based on the hinge loss. It is well known that L2 based regularization does not typically yield sparse solutions when used with traditional kernel logistic regression. Our analysis of the previous experiments reveals that the L2 regularized smooth zero one loss approximation approach proposed here does not in general lead to sparse models as well. The well known L1 or lasso regularization method can yield sparse solutions, but often at the cost of prediction performance. Recently the so called elastic net regularization approach [26] based on a weighted combination of L1 and L2 regularization has been shown more effective at encouraging sparsity with a less negative impact on performance. The elastic net approach of course can be viewed as a prior consisting of the product of a Gaussian and a Laplacian distribution. However, part of the motivation for the use of these methods is that they yield convex optimization problems when combined with the log logistic loss. Since we have developed a robust approach for optimizing a non-convex objective function above, this opens the door to the use of non-convex sparsity encouraging regularizers. Correspondingly, we propose and explore below a prior on parameters, or equivalently, a novel regularization approach based on a mixture of a Gaussian and a Laplacian distribution. This formulation can behave like a smooth approximation to an L0 counting \u201cnorm\u201d prior on parameters in the limit as the Laplacian scale parameter goes to zero and the Gaussian variance goes to infinity.\nWith a (marginalized) Gaussian-Laplace mixture prior, our KBBLR log-likelihood\nbecomes\nL(D|a) = n\u2211 i=1 ( yi log\u00b5i + (1\u2212 yi) log(1\u2212 \u00b5i) ) (41)\n+ \u2211 j ln ( \u03c0gN (aj ; 0, \u03c32g) + \u03c0lL(aj ; 0, bl) ) where \u00b5i = \u00b5K\u03b2B(a,xi) is our kernel Beta-Bernoulli model as defined in section 3.4, equation (37). For each aj , its prior is modeled through a mixture of a zero mean Gaussian N (aj ; 0, \u03c32g) with variance \u03c32g and a Laplacian distribution L(aj ; 0, bl), located a zero with shape parameter bl. For convenience we give the relevant partial derivatives for this prior in Appendix B. In our approach we also optimize the hyper-parameters {\u03c0g, \u03c0l, \u03c32g , bl} of this prior using hard assignment Expectation Maximization steps that are performed after step 3 of Algorithm 2. For precision we outline the steps of the modified range-optimization for Kernel BBLR (KBBLR) in Algorithm 3 found in Appendix C.\nIn Table 15, we compare sparse KBBLR and the SVM using a Radial Basis Function (RBF) kernel. The SVM free parameters were tuned by a cross validation run over the training data. For a sparse KBBLR solution, we used a mixture of a Gaussian and a Laplacian prior on the kernel weight parameters as presented above.\nTable 15 compares sparse Kernel BBLR with SVMs on the standard UCI datasets. Figure 7 shows trends in the sparsity curves for an increase in the number of training instances comparing KBBLR with SVMs for one of the product review databases. We can see that KBBLR scales up well compared to an SVM solution when training data size increases. Support vectors for SVMs increase almost linearly for an increase in the database size, an effect that has been confirmed in a number of other studies [17, 2]. In comparison we can see that KBBLR with a Gaussian-Laplacian mixture prior produces a logarithmic curve for an increase in the database size. The right panel of the same figure also shows the weight distribution before and after the KBBLR optimization with a Gaussian-Laplacian mixture prior which yields the observed sparse solution."}, {"heading": "6 Discussion and Conclusions", "text": "We have presented a novel formulation for learning with an approximation to the zero one loss. Through our generalized Beta-Bernoulli formulation, we have provided both a new smooth 0-1 loss approximation method and a new class of probabilistic classifiers. Our experimental results indicate that our generalized Beta-Bernoulli formulation is capable of yielding superior performance to traditional logistic regression and maximum margin linear SVMs for binary classification. Like other ramp like loss functions one of the principal advantages of our approach is that it is more robust dealing with outliers compared to traditional convex loss functions. Our modified SLA algorithm, which adds a learning hyper-parameter optimization step shows improved performance over the original SLA optimization algorithm in [14].\nWe have also presented and explored a kernelized version of our approach which yields performance competitive with non-linear SVMs for binary classification. Furthermore, with a Gaussian-Laplacian mixture prior on parameters our kernel BetaBernoulli model is able to yield sparser solutions than SVMs while retaining competitive classification performance. Interestingly, for an increase in training database size, our approach exhibited logarithmic scaling properties which compares favourably to the linear scaling properties of SVMs. To the best of our knowledge this is the first exploration of a Gauss-Laplace mixture prior for parameters \u2013 certainly in combination with our novel smooth zero-one loss formulation. The ability of this prior to behave like a smooth approximation to a counting prior is similar to an approach known as bridge regression in statistics. However, our mixture formulation has more flexibility compared to the simpler functional form of bridge regression. Interestingly, the combination of our generalized Beta-Bernoulli loss with a Gaussian-Laplacian parameter prior can be though of a smooth relaxation to learning with a zero one loss and an L0 counting prior or regularization \u2013 a formulation for classification that is intuitively attractive, but has remained elusive in practice until now.\nWe also tested our generalized Beta-Bernoulli models for a structured prediction task arising from the problem of face mining in Wikipedia biographies. Here also our model showed better performance than traditional logistic regression based approaches, both when they were tested as independent models, and when they were compared as sub-parts of a Bayesian network based structured prediction framework. This experiment shows signs that the model and optimization approach proposed here may have further potential to be used in complex structured prediction tasks."}, {"heading": "Acknowledgements", "text": "We thank the NSERC Discovery Grants program and Google for a Faculty Research Award which helped support this work."}, {"heading": "A Experimental Details", "text": "In the interests of reproducibility, we also list below the algorithm parameters and the recommended settings as given in [14] :\nrR = 2 \u22121, a search radius reduction factor; R0 = 8, the initial search radius; r = 2\n\u22121, a grid spacing reduction factor; S0 = 0.2, the initial grid spacing for 1-D search; r\u03b3 = 10, the gamma parameter reduction factor; \u03b3MIN = 2, the starting point for the search over \u03b3; \u03b3MAX = 200, the end point for the search over \u03b3.\nAs a part of the Range Optimization procedure there is also a standard gradient descent procedure using a slowly reduced learning rate. The procedure has the following specified and unspecified default values for the constants defined below:\nrG = 0.1, a learning rate reduction factor; rGMAX , the initial learning rate; rGMIN , the minimal learning rate; L, used for a while loop stopping criterion based on the smallest change in the like-\nlihood; G, used for outer stopping criterion based on magnitude of gradient"}, {"heading": "B Gradients for a Gaussian-Laplacian Mixture Prior", "text": "The gradient of the KBBLR likelihood is given in section 3.4. Below we provide the gradient of the log Gaussian-Laplace mixture prior or regularization term, R =\u2211 j ln(\u03c0gN (aj ; 0, \u03c3g) + \u03c0lL(aj ; 0, bl))\ndR dai = \u2211 i \u03c0g d dai N (ai; 0, \u03c3g) + \u03c0l ddaiL(ai; 0, bl) \u03c0gN (ai|0, \u03c3g) + \u03c0lL(ai|0, bl) (42)\nd\ndai N (ai; 0, \u03c3g) = \u2212\nai\n\u03c33g \u221a 2\u03c0\nexp(\u2212 a 2 i\n2\u03c32g ) (43)\nd\ndai L(ai; 0, bl) = \u2212\n1 2b2l exp(\u2212|ai| bl ) d dai (|ai|) (44)\nd dai (|ai|) =  1 if ai > 00 if ai == 0\u22121 if ai < 0"}, {"heading": "C Algorithm Modifications for Sparse Gauss-Laplace KBBLR", "text": "Algorithm 3 Range Optimization for Sparse KBBLR Input: w,X, t, \u03b3, radius R, step size S Output: Updated estimate for w\u2217, minimizing 0-1 loss.\n1: function GRAD-DESC-IN-RANGE-KBBLR(w,X, t, \u03b3, R, S) 2: repeat B Stage 1: Find local minimum 3: w\u2217 \u2190 VANILLA-GRAD-DESC(w) B Stage 2: Sparsify w 4: Assign initial near zero weights {wi} to Laplacian cluster, Cl, or to Gaus-\nsian cluster, Cg using posterior. 5: Estimate priors, {p(Cl(w)), p(Cg(w)} 6: repeat 7: {\u03c0g, \u03c0l, \u03c3g, bl} \u2190 hyper-parameter updates 8: w\u2217 \u2190 model updates using (41) 9: Estimate posteriors {p(Cl|w\u2217),p(Cg|w\u2217)}\n10: Update clusters, Cl and Cg 11: until No clusters change, or a finite # of iterations completed\nB Stage 3: Probe each dimension in a radius R B to escape local minimum (if possible)\n12: for i = 1 . . . d do . For each dimension, wi 13: for step \u2208 { S ,\u2212 S , 2 S ,\u22122 S , . . . , R,\u2212R} do 14: w\u2190 w\u2217 15: wi \u2190 wi + step 16: if L\u03b3(w\u2217)\u2212 L\u03b3(w) \u2265 L then 17: break . Goto step 3 18: end if 19: end for 20: end for 21: until L\u03b3(w\u2217)\u2212 L\u03b3(w) < L 22: return w\u2217 23: end function"}], "references": [{"title": "Trading convexity for scalability", "author": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou"], "venue": "Proceedings of the 23rd international conference on Machine learning, pages 201\u2013208. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning optimally sparse support vector machines", "author": ["A. Cotter", "S. Shalev-Shwartz", "N. Srebro"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 266\u2013274", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Tighter bounds for structured estimation", "author": ["C.B. Do", "Q. Le", "C.H. Teo", "O. Chapelle", "A. Smola"], "venue": "Proc. of NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Confidence-weighted linear classification", "author": ["M. Dredze", "K. Crammer", "F. Pereira"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 264\u2013271. ACM New York, NY, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Nonconvex online support vector machines", "author": ["S. Ertekin", "L. Bottou", "C.L. Giles"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(2):368\u2013 381", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Agnostic learning of monomials by halfspaces is hard", "author": ["V. Feldman", "V. Guruswami", "P. Raghavendra", "Y. Wu"], "venue": "SIAM Journal on Computing, 41(6):1558\u2013 1590", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Structured ramp loss minimization for machine translation", "author": ["K. Gimpel", "N.A. Smith"], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 221\u2013231. Association for Computational Linguistics", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Experiments on visual information extraction with the faces of wikipedia", "author": ["M.K. Hasan", "C. Pal"], "venue": "AAAI Conference on Artificial Intelligence (AI)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Sparse probabilistic classifiers", "author": ["R. H\u00e9rault", "Y. Grandvalet"], "venue": "Proceedings of the 24th international conference on Machine learning, pages 337\u2013344. ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "An automatic method of solving discrete programming problems", "author": ["A.H. Land", "A.G. Doig"], "venue": "Econometrica, 28(3):497\u2013520", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1960}, {"title": "Breast cancer diagnosis and prognosis via linear programming", "author": ["O.L. Mangasarian", "W.N. Street", "W.H. Wolberg"], "venue": "Operations Research, 43(4):570\u2013577", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "Boosting algorithms as gradient descent in function space", "author": ["L. Mason", "J. Baxter", "P. Bartlett", "M. Frean"], "venue": "NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Algorithms for direct 0\u20131 loss optimization in binary classification", "author": ["T. Nguyen", "S. Sanner"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 1085\u20131093", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Empirical risk minimization for support vector classifiers", "author": ["F. P\u00e9rez-Cruz", "A. Navia-V\u00e1zquez", "A.R. Figueiras-Vidal", "A. Artes-Rodriguez"], "venue": "Neural Networks, IEEE Transactions on, 14(2):296\u2013303", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Maximum margin bayesian network classifiers", "author": ["F. Pernkopf", "M. Wohlmayr", "S. Tschiatschek"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(3):521\u2013532", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparseness of support vector machines", "author": ["I. Steinwart"], "venue": "The Journal of Machine Learning Research, 4:1071\u20131105", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": "springer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Mod\u00e8les \u00e0 noyaux \u00e0 structure locale", "author": ["P. Vincent"], "venue": "Citeseer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Kernel matching pursuit", "author": ["P. Vincent", "Y. Bengio"], "venue": "Machine Learning, 48(1- 3):165\u2013187", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Evolutionary study of web spam: Webb spam corpus 2011 versus webb spam corpus 2006", "author": ["D. Wang", "D. Irani", "C. Pu"], "venue": "Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom), 2012 8th International Conference on, pages 40\u201349. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust truncated hinge loss support vector machines", "author": ["Y. Wu", "Y. Liu"], "venue": "Journal of the American Statistical Association, 102(479)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "The concave-convex procedure", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Neural Computation, 15(4):915\u2013936", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Text categorization based on regularized linear classification methods", "author": ["T. Zhang", "F.J. Oles"], "venue": "Information retrieval, 4(1):5\u201331", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Smoothing multivariate performance measures", "author": ["X. Zhang", "A. Saha", "S. Vishwanathan"], "venue": "Journal of Machine Learning Research, 10:1\u201355", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 16, "context": "In the classical statistical literature, this is known as Empirical Risk Minimization (ERM) [18], where learning is performed by minimizing the average risk or loss over", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "Of course, loss functions can be more complex, for example defined and learned through a linear combination of simpler basis loss functions [20], but we focus on the widely used losses above for now.", "startOffset": 140, "endOffset": 144}, {"referenceID": 5, "context": "While seemingly a sensible objective for a classification problem, empirical risk minimization with the 0-1 loss function is known to be an NP-hard problem [7].", "startOffset": 156, "endOffset": 159}, {"referenceID": 22, "context": "The zero-one loss captures the intuitive goal of simply minimizing classification errors and recent research has been directed to learning models using a smoothed zero-one loss approximation [24, 14].", "startOffset": 191, "endOffset": 199}, {"referenceID": 12, "context": "The zero-one loss captures the intuitive goal of simply minimizing classification errors and recent research has been directed to learning models using a smoothed zero-one loss approximation [24, 14].", "startOffset": 191, "endOffset": 199}, {"referenceID": 22, "context": "Previous work has shown that both the hinge loss [24] and more recently the 0-1 loss [14] can be efficiently and effectively optimized directly using smooth approximations.", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "Previous work has shown that both the hinge loss [24] and more recently the 0-1 loss [14] can be efficiently and effectively optimized directly using smooth approximations.", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "The work in [14] also underscored the robustness advantages of the 0-1 loss to outliers.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "While the 0-1 loss is not convex, the current flurry of activity in the area of deep neural networks as well as the award winning work on 0-1 loss approximations in [2] have highlighted numerous other advantages to the use of non-convex loss functions.", "startOffset": 165, "endOffset": 168}, {"referenceID": 22, "context": "The answer to our second question is indeed yes; and more specifically, to control the sharpness of our approximation, we use a \u03b3 factor reminiscent of a technique used in previous work which has created smooth approximations to the hinge loss [24] as well as smooth approximations of the 0-1 loss [14].", "startOffset": 244, "endOffset": 248}, {"referenceID": 12, "context": "The answer to our second question is indeed yes; and more specifically, to control the sharpness of our approximation, we use a \u03b3 factor reminiscent of a technique used in previous work which has created smooth approximations to the hinge loss [24] as well as smooth approximations of the 0-1 loss [14].", "startOffset": 298, "endOffset": 302}, {"referenceID": 8, "context": "In Figure 2, we show the probability given by the model as a function of wx at the right and the negative log probability or the loss on the left as \u03b3 is varied over the integer powers in the interval [0, 10].", "startOffset": 201, "endOffset": 208}, {"referenceID": 12, "context": "(2) A second key contribution of our work is that we present and explore an adapted version of the optimization algorithm proposed in [14] in which we optimize the meta parameters of learning using validation sets.", "startOffset": 134, "endOffset": 138}, {"referenceID": 12, "context": "We present a series of experiments in which we optimize the BB\u03b3 loss using the basic algorithm from [14] and our modified version.", "startOffset": 100, "endOffset": 104}, {"referenceID": 22, "context": "It has been shown in [24] that it is possible to define a generalized logistic loss and produce a smooth approximation to the hinge loss using the following formulation", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "The maximum margin Bayesian network formulation in [16] also employs a smooth differentiable hinge loss inspired by the Huber loss, having a similar shape tomin[1, zi].", "startOffset": 51, "endOffset": 55}, {"referenceID": 8, "context": "The sparse probabilistic classifier approach in [10] truncates the logistic loss leading to a sparse kernel logistic regression models.", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "[15] proposed a technique for learning support vector classifiers based on arbitrary loss functions composed of using the combination of a hyperbolic tangent loss function and a polynomial loss function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Other recent work [14] has created a smooth approximation to the 0-1 loss by directly defining the loss as a modified sigmoid.", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "Figure 3: (left) The way in which the generalized log logistic loss, Lglog proposed in [24] can approximate the hinge loss, Lhinge through translating the log logistic loss, Llog then increasing the \u03b3 factor.", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "(right) The way in a sigmoid function is used in [14] to directly approximate the 0-1 loss, L01.", "startOffset": 49, "endOffset": 53}, {"referenceID": 22, "context": "The approach also uses a similar \u03b3 factor to [24] and we show \u03b3 = 1, 2 and 32.", "startOffset": 45, "endOffset": 49}, {"referenceID": 12, "context": "Another important aspect of [14] is that they compared a variety of algorithms for directly optimizing the 0-1 loss with a novel algorithm for optimizing the sigmoid loss, Lsig(zi, \u03b3).", "startOffset": 28, "endOffset": 32}, {"referenceID": 9, "context": "The compared direct 0-1 loss optimization algorithms are: (1) a Branch and Bound (BnB) [11] technique, (2) a Prioritized Combinatorial Search (PCS) technique and (3) an algorithm referred to as a Combinatorial Search Approximation (CSA), both of which are presented in more detail in [14].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "The compared direct 0-1 loss optimization algorithms are: (1) a Branch and Bound (BnB) [11] technique, (2) a Prioritized Combinatorial Search (PCS) technique and (3) an algorithm referred to as a Combinatorial Search Approximation (CSA), both of which are presented in more detail in [14].", "startOffset": 284, "endOffset": 288}, {"referenceID": 12, "context": "To evaluate and compare the quality of the non-convex optimization results produced by the BnB, PCS and CSA, with their SLA algorithm for the sigmoid loss, [14] also presents training set errors for a number of standard evaluation datasets.", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "Furthermore, in [14], they also provide an analysis of the run-time performance for each of the algorithms.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "The award winning work of [2] produced an approximation to the 0-1 loss by creating a ramp loss, Lramp, obtained by combining the traditional hinge loss with a shifted and inverted hinge loss as illustrated in Figure 4.", "startOffset": 26, "endOffset": 29}, {"referenceID": 21, "context": "They showed how to optimize the ramp loss using the Concave-Convex Procedure (CCCP) of [23] and that this yields faster training times compared to traditional SVMs.", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "Other more recent work has proposed an alternative online SVM learning algorithm for the ramp loss [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 20, "context": "[22] explored", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Table 1: An excerpt from [14] of the total 0-1 loss for a variety of algorithms on some standard datasets.", "startOffset": 25, "endOffset": 29}, {"referenceID": 1, "context": "More recent work [3] has explored a similar ramp like construction which they refer to as the slant loss.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "Interestingly, the ramp loss formulation has also been generalized to structured predictions [4, 8].", "startOffset": 93, "endOffset": 99}, {"referenceID": 6, "context": "Interestingly, the ramp loss formulation has also been generalized to structured predictions [4, 8].", "startOffset": 93, "endOffset": 99}, {"referenceID": 0, "context": "Figure 4: The way in which shifted hinge losses are combined in [2] to produce the ramp loss, Lramp.", "startOffset": 64, "endOffset": 67}, {"referenceID": 11, "context": "There has been the activity of using zero-one loss like functional losses in machine learning, specially by the boosting [13] and neural network [19] communities.", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "There has been the activity of using zero-one loss like functional losses in machine learning, specially by the boosting [13] and neural network [19] communities.", "startOffset": 145, "endOffset": 149}, {"referenceID": 17, "context": "Vincent [19] analyzes that the loss defined through a functional of the hyperbolic tangent, 1\u2212 tanh(zi), is more robust as it doesn\u2019t penalize the outliers too excessively compared to other { log logistic loss, hinge loss, and squared loss } loss functions.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "A variant of this loss has been used in boosting algorithms [13].", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "Other work [19] has also shown that a hyperbolic tangent parametrized squared error loss, (0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "In this way our work is similar to that of [8] which explored the use of the ramp loss of [2] in the context of structured prediction for machine translation.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "In this way our work is similar to that of [8] which explored the use of the ramp loss of [2] in the context of structured prediction for machine translation.", "startOffset": 90, "endOffset": 93}, {"referenceID": 12, "context": "As we have discussed in the relevant recent work section above, the work of [14] has shown that their SLA algorithm applied to Lsig(zi, \u03b3) outperformed a number of other techniques in terms of both true 0-1 loss minimization performance and run time.", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "Recall that if one compares our generalized Beta-Bernoulli logistic loss with the directly defined sigmoidal loss used in the SLA work of [14], it becomes apparent that the BBLR formulation has three additional hyper-parameters, {\u03b1, \u03b2, w}.", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "In contrast, the plateaus of the sigmoidal loss in [14] are located at zero and one.", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "Here we present our meta-optimization extension and various other modifications to the SLA approach of [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "The SLA algorithm proposed in [14] can be decomposed into two different parts; an outer loop that initializes a model then enters a loop in which one slowly increases the \u03b3 factor of their sigmoidal loss, repeatedly calling an algorithm they refer to as Range Optimization for SLA or Gradient Descent in Range.", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "The first minor difference between the SLA optimization algorithm of [14] and our extension to it are the selection of the initial w0 that the SLA algorithm starts optimizing.", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "We use the suggested values in the original SLA algorithm [14] for the parameters rR, R0, r , and S0 .", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "Below, we present results for three different groups of benchmark problems: (1) a selection from the University of California Irvine (UCI) repository, (2) some larger and higher dimensionality text processing tasks from the LibSVM evaluation archive 1, and (3) the product review sentiment prediction datasets used in [5].", "startOffset": 318, "endOffset": 321}, {"referenceID": 12, "context": "We use these datasets in part so as to compare directly with results in [14], to understand the behaviour of our novel logistic function formulation and to explore the behavior of our learning parameter optimization procedure.", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "Dataset # Examples # Dimensions Description Breast 683 10 Breast Cancer Diagnosis [12] Heart 270 13 Statlog Liver 345 6 Liver Disorders Pima 768 8 Pima Indians Diabetes", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "To facilitate comparisons with previous results presented [14] such as those summarized in Table 3 of our literature review in Section 2, we provide a small set of initial experiments here following their experimental protocols.", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "In our experiments here we compare our BBLRs with the following models: our own L2 Logistic Regression (LR) implementation, a linear SVM - using the same implementation (liblinear) that was used in [14], and the optimization of the sigmoid loss, Lsig(zi, \u03b3) of [14] using the SLA algorithm and the code distributed on the web site associated with [14] (indicated by SLA in our tables).", "startOffset": 198, "endOffset": 202}, {"referenceID": 12, "context": "In our experiments here we compare our BBLRs with the following models: our own L2 Logistic Regression (LR) implementation, a linear SVM - using the same implementation (liblinear) that was used in [14], and the optimization of the sigmoid loss, Lsig(zi, \u03b3) of [14] using the SLA algorithm and the code distributed on the web site associated with [14] (indicated by SLA in our tables).", "startOffset": 261, "endOffset": 265}, {"referenceID": 12, "context": "In our experiments here we compare our BBLRs with the following models: our own L2 Logistic Regression (LR) implementation, a linear SVM - using the same implementation (liblinear) that was used in [14], and the optimization of the sigmoid loss, Lsig(zi, \u03b3) of [14] using the SLA algorithm and the code distributed on the web site associated with [14] (indicated by SLA in our tables).", "startOffset": 347, "endOffset": 351}, {"referenceID": 12, "context": "Despite the fact that we used the code distributed on the website associated with [14] we found that the SLA algorithm applied to their sigmoid loss, Lsig(zi, \u03b3) gave errors that are slightly higher than those given in [14].", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "Despite the fact that we used the code distributed on the website associated with [14] we found that the SLA algorithm applied to their sigmoid loss, Lsig(zi, \u03b3) gave errors that are slightly higher than those given in [14].", "startOffset": 219, "endOffset": 223}, {"referenceID": 12, "context": "We use the term SLA in Table 3 and subsequent tables to denote experiments performed using both the sigmoidal loss explored in [14] and their algorithm for minimizing it.", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "We therefore provide the next set of experiments using traditional training, validation and testing splits, again following the protocols used in [14]; however, as we shall soon see, these experiments underscored the importance of extending the original SLA algorithm to automate the adjustment of learning parameters.", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "The same observation was made in [14] and it motivated their own exploration of learning with noisy feature vectors.", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "In Table 6, we present the sum of the mean 0-1 loss over 10 repetitions of a 5 fold leave one out experiment where 10% noise has been added to the data following the protocol given in [14].", "startOffset": 184, "endOffset": 188}, {"referenceID": 12, "context": "However, the fact that the SLA approach failed to outperform the LR and SVM baselines in our experiments here; whereas in a similar experiment in [14] the SLA algorithm and sigmoidal loss did outperform these methods leads us to believe", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "Table 6: The sum of the mean 0-1 loss over 10 repetitions of a 5 fold leave one out experiment where 10% noise has been added to the data following the protocol given in [14].", "startOffset": 170, "endOffset": 174}, {"referenceID": 23, "context": "These datasets have predefined training and testing splits, which are distributed on the web site accompanying [25]2.", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "The webspam unigrams data originally came from the study in [21]4.", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "For this set of experiments, we used the count based unigram features for four databases from the website associated with [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "We present results in Table 12 using a ten fold cross validation setup as performed by [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "In [9], we performed a set of face mining experiments from Wikipedia biography pages using a technique that relies on probabilistic inference in a joint probability model.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "The bottom layer or set of random variables {X} in Figure 6 are used to encode these features, and we discuss the precise nature and definition of these features in more detail in [9].", "startOffset": 180, "endOffset": 183}, {"referenceID": 7, "context": "As mentioned above and discussed in more detail in [9], we used Maximum Entropy Models (MEMs) or Logistic Regression models for these local binary predictions working on multimedia features in our previous work.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "As shown in [2], one of the advantages of using the ramp loss for kernel based classification is that it can yield models that are even sparser than traditional SVMs based on the hinge loss.", "startOffset": 12, "endOffset": 15}, {"referenceID": 24, "context": "Recently the so called elastic net regularization approach [26] based on a weighted combination of L1 and L2 regularization has been shown more effective at encouraging sparsity with a less negative impact on performance.", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "Support vectors for SVMs increase almost linearly for an increase in the database size, an effect that has been confirmed in a number of other studies [17, 2].", "startOffset": 151, "endOffset": 158}, {"referenceID": 0, "context": "Support vectors for SVMs increase almost linearly for an increase in the database size, an effect that has been confirmed in a number of other studies [17, 2].", "startOffset": 151, "endOffset": 158}, {"referenceID": 12, "context": "Our modified SLA algorithm, which adds a learning hyper-parameter optimization step shows improved performance over the original SLA optimization algorithm in [14].", "startOffset": 159, "endOffset": 163}], "year": 2015, "abstractText": "We examine a new form of smooth approximation to the zero one loss in which learning is performed using a reformulation of the widely used logistic function. Our approach is based on using the posterior mean of a novel generalized BetaBernoulli formulation. This leads to a generalized logistic function that approximates the zero one loss, but retains a probabilistic formulation conferring a number of useful properties. The approach is easily generalized to kernel logistic regression and easily integrated into methods for structured prediction. We present experiments in which we learn such models using an optimization method consisting of a combination of gradient descent and coordinate descent using localized grid search so as to escape from local minima. Our experiments indicate that optimization quality is improved when learning meta-parameters are themselves optimized using a validation set. Our experiments show improved performance relative to widely used logistic and hinge loss methods on a wide variety of problems ranging from standard UC Irvine and libSVM evaluation datasets to product review predictions and a visual information extraction task. We observe that the approach: 1) is more robust to outliers compared to the logistic and hinge losses; 2) outperforms comparable logistic and max margin models on larger scale benchmark problems; 3) when combined with GaussianLaplacian mixture prior on parameters the kernelized version of our formulation yields sparser solutions than Support Vector Machine classifiers; and 4) when integrated into a probabilistic structured prediction technique our approach provides more accurate probabilities yielding improved inference and increasing information extraction performance.", "creator": "LaTeX with hyperref package"}}}