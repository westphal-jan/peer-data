{"id": "1511.05653", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Why are deep nets reversible: A simple theory, with implications for training", "abstract": "generative model approaches to enhancing learning are raising interest in the classroom concerning both better effectiveness although well be training environments requiring fewer labeled samples.", "histories": [["v1", "Wed, 18 Nov 2015 04:33:09 GMT  (31kb)", "http://arxiv.org/abs/1511.05653v1", null], ["v2", "Thu, 19 Nov 2015 23:48:36 GMT  (862kb,D)", "http://arxiv.org/abs/1511.05653v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sanjeev arora", "yingyu liang", "tengyu ma"], "accepted": false, "id": "1511.05653"}, "pdf": {"name": "1511.05653.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["arora@cs.princeton.edu", "yingyul@cs.princeton.edu", "tengyu@cs.princeton.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n05 65\n3v 1\n[ cs\n.L G\n] 1\n8 N\nov 2\nGenerative model approaches to deep learning are of interest in the quest for both better understanding as well as training methods requiring fewer labeled samples.\nRecent works use generative model approaches to produce the deep net\u2019s input given the value of a hidden layer several levels above. However, there is no accompanying \u201cproof of correctness,\u201d for the generative model, showing that the feedforward deep net is the correct inference method for recovering the hidden layer given the input. Furthermore, these models are complicated.\nThe current paper takes a more theoretical tack. It presents a very simple generative model for RELU deep nets, with the following characteristics: (a) The generative model is just the reverse of the feedforward net: if the forward transformation at a layer is A then the reverse transformation is AT . (This can be seen as an explanation of the old weight tying method for denoising autoencoders.) (b) Its correctness can be proven under a clean theoretical assumption: the edge weights in real-life deep nets behave like random numbers. Under this assumption \u2014which is experimentally tested on real-life nets like AlexNet\u2014 it is formally proved that feed forward net is a correct inference method for recovering the hidden layer. (c) The generative model suggests a simple modification for training\u2014use an input to produce several synthetic inputs with the same label, and include them in the backprop training. This appears to yield benefits similar to dropout, and can also be seen as a generative explanation for the efficacy of dropout."}, {"heading": "1 INTRODUCTION", "text": "Discriminative/generative pairs of models for classification tasks are an old theme in machine learning (Ng and Jordan (2001)) A good generative model analog for for deep learning may not only cast new light on the discriminative backpropagation algorithm, but also allow learning with fewer labeled samples. A seeming obstacle in this quest is that deep nets are successful in a variety of domains, and it is unlikely that problem inputs in these domains share common families of generative models.\nSome generic (i.e., not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al. (2006); Vincent et al. (2008). Surprisingly, these suggest that deep nets are reversible: the generative model is a essentially the feedforward net run in reverse. There does not seem to be an explanation for why such reversible nets fit problems in varied domains. The above models have since led to refinements such as Stacked Denoising Autoencoders Vincent et al. (2010), Generalized Denoising Auto-Encoders Bengio et al. (2013b) and Deep Generative Stochastic Networks Bengio et al. (2013a).\nIn case of image recognition it is possible to working harder \u2014using a custom deep net to invert the feedforward net \u2014-and reproduce the input very well from the values of hidden layers much higher up (e.g., (Mahendran and Vedaldi (2015))), and in fact to generate new images very different from any that were used to train the net.\nNow we formalize the problem of defining a generative model. Let x denote the data/input and let z denote the hidden representation (or the labels). The model has to satisfy the following: Property (a): Specify a joint distribution of x, z, or at least p(x|z). Property (b) A proof that the deep net itself is a method of computing the (most likely) z given x. As explained below, the past work doesn\u2019t satisfy both (a) and (b).\nThe current paper introduces a simple mathematical explanation for why such a model should exist for deep nets with fully connected layers. We propose the random-like nets hypothesis, which says that real-life deep nets \u2013even those obtained from standard supervised learning\u2014are \u201crandom-like,\u201d meaning their edge weights behave like random numbers. Note that this is distinct from saying that the edge weights actually are randomly generated or uncorrelated. Instead it says that the weighted graph has bulk properties similar to those of random weighted graphs. To give an example, it is believed that matrices in a host of practical settings display properties \u2014eg eigenvalue distribution\u2014 similar to matrices with Gaussian entries; this phenomenon as called Universality. This appears to be true of real-life deep nets as well.\nIf a deep net is random-like, we can show mathematically that it has an associated simple generative model (Property (a)) that we call the shadow distribution, and for which Property (b) also automatically holds in an approximate sense. This generative model is very reminiscent of denoising autoencoders. It is important to note that Properties (a), (b) hold even for random (and hence untrained) deep nets. The effect of training is to make the shadow distribution more like the real input distribution. The shadow distribution defined by the final net generates somewhat reasonable images, albeit cruder compared to say Mahendran and Vedaldi (2015)."}, {"heading": "1.1 RELATED WORK", "text": "Deep Boltzmann Machine( Hinton et al. (2006)) is an attempt to define a generative model in the above sense, and is related to the older notion of autoencoder. For a single layer, it posits a joint distribution of the observed layer x and the hidden layer h of the form exp(\u2212hTAx). This makes it reversible, in the sense that the conditional distributions x | h and h |x are both easy to sample from, using essentially the same distributional form and with shared parameters. Thus a single layer net indeed satisfies properties (a) and (b). To get a multilayer net, the DBN stacks such units. But as pointed out in Bengio (2009) this does not yield a generative model per se since one cannot ensure that the marginal distributions of two adjacent layers match. On the other hand, if one defines the generative model by stacking the conditional probability of RBM, then the reversibility of RBM is lost since the joint probability is no longer nice. Thus the model violates one of (a) or (b). We know of no prior solution to this issue. (In the current paper it is resolved under the random-like nets hypothesis.)\nIn (Lee et al. (2009b)) the RBM notion is extended to convolutional RBMs. In (Nair and Hinton (2010)), the theory of RBMs is extended to allow rectifier linear units, but this involves approximating RELU\u2019s with multiple binary units, which seems inefficient. (Our generative model below will sidestep this inefficient conversion to binary and work directly with RELUs in forward and backward direction.) Recently, a sequence of papers define hierarchichal probabilistic models (Ranganath et al. (2015); Kingma et al. (2014); Patel et al. (2015)) that are plausibly reminiscent of standard deep nets. Some such models can be used to model very lifelike distributions on documents or images, and thus plausibly satisfy Property (a). But they are not accompanied by any proof that the feedforward deep net solves the inference problem of recovering the top layer z, and thus don\u2019t satisfy (b).\nThe paper of (Arora et al. (2014)) defines a consistent generative model satisfying (a) and (b) under some restrictive conditions: the neural net edge weights are random numbers, and the connections satisfy some conditions on degrees, sparsity of each layer etc. However, the restrictive conditions limit its applicability to real-life nets. The current paper doesn\u2019t impose such restrictive conditions, but then loses provably efficient training \u2014-we use backpropagation, which has no proof of fast convergence.\nFinally, several works have tried to show that the deep nets can be inverted, such that the observable layer can be recovered from its representation at some very high hidden layer of the net Lee et al. (2009a). The recovery problem is solved in the recent pape Mahendran and Vedaldi (2015) also\nusing a deep net. While very interesting, this does not define a generative model (i.e., doesn\u2019t satisfy Property (a)).\nNotations: We use \u2016 \u00b7 \u2016 to denote the Euclidean norm of a vector and spectral norm of a matrix. The infinity norm of a vector x is denoted \u2016x\u2016\u221e, and \u2016x\u20160 denotes the number of non-zeros in vector x. We use [p] to denote the set of integers {1, . . . , p}. Our calculations will use asymptotic notation O(\u00b7), which assumes that parameters such as the size of network, the total number of nodes (denoted by N ), the sparsity parameters (e.g kj\u2019s introduced later) are all sufficiently large. (But the notation will not hide any impractically large constants.) Throughout, the \u201cwith high probability Event E happens\u201d means the failure probability is bounded by N\u2212c for every constant c, as N tends to infinity. Finally, O\u0303(\u00b7) notation hides terms that depend on logN ."}, {"heading": "807\u2013814, 2010.", "text": "Andrew Y. Ng and Michael I. Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada], pages 841\u2013848, 2001.\nA. B. Patel, T. Nguyen, and R. G. Baraniuk. A Probabilistic Theory of Deep Learning. ArXiv e-prints, 2015.\nRajesh Ranganath, Linpeng Tang, Laurent Charlin, and David M. Blei. Deep exponential families. In AISTATS, 2015.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, pages 1096\u20131103, 2008.\nPascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 11:3371\u20133408, 2010."}], "references": [{"title": "Provable bounds for learning some deep representations", "author": ["Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Arora et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2014}, {"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Yoshua Bengio", "Eric Thibodeau-Laufer", "Guillaume Alain", "Jason Yosinski"], "venue": "arXiv preprint arXiv:1306.1091,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Yoshua Bengio", "Li Yao", "Guillaume Alain", "Pascal Vincent"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Unsupervised learning of distributions on binary vectors using two layer networks", "author": ["Yoav Freund", "David Haussler"], "venue": "Technical report,", "citeRegEx": "Freund and Haussler.,? \\Q1994\\E", "shortCiteRegEx": "Freund and Haussler.", "year": 1994}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Semisupervised learning with deep generative models", "author": ["Diederik P. Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "Andrew Y. Ng"], "venue": "In ICML,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["Honglak Lee", "Roger Grosse", "Rajesh Ranganath", "Andrew Y Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Understanding deep image representations by inverting them", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Mahendran and Vedaldi.,? \\Q2015\\E", "shortCiteRegEx": "Mahendran and Vedaldi.", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes", "author": ["Andrew Y. Ng", "Michael I. Jordan"], "venue": "NIPS 2001, December", "citeRegEx": "Ng and Jordan.,? \\Q2001\\E", "shortCiteRegEx": "Ng and Jordan.", "year": 2001}, {"title": "A Probabilistic Theory of Deep Learning", "author": ["A.B. Patel", "T. Nguyen", "R.G. Baraniuk"], "venue": "ArXiv e-prints,", "citeRegEx": "Patel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Patel et al\\.", "year": 2015}, {"title": "Deep exponential families", "author": ["Rajesh Ranganath", "Linpeng Tang", "Laurent Charlin", "David M. Blei"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In ICML,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Discriminative/generative pairs of models for classification tasks are an old theme in machine learning (Ng and Jordan (2001)) A good generative model analog for for deep learning may not only cast new light on the discriminative backpropagation algorithm, but also allow learning with fewer labeled samples.", "startOffset": 105, "endOffset": 126}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al.", "startOffset": 105, "endOffset": 132}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al.", "startOffset": 105, "endOffset": 165}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al. (2006); Vincent et al.", "startOffset": 193, "endOffset": 214}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al. (2006); Vincent et al. (2008). Surprisingly, these suggest that deep nets are reversible: the generative model is a essentially the feedforward net run in reverse.", "startOffset": 193, "endOffset": 237}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al. (2006); Vincent et al. (2008). Surprisingly, these suggest that deep nets are reversible: the generative model is a essentially the feedforward net run in reverse. There does not seem to be an explanation for why such reversible nets fit problems in varied domains. The above models have since led to refinements such as Stacked Denoising Autoencoders Vincent et al. (2010), Generalized Denoising Auto-Encoders Bengio et al.", "startOffset": 193, "endOffset": 581}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al. (2006); Vincent et al. (2008). Surprisingly, these suggest that deep nets are reversible: the generative model is a essentially the feedforward net run in reverse. There does not seem to be an explanation for why such reversible nets fit problems in varied domains. The above models have since led to refinements such as Stacked Denoising Autoencoders Vincent et al. (2010), Generalized Denoising Auto-Encoders Bengio et al. (2013b) and Deep Generative Stochastic Networks Bengio et al.", "startOffset": 193, "endOffset": 640}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al. (2006); Vincent et al. (2008). Surprisingly, these suggest that deep nets are reversible: the generative model is a essentially the feedforward net run in reverse. There does not seem to be an explanation for why such reversible nets fit problems in varied domains. The above models have since led to refinements such as Stacked Denoising Autoencoders Vincent et al. (2010), Generalized Denoising Auto-Encoders Bengio et al. (2013b) and Deep Generative Stochastic Networks Bengio et al. (2013a). In case of image recognition it is possible to working harder \u2014using a custom deep net to invert the feedforward net \u2014-and reproduce the input very well from the values of hidden layers much higher up (e.", "startOffset": 193, "endOffset": 702}, {"referenceID": 1, "context": ", not tied to specific domain) approaches to defining such models include Restricted Boltzmann Machines (Freund and Haussler (1994); Hinton and Salakhutdinov (2006)) and Denoising Autoencoders Bengio et al. (2006); Vincent et al. (2008). Surprisingly, these suggest that deep nets are reversible: the generative model is a essentially the feedforward net run in reverse. There does not seem to be an explanation for why such reversible nets fit problems in varied domains. The above models have since led to refinements such as Stacked Denoising Autoencoders Vincent et al. (2010), Generalized Denoising Auto-Encoders Bengio et al. (2013b) and Deep Generative Stochastic Networks Bengio et al. (2013a). In case of image recognition it is possible to working harder \u2014using a custom deep net to invert the feedforward net \u2014-and reproduce the input very well from the values of hidden layers much higher up (e.g., (Mahendran and Vedaldi (2015))), and in fact to generate new images very different from any that were used to train the net.", "startOffset": 193, "endOffset": 941}, {"referenceID": 11, "context": "The shadow distribution defined by the final net generates somewhat reasonable images, albeit cruder compared to say Mahendran and Vedaldi (2015).", "startOffset": 117, "endOffset": 146}, {"referenceID": 5, "context": "Deep Boltzmann Machine( Hinton et al. (2006)) is an attempt to define a generative model in the above sense, and is related to the older notion of autoencoder.", "startOffset": 24, "endOffset": 45}, {"referenceID": 0, "context": "But as pointed out in Bengio (2009) this does not yield a generative model per se since one cannot ensure that the marginal distributions of two adjacent layers match.", "startOffset": 22, "endOffset": 36}, {"referenceID": 0, "context": "But as pointed out in Bengio (2009) this does not yield a generative model per se since one cannot ensure that the marginal distributions of two adjacent layers match. On the other hand, if one defines the generative model by stacking the conditional probability of RBM, then the reversibility of RBM is lost since the joint probability is no longer nice. Thus the model violates one of (a) or (b). We know of no prior solution to this issue. (In the current paper it is resolved under the random-like nets hypothesis.) In (Lee et al. (2009b)) the RBM notion is extended to convolutional RBMs.", "startOffset": 22, "endOffset": 543}, {"referenceID": 0, "context": "But as pointed out in Bengio (2009) this does not yield a generative model per se since one cannot ensure that the marginal distributions of two adjacent layers match. On the other hand, if one defines the generative model by stacking the conditional probability of RBM, then the reversibility of RBM is lost since the joint probability is no longer nice. Thus the model violates one of (a) or (b). We know of no prior solution to this issue. (In the current paper it is resolved under the random-like nets hypothesis.) In (Lee et al. (2009b)) the RBM notion is extended to convolutional RBMs. In (Nair and Hinton (2010)), the theory of RBMs is extended to allow rectifier linear units, but this involves approximating RELU\u2019s with multiple binary units, which seems inefficient.", "startOffset": 22, "endOffset": 621}, {"referenceID": 0, "context": "But as pointed out in Bengio (2009) this does not yield a generative model per se since one cannot ensure that the marginal distributions of two adjacent layers match. On the other hand, if one defines the generative model by stacking the conditional probability of RBM, then the reversibility of RBM is lost since the joint probability is no longer nice. Thus the model violates one of (a) or (b). We know of no prior solution to this issue. (In the current paper it is resolved under the random-like nets hypothesis.) In (Lee et al. (2009b)) the RBM notion is extended to convolutional RBMs. In (Nair and Hinton (2010)), the theory of RBMs is extended to allow rectifier linear units, but this involves approximating RELU\u2019s with multiple binary units, which seems inefficient. (Our generative model below will sidestep this inefficient conversion to binary and work directly with RELUs in forward and backward direction.) Recently, a sequence of papers define hierarchichal probabilistic models (Ranganath et al. (2015); Kingma et al.", "startOffset": 22, "endOffset": 1022}, {"referenceID": 0, "context": "But as pointed out in Bengio (2009) this does not yield a generative model per se since one cannot ensure that the marginal distributions of two adjacent layers match. On the other hand, if one defines the generative model by stacking the conditional probability of RBM, then the reversibility of RBM is lost since the joint probability is no longer nice. Thus the model violates one of (a) or (b). We know of no prior solution to this issue. (In the current paper it is resolved under the random-like nets hypothesis.) In (Lee et al. (2009b)) the RBM notion is extended to convolutional RBMs. In (Nair and Hinton (2010)), the theory of RBMs is extended to allow rectifier linear units, but this involves approximating RELU\u2019s with multiple binary units, which seems inefficient. (Our generative model below will sidestep this inefficient conversion to binary and work directly with RELUs in forward and backward direction.) Recently, a sequence of papers define hierarchichal probabilistic models (Ranganath et al. (2015); Kingma et al. (2014); Patel et al.", "startOffset": 22, "endOffset": 1044}, {"referenceID": 0, "context": "But as pointed out in Bengio (2009) this does not yield a generative model per se since one cannot ensure that the marginal distributions of two adjacent layers match. On the other hand, if one defines the generative model by stacking the conditional probability of RBM, then the reversibility of RBM is lost since the joint probability is no longer nice. Thus the model violates one of (a) or (b). We know of no prior solution to this issue. (In the current paper it is resolved under the random-like nets hypothesis.) In (Lee et al. (2009b)) the RBM notion is extended to convolutional RBMs. In (Nair and Hinton (2010)), the theory of RBMs is extended to allow rectifier linear units, but this involves approximating RELU\u2019s with multiple binary units, which seems inefficient. (Our generative model below will sidestep this inefficient conversion to binary and work directly with RELUs in forward and backward direction.) Recently, a sequence of papers define hierarchichal probabilistic models (Ranganath et al. (2015); Kingma et al. (2014); Patel et al. (2015)) that are plausibly reminiscent of standard deep nets.", "startOffset": 22, "endOffset": 1065}, {"referenceID": 0, "context": "The paper of (Arora et al. (2014)) defines a consistent generative model satisfying (a) and (b) under some restrictive conditions: the neural net edge weights are random numbers, and the connections satisfy some conditions on degrees, sparsity of each layer etc.", "startOffset": 14, "endOffset": 34}, {"referenceID": 9, "context": "Finally, several works have tried to show that the deep nets can be inverted, such that the observable layer can be recovered from its representation at some very high hidden layer of the net Lee et al. (2009a). The recovery problem is solved in the recent pape Mahendran and Vedaldi (2015) also", "startOffset": 192, "endOffset": 211}, {"referenceID": 9, "context": "Finally, several works have tried to show that the deep nets can be inverted, such that the observable layer can be recovered from its representation at some very high hidden layer of the net Lee et al. (2009a). The recovery problem is solved in the recent pape Mahendran and Vedaldi (2015) also", "startOffset": 192, "endOffset": 291}], "year": 2017, "abstractText": "Generative model approaches to deep learning are of interest in the quest for both better understanding as well as training methods requiring fewer labeled samples. Recent works use generative model approaches to produce the deep net\u2019s input given the value of a hidden layer several levels above. However, there is no accompanying \u201cproof of correctness,\u201d for the generative model, showing that the feedforward deep net is the correct inference method for recovering the hidden layer given the input. Furthermore, these models are complicated. The current paper takes a more theoretical tack. It presents a very simple generative model for RELU deep nets, with the following characteristics: (a) The generative model is just the reverse of the feedforward net: if the forward transformation at a layer is A then the reverse transformation is A . (This can be seen as an explanation of the old weight tying method for denoising autoencoders.) (b) Its correctness can be proven under a clean theoretical assumption: the edge weights in real-life deep nets behave like random numbers. Under this assumption \u2014which is experimentally tested on real-life nets like AlexNet\u2014 it is formally proved that feed forward net is a correct inference method for recovering the hidden layer. (c) The generative model suggests a simple modification for training\u2014use an input to produce several synthetic inputs with the same label, and include them in the backprop training. This appears to yield benefits similar to dropout, and can also be seen as a generative explanation for the efficacy of dropout.", "creator": "LaTeX with hyperref package"}}}