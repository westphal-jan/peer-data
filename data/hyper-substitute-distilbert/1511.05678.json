{"id": "1511.05678", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2015", "title": "Expressiveness of Rectifier Networks", "abstract": "rectified operator operators ( n ) in been shown to ameliorate the vanishing gradient problem, allow for efficient back - propagation, and ultimately promote estimation underlying the learned parameters. particular use has concerned most tip - of - the - thing results about a few nonlinear circuits. this approximation paradigm, mathematics characterize the expressiveness of binary networks. from this perspective, observing the sign ( threshold ) and input activations, parallel networks are lightly explored. we show first, does the geometric boundary via a sliding - layer learning lake be typically captured by a sign network, linear approximation kernel can collect locally exponentially larger value of hidden units. furthermore, we formulate the sufficient conditions for a binary logarithmic spectrum let the number of hidden units here represent per sign filter as under relu solution. fortunately, using synthetic filters, we here demonstrate that back propagation diagrams recover two naturally fuller relu networks fully predicted to the conjecture.", "histories": [["v1", "Wed, 18 Nov 2015 07:26:12 GMT  (136kb,D)", "http://arxiv.org/abs/1511.05678v1", null], ["v2", "Thu, 7 Jan 2016 18:53:11 GMT  (87kb,D)", "http://arxiv.org/abs/1511.05678v2", null], ["v3", "Fri, 27 May 2016 05:11:55 GMT  (96kb,D)", "http://arxiv.org/abs/1511.05678v3", "Published in ICML 2016. Supplementary material included"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xingyuan pan", "vivek srikumar"], "accepted": true, "id": "1511.05678"}, "pdf": {"name": "1511.05678.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["RECTIFIER NETWORKS", "Xingyuan Pan"], "emails": ["xpan@cs.utah.edu", "svivek@cs.utah.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "A neural network is characterized by its architecture, the choices of activation functions, and its parameters. We see several activation functions in the literature \u2013 the most common ones being threshold, logistic, hyperbolic tangent and rectified linear units (ReLUs). In recent years, deep neural networks with rectifing neurons \u2013 defined as R (x) = max(0, x) \u2013 have shown state-of-theart performance in several tasks such as image and speech classification (Glorot et al., 2011; Nair & Hinton, 2010; Krizhevsky et al., 2012; Maas et al., 2013; Zeiler et al., 2013, inter alia).\nReLUs possess several attractive computational properties. First, while deep networks with sigmoidal activation units suffer from the vanishing gradient problem (Bengio et al., 1994; Hochreiter, 1998), ReLU networks do not. Second, rectifying neurons encourage sparsity in the hidden layers (Glorot et al., 2011). Third, gradient back propagation is efficient because of the piece-wise linear nature of the function. For example, Krizhevsky et al. (2012) report convolutional neural network with ReLUs is six times faster than an equivalent one with hyperbolic tangent neurons. Finally, they have been empirically been shown to generalize very well.\nDespite these clear computational and empirical advantages, the expressiveness of rectifier units is less studied unlike sigmoid and threshold units. In this paper, we address the following question: Which Boolean functions do ReLU networks express? We analyze the expressiveness of shallow ReLU networks by characterizing their equivalent threshold networks. The goal of our analysis is to offer a formal understanding for the success of ReLUs. First, we show that two layer ReLU networks express Boolean functions that require an exponentially larger number of hidden-layer neurons if represented with threshold units (\u00a72). We use this characterization to define the sufficient conditions to compress an arbitrary threshold networks into a ReLU network with logarithmically fewer parameters (\u00a73). We then identify a relaxation of this condition if that is sufficient if we treat the hidden layer predictions as a multiclass classification problem, thus requiring equivalence of hidden layer states instead of the output state (\u00a74). Finally, we empirically test the predictions of our theory using synthetic data (\u00a75)."}, {"heading": "1.1 EXPRESSIVENESS OF NETWORKS: RELATED WORK", "text": "From the learning point of view, the selection of an activation function is driven by two interconnected aspects. The first is the expressiveness of the network using the activation function for a\nar X\niv :1\n51 1.\n05 67\n8v 1\n[ cs\n.L G\n] 1\n8 N\nov 2\n01 5\ngiven network architecture. The second is the computational complexity of learning. Though this work studies the first aspect, we briefly summarize prior work along both these lines.\nIt is known that any continuous function can be approximated to arbitrary accuracy with only one hidden layer of sigmoid units (Cybenko, 1989), leading to neural networks being called \u201cuniversal approximators\u201d. With two layers, even discontinuous functions can be represented. Similarly, two layer threshold networks are capable of representing any Boolean function. However, these are existence statements; for a general target function, the number of hidden units may be exponential in the input dimensionality.\nThere has been some recent work that looks at the expressivity of feed-forward ReLU networks. Because rectifier function is piece-wise linear, any network using only ReLUs can only represent piece-wise linear functions. Thus, the number of linear partitions of input space by the network can be viewed as a measure of is complexity. Pascanu et al. (2014) and Montufar et al. (2014) show that given the same number of ReLUs, a deep architecture can represent functions with exponentially more linear regions than a shallow architecture. More linear regions definitely indicate that more complex functions can be represented. However, this does not directly tell us how expressive a function is. This is because, at prediction time, we cannot directly correlate the number of regions to the way we make the prediction.\nThe learning complexity of neural networks using various activation functions has also been studied. It is known that for inputs from the Boolean hypercube, the two-layer networks with threshold activation functions is not efficiently learnable (Klivans & Sherstov, 2006; Daniely, 2014). Without restricting the weights, two layer networks with sigmoid or ReLU activations are also not efficiently learnable. The recent work of Livni et al. (2014) describe positive and negative learnability results for various activation functions."}, {"heading": "2 BOOLEAN FUNCTIONS EXPRESSED BY RECTIFIER NETWORKS", "text": "To simplify our analysis, in this paper, we primarily focus on shallow networks with one hidden layer with n units and a single binary output. In all cases, the hidden layer neurons are the object of study, while the output activation function is always the threshold function. In the rest of the paper, we use boldfaced letters to denote vectors. Input feature vectors and output binary labels are represented by x and y \u2208 {\u00b11} respectively. The number of hidden units is n. The weights and bias for the kth rectifier are uk and bk; the weights and bias for the kth sign units are vk and dk. The weights for the output unit are w1 through wn, and the bias for the output unit is w0."}, {"heading": "2.1 THRESHOLD NETWORKS", "text": "Before coming to the main results, we will first review the expressiveness of threshold networks. Assuming there are n hidden units and one output unit, the output of the network can be written as\ny = sgn ( w0 +\nn\u2211 k=1 wk sgn (vk \u00b7 x+ dk)\n) . (1)\nHere, both hidden and output activations are the sign function. Each hidden unit represents hyperplane (parameterized by vk and dk) that bisects the input space into two half spaces. By choosing different weights in the hidden layer we can obtain arbitrary arrangement of n hyperplanes. Theory of hyperplane arrangement (Zaslavsky, 1975) shows that for a general arrangement of n hyperplanes in d dimension, the space is divided into \u2211d s=0 ( n s ) regions. The output unit computes a linear combination of the hidden output (using the w\u2019s) and thresholds it. Thus, for various values of the w\u2019s, threshold networks can express intersections and unions of those regions. Figure 1 shows an example of the decision boundary of a two-layer network with three threshold units in the hidden layer."}, {"heading": "2.2 RECTIFIER NETWORKS", "text": "In this section, we will show that the decision boundary of every two-layer neural network with rectifier activations can be represented using a network with threshold activations with two or three layers. However, the number of threshold units is exponential, compared to the number of ReLUs.\nConsider a network with one hidden layer of n ReLUs, denoted by R (\u00b7). For a d dimensional input x, the output is computed y as the following binary function:\ny = sgn ( w0 +\nn\u2211 k=1 wkR (uk \u00b7 x+ bk)\n) . (2)\nHere uk and bk are weight and bias parameters for the ReLUs in the hidden layer, and the wk\u2019s parameterize the output unit. To simplify notation, we will use ak to denote the the pre-activation input of the kth hidden unit. That is, ak(x) = uk \u00b7 x + bk. This allows us to simplify the output as sgn ( w0 + \u2211 k\u2208[n] wkR (ak(x)) ) . Here, [n] is the set of positive integers not more than n. Note that even when not explicitly mentioned, each ak depends on the uk and the bk parameters.\nBy definition of the rectifier, for any real number c, we have cR (x) = sgn(x)R (|c|x). Thus, we can absorb |wk| into the rectifier function in Eq. (2) without losing generality. That is, other than w0, all the other output layer weights are only relevant up to sign because their magnitude can be absorbed into hidden layer weights. We can partition the hidden units into two sets P and N , depending on the sign of the corresponding wk. That is, let P = {k : k \u2208 [n] and wk = +1} and let N = {k : k \u2208 [n] and wk = \u22121}. We will refer to these partitions as the positive and negative hidden units respectively.\nThis lets us state the general form of two-layer ReLU networks as:\ny = sgn ( w0 +\n\u2211 k\u2208P R (ak(x))\u2212 \u2211 k\u2208N R (ak(x))\n) . (3)\nThe following two layer rectifier network will serve as our running example through the paper:\ny = sgn (w0 +R (a1(x))\u2212R (a2(x))\u2212R (a3(x))) . (4) This network consists of three ReLUs in the hidden layer, one of which positively affects the preactivation output and the other two decrease it. Hence, the set P = {1} and the set N = {2, 3}. Eq. 3 represents the general form of a two layer network with rectifier activation in the hidden layer. We can now state our main theorem that uses this representation to analyze the decision boundary of rectifier networks. (For brevity, we only state the result here; the proof is in appendix A) Theorem 1 (Main Theorem). Consider a two-layer rectifier network with n hidden units represented in its general form (Eq. (3)). Then, for any input x, the following are equivalent:\n1. The network classifies the example x as positive. 2. There exists a subset S1 of P such that, for every subset S2 of N , we have w0 +\u2211 k\u2208S1 ak(x)\u2212 \u2211 k\u2208S2 ak(x) \u2265 0.\n3. For every subset S2 of N , there exists a subset S1 of P such that w0 + \u2211 k\u2208S1 ak(x) \u2212\u2211\nk\u2208S2 ak(x) \u2265 0.\nBefore discussing its interpretation, let see how it applies to our running example in Eq. 4. In this example, P has two subsets: \u2205 and {1}, and N has four subsets: \u2205, {2}, {3} and {2, 3}. The first and second conditions of Theorem 1 indicate that the prediction is positive if, and only if, at least one of the following sets of conditions hold in entirety:\nEither  w0 \u2265 0, (with S1 = \u2205,S2 = \u2205) w0 \u2212 a2(x) \u2265 0, (with S1 = \u2205,S2 = {2}) w0 \u2212 a3(x) \u2265 0, (with S1 = \u2205,S2 = {3}) w0 \u2212 a2(x)\u2212 a3(x) \u2265 0. (with S1 = \u2205,S2 = {2, 3})\n(5)\nor\n w0 + a1(x) \u2265 0, (with S1 = {1},S2 = \u2205) w0 + a1(x)\u2212 a2(x) \u2265 0, (with S1 = {1},S2 = {2}) w0 + a1(x)\u2212 a3(x) \u2265 0, (with S1 = {1},S2 = {3}) w0 + a1(x)\u2212 a2(x)\u2212 a3(x) \u2265 0. (with S1 = {1},S2 = {2, 3})\n(6)\nHere each big left brace indicates a system of inequalities all of which should hold; thus essentially the conjunction of the individual inequalities contained within it. We can interpret of the subsets of P as certificates. In order for the output of Eq. 4 to be positive, we need at least one certificate S1 (one subset of P) such that for every subset S2 ofN , w0 + \u2211 k\u2208S1 ak(x)\u2212 \u2211 k\u2208S2 ak(x) \u2265 0. The two sets of inequalities show the choices of subsets of N for each of the two possible choices of S1 (i.e. either \u2205 or {1}). The above conditions represent a disjunction of conjunctions. Similarly, employing the first and third conditions of the theorem to our running example gives us the following condition: w0 \u2265 0, or w0 + a1(x) \u2265 0 w0 \u2212 a2(x) \u2265 0, or w0 + a1(x)\u2212 a2(x) \u2265 0 w0 \u2212 a3(x) \u2265 0, or w0 + a1(x)\u2212 a3(x) \u2265 0 w0 \u2212 a2(x)\u2212 a3(x) \u2265 0, or w0 + a1(x)\u2212 a2(x)\u2212 a3(x) \u2265 0. (7)\nNote that unlike the previous case, this gives us a condition that is a conjunction of disjunctions.\nDiscussion. The only difference between the second and the third conditions of the theorem is the order of the universal and existential quantifiers over the positive and negative hidden units, P andN respectively. More importantly, in both cases, the inequality condition over the subsets S1 and S2 is identical. Normally, swapping the order of the quantifiers does not give us an equivalent statement; but here, we see that doing so retains meaning because, in both cases, the output is positive for the corresponding input.\nFor any subset S1 of P and any subset S2 of N , we can write the inequality condition as a Boolean function BS1,S2 defined as:\nBS1,S2(x) =\n{ true, if w0 + \u2211 k\u2208S1 ak(x)\u2212 \u2211 k\u2208S2 ak(x) \u2265 0\nfalse, if w0 + \u2211 k\u2208S1 ak(x)\u2212 \u2211 k\u2208S2 ak(x) < 0\n(8)\nIf the sizes of the positive and negative subsets are n1 and n2 respectively (i.e, n1 = |P| and n2 = |N |), then we know that P has 2n1 subsets and N has 2n2 subsets. Thus, there are 2n1+n2 such Boolean functions. Then, by virtue of conditions 1 and 2 of theorem 1, we have1\ny = \u2228S1 [ \u2227S2BS1,S2(x) ] ,\nwhere \u2227S2 means conjunction over all different subset S2 of N , and \u2228S1 means disjunction over all different subset S1 of P . This expression is in the disjunctive normal form (DNF), where each conjunct contains 2n2 B\u2019s and there are 2n1 such terms. Since eachB simplifies into a hyperplane in the input space, this characterizes the decision boundary of the ReLU network as a DNF expression over these hyperplane decisions.\nSimilarly, by conditions 1 and 3, we have y = \u2227S2 [ \u2228S1BS1,S2(x) ] . This is in the conjunctive normal form (CNF), where each disjunctive clause contains 2n1 Boolean values and there are 2n2 such clauses.\nAn important corollary is that if the hidden layer units of the ReLU network are all exclusively positive (or negative), then the equivalent sign network is a pure disjunction (or conjunction).\n1We assume that we write y as a Boolean with y = 1 meaning true and y = \u22121 meaning false."}, {"heading": "3 TRANSFORMING BETWEEN RELU AND THRESHOLD NETWORKS", "text": "In this section, we will address two related questions. First, given an arbitrary ReLU network, can we construct an equivalent threshold network? Second, given an arbitrary threshold network how can we represent it using ReLU network?"}, {"heading": "3.1 FROM RELU TO THRESHOLD", "text": "Theorem 1 essentially gives us a constructive way to represent an arbitrary two layer ReLU network given in Eq. 3 as a three-layer threshold network. For every choice of the subsets S1 and S2 of the positive and negative units, we can define a Boolean function BS1,S2 as shown in Eq. 8. By definition, each of these is,a threshold unit, giving us 2n1+n2 threshold units in all. (Recall that n1 and n2 are the sizes of P and N respectively.) Since the decision function is a CNF or a DNF over these functions, it can be represented using a two-layer network over the B\u2019s, giving us three layers in all. We put all 2n1+n2 threshold units in the first hidden layer, separated into 2n1 groups, with each group comprising of 2n2 units.\nFigure 2 shows the threshold network corresponding to our running example from Eq. (4). For brevity, we use the notation Bi,j to represent the first hidden layer, with i and j indexing over the subsets of P and N respectively. The B\u2019s can be grouped into two groups, with units in each group sharing the same subset S1 but with different S2. Note that, these nodes are linear threshold units corresponding to the inequalities in in (5) and (6). In the second hidden layer, we have one threshold unit connected to each group of units in the layer below. The weight for each connection unit is 1 and the bias is 2n2 \u2212 1, effectively giving us a conjunction of the previous layer nodes. The second hidden layer has 2n1 such units. Finally, we have one unit in the output layer, with all weights being 1 and bias being 1\u2212 2n1 , simulating a disjunction of the decisions of the layer below. Note that, as discussed in the previous section, we can also construct a threshold networks using CNFs."}, {"heading": "3.2 FROM THRESHOLD TO RELU", "text": "The next question we want to answer is under what condition we can use ReLUs to represent the same decision boundary as a threshold network. In this section, we show a series of results that address various facets of this question. All proofs are in the appendices.\nFirst, with no restrictions in the number of ReLUs in the hidden layer, then we can always construct a rectifier network that is equivalent to a threshold network. In fact, we have the following lemma:\nLemma 1. Any threshold network with n units can be approximated to arbitrary accuracy by a rectifier network with 2n units.\nThe proof of this lemma is in Appendix B.\nGiven the exponential increase in the size of the threshold network to represent a ReLU network, a natural question is whether we can use only logarithmic number of ReLUs to represent a threshold network. In general, the following lemma points out that this is not possible.\nLemma 2. For a two-layer network with threshold activations, it is not always possible to construct an equivalent two-layer ReLU network with fewer number of hidden units.\nLemma 2 can be proved by just showing one example threshold network that cannot be represented by a ReLU network with fewer hidden units. In Appendix C, we provide such an example.\nDespite the negative result of lemma 2 in the general case, we can identify certain specific threshold networks that can be compressed into logarithmically smaller ones using ReLUs. Suppose we wish to compress a two layer threshold network with three sign hidden units into a ReLU network with dlog2 3e = 2 hidden units. The sign network can be represented by\ny = sgn (2 + sgn (v1 \u00b7 x+ d1) + sgn (v2 \u00b7 x+ d2) + sgn (v3 \u00b7 x+ d3))\nSuppose one of the weight vectors can be written as the linear combination of the other two but its bias can not. That is, for some p and q, if v3 = pv1 + qv2 and d3 6= pd1 + qd2. Then, we can construct the following equivalent ReLU network that is equivalent:\ny = sgn (\u22121 +R (u1 \u00b7 x+ b1) +R (u2 \u00b7 x+ b2))\nwhere r = 1\nd3 \u2212 pd1 \u2212 qd2 ,u1 = prv1,u2 = qrv2, b1 = prd1 + 1, and b2 = qrd2 + 1.\nThis equivalence can be proved by applying Theorem 1 to the constructed ReLU network. It shows that in two dimensions, we can use two ReLUs to represent three linearly independent sign units.\nWe can generalize this result to the case of a two-layer threshold network with 2n hidden threshold units that represents a disjunction over the hidden units. The goal is to find that under what condition we can use only n rectifier units to represent the same decision. To do so, we will use binary encoding matrix Tn of size n \u00d7 2n whose i\u2019th column is the binary representation of i \u2212 1. For example, the binary encoding matrix for n = 3 is given by T3,\nT3 = [ 0 0 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 ]\nLemma 3. Consider a two-layer threshold network with 2n threshold units in the hidden layer whose output represents a disjunction over the hidden units, i.e., the final output is positive if and only if at least one of the hidden-unit outputs is positive. That is,\ny = sgn ( 2n \u2212 1 +\n2n\u2211 k=1 sgn (vk \u00b7 x+ dk)\n) .\nThis decision can be represented using a two-layer rectifier network with n hidden units, if the weight parameters of the threshold units can be factored in the following form:[\nv1 \u00b7 \u00b7 \u00b7 v2n d1 \u00b7 \u00b7 \u00b7 d2n\n] = [ u1 \u00b7 \u00b7 \u00b7 un 0 b1 \u00b7 \u00b7 \u00b7 bn w0 ] [ Tn e2n ] (9)\nwhere e2n is a 2n dimensional row vector of all ones and 0 is a vector of all zeros.\nAppendix D gives the proof of this lemma and directly uses the main theorem. Note that this lemma only identifies sufficient conditions for the logarithmic reduction in network size. Identifying both necessary and sufficient conditions for such a reduction is an open research question."}, {"heading": "4 HIDDEN LAYER EQUIVALENCE", "text": "Lemma 3 studies a specific threshold network, where the output layer is a disjunction over the hidden layer units. For this network, we can define an additional notion of equivalence by studying the hidden layer activations. The hidden layer activations of this network can be interpreted as a specific kind of a multiclass classifier that either rejects inputs or labels them. If the output is negative, then clearly none of the hidden layer units are active and the input is rejected. If the output layer is positive, then at least one of the hidden layer units is active and the multiclass label is given by the maximum scoring hidden unit, namely argmaxk vk \u00b7 x+ dk. For threshold networks, the number of hidden units we need to learn is equal to the number of classes. The goal is to learn the same concept with rectifier units, hopefully with fewer rectifier units than the number of classes. Suppose a ReLU network has n hidden units, then its hidden layer prediction is the highest scoring hidden unit of the corresponding threshold network that has 2n hidden units. We now define hidden layer equivalence of two networks as follows: A threshold network and a ReLU network are equivalent if both their hidden layer predictions are identical.\nWe already know from lemma 3 that if the weight parameters of the true concept satisfy Eq. (9), then instead of learning 2n threshold units we can just learn n rectifier units. For simplicity, we write Eq. (9) as V = UT where\nV = [ v1 \u00b7 \u00b7 \u00b7 v2n d1 \u00b7 \u00b7 \u00b7 d2n ] U = [ u1 \u00b7 \u00b7 \u00b7 un 0 b1 \u00b7 \u00b7 \u00b7 bn w0 ] T = [ Tn e2n ] For simplicity of notation, we will assume that the input features x includes a constant bias feature in the last position. Thus, the vector V Tx represents the pre-activation score for each class.\nNow, we consider threshold networks with parameters such that there is no ReLU (defined by the matrix U ) that satisfies this condition. Instead, we find a rectifier network with parameters U that satisfies the following condition:\nU = argminU\u2016(V \u2212 UT )T \u2016\u221e, (10)\nHere \u2016 \u00b7 \u2016\u221e is the induced infinity norm, defined for any matrix A as \u2016A\u2016\u221e = supx6=0 \u2016Ax\u2016\u221e \u2016x\u2016\u221e .\nIf we have a matrix U such that V and UT are close in the sense of induced infinity norm, then we have the following about their equivalence.\nTheorem 2. If the true concept of a 2n-class classifier is given by a two-level threshold network in Eq. (3), then we can learn a two-layer rectifier network with only n hidden units of the form in Eq. (15) that is hidden layer equivalent to it, if for any example x, we have\n\u2016(V \u2212 UT )T \u2016\u221e \u2264 \u03b3(x)\n2\u2016x\u2016\u221e , (11)\nwhere \u03b3(x) is the multiclass margin for x, defined as the difference between its highest score and second-highest scoring classes.\nThe proof is given in appendix E. The proof is based on the intuition that for hidden layer equivalence, as defined above, only requires that the highest scoring label needs to be the same in the two networks rather than the actual values of the scores."}, {"heading": "5 EXPERIMENTS", "text": "We have seen that every two-layer rectifier network expresses the decision boundary of a three-layer threshold network. If the output weights of the former are all positive, then a two-layer threshold network is sufficient. (See the discussion at the end of \u00a72.2.) However, the fact that rectifier network can express the same decision boundary more compactly does not guarantee learnability. Specifically, in this section, we study the following question using synthetic data: Given a rectifier network and a threshold network with same decision boundary, can we learn one using the data generated from another using backpropagation?"}, {"heading": "5.1 DATA GENERATION", "text": "We use randomly constructed two-layer rectifier networks to generate labeled examples. To do so, we specify various values of the input dimensionality and the number of hidden ReLU units in the network. Once we have the network, we randomly generate the input points and label them using the network. Using generated data we try to recover both the rectifier network and the threshold network, with varying number of hidden units. We considered input dimensionalities 3, 10 and 50 and in each case, used 3 or 10 hidden units. This gave us six networks in all. For each network, we generated 10000 examples and 1500 of which are used as test examples."}, {"heading": "5.2 RESULTS AND ANALYSIS", "text": "For each dataset, we compare three different network architectures. The key parameter that varies across datasets is n, the number of hidden ReLU units in the network that generated the data. The first setting learns using a ReLU network with n hidden units. The second setting uses the activation function tanh(cx), which we call the compressed tanh activation. For large values of c, this effectively simulates the threshold function. In the second setting, the number of hidden units is still n. The final setting learns using the compressed tanh activation, but with 2n hidden units as suggested by the discussion in \u00a72.2.\nFigure 3 shows our results on the six datasets. These results verify several aspects of our theory. First, learning using ReLUs always succeed with low error, as shown in the left most bar of all six group. This is expected because, we know that our hypothesis class can express the true concept and training using backpropagation can successfully find it. Second, learning using compressed tanh with same number of units cannot recover the true concept, as shown in the middle bar of all six groups. This performance drop is as expected, since compressed tanh is just like sign activation, and we know in this case we need exponentially more hidden units. Third, the performances of learning using exponential number of compressed tanh are not as good as our expectations. In this case, from the analysis in \u00a72, we know the hypothesis can certainly express the true concept; yet learning does not always succeed. In fact, for the first three groups, where we have three ReLUs for data generation, the error for the learned classifier is rather large, suggesting that even though the true concept can be expressed, it is not found by backpropagation. We posit that this is due to non-convexity of the objective function. For the last three groups, where we have 10 hidden ReLUs for data geneartion, using exponential number of compressed tanh does achieve better performance."}, {"heading": "6 CONCLUSIONS", "text": "In this paper, we discuss the advantage of rectifier neural networks in terms of its expressiveness. Specifically, for binary classification we show that even though the decision boundary of two-layer rectifier network can be represented using threshold unit network, the number of threshold units required is exponential. Further, while a corresponding general logarithmic reduction of threshold units is not possible, for specific networks, we give sufficient conditions which we can reduce the threshold network to a much smaller rectifier network. Finally, we also present a relaxed condition where we can approximately recover a rectifier network that is hidden layer equivalent to an exponentially larger threshold network.\nOur work presents a natural next step: can we use the equivalence of the expressiveness results given in this paper to help us study the sample complexity of rectifier networks? Another open question is the generalization of these results to deep networks. Finally, from our experiments we see that expressiveness is not enough to guarantee learnability. Studying the interplay of expressiveness, sample complexity and the convexity properties of the training objective function for rectifier networks is another direction of future research."}, {"heading": "A PROOF OF THEOREM 1", "text": "Step 1: Equivalence of conditions 1 and 2\nLet us prove that condition 1 implies condition 2 first. Assume y > 0, we can construct a subset S1 of P S1 = {k : k \u2208 P and ak(x) \u2265 0}. The decision function gives us\n1 = sgn [ w0 + \u2211 k\u2208S1 ak(x)\u2212 \u2211 k\u2208N R(ak(x)) ]\nwhich means w0 + \u2211 k\u2208S1 ak(x) \u2265 \u2211 k\u2208N R(ak(x)).\nFor any subset S2 of N , we have\u2211 k\u2208N R(ak(x)) \u2265 \u2211 k\u2208S2 R(ak(x)) \u2265 \u2211 k\u2208S2 ak(x).\nTherefore for any subset S2 of N , w0 + \u2211 k\u2208S1 ak(x) \u2265 \u2211 k\u2208S2 ak(x).\nNow we need to that condition 2 implies condition 1. Assume there is a subset S1 of P such that for any subset S2 of N , w0 + \u2211 k\u2208S1 ak(x) \u2265 \u2211 k\u2208S2 ak(x). Let us define a specific subset S \u2032 2 of N ,\nS \u20322 = {k : k \u2208 N and ak(x) \u2265 0}.\nWe know w0 + \u2211 k\u2208P R(ak(x)) \u2265 w0 + \u2211 k\u2208S1 R(ak(x)) \u2265 w0 + \u2211 k\u2208S1 ak(x) \u2265 \u2211 k\u2208S\u20322 ak(x) = \u2211 k\u2208N R(ak(x))\nTherefore the decision function y in Eq. (3) is positive.\nStep 2: Equivalence of conditions 1 and 3\nThat condition 1 implies condition 3 holds by virtue of the first part of the previous step. We only need to prove that condition 3 implies 1 here. Assume for all subset S2 of N there is a subset S1 of P such that w0 + \u2211 k\u2208S1 ak(x)\u2212 \u2211 k\u2208S2 ak(x) \u2265 0. Let us define a specific subset S \u2032 2 of N ,\nS \u20322 = {k : k \u2208 N and ak(x) \u2265 0}.\nWe know w0 + \u2211 k\u2208P R(ak(x)) \u2265 w0 + \u2211 k\u2208S1 R(ak(x)) \u2265 w0 + \u2211 k\u2208S1 ak(x) \u2265 \u2211 k\u2208S\u20322 ak(x) = \u2211 k\u2208N R(ak(x))\nTherefore the decision function y in Eq. (3) is positive."}, {"heading": "B PROOF OF LEMMA 1", "text": "Consider a threshold unit with weight vector v and bias d, we have\nsgn(v \u00b7 x+ d) ' 1 [ R(v \u00b7 x+ d+ )\u2212R(v \u00b7 x+ d\u2212 ) ] \u2212 1.\nwhere is an arbitrary small number which determine the approximation accuracy."}, {"heading": "C PROOF OF LEMMA 2", "text": "In this appendix we provide a simple example of a two-layer threshold network, whose decision boundary cannot be represented by a two-layer rectifier network with fewer hidden units. Consider the case where we have three dimensional input vectors and three threshold units in the hidden layer. The decision function is given by\ny = sgn[2 + sgn(x1) + sgn(x2) + sgn(x3)], (12)\ni.e., the weight for each hidden units are the three basis vectors, the bias for each hidden units are all zero; the weights for the output unit are 1\u2019s and the bias is 2. It is easy to see that the decision function of this network is positive, if and only if at least one of x1, x2 and x3 is greater than or equal to zero. From a geometric point of view, one out of eight octant is labeled as negative and all the other seven octants are labeled as positive.\nSuppose we construct a two-layer rectifier network of the form\ny = sgn[w0 + w1 max(0,u1 \u00b7 x+ b1) + w2 max(0,u2 \u00b7 x+ b2)] (13)\nwith w1, w2 = \u00b11. From theorem 1 we know the decision boundary of Eq. 13 is determined by three plane equations:\nw0 + w1(u1 \u00b7 x+ b1) = 0 w0 + w2(u2 \u00b7 x+ b2) = 0\nw0 + w1(u1 \u00b7 x+ b1) + w2(u2 \u00b7 x+ b2) = 0\nThe sign of w1 and w2 determines how we combine the above three plane equations to form the decision boundary. Regardless how we combine them, if we want to use this rectifier network to represent the same decision boundary as the threshold network in Eq. 12, we should have w0 + w1b1 = 0 w0 + w2b2 = 0\nw0 + w1b1 + w2b2 = 0\n(14)\nwhich means w0 = b1 = b2 = 0. Now the three planes representing the decision boundary of the rectifier network can be simplified as\nu1 \u00b7 x = 0 u2 \u00b7 x = 0\n(u1 + u2) \u00b7 x = 0\nThese three planes cannot be perpendicular to each other, therefore it is impossible to use less than three ReLUs in the hidden layer to represent the decision boundary of Eq. 12."}, {"heading": "D PROOF OF LEMMA 3", "text": "If the weight parameters vk and dk can be written in the form as in Eq. (9), then we can construct the two-layer rectifier network,\ny = sgn [ w0 + n\u2211 k=1 R(uk \u00b7 x+ bk) ] . (15)\nThen by virtue of theorem 1, the decision boundary of the rectifier network in Eq. (15) is the same as the decision boundary of the threshold network in Eq. (3)."}, {"heading": "E PROOF OF THEOREM 2", "text": "Let us define = \u2016(V \u2212 UT )T \u2016\u221e \u2264 \u03b3(x)2\u2016x\u2016\u221e . From the definition of the L\u221e vector norm we have\n\u2016(V \u2212 UT )Tx\u2016\u221e \u2265 |((V \u2212 UT )Tx)k|\nfor all x and all k. The subscript k labels the kth comonent of the vector. From the definition of the induced norm we have \u2016(V \u2212 UT )Tx\u2016\u221e \u2264 \u2016x\u2016. Combining the above two inequalities we have\n|((UT )Tx)k \u2212 (V Tx)k| \u2264 \u2016x\u2016\nfor all x and all k. Specifically for k\u2217 we have\n(V Tx)k\u2217 \u2212 ((UT )Tx)k\u2217 \u2264 \u2016x\u2016, (16)\nand for any other k\u2032 6= k\u2217 we have\n((UT )Tx)k\u2032 \u2212 (V Tx)k\u2032 \u2264 \u2016x\u2016. (17)\nFrom the definition of the margin \u03b3(x) we also know\n(V Tx)k\u2217 \u2212 (V Tx)k\u2032 \u2265 \u03b3(x) \u2265 2 \u2016x\u2016. (18)\nCombining (16), (17) and (18) we have\n((UT )Tx)k\u2032 \u2264 ((UT )Tx)k\u2217\nwhich means if k\u2217 is the correct class with the highest score according to the weight parameters V , it will still be the highest scoring class according to the weight parameters UT , even if V 6= UT ."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Approximation by superpositions of a sigmoidal function", "author": ["G. Cybenko"], "venue": "Mathematics of Control, Signals, and Systems,", "citeRegEx": "Cybenko,? \\Q1989\\E", "shortCiteRegEx": "Cybenko", "year": 1989}, {"title": "From average case complexity to improper learning complexity", "author": ["Daniely", "Amit"], "venue": "In SToC,", "citeRegEx": "Daniely and Amit.,? \\Q2014\\E", "shortCiteRegEx": "Daniely and Amit.", "year": 2014}, {"title": "Deep Sparse Rectifier", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "Neural Networks. AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Hochreiter", "Sepp"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1998}, {"title": "Cryptographic hardness for learning intersections of halfspaces", "author": ["Klivans", "Adam R", "Sherstov", "Alexander a"], "venue": "In FOCS,", "citeRegEx": "Klivans et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Klivans et al\\.", "year": 2006}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "NIPS, pp", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "On the computational efficiency of training neural networks", "author": ["Livni", "Roi", "Shalev-Shwartz", "Shai", "Shamir", "Ohad"], "venue": "In NIPS, pp", "citeRegEx": "Livni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Livni et al\\.", "year": 2014}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Maas", "Andrew L", "Hannun", "Awni Y", "Ng", "Andrew Y"], "venue": "In ICML,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "On the Number of Linear Regions of Deep Neural Networks", "author": ["Montufar", "Guido", "Pascanu", "Razvan", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In NIPS, pp", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Rectified linear units improve Restricted Boltzmann Machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In ICML, pp", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "On the number of response regions of deep feedforward networks with piecewise linear activations", "author": ["Pascanu", "Razvan", "Montufar", "Guido", "Bengio", "Yoshua"], "venue": "In ICLR, pp", "citeRegEx": "Pascanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Facing up to arrangements: face-count formulas for partitions of space by hyperplanes", "author": ["Zaslavsky", "Thomas"], "venue": "American Mathematical Society,", "citeRegEx": "Zaslavsky and Thomas.,? \\Q1975\\E", "shortCiteRegEx": "Zaslavsky and Thomas.", "year": 1975}, {"title": "On rectified linear units for speech processing", "author": ["Zeiler", "Matthew D", "Ranzato", "Marc\u2019Aurelio", "Monga", "Rajat", "Mao", "Min", "Yang", "Kun", "Le", "Quoc Viet", "Nguyen", "Patrick", "Senior", "Alan", "Vanhoucke", "Vincent", "Dean", "Jeffrey"], "venue": "In ICASSP,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "First, while deep networks with sigmoidal activation units suffer from the vanishing gradient problem (Bengio et al., 1994; Hochreiter, 1998), ReLU networks do not.", "startOffset": 102, "endOffset": 141}, {"referenceID": 3, "context": "Second, rectifying neurons encourage sparsity in the hidden layers (Glorot et al., 2011).", "startOffset": 67, "endOffset": 88}, {"referenceID": 0, "context": "First, while deep networks with sigmoidal activation units suffer from the vanishing gradient problem (Bengio et al., 1994; Hochreiter, 1998), ReLU networks do not. Second, rectifying neurons encourage sparsity in the hidden layers (Glorot et al., 2011). Third, gradient back propagation is efficient because of the piece-wise linear nature of the function. For example, Krizhevsky et al. (2012) report convolutional neural network with ReLUs is six times faster than an equivalent one with hyperbolic tangent neurons.", "startOffset": 103, "endOffset": 396}, {"referenceID": 1, "context": "It is known that any continuous function can be approximated to arbitrary accuracy with only one hidden layer of sigmoid units (Cybenko, 1989), leading to neural networks being called \u201cuniversal approximators\u201d.", "startOffset": 127, "endOffset": 142}, {"referenceID": 1, "context": "It is known that any continuous function can be approximated to arbitrary accuracy with only one hidden layer of sigmoid units (Cybenko, 1989), leading to neural networks being called \u201cuniversal approximators\u201d. With two layers, even discontinuous functions can be represented. Similarly, two layer threshold networks are capable of representing any Boolean function. However, these are existence statements; for a general target function, the number of hidden units may be exponential in the input dimensionality. There has been some recent work that looks at the expressivity of feed-forward ReLU networks. Because rectifier function is piece-wise linear, any network using only ReLUs can only represent piece-wise linear functions. Thus, the number of linear partitions of input space by the network can be viewed as a measure of is complexity. Pascanu et al. (2014) and Montufar et al.", "startOffset": 128, "endOffset": 869}, {"referenceID": 1, "context": "It is known that any continuous function can be approximated to arbitrary accuracy with only one hidden layer of sigmoid units (Cybenko, 1989), leading to neural networks being called \u201cuniversal approximators\u201d. With two layers, even discontinuous functions can be represented. Similarly, two layer threshold networks are capable of representing any Boolean function. However, these are existence statements; for a general target function, the number of hidden units may be exponential in the input dimensionality. There has been some recent work that looks at the expressivity of feed-forward ReLU networks. Because rectifier function is piece-wise linear, any network using only ReLUs can only represent piece-wise linear functions. Thus, the number of linear partitions of input space by the network can be viewed as a measure of is complexity. Pascanu et al. (2014) and Montufar et al. (2014) show that given the same number of ReLUs, a deep architecture can represent functions with exponentially more linear regions than a shallow architecture.", "startOffset": 128, "endOffset": 896}, {"referenceID": 1, "context": "It is known that any continuous function can be approximated to arbitrary accuracy with only one hidden layer of sigmoid units (Cybenko, 1989), leading to neural networks being called \u201cuniversal approximators\u201d. With two layers, even discontinuous functions can be represented. Similarly, two layer threshold networks are capable of representing any Boolean function. However, these are existence statements; for a general target function, the number of hidden units may be exponential in the input dimensionality. There has been some recent work that looks at the expressivity of feed-forward ReLU networks. Because rectifier function is piece-wise linear, any network using only ReLUs can only represent piece-wise linear functions. Thus, the number of linear partitions of input space by the network can be viewed as a measure of is complexity. Pascanu et al. (2014) and Montufar et al. (2014) show that given the same number of ReLUs, a deep architecture can represent functions with exponentially more linear regions than a shallow architecture. More linear regions definitely indicate that more complex functions can be represented. However, this does not directly tell us how expressive a function is. This is because, at prediction time, we cannot directly correlate the number of regions to the way we make the prediction. The learning complexity of neural networks using various activation functions has also been studied. It is known that for inputs from the Boolean hypercube, the two-layer networks with threshold activation functions is not efficiently learnable (Klivans & Sherstov, 2006; Daniely, 2014). Without restricting the weights, two layer networks with sigmoid or ReLU activations are also not efficiently learnable. The recent work of Livni et al. (2014) describe positive and negative learnability results for various activation functions.", "startOffset": 128, "endOffset": 1779}], "year": 2017, "abstractText": "Rectified Linear Units (ReLUs) have been shown to ameliorate the vanishing gradient problem, allow for efficient back-propagation, and empirically promote sparsity in the learned parameters. Their use has led to state-of-the-art results in a variety of applications. In this paper, we characterize the expressiveness of ReLU networks. From this perspective, unlike the sign (threshold) and sigmoid activations, ReLU networks are less explored. We show that, while the decision boundary of a two-layer ReLU network can be captured by a sign network, the sign network can require an exponentially larger number of hidden units. Furthermore, we formulate the sufficient conditions for a corresponding logarithmic reduction in the number of hidden units to represent a sign network as a ReLU network. Finally, using synthetic data, we experimentally demonstrate that back propagation can recover the much smaller ReLU networks as predicted by the theory.", "creator": "LaTeX with hyperref package"}}}