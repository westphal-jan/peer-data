{"id": "1511.06394", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Geodesics of learned representations", "abstract": "mri develop its proposed recipe for visualizing wherein refining conceptual approaches are learned representations. given two reference poses ( hence, grouped by spinal transformation ), we synthesize a sequence of faces lying on a groove between them subsequently becomes roughly minimal length outside the space frame a sensor ( then \" representational geodesic \" ). where anatomical transformation relating the two real subjects is an actual canonical neural representation, this relation should follow the gradual description of this transformation. we use this method to assess numerical invariances unlike green - or - the - art optical classification networks and find \u2013 surprisingly, they usually not express invariance on abstract parametric transformations of translation, rotation, coordinate dilation. our resolution also suggests different remedy across numerical failures, and following this prescription, we demonstrate that the modified representation deserves a quite deal of entropy for your range of geometric statistical representation.", "histories": [["v1", "Thu, 19 Nov 2015 21:40:13 GMT  (1059kb,D)", "https://arxiv.org/abs/1511.06394v1", "Submitted to ICLR2016"], ["v2", "Thu, 7 Jan 2016 21:10:58 GMT  (1059kb,D)", "http://arxiv.org/abs/1511.06394v2", "Response to first set of reviews for ICLR2016"], ["v3", "Tue, 19 Jan 2016 21:05:40 GMT  (981kb,D)", "http://arxiv.org/abs/1511.06394v3", "Final response to reviews for ICLR2016"], ["v4", "Mon, 22 Feb 2016 17:42:25 GMT  (981kb,D)", "http://arxiv.org/abs/1511.06394v4", "Published as a conference paper at ICLR 2016"]], "COMMENTS": "Submitted to ICLR2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["olivier j h\\'enaff", "eero p simoncelli"], "accepted": true, "id": "1511.06394"}, "pdf": {"name": "1511.06394.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["eero}@cns.nyu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "A fundamental requirement of pattern recognition is the ability to ignore irrelevant variations in the input (Duda et al., 2001). Most visual recognition problems are thwarted by variations in position, size, pose, lighting, and other viewing conditions that can bring objects from different classes closer, while increasing within-class variability (DiCarlo & Cox, 2007), and the construction of representations that are invariant to these variations remains an active area of research. Recent examples of learned visual representations have proven highly effective for recognition (Krizhevsky et al., 2012), but a precise understanding of exactly what they represent remains elusive. And although these representations are hypothesized to be invariant to various identity-preserving deformations, apart from a few exceptions, these claims are rarely tested directly (Kavukcuoglu et al., 2009).\nImage synthesis provides a powerful methodology for examining the invariances of arbitrary representations. It has been used to explore and refine texture models, incrementally augmenting the representation with new statistical constraints until images synthesized with matching parameters are indistinguishable to human observers (Portilla & Simoncelli, 2000). When applied to deep recognition networks, synthesis has revealed failures in the form of \u201cadversarial examples\u201d: images that appear entirely different to a human observer, and yet are identified by the network as belonging to the same category (Szegedy et al., 2013). In these cases, samples from the equivalence class of images that map to the same representation vector provide a means of verifying or falsifying the hypothesis that the invariances of the representation are also invariances for human observers.\nBut the synthesis test, in which human observers try to discriminate synthesized images, is onesided: failures (i.e. visually distinct images) can reveal inappropriate invariances of a representation, but successes can mask a lack of desired invariances. Consider the standard case of translationinvariance. The Fourier amplitude spectrum (i.e., the set of magnitudes of Fourier transform coefficients) provides a well-known example of a translation-invariant representation, but it is invariant to far more than translations, and this is immediately revealed by a synthesis test (figure 1, top right). On the other hand, simply representing an image with its raw pixel values (the identity representa-\nar X\niv :1\n51 1.\n06 39\n4v 4\n[ cs\n.C V\n] 2\n2 Fe\nb 20\n16\ntion) will trivially produce visually perfect synthetic examples (figure 1, top center) despite the fact that it has no invariance properties at all.\nWe seek a more general method of evaluation that penalizes a model for discarding too much information (as with synthesis) but also for discarding too little information. Each of these failures can be seen as an inadequacy of the image metric induced by the representation. Specifically, an image representation deforms the input space, bringing some images closer to each other while spreading others out, and thus inducing a new metric in image space. We can expose properties of this image metric by generating a geodesic sequence of images. Specifically, given an initial and final image, we synthesize a sequence of images that follow a minimal-length path in the response space of the representation. In the absence of any other constraints, this path will be a straight line connecting the representations of the two images; more generally, it will be the straightest path connecting the two points. In the case where the two images differ by a simple transformation (e.g. a translation, figure 1, left column) that is not linearized by the representation (i.e. mapped to the straightest path connecting the two representations), the geodesic will differ from the original transformation connecting the images (figure 1, middle column). Similarly, if the representation is invariant to many\ntransformations, the geodesic may correspond to a path that uses a mixture of transformations, and thus differ from the ground truth path (figure 1, right column). As a result, by visualizing whether a representation has linearized the action of various deformations, representational geodesics can reveal both excessive and insufficient invariance in an image model.\nWe develop an algorithm for synthesizing geodesic sequences for a representation, and use it to examine whether learned representations linearize various real-world transformations such as translation, rotation, and dilation. We find that a current state-of-the-art object recognition network fails to linearize these basic transformations. However, these failures point to a deficiency in the representation, leading to a simple way of improving it. We show that the improved representation is able to linearize a range of parametric transformations as well as generic distortions found in natural image sequences."}, {"heading": "2 SYNTHESIZING GEODESIC SEQUENCES", "text": "Suppose we have an image representation, y = f(x), where x is the vector of image pixel intensities and f(\u00b7) a continuous function that maps it to an abstract vector-valued representation y (e.g. the responses of an intermediate stage of a hierarchical neural network). Given initial and final images, we wish to synthesize a sequence of images that lies along the path of minimal length in the representation space (a representational geodesic). If the mapping is many-to-one (as is usually the case), this sequence of images is not unique. We resolve this ambiguity by selecting the representational geodesic that is also of minimal length in the space of images (i.e., a conditional geodesic in image space)."}, {"heading": "2.1 OBJECTIVE FUNCTION", "text": "In order to generate such a sequence, we optimize an objective function that expresses a discrete approximation of the problem, directly in terms of images sampled along the path. Given a desired sequence length N and initial and final images, {x0, xN}, we wish to synthesize a sequence of images, \u03b3 = {xn;n = 0 . . . N}, lying along a geodesic in representation space. The representational path length is\nL[f(\u03b3)] = N\u2211 n=1 \u2016f(xn)\u2212 f(xn\u22121)\u20162\nwhich is bounded by the representational energy\nE[f(\u03b3)] = N\u2211 n=1 \u2016f(xn)\u2212 f(xn\u22121)\u201622\nthanks to the Cauchy-Schwartz inequality\nL[f(\u03b3)]2 \u2264 NE[f(\u03b3)]\nwith equality if and only if the representations are equispaced, which is encouraged by minimizing the representational energy. As a result, a path that meets this condition (e.g. the red curve in figure 2) while minimizing the representational energy E[f(\u03b3)] is a representational geodesic.\nWhen the mapping to representation space is many-to-one, there are many possible solutions to this problem. To uniquely constrain the solution, we define an analogous energy term that ensures that this path is also of minimal length in the image domain\nE[\u03b3] = N\u2211 n=1 \u2016xn \u2212 xn\u22121\u201622\nSince we are looking for the shortest path in image space that is also a geodesic in representation space, we minimize E[\u03b3] conditioned on the path also minimizing E[f(\u03b3)]. Furthermore, during the optimization we constrain image pixel intensities to the [0, 1] range."}, {"heading": "2.2 OPTIMIZATION", "text": "We optimize this objective in three steps. First, we initialize the path with the minimum of E[\u03b3], which is simply a sequence of images that are linearly interpolated between the initial and final images. Next we minimize the representational geodesic objective E[f(\u03b3)]. Finally, we minimize the image-domain geodesic objective, conditioned on staying in the set of representational geodesics.\nMinimizing the representational geodesic objective in the second step requires optimizing an image for its representation via a non-linear function, and thus shares much of the non-convexity found in training deep neural networks. In particular, the curvature of the energy surface can vary widely over the course of the optimization. For this reason, we used the Adam optimization method (Kingma & Ba, 2014), which scales gradients by a running estimate of their variance, providing robustness to these changes in the energy landscape. We run Adam, using the default parameters, for 104 iterations to ensure that we reach the minimum of the representational geodesic cost.\nTo optimize the image-domain geodesic objective while constraining the solution to remain in the set of representational geodesics, we start by computing a descent direction for the image-domain geodesic objective. We then project out the component of this direction that lies along the gradient of the representational geodesic objective. We take a step in that direction, then project back onto the set of representational geodesics by re-minimizing the representational geodesic cost (again using Adam), and repeat until convergence. We summarize our method with the following algorithm.\nConditional geodesic computation\nRequire: f : continuous mapping Require: x0, xN : initial and final images Require: N : number of steps along geodesic path (N = 10 in all our experiments) Require: \u03bb: gradient descent step size Ensure: \u03b3 = {xn;n = 0 . . . N} minimizes E[\u03b3] conditioned on minimizing E[f(\u03b3)] xn \u2190 N\u2212nN x0 + n N xN n \u2208 J0, 1, . . . NK initialize with pixel-based interpolation\nminimize E[f(\u03b3)] project onto set of representational geodesics while \u03b3 has not converged do dr \u2190 \u2207\u03b3E[f(\u03b3)] dp \u2190 \u2207\u03b3E[\u03b3] d\u0302p \u2190 dp \u2212 <dr,dp>\u2016dr\u201622 dr project out representational gradient\n\u03b3 \u2190 \u03b3 \u2212 \u03bbd\u0302p minimize E[f(\u03b3)] re-project onto set of representational geodesics\nend while return \u03b3\nDespite the non-convexity of the problem, we have good reason to believe that solving this optimization problem should be feasible for trained neural networks. Since the output of the first layer is equal to the convolution of the input image with a filter bank, our problem is similar in complexity to optimizing the weights of the first layer of a network, for the same objective. Recent theoretical work shows that optimizing all layers of a network jointly makes the problem significantly more difficult than optimizing a single layer in isolation (Saxe et al., 2013). Hence optimizing E[f(\u03b3)] should be easier than training the full network for recognition. In practice we were able to solve the optimization problem for a variety of deep networks.\nIt should be noted that if the mapping f(\u00b7) is not surjective, not all vectors in the representation space are attainable from an input image. Specifically, if the mapping is non-linear (as for most representations of interest) the set of attainable vectors is non-convex, and vectors lying along the straight line connecting two representations are not necessarily attainable. As such, we can only expect to find a geodesic path whose representation is as close as possible to this straight line by minimizing the representational geodesic cost E[f(\u03b3)]. Figure 2 shows an example of this, for the case of image\ndi st\nan ce\nfr om\nre pr\nes en\nta tio\nn lin\ne\nprojection on representation line\ngeodesic\nground truth\npixel fade\nFigure 2: Deviation from the straight line connecting the representations of a pair of images, for different paths in representation space. Due to the non-linearity of the representation (the third stage of L2 pooling of a deep neural network, see section 3) the geodesic deviates slightly from the straight line. The ground truth transformation (here, a translation) deviates similarly, indicating that the representation has linearized the transformation to a large extent. For reference, a pixel-based interpolation deviates significantly more from a straight line. Axes are in the same units, normalized by the distance separating the end point representations. Knots along each curve indicate samples used to compute the path.\ntranslation. By construction, the geodesic is closer to a straight line in representation space than either the ground truth transformation or a pixel interpolation. The ground truth transformation lies close to the geodesic, indicating that this representation has almost (but not completely) linearized this transformation. The differences between these two paths can be made explicit by visualizing the geodesic sequence, as detailed in the following section."}, {"heading": "3 VISUALIZING GEODESIC SEQUENCES", "text": "We used our geodesic framework to examine the invariance properties of the 16-layer VGG network (Simonyan & Zisserman, 2014), which we chose for its conceptual simplicity and strong performance on object recognition benchmarks. As a \u201crepresentation\u201d for our tests, we used the output of the third stage of pooling. Each stage of this continuous non-linear mapping is constructed as a composition of three elementary operations: linear filtering, half-wave rectification, and max pooling (which summarizes a local region with its maximum). We followed the preprocessing steps described in the original work: images are rescaled to the [0, 255] range, color channels are permuted from RGB to BGR, and the mean BGR pixel value, [104, 117, 124], is subtracted. We verified that our implementation could replicate the published object recognition results."}, {"heading": "3.1 GEODESICS AS A DIAGNOSTIC TOOL", "text": "We first examined whether this representation linearizes basic geometric transformations: translation, rotation and dilation. To do so, we compute the geodesic sequence between two images that differ by one of these transformations, and compare it to the ground truth sequence obtained by incremental application of the same transformation. The extent of the overall transformation determines the difficulty of this task: all representations (even trivial ones) will produce geodesics that are close to the ground truth for very small transformations, whereas all are likely to fail for very large transformations. For our discriminative test we chose intermediate values: an 8 pixel translation, a 4\u25e6 rotation, and a 10% dilation.\nWe found that the VGG network, despite its impressive classification performance, failed to linearize these simple geometric deformations and produced geodesics with salient aliasing artifacts (figure 3, middle column). Given that no subsampling is used in the convolutional layers, we attributed this failure to the max pooling layers, which subsample the representation by a factor of 2 in each direction, despite their small spatial extent (a 2\u00d72 pooling region). To avoid aliasing artifacts when subsampling by a factor of 2, the Nyquist theorem requires blurring with a filter whose cutoff frequency is below \u03c02 . Following this indication, we replaced the max pooling layers with L2 pooling:\nL2(x) = \u221a g \u2217 x2\nwhere the squaring and square-root operations are point-wise, and the blurring kernel g(\u00b7) is chosen as a 6\u00d76 pixel Hanning window that approximately enforces the Nyquist criterion. This type of pooling is often used to describe the behavior of neurons in primary visual cortex (Vintch et al.,\n2015), and also bears resemblance to the complex modulus used in the \u201cscattering transform\u201d (Mallat, 2011) which has been shown to be robust to smooth deformations.\nWe found that this modified VGG network not only produced geodesic sequences that were free of most aliasing artifacts, but also linearized these geometric transformations convincingly, as can be seen in the temporal slices of the geodesic (figure 3, right column). This confirms that, as with the Fourier magnitude and the scattering transform, smooth, quadratic pooling operators are able to linearize local deformations. Unlike the Fourier magnitude however, the locality and hierarchical nature of these representations tailors their invariances to a much more limited set of transformations. Furthermore, this demonstrates the power of geodesics as a visualization tool for understanding learned representations. Not only does this diagnostic report a deficiency of a representation (figure 3, middle column), it also points to the mechanism of this failure, suggesting a simple way to improve the model.\nThis suggests that the VGG network\u2019s performance on object recognition tasks could be improved by substituting max pooling with L2 pooling, and retraining the network to decode this new representation. Indeed, the added invariance of this representation could enable the network to generalize to new viewing conditions more robustly."}, {"heading": "3.2 DISAMBIGUATING SPATIAL SCALE AND NONLINEAR COMPLEXITY WITH GEODESICS", "text": "Thus far we have found that a deep representation is able to linearize a range of real-world transformations (figure 3, right column) whereas a shallow one (e.g., the pixel intensities) is not (figure 1, middle column). It is unclear, however, whether the improved invariance of the deep representation is due to the spatial extent over which it computes its responses, or its nonlinear complexity. Indeed, as we progress up the hierarchy of a neural network, the effective input region for each unit (the \u201creceptive field\u201d) increases in size, simply due to cascaded convolution and subsampled pooling. At the same time, the complexity of the representation increases as a longer sequence of non-linear operations are composed.\nIn order to separate these two effects, we varied the complexity of the representation while keeping the size of the receptive field constant. For an artificial neuron, the receptive field quantifies the strength of the connection between a location in the image and that neuron\u2019s activity, and can be measured by computing the magnitude of the gradient of the neuron\u2019s activity with respect to the image. Hence, the receptive field of a non-linear neuron changes as a function of the input image. In order to measure the extent of a neuron\u2019s receptive field across all images, we averaged the\nmagnitude of the gradient of its activity over a large set of white noise images. We generalized this method to measuring the receptive field of an entire population by computing the average magnitude of the gradient of an entire \u2018cortical column\u2019, or set of hidden units at a given location.\nUsing this method, we measured the receptive field size of the representation used in our previous experiments (third pooling layer of the VGG network with L2 pooling). We then computed geodesics from shallower representations (first and second pooling layers of the VGG network) for which we increased the pooling extent (from 6\u00d76 to 36\u00d736 and 18\u00d718 respectively) in order to match the receptive field size of the deep representation. These experiments show that shallower layers, despite being matched for receptive field size, are unable to linearize translations as well as deeper ones (figure 4). Interestingly, we find a gradual increase in the quality of the geodesics as the complexity of the representation increases. Hence the curvature of representational geodesics, more than their dimensionality, is essential for capturing these non-linear deformations of the image."}, {"heading": "3.3 LINEARIZING NATURAL IMAGE SEQUENCES", "text": "Having tested for the modified VGG network\u2019s ability to linearize simple parametric transformations, we asked whether it can linearize compositions of these transformations that arise in natural image sequences. To explore this, we extracted 5 frames from the movie Melancholia and generated a geodesic from the first to the last of these. We find that this geodesic smoothly transitions between the two images, and captures much of the true temporal evolution of the video (figure 5, left and right panels). Relative to the original sequence, the only errors it produces are due to well known problems in motion estimation. A large component of the transformation in the video is an out-of-plane rotation due to the camera panning, creating a composition of translations and dilations throughout the image. In a region of the image with periodic structure (e.g. the woven cane texture of the chair), the motion between the two end frames is ambiguous, because the translation between them exceeds one half of this period. This problem, known as temporal aliasing, can be seen in the temporal slices, which reveal that the back of the chair is smoothly shifted in the opposite direction of the rest of the image (figure 5, right panel). In motion estimation, this problem is usually solved using a coarse-to-fine approach, in which the motion of the low frequencies is estimated first, and used to condition (or initialize) motion estimates derived from higher frequencies. This method can be naturally embedded in our framework by generalizing the nested conditionalization of geodesic\nobjective functions (section 2.1). That is, each layer of the network can impose its own geodesic constraints, conditioned on those imposed by deeper layers. This hierarchical construction provides a means of solving the problem of temporal aliasing, and more generally should allow the network to linearize a broader class of transformations."}, {"heading": "4 DISCUSSION", "text": "The synthesis of geodesic sequences provide a means of visualizing and assessing metric properties of a representation. We have developed a methodology for generating such sequences, and shown that they can be used as a powerful diagnostic tool for evaluating the invariance properties of learned representations. Specifically, evaluating geodesics enables one to test the \u201cuntangling hypothesis\u201d by which hierarchical representations (in particular, biological sensory systems) linearize the action of identity-preserving transformations (DiCarlo & Cox, 2007). Such an \u201cuntangled\u201d representation can be linearly decoded, projecting out unwanted variations arising from image-domain renderings to achieve invariant object recognition, or projecting out the orthogonal space, so as to estimate latent rendering variables such as position, lighting, and pose.\nWe used this methodology to test a state-of-the-art recognition network and found that it was unable to linearize basic image transformations such as translation, rotation and dilation. Importantly, these results suggested a simple improvement in the architecture of the network, which in turn enabled it to linearize parametric distortions as well as those found in a natural image sequence. Hence, our geodesic visualization method provides both a means of testing the untangling hypothesis for artificial networks, as well as a design tool for guiding improvements in learned representations.\nAlternatively, one could directly test the invariance of a system to a given transformation by examining the variability of responses to objects deformed by the corresponding operation. But such a test relies on establishing a meaningful measure of variability in the representation space, which is undermined by the fact that essentially equivalent representations (e.g., that differ by an invertible affine transformation) can have dramatically different distance or variability measures. As a result, it can be difficult to compare invariance properties of different models, or even across different stages of the same network, with this direct method. The use of geodesic sequences (which are unaffected by invertible affine transformations) avoids this problem by expressing the invariance properties of the representation back in the input (image) domain, where they can be directly compared.\nMoreover, our method can be applied to arbitrary image pairs, including but not limited to parametrically transformed images and frames from natural videos. For example, generating geodesics between two arbitrary images from the same object category can reveal whether object identity is an invariant of a representation. An affirmative answer implies that, back in the representation space, all of the images along the geodesic could be correctly identified using a linear decoder (as is commonly done when reading out the penultimate layer of a deep neural network).\nFinally, our method suggests a natural extension to hierarchical representations. Our geodesic sequences were computed by minimizing path length in the pixel domain, conditioned on minimizing path length in a network representation. This process could be applied recursively in a hierarchical representation, minimizing path length at each stage conditioned on minimal path length at higher stages. The resulting non-linear coarse-to-fine computation has the potential to solve well-known problems of temporal aliasing, and to enable hierarchical representations to linearize a much broader class of naturalistic transformations. The resulting image sequences, in turn, could be used to probe and characterize perceptual and physiological aspects of the representation of these transformations in biological visual systems."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the Howard Hughes Medical Institute."}], "references": [{"title": "Untangling invariant object recognition", "author": ["DiCarlo", "James J", "Cox", "David D"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "DiCarlo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "DiCarlo et al\\.", "year": 2007}, {"title": "Learning invariant features through topographic filter maps", "author": ["Kavukcuoglu", "Koray", "Ranzato", "Marc\u2019Aurelio", "Fergus", "Rob", "LeCun", "Yann"], "venue": null, "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2009}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoff"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Group Invariant Scattering", "author": ["Mallat", "St\u00e9phane"], "venue": "arXiv.org, math.FA,", "citeRegEx": "Mallat and St\u00e9phane.,? \\Q2011\\E", "shortCiteRegEx": "Mallat and St\u00e9phane.", "year": 2011}, {"title": "A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefficients", "author": ["Portilla", "Javier", "Simoncelli", "Eero P"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Portilla et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Portilla et al\\.", "year": 2000}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": null, "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv.org, cs.CV", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": null, "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "A convolutional subunit model for neuronal responses in macaque V1", "author": ["B Vintch", "J A Movshon", "Simoncelli", "Eero P"], "venue": "The Journal of Neuroscience,", "citeRegEx": "Vintch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vintch et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Recent examples of learned visual representations have proven highly effective for recognition (Krizhevsky et al., 2012), but a precise understanding of exactly what they represent remains elusive.", "startOffset": 95, "endOffset": 120}, {"referenceID": 1, "context": "And although these representations are hypothesized to be invariant to various identity-preserving deformations, apart from a few exceptions, these claims are rarely tested directly (Kavukcuoglu et al., 2009).", "startOffset": 182, "endOffset": 208}, {"referenceID": 8, "context": "When applied to deep recognition networks, synthesis has revealed failures in the form of \u201cadversarial examples\u201d: images that appear entirely different to a human observer, and yet are identified by the network as belonging to the same category (Szegedy et al., 2013).", "startOffset": 245, "endOffset": 267}, {"referenceID": 6, "context": "Recent theoretical work shows that optimizing all layers of a network jointly makes the problem significantly more difficult than optimizing a single layer in isolation (Saxe et al., 2013).", "startOffset": 169, "endOffset": 188}], "year": 2016, "abstractText": "We develop a new method for visualizing and refining the invariances of learned representations. Specifically, we test for a general form of invariance, linearization, in which the action of a transformation is confined to a low-dimensional subspace. Given two reference images (typically, differing by some transformation), we synthesize a sequence of images lying on a path between them that is of minimal length in the space of the representation (a \u201crepresentational geodesic\u201d). If the transformation relating the two reference images is linearized by the representation, this sequence should follow the gradual evolution of this transformation. We use this method to assess the invariance properties of a state-of-the-art image classification network and find that geodesics generated for image pairs differing by translation, rotation, and dilation do not evolve according to their associated transformations. Our method also suggests a remedy for these failures, and following this prescription, we show that the modified representation is able to linearize a variety of geometric image transformations.", "creator": "LaTeX with hyperref package"}}}