{"id": "1511.06432", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Delving Deeper into Convolutional Networks for Learning Video Representations", "abstract": "advocates propose generic approach to learn spatio - temporal features in videos from intermediate 3d representations we call \" spatial \" discrete coordinate - variable - aligned recurrent coordinates ( grus ). our method results on algorithms that only extracted to all higher resolution scanning deep convolutional network trained on the large wavelength dataset. instead normal - level resolutions record maximal discriminative information, they operate to have a low - magnitude consistency. low - level percepts, satisfying many similar hand, preserve a higher spatial resolution from which we assume preserve finer motion patterns. using normal - level computing methods simulate optimal focal - intensity video representations. help lower this effect and maximize the average number of edges, educators outline an variant of each gru model that yields different convolution operations to maintain directional coefficients versus the model units and dynamic parameters upon the input vertex connectivity.", "histories": [["v1", "Thu, 19 Nov 2015 22:46:13 GMT  (387kb,D)", "http://arxiv.org/abs/1511.06432v1", "ICLR submission"], ["v2", "Mon, 23 Nov 2015 02:46:54 GMT  (386kb,D)", "http://arxiv.org/abs/1511.06432v2", "ICLR submission"], ["v3", "Thu, 7 Jan 2016 19:43:19 GMT  (387kb,D)", "http://arxiv.org/abs/1511.06432v3", "ICLR submission"], ["v4", "Tue, 1 Mar 2016 18:54:11 GMT  (388kb,D)", "http://arxiv.org/abs/1511.06432v4", "ICLR 2016"]], "COMMENTS": "ICLR submission", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["nicolas ballas", "li yao", "chris pal", "aaron courville"], "accepted": true, "id": "1511.06432"}, "pdf": {"name": "1511.06432.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["DELVING DEEPER", "Nicolas Ballas", "Li Yao", "Chris Pal", "Aaron Courville"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "Video analysis and understanding represents a major challenge for computer vision and machine learning research. While previous work has traditionally relied on hand-crafted and task-specific representations (Wang et al., 2011; Sadanand & Corso, 2012), there is a growing interest in designing general video representations that could help solve tasks in video understanding such as human action recognition, video retrieval or video captionning (Tran et al., 2014).\nTwo-dimensional Convolutional Neural Networks (CNN) have exhibited state-of-art performance in still image tasks such as classification or detection (Simonyan & Zisserman, 2014b). However, such models discard temporal information that has been shown to provide important cues in videos (Wang et al., 2011). On the other hand, recurrent neural networks (RNN) have demonstrated the ability to understand temporal sequences in various learning tasks such as speech recognition (Graves & Jaitly, 2014) or machine translation (Bahdanau et al., 2014). Consequently, Recurrent Convolution Networks (RCN) (Srivastava et al., 2015; Donahue et al., 2014; Ng et al., 2015) that leverage both recurrence and convolution have recently been introduced for learning video representation. Such approaches typically extract \u201cvisual percepts\u201d by applying a 2D CNN on the video frames and then feed the CNN activations to the proceeding RNN in order to characterize the video temporal variation.\nPrevious works on RCNs tend to focus on high-level visual percepts extracted from the 2D CNN top-layers. CNNs, however, hierachically build-up spatial invariance through pooling layers (LeCun et al., 1998; Simonyan & Zisserman, 2014b) as Figure 2 highlights. While CNN tends to discard local information in its top layers, frame-to-frame temporal variation is known to be smooth, motion of video patches are restricted to a local neighborhood (Brox & Malik, 2011). For this reason, we argue that current RCN architectures are not well suited for capturing fine motion information. Instead, they are more likely focus on global appearance changes such as shot transitions. To address this issue, we introduce a novel RCN architecture that applies an RNN not solely on the 2D CNN top-layer but also on the intermediate convolutional layers. Convolutional layer activations, or convolutional\nar X\niv :1\n51 1.\n06 43\n2v 1\n[ cs\n.C V\n] 1\n9 N\nov 2\n01 5\nmaps, preserve a finer spatial resolution of the input video from which local spatio-temporal patterns are extracted.\nApplying an RNN directly on intermediate convolutional maps, however, inevitably results in a drastic number of parameters characterizing the input-to-hidden transformation due to the convolutional maps size. On the other hand, convolutional maps preserve the frame spatial topology. We propose to leverage this topology by introducing sparsity and locality in the RNN units to reduce the memory requirement. We extend the GRU-RNN model (Cho et al., 2014) and replace the fully-connected RNN linear product operation with a convolution. Our GRU-extension therefore encodes the locality and temporal smoothness prior of videos directly in the model structure.\nWe evaluate our solution on UCF101 human action recognition from Soomro et al. (2012) as well as YouTube2text video captioning dataset from Chen & Dolan (2011). Experiments show that leveraging \u201cpercepts\u201d at mutiple resolutions to model temporal variation improve over baseline model with respective gain of 3.4% and 10% for the action recognition and video captions tasks."}, {"heading": "2 GRU: GATED RECURRENT UNITS", "text": "In this section, we review Gated-Recurrent-Unit (GRU) network which is a particular type of RNN. An RNN model is applied to a sequence of inputs, which can have variable lenghts. It defines a recurrent hidden state whose activation at each time is dependent on that of the previous time. Specificaly, given a sequence X = (x1,x2, ...,xT ), the RNN hidden state at time t is defined as ht = \u03c6(ht\u22121,xt), where \u03c6 is a nonlinear activation function. RNNs are known to be difficult to train due to the exploding or vanishing gradient effect (Bengio et al., 1994). However, variants of RNNs such as Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) or Gated Recurrent Units (GRU) (Cho et al., 2014) have empirically demonstrated their ability to model long-term temporal dependency in various task such as machine translation or image/video caption generation. In this paper, we will mainly focus on the GRU as they have shown similar performance than LSTM but with a lower memory requirement (Chung et al., 2014).\nGRU makes each recurrent unit to adaptively capture dependencies of different time scales. The activation ht of the GRU is defined by the following equations:\nzt = \u03c3(Wzxt +Uzht\u22121), (1) rt = \u03c3(Wrxt +Urht\u22121), (2)\nh\u0303t = tanh(Wxt +U(rt ht\u22121), (3) ht = (1\u2212 zt)ht\u22121 + zth\u0303t, (4)\nwhere is an element-wise multiplication. zt is an update gate that decides the degree to which the unit updates its activation, or content. rt is a reset gate. \u03c3 is the sigmoid function. When one unit rit is close to 0, the reset gate forgets the previously computed state, and makes the unit act as if it is reading the first symbol of an input sequence. h\u0303t is a candidate activation which is computed similarly to that of the traditional recurrent unit in an RNN."}, {"heading": "3 DELVING DEEPER IN THE CONVOLUTIONAL NEURAL NETWORK", "text": "This section delves into the main contributions of this work. We aim at leveraging visual percepts from different convolutional levels in order to capture temporal patterns that occur at different spatial resolution.\nLet\u2019s consider (x1t , ...,x L\u22121 t ,x L t )(t=1..T ), a set of 2D convolutional maps extracted from L layers at different time steps in a video. We propose two alternative RCN architectures, GRU-RCN, and Stacked-GRU-RCN (illustrated in Figure 2) that combines information extracted from those convolutional maps."}, {"heading": "3.1 GRU-RCN:", "text": "In the first RCN architecture, we propose to apply L RNNs independently on each convolutional map. We define L RNNs as \u03c61, ..., \u03c6L, such that hlt = \u03c6 l(xlt,h l t\u22121). The hidden representation of the final time step h1T , ...,h L T are then fed to classifier in the case of action recognition, or to a text-decoder RNN for the caption generation.\nTo implement the RNN recurrent function \u03c6l, we propose to leverage Gated Recurrent Unit (Cho et al., 2014). GRUs have been originaly introduced for machine translation. It models input to hidden-state and hidden to hidden transitions using fully connected units. However, convolutional map inputs are 3D tensors (spatial dimension and input channels). Applying a GRU directly can lead to a drastic number of parameters. Let N1, N2 and Ox be the input convolutional map spatial size and number of channels. Applying a GRU directly would require input-to-hidden parameters Wl, Wlz and W l r to be of size N1 \u00d7 N2 \u00d7 Ox \u00d7 Oh where Oh is the dimensionality of the GRU hidden representation.\nFully-connected GRUs do not take advantage of the underlying structure of convolutional maps. Indeed, convolutional maps are extracted from images that are composed of patterns with strong local correlation which are repeated over different spatial locations. In addition, videos have smooth temporal variation over time, i.e. motion associated with a given patch in successive frames will be restricted in a local spatial neighborhood. We embed such a prior in our model structure and replace the fully-connected units in GRU with convolution operations. We therefore obtain reccurent units that have sparse connectivity and share their parameters across different input spatial locations:\nzlt = \u03c3(W l z \u2217 xlt +Ulz \u2217 hlt\u22121), (5)\nrlt = \u03c3(W l r \u2217 xlt +Ulr \u2217 hlt\u22121), (6)\nh\u0303lt = tanh(W l \u2217 xlt +U \u2217 (rlt hlt\u22121), (7)\nhlt = (1\u2212 zlt)hlt\u22121 + zlth\u0303lt, (8)\nwhere \u2217 denotes a convolution operation. In this formulation, Model parameters W,Wlz,Wlr and Ul,Ulz,U l r are 2D-convolutional kernels. Our model results hidden recurrent representation that preserves the spatial topology, hlt = (h l t(i, j)) where h l t(i, j)) is a feature vector defined at the location (i, j). To ensure that the spatial size of the hidden representation remains fixed over time, we use zero-padding in the recurrent convolutions.\nUsing convolution, parameters Wl, Wlz and W l r have a size of k1\u00d7k2\u00d7Ox\u00d7Oh where k1\u00d7k2 is the convolutional kernel spatial size (usually 3\u00d73), choosen to be significantly lower than convolutional map size N1\u00d7N2. The candidate hidden representation h\u0303t(i, j), the activation gate zk(i, j) and the reset gate rk(i, j) are defined based on a local neigborhood of size (k1 \u00d7 k2) at the location(i, j) in both the input data xt and the previous hidden-state ht\u22121. In addition, the size of receptive field associated with hl(i, j)t increases in the previous presentation hlt\u22121,h l t\u22122... as we go back further in time. Our model is therefore capable of characterizing spatio-temporal pattern with high spatial variation in time."}, {"heading": "3.2 STACKED GRU-RCN:", "text": "In the second RCN architecture, we investigate the importance of bottom-up connection across RNNs. While GRU-RCN applies each layer-wise GRU-RNN in an independent fashion, Stacked GRU-RCN preconditions each GRU-RNN on the output of the previous GRU-RNN at the current time step: hlt = \u03c6 l(hlt\u22121,h l\u22121 t ,x l t). The previous RNN hidden representation is given as an extrainput to the GRU convolutional units:\nzll = \u03c3(W l z \u2217 xlt +Wlzl \u2217 h l\u22121 t +U l z \u2217 hlt\u22121), (9)\nrlt = \u03c3(W l r \u2217 xlt +Wlrl \u2217 h l\u22121 t +U l rh l t\u22121), (10)\nh\u0303lt = tanh(W l \u2217 xlt +Ul \u2217 (rt hlt\u22121), (11)\nhlt = (1\u2212 zlt)hlt\u22121 + zlth\u0303lt, (12) Adding this extra-connection brings more flexibility and gives the opportunity for the model to leverage representations with different resolutions."}, {"heading": "4 RELATED WORK", "text": "Deep learning approaches have recently been used to learn video representation and produced stateof-art results (Karpathy et al., 2014; Simonyan & Zisserman, 2014a; Wang et al., 2015b; Tran et al., 2014). Karpathy et al. (2014); Tran et al. (2014) proposed to use 3D CNN learn a video representations, leveraging large training datasets such as Sport 1 Million. However, unlike image classification (Simonyan & Zisserman, 2014b), CNNs did not yield large improvement over these traditional methods (Lan et al., 2014) highlighting the difficulty of learning video representation even with large training dataset. Simonyan & Zisserman (2014a) introduced a two-stream framework where they train CNNs independently on RGB and optical flow inputs. While the flow stream focuses only on motion information, the RGB can leverage 2D CNN pre-trained on image datasets. Based on the Two Stream representation, Wang et al. (2015a) extracted deep feature and conducted trajectory constrained pooling to aggregate convolutional feature as video representations.\nRNN models have also been used to encode temporal information for learning video representations in conjonction with 2D CNNs. Ng et al. (2015); Donahue et al. (2014) applied an RNN on top of the the two-stream framework, while Srivastava et al. (2015) proposed, in addition, to investigate the benefit of learning a video representation in an unsupervised manner. Previous works on this topic tend to focus only on high-level CNN \u201cvisual percepts\u201d. By contrast, our approach proposes to leverage visual \u201cpercepts\u201d extracted from different layers in the 2D-CNN.\nRecently, Shi et al. (2015) also proposed to leverage convolutional units inside an RNN network. However, they focus on different task (now-casting) and RNN model (LSTM). In addition, they applied their model directly on pixels. Here, we use recurrent convolutional units on pre-trained CNN convolutional maps, to extract temporal pattern from visual \u201cpercepts\u201d with different spatial sizes."}, {"heading": "5 EXPERIMENTATION", "text": "This section presents an empirical evaluation of the proposed GRU-RCN and Stacked GRU-RCN architectures. We conduct experimentations on two different tasks: human action recognition and video caption generation."}, {"heading": "5.1 ACTION RECOGNITION", "text": "We evaluate our approach on the UCF101 dataset Soomro et al. (2012). This dataset has 101 action classes spanning over 13320 YouTube videos clips. Videos composing the dataset are subject to large camera motion, viewpoint change and cluttered backgrounds. We report results on the dataset UCF101 first split, as this is most commonly used split in the literature. To perform proper hyperparameter seach, we use the videos from the UCF-Thumos validation split Jiang et al. (2014) as the validation set."}, {"heading": "5.1.1 MODEL ARCHITECTURE", "text": "In this experiment, we only consider on the video RGB inputs. One could further improve the classification performance by considering other inputs such as the video optical flow Simonyan & Zisserman (2014a); Wang et al. (2015b). However, the main purpose of this experiment is to demonstrate the benefit of visual clues from multiple CNN layers when modeling the temporal information in a videos.\nWe design and evaluate three RCN architectures for action recognition. We use a VGG-16 CNN pretrained on ImageNet (Simonyan & Zisserman, 2014b) that is fine-tuned on the UCF-101 dataset, following the protocol in Wang et al. (2015b). We then extract the convolution maps from pool2, pool3, pool4, pool5 layers and the fully-connected map from layer fc-7 (which can be view as a feature map with a 1 \u00d7 1 spatial dimension). Those features maps are given as inputs to our RCN models.\nIn the first RCN architecture, GRU-RCN, we apply 5 convolutional GRU-RNNs independently on each convolutional map. Each convolution in the GRU-RCN has zero-padded 3\u00d73 convolution that preserves the spatial dimension of the inputs . The number of channels of each respective GRU-RNN hidden-representations are 64, 128, 256, 256, 512. After the RCN operation we obtain 5 hiddenrepresentations for each time step. We apply average pooling on the hidden-representations of the last time-step to reduce their spatial dimension to 1\u00d71 , and feed the representations to 5 classifiers, composed by a linear layer with a softmax nonlineary. Each classifier therefore focuses on only 1 hidden-representations extracted from the convolutional map of a specific layer. The classifier outputs are then averaged to get the final decision. A dropout ratio of 0.7 is applied on the input of each classifiers.\nIn the second RCN architecture, Stacked GRU-RCN, we investigate the usefulness of bottom-up connections. Stacked GRU-RCN uses the same base architecture as GRU-RCN, 5 convolutional GRU-RNN having 64, 128, 256, 256 channels respectively. However, each convolutional GRURNNs is now preconditioned on the hidden-represention that the GRU-RNN applied on the previous convolution-maps outputs. We apply max-pooling on the hidden representations between the GRU-\nRNN layers for the compatibility of the spatial dimensions. As for the previous architecture, each GRU-RNN hidden-representation at the last time step is pooled and then given as input to a classifer.\nFinally, in Bi-directional GRU-RCN, we investigate the importance of reverse temporal information. Given convolutional maps extracted from one layer, we run the GRU-RCN twice, considering the inputs in both sequential and reverse temporal order. We then concatenate the last hiddenrepresentations of the foward GRU-RCN and backward GRU-RCN, and give the resulting vector to a classifier."}, {"heading": "5.1.2 MODEL TRAINING AND EVALUATION", "text": "We follow the training procedure introduced by the two-stream framework Simonyan & Zisserman (2014a). At each iteration, a batch of 64 videos are sampled randomly from the the training set. To perform scale-augmentation, we randomly sample the cropping width and height from 256, 224, 192, 168. The temporal cropping size is set to 10. We then resize the cropped volume to 224\u00d7 224\u00d7 10. We estimate each model parameters by maximizing the model log-likelihood:\nL(\u03b8) = 1 N N\u2211 n=1 log p(yn | c(xn),\u03b8),\nwhere there are N training video-action pairs (xn, yn), c is a function that takes a crop at random. We use Adam Kingma & Ba (2014) with the gradient computed by the backpropagation algorithm. We perform early stopping and choose the parameters that maximize the log-probability of the validation set.\nWe also follow the evaluation protocol of the two-stream framework Simonyan & Zisserman (2014a). At the test time, we sample 25 equally spaced video sub-volumes with a temporal size of 10 frames. From each of these selected sub-volumes, we obtain 10 inputs for our model, i.e. 4 corners, 1 center, and their horizontal flipping. The final pre- diction score is obtained by averaging across the sampled sub-volumes and their cropped regions.\n5.1.3 RESULTS\nWe compare our approach with two different baselines, VGG-16 and VGG-16 RNN. VGG-16 is the 2D spatial stream that is described in Wang et al. (2015b). We take the VGG-16 model, pretrained on Image-Net and fine-tune it on the UCF-101 dataset. VGG-16 RNN baseline applied an RNN, using fully-connected gated-recurrent units, on top-of VGG-16. It takes as input the VGG-16 fully-connected representation fc-7. Following GRU-RCN top-layer, the VGG-16 RNN has hiddenrepresentation of dimensionality 512.\nIn Table 1, we first report results of different GRU-RCN variants and compare them with the two baselines: VGG-16 and VGG-16 RNN. Our GRU-RCN variants all outperform the baselines, show-\ning the benefit of delving deeper into a CNN in order to learn a video representation. We notice that VGG-16 RNN only slightly improve over the VGG-16 baseline, 78.1 against 78.0. This result confirms that CNN top-layer tends to discard temporal variation over short temporal windows. Stacked-GRU RCN performs significantly lower than GRU-RCN and Bi-directional GRU-RCN. We argue that bottom-up connection, increasing the depth of the model, combined with the lack of training data (UCF-101 is train set composed by only 9500 videos) make the Stacked-GRU RCN learning difficult. Bi-directional GRU-RCN perform the best among the GRU-RCN variant with an accuracy of 80.7, showing the advantage of modeling temporal information in both sequential and reverse orer. Bi-directional GRU-RCN obtains a gain 3.4% in term of performances, relatively to the baselines that focus only the VGG-16 top layer.\nTable 1 also reports results from other state-of-art approaches using RGB inputs. C3D Tran et al. (2014) obtains the best performance on UCF-101 with 85.2. However, it should be noted that C3D is trained over 1 million videos. Other approaches use only the 9500 videos of UCF101 training set for learning temporal pattern. Our Bi-directional GRU-RCN compare favorably with other Recurrent Convolution Network (second blocks), confirming the benefit of using different CNN layers to model temporal variation."}, {"heading": "5.2 VIDEO CAPTIONNING", "text": "We also evaluate our representation on the video captioning task using YouTube2Text video corpus Chen & Dolan (2011). The dataset has 1,970 video clips with multiple natural language descriptions for each video clip. The dataset is open-domain and covers a wide range of topics such as sports, animals, music and movie clips. Following Yao et al. (2015b), we split the dataset into a training set of 1,200 video clips, a validation set of 100 clips and a test set consisting of the remaining clips."}, {"heading": "5.2.1 MODEL SPECIFICATIONS", "text": "To perform video captioning, we use the so-called encoder-decoder framework Cho et al. (2014). Encoder maps input videos into abstract representations that precondition caption-generating decoder.\nAs for encoder, we compare both VGG-16 CNN and Bi-directional GRU-RCN. Both models have been fine-tuned on the UCF-101 dataset and therefore focus on detecting actions. To extract an abstract representation from a video, we sample K equally-space segments. When using the VGG16 encoder, we provide the fc7 layer activations of the each segment\u2019s first frame as the input to the text-decoder. For the GRU-RCN, we apply our model on the segment\u2019s 10 first frames. We concatenate the GRU-RCN hidden-representation from the last time step. The concatenated vector is given as the input to the text decoder. As it has been shown that characterizing entities in addition of action is important for the caption-generation task Yao et al. (2015a), we also use as encoder a CNN Szegedy et al. (2014), pretrained on ImageNet, that focuses on detecting static visual object categories.\nAs for the decoder, we use LSTM text-generator with soft-attention on the video temporal frames Yao et al. (2015b)."}, {"heading": "5.2.2 TRAINING", "text": "For all video captioning models, we estimated the parameters \u03b8 of the decoder by maximizing the log-likelihood:\nL(\u03b8) = 1 N N\u2211 n=1 tn\u2211 i=1 log p(yni | yn<i,xn,\u03b8),\nwhere there are N training video-description pairs (xn, yn), and each description yn is tn words long. We used Adadelta Zeiler (2012) We optimized the hyperparameters (e.g. number of LSTM units and the word embedding dimensionality, number of segmentK) using random search (Bergstra & Bengio, 2012) to maximize the log-probability of the validation set."}, {"heading": "5.2.3 RESUTS", "text": "Table 2 reports the performance of our proposed method using three automatic evaluation metrics. These are BLEU in Papineni et al. (2002), METEOR in Denkowski & Lavie (2014) and CIDEr in Vedantam et al. (2014). We use the evaluation script prepared and introduced in Chen et al. (2015). All models are early-stopped based on the negative-log-likelihood (NLL) of the validation set. We then select the model that performs best on the validation set according to the metric at consideration.\nThe first two lines of Table 2 compare the performances of the VGG-16 and Bi-direcitonal GRURCN encoder. Results clearly show the superiority of the Bi-Directional GRU-RCN Encoder as it outperforms the VGG-16 Encoder on all three metrics. In particular, GRU-RCN Encoder obtains a performance gain of 10% compared to the VGG-16 Encoder according to the BLEU metric. Combining our GRU-RCN Encoder that focuses on action with a GoogleNet Encoder that captures visual entities further improve the performances.\nOur GoogleNet + Bi-directional GRU-RCN approach signficantly outperforms Soft-attention Yao et al. (2015b) that relies on a GoogLeNet and cuboids-based 3D-CNN Encoder, in conjunction to a similar soft-attention decoder. This result indicates that our approach is able to offer more effective representions. According to the BLEU metric, we also outperform other approaches that uses more complex decoder scheme such as spatial and temporal attention decoder (Yu et al., 2015) or a Hierarchical RNN decoder (Pan et al., 2015) Our approach is on par with Yu et al. (2015), without the need of using a C3D-encoder that requires to be trained on large-scale video dataset."}, {"heading": "6 CONCLUSION", "text": "In this work, we address the challenging problem of learning discriminative and abstract representations from videos. We identify and underscore the importance of modeling temporal variation from \u201cvisual percepts\u201d at different spatial resolutions. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. We introduce a novel recurrent convolutional network architecture that leverages convolutional maps, from all levels of a deep convolutional network trained on the ImageNet dataset, to take advantage of \u201cpercepts\u201d from different spatial resolutions.\nWe have empirically validated our approach on the Human Action Recognition and Video Captioning tasks using the UCF-101 and YouTube2Text datasets. Experiments demonstrate that leveraging\n\u201cpercepts\u201d at mutiple resolutions to model temporal variation improve over baseline model, with respective gain of 3.4% and 10% for the action recognition and video captions tasks. In particular, we achieve results comparable to state-of-art on YouTube2Text using a simpler text-decoder model and without extra 3D CNN features."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Que\u0301bec, Compute Canada, the Canada Research Chairs and CIFAR. We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) , for developing such a powerful tool for scientific computing."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Random search for hyper-parameter optimization", "author": ["Bergstra", "James", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Large displacement optical flow: descriptor matching in variational motion estimation", "author": ["T. Brox", "J. Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Brox and Malik,? \\Q2011\\E", "shortCiteRegEx": "Brox and Malik", "year": 2011}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["Chen", "David L", "Dolan", "William B"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Microsoft coco captions: Data collection and evaluation", "author": ["Chen", "Xinlei", "Fang", "Hao", "Lin", "Tsung-Yi", "Vedantam", "Ramakrishna", "Gupta", "Saurabh", "Dollar", "Piotr", "Zitnick", "C Lawrence"], "venue": "server. arXiv 1504.00325,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "KyungHyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Denkowski", "Michael", "Lavie", "Alon"], "venue": "In EACL Workshop,", "citeRegEx": "Denkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4389,", "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In ICML,", "citeRegEx": "Graves and Jaitly,? \\Q2014\\E", "shortCiteRegEx": "Graves and Jaitly", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Thumos challenge: Action recognition with a large number of classes", "author": ["YG Jiang", "J Liu", "A Roshan Zamir", "G Toderici", "I Laptev", "M Shah", "R. Sukthankar"], "venue": "Technical Report,", "citeRegEx": "Jiang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2014}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["Karpathy", "Andrej", "Toderici", "George", "Shetty", "Sachin", "Leung", "Tommy", "Sukthankar", "Rahul", "Fei-Fei", "Li"], "venue": "In CVPR. IEEE,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Beyond gaussian pyramid: Multi-skip feature stacking for action recognition", "author": ["Lan", "Zhenzhong", "Lin", "Ming", "Li", "Xuanchong", "Hauptmann", "Alexander G", "Raj", "Bhiksha"], "venue": "arXiv preprint arXiv:1411.6660,", "citeRegEx": "Lan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Beyond short snippets: Deep networks for video classification", "author": ["Ng", "Joe Yue-Hei", "Hausknecht", "Matthew", "Vijayanarasimhan", "Sudheendra", "Vinyals", "Oriol", "Monga", "Rajat", "Toderici", "George"], "venue": "arXiv preprint arXiv:1503.08909,", "citeRegEx": "Ng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2015}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["Pan", "Pingbo", "Xu", "Zhongwen", "Yang", "Yi", "Wu", "Fei", "Zhuang", "Yueting"], "venue": "arXiv preprint arXiv:1511.03476,", "citeRegEx": "Pan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni", "Kishore", "Roukos", "Salim", "Ward", "Todd", "Zhu", "Wei-Jing"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Action bank: A high-level representation of activity in video", "author": ["S. Sadanand", "J. Corso"], "venue": "In CVPR. IEEE,", "citeRegEx": "Sadanand and Corso,? \\Q2012\\E", "shortCiteRegEx": "Sadanand and Corso", "year": 2012}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["Shi", "Xingjian", "Chen", "Zhourong", "Wang", "Hao", "Yeung", "Dit-Yan", "Wong", "Wai-Kin", "Woo", "Wangchun"], "venue": "arXiv preprint arXiv:1506.04214,", "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Ucf101: A dataset of 101 human actions classes from videos in the wild", "author": ["Soomro", "Khurram", "Zamir", "Amir Roshan", "Shah", "Mubarak"], "venue": "arXiv preprint arXiv:1212.0402,", "citeRegEx": "Soomro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Soomro et al\\.", "year": 2012}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": "In ICML,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Integrating language and vision to generate natural language descriptions of videos in the wild", "author": ["Thomason", "Jesse", "Venugopalan", "Subhashini", "Guadarrama", "Sergio", "Saenko", "Kate", "Mooney", "Raymond"], "venue": "In COLING,", "citeRegEx": "Thomason et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Thomason et al\\.", "year": 2014}, {"title": "C3d: generic features for video analysis", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "arXiv preprint arXiv:1412.0767,", "citeRegEx": "Tran et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2014}, {"title": "CIDEr: Consensus-based image description evaluation", "author": ["Vedantam", "Ramakrishna", "Zitnick", "C Lawrence", "Parikh", "Devi"], "venue": null, "citeRegEx": "Vedantam et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2014}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Venugopalan", "Subhashini", "Xu", "Huijuan", "Donahue", "Jeff", "Rohrbach", "Marcus", "Mooney", "Raymond", "Saenko", "Kate"], "venue": null, "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Action recognition by dense trajectories", "author": ["H. Wang", "A. Kl\u00e4ser", "C. Schmid", "C. Liu"], "venue": "In CVPR. IEEE,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Action recognition with trajectory-pooled deepconvolutional descriptors", "author": ["Wang", "Limin", "Qiao", "Yu", "Tang", "Xiaoou"], "venue": "arXiv preprint arXiv:1505.04868,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Towards good practices for very deep two-stream convnets", "author": ["Wang", "Limin", "Xiong", "Yuanjun", "Zhe", "Qiao", "Yu"], "venue": "arXiv preprint arXiv:1507.02159,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Trainable performance upper bounds for image and video captioning", "author": ["Yao", "Li", "Ballas", "Nicolas", "Cho", "Kyunghyun", "Smith", "John R", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1511.0459,", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Yao", "Li", "Torabi", "Atousa", "Cho", "Kyunghyun", "Ballas", "Nicolas", "Pal", "Christopher", "Larochelle", "Hugo", "Courville", "Aaron"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["Yu", "Haonan", "Wang", "Jiang", "Huang", "Zhiheng", "Yang", "Yi", "Xu", "Wei"], "venue": "arXiv 1510.07712,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "Technical report,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 33, "context": "While previous work has traditionally relied on hand-crafted and task-specific representations (Wang et al., 2011; Sadanand & Corso, 2012), there is a growing interest in designing general video representations that could help solve tasks in video understanding such as human action recognition, video retrieval or video captionning (Tran et al.", "startOffset": 95, "endOffset": 138}, {"referenceID": 30, "context": ", 2011; Sadanand & Corso, 2012), there is a growing interest in designing general video representations that could help solve tasks in video understanding such as human action recognition, video retrieval or video captionning (Tran et al., 2014).", "startOffset": 226, "endOffset": 245}, {"referenceID": 33, "context": "However, such models discard temporal information that has been shown to provide important cues in videos (Wang et al., 2011).", "startOffset": 106, "endOffset": 125}, {"referenceID": 0, "context": "On the other hand, recurrent neural networks (RNN) have demonstrated the ability to understand temporal sequences in various learning tasks such as speech recognition (Graves & Jaitly, 2014) or machine translation (Bahdanau et al., 2014).", "startOffset": 214, "endOffset": 237}, {"referenceID": 27, "context": "Consequently, Recurrent Convolution Networks (RCN) (Srivastava et al., 2015; Donahue et al., 2014; Ng et al., 2015) that leverage both recurrence and convolution have recently been introduced for learning video representation.", "startOffset": 51, "endOffset": 115}, {"referenceID": 11, "context": "Consequently, Recurrent Convolution Networks (RCN) (Srivastava et al., 2015; Donahue et al., 2014; Ng et al., 2015) that leverage both recurrence and convolution have recently been introduced for learning video representation.", "startOffset": 51, "endOffset": 115}, {"referenceID": 19, "context": "Consequently, Recurrent Convolution Networks (RCN) (Srivastava et al., 2015; Donahue et al., 2014; Ng et al., 2015) that leverage both recurrence and convolution have recently been introduced for learning video representation.", "startOffset": 51, "endOffset": 115}, {"referenceID": 18, "context": "CNNs, however, hierachically build-up spatial invariance through pooling layers (LeCun et al., 1998; Simonyan & Zisserman, 2014b) as Figure 2 highlights.", "startOffset": 80, "endOffset": 129}, {"referenceID": 8, "context": "We extend the GRU-RNN model (Cho et al., 2014) and replace the fully-connected RNN linear product operation with a convolution.", "startOffset": 28, "endOffset": 46}, {"referenceID": 8, "context": "We extend the GRU-RNN model (Cho et al., 2014) and replace the fully-connected RNN linear product operation with a convolution. Our GRU-extension therefore encodes the locality and temporal smoothness prior of videos directly in the model structure. We evaluate our solution on UCF101 human action recognition from Soomro et al. (2012) as well as YouTube2text video captioning dataset from Chen & Dolan (2011).", "startOffset": 29, "endOffset": 336}, {"referenceID": 8, "context": "We extend the GRU-RNN model (Cho et al., 2014) and replace the fully-connected RNN linear product operation with a convolution. Our GRU-extension therefore encodes the locality and temporal smoothness prior of videos directly in the model structure. We evaluate our solution on UCF101 human action recognition from Soomro et al. (2012) as well as YouTube2text video captioning dataset from Chen & Dolan (2011). Experiments show that leveraging \u201cpercepts\u201d at mutiple resolutions to model temporal variation improve over baseline model with respective gain of 3.", "startOffset": 29, "endOffset": 410}, {"referenceID": 2, "context": "RNNs are known to be difficult to train due to the exploding or vanishing gradient effect (Bengio et al., 1994).", "startOffset": 90, "endOffset": 111}, {"referenceID": 8, "context": "However, variants of RNNs such as Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) or Gated Recurrent Units (GRU) (Cho et al., 2014) have empirically demonstrated their ability to model long-term temporal dependency in various task such as machine translation or image/video caption generation.", "startOffset": 128, "endOffset": 146}, {"referenceID": 9, "context": "In this paper, we will mainly focus on the GRU as they have shown similar performance than LSTM but with a lower memory requirement (Chung et al., 2014).", "startOffset": 132, "endOffset": 152}, {"referenceID": 8, "context": "To implement the RNN recurrent function \u03c6, we propose to leverage Gated Recurrent Unit (Cho et al., 2014).", "startOffset": 87, "endOffset": 105}, {"referenceID": 15, "context": "Deep learning approaches have recently been used to learn video representation and produced stateof-art results (Karpathy et al., 2014; Simonyan & Zisserman, 2014a; Wang et al., 2015b; Tran et al., 2014).", "startOffset": 112, "endOffset": 203}, {"referenceID": 30, "context": "Deep learning approaches have recently been used to learn video representation and produced stateof-art results (Karpathy et al., 2014; Simonyan & Zisserman, 2014a; Wang et al., 2015b; Tran et al., 2014).", "startOffset": 112, "endOffset": 203}, {"referenceID": 17, "context": "However, unlike image classification (Simonyan & Zisserman, 2014b), CNNs did not yield large improvement over these traditional methods (Lan et al., 2014) highlighting the difficulty of learning video representation even with large training dataset.", "startOffset": 136, "endOffset": 154}, {"referenceID": 15, "context": "Deep learning approaches have recently been used to learn video representation and produced stateof-art results (Karpathy et al., 2014; Simonyan & Zisserman, 2014a; Wang et al., 2015b; Tran et al., 2014). Karpathy et al. (2014); Tran et al.", "startOffset": 113, "endOffset": 228}, {"referenceID": 15, "context": "Deep learning approaches have recently been used to learn video representation and produced stateof-art results (Karpathy et al., 2014; Simonyan & Zisserman, 2014a; Wang et al., 2015b; Tran et al., 2014). Karpathy et al. (2014); Tran et al. (2014) proposed to use 3D CNN learn a video representations, leveraging large training datasets such as Sport 1 Million.", "startOffset": 113, "endOffset": 248}, {"referenceID": 15, "context": "Deep learning approaches have recently been used to learn video representation and produced stateof-art results (Karpathy et al., 2014; Simonyan & Zisserman, 2014a; Wang et al., 2015b; Tran et al., 2014). Karpathy et al. (2014); Tran et al. (2014) proposed to use 3D CNN learn a video representations, leveraging large training datasets such as Sport 1 Million. However, unlike image classification (Simonyan & Zisserman, 2014b), CNNs did not yield large improvement over these traditional methods (Lan et al., 2014) highlighting the difficulty of learning video representation even with large training dataset. Simonyan & Zisserman (2014a) introduced a two-stream framework where they train CNNs independently on RGB and optical flow inputs.", "startOffset": 113, "endOffset": 641}, {"referenceID": 15, "context": "Deep learning approaches have recently been used to learn video representation and produced stateof-art results (Karpathy et al., 2014; Simonyan & Zisserman, 2014a; Wang et al., 2015b; Tran et al., 2014). Karpathy et al. (2014); Tran et al. (2014) proposed to use 3D CNN learn a video representations, leveraging large training datasets such as Sport 1 Million. However, unlike image classification (Simonyan & Zisserman, 2014b), CNNs did not yield large improvement over these traditional methods (Lan et al., 2014) highlighting the difficulty of learning video representation even with large training dataset. Simonyan & Zisserman (2014a) introduced a two-stream framework where they train CNNs independently on RGB and optical flow inputs. While the flow stream focuses only on motion information, the RGB can leverage 2D CNN pre-trained on image datasets. Based on the Two Stream representation, Wang et al. (2015a) extracted deep feature and conducted trajectory constrained pooling to aggregate convolutional feature as video representations.", "startOffset": 113, "endOffset": 920}, {"referenceID": 18, "context": "Ng et al. (2015); Donahue et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 11, "context": "(2015); Donahue et al. (2014) applied an RNN on top of the the two-stream framework, while Srivastava et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 11, "context": "(2015); Donahue et al. (2014) applied an RNN on top of the the two-stream framework, while Srivastava et al. (2015) proposed, in addition, to investigate the benefit of learning a video representation in an unsupervised manner.", "startOffset": 8, "endOffset": 116}, {"referenceID": 11, "context": "(2015); Donahue et al. (2014) applied an RNN on top of the the two-stream framework, while Srivastava et al. (2015) proposed, in addition, to investigate the benefit of learning a video representation in an unsupervised manner. Previous works on this topic tend to focus only on high-level CNN \u201cvisual percepts\u201d. By contrast, our approach proposes to leverage visual \u201cpercepts\u201d extracted from different layers in the 2D-CNN. Recently, Shi et al. (2015) also proposed to leverage convolutional units inside an RNN network.", "startOffset": 8, "endOffset": 453}, {"referenceID": 25, "context": "We evaluate our approach on the UCF101 dataset Soomro et al. (2012). This dataset has 101 action classes spanning over 13320 YouTube videos clips.", "startOffset": 47, "endOffset": 68}, {"referenceID": 14, "context": "To perform proper hyperparameter seach, we use the videos from the UCF-Thumos validation split Jiang et al. (2014) as the validation set.", "startOffset": 95, "endOffset": 115}, {"referenceID": 33, "context": "One could further improve the classification performance by considering other inputs such as the video optical flow Simonyan & Zisserman (2014a); Wang et al. (2015b). However, the main purpose of this experiment is to demonstrate the benefit of visual clues from multiple CNN layers when modeling the temporal information in a videos.", "startOffset": 146, "endOffset": 166}, {"referenceID": 33, "context": "One could further improve the classification performance by considering other inputs such as the video optical flow Simonyan & Zisserman (2014a); Wang et al. (2015b). However, the main purpose of this experiment is to demonstrate the benefit of visual clues from multiple CNN layers when modeling the temporal information in a videos. We design and evaluate three RCN architectures for action recognition. We use a VGG-16 CNN pretrained on ImageNet (Simonyan & Zisserman, 2014b) that is fine-tuned on the UCF-101 dataset, following the protocol in Wang et al. (2015b). We then extract the convolution maps from pool2, pool3, pool4, pool5 layers and the fully-connected map from layer fc-7 (which can be view as a feature map with a 1 \u00d7 1 spatial dimension).", "startOffset": 146, "endOffset": 568}, {"referenceID": 11, "context": "8 RGB-Stream + LSTM Donahue et al. (2014) 71.", "startOffset": 20, "endOffset": 42}, {"referenceID": 11, "context": "8 RGB-Stream + LSTM Donahue et al. (2014) 71.1 RGB-Stream + LSTM + Unsupervised Srivastava et al. (2015) 77.", "startOffset": 20, "endOffset": 105}, {"referenceID": 11, "context": "8 RGB-Stream + LSTM Donahue et al. (2014) 71.1 RGB-Stream + LSTM + Unsupervised Srivastava et al. (2015) 77.7 Improved RGB Stream Wang et al. (2015b) 79.", "startOffset": 20, "endOffset": 150}, {"referenceID": 11, "context": "8 RGB-Stream + LSTM Donahue et al. (2014) 71.1 RGB-Stream + LSTM + Unsupervised Srivastava et al. (2015) 77.7 Improved RGB Stream Wang et al. (2015b) 79.8 C3D one network Tran et al. (2014), 1 million videos as training 82.", "startOffset": 20, "endOffset": 190}, {"referenceID": 11, "context": "8 RGB-Stream + LSTM Donahue et al. (2014) 71.1 RGB-Stream + LSTM + Unsupervised Srivastava et al. (2015) 77.7 Improved RGB Stream Wang et al. (2015b) 79.8 C3D one network Tran et al. (2014), 1 million videos as training 82.3 C3D ensemble Tran et al. (2014), 1 million videos as training 85.", "startOffset": 20, "endOffset": 257}, {"referenceID": 11, "context": "8 RGB-Stream + LSTM Donahue et al. (2014) 71.1 RGB-Stream + LSTM + Unsupervised Srivastava et al. (2015) 77.7 Improved RGB Stream Wang et al. (2015b) 79.8 C3D one network Tran et al. (2014), 1 million videos as training 82.3 C3D ensemble Tran et al. (2014), 1 million videos as training 85.2 Deep networks Karpathy et al. (2014), 1 million videos as training 65.", "startOffset": 20, "endOffset": 329}, {"referenceID": 33, "context": "VGG-16 is the 2D spatial stream that is described in Wang et al. (2015b). We take the VGG-16 model, pretrained on Image-Net and fine-tune it on the UCF-101 dataset.", "startOffset": 53, "endOffset": 73}, {"referenceID": 30, "context": "C3D Tran et al. (2014) obtains the best performance on UCF-101 with 85.", "startOffset": 4, "endOffset": 23}, {"referenceID": 36, "context": "Following Yao et al. (2015b), we split the dataset into a training set of 1,200 video clips, a validation set of 100 clips and a test set consisting of the remaining clips.", "startOffset": 10, "endOffset": 29}, {"referenceID": 8, "context": "To perform video captioning, we use the so-called encoder-decoder framework Cho et al. (2014). Encoder maps input videos into abstract representations that precondition caption-generating decoder.", "startOffset": 76, "endOffset": 94}, {"referenceID": 8, "context": "To perform video captioning, we use the so-called encoder-decoder framework Cho et al. (2014). Encoder maps input videos into abstract representations that precondition caption-generating decoder. As for encoder, we compare both VGG-16 CNN and Bi-directional GRU-RCN. Both models have been fine-tuned on the UCF-101 dataset and therefore focus on detecting actions. To extract an abstract representation from a video, we sample K equally-space segments. When using the VGG16 encoder, we provide the fc7 layer activations of the each segment\u2019s first frame as the input to the text-decoder. For the GRU-RCN, we apply our model on the segment\u2019s 10 first frames. We concatenate the GRU-RCN hidden-representation from the last time step. The concatenated vector is given as the input to the text decoder. As it has been shown that characterizing entities in addition of action is important for the caption-generation task Yao et al. (2015a), we also use as encoder a CNN Szegedy et al.", "startOffset": 76, "endOffset": 936}, {"referenceID": 8, "context": "To perform video captioning, we use the so-called encoder-decoder framework Cho et al. (2014). Encoder maps input videos into abstract representations that precondition caption-generating decoder. As for encoder, we compare both VGG-16 CNN and Bi-directional GRU-RCN. Both models have been fine-tuned on the UCF-101 dataset and therefore focus on detecting actions. To extract an abstract representation from a video, we sample K equally-space segments. When using the VGG16 encoder, we provide the fc7 layer activations of the each segment\u2019s first frame as the input to the text-decoder. For the GRU-RCN, we apply our model on the segment\u2019s 10 first frames. We concatenate the GRU-RCN hidden-representation from the last time step. The concatenated vector is given as the input to the text decoder. As it has been shown that characterizing entities in addition of action is important for the caption-generation task Yao et al. (2015a), we also use as encoder a CNN Szegedy et al. (2014), pretrained on ImageNet, that focuses on detecting static visual object categories.", "startOffset": 76, "endOffset": 988}, {"referenceID": 8, "context": "To perform video captioning, we use the so-called encoder-decoder framework Cho et al. (2014). Encoder maps input videos into abstract representations that precondition caption-generating decoder. As for encoder, we compare both VGG-16 CNN and Bi-directional GRU-RCN. Both models have been fine-tuned on the UCF-101 dataset and therefore focus on detecting actions. To extract an abstract representation from a video, we sample K equally-space segments. When using the VGG16 encoder, we provide the fc7 layer activations of the each segment\u2019s first frame as the input to the text-decoder. For the GRU-RCN, we apply our model on the segment\u2019s 10 first frames. We concatenate the GRU-RCN hidden-representation from the last time step. The concatenated vector is given as the input to the text decoder. As it has been shown that characterizing entities in addition of action is important for the caption-generation task Yao et al. (2015a), we also use as encoder a CNN Szegedy et al. (2014), pretrained on ImageNet, that focuses on detecting static visual object categories. As for the decoder, we use LSTM text-generator with soft-attention on the video temporal frames Yao et al. (2015b).", "startOffset": 76, "endOffset": 1187}, {"referenceID": 20, "context": "6801 GoogleNet + HRNE (Pan et al., 2015) 0.", "startOffset": 22, "endOffset": 40}, {"referenceID": 38, "context": "321 VGG + p-RNN (Yu et al., 2015) 0.", "startOffset": 16, "endOffset": 33}, {"referenceID": 38, "context": "311 VGG + C3D + p-RNN (Yu et al., 2015) 0.", "startOffset": 22, "endOffset": 39}, {"referenceID": 20, "context": "6801 GoogleNet + HRNE (Pan et al., 2015) 0.436 0.321 VGG + p-RNN (Yu et al., 2015) 0.443 0.311 VGG + C3D + p-RNN (Yu et al., 2015) 0.499 0.326 Soft-attention Yao et al. (2015b) 0.", "startOffset": 23, "endOffset": 177}, {"referenceID": 20, "context": "6801 GoogleNet + HRNE (Pan et al., 2015) 0.436 0.321 VGG + p-RNN (Yu et al., 2015) 0.443 0.311 VGG + C3D + p-RNN (Yu et al., 2015) 0.499 0.326 Soft-attention Yao et al. (2015b) 0.4192 0.2960 0.5167 Venugopalan et al. Venugopalan et al. (2015) 0.", "startOffset": 23, "endOffset": 243}, {"referenceID": 20, "context": "6801 GoogleNet + HRNE (Pan et al., 2015) 0.436 0.321 VGG + p-RNN (Yu et al., 2015) 0.443 0.311 VGG + C3D + p-RNN (Yu et al., 2015) 0.499 0.326 Soft-attention Yao et al. (2015b) 0.4192 0.2960 0.5167 Venugopalan et al. Venugopalan et al. (2015) 0.3119 0.2687 + Extra Data (Flickr30k, COCO) 0.3329 0.2907 Thomason et al. Thomason et al. (2014) 0.", "startOffset": 23, "endOffset": 341}, {"referenceID": 36, "context": "Representations obtained with the proposed RCN architecture combined with decoders from Yao et al. (2015b) offer a significant performance boost, reaching the performance of the other state-ofthe-art models.", "startOffset": 88, "endOffset": 107}, {"referenceID": 38, "context": "According to the BLEU metric, we also outperform other approaches that uses more complex decoder scheme such as spatial and temporal attention decoder (Yu et al., 2015) or a Hierarchical RNN decoder (Pan et al.", "startOffset": 151, "endOffset": 168}, {"referenceID": 20, "context": ", 2015) or a Hierarchical RNN decoder (Pan et al., 2015) Our approach is on par with Yu et al.", "startOffset": 38, "endOffset": 56}, {"referenceID": 18, "context": "These are BLEU in Papineni et al. (2002), METEOR in Denkowski & Lavie (2014) and CIDEr in Vedantam et al.", "startOffset": 18, "endOffset": 41}, {"referenceID": 18, "context": "These are BLEU in Papineni et al. (2002), METEOR in Denkowski & Lavie (2014) and CIDEr in Vedantam et al.", "startOffset": 18, "endOffset": 77}, {"referenceID": 18, "context": "These are BLEU in Papineni et al. (2002), METEOR in Denkowski & Lavie (2014) and CIDEr in Vedantam et al. (2014). We use the evaluation script prepared and introduced in Chen et al.", "startOffset": 18, "endOffset": 113}, {"referenceID": 6, "context": "We use the evaluation script prepared and introduced in Chen et al. (2015). All models are early-stopped based on the negative-log-likelihood (NLL) of the validation set.", "startOffset": 56, "endOffset": 75}, {"referenceID": 6, "context": "We use the evaluation script prepared and introduced in Chen et al. (2015). All models are early-stopped based on the negative-log-likelihood (NLL) of the validation set. We then select the model that performs best on the validation set according to the metric at consideration. The first two lines of Table 2 compare the performances of the VGG-16 and Bi-direcitonal GRURCN encoder. Results clearly show the superiority of the Bi-Directional GRU-RCN Encoder as it outperforms the VGG-16 Encoder on all three metrics. In particular, GRU-RCN Encoder obtains a performance gain of 10% compared to the VGG-16 Encoder according to the BLEU metric. Combining our GRU-RCN Encoder that focuses on action with a GoogleNet Encoder that captures visual entities further improve the performances. Our GoogleNet + Bi-directional GRU-RCN approach signficantly outperforms Soft-attention Yao et al. (2015b) that relies on a GoogLeNet and cuboids-based 3D-CNN Encoder, in conjunction to a similar soft-attention decoder.", "startOffset": 56, "endOffset": 893}, {"referenceID": 6, "context": "We use the evaluation script prepared and introduced in Chen et al. (2015). All models are early-stopped based on the negative-log-likelihood (NLL) of the validation set. We then select the model that performs best on the validation set according to the metric at consideration. The first two lines of Table 2 compare the performances of the VGG-16 and Bi-direcitonal GRURCN encoder. Results clearly show the superiority of the Bi-Directional GRU-RCN Encoder as it outperforms the VGG-16 Encoder on all three metrics. In particular, GRU-RCN Encoder obtains a performance gain of 10% compared to the VGG-16 Encoder according to the BLEU metric. Combining our GRU-RCN Encoder that focuses on action with a GoogleNet Encoder that captures visual entities further improve the performances. Our GoogleNet + Bi-directional GRU-RCN approach signficantly outperforms Soft-attention Yao et al. (2015b) that relies on a GoogLeNet and cuboids-based 3D-CNN Encoder, in conjunction to a similar soft-attention decoder. This result indicates that our approach is able to offer more effective representions. According to the BLEU metric, we also outperform other approaches that uses more complex decoder scheme such as spatial and temporal attention decoder (Yu et al., 2015) or a Hierarchical RNN decoder (Pan et al., 2015) Our approach is on par with Yu et al. (2015), without the need of using a C3D-encoder that requires to be trained on large-scale video dataset.", "startOffset": 56, "endOffset": 1356}], "year": 2017, "abstractText": "We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call \u201cpercepts\u201d using Gated-Recurrent-Unit Recurrent Networks (GRUs). Our method relies on percepts that are extracted from all levels of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts, however, can lead to high-dimensionality video representations. To mitigate this effect and control the number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations. We empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler caption-decoder model and without extra 3D CNN features.", "creator": "LaTeX with hyperref package"}}}