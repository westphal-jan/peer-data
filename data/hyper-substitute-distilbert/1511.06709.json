{"id": "1511.06709", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Improving Neural Machine Translation Models with Monolingual Data", "abstract": "neural machine translation ( net ) has studied state - past - the equilibrium methods for several language pairs, probably nevertheless using parallel symbols for expression. monolingual data reflects an important role in recognizing fluency for brain - like statistical machine translation, and we allow specific use of monolingual tests underlying neural machine simulations ( sim ). in contrast the previous work, simulation integrates a separately trained input language model into an animation architecture, do succeed in encoder - binding mouse architectures already have one possible to learn the same information as a language mimic, that perhaps explore possible if include substantial training artifacts in the breeding procedure. through our use both monolingual data, scientists encounter systemic improvements with using ~ 15 ( + 2. 06 - - 47. 28 grammar ) query for spanish - & gt ; german, and for how deeply - resourced iwslt examinations \" turkish - & gt ; korean ( + 2. 1 - - 77. 4 math ), which new accurately - called - [ - art results. do also recall that click - switching on left - domain logic and parallel stimuli caused dramatic improvements establishing the model 14 task for english - & hk ; german.", "histories": [["v1", "Fri, 20 Nov 2015 17:58:37 GMT  (27kb)", "http://arxiv.org/abs/1511.06709v1", null], ["v2", "Thu, 17 Mar 2016 14:52:55 GMT  (32kb)", "http://arxiv.org/abs/1511.06709v2", null], ["v3", "Thu, 31 Mar 2016 19:54:58 GMT  (32kb)", "http://arxiv.org/abs/1511.06709v3", "fixed tokenization inconsistency in de-&gt;en evaluation"], ["v4", "Fri, 3 Jun 2016 15:09:54 GMT  (32kb)", "http://arxiv.org/abs/1511.06709v4", "accepted to ACL 2016; new section on effect of back-translation quality"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rico sennrich", "barry haddow", "alexandra birch"], "accepted": true, "id": "1511.06709"}, "pdf": {"name": "1511.06709.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["rico.sennrich@ed.ac.uk,", "a.birch@ed.ac.uk,", "bhaddow@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n06 70\n9v 1\n[ cs\n.C L\n] 2\n0 N\nov 2\n01 5"}, {"heading": "1 Introduction", "text": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Monolingual data plays an important role in boosting fluency for phrase-based statistical machine\nThe research results presented in this publication were conducted in the cooperation with Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute Poland.\ntranslation, and we investigate the use of monolingual data for neural machine translation (NMT).\nLanguage models trained on monolingual data have played a central role in statistical machine translation since the first IBM models (Brown et al., 1990). There are two major reasons for their importance. Firstly, word-based and phrase-based translation models make strong independence assumptions, with the probability of translation units estimated independently from context, and language models, by making different independence assumptions, can model how well these translation units fit together. Secondly, the amount of available monolingual data in the target language typically far exceeds the amount of parallel data, and models typically improve when trained on more data, or data more similar to the translation task.\nIn (possibly attention-based) encoder-decoder architectures for neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), the decoder is essentially an RNN language model that is conditioned on additional source context, so the first rationale, adding a language model to compensate for the independence assumptions of the translation model, does not apply. However, the data argument is still valid in NMT, and we expect monolingual data to be especially helpful if parallel data is sparse, or a poor fit for the translation task, for instance because of a domain mismatch.\nIn contrast to previous work, which integrates a separately trained RNN language model into the NMT model (G\u00fcl\u00e7ehre et al., 2015), we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to include monolingual training data in the training process without changing the neural network architecture. This makes our approach applicable to different NMT architectures and systems.\nThe main contributions of this paper are as follows:\n\u2022 we show that we can improve the machine translation quality by simply mixing monolingual target sentences into the training set of NMT systems.\n\u2022 we investigate two different methods to fill the source side of monolingual training instances: using a dummy source sentence, and using a source sentence obtained via backtranslation, which we call synthetic. We find that the latter is very effective.\n\u2022 we successfully adapt NMT models to a new domain by fine-tuning with either monolingual or parallel in-domain data."}, {"heading": "2 Neural Machine Translation", "text": "We follow the neural machine translation architecture by Bahdanau et al. (2014), which we will briefly summarize here. However, we note that our approach is not specific to this architecture.\nThe neural machine translation system is implemented as an encoder-decoder network with recurrent neural networks.\nThe encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014) that reads an input sequence x = (x1, ..., xm) and calculates a forward sequence of hidden states ( \u2212\u2192 h1, ..., \u2212\u2192 hm), and a backward sequence ( \u2190\u2212 h1, ..., \u2190\u2212 hm). The hidden states \u2212\u2192 hj and \u2190\u2212\nhj are concatenated to obtain the annotation vector hj .\nThe decoder is a recurrent neural network that predicts a target sequence y = (y1, ..., yn). Each word yi is predicted based on a recurrent hidden state si, the previously predicted word yi\u22121, and a context vector ci. ci is computed as a weighted sum of the annotations hj . The weight of each annotation hj is computed through an alignment model \u03b1ij , which models the probability that yi is aligned to xj . The alignment model is a singlelayer feedforward neural network that is learned jointly with the rest of the network through backpropagation.\nA detailed description can be found in (Bahdanau et al., 2014). Training is performed on a parallel corpus with stochastic gradient descent. For translation, a beam search with small beam size is employed."}, {"heading": "3 NMT Training with Monolingual Training Data", "text": "The main motivation for using more monolingual data for training encoder-decoder networks is to allow the output layer to learn better priors, and be better conditioned on the previous target words thanks to being trained on more (or more relevant) target-side training data. In contrast to (G\u00fcl\u00e7ehre et al., 2015), who train separate language models on monolingual training data and incorporate them into the neural network through shallow or deep fusion, our goal is to use monolingual data for training without increasing the complexity of the neural network, based on the fact that encoder-decoder neural networks (Sutskever et al., 2014; Bahdanau et al., 2014) already have the capability to condition the probability distribution of the next target word on the previous target words. We describe two strategies to do this: providing monolingual training examples with an empty (or dummy) source sentence, or providing monolingual training data with a synthetic source sentence that is obtained from automatically translating the target sentence into the source language, which we will refer to as backtranslation."}, {"heading": "3.1 Dummy Source Sentences", "text": "The first technique we employ is to treat monolingual training examples as parallel examples with empty input.\nDuring training, we use both parallel and monolingual training examples in the ratio 1-to-1, and randomly shuffle them. We define an epoch as one iteration through the parallel data set, and resample from the monolingual data set for every epoch. We pair monolingual sentences with a single-word dummy input <null> to allow processing of of both parallel and monolingual training examples with the same network graph.1 For monolingual minibatches2, we freeze the network parameters of the encoder and the attention model.\n1One could force the context vector ci to be 0 for monolingual training instances, but we found that this does not solve the main problem with this approach, discussed below.\n2Bahdanau et al. (2014) sorts sets of 20 minibatches according to length to improve speed, since training time is proportional to the longest sentence in a minibatch. This also sorts monolingual training examples together. If a minibatch contains both parallel and monolingual examples, we treat it as monolingual if the majority of training instances are monolingual.\nTraining with an empty input could be conceived as a form of dropout (Hinton et al., 2012), with the difference that the training instances that have the context vector dropped out constitute novel training data.\nOne problem with this integration of monolingual data is that we cannot arbitrarily increase the ratio of monolingual training instances, or finetune a model with only monolingual training data, because the network \u2018unlearns\u2019 its conditioning on the source context in the output layer, even if the encoder and attention model are fixed."}, {"heading": "3.2 Synthetic Source Sentences", "text": "To ensure that the output layer remains sensitive to the source context, and that good parameters are not unlearned from monolingual data, we propose to pair monolingual training instances with a synthetic source sentence from which a context vector can be approximated. We obtain these through back-translation, i.e. an automatic translation of the monolingual target text into the source language. In principle, we can use any MT system for this task.\nDuring training, we mix synthetic parallel text into the original (human-translated) parallel text and do not distinguish between the two. Importantly, only the source side of these additional training examples is synthetic, and the target side comes from the monolingual corpus."}, {"heading": "3.3 Unknown Words", "text": "G\u00fcl\u00e7ehre et al. (2015) determine the network vocabulary based on the parallel training data, and replace out-of-vocabulary words with a special UNK symbol. They remove monolingual sentences with more than 10% UNK symbols. In contrast, we represent unseen words as sequences of subword units (Sennrich et al., 2015). As a consequence, our model can potentially learn valuable information about previously unknown words from monolingual corpora, for instance whether a compound that can be formed via subword units is supported by the (monolingual) training data. In all experiments, we determine the network vocabulary (which includes subword units) based on the parallel training corpus, and keep the vocabulary fixed when adding monolingual or synthetic training instances."}, {"heading": "4 Evaluation", "text": "We evaluate NMT training on parallel text, and with additional monolingual data, on English\u2192German and Turkish\u2192English, using training and test data from WMT 15 and IWSLT 15 for English\u2192German, and IWSLT 14 for Turkish\u2192English."}, {"heading": "4.1 Data and Methods", "text": "We use Groundhog3 as the implementation of the NMT system for all experiments (Bahdanau et al., 2014; Jean et al., 2015a). We generally follow the settings and training procedure described by Sennrich et al. (2015). Unless we specify that we use early stopping, we train models for a week. Ensembles are sampled from the last 4 saved models of the training run (saved at intervals of 12 hours). Each of these 4 models is fine-tuned with fixed embeddings for 12 hours.\nFor English\u2192German, we report case-sensitive BLEU on detokenized text with mteval-v13a.pl for comparison to official WMT and IWSLT results. For Turkish\u2192English, we report case-sensitive BLEU on tokenized text with multi-bleu.perl for comparison to results by G\u00fcl\u00e7ehre et al. (2015)."}, {"heading": "4.1.1 English\u2192German", "text": "As parallel training data, we use data provided by the 2015 Workshop on Statistical Machine Translation (WMT 15) (Bojar et al., 2015)4. We use the German News Crawl corpora as additional training data for the experiments with monolingual data. The amount of training data is shown in Table 1.\nFor the experiments with synthetic parallel data, we back-translate a random sample of 3 600 000 sentences from the German monolingual data set into English. The German\u2192English system used for this has been trained on the parallel data, using a single model trained with the same architecture\n3https://github.com/sebastien-j/LV_groundhog 4http://www.statmt.org/wmt15/\nas our baseline English\u2192German system. Translation took about a week on an NVIDIA Titan Black GPU. For reference, the German\u2192English system achieves a BLEU score of 25.0 on newstest2015, and is thus behind the SOTA of 29.3 for that translation direction (Haddow et al., 2015). We leave it to future work to explore how sensitive NMT training with synthetic data is to the quality of the back-translation.\nWe tokenize and truecase the training data, and represent rare words via BPE (Sennrich et al., 2015). Specifically, we follow Sennrich et al. (2015) in performing BPE on the joint vocabulary with 89 500 merge operations. The network vocabulary size is 90 000.\nWe also perform experiments on the IWSLT 15 test sets to investigate a cross-domain setting.5 The test sets consist of TED talk transcripts. As indomain training data, IWSLT provides the WIT3 parallel corpus (Cettolo et al., 2012), which also consists of TED talks."}, {"heading": "4.1.2 Turkish\u2192English", "text": "We use data provided for the IWSLT 14 machine translation track (Cettolo et al., 2014), namely the WIT3 parallel corpus (Cettolo et al., 2012), which consists of TED talks, and the SETimes corpus (Tyers and Alperen, 2010).6 After removal of sentence pairs which contain empty lines or lines with a length ratio above 9, we retain 320 000 sentence pairs of training data. For the experiments with monolingual training data, we use the English LDC Gigaword corpus (Fifth Edition). The amount of training data is shown in Table 2. With only 320 000 sentences of parallel data available for training, this is a much lower-resourced translation setting than English\u2192German.\nG\u00fcl\u00e7ehre et al. (2015) segment the Turkish text with the morphology tool Zemberek, followed by a disambiguation of the morphological analysis (Sak et al., 2007), and removal of non-surface tokens produced by the analysis. We use the same\n5http://workshop2015.iwslt.org/ 6http://workshop2014.iwslt.org/\npreprocessing7 . For both Turkish and English, we represent rare words (or morphemes in the case of Turkish) as character bigram sequences (Sennrich et al., 2015). The 20 000 most frequent words/morphemes are left unsegmented. Reserving some symbols for character bigrams, the network has a vocabulary size of 23 000 symbols.\nTo obtain a synthetic parallel training set, we back-translate a random sample of 3 200 000 sentences from the Gigaword corpus, or about 10 times the amount of real parallel data. We use an English\u2192Turkish NMT system trained with the same settings as the Turkish\u2192English baseline system. Translation took about a week on an NVIDIA Titan Black GPU.\nWe found overfitting to be a bigger problem than with the larger English\u2192German data set, and follow G\u00fcl\u00e7ehre et al. (2015) in using Gaussian noise (stddev 0.001) (Graves, 2011), and dropout on the output layer (p=0.5) (Hinton et al., 2012). We also use early stopping, based on BLEU measured every three hours on tst2010, which we treat as development set. For Turkish\u2192English, we use gradient clipping with threshold 5, following G\u00fcl\u00e7ehre et al. (2015), in contrast to the threshold 1 that we use for English\u2192German, following (Jean et al., 2015a)."}, {"heading": "4.2 Results", "text": ""}, {"heading": "4.2.1 English\u2192German WMT 15", "text": "Table 3 shows English\u2192German results with WMT training and test data. We find that mixing parallel training data with monolingual data with a dummy source side in a ratio of 1-1 improves quality of single systems by 0.4\u20130.5 BLEU, and of ensemble systems by 1 BLEU. We train the system for twice as long as the baseline to provide the training algorithm with a similar amount of parallel training instances. To ensure that the quality improvement is due to the monolingual training instances, and not just increased training time, we also continued training our baseline system for another week, but saw no improvements in BLEU.\nIncluding synthetic data during training is very effective, and yields an improvement over our baseline by 2.8\u20133.4 BLEU. Our best ensemble system also outperforms a syntaxbased baseline (Sennrich and Haddow, 2015) by 1.2\u20132.1 BLEU. We also substantially outperform NMT results reported by Jean et al. (2015a)\n7https://github.com/orhanf/zemberekMorphTR\nand Luong et al. (2015), who previously reported SOTA result.8 We note that the difference is particularly large for single systems, and that our ensemble is presumably weaker because Luong et al. (2015) used 8 ensemble components, and trained them independently, whereas we sampled 4 ensemble components from the same training run."}, {"heading": "4.2.2 English\u2192German IWSLT 15", "text": "Table 4 shows English\u2192German results on IWSLT test sets. IWSLT test sets consist of TED talks, and are thus very dissimilar from the WMT test sets, which are news texts. We investigate if monolingual training data is especially valuable if it can be used to adapt a model to a new genre or domain, i.e. adapting a system trained on WMT data to translating TED talks.\nSystems 1 and 2 correspond to systems in Table 3, trained only on WMT data. System 2, trained on parallel and synthetic WMT data, obtains a BLEU score of 25.5 on tst2015. We observe that even a small amount of fine-tuning9, i.e. continued training of an existing model, on WIT data (200k training instances, or about 30 minutes of training on an NVIDIA Titan Black GPU) can adapt a system trained on WMT data to the TED domain. By back-translating the monolingual WIT corpus (using a German\u2192English system trained on WMT data, i.e. without in-domain knowledge), we obtain the synthetic data set WITsynth. A single epoch of fine-tuning on WITsynth (system 4) results in a BLEU score of 26.7 on tst2015, or an improvement of 1.2 BLEU. We observed no improvement from fine-tuning on WITmono, the monolin-\n8Luong et al. (2015) report 20.9 BLEU (tokenized) on newstest2014 with a single model, and 23.0 BLEU with an ensemble of 8 models. Our best single system achieves a tokenized BLEU (as opposed to untokenized scores reported in Table 3) of 23.8, and our ensemble reaches 25.0 BLEU.\n9We leave the word embeddings fixed for fine-tuning.\ngual TED corpus with dummy input (system 3). These adaptation experiments with monolingual data are slightly artificial in that parallel training data is available. System 5, which is finetuned with the original WIT training data, obtains a BLEU of 28.4 on tst2015, which is an improvement of 2.9 BLEU. While it is unsurprising that in-domain parallel data is more valuable than indomain monolingual data, we find it encouraging that NMT domain adaptation with monolingual data is possible, and effective, since there are settings where only monolingual in-domain data is available. Note that all results are obtained from single models, and we expect that higher scores are achievable with ensembles."}, {"heading": "4.2.3 Turkish\u2192English IWSLT 14", "text": "Table 5 shows results for Turkish\u2192English. On average, we saw an improvement of 0.6 BLEU on the test sets from adding monolingual data with a dummy source side in a 1-1 ratio, although we note a high variance between different test sets. Because of the small amount of available training data, we observe optimal BLEU after a relatively small number of epochs. We also experimented with higher ratios of monolingual data, but this led to decreased BLEU scores.\nWith synthetic training data, we outperform the baseline by 2.7 BLEU on average, and also outperform results obtained via deep fusion by G\u00fcl\u00e7ehre et al. (2015) by 0.5 BLEU on average. To compare to what extent synthetic data has a regularization effect, even without novel training data, we also back-translate the target side of the parallel training text to obtain the training corpus parallelsynth. Mixing the original parallel corpus with parallelsynth (ratio 1-1) gives some improvement over the baseline (1.7 BLEU on average), but the novel monolingual training data from the Gigaword corpus gave higher improvements, despite\nbeing out-of-domain in relation to the test sets. We speculate that novel in-domain monolingual data would lead to even higher improvements."}, {"heading": "4.2.4 Analysis", "text": "We previously indicated that overfitting is a concern with our baseline system, especially on small data sets of several hundred thousand training\nsentences, despite the regularization employed. This overfitting is illustrated in Figure 1, which plots training and development set cross-entropy by training time for Turkish\u2192English models. For comparability, we measure training set crossentropy for all models on the same random sample of the parallel training set. We can see that the model trained on only parallel training data quickly overfits, while all three monolingual data sets (parallelsynth , GWmono, or GWsynth) delay overfitting, and give better perplexity on the development set. Adding synthetic training data results in better cross-entropy than adding monolingual data with a dummy source side on both the training and the development set, even though the synthetic data set is smaller.\nFigure 2 shows cross-entropy for English\u2192German, comparing the system trained on only parallel data and the system that includes synthetic training data. Since more training data is available for English\u2192German, there is no indication that overfitting happens during the first 40 million training instances (or 7 days of training); while both systems obtain comparable training set cross-entropies, the system with synthetic data reaches a lower cross-entropy on the development set. We note that the inclusion of WMTsynth gave particularly high gains for the WMT test sets (2.9 BLEU on newstest2015; 23.6\u219226.5), and more modest gains on TED test sets (1.5 BLEU on tst2015; 24.0\u219225.5). We speculate that our WMT experiments are also affected by a domain adaptation effect, which partially explains the strong improvements we observe. Both the WMT test sets and the News Crawl corpora which we used as monolingual data come from the same source, a web crawl of newspaper articles.10 In contrast, News Crawl is out-of-domain for the IWSLT test sets.\nOne salient aspect of our NMT models is that they use subword units to represent rare words, and can thus produce words that do not occur in the training set. Some of those novel words are plausible, while others are not, and monolingual data should improve fluency of subword units, and create more words via subword units that are attested in monolingual data. We compare the NMT systems\u2019 unigram precision and recall for in-domain and out-of-domain words, con-\n10The WMT test sets are held-out from the News Crawl corpora.\nsidering both the target vocabulary of the original (parallel) training corpus, and the one enhanced with monolingual data. Results are shown in Table 6. We observe strong improvements in recall for words that do not occur in the parallel training text (35.8%\u219238.9% for words that occur in the synthetic training data; 19.9%\u219223.8% for OOVs), which supports our expectation that the system trained with additional synthetic data learns to produce more of the words observed in the target side of the synthetic data set, and also improves at generalizing to produce OOVs from subword units. As an example, consider the German compound Achillessehnenproblemen \u2018achilles tendon problems\u2019, which is unseen in the parallel text, but occurs in the subsample of the monolingual data used for synthetic data. The baseline system produces Ach|ill|es|sig|-|Problemen, mistranslating tendon as the nonsensical sig instead of sehnen, whereas the system +synthetic correctly produces Ach|ill|ess|ehn|en|problemen.\nThe number of target-side OOVs in the test set is relatively small, and we do not claim that the improvement to the translation of rare words is the main cause of the overall improvement in translation quality. Instead, we see it as a proxy of sentence-level fluency, which is less amenable to analysis."}, {"heading": "4.2.5 Summary", "text": "We find that adding monolingual training instances with a dummy source sentence is a successful and cheap strategy to improve translation quality. However, pairing monolingual data with a synthetic source side, obtained via backtranslation, is even more effective.\nIf there is a domain mismatch between the training and the test set, even small amounts of in-domain training data (monolingual with backtranslation, or parallel) can substantially boost\nNMT performance through fine-tuning on the indomain data."}, {"heading": "5 Related Work", "text": "To our knowledge, the integration of monolingual data for purely neural machine translation architectures was first investigated by (G\u00fcl\u00e7ehre et al., 2015), who train monolingual language models independently, and then integrate them during decoding through reranking (shallow fusion), or by adding the recurrent hidden state of the language model to the decoder state of the encoder-decoder network, with an additional controller mechanism that controls the magnitude of the LM signal (deep fusion). In deep fusion, the controller parameters and output parameters are tuned on further parallel training data, but the language model parameters are fixed during the finetuning stage. Jean et al. (2015b) also report on experiments with reranking of NMT output with a 5-gram language model, but improvements are small, with improvements between 0.1\u20130.5 BLEU reported.\nThe back-translation of monolingual target data into the source language to produce synthetic parallel text has been previously explored for phrase-based SMT (Bertoldi and Federico, 2009; Lambert et al., 2011). We note that our motivation is slightly different. Whereas previous work aims to adapt the probabilities of phrasebased translation models to a new domain based on in-domain monolingual data, our primary goal is not (only) domain adaptation, but to include monolingual training data into NMT training without requiring architectural changes. For NMT, we find that even out-of-domain synthetic parallel data is immensely helpful, like in the case of Turkish\u2192English, where we add synthetic parallel data from the (out-of-domain) Gigaword corpus to the (in-domain) WIT3 training corpus."}, {"heading": "6 Conclusion", "text": "In this paper, we propose two simple methods to use monolingual training data during training of NMT systems, with no changes to the network architecture. Providing training examples with dummy source context was successful to some extent, but we achieve substantial gains in all tasks, and new SOTA results, via back-translation of monolingual target data into the source language, and treating this synthetic data as additional train-\ning data. We also show that small amounts of indomain monolingual data, back-translated into the source language, can be effectively used for domain adaptation.\nWhile our experiments did make use of monolingual training data, we only used a small random sample of the available data, especially for the experiments with synthetic parallel data. It is conceivable that larger synthetic data sets, or data sets obtained via data selection, will provide bigger performance benefits.\nBecause we do not change the neural network architecture to integrate monolingual training data, our approach can be easily applied to other NMT systems. We expect differences in effectiveness depending on the amount (and similarity to the test set) of available parallel and monolingual data, the quality of the MT system used for back-translation, and the extent of overfitting of the baseline model. Future work will explore the effectiveness of our approach in more settings."}, {"heading": "Acknowledgments", "text": "The research results presented in this publication were conducted in the cooperation with Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute Poland. This project received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement 645452 (QT21)."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Domain adaptation for statistical machine translation with monolingual resources", "author": ["Bertoldi", "Federico2009] Nicola Bertoldi", "Marcello Federico"], "venue": "In Proceedings of the Fourth Workshop on Statistical Machine Translation StatMT 09", "citeRegEx": "Bertoldi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bertoldi et al\\.", "year": 2009}, {"title": "Findings of the 2015 Workshop on Statistical Machine Translation", "author": ["Turchi."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 1\u201346, Lisbon, Portugal. Association for Computational Linguistics.", "citeRegEx": "Turchi.,? 2015", "shortCiteRegEx": "Turchi.", "year": 2015}, {"title": "WIT: Web Inventory of Transcribed and Translated Talks", "author": ["Christian Girardi", "Marcello Federico"], "venue": "In Proceedings of the 16 Conference of the European Association for Machine Translation (EAMT),", "citeRegEx": "Cettolo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Report on the 11th IWSLT Evaluation Campaign, IWSLT", "author": ["Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Marcello Federico"], "venue": "In Proceedings of the 11th Workshop on Spoken Language Translation,", "citeRegEx": "Cettolo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Transla", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Combined Spoken Language Translation", "author": ["Cettolo", "Marcello Federico."], "venue": "International Workshop on Spoken Language Translation, pages 57\u201364, Lake Tahoe, CA, USA.", "citeRegEx": "Cettolo and Federico.,? 2014", "shortCiteRegEx": "Cettolo and Federico.", "year": 2014}, {"title": "Practical Variational Inference for Neural Networks", "author": ["Alex Graves"], "venue": "In J. ShaweTaylor,", "citeRegEx": "Graves.,? \\Q2011\\E", "shortCiteRegEx": "Graves.", "year": 2011}, {"title": "The Edinburgh/JHU Phrase-based Machine Translation Systems", "author": ["Haddow et al.2015] Barry Haddow", "Matthias Huck", "Alexandra Birch", "Nikolay Bogoychev", "Philipp Koehn"], "venue": "WMT", "citeRegEx": "Haddow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Haddow et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580", "author": ["lan Salakhutdinov"], "venue": null, "citeRegEx": "Salakhutdinov.,? \\Q2012\\E", "shortCiteRegEx": "Salakhutdinov.", "year": 2012}, {"title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "author": ["Jean et al.2015a] S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "2015b. Montreal Neural Machine Translation Systems for WMT\u201915", "author": ["Jean et al.2015b] S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Investigations on Translation Model Adaptation Using Monolingual Data", "author": ["Holger Schwenk", "Christophe Servan", "Sadaf Abdul-Rauf"], "venue": "In Proceedings of the Sixth Workshop on Statistical Machine Translation,", "citeRegEx": "Lambert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lambert et al\\.", "year": 2011}, {"title": "Effective Approaches to Attention-based Neural Machine Translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Morphological Disambiguation of Turkish Text with Perceptron Algorithm", "author": ["Sak et al.2007] Ha\u015fim Sak", "Tunga G\u00fcng\u00f6r", "Murat Sara\u00e7lar"], "venue": "CICLing", "citeRegEx": "Sak et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2007}, {"title": "A Joint Dependency Model of Morphological and Syntactic Structure for Statistical Machine Translation", "author": ["Sennrich", "Haddow2015] Rico Sennrich", "Barry Haddow"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural Machine Translation of Rare Words with Subword Units. CoRR, abs/1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Sys-", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "SETimes: A parallel corpus of Balkan languages", "author": ["Tyers", "Alperen2010] Francis M. Tyers", "Murat S. Alperen"], "venue": "In Workshop on Exploitation", "citeRegEx": "Tyers et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tyers et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 17, "context": "In (possibly attention-based) encoder-decoder architectures for neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), the decoder is essentially an RNN language model", "startOffset": 91, "endOffset": 138}, {"referenceID": 0, "context": "In (possibly attention-based) encoder-decoder architectures for neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), the decoder is essentially an RNN language model", "startOffset": 91, "endOffset": 138}, {"referenceID": 0, "context": "We follow the neural machine translation architecture by Bahdanau et al. (2014), which we will briefly summarize here.", "startOffset": 57, "endOffset": 80}, {"referenceID": 5, "context": "The encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014) that reads an input sequence x = (x1, .", "startOffset": 73, "endOffset": 91}, {"referenceID": 0, "context": "A detailed description can be found in (Bahdanau et al., 2014).", "startOffset": 39, "endOffset": 62}, {"referenceID": 17, "context": ", 2015), who train separate language models on monolingual training data and incorporate them into the neural network through shallow or deep fusion, our goal is to use monolingual data for training without increasing the complexity of the neural network, based on the fact that encoder-decoder neural networks (Sutskever et al., 2014; Bahdanau et al., 2014) already have the capability to condition the probability distribution of the next target word on the previous target words.", "startOffset": 311, "endOffset": 358}, {"referenceID": 0, "context": ", 2015), who train separate language models on monolingual training data and incorporate them into the neural network through shallow or deep fusion, our goal is to use monolingual data for training without increasing the complexity of the neural network, based on the fact that encoder-decoder neural networks (Sutskever et al., 2014; Bahdanau et al., 2014) already have the capability to condition the probability distribution of the next target word on the previous target words.", "startOffset": 311, "endOffset": 358}, {"referenceID": 0, "context": "Bahdanau et al. (2014) sorts sets of 20 minibatches according to length to improve speed, since training time is proportional to the longest sentence in a minibatch.", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": "subword units (Sennrich et al., 2015).", "startOffset": 14, "endOffset": 37}, {"referenceID": 0, "context": "We use Groundhog3 as the implementation of the NMT system for all experiments (Bahdanau et al., 2014; Jean et al., 2015a).", "startOffset": 78, "endOffset": 121}, {"referenceID": 0, "context": "We use Groundhog3 as the implementation of the NMT system for all experiments (Bahdanau et al., 2014; Jean et al., 2015a). We generally follow the settings and training procedure described by Sennrich et al. (2015). Unless we specify that we use early stopping, we train models for a week.", "startOffset": 79, "endOffset": 215}, {"referenceID": 8, "context": "3 for that translation direction (Haddow et al., 2015).", "startOffset": 33, "endOffset": 54}, {"referenceID": 15, "context": "We tokenize and truecase the training data, and represent rare words via BPE (Sennrich et al., 2015).", "startOffset": 77, "endOffset": 100}, {"referenceID": 3, "context": "As indomain training data, IWSLT provides the WIT parallel corpus (Cettolo et al., 2012), which also consists of TED talks.", "startOffset": 66, "endOffset": 88}, {"referenceID": 6, "context": "3 for that translation direction (Haddow et al., 2015). We leave it to future work to explore how sensitive NMT training with synthetic data is to the quality of the back-translation. We tokenize and truecase the training data, and represent rare words via BPE (Sennrich et al., 2015). Specifically, we follow Sennrich et al. (2015) in performing BPE on the joint vocabulary with 89 500 merge operations.", "startOffset": 34, "endOffset": 333}, {"referenceID": 4, "context": "We use data provided for the IWSLT 14 machine translation track (Cettolo et al., 2014), namely the WIT parallel corpus (Cettolo et al.", "startOffset": 64, "endOffset": 86}, {"referenceID": 3, "context": ", 2014), namely the WIT parallel corpus (Cettolo et al., 2012), which consists of TED talks, and the SETimes corpus (Tyers and Alperen, 2010).", "startOffset": 40, "endOffset": 62}, {"referenceID": 14, "context": "(Sak et al., 2007), and removal of non-surface tokens produced by the analysis.", "startOffset": 0, "endOffset": 18}, {"referenceID": 15, "context": "For both Turkish and English, we represent rare words (or morphemes in the case of Turkish) as character bigram sequences (Sennrich et al., 2015).", "startOffset": 122, "endOffset": 145}, {"referenceID": 7, "context": "001) (Graves, 2011), and dropout on the output layer (p=0.", "startOffset": 5, "endOffset": 19}, {"referenceID": 7, "context": "001) (Graves, 2011), and dropout on the output layer (p=0.5) (Hinton et al., 2012). We also use early stopping, based on BLEU measured every three hours on tst2010, which we treat as development set. For Turkish\u2192English, we use gradient clipping with threshold 5, following G\u00fcl\u00e7ehre et al. (2015), in contrast to the threshold 1 that we use for English\u2192German, following (Jean et al.", "startOffset": 6, "endOffset": 297}, {"referenceID": 10, "context": "form NMT results reported by Jean et al. (2015a)", "startOffset": 29, "endOffset": 49}, {"referenceID": 13, "context": "and Luong et al. (2015), who previously reported SOTA result.", "startOffset": 4, "endOffset": 24}, {"referenceID": 13, "context": "and Luong et al. (2015), who previously reported SOTA result.8 We note that the difference is particularly large for single systems, and that our ensemble is presumably weaker because Luong et al. (2015) used 8 ensemble components, and trained them independently, whereas we sampled 4 ensemble components from the same training run.", "startOffset": 4, "endOffset": 204}, {"referenceID": 10, "context": "Jean et al. (2015b) also report on experiments with reranking of NMT output with a 5-gram language model, but improvements are small, with improvements between 0.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "The back-translation of monolingual target data into the source language to produce synthetic parallel text has been previously explored for phrase-based SMT (Bertoldi and Federico, 2009; Lambert et al., 2011).", "startOffset": 158, "endOffset": 209}], "year": 2017, "abstractText": "Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for neural machine translation (NMT). In contrast to previous work, which integrates a separately trained RNN language model into an NMT architecture (G\u00fcl\u00e7ehre et al., 2015), we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to include monolingual training data in the training process. Through our use of monolingual data, we obtain substantial improvements on the WMT 15 (+2.8\u20133.4 BLEU) task for English\u2192German, and for the low-resourced IWSLT 14 task Turkish\u2192English (+2.1\u20133.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task for English\u2192German.", "creator": "LaTeX with hyperref package"}}}