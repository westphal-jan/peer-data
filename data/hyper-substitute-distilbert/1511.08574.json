{"id": "1511.08574", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2015", "title": "A Stochastic Process Model of Classical Search", "abstract": "among real network games confirming the same heuristic information, proving dynamic memory a * is essentially sufficiently fast though possible except finding a convenient relational database. here, in many situations optimal solutions are simply infeasible, and technically expensive algorithms that trade maximum quality for speed are violated. in analogous paper, we recommend theoretical process of computational search above a metalevel basic language, the expected search model. for that given time instance, this establishes this self - defined notion of the smallest possible true search fastest search algorithm and denotes formal theoretical approach analyzing the design of preferences for availability criterion. next proceed to approximately solve fastest version providing the weighted first mdp for anytime algorithms and thus derive 1 novel fast algorithm, search by maximizing the discount rate of improvement ( spike ). * is shown explicitly outperform both little - an - living - art anytime solving algorithms on a parametrized stochastic tree model at some of the tested parameter values.", "histories": [["v1", "Fri, 27 Nov 2015 07:34:41 GMT  (239kb,D)", "http://arxiv.org/abs/1511.08574v1", "Submitted to ICAPS 2016"]], "COMMENTS": "Submitted to ICAPS 2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["dimitri klimenko", "hanna kurniawati", "marcus gallagher"], "accepted": false, "id": "1511.08574"}, "pdf": {"name": "1511.08574.pdf", "metadata": {"source": "CRF", "title": "A Stochastic Process Model of Classical Search", "authors": ["Dimitri Klimenko", "Hanna Kurniawati"], "emails": ["dimitri.klimenko@uqconnect.edu.au", "hannakur@uq.edu.au", "marcusg@uq.edu.au"], "sections": [{"heading": null, "text": "ation, with sufficient memory A* is essentially as fast as possible in finding a proven optimal solution. However, in many situations optimal solutions are simply infeasible, and thus search algorithms that trade solution quality for speed are desirable. In this paper, we formalize the process of classical search as a metalevel decision problem, the Abstract Search MDP. For any given optimization criterion, this establishes a well-defined notion of the best possible behaviour for a search algorithm and offers a theoretical approach to the design of algorithms for that criterion. We proceed to approximately solve a version of the Abstract Search MDP for anytime algorithms and thus derive a novel search algorithm, Search by Maximizing the Incremental Rate of Improvement (SMIRI). SMIRI is shown to outperform current state-of-the-art anytime search algorithms on a parametrized stochastic tree model for most of the tested parameter values."}, {"heading": "1 Introduction", "text": "Since the early 1960\u2019s, the general idea of best-first search has been fundamental to the design of a great many informed search algorithms. Of particular note is A* (Hart et al., 1968), which remains ubiquitous. At a basic level, all best-first search algorithms work in the same way: they incrementally build a search tree, in which nodes represent states in the problem\u2019s state space, and edges represent state transitions. The key atomic task in this process is edge expansion, which is the addition of a new node to the search tree via a previously unvisited edge.\nThe design of any best-first search algorithm comes down to a single fundamental question\u2014how does the algorithm decide which edge to expand\nar X\niv :1\n51 1.\n08 57\n4v 1\n[ cs\n.A I]\n2 7\nin each iteration? In doing so there are trade-offs between minimizing cost of solutions and the amount of time taken to find the solution. Most work addresses the question in an ad hoc manner, on the basis of diverse heuristic arguments. However, some researchers have taken more formal approaches to such questions; of particular note is recent work (Hay et al., 2012) which formulates the Bayesian selection problem as a metalevel decision problem. Unfortunately, the selection problem formalism is not sufficient to theoretically model the behaviour of tree-searching algorithms. In the words of Hay et al., \u201cA more ambitious goal is extending the formalism to trees\u2014in particular, achieving better sampling at non-root nodes, for which the purpose of sampling differs from that at the root.\u201d\nIn this paper, we propose a decision-theoretic framework, the Abstract Search Markov Decision Process (ASMDP), that achieves this ambitious goal for classical search problems (in which results of actions are fully deterministic). To show its benefit, we apply the framework to derive a novel model-based anytime search algorithm, called Search by Maximizing the Incremental Rate of Improvement (SMIRI). Experimental results on a random tree model indicate that SMIRI offers state-of-the-art anytime performance in this domain."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Metareasoning and Models of Search", "text": "Although we would like search algorithms to exhibit perfect rationality, in the real world this is not not practical due to the high computational complexity of many decision problems. To develop the most rational agent under limited computational resources, metareasoning applies decision-theoretic principles. The basic idea is simple\u2014as with object-level decisions, when choosing between computations an agent should select whichever one has the highest expected utility. Matheson (1968) showed that metareasoning can be formalized by combining an object-level model with a model of the computations to form the metalevel decision problem. The Decision-Theoretic A* algorithm (Russell and Wefald, 1988) applies metareasoning to real-time problem-solving search, but the utility estimates used in DTA* cannot obey the standard axioms of probability and utility theory and thus the approach lacks solid theoretical grounding (Russell and Wefald, 1988). More recent work (Hay et al., 2012) has formalized the metalevel decision problem for Bayesian selection problems. However, this formalism can only be applied at the root node of a search tree, and thus is not a full-fledged decision-theoretic framework for tree search algorithms. In contrast, we propose a probabilistic approach that can be applied to tree search.\nProbabilistic models of search have been proposed in the past, but they have typically been used to analyze time complexity of pre-existing\nalgorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem. One exception is the modelling by Mutchler (1986) of search with severely limited edge expansions, who showed theoretically that a simple best-first search algorithm was approximately optimal in a simple random tree model.\nThe aforementioned prior works have offered interesting domains to study the behaviour of search algorithms, and we continue this trend by formulating a novel random tree model that should be more representative of classical search problems. On the other hand another line of research focused on predicting the sizes of search trees has resulted in much more realistic models of search problems, primarily due to a focus on predicting real-world performance. The earliest work on this problem is that of Knuth (1975); later, Korf et al. (2001) were able to accurately predict the number of nodes expanded by the Iterative Deepening A* algorithm (Korf, 1985) on a number of different problems. This work was later extended by Zahavi et al. (2010) to increase its accuracy, by using conditional distributions over what Zahavi et al. refer to as a type system. In essence, the theoretical framework of this paper can be seen as building on the \u201ctype system\u201d idea by viewing it as a Bayesian model of the underlying search problem, and building a metalevel decision problem (the Abstract Search MDP) on top of it."}, {"heading": "2.2 Existing Algorithms", "text": "As mentioned in Section 1, different requirements with regard to solution quality and execution time result in a spectrum of very different search algorithms to meet them. The A* algorithm dominates one extreme within this spectrum\u2014when used with an admissible heuristic, A* ensures with complete certainty that the solution it returns will be optimal. Moreover, Dechter and Pearl (1985) demonstrate that A* is (down to the tie-breaking criterion) essentially the fastest general algorithm that can make this guarantee\u2014other algorithms can only do better by \u201ccheating\u201d on specific problem instances. However, despite many adaptations of A*, the issue of computational complexity remains hard to evade. For many kinds of problems, the time taken to find optimal solution will grow exponentially (or worse!) with the size of the problem, no matter how good the search algorithm.\nThe only way to avoid this fundamental issue is to relax the optimality requirement, e.g. to w -admissibility which requires the solution to be within a factor w of optimal. For a long time, the best-performing algorithm for this purpose was Weighted A* (Pohl, 1970); this approach multiples the heuristic in A* by a constant factor of w , resulting in an algorithm that typically runs faster than A\u2217 but is w -admissible. However, it tends to waste time exploring equally meritorious solutions in parallel, even when any one of those solutions would be acceptable (Kowalski, 1972; Pearl and Kim, 1982).\nExplicit Estimation Search (EES) (Thayer and Ruml, 2011) resolves this issue by maintaining multiple queues, so as to maintain admissibility while focusing search effort on particular candidates. An alternative suboptimality criterion is bounded-cost search, which requires an algorithm to find a solution with cost strictly less than a fixed cost bound C . The most prominent algorithm for this criterion is Potential Search (Stern et al., 2011), which aims to expand the node with the highest probability of having a path cost below the bound.\nIn this paper, we focus on the design of anytime algorithms (Dean and Boddy, 1988), which quickly find initial solutions and then gradually improve upon them. A number of anytime heuristic search algorithms have been proposed in the literature; the most basic is Anytime Weighted A* (Hansen et al., 1997), which is Weighted A* but continues to search after a solution is found, and prunes nods that cannot lead to an improvement. Anytime Repairing A* (Likhachev et al., 2003) adapts this by also decreasing the weight every time a solution is found. Finally, the latest state-of-the-art anytime algorithms have made further improvements by obviating the need for tuning the weight parameters; these are APTS/ANA* (Stern et al., 2011; van den Berg et al., 2011), an anytime version of Potential Search, and AEES (Thayer et al., 2012), an anytime version of EES. Experiments on random trees indicate that SMIRI solidly outperforms these other algorithms."}, {"heading": "3 Framework", "text": "To define the ASMDP\u2014the proposed metalevel decision problem for classical search\u2014we first define the search problem and abstract search problem, upon which it is built."}, {"heading": "3.1 The Classical Search Problem", "text": "Here we use a simplified version of the extensive form game (Osborne and Rubinstein, 1994, p. 89), so that the definition can easily be extended for all perfect information games, including MDPs and stochastic games. This definition is based on the idea of a history, which is a sequence of actions. In particular, for a given set of possible actions A, we denote by A\u2264\u03c9 the set of all possible histories; this can be split into the set of all infinite histories A\u03c9 and the set of all finite histories A<\u03c9. Additionally, we use the symbol a to denote appending an action to the end of a history, e.g. h a a.\nAt the core of any problem is a graph structure which we refer to as a search tree;\nDefinition 1. A search tree T is a set of histories which satisfies the following:\n\u2022 The empty sequence or root \u3008 \u3009 is a member of T .\n\u2022 Any prefix of a sequence in T is also in T .\n\u2022 Any infinite history all of whose prefixes are in T must also be in T .\nThus a search tree represents a valid tree structure, in which the nodes of the tree are sequences of actions and the edges are actions.\nDefinition 2. A search problem is a tuple G = (A, T , TGoal, c), where:\n\u2022 A is the action space\u2014the finite set of all possible actions.\n\u2022 T is a search tree, which we refer to as the complete search tree of the search problem. Histories in T are called legal, and finite legal histories are called states, which are members of the state space U = T \u2229A<\u03c9.\n\u2022 TGoal \u2282 U is the set of solutions to the search problem.\n\u2022 c : U \u2192 (0,\u221e) is the step cost function, which maps each history to the cost of the last step taken. From c, the path cost function g is defined as the sum of c(n \u2032) over all histories n \u2032 such that n \u2032 is a prefix of n.\nNote that we represent the problem by a tree of histories, the complete search tree, rather than the usual state-space graph (Russell and Norvig, 2010). Since the complete tree can be infinite, problems with cyclic state spaces can still be correctly represented; thus, this approach can still be used for searching on graphs, although it does limit the ability of the theoretical model to account for redundant paths."}, {"heading": "3.2 The Abstract Search Problem", "text": "Decision-theoretic analysis of classical search may appear to be straightforward, but in reality there is a subtle issue that needs to be resolved (Russell and Wefald, 1991): any information obtained solely as the result of a computation is information that, from the point of view of utility and probability theory, that agent already had. This fundamental issue is a rather difficult one; it continues to be an area of active research, sometimes referred to as the question of \u201clogical uncertainty\u201d (Soares and Fallenstein, 2015).\nTo address the issue, we reformulate the problem into a more standard question of environmental uncertainty, to which Bayesian probability and decision theory can be applied. The idea is based on the simple observation that a search algorithm never actually makes full use of the information available about the states in the search tree. Therefore, a search algorithm only operates on an abstracted representation of a state which we refer to as an abstract state residing in the feature space F . For generality, we consider the feature space to be an arbitrary measurable space, i.e. F = (F ,\u03a6), where F is the set of all features, and \u03a6 is a \u03c3-algebra of subsets of F , which defines the measurable subsets of F . The information available to the search algorithm is defined by the abstraction function \u03c6 : U \u2192 F , which maps a\nstate to the features the search algorithm will actually use\u2014a well-known example is the Manhattan distance heuristic function in the sliding tile puzzle. This idea can be seen as building upon the \u201ctype system\u201d idea of Lelis et al. (2013) by viewing abstract states as residing in an arbitrary space, and viewing the type system as representing a whole class of problems rather than just one. This means that the same partial search tree can be consistent with distinctly different search problems and thus the algorithm must be uncertain about the true underlying search problem. In this way, the process of search can be formally modeled as a process of Bayesian inference on a probability distribution over search problems. In particular, we view a search algorithm as starting with a prior distribution over search problems, which encodes initial knowledge about how features of states are related to one another; this distribution is continually narrowed via Bayesian updating upon observing the features of states.\nIn order to make Bayesian inference amenable, we require the algorithm\u2019s prior knowledge to satisfy the local directed Markov property over the structure of the search tree. In other words, the distribution of a subtree rooted at a particular node should be conditionally independent of everything outside of that subtree, given the features of that node. Although this might appear to be a severe restriction, it has intuitive appeal and still has the capacity to model complex dependencies by adding extra features to the feature space. Given the local directed Markov property, it follows that the prior can be defined entirely in terms of the conditional distributions of child abstract states given parent abstract states, i.e. a Markov kernel. With this, we define the abstract problem as:\nDefinition 3. An abstract problem is a tuple (A,F ,FGoal, x0, \u03ba, c\u0304), where\n\u2022 A is the action space, as per Definition 2.\n\u2022 F = (F ,\u03a6) is the feature space. This is a measurable space, i.e. F is the set of all possible abstract states, and \u03a6 is a \u03c3-algebra of subsets of F . This becomes the extended feature space F\u2217 = (F \u2217,\u03a6\u2217) by adding the illegal state \u0393 to represent illegal actions.\n\u2022 FGoal \u2282 F specifies the goal states. These are analogous to set of solutions TGoal of a non-abstract search problem, but reside in the feature space rather than the state space.\n\u2022 x0 \u2208 F is the initial state; as with the goal states, this is analogous to the root history \u3008 \u3009 but lies in the feature space.\n\u2022 \u03ba : (F \u2217 \u00d7A)\u00d7 \u03a6\u2217 \u2192 [0, 1] is the transition function, a Markov kernel from state-action pairs to the extended feature space, such that:\n\u2013 Any state/action pair must either map to the illegal state with probability 1, in which case it is illegal, or with probability 0, in which case it is legal.\n\u2013 Any action leading out of the illegal state is also illegal, i.e. \u03ba(\u0393, \u00b7, {\u0393}) \u2261 1.\nStates with no legal actions are referred to as a sink states, which includes \u0393 as well as the terminal states FTerm.\n\u2022 c\u0304 : F \u2192 (0,\u221e) specifies step costs, as in Definition 2.\nFor a given abstract problem (A,F ,FGoal, x0, \u03ba, c\u0304), the induced tree generation process is a stochastic process {\u03a8n | n \u2208 A<\u03c9}, where each \u03a8n represents the abstract state associated with a history n.\nDefinition 4. A realization of the tree generation process (or, equivalently, the underlying abstract problem), is a pair (U , \u03c6), where U specifies which histories from A<\u03c9 are considered to be legal, and \u03c6 : U \u2192 F is an abstraction function (per the previous definition), which maps each legal history to an abstract state.\nNotably, a realization (U , \u03c6) fully defines a search problem per Definition 2; the action space is the same, and the complete search tree T is simply U with the addition of any viable infinite histories. The solutions to the realized search problem are TGoal = \u03c6\u22121(FGoal), and its step cost function is c(n) = c\u0304(\u03c6(n)). Thus, with the tree generation process we have fulfilled the task of constructing a well-defined prior probability distribution over search problems. Moreover, this distribution factorizes neatly into a tree of conditional distributions (in essence, a finite or infinite Bayesian network); since the features of a state are observed by the search algorithm, a Bayesian update simply replaces unknown values for the random variables \u03a8n in the conditional distributions with known ones."}, {"heading": "3.3 The Abstract Search MDP (ASMDP)", "text": "In the context of the tree generation process as defined in the previous section, the process of search can be viewed as a Partially Observable Markov Decision Process (POMDP) in which the agent is unaware of the true underlying problem, but can observe the abstract states within its search tree.\nHowever, to avoid solving the full-blown POMDP directly, we instead apply the concept of sufficient information states (Hauskrecht, 2000) to define the ASMDP. Due to the Markov assumption (Section 3.2), the structure of the search tree and the observed feature values within that tree are a sufficient statistic.\nDefinition 5. Given an action set A, a partial search tree is a finite set of finite histories H \u2282 A<\u03c9 that is also a search tree (per Definition 1). A partial realization (H , \u03c6H ) of an abstract problem G\u0304 is a partial search tree H and a mapping \u03c6H : H \u2192 F such that x0 = \u03c6H (\u3008 \u3009), and for any n a a \u2208 H we have \u03ba(\u03c6H (n), a, {\u0393}) = 0. In other words, it is a partial search tree labelled with abstract states; the condition on \u03ba ensures that all histories in the partial realization must be legal, as illegal histories are not considered part of the search tree.\nNow, we can define the ASMDP.\nDefinition 6. The ASMDP for a given abstract problem G\u0304 is an MDP (S ,A ,T ,R, s0, \u03b3) over the space of partial realizations, where\n\u2022 S = {H , \u03c6H | (H , \u03c6H ) is a partial realization of G\u0304} is the state space; the tree H is the current search tree of the searching agent.\n\u2022 A = A<\u03c9 is the action space. Legal actions in the ASMDP correspond to expanding previously unexpanded out-edges within the search tree H , i.e. an action n a a is legal if and only if n \u2208 H , n a a 6\u2208 H , and \u03ba(\u03c6H (n), a, {\u0393}) = 0.\n\u2022 T is the transition function, which is a Markov kernel. For any given state s = (H , \u03c6H ) and legal action n a a, the next state s \u2032 takes the form s \u2032 = (H \u222a {n a a}, \u03c6H \u222a {(n a a,X )}), where X is a random variable whose distribution is specified by the transition function of the abstract game, \u03ba(\u03c6H (n), a, \u00b7).\n\u2022 R is the reward function, which specifies the reward obtained by the searching agent for every time step. The choice of reward function will depend upon what we wish to optimize for in the design of the search algorithm.\n\u2022 s0 = ({\u3008 \u3009}, \u03c60) is the initial state, where \u03c60(\u3008 \u3009) = x0. In other words, the initial state is a tree consisting of only the root node, labelled with the initial abstract state x0.\n\u2022 \u03b3 is the discount factor, which depends on the type of search being modeled.\nWithin this formalism, an optimal search algorithm corresponds directly to an optimal policy in the ASMDP.\nFor the case of anytime search, we view the search as being halted at an arbitrary time (which the agent is uncertain about), and evaluated based on the cost of the incumbent solution when the search is terminated. In particular, we assume that there is a constant probability p of the search terminating at time k + 1 given that it has continued for k steps. Notably,\nrather than explicitly build this probability of halting into the ASMDP, we can model this by using a discounting scheme where \u03b3 = 1 \u2212 p, and the reward is\nR(H , \u03c6H ) = \u2212min{Cmax , min n\u2208TGoal\u2229H g(n)}, (1)\ni.e. the negation of the cost of the best solution present within the search tree, up to an upper bound of Cmax . This reward is accumulated over time steps, but in conjunction with the discounting scheme the effect is that the total expected reward for the MDP is essentially a weighted average of solution costs at different times, weighted by how likely the search is to stop at that time."}, {"heading": "4 Approximately Solving the ASMDP", "text": "Although the ASMDP results in a well-defined notion of what constitutes an optimal search algorithm, most general-purpose MDP solvers are too slow to handle the ASMDP due to state and action spaces that grow hyperexponentially with the number of steps taken.\nHowever, by exploiting the local directed Markov property, we can derive an efficient approximate solver. The idea is analogous to index policies (Gittins and Jones, 1979) in multi-armed bandits: it independently computes a single quantity for each action and selects the action with the greatest index. Our index policy is derived based on the notion of incremental rate of improvement, denoted as r , which is the rate at which the incumbent cost Cinc improves over coming time steps (thus \u201crate of improvement\u201d), but only in the near future (thus \u201cincremental\u201d). 1\nTo derive an approximate solution to the ASMDP, we first note that an optimal policy of the ASMDP (i.e., an optimal search algorithm) should only ever expand edges that might either lead to a better solution, or be \u201cfollowed up\u201d on by the policy, as these are the only ways to gain utility. Consequently, the choice to expand an edge can be viewed as a \u201cmacro-action\u201d with outcomes o \u2208 O ; which are either successes o \u2208 Os , in which a better solution is found in the subtree for that edge and the cost bound is reduced, or failures o \u2208 Of in which the algorithm switches over to a different subtree without finding an improved solution. In particular, let s be a state of the ASMDP with incumbent cost Cinc , and \u03c0 be a policy that selects out-edge n a a in state s, and acts optimally otherwise. We describe an outcome o \u2208 O by three key parameters: its probability p(o), the subsequent resulting state s \u2032(o), the amount \u2206(o) by which the cost bound is reduced (if at all), and the number of steps t(o) taken to reach that outcome. This means that\n1Thayer et al. (2012) have argued against maximizing the rate of improvement, but their argument applies to long-term improvement and not the short-term criterion used by SMIRI.\nany outcome o is a series of t(o) steps during which the agent receives a reward of \u2212Cinc on every step, until finally reaching a new ASMDP state s \u2032 which might (or might not) have an improved cost bound. Then the value function of \u03c0 at state s can be expressed as\nV \u03c0(s) = \u2211 o p(o)\n[ \u03b3t(o)V \u03c0(s \u2032(o))\u2212 Cinc 1\u2212 \u03b3t(o)\n1\u2212 \u03b3\n] . (2)\nAs there will typically be a large number of available out-edges with few leading to improvements, for a failed outcome it is likely that V \u03c0(s \u2032) \u2248 V \u03c0(s). On the other hand, if the solution is improved the algorithm will gain by having a better solution for some time. In principle, the amount of utility this gains could depend on the particular cost bound and state, but as a rough approximation we assume that the utility gained is proportional to the reduction in the cost bound, \u2206, i.e. V \u03c0(s \u2032) \u2248 V \u03c0(s) + \u03be\u2206 for some \u03be. Hence Eq. (2) reduces to\nV \u03c0(s) \u2248\n\u2211 o p(o) [ \u03b3t(o)\u03be\u2206(o)\u2212 Cinc 1\u2212\u03b3 t(o) 1\u2212\u03b3 ] 1\u2212 \u2211 o p(o)\u03b3 t(o) . (3)\nTo further simplify this equation, we also assume that the times t(o) are much smaller than the discounting horizon, such that \u03b3t(o) \u2248 1 + t(o) ln \u03b3. In general, we expect that this assumption will be reasonable, as the overall time spent searching should be much greater than the time spent focusing on any one particular subtree for any one particular cost bound. Thus, we expect that Eq. (2) can be approximated as\nV \u03c0(s) \u2248 \u2212\u03be ln \u03b3 \u2211 o p(o)\u2206(o)\u2211 o p(o)t(o) \u2212 Cinc 1\u2212 \u03b3 . (4)\nThe incremental rate of imporvement r is then the expected value of \u2206 divided by the expected value of t , i.e.\nr(s,n a a) = \u2211 o p(o)\u2206(o)\u2211 o p(o)t(o) , (5)\nwhich is also the only term in Eq. (4) that depends on \u03c0. Consequently, a policy that always expands the edge with the maximal value of r is an approximately optimal policy for the ASMDP. At this point, we have not yet precisely defined r , since the outcomes o in Eq. (5) depend on the r -values themselves. In particular, under our current definition a \u201cfailure\u201d is when the algorithm switches over to a different subtree without improving, and this only happens when all unexpanded edges in the subtree being searched have lower r -values than the best out-edge outside of that subtree.\nAs specified, there is no obvious way to calculate r without knowing the optimal policy, since the stopping criterion for failure introduces an interdependency between r -values of parallel out-edges. However, we can derive an alternative form of r without this flaw by specifying a different stopping criterion for failures o \u2208 Of . In particular, define the thresholded incremental rate of improvement rt as the incremental rate of improvement obtained by searching optimally in a particular subtree until all out-edges in that subtree are below the threshold t , i.e. have rt(\u00b7 \u00b7 \u00b7 ) < t . Finally, we define the peak incremental rate of improvement r\u2217 as the maximum value of rt over all possible thresholds, i.e. r\u2217(\u00b7 \u00b7 \u00b7 ) = maxt\u2208[0,\u221e) rt(\u00b7 \u00b7 \u00b7 ). This corresponds to expanding all edges in a subtree that are equal to or better than the root edge of that subtree. The thresholded and peak r -values still depend on other r -values, but only those of their children; this results in a well-defined (via induction) notion of rt and r\u2217.\nMoreover, it is clear that selecting whichever out-edge has the highest r\u2217 value will maximise the overall incremental rate of improvement\u2014a policy that selects another edge first is clearly dominated by a similar policy that first expands the max-r\u2217 edge up to a threshold of r\u2217, and then does whatever the former policy did whenever that fails. This policy cannot be improved by including any edges below the threshold, or failing to include any above the threshold, since the form of Eq. (5) means that this necessarily results in a lower rate of improvement.\nFinally, due to the local directed Markov property, the distribution of edges within the subtree for n a a depends only on \u03c6(n) and a, while \u2206 additionally depends on the relative cost bound Cinc \u2212 g(n) within that subtree. Since the peak rate of improvement does not depend on any other aspects of the ASMDP state s, it can be expressed as a function r\u2217(Cinc \u2212 g(n), \u03c6(n), a). Consequently, a policy that always chooses to expand the out-edge naa with the maximal value of r\u2217(Cinc\u2212g(n), \u03c6(n), a) is an approximately optimal policy for the ASMDP. This is essentially a best-first search on r\u2217, although it is important to note that the values of r\u2217 will change whenever an improved solution is found, reducing the incumbent cost Cinc ."}, {"heading": "5 Search by Maximizing the Incremental Rate of Improvement", "text": "In order to make practical use of the approximately optimal policy of Section 4, we need an efficient method for computing the peak incremental rate of improvement r\u2217 for each equivalence class of edges e = (C , x , a); where C = Cinc\u2212g(n) is the cost bound relative to the parent node of the out-edge, x is the abstract state of that node, and a is the action along the edge. When path costs g(\u00b7) and abstract states x are limited to finite sets, Algorithm 1\ndescribes how to pre-compute r\u2217 in polynomial time. The key idea is that since g(n) can only increase along a path, the relative cost bound C will always decrease; thus r\u2217 can be computed via dynamic programming along its first argument. In particular, line 1 loops in increasing order of C , ensuring that r\u2217 values of descendant edge types are always computed before their ancestors.\nThe inner loop on line 2 loops over all x , a pairs for a given value of C , thus covering all possible equivalence classes of edges e = (C , x , a). In order to compute the true r\u2217 value of e, it is necessary to find which descendants of e fall below the threshold, and sum the possible outcomes over all of those edges. Lines 3-10 of the algorithm set all of the initial values for this calculation, and then lines 11-16 initialise the possible descendants to include with the children of (x , a), using the transition function \u03ba for the distribution of possible next-states y resulting from the x , a-transition. In order to correctly find the optimal threshold value r\u2217, the value of r\u2217(C , x , a) is calculated incrementally, gradually including all possible decendants (C \u2212 c\u0304(y), y , b) in order from the highest r\u2217(\u00b7 \u00b7 \u00b7 ) values to the lowest. This is the core function of the loop over lines 17-22; the process stops as soon as the current value of r\u2217 exceeds the r\u2217 values of all unused descendants.\nFor each equivalence class of edges e = (C , x , a), the following variables are used to store cumulative values that are updated as additional descendants are included in the threshold:\n\u2022 ps \u2248 \u2211\no\u2208Os p(o), the probability of success. \u2022 ts \u2248 \u2211\no\u2208Os p(o)t(o), the unnormalized expected number of steps in a successful outcome.\n\u2022 tf \u2248 \u2211\no\u2208Of p(o)t(o), the same for a failed outcome.\n\u2022 \u2206 \u2248 \u2211\no\u2208O p(o)\u2206(o): the expected improvement.\n\u2022 r\u2217 = \u2206ts+tf : the peak incremental rate of improvement.\n\u2022 edgesf : the expected frequencies of resulting edges given a failed outcome.\nThese quantities are updated by the ProcessDescendant subroutine, which adds the success and failure statistics of one particular descendant e \u2032 to the overall stats for e. This also adds new descendants to the queue, since the descendants of e \u2032 from edgesf (e \u2032, \u2217) are added. Critically, any particular edge e can only be evaluated once in lines 17-22, as they are evaluated in descending order and the descendants of any edge always have lower r\u2217 values than the edge itself.\nAlso essential is that the quantities ps , ts , . . . are are approximated. Rather than computing them for hyper-exponentially many possibilities for\nexpanded subtrees and taking expectations over those, we view the subtrees as having only a single distinct outcome for each possible child abstract state y resulting from the transition (x , a), i.e. all y \u2208 F | \u03ba(x , a, {y}) > 0. For this, we also store the quantities p(y) and t(y) to represent the total probability of failure and aggregate time spent on failure within the subtree for y . Within each of those subtrees, out-edges that may or may not appear are evaluated as though they occur the expected number of times, represented by non-integer exponents in lines 28 and 29 of Algorithm 1. Following this assumption, the expected frequencies for each type of edge e, descendant edge e \u2032 and child state y are stored in the array freq(e, y , e \u2032), which is updated on lines 27 and 37.\nOverall, if D is the number of distinct possible values that g(\u00b7) can take, then W = D |F| |A| is the number of possible distinct equivalence classes of edges or (C , x , a) tuples; the final output of SMIRI is a lookup table consisting of at most W entries. SMIRI has worst-case space complexity of O(W 2) (needed to store the descendant frequencies edgesf ), and worst-case time complexity of O(|F|W 3 logW ). For homogeneous actions this reduces further as W = D |F|, and for typical cases with sparse transition functions the leading |F| term reduces to a constant K ."}, {"heading": "6 Experiments", "text": "In order to evaluate the performance of SMIRI, a random binary tree model T (p, h0) was designed to exhibit the typical characteristics of classical search problems. We define T as having a feature space of the natural numbers N, initial state h0, goal state 0, and legal actions Left and Right having cost c\u0304 \u2261 1. The transition function for T (p, \u00b7) from state h with either action leads to state h \u2212 1 with probability p and h + 1 with probability 1\u2212 p. This model has two key properties that make it a good testbed for anytime classical search. First of all, the goal state can occur many times at many different depths in the tree, resulting in solutions that vary widely in quality. Secondly, for this problem \u03c6 is, in a natural manner, the most\naccurate admissible and consistent heuristic that can be constructed from the feature space. Thus, although SMIRI does not require any kind of admissible heuristic, the model T (p, h0) offers a framework within which methods that rely on admissibility can also be evaluated. This model was implemented with 7 anytime classical search algorithms:\n\u2022 SMIRI, using r\u2217 as precomputed by Algorithm 1.\n\u2022 Anytime Potential Search (APTS/ANA*), as per Stern et al. (2014), which selects the node with maximal C\u2212g(n)h(n) .\n\u2022 Anytime Generalized Potential Search (AGPTS), as per Stern et al. (2014). For AGPTS, we explicitly precomputed a table for the potential PTC (n) for all n and C .\n\u2022 Anytime Explicit Estimation Search (AEES), as per Thayer et al. (2012). For AEES, we precomputed an unbiased inadmissible heuristic h\u0302 \u2261 d\u0302 by calculating the expected value of the shortest-cost path from any node given h.\n\u2022 Anytime Repairing A* (ARA*) per Likachev et al. (2003); with weights 5, 3, 2, 1.5, 1 per Richter et al. (2010).\nPerformance was evaluated in terms of solution cost vs number of edges expanded in the search, thus the details of hardware and algorithm implementations should not be relevant to the results. Although using step counts instead of time neglects the relative overhead of the different algorithms, the average time per step was very similar for most of them; the only notable exception was EES, which can be several times slower per expansion if node generation operations are cheap (Thayer and Ruml, 2011).\nEach of the algorithms was run on 6 different sets of parameter values, which are specified in detail in Table 1. The cost bound Cmax was chosen to make it relatively easy to find an improved initial solution in each test case, whereas the edge expansion limit N was chosen to allow the higherquality algorithms sufficient time to closely approach optimal solutions where possible. The discount factor was then chosen as \u03b3 = 1\u2212 2/N (\u03b3N \u2248 0.135). Tables for r\u2217, PTC and h\u0302 were precomputed, as was the expected cost of an optimal solution E [Copt ], which was used to estimate suboptimality as calculating optimal solution costs for particular instances quickly becomes impractical. For each test case 1000 instances (except in test cases 1 and 2, which only used 100) of T (p, h0) were created as generative models\u2014this is necessary since instances of T (p, h0) are typically infinite and cannot be explicitly instantiated. Each algorithm was then run on each instance of the random tree model for N iterations, recording the sequences of improved solutions and the time step at which each was attained. Note that each model instance was shared between all of the algorithms, which significantly reduces variance in the relative performance of different algorithms due to luck on particular model instances.\nTable 1 depicts the profile of incumbent solution cost C vs. the number of edges expanded for each algorithm and test case. For easier comparison, the y-axes are presented as the estimated suboptimality, i.e. C/E [Copt ], while the x -axes are the number of edge expansions divided by the maximum N allowed for each test case. Finally, Table 2 depicts mean discounted total cost; these values are normalized, with the total discounted cost for each policy being divided by the expected cost of a hypothetical \u201cperfectly rational\u201d anytime algorithm which has a solution of mean quality E [Copt ] after 0 time steps."}, {"heading": "7 Discussion", "text": "The decreasing differences between algorithms indicate that the test cases become progressively easier from 1 to 6, with a large decrease in difficulty\nbetween p = 0.6 and p = 0.4; this suggests the model may have a complexity transition at p = 0.5, as with the random tree model of Karp and Pearl (Karp and Pearl, 1983) Overall, SMIRI and APTS/ANA* are quite clearly the bestperforming algorithms, particularly for the hardest test cases. By contrast, ARA* performed poorly for these cases, suggesting that for difficult problems it is much better to use algorithms that don\u2019t rely on parameter tuning.\nAs can be seen in cases 1, 2, and 3, APTS appears to level off at highercost solutions than SMIRI, and thus is likely could be far slower to attain solutions of similar quality than SMIRI. More critically, there is a significant disparity between APTS and AGPTS, which is a significant theoretical concern for APTS, because if one follows the theoretical derivation given by Stern et al. (2014) AGPTS does the \u201ccorrect thing\u201d in always selecting for maximum potential PTC , whereas the linear-relative assumption APTS relies on clearly fails for the random tree model we have used. In general the results for APTS in this domain are much better than those of AGPTS, although for later timesteps in case 3 AGPTS overtakes APTS in solution quality. The most likely explanation for this issue is one that has been raised by Stern et al. themselves: although AGPTS correctly selects for maximum potential, this is not actually the correct thing for an anytime search algorithm to do. In particular, considering only the probability of a solution in a given subtree neglects the expected search effort to find a solution there. For example, a node with 10 20-step step branches, each of which has a 50% chance of being a solution path, has a 99.9% being part of a solution, whereas a node with a single 5-step branch that has a 90% chance of being a solution path has a 90% chance of being part of a solution. Clearly the potential of the first node is higher, but a reasonable search algorithm should always expand the latter node first as it is likely to lead to a better solution than the former, and more quickly to boot.\nThe idea of minimising search effort is one of the key motivations for AEES (Thayer and Ruml, 2011), but unfortunately it performed significantly worse than SMIRI or APTS and sometimes ARA*. It is difficult to say why this occurs, but it is likely that the problem is related to the nature of the inadmissible heuristic h\u0302\u2014it appears that, despite being an unbiased estimator, the statistical properties of h\u0302 and/or the underlying random tree model cause issues for AEES. Nevertheless, AEES has one key advantage, which is the use of a distance-to-go estimate d(n) to better estimate search effort; this does not give any advantage in our test domains since every edge has unit cost, and thus d\u0302 \u2261 h\u0302. However, SMIRI also makes an explicit distinction between estimates of cost and distance, as evidenced by the distinct quantities \u2206, ts and tf in Algorithm 1.\nIn summary, SMIRI was the best overall algorithm in this test domain, with a strong edge over all other algorithms except APTS (over which it has only a slight edge). The very narrow losses to APTS in test cases 5 and 6 suggest that both algorithms may be acting quite close to optimally;\nthe approximations made in Sections 4 and 5 are likely to be the cause of suboptimality in SMIRI. More critically, the results for AGPTS demonstrate that the good performance for APTS comes in spite of the justification by Stern et al. for selecting on the basis of potential, and not because of it. This is highlighted by the benchmarking results of Thayer et al. (2012), which show AEES significantly outperforming APTS in all but one domain. On the whole, SMIRI has shown solid results in the synthetic benchmark, while also lacking some of the theoretical shortcomings of APTS."}, {"heading": "8 Conclusion", "text": "Overall, the experimental results for SMIRI are quite promising, although SMIRI is not without limitations\u2014it requires pre-processing time, and is limited to domains in which the possible path costs (up to Cmax ) and the abstract states are finite sets. However, such pre-processing costs can become insignificant when compared to a large search problem or amortized over many searches that share an abstract model. Furthermore, one possible approach that could resolve both of these issues is functional approximation methods, which could be used to capture the structure of r\u2217. A greater problem with the results is that they are limited to an artificial problem domain, rather than real-world problems or typical benchmark problems from the literature. It would be very informative to examine the performance of SMIRI in more realistic problem domains, and particularly ones with non-uniform edge costs to check the effectiveness of SMIRI\u2019s use of estimated search effort.\nHowever, the main purpose of this paper is not to evaluate the SMIRI algorithm, but rather to demonstrate the effectiveness of applying explicit metareasoning techniques to the problem of classical search. The benchmarks, although limited, are solid evidence that metareasoning in general, and the ASMDP in particular, are a useful tool for developing better search algorithms. In particular, this paper demonstrates how formal decision-theoretic methods can be used to formulate the problem of heuristic search as a metalevel decision problem with a well-defined optimal solution, rather than relying on ad hoc heuristic arguments to justify a particular method. Furthermore, although the benchmarking here is limited, many of the latest results in probabilistic models of deterministic search (Lelis et al., 2014) indicate that such models can be effectively applied in estimating the performance of search algorithms, and there is little reason to believe that the additional step of applying metareasoning to such models is fundamentally flawed.\nAlthough SMIRI is derived from a particular application of the ASMDP framework to anytime search, the ASMDP framework applies more generally to other kinds of design criteria for search algorithms. Moreover, the core framework should not be difficult to extend to search with non-classical\nelements such as adversaries or stochastic environments. A far more serious limitation is the assumption of a tree structure and the local directed Markov property in the construction of the tree generation process, as this restricts the ASMDP\u2019s ability to handle correlations that are induced by cycles in graph-structured problems. Nevertheless, the success of probabilistic prediction methods for tree search in domains with cyclic graphs (Lelis et al., 2014) indicates that such models can be useful even if they don\u2019t account for those factors. Moreover, despite this limitation of the theory, SMIRI can still be adapted to function as a graph search algorithm in the usual way, by adding a closed list and re-expanding edges when necessary. On the whole, both the theoretical and experimental results of this paper are quite promising, and indicate a clear need both for further experimental results with the SMIRI algorithm, as well as a broader theoretical investigation of the ASMDP framework.\nAlgorithm 1 SMIRI: computing r\u2217 1: for C \u2190 possible path costs from 0 to Cmax do 2: for all legal (x , a) \u2208 F \u00d7A do 3: e \u2190 (C , x , a) 4: ts(e)\u2190 0; tf (e)\u2190 0 5: edgesf (e, \u2217)\u2190 0 6: if x \u2208 FGoal then 7: ps(e)\u2190 1; \u2206(e)\u2190 C ; r\u2217(e)\u2190\u221e 8: continue 9: ps(e)\u2190 0; \u2206(e)\u2190 0; r\u2217(e)\u2190 0 10: queue \u2190 \u2205; freq(e, \u2217, \u2217)\u2190 0 11: for all y \u2208 F : py = \u03ba(x , a, {y}) > 0 do 12: p(y)\u2190 py ; t(y)\u2190 1 13: for all legal actions b from y do 14: e \u2032 \u2190 (C \u2212 c\u0304(y), y , b) 15: queue \u2190 queue \u222a {(C \u2212 c\u0304(y), y , b)} 16: freq(e, y , e \u2032)\u2190 1 17: while maxe\u2032\u2208queue r\u2217(e \u2032) \u2265 r\u2217(e) do 18: e \u2032 \u2190 arg maxe\u2032\u2208queue r\u2217(e \u2032) 19: Remove e \u2032 from queue 20: ProcessDescendant(e, e\u2019) 21: tf (e)\u2190 \u2211 y\u2208F p(y)t(y) 22: r\u2217(e)\u2190 \u2206(e)/(ts(e) + tf (e)) 23: for all y , e \u2032 : m = freq(e, y , e \u2032) > 0 do 24: edgesf (e, e \u2032)\u2190 edgesf (e, e \u2032) + m p(y)1\u2212ps(e) 25: procedure ProcessDescendant(e, e \u2032) 26: for all y \u2208 F : m = freq(e, y , e \u2032) > 0 do 27: freq(e, y , e \u2032)\u2190 0 28: psuc \u2190 1\u2212 (1\u2212 ps(e \u2032))m\n29: tsuc \u2190 \u2211m\u22121\nk=0\n[ ts(e \u2032) + ktf (e \u2032) ps(e \u2032) 1\u2212ps(e\u2032) ] [1\u2212 ps(e \u2032)]k\n30: ps(e)\u2190 ps(e) + p(y)psuc 31: ts(e)\u2190 ts(e) + p(y) (tsuc + psuct(y)) 32: \u2206(e)\u2190 \u2206(e) + p(y)\u2206(e \u2032)psuc/ps(e \u2032) 33: p(y)\u2190 p(y)(1\u2212 psuc) 34: t(y)\u2190 t(y) + tf (e \u2032)/(1\u2212 ps(e \u2032)) 35: for all e \u2032\u2032 : m2 = edgesf (e \u2032, e \u2032\u2032) > 0 do 36: Add e \u2032\u2032 to the queue if it isn\u2019t there. 37: freq(e, y , e \u2032\u2032)\u2190 freq(e, y , e \u2032\u2032) + mm2"}], "references": [{"title": "An analysis of time-dependent planning", "author": ["Thomas L. Dean", "Mark S. Boddy"], "venue": "In Proc. AAAI. St. Paul, MN, August 21-26,", "citeRegEx": "Dean and Boddy.,? \\Q1988\\E", "shortCiteRegEx": "Dean and Boddy.", "year": 1988}, {"title": "Generalized best-first search strategies and the optimality of a", "author": ["Rina Dechter", "Judea Pearl"], "venue": "J. ACM,", "citeRegEx": "Dechter and Pearl.,? \\Q1985\\E", "shortCiteRegEx": "Dechter and Pearl.", "year": 1985}, {"title": "A dynamic allocation index for the discounted multiarmed bandit problem", "author": ["J.C. Gittins", "D.M. Jones"], "venue": null, "citeRegEx": "Gittins and Jones.,? \\Q1979\\E", "shortCiteRegEx": "Gittins and Jones.", "year": 1979}, {"title": "Anytime heuristic search: First results", "author": ["Eric A. Hansen", "Shlomo Zilberstein", "Victor A. Danilchenko"], "venue": "Technical Report 97-50,", "citeRegEx": "Hansen et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 1997}, {"title": "A formal basis for the heuristic determination of minimum cost paths", "author": ["Peter E. Hart", "Nils J. Nilsson", "Bertram Raphael"], "venue": "IEEE Trans. Systems Science and Cybernetics,", "citeRegEx": "Hart et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Hart et al\\.", "year": 1968}, {"title": "Value-function approximations for partially observable markov decision processes", "author": ["Milos Hauskrecht"], "venue": "JAIR, 13:33\u201394,", "citeRegEx": "Hauskrecht.,? \\Q2000\\E", "shortCiteRegEx": "Hauskrecht.", "year": 2000}, {"title": "Selecting computations: Theory and applications", "author": ["Nicholas Hay", "Stuart J. Russell", "David Tolpin", "Solomon Eyal Shimony"], "venue": "In Proc. of UAI, August 14-18,", "citeRegEx": "Hay et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hay et al\\.", "year": 2012}, {"title": "Searching for an optimal path in a tree with random costs", "author": ["Richard M. Karp", "Judea Pearl"], "venue": "Artif. Intell.,", "citeRegEx": "Karp and Pearl.,? \\Q1983\\E", "shortCiteRegEx": "Karp and Pearl.", "year": 1983}, {"title": "Estimating the efficiency of backtrack programs", "author": ["Donald E. Knuth"], "venue": "Math. Comp.,", "citeRegEx": "Knuth.,? \\Q1975\\E", "shortCiteRegEx": "Knuth.", "year": 1975}, {"title": "An analysis of alpha-beta pruning", "author": ["Donald E. Knuth", "Ronald W. Moore"], "venue": "Artif. Intell.,", "citeRegEx": "Knuth and Moore.,? \\Q1975\\E", "shortCiteRegEx": "Knuth and Moore.", "year": 1975}, {"title": "Depth-first iterative-deepening: An optimal admissible tree search", "author": ["Richard E. Korf"], "venue": "Artif. Intell.,", "citeRegEx": "Korf.,? \\Q1985\\E", "shortCiteRegEx": "Korf.", "year": 1985}, {"title": "Time complexity of iterative-deepening-a", "author": ["Richard E. Korf", "Michael Reid", "Stefan Edelkamp"], "venue": "Artif. Intell.,", "citeRegEx": "Korf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Korf et al\\.", "year": 2001}, {"title": "And-or graphs, theorem-proving graphs and bi-directional graphs", "author": ["Robert Kowalski"], "venue": "In Machine Intelligence 7, Proceedings of the Seventh Machine Intelligence Workshop,", "citeRegEx": "Kowalski.,? \\Q1972\\E", "shortCiteRegEx": "Kowalski.", "year": 1972}, {"title": "Predicting the size of ida*\u2019s search", "author": ["Levi H.S. Lelis", "Sandra Zilles", "Robert C. Holte"], "venue": "tree. Artif. Intell.,", "citeRegEx": "Lelis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lelis et al\\.", "year": 2013}, {"title": "Predicting optimal solution cost with conditional probabilities predicting optimal solution cost", "author": ["Levi H.S. Lelis", "Roni Stern", "Ariel Felner", "Sandra Zilles", "Robert C. Holte"], "venue": "Ann. Math. Artif. Intell.,", "citeRegEx": "Lelis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lelis et al\\.", "year": 2014}, {"title": "Ara*: Anytime a* with provable bounds on suboptimality", "author": ["Maxim Likhachev", "Geoffrey J. Gordon", "Sebastian Thrun"], "venue": "NIPS", "citeRegEx": "Likhachev et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Likhachev et al\\.", "year": 2003}, {"title": "The economic value of analysis and computation", "author": ["James E. Matheson"], "venue": "IEEE Trans. Systems Science and Cybernetics,", "citeRegEx": "Matheson.,? \\Q1968\\E", "shortCiteRegEx": "Matheson.", "year": 1968}, {"title": "Optimal allocation of very limited search resources", "author": ["David Mutchler"], "venue": "In Proc. AAAI. Philadelphia, PA, August 11-15,", "citeRegEx": "Mutchler.,? \\Q1986\\E", "shortCiteRegEx": "Mutchler.", "year": 1986}, {"title": "Pathology on game trees: A summary of results", "author": ["Dana S. Nau"], "venue": "In Proc. AAAI. Stanford University, August 18-21,", "citeRegEx": "Nau.,? \\Q1980\\E", "shortCiteRegEx": "Nau.", "year": 1980}, {"title": "A course in game theory", "author": ["Martin J Osborne", "Ariel Rubinstein"], "venue": "MIT press,", "citeRegEx": "Osborne and Rubinstein.,? \\Q1994\\E", "shortCiteRegEx": "Osborne and Rubinstein.", "year": 1994}, {"title": "Studies in semi-admissible heuristics", "author": ["Judea Pearl", "Jin H. Kim"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Pearl and Kim.,? \\Q1982\\E", "shortCiteRegEx": "Pearl and Kim.", "year": 1982}, {"title": "Heuristic search viewed as path finding in a graph", "author": ["Ira Pohl"], "venue": "Artif. Intell.,", "citeRegEx": "Pohl.,? \\Q1970\\E", "shortCiteRegEx": "Pohl.", "year": 1970}, {"title": "The joy of forgetting: Faster anytime search via restarting", "author": ["Silvia Richter", "Jordan Tyler Thayer", "Wheeler Ruml"], "venue": "In Proc. ICAPS", "citeRegEx": "Richter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Richter et al\\.", "year": 2010}, {"title": "Decision-theoretic control of reasoning: General theory and an application to game-playing", "author": ["Stuart Russell", "Eric Wefald"], "venue": "Technical Report UCB/CSD-88-435,", "citeRegEx": "Russell and Wefald.,? \\Q1988\\E", "shortCiteRegEx": "Russell and Wefald.", "year": 1988}, {"title": "Artificial Intelligence - A Modern Approach (3", "author": ["Stuart J. Russell", "Peter Norvig"], "venue": "internat. ed.). Pearson Education,", "citeRegEx": "Russell and Norvig.,? \\Q2010\\E", "shortCiteRegEx": "Russell and Norvig.", "year": 2010}, {"title": "Do the right thing - studies in limited rationality", "author": ["Stuart J. Russell", "Eric Wefald"], "venue": null, "citeRegEx": "Russell and Wefald.,? \\Q1991\\E", "shortCiteRegEx": "Russell and Wefald.", "year": 1991}, {"title": "Questions of reasoning under logical uncertainty", "author": ["Nate Soares", "Benja Fallenstein"], "venue": "Technical Report 2015-1, Machine Intelligence Research Institute,", "citeRegEx": "Soares and Fallenstein.,? \\Q2015\\E", "shortCiteRegEx": "Soares and Fallenstein.", "year": 2015}, {"title": "Potential-based bounded-cost search and anytime nonparametric a", "author": ["Roni Stern", "Ariel Felner", "Jur van den Berg", "Rami Puzis", "Rajat Shah", "Ken Goldberg"], "venue": "Artif. Intell.,", "citeRegEx": "Stern et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stern et al\\.", "year": 2014}, {"title": "Potential search: A boundedcost search algorithm", "author": ["Roni Tzvi Stern", "Rami Puzis", "Ariel Felner"], "venue": "In Proc. ICAPS", "citeRegEx": "Stern et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Stern et al\\.", "year": 2011}, {"title": "Bounded suboptimal search: A direct approach using inadmissible estimates", "author": ["Jordan Tyler Thayer", "Wheeler Ruml"], "venue": "IJCAI", "citeRegEx": "Thayer and Ruml.,? \\Q2011\\E", "shortCiteRegEx": "Thayer and Ruml.", "year": 2011}, {"title": "Better parameter-free anytime search by minimizing time between solutions", "author": ["Jordan Tyler Thayer", "J. Benton", "Malte Helmert"], "venue": "In Proc. SOCS", "citeRegEx": "Thayer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Thayer et al\\.", "year": 2012}, {"title": "Anytime nonparametric A", "author": ["Jur van den Berg", "Rajat Shah", "Arthur Huang", "Kenneth Y. Goldberg"], "venue": "In Proc. AAAI", "citeRegEx": "Berg et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Berg et al\\.", "year": 2011}, {"title": "Predicting the performance of ida* using conditional distributions", "author": ["Uzi Zahavi", "Ariel Felner", "Neil Burch", "Robert C. Holte"], "venue": "JAIR, 37:41\u201383,", "citeRegEx": "Zahavi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zahavi et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "Of particular note is A* (Hart et al., 1968), which remains ubiquitous.", "startOffset": 25, "endOffset": 44}, {"referenceID": 6, "context": "However, some researchers have taken more formal approaches to such questions; of particular note is recent work (Hay et al., 2012) which formulates the Bayesian selection problem as a metalevel decision problem.", "startOffset": 113, "endOffset": 131}, {"referenceID": 16, "context": "Matheson (1968) showed that metareasoning", "startOffset": 0, "endOffset": 16}, {"referenceID": 23, "context": "The Decision-Theoretic A* algorithm (Russell and Wefald, 1988) applies metareasoning to real-time problem-solving search, but the utility estimates used in DTA* cannot obey the standard axioms of probability and utility theory and thus the approach lacks solid theoretical grounding (Russell and Wefald, 1988).", "startOffset": 36, "endOffset": 62}, {"referenceID": 23, "context": "The Decision-Theoretic A* algorithm (Russell and Wefald, 1988) applies metareasoning to real-time problem-solving search, but the utility estimates used in DTA* cannot obey the standard axioms of probability and utility theory and thus the approach lacks solid theoretical grounding (Russell and Wefald, 1988).", "startOffset": 283, "endOffset": 309}, {"referenceID": 6, "context": "More recent work (Hay et al., 2012) has formalized the metalevel decision problem for Bayesian selection problems.", "startOffset": 17, "endOffset": 35}, {"referenceID": 7, "context": "algorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem.", "startOffset": 11, "endOffset": 67}, {"referenceID": 9, "context": "algorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem.", "startOffset": 11, "endOffset": 67}, {"referenceID": 18, "context": "algorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem.", "startOffset": 11, "endOffset": 67}, {"referenceID": 10, "context": "(2001) were able to accurately predict the number of nodes expanded by the Iterative Deepening A* algorithm (Korf, 1985) on a number of different problems.", "startOffset": 108, "endOffset": 120}, {"referenceID": 7, "context": "algorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem. One exception is the modelling by Mutchler (1986) of search with severely limited edge expansions, who showed theoretically that a simple best-first search algorithm was approximately optimal in a simple random tree model.", "startOffset": 12, "endOffset": 173}, {"referenceID": 7, "context": "algorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem. One exception is the modelling by Mutchler (1986) of search with severely limited edge expansions, who showed theoretically that a simple best-first search algorithm was approximately optimal in a simple random tree model. The aforementioned prior works have offered interesting domains to study the behaviour of search algorithms, and we continue this trend by formulating a novel random tree model that should be more representative of classical search problems. On the other hand another line of research focused on predicting the sizes of search trees has resulted in much more realistic models of search problems, primarily due to a focus on predicting real-world performance. The earliest work on this problem is that of Knuth (1975); later, Korf et al.", "startOffset": 12, "endOffset": 863}, {"referenceID": 7, "context": "algorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem. One exception is the modelling by Mutchler (1986) of search with severely limited edge expansions, who showed theoretically that a simple best-first search algorithm was approximately optimal in a simple random tree model. The aforementioned prior works have offered interesting domains to study the behaviour of search algorithms, and we continue this trend by formulating a novel random tree model that should be more representative of classical search problems. On the other hand another line of research focused on predicting the sizes of search trees has resulted in much more realistic models of search problems, primarily due to a focus on predicting real-world performance. The earliest work on this problem is that of Knuth (1975); later, Korf et al. (2001) were able to accurately predict the number of nodes expanded by the Iterative Deepening A* algorithm (Korf, 1985) on a number of different problems.", "startOffset": 12, "endOffset": 890}, {"referenceID": 7, "context": "algorithms (Karp and Pearl, 1983; Knuth and Moore, 1975; Nau, 1980) rather than to formulate a metalevel decision problem. One exception is the modelling by Mutchler (1986) of search with severely limited edge expansions, who showed theoretically that a simple best-first search algorithm was approximately optimal in a simple random tree model. The aforementioned prior works have offered interesting domains to study the behaviour of search algorithms, and we continue this trend by formulating a novel random tree model that should be more representative of classical search problems. On the other hand another line of research focused on predicting the sizes of search trees has resulted in much more realistic models of search problems, primarily due to a focus on predicting real-world performance. The earliest work on this problem is that of Knuth (1975); later, Korf et al. (2001) were able to accurately predict the number of nodes expanded by the Iterative Deepening A* algorithm (Korf, 1985) on a number of different problems. This work was later extended by Zahavi et al. (2010) to increase its accuracy, by using conditional distributions over what Zahavi et al.", "startOffset": 12, "endOffset": 1092}, {"referenceID": 21, "context": "For a long time, the best-performing algorithm for this purpose was Weighted A* (Pohl, 1970); this approach multiples the heuristic in A* by a constant factor of w , resulting in an algorithm that typically runs faster than A\u2217 but is w -admissible.", "startOffset": 80, "endOffset": 92}, {"referenceID": 12, "context": "However, it tends to waste time exploring equally meritorious solutions in parallel, even when any one of those solutions would be acceptable (Kowalski, 1972; Pearl and Kim, 1982).", "startOffset": 142, "endOffset": 179}, {"referenceID": 20, "context": "However, it tends to waste time exploring equally meritorious solutions in parallel, even when any one of those solutions would be acceptable (Kowalski, 1972; Pearl and Kim, 1982).", "startOffset": 142, "endOffset": 179}, {"referenceID": 1, "context": "Moreover, Dechter and Pearl (1985) demonstrate that A* is (down to the tie-breaking criterion) essentially the fastest general algorithm that can make this guarantee\u2014other algorithms can only do better by \u201ccheating\u201d on specific problem instances.", "startOffset": 10, "endOffset": 35}, {"referenceID": 29, "context": "Explicit Estimation Search (EES) (Thayer and Ruml, 2011) resolves this issue by maintaining multiple queues, so as to maintain admissibility while focusing search effort on particular candidates.", "startOffset": 33, "endOffset": 56}, {"referenceID": 28, "context": "The most prominent algorithm for this criterion is Potential Search (Stern et al., 2011), which aims to expand the node with the highest probability of having a path cost below the bound.", "startOffset": 68, "endOffset": 88}, {"referenceID": 0, "context": "In this paper, we focus on the design of anytime algorithms (Dean and Boddy, 1988), which quickly find initial solutions and then gradually improve upon them.", "startOffset": 60, "endOffset": 82}, {"referenceID": 3, "context": "A number of anytime heuristic search algorithms have been proposed in the literature; the most basic is Anytime Weighted A* (Hansen et al., 1997), which is Weighted A* but continues to search after a solution is found, and prunes nods that cannot lead to an improvement.", "startOffset": 124, "endOffset": 145}, {"referenceID": 15, "context": "Anytime Repairing A* (Likhachev et al., 2003) adapts this by also decreasing the weight every time a solution is found.", "startOffset": 21, "endOffset": 45}, {"referenceID": 28, "context": "Finally, the latest state-of-the-art anytime algorithms have made further improvements by obviating the need for tuning the weight parameters; these are APTS/ANA* (Stern et al., 2011; van den Berg et al., 2011), an anytime version of Potential Search, and AEES (Thayer et al.", "startOffset": 163, "endOffset": 210}, {"referenceID": 30, "context": ", 2011), an anytime version of Potential Search, and AEES (Thayer et al., 2012), an anytime version of EES.", "startOffset": 58, "endOffset": 79}, {"referenceID": 24, "context": "Note that we represent the problem by a tree of histories, the complete search tree, rather than the usual state-space graph (Russell and Norvig, 2010).", "startOffset": 125, "endOffset": 151}, {"referenceID": 25, "context": "Decision-theoretic analysis of classical search may appear to be straightforward, but in reality there is a subtle issue that needs to be resolved (Russell and Wefald, 1991): any information obtained solely as the result of a computation is information that, from the point of view of utility and probability theory, that agent already had.", "startOffset": 147, "endOffset": 173}, {"referenceID": 26, "context": "This fundamental issue is a rather difficult one; it continues to be an area of active research, sometimes referred to as the question of \u201clogical uncertainty\u201d (Soares and Fallenstein, 2015).", "startOffset": 160, "endOffset": 190}, {"referenceID": 13, "context": "This idea can be seen as building upon the \u201ctype system\u201d idea of Lelis et al. (2013) by viewing abstract states as residing in an arbitrary space, and viewing the type system as representing a whole class of problems rather than just one.", "startOffset": 65, "endOffset": 85}, {"referenceID": 5, "context": "However, to avoid solving the full-blown POMDP directly, we instead apply the concept of sufficient information states (Hauskrecht, 2000) to define the ASMDP.", "startOffset": 119, "endOffset": 137}, {"referenceID": 2, "context": "The idea is analogous to index policies (Gittins and Jones, 1979) in multi-armed bandits: it independently computes a single quantity for each action and selects the action with the greatest index.", "startOffset": 40, "endOffset": 65}, {"referenceID": 30, "context": "1Thayer et al. (2012) have argued against maximizing the rate of improvement, but their argument applies to long-term improvement and not the short-term criterion used by SMIRI.", "startOffset": 1, "endOffset": 22}, {"referenceID": 27, "context": "\u2022 Anytime Potential Search (APTS/ANA*), as per Stern et al. (2014), which selects the node with maximal C\u2212g(n) h(n) .", "startOffset": 47, "endOffset": 67}, {"referenceID": 27, "context": "\u2022 Anytime Generalized Potential Search (AGPTS), as per Stern et al. (2014). For AGPTS, we explicitly precomputed a table for the potential PTC (n) for all n and C .", "startOffset": 55, "endOffset": 75}, {"referenceID": 30, "context": "\u2022 Anytime Explicit Estimation Search (AEES), as per Thayer et al. (2012). For AEES, we precomputed an unbiased inadmissible heuristic \u0125 \u2261 d\u0302 by calculating the expected value of the shortest-cost path from any node given h.", "startOffset": 52, "endOffset": 73}, {"referenceID": 22, "context": "5, 1 per Richter et al. (2010).", "startOffset": 9, "endOffset": 31}, {"referenceID": 29, "context": "exception was EES, which can be several times slower per expansion if node generation operations are cheap (Thayer and Ruml, 2011).", "startOffset": 107, "endOffset": 130}, {"referenceID": 7, "context": "5, as with the random tree model of Karp and Pearl (Karp and Pearl, 1983) Overall, SMIRI and APTS/ANA* are quite clearly the bestperforming algorithms, particularly for the hardest test cases.", "startOffset": 51, "endOffset": 73}, {"referenceID": 29, "context": "The idea of minimising search effort is one of the key motivations for AEES (Thayer and Ruml, 2011), but unfortunately it performed significantly worse than SMIRI or APTS and sometimes ARA*.", "startOffset": 76, "endOffset": 99}, {"referenceID": 7, "context": "5, as with the random tree model of Karp and Pearl (Karp and Pearl, 1983) Overall, SMIRI and APTS/ANA* are quite clearly the bestperforming algorithms, particularly for the hardest test cases. By contrast, ARA* performed poorly for these cases, suggesting that for difficult problems it is much better to use algorithms that don\u2019t rely on parameter tuning. As can be seen in cases 1, 2, and 3, APTS appears to level off at highercost solutions than SMIRI, and thus is likely could be far slower to attain solutions of similar quality than SMIRI. More critically, there is a significant disparity between APTS and AGPTS, which is a significant theoretical concern for APTS, because if one follows the theoretical derivation given by Stern et al. (2014) AGPTS does the \u201ccorrect thing\u201d in always selecting for maximum potential PTC , whereas the linear-relative assumption APTS relies on clearly fails for the random tree model we have used.", "startOffset": 36, "endOffset": 752}, {"referenceID": 27, "context": "More critically, the results for AGPTS demonstrate that the good performance for APTS comes in spite of the justification by Stern et al. for selecting on the basis of potential, and not because of it. This is highlighted by the benchmarking results of Thayer et al. (2012), which show AEES significantly outperforming APTS in all but one domain.", "startOffset": 125, "endOffset": 274}, {"referenceID": 14, "context": "probabilistic models of deterministic search (Lelis et al., 2014) indicate that such models can be effectively applied in estimating the performance of search algorithms, and there is little reason to believe that the additional step of applying metareasoning to such models is fundamentally flawed.", "startOffset": 45, "endOffset": 65}, {"referenceID": 14, "context": "Nevertheless, the success of probabilistic prediction methods for tree search in domains with cyclic graphs (Lelis et al., 2014) indicates that such models can be useful even if they don\u2019t account for those factors.", "startOffset": 108, "endOffset": 128}], "year": 2015, "abstractText": "Among classical search algorithms with the same heuristic information, with sufficient memory A* is essentially as fast as possible in finding a proven optimal solution. However, in many situations optimal solutions are simply infeasible, and thus search algorithms that trade solution quality for speed are desirable. In this paper, we formalize the process of classical search as a metalevel decision problem, the Abstract Search MDP. For any given optimization criterion, this establishes a well-defined notion of the best possible behaviour for a search algorithm and offers a theoretical approach to the design of algorithms for that criterion. We proceed to approximately solve a version of the Abstract Search MDP for anytime algorithms and thus derive a novel search algorithm, Search by Maximizing the Incremental Rate of Improvement (SMIRI). SMIRI is shown to outperform current state-of-the-art anytime search algorithms on a parametrized stochastic tree model for most of the tested parameter values.", "creator": "LaTeX with hyperref package"}}}