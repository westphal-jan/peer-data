{"id": "1512.01110", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2015", "title": "Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization", "abstract": "optimization matrix integration has more studied based on a three - rank matrix query formulation with misleading results. basically, minimal focusing has been done on other numerical implementations building onwards the more specialized spectral regularization requirements. we cover this gap by presenting a novel bayesian rayon completion as to incorporating pattern regularization. in desperation to clear the difficulties always dealing with the orthonormality constraints of indexed vectors, we derive an new equivalent modeling with entropy constraints, meaning also reminds us its design : adaptive version of delta regularization feasible for bayesian networks. our bayesian method requires no parameter tuning and methods infer the prevalence of latent factors necessary. experiments on pure and real valued demonstrate encouraging results concerning rank recovery simple structure modeling, for notably good acceleration for very inexpensive matrices.", "histories": [["v1", "Thu, 3 Dec 2015 15:16:19 GMT  (133kb,D)", "https://arxiv.org/abs/1512.01110v1", "Accepted to AAAI 2016"], ["v2", "Fri, 25 Dec 2015 02:51:22 GMT  (134kb,D)", "http://arxiv.org/abs/1512.01110v2", "Accepted to AAAI 2016"]], "COMMENTS": "Accepted to AAAI 2016", "reviews": [], "SUBJECTS": "cs.NA cs.AI cs.LG", "authors": ["yang song", "jun zhu"], "accepted": true, "id": "1512.01110"}, "pdf": {"name": "1512.01110.pdf", "metadata": {"source": "CRF", "title": "Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization", "authors": ["Yang Song", "Jun Zhu"], "emails": ["yang.song@zoho.com", "dcszj@mail.tsinghua.edu.cn"], "sections": [{"heading": "Introduction", "text": "Matrix completion has found applications in many situations, such as collaborative filtering. Let Zm\u00d7n denote the data matrix with m rows and n columns, of which only a small number of entries are observed, indexed by \u2126 \u2282 [m] \u00d7 [n]. We denote the possibly noise corrupted observations of Z on \u2126 as P\u2126(X), where P\u2126 is a projection operator that retains entries with indices from \u2126 and replaces others with 0. The matrix completion task aims at completing missing entries of Z based on P\u2126(X), under the low-rank assumption rank(Z) min(m,n). When a squared-error loss is adopted, it can be written as solving:\nmin Z\n1\n2\u03c32 \u2016P\u2126(X \u2212 Z)\u20162F + \u03bb rank(Z), (P0)\nwhere \u2016P\u2126(A)\u20162F = \u2211 (i,j)\u2208\u2126 a 2 ij ; \u03bb is a positive regularization parameter; and \u03c32 is the noise variance. Unfortunately, the term rank(Z) makes P0 NP-hard. Therefore, the nuclear norm \u2016Z\u2016\u2217 has been widely adopted as a convex surrogate (Fazel 2002) to the rank function to turn P0 to a convex problem:\nmin Z\n1\n2\u03c32 \u2016P\u2126(X \u2212 Z)\u20162F + \u03bb \u2016Z\u2016\u2217 . (P1)\nCopyright \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nThough P1 is convex, the definition of nuclear norm makes the problem still not easy to solve. Based on a variational formulation of the nuclear norm, it has been popular to solve an equivalent and easier low-rank matrix factorization (MF) form of P1:\nmin A,B\n1\n2\u03c32 \u2016P\u2126(X \u2212AB\u1d40)\u20162F +\n\u03bb 2 (\u2016A\u20162F + \u2016B\u2016 2 F ). (1)\nThough not joint convex, this MF formulation can be solved by alternately optimizing over A and B for local optima.\nAs the regularization terms of MF are friendlier than the nuclear norm, many matrix factorization methods have been proposed to complete matrices, including maximummargin matrix factorization (M3F) (Srebro, Rennie, and Jaakkola 2004; Rennie and Srebro 2005) and Bayesian probabilistic matrix factorization (BPMF) (Lim and Teh 2007; Salakhutdinov and Mnih 2008). Furthermore, the simplicity of the MF formulation helps people adapt it and generalize it; e.g., (Xu, Zhu, and Zhang 2012; Xu, Zhu, and Zhang 2013) incorporate maximum entropy discrimination (MED) and nonparametric Bayesian methods to solve a modified MF problem.\nIn contrast, there are relatively fewer algorithms to directly solve P1 without the aid of matrix factorization. Such methods need to handle the spectrum of singular values. These spectral regularization algorithms require optimization on a Stiefel manifold (Stiefel 1935; James 1976), which is defined as the set of k-tuples (u1,u2, \u00b7 \u00b7 \u00b7 ,uk) of orthonormal vectors in Rn. This is the main difficulty that has prevented the attempts, if any, to develop Bayesian methods based on the spectral regularization formulation.\nThough matrix completion via spectral regularization is not easy, there are potential advantages over the matrix factorization approach. One of the benefits is the direct control over singular values. By imposing various priors on singular values, we can incorporate abundant information to help matrix completion. For example, Todeschini et al. (Todeschini, Caron, and Chavent 2013) put sparsity-inducing priors on singular values, naturally leading to hierarchical adaptive nuclear norm (HANN) regularization, and they reported promising results.\nIn this paper, we aim to develop a new formulation of the nuclear norm, hopefully having the same simplicity as MF and retaining some good properties of spectral regular-\nar X\niv :1\n51 2.\n01 11\n0v 2\n[ cs\n.N A\n] 2\n5 D\nec 2\n01 5\nization. The idea is to prove the orthonormality insignificance property of P1. Based on the new formulation, we develop a novel Bayesian model via a sparsity-inducing prior on singular values, allowing various dimensions to have different regularization parameters and automatically infer them. This involves some natural modifications to our new formulation to make it more flexible and adaptive, as people typically do in Bayesian matrix factorization. Empirical Bayesian methods are then employed to avoid parameter tuning. Experiments about rank recovery on synthetic matrices and collaborative filtering on some popular benchmark datasets demonstrate competitive results of our method in comparison with various state-of-the-art competitors. Notably, experiments on synthetic data show that our method performs considerably better when the matrices are very sparse, suggesting the robustness offered by using sparsityinducing priors."}, {"heading": "Relaxed Spectral Regularization", "text": "Bayesian matrix completion based on matrix factorization is relatively easy, with many examples (Lim and Teh 2007; Salakhutdinov and Mnih 2008). In fact, we can view (1) as a maximum a posterior (MAP) estimate of a simple Bayesian model, whose likelihood is Gaussian, i.e., for (i, j) \u2208 \u2126, Xij \u223c N ((AB\u1d40)ij , \u03c32), and the priors on A and B are also Gaussian, i.e., p(A) \u221d exp(\u2212\u03bb \u2016A\u20162F /2) and p(B) \u221d exp(\u2212\u03bb \u2016B\u20162F /2). It is now easy to do the posterior inference since the prior and likelihood are conjugate.\nHowever, the same procedure faces great difficulty when we attempt to develop Bayesian matrix completion based on the more direct spectral regularization formulation P1. This is because the prior p(Z) \u221d exp(\u2212\u03bb \u2016Z\u2016\u2217) is not conjugate to the Gaussian likelihood (or any other common likelihood). To analyze p(Z) more closely, we can conduct singular value decomposition (SVD) on Z to get Z = \u2211r k=1 dkukv \u1d40 k , where ~d := {dk : k \u2208 [r]} are singular values; U := {uk : k \u2208 [r]} and V := {vk : k \u2208 [r]} are orthonormal singular vectors on Stiefel manifolds. Though we can define a factorized prior p(Z) = p(~d)p(U)p(V ), any prior on U or V (e.g., the uniform Haar prior (Todeschini, Caron, and Chavent 2013)) needs to deal with a Stiefel manifold, which is highly nontrivial.\nIn fact, handling distributions embedded on Stiefel manifolds still remains a largely open problem, though some results (Byrne and Girolami 2013; Hoff 2009; Dobigeon and Tourneret 2010) exist in the literature of directional statistics. Fortunately, as we will prove in Theorem 1 that the orthonormality constraints on U and V are not necessary for spectral regularization. Rather, the unit sphere constraints \u2016uk\u2016 \u2264 1 and \u2016vk\u2016 \u2264 1, for all k \u2208 [r], are sufficient to get the same optimal solutions to P1. We call this phenomenon orthonormality insignificance. We will call spectral regularization with orthonormality constraints relaxed by unit sphere constraints relaxed spectral regularization.\nOrthonormality insignificance for spectral regularization We now present an equivalent formulation of the spectral regularization in P1 by proving its orthornormality insignificance property.\nWith the SVD of Z, we first rewrite P1 equivalently as P1\u2032 to show all constraints explicitly:\nmin ~d,U,V\n1\n2\u03c32 \u2225\u2225\u2225\u2225\u2225P\u2126 ( X \u2212 r\u2211 k=1 dkukv \u1d40 k )\u2225\u2225\u2225\u2225\u2225 2\nF\n+ \u03bb r\u2211 k=1 dk (P1\u2032)\ns.t. dk \u2265 0, \u2016uk\u2016 = 1, \u2016vk\u2016 = 1, \u2200k \u2208 [r] u\u1d40i uj = 0, v \u1d40 i vj = 0, \u2200i, j \u2208 [r] and i 6= j,\nwhere r = min(m,n). Then, we can have an equivalent formulation of P1 as summarized in Theorem 1, which lays the foundation for the validity of relaxed spectral regularization.\nTheorem 1. Let s be the optimal value of P1 (P1\u2032), and let t be the optimal value of P2 as defined below:\nmin \u03b1,\u03b2,~d\n1\n2\u03c32 \u2225\u2225\u2225\u2225\u2225P\u2126 ( X \u2212 r\u2211 k=1 dk\u03b1k\u03b2 \u1d40 k )\u2225\u2225\u2225\u2225\u2225 2\nF\n+ \u03bb r\u2211 k=1 dk (P2)\ns.t. dk \u2265 0, \u2016\u03b1k\u20162 \u2264 1, \u2016\u03b2k\u20162 \u2264 1, \u2200k \u2208 [r],\nThen, we have s = t. Furthermore, suppose an optimal solution for P2 is (~d\u2217,\u03b1\u2217,\u03b2\u2217), then Z\u2217 = \u2211r k=1 d \u2217 k\u03b1 \u2217 k\u03b2 \u2217\u1d40 k is also an optimal solution for P1. Similarly, for any optimal solution Z\u2020 for P1, there exists a decomposition Z\u2020 =\u2211r k=1 d \u2020 k\u03b1 \u2020 k\u03b2 \u2020 k \u1d40 optimal for P2.\nSketch of the proof. Let Z\u2217 = \u2211r k=1 d \u2217 k\u03b1 \u2217 k\u03b2 \u2217 k \u1d40 be an optimal solution for P2 with the optimal value t. Since P1\u2032 is basically the same optimization problem as P2 with stricter constraints, we have s \u2265 t. Conduct singular value decomposition to obtain Z\u2217 =\u2211r k=1 \u03c3 \u2217 ku \u2217 kv \u2217 k \u1d40 and we can prove that \u2016Z\u2217\u2016\u2217 =\u2211r\nk=1 \u03c3 \u2217 k \u2264 \u2211r k=1 d \u2217 k. If \u2211r k=1 \u03c3 \u2217 k < \u2211r k=1 d \u2217 k, then we can plug Z\u2217 into P1 to get a smaller value than t, contradicting s \u2265 t. As a result, \u2211r k=1 \u03c3 \u2217 k = \u2211r k=1 d \u2217 k and s = t.\nFurthermore, since s = t and plugging Z\u2217 into P1 can lead to a value at least as small as t, we conclude that Z\u2217 is also an optimal solution for P1. Let Z\u2020 be any optimal solution for P1, we can also prove that there is a decomposition Z\u2020 = \u2211r k=1 d \u2020 k\u03b1 \u2020 k\u03b2 \u2020 k \u1d40 to be an optimal solution for P2.\nThe formal proof and some remarks are provided in the supplementary material.\nNow we have justified the orthonormality insignificance property of spectral regularization. As a result, P2 serves to be another equivalent form of P1, similar to the role played by MF. This relaxed spectral regularization formulation lies somewhere between MF and spectral regularization, since it has more (but easily solvable) contraints than MF and still retains the form of SVD. As discussed before, it is easier to conduct Bayesian inference on a posterior without strict orthonormality constraints, and therefore models on relaxed spectral regularization are our focus of investigation.\nIn addition, Theorem 1 can be generalized to arbitrary loss besides squared-error loss, which means it is as widely applicable as MF. See Remark 2 in the supplementary material for more details.\nAdaptive relaxed spectral regularization Based on the relaxed spectral regularization formulation in Theorem 1, a Bayesian matrix completion algorithm similar to BPMF can be directly derived. Let the priors of \u03b1k, \u03b2k be uniform Haar priors within unit spheres; and the prior of dk\u2019s to be exponential distributions, then the posterior has exactly the same form as P2. Such an algorithm should have similar performance to BPMF.\nInstead of building models on P2 exactly, we consider another modified form where each dk has its own positive regularization parameter \u03b3k. Obviously this is a generalization of relaxed spectral regularization and admits it as a special case. We define the adaptive relaxed spectral regularization problem as\nmin ~d,U,V\n1\n2\u03c32 \u2225\u2225\u2225\u2225\u2225P\u2126 ( X \u2212 r\u2211 k=1 dkukv \u1d40 k )\u2225\u2225\u2225\u2225\u2225 2\nF\n+ r\u2211 k=1 \u03b3kdk (P3)\ns.t. dk \u2265 0, \u2016uk\u20162 \u2264 1, \u2016vk\u20162 \u2264 1, k \u2208 [r]. Such a variation is expected to be more flexible and better at bridging the gap between the nuclear norm and the rank functions, thus being more capable of approximating rank regularization than the standard nuclear norm. Similar ideas arose in (Todeschini, Caron, and Chavent 2013) before and was called hierarchical adaptive nuclear norm (HANN). But note that although we propose a similar approach to HANN, our regularization is substantially different because of the relaxed constraints.\nHowever, P3 may be harder to solve than the original P2 due to the difficulty in hyperparameter tuning, since adaptive regularization introduces dramatically more hyperparameters. We will build hierarchical priors for these hyperparameters and derive a Bayesian algorithm for solving P3 and inferring hyperparameters simultaneously in the following section."}, {"heading": "Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization", "text": ""}, {"heading": "Probabilistic model", "text": "We now turn P3 into an equivalent MAP estimation task. Naturally, the squared-error loss in P3 corresponds to the negative logarithm of the Gaussian likelihood, Xij \u223c N ( \u2211r k=1 dkukivkj , \u03c3\n2), where uki denotes the ith term in uk; likewise for vkj . For priors, we adopt uniform Haar priors on U and V within unit spheres, and exponential priors on ~d, as summarized below:\np\u0303(uk) = { 1, \u2016uk\u2016 \u2264 1 0, \u2016uk\u2016 > 1 , \u2200k \u2208 [r]\np\u0303(vk) = { 1, \u2016vk\u2016 \u2264 1 0, \u2016vk\u2016 > 1 , \u2200k \u2208 [r]\np(dk | \u03b3k) = \u03b3ke\u2212\u03b3kdk , dk \u2265 0, \u2200k \u2208 [r]\nwhere p\u0303 denotes an unnormalized probability density function (p.d.f.). It can be checked that under this probabilistic model, the negative log posterior p.d.f. with respect to (~d, U, V ) is exactly proportional to P3.\nNow we precede to treat regularization coefficients ~\u03b3 := {\u03b3k : k \u2208 [r]} as random variables and assume gamma priors on them, i.e., p(\u03b3k) \u221d \u03b3a\u22121k e\u2212b\u03b3k , \u03b3k \u2265 0, \u2200k \u2208 [r]. This has two obvious advantages. First, it includes ~\u03b3 into the Bayesian framework so that values of these coefficients can be inferred automatically without being tuned as hyperparameters. Second, the prior on dk after marginalizing out \u03b3k\u2019s becomes p(dk) = \u222b\u221e 0 p(dk | \u03b3k)p(\u03b3k)d\u03b3k =\naba\n(dk+b)a+1 , which is effectively a Pareto distribution. This distribution has a heavier tail compared to the exponential distribution (Todeschini, Caron, and Chavent 2013), and is therefore expected to be better at sparsity-inducing (Bach et al. 2012).\nThe graphical model is shown in Figure 1, where we explicitly separate the observed entries of X , i.e., P\u2126(X), and the unobserved ones, i.e., P\u22a5\u2126 (X). Due to the conditional independency structure, we can simply marginalize out P\u22a5\u2126 (X) and get the joint distribution\np(~d, U, V,~\u03b3, P\u2126(X) | a, b, \u03c3)\n\u221d ( 1\n2\u03c32\n)|\u2126|/2 exp \u2212 1 2\u03c32 \u2225\u2225\u2225\u2225\u2225P\u2126(X \u2212 r\u2211\nk=1\ndkukv \u1d40 k) \u2225\u2225\u2225\u2225\u2225 2\nF  \u00b7 r\u220f\nk=1\nba\n\u0393(a) \u03b3ake \u2212(b+dk)\u03b3k , (2)\nwith all the variables implicitly constrained to their corresponding valid domains."}, {"heading": "Inference", "text": "We now present the GASR (Gibbs sampler for Adaptive Relaxed Spectral Regularization) algorithm to infer the posterior, make predictions, and estimate the hyperparameters via Monte Carlo EM (Casella 2001).\nPosterior Inference Let N (\u00b5, \u03c32; a, b) denote the normal distribution N (\u00b5, \u03c32) truncated in [a, b]. We infer the posterior distribution p(~\u03b3, ~d, U, V | a, b, \u03c3, P\u2126(X)) via a Gibbs sampler as explained below:\nSample ~\u03b3: The conditional distributions for regularization coefficients ~\u03b3 are gamma distributions. We sample ~\u03b3 by the formula \u03b3k \u223c \u0393(a+ 1; b+ dk), k \u2208 [r].\nSample ~d: Conditioned on (~\u03b3, U, V ), the distribution for each d\u03b1 (\u03b1 \u2208 [r]) is a truncated Gaussian, d\u03b1 \u223c N (\u2212BA , \u03c32 A ; 0,\u221e), where A = \u2211 (i,j)\u2208\u2126 (u\u03b1iv\u03b1j) 2 and\nB = \u2211\n(i,j)\u2208\u2126 (\u2211 k 6=\u03b1 dku\u03b1iukiv\u03b1jvkj \u2212Xiju\u03b1iv\u03b1j ) +\n\u03c32\u03b3\u03b1. Sample U and V : Given the other variables, the distribution for each element in u\u03b1\u2019s (or v\u03b1\u2019s) is a truncated Gaussian, u\u03b1\u03b2 \u223c N ( \u2212DC , \u03c32 C ;\u2212\u03c1, \u03c1 ) , \u03b1 \u2208\n[m], \u03b2 \u2208 [r], where C = \u2211\n(\u03b2,j)\u2208\u2126 d 2 \u03b1v 2 \u03b1j , D =\u2211\n(\u03b2,j)\u2208\u2126 (\u2211 k 6=\u03b1 d\u03b1dkuk\u03b2v\u03b1jvkj \u2212 d\u03b1X\u03b2jv\u03b1j ) and \u03c1 =\u221a\n1\u2212 \u2211 k 6=\u03b2 u 2 \u03b1k. A similar procedure can be derived to\nsample v\u03b1\u03b2 and is therefore omitted here. The time complexity of this Gibbs sampler is O(|\u2126|r2) per iteration. Although there is a unified scheme on sampling truncated distributions by cumulative distribution function (c.d.f.) inversion, we did not use it due to numerical instabilities found in experiments. In contrast, simple rejection sampling methods prove to work well.\nPrediction With the posterior distribution, we can complete the missing elements using the posterior mean:\nE [ P\u22a5\u2126 (X) | P\u2126(X), a, b, \u03c3 ] = \u222b \u00b7 \u00b7 \u00b7 \u222b E [ P\u22a5\u2126 (X) | P\u2126(X), a, b, \u03c3,~\u03b3, U, V, ~d\n] \u00b7 p(~\u03b3, ~d, U, V | a, b, \u03c3, P\u2126(X))d~\u03b3d~ddUdV.\nThis integral is intractable. But we can use samples to approximate the integral and complete the matrix. Since we use the Gaussian likelihood, we have\nE [ P\u22a5\u2126 (X) | P\u2126(X), a, b, \u03c3,~\u03b3, U, V, ~d ] = r\u2211 k=1 dkukv \u1d40 k .\nAs a result, we can represent missing elements as xij = \u3008 \u2211r k=1 dkukivkj\u3009 , (i, j) \u2208 \u2126\u22a5, which is the posterior sample mean of P\u22a5\u2126 (X). Here we denote the sample mean for f(x) as \u3008f(x)\u3009 := 1n \u2211n i=1 f(xi), with xi being individual samples and n being the number of samples.\nHyperparameter Estimation We choose the hyperparameters (a, b, \u03c3) by maximizing model evidence p(P\u2126(X) | a, b, \u03bb). Since direct optimization is intractable, we adopt an EM algorithm, with latent variable L := (~d, U, V,~\u03b3). In order to compute the joint expectation with respect to P\u2126(X) and L, we use Monte Carlo EM (Casella 2001), which can fully exploit the samples obtained in the Gibbs sampler.\nThe expectation of P\u2126(X) and L with respect to p(L | P\u2126(X)) can be written as\nEp(L|P\u2126(X)) [ln p(P\u2126(X), L)] = E [ ln p(~d, U, V,~\u03b3, P\u2126(X) | a, b, \u03c3) ] \u2248 \u2212|\u2126| ln\u03c3 \u2212 1\n2 \u2329\u2225\u2225\u2225\u2225\u2225P\u2126 ( X \u2212 r\u2211 k=1 dkukv \u1d40 k )\u2225\u2225\u2225\u2225\u2225 2\nF\n\u232a\n+ r\u2211 i=1 [a ln b\u2212 ln \u0393(a) + a\u3008ln \u03b3i\u3009 \u2212 b\u3008\u03b3i\u3009] + C, (3)\nwhere C is a constant. Eq. (3) can be maximized with respect to a, b, \u03c3 using Newton\u2013Raphson method. The fixpoint equations are\nat+1 = at \u2212 \u03a8(at)\u2212 ln (rat/\n\u2211 i\u3008\u03b3i\u3009)\u2212 \u2211 i\u3008ln \u03b3i\u3009/r\n\u03a8\u2032(at)\u2212 1/at bt+1 =\nrat+1\u2211r i=1\u3008\u03b3i\u3009\n\u03c32 = 1\n|\u2126| \u2329\u2225\u2225\u2225\u2225\u2225P\u2126 ( X \u2212 r\u2211 k=1 dkukv \u1d40 k )\u2225\u2225\u2225\u2225\u2225 2\nF\n\u232a ,\nwhere \u03a8(x) and \u03a8\u2032(x) are digamma and trigamma functions respectively. In our experiments, we found the results not very sensitive to the number of samples used in \u3008\u00b7\u3009, so we fixed it to 5."}, {"heading": "Experiments", "text": "We now present experimental results on both synthetic and real datasets to demonstrate the effectiveness on rank recovery and matrix completion."}, {"heading": "Experiments on synthetic data", "text": "We have two experiments on synthetic data, one is for rank recovery, the other is for examining how well the algorithms perform in situations that matrices are very sparse.\nIn both experiments, we generate standard normal random matrices Am\u00d7q and Bn\u00d7q and produce a rank-q matrix Z = AB\u1d40. Then we corrupt Z with standard Gaussian noise to get the observations X , using a signal to noise ratio of 1.\nRank recovery In this experiment, we set m = 10q and n = 10q. The algorithm was tested with q ranging from 5 to 45. We set the rank truncation r to 100, which is sufficiently large for all data. For each matrix Z, the iteration number was fixed to 1000 and the result was averaged from last 200 samples (with first 800 discarded as burn-in). We simply initialize our sampler with uniformly distributed U and V with norms fixed to 0.9 and all ~d fixed to zero. We run our Gibbs sampler on all the entries of X to recover Z.\nIn the spectral regularization formulation, we can get the number of latent factors by simply counting the number of nonzero dk\u2019s. However, since our method uses MCMC sampling, it is diffcult to find some dk to vanish exactly. Instead of counting nonzero elements of (d1, d2, \u00b7 \u00b7 \u00b7 , dr) directly, we sort the tuple in an ascending order and try to\nlocate w = arg maxk\u22652 dk/dk\u22121 and then discard the set {dk : dk < dw}. As a result, the recovered rank is r\u2212w+1. The middle panel of Figure 2 provides an example about how to determine the number of latent factors.\nThe results of this experiment are summarized in Figure 2(c), showing that the recovered ranks are fairly aligned with the ground truth. Our algorithm performs perfectly well when the true rank is relatively small and slightly worse when the rank gets higher. This may be due to that larger rank requires more iterations for convergence.\nWe also illustrate how vectors get orthonormalized automatically on one of our synthetic matrices in Figure 2(a). The orthonormality of vectors are measured by the average values of their `2 norms and inner products with each other. Figure 2(a) shows that U and V get nearly orthonormalized after only one iteration. This phenomenon indicates that the vectors still tend to get orthonormalized even in the hierarchial Bayesian model.\nDifferent missing rates We generate matrices of different sizes and different missing rates to test the performance of our method, in comparison with BPMF, as it is the only one that can compete with GASR on real datasets, as illustrated in detail in the next section.\nThe Root Mean Squared Error (RMSE) results are listed in Table 3. The deviations and some additional settings are reported in supplementary material. We can see that GASR is considerably better than BPMF when the observed elements of a matrix are of a small number, demonstrating the robust estimates of GASR via sparsity-inducing priors.\nCollaborative filtering on real datasets We test our algorithm on the MovieLens 1M1 and EachMovie datasets, and compare results with various\n1MovieLens datasets can be downloaded from http://grouplens.org/datasets/movielens/.\nstrong competitors, including max-margin matrix factorization (M3F) (Rennie and Srebro 2005), infinite probabilistic max-margin matrix factorization (iPM3F) (Xu, Zhu, and Zhang 2012), softImpute (Mazumder, Hastie, and Tibshirani 2010), softImpute-ALS (\u201cALS\u201d for \u201calternating least squares\u201d) (Hastie et al. 2014), hierarchical adaptive soft impute (HASI) (Todeschini, Caron, and Chavent 2013) and Bayesian probabilistic matrix factorization (BPMF) (Salakhutdinov and Mnih 2008).\nThe MovieLens 1M dataset contains 1,000,209 ratings provided by 6,040 users on 3,952 movies. The ratings are integers from {1, 2, 3, 4, 5} and each user has at least 20 ratings. The EachMovie dataset consists of 2,811,983 ratings provided by 74,424 users on 1,648 movies. As in (Marlin 2004), we removed duplicates and discarded users with less than 20 ratings. This left us with 36,656 users. There are 6 possible ratings from 0 to 1 and we mapped them to {1, 2, \u00b7 \u00b7 \u00b7 , 6}.\nProtocol: We randomly split the dataset into 80% training and 20% test. We further split 20% training data for validation for M3F, iPM3F, SoftImpute, SoftImpute-ALS and HASI to tune their hyperparameters. BPMF and GASR\ncan infer hyperparameters from training data and thus do not need validation. We measure the performance using both RMSE and normalized mean absolute error (NMAE), where NMAE (Goldberg et al. 2001) is defined as\nNMAE = 1\n|\u2126test| \u2211 (i,j)\u2208\u2126test |Xij \u2212 Zij | max(X)\u2212min(X) , (4)\nand \u2126test is the index set of entries for testing. Implementation details: The number of iterations of\nour sampler is fixed to 100 and the point estimates of E [ P\u22a5\u2126 (X) ] are taken from the average of all 100 samples. We initialize our algorithm by generating uniformly distributed items for U and V and set all dk to 0. We also scale the norms of uk and vk to 0.9 for initialization. Figure 3 shows that our sampler converges after a few iterations with this fairly na\u0131\u0308ve initialization.\nWe use the R package softImpute for SoftImpute and SoftImpute-ALS, and use the code provided by the corresponding authors for M3F, iPM3F, HASI and BPMF. The hyperparameters for M3F, iPM3F, SoftImpute, SoftImputeALS and HASI are selected via grid search on the validation set. We randomly initialize all methods except HASI for which the initialization is the result of SoftImpute, as suggested in (Todeschini, Caron, and Chavent 2013). The results of BPMF are the averages over 100 samples, the same as ours.\nFor all the algorithms, we set the maximum number of iterations to 100. The rank truncation r for MovieLens 1M and EachMovie is set to 30, where we follow the setting in (Todeschini, Caron, and Chavent 2013) and find in experiments that larger r does not lead to significant improvements.\nResults: Table 2 presents the averaged NMAE and RMSE over 5 replications and their standard deviations.2 Overall,\n2The NMAE of HASI on MovieLens is slightly different from that in (Todeschini, Caron, and Chavent 2013), which was 0.172, still inferior to ours. This may be due to differences in parameter selecting methods.\nwe can see that our GASR achieves superior performance than most of the baselines. More specifically, we have the following observations:\nFirst, GASR is comparable to BPMF, the state-of-theart Bayesian method for low-rank matrix completion, on the MovieLens dataset; while it outperforms BPMF on the EachMovie dataset, with the observation that EachMovie dataset (97.8% missing) is sparser than MovieLens (95.8% missing). On both datasets, GASR also obtains much lower RMSE than iPM3F, a state-of-the-art nonparametric Bayesian method based on IBP (Griffiths and Ghahramani 2011) for matrix completion. Such results demonstrate the promise of performing Bayesian matrix completion via spectral regularization. Furthermore, GASR produces sparser solutions due to its sparsity-inducing priors on ~d. The ranks it infers on MovieLens and EachMovie are both 10 on average, but the numbers of latent factors inferred by iPM3F are both 30, which is the rank truncation level. It is reported in (Xu, Zhu, and Zhang 2013) with a similar setting that the optimal latent dimensions inferred by Gibbs iPM3F, a Gibbs sampling version for iPM3F model without rank truncations, are around 450 for MovieLens and 200 for EachMovie, which are much larger than ours.\nSecond, compared to HASI, a non-Bayesian method that adopts similar adaptive spectral regularization, and the other non-Bayesian methods based on squared-error losses (i.e., SoftImpute and SoftImpute-ALS), we achieve much better results on both datasets, demonstrating the advantages of Bayesian inference. Moreover, the better performance of HASI over SoftImpute demonstrates the benefit of adaptivity.\nFinally, the max-margin based methods (i.e., M3F and iPM3F) have slightly better performance on NMAE but much worse results on RMSE than our GASR. One possible reason is that these methods are based on the maximummargin criterion, which naturally minimizes absolute errors, while our method (and the others) is based on minimizing a squared-error loss. Another reason, which may be the most important one, is that both M3F and iPM3F predict integer values while our method (and the others) gives real value predictions. We found that simply rounding these real value predictions to integers can greatly improve the performance on NMAE. For example, our GASR gives NMAE value 0.1569\u00b1 0.0006 and 0.1877\u00b1 0.0003 respectively on MovieLens and EachMovie datasets after rounding predictions to nearest integers."}, {"heading": "Conclusions and Discussions", "text": "We present a novel Bayesian matrix completion method with adaptive relaxed spectral regularization. Our method exhibits the benefits of hierarchical Bayesian methods on inferring the parameters associated with adaptive relaxed spectral regularization, thereby avoiding parameter tuning. We estimate hyperparameters using Monte Carlo EM. Our Gibbs sampler exhibits good performance both in rank inference on synthetic data and collaborative filtering on real datasets.\nOur method is based on a new formulation in Theorem 1. These results can be further generalized to other noise poten-\ntials with minor effort. For the Gibbs sampler, we can also extend to non-Gaussian potentials as long as this potential has a regular p.d.f. that enables efficient sampling.\nFinally, though we stick to Gibbs sampling in this paper, it would be interesting to investigate other Bayesian inference methods based on Theorem 1 since it gets rid of many difficulties related to Stiefel manifolds. Such an investigation may lead to more scalable algorithms with better convergence property. Furthermore, better initialization methods other than uniformly generated random numbers may lead to much faster convergence, e.g., results from several iterations of HASI can usually provide a good starting point."}, {"heading": "Acknowledgments", "text": "The work was supported by the National Basic Research Program (973 Program) of China (Nos. 2013CB329403, 2012CB316301), National NSF of China (Nos. 61322308, 61332007), Tsinghua National Laboratory for Information Science and Technology Big Data Initiative, and Tsinghua Initiative Scientific Research Program (No. 20141080934). We thank the Department of Physics at Tsinghua University for covering part of the travel costs."}, {"heading": "Supplementary Materials", "text": ""}, {"heading": "Proof for Theorem 1", "text": "Proof. Denote Z = \u2211r k=1 dk\u03b1k\u03b2 \u1d40 k and conduct singular value decomposition to give\nZ = r\u2211 k=1 \u03c3ku\u00afk v \u00af \u1d40 k = r\u2211 k=1 dk\u03b1k\u03b2 \u1d40 k (5)\nwhere \u03c31:r are singular values of Z and U = {u1,u2, \u00b7 \u00b7 \u00b7 ,um} and V = {v1,v2, \u00b7 \u00b7 \u00b7 ,vn} are corresponding orthogonal matrices. Note that we denote r = min(m,n) throughout this paper. If the true rank is smaller than min(m,n), then singular values with indices larger than the rank are assumed to be zero. We will try to prove\nr\u2211 k=1 \u03c3k \u2264 r\u2211 k=1 dk, (6)\nwhich actually implies all the assertions in the theorem. For \u2200i \u2208 [r], left multiply equation (5) with u\n\u00af \u1d40 i and right\nmultiply with v \u00afi to obtain\n\u03c3i = r\u2211 k=1 dku\u00af \u1d40 i\u03b1k\u03b2 \u1d40 kv\u00afi . (7)\nSince U and V are orthogonal matrices with full ranks in a singular value decomposition, we can regard column vectors of U and V to be eigenbases of space Rm and Rn. Hence it is natural to obtain\n\u03b1i = m\u2211 j=1 xijuj , \u03b2i = n\u2211 j=1 yijvj , (8)\nwhere \u2200i \u2208 [r], \u2211m j=1 x 2 ij \u2264 1 and \u2211n j=1 y 2 ij \u2264 1.\nWe then rewrite Eq. (7) to give\n\u03c3i = r\u2211 k=1 dkxkiyki. (9)\nAs a result, r\u2211 i=1 \u03c3i = r\u2211 i=1 r\u2211 k=1 dkxkiyki = r\u2211 k=1 dk r\u2211 i=1 xkiyki\n\u2264 r\u2211\nk=1\ndk ( r\u2211 i=1 x2ki )1/2( r\u2211 i=1 y2ki )1/2\n\u2264 r\u2211\nk=1\ndk ( m\u2211 i=1 x2ki )1/2( n\u2211 i=1 y2ki )1/2\n\u2264 r\u2211\nk=1\ndk, (10)\nwhich means for any valid tuples of (d1:r,\u03b11:r,\u03b21:r), replacing \u2211r k=1 dk\u03b1k\u03b2 \u1d40 k with \u2211r k=1 \u03c3ku\u00afk v \u00af \u1d40 k in P2, according to (6), will not make the solution worse. This indicates that there is at least one optimal solution of P2 having the form of singular value decompositions like those in P1\u2032.\nAs a result, we always have s \u2264 t, because for any optimal solution of P2, we can get an SVD form compatible to the constraints of P1\u2032 with an objective value not larger. However, considering the fact that P2 is basically the same problem as P1\u2032 with looser constraints, we conclude that t \u2264 s. Following the reasoning above we get s \u2264 t and s \u2265 t, which exactly means s = t.\nSuppose we have got the optimal solutions of P2, which is denoted as (d\u22171:r,\u03b1 \u2217 1:r,\u03b2 \u2217 1:r). We assert that Z \u2217 =\u2211r k=1 d \u2217 k\u03b1 \u2217 k\u03b2 \u2217\u1d40 k is the optimal solution of P1, because plugging Z\u2217 into P1 will yield a value not greater than t. Since s = t and s is the minimum possible value of P1, we conclude that pluggingZ\u2217 into P1 gets the value s, which means Z\u2217 is the optimal solution for P1.\nSimilarly, suppose that the optimal solution of P1 is Z\u2020, we compute its singular value decomposition to get Z\u2020 =\u2211r k=1 \u03c3 \u2020 ku \u2020 kv \u2020 k \u1d40 . Then plugging (\u03c3\u20201:r,u \u2020 1:r,v \u2020 1:r) into P2 will give the value s. Since s = t, we conclude that Z\u2020 is an optimal solution for P2.\nNote that it is practically very difficult for \u2211r k=1 \u03c3k =\u2211r\nk=1 dk to hold as this requires \u2211r i=1 x 2 ki = \u2211r i=1 y 2 ki = 1 and xki = yki, for all k, i \u2208 [r]. This means that conducting singular value decomposition to any Z = \u2211r k=1 dk\u03b1k\u03b2 \u1d40 k and substituting singular values and vectors into P2 can typically get a better result, which indicates that \u03b11:r and \u03b21:r will nearly always get orthonormalized automatically under the unit sphere constraints.\nRemark 1. The optimal solutions for P2 are not necessarily unique. As a result, the optimal \u03b11:r and \u03b21:r for P2 are not always orthonormalized, though orthonormal vectors provide a solution. However, what matters is not the orthonormality of \u03b11:r and \u03b21:r, but the equivalence of optimal solution Z = \u2211r k=1 dk\u03b1k\u03b2 \u1d40 k . Theorem 1 asserts that P1 and P2 produce the same set of optimal matrix completion results. As a result, the MAP problem constructed according to P2 is anticipated to function similarly as the one constructed from P1.\nNote that the condition for strict equality in \u2211r k=1 \u03c3 \u2217 k \u2264\u2211r\nk=1 d \u2217 k is practically very hard to satisfy.\nRemark 2. This special relationship between P1 and P2 in Theorem 1 can be generalized to other forms of noise potentials besides the squared-error loss as well as the maxmargin hinge loss used in MMMF, as we do not need the property of \u2016\u00b7\u2016F in our proof. The theorem should still hold if we replace \u2016\u00b7\u2016F with \u2016\u00b7\u20161, \u2016\u00b7\u2016\u221e, etc."}, {"heading": "Detailed Experimental Results for Different Missing Rates", "text": "In this experiment, we run both BPMF and GASR for 100 iterations and average all 100 samples to produce the final result. The average RMSE and corresponding deviations on 3 randomly generated datasets are reported in Table 3."}], "references": [{"title": "Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning 4(1):1\u2013106", "author": ["Bach"], "venue": null, "citeRegEx": "Bach,? \\Q2012\\E", "shortCiteRegEx": "Bach", "year": 2012}, {"title": "and Girolami", "author": ["S. Byrne"], "venue": "M.", "citeRegEx": "Byrne and Girolami 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Tourneret", "author": ["N. Dobigeon"], "venue": "J.-Y.", "citeRegEx": "Dobigeon and Tourneret 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Eigentaste: A constant time collaborative filtering algorithm. Information Retrieval 4(2):133\u2013151", "author": ["Goldberg"], "venue": null, "citeRegEx": "Goldberg,? \\Q2001\\E", "shortCiteRegEx": "Goldberg", "year": 2001}, {"title": "and Ghahramani", "author": ["T.L. Griffiths"], "venue": "Z.", "citeRegEx": "Griffiths and Ghahramani 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Matrix completion and low-rank SVD via fast alternating least squares. arXiv preprint arXiv:1410.2596", "author": ["Hastie"], "venue": null, "citeRegEx": "Hastie,? \\Q2014\\E", "shortCiteRegEx": "Hastie", "year": 2014}, {"title": "P", "author": ["Hoff"], "venue": "D.", "citeRegEx": "Hoff 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "I", "author": ["James"], "venue": "M.", "citeRegEx": "James 1976", "shortCiteRegEx": null, "year": 1976}, {"title": "Y", "author": ["Y.J. Lim", "Teh"], "venue": "W.", "citeRegEx": "Lim and Teh 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "author": ["Hastie Mazumder", "R. Tibshirani 2010] Mazumder", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Mazumder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mazumder et al\\.", "year": 2010}, {"title": "and Srebro", "author": ["J.D. Rennie"], "venue": "N.", "citeRegEx": "Rennie and Srebro 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "and Mnih", "author": ["R. Salakhutdinov"], "venue": "A.", "citeRegEx": "Salakhutdinov and Mnih 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "T", "author": ["N. Srebro", "J. Rennie", "Jaakkola"], "venue": "S.", "citeRegEx": "Srebro. Rennie. and Jaakkola 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Nonparametric max-margin matrix factorization for collaborative prediction", "author": ["Zhu Xu", "M. Zhang 2012] Xu", "J. Zhu", "B. Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}, {"title": "Fast max-margin matrix factorization with data augmentation", "author": ["Zhu Xu", "M. Zhang 2013] Xu", "J. Zhu", "B. Zhang"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Xu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2013}], "referenceMentions": [], "year": 2015, "abstractText": "Bayesian matrix completion has been studied based on a low-rank matrix factorization formulation with promising results. However, little work has been done on Bayesian matrix completion based on the more direct spectral regularization formulation. We fill this gap by presenting a novel Bayesian matrix completion method based on spectral regularization. In order to circumvent the difficulties of dealing with the orthonormality constraints of singular vectors, we derive a new equivalent form with relaxed constraints, which then leads us to design an adaptive version of spectral regularization feasible for Bayesian inference. Our Bayesian method requires no parameter tuning and can infer the number of latent factors automatically. Experiments on synthetic and real datasets demonstrate encouraging results on rank recovery and collaborative filtering, with notably good results for very sparse matrices. Introduction Matrix completion has found applications in many situations, such as collaborative filtering. Let Zm\u00d7n denote the data matrix with m rows and n columns, of which only a small number of entries are observed, indexed by \u03a9 \u2282 [m] \u00d7 [n]. We denote the possibly noise corrupted observations of Z on \u03a9 as P\u03a9(X), where P\u03a9 is a projection operator that retains entries with indices from \u03a9 and replaces others with 0. The matrix completion task aims at completing missing entries of Z based on P\u03a9(X), under the low-rank assumption rank(Z) min(m,n). When a squared-error loss is adopted, it can be written as solving: min Z 1 2\u03c32 \u2016P\u03a9(X \u2212 Z)\u20162F + \u03bb rank(Z), (P0) where \u2016P\u03a9(A)\u20162F = \u2211 (i,j)\u2208\u03a9 a 2 ij ; \u03bb is a positive regularization parameter; and \u03c3 is the noise variance. Unfortunately, the term rank(Z) makes P0 NP-hard. Therefore, the nuclear norm \u2016Z\u2016\u2217 has been widely adopted as a convex surrogate (Fazel 2002) to the rank function to turn P0 to a convex problem: min Z 1 2\u03c32 \u2016P\u03a9(X \u2212 Z)\u20162F + \u03bb \u2016Z\u2016\u2217 . (P1) Copyright \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Though P1 is convex, the definition of nuclear norm makes the problem still not easy to solve. Based on a variational formulation of the nuclear norm, it has been popular to solve an equivalent and easier low-rank matrix factorization (MF) form of P1: min A,B 1 2\u03c32 \u2016P\u03a9(X \u2212AB)\u20162F + \u03bb 2 (\u2016A\u20162F + \u2016B\u2016 2 F ). (1) Though not joint convex, this MF formulation can be solved by alternately optimizing over A and B for local optima. As the regularization terms of MF are friendlier than the nuclear norm, many matrix factorization methods have been proposed to complete matrices, including maximummargin matrix factorization (M3F) (Srebro, Rennie, and Jaakkola 2004; Rennie and Srebro 2005) and Bayesian probabilistic matrix factorization (BPMF) (Lim and Teh 2007; Salakhutdinov and Mnih 2008). Furthermore, the simplicity of the MF formulation helps people adapt it and generalize it; e.g., (Xu, Zhu, and Zhang 2012; Xu, Zhu, and Zhang 2013) incorporate maximum entropy discrimination (MED) and nonparametric Bayesian methods to solve a modified MF problem. In contrast, there are relatively fewer algorithms to directly solve P1 without the aid of matrix factorization. Such methods need to handle the spectrum of singular values. These spectral regularization algorithms require optimization on a Stiefel manifold (Stiefel 1935; James 1976), which is defined as the set of k-tuples (u1,u2, \u00b7 \u00b7 \u00b7 ,uk) of orthonormal vectors in R. This is the main difficulty that has prevented the attempts, if any, to develop Bayesian methods based on the spectral regularization formulation. Though matrix completion via spectral regularization is not easy, there are potential advantages over the matrix factorization approach. One of the benefits is the direct control over singular values. By imposing various priors on singular values, we can incorporate abundant information to help matrix completion. For example, Todeschini et al. (Todeschini, Caron, and Chavent 2013) put sparsity-inducing priors on singular values, naturally leading to hierarchical adaptive nuclear norm (HANN) regularization, and they reported promising results. In this paper, we aim to develop a new formulation of the nuclear norm, hopefully having the same simplicity as MF and retaining some good properties of spectral regularar X iv :1 51 2. 01 11 0v 2 [ cs .N A ] 2 5 D ec 2 01 5 ization. The idea is to prove the orthonormality insignificance property of P1. Based on the new formulation, we develop a novel Bayesian model via a sparsity-inducing prior on singular values, allowing various dimensions to have different regularization parameters and automatically infer them. This involves some natural modifications to our new formulation to make it more flexible and adaptive, as people typically do in Bayesian matrix factorization. Empirical Bayesian methods are then employed to avoid parameter tuning. Experiments about rank recovery on synthetic matrices and collaborative filtering on some popular benchmark datasets demonstrate competitive results of our method in comparison with various state-of-the-art competitors. Notably, experiments on synthetic data show that our method performs considerably better when the matrices are very sparse, suggesting the robustness offered by using sparsityinducing priors. Relaxed Spectral Regularization Bayesian matrix completion based on matrix factorization is relatively easy, with many examples (Lim and Teh 2007; Salakhutdinov and Mnih 2008). In fact, we can view (1) as a maximum a posterior (MAP) estimate of a simple Bayesian model, whose likelihood is Gaussian, i.e., for (i, j) \u2208 \u03a9, Xij \u223c N ((AB)ij , \u03c3), and the priors on A and B are also Gaussian, i.e., p(A) \u221d exp(\u2212\u03bb \u2016A\u20162F /2) and p(B) \u221d exp(\u2212\u03bb \u2016B\u20162F /2). It is now easy to do the posterior inference since the prior and likelihood are conjugate. However, the same procedure faces great difficulty when we attempt to develop Bayesian matrix completion based on the more direct spectral regularization formulation P1. This is because the prior p(Z) \u221d exp(\u2212\u03bb \u2016Z\u2016\u2217) is not conjugate to the Gaussian likelihood (or any other common likelihood). To analyze p(Z) more closely, we can conduct singular value decomposition (SVD) on Z to get Z = \u2211r k=1 dkukv T k , where ~ d := {dk : k \u2208 [r]} are singular values; U := {uk : k \u2208 [r]} and V := {vk : k \u2208 [r]} are orthonormal singular vectors on Stiefel manifolds. Though we can define a factorized prior p(Z) = p(~ d)p(U)p(V ), any prior on U or V (e.g., the uniform Haar prior (Todeschini, Caron, and Chavent 2013)) needs to deal with a Stiefel manifold, which is highly nontrivial. In fact, handling distributions embedded on Stiefel manifolds still remains a largely open problem, though some results (Byrne and Girolami 2013; Hoff 2009; Dobigeon and Tourneret 2010) exist in the literature of directional statistics. Fortunately, as we will prove in Theorem 1 that the orthonormality constraints on U and V are not necessary for spectral regularization. Rather, the unit sphere constraints \u2016uk\u2016 \u2264 1 and \u2016vk\u2016 \u2264 1, for all k \u2208 [r], are sufficient to get the same optimal solutions to P1. We call this phenomenon orthonormality insignificance. We will call spectral regularization with orthonormality constraints relaxed by unit sphere constraints relaxed spectral regularization. Orthonormality insignificance for spectral regularization We now present an equivalent formulation of the spectral regularization in P1 by proving its orthornormality insignificance property. With the SVD of Z, we first rewrite P1 equivalently as P1\u2032 to show all constraints explicitly: min ~ d,U,V 1 2\u03c32 \u2225\u2225\u2225\u2225P\u03a9 ( X \u2212 r \u2211 k=1 dkukv T k \u2225\u2225\u2225\u2225 2", "creator": "LaTeX with hyperref package"}}}