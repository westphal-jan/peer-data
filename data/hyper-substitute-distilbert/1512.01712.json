{"id": "1512.01712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2015", "title": "Generating News Headlines with Recurrent Neural Networks", "abstract": "this obtain improved application optimization filter encoder - series of auditory graph linking lstm units termed attention ^ talk headlines from the text & research articles. we find that the filter goes fairly effective at dramatically paraphrasing new articles. furthermore, we study how the neural network decides multiple input words to communicate attention to, thereby specifically we identify optimal function of describing different neurons including a closed temporal path. interestingly, our simplified attention language performs better that the more restrictive attention mechanism shows a held out set of articles.", "histories": [["v1", "Sat, 5 Dec 2015 23:41:22 GMT  (215kb,D)", "http://arxiv.org/abs/1512.01712v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["konstantin lopyrev"], "accepted": false, "id": "1512.01712"}, "pdf": {"name": "1512.01712.pdf", "metadata": {"source": "CRF", "title": "Generating News Headlines with Recurrent Neural Networks", "authors": ["Konstantin Lopyrev"], "emails": ["klopyrev@stanford.edu"], "sections": [{"heading": "1 Background", "text": "Recurrent neural networks have recently been found to be very effective for many transduction tasks - that is transforming text from one form to another. Examples of such applications include machine translation [1,2] and speech recognition [3]. These models are trained on large amounts of input and expected output sequences, and are then able to generate output sequences given inputs never before presented to the model during training.\nRecurrent neural networks have also been applied recently to reading comprehension [4]. There, the models are trained to recall facts or statements from input text.\nOur work is closely related to [5] who also use a neural network to generate news headlines using the same dataset as this work. The main difference to this work is that they do not use a recurrent neural network for encoding, instead using a simpler attention-based model."}, {"heading": "2 Model", "text": ""}, {"heading": "2.1 Overview", "text": "We use the encoder-decoder architecture described in [1] and [2], and shown in figure 1. The architecture consists of two parts - an encoder and a decoder - both by themselves recurrent neural networks.\nar X\niv :1\n51 2.\n01 71\n2v 1\n[ cs\n.C L\n] 5\nD ec\n2 01\nThe encoder is fed as input the text of a news article one word of a time. Each word is first passed through an embedding layer that transforms the word into a distributed representation. That distributed representation is then combined using a multi-layer neural network with the hidden layers generated after feeding in the previous word, or all 0\u2019s for the first word in the text.\nThe decoder takes as input the hidden layers generated after feeding in the last word of the input text. First, an end-of-sequence symbol is fed in as input, again using an embedding layer to transform the symbol into a distributed representation. Then, the decoder generates, using a softmax layer and the attention mechanism, described in the next section, each of the words of the headline, ending with an end-of-sequence symbol. After generating each word that same word is fed in as input when generating the next word.\nThe loss function we use is the log loss function:\n\u2212 log p(y1, . . . , yT \u2032 |x1, . . . , xT ) = \u2212 T \u2032\u2211 t=1 log p(yt|y1, . . . , yt\u22121, x1, . . . , xT )\nwhere y represent output words and x represent input words.\nNote that during training of the model it is necessary to use what is called \u201cteacher forcing\u201d [6]. Instead of generating a new word and then feeding in that word as input when generating the next word, the expected word in the actual headline is fed in. However, during testing the previously generated word is fed in when generating the next word. That leads to a disconnect between training and testing. To overcome this disconnect, during training we randomly feed in a generated word, instead of the expected word, as suggested in [7]. Specifically, we do this 10% of the time, as also done in [8]. During testing we use a beam-search decoder which generates input words one at a time, at each step extending the B highest probability sequences.\nWe use 4 hidden layers of LSTM units, specifically the variant described in [9]. Each layer has 600 hidden units. We attempted using dropout as is also described in [9]. However we did not find it to be useful. Thus, the models analyzed below do not use dropout. We initialize most parameters of the model uniformly in the range [\u22120.1; 0.1]. We initialize the biases for each word in the softmax layer to the log-probability of its occurence in the training data, as suggested in [10].\nWe use a learning rate of 0.01 along with the RMSProp [11] adaptive gradient method. For RMSProp we use a decay of 0.9 and a momentum of 0.9. We train for 9 epochs, starting to half the learning rate at the end of each epoch after 5 epochs.\nAdditionally, we batch examples, processing 384 examples at a time. This batching complicates the implementation due to the varying lengths of different sequences. We simply fix the maximum lengths of input and output sequences and use special logic to ensure that the correct hidden states are fed in during the first step of the decoder, and that no loss is incurred past the end of the output sequence."}, {"heading": "2.2 Attention", "text": "Attention is a mechanism that helps the network remember certain aspects of the input better, including names and numbers. The attention mechanism is used when outputting each word in the decoder. For each output word the attention mechanism computes a weight over each of the input words that determines how much attention should be paid to that input word. The weights sum up to 1, and are used to compute a weighted average of the last hidden layers generated after processing each of the input words. This weighted average, referred to as the context, is then input into the softmax layer along with the last hidden layer from the current step of the decoding.\nWe experiment with two different attention mechanisms. The first attention mechanism, which we refer to as complex attention, is the same as the dot mechanism in [2]. This mechanism is shown in figure 2. The attention weight for the input word at position t, computed when outputting the t\u2032-th word is:\nayt\u2032 (t) = exp(hTxthyt\u2032 )\u2211T t\u0304 exp(h T xt\u0304 hyt\u2032 )\nwhere hxt represents the last hidden layer generated after processing the t-th input word, and hyt\u2032 represents the last hidden layer from the current step of decoding. Note one of the characteristics\nof this mechanism is that the same hidden units are used for computing the attention weight as for computing the context.\nThe second attention mechanism, which we refer to as simple attention, is a slight variation of the complex mechanism that makes it easier to analyze how the neural network learns to compute the attention weights. This mechanism is shown in figure 3. Here, the hidden units of the last layer generated after processing each of the input words are split into 2 sets: one set of size 50 used for computing the attention weight, and the other of size 550 used for computing the context. Analogously, the hidden units of the last layer from the current step of decoding are split into 2 sets: one set of size 50 used for computing the attention weight, and the other of size 550 fed into the softmax layer. Aside from these changes the formula for computing the attention weights, given the correponding hidden units, and the formula for computing the context are kept the same."}, {"heading": "3 Dataset", "text": ""}, {"heading": "3.1 Overview", "text": "The model is trained using the English Gigaword dataset, as available from the Stanford Linguistics department. This dataset consists of several years of news articles from 6 major news agencies, including the New York Times and the Associated Press. Each of the news articles has a clearly delineated headline and text, where the text is broken up into paragraphs. After the preprocessing described below the training data consists of 5.5M news articles with 236M words."}, {"heading": "3.2 Preprocessing", "text": "The headline and text are lowercased and tokenized, separating punctuation from words. Only the first paragraph of the text is kept. An end-of-sequence token is added to both the headline and the text. Articles that have no headline or text, or where the headline or text lengths exceed 25 and 50 tokens, respectively, are filtered out, for computational efficiency purposes. All rare words are replaced with the < unk > symbol, keeping only the 40,000 most frequently occuring words.\nThe data is split into a training and a holdout set. The holdout set consists of articles from the last month of data, with the second last month not included in either the training or holdout sets. This split helps ensure that no nearly duplicate articles make it into both the training and holdout sets.\nFinally, the training data is randomly shuffled."}, {"heading": "3.3 Dataset Issues", "text": "The dataset as used has a number of issues. There are many training examples where the headline does not in fact summarize the text very well or at all. These include many articles that are formatted incorrectly, having the actual headline in the text section and the headline section containing words such as \u201c(For use by New York Times News service clients)\u201d. There are many articles where the headline has some coded form, such as \u201cbiz-cover-1stld-writethru-nyt\u201c or \u201cbc-iraq-post 1stld-subpickup4thgraf\u201c.\nNo filtering of such articles was done. An ideal model should be able to handle such issues automatically, and attempts were made to do so using, for example, randomly feeding in generated words during training, as described in the Model section."}, {"heading": "4 Evaluation", "text": "The performance of the model was measured in two different ways. First, we looked at the training and holdout loss. Second, we used the BLEU [12] evaluation metric over the holdout set, defined next. For efficiency reasons, the holdout metrics were computed over only 384 examples.\nThe BLEU evaluation metric looks at what fraction of n-grams of different lengths from the expected headlines are actually output by the model. It also considers the number of words generated in comparison to the number of words used in the expected headlines. Both of these are computed over all 384 heldout example, instead of over each example separately. For the exact definition see [12]."}, {"heading": "5 Analysis", "text": "Each model takes 4.5 days to train on a GTX 980 Ti GPU. Figures 4 and 5 show the evaluation metrics as a function of training epoch. Note that in our setup the training loss is generally higher than holdout loss, since when computing the holdout loss we don\u2019t feed in generated words 10% of time.\nThe model is quite effective in predicting headlines from the same newspapers as it was trained on. Table 1 lists 7 examples chosen at random from the held-out examples. The model generally seems to capture the gist of the text and manages to paraphrase the text, sometimes using completely new words. However, it does make mistakes, for example, in sentences 2, 4 and 7.\nThe model has much more mixed performance when used to generate headlines for news articles from sources that are different from training. Table 2 shows generated headlines for articles from several major news websites. The model does quite well with articles from the BBC, the Wall Street Journal and the Guardian. However, it performs very poorly on articles from the Huffington Post and Forbes. In fact, the model performed poorly on almost all tested articles from Forbes. It seems that there is a major difference in how articles from Forbes are written, when compared to articles used to train the model."}, {"heading": "5.1 Understanding information stored in last layer of the neural network", "text": "We notice that there are multiple ways to go about understanding the function of the attention mechanism. Consider the formula for computing the input to the softmax function:\noyt\u2032 = Wcocyt\u2032 +Whohyt\u2032 + bo\nwhere cyt\u2032 is the context computed for the current step of decoding, hyt\u2032 is the last hidden layer from the current step of decoding, and Wco, Who and bo are model parameters. First, note that by looking at the word with the highest values for\nWhohyt\u2032 + bo\nwe can get an idea of what exactly the hidden layer from the current step of decoding is contributing to the final generated output. Analogously, by looking at the words with the highest values for\nWcocyt\u2032 + bo\nwe can do the same for the attention context. Moreover, since the context is just a weighted sum over the hidden layers of the decoder we can compute\nWcohxt + bo\nfor each of the input positions and get a good idea of what words would be recalled if the network paid attention to each of the input positions.\nAs an example, the last hidden layer generated after processing the \u201cand\u201d in the first example in table 1 is closest to the following words: 72, at, death, in, died, dead, to, <eos>, toll, people. We see many of the words that appeared before the \u201cand\u201d. It is interesting note that the words closest to the hidden layer for \u201cdied\u201d are: 72, at, to, in, <eos>, <unk>, for, as, \u201c,\u201d, more. Note that the word \u201cdied\u201d doesn\u2019t appear. This example demonstrates our observation that the network sometimes takes multiple steps to encode a particular word. That makes it slightly trickier to understand what the network is paying attention to.\nAs an example of what the hidden layer during decoding contains, the hidden layer during the first step of decoding for the same example as above is closest to the following words: urgent, (, at, one, two, three, <unk>, four, 1st, 1. After generating the \u201cat\u201d the hidden layer during the next step is closest to: least, :, -, killed, \u201c,\u201d, ld, lead, in, <unk>, to."}, {"heading": "5.2 Understanding how the attention weight vector is computed", "text": "We had an initial hypothesis for how the network learned to compute the attention weight vector. We hypothesized that the network remembered roughly which word should be generated next, and the dot product hTxthyt\u2032 would compute the similarity between the remembered word and the actual word at the position. For example, if the network remembered that the text talked about some\nnumber and was able to output a representation that was close to numbers in general, the attention mechanism would then allow it to get the exact number.\nOne implication of this hypothesis is that the units of the last hidden layer of the decoder would be in the same space as the units of the last hidden layer of the encoder. Thus, computing\nWcohyt\u2032 + bo\nand looking at the words with the highest values would tell us something meaningful. It turned out that this was not the case.\nFurther analysis into which units contributed to the attention weight vector computation revealed the existence of a few units which played key roles. That led us to simplify the neural network architecture as described in the Model section. Interestingly, as shown in figures 4 and 5, the simplified model performs significantly better. One possible explanation is that by separating the attention weight vector computation from the context computation we reduce the noise in both of these computations.\nThe simplified attention mechanism is also much easier to understand since only a small number of hidden units is used to compute the attention weight vector. In one of the smaller model that we trained we used only 20 units for the attention weight vector computation, and were able to figure out some of the functions of these 20 units. Table 3 catalogs the functions of the 20 neurons in the encoding part of the network. Each position in the input text has 20 neurons that serve these functions.\nThe neural network learns to spot many linguistic phenomena. Our analysis was also somewhat shallow due to time constraints. We suspect that some of the neurons activate for even more complex phenomena than we are able to find, such as different types of sentence structure.\nIt is important to note that the neurons in the encoding part of the network interact with the neurons in the decoding part of the network. Indeed, the neurons in the decoding part of the network activate\nat different times to test for different phenomena. For example, unit 12 starts off being positive so that the network pays attention to the object first. Then, unit 12 becomes negative so that the network pays attention to the subject. Interestingly, unit 9, which appears to work almost the same at unit 12 in the encoder, is usually 0 at the beginning of decoding, and only later on becomes activated."}, {"heading": "5.3 Errors", "text": "The network makes a lot of different types of errors. While we didn\u2019t do an in-depth error analysis, a few error still stood out.\nOne flaw with the neural network mechanism is its tendency to fill in details when details are missing. For example, after simplifying the text given in table 1, example 1 to \u201c72 people died when a truck plunged into a gorge on Friday.\u201d the model predicts \u201c72 killed in truck accident in Russia\u201d. The model makes up the fact that the accident happened in Russia. These errors happen most often when the number of decoding beams is small, since the model stops considering the decoding where the sentence ends early before outputing the made up details.\nWhat also happens occasionally is the network outputs some headline that is completely unrelated to the input text (e.g. \u201curgent\u201d, \u201cbc-times\u201d or even \u201ccan make individual purchases by calling 212 - 556 - 4204 or - 1927 .)\u201d). This problem is caused by the fact that such headlines occur somewhat often in the input dataset. These errors happen most often when the number of decoding beams is large, since that increases the probability of the model starting to generate one such high probability sequence.\nThese 2 examples demonstrate that our network is very sensitive to the number of decoding beams used. We used only 2 decoding beams for the BLEU evaluation. We suspect that if we fixed the second problem we could get much better results with a large number of beams. One solution worthy of investigation is to use the scheduled sampling mechanism described in [7]."}, {"heading": "6 Future Work", "text": "We demostrate above that the recurrent neural network learns to model complex linguistic phenomena, given large amounts of training data. However, such large amounts of training data are usually not available for real-world NLP problems. One interesting direction to pursue is using a dataset, like Gigaword, to pretrain a recurrent neural network that is then fine-tuned to solve a task such as part-of-speech tagging on a much smaller dataset.\nMore immediately, in addition to the already discussed idea to use scheduled sampling, another way to improve the model is to use a bi-directional RNN. We suspect that the attention mechanism would work better with a bi-directional RNN, since more information would be available to model some of the phenomena outlined in table 3. In our current model, the network must make a decision about which values to assign the neurons used to compute the attention weight for the current input word given only the current and previous words and not any of the following words. Giving the model information about the following words would make this decision easier for the network to make."}, {"heading": "7 Conclusion", "text": "We\u2019ve trained an encoder-decoder recurrent neural network with LSTM units and attention for generating news headlines using the texts of news articles from the Gigaword dataset. Using only the first 50 words of a news article, the model generates a concise summary of those 50 words, and most of the time the summary is valid and grammatically correct. The model doesn\u2019t perform quite as well on general text, demonstrating that a lot of the articles in Gigaword follow a particular form.\nWe study 2 different versions of the attention mechanism with the goal of understanding how the model decides which words of the input text to pay attention to when generating each output word. We introduce a simplified attention mechanism that uses a small set of neurons for computing the attention weights. This simplified mechanism makes it easier to study the function of the network. We find that the network learns to detect linguistic phenomena, such as verbs, objects and subjects of a verb, ends of noun phrases, names, prepositions, negations, and so on. Interestingly, we find that our simplified attention mechanism does better on the evaluation metrics."}, {"heading": "Acknowledgements", "text": "We\u2019d like to thank Thang Luong for his helpful suggestions and for sharing his machine translation code. We\u2019d like to thank Samy Bengio and Andrej Karpathy for suggesting a few interesting design alternatives. Finally, we\u2019d like to thank Chris Re\u0301. Some of the code written as part of doing research in his lab was used to complete this work."}], "references": [{"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "CoRR, abs/1409.3215,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "CoRR, abs/1508.04025,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "venue": "CoRR, abs/1303.5778,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "CoRR, abs/1506.03340,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "venue": "CoRR, abs/1509.00685,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Deep learning. Book in preparation for", "author": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": "CoRR, abs/1506.03099,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals"], "venue": "CoRR, abs/1508.01211,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "CoRR, abs/1409.2329,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Fei-Fei Li"], "venue": "CoRR, abs/1412.2306,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Examples of such applications include machine translation [1,2] and speech recognition [3].", "startOffset": 58, "endOffset": 63}, {"referenceID": 1, "context": "Examples of such applications include machine translation [1,2] and speech recognition [3].", "startOffset": 58, "endOffset": 63}, {"referenceID": 2, "context": "Examples of such applications include machine translation [1,2] and speech recognition [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "Recurrent neural networks have also been applied recently to reading comprehension [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "Our work is closely related to [5] who also use a neural network to generate news headlines using the same dataset as this work.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "We use the encoder-decoder architecture described in [1] and [2], and shown in figure 1.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "We use the encoder-decoder architecture described in [1] and [2], and shown in figure 1.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "Note that during training of the model it is necessary to use what is called \u201cteacher forcing\u201d [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "To overcome this disconnect, during training we randomly feed in a generated word, instead of the expected word, as suggested in [7].", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "Specifically, we do this 10% of the time, as also done in [8].", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "We use 4 hidden layers of LSTM units, specifically the variant described in [9].", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "We attempted using dropout as is also described in [9].", "startOffset": 51, "endOffset": 54}, {"referenceID": 9, "context": "We initialize the biases for each word in the softmax layer to the log-probability of its occurence in the training data, as suggested in [10].", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "01 along with the RMSProp [11] adaptive gradient method.", "startOffset": 26, "endOffset": 30}, {"referenceID": 1, "context": "The first attention mechanism, which we refer to as complex attention, is the same as the dot mechanism in [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 11, "context": "Second, we used the BLEU [12] evaluation metric over the holdout set, defined next.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "For the exact definition see [12].", "startOffset": 29, "endOffset": 33}, {"referenceID": 6, "context": "One solution worthy of investigation is to use the scheduled sampling mechanism described in [7].", "startOffset": 93, "endOffset": 96}], "year": 2015, "abstractText": "We describe an application of an encoder-decoder recurrent neural network with LSTM units and attention to generating headlines from the text of news articles. We find that the model is quite effective at concisely paraphrasing news articles. Furthermore, we study how the neural network decides which input words to pay attention to, and specifically we identify the function of the different neurons in a simplified attention mechanism. Interestingly, our simplified attention mechanism performs better that the more complex attention mechanism on a held out set of articles.", "creator": "LaTeX with hyperref package"}}}