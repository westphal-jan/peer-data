{"id": "1512.01818", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2015", "title": "SentiBench - a benchmark comparison of state-of-the-practice sentiment analysis methods", "abstract": "in nearly about 7 years thousands of human organizations have explored association analysis, several startups that use opinions on real data have emerged, and a number newly innovative products related to this theme have taken filed. foremost comes multiple methods for measuring sentiments, including lexical - scale approaches and supervised machine learning tests. despite the vast interest broad historical theme and wide popularity of synthetic methods, it is unclear sure method exists inappropriate for administering their label ( v. e., singular or negative ) defining a message. thus, its appeared consistently strong need between conduct assessing possible apple - to - apple fusion of sentiment processing methods, as computers are used in practice, involving multiple datasets made from different data sources. such a comparison is helpful for understanding the potential limitations, advantages, overall disadvantages of current systems. every study assisted at filling my gap from presenting a linear performance of first nine popular sentiment estimation methods ( those we call : state - upon - the - practice methods ). our evaluation protocols build upon a benchmark of twenty term datasets, covering messages written on social networks, movie and product reviews, as possibly as opinions / coverage in news articles. such assessments identifies differing extent unto which the prediction level of these methods varies generally across datasets. aiming that surveying the gross value relevance research area, we collect generic methods'codes and datasets used today this paper plus we deploy a benchmark revision, which houses an applicable method for accessing help compiling sentence - enriched sentiment analytic software.", "histories": [["v1", "Sun, 6 Dec 2015 18:52:51 GMT  (233kb,D)", "http://arxiv.org/abs/1512.01818v1", null], ["v2", "Wed, 16 Dec 2015 00:32:23 GMT  (279kb,D)", "http://arxiv.org/abs/1512.01818v2", null], ["v3", "Mon, 1 Feb 2016 18:54:51 GMT  (171kb)", "http://arxiv.org/abs/1512.01818v3", null], ["v4", "Sat, 4 Jun 2016 16:52:29 GMT  (185kb)", "http://arxiv.org/abs/1512.01818v4", null], ["v5", "Thu, 14 Jul 2016 22:51:39 GMT  (181kb,D)", "http://arxiv.org/abs/1512.01818v5", null]], "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["filipe nunes ribeiro", "matheus ara\\'ujo", "pollyanna gon\\c{c}alves", "fabr\\'icio benevenuto", "marcos", "r\\'e gon\\c{c}alves"], "accepted": false, "id": "1512.01818"}, "pdf": {"name": "1512.01818.pdf", "metadata": {"source": "CRF", "title": "A Benchmark Comparison of State-of-the-Practice Sentiment Analysis Methods", "authors": ["Pollyanna Gon\u00e7alves", "Matheus Ara\u00fajo", "Filipe Ribeiro", "Marcos Gon\u00e7alves"], "emails": [], "sections": [{"heading": null, "text": "39"}, {"heading": "A Benchmark Comparison of State-of-the-Practice Sentiment Analysis Methods", "text": "Pollyanna Gonc\u0327alves, Federal University of Minas Gerais Matheus Arau\u0301jo, Federal University of Minas Gerais Filipe Ribeiro, Federal University of Minas Gerais and Federal University of Ouro Preto Fabr\u0131\u0301cio Benevenuto, Federal University of Minas Gerais Marcos Gonc\u0327alves, Federal University of Minas Gerais\nIn the last few years thousands of scientific papers have explored sentiment analysis, several startups that measures opinions on real data have emerged, and a number of innovative products related to this theme have been developed. There are multiple methods for measuring sentiments, including lexical-based approaches and supervised machine learning methods. Despite the vast interest on the theme and wide popularity of some methods, it is unclear which method is better for identifying the polarity (i.e., positive or negative) of a message. Thus, there is a strong need to conduct a thorough apple-to-apple comparison of sentiment analysis methods, as they are used in practice, across multiple datasets originated from different data sources. Such a comparison is key for understanding the potential limitations, advantages, and disadvantages of popular methods. This study aims at filling this gap by presenting a benchmark comparison of twenty one popular sentiment analysis methods (which we call the state-of-the-practice methods). Our evaluation is based on a benchmark of twenty labeled datasets, covering messages posted on social networks, movie and product reviews, as well as opinions and comments in news articles. Our results highlight the extent to which the prediction performance of these methods varies widely across datasets. Aiming at boosting the development of this research area, we open the methods\u2019 codes and datasets used in this paper and we deploy a benchmark system, which provides an open API for accessing and comparing sentence-level sentiment analysis methods.\nCCS Concepts: rInformation systems \u2192 Sentiment analysis; rNetworks \u2192 Social media networks; Online social networks;\nAdditional Key Words and Phrases: Sentiment analysis, social media, online social networks, sentence-level"}, {"heading": "1. INTRODUCTION", "text": "Sentiment analysis has become an extremely popular tool, applied in several analytical domains, especially on the Web and social media. To illustrate the growth of interest in the field , Figure 1 shows the steady increase on the number of searches on the topic, according to Google Trends1, mainly after the popularization of the online social networks (OSNs). More than 7,000 articles have been written about sentiment analysis and various startups are developing tools and strategies to extract sentiments from text [Feldman 2013].\nThe number of possible applications of such a technique is also considerable. Many of them are focused on monitoring the reputation or opinion of a company or a brand with the analysis of reviews of consumer products or services [Hu and Liu 2004]. Sentiment analysis can also provide analytical perspectives for financial investors who want to discover and respond to market opinions [Oliveira et al. 2013; Bollen et al. 2010]. Another important set of applications is in politics, where marketing campaigns are interested in tracking sentiments expressed by voters associated with candidates [Tumasjan et al. 2010].\nDue to the enormous interest and applicability, there has been a corresponding increase in the number of proposed sentiment analysis methods in the last years. The proposed methods rely on many different techniques from different computer science fields. Some of them employ machine learning methods that often rely on supervised classification approaches, requiring labeled data to train classifiers [Pang et al. 2002]. Others are lexical-based methods that make use of predefined lists of words, in which each word is associated with a specific sentiment. The lexical methods vary according to the context in which they were created. For instance, LIWC [Tausczik and Pennebaker 2010] was originally proposed to analyze sentiment patterns in formally written English\n1https://www.google.com/trends/explore\\#q=sentiment\\%20analysis\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010.\nar X\niv :1\n51 2.\n01 81\n8v 1\n[ cs\n.C L\n] 6\nD ec\n2 01\n5\nsult consistent with the \u201cthere is no free lunch theorem\u201d [Wolpert and Macready 1997]. We also show that existing methods vary widely regarding their agreement, even across similar datasets (e.g. random tweets). This suggests that the same content could be interpreted very differently depending on the choice of a sentiment method. We noted that most methods are more accurate in correctly classifying positive than negative text, suggesting that current existing approaches tend to be biased in their analysis towards positivity. Finally, we quantify the relative prediction performance of existing efforts in the field across different types of datasets, identifying those with higher prediction performance across different datasets.\nBased on these observations, our final contribution consists on releasing our gold standard dataset and the codes of the compared methods2. We also created a Web system through which we allow other researchers to easily use our data and codes to compare results with the existing methods. More important, by using our system one could easily test which method would be the most suitable to a particular dataset and/or application. We hope that our tool will not only help researchers and practitioners for accessing and comparing a wide range of sentiment analysis techniques, but can also help towards the development of this research field as a whole.\nThe remainder of this paper is organized as follows. In Section 2, we briefly describe related efforts. Then, in Section 3 we describe the sentiment analysis methods we compare. Section 4 presents the gold standard data used for comparison. Section 5 summarizes our results and findings. Finally, Section 6 concludes the article and discusses directions for future work."}, {"heading": "2. BACKGROUND AND RELATED WORK", "text": "Next we discuss important definitions and justify the focus of our benchmark comparison. We also briefly survey existing related efforts that compare sentiment analysis methods."}, {"heading": "2.1. Focus on Sentiment Level", "text": "Since sentiment analysis can be applied to different tasks, we restrict our focus on comparing those efforts related to detect the polarity (i.e. positivity or negativity) of a given short text (i.e. sentencelevel). Polarity detection is a common function across all sentiment methods considered in our work, providing valuable information to a number of different applications, specially those that explore short messages that are commonly available in social media [Feldman 2013].\nSentence-level sentiment analysis can be performed with supervision (i.e. requiring labeled training data) or not. An advantage of supervised methods is at their ability to adapt and create trained models for specific purposes and contexts. A drawback is the need of labeled data, which might be highly costly and even prohibitive for some tasks. On the other hand, the lexical-based methods make use of a pre-defined list of words, where each word is associated with a specific sentiment. The lexical methods vary according to the context in which they were created. For instance, LIWC [Tausczik and Pennebaker 2010] was originally proposed to analyze sentiment patterns in English texts, whereas PANAS-t [Gonc\u0327alves et al. 2013b] and POMS-ex [Bollen et al. 2009] are psychometric scales adapted to the Web context. Although lexical-based methods do not rely on labeled data, it is hard to create a unique lexical-based dictionary to be used for different contexts.\nWe focus our effort on evaluating unsupervised efforts as they can be easily deployed in Web services and applications without the need of human labeling or any other type of manual intervention. As described in Section 3, some of the methods we consider have used machine learning to build lexicon dictionaries or even to build models and tune specific parameters. We incorporate those methods in our study, since they have been released as black-box tools that can be used in an unsupervised manner."}, {"heading": "2.2. Existing Efforts on Methods\u2019 Comparison", "text": "Despite the large number of existing methods, only a limited of them have performed a comparison among sentiment analysis methods, usually with limited datasets. Overall, lexical methods and ma-\n2Except for one paid method\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010.\nchine learning approaches have been evolving in parallel in the last years, and it comes as no surprise that studies have started to compare their performance on specific datasets and use one or another strategy as baseline for comparison. A recent survey summarizes several of these efforts [Tsytsarau and Palpanas 2012] and conclude that a systematic comparative study that implements and evaluates all relevant algorithms under the same framework is still missing in the literature. As new methods emerge and compare themselves only against one or other method using different evaluation datasets and testing methodologies, it is hard to conclude if a single method triumphs over the other methods, even under specific scenarios. To the best of our knowledge, our effort is the first of kind to create a benchmark and provide such a comparison.\nAnother important worth noticing effort consists of an annual workshop namely International Workshop on Semantic Evaluation (SemEval). It consists of a series of exercises grouped in tracks, that include sentiment analysis, text similarity, among others, that put together competitors. Some new methods such as Umigon [Levallois 2013] have been proposed after obtaining good results on part of these tracks. Although, SemEval has been playing an important role for identifying the current important methods, it requires authors of the methods to register for the challenge and many popular methods have been evaluated in these exercises. Additionally, SemEval labeled datasets are usually focused on one specific types of data, such as tweets, and do not represent a wide range of social media data. In our evaluation effort, we consider one dataset from Semeval 2013 and two methods that participated in the competition in that same year.\nFinally, in a previous effort [Gonc\u0327alves et al. 2013a], we compared eight sentence-level sentiment analysis methods, based on one public dataset used to evaluate the method sentistrength [Thelwall 2013]. Our effort largely extents our previous work by comparing much more methods across many different datasets, providing a much deeper benchmark evaluation of current existing popular sentiment analysis methods. The methods used in this paper were also incorporated as part of an existing system, namely ifeel [Araujo et al. 2014]."}, {"heading": "3. SENTIMENT ANALYSIS METHODS", "text": "This section provides a brief description of the twenty one sentence-level sentiment analysis methods investigated in this paper.\nOur effort to identify important sentence-level sentiment analysis methods consisted of systematically search for them in the main conferences in the field and then checking for papers that cited them as well as their own references. Some of the methods are available for download on the Web; others were kindly shared by their authors under request; and a small part of them were implemented by us based on their descriptions in the original paper. This usually happened when authors shared only the lexical dictionaries they created, letting the implementation of the method that use the lexical resource to ourselves.\nTable I and Table II present an overview of these methods, providing a description of each method as well as the techniques they employ (L for Lexicon Dictionary and ML for Machine Learning), their outputs (e.g. -1,0,1, meaning negative, neutral, and positive, respectively), the datasets they used to validate, the baseline methods used for comparison and finally lexicon details. The methods are organized in chronological order to allow a better overview of the existing efforts over the years. We can note that the methods generate different outputs formats. We colored in blue the positive outputs, in black the neutral ones, and in red those that are negative.\nSince we are comparing sentiment analysis methods on a sentence-level basis, we need to work with mechanisms that are able to receive sentences as input and give polarities as output. Some of the approaches considered in this paper, shown in Table II, are complex dictionaries built with great effort. However, a lexicon alone has no natural ability to infer polarity in sentence level tasks. The purpose of a lexicon goes beyond the detection of polarity of a sentence [Feldman 2013; Liu 2012], but it can also be used to that end [Godbole et al. 2007; Kouloumpis et al. 2011].\nSeveral existing sentence-level sentiment analysis methods like Vader [Hutto and Gilbert 2014] and SO-CAL [Taboada et al. 2011], combine a Lexicon and the processing of the sentence char-\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010.\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010.\nacteristics to determine a sentence polarity. These approaches make use of a series of intensifiers, punctuation transformation, emoticons, and many other heuristics.\nThus, to evaluate each lexicon dictionaries as the base for a sentence-level sentiment analysis method, we considered the Vader\u2019s implementation. In other words, we used Vader\u2019s code for determining if a sentence is positive or not considering different lexicons as dictionaries.\nVader\u2019s heuristics were proposed by means of qualitative analyses of textual properties and characteristics which affect the perceived sentiment intensity of the text. Vader\u2019s author identified five heuristics based on grammatical and syntactical cues to convey changes to sentiment intensity that go beyond the bag-of-words model. The heuristics include treatments for: 1) punctuation (e.g number of \u2018!\u2019s); 2) capitalization (e.g \u201dI HATE YOU\u201d is more intense than \u201di hate you\u201d); 3) degree modifiers (e.g \u201dThe service here is extremely good\u201d is more intense than \u201dThe service here is good\u201d); 4) constructive conjunction \u201dbut\u201d to shift the polarity; 5) Tri-gram examination to identify negation (e.g \u201dThe food here isnt really all that great.\u201d). We choose Vader as it is the newest method among those we considered, it is becoming widely used as it was even implemented as part of the well known NLTK python library5.\nWe applied such heuristics with the following lexicons: Emolex, EmoticonsDS, NRC Hashtag, Opinion Lexicon, Panas, Sentiment 140, SentiWordNet. We notice that this strategy drastically improved results of lexicon for sentence-level sentimente analysis in comparison with a simple baseline approach that averages the occurrence of positive and negative words to classify the polarity of a sentence. Table II has also a column Lexicon that describes the number of terms the proposed dictionary contains and column C (changed) indicates some methods we slightly modified to adequate their output formats to the polarity detection task.\nSome other methods required similar adaptations. Methods that are based on machine learning, like SASA and SentiStrength, are used here as unsupervised approaches as their trained models were released by the authors and they have been used in other efforts as tools that require no training data.\nWe plan to release all the codes used in this article, except for paid softwares like LIWC and SentiStrength, as an attempt to allow reproducibility as well as possible corrections in our decisions. There are a few other methods for sentiment detection proposed in the literature and not considered here. Most of them consist of variations of the techniques used by the above methods, such as WordNet-Affect[Valitutti 2004] and ANEW [Bradley and Lang 1999] (the same used by Happiness Index, SentiWordNet, SenticNet, etc.). Finally, there exist a few other methods which are not available on the Web or request and that could not be re-implemented based on their descriptions in the original papers (e.g, Profile of Mood States (POMS) [Bollen et al. 2009]).\nFrom Table II we can also note that the validation strategy, the datasets used, and the baseline comparison of these methods varies greatly, from toy examples to large labeled datasets. Panas-t and Happiness Index use labeled examples to validate their methods, by presenting evaluations of events in which some bias towards positivity and negativity would be expected. Panas-t is tested with unlabeled twitter data related to Michael Jackson\u2019s death and the release of a Harry Potter movie whereas Happiness Index was used to measure song lyrics happiness from 1967 to 2007. Lexical dictionaries were validated in very different ways. AFINN[Nielsen 2011] compared its Lexicon with other dictionaries. Emoticon Distance Supervised [Hannak et al. 2012] used Pearson Correlation between human labeling and the predicted value. SentiWordNet [Esuli and Sebastiani 2006] validates the proposed dictionary with comparisons with other dictionaries, but it also used human validation of the proposed lexicon. These efforts attempt to validate the lexicon created, without comparing the lexicon as a sentiment analysis method itself. Vader [Hutto and Gilbert 2014] compared results with lexical approaches considering labeled datasets from different social media data. SenticNet [Cambria et al. 2010] was compared with SentiStrength [Thelwall 2013] with a specific dataset related to patient opinions, which could not be made available. Stanford Recursive Deep Model [Socher et al. 2013] and SentiStrength [Thelwall 2013] were both compared with standard machine learning approaches, with their own datasets.\n5http://www.nltk.org/ modules/nltk/sentiment/vader.html\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010."}, {"heading": "4. GOLD STANDARD DATA", "text": "sentence has an agreed-upon polarity assigned to it. The number of annotators used to build the datasets is also shown in Table III.\nTweets DBT was the unique dataset built with a combination of AMT Labeling with Expert validation. They selected 200 random tweets to be classified by experts and compared with AMT results to ensure accurate ratings. We note that the Tweets Semeval dataset was provided as a list of Twitter IDs, due to the Twitter policies related to data sharing. While crawling the respective tweets, a a small part of them could not be accessed, as they were deleted. In any case, we plan to release all gold standard datasets in a request basis, which is in agreement with Twitter policies.\nIn order to assess the extent to which these datasets are trustful, we used a strategy similar to the one used by Tweets DBT. Our goal was not to redo all the performed human evaluation, but simply inspecting a small sample of them to infer the level of agreement with our own evaluation. We randomly select 1% of all sentences to be evaluated by experts (two of the authors) as an attempt to assess if these gold standard data are really trustful. It is important to mention that we do not have access to the instructions provided by the authors. We also could not get access to small amount of the raw data in a few datasets, which was discarded. Finally, our manual inspection unveiled a few sentences in idioms other than English in a few datasets, such as Tweets STA and TED, which were obviously discarded.\nColumn R from Table III exhibits the level of agreement of each dataset in our evaluation. After a close look in the cases we disagree with the evaluations in the Gold standard, we understand that other interpretations could be given to the text, finding cases of sentences with mixed polarity. Some of then are strongly linked to context and very hard to evaluate. Some NYT comments, for instance, are directly related to the news they were inserted to. We can also note that some of the datasets do not contain neutral messages. This might be a characteristic of the data or even a result of how annotators were instructed to label their pieces of text. Most of the cases of disagreement involve neutral messages. Thus, we considered these cases as well as the amount of disagreement we had with the gold standard data as reasonable and expected."}, {"heading": "5. COMPARISON RESULTS", "text": "Next, we present comparison results for the twenty one methods considered in this paper based on the twenty considered gold standard datasets."}, {"heading": "5.1. Experimental details", "text": "At least three distinct approaches have been proposed to deal with sentiment analysis of sentences. The first of them, splits this task into two steps: (i) identifying sentences with no sentiment, also named as objective vs. neutral sentences and then (ii) detecting polarity (positive or negative), only for the subjective sentences. Another common way to detect sentence polarity is considers in a single task three distinct classes (positive, negative and neutral). Finally, some methods classify a sentence as positive or negative only, assuming that only polarized sentences are present, given the context of a given application. As example, review of products are expected to contain only polarized opinion.\nAiming at providing a more thorough comparison among these distinct approaches, we perform two rounds of tests. In the first we consider the performance of methods to identify 3-classes (positive, negative and neutral). The second considers only positive and negative as output and assumes that a first step of removing the neutral messages was already performed. In the 3-classes experiments we used only datasets containing a considerable number of neutral messages (which excludes Tweets RND II, Amazon and Reviews II that contain an insignificant number of neutral sentences). Despite being 2-classes methods, as highlighted in Table IV, we decided to include LIWC, Emoticons and Senticnet in the 3-classes experiments to present a full set of comparative experiments. LIWC, Emoticons and Senticnet cannot define, for some sentences, their positive or negative polarity, consering it as undefined. It occurs due to the absence in the sentence of emoticons (in the case of Emoticons method) or of twords beloging to the methods\u2019 sentiment lexicon. As a neutral (objective) sentence is one that express no sentiment at all about a topic, we assumed, in the case of these 2-class methods, undefined polarities as being equivalent to neutral ones.\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010.\nThe 2-classes experiments, in turn, were performed with all datasets described in Table III without the neutral sentences. We also included all methods in these experiments, even those that produce neutral outputs. As discussed before, when 2-classes methods cannot detect the polarity (positive or negative) of a sentences they usually assign it to an undefined polarity. As we know all sentences in the 2-classes experiments are positive or negative we create the coverage metric to determine the percentage of sentences a method can in fact classify as positive or negative. For instance, suppose that Emoticons\u2019 method can classify only 10% of the sentences in a dataset, corresponsing to the actual percentage of sentences with emoticons. It means that the coverage of this method in this specific dataset is 10%. Note that, the coverage is quite an important metric for a more complete evaluation in the 2-classes experiments. Even though Emoticons presents high accuracy for the classified phrases it was not able to make a prediction for 90% of the sentences. More formally, coverage is calculated as the number of total sentences minus the number of undefined sentences, all of this divided by the total of sentences, where the number of undefined sentences includes neutral outputs for 3-classes methods.\nCoverage = #Sentences\u2212#Undefined\n#Sentences"}, {"heading": "5.2. Comparison Metrics", "text": "Considering the 3-classes comparison experiments, we used the traditional Precision, Recall and F1 measures for the automated classification.\nPredicted Positive Neutral Negative\nPositive a b c Actual Neutral d e f\nNegative g h i\nEach letter in the above table represents the number of instances which are actually in class X and predicted in class Y, where X;Y \u2208 positive; neutral; negative. The recall (R) of a class X is the ratio of the number of elements correctly classified as X to the number of known elements in class X . Precision (P) of a class X is the ratio of the number of elements classified correctly as X to the total predicted as the class X . For example, the precision of the negative class is computed as: P (neg) = i/(c+f + i); its recall, as:R(neg) = i/(g+h+ i); and the F1 measure is the harmonic mean between both precision and recall. In this case, F1(neg) = 2P (neg)\u00b7R(neg)P (neg)+R(neg) .\nWe also compute the overall accuracy as: A = a+e+ia+b+c+d+e+f+g+h+i . It considers equally important the correct classification of each sentence, independently of the class, and basically measures the capability of the method to predict the correct output. A variation of F1, namely, macro-F1, is normally reported to evaluate classification effectiveness on skewed datasets. Macro-F1 values are computed by first calculating F1 values for each class in isolation, as exemplified above for negative, and then averaging over all classes. Macro-F1 considers equally important the effectiveness in each class, independently of the relative size of the class. Thus, accuracy and Macro-F1 provide complementary assessments of the classification effectiveness. Macro-F1 is especially important when the class distribution is very skewed, to verify the capability of the method to perform well in the smaller classes.\nThe described metrics can easily computed for the 2-classes experiments by just removing neutral columns and rows as per below. In this case, the precision of positive class is computed as: P (pos) = a/(a+ c); its recall as: R(pos) = a/(a+ b); while its F1 is F1(pos) = 2P (pos)\u00b7R(pos)P (pos)+R(pos)\nAs we have a large number of combination among base methods, metrics and datasets, a global analysis of the performance of all these combinations is not an easy task. We propose a simple but\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010.\nPredicted Positive Negative\nPositive a b Actual Negative c d\ninformative measure to assess the overall performance ranking. The Mean Ranking is basically the sum of ranks obtained by a method in each dataset divided by the total number of datasets, as per below:\nMR =\nnd\u2211 j=1 ri\nnd\nwhere nd is the number of datasets and ri is the rank of the method for dataset i.It is important to notice that rank was calculated based on Macro F1.\nThe last evaluation metric we exploit is the Friedman\u2019s Test [Berenson et al. 2014]. It allows to verify whether, in a specific experiment, the observed values are globally similar. In other words, are the methods presenting similar performance across different datasets? To exemplify the application of this test, suppose that n restaurants are each rated by k judges. The question that arises is: are the judges ratings consistent with each other or are they following completely different patterns? The application in our context is very similar: the datasets as the restaurants and the macro-F1 achieved by a method is the rating from the judges.\nThe Friedman\u2019s Test is applied to rankings. Then, to proceed with this statistical test, we sort the methods for each dataset using for comparison the macro-F1 metric. In other words, the method with highest macro-F1 received rank \u20181\u2019 while the slowest macro-F1 method was ranked as \u201821\u2019 for each dataset.\nMore formally, the Friedman\u2019s rank test is defined as:\nFR = ( 12\nrc(c+ 1) c\u2211 j=1 R2j )\u2212 3r(c+ 1)\nwhere R2j = square of the total of the ranks for group j (j = 1,2,..., c) r = number of blocks c = number of groups\nIn our case, the number of blockes corresponds to the number of datasets and the number of the number of groups is the number of methods evaluated. As the number of blocks increases, the statistical test can be aproximated by using the chi-square distribution with c\u22121 degrees of freedom. Then, if the FR computed value is greater than the critical value for the chi-square distribution the null hypothesis is rejected. This null hypothesis states that ranks obtained by judges are globally similar, then rejecting the null hypothesis means that there are significant differences in the judgment ranks (datasets). It is important to note that, in general, the critical value is obtained with significance level \u03b1 = 0.05 and. Synthesizing, the null hypothesis should be rejected if FR > X2\u03b1, where X 2 \u03b1 is the critical value verified in the chi-square distribution table with c \u2212 1 degrees of freedom and \u03b1 equals 0.05."}, {"heading": "5.3. Comparing Prediction Performance", "text": "We start the analysis of our experiments by comparing the results of all metrics previously discussed for all datasets. Table V and Table IV presents accuracy, precision and macro-F1 for all methods in 4 datasets for the 3-classes experiments and 2-classes experiments respectively. For simplicity, results fot the other 16 datasets are presented in the appendix.\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010.\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010.\nassociated to words, hashtags, and any sort of tokens according to the frequency with which these tokens appear in tweets containing positive and negative emoticons. This method showed to be biased towards positivity due to the larger amount of positivity in the data they used to build the lexicon. The overall poor performance of this specific method is credited to its lack of treatment of neutral messages and the focus on Twitter messages.\nAs it can been seen in Table VI, the top seven methods based on Macro-F1 are SentiStrength, Semantria, AFINN, OpinionLexicon, Umigon, Vader and SO-CAL. This means that these methods produce good results across several datasets in both, 2 and 3-class tasks. These methods would be preferable in situations in which any sort of preliminary evaluation would be performed. We also note those methods usually perform better in the datasets in which they were originally validated, which is somewhat expected due to fine tuning procedures. This is specially true for SentiStrength and VADER. To understand the impact of such factor, we calculated Mean Rank for these methods without their \u2018original\u2019 datasets and put(results in parenthesis). Note that in some cases the rank order changes towards a lower value.\nTable VII presents Friedman\u2019s test results and, as expected, we can conclude that there are significant differences in the mean ratings observed for the methods across all datasets. It statistically indicates that in terms of accuracy and Macro-F1 there is no single method that always achieves the best prediction performance for different datasets, which is similar to the well-known \u201cno-free lunch theorem\u201d [Wolpert and Macready 1997]. This suggests that at least a preliminary investigation should be performed when sentiment analysis is used in a new dataset in order to guarantee a reasonable prediction performance.\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010.\nIn order to verify whether this behavior also occurs in specific contexts such as tweets or comments we divided all datasets in three contexts and perform the Friedman\u2019s test for each one. The contexts are Social Networks, Comments and Reviews and the datasets were grouped as presented below. In spite of being sentences extracted from forums we defined the RW dataset as belonging to the Comments context as the properties of sentences in this dataset are similar to those of other Comments datasets, as seen in Table (table III).\nContext Groups\nSocial Networks Myspace, Tweets DBT, Tweets DBT, Tweets RND I, Tweets RND II,\nTweets RND III, Tweets RND IV, Tweets STF, Tweets SAN, Tweets Semeval\nComments Comments BBC, Comments DIGG, Comments NYT,Comments TED, Comments YTB, RW Reviews Reviews I, Reviews I, Amazon\nEven after grouping the datasets in such contexts, we still find out that there are significant differences in the observed ranks across the datasets. Although the values obtained for each context were quite smaller than Friedman\u2019 global value, they are still above the critical value. Table VIII presents the results of Friedman\u2019s test for the individual contexts in both experiments, 2 and 3-class. Recall that for the 3-class experiments, datasets with no neutral sentences or with an unrepresentative number of neutral sentences were not removed. For this reason, the results for 3-class experiments in the Reviews context has no values as none of the Review datasets has a significant number of neutral sentences."}, {"heading": "6. CONCLUDING REMARKS", "text": "To perform such comparison we have made significant efforts to obtain the latest working versions of the various sentiment analysis tools and datasets, which we put together in a single webpage 6. We are releasing this Web system so that other researchers can easily compare results of those methods in their own datasets. With this system one could easily test which method would be most suitable for a particular dataset and application. We hope that our tool will not only help researchers and practitioners for accessing and comparing a wide range of sentiment analysis techniques. We also hope that this can help the development of new research in this area.\nAPPENDIX In this appendix, we present the full results of prediction performance of all twenty one sentiment analysis methods on all labeled datasets.\n6http://homepages.dcc.ufmg.br/\u223cfabricio/benchmark\\ sentiment\\ analysis.html\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010.\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010.\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010.\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010.\nACM Transactions on Embedded Computing Systems, Vol. 9, No. 4, Article 39, Publication date: March 2010."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by grants from CAPES, Fapemig, and CNPq."}], "references": [{"title": "Manually Annotated Sentiment Analysis Twitter Dataset NTUA", "author": ["Fotis Aisopos."], "venue": "(2014). www.grid.ece.ntua.gr.", "citeRegEx": "Aisopos.,? 2014", "shortCiteRegEx": "Aisopos.", "year": 2014}, {"title": "iFeel: A System that Compares and Combines Sentiment Analysis Methods", "author": ["Matheus Araujo", "Pollyanna Gon\u00e7alves", "Fabrcio Benevenuto", "Meeyoung Cha."], "venue": "WWW (Companion Volume). International World Wide Web Conference (WWW\u201914), 4.", "citeRegEx": "Araujo et al\\.,? 2014", "shortCiteRegEx": "Araujo et al\\.", "year": 2014}, {"title": "SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining", "author": ["Stefano Baccianella", "Andrea Esuli", "Fabrizio Sebastiani"], "venue": "Tapias (Eds.). European Language Resources Association", "citeRegEx": "Baccianella et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baccianella et al\\.", "year": 2010}, {"title": "Twitter mood maps reveal emotional states of America", "author": ["Celeste Biever."], "venue": "The New Scientist 207 (2010). Issue 2771.", "citeRegEx": "Biever.,? 2010", "shortCiteRegEx": "Biever.", "year": 2010}, {"title": "Twitter Mood Predicts the Stock Market", "author": ["Johan Bollen", "Huina Mao", "Xiao-Jun Zeng."], "venue": "CoRR abs/1010.3003 (2010).", "citeRegEx": "Bollen et al\\.,? 2010", "shortCiteRegEx": "Bollen et al\\.", "year": 2010}, {"title": "Modeling Public Mood and Emotion: Twitter Sentiment and SocioEconomic Phenomena", "author": ["Johan Bollen", "Alberto Pepe", "Huina Mao."], "venue": "CoRR abs/0911.1583 (2009).", "citeRegEx": "Bollen et al\\.,? 2009", "shortCiteRegEx": "Bollen et al\\.", "year": 2009}, {"title": "Affective norms for English words (ANEW): Stimuli, instruction manual, and affective ratings", "author": ["M.M. Bradley", "P.J. Lang."], "venue": "Technical Report. Center for Research in Psychophysiology, University of Florida, Gainesville, Florida.", "citeRegEx": "Bradley and Lang.,? 1999", "shortCiteRegEx": "Bradley and Lang.", "year": 1999}, {"title": "SenticNet: A Publicly Available Semantic Resource for Opinion Mining", "author": ["Erik Cambria", "Robert Speer", "Catherine Havasi", "Amir Hussain."], "venue": "AAAI Fall Symposium Series.", "citeRegEx": "Cambria et al\\.,? 2010", "shortCiteRegEx": "Cambria et al\\.", "year": 2010}, {"title": "Measuring User Influence in Twitter: The Million Follower Fallacy", "author": ["Meeyoung Cha", "Hamed Haddadi", "Fabricio Benevenuto", "Krishna P. Gummadi."], "venue": "International AAAI Conference on Weblogs and Social Media (ICWSM).", "citeRegEx": "Cha et al\\.,? 2010", "shortCiteRegEx": "Cha et al\\.", "year": 2010}, {"title": "Pattern for python", "author": ["Tom De Smedt", "Walter Daelemans."], "venue": "The Journal of Machine Learning Research 13, 1 (2012), 2063\u20132067.", "citeRegEx": "Smedt and Daelemans.,? 2012", "shortCiteRegEx": "Smedt and Daelemans.", "year": 2012}, {"title": "Characterizing debate performance via aggregated twitter sentiment", "author": ["N.A. Diakopoulos", "D.A. Shamma."], "venue": "Proceedings of the 28th international conference on Human factors in computing systems. ACM, 1195\u20131198.", "citeRegEx": "Diakopoulos and Shamma.,? 2010", "shortCiteRegEx": "Diakopoulos and Shamma.", "year": 2010}, {"title": "Human language reveals a universal positivity bias", "author": ["Peter Sheridan Dodds", "Eric M. Clark", "Suma Desu", "Morgan R. Frank", "Andrew J. Reagan", "Jake Ryland Williams", "Lewis Mitchell", "Kameron Decker Harris", "Isabel M. Kloumann", "James P. Bagrow", "Karine Megerdoomian", "Matthew T. McMahon", "Brian F. Tivnan", "Christopher M. Danforth."], "venue": "Proceedings of the National Academy of Sciences 112, 8 (2015), 2389\u20132394. DOI:http://dx.doi.org/10.1073/pnas.1411678112", "citeRegEx": "Dodds et al\\.,? 2015", "shortCiteRegEx": "Dodds et al\\.", "year": 2015}, {"title": "Measuring the happiness of large-scale written expression: songs, blogs, and presidents", "author": ["Peter Sheridan Dodds", "Christopher M Danforth."], "venue": "Journal of Happiness Studies 11, 4 (2009), 441\u2013456. DOI:http://dx.doi.org/10.1007/s10902-009-9150-9", "citeRegEx": "Dodds and Danforth.,? 2009", "shortCiteRegEx": "Dodds and Danforth.", "year": 2009}, {"title": "SentiWordNet: A Publicly Available Lexical Resource for Opinion Mining", "author": ["Esuli", "Sebastiani."], "venue": "International Conference on Language Resources and Evaluation (LREC). 417\u2013422.", "citeRegEx": "Esuli and Sebastiani.,? 2006", "shortCiteRegEx": "Esuli and Sebastiani.", "year": 2006}, {"title": "Techniques and Applications for Sentiment Analysis", "author": ["Ronen Feldman."], "venue": "Commun. ACM 56, 4 (April 2013), 82\u201389. DOI:http://dx.doi.org/10.1145/2436256.2436274", "citeRegEx": "Feldman.,? 2013", "shortCiteRegEx": "Feldman.", "year": 2013}, {"title": "Twitter Sentiment Classification using Distant Supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang."], "venue": "Processing (2009), 1\u20136.", "citeRegEx": "Go et al\\.,? 2009", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Large-Scale Sentiment Analysis for News and Blogs", "author": ["Namrata Godbole", "Manjunath Srinivasaiah", "Steven Skiena."], "venue": "Proceedings of the International Conference on Weblogs and Social Media (ICWSM).", "citeRegEx": "Godbole et al\\.,? 2007", "shortCiteRegEx": "Godbole et al\\.", "year": 2007}, {"title": "Comparing and Combining Sentiment Analysis Methods", "author": ["Pollyanna Gon\u00e7alves", "Matheus Araujo", "Fabrcio Benevenuto", "Meeyoung Cha."], "venue": "Proceedings of the 1st ACM Conference on Online Social Networks (COSN\u201913). 12.", "citeRegEx": "Gon\u00e7alves et al\\.,? 2013a", "shortCiteRegEx": "Gon\u00e7alves et al\\.", "year": 2013}, {"title": "PANAS-t: A Pychometric Scale for Measuring Sentiments on Twitter", "author": ["Pollyanna Gon\u00e7alves", "Fabr\u0131\u0301cio Benevenuto", "Meeyoung Cha"], "venue": null, "citeRegEx": "Gon\u00e7alves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gon\u00e7alves et al\\.", "year": 2013}, {"title": "Tweetin\u2019 in the Rain: Exploring societal-scale effects of weather on mood", "author": ["Aniko Hannak", "Eric Anderson", "Lisa Feldman Barrett", "Sune Lehmann", "Alan Mislove", "Mirek Riedewald."], "venue": "Int\u2019l AAAI Conference on Weblogs and Social Media (ICWSM).", "citeRegEx": "Hannak et al\\.,? 2012", "shortCiteRegEx": "Hannak et al\\.", "year": 2012}, {"title": "Mining and summarizing customer reviews (KDD \u201904)", "author": ["Minqing Hu", "Bing Liu."], "venue": "168\u2013177. http://doi.acm.org/10. 1145/1014052.1014073", "citeRegEx": "Hu and Liu.,? 2004", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "Vader: A parsimonious rule-based model for sentiment analysis of social media text", "author": ["CJ Hutto", "Eric Gilbert."], "venue": "Eighth International AAAI Conference on Weblogs and Social Media (ICWSM).", "citeRegEx": "Hutto and Gilbert.,? 2014", "shortCiteRegEx": "Hutto and Gilbert.", "year": 2014}, {"title": "Twitter Sentiment Analysis: The Good the Bad and the OMG", "author": ["Efthymios Kouloumpis", "Theresa Wilson", "Johanna Moore."], "venue": "Int\u2019l AAAI Conference on Weblogs and Social Media (ICWSM).", "citeRegEx": "Kouloumpis et al\\.,? 2011", "shortCiteRegEx": "Kouloumpis et al\\.", "year": 2011}, {"title": "Experimental evidence of massive-scale emotional contagion through social networks", "author": ["Adam D I Kramer", "Jamie E Guillory", "Jeffrey T Hancock."], "venue": "Proceedings of the National Academy of Sciences of the United States of America 111, 24 (June 2014), 8788\u201390. DOI:http://dx.doi.org/10.1073/pnas.1320040111", "citeRegEx": "Kramer et al\\.,? 2014", "shortCiteRegEx": "Kramer et al\\.", "year": 2014}, {"title": "Umigon: sentiment analysis for tweets based on terms lists and heuristics", "author": ["Clement Levallois."], "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013). Association for Computational Linguistics, Atlanta, Georgia, USA, 414\u2013417. http://www.aclweb.org/anthology/S13-2068", "citeRegEx": "Levallois.,? 2013", "shortCiteRegEx": "Levallois.", "year": 2013}, {"title": "Sentiment Extraction - Measuring the Emotional Tone of Content", "author": ["Lexalytics."], "venue": "Technical Report. Lexalytics.", "citeRegEx": "Lexalytics.,? 2015", "shortCiteRegEx": "Lexalytics.", "year": 2015}, {"title": "Sentiment Analysis and Opinion Mining", "author": ["Bing Liu."], "venue": "Synthesis Lectures on Human Language Technologies 5, 1 (May 2012), 1\u2013167. DOI:http://dx.doi.org/10.2200/s00416ed1v01y201204hlt016", "citeRegEx": "Liu.,? 2012", "shortCiteRegEx": "Liu.", "year": 2012}, {"title": "WordNet: a lexical database for English", "author": ["George A. Miller."], "venue": "Commun. ACM 38, 11 (1995), 39\u201341.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Emotional Tweets", "author": ["Saif Mohammad."], "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012). Association for Computational Linguistics, Montr\u00e9al, Canada, 246\u2013255. http://www.aclweb.org/anthology/S12-1033", "citeRegEx": "Mohammad.,? 2012", "shortCiteRegEx": "Mohammad.", "year": 2012}, {"title": "Generating High-coverage Semantic Orientation Lexicons from Overtly Marked Words and a Thesaurus", "author": ["Saif Mohammad", "Cody Dunne", "Bonnie Dorr."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2 (EMNLP \u201909). Association for Computational Linguistics, Stroudsburg, PA, USA, 599\u2013608. http://dl.acm.org/citation.cfm?id=1699571.1699591", "citeRegEx": "Mohammad et al\\.,? 2009", "shortCiteRegEx": "Mohammad et al\\.", "year": 2009}, {"title": "Crowdsourcing a Word-Emotion Association Lexicon", "author": ["Saif Mohammad", "Peter D. Turney."], "venue": "Computational Intelligence 29, 3 (2013), 436\u2013465.", "citeRegEx": "Mohammad and Turney.,? 2013", "shortCiteRegEx": "Mohammad and Turney.", "year": 2013}, {"title": "NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets", "author": ["Saif M. Mohammad", "Svetlana Kiritchenko", "Xiaodan Zhu."], "venue": "Proceedings of the seventh international workshop on Semantic Evaluation Exercises (SemEval2013). Atlanta, Georgia, USA.", "citeRegEx": "Mohammad et al\\.,? 2013", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "SemEval-2013 Task 2: Sentiment Analysis in Twitter", "author": ["Preslav Nakov", "Zornitsa Kozareva", "Alan Ritter", "Sara Rosenthal", "Veselin Stoyanov", "Theresa Wilson."], "venue": "(2013).", "citeRegEx": "Nakov et al\\.,? 2013", "shortCiteRegEx": "Nakov et al\\.", "year": 2013}, {"title": "Language-independent Twitter sentiment analysis", "author": ["Sascha Narr", "Michael Hlfenhaus", "Sahin Albayrak."], "venue": "Knowledge Discovery and Machine Learning (KDML) (2012), 12\u201314.", "citeRegEx": "Narr et al\\.,? 2012", "shortCiteRegEx": "Narr et al\\.", "year": 2012}, {"title": "A new ANEW: Evaluation of a word list for sentiment analysis in microblogs", "author": ["Finn \u00c5rup Nielsen."], "venue": "arXiv preprint arXiv:1103.2903 (2011).", "citeRegEx": "Nielsen.,? 2011", "shortCiteRegEx": "Nielsen.", "year": 2011}, {"title": "On the Predictability of Stock Market Behavior Using StockTwits Sentiment and Posting Volume", "author": ["Nuno Oliveira", "Paulo Cortez", "Nelson Areal"], "venue": "In EPIA (Lecture Notes in Computer Science),", "citeRegEx": "Oliveira et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Oliveira et al\\.", "year": 2013}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Bo Pang", "Lillian Lee."], "venue": "In Proceedings of the ACL. 271\u2013278.", "citeRegEx": "Pang and Lee.,? 2004", "shortCiteRegEx": "Pang and Lee.", "year": 2004}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan."], "venue": "ACL Conference on Empirical Methods in Natural Language Processing. 79\u201386.", "citeRegEx": "Pang et al\\.,? 2002", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Sentiment analysis of user comments for one-class collaborative filtering over TED talks", "author": ["Nikolaos Pappas", "Andrei Popescu-Belis."], "venue": "Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval. ACM, 773\u2013776.", "citeRegEx": "Pappas and Popescu.Belis.,? 2013", "shortCiteRegEx": "Pappas and Popescu.Belis.", "year": 2013}, {"title": "A general psychoevolutionary theory of emotion", "author": ["R. Plutchik."], "venue": "Academic press, New York, 3\u201333.", "citeRegEx": "Plutchik.,? 1980", "shortCiteRegEx": "Plutchik.", "year": 1980}, {"title": "Breaking the News: First Impressions Matter on Online News", "author": ["Julio Reis", "Fabricio Benevenuto", "Pedro Vaz de Melo", "Raquel Prates", "Haewoon Kwak", "Jisun An."], "venue": "Proceedings of the 9th International AAAI Conference on Web-Blogs and Social Media (ICWSM).", "citeRegEx": "Reis et al\\.,? 2015", "shortCiteRegEx": "Reis et al\\.", "year": 2015}, {"title": "Magnet News: You Choose the Polarity of What you Read", "author": ["Julio Reis", "Pollyanna Goncalves", "Pedro Vaz de Melo", "Raquel Prates", "Fabricio Benevenuto."], "venue": "International AAAI Conference on Web-Blogs and Social Media.", "citeRegEx": "Reis et al\\.,? 2014", "shortCiteRegEx": "Reis et al\\.", "year": 2014}, {"title": "Twitter Sentiment Corpus by Niek Sanders", "author": ["Niek Sanders."], "venue": "(2011). http://www.sananalytics.com/lab/twitter-sentiment/.", "citeRegEx": "Sanders.,? 2011", "shortCiteRegEx": "Sanders.", "year": 2011}, {"title": "Cheap and Fast\u2014but is It Good?: Evaluating Non-expert Annotations for Natural Language Tasks", "author": ["Rion Snow", "Brendan O\u2019Connor", "Daniel Jurafsky", "Andrew Y. Ng"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts."], "venue": "2013 Conference on Empirical Methods in Natural Language Processing. 1631\u20131642.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "The General Inquirer: A Computer Approach to Content Analysis", "author": ["Philip J. Stone", "Dexter C. Dunphy", "Marshall S. Smith", "Daniel M. Ogilvie."], "venue": "MIT Press. http://www.webuse.umd.edu:9090/", "citeRegEx": "Stone et al\\.,? 1966", "shortCiteRegEx": "Stone et al\\.", "year": 1966}, {"title": "SemEval-2007 Task 14: Affective Text", "author": ["Carlo Strapparava", "Rada Mihalcea."], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval \u201907). Association for Computational Linguistics, Stroudsburg, PA, USA, 70\u201374. http://dl.acm.org/citation.cfm?id=1621474.1621487", "citeRegEx": "Strapparava and Mihalcea.,? 2007", "shortCiteRegEx": "Strapparava and Mihalcea.", "year": 2007}, {"title": "Methods for Creating Semantic Orientation Dictionaries", "author": ["Maite Taboada", "Caroline Anthony", "Kimberly Voll."], "venue": "Conference on Language Resources and Evaluation (LREC). 427\u2013432.", "citeRegEx": "Taboada et al\\.,? 2006a", "shortCiteRegEx": "Taboada et al\\.", "year": 2006}, {"title": "Methods for Creating Semantic Orientation Dictionaries", "author": ["Maite Taboada", "Caroline Anthony", "Kimberly Voll."], "venue": "Conference on Language Resources and Evaluation (LREC). 427\u2013432.", "citeRegEx": "Taboada et al\\.,? 2006b", "shortCiteRegEx": "Taboada et al\\.", "year": 2006}, {"title": "Lexicon-based Methods for Sentiment Analysis", "author": ["Maite Taboada", "Julian Brooke", "Milan Tofiloski", "Kimberly Voll", "Manfred Stede."], "venue": "Comput. Linguist. 37, 2 (June 2011), 267\u2013307. DOI:http://dx.doi.org/10.1162/COLI a 00049", "citeRegEx": "Taboada et al\\.,? 2011", "shortCiteRegEx": "Taboada et al\\.", "year": 2011}, {"title": "Characterizing Smoking and Drinking Abstinence from Social Media", "author": ["Acar Tamersoy", "Munmun De Choudhury", "Duen Horng Chau."], "venue": "Proceedings of the 26th ACM Conference on Hypertext and Social Media (HT).", "citeRegEx": "Tamersoy et al\\.,? 2015", "shortCiteRegEx": "Tamersoy et al\\.", "year": 2015}, {"title": "The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods", "author": ["Yla R. Tausczik", "James W. Pennebaker."], "venue": "Journal of Language and Social Psychology 29, 1 (2010), 24\u201354.", "citeRegEx": "Tausczik and Pennebaker.,? 2010", "shortCiteRegEx": "Tausczik and Pennebaker.", "year": 2010}, {"title": "Heart and soul: Sentiment strength detection in the social web with SentiStrength", "author": ["Mike Thelwall."], "venue": "(2013). http:// sentistrength.wlv.ac.uk/documentation/SentiStrengthChapter.pdf.", "citeRegEx": "Thelwall.,? 2013", "shortCiteRegEx": "Thelwall.", "year": 2013}, {"title": "Survey on Mining Subjective Data on the Web", "author": ["Mikalai Tsytsarau", "Themis Palpanas."], "venue": "Data Min. Knowl. Discov. 24, 3 (May 2012), 478\u2013514. DOI:http://dx.doi.org/10.1007/s10618-011-0238-6", "citeRegEx": "Tsytsarau and Palpanas.,? 2012", "shortCiteRegEx": "Tsytsarau and Palpanas.", "year": 2012}, {"title": "Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment", "author": ["Andranik Tumasjan", "Timm O. Sprenger", "Philipp G. Sandner", "Isabell M. Welpe."], "venue": "International AAAI Conference on Weblogs and Social Media (ICWSM).", "citeRegEx": "Tumasjan et al\\.,? 2010", "shortCiteRegEx": "Tumasjan et al\\.", "year": 2010}, {"title": "WordNet-Affect: an Affective Extension of WordNet", "author": ["Ro Valitutti."], "venue": "In Proceedings of the 4th International Conference on Language Resources and Evaluation. 1083\u20131086.", "citeRegEx": "Valitutti.,? 2004", "shortCiteRegEx": "Valitutti.", "year": 2004}, {"title": "A system for real-time Twitter sentiment analysis of 2012 U.S. presidential election cycle", "author": ["Hao Wang", "Dogan Can", "Abe Kazemzadeh", "Fran\u00e7ois Bar", "Shrikanth Narayanan"], "venue": "In ACL System Demonstrations", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Development and validation of brief measures of positive and negative affect: the PANAS scales", "author": ["D. Watson", "L. Clark."], "venue": "Journal of Personality and Social Psychology 54, 1 (1985), 1063\u20131070.", "citeRegEx": "Watson and Clark.,? 1985", "shortCiteRegEx": "Watson and Clark.", "year": 1985}, {"title": "Annotating Expressions of Opinions and Emotions in Language", "author": ["Janyce Wiebe", "Theresa Wilson", "Claire Cardie."], "venue": "Language Resources and Evaluation 1, 2 (2005), 0. http://www.cs.pitt.edu/\\\u223c", "citeRegEx": "Wiebe et al\\.,? 2005", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "OpinionFinder: a system for subjectivity analysis", "author": ["Theresa Wilson", "Paul Hoffmann", "Swapna Somasundaran", "Jason Kessler", "Janyce Wiebe", "Yejin Choi", "Claire Cardie", "Ellen Riloff", "Siddharth Patwardhan."], "venue": "HLT/EMNLP on Interactive Demonstrations. 34\u201335.", "citeRegEx": "Wilson et al\\.,? 2005a", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis", "author": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann."], "venue": "ACL Conference on Empirical Methods in Natural Language Processing. 347\u2013354.", "citeRegEx": "Wilson et al\\.,? 2005b", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "No free lunch theorems for optimization", "author": ["David H. Wolpert", "William G. Macready."], "venue": "IEEE Transactions on Evlutionary Computation 1, 1 (1997), 67\u201382.", "citeRegEx": "Wolpert and Macready.,? 1997", "shortCiteRegEx": "Wolpert and Macready.", "year": 1997}], "referenceMentions": [{"referenceID": 35, "context": "Sentiment analysis can also provide analytical perspectives for financial investors who want to discover and respond to market opinions [Oliveira et al. 2013; Bollen et al. 2010].", "startOffset": 136, "endOffset": 178}, {"referenceID": 4, "context": "Sentiment analysis can also provide analytical perspectives for financial investors who want to discover and respond to market opinions [Oliveira et al. 2013; Bollen et al. 2010].", "startOffset": 136, "endOffset": 178}, {"referenceID": 54, "context": "Another important set of applications is in politics, where marketing campaigns are interested in tracking sentiments expressed by voters associated with candidates [Tumasjan et al. 2010].", "startOffset": 165, "endOffset": 187}, {"referenceID": 37, "context": "Some of them employ machine learning methods that often rely on supervised classification approaches, requiring labeled data to train classifiers [Pang et al. 2002].", "startOffset": 146, "endOffset": 164}, {"referenceID": 5, "context": "2013b] and POMS-ex [Bollen et al. 2009] were proposed as psychometric scales adapted to the Web context.", "startOffset": 19, "endOffset": 39}, {"referenceID": 23, "context": "For example, the famous Facebook experiment [Kramer et al. 2014] which manipulated users feeds to study emotional contagion, used LIWC [Tausczik and Pennebaker 2010].", "startOffset": 44, "endOffset": 64}, {"referenceID": 41, "context": "used Sentistrength [Thelwall 2013] to measure the negativeness or positiveness of online news headlines [Reis et al. 2014; Reis et al. 2015], whereas Tamersoy [Tamersoy et al.", "startOffset": 104, "endOffset": 140}, {"referenceID": 40, "context": "used Sentistrength [Thelwall 2013] to measure the negativeness or positiveness of online news headlines [Reis et al. 2014; Reis et al. 2015], whereas Tamersoy [Tamersoy et al.", "startOffset": 104, "endOffset": 140}, {"referenceID": 50, "context": "2015], whereas Tamersoy [Tamersoy et al. 2015] explored Vader\u2019s lexicon [Hutto and Gilbert 2014] to study patterns of smoking and drinking abstinence in social media.", "startOffset": 24, "endOffset": 46}, {"referenceID": 5, "context": "2013b] and POMS-ex [Bollen et al. 2009] are psychometric scales adapted to the Web context.", "startOffset": 19, "endOffset": 39}, {"referenceID": 17, "context": "Finally, in a previous effort [Gon\u00e7alves et al. 2013a], we compared eight sentence-level sentiment analysis methods, based on one public dataset used to evaluate the method sentistrength [Thelwall 2013].", "startOffset": 30, "endOffset": 54}, {"referenceID": 1, "context": "The methods used in this paper were also incorporated as part of an existing system, namely ifeel [Araujo et al. 2014].", "startOffset": 98, "endOffset": 118}, {"referenceID": 16, "context": "The purpose of a lexicon goes beyond the detection of polarity of a sentence [Feldman 2013; Liu 2012], but it can also be used to that end [Godbole et al. 2007; Kouloumpis et al. 2011].", "startOffset": 139, "endOffset": 184}, {"referenceID": 22, "context": "The purpose of a lexicon goes beyond the detection of polarity of a sentence [Feldman 2013; Liu 2012], but it can also be used to that end [Godbole et al. 2007; Kouloumpis et al. 2011].", "startOffset": 139, "endOffset": 184}, {"referenceID": 49, "context": "Several existing sentence-level sentiment analysis methods like Vader [Hutto and Gilbert 2014] and SO-CAL [Taboada et al. 2011], combine a Lexicon and the processing of the sentence char-", "startOffset": 106, "endOffset": 127}, {"referenceID": 17, "context": "Name Description L ML Emoticons [Gon\u00e7alves et al. 2013a] Messages containing positive/negative emoticons are positive/negative.", "startOffset": 32, "endOffset": 56}, {"referenceID": 59, "context": "X Opinion Finder (MPQA) [Wilson et al. 2005a] [Wilson et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 60, "context": "2005a] [Wilson et al. 2005b] Performs subjectivity analysis trough a framework with lexical analysis former and a machine learning approach latter.", "startOffset": 7, "endOffset": 28}, {"referenceID": 2, "context": "SentiWordNet [Esuli and Sebastiani 2006] [Baccianella et al. 2010] Construction of a lexical resource for Opinion Mining based on WordNet [Miller 1995].", "startOffset": 41, "endOffset": 66}, {"referenceID": 7, "context": "SenticNet [Cambria et al. 2010] Uses dimensionality reduction to infer the polarity of common sense concepts and hence provide a public resource for mining opinions from natural language text at a semantic, rather than just syntactic level.", "startOffset": 10, "endOffset": 31}, {"referenceID": 49, "context": "SO-CAL [Taboada et al. 2011] Creates a new Lexicon with unigrams (verbs, adverbs, nouns and adjectives) and multi-grams (phrasal verbs and intensifiers) hand ranked with scale +5 (strongly positive) to -5 (strongly negative).", "startOffset": 7, "endOffset": 28}, {"referenceID": 19, "context": "Emoticons DS (Distant Supervision)[Hannak et al. 2012] Creates a scored lexicon based on a large dataset of tweets.", "startOffset": 34, "endOffset": 54}, {"referenceID": 56, "context": "SASA [Wang et al. 2012] Detects public sentiments on Twitter during the 2012 U.", "startOffset": 5, "endOffset": 23}, {"referenceID": 31, "context": "Sentiment140 Lexicon [Mohammad et al. 2013] Creation of a lexicon dictionary in a similar way to [Mohammad 2012] and a SVM Classifier with features like: number and categories of emoticons, sum of the sentiment scores for all tokens (calculated with lexicons), etc.", "startOffset": 21, "endOffset": 43}, {"referenceID": 44, "context": "Stanford Recursive Deep Model [Socher et al. 2013] Proposes a model called Recursive Neural Tensor Network (RNTN) that processes all sentences dealing with their structures and compute the interactions between them.", "startOffset": 30, "endOffset": 50}, {"referenceID": 58, "context": "Opinion Finder (MPQA) Negative, Neutral, Positive MPQA [Wiebe et al. 2005] Compared to itself in different versions.", "startOffset": 55, "endOffset": 74}, {"referenceID": 45, "context": "SentiWordNet -1, 0, 1 General Inquirer (GI)[Stone et al. 1966] 117658", "startOffset": 43, "endOffset": 62}, {"referenceID": 59, "context": "AFINN -1, 0, 1 Twiter [Biever 2010] OpinonFinder [Wilson et al. 2005a], ANEW [Bradley and Lang 1999], GI [Stone et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 45, "context": "2005a], ANEW [Bradley and Lang 1999], GI [Stone et al. 1966] and Sentistrength [Thelwall 2013] 2477 X", "startOffset": 41, "endOffset": 60}, {"referenceID": 47, "context": "SO-CAL [<0), 0, (>0] Epinion [Taboada et al. 2006a], MPQA[Wiebe et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 58, "context": "2006a], MPQA[Wiebe et al. 2005], Myspace[Thelwall 2013], MPQA[Wiebe et al.", "startOffset": 12, "endOffset": 31}, {"referenceID": 58, "context": "2005], Myspace[Thelwall 2013], MPQA[Wiebe et al. 2005], GI[Stone et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 45, "context": "2005], GI[Stone et al. 1966], SentiWordNet [Esuli and Sebastiani 2006],\u201dMaryland\u201d Dict.", "startOffset": 9, "endOffset": 28}, {"referenceID": 29, "context": "[Mohammad et al. 2009], Google Generated Dict.", "startOffset": 0, "endOffset": 22}, {"referenceID": 48, "context": "[Taboada et al. 2006b] 9928", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "Emoticons DS (Distant Supervision) -1, 0, 1 Validation with unlabeled twitter data [Cha et al. 2010] 1162894 X", "startOffset": 83, "endOffset": 100}, {"referenceID": 56, "context": "SASA [Wang et al. 2012] Negative, Neutral, Unsure, Positive \u201cPolitical\u201d Tweets labeled by \u201cturkers\u201d (AMT) (unavailable) 21012", "startOffset": 5, "endOffset": 23}, {"referenceID": 8, "context": "PANAS-t -1, 0, 1 Validation with unlabeled twitter data [Cha et al. 2010] 50 X", "startOffset": 56, "endOffset": 73}, {"referenceID": 32, "context": "Sentiment140 Negative, Neutral, Positive Twitter and SMS from Semeval 2013, task 2 [Nakov et al. 2013].", "startOffset": 83, "endOffset": 102}, {"referenceID": 32, "context": "Umigon Negative, Neutral, Positive Twitter and SMS from Semeval 2013, task 2 [Nakov et al. 2013].", "startOffset": 77, "endOffset": 96}, {"referenceID": 31, "context": "[Mohammad et al. 2013] 1053", "startOffset": 0, "endOffset": 22}, {"referenceID": 45, "context": "(GI)[Stone et al. 1966], LIWC, [Tausczik and Pennebaker 2010], SentiWordNet [Esuli and Sebastiani 2006], ANEW [Bradley and Lang 1999], SenticNet [Cambria et al.", "startOffset": 4, "endOffset": 23}, {"referenceID": 7, "context": "1966], LIWC, [Tausczik and Pennebaker 2010], SentiWordNet [Esuli and Sebastiani 2006], ANEW [Bradley and Lang 1999], SenticNet [Cambria et al. 2010] and some Machine Learning Approaches.", "startOffset": 127, "endOffset": 148}, {"referenceID": 5, "context": "g, Profile of Mood States (POMS) [Bollen et al. 2009]).", "startOffset": 33, "endOffset": 53}, {"referenceID": 19, "context": "Emoticon Distance Supervised [Hannak et al. 2012] used Pearson Correlation between human labeling and the predicted value.", "startOffset": 29, "endOffset": 49}, {"referenceID": 7, "context": "SenticNet [Cambria et al. 2010] was compared with SentiStrength [Thelwall 2013] with a specific dataset related to patient opinions, which could not be made available.", "startOffset": 10, "endOffset": 31}, {"referenceID": 44, "context": "Stanford Recursive Deep Model [Socher et al. 2013] and SentiStrength [Thelwall 2013] were both compared with standard machine learning approaches, with their own datasets.", "startOffset": 30, "endOffset": 50}, {"referenceID": 33, "context": "Tweets (Random) Tweets RND III 3,771 739 488 2,536 1,54 14,32 AMT 3 90,0 [Narr et al. 2012]", "startOffset": 73, "endOffset": 91}, {"referenceID": 15, "context": "97,0 [Go et al. 2009]", "startOffset": 5, "endOffset": 21}, {"referenceID": 32, "context": "Tweets (Semeval2013 Task2) Tweets Semeval 6,087 2,223 837 3027 1,86 20,05 AMT 5 100,0 [Nakov et al. 2013]", "startOffset": 86, "endOffset": 105}, {"referenceID": 43, "context": "Previous studies suggest that both efforts are valid as non-expert labeling may be as effective as annotations produced by experts for affect recognition, a very related task [Snow et al. 2008].", "startOffset": 175, "endOffset": 193}, {"referenceID": 11, "context": "Recent efforts show that human language have a universal positivity bias [Dodds et al. 2015].", "startOffset": 73, "endOffset": 92}, {"referenceID": 19, "context": "For instance, [Hannak et al. 2012] developed a lexicon in which positive and negative values are", "startOffset": 14, "endOffset": 34}], "year": 2017, "abstractText": "In the last few years thousands of scientific papers have explored sentiment analysis, several startups that measures opinions on real data have emerged, and a number of innovative products related to this theme have been developed. There are multiple methods for measuring sentiments, including lexical-based approaches and supervised machine learning methods. Despite the vast interest on the theme and wide popularity of some methods, it is unclear which method is better for identifying the polarity (i.e., positive or negative) of a message. Thus, there is a strong need to conduct a thorough apple-to-apple comparison of sentiment analysis methods, as they are used in practice, across multiple datasets originated from different data sources. Such a comparison is key for understanding the potential limitations, advantages, and disadvantages of popular methods. This study aims at filling this gap by presenting a benchmark comparison of twenty one popular sentiment analysis methods (which we call the state-of-the-practice methods). Our evaluation is based on a benchmark of twenty labeled datasets, covering messages posted on social networks, movie and product reviews, as well as opinions and comments in news articles. Our results highlight the extent to which the prediction performance of these methods varies widely across datasets. Aiming at boosting the development of this research area, we open the methods\u2019 codes and datasets used in this paper and we deploy a benchmark system, which provides an open API for accessing and comparing sentence-level sentiment analysis methods. CCS Concepts: rInformation systems \u2192 Sentiment analysis; rNetworks \u2192 Social media networks; Online social networks;", "creator": "TeX"}}}