{"id": "1512.01927", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "Fast Optimization Algorithm on Riemannian Manifolds and Its Application in Low-Rank Representation", "abstract": "the paper addresses the problem : computational robust graded - composite complexity on riemannian spaces and a new first execution optimization algorithm ( nsa ) with a geometric convergence framework is constructed. through the theoretical analysis for foa, knowledge includes been proving that pursuit algorithm has quadratic convergence. the experiments in the matrix classification paper show that foa yielded efficient performance performing other first order nonlinear methods on riemannian manifolds. a complete subspace pursuit iteration implementation on space is proposed to solve euclidean low - point representation model based on augmented bounded vectors with 2d low rank group dimensions. experimental results on fixed and sparse data sets are discovered making such accuracy formula foa \u2212 sp - rprg ( alm ) can result superior performance in alternating its geometric convergence and higher retention.", "histories": [["v1", "Mon, 7 Dec 2015 06:44:23 GMT  (614kb,D)", "http://arxiv.org/abs/1512.01927v1", null]], "reviews": [], "SUBJECTS": "cs.NA cs.CV cs.LG", "authors": ["haoran chen", "yanfeng sun", "junbin gao", "yongli hu"], "accepted": false, "id": "1512.01927"}, "pdf": {"name": "1512.01927.pdf", "metadata": {"source": "CRF", "title": "Fast Optimization Algorithm on Riemannian Manifolds and Its Application in Low-Rank Representation", "authors": ["Haoran Chen", "Yanfeng Sun", "Junbin Gao", "Yongli Hu"], "emails": ["chen@emails.bjut.edu.cn,", "yfsun@bjut.edu.cn,", "huyongli@bjut.edu.cn", "jbgao@csu.edu.au"], "sections": [{"heading": null, "text": "Keywords Fast optimization algorithm \u00b7 Riemannian manifold \u00b7 Proximal Riemannian gradient \u00b7 Subspace pursuit \u00b7 Low rank matrix variety \u00b7 Low rank representation \u00b7 Augmented Lagrange method \u00b7 clustering"}, {"heading": "1 Introduction", "text": "Recent years have witnessed the growing attention in optimization on Riemannian manifolds. Since Riemannian optimization is directly based on the curved manifolds, one can eliminate those constraints such as orthogonality to obtain an unconstrained optimization problem that, by construction, will only use feasible points. This allows one to incorporate Riemannian geometry in the resulting\nHaoran Chen, Yanfeng Sun, Yongli Hu. Beijing Key Laboratory of Multimedia and Intelligent Technology, College of Metropolitan Transportation, Beijing University of Technology, Beijing 100124, P. R. China Tel.: +86-130-5118-1821 E-mail: hr chen@emails.bjut.edu.cn, yfsun@bjut.edu.cn, huyongli@bjut.edu.cn\nJunbin Gao School of Computing and Mathematics, Charles Sturt University, Bathurst, NSW 2795, Australia. E-mail: jbgao@csu.edu.au\nar X\niv :1\n51 2.\n01 92\n7v 1\n[ cs\n.N A\n] 7\nD ec\n2 01\noptimization problems, thus producing far more accurate numerical results. The Riemannian optimization have been successfully applied in machine learning, computer vision and data mining tasks, including fixed low rank optimization [5, 33], Riemannian dictionary learning [7], and computer vision [?, 30, 34], and tensor clustering [25].\nThe constrained optimization on Euclidean space often has much larger dimension than that on manifolds implicitly defined by the constraints. An optimization algorithm on a manifold has therefore a lower complexity and better numerical properties. Methods of solving minimization problems on Riemannian manifolds have been extensively researched [2, 31]. In fact, traditional optimization methods such as the steepest gradient method, conjugate gradient method and Newton method in Euclidean space can be easily adopted to optimization on Riemannian manifolds and a solid conceptual framework was built over the last decades and the generic algorithm implementation is openly available, see http://www.manopt.org.\nAs one of fundamental optimization algorithms, the steepest descent method was first proposed for optimization on Riemmanian manifolds in [6]. The algorithm is computationally simple, however its convergence is very slow, particularly, for large scale complicated optimization problems in modern machine learning. In contrast, the Newton\u2019s method [6] and the BFGS quasi-Newton scheme (BFGS rank2-update) [21] have higher convergence rate, however, in practical applications, using the full second-order Hessian information is computationally prohibitive.\nIn order to obtain a method offering high convergence rate and avoiding calculating the inverse Hessian matrix, Absil et al. [1] proposed the Trust-region method on Riemannian manifolds. For example, the Trust-region method has been applied the optimization problem on Grassmann manifold for the matrix completion problem [5]. Each iteration of the trust-region method involves solving the Riemannian Newton equation [23], which increases the complexity of the algorithm. Huang et al. [8] generalized symmetric rank-one trust-region method to a vector variable optimization problem on a d-dimensional Riemannian manifold, where an approximated Hessian matrix was generated by using the symmetric rank-one update without solving Riemannian Newton equation. This method has superlinear convergence. However, the symmetric rank-one update matrices are generated only by vectors rather than matrices, so this algorithm is not feasible for the variable is the m\u00d7 n matrix. This is a great barrier in practical applications.\nGenerally, optimization methods which use second-order function information can get faster convergent speed than just using first-order information, however, this will greatly increase computational complexity at the same time. FletcherReeves conjugate gradient method on Riemannian manifolds [21] is a typical type of methods which need only the first-order function information, and its convergence is superlinear but lower than the desired 2-order speed.\nFor the gradient method, it is known that the sequence of function values F (Xk) converges to the optimal function value F\u2217 at a rate of convergence that is no worse than O(1/k), which is also called a superlinear convergence. For optimization on Euclidean spaces, researchers have sought for acceleration strategies to speed up the optimization algorithms requiring only first-order function information, e.g. based on linear search [17]. More strategies have been proposed by considering special structures of objectives, such as composite objective functions [3,18,29]. In these strategies, the algorithms need only the first-order function\ninformation with a guaranteed quadratic convergence rate for F (Xk). In this paper we extend the method to Algorithm 1 for the general model (13) on Riemannina manifolds and we establish the improved complexity and convergence results, see Theorem 1 in Section 3.\nAs a practical application of the proposed fast gradient-like first-order algorithm, we further consider the learning problems on low-rank manifolds. Low-rank learning seeks the lowest rank representation among all the candidates that can contain most of the data samples information. The low-rank learning has been widely applied in the low-rank matrix recovery (MR)/low-rank matrix completion [14\u201316] and low-rank representation (LRR) for subspace clustering [11\u201313]. One of low-rank matrix completion application examples is the Netflix problem [4], in which one would like to recover a low-rank matrix from a sparse sampled matrix entries.\nHowever, the lowest rank optimization problems are NP hard and generally extremely hard to solve (and also hard to approximate [20]). Typical approaches are either to transform the lowest rank problem into minimization of nuclear norm by using effective convex relaxation or parametrize the fixed-rank matrix variables based on matrix decomposition.\nAs the set of rank-r matrix Mr is a smooth Riemannian manifold under an appropriately chosen metric [2, 14, 16, 33], one can convert a rank constrained optimization problem to an unconstrained optimization problem on the low rank matrix manifold. Vandereycken [33] considered this problem as an optimization problem on fixed-rank matrix Riemannian manifold. Considering different types of matrix decomposition, Mishra et al. [14, 16] proposed a Riemannian quotient manifold for low-rank matrix completion. Ngo et al. [19] addressed this problem based on a scaled metric on the Grassmann manifold.\nThe aforementioned works transform/simplify a low-rank matrix completion problem into the fixed-rank optimization. However, low-rank optimization of matrix completion is not equivalent to the fixed-rank optimization. In order to exploit the fixed-rank optimization for low-rank optimization, Mishra et al. [15] proposed a pursuit algorithm that alternates between fixed-rank optimization and rank-one updates to search the best rank value. Tan et al. [27] proposed a Riemannian pursuit approach which converts low-rank problem into a series of fixed rank problems, and further confirmed that low-rank problem can be considered as optimization whose search space is varieties of low-rank matrices M\u2264r [26], see (7), using the subspace pursuit approach on Riemannian manifold to look for the desired rank value r. Since M\u2264r is a closure of the Riemannian submanifold Mr, see (4), an iteration algorithm over M\u2264r is more safe-guarded than on the fixed-rank open manifold Mr. We abuse \u201dRiemannian\u201d for simplicity. In this paper, we consider low-rank learning as optimization problems on varieties of low-rank matrix Riemannian manifolds M\u2264r.\nThe contributions of this paper are:\n1. We develop a fast optimization algorithm (FOA) for the model (13) on general Riemannian manifolds. This algorithm only uses first-order function information, but has the convergence rate O(1/k2). 2. We provide a theoretical analysis for the proposed FOA and prove its quadratic convergence.\n3. We propose fast subspace pursuit approach (SP-RPRG(ALM)) on low rank matrix varieties based on the augmented Lagrangian method for the model (27).\nThe rest of this paper is organized as follows. Section 2 introduces the necessary symbol and concepts. In section 3, we propose the fast optimization algorithm on Riemannian manifold, and discuss its convergence in detail. In section 4, we extend the previous algorithm to the low-rank representation with noise on lowrank matrices varieties. Moreover, we proposed the fast subspace pursuit method (SP-RPRG(ALM)) based on augmented Lagrange approach and FOA on low rank matrices varieties and applied SP-RPRG(ALM) to this model. Some experimental results are presented for evaluating the performance of our algorithm in Section 5. Finally, the conclusions are summarized in Section 6."}, {"heading": "2 Notations and Preliminaries", "text": "This section will briefly describe some notations and concepts that will be used throughout this paper.\n2.1 Notations\nWe denote matrices by boldface capital letters, e.g., A, vectors by boldface lowercase letters, e.g., a, and scalars by letters, e.g., a. The `p-norm of a vector v is denoted by \u2016v\u2016p. \u2016v\u20160 is defined as the number of non-zero elements in the vector v. \u2016A\u201621 denotes the sum of all `2-norm of its columns. The operator max{u,v} means the vector resulting from the component-wise maximum of u and v. The inner product of two matrices A and B at the same size is defined as \u3008A,B\u3009 = tr(ATB). The superscript T denotes the transpose of a vector/matrix. The diag(\u03c3) is the diagonal matrix whose diagonal elements are the components of vector \u03c3. The SVD decomposition of X \u2208 Rm\u00d7n is denoted as X = Udiag(\u03c3)VT, where \u03c3i is the i-th singular value of X. The nuclear norm of X is defined as \u2016X\u2016\u2217 = \u2211 i \u03c3i. For any convex function F (X), its subdifferential at X is denoted by \u2202F (X).\n2.2 Riemannian Manifolds\nA manifold M of dimension m [10] is a topological space that locally resembles a Euclidean space Rm in a neighbourhood of each point X \u2208M. For example, lines and circles are 1D manifolds, and surfaces, such as a plane, a sphere, and a torus, are 2D manifolds. Geometrically, a tangent vector is a vector that is tangent to a manifold at a given point X. Abstractly, a tangent vector is a functional, defined on the set of all the smooth functions over the manifoldM, which satisfies the Leibniz differential rule. All the possible tangent vectors at X constitute a Euclidean space, named the tangent space of M at X and denoted by TXM. If we have a smoothly defined metric (inner-product) across all the tangent spaces \u3008\u00b7, \u00b7\u3009X : TXM\u00d7TXM\u2192 R on every point X \u2208M, then we callM a Riemannian manifold.\nThe tangent bundle is defined as the disjoint union of all tangent spaces TM := \u22c3\nX\u2208M\n{X} \u00d7 TXM. (1)\nFor a Riemannian manifold, the Riemannian gradient of a smooth function f : M \u2192 R at X \u2208 M is defined as the unique tangent vector gradf(X) in TXM such that \u3008gradf(X), \u03be\u3009X = Df(X)[\u03be] for all \u03be \u2208 TXM, where Df(X) denotes the (Euclidean) directional derivative. If M is embedded in a Euclidean space, then gradf(X) is given by the projection of Df(X) onto the tangent space TXM.\nWith a globally defined differential structure, manifold M becomes a differentiable manifold. A geodesic \u03b3 : [0, 1] \u2192 M is a smooth curve with a vanishing covariant derivative of its tangent vector field, and in particular, the Riemmannian distance between two points Xi,Xj \u2208 M is the shortest smooth path connecting them on the manifold, that is the infimum of the lengths of all paths joining Xi and Xj .\nThere are predominantly two operations for computations on a Riemannian manifold, namely (1) the exponential map at point X, denoted by expX : TXM\u2192 M, and (2) the logarithmic map, at point X, logX : M \u2192 TXM. The former projects a tangent vector in the tangent space onto the manifold, the latter does the reverse. Locally both mappings are diffeomorphic. Note that these maps depend on the manifold point X at which the tangent spaces are computed.\nGiven two points Xi,Xj \u2208M that are close to each other, the distance between them can be calculated through the following formula as the norm in tangent space.\ndistM(Xi,Xj) = \u2016 logXi(Xj)\u2016Xi (2)\nThe squared distance function dist2M(X, \u00b7) is a smooth function for all X \u2208 M. In this setting, it is tempting to define a retraction and lifting along the following lines. Given X \u2208 M and \u03be \u2208 TXM, compute Retraction operator RX(\u03be) by (1) moving along \u03be to get the point X + \u03be in the linear embedding space, and (2) projecting the point X + \u03be back to the manifold M. And we compute Lifting operator LXi(Xj) by projecting the point Xj onto the tangent space TXiM. From another point of view, since LXi(Xi) = 0, so LXi(Xj) could be perceived as a direction denoting from point Xi to Xj on Riemannian manifold.\nAt last, we need to define vector transport on M.\nTX\u2192Y : TXM\u2192 TYM, \u03be \u2192 PTYM(\u03be).\n(3)\n2.3 The Riemannian Manifold of Fixed-rank Matrices\nLet\nMr = {X \u2208 Rm\u00d7n : rank(X) = r}. (4)\nIt can be proved that Mr is a Riemannian manifold of dimension m+ n\u2212 r [33]. It has the following equivalent expression by SVD decomposition:\nMr = {Udiag(\u03c3)VT : U \u2208 Stmr ,V \u2208 Stnr , \u2016\u03c3\u20160 = r}\nwhere Stmr is the Stiefel manifold of m \u00d7 r real, orthonormal matrices, and the entries in \u03c3 are arranged in descending order. Furthermore, the tangent space TXMr at X = Udiag(\u03c3)VT \u2208Mr is given by\nTXMr = { UMVT + UVTp + UpV T : M \u2208 Rr\u00d7r,\nUp \u2208 Rm\u00d7r,Vp \u2208 Rn\u00d7r,UTpU = 0,VTp V = 0 } .\n(5)\nSinceMr is embedded in Rm\u00d7n, the Riemannian gradient of f is given as the orthogonal projection of the Euclidean gradient of f onto the tangent space. Here, we denote the orthogonal projection of any Z \u2208 Rm\u00d7n onto the tangent space at X = Udiag(\u03c3)VT as\nPTXMr : R m\u00d7n \u2192 TXMr\nZ 7\u2192 PUZPV + P\u22a5UZPV + PUZP\u22a5V, (6)\nwhere PU := UU T,P\u22a5U := I \u2212 PU,PV := VVT and P\u22a5V := I \u2212 PV, for any U \u2208 Stmr ,V \u2208 Stnr .\n2.4 Varieties of Low-rank Matrices\nDefine\nM\u2264r = {X \u2208 Rm\u00d7n : rank(X) \u2264 r}, (7)\nthenM\u2264r is the closure of the matrices set with constant rankMr. At a singular point X where rank(X)= s < r, we have to use search directions in the tangent cone (instead of tangent space), The tangent cones of M\u2264r are explicitly known [22],\nTXM\u2264r = TXMs \u2295 {\u039er\u2212s \u2208 U\u22a5 \u2297 V\u22a5}, (8)\nwhere U = rang(X) and V = rang(XT), X = U\u03a3VT, rank(X) = s < r. Projection operator PTXM\u2264r (\u00b7), projecting an element \u03b7 onto the tangent cone TXM\u2264r, can be calculated as\nPTXM\u2264r (\u03b7) = PTXMs(\u03b7) + \u039er\u2212s, (9)\nwhere\nPTXMs(\u03b7) = PU\u03b7PV + P \u22a5 U\u03b7PV + PU\u2207f(X)P\u22a5V;\n\u039er\u2212s = argmin rank(Y)\u2264r\u2212s\n\u2016\u03b7 \u2212 PTXMs(\u03b7)\u2212Y\u2016 2 F .\nNote that \u039er\u2212s is the best rank r\u2212 s approximation of \u03b7\u2212PTXMs(\u03b7). Moreover, \u039er\u2212s can be efficiently computed with the same complexity as on Mr in [26].\nThe gradient of function f(X) can be calculated by the following formula\ngradf(X) = PTXM\u2264r (\u2207f(X)), (10)\nwhere \u2207f(X) denotes the Euclidean gradient of f(X).\nAnother ingredient we need is the so-called retraction operator which maps a tangent vector \u03be in the tangent cone TXM\u2264r at a given point X on M\u2264r, which is defined as\nRX(\u03be) = argmin Y\u2208M\u2264r\n\u2016X + \u03be \u2212Y\u20162F . (11)\nNext, we introduce the inverse of retraction operator called the Lifting operator, denoted by LX(Y) which maps a point Y on manifold back to the tangent tone space TXM. While the two point X,Y are close to each other, we can compute approximately by\nLX(Y) :M\u2264r 7\u2192 TXM, Y \u2192 PTYM\u2264r (Y \u2212 X).\n(12)\nAlso, LX(Y) can denote a direction form X to Y on Varieties since LX(X) = 0."}, {"heading": "3 The Fast Optimization Algorithm on Riemannian manifold", "text": "We consider the following general model (13) on Riemannian manifold which also naturally extends the problem formulation in [3]\nmin X\u2208M F (X) = f(X) + g(X) (13)\nwhereM is a Riemannian manifold. We make the following assumptions throughout the section:\n1. g : Rm\u00d7n \u2192 R is a continuous convex function which is possibly non-smooth. 2. f : Rm\u00d7n \u2192 R is a smooth convex function of the type C2, there exists a finite\nreal number L(f) such that \u03bbmax(H) \u2264 L(f), where \u03bbmax(H) is the largest singular value of the Hessian matrix of the function f .\n3. F (X) satisfies the following inequality, F (X) \u2265 F (Y) + \u2329 \u2202F (X), LY(X) \u232a (14)\nwhere \u2200 X,Y \u2208M, and LX(\u00b7) is a Lifting operator defined in Section 2.2.\nIt is usually difficult to directly solve (13), in particular, in some applications such as matrix completion and low rank representation. While introducing auxiliary variables, expensive matrix inversions are often necessary. Here we adopt the following proximal Riemannian gradient (PRG) method [26] to update X. For any \u03b1 > 0, we consider the following quadratic approximation of F (X) = f(X)+g(X) at a given point Y \u2208M,\nQ\u03b1(X,Y) := f(Y) + \u3008gradf(Y), LY(X)\u3009+ \u03b1\n2 \u2016LY(X)\u20162Y + g(X), (15)\nNote the above local model is different from that on vector spaces [18, 29]. We assume that the local model admits a unique minimizer\nP\u03b1(Y) : = argmin X\u2208M Q\u03b1(X,Y)\n= argmin X\u2208M\n{ f(Y) + \u3008gradf(Y), LY(X)\u3009+ \u03b1\n2 \u2016LY(X)\u20162Y + g(X)\n} . (16)\nSimple manipulation gives that\nP\u03b1(Y) = argmin X\u2208M\n{ g(X) + \u03b1\n2 \u2016LY(X) +\n1 \u03b1 gradf(Y)\u20162Y\n} . (17)\nTaking Y = Xk\u22121 in (17), we define the local optimal point of (17) as Xk,\nXk = P\u03b1(Xk\u22121), (18)\nwhere 1/\u03b1 plays the role of a step size. Moreover, \u03b1 needs to satisfy the inequality\nF (P\u03b1(Xk\u22121)) \u2264 Q\u03b1(P\u03b1(Xk\u22121),Xk\u22121). (19)\nNote that the sequence of function values {F (Xk)} produced by (18) is nonincreasing. Indeed, for every k \u2265 1\nF (Xk) \u2264 Q\u03b1k(Xk,Xk\u22121) \u2264 Q\u03b1k(Xk\u22121,Xk\u22121) = F (Xk\u22121). (20)\nThe convergent speed is a major concern for any optimization algorithms. Linear search on Riemannian manifold has linear convergence. This conclusion has been shown in [2] when g(X) = 0. In order to obtain linear search with quadratic convergence, we propose a fast optimization algorithm (FOA) which extends the acceleration methods in [3,9,29] onto Riemannian manifold. A specific linear combination of the previous two points {Xk\u22121,Xk\u22122} was employed in the acceleration methods in Euclidean space. However the linear operation is a barrier for Riemannian manifold since Riemannian manifold may be not a linear space. So we need to use necessary tools on Riemannian manifold to overcome this difficulty.\nAccording to (19) and (20), we can obtain a convergent sequence {Xk} and its corresponding function value sequence {F (Xk)} which is monotonically declined. So the vector LXk\u22121(Xk\u22122) (see Section 2.2) is an ascent direction and its negative direction is a descent direction. We start at the point Xk\u22121 and walk a specific step size along the descent direction \u2212LXk\u22121(Xk\u22122) on TXk\u22121M to reach a new point Y\u2217k. Then we retract Y \u2217 k onto M denoted by RXk\u22121(Y\u2217k), i.e. the new auxiliary variable Yk. This is an acceleration part of the fast optimization algorithm. Next, the point Xk was generated by P\u03b1(Yk), i.e., replacing Xk\u22121 with Yk in (18) and (19), which is the difference between FOA and the linear search algorithm. In this way, we can generate the iterative sequence {f(Xk)} with the convergence rate O(1/k2).\nStopping Conditions I. The iteration with backtracking can be terminated if one of the following conditions is satisfied:\nCondition 1: (F (Xk\u22121)\u2212 F (Xk))/F (Xk\u22121) \u2264 1; Condition 2: 1/\u03b1k \u2264 2; Condition 3: Number of iterations \u2265 N1.\nwhere 1 and 2 denote tolerance values, and N1 denotes the given positive integer. The FOA is summarized in Algorithm 1. Now we are in a position to present the major conclusion of the paper.\nLemma 1 (Optimality Condition) A point X\u2217 \u2208 M is a local minimizer of (13) if and only if there exists \u03b7 \u2208 \u2202g(X) such that [2]\ngradf(X) + \u03b7 = 0.\nwhere gradf(X),\u03b7 \u2208 TXM.\nAlgorithm 1 FOA on Riemannian manifold for model (13)\n1: Initial Take \u03b10 > 0, \u03b7 > 1, \u03b2 \u2208 (0, 1). Set Y1 = X0, and t1 = 1. 2: Find the smallest nonegative integers ik such that with \u03b1\u0304 = \u03b7 ik\u03b1k\u22121\nF (P\u03b1\u0304(Yk)) \u2264 Q\u03b1\u0304(P\u03b1\u0304(Yk),Yk). (21)\nSet \u03b1k = \u03b7 ik\u03b1k\u22121 and compute\nXk = P\u03b1k (Yk),\ntk+1 = 1 +\n\u221a 1 + 4t2k\n2 Yk+1 = RXk ( \u2212 tk \u2212 1 tk+1 LXk (Xk\u22121) ) (22)\n3: Terminate if stopping conditions I are achieved. Output: Xk\nLemma 2 Let Xk \u2208M and \u03b7 \u2208 TXkM be a descent direction. Then there exists an \u03b1k that satisfies the condition in (21).\nProof According to the quality of function F (X), Y is close to X, Y \u2208 M, we can obtain F (X) \u2264 f(Y) + \u3008gradf(Y), LY(X)\u3009+\u03b7L(f)\u2016LY(X)\u20162Y/2 + g(X), i.e. equality (21) to hold when \u03b1k \u2265 \u03b7L(f).\nThe following Theorem 1 gives the quadratic convergence of Algorithm 1.\nTheorem 1 Let {Xk} be generated by Algorithm 1, then for any k \u2265 1\nF (Xk)\u2212 F (X\u2217) \u2264 2\u03b7L(f)\u2016LX\u2217(X0)\u20162X\u2217\n(k + 1)2\nThe proof of Theorem 1 is given in Appendix A. When noise exists in data, the model (13) can be changed into the following form: min\nX\u2208M,E\u2208Rm\u00d7n F (X,E) = f(X,E) + g(X,E). (23)\nwhere the properties of function f , g is the same as (13). In order to solve the problem (23) following [11], we can optimize the variables X and E by alternating direction approach. This approach decomposes the minimization of F (X,E) into two subproblems that minimize w.r.t. X and E, respectively. The iteration of each subproblem goes as follows,\nXk = argmin X\u2208M F (X,Ek\u22121) (24)\nEk = argmin E\u2208Rm\u00d7n F (Xk,E) (25)\nWhile solving (24), the variable Ek\u22121 is fixed. We consider the following robust proximal Riemannian gradient (RPRG) method of F (X,Ek\u22121) at a given point Yk on M\nQ\u03b1(X,Yk,Ek\u22121) :=f(Yk,Ek\u22121) + \u2329 gradfX(Yk,Ek\u22121), LYk(X) \u232a + L\n2 \u2016LYk(X)\u2016 2 Yk + g(X,Ek\u22121)\nDefine\nXk = P\u03b1(Yk,Ek\u22121) := argmin X\u2208M Q\u03b1(X,Yk,Ek\u22121),\n= argmin X\u2208M\n{ g(X,Ek\u22121) + \u03b1\n2 \u2016LYk(X) +\n1 \u03b1 gradfX(Yk,Ek\u22121)\u2016\n} (26)\nwhere the point Yk \u2208M is generated by the previous point (Xk\u22122,Xk\u22121) in same methods as Algorithm 1.\nIn most cases, subproblem (25) has a closed solution. Similar to Algorithm 1, we propose the following stopping conditions II:\nCondition 1: \u2016Xk \u2212Xk+1\u2016 \u2264 \u03b51 and \u2016Ek \u2212Ek+1\u2016 \u2264 \u03b52; Condition 2: Number of iterations \u2265 N1. Condition 3: The number of alternating direction iterations \u2265 N2.\nwhere \u03b51 is a threshold for the change in the solutions, \u03b52 is a threshold for the error constraint, and N1, N2 are positive integers.\nAlgorithm 2 summarizes the FOA for model (23).\nAlgorithm 2 FOA on Riemannian manifold for model (23)\n1: Initial Take \u03b10 > 0, \u03b7 > 1, \u03b2 \u2208 (0, 1). X0 = 0,E0 = 0, Set Y1 = X0, and t1 = 1. 2: for k = 1, 2, \u00b7 \u00b7 \u00b7 , N2 do 3: for i = 1, 2, \u00b7 \u00b7 \u00b7 , N1 do 4: Find the smallest nonegative integers ij such that with \u03b1\u0304 = \u03b7 ij\u03b1i\u22121\nF ( P\u03b1\u0304(Yi),Ei\u22121 ) \u2264 Q\u03b1\u0304 ( P\u03b1\u0304(Yi),Yi,Ei\u22121 ) Set \u03b1i = \u03b7 ij\u03b1i\u22121 and compute\nXi = P\u03b1i (Yi,Ei\u22121), by (26)\nti+1 = 1 +\n\u221a 1 + 4t2i\n2 Yi+1 = RXi ( \u2212 ti \u2212 1 ti+1 LXi (Xi\u22121) )\n5: Terminate if stopping conditions I are achieved. 6: end for 7: Xk = Xi. 8: Ek = argmin\nE\u2208Rm\u00d7n F (Xk,E)\n9: Terminate if stopping conditions II are achieved. 10: end for Output: (Xk,Ek).\nBy Theorem 1, it is easy to infer the following theorem to guarantee the convergence of Algorithm 2.\nTheorem 2 Let F (X,E) = f(X,E) + g(X,E), and {(Xk,Ek)} be an infinite sequence generated by Algorithm 2, then it follows that F (Xk+1,Ek+1) \u2264 F (Xk,Ek), {(Xk,Ek)} converges to a limit point (X\u2217,E\u2217)."}, {"heading": "4 The application of FOA in low rank representation on low rank matrix varieties", "text": "Low rank representation model has been widely studied in recent years. In this section, we try to apply algorithm 2 to low rank representation model on varieties. The model is defined as follows:\nmin X,E\n\u2016X\u2016\u2217 + \u03bb\u2016E\u201621,\ns.t. DX + E = D, X \u2208M\u2264r. (27)\nwhere \u2016X\u2016\u2217 denotes the nuclear-norm of matrix X \u2208 Rm\u00d7n, \u03bb is a regularization parameter, D is the data matrix, \u2016E\u201621 is a regularizer (see Section 2.1), andM\u2264r denotes Low rank matrix varieties of rank at most r.\nIn order to solve model (27), we generalize the augmented Lagrangian methods (ALM) on Euclidean space to optimization on Low rank matrix varieties. The augmented Lagrangian function is as follows\nF (X,E,U) =\u2016X\u2016\u2217 + \u03bb\u2016E\u201621 + \u2329 U,D\u2212DX\u2212E \u232a + \u03c1\n2 \u2016D\u2212DX\u2212E\u20162F , (28)\nwhere U is the Lagrange multiplier, \u3008\u00b7, \u00b7\u3009 is the inner product, and \u03bb, \u03c1 > 0 are the penalty parameters. The augmented Lagrangian function consists of two parts:\n\u2013 The first part is a smooth and differentiable function f(X,E,U) = \u2329 U,D \u2212\nDX\u2212E \u232a\n+ \u03c12\u2016D\u2212DX\u2212E\u2016 2 F .\n\u2013 The second is a continuous non-smooth functions g(X,E) = \u2016X\u2016\u2217 + \u03bb\u2016E\u201621.\nHence, model (27) is equivalent to following optimization problem\nmin X\u2208M\u2264r\nF (X,E,U) = \u2016X\u2016\u2217+\u03bb\u2016E\u201621+ \u2329 U,D\u2212DX\u2212E \u232a + \u03c1\n2 \u2016D\u2212DX\u2212E\u20162F , (29)\nSinceM\u2264r is a closure of the Riemannian submanifoldMr, it can be guaranteed that (29) has optimal solution. In spite of the non-smooth of setM\u2264r, it has been shown in [22] that the tangent cone of M\u2264r at singular points with rank(X) < r has a rather simple characterization and can be calculated easily (see section2.4). Hence the model (29) can be directly optimized on M\u2264r [32] as section 3.\nSubspace pursuit approach is an elegant optimization method on Riemannian manifold. In order to better optimize model (29) , we propose a fast subspace pursuit method denoted as SP-RPRG(ALM) based on FOA. In the fast subspace pursuit method, while achieving the local optimum (Xrk,Ek) for the problem (29) on M\u2264r (r denotes most rank of Xk, r = 0, 1, \u00b7 \u00b7 \u00b7 . k is the kth alternating iteration, k = 0, 1, \u00b7 \u00b7 \u00b7 .), we will start from the local optimum (Xrk,Ek) as the initial (Xr+l0 ,E0) to warm-start the subsequent problem on low rank matrix varieties M\u2264r+l. Some related ingredients on low rank matrix verities are considered as follows (see section 2.4):\n\u2013 Projection operator PTXM\u2264r (\u00b7) defined by (9); \u2013 Gradient of function f(X,E,U) on low rank matrix verities M\u2264r, i.e.,\ngradf(X,E,U) = PTXM\u2264r (\u2207Xf(X,E,U)),\nwhere \u2207Xf(X,E,U)) = \u2212DTU\u2212 \u03c1DT (D\u2212DTX\u2212E);\n\u2013 Retraction operator RX(\u03be) defined by (11)\nRX(\u03be) = argmin Y\u2208M\u2264r\n\u2016X + \u03be \u2212Y\u20162F = Udiag(\u03c3+)VT\nwhere X + \u03be = Udiag(\u03c3)VT , \u03c3+ is the first min{r, rank(diag(\u03c3))} elements in the vector of \u03c3;\n\u2013 Lifting operator LX(Y) and the vector LX(Y) denoted by (12).\nFor the concrete function F (X,E,U) in (29), solving the problem\nXk = argmin X\u2208M\u2264r F (X,Ek\u22121,Uk\u22121)\n= argmin X\u2208M\u2264r\n\u2016X\u2016\u2217 + \u2329 Uk\u22121,D\u2212DX\u2212Ek\u22121 \u232a + \u03c1\n2 \u2016D\u2212DX\u2212Ek\u22121\u20162F\nAccording to (26), we can obtain\nXk = P\u03b1(Yk,Ek\u22121,Uk\u22121) = U+diag ( max{\u03c3+ \u2212 1/\u03b1, 0} ) VT+. (30)\nwhere U+diag(\u03c3)+V T + = RYk(\u2212gradfX(Yk,Ek)/\u03b1) (see [26]), and P\u03b1(Yk) can be efficiently computed in the sense that RYk(\u2212gradfX(Yk,Ek)/\u03b1) can be cheaply computed without expensive SVDs [33].\nNext, we directly update E for (29) thought fixing the variables X = Xk and U = Uk\u22121, we have\nEk = argmin E\u2208Rm\u00d7n F (Xk,E,Uk\u22121)\n= argmin E\u2208Rm\u00d7n\n\u03bb\u2016E\u201621 + \u2329 Uk\u22121,D\u2212DXk \u2212E \u232a + \u03c1\n2 \u2016D\u2212DXk \u2212E\u20162F ,\nlet W = D\u2212DXk + 1/\u03c1Uk\u22121, then the closed-form solution\nEki = max{\u2016Wi\u20162 \u2212 \u03bb\u03c1 , 0}\n\u2016Wi\u20162 Wi. (31)\nwhere Eki denotes the ith column of Ek, \u2200i [26]. The ALM with fast subspace pursuit on M\u2264r is described in detail in Algorithm 3. And Stopping conditions III. In theory, we stop Algorithm 3 if r > n = rank(D)\u2212 1."}, {"heading": "5 Experimental Results and Analysis", "text": "In this section, we conduct several experiment to assess the proposed algorithms. We focus on the low rank matrix completion problem on a synthetic data and clustering on some public data sets for evaluating the performance of FOA in convergent rate and error rate. All algorithms are coded in Matlab (R2014a) and implement on PC machine installed a 64 bit operating system with an iterl(R) Core (TM) i7 CPU (3.4GHz with single-thread mode) and 4GB memory.\nAlgorithm 3 Fast subspace pursuit based on ALM on M\u2264r for model (27) Input: Parameters \u03bb > 0, \u03c1 > 0, 0 < \u03b1 < 1, \u03b2 > 1, l and k is positive integers. Data matrices\nD \u2208 Rm\u00d7n. 1: Initial X00,E 0 0 and U 0 0 are zero matrices, r = l, r denotes the most rank of current matrix\nXr, t1 = 1. 2: for r = l : l : n do 3: for k = 1, 2, \u00b7 \u00b7 \u00b7 , N2 do 4: for i = 1, 2, \u00b7 \u00b7 \u00b7 , N1 do 5: Fast optimization algorithm\nXri = P\u03b1i (Yi,Ei\u22121), by (30)\nti+1 = 1 +\n\u221a 1 + 4t2i\n2 Yi+1 = RXi ( \u2212 ti \u2212 1 ti+1 S(Xi,Xi\u22121) )\n6: Terminate if stopping conditions I are achieved. 7: Xrk = X r i . 8: end for 9: Compute Ek by (31).\n10: Compute Uk = Uk\u22121 + \u03c1 ( D\u2212DXrk \u2212Ek ) 11: Compute \u03c1 = min(\u03b2\u03c1, 1e5). 12: Terminate if stopping conditions II are achieved. 13: end for 14: end for Output: (Xrk,E r k).\n5.1 Matrix Completion\nLet A \u2208 Rm\u00d7n be a matrix whose elements are known only on a subset \u2126 of the complete set of indexes {1, . . .m} \u00d7 {1, . . . n}. The low-rank matrix completion problem consists of finding the matrix with lowest rank that agrees with A on \u2126. This problem can be consider as optimization with the objective function F (X) = 1/2\u2016P\u2126(X\u2212A)\u20162F , where the P\u2126 denotes projection operator onto \u2126, P\u2126(Xi,j) = Xi,j if (i, j) \u2208 \u2126, otherwise 0.\nFollowing [33], we generate ground-truth low-rank matrices A = LR \u2208 Rm\u00d7n of rank r, where L \u2208 Rm\u00d7r,R \u2208 Rr\u00d7n is rank r matrix generated randomly. The size of the complete set \u2126 is r(m+n\u2212 r)OS with the oversampling factor OS > 2.\nWe test FOA on the matrix completion task and compare their optimization performance against qGeomMC [15], LRGeomCG [33], and LRGeomSD (steepest descent method) on fixed-rank Riemannian manifold.1 Fig.1 and Fig.3 give the needed iteration number while stop condition is satisfied with = 10\u221212 for different matrix size. Fig.2 and Fig.4 give the time cost in the same stop condition for different matrix size. We can find from these figures that qGeomMC need the least iteration number, ours is second. The convergent rate of our algorithm is the fastest. But qGeomMC employ the trust region method which uses the second-order function information. So the proposed FOA has the advantage in\n1 qGeomMC and LRGeomCG are from http://bamdevmishra.com/ codes /fixedranklist/ and http://www.unige.ch/math/vandereycken/matrix completion.html, respectively.\nconvergence rapidity over existing optimization algorithm with first order function information on Riemannian manifold.\n5.2 Clustering on Extended Yale B and COIL-20 Database\nIn this subsection, we apply FOA to model (29) to evaluate its performance in calculating time and clustering error by compared with existing optimization method on Riemannian manifold. The compared algorithm include LRR [12](solving model (27) on euclidean space), SP-RPRG [26](solving model (27) based on low rank matrix varieties) and SP-RPRG(ALM)(solving model (27) based on augmented Lagrange method on low rank matrix vatieties). For SP-RPRG and SP-RPRG(ALM), we give respectively optimal solution by conjugate gradient method and FOA method, and compare their time cost. We set \u03b3(E) = \u2016E\u201621, A(X) = DX, B(E) = E for model (27).\nTwo public databases are used for evaluation. One is the extended Yale B face database which contains 2,414 frontal face images of 38 individuals. The images were captured under different poses and illumination conditions. Fig. 5 shows some sample images. To reduce the computational cost and the memory requirements, all face images are resized from 192\u00d7168 pixels to 48\u00d742 pixels and then vectorized as 2016-dimensional vectors.\nAnother database is Columbia Object Image Library (COIL-20) which includes 1,440 gray-scale images of 20 objects (72 images per object). The objects have a wide variety in geometric and reflectance characteristics. Each image is clipped out from the black background using a rectangular bounding box and resized from 128 \u00d7 128 pixels to 32 \u00d7 32 pixels, then shaped as a 1024-dimensional gray-level intensity feature. Fig. 6 shows some sample images.\nIn the clustering experiments on extended Yale B database, we initialize parameters \u03bb = 0.001, \u03c1 = 0.5 for SP-RPRG(ALM), \u03bb = 0.1 for LRR 2 [12], and \u03bb = 0.01, \u03c1 = 1 for SP-RPRG [26]. The first c = {2, 3, 5, 8, 10} classes are select for experiments, each class contains 64 images.\n2 LRR code is from http://www.cis.pku.edu.cn/faculty/vision/zlin/zlin.htm\nWe compare face clustering error rate for LRR, SP-RPRG and SP-RPRG(ALM), and time cost using our FOA and conjugate gradient (CG) method for SP-RPRG and SP-RPRG (ALM) with different c value. Table 1 gives the experimental results. We can observe from these results that the proposed SP-LRR(ALM) has the lowest error rate for each c value. FOA applied in SP-RPRG and SP-RPRG(ALM) spend less time than conjugate gradient method to achieve the same accuracy.\nIn the clustering experiments on COIL-20 database, we set the initialization parameters \u03bb = 0.001, \u03c1 = 1 for SP-LRR(ALM), \u03bb = 0.1 for LRR and \u03bb = 0.001, \u03c1 = 2 for SP-LRR. In experiment, we randomly select c (ranging from 2 to 11) classes from the 20 classes and 36 samples for each class in the whole COIL-20 databases. For each value c, we run 50 times using different randomly chosen classes and samples. The results are shown in Table 2. From this table we can see that our proposed SP-LRR(ALM) has the lowest average clustering error. FOA applied in SP-LRR and SP-LRR(ALM) spend less time than conjugate gradient method to achieve the same accuracy.This results demonstrates that the SP-LRR(ALM) significantly enhances the accuracy and FOA reduce the time cos during the affinity matrix construction.\n5.3 Clustering on Video Scene Database\nThis experiment test the performance of FOA based on model (29) on video scene database. The compared optimization method and performance are same as previous experiments.\nThe aim of experiment is to segment individual scenes from a video sequence. The video sequences are drawn from two short animations freely available from the Internet Archive 3, this are same as the data used in [28]. The sequence is around 10 seconds in length (approximately 300 frames) and contains three scenes each. To segmented scenes according to significant translation and morphing of objects and sometimes\ncamera or perspective changes, as a result, there are 19 and 24 clips for Video 1 and Video 2 respectively, each clip include 3 scenes. Scene changes (or keyframes) were collected and labeled by hand to form ground truth data. The pre-processing of a sequence consists of converting colour video to grayscale and down sampling to a resolution of 129\u00d796. The video clips were corrupted with various magnitudes of Gaussian noise to evaluate the clustering performance of different methods. Fig.7 show some frames from video 2.\nEach frame of clips is vectorized as Xi \u2208 R12384 and concatenated with consecutive frames to form the samples X \u2208 R12384\u00d7300 for segmentation. The model parameters are set \u03bb = 0.0002 and \u03c1 = 2 in SP-RPRG(ALM), \u03bb = 0.01 for LRR, \u03bb = 0.01 and \u03c1 = 1 for SP-RPRG.\nTABLE 3 and 4 list the segmentation results and average time cost by our FOA and conjugate gradient (CG) method for SP-RPRG and SP-RPRG(ALM). These results are the average segmentation value of all clips for each Video. We can see from these results that SP-RPRG (ALM) has consistently low average error rates and standard deviation with various magnitudes of Gaussian noise. In particular, SP-RPRG (ALM) has not been influenced by Gaussian noise in the video 2. In most case, the FOA has fast convergent rate for SP-RPRG and SP-RPRG(ALM)."}, {"heading": "6 Conclusions", "text": "This paper propose a fast optimization algorithm (FOA) on a Riemannian manifold for a class of composite function, and prove its convergence. Different from\n3 http://archive.org/.\nmost optimization methods on Riemannian manifold, our algorithm use only firstorder function information, but has the convergence rate O(k\u22122). Experiments on some data set show the optimization performance largely outperform existing method in terms of convergence rate and accuracy. Besides, we transform low rank representation model into optimization problem on varieties based on augmented Lagrange approach, then fast subspace pursuit methods based on FOA is applied to solve optimization problem. Extensive experiments results demonstrate the superiority of our proposed ALM with fast subspace purist approach .\nAcknowledgements The research project is supported by the Australian Research Council (ARC) through the grant DP140102270 and also partially supported by National Natural Science Foundation of China under Grant No. 61390510, 61133003, 61370119, 61171169 and 61227004."}, {"heading": "Appendix A", "text": "The requirement that the vector transport Ts is isometric means that, for all X,Y \u2208 M and all \u03beX, \u03b6X \u2208 TXM, the equation \u3008TX\u2192Y\u03beX, TX\u2192Y\u03b6X\u3009 = \u3008\u03beX, \u03b6X\u3009 holds.To prove Theorem 1, we give some lemmas in following.\nLemma 3 Let ak, bk be positive real sequences satisfying\nak \u2212 ak+1 \u2265 bk+1 \u2212 bk,\u2200k \u2265 1, (32)\nwith a1 + b1 \u2264 c, c > 0. Then ak \u2264 c for every k \u2265 1.\nProof Let ck = ak + bk, we can obtain from (32)\nak + bk \u2265 ak+1 + bk+1\ni.e. ck \u2265 ck+1, so ck is a decreasing sequence. Then\nck = ak + bk \u2264 c1 = a1 + b1 \u2264 c, \u2200k \u2265 1.\nSince ak, bk is positive sequences, we can conclude\nak \u2264 c, \u2200 k \u2265 1.\nLemma 4 Let positive sequence tk generated in Algorithm 1 via (22) with t1 = 1. Then tk \u2265 (k + 1)/2 for all k \u2265 1.\nProof 1) When k = 1, t1 = 1 \u2265 1+12 = 1. 2) Suppose that when k = n, the conclusion is correct. Then, while k = n+ 1\ntk+1 = 1 + \u221a 1 + 4t2k 2 \u2265 1 + \u221a 1 + 4(k+12 ) 2 2 = 1 + \u221a 1 + (k + 1)2 2 \u2265 (k + 1 + 1) 2\n2 (33)\nAccording to 1), 2), conclusion is correct.\nLemma 5 For any Y \u2208M, the point X = p\u03b1(Y) is a local minimizer of (16) if and only if there exists \u03b3(Y) \u2208 \u2202g(X), such that [15]\nTY\u2192P\u03b1(Y) ( gradf(Y) + \u03b1LY(P\u03b1(Y) ) + \u03b3(Y) = 0.\nwhere the vector transport Ts is isometric means.\nProof From (16),\nQ\u03b1(X,Y) :=f(Y) + \u2329 gradf(Y), LY(X) \u232a + \u03b1\n2 \u2016LY(X)\u20162Y + g(X) (34)\nis a convex function. From [15], we obtain that the point X = P\u03b1(Y) is a local minimizer of Q\u03b1(X,Y), if and only if there exists \u03b3(Y) \u2208 \u2202g(X), such that\ngrad ( f(Y) + \u3008gradf(Y), LY(X)\u3009+ \u03b1\n2 \u2016LY(X)\u20162Y )\u2223\u2223 X=P\u03b1(Y) + \u03b3(Y) = 0.\nSince\ngrad ( f(Y) + \u3008gradf(Y), LY(X)\u3009+ \u03b1\n2 \u2016LY(X)\u20162Y )\u2223\u2223 X=p\u03b1(Y)\n=TY\u2192P\u03b1(Y)(gradf(Y) + \u03b1LY(P\u03b1(Y))),\nthe conclusion is right.\nLemma 6 Let Yk \u2208M, k = 1, 2, \u00b7 \u00b7 \u00b7 and \u03b1 > 0 be such that\nF (P\u03b1(Yk)) \u2264 Q\u03b1(P\u03b1(Yk),Yk),\nThen for any X \u2208M\nF (X)\u2212 F (P\u03b1(Y)) \u2265 \u03b1\n2 \u2016LY(P\u03b1(Y))\u20162Y \u2212 \u03b1\u3008LY(P\u03b1(Y)), LY(X)\u3009\nProof From (21), (14), we have\nF (X)\u2212 F (P\u03b1(Y)) \u2265F (X)\u2212Q(P\u03b1(Y),Y) \u2265f(Y) + \u3008gradf(Y), LY(X)\u3009+ g(P\u03b1(Y)) + \u3008\u03b3(Y), LP\u03b1(Y)(X)\u3009\n\u2212 f(Y)\u2212 \u3008gradf(Y), LY(P\u03b1(Y))\u3009 \u2212 \u03b1\n2 \u3008LY(P\u03b1(Y)), LY(P\u03b1(Y))\u3009 \u2212 g(P\u03b1(Y))\n=\u3008gradf(X), LY(X)\u2212 LY(P\u03b1(Y))\u3009+ \u3008LP\u03b1(Y)(Y),\n\u03b3(Y)\u3009 \u2212 \u03b1 2 \u3008LY(P\u03b1(Y)), LY(P\u03b1(Y))\u3009\n=\u3008gradf(X), LY(X)\u2212 LY(P\u03b1(Y))\u3009 \u2212 \u3008LP\u03b1(Y)(Y), TY\u2192P\u03b1(Y)(gradf(Y)\n+ \u03b1LY(P\u03b1(Y)))\u3009 \u2212 \u03b1\n2 \u3008LY(P\u03b1(Y)), LY(P\u03b1(Y))\u3009)\n=\u3008gradf(X), LY(X)\u2212 LY(P\u03b1(Y))\u3009 \u2212 \u3008LY(X)\u2212 LY(P\u03b1(Y)), gradf(Y)\n+ \u03b1LY(P\u03b1(Y))\u3009 \u2212 \u03b1\n2 \u3008LY(P\u03b1(Y)), LY(P\u03b1(Y))\u3009)\n= \u03b1\n2 \u2016LY(P\u03b1(Y))\u20162Y \u2212 \u03b1\u3008LY(P\u03b1(Y)), LY(X)\u3009\nLemma 7 The sequences {Xk,Yk} generated via Algorithm 1 satisfy for every k \u2265 1\n2\n\u03b1k t2kvk \u2212\n2\n\u03b1k + 1 t2k+1vk+1 \u2264 \u2016Uk+1\u20162Xk+1 \u2212 \u2016Uk\u2016 2 Xk (35)\nwhere\nvk := F (Xk)\u2212 F (X\u2217) Uk+1 := \u2016 \u2212 (tk+1 \u2212 1)LXk+1(Xk)\u2212 LXk+1(X\u2217)\u2016 2 Xk+1 .\nProof Frist we apply Lemma (6) at the points X := Xk,Y := Yk+1 with \u03b1 = \u03b1k+1, and likewise at the points X := X\u2217,Y := Yk+1 to get\n2\u03b1\u22121k+1(vk \u2212 vk+1)\n\u2265\u2016LY(P\u03b1(Y))\u20162Y \u2212 2\u3008LYk+1(Xk+1), LYk+1(Xk)\u3009 \u2212 2\u03b1 \u22121 k+1vk+1 \u2265\u2016LY(P\u03b1(Y))\u20162Y \u2212 2\u3008LYk+1(Xk+1), LYk+1(X\u2217)\u3009\nwhere we used the fact that Xk+1 = P\u03b1k+1(Yk+1). Now we multiply the first inequality above by tk+1 \u2212 1 and add it to the second inequality:\n2\n\u03b1k+1 ((tk+1 \u2212 1)vk \u2212 tk+1vk+1)\n\u2265tk+1\u3008LYk+1(Xk+1), LYk+1(Xk+1)\u3009 \u2212 2\u3008LYk+1(Xk+1), (tk+1 \u2212 1)LYk+1(Xk) + LYk+1(X\u2217)\u3009\nMultiplying the last inequality by tk+1 and using the relation t 2 k = t 2 k+1 \u2212 tk+1, we obtain\n2\n\u03b1k+1 (t2kvk \u2212 t2k+1vk+1)\n\u2265t2k+1\u3008LYk+1(Xk+1), LYk+1(Xk+1)\u3009 \u2212 2\u3008tk+1LYk+1(Xk+1), (tk+1 \u2212 1)LYk+1(Xk) + LYk+1(X\u2217)\u3009 =\u2016tk+1LYk+1(Xk+1)\u2212 (tk+1 \u2212 1)LYk+1(Xk)\u2212 LYk+1(X\u2217)\u2016 2 Yk+1\n\u2212 \u2016 \u2212 (tk+1 \u2212 1)LYk+1(Xk)\u2212 LYk+1(X\u2217)\u2016 2 Yk+1\nHere the vector transport Ts is isometric means, Therefore,\nUk+1 = \u2016tk+1LYk+1(Xk+1)\u2212 (tk+1 \u2212 1)LYk+1(Xk)\u2212 LYk+1(X\u2217)\u2016 2 Xk+1\n= \u2016 \u2212 (tk+1 \u2212 1)LXk+1(Xk)\u2212 LXk+1(X\u2217)\u2016 2 Xk+1\nwith LXk(Yk+1) = \u2212((tk \u2212 1)/tk+1)LXk(Xk\u22121),\nUk = \u2016 \u2212 (tk+1 \u2212 1)LYk+1(Xk)\u2212 LYk+1(X\u2217)\u2016 2 Xk+1\n= \u2016(tk+1 \u2212 1)LXk(Yk+1) + LXk(Yk+1)\u2212 LXk(X\u2217)\u2016 2 Xk = \u2016 \u2212 (tk \u2212 1)LXk(Xk\u22121)\u2212 LXk(X\u2217)\u2016 2 Xk\nand with \u03b1k+1 \u2264 \u03b1k, yields\n2\n\u03b1k t2kvk \u2212\n2\n\u03b1k t2k+1vk+1 \u2265 Uk+1 \u2212Uk.\nNow we prove the promised improved complexity result of Theorem 1.\nProof Define the quantities\nak := 2\n\u03b1k t2kvk, bk := Uk, c := \u2016LX0(X\u2217)\u2016 2 X0 = \u2016LY1(X\u2217)\u2016 2 X0\nand recall Lemma (3) that vk = F (Xk)\u2212 F (X\u2217). Then,\nak \u2212 ak+1 \u2265 bk+1 \u2212 bk\nand t1 = 1, applying Lemma (6) to the points X := X\u2217, Y := Y1 with \u03b1 = \u03b11, we get\nF (X\u2217)\u2212 F (p(Y1))\n\u2265 \u03b11 2 \u3008LY1(P\u03b1(Y1)), LY1(P\u03b1(Y1))\u3009 \u2212 \u03b11\u3008LY1(X\u2217), LY1(P\u03b1(Y1))\u3009\nThen\na1 + b1 = 2\n\u03b11 v1 + \u2016LX1(X\u2217)\u2016 2 X1\n= 2\n\u03b11 (F (X1)\u2212 F (X\u2217)) + \u2016LX1(X\u2217)\u2016 2 X1\n= \u2212 2 \u03b11 (F (X\u2217)\u2212 F (P\u03b1(Y1)) + \u2016LX1(X\u2217)\u2016 2 X1 (17)\n\u2264 \u2212(\u2016LY1(P\u03b1(Y1))\u2016 2 Y1 \u2212 2\u3008LY1(X\u2217), LY1(P\u03b1(Y1))\u3009) + \u2016LX1(X\u2217)\u2016 2 X1\n= \u2212\u2016LY1(X1)\u2212 LY1(X\u2217)\u2016 2 Y1 + \u2016LY1(X\u2217)\u2016 2 Y1 + \u2016LX1(X\u2217)\u2016 2 X1 = \u2016LY1(X\u2217)\u2016 2 Y1 = c.\nBy Lemma (3) we have for every k \u2265 1,\nak = 2\n\u03b1k t2kvk \u2264 \u2016LX0(X\u2217)\u2016 2 X0\nInvoking Lemma (4), tk \u2265 (k + 1)/2, \u03b1k \u2264 \u03b7L(f), we obtain that\nF (Xk)\u2212 F (X\u2217) \u2264 2\u03b7L(f)\u2016LX0(X\u2217)\u20162X0\n(k + 1)2"}], "references": [{"title": "Trust-region methods on riemannian manifolds", "author": ["P.A. Absil", "C. Baker", "K.A. Gallivan"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Optimization Algorithm on Matrix Manifolds", "author": ["P.A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Kdd cup and workshop", "author": ["J. Bennett", "C. Elkan", "B. Liu", "P. Smyth", "D. Tikk"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "RTRMC: A riemannian trust-region method for low-rank matrix completion", "author": ["N. Boumal", "P.A. Absil"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Minimizing a differentiable function over a differential manifold", "author": ["D. Gabay"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Extrinsic methods for coding and dictionary learning on", "author": ["M. Harandi", "R. Hartley", "C. Shen", "B. Lovell", "C. Sanderson"], "venue": "Grassmann manifolds,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A riemannian symmetric rank-one trust-region method", "author": ["W. Huang", "P.A. Absil", "K.A. Gallivan"], "venue": "Mathematical Programming,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "An accelerated gradient method for trace norm minimization", "author": ["S. Ji", "J. Ye"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Introduction to smooth manifolds, Springer New York", "author": ["J.M. Lee"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Linearized alternating direction method with adaptive penalty for low-rank representation", "author": ["Z. Lin", "R. Liu", "Z. Su"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["G. Liu", "Z. Lin", "S. Yan", "J. Sun", "Y. Yu", "Y. Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["G. Liu", "Z. Lin", "Y. Yu"], "venue": "Pro- ceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "A riemannian geometry for low-rank matrix com- pletion", "author": ["B. Mishra", "K. Apuroop", "R. Sepulchre"], "venue": "arXiv preprint arXiv:1211.1550", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Low-rank optimization with trace norm penalty", "author": ["B. Mishra", "G. Meyer", "F. Bach", "R. Sepulchre"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Fixed-rank matrix factorizations and riemannian low-rank optimization", "author": ["B. Mishra", "G. Meyer", "S. Bonnabel", "R. Sepulchre"], "venue": "Computational Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A method for solving the convex programming problem with convergence rate O(1/k2)", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1983}, {"title": "Gradient methods for minimizing composite functions", "author": ["Y. Nesterov"], "venue": "Mathematical Pro- gramming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Scaled gradients on grassmann manifolds for matrix completion", "author": ["T. Ngo", "Y. Saad"], "venue": "Ad- vances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": "SIAM Review,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Optimization methods on riemannian manifolds and their application to shape space", "author": ["W. Ring", "B. Wirth"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Convergence results for projected line-search methods on varieties of low-rank matrices via lojasiewicz inequality", "author": ["R. Schneider", "A. Uschmajew"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Geometric optimization methods for adaptive filtering, Division of applied sciences", "author": ["S.T. Smith"], "venue": "Harvard University,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1993}, {"title": "Rate-invariant analysis of trajectories on riemannian manifolds with application in visual speech recognition", "author": ["J. Su", "A. Srivastava", "F. de Souza", "S. Sarkar"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Scalable nuclear-norm minimization by subspace pursuit proximal riemannian gradient", "author": ["M. Tan", "J. Shi", "J. Gao", "A. Hengel", "D. Xu", "S. Xiao"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Riemannian pursuit for big matrix recovery", "author": ["M. Tan", "I. Tsang", "L. Wang", "B. Vandereycken", "S. Pan"], "venue": "Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Subspace clutering for sequential data", "author": ["S. Tierney", "J. Gao", "Y. Guo"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems, Pacific", "author": ["K. Toh", "S. Yun"], "venue": "Journal of Optimization", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Statistical computations on grassmann and stiefel manifolds for image and video-based recognition", "author": ["P. Turaga", "A. Veeraraghavan", "A. Srivastava", "R. Chellappa"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Convex functions and optimization methods on Riemannian manifolds, Springer Science & Business Media", "author": ["C. Udriste"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1994}, {"title": "Vandereycken,B., Line-search methods and rank increase on low-rank matrix varieties", "author": ["A. Uschmajew"], "venue": "Proceedings of the 2014 International Symposium on Nonlinear Theory and its Applications,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Low-rank matrix completion by riemannian optimization", "author": ["B. Vandereycken"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "The Riemannian optimization have been successfully applied in machine learning, computer vision and data mining tasks, including fixed low rank optimization [5, 33], Riemannian dictionary learning [7], and computer vision [?, 30, 34], and tensor clustering [25].", "startOffset": 157, "endOffset": 164}, {"referenceID": 31, "context": "The Riemannian optimization have been successfully applied in machine learning, computer vision and data mining tasks, including fixed low rank optimization [5, 33], Riemannian dictionary learning [7], and computer vision [?, 30, 34], and tensor clustering [25].", "startOffset": 157, "endOffset": 164}, {"referenceID": 6, "context": "The Riemannian optimization have been successfully applied in machine learning, computer vision and data mining tasks, including fixed low rank optimization [5, 33], Riemannian dictionary learning [7], and computer vision [?, 30, 34], and tensor clustering [25].", "startOffset": 197, "endOffset": 200}, {"referenceID": 1, "context": "Methods of solving minimization problems on Riemannian manifolds have been extensively researched [2, 31].", "startOffset": 98, "endOffset": 105}, {"referenceID": 29, "context": "Methods of solving minimization problems on Riemannian manifolds have been extensively researched [2, 31].", "startOffset": 98, "endOffset": 105}, {"referenceID": 5, "context": "As one of fundamental optimization algorithms, the steepest descent method was first proposed for optimization on Riemmanian manifolds in [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "In contrast, the Newton\u2019s method [6] and the BFGS quasi-Newton scheme (BFGS rank2-update) [21] have higher convergence rate, however, in practical applications, using the full second-order Hessian information is computationally prohibitive.", "startOffset": 33, "endOffset": 36}, {"referenceID": 20, "context": "In contrast, the Newton\u2019s method [6] and the BFGS quasi-Newton scheme (BFGS rank2-update) [21] have higher convergence rate, however, in practical applications, using the full second-order Hessian information is computationally prohibitive.", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "[1] proposed the Trust-region method on Riemannian manifolds.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "For example, the Trust-region method has been applied the optimization problem on Grassmann manifold for the matrix completion problem [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 22, "context": "Each iteration of the trust-region method involves solving the Riemannian Newton equation [23], which increases the complexity of the algorithm.", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "[8] generalized symmetric rank-one trust-region method to a vector variable optimization problem on a d-dimensional Riemannian manifold, where an approximated Hessian matrix was generated by using the symmetric rank-one update without solving Riemannian Newton equation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "FletcherReeves conjugate gradient method on Riemannian manifolds [21] is a typical type of methods which need only the first-order function information, and its convergence is superlinear but lower than the desired 2-order speed.", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "based on linear search [17].", "startOffset": 23, "endOffset": 27}, {"referenceID": 2, "context": "More strategies have been proposed by considering special structures of objectives, such as composite objective functions [3,18,29].", "startOffset": 122, "endOffset": 131}, {"referenceID": 17, "context": "More strategies have been proposed by considering special structures of objectives, such as composite objective functions [3,18,29].", "startOffset": 122, "endOffset": 131}, {"referenceID": 27, "context": "More strategies have been proposed by considering special structures of objectives, such as composite objective functions [3,18,29].", "startOffset": 122, "endOffset": 131}, {"referenceID": 13, "context": "The low-rank learning has been widely applied in the low-rank matrix recovery (MR)/low-rank matrix completion [14\u201316] and low-rank representation (LRR) for subspace clustering [11\u201313].", "startOffset": 110, "endOffset": 117}, {"referenceID": 14, "context": "The low-rank learning has been widely applied in the low-rank matrix recovery (MR)/low-rank matrix completion [14\u201316] and low-rank representation (LRR) for subspace clustering [11\u201313].", "startOffset": 110, "endOffset": 117}, {"referenceID": 15, "context": "The low-rank learning has been widely applied in the low-rank matrix recovery (MR)/low-rank matrix completion [14\u201316] and low-rank representation (LRR) for subspace clustering [11\u201313].", "startOffset": 110, "endOffset": 117}, {"referenceID": 10, "context": "The low-rank learning has been widely applied in the low-rank matrix recovery (MR)/low-rank matrix completion [14\u201316] and low-rank representation (LRR) for subspace clustering [11\u201313].", "startOffset": 176, "endOffset": 183}, {"referenceID": 11, "context": "The low-rank learning has been widely applied in the low-rank matrix recovery (MR)/low-rank matrix completion [14\u201316] and low-rank representation (LRR) for subspace clustering [11\u201313].", "startOffset": 176, "endOffset": 183}, {"referenceID": 12, "context": "The low-rank learning has been widely applied in the low-rank matrix recovery (MR)/low-rank matrix completion [14\u201316] and low-rank representation (LRR) for subspace clustering [11\u201313].", "startOffset": 176, "endOffset": 183}, {"referenceID": 3, "context": "One of low-rank matrix completion application examples is the Netflix problem [4], in which one would like to recover a low-rank matrix from a sparse sampled matrix entries.", "startOffset": 78, "endOffset": 81}, {"referenceID": 19, "context": "However, the lowest rank optimization problems are NP hard and generally extremely hard to solve (and also hard to approximate [20]).", "startOffset": 127, "endOffset": 131}, {"referenceID": 1, "context": "As the set of rank-r matrix Mr is a smooth Riemannian manifold under an appropriately chosen metric [2, 14, 16, 33], one can convert a rank constrained optimization problem to an unconstrained optimization problem on the low rank matrix manifold.", "startOffset": 100, "endOffset": 115}, {"referenceID": 13, "context": "As the set of rank-r matrix Mr is a smooth Riemannian manifold under an appropriately chosen metric [2, 14, 16, 33], one can convert a rank constrained optimization problem to an unconstrained optimization problem on the low rank matrix manifold.", "startOffset": 100, "endOffset": 115}, {"referenceID": 15, "context": "As the set of rank-r matrix Mr is a smooth Riemannian manifold under an appropriately chosen metric [2, 14, 16, 33], one can convert a rank constrained optimization problem to an unconstrained optimization problem on the low rank matrix manifold.", "startOffset": 100, "endOffset": 115}, {"referenceID": 31, "context": "As the set of rank-r matrix Mr is a smooth Riemannian manifold under an appropriately chosen metric [2, 14, 16, 33], one can convert a rank constrained optimization problem to an unconstrained optimization problem on the low rank matrix manifold.", "startOffset": 100, "endOffset": 115}, {"referenceID": 31, "context": "Vandereycken [33] considered this problem as an optimization problem on fixed-rank matrix Riemannian manifold.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "[14, 16] proposed a Riemannian quotient manifold for low-rank matrix completion.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[14, 16] proposed a Riemannian quotient manifold for low-rank matrix completion.", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "[19] addressed this problem based on a scaled metric on the Grassmann manifold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] proposed a pursuit algorithm that alternates between fixed-rank optimization and rank-one updates to search the best rank value.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] proposed a Riemannian pursuit approach which converts low-rank problem into a series of fixed rank problems, and further confirmed that low-rank problem can be considered as optimization whose search space is varieties of low-rank matrices M\u2264r [26], see (7), using the subspace pursuit approach on Riemannian manifold to look for the desired rank value r.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27] proposed a Riemannian pursuit approach which converts low-rank problem into a series of fixed rank problems, and further confirmed that low-rank problem can be considered as optimization whose search space is varieties of low-rank matrices M\u2264r [26], see (7), using the subspace pursuit approach on Riemannian manifold to look for the desired rank value r.", "startOffset": 249, "endOffset": 253}, {"referenceID": 9, "context": "A manifold M of dimension m [10] is a topological space that locally resembles a Euclidean space R in a neighbourhood of each point X \u2208M.", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "A geodesic \u03b3 : [0, 1] \u2192 M is a smooth curve with a vanishing covariant derivative of its tangent vector field, and in particular, the Riemmannian distance between two points Xi,Xj \u2208 M is the shortest smooth path connecting them on the manifold, that is the infimum of the lengths of all paths joining Xi and Xj .", "startOffset": 15, "endOffset": 21}, {"referenceID": 31, "context": "It can be proved that Mr is a Riemannian manifold of dimension m+ n\u2212 r [33].", "startOffset": 71, "endOffset": 75}, {"referenceID": 21, "context": "At a singular point X where rank(X)= s < r, we have to use search directions in the tangent cone (instead of tangent space), The tangent cones of M\u2264r are explicitly known [22], TXM\u2264r = TXMs \u2295 {\u039er\u2212s \u2208 U \u2297 V}, (8) where U = rang(X) and V = rang(X), X = U\u03a3V, rank(X) = s < r.", "startOffset": 171, "endOffset": 175}, {"referenceID": 24, "context": "Moreover, \u039er\u2212s can be efficiently computed with the same complexity as on Mr in [26].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "We consider the following general model (13) on Riemannian manifold which also naturally extends the problem formulation in [3]", "startOffset": 124, "endOffset": 127}, {"referenceID": 24, "context": "Here we adopt the following proximal Riemannian gradient (PRG) method [26] to update X.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "Note the above local model is different from that on vector spaces [18, 29].", "startOffset": 67, "endOffset": 75}, {"referenceID": 27, "context": "Note the above local model is different from that on vector spaces [18, 29].", "startOffset": 67, "endOffset": 75}, {"referenceID": 1, "context": "This conclusion has been shown in [2] when g(X) = 0.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "In order to obtain linear search with quadratic convergence, we propose a fast optimization algorithm (FOA) which extends the acceleration methods in [3,9,29] onto Riemannian manifold.", "startOffset": 150, "endOffset": 158}, {"referenceID": 8, "context": "In order to obtain linear search with quadratic convergence, we propose a fast optimization algorithm (FOA) which extends the acceleration methods in [3,9,29] onto Riemannian manifold.", "startOffset": 150, "endOffset": 158}, {"referenceID": 27, "context": "In order to obtain linear search with quadratic convergence, we propose a fast optimization algorithm (FOA) which extends the acceleration methods in [3,9,29] onto Riemannian manifold.", "startOffset": 150, "endOffset": 158}, {"referenceID": 1, "context": "Lemma 1 (Optimality Condition) A point X\u2217 \u2208 M is a local minimizer of (13) if and only if there exists \u03b7 \u2208 \u2202g(X) such that [2]", "startOffset": 123, "endOffset": 126}, {"referenceID": 10, "context": "In order to solve the problem (23) following [11], we can optimize the variables X and E by alternating direction approach.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "In spite of the non-smooth of setM\u2264r, it has been shown in [22] that the tangent cone of M\u2264r at singular points with rank(X) < r has a rather simple characterization and can be calculated easily (see section2.", "startOffset": 59, "endOffset": 63}, {"referenceID": 30, "context": "Hence the model (29) can be directly optimized on M\u2264r [32] as section 3.", "startOffset": 54, "endOffset": 58}, {"referenceID": 24, "context": "(30) where U+diag(\u03c3)+V T + = RYk(\u2212gradfX(Yk,Ek)/\u03b1) (see [26]), and P\u03b1(Yk) can be efficiently computed in the sense that RYk(\u2212gradfX(Yk,Ek)/\u03b1) can be cheaply computed without expensive SVDs [33].", "startOffset": 56, "endOffset": 60}, {"referenceID": 31, "context": "(30) where U+diag(\u03c3)+V T + = RYk(\u2212gradfX(Yk,Ek)/\u03b1) (see [26]), and P\u03b1(Yk) can be efficiently computed in the sense that RYk(\u2212gradfX(Yk,Ek)/\u03b1) can be cheaply computed without expensive SVDs [33].", "startOffset": 189, "endOffset": 193}, {"referenceID": 24, "context": "where Eki denotes the ith column of Ek, \u2200i [26].", "startOffset": 43, "endOffset": 47}, {"referenceID": 31, "context": "Following [33], we generate ground-truth low-rank matrices A = LR \u2208 Rm\u00d7n of rank r, where L \u2208 Rm\u00d7r,R \u2208 Rr\u00d7n is rank r matrix generated randomly.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "We test FOA on the matrix completion task and compare their optimization performance against qGeomMC [15], LRGeomCG [33], and LRGeomSD (steepest descent method) on fixed-rank Riemannian manifold.", "startOffset": 101, "endOffset": 105}, {"referenceID": 31, "context": "We test FOA on the matrix completion task and compare their optimization performance against qGeomMC [15], LRGeomCG [33], and LRGeomSD (steepest descent method) on fixed-rank Riemannian manifold.", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "The compared algorithm include LRR [12](solving model (27) on euclidean space), SP-RPRG [26](solving model (27) based on low rank matrix varieties) and SP-RPRG(ALM)(solving model (27) based on augmented Lagrange method on low rank matrix vatieties).", "startOffset": 35, "endOffset": 39}, {"referenceID": 24, "context": "The compared algorithm include LRR [12](solving model (27) on euclidean space), SP-RPRG [26](solving model (27) based on low rank matrix varieties) and SP-RPRG(ALM)(solving model (27) based on augmented Lagrange method on low rank matrix vatieties).", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "1 for LRR 2 [12], and \u03bb = 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 24, "context": "01, \u03c1 = 1 for SP-RPRG [26].", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "The video sequences are drawn from two short animations freely available from the Internet Archive , this are same as the data used in [28].", "startOffset": 135, "endOffset": 139}], "year": 2015, "abstractText": "The paper addresses the problem of optimizing a class of composite functions on Riemannian manifolds and a new first order optimization algorithm (FOA) with a fast convergence rate is proposed. Through the theoretical analysis for FOA, it has been proved that the algorithm has quadratic convergence. The experiments in the matrix completion task show that FOA has better performance than other first order optimization methods on Riemannian manifolds. A fast subspace pursuit method based on FOA is proposed to solve the low-rank representation model based on augmented Lagrange method on the low rank matrix variety. Experimental results on synthetic and real data sets are presented to demonstrate that both FOA and SP-RPRG(ALM) can achieve superior performance in terms of faster convergence and higher accuracy.", "creator": "LaTeX with hyperref package"}}}