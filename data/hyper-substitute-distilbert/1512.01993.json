{"id": "1512.01993", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "A Novel Approach to Distributed Multi-Class SVM", "abstract": "with web structures constantly expanding, machines with neural tree learning algorithms regularly analyze fewer data having larger and larger amounts of prediction time plus storage space, the need to duplicate maintenance and processing requirements among several computers has become apparent. although substantial attempts have been done in arranging distributed network svm algorithms by processor - class svm tables individually, the field topology multi - object distributed svms occupies largely unexplored. this research proposes proposed novel algorithm that implements the bus driver machine over http multi - class setting and feels efficient in a sequential heap ( here, click ). different idea assumes to blend the dataset into simpler sections and thus assign the smallest availability vector machine for this optimization of immediate coding phase, therefore like a divide that conquer protocol. while testing, weak structure has somehow effectively exploited to entirely reduce the prediction time. our design has shown better computation time during the prediction phase than regular traditional sequential svm variants ( one lt. small, one vs. zero ) and fail - performs uniformly as auto products of the dataset fit. no approach also classifies signal processing with smaller flexibility than the normal multi - class structures.", "histories": [["v1", "Mon, 7 Dec 2015 11:44:35 GMT  (812kb)", "http://arxiv.org/abs/1512.01993v1", "8 Pages"]], "COMMENTS": "8 Pages", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["aruna govada", "shree ranjani", "aditi viswanathan", "s k sahay"], "accepted": false, "id": "1512.01993"}, "pdf": {"name": "1512.01993.pdf", "metadata": {"source": "CRF", "title": "A Novel Approach to Distributed Multi-Class SVM", "authors": ["Aruna Govada", "Shree Ranjani", "Aditi Viswanathan"], "emails": ["garuna@goa.bits-pilani.ac.in"], "sections": [{"heading": "1. Introduction and Related Work", "text": "In the machine learning world, SVMs offer one of the most accurate results. SVMs are accurate because of their high generalization property to classify unknown examples. Yet SVM algorithms have been largely restricted to simple 2-class (binary) classification problems. However, numerous practical applications involve multi-class classifications - like identifying the galaxy that a star belongs to, remote sensing applications, etc. Some of the most used multi-class SVM approaches include One vs One, One vs Rest, DAG and Error correcting codes (all of which have their own drawbacks and are not as efficient as binary SVM algorithms). In One vs. Rest classification, the n-class problem is converted into n 2-class sub problems with one positive class and (n-1) negative classes. In One vs. Rest classification, the n-class problem is converted into n(n-1)/2 two-class problems. Krebel [1] showed that by this formulation, unclassifiable regions reduce, but still they remain. To solve the problem of unclassifiable regions, Taylor et al. [2] proposed decision-tree based pairwise classification Graph. Pontil et al. [3] proposed to use rules of a tennis tournament to solve unclassified regions. Kiksirikul et al. [4] proposed the same method and called it Adaptive Directed Acyclic Graph. A comparison of these approaches [5] suggest the usefulness of One vs One in terms of accuracy and computation and this is why we have chosen to compare our approach with this.\nFor both binary SVMs as well as multi class SVMs, in the recent years, handling large datasets has become an arduous task. Data Scientists are overwhelmed with the amount of data and the need for excessive data preprocessing that this explosion has caused. Given that data handling has become tough, data mining \u2013 the process of discovering new patterns from large data datasets \u2013 is a herculean task. This has given rise to scientists developing distributed parallel algorithms to meet the scalability and performance requirements for big data. Computation time and computation complexity (which involves solving the quadratic optimization problem) has been a limiting factor for SVMs especially for large data sets. To overcome this, many parallel and distributed SVMs were proposed. Initially most of the parallel SVM was based on MPI programming model. Moving from the MPI programming model based parallel SVM, parallelization has been achieved through the MapReduce Framework now. Fox [6] developed parallel SVM based on iterative MapReduce model Twister. A parallelization scheme was proposed where the kernel matrix is approximated by a block-diagonal approach [7]. Further improvements to parallel SVM implementations like Cascade SVMs [8] have been proposed which heavily reduce the communication overhead among the computers. In this method, dataset is split into parts in feature space. Non-support vectors of each sub dataset are filtered and only support vectors are transmitted. Collobert et al. [9] proposed a new parallel SVM training and classification algorithm that each subset of a dataset is trained with SVM and then the classifiers are combined into a final single classifier function. Lu et al. [10] proposed a connected network based distributed support vector machine algorithm. In this method, the dataset is split into roughly equal part for each computer in a network then, support vectors are exchanged among these computers. Sun et al. proposed a novel method for parallelized SVM based on MapReduce technique. This method is based on the cascade SVM model. Their approach is based on iterative MapReduce model Twister which is different from our implementation which is a recursive MapReduce algorithm. Ferhat et al. [11] proposed a novel MapReduce based binary SVM training method in which the whole training dataset is distributed over data nodes of cloud computing system using Hadoop streaming and MRjob python library. Despite such extensive work on multi class SVMs as well as distributed binary SVMs, the arena of multi class distributed SVMs has remained largely unexplored. In this paper, we propose a novel algorithm for distributed multi class SVMs and have compared our results with the most popular multi class SVM approaches (One vs. One and One vs. Rest)"}, {"heading": "2. Proposed Framework", "text": "The proposed algorithm is based on binary tree kind of structure created during the training phase. Our algorithm aims to reduce the total number of SVMs required to classify a data point, thus enabling better efficiency during run-time of the model that was built out of our algorithm. While One vs. One, One vs. Rest and DAGSVM classify using ! !!! ! , n, and ! !!! !\nSVMs respectively, we use \ud835\udc59\ud835\udc5c\ud835\udc542(\ud835\udc5b + 1) SVMs to classify the data point at run-time. One possible structure that can be obtained in depicted in Figure II. It is critical to choose the most appropriate combination to obtain the most optimal case while testing for a new sample data point. For this, we have separated the training stage into 2 significant phases where the first stage (Training) is devoted to compute all possible support vectors and the second stage (Cross Validation) evaluates all of them and returns the best division."}, {"heading": "2.1 Training", "text": "Given N classes, we partition the entire dataset into 2 halves each containing [n/2] classes using support vectors. This is done neglecting the differences among the classes on one side. Without loss of generality, one half has been assigned as positive class and the other negative class. To generate support vectors, Atbrox\u2019s [12] method for parallel machine learning has been used which gave us the mapper and reducer implementation for binary classification. Atbrox\u2019s method implements incremental SVM algorithm for binary classification as described below:\nThe SVM classifier solves the following problem of finding w,y i.e. the coefficients of the support vector formulated as Where I \u2013 identity matrix \u00b5 - parameter >0 E = [A \u2013e] D \u2013 Diagonal matrix with plus ones or minus ones To classify a test sample with feature vector x, following equation is used. Where A+ and A- denote the positive and negative classes respectively. Mappers and reducers have been used to parallelize the calculation of ETE and ETDe and Figure I. depicts a brief outline of the algorithm which explains the function of each mapper and reducer used in this approach. As at any point, binary classification is performed where each class represents many, mappers and reducers from Atbrox have been modified to suit our purpose. A single run of the training stage is as follows:\n\u2022 Divide the dataset into 2 regions using (nCn/2) / 2 planes where \u2018n\u2019 stands for the number of classes in the dataset. This figure is arrived based on the intuition that a plane divides the data points into roughly half the number of classes on each side. i.e choose n/2 out of n and and to avoid repeated counting , the number of possible combinations was divided by 2.\n\u2022 For all possible combinations support vectors are formed."}, {"heading": "2.2 Cross Validation", "text": "This stage of training primarily involves identifying the best plane from the possible options obtained from previous stage. A single run of the second stage is as follows.\n\u2022 For each of the partitions thus obtained, accuracy with which each plane divides is calculated using the classification accuracy metric ((true positives + true negatives)/total samples).\n\u2022 Mappers split the task of obtaining the confusion matrix (The matrix which contains true positives, true negatives, false positives, false negatives). Reducers assimilate the values in the confusion matrix from each node and compute the classification accuracy metric. This metric is used to identify the best split and store the 2 separated lists of classes for further computation.\n\u2022 At the end of this second stage, we obtain a set of positive and negative classes along with their corresponding accuracy calculation.\nBoth the stages are repeated until the number of classes in the positive and negative become one, which effectively means that the dataset has been successfully divided into all N classes."}, {"heading": "2.3 Testing", "text": "The classification of a test sample starts at the root of the tree. At each node of the binary tree a decision is being made about the assignment of the input pattern into one of the two possible groups obtained after the training phase. Each of these groups may contain multiple classes. This is repeated recursively downward the tree until the sample\n\ud835\udc60\ud835\udc54\ud835\udc5b \u00a0(\ud835\udc65T\ud835\udc64 \u00a0\u2013  \u00a0\ud835\udc66)  \u00a0=  \u00a0  \u00a01, \ud835\udc65 \u2208 \ud835\udc34 +\u22121, \ud835\udc65 \u2208 \ud835\udc34 \u2212 \u00a0 \u00a0\n \u00a0\n(\ud835\udc64, \ud835\udc66)  \u00a0=  \u00a0 ( \u00a0\ud835\udc3c/\u00b5\u03bc \u00a0 +  \u00a0\ud835\udc38T\ud835\udc38)\u00af\u02c9\u00b9 \u00a0\ud835\udc38T\ud835\udc37\ud835\udc52 \u00a0\nreaches a leaf node that represents the class it has been assigned to (Figure II). Any test sample will go through a maximum of \ud835\udc59\ud835\udc5c\ud835\udc542\ud835\udc41 SVMs during the test phase.\nSTEP \u00a01: \u00a0Training \u00a0of \u00a0SVMs \u00a0with \u00a0the \u00a0Training \u00a0Dataset. \u00a0 Output \u00a0 \u2013 \u00a0 1 \u00a0 SVM \u00a0 is \u00a0 learned \u00a0 for \u00a0 each \u00a0 of \u00a0 the \u00a0 combinations \u00a0 listed \u00a0in \u00a0(i) \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0  \u00a0 ..\u2026\u2026\u2026..(ii) \u00a0 Refer \u00a0to \u00a0(A) \u00a0below \u00a0for \u00a0details \u00a0  \u00a0\nSTEP \u00a02: \u00a0Cross \u00a0Validation \u00a0of \u00a0SVMs \u00a0with \u00a0the \u00a0Cross \u00a0Validation \u00a0 Dataset. \u00a0 Output \u00a0\u2013 \u00a01 \u00a0most \u00a0accurate \u00a0SVM \u00a0is \u00a0chosen \u00a0from \u00a0those \u00a0 learned \u00a0in \u00a0(ii) \u00a0  \u00a0  \u00a0  \u00a0 .\u2026\u2026\u2026\u2026(iii) \u00a0 Refer \u00a0to \u00a0(B) \u00a0below \u00a0for \u00a0details \u00a0  \u00a0 (A) \u00a0 D \u00a0-\u2010 \u00a0 \u00a0matrix \u00a0of \u00a0training \u00a0classes(1.0(+veclass), \u00a0-\u20101.0 \u00a0(-\u2010ve \u00a0class)) \u00a0 A \u00a0-\u2010 \u00a0matrix \u00a0with \u00a0feature \u00a0vectors, \u00a0 \u00a0 e \u00a0-\u2010 \u00a0vector \u00a0filled \u00a0with \u00a0ones, \u00a0 \u00a0 E \u00a0= \u00a0[A \u00a0-\u2010e] \u00a0 mu \u00a0= \u00a0scalar \u00a0constant \u00a0# \u00a0used \u00a0to \u00a0tune \u00a0classifier \u00a0 MAPPER \u00a0 Input: \u00a0Rows \u00a0of \u00a0data \u00a0from \u00a0the \u00a0Training \u00a0Dataset \u00a0 Computation: \u00a0Matrices \u00a0E.T*E \u00a0and \u00a0E.T*D*e \u00a0 \u00a0 Output: \u00a0Base \u00a064 \u00a0encoded \u00a0(E.T*E, \u00a0E.T*D*e) \u00a0\nREDUCER \u00a0 Input: \u00a0Output \u00a0of \u00a0mapper \u00a0(key \u00a0\u2013 \u00a0 index \u00a0of \u00a0combinations \u00a0of \u00a0 classes \u00a0for \u00a0which \u00a0SVM \u00a0is \u00a0being \u00a0computed \u00a0\u2013 \u00a0from \u00a0cases.txt; \u00a0 value \u00a0\u2013 \u00a0base \u00a064 \u00a0encoded \u00a0E.T*E, \u00a0E.T*D*e) \u00a0 Computation: \u00a0 (Large \u00a0margin \u00a0with \u00a0 linear \u00a0 kernel): \u00a0 (omega, \u00a0 gamma) \u00a0= \u00a0inverse(I/mu \u00a0+ \u00a0sum(E.T*E)) \u00a0* \u00a0sum(E.T*D*e) \u00a0 Output: \u00a0 Coefficients \u00a0 (thetas) \u00a0 for \u00a0 SVM \u00a0 computation \u00a0 for \u00a0 each \u00a0line \u00a0in \u00a0cases.txt \u00a0  \u00a0\nThe \u00a0SVM \u00a0computed \u00a0from \u00a0(iii) \u00a0is \u00a0added \u00a0to \u00a0the \u00a0 final \u00a0tree \u00a0structure. \u00a0 (nCn/2)/2 \u00a0combinations \u00a0are \u00a0computed \u00a0again \u00a0for \u00a0 each \u00a0of \u00a0the \u00a0set \u00a0of \u00a0+ve \u00a0and \u00a0\u2013ve \u00a0classes \u00a0 recursively. \u00a0\nEg: \u00a0Suppose \u00a0combination \u00a0-\u2010 \u00a0 2,3 \u00a0 1 \u00a0 gave \u00a0the \u00a0most \u00a0accurate \u00a0SVM. \u00a0 In \u00a0the \u00a0next \u00a0step, \u00a0combinations \u00a0would \u00a0be: \u00a0 2 \u00a0 3 \u00a0 \u00a0\n \u00a0  \u00a0  \u00a0 (B) \u00a0 (omega, \u00a0gamma) \u00a0= \u00a0(I/mu \u00a0+ \u00a0E.T*E).I*(E.T*D*e) \u00a0 x \u00a0\u2013incoming \u00a0feature \u00a0vector \u00a0 MAPPER \u00a0 Input: \u00a0 \u00a0Rows \u00a0of \u00a0data \u00a0from \u00a0the \u00a0Cross \u00a0Validation \u00a0Dataset \u00a0 Computation: \u00a0x.T*omega \u00a0-\u2010 \u00a0gamma \u00a0 Output: \u00a0Number \u00a0of \u00a0true \u00a0positives, \u00a0true \u00a0negatives, \u00a0false \u00a0 positives \u00a0and \u00a0false \u00a0negatives \u00a0for \u00a0each \u00a0case \u00a0in \u00a0cases.txt \u00a0\nREDUCER \u00a0 Input: \u00a0Output \u00a0of \u00a0mapper \u00a0 (key \u00a0\u2013 \u00a0 index \u00a0of \u00a0combinations \u00a0 of \u00a0 classes \u00a0 for \u00a0 which \u00a0 SVM \u00a0 is \u00a0 being \u00a0 computed \u00a0 \u2013 \u00a0 from \u00a0 cases.txt; \u00a0value \u00a0\u2013 \u00a0TPs, \u00a0FPs, \u00a0TNs, \u00a0FNs) \u00a0 Computation: \u00a0 Accuracy, \u00a0 Precision, \u00a0 Recall, \u00a0 F1 \u00a0 measure \u00a0 for \u00a0each \u00a0case \u00a0in \u00a0cases.txt. \u00a0Selection \u00a0of \u00a0best \u00a0SVM \u00a0based \u00a0 on \u00a0this. \u00a0 Output: \u00a0Coefficients \u00a0of \u00a0SVM \u00a0with \u00a0the \u00a0best \u00a0accuracy/F1 \u00a0 measure \u00a0  \u00a0\nNo \u00a0of \u00a0classes \u00a0 ==1? \u00a0 STOP \u00a0\nFigure I. A brief outline of the algorithm\nCompute \u00a0 the \u00a0 (nCn/2)/2 \u00a0 binary \u00a0 combinations \u00a0 of \u00a0 the \u00a0 set \u00a0 of \u00a0 classes. \u00a0 Eg: \u00a0 For \u00a0 a \u00a0 3 \u00a0 class \u00a0 problem \u00a0 (with \u00a0 classes \u00a0 -\u2010 \u00a0 1,2 \u00a0 and \u00a0 3), \u00a0 the \u00a0 combinations \u00a0will \u00a0be: \u00a0 1,2 \u00a0 3 \u00a0  \u00a0 (+ve \u00a0classes \u00a0 -\u2010ve \u00a0classes) \u00a0 1,3 \u00a0 2 \u00a0 2,3 \u00a0 1 \u00a0  \u00a0  \u00a0  \u00a0 \u2026\u2026\u2026\u2026.(i) \u00a0"}, {"heading": "3. Experiments and Results", "text": "For all of these experiments, we have used a 3 node cluster to measure the metrics of our approach, and Python\u2019s Scikit-learn library for One vs. One and One vs. Rest). Datasets used for experimentation are described below and the sources for those are indicated in references."}, {"heading": "3.1 Datasets used", "text": ""}, {"heading": "3.2 Accuracy (Figure IV.)", "text": "We have measured the accuracy of the algorithm using the following formula on the testing samples\nI. \ud835\udc34\ud835\udc50\ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc66 \u00a0 =  \u00a0 !\"# \u00a0! \u00a0!\"# !\"#$% \u00a0!\"#$%&'  \u00a0\nOur approach gives better accuracies in all the datasets except the SDSS dataset. SDSS is a skewed dataset, so accuracy is not the best performance metric to use during cross-validation. We will have to use metrics other than the accuracy (such as precision, recall and F1 measure) to select the best SVM here\nDataset name SDSS[15] Iris[16] Mfeat[17]\n# Training samples\n40000 150 1500\n# Testing samples\n10000 50 500\n# Features 6 3 6 (mor) 47 (zer) 64 (kar)\n# Classes 3 4 10\nFigure III: Datasets used\n1 \u00a0 4 \u00a0\n1,2,3,4,5 \u00a0\n1,4 \u00a0 2,3,5 \u00a0\n2,5 \u00a0 3 \u00a0\n2 \u00a0 5 \u00a0\n \u00a0\n \u00a0\nFigure II: This structure is created during the training phase. A test sample belonging to class 2 will follow the path depicted by the arrows"}, {"heading": "3.3 Training Time (Figure V.)", "text": "While the single-machine implementations are more efficient for the smaller datasets, in the SDSS dataset we see that our training time is comparable to the single-machine implementations due to the large data size of SDSS. We can thus show that distribution of the computation gets more beneficial as the data size increases."}, {"heading": "3.4 Testing Time (Figure VI.)", "text": "We show a significant reduction (53.7%) in testing time for the SDSS dataset, a result of the distributed approach working hand-in-hand with the decision tree based algorithm.\n4. Conclusion and Future work\n0 \u00a0 100 \u00a0 200 \u00a0 300 \u00a0 400 \u00a0 500 \u00a0 600 \u00a0 700 \u00a0\nIris \u00a0 Mfeat-\u2010mor \u00a0 Mfeat-\u2010zer \u00a0 Mfeat-\u2010kar \u00a0 SDSS \u00a0\nFigure \u00a0V: \u00a0Comparison \u00a0of \u00a0Training \u00a0Time \u00a0(seconds) \u00a0\nOne \u00a0vs. \u00a0One \u00a0\nOne \u00a0vs. \u00a0Rest \u00a0\nOur \u00a0Approach \u00a0\n0 \u00a0\n50 \u00a0\n100 \u00a0\n150 \u00a0\n200 \u00a0\n250 \u00a0\nIris \u00a0 Mfeat-\u2010mor \u00a0 Mfeat-\u2010zer \u00a0 Mfeat-\u2010kar \u00a0 SDSS \u00a0\nFigure \u00a0VI: \u00a0Comparison \u00a0of \u00a0Tescng \u00a0Time \u00a0(seconds) \u00a0\nOne \u00a0vs. \u00a0One \u00a0\nOne \u00a0vs. \u00a0Rest \u00a0\nOur \u00a0Approach \u00a0\n0 \u00a0\n20 \u00a0\n40 \u00a0\n60 \u00a0\n80 \u00a0\n100 \u00a0\n120 \u00a0\nIris \u00a0 Mfeat-\u2010mor \u00a0 Mfeat-\u2010zer \u00a0 Mfeat-\u2010kar \u00a0 SDSS \u00a0\nFigure \u00a0IV: \u00a0Comparison \u00a0of \u00a0Accuracy \u00a0(%) \u00a0\nOne \u00a0vs. \u00a0One \u00a0\nOne \u00a0vs. \u00a0Rest \u00a0\nOur \u00a0Approach \u00a0\nIn this research, we have proposed a novel distributed multi class SVM algorithm in which instead of extending binary SVMs or all-together methods, the idea is to divide the dataset into half at any point of time and obtain the visual distribution during the training phase. While testing, this structure has been effectively exploited and hence saving huge amount of testing time. This approach has been found to excel as data size increases which caters to our needs of handling big data.\nIn the future, we hope to enhance this algorithm by doing the following:\n\u2022 Implementing a distributed Gaussian Kernel (we are currently using a linear kernel) \u2022 Optimizing the algorithm for skewed datasets by using performance metrics such as the F1 measure\n(instead of Accuracy that we use currently) \u2022 Running the algorithm with data sizes of about 20-30 GB with very large clusters (which we haven\u2019t been\nable to do so far for lack of resources) \u2022 Comparing this algorithm with other multi-class Machine Learning techniques (non-SVM)"}], "references": [{"title": "Krebel Pairwise classification and support vector machines", "author": ["H.G. U"], "venue": "In B.Scholkopf,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Support Vector machines for 3-d object recognition IEEE Transactions On Pattern Analysis and Machine", "author": ["M. Pontil", "A.Verri"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Ussikavul Multiclass support vector machines using adaptive directed acyclic graph", "author": ["N.B.Kijsirikul"], "venue": "In Proceedings of International Joint Conference on Neural Networks (IJCNN", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "A fast Parallel Optimization for Training Support Vector Machine.", "author": ["J X Dong", "A Krzyzak", "C Y Suen"], "venue": "Proceedings of 3rd International Conference on Machine Learning and Data Mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "A Parallel Mixture of SVMs for Very Large Scale Problems", "author": ["R. Collobert", "Y. Bengio", "S. Bengio"], "venue": "Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Distributed Parallel Support Vector Machines in Strongly Connected Networks", "author": ["Y Lu", "V Roychowdhury", "L. Vandenberghe"], "venue": "IEEE Transactions on Neural Networks 2008;", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "LIBSVM: a library for support vector machines", "author": ["C C Chang", "C J Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Krebel [1] showed that by this formulation, unclassifiable regions reduce, but still they remain.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "[3] proposed to use rules of a tennis tournament to solve unclassified regions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] proposed the same method and called it Adaptive Directed Acyclic Graph.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "A parallelization scheme was proposed where the kernel matrix is approximated by a block-diagonal approach [7].", "startOffset": 107, "endOffset": 110}, {"referenceID": 4, "context": "[9] proposed a new parallel SVM training and classification algorithm that each subset of a dataset is trained with SVM and then the classifiers are combined into a final single classifier function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[10] proposed a connected network based distributed support vector machine algorithm.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "With data sizes constantly expanding, and with classical machine learning algorithms that analyze such data requiring larger and larger amounts of computation time and storage space, the need to distribute computation and memory requirements among several computers has become apparent. Although substantial work has been done in developing distributed binary SVM algorithms and multi-class SVM algorithms individually, the field of multi-class distributed SVMs remains largely unexplored. This research proposes a novel algorithm that implements the Support Vector Machine over a multi-class dataset and is efficient in a distributed environment (here, Hadoop). The idea is to divide the dataset into half recursively and thus compute the optimal Support Vector Machine for this half during the training phase, much like a divide and conquer approach. While testing, this structure has been effectively exploited to significantly reduce the prediction time. Our algorithm has shown better computation time during the prediction phase than the traditional sequential SVM methods (One vs. One, One vs. Rest) and out-performs them as the size of the dataset grows. This approach also classifies the data with higher accuracy than the traditional multiclass algorithms.", "creator": "Word"}}}