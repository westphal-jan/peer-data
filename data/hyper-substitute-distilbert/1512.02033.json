{"id": "1512.02033", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2015", "title": "Risk Minimization in Structured Prediction using Orbit Loss", "abstract": "we introduce a large surrogate loss function called orbit loss in the strategic prediction framework, it has good symbolic application practical advantages. by empirical orbit loss is quite implicit, it has overly low semantic gradient and a generalized prediction - like learning slope. evaluated typically then new risk theoretically and state a pac - only positive bound. examples also prove that risking new loss may beneficial in the general relationship ; ideally, the risk achieved : the observation of the trained parameters explicitly generates infimum risk achievable by rational probability parameter assuming the matrix parameter. methods that systematically aimed per risk evaluate, such it the structured ramp loss, complex structured probit loss and the direct loss should require at least fewer inference operations per training application. in intrinsic context, the orbit loss is more distinct as correlation requires requires one inference operation whose training iteration, both yields similar performance. he conclude the paper included numerical empirical comparison of thirteen proposed independent criteria ( synthetic exponential hinge integral, the structured ramp requirement, intrinsic structured probit transformation and the direct excess minimization method on several separate datasets and tasks.", "histories": [["v1", "Mon, 7 Dec 2015 13:30:27 GMT  (23kb)", "http://arxiv.org/abs/1512.02033v1", null], ["v2", "Wed, 9 Dec 2015 09:59:56 GMT  (23kb)", "http://arxiv.org/abs/1512.02033v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["danny karmon", "joseph keshet"], "accepted": false, "id": "1512.02033"}, "pdf": {"name": "1512.02033.pdf", "metadata": {"source": "CRF", "title": "Risk Minimization in Structured Prediction using Orbit Loss", "authors": ["Danny Karmon"], "emails": ["danny.karmon@biu.ac.il", "joseph.keshet@biu.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n02 03\n3v 1\n[ cs\n.L G"}, {"heading": "1 Introduction", "text": "There are three main differences between binary classification problems and structured prediction problems. First, the input to a binary classifier is a feature vector of a fixed length and the output is restricted to two possible labels, whereas in structured prediction both the input and the output are structured objects (a graph, an acoustic speech utterance, a sequence of words, an image). Second, the structured output space is potentially exponentially large (all possible phoneme or word sequences, all possible taxonomy graphs, all possible human poses, etc.). And third, while in binary classification the system\u2019s performance is evaluated using the error rate, i.e., 0-1 loss, in structured prediction each task often has its own evaluation metric or cost, such as word error rate in speech recognition, the BLEU score in machine translation, the NDCG score in\ninformation retrieval, or the intersection-over-union score in visual object segmentation. Some of these are involved functions, which are non-decompostable in the output space.\nThere is significant literature on learning parameters for structured prediction and graphical models. Ultimately, the goal in learning is to find the model parameters so as to minimize the expected cost, or risk, where the expectation is taken with respect to a random draw of input-output pairs from a fixed but unknown distribution. Since the expectation cannot be evaluated because the underlying probability is unknown, and since the cost is often a non-convex combinatorial function (which is hard to minimize directly), the learning problem is formulated as an optimization problem where the parameters are found by minimizing a trade-off between a measure of the goodness of fit (loss) to the training data and a regularization term. In discriminative training, the loss function should be directly related to the cost between the model prediction and the target label, averaged over the training set.\nThe most common approaches to structured prediction, namely structured perceptron, structural support vector machine (SVM) and conditional random fields (CRF), do not directly minimize the risk. The structured perceptron (Collins, 2002) solves a feasibility problem, which is independent of the cost. In structural SVM (Joachims et al., 2005) the measure of goodness is a convex upper bound to the cost called structural hinge loss. It is based on a generalization of the binary SVM hinge loss to the structured case, and there is no guarantee for the risk. While there exists generalization bounds for the structured hinge loss (e.g., McAllester, 2006; Taskar et al., 2003), they all include terms which are not directly related to the cost, such as the Hamming loss, and inherently the structured hinge loss cannot be consistent as it fails to converge to the performance of the optimal linear predictor in the limit of infinite training data (McAllester, 2006). In CRFs the measure of goodness is the log loss function, which is independent of the cost (Lafferty et al., 2001). Smith and Eisner (2006) tried to address this shortcoming of CRFs and proposed to minimize the risk under the Gibbs measure. While it seems that this loss function is consistent, we are not aware of any formal analysis.\nRecently, several works have focused on directly minimizing the expected cost. In particular, McAllester et al. (2010) presented a theorem stating that a certain perceptron-like learning rule, involving feature vectors derived from costaugmented inference, directly corresponds to the gradient of the risk. Direct loss needs two inference operations per training iteration and is extremely sensitive to its hyper-parameter. Do et al. (2008) generalized the notion of the ramp loss from binary classification to structured prediction and proposed a loss function, which is a non-convex bound to the cost, and was found to be a tighter bound than the structured hinge loss function. The structured ramp loss also needs two inference operations per training iteration. Keshet et al. (2011) generalized the notion of the binary probit loss to the structured prediction case. The gradient of this non-convex loss function can be approximated by averaging over samples from the unit-variance isotropic normal distribution, where for each sample an inference with a perturbed weight vector is computed. In order to gain stability\nin the gradient computation, hundreds to thousands of inference operations are required per training iteration, hence the update rule is computationally-heavy.\nThe goal of this work is to propose a new learning update rule for structured prediction which results in fast training on one hand and aims at minimizing the risk on the other hand. We define a new loss function, called orbit, where its gradient has a close simple analytical form, which is very close to the structured perceptron update rule. We state a finite sample generalization bound for this loss function and show that it is consistent in the strong sense. That is, for any feature map (finite or infinite dimensional) the loss function yields predictors approaching the infimum risk achievable by any linear predictor over the given features. The update rule of this new loss involves one inference operation per training iteration, similar to the structured perceptron or the structural SVM, and hence faster (per training iteration) than ramp, probit and direct loss minimization. In a series of experiments we showed that the new loss function performs similar to other approaches that were designed to minimize the risk.\nThe paper is organized as follows. In Section 2 we state the problem formally. In Section 3 we introduce the new surrogate loss function and its update rule. In Section 4 we present the analysis for our new methods, including proofs for both consistency and generalization bound. In Section 5 we present a set of experiments and compare the new learning rule to other algorithms. We conclude the paper in Section 6."}, {"heading": "2 Formal Settings", "text": "We formulate the structured supervised learning problem by setting X to be an abstract set of all possible input objects and Y to be an abstract set of all possible output targets. We assume that the input objects x \u2208 X and the target labels y \u2208 Y are drawn from an unknown joint distribution \u03c1. We define a set of d fixed mappings \u03c6 : X \u00d7 Y \u2192 Rd called feature functions from the set of input objects and target labels to a real vector of length d.\nHere we consider a linear decoder with parameters w \u2208 Rd, such that the parameters weight the feature functions. We denote the score of label y by w \u00b7 \u03c6(x,y), given the input x. The decoder predicts the label y\u0302w with the highest score:\ny\u0302w(x) = argmax y\u2208Y\nw \u00b7 \u03c6(x,y) (1)\nIdeally, we would like to find the parameters w that optimize the risk for unseen data. Formally, we define the cost function, \u2113(y, y\u0302w), to be a non-negative measure of error when predicting y\u0302w instead of y as the label of x. We assume that \u2113(y,y) = 0 for all y. Often the desired evaluation metric is a utility function that needs to be maximized (like BLEU or NDCG) and then we define the cost to be 1 minus the evaluation metric.\nOur goal is to minimize the risk:\nw\u2217 = argmin w E(x,y)\u223c\u03c1[\u2113(y, y\u0302w(x))]. (2)\nSince the distribution \u03c1 is unknown, we use a training set S = {(xi,yi)}mi=1 of m examples that are drawn i.i.d. from \u03c1, and replace the expectation in (2) with a mean over the training set and a regularization factor 12\u2016w\u20162. The cost is often a combinatorial non-convex quantity, which is hard to minimize, hence it is replaced with a surrogate loss, denoted \u2113\u0304(w,x,y). Different algorithms use different surrogate loss functions. Overall the objective function of (2) transforms into the following objective function\nw\u2217 = argmin w\n1\nm\nm \u2211\ni=1\n\u2113\u0304(w,xi,yi) + \u03bb\n2 \u2016w\u20162, (3)\nwhere \u03bb is a trade-off parameter between the loss term and the regularization factor."}, {"heading": "3 Orbit Loss", "text": "Denote by \u2206\u03c6(y,y\u2032) the difference between the feature functions of the labels y,y\u2032 \u2208 Y, respectively:\n\u2206\u03c6(y,y\u2032) = \u03c6(x,y)\u2212 \u03c6(x,y\u2032).\nDefine by \u03b4\u03c6(y,y\u2032) the normalized version of \u2206\u03c6(y,y\u2032) as follows:\n\u03b4\u03c6(y,y\u2032) =\n{\n\u2206\u03c6(y,y\u2032)/\u2016\u2206\u03c6(y,y\u2032)\u2016 if y 6= y\u2032 0 if y = y\u2032 . (4)\nThe orbit surrogate loss function is formally defined as follows:\n\u2113\u0304orbit(w,x,y) =\nP\u03b5\u223cN (0,1)\n[ \u03b5 > w \u00b7 \u03b4\u03c6(y, y\u0302w) ] \u2113(y, y\u0302w). (5)\nThat is, the orbit loss is equal to the cost multiplied by the probability that the prediction score w \u00b7\u03c6(x, y\u0302w) plus a small number \u03b5 is greater than the score of the target label w \u00b7 \u03c6(x,y).\nWe now derive the gradient-based learning rule for this loss function, which helps to describe some of its properties. The loss has a simple analytical gradient:\n\u2207w [ P\u03b5\u223cN (0,1) [\u03b5 > w \u00b7 \u03b4\u03c6(y, y\u0302)] \u2113(y, y\u0302) ]\n(6)\n= \u2207w [\n1\u221a 2\u03c0\n\u222b \u221e\nw\u00b7\u03b4\u03c6(y,y\u0302)\ne\u2212z 2/2dz \u2113(y, y\u0302)\n]\n(7)\n= \u2212 1\u221a 2\u03c0 e\u2212|w\u00b7\u03b4\u03c6(y,y\u0302)| 2/2\u2113(y, y\u0302) \u03b4\u03c6(y, y\u0302). (8)\nThe update rule of the orbit loss is the following:\nw \u2190 (1\u2212 \u03b7\u03bb)w + \u03b7 e\u2212|w\u00b7\u03b4\u03c6(y,y\u0302w)| 2/2 \u2113(y, y\u0302w) \u03b4\u03c6(y, y\u0302w). (9)\nNote that when the prediction label y\u0302w is close to the target label y in terms of the decoding score, that is, when the term w \u00b7 \u03b4\u03c6(y\u0302w,y) is relatively small, the exponent is close to 1. Under this condition the update rule becomes\nw \u2190 (1\u2212 \u03b7\u03bb)w + \u03b7 \u2113(y, y\u0302) \u03b4\u03c6(y, y\u0302w), (10)\nwhich generalizes the regularized structured perceptron\u2019s update rule (Collins, 2002; Zhang et al., 2014). Namely\nw \u2190 (1 \u2212 \u03b7\u03bb)w + \u03b7 1{y 6= y\u0302w} \u03b4\u03c6(y, y\u0302w), (11)\nwhere 1{\u03c0} is an indicator function, equals 1 if the predicate \u03c0 holds and equals 0 otherwise.\nA nice property of this update rule is that the cost function does not need to be decomposable in the size of the output. Decomposable cost functions are needed in order to solve the cost-augmented inference that is used in the training of structural SVMs (Joachims et al., 2005; Ranjbar et al., 2013), direct loss minimization (McAllester et al., 2010), or structured ramp loss (Do et al., 2008). It means that cost functions like word error rate or intersection-overunion can be used without being approximated.\nAnother property of the orbit loss is its similarity to the structured probit loss (Keshet et al., 2011). The probit loss was derived from the concept of stochastic decoder in the PAC-Bayesian framework (McAllester, 2003, 1998) and was shown to have both good theoretical properties and practical advantages (Keshet et al., 2011). The structured probit loss is defined as follows\n\u2113\u0304probit(w,x,y) = E\u01eb\u223cN (0,I) [\u2113(y, y\u0302w+\u01eb)] , (12)\nwhere \u01eb \u2208 Rd is a d-dimensional isotropic Normal random vector. Note that the orbit loss (5) can be written as follows:\nP\u03b5\u223cN (0,1) [\u03b5 > w \u00b7 \u03b4\u03c6(y, y\u0302w)] \u2113(y, y\u0302w) (13) = P\u01eb\u223cN (0,I) [\u2212\u01eb \u00b7 \u03b4\u03c6(y, y\u0302w) > w \u00b7 \u03b4\u03c6(y, y\u0302w)] \u2113(y, y\u0302w).\nThe last equation holds since the inner product of an isotropic normal random vector \u01eb with a unit-norm vector \u03b4\u03c6(y, y\u0302) is a zero-mean unit variance normal random variable. Writing the probability as the expectation of an indicator function, we have\n\u2113\u0304orbit(w,x,y) = (14)\nE\u01eb\u223cN (0,I)\n[ I {(w + \u01eb) \u00b7 \u03b4\u03c6(y, y\u0302w)<0} ] \u2113(y, y\u0302w).\nAssuming y\u0302w+\u01eb = y\u0302w for a value r small enough that \u01eb \u2208 B(0, r), where B(0, r) is a ball of radius r centered at 0, we can bring the cost function into the expectation term, that is\nE\u01eb\u223cN (0,I) [I{(w + \u01eb) \u00b7 \u03b4\u03c6(y, y\u0302w+\u01eb)<0} \u2113(y, y\u0302w+\u01eb)] = E\u01eb\u223cN (0,I) [\u2113(y, y\u0302w+\u01eb)] , (15)\nwhich is the structured probit loss."}, {"heading": "4 Analysis", "text": "In this section we analyze the orbit loss. We derive a generalization bound based on the PAC-Bayesian theory, where we start by upper-bounding the probit loss with the orbit loss and then plugging it into a PAC-Bayesian generalization bound. Then we show that the decoder\u2019s parameters, which are estimated by optimizing the regularized orbit loss in the limit of infinite data, approach the infimum risk achievable by any linear decoder.\nRecall that the structured probit loss is defined as:\n\u2113\u0304probit(w,x,y) = E\u01eb\u223cN (0,I) [\u2113(y, y\u0302w+\u01eb)] . (16)\nThe following theorem states a generalization bound for the probit loss function (Keshet et al., 2011).\nTheorem 1 (Generalization of probit loss). For a fixed \u03b3 > 1/2 we know that, with a probability of at least 1\u2212\u03b4 over the draw of the training data, the following holds simultaneously for all w:\nE(x,y)\u223c\u03c1\n[ \u2113\u0304probit(w,x,y) ]\n\u2264 1 1\u2212 12\u03b3\n(\n1\nm\nm \u2211\ni=1\n\u2113\u0304probit(w,xi,yi)\n+ \u03b3 2m \u2016w\u20162 + \u03b3 m ln 1 \u03b4\n)\n. (17)\nLater this generalization bound will help us state a similar bound for the orbit loss.\nWe now analyze the orbit loss. Let \u03b7 be the minimal distance between the score of the predicted label y\u0302 to the score of its closest different label y\u2032 by a constant \u03b7:\nmin y\u2032 6=y\u0302\nw \u00b7 \u03b4\u03c6(y\u0302,y\u2032) \u2265 \u03b7 (18)\nfor y\u0302 6= y\u2032. The following lemma upper bounds the probit loss with the orbit loss.\nLemma 2. For a finite \u03c3 > 0 and a cost function \u2113(y,y\u2032) \u2208 [0, 1] for all y, y\u2032, the following holds:\n\u2113\u0304probit(w/\u03c3,x,y) \u2264 \u2113\u0304orbit(w/\u03c3,x,y) + \u03c3, (19) for \u03b7 \u2265 \u03c3 \u221a\n2 ln m\u03c3 .\nFor the brevity of the explanation we call y\u0302w the predicted label and we call y\u0302w+\u01eb the perturbed label. The idea behind the proof is to split the structured probit loss cases of \u01eb for which the predicted label and the perturbed label are the same, and the case in which they differ. We show that the probability of the labels being equal is upper-bounded by the orbit loss, and the probability of the labels being different is upper-bounded by an exponential term that approaches zero when the norm of \u01eb approaches zero.\nProof. From the law of total expectation we have\nE\u01eb\u223cN (0,I)\n[ \u2113(y\u0302w+\u01eb,y) ]\n\u2264 E\u01eb [ 1{y\u0302w+\u01eb = y\u0302w}\u2113(y\u0302w+\u01eb,y) ]\n+ P\u01eb\n[ y\u0302w+\u01eb 6= y\u0302w ] ,\nwhere we upper bound the cost by 1 in the second term. First, let us focus on the first term of the inequality. For this term y\u0302w+\u01eb = y\u0302w, which means that\nE\u01eb\n[ 1{y\u0302w+\u01eb = y\u0302w}\u2113(y\u0302w+\u01eb,y) ]\n= P\u01eb\n[\ny\u0302w+\u01eb = y\u0302w\n]\n\u2113(y\u0302w,y). (20)\nBy definition of the inference rule (1) for any vector u, we have u \u00b7\u03b4\u03c6(y\u0302u,y) \u2265 0 for all y. Therefore the probability that y\u0302w+\u01eb = y\u0302w can be expressed as follows:\nP\u01eb\n[\ny\u0302w+\u01eb = y\u0302w\n]\n= P\u01eb\n[ (w + \u01eb) \u00b7 \u03b4\u03c6(y\u0302w+\u01eb, y\u0302w) \u2264 0 ]\nwhich, in turn, can be expressed as\nP\u01eb\n[ w \u00b7 \u03b4\u03c6(y\u0302w+\u01eb, y\u0302w) \u2264 \u2212\u01eb \u00b7 \u03b4\u03c6(y\u0302w+\u01eb, y\u0302w) ]\n(21)\n\u2264 P\u01eb [ w \u00b7 \u03b4\u03c6(y, y\u0302w) \u2264 \u2212\u01eb \u00b7 \u03b4\u03c6(y\u0302w+\u01eb, y\u0302w) ] ,\nwhere replacing y\u0302w+\u01eb with y increases the event size, thereby increasing the probability. Replacing the inner product of an isotropic normal random vector \u01eb with a unit-norm vector \u03b4\u03c6(y, y\u0302w) with a zero-mean unit variance normal random variable, we get:\nP\u01eb\n[ w \u00b7 \u03b4\u03c6(y, y\u0302w) \u2264 \u2212\u01eb \u00b7 \u03b4\u03c6(y\u0302w+\u01eb, y\u0302w) ]\n= P\u03b5\u223cN (0,1)\n[ \u03b5 > w \u00b7 \u03b4\u03c6(y, y\u0302w) ] . (22)\nThe second term of the left-hand side of (20) can be expressed as follows:\nP\u01eb[y\u0302w+\u01eb 6= y\u0302w] = P\u01eb[(w + \u01eb) \u00b7 \u03b4\u03c6(y\u0302w+\u01eb, y\u0302w) > 0].\nWe have\nP\u01eb[(w + \u01eb) \u00b7 \u03b4\u03c6(y\u0302w+\u01eb, y\u0302w) > 0] = P\u01eb[\u01eb \u00b7 \u03b4\u03c6(y\u0302w+\u01eb, y\u0302w) > w \u00b7 \u03b4\u03c6(y\u0302w, y\u0302w+\u01eb)] \u2264 P\u01eb[\u01eb \u00b7 \u03b4\u03c6(y\u0302w+\u01eb, y\u0302w) > \u03b7]. (23)\nWe finalized the proof by bounding the last equation for a \u03c3-scaled version of \u01eb,\nP\u01eb\u223cN (0,I)[\u03c3\u01eb \u00b7 \u03b4\u03c6(y\u0302w+\u03c3\u01eb, y\u0302w) > \u03b7]\n= P\u03b5\u223cN (0,1)[\u03c3\u03b5 > \u03b7] \u2264 exp ( \u2212 \u03b7 2\n2\u03c32\n)\n= \u03c3\nm , (24)\nwhere the first equation holds since the inner product of an isotropic normal random vector \u01eb with a unit-norm vector \u03b4\u03c6(y\u0302w+\u03c3\u01eb, y\u0302w) is a zero-mean unit variance normal random variable; and the second equation holds for \u03b7 \u2265 \u03c3 \u221a\n2 ln m\u03c3 . Using the union bound over the draw of a sample of size m concludes the proof.\nPlugging Lemma 2 into the bound of Theorem 1, we get the following generalization bound for the orbit loss.\nTheorem 3 (Generalization of orbit loss). For a fixed \u03b3 > 1/2 and assuming (18) holds with \u03b7 \u2265 \u03c3 \u221a\n2 ln(m/\u03c3), we know that with a probability of at least 1\u2212 \u03b4 over the draw of the training data the following holds true simultaneously for all w and for all \u03c3 > 0:\nE(x,y)\u223c\u03c1\n[ \u2113\u0304probit(w/\u03c3,x,y) ]\n\u2264 1 1\u2212 12\u03b3\n(\n1\nm\nm \u2211\ni=1\n\u2113\u0304orbit(w/\u03c3,xi,yi)\n+ \u03b3 2m\u03c32 \u2016w\u20162 + \u03c3 + \u03b3 m ln 1 \u03b4\n)\n. (25)\nWe will now prove that the orbit loss is consistent. We start with the observation that when the norm of the weight vector w goes to infinity, the orbit loss approaches the cost:\nLemma 4.\nlim \u03b1\u2192\u221e \u2113\u0304orbit(\u03b1w,x,y) = \u2113(y, y\u0302w), (26)\nassuming that \u2113(y,y) = 0 for all y.\nProof. Recall that y\u0302w = argminy\u2032 w\u00b7\u03b4\u03c6(y,y\u2032) thereforew\u00b7\u03b4\u03c6(y, y\u0302w) \u2264 0. Also note that scaling the parameters w does not change the prediction, y\u0302\u03b1w = y\u0302w. We have:\nlim \u03b1\u2192\u221e\nP\u03b5\u223cN (0,1)[\u03b5 > \u03b1w \u00b7 \u03b4\u03c6(y, y\u0302w)] \u2113(y, y\u0302w)\n= P\u03b5\u223cN (0,1)[\u03b5 > \u2212\u221e] \u2113(y, y\u0302w) = \u2113(y, y\u0302w).\nConsider the following training objective:\nw\u0302m = argmin w\n1\nm\nm \u2211\ni=1\n\u2113\u0304orbit(w,xi,yi) + \u03bbm 2m \u2016w\u20162. (27)\nTheorem 5 (Consistency of orbit loss). For w\u0302m defined by (27), if the sequence \u03bbm/ ln\n2 m increases without bound, and the sequence \u03bbm/(m lnm) converges to zero, then with a probability of one over the draw of the infinite sample we have:\nlim m\u2192\u221e E(x,y)\u223c\u03c1\n[ \u2113\u0304probit((lnm)w\u0302m,x,y) ]\n(28)\n= inf w E(x,y)\u223c\u03c1\n[ \u2113(y, y\u0302w(x)) ] .\nProof. Set \u03b4 = 1/m2, \u03c3 = 1/ lnm, and \u03b3m = \u03bbm/ ln 2 m into the bound (25). We decompose w into a scalar \u03b1, corresponding to the norm of w, and a unit norm vector w\u2217. Last, using Chernoff we upper-bound 1m \u2211m i=1 \u2113\u0304orbit(w,xi,yi) by E(x,y)\u223c\u03c1 [ \u2113\u0304orbit(w,x,y) ] + \u221a lnm/m to get\nE(x,y)\u223c\u03c1\n[ \u2113\u0304probit((lnm)wm,x,y) ]\n(29)\n\u2264 E(x,y)\u223c\u03c1 [ \u2113\u0304probit((lnm)\u03b1w \u2217,x,y) ]\n\u2264 1 1\u2212 ln2 m2\u03bbm\n(\nE(x,y)\u223c\u03c1\n[\n\u2113\u0304orbit(\u03b1w \u2217,x,y)\n]\n+\n\u221a\nlnm\nm +\n\u03bbm\u03b1 2\n2m +\n1\nlnm + 2\u03bbm m lnm\n)\n,\nwhere wm is the minimizer of the right-hand side of the bound in (25), as well as of the optimization problem (27). Taking the limit when the number of examples m approaches infinity on both sides we have\nlim m\u2192\u221e E(x,y)\u223c\u03c1\n[ \u2113\u0304probit((lnm)wm,x,y) ]\n(30)\n\u2264 E(x,y)\u223c\u03c1 [ \u2113\u0304orbit(\u03b1w \u2217,x,y) ]\nNoting that by\nE(x,y)\u223c\u03c1\n[ \u2113\u0304probit(w,x,y) ]\n(31)\n\u2265 inf w E(x,y)\u223c\u03c1 [\u2113(y,yw)] ,\nand letting \u03b1 approach infinity using Lemma 4 concludes the proof."}, {"heading": "5 Experiments", "text": "We evaluated the performance of the orbit loss by executing a number of experiments on several domains and tasks and compared the results with other approaches that are aimed at risk minimization, namely direct loss minimization (McAllester et al., 2010), structured ramp loss (Do et al., 2008), and structured probit loss (Keshet et al., 2011). For a reference, we present results for the structured perceptron, as we wanted to stress the empirical differences between the update rule in (10) and the one in (11), as well as for the structured hinge loss."}, {"heading": "5.1 MNIST", "text": "In our first experiment we tested the orbit update rule on a multiclass problem. MNIST is a dataset of handwritten digit images (10 classes). It is divided into a training set of 50,000 examples, a test set of 10,000 examples and a validation set of 10,000 examples. We preprocess the data by normalizing the input images, and reducing the dimension from the original 784 attributes to 100 using PCA.\nWe used the orbit update rule as in (9). We defined the weight vector w as a concatenation of 10 weight vectors w = (w0,w1, . . . ,w9), each corresponding to one of the 10 digits. The update rule of example (xi, yi), xi \u2208 R100, yi \u2208 {0, . . . , 9} can be simplified based on Kesler\u2019s construction (Crammer and Singer, 2001) as follows:\nwyi \u2190 (1\u2212 \u03b7\u03bb)wyi + \u03b7 e\u2212|wyi \u00b7xi\u2212wy\u0302\u00b7xi|2/2 \u2113(y\u0302, yi) xi wy\u0302 \u2190 (1\u2212 \u03b7\u03bb)wy\u0302 \u2212 \u03b7 e\u2212|wyi \u00b7xi\u2212wy\u0302\u00b7xi|2/2 \u2113(y\u0302, yi)xi wr \u2190 (1\u2212 \u03b7\u03bb)wr for all r 6= yi, y\u0302\nNote that the exponent values throughout the training were very close to 1 and, practically, the update rule (10) could be used.\nTo properly evaluate the orbit loss we ran the experiment with two different cost functions for \u2113(y\u0302, y): 0-1 loss and a semi-randomized matrix. We did so because the update rule (10) is identical to the structured perceptron update rule under the 0-1 loss.\nIn the first case, we set \u03b7 = \u03b70/ \u221a t, for \u03b70 = 0.1, t is the iteration number, and \u03bb = 0.001. We also trained a multiclass perceptron with \u03b7 = 1, and \u03bb = 0, and a multiclass SVM with C = 0.01 (Crammer and Singer, 2001). All of the\nhyper-parameters were chosen on the validation set. In all of the experiments we ran 4 epochs over the training data and used a linear kernel.\nThe results are given in Table 1 and suggest that there is a slight advantage for the orbit loss over the other algorithms. Recall that we previously showed that the perceptron is a special case of orbit loss and under this setting, in which \u2113(y\u0302, y) = 0-1 loss, hence the only difference between the results in the table is due to the regularization factor used with the orbit loss.\nAs mentioned above, this experiment was executed once again, setting the cost function, \u2113(y\u0302, y), to be a semi-randomized matrix. We generated a randomized cost matrix of size 10 \u00d7 10, such that the elements on the diagonal were all 0, and the rest of the elements were chosen uniformly at random to be either 1 or 2. We trained multiclass perceptron, multiclass SVM, and orbit using the following hinge loss for the cost function:\n\u2113\u0304hinge(w,x, y) = max y\u0302\n[ \u2113(y, y\u0302)\u2212wy \u00b7 x+wy\u0302 \u00b7 x ]\n(32)\nTo ensure reliability, we ran the second experiment for each algorithm with 10 different sampled matrices and averaged the results. The results are presented in Table 2. The results show a clear advantage for the orbit loss update rule in regards to the task loss. The reason is that the orbit loss can take advantage of minimizing a non 0-1 loss, as compared to perceptron."}, {"heading": "5.2 Phoneme alignment", "text": "Our next experiment focused on the phoneme alignment, which is used as a tool in developing speech recognition and text-to-speech systems. This is a structured prediction task \u2014 the input x represents a speech utterance, and consists of a pair x = (s,p) of a sequence of acoustic feature vectors (melfrequency cepstral coefficients) , s = (s1, . . . , sT ), where st \u2208 Rd, 1 \u2264 t \u2264 T ; and a sequence of phonemes p = (p1, . . . , pK), where pk \u2208 P , 1 \u2264 k \u2264 K is a phoneme symbol and P is a finite set of phoneme symbols. The lengths K and T can differ for different inputs, although typically T is significantly larger than K. The goal is to generate an alignment between the two sequences in the input. The output y is a sequence (y1, . . . , yK), where 1 \u2264 yk \u2264 T is an integer giving the start frame in the acoustic sequence of the k-th phoneme in the phoneme sequence. Hence the k-th phoneme starts at frame yk and ends at frame yk+1\u22121.\nFor this task we used the TIMIT speech corpus for which there are published benchmark results (Brugnara et al., 1993; Hosom, 2009; Keshet et al., 2007). We divided a portion of the TIMIT corpus (excluding the SA1 and SA2 utterances) into three disjoint parts containing 1500, 1796 and 400 utterances, respectively. The first part was used to train a phoneme frame-based classifier, which given the pair of speech frame and a phoneme, returns the level of confidence that the phoneme was uttered in that frame. The output classifier is then used along with other features as a seven dimensional feature map \u03c6(x,y) = \u03c6((s,p),y) as described in Keshet et al. (2007).\nThe seven dimensional weight vector w was trained on the second set of 150 aligned utterances for \u03c4 -insensitive loss\n\u2113(y, y\u0302) = 1\n|y| max {|yk \u2212 y\u0302k| \u2212 \u03c4, 0} , (33)\nwith \u03c4 = 10 ms. This cost measures the average disagreement between all of the boundaries of the desired alignment sequence y and the boundaries of predicted alignment sequence y\u0302 where a disagreement of less than \u03c4 is ignored.\nWe trained the system with the orbit update rule where \u03b7 = 1.0/ \u221a t and \u03bb = 0.2; the structured perceptron update rule; the structural SVM optimized using stochastic gradient descent with C=5 (Shalev-Shwartz et al., 2011); structured ramp-loss with \u03b7 = 1.0/ \u221a t, \u03bb = 0.4; and direct loss minimization algorithm with \u01eb = 1.1 on a reduced training set of 150 examples (out of 1796) and a reduced validation set of 100 examples (out of 400). We were not able to train the system with the probit loss in a reasonable time.\nThe results are given in Table 3. The results in the first 4 columns should be read as the accuracy (in percentage) that the prediction was within \u03c4 . The higher the better. The last column of the table is the actual loss computed by (33) - the smaller the better. In those results the orbit update rule outperforms other algorithms, and yields state-of-the-art results.\nWe would like to note that as in the MNIST experiment, the exponent values in the update rule were very close to 1 and, practically, the update rule (10) could be used."}, {"heading": "5.3 Vowel duration", "text": "In the problem of vowel duration measurement we are provided with a speech signal which includes exactly one vowel preceded and followed by consonants (i.e., CVC). Our goal is to predict the vowel duration accurately. Precise measurement of vowel duration in a given context is needed in many phonological experiments, and currently is done manually (Heller and Goldrick, 2014).\nThe speech signal is represented as a sequence of acoustic features x = (x1, x2, . . . , xT ) where each xi (1 \u2264 i \u2264 T) is a d-dimensional vector representing acoustic parameters, such as high and low energy, pitch, voicing, correlation coefficient, and so on (we extract d=22 acoustic features every 5 msec). We denote the domain of the feature vectors by X \u2282 Rd. The length of the input signal varies from one signal to another, thus T is not fixed. We denote by X \u2217 the set of all finite length sequences over X . In addition, we denote by tb \u2208 T\nand te \u2208 T the vowel onset and offset times, respectively, where T = {1, ..., T }. For brevity we set t = (tb, te). The typical duration of an utterance is around 2 sec. There were n=116 feature functions that described the typical duration of a vowel, the mean high energy before and after the vowel onset, and so on. The cost function we use is:\n\u2113(t\u0302, t) = [ |t\u0302b \u2212 tb| \u2212 \u03c4b ] + + [ |t\u0302e \u2212 te| \u2212 \u03c4e ] + , (34)\nwhere [\u03c0]+ = max{0, \u03c0}, and \u03c4b, \u03c4e are pre-defined constants. The above function measures the absolute differences between the predicted and the manually annotated vowel onsets and offsets. Since the manual annotations are not exact, we allow a mistake of \u03c4b and \u03c4e frames at the vowel onset and offset respectively.\nWe trained the system using the orbit update rule with \u03b7 = 1/ \u221a t, \u03bb = 0.001; the structured perceptron update rule; structured ramp loss with \u03b7 = 0.1/ \u221a t, \u03bb = 0.8; probit loss with the expectation approximated by a mean of 100 random samples \u03b7 = 0.001/ \u221a t, \u03bb = 0.005; and direct loss minimization with \u03b7 = 0.1/ \u221a t and \u01eb=-1.52. All of those hyper-parameters were chosen for the validation set. The results are presented in Table 4 for different values of \u03c4b and \u03c4e in the cost function. It can be seen that the orbit is close to the direct loss minimization (differences of a frame or two on average) and is better than other approaches. Also note that as describe earlier, the efficiency of the orbit loss is similar to the structured perceptron update and better than other approaches."}, {"heading": "6 Discussion and Future Work", "text": "We introduced a new surrogate loss function that offers an efficient and effective learning rule. We gave a qualitative theoretical analysis presenting a PACBayesian generalization bound and a consistency theorem. Despite the fact that the consistency property concerns the training performance when the number of training examples is big, the proposed loss function was shown to perform well on several tasks, even when the training set was of small or medium size.\nIn terms of theoretical properties, we think that the theoretical analysis can be improved, and in particular we would like to have a better upper-bound of the probit loss in terms of the orbit loss, as expressed in Lemma 2, which depends on the minimal distance between the predicted label and its closest neighbor label. Anyways, it is clear that when the norm of the weight vector becomes large relative to the norm of the noise, the inference with the weight vector and the inference with the perturbed weight vector \u2013 both lead to the same predicted label with a high probability.\nThis work is part of our research on surrogate loss functions in the structured prediction setting. We believe that in order to understand what are good loss functions, we have to understand the interrelationship between them. While we showed some relation between the orbit loss, the Perceptron and the probit loss, we still think that more work should be done. We are especially interested in understanding the connection between the orbit, the probit, and the direct loss minimization approach."}], "references": [{"title": "Automatic segmentation and labeling of speech based on hidden markov models", "author": ["F. Brugnara", "D. Falavigna", "M. Omologo"], "venue": "Speech Communication,", "citeRegEx": "Brugnara et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brugnara et al\\.", "year": 1993}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Collins,? \\Q2002\\E", "shortCiteRegEx": "Collins", "year": 2002}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "Jornal of Machine Learning Research,", "citeRegEx": "Crammer and Singer,? \\Q2001\\E", "shortCiteRegEx": "Crammer and Singer", "year": 2001}, {"title": "Tighter bounds for structured estimation", "author": ["C. Do", "Q. Le", "Teo", "C.-H", "O. Chapelle", "A. Smola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Do et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Do et al\\.", "year": 2008}, {"title": "Grammatical constraints on phonological encoding in speech production", "author": ["J.R. Heller", "M. Goldrick"], "venue": "Psychonomic bulletin & review,", "citeRegEx": "Heller and Goldrick,? \\Q2014\\E", "shortCiteRegEx": "Heller and Goldrick", "year": 2014}, {"title": "Speaker-independent phoneme alignment using transitiondependent states", "author": ["Hosom", "J.-P"], "venue": "Speech Communication,", "citeRegEx": "Hosom and J..P.,? \\Q2009\\E", "shortCiteRegEx": "Hosom and J..P.", "year": 2009}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Joachims", "T. Tsochantaridis", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Joachims et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Joachims et al\\.", "year": 2005}, {"title": "PAC-Bayesian approach for minimization of phoneme error rate", "author": ["J. Keshet", "D. McAllester", "T. Hazan"], "venue": "In International Conference on Acoustics, Speech, and Signal Processing (ICASSP)", "citeRegEx": "Keshet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Keshet et al\\.", "year": 2011}, {"title": "A large margin algorithm for speech and audio segmentation", "author": ["J. Keshet", "S. Shalev-Shwartz", "Y. Singer", "D. Chazan"], "venue": "IEEE Trans. on Audio, Speech and Language Processing,", "citeRegEx": "Keshet et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Keshet et al\\.", "year": 2007}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Proceedings of the Eightneenth International Conference on Machine Learning (ICML),", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Simplified PAC-Bayesian margin bounds", "author": ["D. McAllester"], "venue": "In Proceedings of the Sixteenth Annual Conference on Computational Learning Theory", "citeRegEx": "McAllester,? \\Q2003\\E", "shortCiteRegEx": "McAllester", "year": 2003}, {"title": "Generalization bounds and consistency for structured labeling", "author": ["D. McAllester"], "venue": "Predicting Structured Data,", "citeRegEx": "McAllester,? \\Q2006\\E", "shortCiteRegEx": "McAllester", "year": 2006}, {"title": "Direct loss minimization for structured prediction", "author": ["D. McAllester", "T. Hazan", "J. Keshet"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "McAllester et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McAllester et al\\.", "year": 2010}, {"title": "Some pac-bayesian theorems", "author": ["D.A. McAllester"], "venue": "In Proceedings of the Eleventh Annual Conference on Computational Learning Theory", "citeRegEx": "McAllester,? \\Q1998\\E", "shortCiteRegEx": "McAllester", "year": 1998}, {"title": "Optimizing nondecomposable loss functions in structured prediction", "author": ["M. Ranjbar", "T. Lan", "Y. Wang", "S.N. Robinovitch", "Li", "Z.-N", "G. Mori"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Ranjbar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ranjbar et al\\.", "year": 2013}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Minimum risk annealing for training loglinear models", "author": ["D.A. Smith", "J. Eisner"], "venue": "In Proc. of the COLING/ACL,", "citeRegEx": "Smith and Eisner,? \\Q2006\\E", "shortCiteRegEx": "Smith and Eisner", "year": 2006}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Taskar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2003}, {"title": "Regularized structured perceptron: A case study on chinese word segmentation, pos tagging and parsing. The 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL), page 164", "author": ["K. Zhang", "P. Fujian", "J. Su", "C. Zhou"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "The structured perceptron (Collins, 2002) solves a feasibility problem, which is independent of the cost.", "startOffset": 26, "endOffset": 41}, {"referenceID": 6, "context": "In structural SVM (Joachims et al., 2005) the measure of goodness is a convex upper bound to the cost called structural hinge loss.", "startOffset": 18, "endOffset": 41}, {"referenceID": 17, "context": "While there exists generalization bounds for the structured hinge loss (e.g., McAllester, 2006; Taskar et al., 2003), they all include terms which are not directly related to the cost, such as the Hamming loss, and inherently the structured hinge loss cannot be consistent as it fails to converge to the performance of the optimal linear predictor in the limit of infinite training data (McAllester, 2006).", "startOffset": 71, "endOffset": 116}, {"referenceID": 11, "context": ", 2003), they all include terms which are not directly related to the cost, such as the Hamming loss, and inherently the structured hinge loss cannot be consistent as it fails to converge to the performance of the optimal linear predictor in the limit of infinite training data (McAllester, 2006).", "startOffset": 278, "endOffset": 296}, {"referenceID": 9, "context": "In CRFs the measure of goodness is the log loss function, which is independent of the cost (Lafferty et al., 2001).", "startOffset": 91, "endOffset": 114}, {"referenceID": 1, "context": "The structured perceptron (Collins, 2002) solves a feasibility problem, which is independent of the cost. In structural SVM (Joachims et al., 2005) the measure of goodness is a convex upper bound to the cost called structural hinge loss. It is based on a generalization of the binary SVM hinge loss to the structured case, and there is no guarantee for the risk. While there exists generalization bounds for the structured hinge loss (e.g., McAllester, 2006; Taskar et al., 2003), they all include terms which are not directly related to the cost, such as the Hamming loss, and inherently the structured hinge loss cannot be consistent as it fails to converge to the performance of the optimal linear predictor in the limit of infinite training data (McAllester, 2006). In CRFs the measure of goodness is the log loss function, which is independent of the cost (Lafferty et al., 2001). Smith and Eisner (2006) tried to address this shortcoming of CRFs and proposed to minimize the risk under the Gibbs measure.", "startOffset": 27, "endOffset": 910}, {"referenceID": 1, "context": "The structured perceptron (Collins, 2002) solves a feasibility problem, which is independent of the cost. In structural SVM (Joachims et al., 2005) the measure of goodness is a convex upper bound to the cost called structural hinge loss. It is based on a generalization of the binary SVM hinge loss to the structured case, and there is no guarantee for the risk. While there exists generalization bounds for the structured hinge loss (e.g., McAllester, 2006; Taskar et al., 2003), they all include terms which are not directly related to the cost, such as the Hamming loss, and inherently the structured hinge loss cannot be consistent as it fails to converge to the performance of the optimal linear predictor in the limit of infinite training data (McAllester, 2006). In CRFs the measure of goodness is the log loss function, which is independent of the cost (Lafferty et al., 2001). Smith and Eisner (2006) tried to address this shortcoming of CRFs and proposed to minimize the risk under the Gibbs measure. While it seems that this loss function is consistent, we are not aware of any formal analysis. Recently, several works have focused on directly minimizing the expected cost. In particular, McAllester et al. (2010) presented a theorem stating that a certain perceptron-like learning rule, involving feature vectors derived from costaugmented inference, directly corresponds to the gradient of the risk.", "startOffset": 27, "endOffset": 1225}, {"referenceID": 1, "context": "The structured perceptron (Collins, 2002) solves a feasibility problem, which is independent of the cost. In structural SVM (Joachims et al., 2005) the measure of goodness is a convex upper bound to the cost called structural hinge loss. It is based on a generalization of the binary SVM hinge loss to the structured case, and there is no guarantee for the risk. While there exists generalization bounds for the structured hinge loss (e.g., McAllester, 2006; Taskar et al., 2003), they all include terms which are not directly related to the cost, such as the Hamming loss, and inherently the structured hinge loss cannot be consistent as it fails to converge to the performance of the optimal linear predictor in the limit of infinite training data (McAllester, 2006). In CRFs the measure of goodness is the log loss function, which is independent of the cost (Lafferty et al., 2001). Smith and Eisner (2006) tried to address this shortcoming of CRFs and proposed to minimize the risk under the Gibbs measure. While it seems that this loss function is consistent, we are not aware of any formal analysis. Recently, several works have focused on directly minimizing the expected cost. In particular, McAllester et al. (2010) presented a theorem stating that a certain perceptron-like learning rule, involving feature vectors derived from costaugmented inference, directly corresponds to the gradient of the risk. Direct loss needs two inference operations per training iteration and is extremely sensitive to its hyper-parameter. Do et al. (2008) generalized the notion of the ramp loss from binary classification to structured prediction and proposed a loss function, which is a non-convex bound to the cost, and was found to be a tighter bound than the structured hinge loss function.", "startOffset": 27, "endOffset": 1547}, {"referenceID": 1, "context": "The structured perceptron (Collins, 2002) solves a feasibility problem, which is independent of the cost. In structural SVM (Joachims et al., 2005) the measure of goodness is a convex upper bound to the cost called structural hinge loss. It is based on a generalization of the binary SVM hinge loss to the structured case, and there is no guarantee for the risk. While there exists generalization bounds for the structured hinge loss (e.g., McAllester, 2006; Taskar et al., 2003), they all include terms which are not directly related to the cost, such as the Hamming loss, and inherently the structured hinge loss cannot be consistent as it fails to converge to the performance of the optimal linear predictor in the limit of infinite training data (McAllester, 2006). In CRFs the measure of goodness is the log loss function, which is independent of the cost (Lafferty et al., 2001). Smith and Eisner (2006) tried to address this shortcoming of CRFs and proposed to minimize the risk under the Gibbs measure. While it seems that this loss function is consistent, we are not aware of any formal analysis. Recently, several works have focused on directly minimizing the expected cost. In particular, McAllester et al. (2010) presented a theorem stating that a certain perceptron-like learning rule, involving feature vectors derived from costaugmented inference, directly corresponds to the gradient of the risk. Direct loss needs two inference operations per training iteration and is extremely sensitive to its hyper-parameter. Do et al. (2008) generalized the notion of the ramp loss from binary classification to structured prediction and proposed a loss function, which is a non-convex bound to the cost, and was found to be a tighter bound than the structured hinge loss function. The structured ramp loss also needs two inference operations per training iteration. Keshet et al. (2011) generalized the notion of the binary probit loss to the structured prediction case.", "startOffset": 27, "endOffset": 1893}, {"referenceID": 1, "context": "Under this condition the update rule becomes w \u2190 (1\u2212 \u03b7\u03bb)w + \u03b7 l(y, \u0177) \u03b4\u03c6(y, \u0177w), (10) which generalizes the regularized structured perceptron\u2019s update rule (Collins, 2002; Zhang et al., 2014).", "startOffset": 156, "endOffset": 191}, {"referenceID": 18, "context": "Under this condition the update rule becomes w \u2190 (1\u2212 \u03b7\u03bb)w + \u03b7 l(y, \u0177) \u03b4\u03c6(y, \u0177w), (10) which generalizes the regularized structured perceptron\u2019s update rule (Collins, 2002; Zhang et al., 2014).", "startOffset": 156, "endOffset": 191}, {"referenceID": 6, "context": "Decomposable cost functions are needed in order to solve the cost-augmented inference that is used in the training of structural SVMs (Joachims et al., 2005; Ranjbar et al., 2013), direct loss minimization (McAllester et al.", "startOffset": 134, "endOffset": 179}, {"referenceID": 14, "context": "Decomposable cost functions are needed in order to solve the cost-augmented inference that is used in the training of structural SVMs (Joachims et al., 2005; Ranjbar et al., 2013), direct loss minimization (McAllester et al.", "startOffset": 134, "endOffset": 179}, {"referenceID": 12, "context": ", 2013), direct loss minimization (McAllester et al., 2010), or structured ramp loss (Do et al.", "startOffset": 34, "endOffset": 59}, {"referenceID": 3, "context": ", 2010), or structured ramp loss (Do et al., 2008).", "startOffset": 33, "endOffset": 50}, {"referenceID": 7, "context": "Another property of the orbit loss is its similarity to the structured probit loss (Keshet et al., 2011).", "startOffset": 83, "endOffset": 104}, {"referenceID": 7, "context": "The probit loss was derived from the concept of stochastic decoder in the PAC-Bayesian framework (McAllester, 2003, 1998) and was shown to have both good theoretical properties and practical advantages (Keshet et al., 2011).", "startOffset": 202, "endOffset": 223}, {"referenceID": 7, "context": "(16) The following theorem states a generalization bound for the probit loss function (Keshet et al., 2011).", "startOffset": 86, "endOffset": 107}, {"referenceID": 12, "context": "5 Experiments We evaluated the performance of the orbit loss by executing a number of experiments on several domains and tasks and compared the results with other approaches that are aimed at risk minimization, namely direct loss minimization (McAllester et al., 2010), structured ramp loss (Do et al.", "startOffset": 243, "endOffset": 268}, {"referenceID": 3, "context": ", 2010), structured ramp loss (Do et al., 2008), and structured probit loss (Keshet et al.", "startOffset": 30, "endOffset": 47}, {"referenceID": 7, "context": ", 2008), and structured probit loss (Keshet et al., 2011).", "startOffset": 36, "endOffset": 57}, {"referenceID": 2, "context": ", 9} can be simplified based on Kesler\u2019s construction (Crammer and Singer, 2001) as follows: wi \u2190 (1\u2212 \u03b7\u03bb)wi + \u03b7 e\u2212|wy \u00b7xi\u2212w\u00b7xi|/2 l(\u0177, yi) xi w \u2190 (1\u2212 \u03b7\u03bb)w \u2212 \u03b7 e\u2212|wy \u00b7xi\u2212w\u00b7xi|/2 l(\u0177, yi)xi w \u2190 (1\u2212 \u03b7\u03bb)w for all r 6= yi, \u0177 Note that the exponent values throughout the training were very close to 1 and, practically, the update rule (10) could be used.", "startOffset": 54, "endOffset": 80}, {"referenceID": 2, "context": "01 (Crammer and Singer, 2001).", "startOffset": 3, "endOffset": 29}, {"referenceID": 0, "context": "\u03c4 -alignment accuracy [%] \u03c4 -insensitive t \u2264 10ms t \u2264 20ms t \u2264 30ms t \u2264 40ms loss Brugnara et al. (1993)* 79.", "startOffset": 82, "endOffset": 105}, {"referenceID": 0, "context": "\u03c4 -alignment accuracy [%] \u03c4 -insensitive t \u2264 10ms t \u2264 20ms t \u2264 30ms t \u2264 40ms loss Brugnara et al. (1993)* 79.7 92.1 96.2 98.1 Keshet et al. (2007)* 75.", "startOffset": 82, "endOffset": 147}, {"referenceID": 0, "context": "For this task we used the TIMIT speech corpus for which there are published benchmark results (Brugnara et al., 1993; Hosom, 2009; Keshet et al., 2007).", "startOffset": 94, "endOffset": 151}, {"referenceID": 8, "context": "For this task we used the TIMIT speech corpus for which there are published benchmark results (Brugnara et al., 1993; Hosom, 2009; Keshet et al., 2007).", "startOffset": 94, "endOffset": 151}, {"referenceID": 0, "context": "For this task we used the TIMIT speech corpus for which there are published benchmark results (Brugnara et al., 1993; Hosom, 2009; Keshet et al., 2007). We divided a portion of the TIMIT corpus (excluding the SA1 and SA2 utterances) into three disjoint parts containing 1500, 1796 and 400 utterances, respectively. The first part was used to train a phoneme frame-based classifier, which given the pair of speech frame and a phoneme, returns the level of confidence that the phoneme was uttered in that frame. The output classifier is then used along with other features as a seven dimensional feature map \u03c6(x,y) = \u03c6((s,p),y) as described in Keshet et al. (2007). The seven dimensional weight vector w was trained on the second set of 150 aligned utterances for \u03c4 -insensitive loss", "startOffset": 95, "endOffset": 663}, {"referenceID": 15, "context": "2; the structured perceptron update rule; the structural SVM optimized using stochastic gradient descent with C=5 (Shalev-Shwartz et al., 2011); structured ramp-loss with \u03b7 = 1.", "startOffset": 114, "endOffset": 143}, {"referenceID": 4, "context": "Precise measurement of vowel duration in a given context is needed in many phonological experiments, and currently is done manually (Heller and Goldrick, 2014).", "startOffset": 132, "endOffset": 159}], "year": 2017, "abstractText": "We introduce a new surrogate loss function called orbit loss in the structured prediction framework, which has good theoretical and practical advantages. While the orbit loss is not convex, it has a simple analytical gradient and a simple perceptron-like learning rule. We analyze the new loss theoretically and state a PAC-Bayesian generalization bound. We also prove that the new loss is consistent in the strong sense; namely, the risk achieved by the set of the trained parameters approaches the infimum risk achievable by any linear decoder over the given features. Methods that are aimed at risk minimization, such as the structured ramp loss, the structured probit loss and the direct loss minimization require at least two inference operations per training iteration. In this sense, the orbit loss is more efficient as it requires only one inference operation per training iteration, while yields similar performance. We conclude the paper with an empirical comparison of the proposed loss function to the structured hinge loss, the structured ramp loss, the structured probit loss and the direct loss minimization method on several benchmark datasets and tasks.", "creator": "LaTeX with hyperref package"}}}