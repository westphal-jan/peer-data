{"id": "1512.03201", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Dec-2015", "title": "Gated networks: an inventory", "abstract": "gated networks are bundles that contain symmetrical connections, in which the outputs of at least two layers are multiplied. formally, gated communications considered implicit ; maintain relationships between two input sources, such combined pixels for two variables. as recently, definitions commonly been made encompassing learning activity recognition or multi - platform representations. the aims of prevention work differs threefold : rf ) to highlight the basic computations upon gated networks getting interested non - expert, while adopting what standpoint idea speaks on learning symmetric topology. e ) : serve as a valuable reference guide to the science literature, by running an application concerning applications targeting these networks, as well through recent extensions to underlying basic architecture. 3 ) to preview possible research directions and applications.", "histories": [["v1", "Thu, 10 Dec 2015 10:31:13 GMT  (233kb)", "http://arxiv.org/abs/1512.03201v1", "Unpublished manuscript, 17 pages"]], "COMMENTS": "Unpublished manuscript, 17 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["olivier sigaud", "cl\\'ement masson", "david filliat", "freek stulp"], "accepted": false, "id": "1512.03201"}, "pdf": {"name": "1512.03201.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["olivier.sigaud@isir.upmc.fr", "clement.masson@ensta-paristech.fr", "david.filliat@ensta-paristech.fr", "freek.stulp@ensta-paristech.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n03 20\n1v 1\n[ cs\n.L G\n] 1\n0 D\nGated networks are networks that contain gating connections, in which the outputs of at least two neurons are multiplied. Initially, gated networks were used to learn relationships between two input sources, such as pixels from two images. More recently, they have been applied to learning activity recognition or multimodal representations. The aims of this paper are threefold: 1) to explain the basic computations in gated networks to the non-expert, while adopting a standpoint that insists on their symmetric nature. 2) to serve as a quick reference guide to the recent literature, by providing an inventory of applications of these networks, as well as recent extensions to the basic architecture. 3) to suggest future research directions and applications."}, {"heading": "1 INTRODUCTION", "text": "Due to its many successful applications to pattern recognition, deep learning has become one of the most active research trends in the machine learning community (LeCun et al., 2015). The main building blocks in the deep learning literature are Restricted Boltzmann Machines (RBMs) (Smolensky, 1986), autoencoders (Hinton & Salakhutdinov, 2006; Vincent et al., 2008), Convolutional Neural Networks (CNNs) (LeCun et al., 1998) and Recurrent Neural Networks (RNNs) (Bengio, 2013).\nMost of these architectures are used to learn a relationship between a single input source and the corresponding output. They do so by building a representation of the input domain that facilitates the extraction of the adequate relationship. However, there are many domains where the representation to be learned should relate more than one source of input to the output.\nIn reinforcement learning, for instance, value functions take a state and an action as input, and output a expected return. In order to deal with continuous states and actions, finding separately the adequate representations for states and actions to facilitate value function learning might be critical (Mnih et al., 2015; Lillicrap et al., 2015). Moreover, there are cases where learning a reversible tripartite relationship might be particularly useful. For instance, in control problems, forward models take a state and an action as input, and output the next state whereas inverse models take the current state and a desired state as input, and output an action. It would be interesting to learn a single representation for both models which could be used both in the forward and the inverse way.\nGated networks are extensions of the above deep learning building blocks that are designed to learn relationships between at least two sources of input and at least one output. A defining feature of these architectures is that they contain gating connections, as visualized in Figure 1. When the relationships between several sources of data involves multiplicative interactions, such gating connections between neurons result in more natural topologies and increase the expressive power of neural networks, because implementing a multiplicative relationship between two layers of stan-\ndard neurons would require a number of dedicated neurons that would grow exponentially with the required precision (Memisevic, 2013).\nAlthough the history of gating connections dates back at least to 1981 (Memisevic, 2013), there has been a recent surge of interest in these networks. Initially they were mainly used to learn transformation between images (Memisevic & Hinton, 2007), but they have recently also been applied to human activity recognition from videos and moving skeleton data from the kinect sensor (Mocanu et al., 2015), or to recognize offspring relationship from pictures of faces (Dehghan et al., 2014).\nIn robotics, gated networks have been used to learn to write numbers (Droniou et al., 2014), as well as to learn multimodal representations of numbers using images, vocal signal and articular movements with the iCub robot (Droniou et al., 2015). At a higher level, the same tools could be used to learn affordances, that are often represented as object-action-effect complexes (Montesano et al., 2008). All these examples have led to the claim that gated networks might be a particularly suitable tool along the way towards deep developmental robotics (Sigaud & Droniou, 2016, to appear).\nDespite this growing interest, the literature about gated networks is still sparse enough so that it can be covered into a short survey. The aims of this paper are to cover the basics of gated networks for the non-expert, to serve as an inventory of applications of gated networks, and to suggest future research directions and applications.\nThe rest of this paper is structured as followed. In the next section, we give a detailed account of the calculations performed by the standard gated network model and a few variants whose relationship to the standard model is highlighted after some generalization. In this presentation, we emphasize the symmetric nature of these networks because it reveals the connections between some of the surveyed works. Then we present the standard unsupervised learning mechanisms that are used for tuning these networks, and provide an inventory of the various uses which is summarized into a table. Finally, we survey a few recent architectures that include the core ingredient of gated networks, and conclude with directions for future research."}, {"heading": "2 STANDARD GATED NETWORK ARCHITECTURES", "text": "Gated networks are networks where the input of some computational units (or \u201cneurons\u201d) is a function of the product of the output of several other neurons. As illustrated in Figure 1, one can consider two kinds of connections between 3 neurons. In the first family, a neuron h is used as a switch that stops or not the flow of information between two other neurons x and y. This functionality is very similar to that of transistors as electronic switches in digital circuits. This mechanism is used in the LSTM family of networks (Hochreiter & Schmidhuber, 1997a; Srivastava et al., 2015), among others. In the second family, the gating connection implements a multiplicative relationship between two inputs x and y. Note that the latter mechanism is more general than the former, since a value of 0 in h gates y to 0. The most general view is that neuron h modulates the signal between x and y.\nIn this paper, we focus on the specific family of neural networks implementing a multiplicative relationship that are built on RBMs and autoencoders and, to a lesser extent, on CNNs and RNNs."}, {"heading": "2.1 FROM GATED RBMS TO GATED AUTOENCODERS AND BEYOND", "text": "We now briefly introduce Restricted Boltzmann Machines (RBMs) (Smolensky, 1986), autoencoders (Hinton & Salakhutdinov, 2006; Vincent et al., 2008), Convolutional Neural Networks (CNNs) (LeCun et al., 1998) and Recurrent Neural Networks (RNNs) (Bengio, 2013), and show how these networks have been extended to contain gated connections.\nAn RBM is not a neural network but a particular probabilistic graphical model (PGM) (Koller & Friedman, 2009) whose graph is bipartite: one set (or layer) of nodes is called \u201cvisible\u201d and is used as the input of the model, whereas the other layer is \u201chidden\u201d and is interpreted as being the hidden cause explaining the input. Both layers are generally binary (though it is possible to extend them to real-valued units) and fully connected to each other. However, there are no connections within a layer, which facilitates inference and training. Training an RBM consists in finding the parameters (edge\u2019s weights and node\u2019s bias) which maximize the likelihood of the training data. Importantly, RBMs are generative models: they can model the probability density of the joint distribution of visible and hidden units, which enables them to generate samples similar to those of the training data onto the visible layer.\nThe first instance of a gated network in the deep learning literature was a gated RBM (GRBM) (Memisevic & Hinton, 2007). However, this model was using a fully connected multiplicative network that required a lot of memory and computations for inference and training. In the next section, we present a solution to this issue, that was introduced by Memisevic & Hinton (2010) as a direct extension of (Memisevic & Hinton, 2007), still using GRBMs.\nAutoencoders also contain an input and a representation layer but, in contrast to RBMs, they are deterministic models. They are trained to encode the input into the latent representation layer and then to reconstruct (or decode) the input from that representation. In their basic form, they are discriminative models, which can only compute the hidden layer given an input. It was then shown that a particular class of regularized autoencoder, the denoising autoencoder (DAE), could learn a model of the data generating distribution. This endow autoencoders with generative properties similar to those of RBMs (Vincent et al., 2008). More formally, a DAE can be interpreted as a Gaussian RBM (Vincent, 2011).\nThis led to a shift from GRBMs to gated autoencoders (GAEs) (Memisevic, 2008; 2011; 2012) though research on GRBMs is still active (Taylor et al., 2010; Ding & Taylor, 2014).\nConvolutional Neural Networks are an early family of deep learning architectures which are composed of alternating convolutional layers and pooling layers. They are inspired from the human vision system and they proved particularly efficient for image processing applications. Finally, RNNs contain at least one recurrent connection, which makes them adequate for dealing with temporally extended information (Hochreiter & Schmidhuber, 1997b).\nThe gating idea was also applied to RNNs (Sutskever et al., 2011) and CNNs, either combined to GRBMs (Taylor et al., 2010) or to GAEs (Konda & Memisevic, 2015), as we outline in Section 5."}, {"heading": "2.2 REDUCING THE NUMBER OF MULTIPLICATIVE CONNECTIONS", "text": "Implementing a gated network requires memory. Consider the network shown in Figure 2(a), consisting of three layers x, y and h1 whose respective cardinality is nx, ny and nh. Predicting the output layer y\u0302 given x and h with such a multiplicative network consists in computing all the values y\u0302j of y\u0302 using\n\u2200j, y\u0302j = \u03c3y(\nnx\u2211\ni=1\nnh\u2211\nk=1\nWijkxihk) (1)\nwhere \u03c3y is some (optional) non-linear activation function described in more details in Section 2.4.\nAlternatively, one may compute x\u0302 given y and h or compute h\u0302 given x and y using\n1Throughout this document, bold lowercase symbols denote vectors, and bold uppercase symbols denote matrices.\n\u2200i, x\u0302i = \u03c3x(\nny\u2211\nj=1\nnh\u2211\nk=1\nWijkyjhk), \u2200k, h\u0302k = \u03c3h(\nnx\u2211\ni=1\nny\u2211\nj=1\nWijkxiyj).\nRegardless of the \u03c3 functions, these models are called bilinear because, if one input is held fixed, the output is linear in the other input.\nThe weights Wijk define a 3-way tensor, which is used to compute x\u0302, y\u0302 or h\u0302 given both other vectors. This tensor contains nx \u00d7 ny \u00d7 nh connections. If nx, ny and nh are in the same order of magnitude, the number of weights (aka parameters) is cubic in this magnitude.\nFactored architectures are designed to avoid representing this cubic number of weights. Two ways to reduce this memory requirement are:\n\u2022 Projecting the input and output, potentially high-dimensional signals, into a smaller space through factor layers, and then performing the central product between these smaller dimensions.\n\u2022 Constraining the structure of the global 3-way tensor so as to restrict the number of weights.\nIn the next two sections, we show that the standard gated network takes the best of both views, by setting a constraint on the 3-way tensor structure that implements a projection onto factor layers, but that also avoids representing the full central product. Another striking feature of this architecture is that the resulting central product does not contain any tunable parameter."}, {"heading": "2.2.1 PROJECTING ONTO FACTOR LAYERS", "text": "One way of reducing the number of weights consists in projecting the x, y and h layers onto smaller layers noted respectively fx, fy and fh before performing the product between these smaller layers. Given their multiplicative role, these layers are called \u201cfactor\u201d layers. The corresponding approach is illustrated in Figure 2(b). If the respective cardinality of the factors is nfx , nfy and nfh , the number of weights of the central 3-way tensor is nfx \u00d7 nfy \u00d7 nfh . To tune the whole network, additional weights must be added to this 3-way tensor, respectively nx \u00d7 nfx , ny \u00d7 nfy and nh \u00d7 nfh for each layer, so the total number of weights is (nfx \u00d7nfy \u00d7nfh)+ (nx\u00d7nfx)+ (ny \u00d7nfy )+ (nh\u00d7nfh).\nIn summary, the two input layers amongx, y andh are first projected onto feature spaces through the corresponding factor layers, then the central 3-way multiplication is performed and finally projected to the output layer through the last factor layer.\nThis approach, suggested by Memisevic & Hinton (2007), results in fewer tunable parameters provided that factor layers contain fewer neurons than the input layers. In that case, the network performs dimensionality reduction on the inputs before tuning the multiplicative weights between the factors. As a result, the number of weights is still cubic, but of a smaller magnitude. A second benefit of this architecture is that, in contrast to the one illustrated in Figure 2(a), the introduction of factors results in the possibility of feature sharing between the different external layers (Memisevic et al.,\n2010). However, to the best of our knowledge, this way of reducing the number of parameters of the gated architecture has not yet been implemented.\nAnother approach, which is used in all the works surveyed hereafter, consists in rather calling upon over-complete representations (Olshausen, 2003), where factor layers are larger than the input space, but a regularization method like denoising (Vincent, 2011) is used to sparsify the activity of the factors. In this context, introducing the factor layers does not reduce the number of parameters, it even increases it (Memisevic, 2013)."}, {"heading": "2.2.2 CONSTRAINING THE 3-WAY TENSOR", "text": "Another way of reducing the number of parameters consists in restricting the weights Wijk to follow a specific form\nWijk =\nF\u2211\nf=1\nW xifW y jfW h kf . (2)\nWith this constraint, the matrices Wx, Wy and Wh are of respective size nx \u00d7 nf , ny \u00d7 nf and nh \u00d7 nf , thus the total number of weights is just nf \u00d7 (nx + ny + nz), which is quadratic instead of cubic in the size of input or factors.\nConsider again the case where y\u0302 is predicted given x and h. Equation (1) can be rewritten as\n\u2200j, y\u0302j = \u03c3y(\nnx\u2211\ni=1\nnh\u2211\nk=1\nF\u2211\nf=1\nW xifW y jfW h kfxihk), (3)\nwhich can be reorganized into\n\u2200j, y\u0302j = \u03c3y(\nF\u2211\nf=1\nW y jf (\nnx\u2211\ni=1\nW xifxi)(\nnh\u2211\nk=1\nWhkfhk)). (4)\nBy noting\nfxf =\nnx\u2211\ni=1\nW xifxi, f y f = (\nny\u2211\nj=1\nW y jfyj), f h f = (\nnh\u2211\nk=1\nWhkfhk), (5)\nwe finally get\n\u2200j, y\u0302j = \u03c3y(\nF\u2211\nf=1\nW y jff x f .f h f ). (6)\nThe three equations in (5) define three factor layers as explained in Section 2.2.1 and illustrated in Figure 2(b). However, when looking at the structure of (6), one can see that, instead of having a full central product, the output of both factor layers \u2013 fx and fh in the case of (6) \u2013 are multiplied element-wise through the same index f , as illustrated in Figure 2(c).\nThus, using the decomposition of (6), it can be seen that this way of constraining the 3-way tensor corresponds to using projections as in the previous view, but with three factor layers fx, fy and fh of the same size nf , and where the central 3-way tensor has been replaced by 3 two-by-two element-wise products of the factor layers.\nWith a more algebraic notation, (5) can be rewritten\nfx = Wx\u22bax, fy = Wy\u22bay, fh = Wh \u22ba h. (7)\nIn this notation, we omit the representation of an additive bias term by considering the inputs as being a homogeneous representation with an additional constant value, in which biases are implemented implicitly. Equation (6) then becomes\ny\u0302 = \u03c3y(W y(fx \u2297 fh)), (8)\nwhere \u2297 denotes the element-wise multiplication illustrated in Figure 3(b).\nAgain, the same decomposition can be applied to predict h\u0302 given x and y or to predict x\u0302 given y and h, giving rise to\nx\u0302 = \u03c3x(W x(fy \u2297 fh)), h\u0302 = \u03c3h(W h(fx \u2297 fy)). (9)\nA slightly more general version of the same architecture that insists on its symmetric nature can be obtained by noting Wxin, W y in and W h in the matrices oriented from the input layers towards the factors, and Wxout, W y out and W h out those oriented from the factors towards the output. The corresponding architecture is depicted in Figure 3(b).\nFollowing these notations, if we consider computations from the input layers to the factors, the red arrows correspond to\nfxin = W x inx, f y in = W y iny, f h in = W h inh.\nIn the other way, from the factors to other factors, the blue arrows correspond to\nfxout = f y in \u2297 f h in , f y out = f x in \u2297 f h in , f h out = f x in \u2297 f y in.\nFinally, towards the output we have\nx\u0302 = \u03c3x(W x outf x out), y\u0302 = \u03c3y(W y outf y out), h\u0302 = \u03c3h(W h outf h out).\nBy connecting the above elements together, the complete input-output functions are\nh\u0302 = o(x,y) = \u03c3h(W h out((W x inx)\u2297 (W y iny))), (10)\nx\u0302 = p(y,h) = \u03c3x(W x out((W y iny)\u2297 (W h inh))), (11) y\u0302 = q(x,h) = \u03c3y(W y out((W x inx)\u2297 (W h inh))). (12)\nEquations (10) to (12) are identical to (8) and (9), and thus they implement (2), provided that the following weight tying rules are used2: Wx = Wxin = W x out \u22ba, Wy = Wyin = W y out\n\u22ba and Wh = Whin = W h out \u22ba\n. A benefit of using such weight tying rules is that it further reduces the number of parameters. Besides, any pair of the sub-networks described in (10) to (12) shares just one input matrix.\nFrom the above presentation, it should be clear that the standard gated network architecture is completely symmetric: the functional role of the x, y and h layers can be exchanged without changing the computations.\n2Different papers choose different conventions for deciding which matrix is the original and which is the transposed, see for instance (Im & Taylor, 2014), giving rise to different equations to implement (10) to (12)."}, {"heading": "2.3 VARIATIONS ON THE CENTRAL TENSOR", "text": "The architecture outlined in Section 2.2.2 can be seen either as a particular way to parametrize the global 3-way tensor, introducing features into its internal structure, or as a way to replace the central tensor of the approach outlined in Section 2.2.1 by an element-wise product of factor layers. This approach to implementing the central 3-way tensor can be seen as a degenerate case where all its non-diagonal elements are null and its diagonal elements are all set to 1. With this definition, the central product does not contain any tunable parameters. Instead, representation learning is implemented by tuning the weights of the Wx, Wy and Wh matrices connecting the external layers to the factors. Note that using parameters instead of ones onto the diagonal may increase the flexibility of the model for learning, but it would not improve its expressive power, since the effect of changing these parameters can be captured by changing the parameters of the W matrices.\nThe constraint given in (2) is somewhat arbitrary. For instance, the central computation of a gated architecture can be more complex than a simple element-wise product of factors. The architecture proposed in (Droniou & Sigaud, 2013) is an instance of such more complex computation. As outlined in Figure 4, it also uses factors and a parameter-free tensor, but the structure of the central tensor has been specifically designed to learn orthogonal transformations. Several motivations for performing the corresponding computations are given in (Droniou & Sigaud, 2013), together with the detailed mathematical rationale for such computations.\nNote also that, in this architecture, the weight tying rules are unusual. Instead of having Win = Wout \u22ba for all factors, Whin and W h out are untied and W x in = W y in, with standard input-output weight tying rules on the x and y layers, i.e. Wx = Wxin = W x out \u22ba and Wy = Wyin = W y out\n\u22ba. A consequence of this choice is that the model might not be interpreted as an energy-based dynamical system, since Whin = W h out \u22ba\nis required so that Poincare\u0301\u2019s integrability criterion holds (Im & Taylor, 2014)."}, {"heading": "2.4 ACTIVATION FUNCTIONS", "text": "The x, y and h layers can be either binary of real-valued. Depending on this format, different nonlinear activation functions are used, resulting in different functionalities assigned to the network.\nWhen the content of a size n layer is binary, there are two options. First, it can represent 2n elements of a discrete set using standard binary coding rules. In that case, either the model directly represents the probability of each binary value, as is the case in RBMs, or the binary values are obtained from real-valued numbers by using a threshold. This latter case is uncommon because of the non-differentiability of the threshold function. The second option is to represent only n ele-\nments using a \u201cone-hot\u201d representation where one bit is set to 1 and all the others are 0. When the corresponding layer is used as input, this representation is easy to enforce from the external world.\nFor real-valued layers, the role of the activation function is to constrain the values of the output layer into a bounded domain, which can be obtained for instance with a sigmoid or a rectified linear unit, the latter being more popular in large network due to its faster computation. One can also use the softplus function \u03c3+, defined as \u03c3+(x) = log(1+exp(x)), which is a smooth version of the rectified linear unit (Glorot et al., 2011).\nTo get a representation that is close toa \u201cone-hot\u201d in a real-valued output layer, the activity of the most active neuron in that layer can be highlighted by using the softmax function. If used for instance on the h layer of a gated network, the softmax function is\nh = \u03c3max(W h outfh), (13)\nwhere\n\u03c3imax(h = (h1, . . . , hn)) = ehi\u2211 j e hj . (14)\nIn addition to highlighting the most active neuron(s), this function makes sure that all activities sum to 1. Hereafter, we call the obtained representation a \u201csoft one-hot\u201d.\nFinally, if a binary \u201cone-hot\u201d representation was required as output, one could apply a postprocessing \u201cwinner-takes-all\u201d function to a softmax layer, but we are not aware of any such use."}, {"heading": "3 LEARNING IN GATED NETWORKS", "text": "Gated networks have two input layers and one output layer. One way to train such networks would be to use supervised learning: for a given pair of input layers, one would provide the expected output, and then train the network to minimize a function of the error between the expected and the obtained output. This is used in gated CNNs and gated RNNs (see Section 5). But GRBMs and GAEs are not trained in this way. Instead, the training process is designed to perform unsupervised learning, but differs between GRBMs and GAEs. In this paper, we do not cover training GRBMs, which is based on training RBMs. We refer the reader to (Swersky et al., 2010) for a clear presentation of the latter topic. Instead, we focus on training GAEs.\nGiven two input layers, GAEs are trained to reconstruct one of them. In order to explain this learning process, it is useful to recap how it is performed in autoencoders.\nAn autoencoder is composed of two functions:\n\u2022 The encoding function that transforms the input vector x into a latent representation h. A typical function is h = h(x) = \u03c3h(Wx+ b).\n\u2022 The decoding function that reconstructs a representation x\u0302 of x from its latent representation h. A typical function is x\u0302 = r(h) = \u03c3x(W\u2032h+ b\u2032).\nThe cost function for autoencoders is generally related to the reconstruction error. This error is for instance the distance between x and x\u0302, typically the squared error ||x\u0302 \u2212 x||2. Learning then corresponds to applying an optimization algorithm such as a gradient-descent to the weights of the network so as to minimize this cost function. Thus, during training, the network learns the encoding function and the decoding function simultaneously, using x\u0302 = \u03c3x(W\u2032\u03c3h(Wx + b) + b\u2032). The main outcome of this learning process is the generation of the latent representation h, that must be informative enough about the input so as to allow its correct reconstruction.\nTo highlight the relationship between autoencoders and GAEs, we now consider that h is the latent representation and x and y are the input layers. Recalling (10) to (12), there are two ways to define a GAE as equivalent to an autoencoder. The encoding function is always h = o(x,y), while the decoding function can be either\nx\u0302 = p(y,h) = p(y,o(x,y)) (15)\nor\ny\u0302 = q(x,h) = q(x,o(x,y)). (16)\nUsing (10) to (12), (15) can be rewritten\nx\u0302 = \u03c3x(W x out((W y iny)\u2297 (W h in\u03c3h(W h out((W x inx)\u2297 (W y iny)))))), (17)\nand (16) can be rewritten\ny\u0302 = \u03c3y(W y out((W x inx)\u2297 (W h in\u03c3h(W h out((W x inx)\u2297 (W y iny)))))). (18)\nOne can note that Wyout does not appear in (17) and W x out does not appear in (18), thus tuning these weights is not useful during training unless adequate weight tying rules are applied.\nAs outlined in Section 2.1, autoencoders can be endowed with properties similar to those of RBMs by using a denoising regularization function. There are three kinds of such functions, namely Gaussian noise, masking noise and salt and pepper noise (Rudy & Taylor, 2014). It is commonplace to apply to GAEs these regularization functions as they are to autoencoders. They are generally applied to all factor layers, but there are some exceptions. For instance, in (Rudy & Taylor, 2014), the denoising function is applied to x only.\nImportantly, minimizing the squared reconstruction error of a DAE implements a regularized form of score matching (Vincent, 2011), which is itself a training criterion that favors the encoding of the manifolds where most of the input data is lying (Hyva\u0308rinen, 2005). The same applies to GAEs, but the nature of the represented manifolds depends on the encoded input-output relationships and on the format of the external layers. Besides, some other regularizations functions such as dropout (Srivastava et al., 2014) might also be applied to GAEs, but we are not aware of any work in this direction. For other practical hints on training gated networks, see also (Memisevic, 2013).\nFinally, the back-propagation algorithm can perform gradient descent on the weights of some or all the implied W matrices.\nTaken together, the reconstruction function, the regularization function and the learning rules define many different settings to learn representations with GAEs. We study other combinatorial aspects in the next section."}, {"heading": "4 APPLICATIONS OF GATED NETWORKS", "text": "Given what we have presented so far, there are three respects in which the use of gated networks may vary. First, as outlined in Section 2.4 the content of the x, y and h layers is either binary, onehot or real-valued. Second, as outlined in Section 3, gated networks can be trained in various ways using various training signals, regularization functions and cost functions. Third, different layers can be used either as input or output. All these variations give rise to different functional roles for the corresponding networks. The goal of this section is to make an inventory of such uses in the literature, which is finally summarized in Table 1."}, {"heading": "4.1 FORMAT OF THE EXTERNAL LAYERS", "text": "In Section 2.4, we outlined the different activation functions that are used to deal with different format of the external layers. Here, we recapitulate the use of these formats in different models.\nFirst, in all GRBMs, the h layer always uses standard binary encoding (Memisevic & Hinton, 2007; 2010).\nFurthermore, most models use pixels of two images as x and y input. The transformation between these images stored in h is either binary (Memisevic & Hinton, 2007; 2010) or real-valued (Droniou & Sigaud, 2013; Dehghan et al., 2014). In both cases, what is learned is a manifold of the pixels in the x conditioned on those of the y layer, or vice versa (Memisevic & Hinton, 2007).\nThere are two models where the y layer is binary. First, the gated softmax classification model was used in the context of logistic regression, i.e. classification using a log-linear model, where\nthe output y\u0302 consisted of binary class labels, and the values of the h layer were also binary (Memisevic et al., 2010).\nMore recently, in the context of studying the generative property of GAEs, a model was proposed where the y input also consists of a class-conditional, one-hot representation, whereas h is a realvalued representation constrained by a rectified linear unit (Rudy & Taylor, 2014). The network is trained to regenerate examples from the MNIST and Toronto Faces Database images, thus x is a vector of pixels. In this context, the model represents class-conditional manifolds, i.e. a set of manifolds of the input data x with one manifold per corresponding class in y. As the authors state, this use of the GAE \u201cis akin to learning a separate DAE model for each class, but with significant weight sharing between the models. In this light, the gating acts as a means of modulating the model\u2019s weights depending on the class label\u201d (Rudy & Taylor, 2014)."}, {"heading": "4.2 TRAINING SIGNAL", "text": "As outlined in Section 3, GAEs can be trained to reconstruct either x\u0302 or y\u0302. When the input data is binary, the cross-entropy loss function is the default choice (Rudy & Taylor, 2014). When it is real-valued, the standard cost function is a squared reconstruction error. Therefore, when training to reconstruct x\u0302, it is J = 1\n2 ||(x\u0302|y) \u2212 x||2, whereas for reconstructing y\u0302, it is J = 1 2 ||(y\u0302|x)\u2212 y||2.\nThe first option is the one chosen in (Rudy & Taylor, 2014). This makes the connection to autoencoders more explicit because they both take x as input and x\u0302 as output. But this contrasts with the rest of the literature, where it is more common to train to reconstruct y\u0302 (Memisevic & Hinton, 2007; 2010; Memisevic et al., 2010; Droniou & Sigaud, 2013; Michalski et al., 2014b;a).\nA third option exists. If we want the model to be able to reconstruct x\u0302 given y and y\u0302 given x at the same time, we can use (Memisevic, 2011):\nJ = 1\n2 ||(x\u0302|y) \u2212 x||2 +\n1 2 ||(y\u0302|x) \u2212 y||2. (19)\nA particularly relevant case for using this symmetric signal is the case where x = y. In that case, the mapping units h learn covariances within x (Memisevic, 2011).\nInterestingly, a model recognizing offspring relationship from pictures of faces combines generative and discriminative training, using two training signals (Dehghan et al., 2014). From one side, it learns a representation of the transformation between two faces using the symmetric cost function given in (19). But it also tries to determine offspring relationship as a binary representation, so it uses a softmax cost function during a supervised label learning process. Finally, both cost functions are combined into a weighted sum."}, {"heading": "4.3 INPUT-OUTPUT FUNCTION", "text": "We have outlined in Section 2.2.2 that the role of the x, y and h layers could be exchanged. This leads to three permutations where two layers among x, y and h are inputs, the third layer being the output. However, given the unsupervised training procedure described in Section 3, we see that, in addition to the three possibilities outlined above, one can also use it to predict either x\u0302 or y\u0302. Under this view, learning the latent representation h is a side effect, h being used neither as input nor as output, but being \u201creinjected\u201d into the network to reconstruct one of the input layers. The same fact applies mutatis mutandis to all other layers.\nThe different possible output layers result in two main ways to use a gated network. The first one, the predictive coding view, consists in inferring an output y\u0302 (or x\u0302) given an input x and a context h. The temporal predictive coding view is a special case of the above, with xt as input and xt+1 as output. The second one, the transformation coding view, consists in using the latent representation h as output, given two input vectors x and y. The output layer h then expresses some relations between x and y, which may provide abstract representations that can be used for instance in higher level decision modules.\nThe latter view is mostly used to learn transformations between two successive images, so as to extract features containing temporal information (Memisevic & Hinton, 2007; 2010; Memisevic et al., 2010; Droniou & Sigaud, 2013; Michalski et al., 2014b;a). In this context, the input vectors x and\ny are successive images, for instance from a video. The extracted transformations h are contentindependent. For instance, they can represent rotations, independently from what is rotated in the images. Furthermore, they convey a temporal information about these successive images, thus they can be used as elementary features in a higher level to model some temporal information. However, we are not aware of any architecture where these temporal features are actually used to extract temporally extended information from videos, apart from very preliminary attempts among 3 or 4 successive frames in (Michalski et al., 2014b) using a hierarchical sequence of GAEs (see Section 5). The work of (Dehghan et al., 2014) is another instance of the transformation coding view, but where x and y are temporally independent images.\nIn many papers, both the transformation representation h and the reconstructed input signal x\u0302 or y\u0302 are studied. As a consequence, in the absence of an external architecture that uses it, it is often hard to determine which of these signals should be considered as the output of the network. Moreover, it is often the case that, when learning transformations between two successive images, the learned transformation is then applied to a new input image to see what output image is \u201cfantasized\u201d by the network, performing a type of \u201canalogy making\u201d. In this context, the output of the network is both h\u0302 and y\u0302 (Memisevic & Hinton, 2007; 2010; Memisevic et al., 2010; Droniou & Sigaud, 2013; Michalski et al., 2014b;a). Thus in Table 1, we do not strive to determine which layer is the output of the studied algorithm."}, {"heading": "4.3.1 SUMMARY: AN INVENTORY", "text": "Table 1 summarizes many uses of the standard gated networks listed above.\nTable 1 illustrates that there is a wide variety of ways to use gated networks. This variety is even greater if we also consider the non-standard architectures surveyed in the next section."}, {"heading": "5 BEYOND STANDARD GATED ARCHITECTURES", "text": "In this section, we describe a few architectures that contain a gated network. First, we list some architectures where the central tensor connects more than 3 layers. Then, we present some architectures whose set of connections is not restricted to the central tensor."}, {"heading": "5.1 EXTENDED TENSORS", "text": "There are some architectures where the central tensor connects more than 3 external layers. Conditional RBMs (CRBMs) are RBMs where some memory of the past input are included into the input layer so that the architecture can model time-dependent data (Taylor & Hinton, 2009). In (Taylor et al., 2011), a CRBM is used to model human motion data but, as illustrated in Figure 5, it is extended with an additional style layer to model different styles of motion.\nThe x layer corresponds to the motion input at previous time step. The y layer, which is the output, corresponds to the predicted motion at the current time step. The h layer is used as in all GRBMs to learn a representation of the transformation between x and y. But, additionally, the z layer\ncorresponds to real-valued stylistic features that are fed by discrete style labels (encoded in the s layer) and provide some additional contextual information about the motion.\nThe resulting architecture is then factored as described above so as to limit the number of parameters, but it is designed in such a way that the 4 factors are only connected together by triplets using factored 3-way tensors.\nMore recently, a \u201c4-way tensor\u201d and its factorization were introduced based in GRBMs (Mocanu et al., 2015). The central factored operation consists in performing a sum of products of second order tensors. The models of (Taylor & Hinton, 2009) and (Mocanu et al., 2015) are both capable of representing sequential data in the limit of the N previous time steps included in the memory concatenated to the input layer."}, {"heading": "5.2 CLUSTERING WITH GATED NETWORKS", "text": "In some architectures, the central 3-way tensor is not the only ingredient. For instance, the architecture depicted in Figure 6 uses an additional autoencoding connection with respect to a standard GAE (Droniou et al., 2015).\nThe network aims at clustering input data into \u201cconcepts\u201d corresponding to manifolds in the input layer, without using supervised learning. For doing so, the input x is first fed into a standard autoencoder using a softmax activation function that performs unsupervised clustering of the input data.\nThe softmax activation function implements a competition between the bits of the class layer and results in the emergence of a soft one-hot representation of the corresponding class. Then, given the input and the obtained class, the h layer implements a parametrization of the input with respect to the class, using a softplus layer, i.e. h = \u03c3+(Whoutfh).\nSince it uses a soft one-hot, class-conditional y layer and a real-valued input x layer, this model can be seen as a direct extension of the one presented in (Rudy & Taylor, 2014). However, since the weights are trained simultaneously, the network in Figure 6 finds the adequate classes to represent the data with an accurate parametrization by itself, instead of requiring them as training labels. This endows the network with unsupervised clustering capabilities that are well beyond those of standard dimensionality reduction techniques. This model is then extended to deal with multimodal information, showing an even improved clustering performance. We do not further study this aspect here, see (Droniou et al., 2015) for more details."}, {"heading": "5.3 RECURRENT GATED NETWORKS", "text": "Another architecture based on factoring gating connections is the \u201cMultiplicative RNN\u201d architecture (Sutskever et al., 2011) depicted in Figure 7. This is a recurrent architecture trained to deal with temporally organized information such a text or speech signal.\nThe key requirement of the architecture is that the recurrent connection responsible for the dynamics of the hidden variable should be a function of the input layer x. This would lead to a full 3-way tensor, which the authors factorize as described in Section 2.2 to reduce the number of free parameters. With slightly adapted notations to highlight the similarity with other architectures, the internal computation of the multiplicative RNN is given by the following equations:\nft = diag(Wfxxt).Wfhht\u22121 (20)\nht = tanh(Whfft +Whxxt) (21) y\u0302t = W y outht + by. (22)\nA key difference between this work and the other ones presented above is that the architecture is trained in a supervised way, rather than trained to reconstruct its input. The focus is thus not on extracting an abstract representation of the input. Another originality is that, instead of being trained with a standard first order gradient descent algorithm such as back-propagation, the architecture is trained using a second order method based on Hessian-free optimization (Martens, 2010). To our knowledge, despite its efficiency, no other gated network has been trained with this method."}, {"heading": "5.4 CONVOLUTIONAL GATED NETWORKS", "text": "Convolution is a technique which consists in processing a large image by shifting a smaller filter to any position in the image and applying it over all positions. For instance, the same filter can be applied to recognize a pattern at any position in the image. Convolutional gated networks apply the convolution idea to gated networks. This has been done in GRBMs (Taylor et al., 2010) so as to extract spatio-temporal features in the context of human activity recognition, and in GAEs (Konda & Memisevic, 2015) to perform visual odometry from stereo pairs in a sequence of images captured from a moving camera."}, {"heading": "5.5 PREDICTION WITH A SEQUENCE OF GATED NETWORKS", "text": "Another architecture models temporal data using a sequence of GAEs (Michalski et al., 2014b)3. Beyond a sequence, it even uses a hierarchy of GAEs to learn transformations of transformations. The model of (Michalski et al., 2014b), called Predictive Gating Pyramides (PGP), cascades two level of GAEs to predict sequences. As the authors state, the reconstruction error is inadequate in their context, thus the model is trained explictly to predict rather than to reconstruct. Actually, it is trained to predict over multiple steps. A strong assumption in PGP is that the highest-order relational structure in the sequence is constant. It uses Back-Propagation Through Time (BPTT) to perform gradient descent on the weights over time. However, the model is used to learn temporal features, it does not predict long sequences of images. And a major drawback is that the architecture requires as many GAEs as time steps."}, {"heading": "6 CONCLUSION", "text": "In this paper, we have based our presentation of gated networks on a perspective that insists on their symmetric nature. Based on this particular perspective, we could highlight its richness by providing an inventory of the various ways they have been used so far in the literature. Given this richness, we believe standard gated networks still have a largely underexploited potential as a unifying tool for many domains where the relevant information is naturally expressed as tripartite relationships between three interdependent sources. Apart from the ones proposed in this paper, we hope many other application domains to gated networks will emerge in the next years.\nFurthermore, as pointed out in Section 5, there are still rather few non-standard architectures based on the factored gating idea. We believe the list of such architectures will expand in the future, and also that gated networks should be included into more general frameworks that may contain several instances of such networks, as is already the case with (Michalski et al., 2014b) or (Droniou et al., 2014).\nFinally, among other things, an interesting perspective to this work consists in combining it with the contextual learning perspective (Jonschkowski et al., 2015). Indeed, several contextual learning patterns might be implemented with gated networks, and some works about representation learning with gated networks might be interpreted in the framework of contextual learning."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the European Union\u2019s Horizon 2020 research and innovation program within the DREAM project under grant agreement No 640891."}], "references": [{"title": "Deep learning of representations: Looking forward", "author": ["Bengio", "Yoshua"], "venue": "In Statistical language and speech processing,", "citeRegEx": "Bengio and Yoshua.,? \\Q2013\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2013}, {"title": "Who do I look like? determining parent-offspring resemblance via gated autoencoders", "author": ["Dehghan", "Afshin", "Ortiz", "Enrique G", "Villegas", "Ruben", "Shah", "Mubarak"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Dehghan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dehghan et al\\.", "year": 2014}, {"title": "Mental rotation\u201d by optimizing transforming distance", "author": ["Ding", "Weiguang", "Taylor", "Graham W"], "venue": "arXiv preprint arXiv:1406.3010,", "citeRegEx": "Ding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Apprentissage de repr\u00e9sentations et robotique d\u00e9veloppementale: quelques apports de l\u2019apprentissage profond pour la robotique autonome", "author": ["Droniou", "Alain"], "venue": "PhD thesis, UPMC-Paris", "citeRegEx": "Droniou and Alain.,? \\Q2015\\E", "shortCiteRegEx": "Droniou and Alain.", "year": 2015}, {"title": "Gated autoencoders with tied input weights", "author": ["Droniou", "Alain", "Sigaud", "Olivier"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Droniou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Droniou et al\\.", "year": 2013}, {"title": "Learning a repertoire of actions with deep neural networks", "author": ["Droniou", "Alain", "Ivaldi", "Serena", "Sigaud", "Olivier"], "venue": "In ICDL-Epirob, pp", "citeRegEx": "Droniou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Droniou et al\\.", "year": 2014}, {"title": "A deep unsupervised network for multimodal perception, representation and classification", "author": ["Droniou", "Alain", "Ivaldi", "Serena", "Sigaud", "Olivier"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Droniou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Droniou et al\\.", "year": 2015}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Reducing the Dimensionality of Data with", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Neural Networks. Science,", "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "LSTM can solve hard long time lag problems", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in Neural Information Processing Systems 9: Proceedings of the 1996 Conference,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "LSTM can solve hard long time lag problems", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in Neural Information Processing Systems 9: Proceedings of the 1996 Conference,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Estimation of non-normalized statistical models by score matching", "author": ["Hyv\u00e4rinen", "Aapo"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Hyv\u00e4rinen and Aapo.,? \\Q2005\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Aapo.", "year": 2005}, {"title": "Analyzing the dynamics of gated auto-encoders in practice", "author": ["Im", "Daniel Jiwoong", "Taylor", "Graham W"], "venue": "Unpublished manuscript?,", "citeRegEx": "Im et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Im et al\\.", "year": 2014}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Koller", "Daphne", "Friedman", "Nir"], "venue": "MIT press,", "citeRegEx": "Koller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2009}, {"title": "Learning visual odometry with a convolutional network", "author": ["Konda", "Kishore", "Memisevic", "Roland"], "venue": "In International Conference on Computer Vision Theory and Applications,", "citeRegEx": "Konda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Konda et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Deep learning via hessian-free optimization", "author": ["Martens", "James"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Martens and James.,? \\Q2010\\E", "shortCiteRegEx": "Martens and James.", "year": 2010}, {"title": "Non-linear latent factor models for revealing structure in high-dimensional data", "author": ["Memisevic", "Roland"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "Memisevic and Roland.,? \\Q2008\\E", "shortCiteRegEx": "Memisevic and Roland.", "year": 2008}, {"title": "Gradient-based learning of higher-order image features", "author": ["Memisevic", "Roland"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Memisevic and Roland.,? \\Q2011\\E", "shortCiteRegEx": "Memisevic and Roland.", "year": 2011}, {"title": "On multi-view feature learning", "author": ["Memisevic", "Roland"], "venue": "In Proceedings of the 28th Annual International Conference on Machine Learning, pp", "citeRegEx": "Memisevic and Roland.,? \\Q2012\\E", "shortCiteRegEx": "Memisevic and Roland.", "year": 2012}, {"title": "Learning to relate images", "author": ["Memisevic", "Roland"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Memisevic and Roland.,? \\Q2013\\E", "shortCiteRegEx": "Memisevic and Roland.", "year": 2013}, {"title": "Unsupervised learning of image transformations", "author": ["Memisevic", "Roland", "Hinton", "Geoffrey E"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Memisevic et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Memisevic et al\\.", "year": 2007}, {"title": "Learning to represent spatial transformations with factored higher-order boltzmann machines", "author": ["Memisevic", "Roland", "Hinton", "Geoffrey E"], "venue": "Neural Computation,", "citeRegEx": "Memisevic et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Memisevic et al\\.", "year": 2010}, {"title": "Gated softmax classification", "author": ["Memisevic", "Roland", "Zach", "Christopher", "Hinton", "Geoffrey E", "Pollefeys", "Marc"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Memisevic et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Memisevic et al\\.", "year": 2010}, {"title": "Modeling sequential data using higher-order relational features and predictive training", "author": ["Michalski", "Vincent", "Memisevic", "Roland", "Konda", "Kishore"], "venue": "arXiv preprint arXiv:1402.2333,", "citeRegEx": "Michalski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Michalski et al\\.", "year": 2014}, {"title": "Modeling deep temporal dependencies with recurrent \u201dgrammar cells", "author": ["Michalski", "Vincent", "Memisevic", "Roland", "Konda", "Kishore"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Michalski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Michalski et al\\.", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Factored four way conditional restricted boltzmann machines for activity recognition", "author": ["Mocanu", "Decebal Constantin", "Ammar", "Haitham Bou", "Lowet", "Dietwig", "Driessens", "Kurt", "Liotta", "Antonio", "Weiss", "Gerhard", "Tuyls", "Karl"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Mocanu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mocanu et al\\.", "year": 2015}, {"title": "Learning object affordances: From sensory\u2013motor coordination to imitation", "author": ["L. Montesano", "M. Lopes", "A. Bernardino", "J. Santos-Victor"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "Montesano et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Montesano et al\\.", "year": 2008}, {"title": "Principles of image representation in visual cortex", "author": ["Olshausen", "Bruno A"], "venue": "The visual neurosciences,", "citeRegEx": "Olshausen and A.,? \\Q2003\\E", "shortCiteRegEx": "Olshausen and A.", "year": 2003}, {"title": "Generative class-conditional autoencoders", "author": ["Rudy", "Jan", "Taylor", "Graham W"], "venue": "arXiv preprint arXiv:1412.7009,", "citeRegEx": "Rudy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rudy et al\\.", "year": 2014}, {"title": "Towards deep developmental learning", "author": ["Sigaud", "Olivier", "Droniou", "Alain"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Sigaud et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sigaud et al\\.", "year": 2016}, {"title": "Information processing in dynamical systems: foundations of harmony theory", "author": ["Smolensky", "Paul"], "venue": "In Parallel distributed processing: explorations in the microstructure of cognition,", "citeRegEx": "Smolensky and Paul.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky and Paul.", "year": 1986}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["Srivastava", "Nitish", "Mansimov", "Elman", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1502.04681,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "A tutorial on stochastic approximation algorithms for training restricted boltzmann machines and deep belief nets", "author": ["Swersky", "Kevin", "Chen", "Bo", "Marlin", "Benjamin", "De Freitas", "Nando"], "venue": "In Information Theory and Applications Workshop (ITA),", "citeRegEx": "Swersky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2010}, {"title": "Factored conditional restricted boltzmann machines for modeling motion style", "author": ["Taylor", "Graham W", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Taylor et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2009}, {"title": "Convolutional learning of spatio-temporal features", "author": ["Taylor", "Graham W", "Fergus", "Rob", "LeCun", "Yann", "Bregler", "Christoph"], "venue": "In ECCV\u201910,", "citeRegEx": "Taylor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2010}, {"title": "Two Distributed-State Models For Generating High-Dimensional Time Series", "author": ["Taylor", "Graham W", "Hinton", "Geoffrey E", "Roweis", "Sam T"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Taylor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2011}, {"title": "A connection between score matching and denoising autoencoders", "author": ["Vincent", "Pascal"], "venue": "Neural computation,", "citeRegEx": "Vincent and Pascal.,? \\Q2011\\E", "shortCiteRegEx": "Vincent and Pascal.", "year": 2011}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 42, "context": "The main building blocks in the deep learning literature are Restricted Boltzmann Machines (RBMs) (Smolensky, 1986), autoencoders (Hinton & Salakhutdinov, 2006; Vincent et al., 2008), Convolutional Neural Networks (CNNs) (LeCun et al.", "startOffset": 130, "endOffset": 182}, {"referenceID": 15, "context": ", 2008), Convolutional Neural Networks (CNNs) (LeCun et al., 1998) and Recurrent Neural Networks (RNNs) (Bengio, 2013).", "startOffset": 46, "endOffset": 66}, {"referenceID": 27, "context": "In order to deal with continuous states and actions, finding separately the adequate representations for states and actions to facilitate value function learning might be critical (Mnih et al., 2015; Lillicrap et al., 2015).", "startOffset": 180, "endOffset": 223}, {"referenceID": 16, "context": "In order to deal with continuous states and actions, finding separately the adequate representations for states and actions to facilitate value function learning might be critical (Mnih et al., 2015; Lillicrap et al., 2015).", "startOffset": 180, "endOffset": 223}, {"referenceID": 28, "context": "Initially they were mainly used to learn transformation between images (Memisevic & Hinton, 2007), but they have recently also been applied to human activity recognition from videos and moving skeleton data from the kinect sensor (Mocanu et al., 2015), or to recognize offspring relationship from pictures of faces (Dehghan et al.", "startOffset": 230, "endOffset": 251}, {"referenceID": 1, "context": ", 2015), or to recognize offspring relationship from pictures of faces (Dehghan et al., 2014).", "startOffset": 71, "endOffset": 93}, {"referenceID": 5, "context": "In robotics, gated networks have been used to learn to write numbers (Droniou et al., 2014), as well as to learn multimodal representations of numbers using images, vocal signal and articular movements with the iCub robot (Droniou et al.", "startOffset": 69, "endOffset": 91}, {"referenceID": 6, "context": ", 2014), as well as to learn multimodal representations of numbers using images, vocal signal and articular movements with the iCub robot (Droniou et al., 2015).", "startOffset": 138, "endOffset": 160}, {"referenceID": 29, "context": "At a higher level, the same tools could be used to learn affordances, that are often represented as object-action-effect complexes (Montesano et al., 2008).", "startOffset": 131, "endOffset": 155}, {"referenceID": 35, "context": "This mechanism is used in the LSTM family of networks (Hochreiter & Schmidhuber, 1997a; Srivastava et al., 2015), among others.", "startOffset": 54, "endOffset": 112}, {"referenceID": 42, "context": "We now briefly introduce Restricted Boltzmann Machines (RBMs) (Smolensky, 1986), autoencoders (Hinton & Salakhutdinov, 2006; Vincent et al., 2008), Convolutional Neural Networks (CNNs) (LeCun et al.", "startOffset": 94, "endOffset": 146}, {"referenceID": 15, "context": ", 2008), Convolutional Neural Networks (CNNs) (LeCun et al., 1998) and Recurrent Neural Networks (RNNs) (Bengio, 2013), and show how these networks have been extended to contain gated connections.", "startOffset": 46, "endOffset": 66}, {"referenceID": 42, "context": "This endow autoencoders with generative properties similar to those of RBMs (Vincent et al., 2008).", "startOffset": 76, "endOffset": 98}, {"referenceID": 39, "context": "This led to a shift from GRBMs to gated autoencoders (GAEs) (Memisevic, 2008; 2011; 2012) though research on GRBMs is still active (Taylor et al., 2010; Ding & Taylor, 2014).", "startOffset": 131, "endOffset": 173}, {"referenceID": 36, "context": "The gating idea was also applied to RNNs (Sutskever et al., 2011) and CNNs, either combined to GRBMs (Taylor et al.", "startOffset": 41, "endOffset": 65}, {"referenceID": 39, "context": ", 2011) and CNNs, either combined to GRBMs (Taylor et al., 2010) or to GAEs (Konda & Memisevic, 2015), as we outline in Section 5.", "startOffset": 43, "endOffset": 64}, {"referenceID": 15, "context": ", 2008), Convolutional Neural Networks (CNNs) (LeCun et al., 1998) and Recurrent Neural Networks (RNNs) (Bengio, 2013), and show how these networks have been extended to contain gated connections. An RBM is not a neural network but a particular probabilistic graphical model (PGM) (Koller & Friedman, 2009) whose graph is bipartite: one set (or layer) of nodes is called \u201cvisible\u201d and is used as the input of the model, whereas the other layer is \u201chidden\u201d and is interpreted as being the hidden cause explaining the input. Both layers are generally binary (though it is possible to extend them to real-valued units) and fully connected to each other. However, there are no connections within a layer, which facilitates inference and training. Training an RBM consists in finding the parameters (edge\u2019s weights and node\u2019s bias) which maximize the likelihood of the training data. Importantly, RBMs are generative models: they can model the probability density of the joint distribution of visible and hidden units, which enables them to generate samples similar to those of the training data onto the visible layer. The first instance of a gated network in the deep learning literature was a gated RBM (GRBM) (Memisevic & Hinton, 2007). However, this model was using a fully connected multiplicative network that required a lot of memory and computations for inference and training. In the next section, we present a solution to this issue, that was introduced by Memisevic & Hinton (2010) as a direct extension of (Memisevic & Hinton, 2007), still using GRBMs.", "startOffset": 47, "endOffset": 1489}, {"referenceID": 23, "context": "All figures are adapted from (Memisevic et al., 2010).", "startOffset": 29, "endOffset": 53}, {"referenceID": 7, "context": "One can also use the softplus function \u03c3+, defined as \u03c3+(x) = log(1+exp(x)), which is a smooth version of the rectified linear unit (Glorot et al., 2011).", "startOffset": 132, "endOffset": 153}, {"referenceID": 37, "context": "We refer the reader to (Swersky et al., 2010) for a clear presentation of the latter topic.", "startOffset": 23, "endOffset": 45}, {"referenceID": 1, "context": "The transformation between these images stored in h is either binary (Memisevic & Hinton, 2007; 2010) or real-valued (Droniou & Sigaud, 2013; Dehghan et al., 2014).", "startOffset": 117, "endOffset": 163}, {"referenceID": 23, "context": "the output \u0177 consisted of binary class labels, and the values of the h layer were also binary (Memisevic et al., 2010).", "startOffset": 94, "endOffset": 118}, {"referenceID": 1, "context": "Interestingly, a model recognizing offspring relationship from pictures of faces combines generative and discriminative training, using two training signals (Dehghan et al., 2014).", "startOffset": 157, "endOffset": 179}, {"referenceID": 1, "context": "The work of (Dehghan et al., 2014) is another instance of the transformation coding view, but where x and y are temporally independent images.", "startOffset": 12, "endOffset": 34}, {"referenceID": 23, "context": "training (Memisevic & Hinton, 2007; 2010) pixels(t) pixels(t+1) binary proba \u0177 (Memisevic et al., 2010) pixels binary binary proba \u0177 (Memisevic, 2011) pixels pixels = x soft 1-hot relu (x\u0302, \u0177) (Droniou & Sigaud, 2013) pixels(t) pixels(t+1) real softplus \u0177 (Rudy & Taylor, 2014) pixels 1-hot real relu x\u0302 (Dehghan et al.", "startOffset": 79, "endOffset": 103}, {"referenceID": 1, "context": ", 2010) pixels binary binary proba \u0177 (Memisevic, 2011) pixels pixels = x soft 1-hot relu (x\u0302, \u0177) (Droniou & Sigaud, 2013) pixels(t) pixels(t+1) real softplus \u0177 (Rudy & Taylor, 2014) pixels 1-hot real relu x\u0302 (Dehghan et al., 2014) face 1 face 2 soft 1-hot softmax hybrid", "startOffset": 208, "endOffset": 230}, {"referenceID": 40, "context": "In (Taylor et al., 2011), a CRBM is used to model human motion data but, as illustrated in Figure 5, it is extended with an additional style layer to model different styles of motion.", "startOffset": 3, "endOffset": 24}, {"referenceID": 40, "context": "Figure 5: Four layers can be connected by three tripartite connection blocks (adapted from (Taylor et al., 2011)).", "startOffset": 91, "endOffset": 112}, {"referenceID": 28, "context": "More recently, a \u201c4-way tensor\u201d and its factorization were introduced based in GRBMs (Mocanu et al., 2015).", "startOffset": 85, "endOffset": 106}, {"referenceID": 28, "context": "The models of (Taylor & Hinton, 2009) and (Mocanu et al., 2015) are both capable of representing sequential data in the limit of the N previous time steps included in the memory concatenated to the input layer.", "startOffset": 42, "endOffset": 63}, {"referenceID": 6, "context": "For instance, the architecture depicted in Figure 6 uses an additional autoencoding connection with respect to a standard GAE (Droniou et al., 2015).", "startOffset": 126, "endOffset": 148}, {"referenceID": 6, "context": "Figure 6: Gated network for unsupervised classification (adapted from (Droniou et al., 2015)).", "startOffset": 70, "endOffset": 92}, {"referenceID": 6, "context": "We do not further study this aspect here, see (Droniou et al., 2015) for more details.", "startOffset": 46, "endOffset": 68}, {"referenceID": 36, "context": "Another architecture based on factoring gating connections is the \u201cMultiplicative RNN\u201d architecture (Sutskever et al., 2011) depicted in Figure 7.", "startOffset": 100, "endOffset": 124}, {"referenceID": 39, "context": "This has been done in GRBMs (Taylor et al., 2010) so as to extract spatio-temporal features in the context of human activity recognition, and in GAEs (Konda & Memisevic, 2015) to perform visual odometry from stereo pairs in a sequence of images captured from a moving camera.", "startOffset": 28, "endOffset": 49}, {"referenceID": 5, "context": ", 2014b) or (Droniou et al., 2014).", "startOffset": 12, "endOffset": 34}], "year": 2015, "abstractText": "Gated networks are networks that contain gating connections, in which the outputs of at least two neurons are multiplied. Initially, gated networks were used to learn relationships between two input sources, such as pixels from two images. More recently, they have been applied to learning activity recognition or multimodal representations. The aims of this paper are threefold: 1) to explain the basic computations in gated networks to the non-expert, while adopting a standpoint that insists on their symmetric nature. 2) to serve as a quick reference guide to the recent literature, by providing an inventory of applications of these networks, as well as recent extensions to the basic architecture. 3) to suggest future research directions and applications.", "creator": "LaTeX with hyperref package"}}}