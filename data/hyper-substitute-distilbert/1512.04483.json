{"id": "1512.04483", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2015", "title": "Dropout Training of Matrix Factorization and Autoencoder for Link Prediction in Sparse Graphs", "abstract": "adaptive factorization ( fe ) and autoencoder ( ae ) operators naturally present most diverse approaches of unsupervised learning. while mf dynamic models long been extensively explored near the geometric modeling and dynamics prediction literature, the product family has mostly inspired much attention. in here section we suggest algorithm mf via ae's application to the dynamic prediction problem in sparse graphs. we mention explicit connection comparing msc against im from the perspective of visual training, will further visit fc + ae : a model training teacher and ae teachers with shared representations. we recommend dropout to approach both models mf and ae themselves, who recommend how i can adequately prevent transmission by acting backward an adaptive regularization. we conduct experiments establishing many virtual world mesh graph datasets, and show that ff + if jointly outperforms the competing methods, especially enough to also demonstrate strong non - cohesive structures.", "histories": [["v1", "Mon, 14 Dec 2015 19:38:14 GMT  (501kb)", "http://arxiv.org/abs/1512.04483v1", "Published in SDM 2015"]], "COMMENTS": "Published in SDM 2015", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuangfei zhai", "zhongfei zhang"], "accepted": false, "id": "1512.04483"}, "pdf": {"name": "1512.04483.pdf", "metadata": {"source": "CRF", "title": "Dropout Training of Matrix Factorization and Autoencoder for Link Prediction in Sparse Graphs", "authors": ["Shuangfei Zhai", "Zhongfei (Mark) Zhang"], "emails": ["szhai2@binghamton.edu", "zhongfei@cs.binghamton.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n04 48\n3v 1\n[ cs\n.L G\n] 1\n4 D\nec 2\n01 5\nMatrix factorization (MF) and Autoencoder (AE) are among the most successful approaches of unsupervised learning. While MF based models have been extensively exploited in the graph modeling and link prediction literature, the AE family has not gained much attention. In this paper we investigate both MF and AE\u2019s application to the link prediction problem in sparse graphs. We show the connection between AE and MF from the perspective of multiview learning, and further propose MF+AE: a model training MF and AE jointly with shared parameters. We apply dropout to training both the MF and AE parts, and show that it can significantly prevent overfitting by acting as an adaptive regularization. We conduct experiments on six real world sparse graph datasets, and show that MF+AE consistently outperforms the competing methods, especially on datasets that demonstrate strong non-cohesive structures."}, {"heading": "1 Introduction", "text": "Link prediction is one of the fundamental problems of network analysis, as pointed out in [12], \u201da network model is useful to the extent that it can support meaningful inferences from observed network data.\u201d Given a graph G(V,E) together with its adjacency matrix A \u2208 {0, 1}N\u00d7N , the set of nodes V , and the set of edges E, link prediction can be considered as a matrix completion problem on A. The problem is challenging because A is often large and sparse, which means that only a small fraction of the links are observed. As a result, a good model should have enough capacity to accommodate the complex connectivity pattern between all N2 pairs of nodes, as well as strong generalization ability to make accurate predictions on unobserved pairs.\nAmong the large number of models proposed over the decade, Matrix Factorization (MF) is one of the most popular ones in network modeling and relational learning in general [27, 24, 14, 22]. In its simplest form, MF directly models the interaction between a pair of nodes as the inner product of two latent factors, one for each node. This assumes that the large square\n\u2217Binghamton Univeristy, USA. szhai2@binghamton.edu \u2020Binghamton University, USA. zhongfei@cs.binghamton.edu\nadjacency matrix A can be factorized into the product of a tall, thin matrix and a short, wide matrix as A \u2248 W2W1, where W2 \u2208 R N\u00d7K ,W1 \u2208 R K\u00d7N . Each row of W1 and each column of W1 are often called a latent factor, as they often capture the community membership information of the nodes. Training of such a model is usually conducted with stochastic gradient descent, which makes it easily scalable to large datasets [27, 24].\nBayesian models, such as MMSB [2, 15, 26] are another family of latent factor models for studying network structures. Compared with MF which directly learns the latent factors as free parameters by solving an optimization problem, Bayesian models treat the latent factors as random variables and model the stochastic process of the creation of a link. As a result, they can be considered as a stochastic version of MF. By putting various priors on the latent factors, link prediction is reduced to the inference problem of the posterior distribution of the link status. While Bayesian models can often significantly reduce overfitting compared with their deterministic counterparts, the inference processes such as MCMC and Variational Inference are usually much slower to run than direct optimization. As a result, their application has been limited to only moderate sized datasets.\nAutoencoder (AE) together with its variants such as Restricted Boltzman Machine (RBM) has recently achieved great success in various machine learning applications, and is well recognized as the building block of Deep Learning [5, 8]. AE learns useful representations by learning a mapping from an input to itself, which makes it different from the above mentioned approaches. Surprisingly, it is not until recently that AE finds its application to modeling graphs [7, 11]. In this paper we investigate the effectiveness of AE on the link prediction problem. In particular, we show that AE is closely connected to MF when unified in the same architecture. Motivated by this observation, we argue that MF and AE are indeed two complementary views of the same problem, and propose a novel model MF+AE where MF and AE are jointly trained with shared parameters.\nTo prevent overfitting, we train the model with Dropout [19] combined with stochastic gradient descent. We highlight the effect of dropout training to MF+AE, and show that when approximated by the second order Taylor expansion, the dropout training effectively penalizes a scaled \u21132 norm of (the combination of) the rows or columns of the weight matrices. We evaluate MF+AE on six real world sparse graphs and show that dropout significantly mitigates overfitting on both MF and AE, and that MF+AE outperforms the competing methods on all the datasets."}, {"heading": "2 Model", "text": "Matrix Factorization: Given an undirected graph G(V,E) with N nodes and |E| edges, MF approximates its adjacency matrix A \u2208 {0, 1}N\u00d7N with the product of two low rank matrices W2 and W1 by solving the optimization problem:\nmin \u2211\ni,j\nL(Ai,j , g(W j 2W1,i + b1,i + b2,j))(2.1)\nwhere W1 \u2208 R K\u00d7N ,W2 \u2208 R N\u00d7K ; b1 \u2208 R N , b2 \u2208 R N are the biases, W1,i is the i th column of W1, W j 2 is the j th row of W2, g is the link function, L is the loss function defined on each pair of nodes, and K is the dimension of the latent factor. Since A is symmetric for undirected graph, it is sometimes useful to adopt tied weights, i.e., W1 = W2, b1 = b2. We refer to the model with tied weights as the symmetric version of MF in this paper.\nAutoencoder: AE is originally proposed as an unsupervised representation learning method. Given an example represented by a feature vector x, an AE learns a reconstruction of itself by a function x\u0303 = F (x). The typical form of F (x) can be characterized by a neural network with one hidden layer, where x is first mapped to a hidden layer representation h, then a reconstruction x\u0303 is obtained by mapping h to the original feature space:\nh = f(W1x+ b1)\nx\u0303 = g(W2h+ b2) (2.2)\nwith W1 \u2208 R K\u00d7N , b1 \u2208 R K ,W2 \u2208 R N\u00d7K , b2 \u2208 R N . The parameters are learned by solving the following optimization problem:\nmin \u2211\ni\nL(xi, x\u0303i;W1, b1,W2, b2)(2.3)\nHere we have slightly overloaded the loss function L by defining it on the column vector xi. The natural way of applying AE to modeling graphs is to represent each node as the set of its neighbors; in other words, set xi =\nAi. This is analogous to the bag of words representation prevalent in the document modeling community, and we call it the bag of nodes representation. Note that this representation is sufficient since when only the topological structure is available, we can learn an unseen node if we know all its neighbors.\nThe Joint Model: To better see the connection and difference between MF and AE, we now rewrite (2.3) by substituting xi with Ai:\nmin \u2211\ni,j\nL(Ai,j , g(W j 2hi + b2,j))\ns.t. hi = f(W1Ai + b1)\n(2.4)\nAnd we rewrite (2.1) by omitting the b1term:\nmin \u2211\ni,j\nL(Ai,j , g(W j 2hi + b2,j))\ns.t. hi = W1\u03b4i\n(2.5)\nwhere \u03b4i \u2208 R N is the indicator vector of node i, which is a binary vector with 1 at the ith entry and 0 for all the rest. We deliberately organize (2.4) and (2.5) in such a way that in the high level, they share the same architecture. Both models first learn a hidden representation hi, which is then fed through a classifier with link function g and loss function L. The main difference is only in the form of the hidden representation. For each node i, MF only looks at its id, and the hidden layer representation is learned by simply extracting the ith column of W1. For AE, we first sum up the columns of W1 indicated by i\u2019s neighbors, and then pass the sum through an activation function f . As a result, in MF, two nodes propagate \u201dpositive\u201d information to each other only if they are directly connected; in AE, however, two nodes can do so as long as they appear in the same neighborhood of some other node, even if they are not directly connected. The different ways of the information propagation between that two models indicates that MF and AE are complementary to each other to model different aspects of the same topological structure.\nWe can also interpret the distinction between MF and AE as two different views of the same entity: MF uses \u03b4i, and AE uses Ai. We note that the two views are disjoint and sufficient: they do not overlap with each other, but each of them can sufficiently represent a node. This perspective motivates us to build a unified architecture where we train the two models jointly, and require both of them to be able to uncover the graph structure. The idea is similar to co-training [23] in the semisupervised learning setting, where one trains two classifiers on two sufficient views such that the two\nviews can \u201dteach\u201d each other on unlabeled data. While in our problem, there is no \u201dunlabeled data\u201d available, we argue that the model can still benefit from the cotraining idea by requiring the two views to \u201dagree with\u201d each other. We then formulate our model, which we call MF+AE, as follows:\nmin \u2211\ni\nL(Ai, g(W2h1,i + b2))\n+ \u03c1 \u2211\ni\nL(Ai, g(W2h2,i + b4))\ns.t. h1,i = f(W1Ai + b1),\nh2,i = f(W1\u03b4i + b3)\n(2.6)\nIn (2.6), the objective function is composed of two parts, one for AE and the other for MF. h1,i and h2,i are the hidden representation learned by the AE and MF part, respectively, and the \u201dagreement\u201d is achieved by using the same set of weights W1 and W2 for both AE and MF. We modify the architecture of the MF objective by adding the same activation function f and a corresponding bias term b3. \u03c1 is a positive real number which could be simply set to 1 in practice. We show the architecture of MF+AE in Figure 1.\nIn Figure 1, the bottom part and top part correspond to the first and second line of (2.6), respectively. The color of each entry of the vector indicates its value, with white for 0 and black for 1. We see that the MF module is trained to reconstruct the neighborhood structure based on \u03b4i, while the AE module learns to\nreconstruct Ai from itself. The reconstructions from AE and MF are denoted as A\u0303\n(1) i and A\u0303 (2) i , respectively.\nNote that although the 5th node does not appear in the neighbor of i (Ai,5 = 0), the reconstruction is close to 1 (dark gray). This means that we can make a confident prediction of the link of i to node 5. After the two reconstructions are obtained, the final prediction is calculated as the geometric mean of the two:\n(2.7) A\u0303i = 1+\u03c1\n\u221a\nA\u0303 (1) i \u2299 (A\u0303 (2) i ) \u03c1\nwhere \u2299 denotes the element-wise product of two vectors.\nActivation Function and Loss Function: For the activation function f , we choose the Rectified Linear Units (ReLU), which is defined as:\n(2.8) f(x) = max(0, x)\nReLU provides nonlinearity by zeroing out all the negative inputs and keeping positive ones intact. We choose ReLU over other popular choices such as Sigmoid or tanh for two reasons. First, it does not squash the output to a fixed interval as Sigmoid or tanh does. As a result, it is closest to our intuition of approximating the behavior of MF. In fact, from the point of view of MF, the effect of ReLU can be considered as putting a non-negativity constraint on h, which is closely related to the idea of Non-negative Matrix Factorization. Secondly, ReLU is fast to compute compared with its alternatives, and still provides enough nonlinearity to significantly improve the model capacity over linear structures.\nFor g and L, we choose the Sigmoid function combined with cross entropy loss:\ng(x) = 1\n1 + e\u2212x (2.9)\n(2.10) L(x, x\u0303) = \u2212x log x\u0303\u2212 (1 \u2212 x) log(1 \u2212 x\u0303)\nThe saturating property of the Sigmoid function endows the model much flexibility since h1,i and h2,i need only to have similar activation patterns to both achieve good reconstructions; cross entropy is naturally a more appropriate choice than square loss for binary matrix. Moreover, as A is often extremely sparse, reconstructing the whole matrix incurs the class imbalance problem, which means that the loss of the positive entries is dominated by the negative entries. As a result, it is important to reweight the cost of the positive and negative classes by utilizing the cost sensitive strategy. Consequently, our final form of the loss function becomes:\n(2.11) L(Ai, A\u0303i) = \u2211\nj\u2248i\n\u2212 log A\u0303i,j \u2212 \u03b7 \u2211\nj 6\u2248i\nlog(1\u2212 A\u0303i,j)\nwhere A\u0303i = g(W2hi + b2) is the reconstruction of Ai; j \u2248 i if Ai,j = 1, otherwise j 6\u2248 i. In practice, it is sufficient to approximate the second part of L by a few samples; that is to say that at each training iteration we only need to sample part of the non-links for each node. Doing this may greatly speed up the training process on large graphs without sacrificing the performance. \u03b7 is the weight for the loss of the negative entries, which can be simply set as #nonlink samples#links .\nDropout As Regularization: Regularization is critical for most machine learning models to generalize well to unseen data. Instead of putting explicit constraints on the parameters or hidden units, we use dropout training [19, 21] as an implicit regularization. Dropout is a technique originally proposed for training feed forward neural networks to avoid overfitting. It works as follows: in the stochastic gradient training, we randomly drop out half of the hidden units (for both AE and MF) and half of the feature units (for AE) for each node in the current iteration. Mathematically, the effect of dropping out can be simulated as applying an element-wise random dropout mask as follows:\nmin \u2211\ni\nE\u03beh,i,\u03bein,i{L(Ai, g(W2(\u03beh,i \u2299 h1,i) + b2))\n+ L(Ai, g(W2(\u03beh,i \u2299 h2,i) + b4))}\ns.t. h1,i = f(W1(\u03bein,i \u2299Ai) + b1),\nh2,i = f(W1\u03b4i + b3)\n(2.12)\nHere \u03beh,i \u2208 {0, 1} K and \u03bein,i \u2208 {0, 1} N are the dropout masks for the hidden and input units, respectively; each element of them is an iid draw from the Bernoulli distribution. And \u2299 is the element-wise product of two vectors. Note that we use the same dropout mask \u03beh,i for both the AE and MF modules. This is to ensure that dropout does not cause any difference in the architecture between the two views.\nFor the AE module, randomly dropping out the input can be considered as a \u201ddenoising\u201d technique, which was exploited by the previous work [20, 6], and also was applied to link prediction [7]. The motivation is that a model should be able to make a good reconstruction under a noisy or partial input. This property is particularly interesting to our problem because this is exactly the same link prediction problem: prediction of the whole based on parts.\nWhile theoretically explaining the effect of dropout is difficult for complex models, we can still gain an in-\nsight by looking at an approximate surrogate. Previously, [21, 6] used the second order Taylor expansion to explain the effect of feature noising in generalized linear models and AE, respectively. We can borrow the same tool to showcase a simplified version of MF+AE.\nDropout for Matrix Factorization: We consider the effect of dropout training on (2.1). For a concise articulation we ignore the bias terms; the resulting model is described as the following objective function:\n(2.13) O = \u2211\ni,j\nE\u03bei{L(Ai,j , g(W j 2 (\u03bei \u2299W1,i))}\nWhen Sigmoid activation function with the cross entropy loss is used, we compute the second order approximation of (2.13) in a closed form as:\nO\u0303 = \u2211\ni,j\nL(Ai,j , g( 1\n2 W\nj 2W1,i))\n+ E\u03bei{(W j 2 \u2299W T 1,i)(\u03bei \u2212\n1 2 e)(gi,j \u2212Ai,j)}\n+ 1\n2 E\u03bei{((W\nj 2 \u2299W T 1,i)(\u03bei \u2212\n1 2 e))2gi,j(1 \u2212 gi,j)}\n= \u2211\ni,j\nL(Ai,j , g( 1\n2 W\nj 2W1,i))\n+ 1\n8 (W j2 ) 2(W1,i) 2gi,j(1 \u2212 gi,j)\n= \u2211\ni\nL(Ai, g( 1\n2 W2W1,i))\n+ 1 8 ( \u2211\nj\n(W j2 ) 2gi,j(1\u2212 gi,j))\n\ufe38 \ufe37\ufe37 \ufe38\n\u03bbi\n(W1,i) 2\n(2.14)\nwhere e is a column vector of all 1s, and (W j2 ) 2 and (W1,i) 2 are the element-wise square of the row and column vectors, respectively; gi,j is short for g( 1 2W j 2W1,i). The first equality of (2.14) is the result of the second order Taylor expansion; the second equality performs the expectation over the random variable \u03bei whose K entries are iid Bernoulli variables; the third equality is just a reorganization. We see that with the second order approximation, the dropout effect can be split into two factors. The first term is equivalent to the original objective except that the activation of each pair is scaled down by a half. The second part is more interesting; it can be considered as the product of a row vector \u03bbi and the square of the column vector of W1. Note that if we set \u03bbi \u221d e , the second term is reduced to the ordinary \u21132 regularization on each column of W1. In the case of\ndropout, however, \u03bbi is equivalent to a weighted sum of the square of the rows of W2, where the weight of each row of W2 is determined by the degree of uncertainty of the prediction gi,j. The overall effect of this regularization is two folds. First, it encourages the model to make confident predictions everywhere by minimizing gi,j(1\u2212 gi,j); secondly, it performs a scaled version of \u21132 regularization on the columns of W1: coordinates that are highly active in the rows of W2 are less penalized in the columns of W1, and vice versa. In other words, the penalization on the column vectors of W1 is adapted both to the structure of W2 and the uncertainty of the predictions. This is in stark contrast to \u21132 regularization where the penalization is uniformly put on each column of W1. Finally, note that since the roles of W1 and W2 are exchangeable, the discussion of the regularization on the columns of W1 also applies to the rows of W2 by symmetry.\nDropout for Autoencoder: The nonlinear and nonsmooth nature of the ReLU activation function makes it difficult to analyze the behavior of dropout. We thus only show the case when f is set as the identity function. Unsurprisingly, the effect of dropping out the hidden layer is similar to that of MF; the only difference is that we replace W1,i in (2.14) with W1Ai . Following similar reasoning, it is obvious to see that dropping out the hidden layer in AE again penalizes the scaled \u21132 norm of rows of W2 in the same way. Its effect on W1 is more subtle: instead of penalizing the norms of the columns of W1 directly, the regularization is performed on the linear combinations of them.\nNext we proceed to study the effect of dropping out the input. Let us now denote W = W2W1 , and we have the dropout version of objective function:\n(2.15) O = \u2211\ni\nE\u03bei{L(Ai, g(W (\u03bei \u2299Ai)))}\nThe second order approximation immediately follows as:\nO\u0303 = \u2211\ni\nL(Ai, g( 1\n2 WAi))\n+ \u2211\nj\nE\u03bei{(W j \u2299ATi )(\u03bei \u2212\n1 2 e)(gi,j \u2212Ai,j)}\n+ 1\n2\n\u2211\nj\nE\u03bei{((W j \u2299ATi )(\u03bei \u2212\n1 2 e))2gi,j(1 \u2212 gi,j)}\n= \u2211\ni\nL(Ai, g( 1\n2 WAi))\n+ 1\n8\n\u2211\nj\n(W j)2 \u2211\ni\ngi,j(1\u2212 gi,j)(Ai) 2\n\ufe38 \ufe37\ufe37 \ufe38\n\u03bbj\n(2.16)\nwhere W j = W j2W1 is the j th row of W , gi,j is short for g(12W jAi). Recall that dropping out the hidden units in both MF and AE performs a scaled \u21132 norm regularization on (the linear combinations of) columns of W1; dropping out the input performs a scaled \u21132 regularization on the linear combinations of rows of W1. The regularization on W2 is also very different from the case of dropping out the hidden units, but in a less clear way.\nTo summarize, we show that in the simplified case, dropping out the hidden units and inputs can be both interpreted as an adaptive regularization. They both push the model to make confident predictions by minimizing the factor gi,j(1 \u2212 gi,j), while they penalize different aspects of the parameter matrices. When combined in the joint training architecture, they provide complementary regularization to prevent MF+AE from overfitting."}, {"heading": "3 Experiments", "text": "3.1 Experiment Setup We conduct the experiments on six real world datasets: DBLP, Facebook, Youtube, Twitter, GooglePlus, and LiveJournal, all of which are available to download at snap.stanford.edu/data. We summarize the statistics of the six datasets in Table 1. Except for DBLP which is an author collaboration network, all the rest are online social networks. In particular, Youtube, Twitter, Gplus, and LiveJournal are all originally directed network; we convert them to undirected graphs by ignoring the direction of the links.\nFollowing the experimental protocol suggested in [4], we first split the observed links into a training graph Gtrain and a testing graph Gtest. We then train the models on Gtrain, and evaluate the performance of the models on Gtest. In particular, note that a naive algorithm which simply predicts a link for all the pairs that have at least one common neighbor would make a pretty good accuracy. We then only consider the nodes that are 2-hops from the target node as the candidate nodes [4]. The metrics used are Precision at top 10 position(Prec@10) and AUC.\nThe methods we evaluate are: Adamic-Adar Score (AA) [1]. This is a popular score based method, it calculates the pair-wise score of node i, j as S(i, j) = \u2211\nn\u2208CN(i,j)\nlog( 1 dn ), where CN(i, j)\ndenotes the set of common neighbors of node i and j, dn is the degree of node n. Prediction is made by ranking the score of all nodes to the target node. Random Walk with restart (RW)[12]. RW uses the stationary distribution of a random walk from a given node to all the nodes as the score. The restart probability needs to be set in advance. In practice we find that while the performance of RW is pretty robust within a wide range of values, a relatively large value (compared with 0.3 used in [4]) works slightly better on our problem . We set it as 0.5 throughout the experiments.\nMatrix Factorization with \u21132 regularization (MF2). This is a variant of the model proposed in [14]. To be fair for the comparison with the other models, we use the cross entropy loss instead of the rank loss proposed in the paper. We also use a weight decay of 10\u22125.\nAutoencoder with \u21132 regularization (AE2). This is the model corresponding to (2.3) with an additional \u21132 regularization on W1 and W2. The weight decay parameter is also set as 10\u22125.\nMatrix Factorization with dropout (MFd). This corresponds to the model described in (2.13) with the additional bias vectors. No weight decay is used.\nAutoencoder with Dropout (AEd). This is the single AE with ReLU activation function and the cross entropy loss trained by dropout.\nMarginalized Denoising Model (MDM)[7]. This is one of the few existing AE based models where a linear activation together with square loss is used. MDM marginalizes the dropout noise of the features during training, but no hidden layer dropout is adopted. The model requires the input noise level to be set; we set it as 0.9 throughout the experiments.\nThe Joint Model (MF+AE). This corresponds to the model described in (2.12), where we jointly train MF and AE with shared weights using dropout.\nAll the above methods are implemented in Matlab. We use the authors\u2019 implementation for MDM, and use our own implementations for the rest models. Note that MF2, AE2, MDM, MFd, AEd, and MF+AE all require the dimensionality of the latent factor or hidden layer K as the input. To be fair for a comparison, we do not attempt to optimize this parameter for each model. Instead, we set it the same for all of the models on all the six datasets. In the experiments, we use 100.\nAnother important aspect for both MF and AE\nmodels is the choice of a symmetry model vs. an asymmetry model, i.e., whether or not to set W1 = WT2 . We find that AE models are less sensitive to the characteristics of a dataset, and almost always benefit from the tied weights. The optimal choice for MF models is, however, extremely problem-dependent. In our experiments, we use tied weights for all AE based models including MF+AE on all the six datasets. For MF based models, we use tied weights on Facebook, Twitter, DBLP, LiverJournal, and untied weights on Youtube and Gplus.\nIn this paper, we are interested in evaluating the performance of different models on sparse graphs. In other words, we investigate how well a model generalizes given only a sparse training graph Gtrain. To this end, each of the six datasets is chosen as a relatively densely connected subgraph as in Table 1. We then randomly take 10% of links for training and use the rest for testing. In this way, we train the model on a sparse graph, while still have enough held out links for testing. We train all the models (except AA and RW which require no training) to convergence, which means that we do not use a separate validation set to perform early stopping. We do this for two reasons. First, in sparse graphs, splitting out a separate set for validation is expensive. Secondly and more importantly, we are interested in testing the generalization ability of each model. In practice we find that almost all the models we have tested benefit from a properly chosen early stopping point. However, this makes the results very difficult to interpret as it is difficult to evaluate the contribution of early stopping in different models.\nMF+AE Achieves Best Performance We first list the results of the experiments in Table 2 for a comparison. Overall, we see that MF+AE has the best average performance. In particular, it achieves the best performance on all the six datasets evaluated by Prec@10, and on all but the LiveJournal dataset evaluated by AUC. This shows that the joint training of MF and AE consistently boosts the generalization ability without increasing the model size. AEd achieves the second best average performance evaluated by both metrics. MDM, as a variant of AE, achieves the third best performance on Prec@10 and fourth on AUC. This is reasonable since on the one hand, the utilization of feature noising improves the generalization ability, and on the other hand, the linear nature limits its ability to model complex graph structures, and also due to the use of the square loss and ignorance of the class imbalance, the performance further deteriorates in sparse graphs. One seemingly surprising result is that RW performs pretty well despite of its simplicity.\nDropout Improves Generalization We note\nthat for both AE and MF, the dropout version significantly outperforms their \u21132 counterparts on both Prec@10 and AUC. Evaluated by the average performance, AEd outperforms AE2 by 66% on Prec@10, 26% on AUC; MFd also outperforms MF2 by 10% on Prec@10 and 6.5% on AUC. This verifies that dropout as an adaptive regularization performs much better than \u21132 norm regularization, especially with AE whose objective function is highly nonconvex. To better understand this, we visualize the full graph, the training graph of the Facebook dataset together with the predictions made by each model in Figure 2. We do not visualize the results of RW and AA since they do not output direct reconstructions. For each of the other six models, we convert the predictions to binary by a threshold at 0.5. Also for a better visualization, we down sample all the graphs by 80%.\nIn Figure 2 we see that AE2 and MF2 fit the training graph very well, but fail to uncover the much densely connected full graph. However, with dropout training, the predictions of AEd and MFd look much closer to the full graph than AE2 and MF2. This suggests that models trained with dropout generalize much better than their \u21132 regularized counterparts.\nWe then compare the predictions of MF+AE, AEd, MFd, MDM, respectively, which all use (different variants of) dropout training. It is not difficult to see that MF+AE\u2019s prediction resembles the full graph the most. AEd and MFd make a lot of \u201dFalse Positive\u201d predictions which are clearly shown by the pepper salt like pixels off the diagonal. MDM makes more \u201dFalse Negative\u201d pre-\ndictions, such as the missing of the small cluster near the right top corner. We note that the quality of the predictions shown by the visualization is also consistent with the results in Table 2.\nModeling Non-cohesive Graphs\nAmong all the six datasets we have tested, Youtube and Gplus datasets demonstrate least cohesive structures, i.e., they consist of more follower-followee relationships than the other datasets. The cohesive vs. non-cohesive distinction of graph structure has previously been investigated in [29, 9]. To show this, we\nhave trained the symmetric and asymmetric versions of MFd and MF2, respectively, on all the six datasets. We then report the averaged performances of the symmetric MF and asymmetric MF in Figure 3. We see that the symmetric version works better on Facebook, Twitter, DBLP, LiveJournal, and the asymmetric version works better on Youtube and Gplus. This experiment shows that Youtube and Gplus demonstrate more non-cohesive structure which cannot be symmetrically modeled by the inner product of two vectors.\nWith this in mind, let us look at the performances of different models on these two datasets. It is clear that MF+AE and AEd as the best and second best models outperform the other methods by much larger margins than on the other four datasets. Note that AEd and MF+AE still use the tied weights on these two datasets, as we found little difference in performance when switched to untied weights. Also note that even though MDM uses feature dropout, it still fails to model the non-cohesive structures properly. We argue that it is the nonlinear activation function that gives the MF+AE and AEd more modeling power than linear models."}, {"heading": "4 Related Work", "text": "The link prediction problem can be considered as a special case of relational learning and recommender systems [27, 24], and a lot of techniques proposed are directly ap-\nplicable to link prediction as well. Salakhutdinov et al. [30] first apply RBM to movie recommendation. Recently, Chen and Zhang [7] propose a variant of linear AE with marginalized feature noise for link prediction, and Li et al. [11] apply RBM to link prediction in dynamic graphs.\nMF+AE is also related to the supervised learning based methods [3, 13]. While these approaches directly train a classifier on manually collected features, MF+AE directly learns the appropriate features from the adjacency matrix.\nThe utilization of dropout training as an implicit regularization also contrasts with Bayesian models [2, 15]. While both dropout and Bayesian Inference are designed to reduce overfitting, their approaches are essentially orthogonal to each other. It would be an interesting future work to investigate whether they can be combined to further increase the generalization ability. Dropout has also been applied to training generalized linear models [21], log linear models with structured output [28], and distance metric learning [25].\nThis work is also related to graph representation learning. Recently, Perozzi et al. [16] propose to learn node embeddings by predicting the path of a random walk, and they show that the learned representation can boost the performance of the classification task on\ngraph data. It would also be interesting to evaluate the effectiveness of MF+AE in the same setting."}, {"heading": "5 Conclusion", "text": "We propose a novel model MF+AE which jointly trains MF and AE with shared parameters. We show that dropout can significantly improve the generalization ability of both MF and AE by acting as an adaptive regularization on the weight matrices. We conduct experiments on six real world sparse graphs, and show that MF+AE outperforms all the competing methods, especially on datasets with strong non-cohesive structures."}, {"heading": "Acknowledgements", "text": "This work is supported in part by NSF CCF1017828, the National Basic Research Program of China (2012CB316400), and Zhejiang Provincial Engineering Center on Media Data Cloud Processing and Analysis in China."}], "references": [{"title": "Friends and neighbors on the web", "author": ["L.A. Adamic", "E. Adar"], "venue": "SOCIAL NETWORKS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Mixed membership stochastic blockmodels", "author": ["E.M. Airoldi", "D.M. Blei", "S.E. Fienberg", "E.P. Xing"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1981}, {"title": "Link prediction using supervised learning", "author": ["M. Al Hasan", "V. Chaoji", "S. Salem", "M. Zaki"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Supervised random walks: predicting and recommending links in social networks", "author": ["L. Backstrom", "J. Leskovec"], "venue": "In WSDM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Marginalized denoising auto-encoders for nonlinear representations", "author": ["M. Chen", "K. Q Weinberger", "F. Sha", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "A marginalized denoising method for link prediction in relational data", "author": ["Z. Chen", "W. Zhang"], "venue": "In SDM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. E Hinton", "R. R Salakhutdinov"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Modeling homophily and stochastic equivalence in symmetric relational data", "author": ["P. Hoff"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Learning the parts of objects by nonnegative matrix factorization", "author": ["D.D. Lee", "H. Sebastian Seung"], "venue": "Nature, 401:788\u2013791,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "A deep learning approach to link prediction in dynamic networks", "author": ["X. Li", "N. Du", "H. Li", "K. Li", "J. Gao", "A. Zhang"], "venue": "In SDM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "The link-prediction problem for social networks", "author": ["D. Liben-Nowell", "J. Kleinberg"], "venue": "J. Am. Soc. Inf. Sci. Technol.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "New perspectives and methods in link prediction", "author": ["R.N. Lichtenwalter", "J.T. Lussier", "N.V. Chawla"], "venue": "In KDD,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Link prediction via matrix factorization", "author": ["A. Krishna Menon", "C. Elkan"], "venue": "In ECML PKDD Proceedings, Part II,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Nonparametric latent feature models for link prediction", "author": ["K.T. Miller", "T.L. Griffiths", "M.I. Jordan"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "In KDD ,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Probabilistic matrix factorization", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1929}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. A Manzagol"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Dropout training as adaptive regularization", "author": ["S. Wager", "S. Wang", "P. Liang"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Overlapping community detection at scale: a nonnegative matrix factorization approach", "author": ["J. Yang", "J. Leskovec"], "venue": "In WSDM,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In COLT,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Distance metric learning using dropout: A structured regularization approach", "author": ["Q. Qian", "J. Hu", "R. Jin", "J. Pei", "S. Zhu"], "venue": "In KDD ,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Bayesian probabilistic matrix factorization using markov chain monte carlo", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Relational learning via collective matrix factorization", "author": ["A. P Singh", "G. J Gordon"], "venue": "In KDD,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Feature noising for log-linear structured prediction", "author": ["S. Wang", "Mengqiu Wang", "S. Wager", "P. Liang", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Detecting cohesive and 2-mode communities indirected and undirected networks", "author": ["J. Yang", "J. McAuley", "J. Leskovec"], "venue": "In WSDM ,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Restricted Boltzmann Machines for Collaborative Filtering in ICML, pages", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}], "referenceMentions": [{"referenceID": 11, "context": "Link prediction is one of the fundamental problems of network analysis, as pointed out in [12], \u201da network model is useful to the extent that it can support meaningful inferences from observed network data.", "startOffset": 90, "endOffset": 94}, {"referenceID": 26, "context": "Among the large number of models proposed over the decade, Matrix Factorization (MF) is one of the most popular ones in network modeling and relational learning in general [27, 24, 14, 22].", "startOffset": 172, "endOffset": 188}, {"referenceID": 23, "context": "Among the large number of models proposed over the decade, Matrix Factorization (MF) is one of the most popular ones in network modeling and relational learning in general [27, 24, 14, 22].", "startOffset": 172, "endOffset": 188}, {"referenceID": 13, "context": "Among the large number of models proposed over the decade, Matrix Factorization (MF) is one of the most popular ones in network modeling and relational learning in general [27, 24, 14, 22].", "startOffset": 172, "endOffset": 188}, {"referenceID": 21, "context": "Among the large number of models proposed over the decade, Matrix Factorization (MF) is one of the most popular ones in network modeling and relational learning in general [27, 24, 14, 22].", "startOffset": 172, "endOffset": 188}, {"referenceID": 26, "context": "Training of such a model is usually conducted with stochastic gradient descent, which makes it easily scalable to large datasets [27, 24].", "startOffset": 129, "endOffset": 137}, {"referenceID": 23, "context": "Training of such a model is usually conducted with stochastic gradient descent, which makes it easily scalable to large datasets [27, 24].", "startOffset": 129, "endOffset": 137}, {"referenceID": 1, "context": "Bayesian models, such as MMSB [2, 15, 26] are another family of latent factor models for studying network structures.", "startOffset": 30, "endOffset": 41}, {"referenceID": 14, "context": "Bayesian models, such as MMSB [2, 15, 26] are another family of latent factor models for studying network structures.", "startOffset": 30, "endOffset": 41}, {"referenceID": 25, "context": "Bayesian models, such as MMSB [2, 15, 26] are another family of latent factor models for studying network structures.", "startOffset": 30, "endOffset": 41}, {"referenceID": 4, "context": "Autoencoder (AE) together with its variants such as Restricted Boltzman Machine (RBM) has recently achieved great success in various machine learning applications, and is well recognized as the building block of Deep Learning [5, 8].", "startOffset": 226, "endOffset": 232}, {"referenceID": 7, "context": "Autoencoder (AE) together with its variants such as Restricted Boltzman Machine (RBM) has recently achieved great success in various machine learning applications, and is well recognized as the building block of Deep Learning [5, 8].", "startOffset": 226, "endOffset": 232}, {"referenceID": 6, "context": "Surprisingly, it is not until recently that AE finds its application to modeling graphs [7, 11].", "startOffset": 88, "endOffset": 95}, {"referenceID": 10, "context": "Surprisingly, it is not until recently that AE finds its application to modeling graphs [7, 11].", "startOffset": 88, "endOffset": 95}, {"referenceID": 18, "context": "To prevent overfitting, we train the model with Dropout [19] combined with stochastic gradient descent.", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "The idea is similar to co-training [23] in the semisupervised learning setting, where one trains two classifiers on two sufficient views such that the two", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "Instead of putting explicit constraints on the parameters or hidden units, we use dropout training [19, 21] as an implicit regularization.", "startOffset": 99, "endOffset": 107}, {"referenceID": 20, "context": "Instead of putting explicit constraints on the parameters or hidden units, we use dropout training [19, 21] as an implicit regularization.", "startOffset": 99, "endOffset": 107}, {"referenceID": 19, "context": "For the AE module, randomly dropping out the input can be considered as a \u201ddenoising\u201d technique, which was exploited by the previous work [20, 6], and also was applied to link prediction [7].", "startOffset": 138, "endOffset": 145}, {"referenceID": 5, "context": "For the AE module, randomly dropping out the input can be considered as a \u201ddenoising\u201d technique, which was exploited by the previous work [20, 6], and also was applied to link prediction [7].", "startOffset": 138, "endOffset": 145}, {"referenceID": 6, "context": "For the AE module, randomly dropping out the input can be considered as a \u201ddenoising\u201d technique, which was exploited by the previous work [20, 6], and also was applied to link prediction [7].", "startOffset": 187, "endOffset": 190}, {"referenceID": 20, "context": "Previously, [21, 6] used the second order Taylor expansion to explain the effect of feature noising in generalized linear models and AE, respectively.", "startOffset": 12, "endOffset": 19}, {"referenceID": 5, "context": "Previously, [21, 6] used the second order Taylor expansion to explain the effect of feature noising in generalized linear models and AE, respectively.", "startOffset": 12, "endOffset": 19}, {"referenceID": 3, "context": "Following the experimental protocol suggested in [4], we first split the observed links into a training graph Gtrain and a testing graph Gtest.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "We then only consider the nodes that are 2-hops from the target node as the candidate nodes [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "The methods we evaluate are: Adamic-Adar Score (AA) [1].", "startOffset": 52, "endOffset": 55}, {"referenceID": 11, "context": "Random Walk with restart (RW)[12].", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": "3 used in [4]) works slightly better on our problem .", "startOffset": 10, "endOffset": 13}, {"referenceID": 13, "context": "This is a variant of the model proposed in [14].", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "Marginalized Denoising Model (MDM)[7].", "startOffset": 34, "endOffset": 37}, {"referenceID": 28, "context": "non-cohesive distinction of graph structure has previously been investigated in [29, 9].", "startOffset": 80, "endOffset": 87}, {"referenceID": 8, "context": "non-cohesive distinction of graph structure has previously been investigated in [29, 9].", "startOffset": 80, "endOffset": 87}, {"referenceID": 26, "context": "The link prediction problem can be considered as a special case of relational learning and recommender systems [27, 24], and a lot of techniques proposed are directly applicable to link prediction as well.", "startOffset": 111, "endOffset": 119}, {"referenceID": 23, "context": "The link prediction problem can be considered as a special case of relational learning and recommender systems [27, 24], and a lot of techniques proposed are directly applicable to link prediction as well.", "startOffset": 111, "endOffset": 119}, {"referenceID": 29, "context": "[30] first apply RBM to movie recommendation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Recently, Chen and Zhang [7] propose a variant of linear AE with marginalized feature noise for link prediction, and Li et al.", "startOffset": 25, "endOffset": 28}, {"referenceID": 10, "context": "[11] apply RBM to link prediction in dynamic graphs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "MF+AE is also related to the supervised learning based methods [3, 13].", "startOffset": 63, "endOffset": 70}, {"referenceID": 12, "context": "MF+AE is also related to the supervised learning based methods [3, 13].", "startOffset": 63, "endOffset": 70}, {"referenceID": 1, "context": "The utilization of dropout training as an implicit regularization also contrasts with Bayesian models [2, 15].", "startOffset": 102, "endOffset": 109}, {"referenceID": 14, "context": "The utilization of dropout training as an implicit regularization also contrasts with Bayesian models [2, 15].", "startOffset": 102, "endOffset": 109}, {"referenceID": 20, "context": "Dropout has also been applied to training generalized linear models [21], log linear models with structured output [28], and distance metric learning [25].", "startOffset": 68, "endOffset": 72}, {"referenceID": 27, "context": "Dropout has also been applied to training generalized linear models [21], log linear models with structured output [28], and distance metric learning [25].", "startOffset": 115, "endOffset": 119}, {"referenceID": 24, "context": "Dropout has also been applied to training generalized linear models [21], log linear models with structured output [28], and distance metric learning [25].", "startOffset": 150, "endOffset": 154}, {"referenceID": 15, "context": "[16] propose to learn node embeddings by predicting the path of a random walk, and they show that the learned representation can boost the performance of the classification task on", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "Matrix factorization (MF) and Autoencoder (AE) are among the most successful approaches of unsupervised learning. While MF based models have been extensively exploited in the graph modeling and link prediction literature, the AE family has not gained much attention. In this paper we investigate both MF and AE\u2019s application to the link prediction problem in sparse graphs. We show the connection between AE and MF from the perspective of multiview learning, and further propose MF+AE: a model training MF and AE jointly with shared parameters. We apply dropout to training both the MF and AE parts, and show that it can significantly prevent overfitting by acting as an adaptive regularization. We conduct experiments on six real world sparse graph datasets, and show that MF+AE consistently outperforms the competing methods, especially on datasets that demonstrate strong non-cohesive structures.", "creator": "LaTeX with hyperref package"}}}