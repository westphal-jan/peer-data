{"id": "1512.07046", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2015", "title": "News Across Languages - Cross-Lingual Document Similarity and Event Tracking", "abstract": "per today's world, we follow news which is coordinated globally. measuring events independently reported out different agencies and incorporate different languages. in this work, we address theoretical relevance of tracking of pieces in a large multilingual stream. within a recently instituted documentation identity finding services recommend two aspects of this process : how to share articles amongst new languages... how to develop collections involving articles in different languages whereas refer to the unique event. use a multilingual stream evaluating metadata for articles from each language, our compare existing two - lingual document similarity measures focusing on wikipedia. this allows us to compute the similarity of an two pages regardless of degree. building on previous attempts, we assume programs are methods using represent operations like theoretically essentially correct causal similarity between articles describing genres with insignificant or no direct overlap in the training data. requiring dynamic capability, we then show quantitative approach when link clusters of articles across languages hence form two same contents. we provide an extensive evaluation of wikipedia system as a whole, as well as an evaluation of the variance over robustness of its similarity measure and the linking method.", "histories": [["v1", "Tue, 22 Dec 2015 12:11:32 GMT  (1347kb,D)", "http://arxiv.org/abs/1512.07046v1", "Accepted for publication in Journal of Artificial Intelligence Research, Special Track on Cross-language Algorithms and Applications"]], "COMMENTS": "Accepted for publication in Journal of Artificial Intelligence Research, Special Track on Cross-language Algorithms and Applications", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["jan rupnik", "rej muhic", "gregor leban", "primoz skraba", "blaz fortuna", "marko grobelnik"], "accepted": false, "id": "1512.07046"}, "pdf": {"name": "1512.07046.pdf", "metadata": {"source": "CRF", "title": "News Across Languages - Cross-Lingual Document Similarity and Event Tracking", "authors": ["Jan Rupnik", "Andrej Muhi\u010d", "Gregor Leban", "Primo\u017e \u0160kraba", "Bla\u017e Fortuna", "Marko Grobelnik"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Content on the Internet is becoming increasingly multilingual. A prime example is Wikipedia. In 2001, the majority of pages were written in English, while in 2015, the percentage of English articles has dropped to 14%. At the same time, online news has begun to dominate reporting of current events. However, machine translation remains relatively rudimentary. It allows people to understand simple phrases on web pages, but remains inadequate for more advanced understanding of text. In this paper we consider the intersection of these developments: how to track events which are reported about in multiple languages.\nThe term event is vague and ambiguous, but for the practical purposes, we define it as \u201cany significant happening that is being reported about in the media.\u201d Examples of events would include shooting down of the Malaysia Airlines plane over Ukraine on July 18th, 2014 and HSBC\u2019s admittance of aiding their clients in tax evasion on February 9th, 2015 (Figure 1). Events such as these are covered by many articles and the question is how to find all the articles in different languages that are describing a single event. Generally, events are more specific than general themes as the time component plays an important role \u2013 for example, the two wars in Iraq would be considered as separate events.\nAs input, we consider a stream of articles in different languages and a list of events. Our goal is to assign articles to their corresponding events. A priori, we do not know the coverage of the articles, that is, not all the events may be covered and we do not know that all the articles necessarily fit into one of the events. The task is divided into two parts: detecting events within each language and then linking events across languages. In this paper we address the second step.\nWe consider a high volume of articles in different languages. By using a language detector, the stream is split into separate monolingual streams. Within each monolingual stream, an online clustering approach is employed, where tracked clusters correspond to our definition of events - this is based on the Event Registry system [16, 15]. Our main goal in this paper is to connect such clusters (representations of events) across\nar X\niv :1\n51 2.\n07 04\n6v 1\n[ cs\n.I R\n] 2\n2 D\nec 2\n01 5\nlanguages, that is, to detect that a set of articles in language A reports on the same event as a set of articles in language B.\nOur approach to link clusters across languages combines two ingredients: a cross-lingual document similarity measure, which can be interpreted as a language independent topic model, and semantic annotation of documents, which enables an alternative way to comparing documents. Since this work represents a complicated pipeline, we concentrate on these two specific elements. Overall, the approach should be considered from a systems\u2019 perspective (considering the system as a whole) rather than considering these problems in isolation.\nThe first ingredient of our approach to link clusters across languages represents a continuation of previous work [30, 32, 31, 21] where we explored representations of documents which were valid over multiple languages. The representations could be interpreted as multilingual topics, which were then used as proxies to compute cross-lingual similarities between documents. To learn the representations, we use Wikipedia as a training corpus. Significantly, we do not only consider the major or hub languages such as English, German, French, etc. which have significant overlap in article coverage, but also smaller languages (in terms of number of Wikipedia articles) such as Slovenian and Hindi, which may have a negligible overlap in article coverage. We can then define a similarity between any two articles regardless of language, which allows us to cluster the articles according to topic. The underlying assumption is that articles describing the same event are similar and will therefore be put into the same cluster.\nBased on the similarity function, we propose a novel algorithm for linking events/clusters across languages. The approach is based on learning a classification model from labelled data based on several sets of features. In addition to these features, cross-lingual similarity is also used to quickly identify a small list of potential linking candidates for each cluster. This greatly increases the scalability of the system.\nThe paper is organized as follows: we first provide an overview of the system as a whole in Section 2, which includes a subsection that summarizes the main system requirements. We then present related work in Section 3. The related work covers work on cross-lingual document similarity as well as work on cross-lingual cluster linking. In Section 4, we introduce the problem of cross-lingual document similarity computation and describe several approaches to the problem, most notably a new approach based on hub languages. In Section 5, we introduce the central problem of cross-lingual linking of clusters of news articles and our approach that combines the cross-lingual similarity functions with knowledge extraction based techniques. Finally, we present and interpret the experimental results in Section 6 and discuss conclusions and point out several promising future directions."}, {"heading": "2 Pipeline", "text": "We base our techniques of cross-lingual event linking on an online system for detection of world events, called Event Registry [16, 15]. Event Registry is a repository of events, where events are automatically identified by analyzing news articles that are collected from numerous news outlets all over the world. The important components in the pipeline of the Event Registry are shown in Figure 2. We will now briefly describe the main components.\nThe collection of the news articles is performed using the Newsfeed service [36]. The service monitors RSS feeds of around 100,000 mainstream news outlets available globally. Whenever a new article is detected in the RSS feed, the service downloads all available information about the article and sends the article through the pipeline. Newsfeed downloads daily on average around 200,000 news articles in various languages, where English, Spanish and German are the most common.\nCollected articles are first semantically annotated by identifying mentions of relevant concepts \u2013 either entities or important keywords. The disambiguation and entity linking of the concepts is done using Wikipedia as the main knowledge base. The algorithm for semantic annotation uses machine learning to detect significant terms within unstructured text and link them to the appropriate Wikipedia articles. The approach models link probability and combines prior word sense distributions with context based sense distributions. The details are reported in [19] and [40]. As a part of the semantic annotation we also analyze the dateline of the article to identify the location of the described event as well as to extract dates mentioned in the article\nData \u00a0 collec-on \u00a0\nAr-cle-\u2010level \u00a0 processing \u00a0\nEvent \u00a0 construc-on \u00a0\nEvent \u00a0storage \u00a0& \u00a0 maintenance \u00a0\nusing a set of regular expressions. Since articles are frequently revised we also detect if the collected article is just a revision of a previous one so that we can use this information in next phases of the pipeline. The last important processing step on the document level is to efficiently identify which articles in other available languages are most similar to this article. The methodology for this task is one of the main contributions of this paper and is explained in details in Section 4.\nAs the next step, an online clustering algorithm is applied to the articles in order to identify groups of articles that are discussing the same event. For each new article, the clustering algorithm determines if the article should be assigned to some existing cluster or into a new cluster. The underlying assumption is that articles that are describing the same event are similar enough and will therefore be put into the same cluster. For clustering, each new article is first tokenized, stop words are removed and the remaining words are stemmed. The remaining tokens are represented in a vector-space model and normalized using TF-IDF1 (see Section 4.1 for the definition). Cosine similarity is used to find the most similar existing cluster, by comparing the document\u2019s vector to the centroid vector of each cluster. A user-defined threshold is used to determine if the article is not similar enough to any existing clusters (0.4 was used in our experiments). If the highest similarity is above the threshold, the article is assigned to the corresponding cluster, otherwise a new cluster is created, initially containing only the single article. Whenever an article is assigned to a cluster, the cluster\u2019s centroid vector is also updated. Since articles about an event are commonly written only for a short period of time, we remove clusters once the oldest article in the cluster becomes more than 4 days old. This housekeeping mechanism prevents the clustering from becoming slow and also ensures that articles are not assigned to obsolete clusters. Details of the clustering approach are described in [1].\nOnce the number of articles in a cluster reaches a threshold (which is a language dependent parameter), we assume that the articles in the cluster are describing an event. At that point, a new event with a unique ID is created in Event Registry, and the cluster with the articles is assigned to it. By analyzing the articles, we extract the main information about the event, such as the event location, date, most relevant entities and keywords, etc.\nSince articles in a cluster are in a single language, we also want to identify any other existing clusters that report about the same event in other languages and join these clusters into the same event. This task is performed using a classification approach which is the second major contribution of this paper. It is described in detail in Section 5.\nWhen a cluster is identified and information about the event is extracted, all available data is stored in a custom-built database system. The data is then accessible through the API or a web interface at http://eventregistry.org/, which provide numerous search and visualization options."}, {"heading": "2.1 System Requirements", "text": "Our goal is to build a system that monitors global media and analyzes how events are being reported on. Our approach consists of two steps: tracking events separately in each language (based on language detection and an online clustering approach) and then connecting them. The pipeline must be able to process millions of articles per day and perform billions of similarity computations each day. Both steps rely heavily on similarity computation, which implies that this must be highly scalable.\nTherefore, we focus on implementations that run on a single shared memory machine, as opposed to clusters of machines. This simplifies implementation and system maintenance. To summarize, the following properties are desirable:\n\u2022 Training - The training (building cross-lingual models) should scale to many languages and should be robust to the quality of training resources. The system should be able to take advantage of comparable corpora (as opposed to parallel translation-based corpora), with missing data.\n\u2022 Operation efficiency - The similarity computation should be fast - the system must be able to handle billions of similarity computations per day. Computing the similarity between a new document and\n1The IDF weights are dynamically computed for each new article over all news articles within a 10 day window.\na set of known documents should be efficient (the main application is linking documents between two monolingual streams).\n\u2022 Operation cost - The system should run on a strong shared machine server and not rely on paid services.\n\u2022 Implementation - The system is straightforward to implement, with few parameters to tune.\nWe believe that a cross-lingual similarity component that meets such requirements is very desirable in a commercial setting, where several different costs have to be taken into consideration."}, {"heading": "3 Related work", "text": "In this section, we describe previous work described in the literature. Since there are two distinctive tasks that we tackle in this paper (computing cross-lingual document similarity and cross-lingual cluster linking), we have separated the related work into two corresponding parts."}, {"heading": "3.1 Cross-lingual document similarity", "text": "There are four main families of approaches to cross-lingual similarity. Translation and dictionary based. The most obvious way to compare documents written in different languages is to use machine translation and perform monolingual similarity, see [23, 25] for several variations of translation based approaches. One can use free tools such as Moses [14] or translation services, such as Google Translate (https://translate.google.com/). There are two issues with such approaches: they solve a harder problem than needs to be solved and they are less robust to training resource quality - large sets of translated sentences are typically needed. Training Moses for languages with scarce linguistic resources is thus problematic. The issue with using online services such as Google Translate is that the APIs are limited and not free. The operation efficiency and cost requirements make translation-based approaches less suited for our system. Closely related are works Cross-Lingual Vector Space Model (CL-VSM) [25] and the approach presented in [27] which both compare documents by using dictionaries, which in both cases are EuroVoc dictionaries [29]. The generality of such approaches is limited by the quality of available linguistic resources, which may be scarce or non-existent for certain language pairs.\nProbabilistic topic models. There exist many variants to modelling documents in a language independent way by using probabilistic graphical models. The models include: Joint Probabilistic Latent Semantic Analysis (JPLSA) [24], Coupled Probabilistic LSA (CPLSA) [24], Probabilistic Cross-Lingual LSA (PCLLSA) [39] and Polylingual Topic Models (PLTM) [20] which is a Bayesian version of PCLLSA. The methods (except for CPLSA) describe the multilingual document collections as samples from generative probabilistic models, with variations on the assumptions on the model structure. The topics represent latent variables that are used to generate observed variables (words), a process specific to each language. The parameter estimation is posed as an inference problem which is typically intractable and one usually solves it using approximate techniques. Most variants of solutions are based on Gibbs sampling or Variational Inference, which are nontrivial to implement and may require an experienced practitioner to be applied. Furthermore, representing a new document as a mixture of topics is another potentially hard inference problem which must be solved.\nMatrix factorization. Several matrix factorization based approaches exist in the literature. The models include: Non-negative matrix factorization based [38], Cross-Lingual Latent Semantic Indexing CLLSI [4, 23], Canonical Correlation Analysis (CCA) [11], Oriented Principal Component Analysis (OPCA) [24]. The quadratic time and space dependency of the OPCA method makes it impractical for large scale purposes. In addition, OPCA forces the vocabulary sizes for all languages to be the same, which is less intuitive. For our setting, the method in [38] has a prohibitively high computational cost when building models (it uses dense matrices whose dimensions are a product of the training set size and the vocabulary size). Our proposed approach combines CCA and CL-LSI. Another closely related method is Cross-Lingual Explicit Semantic\nAnalysis (CL-ESA) [26], which uses Wikipedia (as do we in the current work) to compare documents. It can be interpreted as using the sample covariance matrix between features of two languages to define the dot product which is used to compute similarities. The authors of CL-ESA compare it to CL-LSI and find that CL-LSI can outperform CL-ESA in an information retrieval, but is costlier to optimize over a large corpus (CL-ESA requires no training). We find that the scalability argument does not apply in our case: based on advances in numerical linear algebra we can solve large CL-LSI problems (millions of documents as opposed to the 10,000 document limit reported in [26]). In addition, CL-ESA is less suited for computing similarities between two large monolingual streams. For example, each day we have to compute similarities between 500,000 English and 500,000 German news articles. Comparing each German news article with 500,000 English news articles is either prohibitively slow (involves projecting all English articles on Wikipedia) or consumes too much memory (involves storing the projected English articles, which for a Wikipedia of size 1,000,000 is a 500,000 by 1000,0000 non-sparse matrix).\nMonolingual. Finally, related work includes monolingual approaches that treat document written in different languages in a monolingual fashion. The intuition is that named entities (for example, \u201cObama\u201d) and cognate words (for example, \u201ctsunami\u201d) are written in the same or similar fashion in many languages. For example, the Cross-Language Character n-Gram Model (CL-CNG) [25] represents documents as bags of character n-grams. Another approach is to use language dependent keyword lists based on cognate words [27]. These approaches may be suitable for comparing documents written in languages that share a writing system, which does not apply to the case of global news tracking.\nBased on our requirements in Section 2.1, we chose to focus on methods based on vector space models and linear embeddings. We propose a method that is more efficient than popular alternatives (a clustering-based approach and latent semantic indexing), but is still simple to optimize and use."}, {"heading": "3.2 Cross-lingual cluster linking", "text": "Although there are a number of services that aggregate news by identifying clusters of similar articles, there are almost no services that provide linking of clusters over different languages. Google News as well as Yahoo! News are able to identify clusters of articles about the same event, but offer no linking of clusters across languages. The only service that we found, which provides cross-lingual cluster linking, is the European Media Monitor (EMM) [27, 35]. EMM clusters articles in 60 languages and then tries to determine which clusters of articles in different languages describe the same event. To achieve cluster linking, EMM uses three different language independent vector representations for each cluster. The first vector contains the weighted list of references to countries mentioned in the articles, while the second vector contains the weighted list of mentioned people and organizations. The last vector contains the weighted list of Eurovoc subject domain descriptors. These descriptors are topics, such as air transport, EC agreement, competition and pollution control into which articles are automatically categorized [28]. Similarity between clusters is then computed using a linear combination of the cosine similarities computed on the three vectors. If the similarity is above the threshold, the clusters are linked. Compared to EMM, our approach uses document similarities to obtain a small set of potentially equivalent clusters. Additionally, we do not decide if two clusters are equivalent based on a hand-set threshold on a similarity value \u2013 instead we use a classification model that uses a larger set of features related to the tested pair of clusters.\nA system, which is significantly different but worth mentioning, is the GDELT project [17]. In GDELT, events are also extracted from articles, but in their case, an event is specified in a form of a triple containing two actors and a relation. The project contains an extensive vocabulary of possible relations, mostly related to political events. In order to identify events, GDELT collects articles in more than 65 languages and uses machine translation to translate them to English. All information extraction is then done on the translated article."}, {"heading": "4 Cross-lingual Document Similarity", "text": "Document similarity is an important component in techniques from text mining and natural language processing. Many techniques use the similarity as a black box, e.g., a kernel in Support Vector Machines. Comparison of documents (or other types of text snippets) in a monolingual setting is a well-studied problem in the field of information retrieval [33]. We first formally introduce the problem followed by a description of our approach."}, {"heading": "4.1 Problem definition", "text": "We will first describe how documents are represented as vectors and how to compare documents in a monolingual setting. We then define a way to measure cross-lingual similarity which is natural for the models we consider.\nDocument representation. The standard vector space model [33] represents documents as vectors, where each term corresponds to a word or a phrase in a fixed vocabulary. Formally, document d is represented by a vector x \u2208 Rn, where n corresponds to the size of the vocabulary, and vector elements xk correspond to the number of times term k occurred in the document, also called term frequency or TFk(d).\nWe also used a term re-weighting scheme that adjusts for the fact that some words occur more frequently in general. A term weight should correspond to the importance of the term for the given corpus. The common weighting scheme is called Term Frequency Inverse Document Frequency (TFIDF ) weighting. An\nInverse Document Frequency (IDF ) weight for the dictionary term k is defined as log (\nN DFk\n) , where DFk is\nthe number of documents in the corpus which contain term k. When building cross-lingual models, the IDF scores were computed with respect to the Wikipedia corpus. In the other part of our system, we computed TFIDF vectors on streams of news articles in multiple languages. There the IDF scores for each language changed dynamically - for each new document we computed the IDF of all news articles within a 10 day window.\nTherefore we can define a document\u2019s TFIDF as\nxij := term frequency in document i\ninverse document frequency of term j .\nThe TFIDF weighted vector space model document representation corresponds to a map \u03c6 : text \u2192 Rn defined by:\n\u03c6(d)k = TF k(d) log\n( N\nDF k\n) .\nMono-lingual similarity. A common way of computing similarity between documents is cosine similarity,\nsim(d1, d2) = \u3008\u03c6(d1), \u03c6(d2)\u3009 \u2016\u03c6(d1)\u2016\u2016\u03c6(d2)\u2016 ,\nwhere \u3008\u00b7, \u00b7\u3009 and \u2016 \u00b7 \u2016 are standard inner product and Euclidean norm. When dealing with two or more languages, one could ignore the language information and build a vector space using the union of tokens over the languages. A cosine similarity function in such a space can be useful to some extent, for example \u201cInternet\u201d or \u201cObama\u201d may appear both in Spanish and English texts and the presence of such terms in both an English and a Spanish document would contribute to their similarity. In general however, large parts of vocabularies may not intersect. This means that given a language pair, many words in both languages cannot contribute to the similarity score. Such cases can make the similarity function very insensitive to the data.\nCross-lingual similarity. Processing a multilingual dataset results in several vector spaces with varying dimensionality, one for each language. The dimensionality of the vector space corresponding to the i-th language is denoted by ni and the vector space model mapping is denoted by \u03c6i : text\u2192 Rni . The similarity\nbetween documents in language i and language j is defined as a bilinear operator represented as a matrix Si,j \u2208 Rni\u00d7nj :\nsimi,j(d1, d2) = \u3008\u03c6i(d1), Si,j\u03c6j(d2)\u3009 \u2016\u03c6i(d1)\u2016\u2016\u03c6j(d2)\u2016 ,\nwhere d1 and d2 are documents written in the i-th and j-th language respectively. If the maximal singular value of Si,j is bounded by 1, then the similarity scores will lie on the interval [\u22121, 1]. We will provide an overview of the models in Section 4.2 and then introduce additional notation in 4.3. Starting with Section 4.4 and ending with Section 4.7 we will describe some approaches to compute Si,j given training data."}, {"heading": "4.2 Cross-Lingual Models", "text": "In this section, we will describe several approaches to the problem of computing the multilingual similarities introduced in Section 4.1. We present four approaches: a simple approach based on k-means clustering in Section 4.4, a standard approach based on singular value decomposition in Section 4.5, a related approach called Canonical Correlation Analysis (CCA) in Section 4.6 and finally a new method, which is an extension of CCA to more than two languages in Section 4.7. CCA can be used to find correlated patterns for a pair of languages, whereas the extended method optimizes a Sum of Squared Correlations (SSCOR) between several language pairs, which was introduced in [12]. The SSCOR problem is difficult to solve in our setting (hundreds of thousands of features, hundreds of thousands of examples). To tackle this, we propose a method which consists of two ingredients. The first one is based on an observation that certain datasets (such as Wikipedia) are biased towards one language (English for Wikipedia), which can be exploited to reformulate a difficult optimization problem as an eigenvector problem. The second ingredient is dimensionality reduction using CL-LSI, which makes the eigenvector problem computationally and numerically tractable.\nWe concentrate on approaches that are based on linear maps rather than alternatives, such as machine translation and probabilistic models, as discussed in the section on related work. We will start by introducing some notation."}, {"heading": "4.3 Notation", "text": "The cross-lingual similarity models presented in this paper are based on comparable corpora. A comparable corpus is a collection of documents in multiple languages, with alignment between documents that are of the same topic, or even a rough translation of each other. Wikipedia is an example of a comparable corpus, where a specific entry can be described in multiple languages (e.g., \u201cBerlin\u201d is currently described in 222 languages). News articles represent another example, where the same event can be described by newspapers in several languages.\nMore formally, a multilingual document d = (u1, . . . um) is a tuple of m documents on the same topic (comparable), where ui is the document written in language i. Note that an individual document ui can be an empty document (missing resource) and each d must contain at least two nonempty documents. This means that in our analysis we discard strictly monolingual documents for which no cross-lingual information is available. A comparable corpus D = d1, . . . , ds is a collection of s multilingual documents. By using the vector space model, we can represent D as a set of m matrices X1, . . . , Xm, where Xi \u2208 Rni\u00d7s is the matrix corresponding to the language i and ni is the vocabulary size of language i. Furthermore, let X ` i denote the `-th column of matrix Xi and the matrices respect the document alignment - the vector X ` i corresponds to the TFIDF vector of the i-th component of multilingual document d`. We use N to denote the total row dimension of X, i.e., N := \u2211m i=1 ni. See Figure 3 for an illustration of the introduced notation.\nWe will now describe four models to cross-lingual similarity computation in the next sub-sections.\n4.4 k-means\nThe k-means algorithm is perhaps the most well-known and widely-used clustering algorithm. Here, we present its application to compute cross-lingual similarities. The idea is based on concatenating the corpus\nmatrices, running standard k-means clustering to obtain the matrix of centroids, \u201creversing\u201d the concatenation step to obtain a set of aligned bases, which are finally used to compute cross-lingual similarities. See Figure 4 for overview of the procedure. The left side of Figure 4 illustrates the decomposition and the right side summarizes the coordinate change.\nIn order to apply the algorithm, we first merge all the term-document matrices into a single matrix X by stacking the individual term-document matrices (as seen in Figure 3):\nX := [ XT1 , X T 2 , \u00b7 \u00b7 \u00b7 , XTm ]T ,\nsuch that the columns respect the alignment of the documents (here MATLAB notation for concatenating matrices is used). Therefore, each document is represented by a long vector indexed by the terms in all languages.\nWe then run the k-means algorithm [10] and obtain a centroid matrix C \u2208 RN\u00d7k, where the k columns represent centroid vectors. The centroid matrix can be split vertically into m blocks:\nC = [CT1 \u00b7 \u00b7 \u00b7CTm]T ,\naccording to the number of dimensions of each language, i.e., Ci \u2208 Rni\u00d7k. To reiterate, the matrices Ci are computed using a multilingual corpus matrix X (based on Wikipedia for example).\nTo compute cross-lingual document similarities on new documents, note that each matrix Ci represents a vector space basis and can be used to map points in Rni into a k-dimensional space, where the new coordinates of a vector x \u2208 Rni are expressed as:\n(CTi Ci) \u22121CTi xi.\nThe resulting matrix for similarity computation between language i and language j is defined up to a scaling factor as:\nCi(C T i Ci) \u22121(CTj Cj) \u22121Cj .\nThe matrix is a result of mapping documents in a language independent space using pseudo-inverses of the centroid matrices Pi = (C T i Ci)\n\u22121Ci and then comparing them using the standard inner product, which results in the matrix PTi Pj . For the sake of presentation, we assumed that the centroid vectors are linearly independent. (An independent subspace could be obtained using an additional Gram-Schmidt step [7] on the matrix C, if this was not the case.)"}, {"heading": "4.5 Cross-Lingual Latent Semantic Indexing", "text": "The second approach we consider is Cross-Lingual Latent Semantic Indexing (CL-LSI) [4] which is a variant of LSI [3] for more than one language. The approach is very similar to k-means, where we first concatenate the corpus matrices, compute a decomposition, which in case of CL-LSI is a truncated Singular Value\nDecomposition (SVD), decouple the column space matrix and use the blocks to compute linear maps to a common vector space, where standard cosine similarity is used to compare documents.\nThe method is based on computing a truncated singular value decomposition of the concatenated corpus matrix X \u2248 USV T . See Figure 5 for the decomposition. Representing documents in \u201ctopic\u201c coordinates is done in the same way as in the k-means case (see Figure 4), we will describe how to compute the coordinate change functions.\nThe cross-lingual similarity functions are based on a rank-k truncated SVD:X \u2248 U\u03a3V T , where U \u2208 RN\u00d7k are basis vectors of interest and \u03a3 \u2208 Rk\u00d7k is a truncated diagonal matrix of singular eigenvalues. An aligned basis is obtained by first splitting U vertically according to the number of dimensions of each language: U = [UT1 \u00b7 \u00b7 \u00b7UTm]T . Then, the same as with k-means clustering, we compute the pseudoinverses Pi = (U T i Ui)\n\u22121UTi . The matrices Pi are used to change the basis from the standard basis in Rni to the basis spanned by the columns of Ui.\nImplementation note Since the matrix X can be large we could use an iterative method like the Lanczos algorithm with reorthogonalization [7] to find the left singular vectors (columns of U) corresponding to the largest singular values. It turns out that the Lanczos method converges slowly as the gap between the leading singular values is small. Moreover, the Lanczos method is hard to parallelize. Instead, we use a randomized version of the SVD [8] that can be viewed as a block Lanczos method. That enables us to use parallelization and speeds up the computation considerably.\nTo compute the matrices Pi we used the QR algorithm [7] to factorize Ui as Ui = QiRi, where Q T i Qi = I\nand Ri is a triangular matrix. Pi is then obtained by solving RiPi = Qi."}, {"heading": "4.6 Canonical Correlation Analysis", "text": "We now present a statistical technique to analyze data from two sources, an extension of which will be presented in the next section. Canonical Correlation Analysis (CCA) [11] is a dimensionality reduction technique similar to Principal Component Analysis (PCA) [22], with the additional assumption that the data consists of feature vectors that arose from two sources (two views) that share some information. Examples include: bilingual document collection [5] and collection of images and captions [9]. Instead of looking for linear combinations of features that maximize the variance (PCA) we look for a linear combination of feature vectors from the first view and a linear combination for the second view, that are maximally correlated.\nInterpreting the columns of Xi as observation vectors sampled from an underlying distribution Xi \u2208 Rni , the idea is to find two weight vectors wi \u2208 Rni and wj \u2208 Rnj so that the random variables wTi \u00b7 Xi and wTj \u00b7 Xj are maximally correlated (wi and wj are used to map the random vectors to random variables, by computing weighted sums of vector components). Let \u03c1(x, y) denote the sample-based correlation coefficient between two vectors of observations x and y. By using the sample matrix notation Xi and Xj (assuming no data is missing to simplify the exposition), this problem can be formulated as the following optimization\nproblem:\nmaximize wi\u2208Rni ,wj\u2208Rnj\n\u03c1(wTi Xi, w T j Xj) = wTi Ci,jwj\u221a wTi Ci,iwi \u221a wTj Cj,jwj ,\nwhere Ci,i and Cj,j are empirical estimates of variances of Xi and Xj respectively and Ci,j is an estimate for the covariance matrix. Assuming that the observation vectors are centered (only for the purposes of presentation), the matrices are computed in the following way: Ci,j = 1 n\u22121XiX T j , and similarly for Ci,i and Cj,j . The optimization problem can be reduced to an eigenvalue problem and includes inverting the variance matrices Ci,i and Cj,j . If the matrices are not invertible, one can use a regularization technique by replacing Ci,i with (1\u2212 \u03ba)Ci,i + \u03baI, where \u03ba \u2208 [0, 1] is the regularization coefficient and I is the identity matrix. (The same can be applied to Cj,j .) A single canonical variable is usually inadequate in representing the original random vector and typically one looks for k projection pairs (w1i , w 1 j ), . . . , (w k i , w k j ), so that (w u i )\nTXi and (wuj )\nTXj are highly correlated and (wui )TXi is uncorrelated with (wvi )TXi for u 6= v and analogously for wuj vectors.\nNote that the method in its original form is only applicable to two languages where an aligned set of observations is available. The next section will describe a scalable extension of CCA to more than two languages."}, {"heading": "4.7 Hub language based CCA Extension", "text": "Building cross-lingual similarity models based on comparable corpora is challenging for two main reasons. The first problem is related to missing alignment data: when a number of languages is large, the dataset of documents that cover all languages is small (or may even be empty). Even if only two languages are considered, the set of aligned documents can be small (an extreme example is given by the Piedmontese and Hindi Wikipedias where no inter-language links are available), in which case none of the methods presented so far are applicable. The second challenge is scale - the data is high-dimensional (many languages with hundreds of thousands of features per language) and the number of multilingual documents may be large (over one million in case of Wikipedia). The optimization problem posed by CCA is not trivial to solve: the covariance matrices themselves are prohibitively large to fit in memory (even storing a 100,000 by 100,000 element matrix requires 80GB of memory) and iterative matrix-multiplication based approaches to solving generalized eigenvalue problems are required (the covariance matrices can be expressed as products of sparse matrices, which means we have fast matrix-vector multiplication).\nWe now describe an extension of CCA to more than two languages, which can be trained on large comparable corpora and can handle missing data. The extension we consider is based on a generalization of CCA to more than two views, introduced in [12], namely the Sum of Squared Correlations SSCOR, which we will state formally later in this section. Our approach exploits a certain characteristic of the data, namely the hub language characteristic (see below) in two ways: to reduce the dimensionality of the data and to simplify the optimization problem.\nHub language characteristic. In the case of Wikipedia, we observed that even though the training resources are scarce for certain language pairs, there often exists indirect training data. By considering a third language, which has training data with both languages in the pair, we can use the composition of learned maps as a proxy. We refer to this third language as a hub language.\nA hub language is a language with a high proportion of non-empty documents in D = {d1, ..., d`}. As we have mentioned, we only focus on multilingual documents that include at least two languages. The prototypical example in the case of Wikipedia is English. Our notion of the hub language could be interpreted in the following way. If a non-English Wikipedia page contains one or more links to variants of the page in other languages, English is very likely to be one of them. That makes English a hub language.\nWe use the following notation to define subsets of the multilingual comparable corpus: let a(i, j) denote the index set of all multilingual documents with non-missing data for the i-th and j-th language:\na(i, j) = {k | dk = (u1, ..., um), ui 6= \u2205, uj 6= \u2205} ,\nand let a(i) denote the index set of all multilingual documents with non missing data for the i-th language.\nWe now describe a two step approach to building a cross-lingual similarity matrix. The first part is related to LSI and reduces the dimensionality of the data. The second step refines the linear mappings and optimizes the linear dependence between data.\nStep 1: Hub language based dimensionality reduction. The first step in our method is to project X1, . . . , Xm to lower-dimensional spaces without destroying the cross-lingual structure. Treating the nonzero columns of Xi as observation vectors sampled from an underlying distribution Xi \u2208 Vi = Rni , we can analyze the empirical cross-covariance matrices:\nCi,j = 1 |a(i, j)| \u2212 1 \u2211\n`\u2208a(i,j)\n(X`i \u2212 ci) \u00b7 (X`j \u2212 cj)T ,\nwhere ci = 1 ai \u2211 `\u2208a(i)X ` i . By finding low-rank approximations of Ci,j we can identify the subspaces of Vi and Vj that are relevant for extracting linear patterns between Xi and Xj . Let X1 represent the hub language corpus matrix. The LSI approach to finding the subspaces is to perform the singular value decomposition on the full N \u00d7 N covariance matrix composed of blocks Ci,j . If |a(i, j)| is small for many language pairs (as it is in the case of Wikipedia), then many empirical estimates Ci,j are unreliable, which can result in overfitting. For this reason, we perform the truncated singular value decomposition on the matrix C = [C1,2 \u00b7 \u00b7 \u00b7C1,m] \u2248 USV T , where U \u2208 Rn1\u00d7k, S \u2208 Rk\u00d7k, V \u2208 R( \u2211m i=2 ni)\u00d7k. We split the matrix V vertically in blocks with n2, . . . , nm rows: V = [V T 2 \u00b7 \u00b7 \u00b7V Tm ]T . Note that columns of U are orthogonal but columns in each Vi are not (columns of V are orthogonal). Let V1 := U . We proceed by reducing the dimensionality of each Xi by setting: Yi = V T i \u00b7Xi, where Yi \u2208 Rk\u00d7N . To summarize, the first step reduces the dimensionality of the data and is based on CL-LSI, but optimizes only the hub language related cross-covariance blocks.\nStep 2: Simplifying and solving SSCOR. The second step involves solving a generalized version of canonical correlation analysis on the matrices Yi in order to find the mappings Pi. The approach is based on the sum of squares of correlations formulation by Kettenring [12], where we consider only correlations between pairs (Y1, Yi), i > 1 due to the hub language problem characteristic. We will present the original unconstrained optimization problem, then a constrained formulation based on the hub language problem characteristic. Then we will simplify the constraints and reformulate the problem as an eigenvalue problem by using Lagrange multipliers.\nThe original sum of squared correlations is formulated as an unconstrained problem:\nmaximize wi\u2208Rk m\u2211 i<j \u03c1(wTi Yi, w T j Yj) 2.\nWe solve a similar problem by restricting i = 1 and omitting the optimization over non-hub language pairs. Let Di,i \u2208 Rk\u00d7k denote the empirical covariance of Yi and Di,j denote the empirical cross-covariance computed based on Yi and Yj . We solve the following constrained (unit variance constraints) optimization problem:\nmaximize wi\u2208Rk m\u2211 i=2 ( wT1 D1,iwi )2 subject to wTi Di,iwi = 1, \u2200i = 1, . . . ,m. (1)\nThe constraints wTi Di,iwi can be simplified by using the Cholesky decomposition Di,i = K T i \u00b7 Ki and substitution: yi := Kiwi. By inverting the Ki matrices and defining Gi := K \u2212T 1 D1,iK \u22121 i , the problem can be reformulated:\nmaximize yi\u2208Rk m\u2211 i=2 ( yT1 Giyi )2 subject to yTi yi = 1, \u2200i = 1, . . . ,m. (2)\nA necessary condition for optimality is that the derivatives of the Lagrangian vanish. The Lagrangian of (2) is expressed as:\nL(y1, . . . , ym, \u03bb1, . . . , \u03bbm) = m\u2211 i=2 ( yT1 Giyi )2 + m\u2211 i=1 \u03bbi ( yTi yi \u2212 1 ) .\nStationarity conditions give us:\n\u2202\n\u2202x1 L = 0\u21d2 m\u2211 i=2 ( yT1 Giyi ) Giyi + \u03bb1y1 = 0, (3)\n\u2202\n\u2202xi L = 0\u21d2\n( yT1 Giyi ) GTi y1 + \u03bbiyi = 0, i > 1. (4)\nMultiplying the equations (4) with yTi and applying the constraints, we can eliminate \u03bbi which gives us:\nGTi y1 = ( yT1 Giyi ) yi, i > 1. (5)\nPlugging this into (3), we obtain an eigenvalue problem:( m\u2211 i=2 GiG T i ) y1 + \u03bb1y1 = 0.\nThe eigenvectors of (\u2211m\ni=2GiG T i ) solve the problem for the first language. The solutions for yi are ob-\ntained from (5): yi := GTi y1 \u2016GTi y1\u2016 . Note that the solution (1) can be recovered by: wi := K \u22121 i yi. The linear transformation of the w variables are thus expressed as:\nY1 := eigenvectors of m\u2211 i=2 GiG T i ,\nW1 = K \u22121 1 Y1\nWi = K \u22121 i G T i Y1N,\nwhere N is a diagonal matrix that normalizes GTi Y1, with N(j, j) := 1 \u2016G(iY1(:,j)\u2016 .\nRemark. The technique is related to Generalization of Canonical Correlation Analysis (GCCA) by Carroll [2], where an unknown group configuration variable is defined and the objective is to maximize the sum of squared correlations between the group variable and the others. The problem can be reformulated as an eigenvalue problem. The difference lies in the fact that we set the unknown group configuration variable as the hub language, which simplifies the solution. The complexity of our method is O(k3), where k is the reduced dimension from the LSI preprocessing step, whereas solving the GCCA method scales as O(s3), where s is the number of samples (see [6]). Another issue with GCCA is that it cannot be directly applied to the case of missing documents.\nTo summarize, we first reduced the dimensionality of our data to k-dimensional features and then found a new representation (via linear transformation) that maximizes directions of linear dependence between the languages. The final projections that enable mappings to a common space are defined as: Pi(x) = W T i V T i x."}, {"heading": "5 Cross-lingual Event Linking", "text": "The main application on which we test the cross-lingual similarity is cross-lingual event linking. In online media streams \u2013 particularly news articles \u2013 there is often duplication of reporting, different viewpoints or opinions, all centering around a single event. The same events are covered by many articles and the question\nwe address is how to find all the articles in different languages that are describing a single event. In this paper we consider the problem of matching events from different languages. We do not address the problem of detection of events and instead base our evaluation on an online system for detection of world events, Event Registry. The events are represented by clusters of articles and so ultimately our problem reduces to finding suitable matchings between clusters with articles in different languages."}, {"heading": "5.1 Problem definition", "text": "The problem of cross-lingual event linking is to match monolingual clusters of news articles that describing the same event across languages. For example, we want to match a cluster of Spanish news articles and a cluster of English news articles that both describe the same earthquake.\nEach article a \u2208 A is written in a language `, where ` \u2208 L = {`1, `2, ..., `m}. For each language `, we obtain a set of monolingual clusters C`. More precisely, the articles corresponding to each cluster c \u2208 C` are written in the language `. Given a pair of languages `a \u2208 L, `b \u2208 L and `a 6= `b, we would like to identify all cluster pairs (ci, cj) \u2208 C`a \u00d7 C`b such that ci and cj describe the same event.\nMatching of clusters is a generalized matching problem. We cannot assume that there is only one cluster per language per event, nor can we assume complete coverage \u2013 i.e., that there exists at least one cluster per event in every language. This is partly due to news coverage which might be more granular in some languages, partly due to noise and errors in the event detection process. This implies that we cannot make assumptions on the matching (e.g., one-to-one or complete matching) and excludes the use of standard weighted bipartite matching type of algorithms for this problem. An example is shown in Figure 6, where a cluster may contain articles which are closely matched with many clusters in a different language.\nWe are also seeking for an algorithm which does not do exhaustive comparison of all clusters, since that can become prohibitively expensive when working in a real-time setting. More specifically, we wish to avoid testing cluster ci with all the clusters from all the other languages. Performing exhaustive comparison would result in O(|C|2) tests, where |C| is the number of all clusters (over all languages), which is not feasible when the number of clusters is on the order of tens of thousands. We address this by testing only clusters that are connected with at least one k-nearest neighbor (marked as candidate clusters in Figure 6)."}, {"heading": "5.2 Algorithm", "text": "In order to identify clusters that are equivalent to cluster ci, we have developed a two-stage algorithm. For a cluster ci, we first efficiently identify a small set of candidate clusters and then find those clusters among\ninput: test cluster ci, a set of clusters C` for each language ` \u2208 L output: a set of clusters C that are potentially equivalent to ci C \u2190 {}; for article ai \u2208 ci do\nfor language ` \u2208 L do /* use hub CCA to find 10 most similar articles to article ai in language ` */ SimilarArticles = getCCASimilarArticles(ai, `); for article aj \u2208 SimilarArticles do\n/* find cluster cj to which article aj is assigned to */ cj \u2190 c, such that c \u2208 C` and aj \u2208 c; /* add cluster cj to the set of candidates C */ C \u2190 C \u222a {cj};\nend\nend\nend Algorithm 1: Algorithm for identifying candidate clusters C that are potentially equivalent to ci\nthe candidates, which are equivalent to ci. An example is shown in Figure 6. The details of the first step are described in Algorithm 1. The algorithm begins by individually inspecting each news article ai in the cluster ci. Using a chosen method for computing cross-lingual document similarity (see Section 4.2), it identifies the 10 most similar news articles to ai in each language ` \u2208 L. For each similar article aj , we identify its corresponding cluster cj and add it to the set of candidates. The set of candidate clusters obtained in this way is several orders of magnitude smaller than the number of all clusters, and at most linear with respect to the number of news articles in cluster ci. In practice, clusters contain highly related articles and as such similar articles from other languages mostly fall in only a few candidate clusters.\nAlthough computed document similarities are approximate, our assumption is that articles in different languages describing the same event will generally have a higher similarity than articles about different events. While this assumption does not always hold, redundancy in the data mitigates these false positives. Since we compute the 10 most similar articles for each article in ci, we are likely to identify all the relevant candidates for cluster ci.\nThe second stage of the algorithm determines which (if any) of the candidate clusters are equivalent to ci. We treat this task as a supervised learning problem. For each candidate cluster cj \u2208 C, we compute a vector of learning features that should be indicative of whether ci and cj are equivalent or not and apply a binary classification model that predicts if the clusters are equivalent or not. The classification algorithm that we used to train a model was a linear Support Vector Machine (SVM) method [34].\nWe use three groups of features to describe cluster pair (ci, cj). The first group is based on cross-lingual article links, which are derived using cross-lingual similarity: each news article ai is linked with its 10- nearest neighbors articles from all other languages (10 per each language). The group contains the following features:\n\u2022 linkCount is the number of times any of the news articles from cj is among 10-nearest neighbors for articles from ci. In other words, it is the number of times an article from ci has a very similar article (i.e., is among 10 most similar) in cj .\n\u2022 avgSimScore is the average similarity score of the links, as identified for linkCount, between the two clusters.\nThe second group are concept-related features. Articles that are imported into Event Registry are annotated by disambiguating mentioned entities and keywords to the corresponding Wikipedia pages [41]. Whenever Barack Obama is, for example, mentioned in the article, the article is annotated with a link to his Wikipedia page. In the same way, all mentions of entities (people, locations, organizations) and\nordinary keywords (e.g., bank, tax, ebola, plane, company) are annotated. Although the Spanish article about Obama will be annotated with his Spanish version of the Wikipedia page, in many cases we can link the Wikipedia pages to their English versions. This can be done since Wikipedia itself provides information regarding which pages in different languages represent the same concept/entity. Using this approach, the word \u201cavio\u0301n\u201d in a Spanish article will be annotated with the same concept as the word \u201cplane\u201d in an English article. Although the articles are in different languages, the annotations can therefore provide a languageindependent vocabulary that can be used to compare articles/clusters. By analyzing all the articles in clusters ci and cj , we can identify the most relevant entities and keywords for each cluster. Additionally, we can also assign weights to the concepts based on how frequently they occur in the articles in the cluster. From the list of relevant concepts and corresponding weights, we consider the following features:\n\u2022 entityCosSim is the cosine similarity between vectors of entities from clusters ci and cj .\n\u2022 keywordCosSim is the cosine similarity between vectors of keywords from clusters ci and cj .\n\u2022 entityJaccardSim is Jaccard similarity coefficient [18] between sets of entities from clusters ci and cj .\n\u2022 keywordJaccardSim is Jaccard similarity coefficient between sets of keywords from clusters ci and cj .\nThe last group of features contains three miscellaneous features that seem discriminative but are unrelated to the previous two groups:\n\u2022 hasSameLocation feature is a boolean variable that is true when the location of the event in both clusters is the same. The location of events is estimated by considering the locations mentioned in the articles that form a cluster and is provided by Event Registry.\n\u2022 timeDiff is the absolute difference in hours between the two events. The publication time and date of the events is computed as the average publication time and date of all the articles and is provided by Event Registry.\n\u2022 sharedDates is determined as the Jaccard similarity coefficient between sets of date mentiones extracted from articles. We use extracted mentions of dates provided by Event Registry, which uses an extensive set of regular expressions to detect and normalize mentions of dates in different forms."}, {"heading": "6 Evaluation", "text": "We will describe the main dataset for building cross-lingual models which is based on Wikipedia and then present two sets of experiments. The first set of experiments establishes that the hub based approach can deal with language pairs where little or no training data is available. The second set of experiments compares the main approaches that we presented on the task of mate retrieval and the task of event linking. Finally, we examine how different choices of features impact the event linking performance."}, {"heading": "6.1 Wikipedia Comparable Corpus", "text": "To investigate the empirical performance of the low-rank approximations we will test the algorithms on a large-scale, real-world multilingual dataset that we extracted from Wikipedia by using inter-language links for alignment. This results in a large number of weakly comparable documents in more than 200 languages. Wikipedia is a large source of multilingual data that is especially important for the languages for which no translation tools, multilingual dictionaries as Eurovoc [29], or strongly aligned multilingual corpora as Europarl [13] are available. Documents in different languages are related with so called \u2019inter-language\u2019 links that can be found on the left of the Wikipedia page. The Wikipedia is constantly growing. There are currently 12 Wikipedias with more than 1 million articles, 52 with more than 100k articles, 129 with more than 10k articles, and 236 with more than 1, 000 articles.\nEach Wikipedia page is embedded in the page tag. First, we check if the title of the page starts with a Wikipedia namespace (which includes categories and discussion pages) and do not process the page if it does. Then, we check if this is a redirection page and we store the redirect link because inter-language links can point to redirection links also. If none of the above applies, we extract the text and parse the Wikipedia markup. Currently, all the markup is removed.\nWe get inter-language link matrix using previously stored redirection links and inter-language links. If an inter-language link points to the redirection we replace it with the redirection target link. It turns out that we obtain the matrix M that is not symmetric, consequently the underlying graph is not symmetric. That means that existence of the inter-language link in one way (i.e., English to German) does not guarantee that there is an inter-language link in the reverse direction (German to English). To correct this we transform this matrix to be symmetric by computing M + MT and obtaining an undirected graph. In the rare case that after symmetrization we have multiple links pointing from the document, we pick the first one that we encountered. This matrix enables us to build an alignment across all Wikipedia languages (for dumps available in 2013)."}, {"heading": "6.2 Experiments With Missing Alignment Data", "text": "In this subsection, we will investigate the empirical performance of hub CCA approach. We will demonstrate that this approach can be successfully applied even in the case of fully missing alignment information. To this purpose, we select a subset of Wikipedia languages containing three major languages, English (4,212k articles)\u2013en (hub language), Spanish (9,686k articles)\u2013es, Russian (9,662k articles)\u2013ru, and five minority (in terms of Wikipedia sizes) languages, Slovenian (136k articles)\u2013sl, Piedmontese (59k articles)\u2013pms, WarayWaray (112k articles)\u2013war (all with about 2 million native speakers), Creole (54k articles)\u2013ht (8 million native speakers), and Hindi (97k articles)\u2013hi (180 million native speakers). For preprocessing, we remove the documents that contain less than 20 different words (referred to as stubs2) and remove words occurring in less than 50 documents as well as the top 100 most frequent words (in each language separately). We represent the documents as normalized TFIDF [33] weighted vectors. The IDF scores are computed for each language based on its aligned documents with the English Wikipedia. The English language IDF scores are based on all English documents for which aligned Spanish documents exist.\nThe evaluation is based on splitting the data into training and test sets. We select the test set documents as all multilingual documents with at least one nonempty alignment from the list: (hi, ht), (hi, pms), (war, ht), (war, pms). This guarantees that we cover all the languages. Moreover this test set is suitable for testing the retrieval through the hub as the chosen pairs have empty alignments. The remaining documents are used for training. In Table 1, we display the corresponding sizes of training and test documents for each language pair.\nOn the training set, we perform the two step procedure to obtain the common document representation as a set of mappings Pi. A test set for each language pair, testi,j = {(x`, y`)|` = 1 : n(i, j)}, consists of comparable document pairs (linked Wikipedia pages), where n(i, j) is the test set size. We evaluate the representation by measuring mate retrieval quality on the test sets: for each `, we rank the projected documents Pj(y1), . . . , Pj(yn(i,j)) according to their similarity with Pi(x`) and compute the rank of the mate document r(`) = rank(Pj(y`)). The final retrieval score (between -100 and 100) is computed as: 100\nn(i,j) \u00b7 \u2211n(i,j) `=1 ( n(i,j)\u2212r(`) n(i,j)\u22121 \u2212 0.5 ) . A score that is less than 0 means that the method performs worse than random retrieval and a score of 100 indicates perfect mate retrieval. The mate retrieval results are included in Table 2.\nWe observe that the method performs well on all pairs of languages, where at least 50,000 training documents are available(en, es, ru, sl). We note that taking k = 500 or k = 1, 000 multilingual topics usually results in similar performance, with some notable exceptions: in the case of (ht, war) the additional topics result in an increase in performance, as opposed to (ht, pms) where performance drops, which suggests overfitting. The languages where the method performs poorly are ht and war, which can be explained by\n2Such documents are typically of low value as a linguistic resource. Examples include the titles of the columns in the table, remains of the parsing process, or Wikipedia articles with very little or no information contained in one or two sentences.\nthe quality of data (see Table 3 and explanation that follows). In case of pms, we demonstrate that solid performance can be achieved for language pairs (pms, sl) and (pms, hi), where only 2,000 training documents are shared between pms and sl and no training documents are available between pms and hi. Also observe that in the case of (pms, ht) the method still obtains a score of 62, even though training set intersection is zero and ht data is corrupted, which we will show in the next paragraph.\nWe further inspect the properties of the training sets by roughly estimating the fraction rank(A)min(rows(A), cols(A)) for each English training matrix and its corresponding mate matrix, where rows(A) and cols(A) denote the number of rows and columns respectively. The denominator represents the theoretically highest possible rank the matrix A could have. Ideally, these two fractions should be approximately the same - both aligned spaces should have reasonably similar dimensionality. We display these numbers as pairs in Table 3.\nIt is clear that in the case of the Creole language only at most 22% documents are unique and suitable for the training. Though we removed the stub documents, many of the remaining documents are nearly the same, as the quality of some smaller Wikipedias is low. This was confirmed for the Creole, Waray-Waray, and Piedmontese languages by manual inspection. The low quality documents correspond to templates about the year, person, town, etc. and contain very few unique words.\nThere is also a problem with the quality of the test data. For example, if we look at the test pair (war, ht) only 386/534 Waray-Waray test documents are unique but on the other side almost all Creole test documents (523/534) are unique. This indicates a poor alignment which leads to poor performance."}, {"heading": "6.3 Evaluation Of Cross-Lingual Event Linking", "text": "In order to determine how accurately we can predict cluster equivalence, we performed two experiments in a multilingual setting using English, German and Spanish languages for which we had labelled data to evaluate the linking performance. In the first experiment, we tested how well the individual approaches for cross-lingual article linking perform when used for linking the clusters about the same event. In the second experiment we tested how accurate the prediction model is when trained on different subsets of learning features. To evaluate the prediction accuracy for a given dataset we used 10-fold cross validation.\nWe created a manually labelled dataset in order to evaluate cross-lingual event linking using two human annotators. The annotators were provided with an interface listing the articles, their content from and top concepts for a pair of clusters and their task was to determine if the clusters were equivalent or not (i.e., discuss same event). To obtain a pair of clusters (ci, cj) to annotate, we first randomly chose a cluster ci, used Algorithm 1 to compute a set of potentially equivalent clusters C and randomly chose a cluster cj \u2208 C. The dataset provided by the annotators contains 808 examples, of which 402 are equivalent clusters pairs and 406 are not. Clusters in each learning example are either in English, Spanish or German. Although Event Registry imports articles in other languages as well, we restricted our experiments to these three languages. We chose only these three languages since they have very large number of articles and clusters per day which makes the cluster linking problem hard due to large number of possible links.\nIn Section 4.2, we described three main algorithms for identifying similar articles in different languages. These algorithms were k-means, LSI and hub CCA. As a training set, we used common Wikipedia alignment for all three languages. To test which of these algorithms performed best, we made the following test. For each of the three algorithms, we analyzed all articles in Event Registry and for each article computed the most similar articles in other languages. To test how informative the identified similar articles are for cluster linking we then trained three classifiers as described in Section 5.2 \u2013 one for each algorithm. Each classifier was allowed to use as learning features only the cross-lingual article linking features for which values are determined based on the selected algorithm (k-means, LSI and hub CCA). The results of the trained models are shown in Table 4. We also show how the number of topics (the dimensions of the latent space) influences the quality, except in the case of the k-means algorithm, where only the performance on 500 topic vectors is reported, due to higher computational cost.\nWe observe that, for the task of cluster linking, LSI and hub CCA perform comparably and both outperform k-means.\nWe also compared the proposed approaches on the task of Wikipedia mate retrieval (the same task as in Section 6.2). We computed the Average (over language pairs) Mean Reciprocal Rank (AMRR) [37] performance of the different approaches on the Wikipedia data by holding out 15, 000 aligned test documents and using 300, 000 aligned documents as the training set. Figure 7 shows AMRR score as the function of the number of feature vectors. It is clear that hub CCA outperforms LSI approach and k-means lags far behind when testing on Wikipedia data. The hub CCA approach with 500 topic vectors manages to perform comparably to the LSI-based approach with 1, 000 topic vectors, which shows that the CCA method can improve both model memory footprint as well as similarity computation time.\nFurthermore, we inspected how the number of topics influences the accuracy of cluster linking. As we can see from Table 4 choosing a number of features larger than 500 barely affects linking performance, which is in contrast with the fact that additional topics helped to improve AMMR, see Figure 7. Such differences may have arisen due to different domains of training and testing (Wikipedia pages versus news articles).\nWe also analyzed how cluster size influences the accuracy of cluster linking. We would expect that if the tested pair of clusters has a larger number of articles then the classifier should be able to more accurately predict whether the clusters should be linked or not. The reasoning is that the large clusters would provide more document linking information (more articles mean more links to other similar articles) as well as more accurately aggregated semantic information. In the case of smaller clusters, the errors of the similarity\nmodels have greater impact which should decrease the performance of the classifier, too. To validate this hypothesis we have split the learning examples into two datasets \u2013 one containing cluster pairs where the combined number of articles from both clusters is below 20 and one dataset where the combined number is 20 or more. The results of the experiment can be seen in Table 5. As it can be seen, the results confirm our expectations: for smaller clusters it is indeed harder to correctly predict if the cluster pair should be merged or not.\nThe hub CCA attains higher precision and classification accuracy on the task of linking small cluster pairs than the other methods, while LSI is slightly better on linking large cluster pairs. The gain in precision of LSI over hub CCA on linking large clusters is much smaller than the gain in precision of hub CCA over LSI on linking small clusters. For that reason we decided to use hub CCA as the similarity computation component in our system.\nIn the second experiment, we evaluate how relevant individual groups of features are to correctly determine cluster equivalence. For this purpose, we computed accuracy using individual groups of features, as well as using different combination of groups. Since hub CCA had the best performance of the three algorithms, we used it to compute the values of the cross-lingual article linking features. The results of the evaluation are shown in Table 6. We can see that using a single group of features, the highest prediction accuracy can be achieved using concept-related features. The classification accuracy in this case is 88.5%. By additionally\nincluding also the cross-lingual article linking features, the classification accuracy rises slightly to 89.4%. Using all three groups of features, the achieved accuracy is 89.2%.\nTo test if the accuracy of the predictions is language dependent we have also performed the evaluations separately on individual language pairs. For this experiment we have split the annotated learning examples into three datasets, where each dataset contained only examples for one language pair. When training the classifier all three groups of features were available. The results are shown in Table 7. We can see that the performance of cluster linking on the English-German dataset is the highest in terms of accuracy, precision, recall and F1. The performance on the English-Spanish dataset is comparable to the performance on the English-German dataset, where the former achieves higher recall (and slightly higher F1 score), while the latter achieves higher precision. A possible explanation of these results is that the higher quantity and quality of English-German language resources leads to a more accurate cross-lingual article similarity measure as well as to a more extensive semantic annotation of the articles.\nBased on the performed experiments, we can make the following conclusions. The cross-lingual similarity algorithms provide valuable information that can be used to identify clusters that describe the same event in different languages. For the task of cluster linking, the cross-lingual article linking features are however significantly less informative compared to the concept-related features that are extracted from the semantic annotations. Nevertheless, the cross-lingual article similarity features are very important for two reasons. The first is that they allow us to identify for a given cluster a limited set of candidate clusters that are potentially equivalent. This is a very important feature since it reduces the search space by several orders of magnitude. The second reason these features are important is that concept annotations are not available for all articles as the annotation of news articles is computationally intensive and can only be done for a subset of collected articles. The prediction accuracies for individual language pairs are comparable although it seems that the achievable accuracy correlates with the amount of available language resources."}, {"heading": "6.4 Remarks on the scalability of the implementation", "text": "One of the main advantages of our approach is that it is highly scalable. It is fast, very robust to quality of training data, easily extendable, simple to implement and has relatively small hardware requirements. The similarity pipeline is the most computationally intensive part and currently runs on a machine with two Intel Xeon E5-2667 v2, 3.30GHz processors with 256GB of RAM. This is sufficient to do similarity computation over a large number of languages if needed. It currently uses Wikipedia as a freely available knowledge base and experiments show that the similarity pipeline dramatically reduces the search space when linking clusters.\nCurrently, we compute similarities over 24 languages with tags: eng, spa, deu, zho, ita, fra, rus, swe, nld, tur, jpn, por, ara, fin, ron, kor, hrv, tam, hun, slv, pol, srp, cat, ukr but we support any language from the top 100 Wikipedia languages. Our data stream is Newsfeed (http://newsfeed.ijs.si/) which provides 430k unique articles per day. Our system currently computes 2 million similarities per second, that means that we compute 16 \u00b7 1010 similarities per day. We store one day buffer for each language which requires 1.5 GB of memory with documents stored as 500-dimensional vectors. We note that the time complexity of the similarity computations scales linearly with dimension of the feature space and does not depend on number of languages. For each article, we compute the top 10 most similar ones in every other language.\nFor all linear algebra matrix and vector operations, we use high performance numerical linear algebra libraries as BLAS, OPENBLAS and Intel MKL, which currently allows us to process more than one million articles per day. In our current implementation, we use the variation of the hub approach. Our projector matrices are of size 500\u00d7 300, 000, so every projector takes about 1.1 GB of RAM. Moreover, we need proxy matrices of size 500 \u00d7 500 for every language pair. That is 0.5 GB for 24 languages and 9.2 GB for 100 languages. All together we need around 135 GB of RAM for the system with 100 languages. Usage of proxy matrices enables the projection of all input documents in the common space and handling language pairs with missing or low alignment. That enables us to do block-wise similarity computations further improving system efficiency. Our code can therefore be easily parallelized using matrix multiplication rather than performing more matrix - vector multiplications. This speeds up our code by a factor of around 4. In this way, we obtain some caching gains and ability to use vectorization. Our system is also easily extendable. Adding a new language requires the computation of a projector matrix and proxy matrices with all other already available languages."}, {"heading": "7 Discussion and Future Work", "text": "In this paper we have presented a cross-lingual system for linking events in different languages. Building on an existing system, Event Registry, we present and evaluate several approaches to compute a cross-lingual similarity function. We also present an approach to link events and evaluate effectiveness of various features. The final pipeline is scalable both in terms of number of articles and number of languages, while accurately linking events.\nOn the task of mate retrieval, we observe that refining the LSI-based projections with hub CCA leads to improved retrieval precision, but the methods perform comparably on the task of event linking. Further inspection showed that the CCA-based approach reached a higher precision on smaller clusters. The inter-\npretation is that the linking features are highly aggregated for large clusters, which compensates the lower per-document precision of LSI. Another possible reason is that the advantage that we show on Wikipedia is lost on the news domain. This hypothesis could be validated by testing the approach on documents from a different domain.\nThe experiments show that the hub CCA-based features present a good baseline, which can greatly benefit from additional semantic-based features. Even though in our experiments the addition of CCA-based features to semantic features did not lead to great performance improvements, there are two important benefits in the approach. First, the linking process can be sped up by using a smaller set of candidate clusters. Second, the approach is robust to languages where semantic extraction is not available, due to scarce linguistic resources."}, {"heading": "7.1 Future Work", "text": "Currently the system is loosely-coupled \u2013 the language component is built independently from the rest of the system, in particular the linking component. It is possible that better embeddings can be obtained by methods that jointly optimize a classification task and the embedding.\nAnother point of interest is to evaluate the system on languages with scarce linguistic resources, where semantic annotation might not be available. For this purpose, the labelled dataset of linked clusters should be extended first. The mate retrieval evaluation showed that even for language pairs with no training set overlap, the hub CCA recovers some signal.\nIn order to further improve the performance of the classifier for cluster linking, additional features should also be extracted from articles and clusters and checked if they can increase the accuracy of the classification. Since the amount of linguistic resources vary significantly from language to language it would also make sense to build a separate classifier for each language pair. Intuitively, this should improve performance since weights of individual learning features could be adapted to the tested pair of languages."}], "references": [{"title": "A high-performance multithreaded approach for clustering a stream of documents", "author": ["Janez Brank", "Gregor Leban", "Marko Grobelnik"], "venue": "In Proceedings of the 17th International Multiconference Information Society", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Generalization of canonical correlation analysis to three or more sets of variables", "author": ["J.D. Carroll"], "venue": "Proceedings of the American Psychological Association,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1968}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "T.K. Landauer", "G.W. Furnas", "R.A. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1990}, {"title": "Automatic cross-language retrieval using latent semantic indexing. In AAAI spring symposium on cross-language text and speech retrieval", "author": ["S.T. Dumais", "T.A. Letsche", "M.L. Littman", "T.K Landauer"], "venue": "American Association for Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Kernel methods in bioengineering, communications and image processing, chapter A Kernel Canonical Correlation Analysis For Learning The Semantics Of Text, pages 263\u2013282", "author": ["B. Fortuna", "N. Cristianini", "J. Shawe-Taylor"], "venue": "Idea Group Publishing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Nonlinear Multivariate Analysis", "author": ["Albert Gifi"], "venue": "Wiley Series in Probability and Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P.G. Martinsson", "J.A. Tropp"], "venue": "Society for Industrial and Applied Mathematics Review,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Using image stimuli to drive fmri analysis", "author": ["David R Hardoon", "Janaina Mourao-Miranda", "Michael Brammer", "John Shawe-Taylor"], "venue": "In Neural Information Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Clustering algorithms", "author": ["John Hartigan"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1975}, {"title": "The most predictable criterion", "author": ["Harold Hotelling"], "venue": "Journal of educational Psychology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1935}, {"title": "Canonical analysis of several sets of variables", "author": ["J.R. Kettenring"], "venue": "Biometrika, 58:433\u201345,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1971}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In The Tenth Machine Translation Summit,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst. Moses"], "venue": "In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Cross-lingual detection of world events from news articles", "author": ["Gregor Leban", "Blaz Fortuna", "Janez Brank", "Marko Grobelnik"], "venue": "In Proceedings of the 13th International Semantic Web Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Event registry: Learning about world events from news", "author": ["Gregor Leban", "Blaz Fortuna", "Janez Brank", "Marko Grobelnik"], "venue": "In Proceedings of the Companion Publication of the 23rd International Conference on World Wide Web Companion, WWW Companion", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Gdelt: Global data on events, location, and tone, 1979\u20132012", "author": ["Kalev Leetaru", "Philip A Schrodt"], "venue": "In International Studies Association (ISA) Annual Convention,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Distance between sets", "author": ["Michael Levandowsky", "David Winter"], "venue": "Nature, 234(5323):34\u201335,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1971}, {"title": "Learning to link with wikipedia", "author": ["David Milne", "Ian H. Witten"], "venue": "In Proceedings of the 17th ACM Conference on Information and Knowledge Management,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Polylingual topic models", "author": ["David Mimno", "Hanna M. Wallach", "Jason Naradowsky", "David A. Smith", "Andrew McCallum"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Cross-lingual document similarity. In Information Technology Interfaces (ITI)", "author": ["Andrej Muhi\u010d", "Jan Rupnik", "Primo\u017e \u0160kraba"], "venue": "Proceedings of the ITI 2012 34th International Conference on,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "Philosophical Magazine,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1901}, {"title": "Multilingual Information Retrieval", "author": ["Carol Peters", "Martin Braschler"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Translingual document representations from discriminative projections", "author": ["John C Platt", "Kristina Toutanova", "Wen-tau Yih"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Cross-language plagiarism detection", "author": ["Martin Potthast", "Alberto Barr\u00f3n-Cede\u00f1o", "Benno Stein", "Paolo Rosso"], "venue": "Language Resources and Evaluation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "A wikipedia-based multilingual retrieval model. In Advances in Information Retrieval", "author": ["Martin Potthast", "Benno Stein", "Maik Anderka"], "venue": "European Conference on Information Retrieval Research (ECIR),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Story tracking: linking similar news over time and across languages", "author": ["Bruno Pouliquen", "Ralf Steinberger", "Olivier Deguernel"], "venue": "In Proceedings of the Workshop on Multi-source Multilingual Information Extraction and Summarization,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Automatic annotation of multilingual text collections with a conceptual thesaurus", "author": ["Bruno Pouliquen", "Ralf Steinberger", "Camelia Ignat"], "venue": "arXiv preprint cs/0609059,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}, {"title": "Promoting government controlled vocabularies for the semantic web: the eurovoc thesaurus and the cpv product classification system", "author": ["Jose Ma\u0155\u0131a \u00c1lvarez Rod\u0155\u0131guez", "Emilio Rubiera Azcona", "Luis Polo Paredes"], "venue": "Semantic Interoperability in the European Digital Library,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Low-rank approximations for large, multi-lingual data. Low Rank Approximation and Sparse Representation, Neural Information Processing Systems", "author": ["Jan Rupnik", "Andrej Muhic", "Primoz Skraba"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Spanning spaces: Learning cross-lingual similarities. Beyond Mahalanobis: Supervised Large-Scale Learning of Similarity, Neural Information Processing Systems", "author": ["Jan Rupnik", "Andrej Muhic", "Primoz Skraba"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Multilingual document retrieval through hub languages", "author": ["Jan Rupnik", "Andrej Muhic", "Primoz Skraba"], "venue": "In Proceedings of the 15th Multiconference on Information Society 2012 (IS-2012),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Gerard Salton", "Christopher Buckley"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1988}, {"title": "Kernel Methods for Pattern Analysis", "author": ["John Shawe-Taylor", "Nello Cristianini"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Newsexplorer: Multilingual news analysis with cross-lingual linking", "author": ["Ralf Steinberger", "Bruno Pouliquen", "Camelia Ignat"], "venue": "Information Technology Interfaces,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2005}, {"title": "The internals of an aggregated web news feed", "author": ["Mitja Trampu\u0161", "Bla\u017e Novak"], "venue": "In Proceedings of 15th Multiconference on Information Society 2012 (IS-2012),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "The trec-8 question answering track report", "author": ["Ellen M Voorhees"], "venue": "In Proceedings of the 8th Text Retrieval Conference (TREC-8),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1999}, {"title": "A novel two-step method for cross language representation learning", "author": ["Min Xiao", "Yuhong Guo"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Cross-lingual latent topic extraction", "author": ["Duo Zhang", "Qiaozhu Mei", "ChengXiang Zhai"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Semantic annotation, analysis and comparison: A multilingual and cross-lingual text analytics toolkit", "author": ["Lei Zhang", "Achim Rettinger"], "venue": "In Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "X-lisa: Cross-lingual semantic annotation", "author": ["Lei Zhang", "Achim Rettinger"], "venue": "Proceedings of the Very Large Data Bases (VLDB) Endowment,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "Within a recently developed system Event Registry [16, 15] we examine two aspects of this problem: how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event.", "startOffset": 50, "endOffset": 58}, {"referenceID": 13, "context": "Within a recently developed system Event Registry [16, 15] we examine two aspects of this problem: how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event.", "startOffset": 50, "endOffset": 58}, {"referenceID": 14, "context": "Within each monolingual stream, an online clustering approach is employed, where tracked clusters correspond to our definition of events - this is based on the Event Registry system [16, 15].", "startOffset": 182, "endOffset": 190}, {"referenceID": 13, "context": "Within each monolingual stream, an online clustering approach is employed, where tracked clusters correspond to our definition of events - this is based on the Event Registry system [16, 15].", "startOffset": 182, "endOffset": 190}, {"referenceID": 28, "context": "The first ingredient of our approach to link clusters across languages represents a continuation of previous work [30, 32, 31, 21] where we explored representations of documents which were valid over multiple languages.", "startOffset": 114, "endOffset": 130}, {"referenceID": 30, "context": "The first ingredient of our approach to link clusters across languages represents a continuation of previous work [30, 32, 31, 21] where we explored representations of documents which were valid over multiple languages.", "startOffset": 114, "endOffset": 130}, {"referenceID": 29, "context": "The first ingredient of our approach to link clusters across languages represents a continuation of previous work [30, 32, 31, 21] where we explored representations of documents which were valid over multiple languages.", "startOffset": 114, "endOffset": 130}, {"referenceID": 19, "context": "The first ingredient of our approach to link clusters across languages represents a continuation of previous work [30, 32, 31, 21] where we explored representations of documents which were valid over multiple languages.", "startOffset": 114, "endOffset": 130}, {"referenceID": 14, "context": "We base our techniques of cross-lingual event linking on an online system for detection of world events, called Event Registry [16, 15].", "startOffset": 127, "endOffset": 135}, {"referenceID": 13, "context": "We base our techniques of cross-lingual event linking on an online system for detection of world events, called Event Registry [16, 15].", "startOffset": 127, "endOffset": 135}, {"referenceID": 34, "context": "The collection of the news articles is performed using the Newsfeed service [36].", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "The details are reported in [19] and [40].", "startOffset": 28, "endOffset": 32}, {"referenceID": 38, "context": "The details are reported in [19] and [40].", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "Details of the clustering approach are described in [1].", "startOffset": 52, "endOffset": 55}, {"referenceID": 21, "context": "The most obvious way to compare documents written in different languages is to use machine translation and perform monolingual similarity, see [23, 25] for several variations of translation based approaches.", "startOffset": 143, "endOffset": 151}, {"referenceID": 23, "context": "The most obvious way to compare documents written in different languages is to use machine translation and perform monolingual similarity, see [23, 25] for several variations of translation based approaches.", "startOffset": 143, "endOffset": 151}, {"referenceID": 12, "context": "One can use free tools such as Moses [14] or translation services, such as Google Translate (https://translate.", "startOffset": 37, "endOffset": 41}, {"referenceID": 23, "context": "Closely related are works Cross-Lingual Vector Space Model (CL-VSM) [25] and the approach presented in [27] which both compare documents by using dictionaries, which in both cases are EuroVoc dictionaries [29].", "startOffset": 68, "endOffset": 72}, {"referenceID": 25, "context": "Closely related are works Cross-Lingual Vector Space Model (CL-VSM) [25] and the approach presented in [27] which both compare documents by using dictionaries, which in both cases are EuroVoc dictionaries [29].", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "Closely related are works Cross-Lingual Vector Space Model (CL-VSM) [25] and the approach presented in [27] which both compare documents by using dictionaries, which in both cases are EuroVoc dictionaries [29].", "startOffset": 205, "endOffset": 209}, {"referenceID": 22, "context": "The models include: Joint Probabilistic Latent Semantic Analysis (JPLSA) [24], Coupled Probabilistic LSA (CPLSA) [24], Probabilistic Cross-Lingual LSA (PCLLSA) [39] and Polylingual Topic Models (PLTM) [20] which is a Bayesian version of PCLLSA.", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "The models include: Joint Probabilistic Latent Semantic Analysis (JPLSA) [24], Coupled Probabilistic LSA (CPLSA) [24], Probabilistic Cross-Lingual LSA (PCLLSA) [39] and Polylingual Topic Models (PLTM) [20] which is a Bayesian version of PCLLSA.", "startOffset": 113, "endOffset": 117}, {"referenceID": 37, "context": "The models include: Joint Probabilistic Latent Semantic Analysis (JPLSA) [24], Coupled Probabilistic LSA (CPLSA) [24], Probabilistic Cross-Lingual LSA (PCLLSA) [39] and Polylingual Topic Models (PLTM) [20] which is a Bayesian version of PCLLSA.", "startOffset": 160, "endOffset": 164}, {"referenceID": 18, "context": "The models include: Joint Probabilistic Latent Semantic Analysis (JPLSA) [24], Coupled Probabilistic LSA (CPLSA) [24], Probabilistic Cross-Lingual LSA (PCLLSA) [39] and Polylingual Topic Models (PLTM) [20] which is a Bayesian version of PCLLSA.", "startOffset": 201, "endOffset": 205}, {"referenceID": 36, "context": "The models include: Non-negative matrix factorization based [38], Cross-Lingual Latent Semantic Indexing CLLSI [4, 23], Canonical Correlation Analysis (CCA) [11], Oriented Principal Component Analysis (OPCA) [24].", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "The models include: Non-negative matrix factorization based [38], Cross-Lingual Latent Semantic Indexing CLLSI [4, 23], Canonical Correlation Analysis (CCA) [11], Oriented Principal Component Analysis (OPCA) [24].", "startOffset": 111, "endOffset": 118}, {"referenceID": 21, "context": "The models include: Non-negative matrix factorization based [38], Cross-Lingual Latent Semantic Indexing CLLSI [4, 23], Canonical Correlation Analysis (CCA) [11], Oriented Principal Component Analysis (OPCA) [24].", "startOffset": 111, "endOffset": 118}, {"referenceID": 9, "context": "The models include: Non-negative matrix factorization based [38], Cross-Lingual Latent Semantic Indexing CLLSI [4, 23], Canonical Correlation Analysis (CCA) [11], Oriented Principal Component Analysis (OPCA) [24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 22, "context": "The models include: Non-negative matrix factorization based [38], Cross-Lingual Latent Semantic Indexing CLLSI [4, 23], Canonical Correlation Analysis (CCA) [11], Oriented Principal Component Analysis (OPCA) [24].", "startOffset": 208, "endOffset": 212}, {"referenceID": 36, "context": "For our setting, the method in [38] has a prohibitively high computational cost when building models (it uses dense matrices whose dimensions are a product of the training set size and the vocabulary size).", "startOffset": 31, "endOffset": 35}, {"referenceID": 24, "context": "Analysis (CL-ESA) [26], which uses Wikipedia (as do we in the current work) to compare documents.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "We find that the scalability argument does not apply in our case: based on advances in numerical linear algebra we can solve large CL-LSI problems (millions of documents as opposed to the 10,000 document limit reported in [26]).", "startOffset": 222, "endOffset": 226}, {"referenceID": 23, "context": "For example, the Cross-Language Character n-Gram Model (CL-CNG) [25] represents documents as bags of character n-grams.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "Another approach is to use language dependent keyword lists based on cognate words [27].", "startOffset": 83, "endOffset": 87}, {"referenceID": 25, "context": "The only service that we found, which provides cross-lingual cluster linking, is the European Media Monitor (EMM) [27, 35].", "startOffset": 114, "endOffset": 122}, {"referenceID": 33, "context": "The only service that we found, which provides cross-lingual cluster linking, is the European Media Monitor (EMM) [27, 35].", "startOffset": 114, "endOffset": 122}, {"referenceID": 26, "context": "These descriptors are topics, such as air transport, EC agreement, competition and pollution control into which articles are automatically categorized [28].", "startOffset": 151, "endOffset": 155}, {"referenceID": 15, "context": "A system, which is significantly different but worth mentioning, is the GDELT project [17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 31, "context": "Comparison of documents (or other types of text snippets) in a monolingual setting is a well-studied problem in the field of information retrieval [33].", "startOffset": 147, "endOffset": 151}, {"referenceID": 31, "context": "The standard vector space model [33] represents documents as vectors, where each term corresponds to a word or a phrase in a fixed vocabulary.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "CCA can be used to find correlated patterns for a pair of languages, whereas the extended method optimizes a Sum of Squared Correlations (SSCOR) between several language pairs, which was introduced in [12].", "startOffset": 201, "endOffset": 205}, {"referenceID": 8, "context": "We then run the k-means algorithm [10] and obtain a centroid matrix C \u2208 RN\u00d7k, where the k columns represent centroid vectors.", "startOffset": 34, "endOffset": 38}, {"referenceID": 3, "context": "The second approach we consider is Cross-Lingual Latent Semantic Indexing (CL-LSI) [4] which is a variant of LSI [3] for more than one language.", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "The second approach we consider is Cross-Lingual Latent Semantic Indexing (CL-LSI) [4] which is a variant of LSI [3] for more than one language.", "startOffset": 113, "endOffset": 116}, {"referenceID": 6, "context": "Instead, we use a randomized version of the SVD [8] that can be viewed as a block Lanczos method.", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "Canonical Correlation Analysis (CCA) [11] is a dimensionality reduction technique similar to Principal Component Analysis (PCA) [22], with the additional assumption that the data consists of feature vectors that arose from two sources (two views) that share some information.", "startOffset": 37, "endOffset": 41}, {"referenceID": 20, "context": "Canonical Correlation Analysis (CCA) [11] is a dimensionality reduction technique similar to Principal Component Analysis (PCA) [22], with the additional assumption that the data consists of feature vectors that arose from two sources (two views) that share some information.", "startOffset": 128, "endOffset": 132}, {"referenceID": 4, "context": "Examples include: bilingual document collection [5] and collection of images and captions [9].", "startOffset": 48, "endOffset": 51}, {"referenceID": 7, "context": "Examples include: bilingual document collection [5] and collection of images and captions [9].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "If the matrices are not invertible, one can use a regularization technique by replacing Ci,i with (1\u2212 \u03ba)Ci,i + \u03baI, where \u03ba \u2208 [0, 1] is the regularization coefficient and I is the identity matrix.", "startOffset": 125, "endOffset": 131}, {"referenceID": 10, "context": "The extension we consider is based on a generalization of CCA to more than two views, introduced in [12], namely the Sum of Squared Correlations SSCOR, which we will state formally later in this section.", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "The approach is based on the sum of squares of correlations formulation by Kettenring [12], where we consider only correlations between pairs (Y1, Yi), i > 1 due to the hub language problem characteristic.", "startOffset": 86, "endOffset": 90}, {"referenceID": 1, "context": "The technique is related to Generalization of Canonical Correlation Analysis (GCCA) by Carroll [2], where an unknown group configuration variable is defined and the objective is to maximize the sum of squared correlations between the group variable and the others.", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "The complexity of our method is O(k), where k is the reduced dimension from the LSI preprocessing step, whereas solving the GCCA method scales as O(s), where s is the number of samples (see [6]).", "startOffset": 190, "endOffset": 193}, {"referenceID": 32, "context": "The classification algorithm that we used to train a model was a linear Support Vector Machine (SVM) method [34].", "startOffset": 108, "endOffset": 112}, {"referenceID": 39, "context": "Articles that are imported into Event Registry are annotated by disambiguating mentioned entities and keywords to the corresponding Wikipedia pages [41].", "startOffset": 148, "endOffset": 152}, {"referenceID": 16, "context": "\u2022 entityJaccardSim is Jaccard similarity coefficient [18] between sets of entities from clusters ci and cj .", "startOffset": 53, "endOffset": 57}, {"referenceID": 27, "context": "Wikipedia is a large source of multilingual data that is especially important for the languages for which no translation tools, multilingual dictionaries as Eurovoc [29], or strongly aligned multilingual corpora as Europarl [13] are available.", "startOffset": 165, "endOffset": 169}, {"referenceID": 11, "context": "Wikipedia is a large source of multilingual data that is especially important for the languages for which no translation tools, multilingual dictionaries as Eurovoc [29], or strongly aligned multilingual corpora as Europarl [13] are available.", "startOffset": 224, "endOffset": 228}, {"referenceID": 31, "context": "We represent the documents as normalized TFIDF [33] weighted vectors.", "startOffset": 47, "endOffset": 51}, {"referenceID": 35, "context": "We computed the Average (over language pairs) Mean Reciprocal Rank (AMRR) [37] performance of the different approaches on the Wikipedia data by holding out 15, 000 aligned test documents and using 300, 000 aligned documents as the training set.", "startOffset": 74, "endOffset": 78}], "year": 2015, "abstractText": "In today\u2019s world, we follow news which is distributed globally. Significant events are reported by different sources and in different languages. In this work, we address the problem of tracking of events in a large multilingual stream. Within a recently developed system Event Registry [16, 15] we examine two aspects of this problem: how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event. Taking a multilingual stream and clusters of articles from each language, we compare different cross-lingual document similarity measures based on Wikipedia. This allows us to compute the similarity of any two articles regardless of language. Building on previous work, we show there are methods which scale well and can compute a meaningful similarity between articles from languages with little or no direct overlap in the training data. Using this capability, we then propose an approach to link clusters of articles across languages which represent the same event. We provide an extensive evaluation of the system as a whole, as well as an evaluation of the quality and robustness of the similarity measure and the linking algorithm.", "creator": "LaTeX with hyperref package"}}}