{"id": "1512.07162", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2015", "title": "Heuristic algorithms for finding distribution reducts in probabilistic rough set model", "abstract": "anomaly detection is concern for the immensely important things in rough set theory. heuristic data composition graphs have traditionally combined to solve the bench reduction problem. it may generally supposed that fitness functions play a key role in developing heuristic attribute killing algorithms. the natural relation fitness maps can guarantee the improvement surrounding heuristic function reduction algorithms. in probabilistic rough decisions research, assignment reducts can ensure identifying decision rules derived from the reducts partly compatible with weights retrieved from comparative valuation class lists. however, there are few experts on developing composite attribute elimination algorithms for finding distribution reducts. observation is solely due to the fact that there are linear monotonic gene combinations that are generated alongside design heuristic relation reduction algorithms in probabilistic rough choice systems. these highest objective of lab paper is to prepare heuristic number winning sequences for checking individual reducts in linear rough fitting design. for one thing, alternatively monotonic recovery methods may constructed, revealing which equivalence components of distribution functional can then assembled. for another, two modified monotonic bench filters recently proposed as evaluate the significance valued attributes more effectively. on this basis, two heuristic attribute reduction variants for relating trait together - developed based on addition - algorithm alongside underlying deletion method. in 2006, the listing related fitness functions becomes the necessity beside the proposed heuristic attribute suppression method. results of experimental analysis are noted. quantify their effectiveness of alternative proposed item functions encoding quantity reducts.", "histories": [["v1", "Tue, 22 Dec 2015 17:17:45 GMT  (1316kb)", "http://arxiv.org/abs/1512.07162v1", "44 pages, 24 figures"]], "COMMENTS": "44 pages, 24 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["xi'ao ma", "guoyin wang", "hong yu"], "accepted": false, "id": "1512.07162"}, "pdf": {"name": "1512.07162.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Xi\u2019ao Ma", "Guoyin Wang", "Hong Yu"], "emails": ["maxiao73559@163.com", "wanggy@ieee.org", "hongyu.cqupt@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n07 16\n2v 1\n[ cs\n.A I]\n2 2\nAttribute reduction is one of the most important topics in rough set theory. Heuristic attribute reduction algorithms have been presented to solve the attribute reduction problem. It is generally known that fitness functions play a key role in developing heuristic attribute reduction algorithms. The monotonicity of fitness functions can guarantee the validity of heuristic attribute reduction algorithms. In probabilistic rough set model, distribution reducts can ensure the decision rules derived from the reducts are compatible with those derived from the original decision table. However, there are few studies on developing heuristic attribute reduction algorithms for finding distribution reducts. This is partly due to the fact that there are no monotonic fitness functions that are used to design heuristic attribute reduction algorithms in probabilistic rough set model. The main objective of this paper is to develop heuristic attribute reduction algorithms for finding distribution reducts in probabilistic rough set model. For one thing, two monotonic fitness functions are constructed, from which equivalence definitions of distribution reducts can be obtained. For another, two modified monotonic fitness functions are proposed to evaluate the significance of attributes more effectively. On this basis, two heuristic attribute reduction algorithms for finding distribution reducts are developed based on addition-deletion method and deletion method. In particular, the monotonicity of fitness functions guarantees the rationality of the proposed heuristic attribute reduction algorithms. Results of experimental analysis are included to quantify the effectiveness of the proposed fitness functions and distribution reducts.\nKeywords: Attribute reduction; heuristic attribute reduction algorithm; the significance of attribute; probabilistic rough set model; distribution reduct.\n\u2217Corresponding author. Tel: +86 13588030950 Email addresses: maxiao73559@163.com (Xi\u2019ao Ma), wanggy@ieee.org (Guoyin Wang),\nhongyu.cqupt@gmail.com (Hong Yu)\nPreprint submitted to Elsevier December 23, 2015"}, {"heading": "1. Introduction", "text": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44]. In rough set theory, attribute reduction is a popular task to select the essential attributes that preserve or improve a certain classification property as the entire set of available attributes. Hence, an attribute reduct can be defined as a minimal attribute set that can preserve or improve specific classification criterion of a given information system. Attribute reduction is often helpful to reduce the computational cost, save storage space, improve learning performance and prevent over-fitting [48]. Studies on attribute reduction in rough set theory can be mainly divided into two categories. The first category concentrates on the study of definition of attribute reduct. The second category focuses on the study of attribute reduction algorithm.\nFor definition of attribute reduct, one mainly emphasizes on selecting what kinds of properties of a given information system to keep unchanged or to improve [15, 33, 34]. For example, Pawlak [38] defined a relative reduct that keeps the quality of classification or classification positive region unchanged. Miao et al. [31] constructed the mutual information based reducts to extract relevant attribute sets that preserve the mutual information of a given decision table. Kryszkiewicz [18, 19] investigated and compared the relationships among possible reduct, approximate reduct, \u00b5-reduct, \u00b5-decision reduct and generalized decision reduct. Zhang et al. [58] introduced the maximum distribution reduct, which preserves all maximum decision rules in a decision table. Deng et al. [7] proposed the notions of conditional knowledge granularity to reflect the relationship between conditional attributes and decision attribute, and defined an attribute reduct based on conditional knowledge granularity. Jiang et al. [17] proposed a new model of relative decision entropy by combining roughness with the degree of dependency, and used it as the reduction criterion.\nFor attribute reduction algorithm, one mainly focuses on designing the effective reduct construction methods for finding a specific type of reduct [45, 47, 62]. According to the different methods, research efforts on attribute reduction algorithm can be divided into three categories. The first class of methods is based on discernibility matrix. Miao et al. [32] discussed the structures of discernibility matrices for three different reducts, including region preservation reduct, decision preservation reduct and relationship preservation reduct. Yao and Zhao [55] introduced a reduct construction method based on discernibility matrix simplification. The second class of methods is based on heuristics. Qian et al. [42] proposed the concept of positive approximation for accelerating heuristic attribute reduction algorithms. Parthalain et al. [35, 36] proposed a distance measure-based attribute reduction algorithm by considering the proximity of objects in the boundary region to those in the lower approximation. The third class of methods is based on stochastic optimization. Chen et al. [4] proposed a novel attribute reduction algorithm based on ant colony optimization for finding\na reduct that keeps the mutual information unchanged. Ye et al. [57] introduced a novel fitness function for designing the particle swarm optimization-based and genetic-based attribute reduction algorithms.\nIn particular, the studies mentioned above are mainly focused on attribute reduction in classical rough set model. Relative to the classical rough set model, probabilistic rough set model allows certain acceptable levels of errors by using the threshold parameters [8, 26, 28, 49]. Hence, the probabilistic rough set model can effectively deal with data sets which have the noisy and uncertain data. By setting the different threshold parameters, one can derive many existing probabilistic rough set models, such as 0.5 probabilistic rough set model [39], decision-theoretic rough set model [52], variable precision rough set model [63], Bayesian rough set model [46] and game rough set model [12]. Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63]. For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43]. Few attempts have been made to study the heuristic attribute reduction algorithms in probabilistic rough set model. This is partly because the attribute reduction in probabilistic rough set model becomes more complex after introducing the threshold parameters.\nThis paper concentrates on constructing heuristic attribute reduction algorithms for finding low and upper distribution reducts in probabilistic rough set model, which are defined by Mi et al. in [30]. Generally speaking, the most common heuristic attribute reduction algorithms are the addition-deletion method and the deletion method [56]. The addition-deletion method starts from an empty set or the core, then it adds attributes one by one on the basis of the significance of attributes until a stopping criterion is reached. On the contrary, the deletion method starts with a set containing all the attributes, then it delete the attributes one by one according to the significance of attributes. In fact, the heuristic attribute reduction algorithms mainly included two aspects: the stopping criterion and the significance measures of attributes. The stopping criterion is implemented by checking the jointly sufficient condition in the definition of attribute reduct, and the significance measures of attributes is used to rank the attributes. As we known, the fitness functions play a non-trivial role in designing stopping criteria and constructing significance measures of attributes. Furthermore, the monotonicity of fitness functions is very important to guarantee the validity of heuristic attribute reduction algorithms. However, very little work have considered the monotonicity of fitness functions in attribute reduction of probabilistic rough set model. In this paper, we first construct two monotonic fitness functions in probabilistic rough set model. The equivalent definitions of the low and upper distribution reducts are obtained based on the constructed monotonic fitness functions. Then we use them to design the stopping criterion of heuristic attribute reduction algorithms. The monotonicity of fitness functions guarantees the validity of heuristic attribute reduction algorithms. In addition, we propose two modified monotonic fitness functions\nto evaluate the significance of attributes more effectively. On this basis, we develop two heuristic attribute reduction algorithms for finding the low and upper distribution reducts based on addition-deletion method and deletion method. In the end, some experimental analyses are covered to validate the effectiveness of the proposed fitness functions and distribution reducts.\nThe remaining sections of this paper are organized as follows. Section 2 briefly reviews some basic notions related to the definition of distribution reducts and heuristic attribute reduction algorithms. Section 3 develops two heuristic attribute reduction algorithms for finding the low and upper distribution reducts in probabilistic rough set model by constructing the monotonic fitness functions. Several experiments are given to illustrate the effectiveness of the constructed fitness functions and distribution reducts in Section 4. Section 5 concludes and offers suggestions for future research."}, {"heading": "2. Preliminary knowledge", "text": "In this section, we recall the basic notions related to attribute reduction on rough set theory. [25, 52].\n2.1. The definition of distribution reducts\nAn information system is a four-tuple IS = (U,A, V, f), where U is a finite nonempty set of objects called universe, A is a nonempty finite set of attributes, V = \u22c3\na\u2208A Va, where Va is a nonempty set of values of attribute a \u2208 A, called the domain of a, f : U \u00d7 A \u2192 V is a mapping that maps an object in U to exactly one value in Va such that \u2200a \u2208 A, x \u2208 U , f(x, a) \u2208 Va. For brevity, IS = (U,A, V, f) can be written as IS = (U,A).\nFor any subset of attributes R \u2286 A, an indiscernibility relation IND(R) on U is defined as:\nIND(R) = {(x, y) \u2208 U \u00d7 U |\u2200a \u2208 R, f(x, a) = f(y, a)}.\nIt can be easily shown that IND(R) is an equivalence relation on U . For R \u2286 A, the equivalence relation IND(R) partitions U into some equivalence classes denoted by U/IND(R) = {[x]R|u \u2208 U}, for simplicity, U/IND(R) will be replaced by U/R, where [x]R is an equivalence class determined by x with respect to R, i.e., [x]R = {y \u2208 U |(x, y) \u2208 IND(R)}.\nTo describe a concept, rough set theory introduces a pair of lower approximation and upper approximation as follows.\nDefinition 1. Given an information system IS = (U,A), R \u2286 A and X \u2286 U , the lower approximation and upper approximation of X with respect to R are defined as:\napr R (X) = {x \u2208 U |[x]R \u2286 X}, aprR(X) = {x \u2208 U |[x]R \u2229X 6= \u2205}.\nThe lower approximation of X is a set of objects that belong to X with certainty, and the upper approximation of X is a set of objects that possibly belong to X .\nGiven an information system IS = (U,A), P,Q \u2286 A, one can define a partial relation \u227a on 2A as follows [6]:\nP\u227aQ \u21d4 \u2200x \u2208 U, [x]P \u2286 [x]Q.\nIf P\u227aQ, Q is said to be coarser than P (or P is finer than Q). If P\u227aQ and P 6= Q, Q is said to be strictly coarser than P or P is strictly finer than Q, denoted by P \u227a Q. In fact, P \u227a Q \u21d4 \u2200x \u2208 U , we have that [x]P \u2286 [x]Q and there exists y \u2208 U such that [y]P \u2282 [y]Q.\nA decision table is a four-tuple DT = (U,C \u222aD,V, f), where C is condition attribute set, D is decision attribute set, and C \u2229 D = \u2205, V is the union of attribute domain, V = VC \u222a VD = {Va|a \u2208 C} \u222a {Vd|d \u2208 D}. DT = (U,C \u222a D,V, f) can be written as DT = (U,C \u222aD) more simply.\nDefinition 2. [22] Given a decision table DT = (U,C\u222aD), R \u2286 C and U/D = {Y1, Y2, . . . , YM} is a classification of the universe U . The positive region of U/D with respect to R is defined as follows:\nPOSR(D) = \u22c3\n1\u2264i\u2264M\napr R (Yi).\nPawlak rough set model does not allow any tolerance of errors. The probabilistic rough set model, which is a main extension of Pawlak rough set model, shows certain levels of tolerance for errors.\nDefinition 3. Given an information system IS = (U,A), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1, R \u2286 A and X \u2286 U . The probabilistic lower approximation and probabilistic upper approximation of X with respect to R are defined as follows:\napr(\u03b1,\u03b2) R (X) = {x \u2208 U |p(X |[x]R) \u2265 \u03b1}, apr (\u03b1,\u03b2) R (X) = {x \u2208 U |p(X |[x]R) > \u03b2},\nwhere p(X |[x]R) = |[x] R \u2229X|\n|[x] R | .\nDefinition 4. [22] Given a decision table DT = (U,C \u222a D), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1, R \u2286 C and U/D = {Y1, Y2, . . . , YM} is a classification of the universe U . The probabilistic positive region of U/D with respect to R is defined as follows:\nPOS (\u03b1,\u03b2) R (D) =\n\u22c3\n1\u2264i\u2264M\napr(\u03b1,\u03b2) R (Yi).\nAttribute reduction is one of the most important topics in rough set theory. An attribute reduct is defined as a subset of attributes that are jointly sufficient and individually necessary for preserving or improving a particular property of the given information system [54]. A general definition of an attribute reduct is given as follows.\nDefinition 5. [60] Given an information system IS = (U,A), R \u2286 A and consider a certain property P, which can be represented by an evaluation function e : 2A \u2192 (L,\u227a), of IS. An attribute set R is called a reduct of IS if it satisfies the following two conditions:\n(1) Jointly sufficient condition: e(A)\u227ae(R),\n(2) Individually necessary condition: for any R\u2032 \u2282 R, \u00ac(e(A)\u227ae(R)).\nAn evaluation or fitness function, e : 2A \u2192 (L,\u227a), maps an attribute set to an element of a poset L equipped with the partial order relation \u227a i.e., \u227a is reflexive, anti-symmetric and transitive. For a certain property P, various fitness functions can be used to evaluate the degree of satisfiability of the property by an attribute set. Generally, the fitness function is not unique. The jointly sufficient condition guarantees that the evaluation of the reduct R with respect to e is the same or superior to e(A), and it has e(R) = e(A) in many cases. The individually necessary condition guarantees that the reduct is minimal, namely, there is no redundant or superfluous attribute in the reduct R.\nAccording to the different properties P of an information system, the different reducts can be defined. There are a huge amount of known properties, such as a description of an object relation [37], partitions of an information system [60], a classification of a set of concepts [38], where the classification of a set of concepts is the most common property in rough set theory.\nIn classical rough set model, the classification of a set of concepts can be evaluated by using the positive region of the classification (Definition 2). In probabilistic rough set model, the classification of a set of concepts can be evaluated by using the probabilistic positive region of the classification (Definition 4). However, the decision rules derived from the reduct preserving probabilistic positive region maybe in conflict with those derived from the original decision table because of non-monotonicity of probabilistic positive region with respect to the set inclusion of attribute sets [30].\nTo derive the conflict free decision rules, Mi et al. [30] presented the concepts of distribution reducts based on variable precision rough set model which is a typical probabilistic rough set model.\nDefinition 6. Given a decision table DT = (U,C \u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1, R \u2286 C and U/D = {Y1, Y2, . . . , YM} is a classification of the universe U . The (\u03b1, \u03b2) lower and upper approximation distribution functions with respect to R are defined as:\napr(\u03b1,\u03b2) R = (apr(\u03b1,\u03b2) R (Y1), apr (\u03b1,\u03b2) R (Y2), \u00b7 \u00b7 \u00b7, apr (\u03b1,\u03b2) R (YM )), apr (\u03b1,\u03b2) R = (apr (\u03b1,\u03b2) R (Y1), apr (\u03b1,\u03b2) R (Y2), \u00b7 \u00b7 \u00b7, apr (\u03b1,\u03b2) R (YM )).\nThe (\u03b1, \u03b2) lower or upper approximation distribution functions can be seen as fitness functions for evaluating the classification of a set of concepts in probabilistic rough set model. By the (\u03b1, \u03b2) lower and upper approximation distribution functions, the (\u03b1, \u03b2) lower and upper approximation distribution reducts based on probabilistic rough set model are defined as follows.\nDefinition 7. [30] Given a decision table DT = (U,C \u222a D), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1 and R \u2286 C, we have\n(1) If apr (\u03b1,\u03b2) R = apr (\u03b1,\u03b2) C , R is referred to as an (\u03b1, \u03b2) lower distribution\nconsistent set of DT ; if apr (\u03b1,\u03b2) R = apr (\u03b1,\u03b2) C and apr (\u03b1,\u03b2) R\u2032 6= apr (\u03b1,\u03b2) C for all R\u2032 \u2282 R, then R is referred to as an (\u03b1, \u03b2) lower distribution reduct of DT .\n(2) If apr (\u03b1,\u03b2) R = apr (\u03b1,\u03b2) C , R is referred to as an (\u03b1, \u03b2) upper distribution\nconsistent set of DT ; if apr (\u03b1,\u03b2) R = apr (\u03b1,\u03b2) C and apr (\u03b1,\u03b2) R\u2032 6= apr (\u03b1,\u03b2) C for all R\u2032\u2282R, then R is referred to as an (\u03b1, \u03b2) upper distribution reduct of DT .\nAn (\u03b1, \u03b2) lower (upper) distribution reduct is a minimal subset of attribute set that preserves the (\u03b1, \u03b2) lower (upper) approximations of all decision classes. For the sake of the simplicity, the (\u03b1, \u03b2) lower and upper distribution reducts are collectively called distribution reducts in the rest of this paper.\nIt is important to note that the monotonicity property of the fitness function used in the definition of attribute reduct with respect to the set inclusion of attribute sets should receive enough attention when we design the heuristic attribute reduction algorithms. If the fitness function is monotonic regarding the set inclusion of attribute sets, individually necessary condition only need to consider the subsets R \u2212 {a} for all a \u2208 R to guarantee a reduct R is minimal. If the fitness function is not monotonic regarding the set inclusion of attribute sets, individually necessary condition must consider all subsets of a reduct R to make sure it is a minimal set [14, 61].\nIn Definition 7, individually necessary conditions must consider all subsets of the reduct R because the (\u03b1, \u03b2) lower and upper approximation distribution functions are not monotonic regarding the set inclusion of attribute sets, which will complicate the algorithm design.\n2.2. Typical heuristic attribute reduction algorithms\nSo far, Yao et al. [56] have summarized three groups of heuristic attribute reduction algorithms based on the addition-deletion method, the deletion method and the addition method, where the addition-deletion method based algorithm and the deletion method based algorithm are two most widely used heuristic attribute reduction algorithms by the rough set community. Hence, we mainly discuss the first two methods based heuristic attribute reduction algorithms in this subsection.\nThe addition-deletion method based algorithm and the deletion method based algorithm are displayed in Algorithm 1 and Algorithm 2, respectively.\nAlgorithm 1 starts with an empty set or the core, and consequently adds attributes to the subset of selected attributes until a candidate reduct satisfies the jointly sufficient condition in the definition of attribute reduct. Each selected attribute maximizes the increment of fitness values of the current attribute subset. One needs the deleting process to delete the superfluous attributes in the candidate reduct one by one after the addition process because the addition process may add the superfluous attributes.\nAlgorithm 1 The addition-deletion method for computing a reduct\nInput: A decision table DT = (U,C \u222aD), threshold (\u03b1, \u03b2) Output: A reduct R Method: Addition-deletion method\n1: // Addition 2: Let R = \u2205, CA = C 3: while R is not jointly sufficient and CA 6= \u2205 do 4: Compute fitness values of all the attributes in CA regarding the property P using a fitness function \u03c3 5: Select an attribute c according to its fitness, let CA = CA\u2212 {c} 6: Let R = R \u222a {c} 7: end while 8: // Deletion 9: Let CD = R 10: while CD 6= \u2205 do 11: Compute fitness values of all the attributes in CD regarding the property P using a fitness function \u03b4 12: Select an attribute a according to its fitness, let CD = CD \u2212 {a} 13: if R\u2212 {a} is jointly sufficient then 14: R = R\u2212 {a} 15: end if 16: end while 17: Return R\nAlgorithm 2 The deletion method for computing a reduct\nInput: A decision table DT = (U,C \u222aD), threshold (\u03b1, \u03b2) Output: A reduct R Method: Deletion method\n1: // Deletion 2: Let R = C, CD = C 3: while CD 6= \u2205 do 4: Compute fitness values of all the attributes in CD regarding the property P using a fitness function \u03b4 5: Select an attribute a according to its fitness, let CD = CD \u2212 {a} 6: if R\u2212 {a} is jointly sufficient then 7: R = R\u2212 {a} 8: end if 9: end while\n10: Return R\nAlgorithm 2 takes the entire condition attributes as a candidate reduct, then selects the attributes for deleting one by one according to the fitness values. If\nthe subset of the remaining attributes satisfies the jointly sufficient condition in the definition of attribute reduct after deleting the selected attribute, then the attribute is the superfluous attribute and can be deleted. A reduct is obtained if and only if each attribute has been checked once.\nAlgorithm 1 and Algorithm 2 mainly consist of two key steps: checking the jointly sufficient condition and evaluating the significance of attributes. Checking the jointly sufficient condition is to ensure that the candidate reduct R meet jointly sufficient condition in the definition of attribute reduct. Evaluating the significance of attributes is to sort attributes and provide the heuristic information for searching a reduct, and the step can be implemented by designing the effective fitness functions. Different fitness functions may get the different orders of attributes, that may obtain the different reducts.\nIt is worth pointing out that the monotonicity of the fitness function in Definition 5 with respect to the set inclusion of attribute sets is very important for the completeness of Algorithm 1 and Algorithm 2 [29]. If the fitness function is monotonic regarding the set inclusion of attribute sets, Algorithm 1 and Algorithm 2 must obtain a reduct, namely, Algorithm 1 and Algorithm 2 are complete. If the fitness function is not monotonic regarding the set inclusion of attribute sets, Algorithm 1 and Algorithm 2 may obtain a super reduct that includes redundancy attributes, namely, Algorithm 1 and Algorithm 2 are incomplete. Moreover, the fitness functions for evaluating the significance of attributes should also satisfy the monotonicity with respect to the set inclusion of attribute sets and provide enough precision to sort the attributes more effectively [17, 20]."}, {"heading": "3. Heuristic algorithms to find distribution reducts in probabilistic rough set model", "text": "Checking the jointly sufficient condition and evaluating the significance of attributes are two key steps in heuristic attribute reduction algorithms based on the addition-deletion strategy and the deletion strategy. Furthermore, the monotonicity of the fitness functions for checking the jointly sufficient condition and evaluating the significance of attributes is very important for the validity of the heuristic attribute reduction algorithms. To obtain the distribution reducts with heuristic attribute reduction algorithms, in this section, we first construct two monotonic fitness functions. Then we give the equivalent definition of distribution reducts based on the monotonic fitness functions constructed. After that, we further proposed two significance measures of attributes by multiplying measures of granularity of partitions by the monotonic fitness functions constructed. Moreover, the core and core computation algorithm for distribution reducts are also presented. On this basis, two heuristic attribute reduction algorithms to find distribution reducts are developed based on addition-deletion method and deletion method. Finally, an illustrative example to the heuristic attribute reduction algorithms proposed is provided step by step.\n3.1. The equivalent definition of distribution reducts with monotonic fitness functions\nFor a certain property in the definition of attribution reduct, the different fitness functions can be used as its indicator. The monotonicity of the fitness functions is very important to guarantee the completeness of heuristic attribute reduction algorithms. To develop the complete heuristic attribute reduction algorithms to obtain distribution reducts, we construct two monotonic fitness functions in this subsection. The equivalent definition of distribution reducts are further given based on the monotonic fitness functions constructed.\nNow, let us first give two monotonic fitness functions as follow.\nDefinition 8. Given a decision table DT = (U,C \u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1, R \u2286 C and U/D = {Y1, Y2, . . . , YM} is a classification of the universe U , we denote\n\u03b7 (\u03b1,\u03b2) R =\n\u2211\nYi\u2208U/D |apr R (apr (\u03b1,\u03b2) C (Yi))|\n|U ||U/D| ,\n\u00b5 (\u03b1,\u03b2) R =\n\u2211\nYi\u2208U/D |aprR(apr (\u03b1,\u03b2) C (Yi))|\n|U ||U/D| .\nIf R = \u2205, then define \u03b7 (\u03b1,\u03b2) R = 0 and \u00b5 (\u03b1,\u03b2) R = 1. Moreover, \u03b7 (\u03b1,\u03b2) R and \u00b5 (\u03b1,\u03b2) R\nare denoted by \u03b7 and \u00b5 respectively if there is no confusion arisen.\nIt is easy to see from above that \u03b7 (\u03b1,\u03b2) R and \u00b5 (\u03b1,\u03b2) R are defined by using the lower approximations of all elements in apr (\u03b1,\u03b2) C with respect R and upper approximations of all elements in apr (\u03b1,\u03b2) C with respect to R, respectively. apr R (apr (\u03b1,\u03b2) C (Yi)) represents the change of certain knowledge with respect to apr (\u03b1,\u03b2) C (Yi) after removing attributes C \u2212 R from the decision table, and aprR(apr (\u03b1,\u03b2) C (Yi)) represents the change of relevant knowledge with respect to apr (\u03b1,\u03b2) C (Yi) after removing attributes C\u2212R from the decision table. Therefore, \u03b7 (\u03b1,\u03b2) R represents the change of certain knowledge with respect to apr (\u03b1,\u03b2) C . \u00b5 (\u03b1,\u03b2) R represents the change of relevant knowledge with respect to apr (\u03b1,\u03b2) C . Hence, we can use \u03b7 (\u03b1,\u03b2) R and \u00b5 (\u03b1,\u03b2) R to compute the (\u03b1, \u03b2) lower and upper distribution reducts, respectively.\nTheorem 1. Given a decision table DT = (U,C \u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1 and P,Q \u2286 C, we have\n(1) P\u227aQ \u21d2 \u03b7 (\u03b1,\u03b2) P \u2265 \u03b7 (\u03b1,\u03b2) Q ,\n(2) P\u227aQ \u21d2 \u00b5 (\u03b1,\u03b2) P \u2264 \u00b5 (\u03b1,\u03b2) Q .\nProof. (1) Suppose P\u227aQ. In terms of the definition of \u227a, we have [x]P \u2286 [x]Q. Let U/D = {Y1, Y2, . . . , YM} and Yi \u2208 U/D, 1 \u2264 i \u2264 M .\nOn the one hand, for \u2200x \u2208 apr Q (apr (\u03b1,\u03b2) C (Yi)), one can obtain [x]Q \u2286\napr (\u03b1,\u03b2) C (Yi), hence [x]P\u2286apr (\u03b1,\u03b2) C (Yi), then we obtain that x\u2208aprP (apr (\u03b1,\u03b2) C (Yi)), thus apr P (apr (\u03b1,\u03b2) C (Yi)) \u2287 aprQ(apr (\u03b1,\u03b2) C (Yi)).\nIn the other hand, for \u2200x \u2208 aprP (apr (\u03b1,\u03b2) C (Yi)), we have [x]P \u2229apr (\u03b1,\u03b2) C (Yi) 6=\n\u2205. Since [x]P \u2286 [x]Q, [x]Q \u2229 apr (\u03b1,\u03b2) C (Yi) 6= \u2205, we have x \u2208 aprQ(apr (\u03b1,\u03b2) C (Yi)). It follows that aprP (apr (\u03b1,\u03b2) C (Yi)) \u2286 aprQ(apr (\u03b1,\u03b2) C (Yi)).\nAs a result, we have\n|apr P (apr(\u03b1,\u03b2) C (Yi))| \u2265 |aprQ(apr (\u03b1,\u03b2) C (Yi))|,\n|aprP (apr (\u03b1,\u03b2) C (Yi))| \u2264 |aprQ(apr (\u03b1,\u03b2) C (Yi))|.\nThus, \u2211\nYi\u2208U/D |apr\nP (apr(\u03b1,\u03b2) C (Yi))| \u2265\n\u2211\nYi\u2208U/D |apr\nQ (apr(\u03b1,\u03b2) C (Yi))|,\n\u2211\nYi\u2208U/D |aprP (apr\n(\u03b1,\u03b2) C (Yi))| \u2264\n\u2211\nYi\u2208U/D |aprQ(apr\n(\u03b1,\u03b2) C (Yi))|.\nHence,\n\u03b7 (\u03b1,\u03b2) P =\n\u2211\nYi\u2208U/D |apr P (apr (\u03b1,\u03b2) C (Yi))|\n|U ||U/D| \u2265 \u03b7\n(\u03b1,\u03b2) Q =\n\u2211\nYi\u2208U/D |apr Q (apr (\u03b1,\u03b2) C (Yi))|\n|U ||U/D| ,\n\u00b5 (\u03b1,\u03b2) P =\n\u2211\nYi\u2208U/D |apr R (apr (\u03b1,\u03b2) C (Yi))|\n|U ||U/D| \u2264 \u00b5\n(\u03b1,\u03b2) Q =\n\u2211\nYi\u2208U/D |apr R (apr (\u03b1,\u03b2) C (Yi))|\n|U ||U/D| .\nThis completes the proof. The proof of (2) is similar to that of (1).\nBy Theorem 1 we immediately get the following corollary.\nCorollary 1. Given a decision table DT = (U,C \u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1 and P,Q \u2286 C, we have\n(1) P \u2287 Q \u21d2 \u03b7 (\u03b1,\u03b2) P \u2265 \u03b7 (\u03b1,\u03b2) Q ,\n(2) P \u2287 Q \u21d2 \u00b5 (\u03b1,\u03b2) P \u2264 \u00b5 (\u03b1,\u03b2) Q .\nTheorem 1 and Corollary 1 show that the fitness function \u03b7 (\u03b1,\u03b2) R increases and the fitness function \u00b5 (\u03b1,\u03b2) R decreases as the equivalence classes become smaller through finer partitioning, which means that adding a new attribute into the existing subset of condition attributes at least does not decrease \u03b7 (\u03b1,\u03b2) R or increase \u00b5 (\u03b1,\u03b2) R , and that deleting an attribute from the existing subset of condition attributes at least does not increases \u03b7 (\u03b1,\u03b2) R or decreases \u00b5 (\u03b1,\u03b2) R . The property is very important for constructing heuristic attribute reduction algorithms. In the following, the performance of Theorem 1 is shown through an illustrative example.\nExample 1. Given a decision table DT = (U,C \u222aD) showed in Table 1, where U = {x1, x2, \u00b7 \u00b7 \u00b7 , x12}, and C = {a1, a2, a3, a4, a5, a6}. Suppose that \u03b1 = 0.60 and \u03b2 = 0.40, P,Q \u2286 C, where P = {a1, a2, a3} and Q = {a1, a2}. As we can see, P \u2287 Q, which means P\u227aQ.\nThus, \u03b7 (0.60,0.40) P > \u03b7 (0.60,0.40) Q and \u00b5 (0.60,0.40) P < \u00b5 (0.60,0.40) Q . It is clear that\n\u03b7 (\u03b1,\u03b2) R increases and \u00b5 (\u03b1,\u03b2) R decreases with R becoming finer.\nNow, let us give equivalence descriptions for each distribution consistent set\nby the fitness functions \u03b7 (\u03b1,\u03b2) R and \u00b5 (\u03b1,\u03b2) R .\nTheorem 2. Given a decision table DT = (U,C \u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1 and R \u2286 C, we have\n(1) R is an (\u03b1, \u03b2) lower distribution consistent set of DT iff \u03b7 (\u03b1,\u03b2) R = \u03b7 (\u03b1,\u03b2) C ,\n(2) R is an (\u03b1, \u03b2) upper distribution consistent set of DT iff \u00b5 (\u03b1,\u03b2) R = \u00b5 (\u03b1,\u03b2) C .\nProof. Since R \u2286 C, it is easy to verify that \u03be([x]R) = {[y]C : [y]C \u2286 [x]R} forms a partition of [x]R. Let U/D = {Y1, Y2, . . . , YM}.\n(1) \u201c \u21d2 \u201d Since R is an (\u03b1, \u03b2) lower distribution consistent set, we have\napr (\u03b1,\u03b2) R = apr (\u03b1,\u03b2) C , therefore for \u2200Yi \u2208 U/D, we obtain that apr (\u03b1,\u03b2) R (Yi) = apr (\u03b1,\u03b2) C (Yi). Hence,\n\u03b7 (\u03b1,\u03b2) R =\n\u2211\nYi\u2208U/D |apr R (apr (\u03b1,\u03b2) C (Yi))|\n|U ||U/D| =\n\u2211\nYi\u2208U/D |apr R (apr (\u03b1,\u03b2) R (Yi))|\n|U ||U/D|\n=\n\u2211\nYi\u2208U/D |apr (\u03b1,\u03b2) R (Yi)|\n|U ||U/D| =\n\u2211\nYi\u2208U/D |apr (\u03b1,\u03b2) C (Yi)|\n|U ||U/D|\n=\n\u2211\nYi\u2208U/D |apr C (apr (\u03b1,\u03b2) C (Yi))|\n|U ||U/D| = \u03b7\n(\u03b1,\u03b2) C .\n\u201c \u21d0 \u201d For \u2200Yi \u2208 U/D, we have aprR(apr (\u03b1,\u03b2) C (Yi)) \u2286 apr (\u03b1,\u03b2) C (Yi). Suppose there exists Yi \u2208 U/D such that aprR(apr (\u03b1,\u03b2) C (Yi)) \u2282 apr (\u03b1,\u03b2) C (Yi). As a result,\n\u03b7 (\u03b1,\u03b2) R =\n\u2211\nYi\u2208U/D |apr R (apr (\u03b1,\u03b2) C (Yi))|\n|U ||U/D| <\n\u2211\nYi\u2208U/D |apr (\u03b1,\u03b2) C (Yi)|\n|U ||U/D|\n=\n\u2211\nYi\u2208U/D |apr C (apr (\u03b1,\u03b2) C (Yi))|\n|U ||U/D| = \u03b7\n(\u03b1,\u03b2) C .\nIt conflicts with condition \u03b7 (\u03b1,\u03b2) R = \u03b7 (\u03b1,\u03b2) C . Hence, for \u2200Yi \u2208 U/D, we have\napr R (apr (\u03b1,\u03b2) C (Yi)) = apr (\u03b1,\u03b2) C (Yi).\nIf x \u2208 apr (\u03b1,\u03b2) C (Yi), there are two cases in which [x]R \u2286 apr (\u03b1,\u03b2) C (Yi) and\n[x]R 6\u2286 apr (\u03b1,\u03b2) C (Yi).\nWhen [x]R 6\u2286 apr (\u03b1,\u03b2) C (Yi), we have x /\u2208 aprR(apr (\u03b1,\u03b2) C (Yi)). It conflicts with condition apr R (apr (\u03b1,\u03b2) C (Yi)) = apr (\u03b1,\u03b2) C (Yi). Thus [x]R \u2286 apr (\u03b1,\u03b2) C (Yi). Since [x]R = \u222a{[y]C : [y]C \u2208 \u03be([x]R)}, we obtain that [y]C \u2286\n[x]R \u2286 apr (\u03b1,\u03b2) C (Yi) for all [y]C \u2208 \u03be([x]R). That is to say, for all [y]C \u2208 \u03be([x]R), it has p(Yi|[y]C) \u2265 \u03b1. Therefore we have that\np(Yi|[x]R) = ( \u2211 {|[y]C \u2229 Yi| : [y]C \u2208 \u03be([x]R)} )/ |[x]R|\n= \u2211\n{\np(Yi|[y]C) \u00b7 |[y]C |\n|[x]R| : [y]C \u2208 \u03be([x]R)\n}\n\u2265 \u03b1 \u2211\n{\n|[y]C | |[x]R| : [y]C \u2208 \u03be([x]R)\n}\n= \u03b1.\nAs a result x \u2208 apr (\u03b1,\u03b2) R (Yi).\nOn the other hand, if x \u2208 apr (\u03b1,\u03b2) R (Yi), then we have [x]R \u2286 apr (\u03b1,\u03b2) R (Yi).\nSince apr R (apr (\u03b1,\u03b2) C (Yi)) = apr (\u03b1,\u03b2) C (Yi), there are two cases in which [x]R \u2286 apr (\u03b1,\u03b2) C (Yi) and [x]R \u2229 apr (\u03b1,\u03b2) C (Yi) = \u2205.\nWhen [x]R \u2229 apr (\u03b1,\u03b2) C (Yi) = \u2205, since [x]R = \u222a{[y]C : [y]C \u2208 \u03be([x]R)}, we have [y]C \u2229 apr (\u03b1,\u03b2) C (Yi) = \u2205 for all [y]C \u2208 \u03be([x]R). That is to say, for all [y]C \u2208 \u03be([x]R), it has p(Yi|[y]C) < \u03b1. Therefore we have that\np(Yi|[x]R) = ( \u2211 {|[y]C \u2229 Yi| : [y]C \u2208 \u03be([x]R)} )/ |[x]R|\n= \u2211\n{\np(Yi|[y]C) \u00b7 |[y]C |\n|[x]R| : [y]C \u2208 \u03be([x]R)\n}\n< \u03b1 \u2211\n{\n|[y]C | |[x]R| : [y]C \u2208 \u03be([x]R)\n}\n= \u03b1.\nAs a result [x]R \u2229 apr (\u03b1,\u03b2) R (Yi) = \u2205, which contradicts with [x]R \u2286 apr (\u03b1,\u03b2) R (Yi).\nHence [x]R \u2286 apr (\u03b1,\u03b2) C (Yi), and in tune x \u2208 apr (\u03b1,\u03b2) C (Yi). Thus we conclude that apr (\u03b1,\u03b2) R (Yi) = apr (\u03b1,\u03b2) C (Yi) for \u2200Yi \u2208 U/D, i.e., R is\nan (\u03b1, \u03b2) lower distribution consistent set. The proof of (2) is similar to that of (1).\nTheorem 3. Given a decision table DT = (U,C \u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1 and R \u2286 C, we have\n(1) An attribute a \u2208 R is dispensable in R with respect to apr (\u03b1,\u03b2) R iff \u03b7 (\u03b1,\u03b2) R\u2212{a} =\n\u03b7 (\u03b1,\u03b2) R ,\n(2) An attribute a \u2208 R is dispensable inR with respect to apr (\u03b1,\u03b2) R iff \u00b5 (\u03b1,\u03b2) R\u2212{a} =\n\u00b5 (\u03b1,\u03b2) R .\nBy Theorem 3 we immediately get the following corollary.\nCorollary 2. Given a decision table DT = (U,C \u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1 and R \u2286 C, we have\n(1) An attribute a \u2208 R is indispensable in R with respect to apr (\u03b1,\u03b2) R iff\n\u03b7 (\u03b1,\u03b2) R\u2212{a} 6= \u03b7 (\u03b1,\u03b2) R ,\n(2) An attribute a \u2208 R is indispensable in R with respect to apr (\u03b1,\u03b2) R iff\n\u00b5 (\u03b1,\u03b2) R\u2212{a} 6= \u00b5 (\u03b1,\u03b2) R .\nTheorem 2 and Corollary 2 yield the following theorem.\nTheorem 4. Given a decision table DT = (U,C \u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1 and R \u2286 C, we have\n(1) R is an (\u03b1, \u03b2) lower distribution reduct of DT iff\n(I) \u03b7 (\u03b1,\u03b2) R = \u03b7 (\u03b1,\u03b2) C ; (II) \u03b7 (\u03b1,\u03b2) R\u2212{a} 6= \u03b7 (\u03b1,\u03b2) C for \u2200a \u2208 R.\n(2) R is an (\u03b1, \u03b2) upper distribution reduct of DT iff\n(I) \u00b5 (\u03b1,\u03b2) R = \u00b5 (\u03b1,\u03b2) C ; (II) \u00b5 (\u03b1,\u03b2) R\u2212{a} 6= \u00b5 (\u03b1,\u03b2) C for \u2200a \u2208 R.\nTheorem 4 gives the equivalent definition of distribution reducts.Compared with Definition 7, individually necessary conditions only need to check the subsets R \u2212 {a} for all a \u2208 R, not all subsets of R because the fitness functions \u03b7 (\u03b1,\u03b2) R and \u00b5 (\u03b1,\u03b2) R are monotonic with respect to the set inclusion of attribute sets. The condition simplifies the algorithm design. In fact, Theorem 4 provides concrete methods to design heuristic algorithms for obtaining the distribution reducts, and guarantees the completeness of heuristic attribute reduction algorithms.\n3.2. The significance measures of attributes\nEvaluating the significance of attributes is one of the most important problem to design the efficient heuristic attribute reduction algorithms. In this subsection, we construct the significance measures of attributes on the basis of the monotonic fitness functions proposed in order to provide the heuristic information that can guide search to distribution reducts.\nFor the significance measures of attributes, the monotonicity of measures are very important. The monotonicity can guarantee the rationality of the measures to evaluate the significance of attributes. Hence, we can use the fitness functions \u03b7 (\u03b1,\u03b2) R and \u00b5 (\u03b1,\u03b2) R as the significance measures of attributes for searching the (\u03b1, \u03b2) lower and upper distribution reducts to a certain degree, respectively. The corresponding significance measures of attributes are defined as follows.\nDefinition 9. Given a decision table DT = (U,C \u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1, R \u2282 C and \u2200a \u2208 C \u2212 R, the significance measures of attribute a in R with respect to apr (\u03b1,\u03b2) C and apr (\u03b1,\u03b2) C are defined as follows:\n(1) SIG\u03b7(a,R, apr (\u03b1,\u03b2) C ) = \u03b7 (\u03b1,\u03b2) R\u222a{a} \u2212 \u03b7 (\u03b1,\u03b2) R .\n(2) SIG\u00b5(a,R, apr (\u03b1,\u03b2) C ) = \u00b5 (\u03b1,\u03b2) R \u2212 \u00b5 (\u03b1,\u03b2) R\u222a{a}.\nDefinition 9 can be used to provide heuristics to guide the mechanism of searching an attribute. SIG\u03b7(a,R, apr (\u03b1,\u03b2) C ) can serve as the heuristic information for searching the (\u03b1, \u03b2) lower distribution reduct. SIG\u00b5(a,R, apr (\u03b1,\u03b2) C ) can serve as the heuristic information for searching the (\u03b1, \u03b2) upper distribution reduct. For convenience, the corresponding significance measures of attributes are also denoted as SIG\u03b7(a, apr (\u03b1,\u03b2) C ) = \u03b7 (\u03b1,\u03b2) {a} and SIG\u00b5(a, apr (\u03b1,\u03b2) C ) = 1\u2212 \u00b5 (\u03b1,\u03b2) {a} for any singleton attribute a \u2208 C.\nAs to the monotonicity of the fitness functions \u03b7 (\u03b1,\u03b2) R and \u00b5 (\u03b1,\u03b2) R , they can be used for evaluating the significance of attributes. However, the accuracy of fitness functions is very important to effectively evaluate the significance of attributes. In some situations, the fitness functions \u03b7 (\u03b1,\u03b2) R and \u00b5 (\u03b1,\u03b2) R cannot supply enough information for evaluating, because they do not taking into full account the granularity of partitions. The limitations are revealed by the following example.\nExample 2. Given a decision table DT = (U,C \u222aD) showed in Table 2, where U = {x1, x2, \u00b7 \u00b7 \u00b7 , x12}, and C = {a1, a2, a3, a4, a5, a6}. Suppose that \u03b1 = 0.60 and \u03b2 = 0.40, P,Q \u2286 C, where P = {a1, a2, a3, a4} and Q = {a1, a2, a3}. As we can see, P \u2287 Q, which means P\u227aQ.\nHence, we have\napr(0.60,0.40) C = ({x1, x2, x10, x11, x12}, {x3, x4, x5, x6, x9}), apr (0.60,0.40) C = ({x1, x2, x7, x8, x10, x11, x12}, {x3, x4, x5, x6, x7, x8, x9}).\nIt can be easily calculated that\nU/P = {{x1, x7, x8}, {x2}, {x3, x4, x5}, {x6}, {x9, x10, x11, x12}},\nU/Q = {{x1, x7, x8, x9, x10, x11, x12}, {x2}, {x3, x4, x5, x6}}.\nObviously, P \u227a Q. According to Definition 8, we have\n\u03b7 (0.60,0.40) P =\n\u2211\nYi\u2208U/D |apr P (apr (0.60,0.40) C (Yi))|\n|U ||U/D| =\n1 + 4\n12\u00d7 2 =\n5\n24 ,\n\u03b7 (0.60,0.40) Q =\n\u2211\nYi\u2208U/D |apr Q (apr (0.60,0.40) C (Yi))|\n|U ||U/D| =\n1 + 4\n12\u00d7 2 =\n5\n24 ,\n\u00b5 (0.60,0.40) P =\n\u2211\nYi\u2208U/D |aprP (apr (0.60,0.40) C (Yi))|\n|U ||U/D| =\n8 + 11 12\u00d7 2 = 19 24 ,\n\u00b5 (0.60,0.40) Q =\n\u2211\nYi\u2208U/D |aprQ(apr (0.60,0.40) C (Yi))|\n|U ||U/D| =\n8 + 11 12\u00d7 2 = 19 24 .\nAs a result, we have \u03b7 (0.60,0.40) P = \u03b7 (0.60,0.40) Q and \u00b5 (0.60,0.40) P = \u00b5 (0.60,0.40) Q .\nFrom Example 2, we can see that there is a partial relation between P and Q. However, we obtained the same values of the fitness functions. It indicates that the fitness functions in Definition 8 can not discern the attribute subsets P and Q clearly. The main reason is because they do not take into full account the granularity of partitions. Hence, it is easy to obtain that the significance of some attributes is zero by using the fitness functions in Definition 8. Therefore, it is necessary to introduce more effective fitness functions to evaluate the significance of attributes.\nIn what follows, we introduce more effective fitness functions based on the measures of granularity of partitions. We review several existing measures of granularity of partitions before introducing the more efficient fitness functions.\nGiven a partition, there are many methods that can be used to measure the granularity of a partition. Currently, Yao and Zhao [53] provided a unified framework for measures of granularity of partitions by considering the expected granularity of blocks in a partition.\nFirst, let us recall the measure of granularity of a set.\nDefinition 10. [53] Suppose U is finite and nonempty universe. A function m : 2U \u2192 R is called a measure of granularity of a set if it satisfies the following conditions: for all X,Y \u2208 2U .\n(1) (nonnegativity) m(X) \u2265 0,\n(2) (monotonicity) X \u2282 Y \u21d2 m(X) < m(Y ),\n(3) (size invariant) X\u2261sY \u21d4 m(X) = m(Y ),\nwhere the binary relation \u2261s is defined by: A\u2261sB if there exists a bijection from A to B.\nThe measure of granularity of a partition is defined as follows based on the measure of granularity of a set.\nDefinition 11. [53] Suppose \u03c0 = {X1, X2, . . . , XK} is a partition of a finite nonempty universe U and m : 2U \u2192 R is a measure of granularity of subsets of U , satisfies (1)\u2212 (3) of Definition 10. The expected granularity of blocks of \u03c0 is defined as:\nEGm(\u03c0) = EP\u03c0 (m(\u00b7)) = K \u2211\ni=1\nm(Xi)p(Xi),\nwhere P\u03c0 = (p(X1), p(X2), . . . , p(XK)) = ( |X1| |U| , |X2| |U| , . . . , |XK | |U| ) is the probability distribution defined by \u03c0 and EP\u03c0(\u00b7) is the mathematical expectation with respect to distribution P\u03c0.\nLet \u03a0 be the set of all partitions of U . The expected granularity EGm(\u03c0) reaches the minimum value if and only if \u03c0 = \u03a00 = {{x}|x \u2208 U} which is the finest partition in \u03a0, and it reaches the maximum value if and only if \u03c0 = \u03a01 = {U} which is the coarsest partition in \u03a0. In general, we have EGm(\u03a00) \u2264 EGm(\u03c0) \u2264 EGm(\u03a01) for any \u03c0 \u2208 \u03a0.\nThe expected granularity EGm(\u03c0) is a class of measures of granularity of partitions. The different measures of granularity of partitions can be derived from the expected granularity by considering various classes of measures of granularity of sets. Hence, many existing measures of granularity of partitions are instances of the expected granularity. The three most widely used measures of granularity of partitions are co-entropy, knowledge granularity and combination granularity in all existing measures of granularity of partitions. The corresponding definitions are as follows.\nDefinition 12. [2, 3, 22] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, . . . , XK}. The co-entropy of U/R is defined as follows:\nCE(U/R) = K \u2211\ni=1\n|Xi| |U | log2|Xi|.\nObviously, one has that 0 \u2264 CE(U/R) \u2264 log2|U |.\nDefinition 13. [22, 23] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, . . . , XK}. The knowledge granulation of U/R is defined as follows:\nGK(U/R) = 1\n|U |2\nK \u2211\ni=1\n|Xi| 2.\nObviously, one has that 1/|U | \u2264 GK(U/R) \u2264 1.\nDefinition 14. [41] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, . . . , XK}. The combination granulation of U/R is defined as follows:\nCG(U/R) =\nK \u2211\ni=1\n|Xi| |U |\n(\n|Xi| 2\n)\n(\n|U| 2\n) .\nObviously, one has that 0 \u2264 CG(U/R) \u2264 1.\nIn the following, we modified the fitness functions \u03b7 (\u03b1,\u03b2) R and \u00b5 (\u03b1,\u03b2) R in Definition 8 based on the expected granularity to evaluate the significance of attributes more effectively. Moreover, an example is provided to show that the modified fitness functions are more effective and suitable for evaluating the significance of attributes.\nDefinition 15. Given a decision table DT = (U,C\u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1, R \u2286 C and U/D = {Y1, Y2, . . . , YM} is a classification of the universe U , we denote\nG\u03b7 (\u03b1,\u03b2) R = EGm(\u03a01)\u2212 (1\u2212 \u03b7 (\u03b1,\u03b2) R )EGm(U/R), G\u00b5 (\u03b1,\u03b2) R = \u00b5 (\u03b1,\u03b2) R EGm(U/R).\nIf R = \u2205, then define G\u03b7 (\u03b1,\u03b2) R = 0 and G\u00b5 (\u03b1,\u03b2) R = EGm(\u03a01). Moreover,\nG\u03b7 (\u03b1,\u03b2) R and G\u00b5 (\u03b1,\u03b2) R are denoted by G\u03b7 and G\u00b5 respectively if there is no confusion arisen.\nClearly, G\u03b7 (\u03b1,\u03b2) R and G\u00b5 (\u03b1,\u03b2) R take into account the granularity of the parti-\ntion.\nTheorem 5. Given a decision table DT = (U,C \u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1 and P,Q \u2286 C, we have\n(1) P\u227aQ \u21d2 G\u03b7 (\u03b1,\u03b2) P \u2265 G\u03b7 (\u03b1,\u03b2) Q ,\n(2) P\u227aQ \u21d2 G\u00b5 (\u03b1,\u03b2) P \u2264 G\u00b5 (\u03b1,\u03b2) Q .\nProof. It can be easily proved according to Theorem 1, Definition 10 and Definition 11.\nBy Theorem 5 we immediately get the following corollary.\nCorollary 3. Given a decision table DT = (U,C \u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1 and P,Q \u2286 C, we have\n(1) P \u2287 Q \u21d2 G\u03b7 (\u03b1,\u03b2) P \u2265 G\u03b7 (\u03b1,\u03b2) Q ,\n(2) P \u2287 Q \u21d2 G\u00b5 (\u03b1,\u03b2) P \u2264 G\u00b5 (\u03b1,\u03b2) Q .\nTheorem 5 and Corollary 3 show that the modified fitness function G\u03b7 (\u03b1,\u03b2) R increases and the modified fitness function G\u00b5 (\u03b1,\u03b2) R decreases as the equivalence classes become smaller through finer partitioning, which means that adding a new attribute into the existing subset of condition attributes at least does not decrease G\u03b7 (\u03b1,\u03b2) R or increase G\u00b5 (\u03b1,\u03b2) R , and that deleting an attribute from the existing subset of condition attributes at least does not increases G\u03b7 (\u03b1,\u03b2) R or decreases G\u00b5 (\u03b1,\u03b2) R .\nExample 3. Continued from Example 2. We take the co-entropy (Definition 12) as an example of the expected granularity (Definition 11). According to Definition 12, we have\nEGm(U/P ) = 3\n12 log23 +\n1\n12 log21 +\n3\n12 log23 +\n1\n12 log21 +\n4\n12 log24\n\u2248 1.46,\nEGm(U/Q) = 7\n12 log27 +\n1\n12 log21 +\n4\n12 log24\n\u2248 2.30.\nAccording to Definition 15, we have\nG\u03b7 (0.60,0.40) P = EGm(\u03a01)\u2212 \u03b7 (0.60,0.40) P EGm(U/P ) = log212\u2212\n5\n23 \u00d7 1.46 \u2248 3.27,\nG\u03b7 (0.60,0.40) Q = EGm(\u03a01)\u2212 \u03b7 (0.60,0.40) Q EGm(U/Q) = log212\u2212\n5\n23 \u00d7 2.30 \u2248 3.08,\nG\u00b5 (0.60,0.40) P = EGm(\u03a01)\u2212 \u00b5 (0.60,0.40) P EGm(U/P ) =\n19 24 \u00d7 1.46 \u2248 1.16,\nG\u00b5 (0.60,0.40) Q = EGm(\u03a01)\u2212 \u00b5 (0.60,0.40) Q EGm(U/Q) =\n19 24 \u00d7 2.30 \u2248 1.82.\nObviously, G\u03b7 (0.60,0.40) P > G\u03b7 (0.60,0.40) Q and G\u00b5 (0.60,0.40) P < G\u00b5 (0.60,0.40) Q . It is clear that the modified fitness function G\u03b7 (\u03b1,\u03b2) R increases and the modified fitness function G\u00b5 (\u03b1,\u03b2) R decreases with R becoming finer.\nExample 3 shows the modified fitness functions G\u03b7 (\u03b1,\u03b2) R andG\u00b5 (\u03b1,\u03b2) R are more powerful for evaluating the attribute subsets in some cases. Hence, we use them as the significance measures of attributes for guiding search to the (\u03b1, \u03b2) lower and upper distribution reducts, respectively.\nThe corresponding significance measures of attributes are defined as follows.\nDefinition 16. Given a decision table DT = (U,C\u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1, R \u2282 C and \u2200a \u2208 C\u2212R, the significance measures of attribute a in R with respect to apr (\u03b1,\u03b2) C and apr (\u03b1,\u03b2) C are defined as follows:\n(1) SIGG\u03b7(a,R, apr (\u03b1,\u03b2) C ) = G\u03b7 (\u03b1,\u03b2) R\u222a{a} \u2212G\u03b7 (\u03b1,\u03b2) R .\n(2) SIGG\u00b5(a,R, apr (\u03b1,\u03b2) C ) = G\u00b5 (\u03b1,\u03b2) R \u2212G\u00b5 (\u03b1,\u03b2) R\u222a{a}.\nSimilar to Definition 9, for convenience, the corresponding significance mea-\nsures of attributes are also denoted as SIGG\u03b7(a, apr (\u03b1,\u03b2) C ) = \u03b7 (\u03b1,\u03b2) {a} and SIGG\u00b5(a, apr (\u03b1,\u03b2) C ) = EGm(\u03a01)\u2212 \u00b5 (\u03b1,\u03b2) {a} for any singleton attribute a \u2208 C.\n3.3. The attribute core\nAttribute core plays important role in heuristic attribute reduction algorithms. The attribute core is the set of all indispensable attributes. In other words, the attribute core is included in all reducts. Hence, it is often selected as the starting point in heuristic attribute reduction algorithms to narrow the search space of attributes. Moreover, the different attribute cores can be defined according to the different definitions of attribute reducts. In this subsection, we define the attribute cores for distribution reducts and provide the method for calculating the attribute cores.\nDefinition 17. Given a decision table DT = (U,C\u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1, the attribute cores for the (\u03b1, \u03b2) lower and upper distribution reducts are defined as follows:\n(1) COREapr(\u03b1,\u03b2)(C) = \u2229REDapr(\u03b1,\u03b2)(C),\n(2) COREapr(\u03b1,\u03b2)(C) = \u2229REDapr(\u03b1,\u03b2)(C).\nwhere, REDapr(\u03b1,\u03b2)(C) and REDapr(\u03b1,\u03b2)(C) denote the set of all (\u03b1, \u03b2) lower and (\u03b1, \u03b2) upper distribution reducts, respectively.\nTheorem 2 and Definition 17 yield the following theorem.\nTheorem 6. Given a decision table DT = (U,C \u222aD), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1 and c \u2208 C, we have\n(1) c \u2208 COREapr(\u03b1,\u03b2)(C) iff \u03b7 (\u03b1,\u03b2) C\u2212{c} 6= \u03b7 (\u03b1,\u03b2) C ,\n(2) c \u2208 COREapr(\u03b1,\u03b2)(C) iff \u00b5 (\u03b1,\u03b2) C\u2212{c} 6= \u00b5 (\u03b1,\u03b2) C .\nProof. (1) \u201c \u21d2 \u201d Suppose \u03b7 (\u03b1,\u03b2) C\u2212{c} = \u03b7 (\u03b1,\u03b2) C . c is dispensable in C according to Theorem 3. It conflicts with condition c \u2208 COREapr(\u03b1,\u03b2)(C), so we have \u03b7 (\u03b1,\u03b2) C\u2212{c} 6= \u00b5 (\u03b1,\u03b2) C .\n\u201c \u21d0 \u201d If \u03b7 (\u03b1,\u03b2) C\u2212{c} 6= \u03b7 (\u03b1,\u03b2) C , then c is indispensable in C according to Corollary 2. Hence, c must appear in all (\u03b1, \u03b2) lower distribution reducts of DT . Thus, c \u2208 \u2229REDapr(\u03b1,\u03b2)(C), i.e., c \u2208 COREapr(\u03b1,\u03b2)(C).\n(2) The proof is similar to that of (1).\nTheorem 6 and Definition 17 yield the following definition.\nDefinition 18. Given a decision table DT = (U,C \u222aD), for any 0\u2264\u03b2<\u03b1\u2264 1, the attribute cores for the (\u03b1, \u03b2) lower and upper distribution reducts are defined as follows:\n(1) COREapr(\u03b1,\u03b2)(C) = {c \u2208 C|\u03b7 (\u03b1,\u03b2) C\u2212{c} 6= \u03b7 (\u03b1,\u03b2) C },\n(2) COREapr(\u03b1,\u03b2)(C) = {c \u2208 C|\u00b5 (\u03b1,\u03b2) C\u2212{c} 6= \u00b5 (\u03b1,\u03b2) C }.\nDefinition 18 states the calculation method of the attribute cores for the (\u03b1, \u03b2) lower and upper distribution reducts.\nAn algorithm for calculating the attribute cores for the (\u03b1, \u03b2) lower and upper distribution reducts is displayed in Algorithm 3.\nAlgorithm 3 The algorithm for calculating the attribute core for the (\u03b1, \u03b2) lower (upper) distribution reduct\nInput: A decision table DT = (U,C \u222aD), threshold values (\u03b1, \u03b2) Output: An attribute core for the (\u03b1, \u03b2) lower (upper) distribution reduct Note: (T,\u2206) \u2208 {(apr, \u03b7), (apr, \u00b5)}\n1: Let CORET (\u03b1,\u03b2)(C) = \u2205 2: for Each c \u2208 C do 3: Calculate \u2206\n(\u03b1,\u03b2) C\u2212{c}\n4: if \u2206 (\u03b1,\u03b2) C\u2212{c} 6= \u2206 (\u03b1,\u03b2) C then 5: CORET (\u03b1,\u03b2)(C) = CORET (\u03b1,\u03b2)(C) \u222a {c} 6: end if 7: end for 8: Return CORET (\u03b1,\u03b2)(C)\n3.4. Attribute reduction algorithms\nIn this subsection, we proposed two heuristic attribute reduction algorithms based on the addition-deletion method (Algorithm 1) and the deletion method (Algorithm 2) to obtain the distribution reducts in probabilistic rough set model. Two algorithm details are shown in Algorithm 4 and Algorithm 5.\nAlgorithm 4 The addition-deletion method for computing the distribution reducts of DT Input: A decision table DT = (U,C \u222aD), threshold values (\u03b1, \u03b2) Output: An (\u03b1, \u03b2) lower (upper) distribution reduct of DT Method: Addition-deletion method Note: (T,\u2206) \u2208 {(apr, \u03b7), (apr, \u00b5)}\n1: Calculate CORET (\u03b1,\u03b2)(C) by Algorithm 3 2: Calculate \u2206 (\u03b1,\u03b2) C 3: Let R = CORET (\u03b1,\u03b2)(C), CA = C \u2212R 4: if \u2206 (\u03b1,\u03b2) R = \u2206 (\u03b1,\u03b2) C then 5: go to Step 28 6: end if 7: // Addition 8: while \u2206 (\u03b1,\u03b2) R 6= \u2206 (\u03b1,\u03b2) C do\n9: for Each a \u2208 CA do 10: Calculate SIGG\u2206(a,R, T (\u03b1,\u03b2) C ) 11: end for 12: if SIGG\u2206(a,R, T (\u03b1,\u03b2) C ) = maxa\u2208C\u2212RSIGG\u2206(a,R, T (\u03b1,\u03b2) C ) then 13: R = R \u222a {a}, CA = CA\u2212 {a} 14: end if 15: end while 16: // Deletion 17: Let CD = R 18: for Each a \u2208 CD do 19: Calculate SIGG\u2206(a, T ) 20: end for 21: Sort attributes in CD according to SIGG\u2206(a, T ) in a ascending order 22: while CD 6= \u2205 do 23: CD = CD \u2212 {a}, where a is the first element of CD 24: if \u2206 (\u03b1,\u03b2) R\u2212{a} = \u2206 (\u03b1,\u03b2) C then 25: R = R\u2212 {a} 26: end if 27: end while 28: Return R\nAlgorithm 5 The deletion method for computing the distribution reducts of DT Input: A decision table DT = (U,C \u222aD), threshold values (\u03b1, \u03b2) Output: An (\u03b1, \u03b2) lower (upper) distribution reduct of DT Method: Deletion method Note: (T,\u2206) \u2208 {(apr, \u03b7), (apr, \u00b5)}\n1: Calculate \u2206 (\u03b1,\u03b2) C 2: Let R = C, CD = C 3: for Each a \u2208 CD do 4: Calculate SIGG\u2206(a, T ) 5: end for 6: Sort attributes in CD according to SIGG\u2206(a, T ) in a ascending order 7: while CD 6= \u2205 do 8: CD = CD \u2212 {a}, where a is the first element of CD 9: if \u2206 (\u03b1,\u03b2) R\u2212{a} = \u2206 (\u03b1,\u03b2) C then\n10: R = R\u2212 {a} 11: end if 12: end while 13: Return R\n3.5. An illustrative example\nWe have developed two heuristic attribute reduction algorithms to obtain distribution reducts. In this subsection, we present an example to show the validity of the proposed algorithms.\nExample 4. (Continued from Example 1) For Table 1 shown in Example 1, we take the (0.6, 0.4) low distribution reduct as an example. Moreover, the co-entropy (Definition 12) is used as an example of the expected granularity (Definition 11). We can calculate the (0.60, 0.40) low distribution reduct by Algorithm 4.\nBy computing, we can obtain \u03b7 (0.60,0.40) C \u2248 0.32. According to core computing step in Algorithm 4, we first calculate \u03b7 (0.60,0.40) C\u2212{ai}\nfor each attribute ai \u2208 C as follows:\n\u03b7 (0.60,0.40) C\u2212{a1} \u2248 0.27, \u03b7 (0.60,0.40) C\u2212{a2} \u2248 0.32, \u03b7 (0.75,0.60) C\u2212{a3} \u2248 0.23, \u03b7 (0.60,0.40) C\u2212{a4} \u2248 0.32, \u03b7 (0.60,0.40) C\u2212{a5} \u2248 0.32, \u03b7 (0.75,0.60) C\u2212{a6} \u2248 0.32. Therefore, COREapr(0.60,0.40) (C) = {a1, a3}. Let R = COREapr(0.60,0.40) (C) = {a1, a3}, then \u03b7 (0.60,0.40) R \u2248 0.09. Because \u03b7 (0.60,0.40) R 6= \u03b7 (0.60,0.40) C , according to the addition step in Algorithm\n4 we calculate G\u03b7\n(0.60,0.40) R\u222a{a2} \u2248 1.64, G\u03b7 (0.60,0.40) R\u222a{a4} \u2248 1.53,\nG\u03b7 (0.60,0.40) R\u222a{a5} \u2248 1.84, G\u03b7 (0.60,0.40) R\u222a{a6} \u2248 1.91.\nBecause G\u03b7 (0.60,0.40) R\u222a{a6} is maximum, we select a6. Let R = R \u222a {a6} =\n{a1, a3, a6}, then \u03b7 (0.60,0.40) R \u2248 0.27.\nBecause \u03b7 (0.60,0.40) R 6= \u03b7 (0.60,0.40) C , we calculate \u03b7 (0.60,0.40) R\u222a{ai} for each attribute\nai \u2208 C \u2212R as follows:\nG\u03b7 (0.60,0.40) R\u222a{a2} \u2248 1.91, G\u03b7 (0.60,0.40) R\u222a{a4} \u2248 2.05, G\u03b7 (0.60,0.40) R\u222a{a5} \u2248 2.14. Because G\u03b7 (0.60,0.40) R\u222a{a5} is maximum, we select a5. Let R = R \u222a {a6} =\n{a1, a3, a6}, then \u03b7 (0.60,0.40) R \u2248 0.32.\nThus, we have \u03b7 (0.60,0.40) R = \u03b7 (0.60,0.40) C when R = {a1, a3, a5, a6}. According to the deletion step in Algorithm 4, we get a (0.60, 0.40) low\ndistribution reduct {a1, a3, a5, a6} because we have \u03b7 (0.60,0.40) R\u2212{ai} 6= \u03b7 (0.60,0.40) C for \u2200ai \u2208 R. It can be easily calculated that\napr (0.60,0.40) {a1,a3,a5,a6} = ({x1, x2, x5}, {x6, x7, x10, x11}). Hence, we have apr (0.60,0.40) {a1,a3,a5,a6} = apr (0.60,0.40) C .\nIn other words, the attribute set {a1, a3, a5, a6} can keep the (0.60, 0.40) lower approximations of all decision classes unchanged.\nSimilar to the computation of the (0.60, 0.40) low distribution reduct, it is not difficult to find that {a1, a3, a4, a6} is a (0.60, 0.40)upper distribution reduct.\nBy Algorithm 5 we can obtain {a1, a3, a4, a6} is a (0.60, 0.40) low distribution reduct and {a1, a2, a3, a5} is a (0.60, 0.40) upper distribution reduct."}, {"heading": "4. Experimental results", "text": "In this section, a series of experiments were designed to demonstrate that our methods proposed are effective and applicable. Ten benchmark real-world data sets were chosen for experimental evaluation. All the data sets were obtained from the UCI Repository of Machine Learning databases [1]. These data sets have been widely used in literatures. The general information about the selected UCI data sets is summarized in Table 3, where |U | and |C| denote the number of objects and the condition attributes, respectively. |Vd| denotes the number of decision classes.\nSince the data sets may contain missing values or continuous attributes, they would be handled in advance prior to attribute reduction. Missing values were filled with mean values for continuous attributes and mode values for nominal attributes. Continuous attributes were discretized using equal-frequency discretization method. All preprocessing methods were implemented by using WEKA filters [9].\n4.1. The monotonicity experiments\nIn this subsection, several experiments were performed to verify the effectiveness of the proposed fitness functions in Section 3. In the experiments, we took the co-entropy (CE,Definition 12), knowledge granulation (KG,Definition 13) and combination granulation (CG,Definition 14) as examples of the expected\ngranularity. Hence, both of fitness functions G\u03b7 and G\u00b5 (Definition 15) can adopt three types of implementations based on CE, KG and CG. The threshold parameters \u03b1 and \u03b2 are set to 0.6 and 0.4 respectively.\nFigures 1 - 10 present the experimental results of the proposed fitness functions on ten data sets. In each of figures, the X-axis represents the size of condition attribute subset. The condition attribute subset is increased from one attribute to all attributes during the experiments. The Y-axis pertains to values of fitness functions. Furthermore, each figure has two subfigures. The subfigure (a) shows that the experimental results of the fitness functions \u03b7, G\u03b7 \u2212 CE, G\u03b7 \u2212 KG and G\u03b7 \u2212 CG, where G\u03b7 \u2212 CE, G\u03b7 \u2212KG and G\u03b7 \u2212 CG represent the CE-based, KG-based and CG-based implementations of G\u03b7 respectively. The subfigure (b) shows that the experimental results of the fitness functions \u00b5, G\u00b5\u2212CE, G\u00b5\u2212KG and G\u00b5\u2212CG, where G\u00b5\u2212CE, G\u00b5\u2212KG and G\u00b5\u2212CG represent the CE-based, KG-based and CG-based implementations of G\u00b5 respectively. Moreover, we rescaled the values of them to the [0, 1] range in order to better visualize the data because the values of the fitness functions G\u03b7\u2212CE and G\u00b5\u2212 CE may be greater than 1.\nIt can be seen from these figures that the values of the fitness functions \u03b7, G\u03b7\u2212CE, G\u03b7\u2212KG and G\u03b7\u2212CG increase with the number of selected attributes becoming bigger, and the values of the fitness functions \u00b5, G\u00b5\u2212CE, G\u00b5\u2212KG and G\u00b5\u2212CG decrease with the number of selected attributes becoming bigger. It indicates that the proposed fitness functions are monotonic with respect to the set inclusion of attributes. The results are consistent with Corollary 1 and Corollary 3. However, it is easy to see that the values of the fitness function \u03b7 are the same when the number of attributes increased from 1 to 2 on all the data sets except Zoo. Similarly, there is no change in the results of the fitness function \u00b5 as the number of attributes increases from 1 to 2 on some data sets, such as Horse-colic, Voting and Kr-vs-kp. In these cases, the fitness functions \u03b7 and \u00b5 can not evaluate the significance of attributes effectively. For the same situation, the values of the fitness functions G\u03b7\u2212CE, G\u03b7\u2212KG and G\u03b7\u2212CG get\nbigger, while the values of the fitness functions G\u00b5\u2212CE, G\u00b5\u2212KG and G\u00b5\u2212CG get smaller when the number of attributes increased from 1 to 2. It shows that the fitness functions G\u03b7 and G\u00b5 can evaluate the significance of attributes more accurately. The results show that the fitness functions G\u03b7 and G\u00b5 can provide more information for evaluating the significance of attributes. In other words, the fitness functions G\u03b7 and G\u00b5 have a better discrimination power than the fitness functions \u03b7 and \u00b5. Hence, we can evaluate the significance of attributes more effectively by using the fitness functions G\u03b7 and G\u00b5.\n4.2. Significance of single attributes\nEvaluating single attributes and ranking them are an important step in Algorithms 4 and 5. Hence, in this subsection, we compared the effectiveness of the proposed fitness functions in evaluating the significance of single attributes.\nTwo data sets Vehicle and Credit Approval were used in experiments. There are 18 attributes in Vehicle and 15 attributes in Credit Approval. The experimental results of the fitness functions \u03b7, G\u03b7 \u2212 CE, G\u03b7 \u2212 KG and G\u03b7 \u2212 CG on two data sets are shown Figures 11 and 13 respectively. The experimental results of the fitness functions \u00b5, G\u00b5 \u2212 CE, G\u00b5 \u2212 KG and G\u00b5 \u2212 CG on two data sets are shown Figures 12 and 14 respectively. Similar to the monotonicity experiments, the values of the fitness functions G\u03b7 \u2212 CE and G\u00b5 \u2212 CE were rescaled to the [0, 1] range.\nAs to data set Vehicle, for each single attribute, the values of the fitness function \u03b7 are equal to zero. It indicates that the fitness function \u03b7 can not evaluate the single attributes effectively. In this situation, we can not rank attributes by the fitness function \u03b7. In comparison, the fitness functions G\u03b7 \u2212 CE, G\u03b7 \u2212 KG and G\u03b7 \u2212 CG can differentiate the different single attributes\neffectively. In other words, we can rank attributes effectively by the fitness function G\u03b7. Similarly, for the most single attributes, the values of the fitness function \u00b5 are equal to 1. Hence, we can not rank attributes by the fitness function \u00b5, but we can rank attributes by the fitness functions G\u00b5\u2212CE, G\u00b5\u2212 KG and G\u00b5 \u2212 CG. As to data set Credit Approval, we can obtain a similar result. The above experimental results show that the fitness functions \u03b7 and \u00b5 are not appropriate to evaluate the single attributes and rank them sometimes, and the fitness functions G\u03b7 and G\u00b5 can evaluate the single attributes and rank them effectively.\n4.3. The classification accuracy experiments\nIn this subsection, we took the (\u03b1, \u03b2) low distribute reduct as an example of the distribute reducts. Then we compared the classification accuracy of\nthe (\u03b1, \u03b2) low distribution reduct with three traditional definitions of attribute reduct, including the qualitative positive region-preserved reduct (QLPRP) [61], the quantitative positive region-preserved reduct (QNPRP) [61] and the positive region extension reduct (EXDPR) [21], in probabilistic rough set model. For each definition, the corresponding reducts are obtained by the addition-deletion method and the deletion method.\nFor the (\u03b1, \u03b2) low distribution reduct, in Algorithms 4 and 5, we considered three different implementations ofG\u03b7 by taking CE, KG and CG as the examples of the expected granularity. In addition, for Algorithm 4 and 5, the (\u03b1, \u03b2) low distribution reducts obtained by three different implementations are denoted as LDRCE, LDRKG and LDRCG respectively.\nIn the experiments, two well-known used classifiers including BayesNet and linear SVM (SMO) were selected to evaluate the different definitions of attribute\nreduct. 10-fold cross-validation scheme was used to assess the performance of the classifiers. All approaches were implemented based on the WEKA data mining software package [9], where the classifiers were implemented with default settings.\nTo make a fair comparison, we took the value of \u03b1 from 0.1 to 1.0 with step 0.1. For each data set, the corresponding attribute reducts can be got according to the different \u03b1. Then we computed the classification accuracies of all attribute reducts using BayesNet and SMO based on 10-fold cross-validation. The average value and standard deviation were recorded as the final classification accuracies.\nTables 4 - 7 show the experimental results of classification accuracies. In each table, the average maximum classification accuracies are depicted in bold. The classification accuracies are performed on the raw data sets also.\nIt is observed from Tables 4 - 7 that, for addition-deletion method, LDRCE,\nLDRKG and LDRCG exhibited the best average classification accuracy based on BayesNet in most cases. In detail, LDRCE and LDRCG achieved the highest average classification accuracy on six data sets, and LDRKG achieved the highest average classification accuracy on seven data sets. It should be emphasized that LDRCE, LDRKG and LDRCG jointly achieved the highest average\nclassification accuracy on five data sets. Moveover, EXDPR got the maximum average classification accuracy on data set Hypothyroid. Compared with the classification accuracy of raw data, LDRCE, LDRKG and LDRCG obtained better classification performance on all the data sets except the data sets Credit Approval and German. In most cases, QLPRPR, QNPRPR and PRER decrease\nthe classification accuracies of raw data to some extent. In addition, the classification accuracies of LDRCE, LDRKG and LDRCG was found to be similar\nor consistent. As to SMO, LDRCE, LDRKG and LDRCG have better average classification accuracy than QLPRP, QNPRP and PRER on all the data sets. Compared with the classification accuracy of raw data, LDRCE, LDRKG and LDRCG obtained better classification performance on more than half of data sets. For the deletion method, the similar results can be obtained by the classification accuracy based on BayesNet and SMO.\nFurthermore, from Tables 4 - 7, we can clearly notice that the standard deviations of classification accuracies derived from LDRCE, LDRKG and LDRCG equal to zero on all the data sets. It implies Algorithms 4 and 5 are insensitive to threshold parameters. This can be explained because the (\u03b1, \u03b2) low distribution reduct has a more stringent reduction condition that preserves the (\u03b1, \u03b2) low approximation of all decision classes. In fact, the (\u03b1, \u03b2) low distribution reduct is equivalent to reduct that keeps Pawlak positive region (Definition 2) unchanged when decision table is consistent (POSR(D) = U). It means that for the consistent decision talbe, the classification accuracies derived from LDRCE, LDRKG and LDRCG are the same for all threshold parameters. What\u2019s more, for inconsistent decision table (POSR(D) 6= U), Algorithms 4 and 5 are also insensitive to threshold parameters when each decision class contains too few inconsistent objects because the (\u03b1, \u03b2) low distribution reduct must keep the (\u03b1, \u03b2) low approximation of all decision classes unchanged.\nTable 8 and 9 outlined the average length of the derived reduct based on the addition-deletion method and the deletion method. From Table 8 and 9, it can be seen that the numbers of attributes obtained by QLPRPR, QNPRPR and PRER are less than those obtained by LDRCE, LDRKG and LDRCG. The reason should be attributed to the fact that the (\u03b1, \u03b2) low distribution reduct have more stringent reduction condition than three other definitions of attribute redcut. For length of the derived reduct, the standard deviations of LDRCE, LDRKG and LDRCG also equal to zero on all the data sets. It further indicates Algorithms 4 and 5 are insensitive to threshold parameters. In addition, for QLPRPR, QNPRPR and PRER, the relatively high standard deviations of reduct length also show that QLPRPR, QNPRPR and PRER\nobtained by the addition-deletion method and the deletion method are sensitive to threshold parameters."}, {"heading": "1 5.5 \u00b1 4.5 4.6 \u00b1 4.4 3.7 \u00b1 4.1 10.0 \u00b1 0.0 10.0 \u00b1 0.0 10.0 \u00b1 0.0", "text": ""}, {"heading": "2 11.9 \u00b1 5.7 11.9 \u00b1 5.7 11.9 \u00b1 5.6 16.0 \u00b1 0.0 16.0 \u00b1 0.0 16.0 \u00b1 0.0", "text": ""}, {"heading": "3 9.5 \u00b1 6.1 9.4 \u00b1 6.1 8.6 \u00b1 6.5 15.0 \u00b1 0.0 15.0 \u00b1 0.0 15.0 \u00b1 0.0", "text": ""}, {"heading": "4 6.2 \u00b1 5.3 6.6 \u00b1 5.6 2.1 \u00b1 3.3 12.0 \u00b1 0.0 12.0 \u00b1 0.0 12.0 \u00b1 0.0", "text": ""}, {"heading": "5 3.8 \u00b1 2.9 3.4 \u00b1 2.4 3.0 \u00b1 2.0 7.0 \u00b1 0.0 6.0 \u00b1 0.0 6.0 \u00b1 0.0", "text": ""}, {"heading": "6 6.0 \u00b1 5.0 5.9 \u00b1 4.9 4.0 \u00b1 4.6 11.0 \u00b1 0.0 11.0 \u00b1 0.0 11.0 \u00b1 0.0", "text": ""}, {"heading": "7 8.6 \u00b1 6.4 9.4 \u00b1 5.1 7.6 \u00b1 5.3 15.0 \u00b1 0.0 14.0 \u00b1 0.0 14.0 \u00b1 0.0", "text": ""}, {"heading": "8 12.3 \u00b1 13.6 15.0 \u00b1 14.0 9.6 \u00b1 12.7 29.0 \u00b1 0.0 29.0 \u00b1 0.0 29.0 \u00b1 0.0", "text": ""}, {"heading": "9 14.5 \u00b1 9.5 14.2 \u00b1 9.3 3.2 \u00b1 6.6 23.0 \u00b1 0.0 23.0 \u00b1 0.0 23.0 \u00b1 0.0", "text": ""}, {"heading": "10 5.5 \u00b1 4.5 5.5 \u00b1 4.5 4.9 \u00b1 4.8 10.0 \u00b1 0.0 10.0 \u00b1 0.0 10.0 \u00b1 0.0", "text": ""}, {"heading": "1 5.1 \u00b1 4.1 5.5 \u00b1 4.5 5.5 \u00b1 4.5 9.0 \u00b1 0.0 9.0 \u00b1 0.0 10.0 \u00b1 0.0", "text": ""}, {"heading": "2 12.0 \u00b1 5.6 12.1 \u00b1 5.5 12.5 \u00b1 5.2 16.0 \u00b1 0.0 16.0 \u00b1 0.0 16.0 \u00b1 0.0", "text": ""}, {"heading": "3 9.6 \u00b1 6.1 9.4 \u00b1 6.1 9.8 \u00b1 5.6 15.0 \u00b1 0.0 15.0 \u00b1 0.0 15.0 \u00b1 0.0", "text": ""}, {"heading": "4 6.2 \u00b1 5.3 6.2 \u00b1 5.3 6.3 \u00b1 5.3 12.0 \u00b1 0.0 12.0 \u00b1 0.0 12.0 \u00b1 0.0", "text": ""}, {"heading": "5 2.9 \u00b1 2.1 2.9 \u00b1 2.1 3.2 \u00b1 1.9 6.0 \u00b1 0.0 6.0 \u00b1 0.0 6.0 \u00b1 0.0", "text": ""}, {"heading": "6 6.0 \u00b1 5.0 6.0 \u00b1 5.0 5.9 \u00b1 4.9 11.0 \u00b1 0.0 11.0 \u00b1 0.0 11.0 \u00b1 0.0", "text": ""}, {"heading": "7 8.7 \u00b1 5.6 8.7 \u00b1 6.2 8.2 \u00b1 5.2 14.0 \u00b1 0.0 14.0 \u00b1 0.0 14.0 \u00b1 0.0", "text": ""}, {"heading": "8 15.0 \u00b1 14.0 15.0 \u00b1 14.0 15.0 \u00b1 14.0 29.0 \u00b1 0.0 29.0 \u00b1 0.0 29.0 \u00b1 0.0", "text": ""}, {"heading": "9 14.5 \u00b1 9.5 14.1 \u00b1 9.3 12.6 \u00b1 9.7 23.0 \u00b1 0.0 23.0 \u00b1 0.0 23.0 \u00b1 0.0", "text": ""}, {"heading": "10 5.5 \u00b1 4.5 5.5 \u00b1 4.5 6.0 \u00b1 5.1 10.0 \u00b1 0.0 10.0 \u00b1 0.0 10.0 \u00b1 0.0", "text": "From the experimental results, we can draw a conclusion that the distribution reducts are relatively the better choices in probabilistic rough set model, as attributes they selected have the higher classification accuracy.\n4.4. Comparison of distribution reducts with ranking based attribute reduction methods\nIn this subsection, we compared the classification accuracy of the distribution reducts with ranking based attribute reduction methods (RBAR). RBAR is a classic attribute reduction algorithm. In RBAR, the attributes are globally ranked based on the significance of single attributes, and the first k best attributes are selected. In our experiments, the significance of single attributes was evaluated by using the fitness functions G\u03b7\u2212CE, G\u03b7\u2212KG and G\u03b7\u2212CG, and the different values of k were specified based on the subset size of the corresponding (\u03b1, \u03b2) low distribution reducts, which are obtained by the size of LDRCE, LDRKG and LDRCG, to compare the distribution reducts with"}, {"heading": "1 0.7745 \u00b1 0.0000 0.7147 \u00b1 0.0000 0.7147 \u00b1 0.0000", "text": ""}, {"heading": "2 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000", "text": ""}, {"heading": "3 0.4504 \u00b1 0.0000 0.4504 \u00b1 0.0000 0.4504 \u00b1 0.0000", "text": ""}, {"heading": "4 0.8943 \u00b1 0.0000 0.8943 \u00b1 0.0000 0.8943 \u00b1 0.0000", "text": ""}, {"heading": "5 0.9307 \u00b1 0.0000 0.8416 \u00b1 0.0000 0.8416 \u00b1 0.0000", "text": ""}, {"heading": "6 0.8652 \u00b1 0.0000 0.8652 \u00b1 0.0000 0.8652 \u00b1 0.0000", "text": ""}, {"heading": "7 0.8260 \u00b1 0.0000 0.8312 \u00b1 0.0000 0.8212 \u00b1 0.0000", "text": ""}, {"heading": "8 0.8767 \u00b1 0.0000 0.8767 \u00b1 0.0000 0.8767 \u00b1 0.0000", "text": ""}, {"heading": "9 0.9178 \u00b1 0.0000 0.9178 \u00b1 0.0000 0.9223 \u00b1 0.0000", "text": ""}, {"heading": "10 0.7430 \u00b1 0.0000 0.7420 \u00b1 0.0000 0.7420 \u00b1 0.0000", "text": "ranking based attribute reduction methods. The classification accuracies were obtained by adopting the same experimental settings and methods that were used in subsection 4.3.\nTables 10-13 present experimental results of RBAR on ten data sets using BayesNet and SMO, where RBARCE, RBARKG and RBARCG represent the attribute reducts obtained by using G\u03b7\u2212CE, G\u03b7\u2212KG and G\u03b7\u2212CG to evaluate the significance of single attributes, respectively. Compare with Tables 4 - 7, it is easy to see that, for the (\u03b1, \u03b2) low distribution reducts obtained by the additiondeletion method, LDRCE and LDRCG outperform RBARCE and RBARCG on seven data sets respectively, and LDRKG outperform RBARKG on eight data sets according to BayesNet. Note that LDRCE, LDRKG and LDRCG get the same average classification accuracy on data set Primary-tumor with RBARCE, RBARKG and RBARCG respectively. This is because the (\u03b1, \u03b2) low distribution reducts obtained by the addition-deletion method and reducts obtained by ranking method are the same. As to SMO, the results is similar to that of BayesNet. For the (\u03b1, \u03b2) low distribution reducts obtained by the deletion method, we can obtain the similar results also.\nNevertheless, from Table 8 and 9, we can easily find that the standard deviations of classification accuracies derived from RBARCE, RBARKG and RBARCG equal to zero on all the data sets. This is because, for given 10 different values of \u03b1, we always obtain the same ranking of attributes according to the fitness function G\u03b7, and the subset size of the (\u03b1, \u03b2) low distribution reduct obtained by the addition-deletion method or deletion method is same every time. It further indicates that Algorithms 4 and 5 are insensitive to threshold parameters.\nThe above experimental results show that the distribution reducts are effective."}, {"heading": "1 0.7391 \u00b1 0.0000 0.7147 \u00b1 0.0000 0.7147 \u00b1 0.0000", "text": ""}, {"heading": "2 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000", "text": ""}, {"heading": "3 0.6040 \u00b1 0.0000 0.6040 \u00b1 0.0000 0.6040 \u00b1 0.0000", "text": ""}, {"heading": "4 0.9563 \u00b1 0.0000 0.9563 \u00b1 0.0000 0.9563 \u00b1 0.0000", "text": ""}, {"heading": "5 0.9307 \u00b1 0.0000 0.8416 \u00b1 0.0000 0.8416 \u00b1 0.0000", "text": ""}, {"heading": "6 0.8551 \u00b1 0.0000 0.8551 \u00b1 0.0000 0.8551 \u00b1 0.0000", "text": ""}, {"heading": "7 0.9216 \u00b1 0.0000 0.9203 \u00b1 0.0000 0.9048 \u00b1 0.0000", "text": ""}, {"heading": "8 0.9528 \u00b1 0.0000 0.9528 \u00b1 0.0000 0.9528 \u00b1 0.0000", "text": ""}, {"heading": "9 0.9236 \u00b1 0.0000 0.9236 \u00b1 0.0000 0.9229 \u00b1 0.0000", "text": ""}, {"heading": "10 0.7490 \u00b1 0.0000 0.7520 \u00b1 0.0000 0.7520 \u00b1 0.0000", "text": ""}, {"heading": "1 0.7663 \u00b1 0.0000 0.7092 \u00b1 0.0000 0.7147 \u00b1 0.0000", "text": ""}, {"heading": "2 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000", "text": ""}, {"heading": "3 0.4504 \u00b1 0.0000 0.4504 \u00b1 0.0000 0.4504 \u00b1 0.0000", "text": ""}, {"heading": "4 0.8943 \u00b1 0.0000 0.8943 \u00b1 0.0000 0.8943 \u00b1 0.0000", "text": ""}, {"heading": "5 0.8416 \u00b1 0.0000 0.8416 \u00b1 0.0000 0.8416 \u00b1 0.0000", "text": ""}, {"heading": "6 0.8652 \u00b1 0.0000 0.8652 \u00b1 0.0000 0.8652 \u00b1 0.0000", "text": ""}, {"heading": "7 0.8312 \u00b1 0.0000 0.8312 \u00b1 0.0000 0.8087 \u00b1 0.0000", "text": ""}, {"heading": "8 0.8767 \u00b1 0.0000 0.8767 \u00b1 0.0000 0.8767 \u00b1 0.0000", "text": ""}, {"heading": "9 0.9178 \u00b1 0.0000 0.9178 \u00b1 0.0000 0.9223 \u00b1 0.0000", "text": ""}, {"heading": "10 0.7430 \u00b1 0.0000 0.7420 \u00b1 0.0000 0.7420 \u00b1 0.0000", "text": ""}, {"heading": "5. Conclusion", "text": "The addition-deletion method based and deletion method based attribute reduction algorithms are two representative heuristic attribute reduction algorithms in rough set theory. The fitness functions play a crucial role in designing heuristic attribute reduction algorithms. The monotonicity of fitness functions is very important to guarantee the validity of heuristic attribute reduction algorithms. This paper aims at developing heuristic attribute reduction algorithms for finding distribution reducts in probabilistic rough set model. To begin with, we proposed two monotonic fitness functions \u03b7 (\u03b1,\u03b2) R and \u00b5 (\u03b1,\u03b2) R . The equivalence definitions of the (\u03b1, \u03b2) low and upper distribution reducts are given respectively based on \u03b7 (\u03b1,\u03b2) R and \u00b5 (\u03b1,\u03b2) R . The equivalence definitions of two distribution reducts can be use to design the stopping criteria of heuristic attribute"}, {"heading": "1 0.7527 \u00b1 0.0000 0.6902 \u00b1 0.0000 0.7147 \u00b1 0.0000", "text": ""}, {"heading": "2 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000 0.4720 \u00b1 0.0000", "text": ""}, {"heading": "3 0.6040 \u00b1 0.0000 0.6040 \u00b1 0.0000 0.6040 \u00b1 0.0000", "text": ""}, {"heading": "4 0.9563 \u00b1 0.0000 0.9563 \u00b1 0.0000 0.9563 \u00b1 0.0000", "text": ""}, {"heading": "5 0.8416 \u00b1 0.0000 0.8416 \u00b1 0.0000 0.8416 \u00b1 0.0000", "text": ""}, {"heading": "6 0.8551 \u00b1 0.0000 0.8551 \u00b1 0.0000 0.8551 \u00b1 0.0000", "text": ""}, {"heading": "7 0.9203 \u00b1 0.0000 0.9203 \u00b1 0.0000 0.9078 \u00b1 0.0000", "text": ""}, {"heading": "8 0.9528 \u00b1 0.0000 0.9528 \u00b1 0.0000 0.9528 \u00b1 0.0000", "text": ""}, {"heading": "9 0.9236 \u00b1 0.0000 0.9236 \u00b1 0.0000 0.9229 \u00b1 0.0000", "text": ""}, {"heading": "10 0.7490 \u00b1 0.0000 0.7520 \u00b1 0.0000 0.7520 \u00b1 0.0000", "text": "reduction algorithms. Additionally, we presented two modified fitness functions G\u03b7 (\u03b1,\u03b2) R and G\u00b5 (\u03b1,\u03b2) R by multiplying measures of granularity of partitions by \u03b7 (\u03b1,\u03b2) R and \u00b5 (\u03b1,\u03b2) R to evaluate the significance of attributes more effectively. On this basis, we developed two heuristic attribute reduction algorithms to find distribution reducts based on the addition-deletion method and deletion method. The monotonicity of fitness functions proposed guarantees that the reduction result is right as well. Finally, experiments on several real-life data sets are conducted to test the effectiveness of the proposed fitness functions and distribution reducts. The experimental results show that G\u03b7 (\u03b1,\u03b2) R and G\u00b5 (\u03b1,\u03b2) R are a more effective alternative than \u03b7 (\u03b1,\u03b2) R and \u00b5 (\u03b1,\u03b2) R for evaluating the significance of attributes. The experimental results also indicate that distribution reducts can achieve better performance in probabilistic rough set model and outperforms ranking based attribute reduction methods.\nThis investigation provides a new insight into the distribution reducts in probabilistic rough set model. In the future, the study work will be focused on designing heuristic attribute reduction algorithms to find other definitions of attribute reduct in probabilistic rough set model by studying monotonic fitness functions."}, {"heading": "Acknowledgements", "text": "This work is supported by the National Natural Science Foundation of China (Grant Nos. 61502419, 61272060, 61379114), and the Key Natural Science Foundation of Chongqing (No. CSTC2013jjB40003)."}], "references": [{"title": "Information entropy and granulation coentropy of partitions and coverings: A summary", "author": ["D. Bianucci", "G. Cattaneo"], "venue": "Transactions on Rough Sets X, 5656:15\u201366", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Entropy and co-entropy of partitions and coverings with applications to roughness theory", "author": ["G. Cattaneo", "D. Ciucci", "D. Bianucci"], "venue": "Granular Computing: At the Junction of Rough Sets and Fuzzy Sets, 224:55\u201377", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "A rough set approach to feature selection based on ant colony optimization", "author": ["Y.M. Chen", "D.Q. Miao", "R.Z. Wang"], "venue": "Pattern Recognitiion Letters, 31:226\u2013233", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "An uncertainty measure for incomplete decision tables and its applications", "author": ["J.H. Dai", "W.T. Wang", "Q. Xu"], "venue": "IEEE Transactions on Cybernetics, 43(4):1277\u20131289", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Approximations and uncertainty measures in incomplete information systems", "author": ["J.H. Dai", "Q. Xu"], "venue": "Information Sciences, 198:62\u201380", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature selection in decision systems based on conditional knowledge granularity", "author": ["T.Q. Deng", "C.D. Yang", "Q.H. Hu"], "venue": "International Journal of Computational intelligence systems, 4(4):655\u2013671", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Decision-theoretic rough set: A multicost strategy", "author": ["H.L. Dou", "X.B. Yang", "X.N. Song", "H.L. Yu", "W.Z. Wu", "J.Y. Yang"], "venue": "Knowledge-Based Systems, 000:1\u201313", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "sigkdd explorations. 11(1):10\u201318", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "A rough set approach for selecting clustering attribute", "author": ["T. Herawan", "M.M. Deris", "J.H. Abawajy"], "venue": "Knowledge-Based Systems, 23(3):220\u2013231", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Criteria for choosing a rough set model", "author": ["J.P. Herbert", "J.T. Yao"], "venue": "Computers and Mathematics with applications, 57(6):908\u2013918", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Game-theoretic rough sets", "author": ["J.P. Herbert", "J.T. Yao"], "venue": "Fundamenta Informaticae, 108(3):267\u2013286", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Several approaches to attribute reduction in variable precision rough set model", "author": ["M. Inuiguch"], "venue": "Modeling Decisions for Artificial Intelligence, pages 215\u2013 226", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Minimum cost attribute reduction in decision-theoretic rough set models", "author": ["X.Y. Jia", "W.H. Liao", "Z.M. Tang", "L. Shang"], "venue": "Information Sciences, 219(4):151C167", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Generalized attribute reduct in rough set theory", "author": ["X.Y. Jia", "L. Shang", "B. Zhou", "Y.Y. Yao"], "venue": "Knowledge-Based Systems, 51(4):453\u2013471", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Tang", "author": ["X.Y. Jia"], "venue": "W. H. Z. M. Liao, and L. Shang. On an optimization representation of decision-theoretic rough set model. International Journal Of Approximate Reasoning, 55(1):156\u2013166", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A relative decision entropy-based feature selection approach", "author": ["F. Jiang", "Y.F. Sui", "L. Zhou"], "venue": "Pattern Recognition, 48:2151\u20132163", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparative study of alternative types of knowledge reduction in inconsistent systems", "author": ["M. Kryszkiewicz"], "venue": "International Journal of Intelligence Systems, 16(1):105\u2013120", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Certain", "author": ["M. Kryszkiewicz"], "venue": "generalized decision, and membership distribution reducts versus functional dependencies in incomplete systems. In Proceeding of RSEISP2007, LNAI 4585, pages 162\u2013174", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Constructing importance measure of attributes in covering decision table", "author": ["F.C. Li", "Z. Zhang", "C.X. Jin"], "venue": "Knowledge-Based Systems, 76:228\u2013 239", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Non-monotonic attribute reduction in decision-theoretic rough sets", "author": ["H.X. Li", "X.Z. Zhou", "J.B. Zhao", "D. Liu"], "venue": "Fundamenta Informaticae, 126(4):415\u2013432", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "The information entropy", "author": ["J.Y. Liang", "Z.Z. Shi"], "venue": "rough entropy and knowledge granulation in rough set theory. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systemss, 12(1):37\u201346", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "A new measure of uncertainty based on knowledge granulation for rough sets", "author": ["J.Y. Liang", "J.H. Wang", "Y.H. Qian"], "venue": "Information Sciences, 179(4):458\u2013470", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Unifying variable precision and classical rough sets: Granular approach", "author": ["T.Y. Lin", "Y.R. Syau"], "venue": "Rough Sets and Intelligent Systems-Professor Zdzis law Pawlak in Memoriam, pages 365\u2013373. Springer", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "A multiple-category classification approach with decision-theoretic rough sets", "author": ["D. Liu", "T.R. Li", "H.X. Li"], "venue": "Fundamenta Informaticae, 115(2):173\u2013 188", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic model criteria with decisiontheoretic rough sets", "author": ["D. Liu", "T.R. Li", "D. Ruan"], "venue": "Information Sciences, 181(17):173\u2013178", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "A set covering based approach to find the reduct of variable precision rough set", "author": ["J.N.K. Liu", "Y.X. Hu", "Y.L. He"], "venue": "Information Sciences, 275(17):83\u2013100", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic rough set over two universes and rough entropy", "author": ["W.M. Ma", "B.Z. Sun"], "venue": "International Journal of Approximate Reasoning, 53(4):608\u2013 619", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Decision region distribution preservation reduction in decision-theoretic rough set model", "author": ["X.A. Ma", "G.Y. Wang", "H. Yu"], "venue": "Information Sciences, 278:614\u2013640", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Approaches to knowledge reduction based on variable precision rough set model", "author": ["J.S. Mi", "W.Z. Wu", "W.X. Zhang"], "venue": "Information Sciences, 159(3):255\u2013272", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "A heuristic algorithm for reduction of knowledge", "author": ["D.Q. Miao", "G.R. Hu"], "venue": "Chinese Journal of Computer Research & Development, 36(6):681\u2013 684", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1999}, {"title": "Relative reducts in consistent and inconsistent decision tables of the Pawlak rough set model", "author": ["D.Q. Miao", "Y. Zhao", "H.X. Li", "F.F. Xu"], "venue": "Information Sciences, 179(24):4140\u20134150", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Test-cost-sensitive attribute reduction", "author": ["F. Min", "H.P. He", "Y.H. Qian", "W. Zhu"], "venue": "Information Sciences, 181(22):4928\u20134942", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Attribute reduction of data with error ranges and test costs", "author": ["F. Min", "W. Zhu"], "venue": "Information Sciences, 211:48\u201367", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploring the boundary region of tolerance rough sets for feature selection", "author": ["N.M. Parthalain", "Q. Shen"], "venue": "Pattern Recognition, 42(5):655\u2013667", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "A distance measure approach to exploring the rough set boundary region for attribute reduction", "author": ["N.M. Parthalain", "Q. Shen", "R. Jensen"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 22(3):305\u2013317", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Rough sets", "author": ["Z. Pawlak"], "venue": "International Journal of Computer and Information Science, 11(5):341\u2013356", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1982}, {"title": "Rough sets: theoretical aspects of reasoning about data", "author": ["Z. Pawlak"], "venue": "Kluwer Academic Publishers, Boston", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1991}, {"title": "Rough sets: probabilistic versus deterministic approach", "author": ["Z. Pawlak", "S.K.M. Wong", "W. Ziarko"], "venue": "International Journal of Man-Machine Studies, 29(1):81\u201395", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1988}, {"title": "Granular Computing: Analysis and Design of Intelligent Systems", "author": ["W. Pedrycz"], "venue": "CRC Press/Francis Taylor, Boca Raton", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Combination entropoy and combination granulation in rough set theory", "author": ["Y.H. Qian", "J.Y. Liang"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 16(2):179\u2013193", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Positive approximation: An accelerator for attribute reduction in rough set theory", "author": ["Y.H. Qian", "J.Y. Liang", "W. Pedrycz", "C.Y. Dang"], "venue": "Artificial Intelligence, 174(9):597\u2013618", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Attribute reduction in decision-theoretic rough set models using genetic algorithm", "author": ["S. Chebrolu", "G. Sanjeevi S"], "venue": "In Proceeding of 2th International Conference on the Swarm, Evolutionary, and Memetic Computing", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "Rough sets", "author": ["Q. Shen", "R. Jensen"], "venue": "their extensions and applications. International Journal of Automation and Computing, 4(3):217\u2013228", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "An incremental approach to attribute reduction from dynamic incomplete decision systems in rough set theory", "author": ["W.H. Shu andW.B. Qian"], "venue": "Data & Knowledge Engineering,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "The investigation of the bayesian rough set model", "author": ["D. Slezak", "W. Ziarko"], "venue": "International Journal Approximate Reasoning, 40(1):81\u201389", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient attribute reduction from the viewpoint of discernibility", "author": ["S.H. Teng", "M. Lu", "A.F. Yang", "J. Zhang", "Y.J. Nian", "M. He"], "venue": "Information Sciences, 326:297\u2013314", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Dimensionality reduction based on rough set theory: A review", "author": ["K. Thangavela", "A. Pethalakshmi"], "venue": "Applied Soft Computing, 9(1):1\u201312", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Granular variable precision fuzzy rough sets with general fuzzy relations", "author": ["C.Y. Wang", "B.Q. Hu"], "venue": "Fuzzy Sets and Systems, 275:39\u201357", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "Research of reduct features in the variable precision rough set model", "author": ["J.Y. Wang", "J. Zhou"], "venue": "Neurocomputing, 72(10):2643\u20132648", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "Novel algorithms of attribute reduction with variable precision rough set model", "author": ["Y.Y. Yang", "D.G. Chen", "Z. Dong"], "venue": "Neurocomputing, 139:336\u2013344", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "A measurement theory view on the granularity of partitions", "author": ["Y.Y. Yao", "L.Z. Zhao"], "venue": "Information Sciences, 213:1\u201313", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}, {"title": "Attribute reduction in decision-theoretic rough set model", "author": ["Y.Y. Yao", "Y. Zhao"], "venue": "Information Sciences, 178(17):3356\u20133373", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2008}, {"title": "Discernibility matrix simplification for constructing attribute reducts", "author": ["Y.Y. Yao", "Y. Zhao"], "venue": "Information Sciences, 179(7):867\u2013882", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "On reduct construction algorithms", "author": ["Y.Y. Yao", "Y. Zhao", "J. Wang"], "venue": "Transactions on Computational Science II, volume 5150, pages 100\u2013117", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2008}, {"title": "A novel and better fitness evaluation for rough set based minimum attribute reduction problem", "author": ["D.Y. Ye", "Z.J. Chen", "S.L. Ma"], "venue": "Information Sciences, 222:413\u2013423", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2013}, {"title": "Approaches to knowledge reduuctions in inconsistent systems", "author": ["W.W. Zhang", "J.S. Mi", "W.Z. Wu"], "venue": "International Journal of Intelligent Systems, 18(9):989\u20131000", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2003}, {"title": "Region-based quantitative and hierarchical attribute reduction in the two-category decision theoretic rough set model", "author": ["X.Y. Zhang", "D.Q. Miao"], "venue": "Knowledge-Based Systems, 71:146\u2013161", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "A general definition of an attribute reduct", "author": ["Y. Zhao", "F. Luo", "S.K.M. Wong", "Y.Y. Yao"], "venue": "Proceeding of RSKT2007, volume 4481, pages 101\u2013108", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2007}, {"title": "A note on attribute reduction in the decision-theoretic rough set model", "author": ["Y. Zhao", "S.K.M. Wong", "Y.Y. Yao"], "venue": "Transactions on Rough Sets XIII, 6499:61\u201370", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2011}, {"title": "and Qi", "author": ["K. Zheng", "J. Hu", "Z.F. Zhan", "J. Ma"], "venue": "J. Jin. An enhancement for heuristic attribute reduction algorithm in rough set. 41(15):6748\u20136754", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2014}, {"title": "Variable precision rough set model", "author": ["W. Ziarko"], "venue": "Journal of Computer and System Sciences, 46(1):39\u201359", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1993}], "referenceMentions": [{"referenceID": 35, "context": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44].", "startOffset": 39, "endOffset": 43}, {"referenceID": 3, "context": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44].", "startOffset": 307, "endOffset": 330}, {"referenceID": 8, "context": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44].", "startOffset": 307, "endOffset": 330}, {"referenceID": 9, "context": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44].", "startOffset": 307, "endOffset": 330}, {"referenceID": 22, "context": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44].", "startOffset": 307, "endOffset": 330}, {"referenceID": 38, "context": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44].", "startOffset": 307, "endOffset": 330}, {"referenceID": 42, "context": "Rough set theory, introduced by Pawlak [37], is a valid mathematical theory that deals well with imprecise, vague and uncertain information, and it has become an area of active research spreading throughout many fields, such as machine learning, data mining, knowledge discovery, intelligent data analyzing [5, 10, 11, 24, 40, 44].", "startOffset": 307, "endOffset": 330}, {"referenceID": 46, "context": "Attribute reduction is often helpful to reduce the computational cost, save storage space, improve learning performance and prevent over-fitting [48].", "startOffset": 145, "endOffset": 149}, {"referenceID": 13, "context": "For definition of attribute reduct, one mainly emphasizes on selecting what kinds of properties of a given information system to keep unchanged or to improve [15, 33, 34].", "startOffset": 158, "endOffset": 170}, {"referenceID": 31, "context": "For definition of attribute reduct, one mainly emphasizes on selecting what kinds of properties of a given information system to keep unchanged or to improve [15, 33, 34].", "startOffset": 158, "endOffset": 170}, {"referenceID": 32, "context": "For definition of attribute reduct, one mainly emphasizes on selecting what kinds of properties of a given information system to keep unchanged or to improve [15, 33, 34].", "startOffset": 158, "endOffset": 170}, {"referenceID": 36, "context": "For example, Pawlak [38] defined a relative reduct that keeps the quality of classification or classification positive region unchanged.", "startOffset": 20, "endOffset": 24}, {"referenceID": 29, "context": "[31] constructed the mutual information based reducts to extract relevant attribute sets that preserve the mutual information of a given decision table.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Kryszkiewicz [18, 19] investigated and compared the relationships among possible reduct, approximate reduct, \u03bc-reduct, \u03bc-decision reduct and generalized decision reduct.", "startOffset": 13, "endOffset": 21}, {"referenceID": 17, "context": "Kryszkiewicz [18, 19] investigated and compared the relationships among possible reduct, approximate reduct, \u03bc-reduct, \u03bc-decision reduct and generalized decision reduct.", "startOffset": 13, "endOffset": 21}, {"referenceID": 55, "context": "[58] introduced the maximum distribution reduct, which preserves all maximum decision rules in a decision table.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[7] proposed the notions of conditional knowledge granularity to reflect the relationship between conditional attributes and decision attribute, and defined an attribute reduct based on conditional knowledge granularity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[17] proposed a new model of relative decision entropy by combining roughness with the degree of dependency, and used it as the reduction criterion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "For attribute reduction algorithm, one mainly focuses on designing the effective reduct construction methods for finding a specific type of reduct [45, 47, 62].", "startOffset": 147, "endOffset": 159}, {"referenceID": 45, "context": "For attribute reduction algorithm, one mainly focuses on designing the effective reduct construction methods for finding a specific type of reduct [45, 47, 62].", "startOffset": 147, "endOffset": 159}, {"referenceID": 59, "context": "For attribute reduction algorithm, one mainly focuses on designing the effective reduct construction methods for finding a specific type of reduct [45, 47, 62].", "startOffset": 147, "endOffset": 159}, {"referenceID": 30, "context": "[32] discussed the structures of discernibility matrices for three different reducts, including region preservation reduct, decision preservation reduct and relationship preservation reduct.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "Yao and Zhao [55] introduced a reduct construction method based on discernibility matrix simplification.", "startOffset": 13, "endOffset": 17}, {"referenceID": 40, "context": "[42] proposed the concept of positive approximation for accelerating heuristic attribute reduction algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35, 36] proposed a distance measure-based attribute reduction algorithm by considering the proximity of objects in the boundary region to those in the lower approximation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 34, "context": "[35, 36] proposed a distance measure-based attribute reduction algorithm by considering the proximity of objects in the boundary region to those in the lower approximation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[4] proposed a novel attribute reduction algorithm based on ant colony optimization for finding", "startOffset": 0, "endOffset": 3}, {"referenceID": 54, "context": "[57] introduced a novel fitness function for designing the particle swarm optimization-based and genetic-based attribute reduction algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Relative to the classical rough set model, probabilistic rough set model allows certain acceptable levels of errors by using the threshold parameters [8, 26, 28, 49].", "startOffset": 150, "endOffset": 165}, {"referenceID": 24, "context": "Relative to the classical rough set model, probabilistic rough set model allows certain acceptable levels of errors by using the threshold parameters [8, 26, 28, 49].", "startOffset": 150, "endOffset": 165}, {"referenceID": 26, "context": "Relative to the classical rough set model, probabilistic rough set model allows certain acceptable levels of errors by using the threshold parameters [8, 26, 28, 49].", "startOffset": 150, "endOffset": 165}, {"referenceID": 47, "context": "Relative to the classical rough set model, probabilistic rough set model allows certain acceptable levels of errors by using the threshold parameters [8, 26, 28, 49].", "startOffset": 150, "endOffset": 165}, {"referenceID": 37, "context": "5 probabilistic rough set model [39], decision-theoretic rough set model [52], variable precision rough set model [63], Bayesian rough set model [46] and game rough set model [12].", "startOffset": 32, "endOffset": 36}, {"referenceID": 60, "context": "5 probabilistic rough set model [39], decision-theoretic rough set model [52], variable precision rough set model [63], Bayesian rough set model [46] and game rough set model [12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 44, "context": "5 probabilistic rough set model [39], decision-theoretic rough set model [52], variable precision rough set model [63], Bayesian rough set model [46] and game rough set model [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 10, "context": "5 probabilistic rough set model [39], decision-theoretic rough set model [52], variable precision rough set model [63], Bayesian rough set model [46] and game rough set model [12].", "startOffset": 175, "endOffset": 179}, {"referenceID": 11, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 12, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 19, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 28, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 48, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 51, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 56, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 58, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 60, "context": "Although attribute reduction in probabilistic rough set model has gained considerable attention in recently, these works generally focus on the study of definition of attribute reduct [13, 14, 21, 30, 50, 54, 59, 61, 63].", "startOffset": 184, "endOffset": 220}, {"referenceID": 11, "context": "For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43].", "startOffset": 118, "endOffset": 134}, {"referenceID": 28, "context": "For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43].", "startOffset": 118, "endOffset": 134}, {"referenceID": 49, "context": "For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43].", "startOffset": 118, "endOffset": 134}, {"referenceID": 58, "context": "For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43].", "startOffset": 118, "endOffset": 134}, {"referenceID": 14, "context": "For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43].", "startOffset": 177, "endOffset": 189}, {"referenceID": 25, "context": "For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43].", "startOffset": 177, "endOffset": 189}, {"referenceID": 41, "context": "For attribute reduction algorithm, the research efforts mainly concentrate on the discernibility matrix based methods [13, 30, 51, 61] and stochastic optimization based methods [16, 27, 43].", "startOffset": 177, "endOffset": 189}, {"referenceID": 28, "context": "in [30].", "startOffset": 3, "endOffset": 7}, {"referenceID": 53, "context": "Generally speaking, the most common heuristic attribute reduction algorithms are the addition-deletion method and the deletion method [56].", "startOffset": 134, "endOffset": 138}, {"referenceID": 23, "context": "[25, 52].", "startOffset": 0, "endOffset": 8}, {"referenceID": 4, "context": "Given an information system IS = (U,A), P,Q \u2286 A, one can define a partial relation \u227a on 2 as follows [6]: P\u227aQ \u21d4 \u2200x \u2208 U, [x]P \u2286 [x]Q.", "startOffset": 101, "endOffset": 104}, {"referenceID": 20, "context": "[22] Given a decision table DT = (U,C\u222aD), R \u2286 C and U/D = {Y1, Y2, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Given a decision table DT = (U,C \u222a D), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1, R \u2286 C and U/D = {Y1, Y2, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "An attribute reduct is defined as a subset of attributes that are jointly sufficient and individually necessary for preserving or improving a particular property of the given information system [54].", "startOffset": 194, "endOffset": 198}, {"referenceID": 57, "context": "[60] Given an information system IS = (U,A), R \u2286 A and consider a certain property P, which can be represented by an evaluation function e : 2 \u2192 (L,\u227a), of IS.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "There are a huge amount of known properties, such as a description of an object relation [37], partitions of an information system [60], a classification of a set of concepts [38], where the classification of a set of concepts is the most common property in rough set theory.", "startOffset": 89, "endOffset": 93}, {"referenceID": 57, "context": "There are a huge amount of known properties, such as a description of an object relation [37], partitions of an information system [60], a classification of a set of concepts [38], where the classification of a set of concepts is the most common property in rough set theory.", "startOffset": 131, "endOffset": 135}, {"referenceID": 36, "context": "There are a huge amount of known properties, such as a description of an object relation [37], partitions of an information system [60], a classification of a set of concepts [38], where the classification of a set of concepts is the most common property in rough set theory.", "startOffset": 175, "endOffset": 179}, {"referenceID": 28, "context": "However, the decision rules derived from the reduct preserving probabilistic positive region maybe in conflict with those derived from the original decision table because of non-monotonicity of probabilistic positive region with respect to the set inclusion of attribute sets [30].", "startOffset": 276, "endOffset": 280}, {"referenceID": 28, "context": "[30] presented the concepts of distribution reducts based on variable precision rough set model which is a typical probabilistic rough set model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] Given a decision table DT = (U,C \u222a D), for any 0 \u2264 \u03b2 < \u03b1 \u2264 1 and R \u2286 C, we have", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "If the fitness function is not monotonic regarding the set inclusion of attribute sets, individually necessary condition must consider all subsets of a reduct R to make sure it is a minimal set [14, 61].", "startOffset": 194, "endOffset": 202}, {"referenceID": 58, "context": "If the fitness function is not monotonic regarding the set inclusion of attribute sets, individually necessary condition must consider all subsets of a reduct R to make sure it is a minimal set [14, 61].", "startOffset": 194, "endOffset": 202}, {"referenceID": 53, "context": "[56] have summarized three groups of heuristic attribute reduction algorithms based on the addition-deletion method, the deletion method and the addition method, where the addition-deletion method based algorithm and the deletion method based algorithm are two most widely used heuristic attribute reduction algorithms by the rough set community.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "It is worth pointing out that the monotonicity of the fitness function in Definition 5 with respect to the set inclusion of attribute sets is very important for the completeness of Algorithm 1 and Algorithm 2 [29].", "startOffset": 209, "endOffset": 213}, {"referenceID": 15, "context": "Moreover, the fitness functions for evaluating the significance of attributes should also satisfy the monotonicity with respect to the set inclusion of attribute sets and provide enough precision to sort the attributes more effectively [17, 20].", "startOffset": 236, "endOffset": 244}, {"referenceID": 18, "context": "Moreover, the fitness functions for evaluating the significance of attributes should also satisfy the monotonicity with respect to the set inclusion of attribute sets and provide enough precision to sort the attributes more effectively [17, 20].", "startOffset": 236, "endOffset": 244}, {"referenceID": 50, "context": "Currently, Yao and Zhao [53] provided a unified framework for measures of granularity of partitions by considering the expected granularity of blocks in a partition.", "startOffset": 24, "endOffset": 28}, {"referenceID": 50, "context": "[53] Suppose U is finite and nonempty universe.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[53] Suppose \u03c0 = {X1, X2, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[2, 3, 22] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, .", "startOffset": 0, "endOffset": 10}, {"referenceID": 1, "context": "[2, 3, 22] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, .", "startOffset": 0, "endOffset": 10}, {"referenceID": 20, "context": "[2, 3, 22] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, .", "startOffset": 0, "endOffset": 10}, {"referenceID": 20, "context": "[22, 23] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, .", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[22, 23] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, .", "startOffset": 0, "endOffset": 8}, {"referenceID": 39, "context": "[41] Given an information system IS = (U,A), R \u2286 A and U/R = {X1, X2, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "All preprocessing methods were implemented by using WEKA filters [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 58, "context": "the (\u03b1, \u03b2) low distribution reduct with three traditional definitions of attribute reduct, including the qualitative positive region-preserved reduct (QLPRP) [61], the quantitative positive region-preserved reduct (QNPRP) [61] and the positive region extension reduct (EXDPR) [21], in probabilistic rough set model.", "startOffset": 158, "endOffset": 162}, {"referenceID": 58, "context": "the (\u03b1, \u03b2) low distribution reduct with three traditional definitions of attribute reduct, including the qualitative positive region-preserved reduct (QLPRP) [61], the quantitative positive region-preserved reduct (QNPRP) [61] and the positive region extension reduct (EXDPR) [21], in probabilistic rough set model.", "startOffset": 222, "endOffset": 226}, {"referenceID": 19, "context": "the (\u03b1, \u03b2) low distribution reduct with three traditional definitions of attribute reduct, including the qualitative positive region-preserved reduct (QLPRP) [61], the quantitative positive region-preserved reduct (QNPRP) [61] and the positive region extension reduct (EXDPR) [21], in probabilistic rough set model.", "startOffset": 276, "endOffset": 280}, {"referenceID": 7, "context": "All approaches were implemented based on the WEKA data mining software package [9], where the classifiers were implemented with default settings.", "startOffset": 79, "endOffset": 82}], "year": 2015, "abstractText": "Attribute reduction is one of the most important topics in rough set theory. Heuristic attribute reduction algorithms have been presented to solve the attribute reduction problem. It is generally known that fitness functions play a key role in developing heuristic attribute reduction algorithms. The monotonicity of fitness functions can guarantee the validity of heuristic attribute reduction algorithms. In probabilistic rough set model, distribution reducts can ensure the decision rules derived from the reducts are compatible with those derived from the original decision table. However, there are few studies on developing heuristic attribute reduction algorithms for finding distribution reducts. This is partly due to the fact that there are no monotonic fitness functions that are used to design heuristic attribute reduction algorithms in probabilistic rough set model. The main objective of this paper is to develop heuristic attribute reduction algorithms for finding distribution reducts in probabilistic rough set model. For one thing, two monotonic fitness functions are constructed, from which equivalence definitions of distribution reducts can be obtained. For another, two modified monotonic fitness functions are proposed to evaluate the significance of attributes more effectively. On this basis, two heuristic attribute reduction algorithms for finding distribution reducts are developed based on addition-deletion method and deletion method. In particular, the monotonicity of fitness functions guarantees the rationality of the proposed heuristic attribute reduction algorithms. Results of experimental analysis are included to quantify the effectiveness of the proposed fitness functions and distribution reducts.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}