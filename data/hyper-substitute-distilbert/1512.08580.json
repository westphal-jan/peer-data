{"id": "1512.08580", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Dec-2015", "title": "A Simple Baseline for Travel Time Estimation using Large-Scale Trip Data", "abstract": "the finer probability than large - gathering trajectory data around the world provides rich information for predict behavior of urban models. for example, south york city taxi limousine commission regularly releases co - destination information about trips in the taxis they regulate. vehicle data generates information about traffic patterns, and thus indicates trajectory patterns defining urban pattern - - what will traffic between traffic organizations ease upward at a fast date and time in the future? additional popular trend methods try to outdo relational approach in dependence on complexity amid symbolic sophistication. in such spirit of \" big numbers beats algorithms \", we determine a typical simple baseline which outperforms state - of - the - peace data, including bing maps by baidu maps ( whose apis warrant template page development ). such a poor direction estimation baseline has attracted niche uses, such as navigation ( combining travel time estimates goals include as useful projections for a search variants for parameter finding ) and trip finding ( which uses fewer hours for prospective destinations to arbitrary associated pattern markers to pick appropriate automobile ).", "histories": [["v1", "Mon, 28 Dec 2015 20:31:38 GMT  (5294kb,D)", "http://arxiv.org/abs/1512.08580v1", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CY", "authors": ["hongjian wang", "zhenhui li", "yu-hsuan kuo", "dan kifer"], "accepted": false, "id": "1512.08580"}, "pdf": {"name": "1512.08580.pdf", "metadata": {"source": "CRF", "title": "A Simple Baseline for Travel Time Estimation using Large-Scale Trip Data", "authors": ["Hongjian Wang", "Zhenhui Li", "Yu-Hsuan Kuo", "Dan Kifer"], "emails": ["hxw186@ist.psu.edu,", "jessieli@ist.psu.edu,", "yzk5145@cse.psu.edu", "dkifer@cse.psu.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION The positioning technology is widely adopted into our daily life. The on-board GPS devices track the operation of vehicles and provide navigation service, meanwhile a significant amount of trajectory data are collected. For example, the New York City Taxi & Limousine Commission has made all the taxi trips in 2013 public under the Freedom of Information Law (FOIL) [1]. The dataset contain more than 173 million taxi trips \u2013 almost half million trips per day. These trajectory data can help us in a lot of ways. In the macro scale, we can study the urban flow, while in the micro level, we can predict the travel time for individual users.\nIn this paper, we focus on a fundamental problem, which is the travel time estimation \u2013 predicting the travel time between an origin and a destination. Existing big data methods try to outdo each other in terms of complexity and algorithmic sophistication. In the spirit of \u201cbig data beats algorithms\u201d, we present a very simple baseline which outperforms state-of-theart approaches, including Bing Maps [2] and Baidu Maps [3]. Figure 1 gives an example of a road network and three given trajectories o,p, and q. Suppose we want to estimate the travel time from point A to I using these three historical trajectories. Most existing methods of travel time estimation use the route-based framework to estimate the travel time [4], [5], [6], [7]. At the first step, they usually find a specific route according to some criteria, examples are shortest route and fastest route. In the example, suppose we identify the route\nA\u2192 B \u2192 E \u2192 H \u2192 I as the optimal route. The second step is to estimate the travel time for each segment of the chosen route. For example, the travel time of A \u2192 B is estimated from o and p, since both trajectories contain this segment. Similarly, B \u2192 E and E \u2192 H are estimated from p, and H \u2192 I can be estimated from p and q.\nThe route-based travel time estimation faces several issues. 1) GPS data have limited precision. Usually the GPS samples are not perfectly aligned with the road segments. For a historic trip, the exact path is very hard to recover, and various intuitions are employed to guess the real route. For example, in Figure 1 trip p could also take the route ADEHI , if p2 is mapped onto the road segment DE. 2) Trajectory data are sparse. Even with millions of trip observations, there are a lot of road segments having no GPS samples available at all, due to the non-uniform spatial distribution of GPS samples. In the Manhattan, we observe 90% of taxi pick-up and dropoff concentrate within middle and south part. The inference of travel time is not accurate due to limited number of trajectories covering the road segment. In addition to the spatial sparseness, the trajectory data are temporally sparse too. Even if a road segment is covered by a few trajectories, these trajectories may not be sufficient to estimate the dynamic travel time that varies all the time.\nTo address the sparsity issue, various methods are proposed, which however suffer from the complexity issue. A probabilistic framework to estimate the probability distribution of\nar X\niv :1\n51 2.\n08 58\n0v 1\n[ cs\n.L G\n] 2\n8 D\nec 2\n01 5\n2 travel time is proposed in [8], where a dynamic Bayesian network is employed to learn the dependence of travel time on road congestion. Learning the Bayesian network is NP-hard, and time complexity is exponential in the number of hidden variables. A more recent work [9] models the travel time over each road segment during different periods for different users as a tensor. And it addresses the missing observations issue with tensor decomposition, which is known to be NP-hard [10]. Is all the complexity necessary or can a simple method, more reliant on data, perform just as well or even better?\nIn this paper, we propose a simple baseline method to predict the travel time. Since we have a huge amount of historical trips, we can avoid finding a specific route by directly using all trips with similar origin and destination to estimate the query trip. We call our method as neighbor-based method. The biggest benefit of our approach is its simplicity. By analogy, consider the area of multidimensional database indexing and similarity search. While many sophisticated indexing algorithms have been proposed, many are outperformed by a simple sequential search of the data [11]. In the case of navigation and trip time estimation, there are no suitable baselines that set the bar for accuracy and help determine whether additional algorithmic complexity is justified. Thus, our method fills this void while, at the same time, being more accurate than the current state of the art.\nTo explain the intuition behind our neighbor-based approach and the reasoning of why it should outperform sophisticated competitors, consider the following scenario. Suppose we are interested in how long it takes to travel from A to I at 11:43 p.m. on a Saturday night, given the information in Figure 1. If we had hundreds of trips from A to I starting exactly at 11:43 p.m. on Saturday, the estimation problem would be easy: just use the average of all those trips. Clearly, if such data existed, then this approach would be more accurate than the routebased method, which reconstructs a path from A to I , estimates the time along each segment, and finally adds them up. This latter approach requires multiple accurate time estimates and any errors will naturally accumulate.\nOn the other hand, the simple average of neighboring trips approach suffers from sparsity issue: although taxi data contain hundreds of millions of taxi trips, very few of them share the similar combination of origin, destination, and time. To address this issue, we propose to model the dynamic traffic conditions as a temporal speed reference, which enables us to average all trips from A to I during different times. More specifically, for any two trips p and q from different time but having similar origin and destination, we can estimate the travel time of p by adjusting the travel time of q with the temporal speed reference. For example, if historical trip p is at 6 p.m. and query trip q is at 1 p.m.; and the data tells us travel time at 6 p.m. is usually twice slower than the travel time at 1 p.m., we could estimate travel time of p using that of q with a scaling factor of 2.\nWe conduct experiments on two large datasets from different countries (NYC in US and Shanghai in China). We evaluate our method using more than 150 million Manhattan trips, and our method is able to outperform the Bing Maps [2] by 33% (even when Bing uses traffic conditions corresponding to the\ntime and the day of week of target trips)1. Since NYC taxi data only contain the information on endpoints of the trips (i.e., pickup and drop off locations), to compare our method with route-based method [9], we use Shanghai taxi data, where more than 5 million trips with complete information of trajectories are available. On this dataset, our method also significantly outperforms the state-of-the-art route-based method [9] by 19% and Baidu Maps2 [3] by 17%. It is noteworthy that our method runs 40 times faster than state-of-the-art method [9].\nOne may argue that our simple method does not provide the specific route information, which limits its application. While this is true, it does not keep our method from being a good baseline for travel time estimation problem. Besides, there are many real scenarios where the route information is not important. One example is the trip planning [13]. For example, a person will take a taxi to catch a flight, which leaves at 5pm. Since he is not driving, the specific route is less of a concern and he only needs to know the trip time. The second example is to estimate the city commuting efficiency [14] [15] in a future time. In the field of urban transportation research, the commuting efficiency measure is a function of observed commuting time and expected minimum commuting time of given origin and destination areas. To estimate the commuting efficiency of a city, the ability to predict the pairwise travel time for any two areas is crucial. In this scenario, the specific path between two areas is not important, either.\nIn summary, the contributions of this paper are as follows: \u2022 We propose to estimate the travel time using neighboring\ntrips from the large-scale historical data. To the best of our knowledge, this is the first work to estimate travel time without computing the routes. \u2022 We improve our neighbor-based approach by addressing the\ndynamics of traffic conditions and by filtering the bad data records. \u2022 Our experiments are conducted on large-scale real data.\nWe show that our method can outperform state-of-the-art methods as well as online map services (Bing Maps and Baidu Maps). The rest of the paper is organized as follows. Section II reviews related work. Section III defines the problem and provides an overview of the proposed approach. Sections IV and V discuss our method to weight the neighboring trips and our outlier filtering method, respectively. We present our experimental results in Section VI and conclude the paper in Section VII."}, {"heading": "II. RELATED WORK", "text": "To the best of our knowledge, most of the studies in literature focus on the problem of estimating travel time for a route (i.e., a sequence of locations). There are two types of approaches for this problem: segment-based method and pathbased method.\nSegment-based method. A straightforward travel time estimation approach is to estimate the travel time on individual\n1Google Maps API rate caps prevent proper large-scale comparisons. 2Baidu Maps is more suitable to query trips in China due to restrictions on\ngeographic data in China [12].\n3 road segments first and then take the sum over all the road segments of the query route as the travel time estimation. There are two types of data that are used to estimate the travel time on road segments: loop detector data and floating-car data (or probe data) [16]. Loop detector can sense whether a vehicle is passing above the sensor. Various methods [17], [18], [19], [20] have been proposed to infer vehicle speed from the loop sensor readings and then infer travel time on individual road segments. Loop detector data provide continuous speed on selected road segments with sensors embedded.\nFloating cars collect timestamped GPS coordinates via GPS receivers on the cars. The speed of individual road segments at a time t can be inferred if a floating car is passing through the road segment at that time point [21], [22]. Due to the low GPS sampling rate, a vehicle typically goes through multiple road segments between two consecutive GPS samplings. A few methods have been proposed to overcome the low sampling rate issue [23], [24], [8], [25], [26]. Another issue with floating-car data is data sparsity \u2013 not all road segments are covered by vehicles all the time. Wang et al. [9] proposes to use matrix factorization to estimate the missing values on individual road segments. In our problem setting, we assume the input is a special type of the floating-car data, where we only know the origin and destination points of the trips. Since the trips could vary from a few minutes to an hour, it will be much more challenging to give an accurate speed estimation on individual road segments using such limited information.\nPath-based method. Segment-based method does not consider the transition time between road segments such as waiting for traffic lights and making left/right turns. Recent research works start considering the time spent on intersections [27], [28], [8], [29]. However, these methods do not directly use sub-paths to estimate travel time. Rahmani et al. [30] first proposes to concatenate sub-paths to give a more accurate estimation of the query route. Wang et al. [9] further improves the path-based method by first mining frequent patterns [31] and then concatenating frequent sub-paths by minimizing an objective function which balances the length and support of the sub-paths. Our proposed solution can be considered as an extreme case of the path-based method, where we use the travel time of the full paths in historical data to estimate the travel time between an origin and a destination. Full paths are better at capturing the time spent on all intersections along the routes, assuming we have enough full paths (i.e., a reasonable support).\nOrigin-destination travel time estimation. In our problem setting, instead of giving a route as the input query, we only take the origin and destination as the input query. To the best of our knowledge, we are the first to directly work on such origin-destination (OD) travel time queries. One could argue that, we could turn the problem into the trajectory travel time estimation by first finding a route (e.g., shortest route or fastest route) [4], [5], [6], [7] and then estimating the travel time for that route. This alternative solution is similar to those provided by online maps services such as Google Maps [32], Bing Maps [2] or Baidu Maps [3], where users input a starting point and an ending point, and the service generates a few\nroute options and their corresponding times. Such solution eventually leads to travel time estimation of a query route. However, the historical trajectory data may only have the starting and ending points of the trips, such as the public NYC taxi data [1] used in our experiment. Different from the data used in literature which has the complete trajectories of floating cars, such data have limited information and are not suitable for the segment-based method or path-based method discussed above. In addition, route-based approach introduces more expensive computations because we need to find the route first and then compute travel time on segments or subpaths. More importantly, such expensive computation does not necessarily lead to better performance compared with our method using limited information, as we will demonstrate in the experiment section."}, {"heading": "III. PROBLEM DEFINITION", "text": "A trip pi is defined as a 5-tuple (oi, di, si, li, ti), which consists of the origin location oi, the destination location di and the starting time si. Both origin and destination locations are GPS coordinates. We use li and ti to denote the distance and travel time for this trip, respectively. Note that, here we assume the intermediate locations of the trips are not available. In the real applications, it is quite possible that we can only obtain such limited information about trips due to privacy concerns and tracking costs. To the best of our knowledge, the largest-scale public trip data we can obtain are recently released NYC taxi data [1]. And due to the privacy concerns of passengers and drivers, no information of intermediate GPS points is released (or even recorded).\nProblem 1 (OD Travel Time Estimation): Suppose we have a database of trips, D = {pi}Ni=1. Given a query q = (oq, dq, sq), our goal is to estimate the travel time tq with given origin oq , destinationdq , and departure time sq , using the historical trips in D."}, {"heading": "A. Approach Overview", "text": "An intuitive solution is that we should find similar trips as the query trip q and use the travel time of those similar trips to estimate the travel time for q. The problem can be decomposed to two sub-problems: (1) how to define similar trips; and (2) how to aggregate the travel time of similar trips. Here, we name similar trips as neighboring trips (or simply neighbors) of query trip q. Note that aggregating is not trivial because of varying starting times and traffic conditions.\nWe call trip pi a neighbor of trip q if the origin (and destination) of pi are spatially close to the origin (and destination) of q. Thus, the set of neighbors of q is defined as:\nN (q) = {pi \u2208 D|dist(oi, oq) \u2264 \u03c4 and dist(di, dq) \u2264 \u03c4}, (1) where dist() is the Euclidean distance of two given points. With the definition of neighbors, a baseline approach is to take the average travel time of these trips as the estimation:\nt\u0302q = 1 |N (q)| \u2211\npi\u2208N (q)\nti. (2)\n4 Here we also point out an alternative definition of neighbors. Specifically, instead of using a hard distance threshold \u03c4 , we could consider a wider range of trips with weights based on the origin distance (i.e., dist(oi, oq)) and destination distance (i.e., dist(di, dq)). For example, let wi \u221d 1\ndist(oi, oq) denote\nthe weight coefficient for neighboring trip pi, we have:\nt\u0302q =\n\u2211 pi\u2208N (q) wi \u00b7 ti\u2211\npi\u2208N (q) wi . (3)\nHowever, such definition will make the computation more expensive since we typically need to consider more neighbors. In addition, we empirically find out that this alternative definition does not necessarily lead to a better performance (refer to Section VI-D4). So we adopt our original definition of neighbors using a hard distance threshold.\nWith the model in Equation 2, there are two major issues with the baseline approach. First, we need to model the dynamic traffic condition across different time. For each neighboring trip pi of q we define the scaling factor si calculated from the speed reference, so that siti \u2248 tq . And our estimation model becomes\nt\u0302q = 1 |N (q)| \u2211\npi\u2208N (q)\nsiti. (4)\nSecond, the data could contain a large portion of noises. It is critical to filter those outliers. Otherwise, the estimation for the query trip will be severely impacted by the noises. Section IV will discuss how to calculate scaling factors to the neighbors and Section V will present our outlier filtering technique."}, {"heading": "IV. CAPTURING THE TEMPORAL DYNAMICS OF TRAFFIC CONDITIONS", "text": "As we discussed earlier, it is not appropriate to simply take the average of all the neighbors of q because of traffic conditions vary at different times. Figure 2 shows the average speed of all NYC taxi trips at different times in a week. Apparently, the average speed is much faster at the midnight compared with the speed during the peak hours. Thus, if we wish to estimate the travel time of a trip q at 2 a.m. using a neighboring trip p at 5 p.m., we should proportionally decrease the travel time of p, because a 2 a.m. trip is often much faster than a 5 p.m. trip.\nNow the question is, how can we derive a temporal scaling reference to correspondingly adjust travel time on the neighboring trips. We first define the scaling factor of a neighboring\n0 1 2 3 0\n0.5\n1\n1.5\n2\n2.5\n3\nTravel time ratio tq/ti\nS p ee d ra ti o v i /v\nq\nOne pair Linear fit y = x\n(a) tq ti \u2248 vi vq\n0 1 2 3 0\n0.5\n1\n1.5\n2\n2.5\n3\nSpeed ratio vi/vq. R 2 = 0.268\nR ef er en ce\nra ti o V (s\ni) /V\n(s q )\nOne pair Linear fit y = x\n(b) vi vq \u2248 V (si) V (sq)\nFig. 3. The validity of our assumptions. We randomly sample a set of neighboring trip pairs, and calculate the different ratios for each pair. We plot each trip pair as a blue point. The solid green line is the linear fit of all the points. The dotted red line is y = x. In (a), we can see that fitted line has a slope approximating 1, which verifies our assumption \u2200pi \u2208 N (q), li ' lq . Similarly, in (b), we conclude that ratio of speed reference approximates the ratio of actual speed.\ntrip pi on query trip q as:\nsi = tq ti . (5)\nOne way to estimate si is using the speed of pi and q. Let vi and vq be the speed of trip pi and q. Since we pick a small \u03c4 to extract neighboring trips of q, it is safe to assume that \u2200pi \u2208 N (q), li ' lq. In Figure 3(a), on a sample set of neighboring trip pairs, we plot the inverse speed ratio against the travel time ratio. The solid line shows the actual relation between\ntq ti and vi vq , which is very close to the dotted line y =\nx, which means that two ratios are approximately equivalent. With this assumption, we have:\nsi = tq ti = lq/vq li/vi \u2248 vi vq .\nHowever, vq is unknown, so we need to estimate vi vq . Since the average speed of all trips are stable and readily available, we try to build a bridge from the actual speed ratio to the corresponding average speed ratio for any given two trips. One solution is to assume the ratio between vq and vi approximately equals to the ratio between the average speed of all trips at sq and si. Formally, let V (s) denote the average speed of all trips at timestamp s, we have an approximation of si as\nsi \u2248 vi vq \u2248 V (si) V (sq) . (6)\nThis assumption is validated in Figure 3(b). For the sample set of neighboring trip pairs, the average speed ratio is plotted against actual speed ratio. Since the points are approximately distributed along the line y = x, we conclude that average speed ratio is a feasible approximation of the scaling factor. We notice that the fitted line has a slope less than 1, which is mainly due to the anomaly in individual trips. Specifically, the speed of individual trips v has a higher variance and some\n5 0 5 10 15 20 25 0 200 400 600 800 1000 1200 1400\nIndex of neighboring trips\nT ri p t ra\nv e l ti m\ne (\ns e c o n d s )\nScaled duration siti Original duration ti Target duration tq\n(a)\n0 5 10 15 20 25 0\n500\n1000\n1500\nIndex of neighboring trips\nT ri p t ra\nv e l ti m\ne (\ns e c o n d s )\nScaled duration siti Original duration ti Target duration tq\n(b)\nFig. 4. Two specific cases demonstrate the effectiveness of the scaling factor. The actual travel time ti of neighboring trips is plotted as blue circle, while the scaled travel time siti is plotted as red triangle. The horizontal green line shows the travel time of target trip q. In (a), the actual travel times of historical trips have very large variance, because they are from different time slots with distinct traffic conditions. The scaling factor successfully reduces the variance in the actual travel time ti. In (b), the target trip q occurred in rush hour, and thus tq is much longer than most of its neighboring trips. The scaling factor successfully fill in the gap between historical trips and q.\nindividual trip pairs have an extreme large ratio. On the other hand, the average speed V (s) has a smaller variance and the ratio is confined to a more reasonable range [0.5, 2].\nNext, we show the effectiveness of the scaling factor calculated from Equation 6. In Figure 4, we present two specific trips to demonstrate the intuition of how the scaling factor works. For each trip, we retrieve its historical neighboring trips, and then plot the actual travel time in circle together with the scaled travel time in triangle. From the Figure 4(a), we can see that the scaling factor helps reduce the variance in the travel time of neighboring trips. In Figure 4(b), trip q happened in rush hour, which takes longer time than most of its neighboring trips. This gap is successfully filled in by the scaling factor.\nConsidering such scaling factors using temporal speeds as the reference, we can estimate the travel time of q using the neighboring trips as follows:\nt\u0302q = 1 |N (q)| \u2211\npi\u2208N (q)\nti \u00b7 V (si)\nV (sq) . (7)\nWe show the effectiveness of this predictor in Figure 5. Each point in the figure is a target trip. The prediction is plotted against the actual trip travel time. We can see that the prediction is close to the actual value. This indirectly implies the validity of assumptions we made previously.\nIn order to compute the average speed V (si), we need to collect all the trips in D which started at time si. However, for the query trip q, the starting time sq may be the current time or some time in the future. Therefore, no trips in D have the same starting time as q. In the following, we discuss two approaches to predict V (sq) using the available data."}, {"heading": "A. Relative Temporal Speed Reference", "text": "In this section, we assume that V (s) exhibits a regular daily or weekly pattern. We fold the time into a relative time window Trela = {1, 2, \u00b7 \u00b7 \u00b7 , T}, where T is the assumed periodicity.\n0 500 1000 1500 2000 2500 0\n500\n1000\n1500\n2000\n2500\nTarget trip travel time tq (seconds)\nP re d ic ti on\nt\u0302 q (s ec on\nd s)\nOne trip Linear fit y = x\nFig. 5. Estimated travel time against actual travel time. Each point is a trip, where the estimation t\u0302q is plotted against tq . The green line is the linear fitting of the points, which is closer to the red line y = x. This means the prediction is close to the actual value.\n8am Sun 8am Mon 8am Tue 8am Wed 8am Thu 8am Fri 8am Sat 5\n10\n15\n20\n25\nStarting time of the trip\nS p\ne e\nd r\ne fe\nre n\nc e\n( m\nile /h\n)\nAbsolute speed reference Relative speed reference\nFig. 6. Comparison between the absolute speed reference and the relative speed reference in the Christmas week (Dec 22, 2015 \u2013 Dec 28, 2013). We can see the traffic condition is much better during Wednesday daytime than usual, because people are celebrating Christmas.\nFor example, using a weekly pattern with 1 hour as the basic unit, we have T = 7 \u00d7 24 = 168. Using this relative time window, we represent the average speed of the k-th time slot as Vk,\u2200k \u2208 Trela. We call {Vk|k \u2208 Trela} relative temporal reference.\nWe use ki to denote the time slot to which si belongs. As a result, we can write V (si) = Vki . To compute Vki , we collect all the trips in D which fall into the same time slot as pi and denote the set as S(pi). Then, we have\nVki = 1 |S(pi)| \u2211\npj\u2208S(pi)\nlj tj\n(8)\nIn Figure 2, we present the weekly relative speed reference on all trips. We can see all the weekdays share similar patterns the rush hour started from 8:00 in the morning. Meanwhile, during the weekends, the traffic is not as much as usual during 8:00 in the morning.\nThe relative speed reference mainly has the following two advantages. First, the relative speed reference is able to alleviate the data sparsity issue. By folding the data into a relative window, we will have more trips to estimate an average speed with a higher confidence. Second, the computation overhead of relative speed reference is small, and we could do it offline."}, {"heading": "B. Absolute Temporal Speed Reference", "text": "In the previous section, the relative speed reference assumes the speed follows daily or weekly regularity. However, in real\n6 scenario, there are always irregularities in the traffic condition. For example, during national holidays the traffic condition will significantly deviate from the usual days. In Figure 6, we show the actual average traffic speed during the Christmas week (Dec 22, 2013 \u2013 Dec 28, 2013). Compared with the relative speed reference, we can see that on Dec 25th, 2013, the traffic condition is better than usual during the day, since people are celebrating Christmas. Therefore, assuming we have enough data, it would be more accurate if we could directly infer the average speed at any time slot t from the historical data.\nIn this section, we propose an alternative approach to directly capture the traffic condition at different time slots. We extend our relative reference from Section IV-A to an absolute temporal speed reference. To this end, we partition the original timeline into time slots based on a certain time interval (i.e., 1 hour). All historical trips are mapped to the absolute time slots Tabs = {1, 2, 3, \u00b7 \u00b7 \u00b7 } accordingly, and the average speed {Vk|k \u2208 Tabs} are calculated as the absolute temporal speed reference.\nThe challenge in absolute temporal speed reference is that for a given query trip with starting time sq in the near future (e.g. next hour), we need to estimate the speed reference V (sq). We estimate V (sq) by taking into account factors such as the average speed of previous hours, seasonality, and random noise. Formally, given the time series of average speed: {V1, V2, . . . , VM}, our goal is to compute VM+1 as follows:\nVM+1 = f(V1, . . . , VM ). (9)\nTo tackle this problem, we adopt the ARIMA model for time series forecasting, and show how it can be used to model the seasonal difference (i.e., the deviation from the periodic pattern) for a time series.\n1) Overview of the ARIMA Model: In statistical analysis of time series, the autoregressive integrated moving average (ARIMA) model [33] is a popular tool for understanding and predicting future values in a time series. Mathematically, for any time series {Xt}, let L be the lag operator:\nLXt = Xt\u22121,\u2200t > 1. (10)\nThen, the ARIMA(p, d, q) model is given by( 1\u2212\np\u2211 i=1 \u03c6iL i\n) (1\u2212 L)dXt = ( 1 +\nq\u2211 i=1 \u03b8iL i\n) t, (11)\nwhere parameters p, d and q are non-negative integers that refer to the order of the autoregressive, integrated, and moving average parts of the model, respectively. In addition, t is a white noise process.\nIn practice, the autocorrelation function (ACF) and partial autocorrelation function (APCF) is frequently used to estimate the order parameters (p, d, q) from a time series of observations {X1, X2, . . . , XM}. Then, the coefficients {\u03c6i}pi=1 and {\u03b8i}qi=1 of the ARIMA(p, d, q) can be learned using standard statistical methods such as the least squares.\n2) Incorporating the Seasonality: In our problem, the average speed Vt exhibits a strong weekly pattern. Thus, instead of directly applying the ARIMA model to {Vt}, we first compute the sequence of seasonal difference {Yt}:\nYt = Vt \u2212 Vt\u2212T , (12)\nwhere T is the period (e.g., one week). Then, we apply the ARIMA model to {Yt}:(\n1\u2212 p\u2211\ni=1\n\u03c6iL i ) (1\u2212 L)d Yt = ( 1 +\nq\u2211 i=1 \u03b8iL i\n) t. (13)\nNote that our ARIMA model with the seasonal difference is a special case of the more general class of Seasonal ARIMA (SARIMA) model for time series analysis. We refer interested readers to [33] for detailed discussion about the model.\nSuppose we use first order difference of Yt, namely d = 1 and (1\u2212 L)dYt = Yt \u2212 Yt\u22121. Then we have\nYt = Yt\u22121 + p\u2211 i=1 \u03c6iL i(Yt \u2212 Yt\u22121) + q\u2211 i=1 \u03b8iL i t + t (14)\nSince the last term t in Equation (14) is white noise, whose value is unknown but the expectation E( t) = 0, we have estimator Y\u0302t = Yt \u2212 t. Together with Equation (12), we have\nV\u0302t = Y\u0302t + Vt\u2212T (15)"}, {"heading": "C. The Effect of Geographic Regions", "text": "So far, we have assumed that the traffic condition follows the same temporal pattern across all geographic locations. However, in practice, trips within a large geographic area (e.g., New York City) may have different traffic patterns, depending on the spatial locations. For example, in Figure 7 we show the speed references of two different pairs of regions. The speed reference in region pairs (A,B) has a larger variation than (C,D). In particular, for (A,B) pair, the average speed is 22.7mph at 4:00 a.m. on Thursday and 8.5mph at 12:00 p.m. on Wednesday. But for (C,D) pair, the average speed is only 11.2mph and 7.0mph at these two corresponding times. The reason could be that A is a residential area, whereas B is a business district. Therefore, the traffic pattern between A and B exhibits a very strong daily peak-hour pattern. On the other hand, regions C and D are popular tourist areas, the speeds are constantly slower compared with that of (A,B).\nThe real case above suggests that refining the temporal references for different spatial regions could help us further improve the travel time estimation. Therefore, we propose to divide the map into a set of K neighborhoods {R1, R2, . . . , RK}. For each pair of neighborhoods (Ri, Rj), 1 \u2264 i, j \u2264 K, we use Vi\u2192j(s) to denote the average speed of all trips from Ri to Rj at starting time s. Let Di\u2192j be the subset of trips in D whose origin and destination fall in Ri and Rj , respectively. Then, Vi\u2192j(s) can be estimated using Di\u2192j in the same way as described in Sections IV-A and IV-B.\n1Figure 7(a) is generated with Google MAP API.\n7 (a) Manhattan 8 Sn 8 M 8 T 8 W 8 Th 8 F 8 S 5 10 15 20 25 12:00 W 4:00 Th Hour in week (hour) S p e e d ( m ile /h ) A\u2212B C\u2212D (b) Speed reference\nFig. 7. Different region pairs have different traffic patterns. 1\n0 1 2 3 0\n0.5\n1\n1.5\n2\n2.5\n3\nSpeed ratio vi/vq. R 2 = 0.347\nR ef er en ce\nra ti o V i\u2192\nj (s\ni) / V i\u2192\nj (s\nq )\nOne pair Linear fit y = x\nFig. 8. Refining temporal reference with regions improves the performance. Vi\u2192j(si) Vi\u2192j(sq) \u2248 vi vq . Compared with Figure 3(b), the slope of linear fit is closer to 1 and the R2 is larger, which means the region-based temporal reference approximates the actual speed ratio better.\nTo show the effectiveness of refining the temporal reference for each region pairs. In Figure 8, we plot the refined temporal reference ratio against the actual speed ratio of a pair of trips. Comparing with the Figure 3(b), we see that the linear fit has a slope closer to 1, which means the two ratios are closer, and the R2 is larger, which means the line fits better."}, {"heading": "D. Time Complexity Analysis", "text": "Our approach has three steps: 1) mapping training trips into grids; 2) extracting neighbors of a given OD pair; and 3) estimating travel time based on the neighbors.\nStep 1. In order to quickly retrieve the neighboring trips, we employ a raster partition of the city (e.g., 50 meters by 50 meters grid in our experiment), and preparing N training trips takes O(N) time. This step can be preprocessed offline.\nStep 2. Given a testing trip q, we find its corresponding origin gird in O(1) time. Then, retrieving all the neighboring trips in the same grid would take O(N/h) time if the trips are uniformly distributed, where h is the total number of grids. In practice, however, the number of trips in each grid follows a long tail distribution. Therefore, the worst case time complexity of retrieving neighboring trips would be O(\u03b1 \u00b7N), where \u03b1 \u00b7 N is the number of trips in the most dense grid (\u03b1 = 0.01 in NYC taxi data).\nStep 3. After retrieving the neighboring trips, the time complexity of calculating travel time of q is O(|N (q)|), where |N (q)| is the number of neighbors of trip q. The temporal\nspeed references can be computed offline. The relative speed reference takes O(N) time, and ARIMA model can be trained in O(M2) time, where M is the number of time slots. During the online estimation, looking-up relative speed reference takes O(1) time, whereas using ARIMA to predict the current speed reference takes O((p+ q) \u00b7 d) time, where p, q, and d are the parameters learned during model training (p = 2, q = 0, and d = 1 in our experiment).\nTherefore, the time complexity of online computation is O(\u03b1 \u00b7N) in the worst case. In order to serve a large amount of batch queries, we can use multi-threading to further boost the estimation time."}, {"heading": "V. FILTERING OUTLIERS", "text": "In the NYC taxi data, we observe many anomalous trips, which cause large errors in our estimation. Malfunction of tracking device might be the main cause of outliers. For example, there are a large amount of trips with reported travel time as zero second, while their actual distances are non-zero.\nSome trip features are naturally correlated, such as distance vs. time and trip distance vs. distance between endpoints. If a trip significantly deviates from such correlation, it is very likely to be an outlier. Specifically, the correlation of distance and time could help us find anomalous trips with extremely fast or slow travel speeds; the correlation of trip distance and distance between endpoints could help us find detour trips. In principle, any outlier detection algorithm can be used to clean the data. In this paper, we assume there exists a linear correlation between some feature pairs. Based on this assumption, we design a linear regression model to fit the data, and employ Maximize Likelihood Estimation to identify outliers.\nOur linear data model is as follows. Suppose we are interested in capturing the linear correlation between feature X and feature Y , we have\nY = \u03c91X + \u03c92 + ,\nwhere \u03c91, \u03c92 are the linear coefficients, and is the fitting error. The error term has a distribution that is a mixture of a Gaussian distribution (with zero mean and unknown variance) with (unknown) probability (1 \u2212 p) and a t-distribution with probability p. We fit the model to the data using variational inference. For every data point, the inference procedure provides a number ti which is an estimate of the probability that trip pi is an outlier. After learning p, we set the top p \u00b7N trips with largest ti values as outliers. Details of the outlier filtering algorithm are put into the supplementary material3 due to space constraints. As an example, if Y is set to trip time and X is set to trip distance, then this approach filters outlying trips with speed outside the range [2.03mph, 52.74mph]."}, {"heading": "VI. EXPERIMENT", "text": "In this section, we present a comprehensive experimental study on two real datasets. All the experiments are conducted on a 3.4 GHz Intel Core i7 system with 16 GB memory. We have released our code and sample data via an anonymous Dropbox link4.\n3https://www.dropbox.com/s/c62ntpeplvxfjla/outlier.pdf?dl=0 4https://www.dropbox.com/s/uq2w50kgmgzymv3/traveltime.tar.gz?dl=0?\n8"}, {"heading": "A. Dataset", "text": "We conduct experiments on datasets from two different countries to show the generality of our approach.\n1) NYC Taxi: A large-scale New York City taxi dataset has been made public online [1]. The dataset contains 173, 179, 771 taxi trips from 2013/01/01 to 2013/12/31. Each trip contains information about pickup location and time, drop off location and time, trip distance, fare amount, etc. We use the subset of trips within the borough of Manhattan (the boundary is obtained from wikimapia.org), which has 132, 766, 605 trips. After filtering the outliers, there are 127,534,711 trips left for our experimental evaluation. On average, we have 349, 410 trips per day.\nFigure 9(a) shows the distribution of GPS points of the pickup and dropoff locations. Figure 9(b) shows the number of trips per day over the year 2013. Figure 9(c) and Figure 9(d) show the empirical CDF plot of the trip distance and trip time. About 56% trips have trip time less than 10 minutes, and about 99% trips have trip time less than 30 minutes. The mean and median trip time are 636 (second) and 546 (second). The mean and median trip distance are 1.935 (mile) and 1.6 (mile).\n2) Shanghai Taxi: In order to compare with the existing route-based methods, we use the Shanghai taxi dataset with the trajectories of 2, 600 taxis during two months in 2006. A GPS record has following fields: vehicle ID, speed, longitude, latitude, occupancy, and timestamp. In total we have over 300 million GPS records. We extract the geographical information of Shanghai road networks from OpenStreetMap.\nTo retrieve taxi trips in this dataset, we rely on the occupancy bit of GPS records. This occupancy bit is 1 if there are passengers on board, and 0 otherwise. For each taxi, we define a trip as consecutive GPS records with occupancy equal to 1.\nWe get 5, 815, 470 trips after processing the raw data. The distribution of trip travel time is similar to that of NYC taxi data in Figure 9(c). Specifically, about half of the trips have travel time less than 10 minutes."}, {"heading": "B. Evaluation Protocol", "text": "Methods for evaluation. We systematically compare the following methods: \u2022 Linear regression (LR). We train a linear regression model\nwith travel time as a function of the L1 distance between pickup and drop off locations. This simple linear regression serves as a baseline for comparison. \u2022 Neighbor average (AVG). This method simply takes the\naverage of travel times of all neighboring trips as the estimation. \u2022 Temporally weighted neighbors (TEMP). This is our pro-\nposed method using temporal speed reference to assign weights on neighboring trips. We name the method using relative-time speed reference as TEMPrel (Section IV-A). And we call the method using ARIMA model to predict absolute temporal reference as TEMPabs (Section IV-B). \u2022 Temporal speed reference by region (TEMP+R). This is\nour improved method based on TEMP by considering the temporal reference for different region pairs (Section IV-C). \u2022 Segment-based estimator (SEGMENT). This method estimates the travel time of each road segment individually, and then aggregate them to get the estimation of a complete trip (a baseline method used in [9]). \u2022 Subpath-based estimator (SUBPATH). One drawback of SEGMENT is that the transition time at intersections cannot be captured. Therefore, Wang et al. [9] propose to concatenate subpaths to estimate the target route, where each subpath is consisted of multiple road segments. For each subpath, SUBPATH estimates its travel time by searching all the trips that contain this sub-path. \u2022 Online map service (BING and BAIDU). We also compare our methods with online map services. We use Bing Maps [2] for NYC taxi dataset and Baidu Maps [3] for Shanghai dataset. We use Bing Maps instead of Google Maps for two reasons: (1) Bing Maps API allows query with current traffic for free whereas Google does not provide that; and (2) Bing Maps API allows 125K queries per key per year but Google only allows 2.5K per day. To consider traffic, we send queries to Bing Maps at the same time of the same day (in a weekly window) as the starting time of the testing trip. Due to national security concerns, the mapping of raw GPS data is restricted in China [12]. So we use Baidu Maps instead of Bing Maps for Shanghai taxi dataset.\nEvaluation metrics. Similar to [9], we use mean absolute error (MAE) and mean relative error (MRE) to evaluate the travel time estimation methods:\nMAE = \u2211 i |yi \u2212 y\u0302i| n ,MRE = \u2211 i |yi \u2212 y\u0302i|\u2211\ni yi ,\nwhere y\u0302i is the travel time estimation of test trip i and yi is the ground truth. Since there are anomalous trips, we also use\n9 the median absolute error (MedAE) and median relative error (MedRE) to evaluate the methods:\nMedAE = median(|yi\u2212y\u0302i|),MedRE = median ( |yi \u2212 y\u0302i| yi ) ,\nwhere median returns the median value of a vector."}, {"heading": "C. Parameter Setting", "text": "Before conducting the comparison experiments, we first discuss how to set the parameter for our proposed method. There is only one parameter in our proposed method, the distance threshold \u03c4 to define the neighbors. We use the first 11 months from NYC dataset to study the performance w.r.t. parameter \u03c4 . For computational efficiency, we partition the map into small grids of 50 meters by 50 meters. The distance in Eq. (1) is now defined as the L1 distance between two grids. For example, if p1 is a neighboring trip of q for \u03c4 = 0, it means that the pickup (and drop off) location of p1 is in the same grid as the pickup (and drop off) location of q. If \u03c4 = 1, a neighboring trip has endpoints in the same or adjacent grids.\nA larger \u03c4 will retrieve more neighboring trips for a testing trip and thus could give an estimation with a higher confidence. However, a larger \u03c4 also implies that the neighboring trips are less similar to the testing trip, which could introduce prediction errors. Therefore, it is crucial to identify the optimal \u03c4 that balance estimation confidence and neighbors\u2019 similarity. In Figure 10(a), we plot the empirical relationship between the MAE and \u03c4 . We can see that MAE is the lowest when \u03c4 is in the range of [3, 6], striking a good balance between confidence and similarity.\nWe also want to point out that we need to have at least one neighboring trip in order to give an estimation of a testing trip. Obviously, the larger \u03c4 is, the more testing trips we can cover. Figure 10(b) plots the percentage of testing trips with at least one neighbor. In the figure is shows that if \u03c4 = 0, there are only 32% of trips are predictable. When \u03c4 = 3, 99% trips are predictable. Considering the both aspects, we find \u03c4 = 3 an appropriate setting for the following experiments."}, {"heading": "D. Performance on NYC Data", "text": "1) Overall Performance on NYC Data: In our experiment, we use trips from the first 11 months (i.e., January to\nNovember) as training and the ones in the last month (i.e., December) as testing. As for the BING method, due to the limited quota, we sample 260k trips in December as testing. The overall accuracy comparison are shown in Table I. Since NYC data only have endpoints of trips, we cannot compare our method with route-based methods. We will compare with theses methods using Shanghai data in Section VI-E.\nWe first observe that our method is better than the linear regression (LR) baseline. This is expected because LR is a simple baseline which does not consider the origin and destination locations.\nConsidering temporal variations of traffic condition improves the estimation performance. All the TEMP methods have significantly lower error compared with the AVG method. The improvement of TEMPabs over AVG is about 35 seconds in MAE. In other words, the MAE is decreased by nearly 20% by considering the temporal factor. Furthermore, using the absolute speed reference (TEMPabs) is better than using the relative (i.e., weekly) speed reference (TEMPrel). This is because the traffic condition does not strictly follow the weekly pattern, but has some irregular days such as holidays.\nBy adding the region factor, the performance is further improved. This means the traffic pattern between regions are actually different. However, we observe that the improvement of TEMPabs + R over TEMPabs is not as significant as the improvement of TEMPrel + R over TEMPrel. This is due to data sparsity when we compute the absolute time reference for each region pair. If two regions have little traffic flow, the absolute time reference might not be accurate. In this case, using the relative time reference between regions make the data more dense and produces better results.\nComparing with BING on the 260k testing trips, TEMPabs+ R significantly outperforms BING by 67 seconds. BING underestimates the trips without considering traffic, where 64.53% testing trips are underestimated. However, when considering traffic (the query was sent to the API at the same time and the same day of the week), 75.02% testing trips are overestimated.\n2) Performance w.r.t. the Size of Historical Data: We expect that, by using more historical data (i.e., training data), the estimation will be more accurate. To verify this, we study how performance changes w.r.t. the size of training data. We choose different time durations (from 1 week, 2 weeks, to 11 months) before December as the training data. Figure 11(a) shows the accuracy w.r.t. the size of the training data. The\n10\nresult meets with our expectation that MAE drops when using more historical data. But the gain of using more data is not obvious when using more than 1 month data. This indicates that using 1-month training data is enough to achieve a stable performance in our experimental setting. Such indication will save running time in real applications as we do not need to search for neighboring trips more than 1 month ago.\nIn addition, using more historical data, we are able to cover more testing trips. Because a testing trip should have at least one neighboring trip in order to be estimated. Figure 11(b) shows the coverage of testing data by using more historical data. With 1-week data, we can estimate 95.16% testing trips; with 1-month data, we can cover 99.20% testing trips.\n3) Performance w.r.t. trip features.: Performance w.r.t. trip time. In Figure 12(a) and 12(b), we plot the MAE and MRE with respect to the trip time. As expected, the longer-time trips have higher MAEs. It is interesting to observe that the MREs are high for both short-time trips and long-time trips. Because the short-time trips (less than 500 seconds) are more sensitive to dynamic conditions of the trips, such as traffic light. On the other hand, the long-time trips usually have less neighbors, which leads to higher MREs.\nPerformance w.r.t. trip distance. The MAE and MRE against trip distance are shown in Figure 12(c) and 12(d). Overall, we have consistent observations as Figure 12(a) and Figure 12(b). However, we notice that the error of TEMPrel+R increases faster than other methods. TEMPrel + R becomes even worse than AVG method when the trip distance is longer than 8 miles. This is because the longer-distance trips have fewer neighboring trips. However, we do not observe the same phenomenon in Figure 12(a) and Figure 12(b). After further inspection, we find that long-distance trips have even less neighboring trips than long-time trips. The reason is that longdistance trips usually have long travel time; however, long-time trips may not have long trip distance (e.g., the long travel time may be due to traffic).\nPerformance w.r.t. number of neighboring trips. The error against number of neighbors per trip is shown in Figure 12(e) and Figure 12(f). MAEs decrease for the trips with more neighbors. However, MREs increase as the number of neighbors increases. The reason is that the short trips usually have more neighbors and short trips have higher relative errors, as shown in Figure 12(b) and Figure 12(d).\n4) An Alternative Definition of Neighbors: As discussed in Section III-A, now we study the performance by defining soft neighbors. We give neighboring trips spatial weights based on the sum of corresponding end-points\u2019 distance, and evaluate the performance on 500k trips. The MAE of TEMPabs + R is 107.705s on this testing set. Adding spatial weights on the neighbors degenerates the performance to 112.392s. It suggests that it is non trivial to assign weights on neighbors based on the distances of end-points. It will require more careful consideration of this factor."}, {"heading": "E. Performance on Shanghai Data", "text": "In this section, we conduct experiments on Shanghai taxi data. This dataset provides the trajectories of the taxi trips so we can compare our method with SEGMENT and SUBPATH methods. Both SEGMENT and SUBPATH methods use the travel time on individual road segments or subpaths to estimate the travel time for a query trip. Due to data sparsity, we cannot obtain travel time for every road segments. Wang et al. [9] propose a tensor decomposition method to estimate the missing values. In our experiment, to avoid the missing value issue, we only select the testing trips that have values on\n11\nevery segments of the trip. However, by doing such selection, the testing trips are biased towards shorter trips. Because the shorter the trip is, the less likely the trip has missing values on the segment. To alleviate this bias issue, we further sampled the biased trip dataset to make the travel time distribution similar to the distribution of the original whole dataset. The testing set contains 2, 138 trips in total.\n1) Overall Performance on Shanghai Data: The comparison among different methods is shown in Table II. Our neighborbased method significantly outperform other methods. The simple method AVG is 17 seconds better than BAIDU in terms of MAE. By considering temporal factors, method TEMPrel and TEMPabs further outperform AVG method. SUBPATH method outperforms SEGMENT method, which is consistent with previous work [9]. SEGMENT simply adds travel time of individual segments, whereas SUBPATH better considers the transition time between segments by concatenating subpaths. However, SUBPATH is still 21 seconds worse than our method TEMPrel method in terms of MAE. Such result is also expected. Because our method can be considered as a special case SUBPATH method, where we use the whole paths from the training data to estimate the travel time for a testing trip. If we have enough number of whole paths, it is better to use the whole paths instead of subpaths.\nSince the Shanghai taxi data are much more sparse than NYC data, we do not show the results of TEMPrel + R and TEMPabs+R on Shanghai dataset. Additionally, the two-month Shanghai data have several days\u2019 trip missing, and the ARIMA estimation require a complete time series. Therefore, we did not show the results of TEMPabs, either. The main idea here is to show that TEMPrel can outperform the existing methods, and with more data our other approaches should outperform the TEMPrel as well as shown in NYC data.\n2) Applicability of Segment-based Method and Neighborbased Method: Segment-based method requires every individual road segment of the testing trip has at least one historical data point to estimate the travel time. On the other hand, our neighbor-based method requires a testing trip has at least one neighboring trip. Among 435, 887 trips in Shanghai dataset, only 54, 530 trips have values on every road segment, but 217, 585 trips have at least one neighbor. Therefore, in this experimental setting, our method not only outperforms segment-based method in terms of accuracy, it is also more applicable to answer more queries (49.9%) compared with segment-based method (12.5%)."}, {"heading": "F. The Impacts of Outliers", "text": "We implemented several outlier filters to work as a pipeline. For NYC data, we have filters using time and distance,\ntime and fare, distance and fare, trip distance and distance between end points, trip time and distance between end points, respectively. In total, an estimated 7% outliers are found.\nThe outliers will impact the experimental performances in two ways: (1) the outliers in the training data will make the travel time estimation less accurate; and (2) the outliers in the testing trips will make the experimental evaluations less reliable because the errors of the outlying testing trips will greatly affect the overall accuracy.\nWe study how outliers impact these two aspects using NYC dataset with November trips as training and December trips as testing. The number of anomalous trips in November and December are 758, 253 out of 11, 915, 496 (i.e., 6.36%) and 801, 503 out of 11, 434, 273 (i.e., 7.00%), respectively. Three different experiment settings are selected as shown in Table III. Comparing the results of the first setting with the second setting, we can see that by taking out 6.4% outliers from training trips, the MAE is reduced by roughly 5 seconds. It is critical to note that the outliers in testing set will incur even a very large error in the evaluation. In setting 3 of Table III, where both testing and training set have outliers, MAE is more than 20 seconds worse than the first setting."}, {"heading": "G. Running Time", "text": "Table IV presents the running time comparison between TEMPabs + R and SUBPATH. We profile the running time of TEMPabs + R on last two months\u2019 trips in NYC dataset, and SUBPATH on whole Shanghai dataset. Note that NYC dataset is bigger than Shanghai dataset and TEMPabs + R is the most computationally expensive method, and also the most accurate method among our proposed methods. The pre-processing time for TEMPabs+R consists of mapping trips to grids, calculating the speed reference, and learning the ARIMA parameters. The preparation time for SUBPATH includes indexing trajectory by road segments.\nIn TEMPabs+R, both training trip preparation and learning ARIMA can be done offline or updated every hour with the\n12\nnew data. The online query part includes querying neighboring trips and estimation using neighbors. The neighbor finding is the most time consuming part. We get 1.09 ms/trip if using one thread and 0.24 ms/trip if using 8 threads. The estimation part only takes 0.02 ms with 8 threads. So the total online query time for our method is 1.505 ms/trip on average and it will be even faster (i.e., 0.26 ms/trip) if using 8 threads. The estimation of SUBPATH includes finding optimal concatenation of segments, which has O(n2 \u00b7m) time complexity, where n is the number of road segments and m is number of trips going through each segment. It takes 42ms on average to estimate a testing trip. Our neighbor-based method is more efficient in answering travel time queries because we avoid computing the route and estimating the time for the subpaths."}, {"heading": "VII. CONCLUSION", "text": "This paper demonstrates that one can use large-scale trip data to estimate the travel time between an origin and a destination in a very efficient and effective way. Our proposed method retrieves all the neighboring historical trips with the similar origin and estimation locations and estimate the travel time using those neighboring trips. We further improve our method by considering the traffic conditions w.r.t. different temporal granularities and spatial dynamics. In addition, we develop an outlier filtering method to make our estimation more accurate and experimental results more reliable. Finally, we conduct experiments on two large-scale real-world datasets and show that our method can greatly outperform the state-ofthe-art methods and online map services. In the future, we are interested in finding out what other new insights could be offered by such big data, for example, how crime and economic status of places correlate with the travel flows."}], "references": [{"title": "Continuous-time dynamic shortest path algorithms", "author": ["B.C. Dean"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, 1999.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "Finding fastest paths on a road network with speed patterns", "author": ["E. Kanoulas", "Y. Du", "T. Xia", "D. Zhang"], "venue": "IEEE International Conference on Data Engineering, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Adaptive fastest path computation on a road network: a traffic mining approach", "author": ["H. Gonzalez", "J. Han", "X. Li", "M. Myslinska", "J.P. Sondag"], "venue": "International Conference on Very large Data Bases, 2007, pp. 794\u2013 805.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "T-drive: driving directions based on taxi trajectories", "author": ["J. Yuan", "Y. Zheng", "C. Zhang", "W. Xie", "X. Xie", "G. Sun", "Y. Huang"], "venue": "ACM SIGSPA- TIAL International Conference on Advances in Geographic Information Systems, 2010, pp. 99\u2013108.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning the dynamics of arterial traffic from probe data using a dynamic bayesian network", "author": ["A. Hofleitner", "R. Herring", "P. Abbeel", "A. Bayen"], "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 13, no. 4, pp. 1679\u20131693, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Travel time estimation of a path using sparse trajectories", "author": ["Y. Wang", "Y. Zheng", "Y. Xue"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2014, pp. 25\u201334.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "On the complexity of nonnegative matrix factorization", "author": ["S.A. Vavasis"], "venue": "SIAM Journal on Optimization, vol. 20, no. 3, pp. 1364\u20131377, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces", "author": ["R. Weber", "H.-J. Schek", "S. Blott"], "venue": "VLDB, vol. 98, 1998, pp. 194\u2013205.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "On trip planning queries in spatial databases", "author": ["F. Li", "D. Cheng", "M. Hadjieleftheriou", "G. Kollios", "S.-H. Teng"], "venue": "Advances in Spatial and Temporal Databases. Springer, 2005, pp. 273\u2013290.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "A spatially disaggregated approach to commuting efficiency", "author": ["M.A. Niedzielski"], "venue": "Urban Studies, vol. 43, no. 13, pp. 2485\u20132502, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Commuting economy: an alternative approach for assessing regional commuting efficiency", "author": ["E. Murphy", "J.E. Killen"], "venue": "Urban studies, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Traffic flow dynamics", "author": ["M. Treiber", "A. Kesting"], "venue": "Traffic Flow Dynamics: Data, Models and Simulation, Springer-Verlag Berlin Heidelberg, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Accurate estimation of travel times from single-loop detectors", "author": ["K.F. Petty", "P. Bickel", "M. Ostland", "J. Rice", "F. Schoenberg", "J. Jiang", "Y. Ritov"], "venue": "Transportation Research Part A: Policy and Practice, vol. 32, no. 1, pp. 1\u201317, 1998.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "The pems algorithms for accurate, real-time estimates of g-factors and speeds from single-loop detectors", "author": ["Z. Jia", "C. Chen", "B. Coifman", "P. Varaiya"], "venue": "IEEE International Conference on Intelligent Transportation Systems, 2001, pp. 536\u2013541.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "A simple and effective method for predicting travel times on freeways", "author": ["J. Rice", "E. Van Zwet"], "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 5, no. 3, pp. 200\u2013207, 2004.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Real-time freeway traffic state estimation based on extended kalman filter: a general approach", "author": ["Y. Wang", "M. Papageorgiou"], "venue": "Transportation Research Part B: Methodological, vol. 39, no. 2, pp. 141\u2013167, 2005.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "An ensemble kalman filtering approach to highway traffic estimation using gps enabled mobile devices", "author": ["D.B. Work", "O.-P. Tossavainen", "S. Blandin", "A.M. Bayen", "T. Iwuchukwu", "K. Tracton"], "venue": "IEEE Conference on Decision and Control, 2008, pp. 5062\u20135068.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "A gravity model for speed estimation over road network", "author": ["P. Cintia", "R. Trasarti", "L.A. Cruz", "C.F. Costa", "J.A.F. de Macedo"], "venue": "Mobile Data Management (MDM), 2013 IEEE 14th International Conference on, vol. 2. IEEE, 2013, pp. 136\u2013141.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Traffic estimation and prediction based on real time floating car data", "author": ["C. De Fabritiis", "R. Ragona", "G. Valenti"], "venue": "IEEE International Conference on Intelligent Transportation Systems, 2008, pp. 197\u2013203.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Path and travel time inference from gps probe vehicle data", "author": ["T. Hunter", "R. Herring", "P. Abbeel", "A. Bayen"], "venue": "NIPS Analyzing Networks and Learning with Graphs, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Travel time estimation for ambulances using bayesian data augmentation", "author": ["B.S. Westgate", "D.B. Woodard", "D.S. Matteson", "S.G. Henderson"], "venue": "The Annals of Applied Statistics, vol. 7, no. 2, pp. 1139\u20131161, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Travel time estimation for urban road networks using low frequency probe vehicle data", "author": ["E. Jenelius", "H.N. Koutsopoulos"], "venue": "Transportation Research Part B: Methodological, vol. 53, pp. 64\u201381, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Using mobile phones to forecast arterial traffic through statistical learning", "author": ["R. Herring", "A. Hofleitner", "S. Amin", "T. Nasr", "A. Khalek", "P. Abbeel", "A. Bayen"], "venue": "89th Transportation Research Board Annual Meeting, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimal decomposition of travel times measured by probe vehicles using a statistical traffic flow model", "author": ["A. Hofleitner", "A. Bayen"], "venue": "IEEE International Conference on Intelligent Transportation Systems, 2011, pp. 815\u2013821.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Inferring movement trajectories from GPS snippets", "author": ["M. Li", "A. Ahmed", "A.J. Smola"], "venue": "ACM International Conference on Web Search and Data Mining, 2015, pp. 325\u2013334.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Route travel time 13 estimation using low-frequency floating car data", "author": ["M. Rahmani", "E. Jenelius", "H.N. Koutsopoulos"], "venue": "IEEE International Conference on Intelligent Transportation Systems, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Press: A novel framework of trajectory compression in road networks", "author": ["R. Song", "W. Sun", "B. Zheng", "Y. Zheng"], "venue": "International Conference on Very Large Data Bases, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Forecasting: principles and practice", "author": ["R.J. Hyndman", "G. Athanasopoulos"], "venue": "OTexts,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Most existing methods of travel time estimation use the route-based framework to estimate the travel time [4], [5], [6], [7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 1, "context": "Most existing methods of travel time estimation use the route-based framework to estimate the travel time [4], [5], [6], [7].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "Most existing methods of travel time estimation use the route-based framework to estimate the travel time [4], [5], [6], [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 3, "context": "Most existing methods of travel time estimation use the route-based framework to estimate the travel time [4], [5], [6], [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "travel time is proposed in [8], where a dynamic Bayesian network is employed to learn the dependence of travel time on road congestion.", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "A more recent work [9] models the travel time over each road segment during different periods for different users as a tensor.", "startOffset": 19, "endOffset": 22}, {"referenceID": 6, "context": "And it addresses the missing observations issue with tensor decomposition, which is known to be NP-hard [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "While many sophisticated indexing algorithms have been proposed, many are outperformed by a simple sequential search of the data [11].", "startOffset": 129, "endOffset": 133}, {"referenceID": 5, "context": ", pickup and drop off locations), to compare our method with route-based method [9], we use Shanghai taxi data, where more than 5 million trips with complete information of trajectories are available.", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "On this dataset, our method also significantly outperforms the state-of-the-art route-based method [9] by 19% and Baidu Maps2 [3] by 17%.", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "It is noteworthy that our method runs 40 times faster than state-of-the-art method [9].", "startOffset": 83, "endOffset": 86}, {"referenceID": 8, "context": "One example is the trip planning [13].", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "The second example is to estimate the city commuting efficiency [14] [15] in a future time.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "The second example is to estimate the city commuting efficiency [14] [15] in a future time.", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "There are two types of data that are used to estimate the travel time on road segments: loop detector data and floating-car data (or probe data) [16].", "startOffset": 145, "endOffset": 149}, {"referenceID": 12, "context": "Various methods [17], [18], [19], [20] have been proposed to infer vehicle speed from the loop sensor readings and then infer travel time on individual road segments.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Various methods [17], [18], [19], [20] have been proposed to infer vehicle speed from the loop sensor readings and then infer travel time on individual road segments.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "Various methods [17], [18], [19], [20] have been proposed to infer vehicle speed from the loop sensor readings and then infer travel time on individual road segments.", "startOffset": 28, "endOffset": 32}, {"referenceID": 15, "context": "Various methods [17], [18], [19], [20] have been proposed to infer vehicle speed from the loop sensor readings and then infer travel time on individual road segments.", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "The speed of individual road segments at a time t can be inferred if a floating car is passing through the road segment at that time point [21], [22].", "startOffset": 139, "endOffset": 143}, {"referenceID": 17, "context": "The speed of individual road segments at a time t can be inferred if a floating car is passing through the road segment at that time point [21], [22].", "startOffset": 145, "endOffset": 149}, {"referenceID": 18, "context": "A few methods have been proposed to overcome the low sampling rate issue [23], [24], [8], [25], [26].", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "A few methods have been proposed to overcome the low sampling rate issue [23], [24], [8], [25], [26].", "startOffset": 79, "endOffset": 83}, {"referenceID": 4, "context": "A few methods have been proposed to overcome the low sampling rate issue [23], [24], [8], [25], [26].", "startOffset": 85, "endOffset": 88}, {"referenceID": 20, "context": "A few methods have been proposed to overcome the low sampling rate issue [23], [24], [8], [25], [26].", "startOffset": 90, "endOffset": 94}, {"referenceID": 21, "context": "A few methods have been proposed to overcome the low sampling rate issue [23], [24], [8], [25], [26].", "startOffset": 96, "endOffset": 100}, {"referenceID": 5, "context": "[9] proposes to use matrix factorization to estimate the missing values on individual road segments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "Recent research works start considering the time spent on intersections [27], [28], [8], [29].", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "Recent research works start considering the time spent on intersections [27], [28], [8], [29].", "startOffset": 78, "endOffset": 82}, {"referenceID": 4, "context": "Recent research works start considering the time spent on intersections [27], [28], [8], [29].", "startOffset": 84, "endOffset": 87}, {"referenceID": 24, "context": "Recent research works start considering the time spent on intersections [27], [28], [8], [29].", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "[30] first proposes to concatenate sub-paths to give a more accurate estimation of the query route.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[9] further improves the path-based method by first mining frequent patterns [31] and then concatenating frequent sub-paths by minimizing an objective function which balances the length and support of the sub-paths.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[9] further improves the path-based method by first mining frequent patterns [31] and then concatenating frequent sub-paths by minimizing an objective function which balances the length and support of the sub-paths.", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": ", shortest route or fastest route) [4], [5], [6], [7] and then estimating the travel time for that route.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": ", shortest route or fastest route) [4], [5], [6], [7] and then estimating the travel time for that route.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": ", shortest route or fastest route) [4], [5], [6], [7] and then estimating the travel time for that route.", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": ", shortest route or fastest route) [4], [5], [6], [7] and then estimating the travel time for that route.", "startOffset": 50, "endOffset": 53}, {"referenceID": 27, "context": "1) Overview of the ARIMA Model: In statistical analysis of time series, the autoregressive integrated moving average (ARIMA) model [33] is a popular tool for understanding and predicting future values in a time series.", "startOffset": 131, "endOffset": 135}, {"referenceID": 27, "context": "We refer interested readers to [33] for detailed discussion about the model.", "startOffset": 31, "endOffset": 35}, {"referenceID": 5, "context": "This method estimates the travel time of each road segment individually, and then aggregate them to get the estimation of a complete trip (a baseline method used in [9]).", "startOffset": 165, "endOffset": 168}, {"referenceID": 5, "context": "[9] propose to concatenate subpaths to estimate the target route, where each subpath is consisted of multiple road segments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Similar to [9], we use mean absolute error (MAE) and mean relative error (MRE) to evaluate the travel time estimation methods:", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "We can see that MAE is the lowest when \u03c4 is in the range of [3, 6], striking a good balance between confidence and similarity.", "startOffset": 60, "endOffset": 66}, {"referenceID": 5, "context": "[9] propose a tensor decomposition method to estimate the missing values.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "SUBPATH method outperforms SEGMENT method, which is consistent with previous work [9].", "startOffset": 82, "endOffset": 85}], "year": 2015, "abstractText": "The increased availability of large-scale trajectory data around the world provides rich information for the study of urban dynamics. For example, New York City Taxi & Limousine Commission regularly releases source/destination information about trips in the taxis they regulate [1]. Taxi data provide information about traffic patterns, and thus enable the study of urban flow \u2013 what will traffic between two locations look like at a certain date and time in the future? Existing big data methods try to outdo each other in terms of complexity and algorithmic sophistication. In the spirit of \u201cbig data beats algorithms\u201d, we present a very simple baseline which outperforms state-of-the-art approaches, including Bing Maps [2] and Baidu Maps [3] (whose APIs permit large scale experimentation). Such a travel time estimation baseline has several important uses, such as navigation (fast travel time estimates can serve as approximate heuristics for A\u2217 search variants for path finding) and trip planning (which uses operating hours for popular destinations along with travel time estimates to create an itinerary).", "creator": "LaTeX with hyperref package"}}}