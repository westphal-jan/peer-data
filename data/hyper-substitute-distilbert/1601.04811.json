{"id": "1601.04811", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jan-2016", "title": "Modeling Coverage for Neural Machine Translation", "abstract": "luckily, attentional nmt generates past translated information, actually leads to over - translation and under - translation loops. in response to this problem, robots maintain a source vector to produce track of optimal tangent curve. the coverage mode remains fed under the attention model to help adjust reflect future attention, which invites others to pay special attention to the visually correct bits. experiments show that frequency - based translation significantly raises total alignment times translation times over nmt without straining.", "histories": [["v1", "Tue, 19 Jan 2016 07:09:38 GMT  (904kb,D)", "http://arxiv.org/abs/1601.04811v1", null], ["v2", "Wed, 20 Jan 2016 01:38:45 GMT  (904kb,D)", "http://arxiv.org/abs/1601.04811v2", null], ["v3", "Mon, 21 Mar 2016 01:45:22 GMT  (558kb,D)", "http://arxiv.org/abs/1601.04811v3", "Results over a stronger NMT baseline, which is now comparable with Moses"], ["v4", "Tue, 7 Jun 2016 09:51:50 GMT  (1198kb,D)", "http://arxiv.org/abs/1601.04811v4", "Accepted by ACL 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (2016)"], ["v5", "Wed, 8 Jun 2016 05:03:33 GMT  (1199kb,D)", "http://arxiv.org/abs/1601.04811v5", "The paper is accepted by ACL 2016"], ["v6", "Sat, 6 Aug 2016 17:13:04 GMT  (606kb,D)", "http://arxiv.org/abs/1601.04811v6", "Add subjective evaluation on top of ACL version: 25% of source words are under-translated by NMT"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zhaopeng tu", "zhengdong lu", "yang liu", "xiaohua liu", "hang li"], "accepted": true, "id": "1601.04811"}, "pdf": {"name": "1601.04811.pdf", "metadata": {"source": "CRF", "title": "Coverage-based Neural Machine Translation", "authors": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "emails": ["tuzhaopeng@gmail.com"], "sections": [{"heading": null, "text": "Attention mechanism advanced state-of-the-art neural machine translation (NMT) by jointly learning to align and translate. However, attentional NMT ignores past alignment information, which leads to over-translation and undertranslation problems. In response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust the future attention, which guides NMT to pay more attention to the untranslated source words. Experiments show that coverage-based NMT significantly improves both alignment and translation quality over NMT without coverage."}, {"heading": "1 Introduction", "text": "The past several years have witnessed the rapid development of end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom2013; Sutskever et al.2014; Bahdanau et al.2015). Unlike conventional statistical machine translation (SMT) (Brown et al.1993; Koehn et al.2003; Chiang2007), NMT proposes to use a single, large neural network instead of latent structures to model the translation process. This leads to the following benefits. First, the use of distributed representations of words proves to alleviate the curse of dimensionality problem (Bengio et al.2003). Second, there is no need to design features to capture translation regularities explicitly, which is very tricky in SMT. Instead, NMT is capable of learning representations directly from the training data. Third, Long Short-Term Memory (Hochreiter and Schmidhuber1997) enables NMT to capture long-distance reordering, which is a notorious challenge in SMT.\n\u2217Corresponding author: tuzhaopeng@gmail.com\nar X\niv :1\n60 1.\n04 81\n1v 1\n[ cs\n.C L\n] 1\nHowever, a serious problem with NMT is the lack of coverage. In phrase-based SMT (Koehn et al.2003), a decoder maintains a coverage vector to indicate whether a source word is translated or not. This is important for ensuring that each source word is translated exactly in decoding. The decoding process is completed when all source words are translated. In NMT, there is no such coverage vector and the decoding process ends only when the end-of-sentence tag is produced. We believe that lacking coverage might result in following problems in NMT:\n1. Over-translation: some words are unnecessarily translated for multiple times;\n2. Under-translation: some words are wrongly untranslated.\nSpecifically, in the state-of-the-art attentional NMT model (Bahdanau et al.2015), generating a target word heavily depends on the relevant parts on the source side. As each source word is involved in calculating the attention for all target words, over-translation and under-translation inevitably happen because of the inappropriate imbalance of the \u201cfertility\u201d (i.e., the number of target words generated) of source words. Figure 1 shows examples: the Chinese phrase \u201czhudao zuoyong\u201d is over translated to \u201cleading role\u201d twice (left panel), while \u201cqunian\u201d (means \u201clast year\u201d) is wrongly untranslated (right panel).\nIn this work, we propose a coverage-based approach to NMT to alleviate the over-translation and under-translation problems. Basically, we append annotation vectors to the intermediate representation of NMT models, which is updated after\neach attentive read during the decoding process to keep track of the attention history. Those annotation vectors, when entering into attention model, can help adjust the future attention and significantly improve the alignment between source and target. This design potentially contains many particular cases for coverage modeling with contrasting characteristics, which all share a clear linguistic intuition and yet can be trained in a data driven fashion. Notably, in a simple and effective case, we achieve by far the best performance by re-defining the concept of fertility, as a successful example of re-introducing linguistic knowledge into neural networkbased NLP models. Experiments on large-scale Chinese-English datasets show that our coverage-based NMT system outperforms conventional attentional NMT significantly on both translation and alignment tasks.\n2 Background\nOur work is built on attention-based NMT (RNNSearch) (Bahdanau et al.2015), which simultaneously conducts dynamic alignment and generation of the target sentence, as illustrated in Figure 2. It produces the translation by generating one target word at every time step conditioned on a context vector, the previous hidden state and the previously generated word. Given an input sentence x = {x1, . . . , xTx} and previous translated words {y1, . . . , yi\u22121}, the probability of next word yi is:\nP (yi|y1, . . . , yi\u22121,x) = g(yi\u22121, si, ci) (1)\nwhere si is an decoder hidden state for time step i, computed by\nsi = f(si\u22121, yi\u22121, ci) (2)\nHere the activation function f(\u00b7) is a gated recurrent unit (GRU) (Cho et al.2014b), and ci is a distinct context vector for time i, which is calculated as a weighted sum of the input annotations hj :\nci = Tx\u2211 j=1 \u03b1i,j \u00b7 hj (3)\nwhere hj = [ \u2212\u2192 h >j ; \u2190\u2212 h >j ] > is the annotation of xj from a bi-directional RNN (Schuster and Paliwal1997), and its weight \u03b1i,j is computed by\n\u03b1i,j = exp(ei,j)\u2211Tx k=1 exp(ei,k)\n(4)\nwhere\nei,j = a(si\u22121, hj)\n= v>a tanh(Wasi\u22121 + Uahj) (5)\nis an alignment model that scores how well yi and hj match. With the alignment model, it avoids the need to represent the entire source sentence with a fixed-length vector. Instead, the decoder selects parts of the source sentence to pay attention to, thus exploits an expected annotation ci over possible alignments \u03b1i,j for each time step i.\nThe parameters are trained to maximize the likelihood of the training data\narg max N\u2211\nn=1\nlogP (yn|xn) (6)\nHowever, the alignment model misses the opportunity to take advantage of past alignment information, which proves useful in traditional statistical machine translation (Koehn et al.2003). For example, if a source word is translated in the past, it is less likely to be translated again, thus should be assigned a lower probability."}, {"heading": "3 Coverage Model for NMT", "text": "In SMT, a coverage set is maintained to keep track of which source words have been translated (\u201ccovered\u201d) in the past. Take an input sentence x = {x1, x2, x3, x4} as an example, the initial coverage set is C = {0, 0, 0, 0} which denotes no source word is yet translated. When a translation rule bp = (x2x3, ymym+1) is used to generate translation, we produce one hypothesis labelled with coverage C = {0, 1, 1, 0}. It means that the second and third source words are translated. The\ngoal is to generate translation with full coverage C = {1, 1, 1, 1}. A source word is translated when it is covered by one translation rule, and it is not allowed to be translated again in the future (i.e. hard coverage). In this way, each source word is guaranteed to be translated and only be translated once. As shown, coverage is essential for SMT since it avoids gaps and overlap when translating source words.\nModeling coverage is also useful for neural machine translators with automatic alignment, since they generally lack a mechanism to tell whether a certain segment of source sentence is translated, and therefore prone to the \u201ccoverage\u201d mistakes: some part of source sentence is translated more than once or not translated. For neural machine translation model, directly modeling coverage is less straightforward, but the problem can be significantly alleviated by keeping track of the attention signal during the decoding process. The most natural way for doing that is to append an annotation vector \u03b2j to every hj , which is uniformly initialized but updated after every attentive read of the corresponding hidden state. This annotation vector will enter the soft attention model for alignment, as illustrated in Figure 3.\nRoughly speaking, since \u03b2i\u22121,j summarizes the attention record for hj ( and therefore for a small neighbor centering at the jth source word), it will discourage further attention to it if it has been heavily attended, and implicitly push the attention to the less attended segments of the source sentence since the attention weights are normalized to one. This could potentially solve both coverage mistakes mentioned above, when modeled and learned properly.\nFormally annotation model is given by \u03b2i,j = gupdate ( \u03b2i\u22121,j , \u03b1i,j ,\u03a6(hj), auxs ) (7)\nwhere\n\u2022 gupdate(\u00b7) is the function that updates \u03b2i,j after the new attention at time step i in the decoding process;\n\u2022 \u03b2i,j is a d-dimensional annotation vector summarizing the history of attention till time step i on hj ;\n\u2022 \u03a6i(hj) is a word-specific feature with its own parameters;\n\u2022 auxs are auxiliary inputs exploited in different sorts of coverage models;\nEquation 5 gives a rather general model, which could take different function forms for gupdate(\u00b7) and \u03a6(\u00b7), and different auxiliary inputs auxs (e.g. previous decoding state si\u22121). In the rest of this section, we will give a number of representative implementations of the annotation model, which either resort to the flexibility of neural network function approximation (Section 3.1) or bear more linguistic intuition (Section 3.2).\n3.1 Neural Network-based Coverage Model\nWhen \u03b2j is a vector (d > 1) and gupdate(\u00b7) takes a neural network (NN) form, we actually have a recurrent neural network (RNN) model for annotation, as illustrated by Figure 4. In our work, we take the following form\n\u03b2i,j = gupdate(\u03b2i\u22121,j , \u03b1i,j , hj , si\u22121)\n= tanh(U\u03b2i\u22121,j + V \u03b1i,j +Bhj +Wsi\u22121) (8)\nwhere U, V,B,W are weights and si\u22121 is the auxiliary input that encodes past translation information. Note that we leave out the the word-specific feature function \u03a6(\u00b7) and only take the input annotation hj as the input to the annotation RNN.\nIt is important to emphasize that the NN-based annotation is able to be fed with arbitrary auxiliary inputs, such as the previous attentional context ci\u22121. Here we only employ \u03b1i\u22121 for past alignment information, si\u22121 for past translation information, and hj for word-specific bias.\nGating To capture long-distance dependencies on past alignment information, we can employ gating activation function for gupdate, such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber1997) or Gated Recurrent Unit (GRU) (Cho et al.2014b). In this work, we adopt GRU since it is simple yet powerful. Then the coverage \u03b2i is computed by\n\u03b2i,j = (1\u2212 zi) \u25e6 \u03b2i\u22121,j + zi \u25e6 \u03b2\u0303i,j\nwhere\n\u03b2\u0303i,j = tanh(U [ri \u25e6 \u03b2i\u22121,j ] + V \u03b1i,j +Bhj +Wsi\u22121) zi = \u03c3(Uz\u03b2i\u22121,j + Vz\u03b1i,j +Bzhj +Wzsi\u22121)\nri = \u03c3(Ur\u03b2i\u22121,j + Vr\u03b1i,j +Brhj +Wrsi\u22121)\nwhere \u03c3(\u00b7) is a logistic sigmoid function, and zi and ri are update and reset gates respectively.\nAlthough the NN-based annotation model enjoys the flexibility brought by the recurrent nonlinear form, its lack of clear linguistic meaning may render it hard to train: the annotation model can only be trained along with the attention model and get the supervision signal from it in back-propagation, which could be weak (relatively distant from the decoding process) and noisy (after the distortion from other under-trained components in the decoder RNN). Partially to overcome this problem, we also propose the linguistically inspired model which has much clearer interpretation but much less parameters."}, {"heading": "3.2 Linguistic Coverage Model", "text": "While linguistically-inspired coverage in NMT is similar in spirit to that in SMT, there is one key difference: it indicates what percentage of source words have been translated (i.e. soft coverage). In NMT, each target word yi is generated from all source words with probabilities \u03b1i,j for source word xj . In other words, each source word xj involves in generating all target words and generates \u03b1i,j target word at time step i. Note that unlike in SMT where each source word is not fully translated at one decoding step, xj is partially translated at each decoding step in NMT . Therefore, the coverage at time step i denotes the translated ratio of that each source word is translated.\nWe use a scalar (d = 1) to represent linguistic coverages for each source word and employ an accumulate operation for gupdate. We iteratively construct linguistic coverages through an accumulation of alignment probabilities generated by the attention model, each of which is normalized by a distinct context-dependent weight. The coverage of source word xj at time step i is computed by\n\u03b2i,j = 1\n\u03a6j i\u2211 k=1 \u03b1k,j (9)\nwhere \u03a6j is a pre-defined weight which indicates the number of target words xj is expected to generate. The simplest way is to follow Xu et al. (2015) in image-tocaption translation to fix \u03a6 = 1 for all source words, which means that we directly use the sum of previous alignment probabilities without normalization as coverage for each word, as done in (Cohn et al.2016).\nHowever, in natural languages, different types of source words contributes differently to the generation of translation. Take the sentence pairs in Figure 1 as an example, the adjective on the source side \u201czhudao\u201d is translated into one target word \u201cleading\u201d, while the noun \u201cjinnian\u201d is translated into two words \u201cthis year\u201d. Therefore, we need to assign a distinct \u03a6j for each source word. Ideally, we expect \u03a6j = \u2211Ty k=1 \u03b1k,j with Ty be the total number of time steps in decoding. However, such desired value is not available before decoding, thus is not suitable in this scenario.\nFertility To predict \u03a6j , we introduce the concept of fertility, which is firstly proposed in word-level SMT (Brown et al.1993). Fertility of source word xj tells how many target words xj produces. In SMT, the fertility is a random variable \u03a6j , whose distribution p(\u03a6j = \u03c6) is determined by the parameters of word alignment models (e.g. IBM models). In this work, we compute the fertility \u03a6j by\n\u03a6j = N (xj |x) = N (hj) = N \u00b7 \u03c3(Ufhj) (10)\nwhere N \u2208 R is a predefined constantto denoting the maximum number of target words one source word can produce, \u03c3(\u00b7) is a logistic sigmoid function, and Uf \u2208 R1\u00d72n is the weight matrix. Here we use hj to denote (xj |x) since hj contains information about the whole input sentence with a strong focus on the parts surrounding xj (Bahdanau et al.2015). Since \u03a6j does not depend on i, we can pre-compute it before decoding to minimize the computational cost.\nWhile our fertility model is similar in spirit to that in SMT, there are two key differences which reflect how we simplify and adapt from the original model. First, fertility in SMT is a random variable with a set of fertility probabilities,\nn(\u03a6j |xj) = p(\u03a6j\u221211 ,x), which depends on the fertilities of previous source words. To simplify the calculation and adapt it to the attention model in NMT, we define the fertility in NMT as a constant number, which is independent of previous fertilities. Second, \u03a6j in SMT is an integer sum over binary alignment decisions whereas that in NMT is a real sum over soft alignment probabilities."}, {"heading": "3.3 Integrating Coverage into NMT", "text": "Although the introduction of alignment model has advanced the state-of-the-art of NMT, it computes soft alignment probabilities without considering useful information in the past. For example, a source word that contributed a lot to the predicted target words in the past, should be assigned lower alignment probabilities in the following decoding. Motivated by this observation, in this work, we propose to calculate the alignment probability by jointly taking into account past alignment information (e.g. which source words have been translated).\nIntuitively, at each time step i in the decoding phase, coverage from time step (i \u2212 1) serves as input to the attention model, which provides complementary information of that how likely the source words are translated in the past. We expect the coverage information would guide the attention model to focus more on untranslated source words (i.e. assign higher probabilities). In practice, we find that the coverage model does come up to expectation (see Section 5). The translated ratios of source words from linguistic coverages negatively correlate to\nthe corresponding alignment probabilities. Figure 5 shows an example, in which coverage-based NMT indeed alleviates the problems of over-translation and undertranslation shown in Figure 1.\nMore formally, we rewrite the alignment model in Equation 5 as\nei,j = a(si\u22121, hj , \u03b2i\u22121,j)\n= v>a tanh(Wasi\u22121 + Uahj +Ba\u03b2i\u22121,j) (11)\nwhere \u03b2i\u22121,j is the translated ratio of source word xj before time i. Bd \u2208 Rn\u00d71 is the additional weight matrix for coverage with n and d be the numbers of hidden units and coverage units respectively."}, {"heading": "4 Training", "text": "In this paper, we take end-to-end learning for our coverage-based NMT model, which jointly learns not only the parameters for the \u201coriginal\u201d RNNsearch (i.e., those for encoding RNN, decoding RNN, and attention model) but also the parameters for coverage modeling (i.e., those for annotation and its role in guiding the attention) . More specially, we choose to maximize the likelihood of reference sentences as most other neural machine translator (see, however(Shen et al.2015))\narg max N\u2211 n=1 logP (yn|xn). (12)\nFor the coverage model with a clearer linguistic interpretation (Section 3.2), it is possible to inject an auxiliary objective function on some intermediate representation. More specifically, we have the following objective\narg max N\u2211\nn=1\n( logP (yn|xn)\n\u2212 \u03bb |xn|\u2211 j=1 (\u03a6j \u2212 |yn|\u2211 i=1 \u03b1i,j) 2 )\n(13)\nwhere the term \u2211|xn| j=1(\u03a6j \u2212 \u2211|yn| i=1 \u03b1i,j) 2 penalizes the discrepancy between the sum of attention and the expect fertility for linguistic coverage. This is similar to the more explicit training for fertility as in Xu et al. (2015), which directly penalizes the discrepancy between each \u03a6j and the sum of attention to the corresponding hj .\nOur end-to-end training strategy poses less constraints on the dependency between {\u03a6j} and the attention than a more explicit strategy taken in (Xu et al.2015),\nand let the objective associated with the translation quality (i.e., the likelihood) drive the training. This strategy is arguably advantageous, since the attention weight on a hidden state hj cannot be interpreted as the proportion of the corresponding word being translated on the target side. For one thing, the hidden state {hj}, after the transformation from encoding RNN, bear the contextual information from other parts of the source sentence and therefore lose the rigid correspondence with the corresponding words. Our empirical study shows that a combined objective as in Equationeqn-coverage-training consistently worsens the translation quality (BLEU score) while gaining slightly on the alignment."}, {"heading": "5 Experiments", "text": "We report our empirical study on applying coverage-based NMT to Chinese-toEnglish translation, and compare it against state-of-the-art NMT and SMT models."}, {"heading": "5.1 Setup", "text": "Dataset and Evaluation Metrics Our training data consists of 1.25M sentence pairs extracted from LDC corpora1, with 27.9M Chinese words and 34.5M English words respectively. We choose NIST 2002 (MT02) dataset as our development set, and the NIST 2005 (MT05), 2006 (MT06) and 2008 (MT08) datasets as our test sets. We use the case-insensitive 4-gram NIST BLEU score (Papineni et al.2002) as our evaluation metric, and sign-test (Collins et al.2005) as statistical significance test. In addition to BLEU score to evaluate the translation quality, we also specifically check the alignment quality with alignment error rate (AER) (Och and Ney2003).\nTraining Neural Networks In training of the neural networks, we limit the source and target vocabularies to the most frequent 16K words in Chinese and English, covering approximately 95.8% and 98.3% of the two corpora respectively. All the out-of-vocabulary words are mapped to a special token UNK. We train each model with the sentences of length up to 50 words in training data. The word embedding dimension is 620 and the size of a hidden layer is 1000. We set the dimension of coverage d = 1 for both NN-based2 and linguistic coverage models and set\n1The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.\n2In a pilot study, increasing the dimension of NN-based coverage did not improve the translation performance.\nN = 2 for the fertility model. We train our models until the BLEU score on the development set stopped improving.\nWe compare our method with two state-of-the-art SMT and NMT3 models:\n\u2022 Moses (Koehn et al.2007): an open source phrase-based translation system with default configuration and a 4-gram language model trained on the target portion of training data;\n\u2022 RNNsearch (Bahdanau et al.2015): an attentional NMT model with default setting.\nWe use the RNNsearch as the NMT baseline, for it represents the state-of-the-art neural machine translation methods with a small vocabulary and modest parameter size (30M\u223c50M)."}, {"heading": "5.2 Translation Quality", "text": "Table 1 shows the translation performances measured in BLEU score. Clearly the proposed COVERAGE-BASED NMT significantly improves the translation quality in all cases, although there are still considerable differences among different variants. More specifically,\n\u2022 NN-based Coverages (Rows 3 and 4 in Table 1): Both variants of NNbased coverages outperform RNNSearch with averaged gains of 0.63 and 0.94 BLEU points, respectively. Introducing gating activation function improves the performance of coverage models, which is consistent with the results in other tasks (e.g. (Cho et al.2014b)).\n3There are recent progress on aggregating multiple models or enlarging the vocabulary(e.g., in (Jean et al.2015)), but here we focus on the generic models.\n\u2022 Linguistic Coverages (Rows 5 and 6 in Table 1): Two observations can be made. First, linguistic coverages overall outperforms its NN-based counterparts, indicating that explicitly linguistic regularities are very important to the attention model. This is further verified on the alignment task (Section 5.3). Second, incorporating fertility model boosts performance by better estimating the covered ratios of source words."}, {"heading": "5.3 Alignment Quality", "text": "In this section, we investigate the quality of different alignments on the ChineseEnglish language pair data. We carried out experiments on the evaluation dataset from (Liu and Sun2015), which contains 900 manually aligned sentence pairs. We evaluate alignments in terms of AER:\nAER(S, P,A) = 1\u2212 |A \u2229 S|+ |A \u2229 P | |A|+ |S|\nwhere S is a set of sure links in a hand-aligned reference alignment, P is a set of possible links in the reference alignment, and A is a candidate alignment. Note that S is a subset of P : S \u2286 P .\nGiven that AER is designed specifically for binary alignments in SMT, we design a variant of AER for soft alignments in NMT, naming SAER:\nSAER(S, P,A) = 1\u2212 |MA \u00d7MS |+ |MA \u00d7MP | |MA|+ |MS |\nwhere M denotes alignment matrix, and for both MS and MP we assign the links in S and P with probabilities 1.0 while assign the other links with probabilities 0.0. In this way, we are able to better evaluate the quality of the soft alignments produced by attentional NMT.\nWe follow Luong et al. (2015) to \u201cforce\u201d decode NMT models to produce translations that match references. We extract both (1) one-to-one alignments by\nselecting the source word with the highest alignment probability for each target word, and (2) alignment matrices that consist of alignment probabilities from all source words for each target word. We measure their qualities with AER and SAER respectively, as shown in Table 2.4\nWe find that coverage information improves attention model as expected by maintain an annotation summarizing the log of previous attention on each source word. More specifically, linguistic coverage with fertility significantly reduces alignment errors under both metrics, in which fertility plays an important role.\n4Our results are basically consistent with Cheng et al. (2015) on the same evaluation data. The overall error rates in Table 2 are around 2 points higher than theirs for two reasons: (1) the size of our training data is half as much as theirs, and (2) we don\u2019t implement the technique in (Jean et al.2015) to address unknown words while they did.\nFigure 6 shows example alignment matrices, which shows linguistic coverages significantly improves the alignment accuracy. NN-based coverages, however, only slightly reduces alignment errors, which is consistent with the performance on the translation task. It reconfirms our claim that linguistic coverages provide more explicit signals to the attention model, which is the key to the success."}, {"heading": "5.4 Effects on Long Sentences", "text": "We follow Bahdanau et al. (2015) to group sentences of similar lengths together and compute a BLEU score and an averaged length of translation per group, as shown in Figure 7. Cho et al. (2014a) shows that the performance of NMT drops rapidly when the length of input sentence increases. Our results confirm these findings. One main reason is that NMT produces much shorter translations on longer sentences (e.g. > 40, see right panel in Figure 7), thus faces a serious undertranslation problem. Coverage-based NMT alleviates this problem through incorporating coverage information into the attention model, which in general pushes the attention to untranslated parts of the input sentence and implicitly discourages the early stop of the decoding process."}, {"heading": "6 Related Work", "text": "Our work is inspired by recent works on improving attentional NMT. Attention mechanism advanced state of the art NMT by jointly learning to align and translate (Bahdanau et al.2015; Luong et al.2015). The notion of attention corresponds well to that of alignment in traditional word-based SMT (Brown et al.1993), giving the opportunities to be further improved with techniques that have been applied with success in SMT. Following the success of minimum risk training (MRT) in conventional SMT (Och2003), Shen et al. (2015) proposed MRT for end-to-end NMT to optimize model parameters directly with repsect to evaluation metrics. Based on the observation that the default unidirectional attentional NMT only captures partial aspects of attentional regularities due to the non-isomorphism of natural languages, Cheng et al. (2015) proposed an agreement-based learning (Liang et al.2006) to encourage bidirectional attention models to agree on parameterized alignment matrices. Along the same direction, inspired by the essential coverage in SMT to avoid gaps and overlap when translating source words, we propose a coverage-based approach to NMT to alleviate the over-translation and undertranslation problems.\nConcurrent with our work, Cohn et al. (2016) and Feng et al. (2016) made use of the concept of \u201cfertility\u201d for the attention model, which is similar in spirit to our method for building the linguistically inspired coverage with fertility. Cohn et al. (2016) introduced a feature-based fertility that includes the total alignment scores for the surrounding source words. In contrast, we build a prediction of fertility to decide how many target words each source produces before decoding. The expected fertility then works as a normalizer to better estimate the covered ratio of each source word, which guides the alignment model to pay more attention to uncovered words. Feng et al. (2016) used the previous attentional context to represent implicit fertility and directly passed it to the decoder , which is in essence similar to the input-feed method proposed in (Luong et al.2015). Comparatively, we predict explicit fertility for each source word based on its encoding annotation, and incorporate it into the linguistic-inspired coverage for attention model. In this work, we show that the explicitly designed fertility (or coverage) outperforms its implicit neural network-based counterpart in both translation and alignment tasks. There is one minor difference as well: we validate the effectiveness of our approach on a large-scale corpus while both Cohn et al. (2016) and Feng et al. (2016) did on small-scale corpora."}, {"heading": "7 Conclusion", "text": "We have presented an approach to maintain a coverage vector for NMT to indicate whether each source word is translated or not. By encouraging attentional NMT to pay more attention to untranslated words and less attention to translated words, coverage-based NMT alleviates the serious over-translation and under-translation problems that attentional NMT suffers. Experimental results show that coveragebased NMT achieves significant improvements in terms of alignment and translation quality over NMT without coverage.\nIn the future, we plan to further validate the effectiveness of our approach on more language pairs. Further directions also include better designs of coverages model and making better use of the coverage information (e.g. directly pass it to the decoder)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR 2015.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "JMLR.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter E. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer."], "venue": "Computational Linguistics, 19(2):263\u2013311.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Agreementbased Joint Training for Bidirectional Attention-based Neural Machine Translation", "author": ["Y. Cheng", "S. Shen", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu."], "venue": "arXiv.", "citeRegEx": "Cheng et al\\.,? 2015", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "CL.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "On the properties of neural machine translation: encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "SSST 2014.", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP 2014.", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model", "author": ["T. Cohn", "C.D.V. Hoang", "E. Vymolova", "K. Yao", "C. Dyer", "G. Haffari."], "venue": "arXiv.", "citeRegEx": "Cohn et al\\.,? 2016", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Clause restructuring for statistical machine translation", "author": ["M. Collins", "P. Koehn", "I. Ku\u010derov\u00e1."], "venue": "ACL 2005.", "citeRegEx": "Collins et al\\.,? 2005", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model", "author": ["S. Feng", "S. Liu", "M. Li", "M. Zhou."], "venue": "arXiv.", "citeRegEx": "Feng et al\\.,? 2016", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber."], "venue": "Neural Computation.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "ACL 2015.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP 2013.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "NAACL 2003.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst."], "venue": "ACL 2007.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Alignment by agreement", "author": ["Percy Liang", "Ben Taskar", "Dan Klein."], "venue": "NAACL 2006.", "citeRegEx": "Liang et al\\.,? 2006", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Contrastive unsupervised word alignment with non-local features", "author": ["Yang Liu", "Maosong Sun."], "venue": "AAAI.", "citeRegEx": "Liu and Sun.,? 2015", "shortCiteRegEx": "Liu and Sun.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "EMNLP 2015.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz J. Och", "Hermann Ney."], "venue": "Computational Linguistics, 29(1):19\u201351.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "ACL 2003.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "ACL 2002.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "IEEE Transactions on Signal Processing, 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Minimum Risk Training for Neural Machine Translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu."], "venue": "arXiv.", "citeRegEx": "Shen et al\\.,? 2015", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "NIPS 2014.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "ICML 2015.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 23, "context": "The simplest way is to follow Xu et al. (2015) in image-tocaption translation to fix \u03a6 = 1 for all source words, which means that we directly use the sum of previous alignment probabilities without normalization as coverage for each word, as done in (Cohn et al.", "startOffset": 30, "endOffset": 47}, {"referenceID": 24, "context": "This is similar to the more explicit training for fertility as in Xu et al. (2015), which directly penalizes the discrepancy between each \u03a6j and the sum of attention to the corresponding hj .", "startOffset": 66, "endOffset": 83}, {"referenceID": 16, "context": "We carried out experiments on the evaluation dataset from (Liu and Sun2015), which contains 900 manually aligned sentence pairs. We evaluate alignments in terms of AER: AER(S, P,A) = 1\u2212 |A \u2229 S|+ |A \u2229 P | |A|+ |S| where S is a set of sure links in a hand-aligned reference alignment, P is a set of possible links in the reference alignment, and A is a candidate alignment. Note that S is a subset of P : S \u2286 P . Given that AER is designed specifically for binary alignments in SMT, we design a variant of AER for soft alignments in NMT, naming SAER: SAER(S, P,A) = 1\u2212 |MA \u00d7MS |+ |MA \u00d7MP | |MA|+ |MS | where M denotes alignment matrix, and for both MS and MP we assign the links in S and P with probabilities 1.0 while assign the other links with probabilities 0.0. In this way, we are able to better evaluate the quality of the soft alignments produced by attentional NMT. We follow Luong et al. (2015) to \u201cforce\u201d decode NMT models to produce translations that match references.", "startOffset": 59, "endOffset": 902}, {"referenceID": 3, "context": "Our results are basically consistent with Cheng et al. (2015) on the same evaluation data.", "startOffset": 42, "endOffset": 62}, {"referenceID": 0, "context": "We follow Bahdanau et al. (2015) to group sentences of similar lengths together and compute a BLEU score and an averaged length of translation per group, as shown in Figure 7.", "startOffset": 10, "endOffset": 33}, {"referenceID": 0, "context": "We follow Bahdanau et al. (2015) to group sentences of similar lengths together and compute a BLEU score and an averaged length of translation per group, as shown in Figure 7. Cho et al. (2014a) shows that the performance of NMT drops rapidly when the length of input sentence increases.", "startOffset": 10, "endOffset": 195}, {"referenceID": 0, "context": "Attention mechanism advanced state of the art NMT by jointly learning to align and translate (Bahdanau et al.2015; Luong et al.2015). The notion of attention corresponds well to that of alignment in traditional word-based SMT (Brown et al.1993), giving the opportunities to be further improved with techniques that have been applied with success in SMT. Following the success of minimum risk training (MRT) in conventional SMT (Och2003), Shen et al. (2015) proposed MRT for end-to-end NMT to optimize model parameters directly with repsect to evaluation metrics.", "startOffset": 94, "endOffset": 457}, {"referenceID": 0, "context": "Attention mechanism advanced state of the art NMT by jointly learning to align and translate (Bahdanau et al.2015; Luong et al.2015). The notion of attention corresponds well to that of alignment in traditional word-based SMT (Brown et al.1993), giving the opportunities to be further improved with techniques that have been applied with success in SMT. Following the success of minimum risk training (MRT) in conventional SMT (Och2003), Shen et al. (2015) proposed MRT for end-to-end NMT to optimize model parameters directly with repsect to evaluation metrics. Based on the observation that the default unidirectional attentional NMT only captures partial aspects of attentional regularities due to the non-isomorphism of natural languages, Cheng et al. (2015) proposed an agreement-based learning (Liang et al.", "startOffset": 94, "endOffset": 763}, {"referenceID": 0, "context": "Attention mechanism advanced state of the art NMT by jointly learning to align and translate (Bahdanau et al.2015; Luong et al.2015). The notion of attention corresponds well to that of alignment in traditional word-based SMT (Brown et al.1993), giving the opportunities to be further improved with techniques that have been applied with success in SMT. Following the success of minimum risk training (MRT) in conventional SMT (Och2003), Shen et al. (2015) proposed MRT for end-to-end NMT to optimize model parameters directly with repsect to evaluation metrics. Based on the observation that the default unidirectional attentional NMT only captures partial aspects of attentional regularities due to the non-isomorphism of natural languages, Cheng et al. (2015) proposed an agreement-based learning (Liang et al.2006) to encourage bidirectional attention models to agree on parameterized alignment matrices. Along the same direction, inspired by the essential coverage in SMT to avoid gaps and overlap when translating source words, we propose a coverage-based approach to NMT to alleviate the over-translation and undertranslation problems. Concurrent with our work, Cohn et al. (2016) and Feng et al.", "startOffset": 94, "endOffset": 1188}, {"referenceID": 0, "context": "Attention mechanism advanced state of the art NMT by jointly learning to align and translate (Bahdanau et al.2015; Luong et al.2015). The notion of attention corresponds well to that of alignment in traditional word-based SMT (Brown et al.1993), giving the opportunities to be further improved with techniques that have been applied with success in SMT. Following the success of minimum risk training (MRT) in conventional SMT (Och2003), Shen et al. (2015) proposed MRT for end-to-end NMT to optimize model parameters directly with repsect to evaluation metrics. Based on the observation that the default unidirectional attentional NMT only captures partial aspects of attentional regularities due to the non-isomorphism of natural languages, Cheng et al. (2015) proposed an agreement-based learning (Liang et al.2006) to encourage bidirectional attention models to agree on parameterized alignment matrices. Along the same direction, inspired by the essential coverage in SMT to avoid gaps and overlap when translating source words, we propose a coverage-based approach to NMT to alleviate the over-translation and undertranslation problems. Concurrent with our work, Cohn et al. (2016) and Feng et al. (2016) made use of the concept of \u201cfertility\u201d for the attention model, which is similar in spirit to our method for building the linguistically inspired coverage with fertility.", "startOffset": 94, "endOffset": 1211}, {"referenceID": 0, "context": "Attention mechanism advanced state of the art NMT by jointly learning to align and translate (Bahdanau et al.2015; Luong et al.2015). The notion of attention corresponds well to that of alignment in traditional word-based SMT (Brown et al.1993), giving the opportunities to be further improved with techniques that have been applied with success in SMT. Following the success of minimum risk training (MRT) in conventional SMT (Och2003), Shen et al. (2015) proposed MRT for end-to-end NMT to optimize model parameters directly with repsect to evaluation metrics. Based on the observation that the default unidirectional attentional NMT only captures partial aspects of attentional regularities due to the non-isomorphism of natural languages, Cheng et al. (2015) proposed an agreement-based learning (Liang et al.2006) to encourage bidirectional attention models to agree on parameterized alignment matrices. Along the same direction, inspired by the essential coverage in SMT to avoid gaps and overlap when translating source words, we propose a coverage-based approach to NMT to alleviate the over-translation and undertranslation problems. Concurrent with our work, Cohn et al. (2016) and Feng et al. (2016) made use of the concept of \u201cfertility\u201d for the attention model, which is similar in spirit to our method for building the linguistically inspired coverage with fertility. Cohn et al. (2016) introduced a feature-based fertility that includes the total alignment scores for the surrounding source words.", "startOffset": 94, "endOffset": 1401}, {"referenceID": 0, "context": "Attention mechanism advanced state of the art NMT by jointly learning to align and translate (Bahdanau et al.2015; Luong et al.2015). The notion of attention corresponds well to that of alignment in traditional word-based SMT (Brown et al.1993), giving the opportunities to be further improved with techniques that have been applied with success in SMT. Following the success of minimum risk training (MRT) in conventional SMT (Och2003), Shen et al. (2015) proposed MRT for end-to-end NMT to optimize model parameters directly with repsect to evaluation metrics. Based on the observation that the default unidirectional attentional NMT only captures partial aspects of attentional regularities due to the non-isomorphism of natural languages, Cheng et al. (2015) proposed an agreement-based learning (Liang et al.2006) to encourage bidirectional attention models to agree on parameterized alignment matrices. Along the same direction, inspired by the essential coverage in SMT to avoid gaps and overlap when translating source words, we propose a coverage-based approach to NMT to alleviate the over-translation and undertranslation problems. Concurrent with our work, Cohn et al. (2016) and Feng et al. (2016) made use of the concept of \u201cfertility\u201d for the attention model, which is similar in spirit to our method for building the linguistically inspired coverage with fertility. Cohn et al. (2016) introduced a feature-based fertility that includes the total alignment scores for the surrounding source words. In contrast, we build a prediction of fertility to decide how many target words each source produces before decoding. The expected fertility then works as a normalizer to better estimate the covered ratio of each source word, which guides the alignment model to pay more attention to uncovered words. Feng et al. (2016) used the previous attentional context to represent implicit fertility and directly passed it to the decoder , which is in essence similar to the input-feed method proposed in (Luong et al.", "startOffset": 94, "endOffset": 1833}, {"referenceID": 0, "context": "Attention mechanism advanced state of the art NMT by jointly learning to align and translate (Bahdanau et al.2015; Luong et al.2015). The notion of attention corresponds well to that of alignment in traditional word-based SMT (Brown et al.1993), giving the opportunities to be further improved with techniques that have been applied with success in SMT. Following the success of minimum risk training (MRT) in conventional SMT (Och2003), Shen et al. (2015) proposed MRT for end-to-end NMT to optimize model parameters directly with repsect to evaluation metrics. Based on the observation that the default unidirectional attentional NMT only captures partial aspects of attentional regularities due to the non-isomorphism of natural languages, Cheng et al. (2015) proposed an agreement-based learning (Liang et al.2006) to encourage bidirectional attention models to agree on parameterized alignment matrices. Along the same direction, inspired by the essential coverage in SMT to avoid gaps and overlap when translating source words, we propose a coverage-based approach to NMT to alleviate the over-translation and undertranslation problems. Concurrent with our work, Cohn et al. (2016) and Feng et al. (2016) made use of the concept of \u201cfertility\u201d for the attention model, which is similar in spirit to our method for building the linguistically inspired coverage with fertility. Cohn et al. (2016) introduced a feature-based fertility that includes the total alignment scores for the surrounding source words. In contrast, we build a prediction of fertility to decide how many target words each source produces before decoding. The expected fertility then works as a normalizer to better estimate the covered ratio of each source word, which guides the alignment model to pay more attention to uncovered words. Feng et al. (2016) used the previous attentional context to represent implicit fertility and directly passed it to the decoder , which is in essence similar to the input-feed method proposed in (Luong et al.2015). Comparatively, we predict explicit fertility for each source word based on its encoding annotation, and incorporate it into the linguistic-inspired coverage for attention model. In this work, we show that the explicitly designed fertility (or coverage) outperforms its implicit neural network-based counterpart in both translation and alignment tasks. There is one minor difference as well: we validate the effectiveness of our approach on a large-scale corpus while both Cohn et al. (2016) and Feng et al.", "startOffset": 94, "endOffset": 2519}, {"referenceID": 0, "context": "Attention mechanism advanced state of the art NMT by jointly learning to align and translate (Bahdanau et al.2015; Luong et al.2015). The notion of attention corresponds well to that of alignment in traditional word-based SMT (Brown et al.1993), giving the opportunities to be further improved with techniques that have been applied with success in SMT. Following the success of minimum risk training (MRT) in conventional SMT (Och2003), Shen et al. (2015) proposed MRT for end-to-end NMT to optimize model parameters directly with repsect to evaluation metrics. Based on the observation that the default unidirectional attentional NMT only captures partial aspects of attentional regularities due to the non-isomorphism of natural languages, Cheng et al. (2015) proposed an agreement-based learning (Liang et al.2006) to encourage bidirectional attention models to agree on parameterized alignment matrices. Along the same direction, inspired by the essential coverage in SMT to avoid gaps and overlap when translating source words, we propose a coverage-based approach to NMT to alleviate the over-translation and undertranslation problems. Concurrent with our work, Cohn et al. (2016) and Feng et al. (2016) made use of the concept of \u201cfertility\u201d for the attention model, which is similar in spirit to our method for building the linguistically inspired coverage with fertility. Cohn et al. (2016) introduced a feature-based fertility that includes the total alignment scores for the surrounding source words. In contrast, we build a prediction of fertility to decide how many target words each source produces before decoding. The expected fertility then works as a normalizer to better estimate the covered ratio of each source word, which guides the alignment model to pay more attention to uncovered words. Feng et al. (2016) used the previous attentional context to represent implicit fertility and directly passed it to the decoder , which is in essence similar to the input-feed method proposed in (Luong et al.2015). Comparatively, we predict explicit fertility for each source word based on its encoding annotation, and incorporate it into the linguistic-inspired coverage for attention model. In this work, we show that the explicitly designed fertility (or coverage) outperforms its implicit neural network-based counterpart in both translation and alignment tasks. There is one minor difference as well: we validate the effectiveness of our approach on a large-scale corpus while both Cohn et al. (2016) and Feng et al. (2016) did on small-scale corpora.", "startOffset": 94, "endOffset": 2542}], "year": 2016, "abstractText": "Attention mechanism advanced state-of-the-art neural machine translation (NMT) by jointly learning to align and translate. However, attentional NMT ignores past alignment information, which leads to over-translation and undertranslation problems. In response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust the future attention, which guides NMT to pay more attention to the untranslated source words. Experiments show that coverage-based NMT significantly improves both alignment and translation quality over NMT without coverage.", "creator": "LaTeX with hyperref package"}}}