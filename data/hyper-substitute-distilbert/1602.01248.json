{"id": "1602.01248", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2016", "title": "Using Hadoop for Large Scale Analysis on Twitter: A Technical Report", "abstract": "sentiment analysis ( or opinion control ) throughout twitter data networks faced much attention thereafter. one distinguishing the respondents'new key results, is the immediacy in familiarity with agency users in an easy, user - friendly and pleasant model. instead, people tend to express their actions freely, customer becomes oneself easily intuitive source platform accumulating often larger amount of opinions towards potentially wide mixture of topics. this amount of information is compelling scope and can facilitate obtained helping receive the sentiment effect versus these topics. however, since none can invest these infinite amount of time to read purely mobile tweets, user automated sound making approach most appropriate. nevertheless, most policy solutions are limited in human environments only. importantly, they will only release at most a few thousand analyses. entering a program, is deliberately structured to distinguish positive sentiment polarity to a topic due to the growing number of tweets published continually. in this paper, we go upwards step further and develop a novel method for sentiment learning in the marketing framework. experimental algorithm takes the hashtags sending emoticons inside static baseline, destroys sentiment labels, and proceeds to deliver complete procedure involving various sentiment types in a hierarchy and hierarchical methodology. moreover, participants utilize insight methods to obtain roughly entire size of whole data and boost this clarity of our algorithm. through an extensive insight evaluation, we discover that our solution is efficient, consider message affordable and confirm the values predict our sentiment identification.", "histories": [["v1", "Wed, 3 Feb 2016 10:19:19 GMT  (260kb)", "http://arxiv.org/abs/1602.01248v1", "8 pages, 3 tables, 3 figures"]], "COMMENTS": "8 pages, 3 tables, 3 figures", "reviews": [], "SUBJECTS": "cs.DB cs.CL cs.IR", "authors": ["nikolaos nodarakis", "spyros sioutas", "athanasios tsakalidis", "giannis tzimas"], "accepted": false, "id": "1602.01248"}, "pdf": {"name": "1602.01248.pdf", "metadata": {"source": "CRF", "title": "Using Hadoop for Large Scale Analysis on Twitter: A Technical Report", "authors": ["Nikolaos Nodarakis", "Spyros Sioutas", "Athanasios Tsakalidis", "Giannis Tzimas"], "emails": ["nodarakis@ceid.upatras.gr", "sioutas@ionio.gr", "tsak@ceid.upatras.gr", "tzimas@cti.gr"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n01 24\n8v 1\n[ cs\n.D B"}, {"heading": "CCS Concepts", "text": "\u2022Computing methodologies\u2192MapReduce algorithms; Feature selection;"}, {"heading": "Keywords", "text": "big data; Bloom filters; classification; MapReduce; Hadoop; sentiment analysis; text mining; twitter"}, {"heading": "1. INTRODUCTION", "text": "Twitter is one of the most popular social network websites and launched in 2006. Since then, it has grown at a very fast pace and at the time speaking numbers 316 million monthly active users, while 500 millions tweets are sent on a daily basis1. Naturally, it is a wide spreading instant messaging platform and people use it to get informed about world news, videos that have become viral, discussions over recently released products or technological advancements, etc. Inevitably, a cluster of different opinions, that carry rich sentiment information and concern a variety of entities or topics, is formed. Sentiment is defined as \u201dA thought, view, or attitude, especially one based mainly on emotion instead of reason\u201d2 and describes someone\u2019s mood or judge towards a specific entity. User-generated content that captures sentiment information has proved to be valuable and its use is widespread among many internet applications and information systems, such as search engines.\nKnowing the overall sentiment inclination towards a topic, provides very useful information and can be captivating in certain cases. For instance, Google would like to know what their users think about the latest Android 5.0 update, in order to proceed to further development and bug fixing until the operating system works smoothly and meets the needs of the users. Thus, it is clear that a concise sentiment analysis towards the topic during a time period is needed. Two of the most known websites that perform sentiment analysis on Twitter are Topsy3 and Sentiment1404.\nIn the context of this work, we utilize hashtags and emoticons as sentiment labels to perform classification of diverse sentiment types. Hashtags are a convention for adding additional context and metadata to tweets. They are created by users as a way to categorize their message and/or highlight\n1https://about.twitter.com/company (Visited 19/9/2015) 2http://www.thefreedictionary.com/sentiment 3http://topsy.com/ 4http://www.sentiment140.com/\na topic and are extensively utilized in tweets [18]. Moreover, they provide the ability to people to search tweets that refer to a common subject. The creation of a hashtag is achieved by prefixing a word with a hash symbol (e.g. #love). Emoticon refers to a digital icon or a sequence of keyboard symbols that serves to represent a facial expression, as :-) for a smiling face5. Both, hashtags and emoticons, provide a fine-grained sentiment learning at tweet level which makes them suitable to be leveraged for opinion mining.\nAlthough the problem of sentiment analysis has been studied extensively during recent years, existing solutions suffer from certain limitations. One problem is that the majority of approaches is bounded in centralized environments. Moreover, sentiment analysis is based on, it terms of methodology, natural language processing techniques and machine learning approaches. However, this kind of techniques are time-consuming and spare many computational resources. Consequently, at most a few thousand records can be processed by such techniques without exceeding the capabilities of a single server. Since millions of tweets are published daily on Twitter, it is more than clear that underline solutions are not sufficient. Consequently, high scalable implementations are required in order to acquire a much better overview of sentiment tendency towards a topic. Cloud computing technologies provide tools and infrastructure to create such solutions and manage the input data in a distributed way among multiple servers. The most popular and notably efficient tool is the MapReduce [7] programming model, developed by Google, for processing large-scale data.\nIn this paper, we propose MR-SAT: a novel MapReduce Algorithm for Big Data Sentiment Analysis on Twitter implemented in Hadoop [17, 19], the open source MapReduce implementation. Our algorithm exploits the hashtags and emoticons inside a tweet, as sentiment labels, in order to avoid the time-intensive manual annotation task. After that, we build the feature vectors of training and test set and proceed to a classification procedure in a fully distributed manner. Additionally, we encode features using Bloom filters to compress the storage space of the feature vectors. We adapt an existing MapReduce classification algorithm based on AkNN queries to achieve the desirable outcome. Through an extensive experimental evaluation we study various parameters that can affect the total computation cost and classification performance. We prove that our solution is efficient, robust and scalable and confirm the quality of our sentiment identification.\nThe rest of the paper is organized as follows: in Section 2 we discuss related work and in Section 3 we present how our algorithm works. More specifically, we explain how to build the feature vectors (for both the training and test dataset), we briefly describe the Bloom filter integration and display our AkNN based classification algorithm. After that, we proceed to the experimental evaluation of our approach in Section 4, while in Section 5 we conclude the paper and present future steps."}, {"heading": "2. RELATED WORK", "text": "The domain of sentiment analysis, or opinion mining, has\n5http://dictionary.reference.com/browse/emoticon\nbeen studied extensively in literature during decent years. Early studies focus on document level sentiment analysis concerning movie or product reviews [9, 25] and posts published on webpages or blogs [24]. Respectively, many efforts have been made towards the sentence level sentiment analysis [20, 21, 23] which examines phrases and assigns to each one of them a sentiment polarity (positive, negative, neutral). A less investigated area is the topic-based sentiment analysis [12, 13] due to the difficulty to provide an adequate definition of topic and how to incorporate the sentiment factor into the opinion mining task.\nMany researchers confront the problem of sentiment analysis by applying machine learning approaches and/or natural language processing techniques. In [16], the authors employ three machine learning techniques to classify movie reviews as positive or negative. On the other hand, Nasukawa and Yi [14] investigate the proper identification of semantic relationships between the sentiment expressions and the subject, in order to enhance the accuracy of sentiment analysis within webpages and online articles. Their approach utilizes a syntactic parser and a sentiment lexicon. Moreover, Ding and Liu [8] propose a set of linguistic rules together with a new opinion aggregation function to detect sentiment orientations in online product reviews.\nNowadays, Twitter has received much attention for sentiment analysis, as it provides a source of massive usergenerated content that captures a wide aspect of published opinions. In [2], the authors propose a 2-step classifier that separates messages as subjective and objective, and further distinguishes the subjective tweets as positive or negative. Davidov et al. [6] exploit the hashtags and smileys in tweets and evaluate the contribution of different features (e.g. unigrams) together with a kNN classifier. In this paper, we adopt this approach and create a parallel and distributed version of the algorithm for large scale Twitter data. Agarwal et al. [1] explore the use of a tree kernel model for detecting sentiment orientation in tweets. A three-step classifier is proposed in [10] that follows a targetdependent sentiment classification strategy by incorporating target-dependent features and taking related tweets into consideration. Moreover, the authors in [18] perform a topic sentiment analysis in Twitter data through a graph-based model. A more recent approach [22], investigates the role of emoticons for multidimensional sentiment analysis of Twitter by constructing a sentiment and emoticon lexicon. A large scale solution is presented in [11] where the authors build a sentiment lexicon and classify tweets using a MapReduce algorithm and a distributed database model. Although the classification performance is quite good, the construction of sentiment lexicon needs a lot of time. Our approach is much simpler and, to our best knowledge, we are the first to present a robust large scale approach for opinion mining on Twitter data without the need of building a sentiment lexicon or proceeding to any manual data annotation."}, {"heading": "3. MR-SAT APPROACH", "text": "We begin this section by providing a formal definition of the problem we try to tackle and then we present the features we use for sentiment classification. Finally, we describe our algorithm using pseudo-codes and proceed to a\nstep by step explanation of each pseudo-code. Assume a set of hashtags H = {h1, h2, . . . , hn} and a set of emoticons E = {em1, em2, . . . , emm} associated with a set of tweets T = {t1, t2, . . . , tl} (training set). Each t \u2208 T carries only one sentiment label from L = H\u222aE. This means that tweets containing more that one labels from L are not candidates for T , since their sentiment tendency may be vague. Given a set of unlabelled tweets TT = {tt1, tt2, . . . , ttk} (test set), we aim to infer the sentiment polarities p = {p1, p2, . . . , pk} for TT , where pi \u2208 L\u222a{neu} and neu means that the tweet carries no sentiment information. We build a tweet-level classifier C and adopt a kNN strategy to decide the sentiment tendency \u2200tt \u2208 TT . We implement C by adapting an existing MapReduce classification algorithm based on AkNN queries [15], as described in Subsection 3.3."}, {"heading": "3.1 Feature Description", "text": "In this subsection, we present in details the features used in order to build classifier C. For each tweet we combine its features in one feature vector. We apply the features proposed in [6] with some necessary modifications to avoid the production of an exceeding amount of calculations, thus boosting the running performance of our algorithm.\n3.1.1 Word and N-Gram Features We treat each word in a tweet as a binary feature. Respectively, we consider 2-5 consecutive words in a sentence as a binary n-gram feature. If f is a word or n-gram feature, then wf = Nf\ncount(f) is the weight of f in the feature\nvector, Nf is the number of times f appear in the tweet and count(f) declares the count of f in the Twitter corpus. Consequently, rare words and n-grams have a higher weight than common words and have a greater effect on the classification task. Moreover, we consider sequences of two or more punctuation symbols as word features. Unlike what authors propose in [6], we do not include the substituted meta-words for URLs, references and hashtags (URL, REF and TAG respectively) as word features (see and Section 4). Also, the common word RT, which means \u201dretweet\u201d, does not constitute a feature. The reason for omission of these words from the feature list lies in the fact that they appear in the majority of tweets inside the dataset. So, their contribution as features is negligible, whilst they lead to a great computation burden during the classification task.\n3.1.2 Pattern Features This is the main feature type and we apply the pattern definitions given in [5] for automated pattern extractions. We classify words into three categories: high-frequency words (HFWs), content words (CWs) and regular words (RWs). A word whose corpus frequency is more (less) than FH (FC) is considered to be a HFW (CW). The rest of the words are characterized as RWs. The word frequency is estimated from the training set rather than from an external corpus. In addition, we treat as HFWs all consecutive sequences of punctuation characters as well as URL, REF, TAG and RT meta-words for pattern extraction. We define a pattern as an ordered sequence of HFWs and slots for content words. The upper bound for FC is set to 1000 words per million and the lower bound for FH is set to 100 words per million.\nObserve that the FH and FC bounds allow overlap between some HFWs and CWs. To address this issue, we follow a simple strategy as described next. Assume fr is the frequency of a word in the corpus; if fr \u2208 ( FH , FH+FC\n2\n)\nthe\nword is classified as HFW, else if fr \u2208 [\nFH+FC 2\n, FC ) the\nword is classified as CW.\nWe seek for patterns containing 2-6 HFWs and 1-5 slots for CWs. Moreover, we require patterns to start and to end with a HFW, thus a minimal pattern is of the form [HFW][CW slot][HFW]. Additionally, we allow approximate pattern matching in order to enhance the classification performance. Approximate pattern matching is the same as exact matching, with the difference that an arbitrary number of RWs can be inserted between the pattern components. Since the patterns can be quite long and diverse, exact matches are not expected in a regular base. So, we permit approximate matching in order to avoid large sparse feature vectors. The weight wp of a pattern feature p is defined as wp = Np\ncount(p) in case of exact pattern matching and\nas wp = \u03b1\u00b7Np\ncount(p) in case of approximate pattern matching,\nwhere \u03b1 = 0.1 in all experiments.\n3.1.3 Punctuation Features The last feature type is divided into five generic features as follows: 1) tweet length in words, 2) number of exclamation mark characters in the tweet, 3) number of question mark characters in the tweet, 4) number of quotes in the tweet and 5) number of capital/capitalized words in the tweet. The weight wp of a punctuation feature p is defined as wp = Np\nMp\u00b7(Mw+Mng+Mpa)/3 , where Np is the number of\ntimes feature p appears in the tweet, Mp is the maximal observed value of p in the twitter corpus and Mw ,Mng ,Mpa declare the maximal values for word, n-gram and pattern feature groups, respectively. So, wp is normalized by averaging the maximal weights of the other feature types."}, {"heading": "3.2 Bloom Filter Integration", "text": "Bloom filters are data structures proposed by Bloom [3] for checking element membership in any given set. A Bloom filter is a bit vector of length z, where initially all the bits are set to 0. We can map an element into the domain between 0 and z \u2212 1 of the Bloom filter, using q independent hash functions hf1, hf2, ..., hfq . In order to store each element e into the Bloom filter, e is encoded using the q hash functions and all bits having index positions hfj(e) for 1 \u2264 j \u2264 q are set to 1.\nBloom filters are quite useful and they compress the storage space needed for the elements, as we can insert multiple objects inside a single Bloom filter. In the context of this work, we employ Bloom filters to transform our features to numbers, thus reducing the space needed to store our feature vectors. More precisely, instead of storing a feature we store the index positions in the Bloom filter that are set to 1. Nevertheless, it is obvious that the usage of Bloom filters may impose errors when checking for element membership, since two different elements may end up having exactly the same bits set to 1. The error probability is decreased as the\nnumber of bits and hash functions used grows. As shown in the experimental evaluation, the side effects of Bloom filters are negligible and boost the performance of our algorithm."}, {"heading": "3.3 kNN Classification Algorithm", "text": "In order to assign a sentiment label for each tweet in TT , we apply a kNN strategy. Initially, we build the feature vectors for all tweets inside the training and test datasets (FT and FTT respectively). Then, for each feature vector u in FTT we find all the feature vectors in V \u2286 FT that share at least one word/n-gram/pattern feature with u (matching vectors). After that, we calculate the Euclidean distance d(u, v),\u2200v \u2208 V and keep the k lowest values, thus forming Vk \u2286 V and each vi \u2208 Vk has an assigned sentiment label Li, 1 \u2264 i \u2264 k. Finally, we assign u the label of the majority of vectors in Vk. If no matching vectors exist for u, we assign a \u201dneutral\u201d label. We build C by adjusting an already implemented AkNN classifier in MapReduce to meet the needs of opinion mining problem."}, {"heading": "3.4 Algorithmic Description", "text": "In this subsection, we describe in detail the sentiment classification process as implemented in the Hadoop framework. We adjust an already implemented MapReduce AkNN classifier to meet the needs of opinion mining problem. Our approach consists of a series of four MapReduce jobs, with each job providing input to the next one in the chain. These MapReduce jobs can be summarized as follows: 1) Feature Extraction: Extract the features from all tweets in T and TT , 2) Feature Vector Construction: Build the feature vectors FT and FTT respectively, 3) Distance Computation: For each vector u \u2208 FTT find the matching vectors (if any exist) in FT , calculate the Euclidean distance d(u, v),\u2200v \u2208 V and form Vk \u2286 V , 4) Sentiment Classification: Assign a sentiment label \u2200tt \u2208 TT .\nThe records provided as input to our algorithm have the format <tweet id, class, text >, where class refers either to a sentiment label for tweets in T either to a no-sentiment flag for tweets in TT . In the following subsections, we describe each MapReduce job separately and analyze the Map and Reduce functions that take place in each one of them.\n3.4.1 Feature Extraction In this MapReduce job, we extract the features, as described in Subsection 3.1, of tweets in T and TT and calculate their weights. The output of the job is an inverted index, where the key is the feature itself and the value is a list of tweets that contain it. In the MapReduce Job 1 pseudo-code, we sum up the Map and Reduce functions of this process.\nThe Map function takes as input the records from T and TT , extracts the features of tweets. Afterwards, for each feature it outputs a key-value record, where the feature itself is the key and the value consists of the id of the tweet, the class of the tweet and the number of times the feature appears inside the sentence. The Reduce function receives the keyvalue pairs from the Map function and calculates the weight of a feature in each sentence. Then, it forms a list l with the format < t1, w1, c1 : ... : tx, wx, cx >, where ti is the id of the i-th tweet, wi is the weight of the feature for this tweet and"}, {"heading": "MapReduce Job 1", "text": "1: function Map(k1, v1) 2: t id = getId(v1); class = getClass(v1); 3: features = getFeatures(v1); 4: for all f \u2208 features do // BF is BloomFilter 5: output(BF(f.text),< t id, f.count, class >); 6: end for 7: end function\n8: function Reduce(k2, v2) 9: feature freq = 0; 10: for all v \u2208 v2 do 11: feature freq = feature freq + v.count; 12: end for 13: l = List{}; 14: for all v \u2208 v2 do 15: weight = v.count/feature freq; 16: l.add(newRecord(v.t id, weight, v.class)); 17: end for 18: output(k2, l); 19: end function\nci is its class. For each key-value pair, the Reduce function outputs a record where the feature is the key and the value is list l.\n3.4.2 Feature Vector Construction In this step, we build the feature vectors FT and FTT needed for the subsequent distance computation process. To achieve this, we combine all features of a tweet into one single vector. Moreover, \u2200tt \u2208 TT we generate a list (training) of tweets in T that share at least one word/n-gram/pattern feature. The Map and Reduce functions are outlined in the following MapReduce Job 2 pseudo-code.\nInitially, the Map function separates \u2200f \u2208 F the tweets that contain f into two lists, training and test respectively. Also, \u2200f \u2208 F it outputs a key-value record, where the key is the tweet id that contains f and the value consists of f and weight of f . Next, \u2200v \u2208 test it generates a record where the key is the id of v and the value is the training list. The Reduce function gathers key-value pairs with the same key and build FT and FTT . For each tweet t \u2208 T (tt \u2208 TT ) it outputs a record where key is the id of t (tt) and the value is its feature vector (feature vector together with the training list).\n3.4.3 Distance Computation In MapReduce Job 3, we create pairs of matching vectors between FT and FTT and compute their Euclidean distance. The Map and Reduce functions are depicted in the pseudocode that follows.\nFor each feature vector u \u2208 FTT , the Map function outputs all pairs of vectors v in training list of u. The output keyvalue record has as key the id of v and the value consists of the class of v, the id of u and the u itself. Moreover, the Map function outputs all feature vectors in FT . The Reduce function concentrates \u2200v \u2208 FT all matching vectors in FTT and computes the Euclidean distances between pairs of vectors. The Reduce function produces key-value pairs"}, {"heading": "MapReduce Job 2", "text": "1: function Map(k1, v1) 2: f = getFeature(v1); t list = getTweetList(v1); 3: test = training = List{}; 4: for all t \u2208 t list do 5: output(t.t id,< f, t.weight >); 6: if t.class 6= NULL then 7: training.add(newRecord(t.t id, t.class)); 8: else 9: test.add(newRecord(t.t id, t.class)); 10: end if 11: end for 12: for all t \u2208 test do 13: output(t.t id, training); 14: end for 15: end function\n16: function Reduce(k2, v2) 17: features = training = List{}; 18: for all v \u2208 v2 do 19: if v instanceOf List then 20: training.addAll(v); 21: else 22: features.add(v); 23: end if 24: end for 25: if training.size() > 0 then 26: output(k2, < training, features >); 27: else 28: output(k2, features); 29: end if 30: end function\nwhere the key is the id of u and the value comprises of the id of v, its class and the Euclidean distance d(u, v) between the vectors.\n3.4.4 Sentiment Classification This is the final step of our proposed approach. In this job, we aggregate for all feature vectors u in the test set, the k vectors with the lowest Euclidean distance to u, thus forming Vk. Then, we assign to u the label (class) l \u2208 L of the majority of Vk, or the neu label if Vk = \u2205. The MapReduce Job 4 pseudo-code is given below.\nThe Map function is very simple and it just dispatches the key-values pairs it receives to the Reduce function. For each feature vector u in the test set, the Reduce function keeps the k feature vectors with the lowest distance to v and then estimates the prevailing sentiment label l (if exists) among these vectors. Finally, it assigns to u the label l."}, {"heading": "4. EXPERIMENTAL EVALUATION", "text": "In this section, we conduct a series of experiments to evaluate the performance of our method under many different perspectives. More precisely, we take into consideration the effect of k and Bloom filters, the space compaction ratio and the size of the dataset in the performance of our solution.\nOur cluster includes 4 computing nodes (VMs), each one of"}, {"heading": "MapReduce Job 3", "text": "1: function Map(k1, v1) 2: t ids = getTrainingIds(v1); v = getVector(v1); 3: t id = getId(v1); 4: if t ids.size() > 0 then 5: for all u \u2208 t ids do 6: output(u.t id,< u.class, t id, v >); 7: end for 8: else 9: output(t id, v); 10: end if 11: end function\n12: function Reduce(k2, v2) 13: ttv = List{}; tv = NULL 14: for all v \u2208 v2 do 15: if v.class 6= NULL then 16: ttv.add(v); 17: else 18: tv = v; 19: end if 20: end for 21: for all tt \u2208 ttv do 22: ouput(tt.t id,< tv.t id, tv.class, d(tt, tv) >); 23: end for 24: end function\nwhich has four 2.4GHz CPU processors, 11.5GB of memory, 45GB hard disk and the nodes are connected by 1 gigabit Ethernet. On each node, we install Ubuntu 14.04 operating system, Java 1.7.0 51 with a 64-bit Server VM, and Hadoop 1.2.1. Moreover, we apply the following changes to the default Hadoop configurations: the replication factor is set to 1; the maximum number of Map and Reduce tasks in each node is set to 3 (consequently we set the number of Reduce tasks to 12), the DFS chunk size is 64MB and the size of virtual memory for each Map and Reduce task is set to 512MB.\nWe evaluate our method using two Twitter datasets (one for hashtags and one for emoticons) we have collected through the Twitter Search API6 between November 2014 to August 2015. We have used two human judges to create a list of hashtags and a list emoticons that express strong sentiment (e.g #bored and :)). We performed some experimentation to exclude from the lists the hashtags and emoticons that either were abused by twitter users or returned a very small number of tweets. We ended up with a list of 13 hashtags and a list of 4 emoticons. We preprocessed the datasets we collected and kept only the English tweets which contained 5 or more proper English words7 and do not contain two or more hashtags or emoticons from the aforementioned lists. Moreover, during preprocessing we have replaced URL links, hashtags and references by URL/REF/TAG meta-words as stated in [6]. The final hashtags dataset contains 942188 tweets (72476 tweets for each class) and the final emoticons dataset contains 1337508 tweets (334377 tweets for each\n6https://dev.twitter.com/rest/public/search 7To identify the proper English word we used an available WN-based English dictionary"}, {"heading": "MapReduce Job 4", "text": "1: function Map(k1, v1) 2: t id = getTweetId(v1); val = getValue(v1); 3: output(t id, val); 4: end function\n5: function Reduce(k2, v2) 6: l k = getKNN(v2); 7: H = HashMap < Class,Occurences > {}; 8: H = findClassOccur(l k); 9: max = 0;maxClass = null; 10: for all entry \u2208 H do 11: if entry.occur > max then 12: max = entry.occur; 13: maxClass = entry.class; 14: end if 15: end for 16: output(k2,maxClass); 17: end function\nclass). In both datasets, hashtags and emoticons are used as sentiment labels and for each sentiment label there is an equal amount of tweets. Finally, we produced two nosentiment datasets by randomly sampling 72476 and 334377 tweets with no hashtags/emoticons from the dataset used in [4] and is publicly available8. We assume that such random samples are unlikely to contain a significant amount of sentiment sentences. These datasets are used for the binary classification experiments (see Section 4.1).\nWe assess the classification performance of our algorithm using the 10-fold cross validation method and measuring the harmonic f-score. For the Bloom filter construction we use 999 bits and 3 hash functions. In order to avoid a significant amount of computations that greatly affect the running performance of the algorithm, we define a weight threshold w = 0.005 for feature inclusion in the feature vectors. In essence, we eliminate the most frequent words that have no substantial contribution to the final outcome."}, {"heading": "4.1 Classification Performance", "text": "In this subsection we measure the classification performance of our solution using the harmonic f-score. We use two experimental settings, the multi-class classification and the binary classification settings. Under multi-class classification we attempt to assign a single label to each of vectors in the test set. In the binary classification experiments, we classified a sentence as either appropriate for a particular label or as not bearing any sentiment. As stated and in [6], the binary classification is a useful application and can be used as a filter that extracts sentiment sentences from a corpus for further processing. We also test how the performance is affected with and without using Bloom filters. The value k for the kNN classifier is equal to 50. The results of the experiments are displayed in Table 1. In case of binary classification, the results depict the average score for all classes.\nFor multi-class classification the results are not very good but still they are way above the random baseline. We also\n8https://archive.org/details/twitter cikm 2010\nobserve that the results with and without the Bloom filters are almost the same. Thus, we deduce that for multi-class classification the Bloom filters marginally affect the classification performance. Furthermore, the outcome for emoticons is significantly better than hashtags which is expected due to the lower number of sentiment types. This behavior can also be explained by the ambiguity of hashtags and some overlap of sentiments. In case of binary classification there is a notable difference between the results with and without Bloom filters. These results may be somewhat unexpected but can be explicated when we take a look in Table 2. Table 2 presents the fraction of test set tweets that are classified as neutral because of the Bloom filters and/or the weight threshold w (no matching vectors are found). Notice that the integration of Bloom filters, leads to a bigger number of tweets with no matching vectors. Obviously, the excluded tweets have an immediate effect to the performance of the kNN classifier in case of binary classification. This happens since the number of tweets in the cross fold validation process is noticeably smaller compared to the multi-class classification. Overall, the results for binary classification with Bloom filters confirm the usefulness of our approach."}, {"heading": "4.2 Effect of k", "text": "In this subsection, we attempt to alleviate the problem of our approach\u2019s low performance for binary classification without Bloom filters. To achieve this we measure the effect of k in the classification performance of the algorithm. We test four different configurations where k \u2208 {50, 100, 150, 200}. The outcome of this experimental evaluation is demonstrated in Table 3. For both binary and multi-class classification, increasing k affects slightly (or not at all) the harmonic f-score when we embody Bloom filters. The same thing does not apply when we do not use Bloom filters. More specifically, there is a great enhancement in the binary classification performance for hashtags and emoticons and a smaller improvement in case of multi-class classification. The inference of this experiment, is that larger values of k can provide a great impulse in the performance of the algorithm when not using Bloom filters.\n4.3 Space Compression\nAs stated and above, the Bloom filters can compact the space needed to store a set of elements, since more than one object can be stored to the bit vector. In this subsection, we elaborate on this aspect and present the compression ratio in the feature vectors when exploiting Bloom filters (in the way presented in Section 3.2) in our framework. The outcome of this measurement is depicted in Fig. 1. In all cases, the Bloom filters manage to diminish the storage space required for the feature vectors by a fraction between 15- 20%. According to the analysis made so far, the importance of Bloom filters in our solution is twofold. They manage to both preserve a good classification performance, despite any errors they impose, and compact the storage space of the feature vectors. Consequently, we deduce that Bloom filters are very beneficial when dealing with large scale sentiment analysis data, that generate an exceeding amount of features during the feature vector construction step."}, {"heading": "4.4 Running Time and Scalability", "text": "In this final experiment, we compare the running time for multi-class and binary classification and measure the scalability of our approach. Initially, we calculate the execution time in all cases in order to detect if the Bloom filters speedup or slow down the running performance of our algorithm. The results when k = 50 are presented in Fig. 2. It is worth noted that in the majority of cases, Bloom filters slightly boost the execution time performance. Despite needing more preprocessing time to produce the features with Bloom filters, in the end they pay off since the feature vector is smaller in size. This leads to lower I/O cost between the Map and Reduce tasks and consequently to less processing time. Multi-class classification for emoticons constitutes the only exception in our example.\nFinally, we investigate the scalability of our approach. We test the scalability only for the multi-class classification case since the produced feature vector in much bigger compared to the binary classification case. We create new chunks smaller in size that are a fraction F of the original datasets, where F \u2208 {0.2, 0.4, 0.6, 0.8}. Moreover, we set the value of k to 50. Figure 3 presents the scalability results of our approach. From the outcome, we deduce that our algorithm scales almost linear as the data size increases in all cases. This proves that our solution is efficient, robust, scalable and therefore appropriate for big data sentiment analysis."}, {"heading": "5. CONCLUSIONS AND FUTURE WORK", "text": "In the context of this work, we presented a novel method for sentiment learning in the MapReduce framework. Our algorithm exploits the hashtags and emoticons inside a tweet, as sentiment labels, and proceeds to a classification procedure of diverse sentiment types in a parallel and distributed manner. Moreover, we utilize Bloom filters to compact the storage size of intermediate data and boost the performance of our algorithm. We conduct a variety of experiments to test the efficiency of our method. Through this extensive experimental evaluation we prove that our system is efficient, robust and scalable.\nIn the near future, we plan to extend and improve our framework by exploring more features that may be added in the feature vector and will increase the classification performance. Furthermore, we wish to explore more strategies for FH and FC bounds in order to achieve better separation between the HFWs and CWs. Finally, we plan to implement our\nsolution in other platforms (e.g. Spark) and compare the performance with the current implementation."}, {"heading": "6. REFERENCES", "text": "[1] A. Agarwal, B. Xie, I. Vovsha, O. Rambow, and\nR. Passonneau. Sentiment analysis of twitter data. In Proceedings of the Workshop on Languages in Social Media, pages 30\u201338, 2011.\n[2] L. Barbosa and J. Feng. Robust sentiment detection on twitter from biased and noisy data. In Proceedings of the 23rd International Conference on\nComputational Linguistics: Posters, pages 36\u201344, 2010.\n[3] B. H. Bloom. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7):422\u2013426, 1970.\n[4] Z. Cheng, J. Caverlee, and K. Lee. You are where you tweet: A content-based approach to geo-locating twitter users. In Proceedings of the 19th ACM International Conference on Information and\nKnowledge Management, pages 759\u2013768, 2010.\n[5] D. Davidov and A. Rappoport. Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words. In Proceedings of the 21st International Conference on Computational\nLinguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 297\u2013304, 2006.\n[6] D. Davidov, O. Tsur, and A. Rappoport. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 241\u2013249, 2010.\n[7] J. Dean and S. Ghemawat. Mapreduce: Simplified data processing on large clusters. In Proceedings of the 6th Symposium on Operating Systems Design and\nImplementation, pages 137\u2013150, 2004.\n[8] X. Ding and B. Liu. The utility of linguistic rules in opinion mining. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research\nand Development in Information Retrieval, pages 811\u2013812, 2007.\n[9] M. Hu and B. Liu. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and\nData Mining, pages 168\u2013177, 2004.\n[10] L. Jiang, M. Yu, M. Zhou, X. Liu, and T. Zhao. Target-dependent twitter sentiment classification. In Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human Language Technologies - Volume 1, pages 151\u2013160, 2011.\n[11] V. N. Khuc, C. Shivade, R. Ramnath, and J. Ramanathan. Towards building large-scale distributed systems for twitter sentiment analysis. In Proceedings of the 27th Annual ACM Symposium on\nApplied Computing, pages 459\u2013464, 2012.\n[12] C. Lin and Y. He. Joint sentiment/topic model for sentiment analysis. In Proceedings of the 18th ACM Conference on Information and Knowledge Management, pages 375\u2013384, 2009.\n[13] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. Topic sentiment mixture: Modeling facets and opinions in weblogs. In Proceedings of the 16th International Conference on World Wide Web, pages 171\u2013180, 2007.\n[14] T. Nasukawa and J. Yi. Sentiment analysis: Capturing favorability using natural language processing. In Proceedings of the 2Nd International Conference on\nKnowledge Capture, pages 70\u201377, 2003.\n[15] N. Nodarakis, E. Pitoura, S. Sioutas, A. K. Tsakalidis, D. Tsoumakos, and G. Tzimas. Efficient multidimensional aknn query processing in the cloud. In Database and Expert Systems Applications - 25th International Conference, DEXA 2014, Munich,\nGermany, September 1-4, 2014. Proceedings, Part I, pages 477\u2013491, 2014.\n[16] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?: Sentiment classification using machine learning techniques. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing\n- Volume 10, pages 79\u201386, 2002.\n[17] The apache software foundation: Hadoop homepage. http://hadoop.apache.org/, 2013. [Online; accessed 20-September-2015].\n[18] X. Wang, F. Wei, X. Liu, M. Zhou, and M. Zhang. Topic sentiment analysis in twitter: A graph-based hashtag sentiment classification approach. In Proceedings of the 20th ACM International Conference\non Information and Knowledge Management, pages 1031\u20131040, 2011.\n[19] T. White. Hadoop: The Definitive Guide, 3rd Edition. O\u2019Reilly Media / Yahoo Press, 2012.\n[20] T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural\nLanguage Processing, pages 347\u2013354, 2005.\n[21] T. Wilson, J. Wiebe, and P. Hoffmann. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Comput. Linguist., 35(3):399\u2013433, Sept. 2009.\n[22] Y. Yamamoto, T. Kumamoto, and A. Nadamoto. Role of emoticons for multidimensional sentiment analysis of twitter. In Proceedings of the 16th International Conference on Information Integration and Web-based\nApplications &#38; Services, pages 107\u2013115, 2014.\n[23] H. Yu and V. Hatzivassiloglou. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of the 2003 Conference on Empirical\nMethods in Natural Language Processing, pages 129\u2013136, 2003.\n[24] W. Zhang, C. Yu, and W. Meng. Opinion retrieval from blogs. In Proceedings of the Sixteenth ACM Conference on Conference on Information and\nKnowledge Management, pages 831\u2013840, 2007.\n[25] L. Zhuang, F. Jing, and X.-Y. Zhu. Movie review mining and summarization. In Proceedings of the 15th ACM International Conference on Information and\nKnowledge Management, pages 43\u201350, 2006."}], "references": [{"title": "Sentiment analysis of twitter data", "author": ["A. Agarwal", "B. Xie", "I. Vovsha", "O. Rambow", "R. Passonneau"], "venue": "In Proceedings of the Workshop on Languages in Social Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Robust sentiment detection on twitter from biased and noisy data", "author": ["L. Barbosa", "J. Feng"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Space/time trade-offs in hash coding with allowable errors", "author": ["B.H. Bloom"], "venue": "Commun. ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1970}, {"title": "You are where you tweet: A content-based approach to geo-locating twitter users", "author": ["Z. Cheng", "J. Caverlee", "K. Lee"], "venue": "In Proceedings of the 19th ACM International Conference on Information and Knowledge Management,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words", "author": ["D. Davidov", "A. Rappoport"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Enhanced sentiment learning using twitter hashtags and smileys", "author": ["D. Davidov", "O. Tsur", "A. Rappoport"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Mapreduce: Simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "In Proceedings of the 6th Symposium on Operating Systems Design and Implementation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "The utility of linguistic rules in opinion mining", "author": ["X. Ding", "B. Liu"], "venue": "In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Target-dependent twitter sentiment classification", "author": ["L. Jiang", "M. Yu", "M. Zhou", "X. Liu", "T. Zhao"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Towards building large-scale distributed systems for twitter sentiment analysis", "author": ["V.N. Khuc", "C. Shivade", "R. Ramnath", "J. Ramanathan"], "venue": "In Proceedings of the 27th Annual ACM Symposium on Applied Computing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Joint sentiment/topic model for sentiment analysis", "author": ["C. Lin", "Y. He"], "venue": "In Proceedings of the 18th ACM Conference on Information and Knowledge Management,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Topic sentiment mixture: Modeling facets and opinions in weblogs", "author": ["Q. Mei", "X. Ling", "M. Wondra", "H. Su", "C. Zhai"], "venue": "In Proceedings of the 16th International Conference on World Wide Web,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Sentiment analysis: Capturing favorability using natural language processing", "author": ["T. Nasukawa", "J. Yi"], "venue": "In Proceedings of the 2Nd International Conference on Knowledge Capture,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Efficient multidimensional aknn query processing in the cloud", "author": ["N. Nodarakis", "E. Pitoura", "S. Sioutas", "A.K. Tsakalidis", "D. Tsoumakos", "G. Tzimas"], "venue": "In Database and Expert Systems Applications - 25th International Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Thumbs up?: Sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Topic sentiment analysis in twitter: A graph-based hashtag sentiment classification approach", "author": ["X. Wang", "F. Wei", "X. Liu", "M. Zhou", "M. Zhang"], "venue": "In Proceedings of the 20th ACM International Conference on Information and Knowledge Management,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Hadoop: The Definitive Guide, 3rd Edition", "author": ["T. White"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "Comput. Linguist.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Role of emoticons for multidimensional sentiment analysis of twitter", "author": ["Y. Yamamoto", "T. Kumamoto", "A. Nadamoto"], "venue": "In Proceedings of the 16th International Conference on Information Integration and Web-based Applications ", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences", "author": ["H. Yu", "V. Hatzivassiloglou"], "venue": "In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Opinion retrieval from blogs", "author": ["W. Zhang", "C. Yu", "W. Meng"], "venue": "In Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Movie review mining and summarization", "author": ["L. Zhuang", "F. Jing", "X.-Y. Zhu"], "venue": "In Proceedings of the 15th ACM International Conference on Information and Knowledge Management,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}], "referenceMentions": [{"referenceID": 16, "context": "a topic and are extensively utilized in tweets [18].", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "The most popular and notably efficient tool is the MapReduce [7] programming model, developed by Google, for processing large-scale data.", "startOffset": 61, "endOffset": 64}, {"referenceID": 17, "context": "In this paper, we propose MR-SAT: a novel MapReduce Algorithm for Big Data Sentiment Analysis on Twitter implemented in Hadoop [17, 19], the open source MapReduce implementation.", "startOffset": 127, "endOffset": 135}, {"referenceID": 8, "context": "Early studies focus on document level sentiment analysis concerning movie or product reviews [9, 25] and posts published on webpages or blogs [24].", "startOffset": 93, "endOffset": 100}, {"referenceID": 23, "context": "Early studies focus on document level sentiment analysis concerning movie or product reviews [9, 25] and posts published on webpages or blogs [24].", "startOffset": 93, "endOffset": 100}, {"referenceID": 22, "context": "Early studies focus on document level sentiment analysis concerning movie or product reviews [9, 25] and posts published on webpages or blogs [24].", "startOffset": 142, "endOffset": 146}, {"referenceID": 18, "context": "Respectively, many efforts have been made towards the sentence level sentiment analysis [20, 21, 23] which examines phrases and assigns to each one of them a sentiment polarity (positive, negative, neutral).", "startOffset": 88, "endOffset": 100}, {"referenceID": 19, "context": "Respectively, many efforts have been made towards the sentence level sentiment analysis [20, 21, 23] which examines phrases and assigns to each one of them a sentiment polarity (positive, negative, neutral).", "startOffset": 88, "endOffset": 100}, {"referenceID": 21, "context": "Respectively, many efforts have been made towards the sentence level sentiment analysis [20, 21, 23] which examines phrases and assigns to each one of them a sentiment polarity (positive, negative, neutral).", "startOffset": 88, "endOffset": 100}, {"referenceID": 11, "context": "A less investigated area is the topic-based sentiment analysis [12, 13] due to the difficulty to provide an adequate definition of topic and how to incorporate the sentiment factor into the opinion mining task.", "startOffset": 63, "endOffset": 71}, {"referenceID": 12, "context": "A less investigated area is the topic-based sentiment analysis [12, 13] due to the difficulty to provide an adequate definition of topic and how to incorporate the sentiment factor into the opinion mining task.", "startOffset": 63, "endOffset": 71}, {"referenceID": 15, "context": "In [16], the authors employ three machine learning techniques to classify movie reviews as positive or negative.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "On the other hand, Nasukawa and Yi [14] investigate the proper identification of semantic relationships between the sentiment expressions and the subject, in order to enhance the accuracy of sentiment analysis within webpages and online articles.", "startOffset": 35, "endOffset": 39}, {"referenceID": 7, "context": "Moreover, Ding and Liu [8] propose a set of linguistic rules together with a new opinion aggregation function to detect sentiment orientations in online product reviews.", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "In [2], the authors propose a 2-step classifier that separates messages as subjective and objective, and further distinguishes the subjective tweets as positive or negative.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "[6] exploit the hashtags and smileys in tweets and evaluate the contribution of different features (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] explore the use of a tree kernel model for detecting sentiment orientation in tweets.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "A three-step classifier is proposed in [10] that follows a targetdependent sentiment classification strategy by incorporating target-dependent features and taking related tweets into consideration.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "Moreover, the authors in [18] perform a topic sentiment analysis in Twitter data through a graph-based model.", "startOffset": 25, "endOffset": 29}, {"referenceID": 20, "context": "A more recent approach [22], investigates the role of emoticons for multidimensional sentiment analysis of Twitter by constructing a sentiment and emoticon lexicon.", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "A large scale solution is presented in [11] where the authors build a sentiment lexicon and classify tweets using a MapReduce algorithm and a distributed database model.", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "We implement C by adapting an existing MapReduce classification algorithm based on AkNN queries [15], as described in Subsection 3.", "startOffset": 96, "endOffset": 100}, {"referenceID": 5, "context": "We apply the features proposed in [6] with some necessary modifications to avoid the production of an exceeding amount of calculations, thus boosting the running performance of our algorithm.", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "Unlike what authors propose in [6], we do not include the substituted meta-words for URLs, references and hashtags (URL, REF and TAG respectively) as word features (see and Section 4).", "startOffset": 31, "endOffset": 34}, {"referenceID": 4, "context": "This is the main feature type and we apply the pattern definitions given in [5] for automated pattern extractions.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "Bloom filters are data structures proposed by Bloom [3] for checking element membership in any given set.", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "Moreover, during preprocessing we have replaced URL links, hashtags and references by URL/REF/TAG meta-words as stated in [6].", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "Finally, we produced two nosentiment datasets by randomly sampling 72476 and 334377 tweets with no hashtags/emoticons from the dataset used in [4] and is publicly available.", "startOffset": 143, "endOffset": 146}, {"referenceID": 5, "context": "As stated and in [6], the binary classification is a useful application and can be used as a filter that extracts sentiment sentences from a corpus for further processing.", "startOffset": 17, "endOffset": 20}], "year": 2016, "abstractText": "Sentiment analysis (or opinion mining) on Twitter data has attracted much attention recently. One of the system\u2019s key features, is the immediacy in communication with other users in an easy, user-friendly and fast way. Consequently, people tend to express their feelings freely, which makes Twitter an ideal source for accumulating a vast amount of opinions towards a wide diversity of topics. This amount of information offers huge potential and can be harnessed to receive the sentiment tendency towards these topics. However, since none can invest an infinite amount of time to read through these tweets, an automated decision making approach is necessary. Nevertheless, most existing solutions are limited in centralized environments only. Thus, they can only process at most a few thousand tweets. Such a sample, is not representative to define the sentiment polarity towards a topic due to the massive number of tweets published daily. In this paper, we go one step further and develop a novel method for sentiment learning in theMapReduce framework. Our algorithm exploits the hashtags and emoticons inside a tweet, as sentiment labels, and proceeds to a classification procedure of diverse sentiment types in a parallel and distributed manner. Moreover, we utilize Bloom filters to compact the storage size of intermediate data and boost the performance of our algorithm. Through an extensive experimental evaluation, we prove that our solution is efficient, robust and scalable and confirm the quality of our sentiment identification.", "creator": "LaTeX with hyperref package"}}}