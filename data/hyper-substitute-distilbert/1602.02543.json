{"id": "1602.02543", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Homogeneity of Cluster Ensembles", "abstract": "the quality and the mean of tasks seemingly unlike independent cluster ensemble are particularly unique in general. however issue lays challenges in random inference and cluster stability. in this contribution, we state sufficient conditions for uniqueness between dominance and mean. all proposed studies show that consistent unique mean is neither exceptional nor generic. similarly cope out this paradox, we considers homogeneity as per measure of something likely is a unique mean, meaningful sample of partitions. we believe best expectation is compatible with cluster survival. this result points to that possible consistency between adaptive integration and diversity making robust clustering. to assess homogeneity / future practical setting, we introduce an efficient tool into update a perceived quality of homogeneity. indirect results using decision by - means algorithm suggest successful computation of least threshold element proves likewise exceptional for real - world resources. moreover, for domains of high homogeneity, uniqueness can partially violated by increasing the number of data trees encountered by promoting random partitions. in further different methodology, this problem let be placed requires a further step towards a future theory of partitions.", "histories": [["v1", "Mon, 8 Feb 2016 12:28:57 GMT  (1486kb,D)", "http://arxiv.org/abs/1602.02543v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["brijnesh j jain"], "accepted": false, "id": "1602.02543"}, "pdf": {"name": "1602.02543.pdf", "metadata": {"source": "CRF", "title": "Homogeneity of Cluster Ensembles", "authors": ["Brijnesh J. Jain"], "emails": ["brijnesh.jain@gmail.com"], "sections": [{"heading": null, "text": "Homogeneity of Cluster Ensembles\nBrijnesh J. Jain Technische Universita\u0308t Berlin, Germany\ne-mail: brijnesh.jain@gmail.com\nThe expectation and the mean of partitions generated by a cluster ensemble are not unique in general. This issue poses challenges in statistical inference and cluster stability. In this contribution, we state sufficient conditions for uniqueness of expectation and mean. The proposed conditions show that a unique mean is neither exceptional nor generic. To cope with this issue, we introduce homogeneity as a measure of how likely is a unique mean for a sample of partitions. We show that homogeneity is related to cluster stability. This result points to a possible conflict between cluster stability and diversity in consensus clustering. To assess homogeneity in a practical setting, we propose an efficient way to compute a lower bound of homogeneity. Empirical results using the k-means algorithm suggest that uniqueness of the mean partition is not exceptional for real-world data. Moreover, for samples of high homogeneity, uniqueness can be enforced by increasing the number of data points or by removing outlier partitions. In a broader context, this contribution can be placed as a further step towards a statistical theory of partitions.\nar X\niv :1\n60 2.\n02 54\n3v 1\n[ cs\n.L G\n] 8\nF eb\n2 01\n6\nContents"}, {"heading": "1. Introduction 3", "text": ""}, {"heading": "2. Partition Spaces 4", "text": "2.1. Partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2. Orbit Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3. Intrinsic Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"}, {"heading": "3. Homogeneity of a Sample 6", "text": "3.1. Fre\u0301chet Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.2. Conditions of Uniqueness . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.3. Asymmetric Partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.4. Homogeneity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.5. Clustering Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.5.1. Clustering Instability . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.5.2. Homogeneity vs. Stability . . . . . . . . . . . . . . . . . . . . . . 11 3.5.3. Stability vs. Diversity . . . . . . . . . . . . . . . . . . . . . . . . 12"}, {"heading": "4. Experiments 13", "text": "4.1. Experiments on Synthetic Data . . . . . . . . . . . . . . . . . . . . . . . 13\n4.1.1. Results on G4 Datasets . . . . . . . . . . . . . . . . . . . . . . . 14 4.1.2. Results on U-Shapes and Gaussians . . . . . . . . . . . . . . . . 18\n4.2. Experiments on UCI Datasets . . . . . . . . . . . . . . . . . . . . . . . . 20"}, {"heading": "5. Conclusion 21", "text": ""}, {"heading": "A. Preliminaries 22", "text": "A.1. Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 A.2. Dirichlet Fundamental Domains . . . . . . . . . . . . . . . . . . . . . . . 23 A.3. Cross Sections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24"}, {"heading": "B. Proofs 24", "text": "B.1. Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 B.2. Proof of Prop. 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 B.3. Proof of Prop. 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 B.4. Proof of Prop. 3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 B.5. Proof of Equation (1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28"}, {"heading": "1. Introduction", "text": "Clustering is a standard technique for exploratory data analysis that finds applications across different disciplines such as computer science, biology, marketing, and social science. The goal of clustering is to group a set of unlabeled data points into several clusters based on some notion of dissimilarity. Inspired by the success of classifier ensembles, consensus clustering has emerged as a research topic [9, 22]. Consensus clustering first generates several partitions of the same dataset. Then it combines the sample partitions to a single consensus partition. The assumption is that a consensus partition better fits to the hidden structure in the data than individual partitions.\nOne standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21]. A mean partition best summarizes the sample partitions with respect to some (dis)similarity function. In general, a mean partition is not unique. Non-uniqueness of a mean partition poses a number of challenges, including (i) comparability, (ii) consistency, (iii) asymptotic behavior, and (iv) clustering stability.\nUniqueness allows us to directly compare the performance of two clustering ensembles via their respective mean partitions. Without uniqueness, comparing two distributions of partitions based on randomly generated samples can be elusive. Moreover, under reasonable conditions, uniqueness implies strong consistency and gives rise to different versions of the law of large numbers [13, 19]. These findings indicate that without uniqueness, statistical inference based on mean partitions can hardly proceed. With regard to clustering stability, non-uniqueness of the mean partition could potentially entail instability of the clustering.\nDespite being a desirable property coming along with several benefits, not much research has been devoted to uniqueness of the mean partition in consensus clustering. In particular, it is unclear under which conditions a sample has a unique mean partition. In addition, it is also unclear whether uniqueness only occurs in trivial and exceptional cases or is a feasible property of practical relevance. To approach these issues, we assume that the set of (hard and soft) partitions is endowed with an intrinsic metric induced by the Euclidean distance.\nThe contributions of this paper are as follows:\n1. Conditions of uniqueness. We establish conditions of uniqueness showing that the mean partition is unique if the cluster ensemble generates sample partitions within a sufficiently small ball.\n2. Homogeneity. We propose homogeneity as a measure of how close a sample is to having a unique mean partition. We present a lower-bound of homogeneity that can easily be computed and at the same time identifies outlier-partitions that need to be removed in order to guarantee a subsample with unique mean partition.\n3. Relationship to cluster stability. We show that homogeneity of a sample is related to cluster stability. This result points to potentially colliding approaches in clustering: standard clustering advocates stability [11, 16] and consensus clustering\nadvocates diversity [5, 22, 23], which will be briefly discussed.\n4. Empirical evidence. In experiments we assessed the homogeneity of samples obtained by the k-means algorithm applied to synthetic and real-world data. The results suggest that uniqueness of the mean partition is not exceptional and can be enforced by a larger dataset or by removing outlier-partitions if homogeneity is high.\nThough uniqueness of the mean partition is not a side issue, it is still a strict property not valid for many samples. Homogeneity relaxes this strict property and gives us an alternative way to assess the performance of clusterings and cluster ensembles that goes beyond uniqueness of the mean partition. In a wider context, the results presented in this paper contribute towards a statistical theory of partitions [13].\nThe rest of this paper is structured as follows: Section 2 represents partitions as points of an orbit space. In Section 3, we present the theoretical contributions. Section 4 discusses experimental results. Finally, Section 5 concludes with a summary of the main results and with an outlook to further research. Proofs are delegated to the appendix."}, {"heading": "2. Partition Spaces", "text": "To analyze partitions, we suggest a geometric representation proposed in [13]. We first show that a partition can be regarded as a point in some geometric space, called orbit space. Orbit spaces are well explored, possess a rich geometrical structure and have a natural connection to Euclidean spaces [2, 12, 17]. Then we endow orbit spaces P of partitions with a distance function \u03b4 related to the Euclidean metric such that (P, \u03b4) becomes a geodesic metric space."}, {"heading": "2.1. Partitions", "text": "Let Z = {z1, . . . , zm} be a set of m data points. A partition X of Z with ` clusters C1, . . . , C` is specified by a matrix X \u2208 [0, 1]`\u00d7m such that XT1` = 1m, where 1` \u2208 R` and 1m \u2208 Rm are vectors of all ones.\nThe rows xk: of matrix X refer to the clusters Ck of partition X. The columns x:j of X refer to the data points zj \u2208 Z. The elements xkj of matrix X = (xkj) represent the degree of membership of data point zj to cluster Ck. The constraint XT1` = 1m demands that the membership values x:j of data point zj across all clusters must sum to one.\nBy P`,m we denote the set of all partitions with ` clusters over m data points. Since some clusters may be empty, the set P`,m also contains partitions with less than ` clusters. Thus, we consider ` \u2264 m as the maximum number of clusters we encounter. If the exact numbers ` and m do not matter or are clear from the context, we also write P for P`,m. A hard partition X is a partition with matrix representation X \u2208 {0, 1}`\u00d7m. The set P+ \u2282 P denotes the subset of all hard partitions."}, {"heading": "2.2. Orbit Spaces", "text": "We define the representation space X of the set P = P`,m of partitions by\nX = { X \u2208 [0, 1]`\u00d7m : XT1` = 1` } .\nThen we have a natural projection\n\u03c0 : X \u2192 P, X 7\u2192 X = \u03c0(X)\nthat sends matrices X to partitions X they represent. The map \u03c0 conveys two properties: (1) each partition can be represented by at least one matrix, and (2) a partition may have several matrix representations.\nSuppose that matrix X \u2208 X represents a partition X \u2208 P. The subset of all matrices representing X forms an equivalence class [X] that can be obtained by permuting the rows of matrix X in all possible ways. The equivalence class of X is of the form\n[X] = {PX : P \u2208 \u03a0},\nwhere \u03a0 is the group of all (`\u00d7 `)-permutation matrices. The orbit space of partitions is the set\nX/\u03a0 = {[X] : X \u2208 X} .\nInformally, the orbit space consists of all equivalence classes [X], we can construct as described above. Mathematically, the orbit space X/\u03a0 is the quotient space obtained by the action of the permutation group \u03a0 on the set X . The equivalence classes [X] are the orbits of X. The orbits [X] are in 1-1-correspondence with the partitions X = \u03c0(X). Therefore, we can identify partitions with orbits and P with X/\u03a0. Consequently, we occasionally write X \u2208 X if X = \u03c0(X)."}, {"heading": "2.3. Intrinsic Metric", "text": "Next, we endow the partition space P with an intrinsic metric \u03b4 related to the Euclidean distance such that (P, \u03b4) becomes a geodesic space. The Euclidean norm for matrices X \u2208 X is defined by\n\u2016X\u2016 = \u2211\u0300 k=1 m\u2211 j=1 |xkj |2 1/2 .\nThe Euclidean norm induces a distance on P of the form\n\u03b4 : P \u00d7 P \u2192 R, (X,Y ) 7\u2192 min {\u2016X \u2212 Y \u2016 : X \u2208 X,Y \u2208 Y } .\nThen the pair (P, \u03b4) is a geodesic metric space [13], Theorem 2.1."}, {"heading": "3. Homogeneity of a Sample", "text": "This section first links consensus clustering to the field of Fre\u0301chet functions from Mathematical Statistics. Then we present conditions of uniqueness. Based on these conditions, we study how likely is a unique mean partition. For this, we propose homogeneity of a sample as a measure of how close a sample is to having a unique mean. We present an efficient way to compute a lower bound of homogeneity. Finally, this section relates homogeneity to cluster stability and points to potentially conflicting approaches in clustering: stability and diversity in consensus clustering."}, {"heading": "3.1. Fre\u0301chet Functions", "text": "In this section, we link the consensus function of the mean partition approach to Fre\u0301chet functions [8]. This link provides access to many results from Statistics in Non-Euclidean spaces [1].\nLet (P, \u03b4) be a partition space endowed with the metric \u03b4 induced by the Euclidean norm. We assume that Q is a probability measure on P with support SQ.1 The function\nFQ : P \u2192 R, Z 7\u2192 \u222b P \u03b4(X,Z)2 dQ(X)\nis the expected Fre\u0301chet function of Q. The minimum of FQ exists but but is not unique, in general [13]. Any partition M \u2208 P that minimizes FQ is an expected partition. We say Q is homogeneous, if the expected partition of Q is unique. Otherwise, Q is said to be heterogeneous. The minimum VQ = FQ(M) is called the variation of Q.\nLet SnQ = SQ \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 SQ denote the n-fold cartesian product of support SQ. If Sn = (X1, X2, . . . , Xn) \u2208 SnQ is a sample of n partitions, then the (empirical) Fre\u0301chet function of Sn is of the form\nFn : P \u2192 R, Z 7\u2192 1\nn n\u2211 i=1 \u03b4(Xi, Z) 2 .\nAs for expected Fre\u0301chet functions FQ, the minimum of Fn exists but is not unique, in general [13]. Any partition M \u2208 P that minimizes Fn is a mean partition. The minimum Vn = Fn(M) is the variation of Sn. We say the sample Sn is homogeneous, if the mean partition of Sn is unique. Otherwise, Sn is said to be heterogeneous."}, {"heading": "3.2. Conditions of Uniqueness", "text": "In this section, we show that the expected and mean partition are unique if the partitions to be summarized are contained in a sufficiently small ball.\n1The support of Q is the smallest closed subset SQ \u2286 P such that Q(SQ) = 1.\nThe ball B(Z, r) with center Z \u2208 P and radius r is a set of the form\nB(Z, r) = {X \u2208 P : \u03b4(X,Z) \u2264 r} .\nWe call the ball B(Z, r) homogeneous if there is a bijective isometry\n\u03c8 : B(Z, r) \u2212\u2192 B(Z, r),\nwhere B(Z, r) is the ball in the Euclidean space X centered at representation Z \u2208 Z. The definition of homogeneous ball is independent of the choice of representation, because two balls in X at different centers but identical radius r are isometric.\nThe maximum homogeneity (max-hom) radius \u03c1Z at Z is the largest radius for which BZ = B(Z, \u03c1Z) is a homogeneous ball. We call BZ the max-hom ball centered at Z. The next result guarantees uniqueness of the expectation and mean if the partitions to be summarized are contained in an open subset of some max-hom ball.\nTheorem 3.1. Let Q be a probability measure on P with support SQ. Suppose that there is a partition Z \u2208 P and an open subset U \u2282 BZ such that SQ \u2286 U . Then Q and any sample Sn \u2208 SnQ are homogeneous.\nNote that Theorem 3.1 makes no statement about the existence and size of max-hom balls. Therefore, it is unclear whether the uniqueness conditions are satisfied only in exceptional cases or are of practical relevance. The following treatment is devoted to this issue."}, {"heading": "3.3. Asymmetric Partitions", "text": "This section sets the stage for understanding how likely and how feasible are unique expectations and mean partitions. To this end, we introduce the notion of asymmetric partition. Based on the notion of asymmetry, we characterize partitions with positive max-hom radius.\nLet \u03a0\u2217 = \u03a0\\{I} denote the subset of (`\u00d7 `)-permutation matrices without identity matrix I. The degree of asymmetry of a partition Z \u2208 P is defined by\n\u03b1Z = min {\u2016Z \u2212 PZ\u2016 : Z \u2208 Z and P \u2208 \u03a0\u2217} .\nA partition Z is asymmetric if \u03b1Z > 0. If \u03b1Z = 0, the partition Z is called symmetric. The next result establishes a relationship between the degree of asymmetry and the max-hom radius.\nProposition 3.2. Let Z \u2208 P be a partition. Then we have\n1. \u03b1Z/4 \u2264 \u03c1Z\n2. \u03b1Z > 0 \u21d4 \u03c1Z > 0.\nProposition 3.2(1) says that AZ = B(Z,\u03b1/4) is a homogeneous ball. We call AZ the asymmetry ball of Z. From Theorem 3.1 and AZ \u2286 BZ follows that expectation and mean are unique if the support SQ is contained in an open subset of AZ .\nProposition 3.2(2) states that being an asymmetric partition and having a positive max-hom radius are equivalent properties. Thus, we can characterize partitions with positive max-hom radius by asymmetric partitions:\nProposition 3.3.\n1. Almost all partitions are asymmetric.\n2. A partition is asymmetric if and only if its clusters are mutually distinct.\nThe first assertion of Prop. 3.3 states that partitions with degenerated max-hom ball that collapse to a single point are the pathological cases in the sense that they are contained in some subset of measure zero.\nThe second assertion of Prop. 3.3 provides us a way of how to compute the degree of asymmetry as we will see shortly. Recall that a cluster of a partition Z is represented by a row zk of a representation Z \u2208 Z. Empty clusters are represented by zero rows. A pair of clusters of Z is distinct if the corresponding rows of Z are distinct. The next results are an immediate consequence of Prop. 3.3(2).\nCorollary 3.4.\n1. Every hard partition with at most one empty cluster is asymmetric.\n2. A symmetric partition has at least one pair of identical clusters.\n3. A partition with more than one empty cluster is symmetric.\nNext, we show how the degree of asymmetry of a partition can be determined.\nProposition 3.5. Let Z \u2208 Pm,` be a partition.\n1. Let Z \u2208 Z be a representation with rows z1, . . . ,z`. Then \u03b1Z = min {\u221a 2 \u00b7 \u2016zp \u2212 zq\u2016 : 1 \u2264 p < q \u2264 ` }\n2. Suppose that Z is a hard partition. Then \u03b1Z = \u221a 2 (m1 +m2),\nwhere m1 \u2264 m2 are the sizes of the two smallest clusters of Z.\n3. Suppose that Z is an asymmetric hard partition. Then\n\u221a 2 \u2264 \u03b1Z \u2264 2 \u00b7 \u221a\u2308m ` \u2309 .\nwhere dxe denotes the smallest integer larger than or equal to x.\nThe first statement of Prop. 3.3 tells us how to compute the degree of asymmetry of an arbitrary partition. The second statement of Prop. 3.3 gives us a simpler formula for computing the degree of asymmetry for the subset of hard partitions. Finally, the last statement of Prop. 3.3 tells us the range of values the degree of asymmetry can take for the subset of asymmetric hard partitions. Hard partitions have largest degree of asymmetry if the data points are evenly distributed across all clusters. Conversely, the degree of asymmetry is small if there are clusters with few data points."}, {"heading": "3.4. Homogeneity", "text": "In this section, we introduce homogeneity as a measure of how close a sample is to having a unique mean and provide a lower bound that can be easily determined.\nSuppose that Sn = (X1, . . . , Xn) \u2208 SnQ is a sample of n partitions. By H (Sn) we denote the set of all homogeneous sub-samples of Sn, that is the set of all subsamples of Sn with unique mean partition. Obviously, H (Sn) is non-empty, because sub-samples consisting of a singleton are homogeneous. If Sn is homogeneous, then H (Sn) coincides with the power set of Sn.\nThe homogeneity of a sample Sn is defined by H(Sn) = max { |S| n : S \u2208 H (Sn) } .\nHomogeneity measures how close a sample is to being homogeneous. Homogeneity quantifies the largest fraction of partitions that have a unique mean partition. Conversely, the value 1 \u2212H tells us how many partitions we need to remove from Sn to obtain a sub-sample with unique mean partition. Homogeneous samples have homogeneity one and heterogeneous samples have homogeneity less than one. In the worst case, the homogeneity of a sample Sn is H(Sn) = 1/n.\nIt is unclear how to compute the homogeneityH(Sn) efficiently. We therefore present a procedure to determine a lower bound of H(Sn) by using the degree of asymmetry. Let Ai be the asymmetry ball of the i-th sample partition Xi. By\nIAi(Xj) = { 1 : Xj \u2208 Ai 0 : otherwise,\nwe denote the indicator function of Ai. We can evaluate IAi(Xj) by first determining the degree of asymmetry \u03b1i of Xi according to Prop. 3.5. Then we test membership of Xj in Ai by evaluating the expression\n\u03b4(Xi, Xj) \u2264 1\n4 \u03b1i.\nThe fraction of sample partitions of Sn that are contained in Ai is given by\nhi = 1\nn n\u2211 j=1 IAi(Xj) .\nThen the approximated homogeneity (\u03b1-homogeneity) of sample Sn is defined as\nh\u2217(Sn) = max i hi.\nObviously, the \u03b1-homogeneity h\u2217(Sn) is a lower bound of H(Sn). Note that h\u2217(Sn) = 1 implies that the sample Sn is homogeneous and therefore has a unique mean partition. If h\u2217(Sn) < 1 no statement can be made about whether Sn is homogeneous or heterogeneous. In this case, h\u2217(Sn) measures how likely a unique mean is."}, {"heading": "3.5. Clustering Stability", "text": "This section links clustering instability to consensus clustering and sketches how homogeneity is related to clustering stability in a simplified setting. Finally, we briefly point to a potential conflict between cluster stability and diversity in consensus clustering."}, {"heading": "3.5.1. Clustering Instability", "text": "Choosing the number ` of clusters is a persisting model selection problem in clustering. One way to select ` is based on the concept of clustering stability. The intuitive idea behind clustering stability is that a clustering algorithm should produce similar partitions if repeatedly applied to slightly different datasets from the same underlying distribution.\nHere, we assume that Sn,k = (X1, . . . , Xn) is a sample of n partitions Xi \u2208 Pk,m of (possibly different) datasets of size m with k clusters. Following [16], model selection in clustering is posed as the problem of minimizing the function\nIn,k = 1\nn2 n\u2211 i=1 n\u2211 j=1 \u2206k(Xi, Xj)\nover all numbers k of clusters such that 1 \u2264 kmin \u2264 k \u2264 kmax \u2264 m. Then one option to choose the number ` of clusters is as follows:\n` = arg min k In,k.\nThe function In,k is called cluster instability and measures the average distance between partitions. Another less common interpretation is that cluster instability measures the average variation Fn,k(Xi) of the sample partitions Xi of Sn,k, where\nFn,k(Xi) = 1\nn n\u2211 j=1 \u2206k(Xi, Xj)\nis the Fre\u0301chet function of Sn,k with respect to the distance \u2206k. Thus, we can equivalently rewrite cluster instability as\nIn,k = 1\nn n\u2211 i=1 Fn,k(Xi) .\nThe last equation links cluster stability to consensus clustering and to Fre\u0301chet functions."}, {"heading": "3.5.2. Homogeneity vs. Stability", "text": "Intuitively, we expect that the average pairwise distance Ik,m between partitions and the average distance Fn,k(Mk) to a mean partition are correlated if the underlying distance function \u2206k is well-behaved. If \u2206k is a metric, we have (see Section B.5)\nFn,k(Mk) \u2264 In,k, (1)\nwhere Mk is a mean or medoid partition of sample Sn,k.2 These considerations suggest that the variation Fn,k(Mk) can serve as an alternative score function for model selection that is related to cluster instability In,k. We choose the number ` of clusters according to the rule\n` = arg min k Fn,k(Mk) . (2)\nTo relate homogeneity to cluster stability consider the (non-symmetric) distance\n\u2206k(Xi, Xj) = 1\u2212 IAi(Xj) .\nThe Fre\u0301chet function of Sn,k takes the form\nFn,k(Z) = 1\nn n\u2211 i=1 1\u2212 IAZ (Xi) = 1\u2212 1 n n\u2211 i=1 IAZ (Xi) = 1\u2212 hZ ,\nwhere hZ is the fraction of sample partitions of Sn,k that are contained in the asymmetry ball AZ of Z. Then we can rewrite the Fre\u0301chet variation Fn,k(Mk) at a medoid Mk by\nFn,k(Mk) = 1\u2212 h\u2217k,\nwhere h\u2217k is the \u03b1-homogeneity of Sn,k. Thus, choosing the number ` of clusters according to Equation (2) is equivalent to choosing ` according to\n` = arg max k\nh\u2217k.\nWe choose ` in such a way that uniqueness of the mean partition is most likely. This shows the relationship between uniqueness of the mean partition and cluster stability.\nIn contrast to cluster instability, \u03b1-homogeneity measures stability with respect to the size of smallest clusters. To see this, recall that the degree of asymmetry of a partition Z is\n\u03b1Z = \u221a 2(m1 +m2),\nwhere m1 and m2 are the sizes of the two smallest clusters (see Prop. 3.5(2)). Then a hard partition X is in the asymmetry ball AZ of hard partition Z if both partitions disagree on at most (m1 +m2)/4 data points. This shows that a clustering is as stable as the smallest clusters in its partition.\n2A medoid is a sample partition Mk \u2208 Sk,m such that Fn,k(Mk) \u2264 Fn,k(Xi) for all 1 \u2264 i \u2264 n."}, {"heading": "3.5.3. Stability vs. Diversity", "text": "In consensus clustering it is recommended to use diverse partitions to improve the performance [5, 22, 23]. Following [5], diversity is measured in the same way as cluster instability, namely by the sum of pairwise distances between sample partitions.\nThough diversity corresponds to cluster instability, its application in consensus clustering does not contradict the goal of cluster stability per se. What matters \u2013 from the point of view of cluster stability \u2013 is whether the resulting consensus partitions are stable. Since diversity corresponds to low homogeneity, it is unlikely that a sample of diverse partitions has a unique mean. To comply with cluster stability, the question is under which conditions are two different mean partitions similar? To answer this question, we need the following result proved by [3]:\nTheorem 3.6. LetM \u2208 P is a mean partition of the sample Sn = (X1, . . . , Xn) \u2208 Pn. Then every representation M \u2208M is of the form\nM = 1\nn n\u2211 i=1 Xi,\nwhere Xi \u2208 Xi are in optimal position with M , that is \u03b4(Xi,M) = \u2016Xi \u2212M\u2016.\nTheorem 3.6 describes the form of a mean partition in terms of representations of the sample partition. Since every partition has only finitely many different representations, the set of mean partitions of a given sample is finite and therefore discrete.\nNow suppose that the sample Sn has two different mean partitions M and M \u2032. Let M \u2208M and M \u2032 \u2208M \u2032 be representations of both mean partitions in optimal position. Applying Theorem 3.6 gives\n\u03b4(M,M \u2032) = \u2016M \u2212M \u2032\u2016 = \u2225\u2225\u2225\u2225\u2225 1n n\u2211\ni=1\nXi \u2212 1\nn n\u2211 i=1 X \u2032i \u2225\u2225\u2225\u2225\u2225 = 1n \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 (Xi \u2212X \u2032i) \u2225\u2225\u2225\u2225\u2225 where Xi,X \u2032 i \u2208 Xi are representations in optimal position with M and M \u2032, respectively. Since both means are different, there is a non-empty subset J \u2286 {1, . . . , n} of indices such that Xj 6= X \u2032j for all j \u2208 J . We obtain\n\u03b4(M,M \u2032) \u2264 1 n n\u2211 i=1 \u2016Xi \u2212X \u2032i\u2016 = 1 n \u2211 j\u2208J \u2225\u2225Xj \u2212X \u2032j\u2225\u2225 . Consider the following conditions:\n1. The index set J is small, that is |J | n,\n2. The degree of asymmetry \u03b1j of partitions Xj is low for all j \u2208 J , 3. The distance \u2225\u2225Xj \u2212X \u2032j\u2225\u2225 is close to \u03b1j for all j \u2208 J .\nThen two different mean partitions are similar if condition (1) or if both conditions (2) and (3) hold. Under these conditions, cluster stability and diversity in consensus clustering are not conflicting approaches.\nIn general, it is not self-evident that different mean partitions of a sample of diverse partitions are similar. Therefore, we point to the possibility that cluster stability and diversity in consensus clustering can be contradictory approaches in achieving the common goal of improved cluster performance."}, {"heading": "4. Experiments", "text": "The goal of this section is to assess the homogeneity of samples obtained by k-means applied to synthetic and real-world data."}, {"heading": "4.1. Experiments on Synthetic Data", "text": "Data. We generated the following types of datasets in R2:\n1. UD: Uniform distribution\n2. G4: Four Gaussians\n3. G9: Nine Gaussians\n4. U2: Two U-Shapes\n5. U4: Four U-Shapes\nThe UD datasets consists of m data points drawn from the uniform distribution on the unit square. The G4 and G9 dataset consists of m data points drawn from four and nine Gaussian distributions, resp., with identical covariance matrix \u03c32I and different mean vectors. The mean vectors of the G2 dataset are the four vertices of the unit square. The nine mean vectors of the G9 dataset are of the form (x, y) with x, y \u2208 {\u22121, 0, 1}. The U2 and U4 dataset consists of m data points forming concave and convex U-Shapes. The data points were generated by imposing Gaussian noise with mean zero and standard deviation \u03c3 on the positive and negative component of the sine-function. The U2 dataset has one concave and one convex U-Shape, whereas the U4 dataset has two of both types of U\u2013Shapes.\nData points were evenly distributed across the different clusters of the G4, G9, U2, and U4 datasets. Figure 1 shows examples of all datasets with the exception of the UD dataset. The parameters for all datasets were \u03c3 = 0.1 and mc = 50, where mc is the number of data points of a single cluster (with mc = 50, we have mUD = 50, mG4 = 200, mG9 = 450, mU2 = 100, and mU4 = 200).\nGeneric Protocol. A single experiment was conducted according to the following generic scheme:\nInput : m \u2013 number of data points \u03c3 \u2013 standard deviation k \u2013 parameter of k-means\nProcedure: Generate a dataset Z of size m with standard deviation \u03c3 Repeat n = 100 times:\nApply the k-means algorithm to dataset Z to obtain sample Sn,k Compute the \u03b1-homogeneity h\u2217(Sn,k)\nOutput : \u03b1-homogeneity h\u2217(Sn,k)\nThe procedure was repeated 100-times using the same input parameters. Finally, the average \u03b1-homogeneity h\u2217 over the 100 trials was recorded.\nThe UD datasets served as a base-line. For these datasets, \u03c3 is a factor with which the uniformly generated data points were multiplied."}, {"heading": "4.1.1. Results on G4 Datasets", "text": "The goal of the first series of experiments is to assess homogeneity as a function of the parameters k, \u03c3, and m under the assumption that the cluster structure in the data can be essentially discovered by the k-means algorithm for a suitable value of k.\nWe considered G4 datasets and contrasted the results to those obtained on UD datasets. Unless otherwise stated, the default input parameters were\n\u2022 k = 4 for the k-means algorithm, \u2022 m = 100 for the size of the datasets, \u2022 \u03c3 = 1 as factor for the UD datasets.\nHomogeneity as a function of k. We considered three types of datasets: (i) G4 generated with standard deviation \u03c3 = 0.05, (ii) G4 generated with standard deviation \u03c3 = 0.7, and (iii) UD generated with factor \u03c3 = 1. For every k \u2208 {2, . . . , 10} and for all three types of datasets, we conducted experiments according to the above described generic protocol.\nFigure 2a shows the average \u03b1-homogeneities h\u2217 as a function of the number k. We made the following observations:\n1. The general trend is that homogeneity decreases with increasing k. To understand why homogeneity decreases with increasing k, recall that the degree of asymmetry of a partition Z is\n\u03b1Z = \u221a 2(m1 +m2),\nwhere m1 and m2 are the sizes of the two smallest clusters (see Prop. 3.5(2)). From the strong form of the pigeonhole principle follows\nm1 +m2 2 \u2264 m k .\nThus, the sum m1 + m2 decreases with increasing number k of clusters. This means that homogeneity is likely to be lower for large k given a fixed number m of data\npoints. Imbalanced cluster sizes further deteriorate the situation. Figure 2b shows the average cluster sizes of k-means with k = 10 for all three types of datasets. The cluster sizes are normalized by m. We see that the cluster sizes of all types of datasets are imbalanced, in particular both types of the G4 datasets. These findings indicate that increasing the parameter k results in increasingly less stable clusterings and makes a unique mean partition increasingly less likely.\n2. The trend is interrupted at k = 4 for UD datasets and for G4 datasets with \u03c3 = 0.05. For \u03c3 = 0.05 the G4 dataset has a clearly visible cluster structure (see Figure 3). The k-means algorithm with k = 4 recovers this structure resulting in high homogeneity. Only a few partitions need to be removed in order to guarantee uniqueness of the mean. For \u03c3 = 0.7 no cluster structure is visible as shown in Figure 3. Consequently, nothing unexpected happened and homogeneity is low. These findings suggest that uniqueness of the mean partition is more likely when k-means is capable to essentially discover the cluster structure of the dataset.\nSurprisingly, there is a moderate peak at k = 4 on UD datasets conveying that k-means is most stable when viewing uniformly distributed data as a 2, and 4 cluster problem (homogeneity of samples of partitions with one cluster is always one). This moderate peak is even more notable when compared to the results on G4 datasets with \u03c3 = 0.7. This result indicates that peaks in homogeneity do not necessarily allow us to draw conclusions about the cluster structure in a dataset.\nBy combining both findings, we hypothesize that an evident cluster structure discovered by the underlying algorithm implies peaks in homogeneity but the converse claim does not necessarily hold.\n3. For k 6= 4 homogeneity is lower the more structure we assume in the dataset. We assume higher structure in G4 datasets with lower variance and we assume higher structure in any G4 datasets than in UD datasets. The results show that the more evident the assumed cluster structure is the lower is the homogeneity for a wrong choice of k. These findings suggest that mismatching an evident cluster structure in the data can introduce additional instability into the clusterings. Further research is necessary to test this hypothesis.\nHomogeneity as a function of \u03c3. For every \u03c3 \u2208 {0.01, 0.05, 0.1, 0.2, . . . , 1.0} and for both datasets of type G4 and UD, we conducted experiments according to the above described generic protocol. Figure 3 depicts examples of G4 datasets with varying standard deviation \u03c3.\nFigure 4 shows the average \u03b1-homogeneity h\u2217 as a function of the standard deviation \u03c3. We observed that the average \u03b1-homogeneity on G4 datasets decreases with increasing standard deviation \u03c3 until saturation. Homogeneity is at a high level with values above 0.9, when the four clusters are clearly visible. For larger standard deviations, the clusters become increasingly blurred and the average \u03b1-homogeneity rapidly drops below 0.4. The turning point between high and low homogeneity is around \u03c3 = 0.3 and roughly corresponds to the subjective turning point of what we might perceive as a visible cluster structure (cf. Figure 3). Moreover, homogeneity on G4 datasets around\nthe turning point is comparable to homogeneity on UD datasets, which is invariant under scaling of \u03c3. Based on these results we raise the hypothesis that an algorithm that essentially discovers a visible cluster structure in the data is stable and guarantees a unique mean partition after removing a small fraction of partitions.\nHomogeneity as a function of m. We considered three types of datasets: (i) G4 with \u03c3 = 0.3, (ii) G4 with \u03c3 = 0.7, and (iii) UD with factor \u03c3 = 1. Let mc denote the number of data points of component c of a dataset. Then G4 datasets have size m = 4mc and UD datasets are of size m = mc. For every\nmc \u2208 {25, 50, 100, 250, 500, 750, 1000, 1500, 2000, 3000, 4000, 5000}\nand for all three types of datasets, we conducted experiments according to the above described generic protocol.\nFigure 5 shows an excerpt of the average \u03b1-homogeneities h\u2217 as a function of the number mc of data points in each component. We observed that homogeneity increases with increasing dataset size m. Moreover, homogeneity increases faster and more substantially for G4 datasets with lower standard deviation. If the four clusters are just visible as for \u03c3 = 0.3, the mean partition is likely to be unique for datasets with\nat least mc = 2000 data points in each cluster. Surprisingly, homogeneity is high even for UD datasets with more than 5, 000 data points. These results indicate that homogeneity can be improved and uniqueness of the mean partition can be eventually enforced by increasing the size of the dataset."}, {"heading": "4.1.2. Results on U-Shapes and Gaussians", "text": "The goal of the second series of experiments is to assess homogeneity as a function of the parameter k under the assumption that k-means is unable to essentially discover a visible cluster structure in the data. For this we considered U2 and U4 datasets and contrasted the results to those obtained on G4 and G9 datasets. The parameters for all datasets were \u03c3 = 0.1 and mc = 50, where mc is the number of data points of a single cluster. Figure 1 shows examples of all four datasets.\nFigure 6 shows the average \u03b1-homogeneities for all four datasets. As before, the general trend is that homogeneity decreases with increasing k and is only interrupted when the clearly visible cluster structure in the dataset can be discovered by the k-\nmeans algorithm. The peaks at k = 4 and k = 9 are evident for the G4 and G9 datasets, respectively. This shows that k-means is most stable and a unique mean partition is most likely when the parameter k coincides with the number of clusters in the datasets.\nThe situation is different for both U-Shape datasets. The k-means algorithm is not able to essentially discover the cluster structure of the U2 and U4 datasets. Nevertheless, homogeneity is at the highest level for k = 2 for both datasets. While this result is desirable for the U2 dataset at the first glance, it is unsatisfactory for the U4 dataset. The result for the U2 dataset with k = 2 is only desirable at the first glance for the following reason: Although k-means performed stable and only a small fraction of partitions need to be removed in order to guarantee a unique mean partition, the discovered cluster structure does not properly match with the underlying cluster structure in the data as indicated by the top-left plot of Figure 7. The same holds for the U4 dataset as shown by the top-right plot of Figure 7. Though the result for the U4 dataset with k = 2 is unsatisfactory, closer inspection of the plots reveals that homogeneity has a moderate peak at k = 4 for the U4 dataset not present for the U2 dataset. A further small peak is at k = 6 for the U2 dataset. In the latter case, k-means frequently refines each of the two U-Shapes into three clusters (c.f. plots at bottom row of Figure 7). Such peaks may indicate that there could be a cluster structure but the underlying algorithm is unable to essentially discover this structure. A counter-example for this claim is the peak at k = 4 for UD datasets as discussed in the\nprevious experiment. The conclusion is that high homogeneity and stability are merely indicators for a possible cluster structure in the data but need further examination."}, {"heading": "4.2. Experiments on UCI Datasets", "text": "The goal of this experiment is to investigate how likely is a unique mean partition for real-world datasets. For this, we considered six datasets from the UCI Machine Learning Repository [15] listed in Table 1. For every dataset and for every k \u2208 {2, . . . , 10}, we applied k-means 100-times and recorded the \u03b1-homogeneity.\nFigure 8 shows the \u03b1-homogeneities for each dataset as a function of the number k. We observed that (i) uniqueness of the mean partition is guaranteed for small values of k, and (ii) homogeneity decreases with increasing k. Exceptions from these general observations are the music and eye dataset.\nExcept for the music dataset, observation (i) shows that uniqueness of the mean partition can be guaranteed for real world data sets. This result indicates that uniqueness\nis of practical relevance and not a matter of exceptional cases. Observation (ii) is in line with the results on synthetic data and an explanation follows the same argumentation as in Section 4.1.1. As shown in Figure 9, the clusters are highly unbalanced for k = 10. Consequently, uniqueness of the mean partition is less likely for large k. Increasing the number m of data points can improve homogeneity. This is possibly one reason why the eye dataset has substantially larger \u03b1-homogeneity than the other datasets for all k and has an additional peak of high homogeneity for k = 9, although it has the most imbalanced partitions for k = 10."}, {"heading": "5. Conclusion", "text": "Uniqueness of the mean partition is a desirable property in consensus clustering that comes along with several benefits. We showed that both, the expected partition and the mean partition, are unique when the support is contained in an open subset of some max-hom ball. According to this condition, uniqueness is neither an exceptional nor a generic property. To cope with this issue, we proposed homogeneity as a measure of how close a sample is to having a unique mean. Homogeneity is not confined to consensus clustering but is also related to cluster stability. This in turn points to the possibility that cluster stability and diversity in consensus clustering can be conflicting\ngoals. Homogeneity can be efficiently bounded from below by \u03b1-homogeneity, which applies the degree of asymmetry of a partition. With \u03b1-homogeneity, we can identify a sub-sample of the largest sub-sample of partitions that can be retained in order to guarantee a unique mean partition. Preliminary empirical results show that uniqueness occurs in real-world data and can be enforced by increasing the size of the dataset or by removing outlier partitions when \u03b1-homogeneity is high. The results also indicate that \u03b1-homogeneity can be used as a criterion for model selection. The results of this paper can be placed into the general context of a statistical theory of partitions, which embraces consensus clustering and cluster stability as special cases.\nThe main limitations of the proposed approach are twofold: restriction to the intrinsic metric on partitions derived from the Euclidean distance and restriction of uniqueness conditions on max-hom balls. Generalizing both restrictions are two possible directions of future research. Another important issue is to understand when cluster stability and diversity in consensus clustering collide."}, {"heading": "A. Preliminaries", "text": "This section presents technicalities useful for proving the results proposed in the main text.\nA.1. Notations\nWe use the following notations: By U we denote the closure of a subset U \u2286 X , by \u2202U the boundary of U , and by U\u25e6 the open subset U \\ \u2202U . The action of permutation P \u2208 \u03a0 on the subset U \u2286 X is the set defined by P U = {PX : X \u2208 U}. A transposition is a permutation matrix P \u2208 \u03a0 that permutes exactly two rows. A basic result from algebra is that any permutation matrix P = \u03a0\u2217 can be written as a matrix product P = Q1 \u00b7 \u00b7 \u00b7Qt of transpositions Qi \u2208 \u03a0 with minimum number t > 0 of factors.\nA.2. Dirichlet Fundamental Domains\nA subset F of X is a fundamental set for \u03a0 if and only if F contains exactly one representation X from each orbit [X] \u2208 X/\u03a0. A fundamental domain of \u03a0 in X is a closed set F \u2286 X that satisfies\n1. X = \u22c3\nP\u2208\u03a0\nPF\n2. PF\u25e6 \u2229 F\u25e6 = \u2205 for all P \u2208 \u03a0\u2217.\nProposition A.1. Let Z be a representation of an asymmetric partition Z \u2208 P. Then"}, {"heading": "DZ = {X \u2208 X : \u2016X \u2212Z\u2016 \u2264 \u2016X \u2212 PZ\u2016 for all P \u2208 \u03a0}", "text": "is a fundamental domain, called Dirichlet fundamental domain centered at Z.\nProof. [17], Theorem 6.6.13.\nThe next result list some properties of Dirichlet fundamental domains.\nProposition A.2. Let DZ be a Dirichlet fundamental domain centered at representation Z of an asymmetric partition Z \u2208 P. Then the following properties hold:\n1. There is a fundamental set FZ such that D\u25e6Z \u2286 FZ \u2286 DZ . 2. We have Z \u2208 D\u25e6Z . 3. Every point X \u2208 D\u25e6Z represents an asymmetric partition. 4. Suppose that X,PX \u2208 DZ for some P \u2208 \u03a0\u2217. Then X,PX \u2208 \u2202DZ . 5. PDZ = DPZ for all P \u2208 \u03a0.\nProof. The proof follows [12], Prop. 3.13 but is adapted to the notation and terminology of this contribution.\n1. [17], Theorem 6.6.11.\n2. Since Z is asymmetric, we have \u2016Z \u2212Z\u2016 < \u2016Z \u2212 PZ\u2016 for all P \u2208 \u03a0\u2217. This shows that Z \u2208 D\u25e6Z . 3. Let X \u2208 D\u25e6Z be a representation of partition X. Suppose that X is symmetric. Then there is a P \u2208 \u03a0\u2217 with X = PX. This implies X \u2208 P DZ \u2229 DZ . Then X \u2208 \u2202DZ is a boundary point of DZ by [17], Theorem 6.6.4. This contradicts our assumption that X \u2208 D\u25e6Z and shows that X is asymmetric.\n4. From X,PX \u2208 DZ follows \u2016X \u2212Z\u2016 = \u2016PX \u2212Z\u2016. Since \u03a0 acts by isometries, we have \u2016X \u2212Z\u2016 = \u2016PX \u2212 PZ\u2016. Thus, we have \u2016PX \u2212Z\u2016 = \u2016PX \u2212 PZ\u2016. This shows that PX \u2208 \u2202DZ . Let P \u2032 \u2208 \u03a0 be the inverse of P . Since P 6= I, we have P \u2032 6= I. Then\n\u2016X \u2212Z\u2016 = \u2016PX \u2212Z\u2016 = \u2225\u2225P \u2032PX \u2212 P \u2032Z\u2225\u2225 = \u2225\u2225X \u2212 P \u2032Z\u2225\u2225,\nFrom \u2016X \u2212Z\u2016 = \u2016X \u2212 P \u2032Z\u2016 follows X \u2208 \u2202DZ . 5. Let X \u2208 P DZ . We have \u2016X \u2212 PZ\u2016 \u2264 \u2016P \u2032X \u2212 PZ\u2016 for all P \u2032 \u2208 \u03a0 showing that X \u2208 DPZ . Now assume that X \u2208 DPZ . Let P \u2032 \u2208 \u03a0 be the inverse of P . Then we have\n\u2016X \u2212 PZ\u2016 = \u2225\u2225P \u2032X \u2212 P \u2032PZ\u2225\u2225 = \u2225\u2225P \u2032X \u2212Z\u2225\u2225\nby isometry of P \u2032. Hence, P \u2032X \u2208 DZ and therefore PP \u2032X = X \u2208 P DZ .\nA.3. Cross Sections\nSuppose that DZ is the Dirichlet fundamental domain centered at representation Z of an asymmetric partition Z \u2208 P. A map \u00b5 : P \u2192 DZ is a cross section into DZ , if \u03c0(\u00b5(X)) = X for all partitions X \u2208 P.\nProposition A.3. Let \u00b5 : P \u2192 DZ be a cross section into a Dirichlet fundamental domain DZ centered at representation Z of an asymmetric partition Z \u2208 P. Then the following properties hold:\n1. \u00b5 is injective.\n2. \u00b5(P) is a fundamental set. 3. \u00b5 is a measurable mapping.\nProof. Both assertions directly follow from the definitions of cross section and fundamental set. max-hom of \u00b5 directly follows from the property \u03c0 \u25e6 \u00b5 = id. Again from \u03c0 \u25e6 \u00b5 = id follows that \u00b5 maps partitions to representations. Finally, since \u00b5 is injective, the image \u00b5(P) contains exactly one representation of each partition. Hence, \u00b5(P) is a fundamental set. Finally, \u00b5 is measurable, because \u00b5\u22121 = \u03c0 and \u03c0 is an open mapping.\nLet (P,B, Q) be a measurable space. A cross section \u00b5 : P \u2192 DZ is a measurable map that gives rise to a measurable space (DZ ,B\u00b5, q)."}, {"heading": "B. Proofs", "text": "B.1. Proof of Theorem 3.1\nParts 1\u20134 show uniqueness of the expected partition and Part 5 shows uniqueness of the mean partition.\n1. Both assertions trivially hold for asymmetric partitions Z, because SQ \u2286 BZ = {Z}.\n2. Let Z \u2208 P be an asymmetric partition such that SQ \u2286 BZ . We select an arbitrary representation Z \u2208 Z and an arbitrary cross section \u00b5 : P \u2192 DZ . Let SZ = \u00b5 (SQ) be the image of the support SQ. Since BZ is a homogeneous ball, we have\nF (Y ) = \u222b P \u03b4(X,Y )2dQ(X) = \u222b SQ \u03b4(X,Y )2dQ(X) = \u222b SZ \u2016\u00b5(X)\u2212 \u00b5(Y )\u20162 dq(\u00b5(X)),\nwhere q is the image measure of measure Q under cross section \u00b5. The function\nf(Y ) = \u222b SZ \u2016X \u2212 Y \u20162 dq(X)\nhas a unique minimum M \u2208 X representing partition M \u2208 P. From\nSZ \u2286 B(Z, \u03b1Z/4) ( D\u25e6Z\ntogether with Prop. A.3 follows that partition M is independent of the choice of a particular cross section along Z. From Prop. A.2 follows that partition M is independent of the choice of a particular representation of Z. It remains to show that partition M of the second part of\nthis proof is independent of the choice of partition Z that satisfies SQ \u2286 BZ . This is proved in the sequel.\n3. Suppose that Z\u2032 \u2208 P is a partition satisfying SQ \u2286 BZ\u2032 . Let Z\u2032 \u2208 Z\u2032 be a representation such that Z,Z\u2032 \u2208 DZ \u2229 DZ\u2032 and let \u00b5\u2032 : P \u2192 DZ\u2032 be a cross section. By S = \u00b5 (SQ) and S \u2032 = \u00b5\u2032 (SQ) we denote the images of SQ under the cross sections \u00b5 and \u00b5\u2032, respectively.\nWe show that there is a permutation matrix P \u2208 \u03a0 such that S \u2032 = PS. Observe that the composition \u00b5\u2032 \u25e6 \u03c0 : S \u2192 S \u2032 is bijective. Let X \u2208 S and X \u2032 \u2208 S \u2032 be representations such that X \u2032 = \u00b5\u2032(\u03c0(X)). Since \u00b5\u2032 is a cross section, both elements X and X \u2032 represent the same partition \u03c0(X) \u2208 SQ. Then there is a permutation matrix P \u2208 \u03a0 such that X \u2032 = PX. We assume that there are representations Y \u2208 S and Y \u2032 \u2208 S \u2032 such that\nY \u2032 = \u00b5\u2032(\u03c0(Y )) 6= PY .\nSince Y and Y \u2032 represent the same partition \u03c0(Y ) \u2208 SQ, there is another permutation matrix Q \u2208 \u03a0\\{P } such that Y \u2032 = QY . We find that PY 6= QY . To see this, observe that Y \u2208 S \u2282 D\u25e6Z is an interior point of DZ . From Prop. A.2 follows that Y = \u03c0(Y ) is asymmetric. This implies that Y 6= RY for all R \u2208 \u03a0\u2217. Since P 6= Q, we obtain PY 6= QY .\nAlthough PY 6= QY , we have\n\u2016X \u2212 Y \u2016 = \u2016PX \u2212 PY \u2016 = \u2016PX \u2212QY \u2016 . (3)\nThe first equation holds, because \u03a0 acts isometrically on X . The second equation follows from X \u2032,Y \u2032 \u2208 BZ\u2032 together with the fact that BZ\u2032 is a homogeneous ball. From Equation (3) follows that the Dirichlet fundamental domain DX\u2032 centered at X \u2032 = PX contains PY and QY . Then by Prop. A.2 both representations PY and QY are elements of the boundary of DX\u2032 . By assumption, only QY \u2208 S \u2032 is an element of BZ\u2032 . Moreover, since S \u2032 ( B\u25e6Z\u2032 , we have V = D\u25e6QX \u2229 BZ\u2032 6= \u2205. Suppose that V \u2208 V is a representation of partition V . Then \u03b4(X,V ) < \u2016X \u2032 \u2212 V \u2016, because V is in the interior of DQX and X \u2032 is in the interior of DX\u2032 . This contradicts the assumption that BZ\u2032 is a homogeneous ball isometric to BZ\u2032 . Hence, Y \u2032 = PY and therefore S \u2032 = PS.\n4. This part shows that partition M is independent of the choice of Z. A permutation matrix P \u2208 \u03a0 gives rise to a diffeomorphism\nLP : X \u2192 X , X 7\u2192 P\u22121X\nwith Jacobi matrix L\u2032P = P \u22121 and |detL\u2032P | = \u2223\u2223detP\u22121\u2223\u2223 = 1. Let q\u2032 be the image measure of Q under the cross section \u00b5\u2032. From\n\u00b5\u2032(SQ) = S \u2032 = PS = L\u22121P \u25e6 \u00b5(SQ)\nfollows \u00b5 = LP \u25e6 \u00b5\u2032. Then we have\nq = \u00b5(Q) = LP \u25e6 \u00b5\u2032(Q) = LP (q\u2032),\nthat is q is the image measure of q\u2032 under LP . For every Y \u2208 S, we define the function\ngY (X) = \u2016X \u2212 Y \u20162 .\nThen we rewrite f(Y ) by\nf(Y ) = \u222b S \u2016X \u2212 Y \u20162 dq(X) = \u222b S gY (X)dq(X).\nApplying the transformation formula for integrals gives\nf(Y ) = \u222b S gY (X)dq(X) = \u222b S\u2032 gY \u25e6 LP (X \u2032) \u00b7 \u2223\u2223detL\u2032P \u2223\u2223 dq\u2032(X \u2032) = \u222b S\u2032 \u2225\u2225P\u22121X \u2032 \u2212 Y \u2225\u22252 dq\u2032(X \u2032). Since \u03a0 acts isometrically on X , we have\nf(Y ) = \u222b S\u2032 \u2225\u2225PP\u22121X \u2032 \u2212 PY \u2225\u22252 dq\u2032(X \u2032) = \u222b S\u2032 \u2225\u2225X \u2032 \u2212 PY \u2225\u22252 dq\u2032(X \u2032) = f \u2032(PY ). From Y \u2208 S and S \u2032 = PS follows PY \u2208 S \u2032. Then the unique minimizer M of f(Y ) gives PM as the unique minimizer of f \u2032(Y \u2032) on S \u2032. Both elements M and PM represent the same partition M . This shows that M is independent of the choice of Z such that SQ \u2286 BZ . Hence, M is the unique minimizer of F (Z).\n5. We show uniqueness of the mean partition. Let Sn = (X1, . . . , Xn) be a sample of n partitions. Let S{n} = {X1, . . . , Xn} denote the the set of partitions induced by Sn. We define a probability measure Qn as a probability mass function of the form\nQn(X) = 1\nn n\u2211 i=1 \u03b4X,Xi\nfor all X \u2208 SQ, where \u03b4X,Y is the Kronecker delta that gives one when X = Y agree and zero, otherwise. We use Qn as probability measure and SQ as set containing the support of Qn. Then the assertion follows from Part 1\u20134 of this proof.\nB.2. Proof of Prop. 3.2\n1. The first assertion holds for symmetric partitions Z, because \u03b1Z = 0 and therefore \u03b1Z/4 \u2264 \u03c1Z . We assume that Z is asymmetric. The group \u03a0 is a discontinuous group acting isometrically on X . The isotropy group\n\u03a0Z = {P \u2208 \u03a0 : PZ = Z}\nis trivial for any representation Z \u2208 Z. Since P \u223c= X/\u03a0, we have a bijective isometry\n\u03c6 : B(Z, \u03c1) \u2212\u2192 B(Z, \u03c1), X 7\u2192 \u03c0(X).\nfor all 0 < \u03c1 \u2264 \u03b1Z/4 by [17], Theorem 13.1.1. Setting \u03c8 = \u03c6\u22121 we find that B(Z, \u03c1) is a homogeneous ball. This shows \u03b1Z/4 \u2264 \u03c1Z .\n2. We show the second assertion. From Part 1 of this proof follows \u03b1Z > 0 \u21d2 \u03c1Z > 0. We show the opposite direction. Let \u03c1Z > 0. We assume that \u03b1Z = 0. Suppose that Z \u2208 Z is a representation. Then there is a permutation matrix P \u2208 \u03a0\u2217 such that Z = PZ. Consider the ball B\u03b5 = B(Z, \u03b5) for \u03b5 > 0. Suppose that X \u2208 B\u03b5 is an element representing an asymmetric partition. Such am element exists according to Prop. 3.3(1). Then X 6= PX and X,PX \u2208 B\u03b5. This shows that there is no bijective mapping between B(Z, \u03b5) and B\u03b5 for any \u03b5 > 0. This contradicts our assumption that \u03c1Z > 0. Hence, we have \u03b1Z > 0.\nB.3. Proof of Prop. 3.3\n1. To prove the first assertion, it is sufficient to show that the set of asymmetric partitions forms an open and dense subset in P. The projection \u03c0 : X \u2192 X/\u03a0 is open and surjective. Then the image \u03c0(U) of an open and dense subset U \u2286 X is open and dense in X/\u03a0. To see this observe that from \u03c0(X ) = \u03c0(U) and surjectivity of \u03c0 follows \u03c0(U) = X/\u03a0. From X/\u03a0 = \u03c0(U) \u2286 \u03c0(U) follows that \u03c0(U) is open and dense in X/\u03a0.\nNow let Z \u2208 X be a representation of an asymmetric partition Z \u2208 P. Suppose that \u00b5 : P \u2192 DZ is a cross section. From Prop. A.3 follows that \u00b5(P) \u2282 DZ is a fundamental set containing the open set D\u25e6Z . Since D\u25e6Z is open and dense in DZ , we find that \u03c0 (D\u25e6Z) is open and dense in P. This shows that almost all partitions are asymmetric.\n2. We show the second assertion. Let Z \u2208 P be an asymmetric partition. Suppose that Z \u2208 Z is a representation matrix with rows z1, . . . .z`. We assume that Z has two identical clusters. Then there are two distinct indices 1 \u2264 p < q \u2264 ` such that zp = zq. Let P \u2208 \u03a0\u2217 be the permutation matrix that swaps rows p and q. Then we have \u03b1Z \u2264 \u2016Z \u2212 PZ\u2016 = 0. This contradicts our assumption that Z is asymmetric. Hence, the clusters of Z are mutually distinct.\nNext, we assume that all clusters of Z are mutually distinct. Suppose that Z is asymmetric. Then there is a representation Z \u2208 Z and a permutation matrix P \u2208 \u03a0\u2217 such that \u03b1Z = \u2016Z \u2212 PZ\u2016. As stated in Section A.1, we can express P as a minimal product of t > 0 transpositions. Hence, representation Z has at least one pair of identical rows. This contradicts our assumption that the clusters of Z are mutually distinct showing that partition Z is asymmetric.\nB.4. Proof of Prop. 3.5\nThe first three parts of this proof prepare the proofs of the assertions shown in Part 4\u20136.\n1. Let P \u2208 \u03a0\u2217 be a transposition that permutes rows p < q. Then we have\n\u2016Z \u2212 PZ\u20162 = \u2016zp \u2212 zq\u20162 + \u2016zq \u2212 zp\u20162 = 2 \u2016zp \u2212 zq\u20162 .\nThis gives \u2016Z \u2212 PZ\u2016 = \u221a 2 \u2016zp \u2212 zq\u2016.\n2. Let P ,Q \u2208 \u03a0\u2217 be two different transpositions. Suppose that P permutes rows p < q and Q permutes rows r < s such that either (i) {p, q}\u2229 {r, s} = \u2205 or (ii) q = r. For case (i), we have\n\u2016Z \u2212 PQZ\u20162 = 2 \u2016zp \u2212 zq\u20162 + 2 \u2016zr \u2212 zs\u20162\naccording to the first part of this proof. This implies \u2016Z \u2212 PZ\u2016 \u2264 \u2016Z \u2212 PQZ\u2016 and \u2016Z \u2212QZ\u2016 \u2264 \u2016Z \u2212 PQZ\u2016.\n3. As stated in Section A.1, we can write P = \u03a0\u2217 as a matrix product P = Q1 \u00b7 \u00b7 \u00b7Qt of transpositions Qi \u2208 \u03a0 with minimum number t > 0 of factors. From the second part of this proof follows\n\u2016Z \u2212QiZ\u2016 \u2264 \u2016Z \u2212Q1Q2 \u00b7 \u00b7 \u00b7QtZ\u2016 = \u2016Z \u2212 PZ\u2016\nfor all i \u2208 {1, . . . , t}. This shows that it is sufficient to restrict to transpositions for determining the degree of asymmetry of a partition.\n4. Let \u03a0\u03c4 denote the subset of all transpositions. From part 1 and 3 of this proof follows \u03b1Z = min {\u2016Z \u2212 PZ\u2016 : P \u2208 \u03a0\u03c4} = min {\u221a 2 \u2016zp \u2212 zq\u2016 : 1 \u2264 p < q \u2264 ` } .\nThis shows the first assertion.\n5. The second assertion assumes that Z is a hard partition. Then the elements of Z take binary values from {0, 1} such that zTp zq = 0 for all 1 \u2264 p < q \u2264 `. Moreover, np = zTk zk is the size of the k-th cluster. Then we have\n\u2016zp \u2212 zq\u20162 = zTp zp \u2212 2zTp zq + zTq zq = np + nq.\nFrom the fourth part of this proof follows \u03b1Z = min {\u221a 2 (np + nq) : 1 \u2264 p < q \u2264 ` } .\nThis implies the second assertion.\n6. The second assertion assumes that Z is an asymmetric hard partition. We first prove the lower bound of \u03b1Z . From Prop. 3.3 follows that Z has at most one empty cluster. Then m1 = 0 and m2 > 0. Using part 5 of this proof, we obtain \u221a 2 \u2264 \u221a 2m2 = \u03b1Z . Next, we show the upper bound of \u03b1Z . From the strong form of the pigeonhole principle follows that there is a cluster with at least \u00b51 = dm/`e elements. Let m\u2032 = m \u2212 \u00b51 be the number of remaining elements. Then again applying the pigeonhole principle gives a cluster with at least \u00b52 = dm\u2032/(`\u2212 1)e elements. From \u00b51 \u2265 m/` follows\n\u00b52 = m\u2212 \u00b51 `\u2212 1 \u2264 m\u2212m/` `\u2212 1 = m ` \u2264 \u2308m ` \u2309 .\nWe can bound the cardinality of the two smallest clusters by m1 +m2 \u2264 \u00b51 + \u00b52 \u2264 2 \u2308m ` \u2309 .\nUsing part 5 of this proof shows the third assertion.\nB.5. Proof of Equation (1)\nWe have\nIn,k = 1\nn2 \u2211 i,j \u2206k(Xi, Xj)\n\u2265 1 n2 \u2211 i,j \u2206k(Mk, Xj) = 1\nn \u2211 j \u2206k(Mk, Xj)\n= Fn,k(Mk) .\nThe inequality in the second line holds, because Mk is a mean or medoid. This shows the assertion."}], "references": [{"title": "Nonparametric Inference on Manifolds with Applications to Shape Spaces", "author": ["A. Bhattacharya", "R. Bhattacharya"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Introduction to Compact Transformation", "author": ["G.E. Bredon"], "venue": "Groups. Elsevier,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1972}, {"title": "A Combination Scheme for Fuzzy Clustering", "author": ["E. Dimitriadou", "A. Weingessel", "K. Hornik"], "venue": "Advances in Soft Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Weighted cluster ensembles: Methods and analysis", "author": ["C. Domeniconi", "M. Al-Razgan"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Cluster ensemble selection", "author": ["X.Z. Fern", "W. Lin"], "venue": "Statistical Analysis and Data Mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Integrating microarray data by consensus clustering", "author": ["V. Filkov", "S. Skiena"], "venue": "International Journal on Artificial Intelligence Tools,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Ensemble clustering by means of clustering embedding in vector spaces", "author": ["L. Franek", "X. Jiang"], "venue": "Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Les \u00e9l\u00e9ments al\u00e9atoires de nature quelconque dans un espace distanci\u00e9", "author": ["M. Fr\u00e9chet"], "venue": "Annales de l\u2019institut Henri Poincare\u0301,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1948}, {"title": "A Survey: Clustering Ensembles Techniques", "author": ["R. Ghaemi", "N. Sulaiman", "H. Ibrahim", "N. Mustapha"], "venue": "Proceedings of World Academy of Science, Engineering and Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Clustering aggregation", "author": ["A. Gionis", "H. Mannila", "P. Tsaparas"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Data clustering: 50 years beyond K-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Geometry of Graph Edit Distance Spaces", "author": ["B.J. Jain"], "venue": "arXiv: 1505.08071,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Asymptotic Behavior of Mean Partitions in Consensus Clustering", "author": ["B.J. Jain"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Solving consensus and semi-supervised clustering problems using nonnegative matrix factorization", "author": ["T. Li", "C. Ding", "M.I. Jordan"], "venue": "IEEE International Conference on Data Mining,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "UCI Machine Learning Repository, [http://archive.ics.uci.edu/ml", "author": ["M. Lichman"], "venue": "University of California, School of Information and Computer Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Clustering stability: An overview", "author": ["U. von Luxburg"], "venue": "Now Publishers Inc.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Foundations of Hyperbolic Manifolds", "author": ["J.G. Ratcliffe"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Cluster Ensembles \u2013 A Knowledge Reuse Framework for Combining Multiple Partitions", "author": ["A. Strehl", "J. Ghosh"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Analysis of consensus partition in cluster ensemble", "author": ["A.P. Topchy", "M.H. Law", "A.K. Jain", "A. Fred"], "venue": "IEEE International Conference on Data Mining,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Clustering ensembles: Models of consensus and weak partitions", "author": ["A.P. Topchy", "A.K. Jain", "W. Punch"], "venue": "IEEE Transactions in Pattern Analysis and Machine Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Weighted partition consensus via kernels", "author": ["S. Vega-Pons", "J. Correa-Morris", "J. Ruiz-Shulcloper"], "venue": "Pattern Recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "A survey of clustering ensemble algorithms", "author": ["S. Vega-Pons", "J. Ruiz-Shulcloper"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Ensemble methods: foundations and algorithms", "author": ["Z.H. Zhou"], "venue": "CRC Press,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "Inspired by the success of classifier ensembles, consensus clustering has emerged as a research topic [9, 22].", "startOffset": 102, "endOffset": 109}, {"referenceID": 21, "context": "Inspired by the success of classifier ensembles, consensus clustering has emerged as a research topic [9, 22].", "startOffset": 102, "endOffset": 109}, {"referenceID": 2, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 3, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 5, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 6, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 9, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 13, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 17, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 19, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 20, "context": "One standard approach of consensus clustering combines the sample partitions to a mean partition [3, 4, 6, 7, 10, 14, 18, 20, 21].", "startOffset": 97, "endOffset": 129}, {"referenceID": 12, "context": "Moreover, under reasonable conditions, uniqueness implies strong consistency and gives rise to different versions of the law of large numbers [13, 19].", "startOffset": 142, "endOffset": 150}, {"referenceID": 18, "context": "Moreover, under reasonable conditions, uniqueness implies strong consistency and gives rise to different versions of the law of large numbers [13, 19].", "startOffset": 142, "endOffset": 150}, {"referenceID": 10, "context": "This result points to potentially colliding approaches in clustering: standard clustering advocates stability [11, 16] and consensus clustering", "startOffset": 110, "endOffset": 118}, {"referenceID": 15, "context": "This result points to potentially colliding approaches in clustering: standard clustering advocates stability [11, 16] and consensus clustering", "startOffset": 110, "endOffset": 118}, {"referenceID": 4, "context": "advocates diversity [5, 22, 23], which will be briefly discussed.", "startOffset": 20, "endOffset": 31}, {"referenceID": 21, "context": "advocates diversity [5, 22, 23], which will be briefly discussed.", "startOffset": 20, "endOffset": 31}, {"referenceID": 22, "context": "advocates diversity [5, 22, 23], which will be briefly discussed.", "startOffset": 20, "endOffset": 31}, {"referenceID": 12, "context": "In a wider context, the results presented in this paper contribute towards a statistical theory of partitions [13].", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "To analyze partitions, we suggest a geometric representation proposed in [13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "Orbit spaces are well explored, possess a rich geometrical structure and have a natural connection to Euclidean spaces [2, 12, 17].", "startOffset": 119, "endOffset": 130}, {"referenceID": 11, "context": "Orbit spaces are well explored, possess a rich geometrical structure and have a natural connection to Euclidean spaces [2, 12, 17].", "startOffset": 119, "endOffset": 130}, {"referenceID": 16, "context": "Orbit spaces are well explored, possess a rich geometrical structure and have a natural connection to Euclidean spaces [2, 12, 17].", "startOffset": 119, "endOffset": 130}, {"referenceID": 0, "context": ", C` is specified by a matrix X \u2208 [0, 1]`\u00d7m such that X1` = 1m, where 1` \u2208 R and 1m \u2208 R are vectors of all ones.", "startOffset": 34, "endOffset": 40}, {"referenceID": 0, "context": "Orbit Spaces We define the representation space X of the set P = P`,m of partitions by X = { X \u2208 [0, 1]`\u00d7m : X1` = 1` } .", "startOffset": 97, "endOffset": 103}, {"referenceID": 12, "context": "Then the pair (P, \u03b4) is a geodesic metric space [13], Theorem 2.", "startOffset": 48, "endOffset": 52}, {"referenceID": 7, "context": "Fr\u00e9chet Functions In this section, we link the consensus function of the mean partition approach to Fr\u00e9chet functions [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 0, "context": "This link provides access to many results from Statistics in Non-Euclidean spaces [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 12, "context": "The minimum of FQ exists but but is not unique, in general [13].", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "As for expected Fr\u00e9chet functions FQ, the minimum of Fn exists but is not unique, in general [13].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "Following [16], model selection in clustering is posed as the problem of minimizing the function", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "Diversity In consensus clustering it is recommended to use diverse partitions to improve the performance [5, 22, 23].", "startOffset": 105, "endOffset": 116}, {"referenceID": 21, "context": "Diversity In consensus clustering it is recommended to use diverse partitions to improve the performance [5, 22, 23].", "startOffset": 105, "endOffset": 116}, {"referenceID": 22, "context": "Diversity In consensus clustering it is recommended to use diverse partitions to improve the performance [5, 22, 23].", "startOffset": 105, "endOffset": 116}, {"referenceID": 4, "context": "Following [5], diversity is measured in the same way as cluster instability, namely by the sum of pairwise distances between sample partitions.", "startOffset": 10, "endOffset": 13}, {"referenceID": 2, "context": "To comply with cluster stability, the question is under which conditions are two different mean partitions similar? To answer this question, we need the following result proved by [3]: Theorem 3.", "startOffset": 180, "endOffset": 183}, {"referenceID": 14, "context": "For this, we considered six datasets from the UCI Machine Learning Repository [15] listed in Table 1.", "startOffset": 78, "endOffset": 82}, {"referenceID": 16, "context": "[17], Theorem 6.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The proof follows [12], Prop.", "startOffset": 18, "endOffset": 22}, {"referenceID": 16, "context": "[17], Theorem 6.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Then X \u2208 \u2202DZ is a boundary point of DZ by [17], Theorem 6.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "for all 0 < \u03c1 \u2264 \u03b1Z/4 by [17], Theorem 13.", "startOffset": 24, "endOffset": 28}], "year": 2016, "abstractText": "The expectation and the mean of partitions generated by a cluster ensemble are not unique in general. This issue poses challenges in statistical inference and cluster stability. In this contribution, we state sufficient conditions for uniqueness of expectation and mean. The proposed conditions show that a unique mean is neither exceptional nor generic. To cope with this issue, we introduce homogeneity as a measure of how likely is a unique mean for a sample of partitions. We show that homogeneity is related to cluster stability. This result points to a possible conflict between cluster stability and diversity in consensus clustering. To assess homogeneity in a practical setting, we propose an efficient way to compute a lower bound of homogeneity. Empirical results using the k-means algorithm suggest that uniqueness of the mean partition is not exceptional for real-world data. Moreover, for samples of high homogeneity, uniqueness can be enforced by increasing the number of data points or by removing outlier partitions. In a broader context, this contribution can be placed as a further step towards a statistical theory of partitions.", "creator": "LaTeX with hyperref package"}}}