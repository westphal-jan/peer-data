{"id": "1602.03218", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "abstract": "making this paper, we described and investigate a concurrent memory architecture involves coding sequences creating direct route scheduling ( ham ). mis is based incorporating a parallel circuit with leaves corresponding pseudo memory cells. this allows ham which perform instruction access in o ( computational per ) complexity, which seems a significant improvement over the standard attention setup that requires o ( discrete ) operations, where n is the image inside the memory.", "histories": [["v1", "Tue, 9 Feb 2016 23:24:33 GMT  (33kb)", "https://arxiv.org/abs/1602.03218v1", "9 pages, 6 figures, 2 tables"], ["v2", "Tue, 23 Feb 2016 10:22:25 GMT  (33kb)", "http://arxiv.org/abs/1602.03218v2", "Added soft attention appendix"]], "COMMENTS": "9 pages, 6 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marcin", "rychowicz", "karol kurach"], "accepted": false, "id": "1602.03218"}, "pdf": {"name": "1602.03218.pdf", "metadata": {"source": "META", "title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "authors": ["Marcin Andrychowicz", "Karol Kurach"], "emails": ["MARCINA@GOOGLE.COM", "KKURACH@GOOGLE.COM"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n03 21\n8v 2\n[ cs\n.L G\n] 2\n3 Fe\nb 20\n16\nWe show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples. In particular, it learns to sort n numbers in time \u0398(n logn) and generalizes well to input sequences much longer than the ones seen during the training. We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue."}, {"heading": "1. Intro", "text": "Deep Recurrent Neural Networks (RNNs) have recently proven to be very successful in real-word tasks, e.g. machine translation (Sutskever et al., 2014) and computer vision (Vinyals et al., 2014). However, the success has been achieved only on tasks which do not require a large memory to solve the problem, e.g. we can translate sentences using RNNs, but we can not produce reasonable translations of really long pieces of text, like books.\nA high-capacity memory is a crucial component necessary to deal with large-scale problems that contain plenty of long-range dependencies. Currently used RNNs do not scale well to larger memories, e.g. the number of parameters in an LSTM (Hochreiter & Schmidhuber, 1997) grows quadratically with the size of the network\u2019s memory. In\n1Work done while at Google.\npractice, this limits the number of used memory cells to few thousands.\nIt would be desirable for the size of the memory to be independent of the number of model parameters. The first versatile and highly successful architecture with this property was Neural Turing Machine (NTM) (Graves et al., 2014). The main idea behind the NTM is to split the network into a trainable \u201ccontroller\u201d and an \u201cexternal\u201d variable-size memory. It caused an outbreak of other neural network architectures with external memories (see Sec. 2).\nHowever, one aspect which has been usually neglected so far is the efficiency of the memory access. Most of the proposed memory architectures have the \u0398(n) access complexity, where n is the size of the memory. It means that, for instance, copying a sequence of length n requires performing \u0398(n2) operations, which is clearly unsatisfactory."}, {"heading": "1.1. Our contribution", "text": "In this paper we propose a novel memory module for neural networks, called Hierarchical Attentive Memory (HAM). The HAM module is generic and can be used as a building block of larger neural architectures. Its crucial property is that it scales well with the memory size \u2014 the memory access requires only \u0398(logn) operations, where n is the size of the memory. This complexity is achieved by using a new attention mechanism based on a binary tree with leaves corresponding to memory cells. The novel attention mechanism is not only faster than the standard one used in Deep Learning (Bahdanau et al., 2014), but it also facilities learning algorithms due to a built-in bias towards operating on intervals.\nWe show that an LSTM augmented with HAM is able to learn algorithms for tasks like merging, sorting or binary searching. In particular, it is the first neural network, which we are aware of, that is able to learn to sort from pure inputoutput examples and generalizes well to input sequences much longer than the ones seen during the training. Moreover, the learned sorting algorithm runs in time \u0398(n logn). We also show that the HAM memory itself is capable of simulating different classic memory structures: a stack, a FIFO queue and a priority queue."}, {"heading": "2. Related work", "text": "In this section we mention a number of recently proposed neural architectures with an external memory, which size is independent of the number of the model parameters.\nMemory architectures based on attention Attention is a recent but already extremely successful technique in Deep Learning. This mechanism allows networks to attend to parts of the (potentially preprocessed) input sequence (Bahdanau et al., 2014) while generating the output sequence. It is implemented by giving the network as an auxiliary input a linear combination of input symbols, where the weights of this linear combination can be controlled by the network.\nAttention mechanism was used to access the memory in Neural Turing Machines (NTMs) (Graves et al., 2014). It was the first paper, that explicitly attempted to train a computationally universal neural network and achieved encouraging results.\nThe Memory Network (Weston et al., 2014) is an early model that attempted to explicitly separate the memory from computation in a neural network model. The followup work of (Sukhbaatar et al., 2015) combined the memory network with the soft attention mechanism, which allowed it to be trained with less supervision. In contrast to NTMs, the memory in these models is non-writeable.\nAnother model without writeable memory is the Pointer Network (Vinyals et al., 2015), which is very similar to the attention model of Bahdanau et al. (2014). Despite not having a memory, this model was able to solve a number of difficult algorithmic problems that include the Convex Hull and the approximate 2D Travelling Salesman Problem.\nAll of the architectures mentioned so far use standard attention mechanisms to access the memory and therefore memory access complexity scales linearly with the memory size.\nMemory architectures based on data structures StackAugmented Recurrent Neural Network (Joulin & Mikolov, 2015) is a neural architecture combining an RNN and a differentiable stack. In another paper (Grefenstette et al., 2015) authors consider extending an LSTM with a stack, a FIFO queue or a double-ended queue and show some promising results. The advantage of the latter model is that the presented data structures have a constant access time.\nMemory architectures based on pointers In two recent papers (Zaremba & Sutskever, 2015; Zaremba et al., 2015) authors consider extending neural networks with nondifferentiable memories based on pointers and trained using Reinforcement Learning. The big advantage of these mod-\nels is that they allow a constant time memory access. They were however only successful on relatively simple tasks.\nAnother model, which can use a pointer-based memory is the Neural Programmer-Interpreter (Reed & de Freitas, 2015). It is very interesting, because it managed to learn sub-procedures. Unfortunately, it requires strong supervision in the form of execution traces.\nAnother type of pointer-based memory was presented in Neural Random-Access Machine (Kurach et al., 2015), which is a neural architecture mimicking classic computers.\nParallel memory architectures There are two recent memory architectures, which are especially suited for parallel computation. Grid-LSTM (Kalchbrenner et al., 2015) is an extension of LSTM to multiple dimensions. Another recent model of this type is Neural GPU (Kaiser & Sutskever, 2015), which can learn to multiply long binary numbers."}, {"heading": "3. Hierarchical Attentive Memory", "text": "In this section we describe our novel memory module called Hierarchical Attentive Memory (HAM). The HAM module is generic and can be used as a building block of larger neural network architectures. For instance, it can be added to feedforward or LSTM networks to extend their capabilities. To make our description more concrete we will consider a model consisting of an LSTM \u201ccontroller\u201d extended with a HAM module.\nThe high-level idea behind the HAM module is as follows. The memory is structured as a full binary tree with the leaves containing the data stored in the memory. The inner nodes contain some auxiliary data, which allows us to efficiently perform some types of \u201cqueries\u201d on the memory. In order to access the memory, one starts from the root of the tree and performs a top-down descent in the tree, which is similar to the hierarchical softmax procedure (Morin & Bengio, 2005). At every node of the tree, one decides to go left or right based on the auxiliary data stored in this node and a \u201cquery\u201d. Details are provided in the rest of this section."}, {"heading": "3.1. Notation", "text": "The model takes as input a sequence x1, x2, . . . and outputs a sequence y1, y2, . . .. We assume that each element of these sequences is a binary vector of size b \u2208 N, i.e. xi, yi \u2208 {0, 1}\nb. Suppose for a moment that we only want to process input sequences of length \u2264 n, where n \u2208 N is a power of two (we show later how to process sequences of an arbitrary length). The model is based on the full binary tree with n leaves. Let V denote the set of the nodes in that\ntree (notice that |V | = 2n \u2212 1) and let L \u2282 V denote the set of its leaves. Let l(e) for e \u2208 V \\ L be the left child of the node e and let r(e) be its right child.\nWe will now present the inference procedure for the model and then discuss how to train it."}, {"heading": "3.2. Inference", "text": "The high-level view of the model execution is presented in Fig. 1. The hidden state of the model consists of two components: the hidden state of the LSTM controller (denoted hLSTM \u2208 R\nl for some l \u2208 N) and the hidden values stored in the nodes of the HAM tree. More precisely, for every node e \u2208 V there is a hidden value he \u2208 Rd. These values change during the recurrent execution of the model, but we drop all timestep indices to simplify the notation.\nThe parameters of the model describe the input-output behaviour of the LSTM, as well as the following 4 transformations, which describe the HAM module: EMBED : R\nb \u2192 Rd, JOIN : Rd \u00d7Rd \u2192 Rd, SEARCH : Rd \u00d7Rl \u2192 [0, 1] and WRITE : Rd \u00d7 Rl \u2192 Rd. These transformations may be represented by arbitrary function approximators, e.g. Multilayer Perceptrons (MLPs). Their meaning will be described soon.\nThe details of the model are presented in 4 figures. Fig. 2 describes the initialization of the model. Each recurrent timestep of the model consists of three phases: the attention phase described in Fig. 3, the output phase described in Fig. 4 and the update phase described in Fig. 5. The whole timestep can be performed in time \u0398(logn).\nThe HAM parameters describe only the 4 mentioned transformations and hence the number of the model parameters does not depend on the size of the binary tree used. Thus, we can use the model to process the inputs of an arbitrary length by using big enough binary trees. It is not clear that the same set of parameters will give good results across different tree sizes, but we showed experimentally that it is indeed the case (see Sec. 4 for more details).\nWe decided to represent the transformations defining HAM with MLPs with ReLU (Nair & Hinton, 2010) activation function in all neurons except the output layer of SEARCH, which uses sigmoid activation function to ensure that the output may be interpreted as a probability. More-\nover, the network for WRITE is enhanced in a similar way as Highway Networks (Srivastava et al., 2015), i.e. WRITE(ha, hLSTM) = T (ha, hLSTM) \u00b7 H(ha, hLSTM) + (1\u2212 T (ha, hLSTM)) \u00b7 ha, where H and T are two MLPs with sigmoid activation function in the output layer. This allows the WRITE transformation to easily leave the value ha unchanged."}, {"heading": "3.3. Training", "text": "In this section we describe how to train our model from purely input-output examples using REINFORCE (Williams, 1992). In Appendix A we also present a different variant of HAM which is fully differentiable and can be trained using end-to-end backpropagation.\nLet x, y be an input-output pair. Recall that both x and y are sequences. Moreover, let \u03b8 denote the parameters of the model and let A denote the sequence of all decisions\nwhether to go left or right made during the whole execution of the model. We would like to maximize the logprobability of producing the correct output, i.e.\nL = log p(y|x, \u03b8) = log\n( \u2211\nA\np(A|x, \u03b8)p(y|A, x, \u03b8) ) .\nThis sum is intractable, so instead of minimizing it directly, we minimize a variational lower bound on it:\nF = \u2211\nA\np(A|x, \u03b8) log p(y|A, x, \u03b8) \u2264 L.\nThis sum is also intractable, so we approximate its gradient using the REINFORCE, which we briefly explain below. Using the identity \u2207p(A|x, \u03b8) = p(A|x, \u03b8)\u2207 log p(A|x, \u03b8), the gradient of the lower bound with respect to the model parameters can be rewritten as:\n\u2207F = \u2211\nA\np(A|x, \u03b8) [ \u2207 log p(y|A, x, \u03b8) +\nlog p(y|A, x, \u03b8)\u2207 log p(A|x, \u03b8) ]\n(1)\nWe estimate this value using Monte Carlo approximation. For every x we sample A\u0303 from p(A|x, \u03b8) and approximate the gradient for the input x as \u2207 log p(y|A\u0303, x, \u03b8) + log p(y|A\u0303, x, \u03b8)\u2207 log p(A\u0303|x, \u03b8).\nNotice that this gradient estimate can be computed using normal backpropagation if we substitute the gradients in the nodes2 which sample whether we should go left or right during the attention phase by\nlog p(y|A\u0303, x, \u03b8)\ufe38 \ufe37\ufe37 \ufe38 return \u2207 log p(A\u0303|x, \u03b8).\nThis term is called REINFORCE gradient estimate and the left factor is called a return in Reinforcement Learning literature. This gradient estimator is unbiased, but it often has a high variance. Therefore, we employ two standard variance-reduction technique for REINFORCE: discounted returns and baselines (Williams, 1992). Discounted returns means that our return at the t-th timestep has the form \u2211 t\u2264i \u03b3\ni\u2212t log p(yi|A\u0303, x, \u03b8) for some discount constant \u03b3 \u2208 [0, 1], which is a hyperparameter. This biases the estimator if \u03b3 < 1, but it often decreases its variance.\nFor the lack of space we do not describe the baselines technique. We only mention that our baseline is case and\n2 For a general discussion of computing gradients in computation graphs, which contain stochastic nodes see (Schulman et al., 2015).\ntimestep dependent: it is computed using a learnable linear transformation from hLSTM and trained using MSE loss function.\nThe whole model is trained with the Adam (Kingma & Ba, 2014) algorithm. We also employ the following three training techniques:\nDifferent reward function During our experiments we noticed that better results may be obtained by using a different reward function for REINFORCE. More precisely, instead of the log-probability of producing the correct output, we use the percentage of the output bits, which have the probability of being predicted correctly (given A\u0303) greater than 50%, i.e. our discounted return is equal\u2211\nt\u2264i,1\u2264j\u2264b \u03b3 i\u2212t [ p(yi,j |A\u0303, x, \u03b8) > 0.5 ] . Notice that it corresponds to the Hamming distance between the most probable outcome accordingly to the model (given A\u0302) and the correct output.\nEntropy bonus term We add a special term to the cost function which encourages exploration. More precisely, for each sampling node we add to the cost function the term\n\u03b1 H(p) , where H(p) is the entropy of the distribution of the decision, whether to go left or right in this node and \u03b1 is an exponentially decaying coefficient. This term goes to infinity, whenever the entropy goes to zero, what ensures some level of exploration. We noticed that this term works better in our experiments than the standard term of the form \u2212\u03b1H(p) (Williams, 1992).\nCurriculum schedule We start with training on inputs with lengths sampled uniformly from [1, n] for some n = 2k and the binary tree with n leaves. Whenever the error drops below some threshold, we increment the value k and start using the bigger tree with 2n leaves and inputs with lengths sampled uniformly from [1, 2n]."}, {"heading": "4. Experiments", "text": "In this section, we evaluate two variants of using the HAM module. The first one is the model described in Sec. 3, which combines an LSTM controller with a HAM module (denoted by LSTM+HAM). Then, in Sec. 4.3 we investigate the \u201craw\u201d HAM (without the LSTM controller) to check its capability of acting as classic data structures: a stack, a FIFO queue and a priority queue."}, {"heading": "4.1. Test setup", "text": "For each test that we perform, we apply the following procedure. First, we train the model with memory of size up to n = 32 using the curriculum schedule described in Sec. 3.3. The model is trained using the minibatch Adam\nalgorithm with exponentially decaying learning rate. We use random search to determine the best hyper-parameters for the model. We use gradient clipping (Pascanu et al., 2012) with constant 5. The depth of our MLPs is either 1 or 2, the LSTM controller has l = 20 memory cells and the hidden values in the tree have dimensionality d = 20. Constant \u03b7 determining a number of memory accesses between producing each output symbols (Fig. 4) is equal either 1 or 2. We always train for 100 epochs, each consisting of 1000 batches of size 50. After each epoch we evaluate the model on 200 validation batches without learning. When the training is finished, we select the model parameters that gave the lowest error rate on validation batches and report the error using these parameters on fresh 2, 500 random examples.\nWe report two types of errors: a test error and a generalization error. The test error shows how well the model is able to fit the data distribution and generalize to unknown cases, assuming that cases of similar lengths were shown during the training. It is computed using the HAM memory with n = 32 leaves, as the percentage of output sequences, which were predicted incorrectly. The lengths of test examples are sampled uniformly from the range [1, n]. Notice that we mark the whole output sequence as incorrect even if only one bit was predicted incorrectly, e.g. a hypothetical model predicting each bit incorrectly with probability 1% (and independently of the errors on the other bits) has an error rate of 96% on whole sequences if outputs consist of 320 bits.\nThe generalization error shows how well the model performs with enlarged memory on examples with lengths exceeding n. We test our model with memory 4 times bigger than the training one. The lengths of input sequences are now sampled uniformly from the range [2n+ 1, 4n].\nDuring testing we make our model fully deterministic by using the most probable outcomes instead of stochastic sampling. More precisely, we assume that during the attention phase the model decides to go right iff p > 0.5 (Fig. 3). Moreover, the output symbols (Fig. 4) are computed by rounding to zero or one instead of sampling."}, {"heading": "4.2. LSTM+HAM", "text": "We evaluate the model on a number of algorithmic tasks described below:\nReverse: Given a sequence of 10-bit vectors, output them in the reversed order., i.e. yi = xm+1\u2212i for 1 \u2264 i \u2264 m, where m is the length of the input sequence.\nSearch: Given a sequence of pairs xi = keyi||valuei for 1 \u2264 i \u2264 m\u22121 sorted by keys and a query xm = q, find the smallest i such that keyi = q and output y1 = valuei.\nKeys and values are 5-bit vectors and keys are compared lexicographically. The LSTM+HAM model is given only two timesteps (\u03b7 = 2) to solve this problem, which forces it to use a form of binary search.\nMerge: Given two sorted sequences of pairs \u2014 (p1, v1), . . . , (pm, vm) and (p\u20321, v \u2032 1), . . . , (p \u2032 m\u2032 , v \u2032 m\u2032), where pi, p \u2032 i \u2208 [0, 1] and vi, v \u2032 i \u2208 {0, 1}\n5, merge them. Pairs are compared accordingly to their priorities, i.e. values pi and p\u2032i. Priorities are unique and sampled uniformly from the set { 1300 , . . . , 300 300}, because neural networks can not easily distinguish two real numbers which are very close to each other. Input is encoded as xi = pi||vi for 1 \u2264 i \u2264 m and xm+i = p \u2032 i||v \u2032 i for 1 \u2264 i \u2264 m\n\u2032. The output consists of the vectors vi and v\u2032i sorted accordingly to their priorities 3.\nSort: Given a sequence of pairs xi = keyi||valuei sort them in a stable way4 accordingly to the lexicographic order of the keys. Keys and values are 5-bit vectors.\nAdd: Given two numbers represented in binary, compute their sum. The input is represented as a1, . . . , am,+, b1, . . . , bm,= (i.e. x1 = a1, x2 = a2 and so on), where a1, . . . , am and b1, . . . , bm are bits of the input numbers and +,= are some special symbols. Input and output numbers are encoded starting from the least significant bits.\nEvery example output shown during the training is finished by a special \u201cEnd Of Output\u201d symbol, which the model learns to predict. It forces the model to learn not only the output symbols, but also the length of the correct output.\nWe compare our model with 2 strong baseline models: encoder-decoder LSTM (Sutskever et al., 2014) and encoder-decoder LSTM with attention (denoted LSTM+A) (Bahdanau et al., 2014). The number of the LSTM cells in the baselines was chosen in such a way, that they have more parameters than the biggest of our models. We also use random search to select an optimal learning rate and some other parameters for the baselines and train them using the same curriculum scheme as LSTM+HAM.\nThe results are presented in Table 1. Not only, does LSTM+HAM solve all the problems almost perfectly, but it also generalizes very well to much longer inputs on all problems except Add. Recall that for the generalization tests we used a HAM memory of a different size than the ones used during the training, what shows that HAM gen-\n3 Notice that we earlier assumed for the sake of simplicity that the input sequences consist of binary vectors and in this task the priorities are real values. It does not however require any change of our model. We decided to use real priorities in this task in order to diversify our set of problems.\n4Stability means that pairs with equal keys should be ordered accordingly to their order in the input sequence.\neralizes very well to new sizes of the binary tree. We find this fact quite interesting, because it means that parameters learned from a small neural network (i.e. HAM based on a tree with 32 leaves) can be successfully used in a different, bigger network (i.e. HAM with 128 memory cells).\nIn comparison, the LSTM with attention does not learn to merge, nor sort. It also completely fails to generalize to longer examples, which shows that LSTM+A learns rather some statistical dependencies between inputs and outputs than the real algorithms.\nThe LSTM+HAM model makes a few errors when testing on longer outputs than the ones encountered during the training. Notice however, that we show in the table the percentage of output sequences, which contain at least one incorrect bit. For instance, LSTM+HAM on the problem Merge predicts incorrectly only 0.03% of output bits, which corresponds to 2.48% of incorrect output sequences. We believe that these rare mistakes could be avoided if one trained the model longer and chose carefully the learning rate schedule. One more way to boost generalization capabilities would be to simultaneously train the models with different memory sizes and shared parameters. We have not tried this as the generalization properties of the model were already very good."}, {"heading": "4.3. Raw HAM", "text": "In this section, we evaluate \u201craw\u201d HAM module (without the LSTM controller) to see if it can act as a drop-in replacement for 3 classic data structures: a stack, a FIFO queue and a priority queue. For each task, the network is given a sequence of PUSH and POP operations in an on-\nline manner: at timestep t the network sees only the t-th operation to perform xt. This is a more realistic scenario for data structures usage as it prevents the network from cheating by peeking into the future.\nRaw HAM module differs from the LSTM+HAM model from Sec. 3 in the following way:\n\u2022 The HAM memory is initialized with zeros.\n\u2022 The t-th output symbol yt is computed using an MLP from the value in the accessed leaf ha.\n\u2022 Notice that in the LSTM+HAM model, hLSTM acted as a kind of \u201cquery\u201d or \u201ccommand\u201d guiding the behaviour of HAM. We will now use the values xt instead. Therefore, at the t-th timestep we use xt instead of hLSTM whenever hLSTM was used in the original model, e.g. during the attention phase (Fig. 3) we use p = SEARCH(hc, xt) instead of p = SEARCH(hc, hLSTM).\nWe evaluate raw HAM on the following tasks:\nStack: The \u201cPUSH x\u201d operation places the element x (a 5-bit vector) on top of the stack, and the \u201cPOP\u201d returns the last added element and removes it from the stack.\nQueue: The \u201cPUSH x\u201d operation places the element x (a 5-bit vector) at the end of the queue and the \u201cPOP\u201d returns the oldest element and removes it from the queue.\nPriorityQueue: The \u201cPUSH x p\u201d operations adds the element x with priority p to the queue. The \u201cPOP\u201d operation returns the value with the highest priority and remove it from the queue. Both x and p are represented as 5-bit vectors and priorities are compared lexicographically. To avoid ties we assume that all elements have different priorities.\nModel was trained with the memory of size up to n = 32 with operation sequences of length n. Sequences of PUSH/POP actions for training were selected randomly. The t-th operation out of n operations in the sequence was POP with probability t\nn and PUSH otherwise. To test gen-\neralization, we report the error rates with the memory of size 4n on sequences of operations of length 4n.\nThe results presented in Table 2 shows that HAM simulates a stack and a queue perfectly with no errors whatsoever even for memory 4 times bigger. For the PriorityQueue task, the model generalizes almost perfectly to large memory, with errors only in 0.2% of output sequences."}, {"heading": "4.4. Analysis", "text": "In this section, we present some insights into the algorithms learned by the LSTM+HAM model, by investigating the the hidden representations he learned for a variant of the problem Sort in which we sort 4-bit vectors lexicographically5. For demonstration purposes, we use a small tree with n = 8 leaves and d = 6.\nThe trained network performs sorting perfectly. It attends to the leaves in the order corresponding to the order of the sorted input values, i.e. at every timestep HAM attends to the leaf corresponding to the smallest input value among the leaves, which have not been attended so far.\nIt would be interesting to exactly understand the algorithm used by the network to perform this operation. A natural solution to this problem would be to store in each hidden node e the smallest input value among the (unattended so far) leaves below e together with the information whether the smallest value is in the right or the left subtree under e.\nWe present two timesteps of our model together with some insights into the algorithm used by the network in Fig.6."}, {"heading": "5. Comparison to other models", "text": "Comparing neural networks able to learn algorithms is difficult for a few reasons. First of all, there are no wellestablished benchmark problems for this area. Secondly, the difficulty of a problem often depends on the way inputs and outputs are encoded. For example, the difficulty of the problem of adding long binary numbers depends on whether the numbers are aligned (i.e. the i-th bit of the second number is \u201cunder\u201d the i-th bit of the first number) or written next to each other (e.g. 10011+10101). Moreover, we could compare error rates on inputs from the same distribution as the ones seen during the training or compare error rates on inputs longer than the ones seen during the training to see if the model \u201creally learned the al-\n5 In the problem Sort considered in the experimental results, there are separate keys and values, which forces the model to learn stable sorting. Here, for the sake of simplicity, we consider the simplified version of the problem and do not use separate keys and values.\ngorithm\u201d. Furthermore, different models scale differently with the memory size, which makes direct comparison of error rates less meaningful.\nAs far as we know, our model is the first one which is able to learn a sorting algorithm from pure input-output examples. In (Reed & de Freitas, 2015) it is shown that an LSTM is able to learn to sort short sequences, but it fails to generalize to inputs longer than the ones seen during the training. It is quite clear that an LSTM can not learn a \u201creal\u201d sorting algorithm, because it uses a bounded memory independent of the length of the input. The Neural Programmer-Interpreter (Reed & de Freitas, 2015) is a neural network architecture, which is able to learn bubble sort, but it requires strong supervision in the form of execution traces. In comparison, our model can be trained from pure input-output examples, which is crucial if we want to use it to solve problems for which we do not know any algorithms.\nAn important feature of neural memories is their efficiency. Our HAM module in comparison to many other recently proposed solutions is effective and allows to access the memory in \u0398(log(n)) complexity. In the context of learning algorithms it may sound surprising that among all the architectures mentioned in Sec. 2 the only ones, which can copy a sequence of length n without \u0398(n2) operations are: ReinforcementLearning NTM (Zaremba & Sutskever, 2015), the model from (Zaremba et al., 2015), Neural Random-Access Ma-\nchine (Kurach et al., 2015), and Queue-Augmented LSTM (Grefenstette et al., 2015). However, the first three models have been only successful on relatively simple tasks. The last model was successful on some synthetic tasks from the domain of Natural Language Processing, which are very different from the tasks we tested our model on, so we can not directly compare the two models.\nFinally, we do not claim that our model is superior to the all other ones, e.g. Neural Turing Machines (NTM) (Graves et al., 2014). We believe that both memory mechanisms are complementary: NTM memory has a built-in associative map functionality, which may be difficult to achieve in HAM. On the other hand, HAM performs better in tasks like sorting due to a built-in bias towards operating on intervals of memory cells. Moreover, HAM allows much more efficient memory access than NTM. It is also quite possible that a machine able to learn algorithms should use many different types of memory in the same way as human brain stores a piece of information differently depending on its type and how long it should be stored (Berntson & Cacioppo, 2009)."}, {"heading": "6. Conclusions", "text": "We presented a new memory architecture for neural networks called Hierarchical Attentive Memory. Its crucial property is that it scales well with the memory size \u2014 the memory access requires only \u0398(logn) operations. This complexity is achieved by using a new attention mecha-\nnism based on a binary tree. The novel attention mechanism is not only faster than the standard one used in Deep Learning, but it also facilities learning algorithms due to the embedded tree structure.\nWe showed that an LSTM augmented with HAM can learn a number of algorithms like merging, sorting or binary searching from pure input-output examples. In particular, it is the first neural architecture able to learn a sorting algorithm and generalize well to sequences much longer than the ones seen during the training.\nWe believe that some concepts used in HAM, namely the novel attention mechanism and the idea of aggregating information through a binary tree may find applications in Deep Learning outside of the problem of designing neural memories."}, {"heading": "Acknowledgements", "text": "We would like to thank Nando de Freitas, Alexander Graves, Serkan Cabi, Misha Denil and Jonathan Hunt for helpful comments and discussions."}, {"heading": "A. Using soft attention", "text": "One of the open questions in the area of designing neural networks with attention mechanisms is whether to use a soft or hard attention. The model described in the paper belongs to the latter class of attention mechanisms as it makes hard, stochastic choices. The other solution would be to use a soft, differentiable mechanism, which attends to a linear combination of the potential attention targets and do not involve any sampling. The main advantage of such models is that their gradients can be computed exactly.\nWe now describe how to modify the model to make it fully differentiable (\u201dDHAM\u201d). Recall that in the original model the leaf which is attended at every timestep is sampled stochastically. Instead of that, we will now at every timestep compute for every leaf e the probability p(e) that this leaf would be attended if we used the stochastic procedure described in Fig. 3. The value p(e) can be computed by multiplying the probabilities of going in the right direction from all the nodes on the path from the root to e.\nAs the input for the LSTM we then use the value\u2211 e\u2208L p(e) \u00b7 he. During the write phase, we update the values of all the leaves using the formula he := p(e) \u00b7 WRITE(he, hROOT) + (1 \u2212 p(e)) \u00b7 he. Then, in the update phase we update the values of all the inner nodes, so that the equation he = JOIN(hl(e), hr(e)) is satisfied for each inner node e. Notice that one timestep of the soft version of the model takes time \u0398(n) as we have to update the values of all the nodes in the tree. Our model may be seen as a special case of Gated Graph Neural Network (Li et al.,\n2015).\nThis version of the model is fully differentiable and therefore it can be trained using end-to-end backpropagation on the log-probability of producing the correct output. We observed that training DHAM is slightly easier than the REINFORCE version. However, DHAM does not generalize as well as HAM to larger memory sizes."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Handbook of Neuroscience for the Behavioral Sciences. Number v. 1 in Handbook of Neuroscience for the Behavioral Sciences", "author": ["G.G. Berntson", "J.T. Cacioppo"], "venue": null, "citeRegEx": "Berntson and Cacioppo,? \\Q2009\\E", "shortCiteRegEx": "Berntson and Cacioppo", "year": 2009}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Neural gpus learn algorithms", "author": ["Kaiser", "\u0141ukasz", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "Kaiser et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Kalchbrenner", "Nal", "Danihelka", "Ivo", "Graves", "Alex"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Neural random-access machines", "author": ["Kurach", "Karol", "Andrychowicz", "Marcin", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.06392,", "citeRegEx": "Kurach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2015}, {"title": "Gated graph sequence neural networks", "author": ["Li", "Yujia", "Tarlow", "Daniel", "Brockschmidt", "Marc", "Zemel", "Richard"], "venue": "arXiv preprint arXiv:1511.05493,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Frederic", "Bengio", "Yoshua"], "venue": "In Aistats,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Understanding the exploding gradient problem", "author": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "Computing Research Repository (CoRR) abs/1211.5063,", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["Schulman", "John", "Heess", "Nicolas", "Weber", "Theophane", "Abbeel", "Pieter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals", "Oriol", "Toshev", "Alexander", "Bengio", "Samy", "Erhan", "Dumitru"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Learning simple algorithms from examples", "author": ["Zaremba", "Wojciech", "Mikolov", "Tomas", "Joulin", "Armand", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1511.07275,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": ", 2014) and computer vision (Vinyals et al., 2014).", "startOffset": 28, "endOffset": 50}, {"referenceID": 2, "context": "The first versatile and highly successful architecture with this property was Neural Turing Machine (NTM) (Graves et al., 2014).", "startOffset": 106, "endOffset": 127}, {"referenceID": 0, "context": "The novel attention mechanism is not only faster than the standard one used in Deep Learning (Bahdanau et al., 2014), but it also facilities learning algorithms due to a built-in bias towards operating on intervals.", "startOffset": 93, "endOffset": 116}, {"referenceID": 0, "context": "This mechanism allows networks to attend to parts of the (potentially preprocessed) input sequence (Bahdanau et al., 2014) while generating the output sequence.", "startOffset": 99, "endOffset": 122}, {"referenceID": 2, "context": "Attention mechanism was used to access the memory in Neural Turing Machines (NTMs) (Graves et al., 2014).", "startOffset": 83, "endOffset": 104}, {"referenceID": 15, "context": "The followup work of (Sukhbaatar et al., 2015) combined the memory network with the soft attention mechanism, which allowed it to be trained with less supervision.", "startOffset": 21, "endOffset": 46}, {"referenceID": 0, "context": ", 2015), which is very similar to the attention model of Bahdanau et al. (2014). Despite not having a memory, this model was able to solve a number of difficult algorithmic problems that include the Convex Hull and the approximate 2D Travelling Salesman Problem.", "startOffset": 57, "endOffset": 80}, {"referenceID": 3, "context": "In another paper (Grefenstette et al., 2015) authors consider extending an LSTM with a stack, a FIFO queue or a double-ended queue and show some promising results.", "startOffset": 17, "endOffset": 44}, {"referenceID": 18, "context": "Memory architectures based on pointers In two recent papers (Zaremba & Sutskever, 2015; Zaremba et al., 2015) authors consider extending neural networks with nondifferentiable memories based on pointers and trained using Reinforcement Learning.", "startOffset": 60, "endOffset": 109}, {"referenceID": 9, "context": "Another type of pointer-based memory was presented in Neural Random-Access Machine (Kurach et al., 2015), which is a neural architecture mimicking classic computers.", "startOffset": 83, "endOffset": 104}, {"referenceID": 7, "context": "Grid-LSTM (Kalchbrenner et al., 2015) is an extension of LSTM to multiple dimensions.", "startOffset": 10, "endOffset": 37}, {"referenceID": 14, "context": "2 For a general discussion of computing gradients in computation graphs, which contain stochastic nodes see (Schulman et al., 2015).", "startOffset": 108, "endOffset": 131}, {"referenceID": 13, "context": "We use gradient clipping (Pascanu et al., 2012) with constant 5.", "startOffset": 25, "endOffset": 47}, {"referenceID": 0, "context": ", 2014) and encoder-decoder LSTM with attention (denoted LSTM+A) (Bahdanau et al., 2014).", "startOffset": 65, "endOffset": 88}, {"referenceID": 18, "context": "2 the only ones, which can copy a sequence of length n without \u0398(n) operations are: ReinforcementLearning NTM (Zaremba & Sutskever, 2015), the model from (Zaremba et al., 2015), Neural Random-Access Machine (Kurach et al.", "startOffset": 154, "endOffset": 176}, {"referenceID": 9, "context": ", 2015), Neural Random-Access Machine (Kurach et al., 2015), and Queue-Augmented LSTM (Grefenstette et al.", "startOffset": 38, "endOffset": 59}, {"referenceID": 3, "context": ", 2015), and Queue-Augmented LSTM (Grefenstette et al., 2015).", "startOffset": 34, "endOffset": 61}, {"referenceID": 2, "context": "Neural Turing Machines (NTM) (Graves et al., 2014).", "startOffset": 29, "endOffset": 50}, {"referenceID": 10, "context": "Our model may be seen as a special case of Gated Graph Neural Network (Li et al., 2015).", "startOffset": 70, "endOffset": 87}], "year": 2016, "abstractText": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM). It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in \u0398(logn) complexity, which is a significant improvement over the standard attention mechanism that requires \u0398(n) operations, where n is the size of the memory. We show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples. In particular, it learns to sort n numbers in time \u0398(n logn) and generalizes well to input sequences much longer than the ones seen during the training. We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.", "creator": "LaTeX with hyperref package"}}}