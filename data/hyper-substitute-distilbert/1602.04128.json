{"id": "1602.04128", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2016", "title": "Coin Betting and Parameter-Free Online Learning", "abstract": "all the recent years a number of parameter - free ideas for elliptic linear optimization over hilbert spaces and for exponential spaces expert advice have indeed developed. what these two families of matrices usually seem different to a criminal eye, the proof methods are hence obviously similar, indeed the courts wonder if such a connection performs fundamentally symbolic.", "histories": [["v1", "Fri, 12 Feb 2016 17:11:42 GMT  (34kb)", "https://arxiv.org/abs/1602.04128v1", null], ["v2", "Mon, 27 Jun 2016 20:16:44 GMT  (57kb,D)", "http://arxiv.org/abs/1602.04128v2", null], ["v3", "Fri, 28 Oct 2016 16:43:55 GMT  (57kb,D)", "http://arxiv.org/abs/1602.04128v3", null], ["v4", "Fri, 4 Nov 2016 01:30:29 GMT  (57kb,D)", "http://arxiv.org/abs/1602.04128v4", "Fixed an compilation error in the latex"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["francesco orabona", "d\u00e1vid p\u00e1l"], "accepted": true, "id": "1602.04128"}, "pdf": {"name": "1602.04128.pdf", "metadata": {"source": "CRF", "title": "Coin Betting and Parameter-Free Online Learning", "authors": ["Francesco Orabona"], "emails": ["francesco@orabona.com", "dpal@yahoo-inc.com"], "sections": [{"heading": null, "text": "We present a new intuitive framework to design parameter-free algorithms for both online linear optimization over Hilbert spaces and for learning with expert advice, based on reductions to betting on outcomes of adversarial coins. We instantiate it using a betting algorithm based on the KrichevskyTrofimov estimator. The resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of regret guarantee and per-round complexity."}, {"heading": "1 Introduction", "text": "We consider the Online Linear Optimization (OLO) Cesa-Bianchi and Lugosi [2006], Shalev-Shwartz [2011] setting. In each round t, an algorithm chooses a point wt from a convex decision set K and then receives a reward vector gt. The algorithm\u2019s goal is to keep its regret small, defined as the difference between its cumulative reward and the cumulative reward of a fixed strategy u \u2208 K, that is\nRegretT (u) = T\u2211 t=1 \u3008gt, u\u3009 \u2212 T\u2211 t=1 \u3008gt, wt\u3009 .\nWe focus on two particular decision sets, the N -dimensional probability simplex \u2206N = {x \u2208 RN : x \u2265 0, \u2016x\u20161 = 1} and a Hilbert space H. OLO over \u2206N is referred to as the problem of Learning with Expert Advice (LEA). We assume bounds on the norms of the reward vectors: For OLO over H, we assume that \u2016gt\u2016 \u2264 1, and for LEA we assume that gt \u2208 [0, 1]N .\nOLO is a basic building block of many machine learning problems. For example, Online Convex Optimization (OCO), the problem analogous to OLO where \u3008gt, u\u3009 is generalized to an arbitrary convex function `t(u), is solved through a reduction to OLO Shalev-Shwartz [2011]. LEA Littlestone and Warmuth [1994], Vovk [1998], Cesa-Bianchi et al. [1997] provides a way of combining classifiers and it is at the heart of boosting Freund and Schapire [1997]. Batch and stochastic convex optimization can also be solved through a reduction to OLO Shalev-Shwartz [2011].\nTo achieve optimal regret, most of the existing online algorithms require the user to set the learning rate (step size) \u03b7 to an unknown/oracle value. For example, to obtain the optimal bound for Online Gradient Descent (OGD), the learning rate has to be set with the knowledge of the norm of the competitor u, \u2016u\u2016; second entry in Table 1. Likewise, the optimal learning rate for Hedge depends on the KL divergence between the prior weighting \u03c0 and the unknown competitor u, D (u\u2016\u03c0); seventh entry in Table 1. Recently, new parameter-free algorithms have been proposed, both for LEA Chaudhuri et al. [2009], Chernov and Vovk [2010], Luo and Schapire [2014, 2015], Koolen and van Erven [2015], Foster et al. [2015] and for OLO/OCO over Hilbert spaces Streeter and McMahan [2012], Orabona [2013], McMahan and Abernethy\nar X\niv :1\n60 2.\n04 12\n8v 4\n[ cs\n.L G\n] 4\nN ov\n2 01\n[2013], McMahan and Orabona [2014], Orabona [2014]. These algorithms adapt to the number of experts and to the norm of the optimal predictor, respectively, without the need to tune parameters. However, their design and underlying intuition is still a challenge. Foster et al. [2015] proposed a unified framework, but it is not constructive. Furthermore, all existing algorithms for LEA either have sub-optimal regret bound (e.g. extra O(log log T ) factor) or sub-optimal running time (e.g. requiring solving a numerical problem in every round, or with extra factors); see Table 1.\nContributions. We show that a more fundamental notion subsumes both OLO and LEA parameterfree algorithms. We prove that the ability to maximize the wealth in bets on the outcomes of coin flips implies OLO and LEA parameter-free algorithms. We develop a novel potential-based framework for betting algorithms. It gives intuition to previous constructions and, instantiated with the Krichevsky-Trofimov estimator, provides new and elegant algorithms for OLO and LEA. The new algorithms also have optimal worst-case guarantees on regret and time complexity; see Table 1."}, {"heading": "2 Preliminaries", "text": "We begin by providing some definitions. The Kullback-Leibler (KL) divergence between two discrete distributions p and q is D (p\u2016q) = \u2211 i pi ln (pi/qi). If p, q are real numbers in [0, 1], we denote by D (p\u2016q) = p ln (p/q)+(1\u2212p) ln ((1\u2212 p)/(1\u2212 q)) the KL divergence between two Bernoulli distributions with parameters p and q. We denote by H a Hilbert space, by \u3008\u00b7, \u00b7\u3009 its inner product, and by \u2016\u00b7\u2016 the induced norm. We denote by \u2016\u00b7\u20161 the 1-norm in RN . A function F : I \u2192 R+ is called logarithmically convex iff f(x) = ln(F (x)) is convex. Let f : V \u2192 R \u222a {\u00b1\u221e}, the Fenchel conjugate of f is f\u2217 : V \u2217 \u2192 R \u222a {\u00b1\u221e} defined on the dual vector space V \u2217 by f\u2217(\u03b8) = supx\u2208V \u3008\u03b8, x\u3009\u2212 f(x). A function f : V \u2192 R\u222a{+\u221e} is said to be proper if there exists x \u2208 V such that f(x) is finite. If f is a proper lower semi-continuous convex function then f\u2217 is also proper lower semi-continuous convex and f\u2217\u2217 = f .\nCoin Betting. We consider a gambler making repeated bets on the outcomes of adversarial coin flips. The gambler starts with an initial endowment > 0. In each round t, he bets on the outcome of a coin flip gt \u2208 {\u22121, 1}, where +1 denotes heads and \u22121 denotes tails. We do not make any assumption on how gt is generated, that is, it can be chosen by an adversary.\nThe gambler can bet any amount on either heads or tails. However, he is not allowed to borrow any additional money. If he loses, he loses the betted amount; if he wins, he gets the betted amount back and, in addition to that, he gets the same amount as a reward. We encode the gambler\u2019s bet in round t by a single number wt. The sign of wt encodes whether he is betting on heads or tails. The absolute value encodes the betted amount. We define Wealtht as the gambler\u2019s wealth at the end of round t and Rewardt as the\n1These algorithms require to solve a numerical problem at each step. The number K is the number of steps needed to reach the required precision. Neither the precision nor K are calculated in these papers.\n2The proof in Koolen and van Erven [2015] can be modified to prove a KL bound, see http://blog.wouterkoolen.info. 3A variant of the algorithm in Foster et al. [2015] can be implemented with the stated time complexity Foster [2016].\ngambler\u2019s net reward (the difference of wealth and initial endowment), that is\nWealtht = + t\u2211 i=1 wigi and Rewardt = Wealtht\u2212 . (1)\nIn the following, we will also refer to a bet with \u03b2t, where \u03b2t is such that\nwt = \u03b2t Wealtht\u22121 . (2)\nThe absolute value of \u03b2t is the fraction of the current wealth to bet, and sign of \u03b2t encodes whether he is betting on heads or tails. The constraint that the gambler cannot borrow money implies that \u03b2t \u2208 [\u22121, 1]. We also generalize the problem slightly by allowing the outcome of the coin flip gt to be any real number in the interval [\u22121, 1]; wealth and reward in (1) remain exactly the same."}, {"heading": "3 Warm-Up: From Betting to One-Dimensional Online Linear", "text": "Optimization\nIn this section, we sketch how to reduce one-dimensional OLO to betting on a coin. The reasoning for generic Hilbert spaces (Section 5) and for LEA (Section 6) will be similar. We will show that the betting view provides a natural way for the analysis and design of online learning algorithms, where the only design choice is the potential function of the betting algorithm (Section 4). A specific example of coin betting potential and the resulting algorithms are in Section 7.\nAs a warm-up, let us consider an algorithm for OLO over one-dimensional Hilbert space R. Let {wt}\u221et=1 be its sequence of predictions on a sequence of rewards {gt}\u221et=1, gt \u2208 [\u22121, 1]. The total reward of the algorithm after t rounds is Rewardt = \u2211t i=1 giwi. Also, even if in OLO there is no concept of \u201cwealth\u201d, define the wealth of the OLO algorithm as Wealtht = + Rewardt, as in (1). We now restrict our attention to algorithms whose predictions wt are of the form of a bet, that is wt = \u03b2t Wealtht\u22121, where \u03b2t \u2208 [\u22121, 1]. We will see that the restriction on \u03b2t does not prevent us from obtaining parameter-free algorithms with optimal bounds.\nGiven the above, it is immediate to see that any coin betting algorithm that, on a sequence of coin flips {gt}\u221et=1, gt \u2208 [\u22121, 1], bets the amounts wt can be used as an OLO algorithm in a one-dimensional Hilbert space R. But, what would be the regret of such OLO algorithms?\nAssume that the betting algorithm at hand guarantees that its wealth is at least F ( \u2211T t=1 gt) starting\nfrom an endowment , for a given potential function F , then\nRewardT = T\u2211 t=1 gtwt = WealthT \u2212 \u2265 F ( T\u2211 t=1 gt ) \u2212 . (3)\nIntuitively, if the reward is big we can expect the regret to be small. Indeed, the following lemma converts the lower bound on the reward to an upper bound on the regret.\nLemma 1 (Reward-Regret relationship McMahan and Orabona [2014]). Let V, V \u2217 be a pair of dual vector spaces. Let F : V \u2192 R \u222a {+\u221e} be a proper convex lower semi-continuous function and let F \u2217 : V \u2217 \u2192 R \u222a {+\u221e} be its Fenchel conjugate. Let w1, w2, . . . , wT \u2208 V and g1, g2, . . . , gT \u2208 V \u2217. Let \u2208 R. Then,\nT\u2211 t=1\n\u3008gt, wt\u3009\ufe38 \ufe37\ufe37 \ufe38 RewardT \u2265 F\n( T\u2211 t=1 gt ) \u2212 if and only if \u2200u \u2208 V \u2217, T\u2211 t=1\n\u3008gt, u\u2212 wt\u3009\ufe38 \ufe37\ufe37 \ufe38 RegretT (u)\n\u2264 F \u2217(u) + .\nApplying the lemma, we get a regret upper bound: RegretT (u) \u2264 F \u2217(u) + for all u \u2208 H. To summarize, if we have a betting algorithm that guarantees a minimum wealth of F ( \u2211T t=1 gt), it can be used to design and analyze a one-dimensional OLO algorithm. The faster the growth of the wealth,\nthe smaller the regret will be. Moreover, the lemma also shows that trying to design an algorithm that is adaptive to u is equivalent to designing an algorithm that is adaptive to \u2211T t=1 gt. Also, most importantly, methods that guarantee optimal wealth for the betting scenario are already known, see, e.g., [Cesa-Bianchi and Lugosi, 2006, Chapter 9]. We can just re-use them to get optimal online algorithms!"}, {"heading": "4 Designing a Betting Algorithm: Coin Betting Potentials", "text": "For sequential betting on i.i.d. coin flips, an optimal strategy has been proposed by Kelly [1956]. The strategy assumes that the coin flips {gt}\u221et=1, gt \u2208 {+1,\u22121}, are generated i.i.d. with known probability of heads. If p \u2208 [0, 1] is the probability of heads, the Kelly bet is to bet \u03b2t = 2p\u2212 1 at each round. He showed that, in the long run, this strategy will provide more wealth than betting any other fixed fraction of the current wealth Kelly [1956].\nFor adversarial coins, Kelly betting does not make sense. With perfect knowledge of the future, the gambler could always bet everything on the right outcome. Hence, after T rounds from an initial endowment , the maximum wealth he would get is 2T . Instead, assume he bets the same fraction \u03b2 of its wealth at each round. Let Wealtht(\u03b2) the wealth of such strategy after t rounds. As observed in McMahan and Abernethy\n[2013], the optimal fixed fraction to bet is \u03b2\u2217 = ( \u2211T t=1 gt)/T and it gives the wealth\nWealthT (\u03b2 \u2217) = exp ( T \u00b7D ( 1 2 + \u2211T t=1 gt 2T \u2225\u2225\u2225 12)) \u2265 exp( (\u2211Tt=1 gt)22T ) , (4) where the inequality follows from Pinsker\u2019s inequality [Cover and Thomas, 2006, Lemma 11.6.1].\nHowever, even without knowledge of the future, it is possible to go very close to the wealth in (4). This problem was studied by Krichevsky and Trofimov [1981], who proposed that after seeing the coin flips g1, g2, . . . , gt\u22121 the empirical estimate kt = 1/2+ \u2211t\u22121 i=1 1[gi=+1]\nt should be used instead of p. Their estimate is commonly called KT estimator.1 The KT estimator results in the betting\n\u03b2t = 2kt \u2212 1 = \u2211t\u22121 i=1 gi t (5)\nwhich we call adaptive Kelly betting based on the KT estimator. It looks like an online and slightly biased version of the oracle choice of \u03b2\u2217. This strategy guarantees2\nWealthT \u2265 WealthT (\u03b2 \u2217)\n2 \u221a T\n= 2 \u221a T\nexp ( T \u00b7D ( 1 2 + \u2211T t=1 gt 2T \u2225\u2225\u2225 12)) . This guarantee is optimal up to constant factors [Cesa-Bianchi and Lugosi, 2006] and mirrors the guarantee of the Kelly bet.\nHere, we propose a new set of definitions that allows to generalize the strategy of adaptive Kelly betting based on the KT estimator. For these strategies it will be possible to prove that, for any g1, g2, . . . , gt \u2208 [\u22121, 1],\nWealtht \u2265 Ft ( t\u2211 i=1 gi ) , (6)\nwhere Ft(x) is a certain function. We call such functions potentials. The betting strategy will be determined uniquely by the potential (see (c) in the Definition 2), and we restrict our attention to potentials for which (6) holds. These constraints are specified in the definition below.\nDefinition 2 (Coin Betting Potential). Let > 0. Let {Ft}\u221et=0 be a sequence of functions Ft : (\u2212at, at) \u2192 R+ where at > t. The sequence {Ft}\u221et=0 is called a sequence of coin betting potentials for initial endowment , if it satisfies the following three conditions:\n(a) F0(0) = .\n1Compared to the maximum likelihood estimate \u2211t\u22121 i=1 1[gi=+1]\nt\u22121 , KT estimator shrinks slightly towards 1/2.\n2See Appendix A for a proof. For lack of space, all the appendices are in the supplementary material.\n(b) For every t \u2265 0, Ft(x) is even, logarithmically convex, strictly increasing on [0, at), and limx\u2192at Ft(x) = +\u221e.\n(c) For every t \u2265 1, every x \u2208 [\u2212(t\u2212 1), (t\u2212 1)] and every g \u2208 [\u22121, 1], (1 + g\u03b2t)Ft\u22121(x) \u2265 Ft(x+ g), where\n\u03b2t = Ft(x+1)\u2212Ft(x\u22121) Ft(x+1)+Ft(x\u22121) . (7)\nThe sequence {Ft}\u221et=0 is called a sequence of excellent coin betting potentials for initial endowment if it satisfies conditions (a)\u2013(c) and the condition (d) below.\n(d) For every t \u2265 0, Ft is twice-differentiable and satisfies x \u00b7 F \u2032\u2032t (x) \u2265 F \u2032t (x) for every x \u2208 [0, at).\nLet\u2019s give some intuition on this definition. First, let\u2019s show by induction on t that (b) and (c) of the definition together with (2) give a betting strategy that satisfies (6). The base case t = 0 is trivial. At time t \u2265 1, bet wt = \u03b2t Wealtht\u22121 where \u03b2t is defined in (7), then\nWealtht = Wealtht\u22121 +wtgt = (1 + gt\u03b2t) Wealtht\u22121\n\u2265 (1 + gt\u03b2t)Ft\u22121 ( t\u22121\u2211 i=1 gi ) \u2265 Ft ( t\u22121\u2211 i=1 gi + gt ) = Ft ( t\u2211 i=1 gi ) .\nThe formula for the potential-based strategy (7) might seem strange. However, it is derived\u2014see Theorem 8 in Appendix B\u2014by minimizing the worst-case value of the right-hand side of the inequality used w.r.t. to gt in the induction proof above: Ft\u22121(x) \u2265 Ft(x+gt)1+gt\u03b2t . The last point, (d), is a technical condition that allows us to seamlessly reduce OLO over a Hilbert space to the one-dimensional problem, characterizing the worst case direction for the reward vectors. Regarding the design of coin betting potentials, we expect any potential that approximates the best\npossible wealth in (4) to be a good candidate. In fact, Ft(x) = exp ( x2/(2t) ) / \u221a t, essentially the potential used in the parameter-free algorithms in McMahan and Orabona [2014], Orabona [2014] for OLO and in Chaudhuri et al. [2009], Luo and Schapire [2014, 2015] for LEA, approximates (4) and it is an excellent coin betting potential\u2014see Theorem 9 in Appendix B. Hence, our framework provides intuition to previous constructions and in Section 7 we show new examples of coin betting potentials.\nIn the next two sections, we presents the reductions to effortlessly solve both the generic OLO case and LEA with a betting potential."}, {"heading": "5 From Coin Betting to OLO over Hilbert Space", "text": "In this section, generalizing the one-dimensional construction in Section 3, we show how to use a sequence of excellent coin betting potentials {Ft}\u221et=0 to construct an algorithm for OLO over a Hilbert space and how to prove a regret bound for it.\nWe define reward and wealth analogously to the one-dimensional case: Rewardt = \u2211t i=1\u3008gi, wi\u3009 and Wealtht = +Rewardt. Given a sequence of coin betting potentials {Ft}\u221et=0, using (7) we define the fraction\n\u03b2t = Ft(\u2016\u2211t\u22121i=1 gi\u2016+1)\u2212Ft(\u2016\u2211t\u22121i=1 gi\u2016\u22121) Ft(\u2016\u2211t\u22121i=1 gi\u2016+1)+Ft(\u2016\u2211t\u22121i=1 gi\u2016\u22121) . (8)\nThe prediction of the OLO algorithm is defined similarly to the one-dimensional case, but now we also need a direction in the Hilbert space:\nwt = \u03b2t Wealtht\u22121 \u2211t\u22121 i=1 gi\u2225\u2225\u2225\u2211t\u22121i=1 gi\u2225\u2225\u2225 = \u03b2t \u2211t\u22121 i=1 gi\u2225\u2225\u2225\u2211t\u22121i=1 gi\u2225\u2225\u2225 ( + t\u22121\u2211 i=1 \u3008gi, wi\u3009 ) . (9)\nIf \u2211t\u22121 i=1 gi is the zero vector, we define wt to be the zero vector as well. For this prediction strategy we can prove the following regret guarantee, proved in Appendix C. The proof reduces the general Hilbert case to the 1-d case, thanks to (d) in Definition 2, then it follows the reasoning of Section 3.\nTheorem 3 (Regret Bound for OLO in Hilbert Spaces). Let {Ft}\u221et=0 be a sequence of excellent coin betting potentials. Let {gt}\u221et=1 be any sequence of reward vectors in a Hilbert space H such that \u2016gt\u2016 \u2264 1 for all t. Then, the algorithm that makes prediction wt defined by (9) and (8) satisfies\n\u2200T \u2265 0 \u2200u \u2208 H RegretT (u) \u2264 F \u2217T (\u2016u\u2016) + ."}, {"heading": "6 From Coin Betting to Learning with Expert Advice", "text": "In this section, we show how to use the algorithm for OLO over one-dimensional Hilbert space R from Section 3\u2014which is itself based on a coin betting strategy\u2014to construct an algorithm for LEA.\nLet N \u2265 2 be the number of experts and \u2206N be the N -dimensional probability simplex. Let \u03c0 = (\u03c01, \u03c02, . . . , \u03c0N ) \u2208 \u2206N be any prior distribution. Let A be an algorithm for OLO over the one-dimensional Hilbert space R, based on a sequence of the coin betting potentials {Ft}\u221et=0 with initial endowment3 1. We instantiate N copies of A.\nConsider any round t. Let wt,i \u2208 R be the prediction of the i-th copy of A. The LEA algorithm computes p\u0302t = (p\u0302t,1, p\u0302t,2, . . . , p\u0302t,N ) \u2208 RN0,+ as\np\u0302t,i = \u03c0i \u00b7 [wt,i]+, (10)\nwhere [x]+ = max{0, x} is the positive part of x. Then, the LEA algorithm predicts pt = (pt,1, pt,2, . . . , pt,N ) \u2208 \u2206N as\npt = p\u0302t \u2016p\u0302t\u20161 . (11)\nIf \u2016p\u0302t\u20161 = 0, the algorithm predicts the prior \u03c0. Then, the algorithm receives the reward vector gt = (gt,1, gt,2, . . . , gt,N ) \u2208 [0, 1]N . Finally, it feeds the reward to each copy of A. The reward for the i-th copy of A is g\u0303t,i \u2208 [\u22121, 1] defined as\ng\u0303t,i = { gt,i \u2212 \u3008gt, pt\u3009 if wt,i > 0 , [gt,i \u2212 \u3008gt, pt\u3009]+ if wt,i \u2264 0 .\n(12)\nThe construction above defines a LEA algorithm defined by the predictions pt, based on the algorithm A. We can prove the following regret bound for it.\nTheorem 4 (Regret Bound for Experts). Let A be an algorithm for OLO over the one-dimensional Hilbert space R, based on the coin betting potentials {Ft}\u221et=0 for an initial endowment of 1. Let f\u22121t be the inverse of ft(x) = ln(Ft(x)) restricted to [0,\u221e). Then, the regret of the LEA algorithm with prior \u03c0 \u2208 \u2206N that predicts at each round with pt in (11) satisfies\n\u2200T \u2265 0 \u2200u \u2208 \u2206N RegretT (u) \u2264 f\u22121T (D (u\u2016\u03c0)) .\nThe proof, in Appendix D, is based on the fact that (10)\u2013(12) guarantee that \u2211N i=1 \u03c0ig\u0303t,iwt,i \u2264 0 and on\na variation of the change of measure lemma used in the PAC-Bayes literature, e.g. McAllester [2013]."}, {"heading": "7 Applications of the Krichevsky-Trofimov Estimator to OLO and", "text": "LEA\nIn the previous sections, we have shown that a coin betting potential with a guaranteed rapid growth of the wealth will give good regret guarantees for OLO and LEA. Here, we show that the KT estimator has associated an excellent coin betting potential, which we call KT potential. Then, the optimal wealth guarantee of the KT potentials will translate to optimal parameter-free regret bounds.\n3Any initial endowment > 0 can be rescaled to 1. Instead of Ft(x) we would use Ft(x)/ . The wt would become wt/ , but pt is invariant to scaling of wt. Hence, the LEA algorithm is the same regardless of .\nAlgorithm 1 Algorithm for OLO over Hilbert space H based on KT potential Require: Initial endowment > 0\n1: for t = 1, 2, . . . do 2: Predict with wt \u2190 1t ( + \u2211t\u22121 i=1\u3008gi, wi\u3009 )\u2211t\u22121 i=1 gi 3: Receive reward vector gt \u2208 H such that \u2016gt\u2016 \u2264 1 4: end for\nThe sequence of excellent coin betting potentials for an initial endowment corresponding to the adaptive Kelly betting strategy \u03b2t defined by (5) based on the KT estimator are\nFt(x) = 2t\u00b7\u0393\n( t+1\n2 + x 2\n) \u00b7\u0393 ( t+1\n2 \u2212 x 2 ) \u03c0\u00b7t! t \u2265 0, x \u2208 (\u2212t\u2212 1, t+ 1), (13)\nwhere \u0393(x) = \u222b\u221e\n0 tx\u22121e\u2212tdt is Euler\u2019s gamma function\u2014see Theorem 13 in Appendix E. This potential\nwas used to prove regret bounds for online prediction with the logarithmic loss Krichevsky and Trofimov [1981][Cesa-Bianchi and Lugosi, 2006, Chapter 9.7]. Theorem 13 also shows that the KT betting strategy \u03b2t as defined by (5) satisfies (7).\nThis potential has the nice property that is satisfies the inequality in (c) of Definition 2 with equality when gt \u2208 {\u22121, 1}, i.e. Ft(x+ gt) = (1 + gt\u03b2t)Ft\u22121(x).\nWe also generalize the KT potentials to \u03b4-shifted KT potentials, where \u03b4 \u2265 0, defined as\nFt(x) = 2t\u00b7\u0393(\u03b4+1)\u00b7\u0393\n( t+\u03b4+1\n2 + x 2\n) \u00b7\u0393 ( t+\u03b4+1\n2 \u2212 x 2 ) \u0393 ( \u03b4+1\n2\n)2 \u00b7\u0393(t+\u03b4+1)\n.\nThe reason for its name is that, up to a multiplicative constant, Ft is equal to the KT potential shifted in time by \u03b4. Theorem 13 also proves that the \u03b4-shifted KT potentials are excellent coin betting potentials\nwith initial endowment 1, and the corresponding betting fraction is \u03b2t = \u2211t\u22121 j=1 gj\n\u03b4+t ."}, {"heading": "7.1 OLO in Hilbert Space", "text": "We apply the KT potential for the construction of an OLO algorithm over a Hilbert space H. We will use (9), and we just need to calculate \u03b2t. According to Theorem 13 in Appendix E, the formula for \u03b2t simplifies\nto \u03b2t = \u2016\u2211t\u22121i=1 gi\u2016 t so that wt = 1 t ( + \u2211t\u22121 i=1\u3008gi, wi\u3009 )\u2211t\u22121 i=1 gi.\nThe resulting algorithm is stated as Algorithm 1. We derive a regret bound for it as a very simple corollary of Theorem 3 to the KT potential (13). The only technical part of the proof, in Appendix F, is an upper bound on F \u2217t since it cannot be expressed as an elementary function. Corollary 5 (Regret Bound for Algorithm 1). Let > 0. Let {gt}\u221et=1 be any sequence of reward vectors in a Hilbert space H such that \u2016gt\u2016 \u2264 1. Then Algorithm 1 satisfies\n\u2200T \u2265 0 \u2200u \u2208 H RegretT (u) \u2264 \u2016u\u2016 \u221a T ln ( 1 + 24T 2\u2016u\u20162 2 ) + ( 1\u2212 1 e \u221a \u03c0T ) .\nIt is worth noting the elegance and extreme simplicity of Algorithm 1 and contrast it with the algorithms in Streeter and McMahan [2012], McMahan and Orabona [2014], Orabona [2013, 2014]. Also, the regret bound is optimal Streeter and McMahan [2012], Orabona [2013]. The parameter can be safely set to any constant, e.g. 1. Its role is equivalent to the initial guess used in doubling tricks Shalev-Shwartz [2011]."}, {"heading": "7.2 Learning with Expert Advice", "text": "We will now construct an algorithm for LEA based on the \u03b4-shifted KT potential. We set \u03b4 to T/2, requiring the algorithm to know the number of rounds T in advance; we will fix this later with the standard doubling trick.\nAlgorithm 2 Algorithm for Learning with Expert Advice based on \u03b4-shifted KT potential Require: Number of experts N , prior distribution \u03c0 \u2208 \u2206N , number of rounds T 1: for t = 1, 2, . . . , T do\n2: For each i \u2208 [N ], set wt,i \u2190 \u2211t\u22121 j=1 g\u0303j,i\nt+T/2\n( 1 + \u2211t\u22121 j=1 g\u0303j,iwj,i ) 3: For each i \u2208 [N ], set p\u0302t,i \u2190 \u03c0i[wt,i]+\n4: Predict with pt \u2190 { p\u0302t/ \u2016p\u0302t\u20161 if \u2016p\u0302t\u20161 > 0 \u03c0 if \u2016p\u0302t\u20161 = 0 5: Receive reward vector gt \u2208 [0, 1]N\n6: For each i \u2208 [N ], set g\u0303t,i \u2190 { gt,i \u2212 \u3008gt, pt\u3009 if wt,i > 0 [gt,i \u2212 \u3008gt, pt\u3009]+ if wt,i \u2264 0 7: end for\nTo use the construction in Section 6, we need an OLO algorithm for the 1-d Hilbert space R. Using the \u03b4-shifted KT potentials, the algorithm predicts for any sequence {g\u0303t}\u221et=1 of reward\nwt = \u03b2t Wealtht\u22121 = \u03b2t 1 + t\u22121\u2211 j=1 g\u0303jwj  = \u2211t\u22121i=1 g\u0303i T/2 + t 1 + t\u22121\u2211 j=1 g\u0303jwj  . Then, following the construction in Section 6, we arrive at the final algorithm, Algorithm 2. We can derive a regret bound for Algorithm 2 by applying Theorem 4 to the \u03b4-shifted KT potential.\nCorollary 6 (Regret Bound for Algorithm 2). Let N \u2265 2 and T \u2265 0 be integers. Let \u03c0 \u2208 \u2206N be a prior. Then Algorithm 2 with input N, \u03c0, T for any rewards vectors g1, g2, . . . , gT \u2208 [0, 1]N satisfies\n\u2200u \u2208 \u2206N RegretT (u) \u2264 \u221a 3T (3 + D (u\u2016\u03c0)) .\nHence, the Algorithm 2 has both the best known guarantee on worst-case regret and per-round time complexity, see Table 1. Also, it has the advantage of being very simple.\nThe proof of the corollary is in the Appendix F. The only technical part of the proof is an upper bound on f\u22121t (x), which we conveniently do by lower bounding Ft(x).\nThe reason for using the shifted potential comes from the analysis of f\u22121t (x). The unshifted algorithm would have a O( \u221a T (log T + D (u\u2016\u03c0)) regret bound; the shifting improves the bound to O( \u221a T (1 + D (u\u2016\u03c0)). By changing T/2 in Algorithm 2 to another constant fraction of T , it is possible to trade-off between the two constants 3 present in the square root in the regret upper bound.\nThe requirement of knowing the number of rounds T in advance can be lifted by the standard doubling trick [Shalev-Shwartz, 2011, Section 2.3.1], obtaining an anytime guarantee with a bigger leading constant,\n\u2200T \u2265 0 \u2200u \u2208 \u2206N RegretT (u) \u2264 \u221a 2\u221a 2\u22121\n\u221a 3T (3 + D (u\u2016\u03c0)) ."}, {"heading": "8 Discussion of the Results", "text": "We have presented a new interpretation of parameter-free algorithms as coin betting algorithms. This interpretation, far from being just a mathematical gimmick, reveals the common hidden structure of previous parameter-free algorithms for both OLO and LEA and also allows the design of new algorithms. For example, we show that the characteristic of parameter-freeness is just a consequence of having an algorithm that guarantees the maximum reward possible. The reductions in Sections 5 and 6 are also novel and they are in a certain sense optimal. In fact, the obtained Algorithms 1 and 2 achieve the optimal worst case upper bounds on the regret, see Streeter and McMahan [2012], Orabona [2013] and Cesa-Bianchi and Lugosi [2006] respectively.\nWe have also run an empirical evaluation to show that the theoretical difference between classic online learning algorithms and parameter-free ones is real and not just theoretical. In Figure 1, we have used three regression datasets4, and solved the OCO problem through OLO. In all the three cases, we have used the absolute loss and normalized the input vectors to have L2 norm equal to 1. From the empirical results, it is clear that the optimal learning rate is completely data-dependent, yet parameter-free algorithms have performance very close to the unknown optimal tuning of the learning rate. Moreover, the KT-based Algorithm 1 seems to dominate all the other similar algorithms.\nFor LEA, we have used the synthetic setting in Chaudhuri et al. [2009]. The dataset is composed of Hadamard matrices of size 64, where the row with constant values is removed, the rows are duplicated to 126 inverting their signs, 0.025 is subtracted to k rows, and the matrix is replicated in order to generate T = 32768 samples. For more details, see Chaudhuri et al. [2009]. Here, the KT-based algorithm is the one in Algorithm 2, where the term T/2 is removed, so that the final regret bound has an additional lnT term. Again, we see that the parameter-free algorithms have a performance close or even better than Hedge with an oracle tuning of the learning rate, with no clear winners among the parameter-free algorithms.\nNotice that since the adaptive Kelly strategy based on KT estimator is very close to optimal, the only possible improvement is to have a data-dependent bound, for example like the ones in Orabona [2014], Koolen and van Erven [2015], Luo and Schapire [2015]. In future work, we will extend our definitions and reductions to the data-dependent case.\n4Datasets available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.\nAcknowledgments. The authors thank Jacob Abernethy, Nicolo\u0300 Cesa-Bianchi, Satyen Kale, Chansoo Lee, Giuseppe Molteni, and Manfred Warmuth for useful discussions on this work."}, {"heading": "A From Log Loss to Wealth", "text": "Guarantees for betting or sequential investement algorithm are often expressed as upper bounds on the regret with respect to the log loss. Here, for the sake of completeness, we show how to convert such a guarantee to a lower bound on the wealth of the corresponding betting algorithm.\nWe consider the problem of predicting a binary outcome. The algorithm predicts at each round probability pt \u2208 [0, 1]. The adversary generates a sequences of outcomes xt \u2208 {0, 1} and the algorithm\u2019s loss is\n`(pt, xt) = \u2212xt ln pt \u2212 (1\u2212 xt) ln(1\u2212 pt) .\nWe define the regret with respect to a fixed probability vector \u03b2 as\nRegretloglossT = T\u2211 t=1 `(pt, xt)\u2212 min \u03b2\u2208[0,1] T\u2211 t=1 `(\u03b2, xt) .\nLemma 7. Assume that an algorithm that predicts pt guarantees Regret logloss T \u2264 RT . Then, the coin betting strategy with endowement and \u03b2t = 2pt \u2212 1 guarantees\nWealthT \u2265 exp ( T \u00b7D ( 1\n2 + \u2211T t=1 gt 2T \u2225\u2225\u2225\u2225\u222512 ) \u2212RT )\nagainst any sequence of outcomes gt \u2208 [\u22121,+1].\nProof. Define xt = 1+gt\n2 . We have\nln WealthT = ln(Wealtht\u22121 +wtgt)\n= ln(Wealtht\u22121(1 + gt\u03b2t))\n= ln T\u220f t=1 (1 + gt\u03b2t)\n= ln + T\u2211 t=1 ln(1 + gt\u03b2t)\n\u2265 ln + T\u2211 t=1 ( 1 + gt 2 ) ln (1 + \u03b2t) + ( 1\u2212 gt 2 ) ln (1\u2212 \u03b2t)\n= ln + T\u2211 t=1 ( 1 + gt 2 ) ln (2pt) + ( 1\u2212 gt 2 ) ln (2(1\u2212 pt))\n= ln + T ln(2) + T\u2211 t=1 ( 1 + gt 2 ) ln(pt) + ( 1\u2212 gt 2 ) ln(1\u2212 pt)\n= ln + T ln(2)\u2212 T\u2211 t=1 `(pt, xt)\n= ln + T ln(2)\u2212 RegretloglossT \u2212 min \u03b2\u2208[0,1] T\u2211 t=1 `(\u03b2, xt)\n\u2265 ln + T ln(2)\u2212RT \u2212 min \u03b2\u2208[0,1] T\u2211 t=1 `(\u03b2, xt) ,\nwhere the first inequality is due to the concavity of ln and the second one is due to the assumption of the regret.\nIt is easy to see that the \u03b2\u2217 = arg min\u03b2\u2208[0,1] \u2211T t=1 `(\u03b2, xt) = \u2211T t=1 xt T . Hence, we have\nmin \u03b2\u2208[0,1] T\u2211 t=1 `(\u03b2, xt) = T (\u2212\u03b2\u2217 ln\u03b2\u2217 \u2212 (1\u2212 \u03b2\u2217) ln(1\u2212 \u03b2\u2217)) .\nAlso, we have that for any \u03b2 \u2208 [0, 1] \u2212\u03b2 ln\u03b2 \u2212 (1\u2212 \u03b2) ln(1\u2212 \u03b2) = \u2212D ( \u03b2 \u2225\u2225\u2225\u222512 ) + ln 2 .\nPutting all together, we have the stated lemma.\nThe lower bound on the wealth of the adaptive Kelly betting based on the KT estimator is obtained simply by the stated Lemma and reminding that the log loss regret of the KT estimator is upper bounded by 12 lnT + ln 2."}, {"heading": "B Optimal Betting Fraction", "text": "Theorem 8 (Optimal Betting Fraction). Let x \u2208 R. Let F : [x\u2212 1, x+ 1]\u2192 R be a logarithmically convex function. Then,\narg min \u03b2\u2208(\u22121,1) max g\u2208[\u22121,1]\nF (x+ g) 1 + \u03b2g = F (x+ 1)\u2212 F (x\u2212 1) F (x+ 1) + F (x\u2212 1) .\nMoreover, \u03b2\u2217 = F (x+1)\u2212F (x\u22121)F (x+1)+F (x\u22121) satisfies\nln(F (x+ 1))\u2212 ln(1 + \u03b2\u2217) = ln(F (x\u2212 1))\u2212 ln(1\u2212 \u03b2\u2217) .\nProof. We define the functions h, f : [\u22121, 1]\u00d7 (\u22121, 1)\u2192 R as\nh(g, \u03b2) = F (x+ g)\n1 + \u03b2g and f(g, \u03b2) = ln(h(g, \u03b2)) = ln(F (x+ g))\u2212 ln(1 + \u03b2g) .\nClearly, arg min\u03b2\u2208(\u22121,1) maxg\u2208[\u22121,1] h(g, \u03b2) = arg min\u03b2\u2208(\u22121,1) maxg\u2208[\u22121,1] f(g, \u03b2) and we can work with f instead of h. The function h is logarithmically convex in g and thus f is convex in g. Therefore,\n\u2200\u03b2 \u2208 (\u22121, 1) max g\u2208[\u22121,1] f(g, \u03b2) = max {f(+1, \u03b2), f(\u22121, \u03b2)} .\nLet \u03c6(\u03b2) = max {f(+1, \u03b2), f(\u22121, \u03b2)}. We seek to find the arg min\u03b2\u2208(\u22121,1) \u03c6(\u03b2). Since f(+1, \u03b2) is decreasing in \u03b2 and f(\u22121, \u03b2) is increasing in \u03b2, the minimum of \u03c6(\u03b2) is at a point \u03b2\u2217 such that f(+1, \u03b2\u2217) = f(\u22121, \u03b2\u2217). In other words, \u03b2\u2217 satisfies\nln(F (x+ 1))\u2212 ln(1 + \u03b2\u2217) = ln(F (x\u2212 1))\u2212 ln(1\u2212 \u03b2\u2217) .\nThe only solution of this equation is\n\u03b2\u2217 = F (x+ 1)\u2212 F (x\u2212 1) F (x+ 1) + F (x\u2212 1) .\nTheorem 9. The functions Ft(x) = exp( x2 2t \u2212 1 2 \u2211t i=1 1 i ) are excellent coin betting potentials.\nProof. The first and second properties of Definition 2 are trivially true. For the third property, we first use Theorem 8 to have\nln(1 + \u03b2tg)\u2212 lnFt(x+ g) \u2265 ln(1 + \u03b2t)\u2212 lnFt(x+ 1) = ln 2\nFt(x+ 1) + Ft(x\u2212 1) ,\nwhere the definition of \u03b2t is from (7). Hence, we have\nln(1 + \u03b2tg)\u2212 lnFt(x+ g) + lnFt\u22121(x) \u2265 ln 2\nFt(x+ 1) + Ft(x\u2212 1) + lnFt\u22121(x)\n= \u2212x 2 + 1\n2t +\n1\n2 t\u2211 i=1 1 i \u2212 ln cosh x t +\nx2\n2(t\u2212 1) \u2212 1 2 t\u22121\u2211 i=1 1 i\n= \u2212x 2 2t \u2212 ln cosh x t +\nx2\n2(t\u2212 1)\n\u2265 \u2212x 2 2t \u2212 x 2 2t2 +\nx2\n2(t\u2212 1)\n\u2265 \u2212x 2 2t \u2212 x 2 2t(t\u2212 1) +\nx2\n2(t\u2212 1) = 0,\nwhere in the second inequality we have used the elementary inequality ln coshx \u2264 x 2\n2 . The fourth property of Definition 2 is also true because Ft(x) is of the form h(x\n2) with h(\u00b7) convex McMahan and Orabona [2014]."}, {"heading": "C Proof of Lemma 11", "text": "First we state the following Lemma from McMahan and Orabona [2014] and reported here with our notation for completeness.\nLemma 10 (Extremes). Let h : (\u2212a, a)\u2192 R be an even twice-differentiable function that satisfies x\u00b7h\u2032\u2032(x) \u2265 h\u2032(x) for all x \u2208 [0, a). Let c : [0,\u221e)\u00d7 [0,\u221e)\u2192 R be an arbitrary function. Then, if vectors u, v \u2208 H satisfy \u2016u\u2016+ \u2016v\u2016 < a, then\nc(\u2016u\u2016 , \u2016v\u2016) \u00b7 \u3008u, v\u3009 \u2212 h(\u2016u+ v\u2016) \u2265 min {c(\u2016u\u2016 , \u2016v\u2016) \u00b7 \u2016u\u2016 \u00b7 \u2016v\u2016 \u2212 h(\u2016v\u2016+ \u2016v\u2016), \u2212c(\u2016u\u2016 , \u2016v\u2016) \u00b7 \u2016u\u2016 \u00b7 \u2016v\u2016 \u2212 h(\u2016u\u2016 \u2212 \u2016v\u2016)} . (14)\nProof. If u or v is zero, the inequality (14) clearly holds. From now on we assume that u, v are non-zero. Let \u03b1 be the cosine of the angle of between u and v. More formally,\n\u03b1 = \u3008u, v\u3009 \u2016u\u2016 \u00b7 \u2016v\u2016 .\nWith this notation, the left-hand side of (14) is f(\u03b1) = c(\u2016u\u2016 , \u2016v\u2016) \u00b7 \u03b1 \u2016u\u2016 \u00b7 \u2016v\u2016 \u2212 h( \u221a \u2016u\u20162 + \u2016v\u20162 + 2\u03b1 \u2016u\u2016 \u00b7 \u2016v\u2016) .\nSince h is even, the inequality (14) is equivalent to\n\u2200\u03b1 \u2208 [\u22121, 1] f(\u03b1) \u2265 min {f(+1), f(\u22121)} .\nThe last inequality is clearly true if f : [\u22121, 1] \u2192 R is concave. We now check that f is indeed concave, which we prove by showing that the second derivative is non-positive. The first derivative of f is\nf \u2032(\u03b1) = c(\u2016u\u2016 , \u2016v\u2016) \u00b7 \u2016u\u2016 \u00b7 \u2016v\u2016 \u2212 h\u2032( \u221a \u2016u\u20162 + \u2016v\u20162 + 2\u03b1\u2016u\u2016 \u00b7 \u2016v\u2016) \u00b7 \u2016u\u2016 \u00b7 \u2016v\u2016\u221a \u2016u\u20162 + \u2016v\u20162 + 2\u03b1\u2016u\u2016 \u00b7 \u2016v\u2016 .\nThe second derivative of f is\nf \u2032\u2032(\u03b1) = \u2212 \u2016u\u2016 2 \u00b7 \u2016v\u20162\n\u2016u\u20162 + \u2016v\u20162 + 2\u03b1\u2016u\u2016 \u00b7 \u2016v\u2016\n\u00b7 ( h\u2032\u2032( \u221a \u2016u\u20162 + \u2016v\u20162 + 2\u03b1\u2016u\u2016 \u00b7 \u2016v\u2016)\u2212 h\u2032( \u221a \u2016u\u20162 + \u2016v\u20162 + 2\u03b1\u2016u\u2016 \u00b7 \u2016v\u2016)\u221a \u2016u\u20162 + \u2016v\u20162 + 2\u03b1\u2016u\u2016 \u00b7 \u2016v\u2016 ) .\nIf we consider x = \u221a \u2016u\u20162 + \u2016v\u20162 + 2\u03b1\u2016u\u2016 \u00b7 \u2016v\u2016, the assumption x \u00b7 h\u2032\u2032(x) \u2265 h\u2032(x) implies that f \u2032\u2032(\u03b1) is non-positive. This finishes the proof of the inequality (14).\nWe also need the following technical Lemma whose proof relies mainly on property (d) of Definition 2.\nLemma 11. Let {Ft}\u221et=0 be a sequence of excellent coin betting potentials. Let g1, g2, . . . , gt be vectors in a Hilbert space H such that \u2016g1\u2016 , \u2016g2\u2016 , . . . , \u2016gt\u2016 \u2264 1. Let \u03b2t be defined by (8) and let x = \u2211t\u22121 i=1 gi. Then,(\n1 + \u03b2t \u3008gt, x\u3009 \u2016x\u2016\n) Ft\u22121(\u2016x\u2016) \u2265 Ft(\u2016x+ gt\u2016) .\nProof. Since Ft(x) is an excellent coin betting potential, it satisfies xF \u2032\u2032 t (x) \u2265 F \u2032t (x). Hence,(\n1 + \u03b2t \u3008gt, x\u3009 \u2016x\u2016\n) Ft\u22121(\u2016x\u2016)\u2212 Ft(\u2016x+ gt\u2016)\n= Ft\u22121(\u2016x\u2016) + \u03b2t \u3008gt, x\u3009 \u2016x\u2016 Ft\u22121(\u2016x\u2016)\u2212 Ft(\u2016x+ gt\u2016)\n\u2265 Ft\u22121(\u2016x\u2016) + min r\u2208{\u22121,1} \u03b2tr \u2016gt\u2016Ft\u22121(\u2016x\u2016)\u2212 Ft(\u2016x\u2016+ r \u2016gt\u2016)\n= min r\u2208{\u22121,1}\n(1 + \u03b2tr \u2016gt\u2016)Ft\u22121(\u2016x\u2016)\u2212 Ft(\u2016x\u2016+ r \u2016gt\u2016)\n\u2265 0 .\nIf x 6= 0, the first inequality comes from Lemma 10 with c(z, \u00b7) = Ft\u22121(z+1)\u2212Ft\u22121(z\u22121)Ft\u22121(z+1)+Ft\u22121(z\u22121)Ft\u22121(z)/z and h(z) = Ft(z), u = gt, v = x. If x = 0 then, according to (8), \u03b2t = 0 and the first inequality trivially holds. The second inequality follows from the property (c) of a coin betting potential.\nProof of Theorem 3. First, by induction on t we show that\nWealtht \u2265 Ft (\u2225\u2225\u2225\u2225\u2225 T\u2211 t=1 gt \u2225\u2225\u2225\u2225\u2225 ) . (15)\nThe base case t = 0 is trivial, since both sides of the inequality are equal to . For t \u2265 1, if we let x = \u2211t\u22121 i=1 gi, we have\nWealtht = \u3008gt, wt\u3009+ Wealtht\u22121 = (\n1 + \u03b2t \u3008gt, x\u3009 \u2016x\u2016\n) Wealtht\u22121\n\u2265 (\n1 + \u03b2t \u3008gt, x\u3009 \u2016x\u2016\n) Ft\u22121(\u2016x\u2016) (*) \u2265 Ft(\u2016x+ gt\u2016) = Ft (\u2225\u2225\u2225\u2225\u2225 t\u2211 i=1 gi \u2225\u2225\u2225\u2225\u2225 ) .\nThe inequality marked with (\u2217) follows from Lemma 11. This establishes (15), from which we immediately have a reward lower bound\nRewardT = T\u2211 t=1 \u3008gt, wt\u3009 = WealthT \u2212 \u2265 FT (\u2225\u2225\u2225\u2225\u2225 T\u2211 t=1 gt \u2225\u2225\u2225\u2225\u2225 ) \u2212 . (16)\nWe apply Lemma 1 to the function F (x) = FT (\u2016x\u2016)\u2212 and we are almost done. The only remaining property we need is that if F is an even function then the Fenchel conjugate of F (\u2016\u00b7\u2016) is F \u2217(\u2016\u00b7\u2016); see Bauschke and Combettes [2011, Example 13.7]."}, {"heading": "D Proof of Theorem 4", "text": "Proof. We first prove that \u2211N i=1 \u03c0ig\u0303t,iwt,i \u2264 0. Indeed,\nN\u2211 i=1 \u03c0ig\u0303t,iwt,i = \u2211\ni :\u03c0iwt,i>0\n\u03c0i[wt,i]+(gt,i \u2212 \u3008gt, pt\u3009) + \u2211\ni :\u03c0iwt,i\u22640\n\u03c0iwt,i[gt,i \u2212 \u3008gt, pt\u3009]+\n= \u2016p\u0302t\u20161 N\u2211 i=1 pt,i(gt,i \u2212 \u3008gt, pt\u3009) + \u2211\ni :\u03c0iwt,i\u22640\n\u03c0iwt,i[gt,i \u2212 \u3008gt, pt\u3009]+\n= 0 + \u2211\ni :\u03c0iwt,i\u22640\n\u03c0iwt,i[gt,i \u2212 \u3008gt, pt\u3009]+ \u2264 0 .\nThe first equality follows from definition of gt,i. To see the second equality, consider two cases: If \u03c0iwt,i \u2264 0 for all i then \u2016p\u0302t\u20161 = 0 and therefore both \u2016p\u0302t\u20161 \u2211N i=1 pt,i(gt,i\u2212\u3008gt, pt\u3009) and \u2211 i :\u03c0iwt,i>0\n\u03c0i[wt,i]+(gt,i\u2212\u3008gt, pt\u3009) are trivially zero. If \u2016p\u0302t\u20161 > 0 then \u03c0i[wt,i]+ = p\u0302t,i = \u2016p\u0302t\u20161 pt,i for all i.\nFrom the assumption on A, we have, for any sequence {g\u0303t}\u221et=1 such that g\u0303t \u2208 [\u22121, 1], satisfies\nWealtht = 1 + t\u2211 i=1 g\u0303iwi \u2265 Ft ( t\u2211 i=1 g\u0303i ) . (17)\nInequality \u2211N i=1 \u03c0ig\u0303t,iwt,i \u2264 0 and (17) imply\nN\u2211 i=1 \u03c0iFT ( T\u2211 t=1 g\u0303t,i ) \u2264 1 + N\u2211 i=1 \u03c0i T\u2211 t=1 g\u0303t,iwt,i \u2264 1 . (18)\nNow, let G\u0303T,i = \u2211T t=1 g\u0303t,i. For any competitor u \u2208 \u2206N ,\nRegretT (u) = T\u2211 t=1 \u3008gt, u\u2212 pt\u3009 = T\u2211 t=1 N\u2211 i=1 ui (gt,i \u2212 \u3008gt, pt\u3009)\n\u2264 T\u2211 t=1 N\u2211 i=1 uig\u0303t,i (by definition of g\u0303t,i)\n\u2264 N\u2211 i=1 ui \u2223\u2223\u2223G\u0303T,i\u2223\u2223\u2223 (since ui \u2265 0, i = 1, . . . , N) =\nN\u2211 i=1 uif \u22121 T ( ln[FT (G\u0303T,i)] ) (since FT (x) = exp(fT (x)) is even)\n\u2264 f\u22121T ( N\u2211 i=1 ui ln [ FT (G\u0303T,i) ]) (by concavity of f\u22121T )\n= f\u22121T ( N\u2211 i=1 ui { ln [ ui \u03c0i ] + ln [ \u03c0i ui FT (G\u0303T,i) ]}) = f\u22121T ( D (u\u2016\u03c0) + N\u2211 i=1 ui ln [ \u03c0i ui FT (G\u0303T,i) ])\n\u2264 f\u22121T\n( D (u\u2016\u03c0) + ln ( N\u2211 i=1 \u03c0iFT (G\u0303T,i) )) (by concavity of ln(\u00b7))\n\u2264 f\u22121T (D (u\u2016\u03c0)) (by (18))."}, {"heading": "E Properties of Krichevsky-Trofimov Potential", "text": "Lemma 12 (Analytic Properties of KT potential). Let a > 0. The function F : (\u2212a, a)\u2192 R+,\nF (x) = \u0393(a+ x)\u0393(a\u2212 x)\nis even, logarithmically convex, strictly increasing on [0, a), satisfies\nlim x\u2197a F (x) = lim x\u2198\u2212a\nF (x) = +\u221e\nand \u2200x \u2208 [0, a) x \u00b7 F \u2032\u2032(x) \u2265 F \u2032(x) . (19)\nProof. F (x) is obviously even. \u0393(z) = \u222b\u221e\n0 tz\u22121e\u2212tdt is defined for any real number z > 0. Hence, F is\ndefined on the interval (\u2212a, a). According to Bohr-Mollerup theorem [Artin, 1964, Theorem 2.1], \u0393(x) is logarithmically convex on (0,\u221e). Hence, F (x) is also logarithmically convex, since ln(F (x)) = ln(\u0393(a + x)) + ln(\u0393(a\u2212 x)) is a sum of convex functions.\nIt is well known that limz\u21980 \u0393(z) = +\u221e. Thus,\nlim x\u2197a F (x) = lim x\u2197a \u0393(a+ x)\u0393(a\u2212 x) = \u0393(2a) lim x\u2197a \u0393(a\u2212 x) = \u0393(2a) lim z\u21980 \u0393(z) = +\u221e ,\nsince \u0393 is continuous and not zero at 2a. Because F (x) is even, we also have limx\u2198\u2212a F (x) = +\u221e. To show that F (x) is increasing and that it satisfies (19), we write f(x) = ln(F (x)) as a Mclaurin series. The derivatives of ln(\u0393(z)) are the so called polygamma functions\n\u03c8(n)(z) = dn+1\ndzn+1 ln(\u0393(z)) for z > 0 and n = 0, 1, 2, . . . .\nPolygamma functions have the well-known integral representation \u03c8(n)(z) = (\u22121)n+1 \u222b \u221e\n0\ntne\u2212zt\n1\u2212 e\u2212t dt for z > 0 and n = 1, 2, . . . .\nUsing polygamma functions, we can write the Mclaurin series for f(x) = ln(F (x)) as\nf(x) = ln(F (x)) = ln(\u0393(a+ x)) + ln(\u0393(a\u2212 x)) = 2 ln(\u0393(a)) + 2 \u2211 n\u22652 n even \u03c8(n\u22121)(a)xn n! .\nThe series converges for x \u2208 (\u2212a, a), since for even n \u2265 2, \u03c8(n\u22121)(a) is positive and can be upper bounded as\n\u03c8(n\u22121)(a) = \u222b \u221e 0 tn\u22121e\u2212at 1\u2212 e\u2212t dt\n= \u222b 1 0 tn\u22121e\u2212at 1\u2212 e\u2212t dt+ \u222b \u221e 1 tn\u22121e\u2212zt 1\u2212 e\u2212t dt\n\u2264 \u222b 1\n0\ntn\u22121e\u2212at\nt(1\u2212 1/e) dt+ \u222b \u221e 1 tn\u22121e\u2212atdt\n\u2264 1 1\u2212 1/e \u222b \u221e 0 tn\u22122e\u2212atdt+ \u222b \u221e 0 tn\u22121e\u2212atdt = 1\n1\u2212 1/e a1\u2212n\u0393(n\u2212 1) + a\u2212n\u0393(n)\n\u2264 1 1\u2212 1/e a\u2212n(a+ 1)(n\u2212 1)! .\nFrom the Mclaurin expansion we see that f(x) is increasing on [0, a) since all the coefficients are positive (except for zero order term).\nFinally, to prove (19), note that for any x \u2208 (\u2212a, a),\nf(x) = c0 + \u221e\u2211 n=2 cnx n\nwhere c2, c3, . . . are non-negative coefficients. Thus\nf \u2032(x) = \u221e\u2211 n=2 ncnx n\u22121 and f \u2032\u2032(x) = \u221e\u2211 n=2 n(n\u2212 1)cnxn\u22122 .\nand hence x \u00b7 f \u2032\u2032(x) \u2265 f \u2032(x) for x \u2208 [0, a). Since F (x) = exp(f(x)),\nF \u2032(x) = f \u2032(x) \u00b7 F (x) and F \u2032\u2032(x) = [ f \u2032\u2032(x) + (f \u2032(x))2 ] \u00b7 F (x) .\nTherefore, for x \u2208 [0, a), x \u00b7 F \u2032\u2032(x) = x [ f \u2032\u2032(x) + (f \u2032(x))2 ] F (x) \u2265 [ f \u2032(x) + x(f \u2032(x))2 ] F (x) \u2265 f \u2032(x)F (x) = F \u2032(x) .\nThis proves (19).\nTheorem 13 (KT potential). Let \u03b4 \u2265 0 and > 0. The sequence of functions {Ft}\u221et=0, Ft : (\u2212t\u2212 \u03b4\u2212 1, t+ \u03b4 + 1)\u2192 R+ defined by\nFt(x) = 2t \u00b7 \u0393(\u03b4 + 1)\u0393( t+\u03b4+12 + x 2 )\u0393( t+\u03b4+1 2 \u2212 x 2 )\n\u0393( \u03b4+12 ) 2\u0393(t+ \u03b4 + 1)\n.\nis a sequence of excellent coin betting potentials for initial endowment . Furthermore, for any x \u2208 (\u2212t \u2212 \u03b4 \u2212 1, t+ \u03b4 + 1),\nFt(x+ 1)\u2212 Ft(x\u2212 1) Ft(x+ 1) + Ft(x\u2212 1) = x t+ \u03b4 . (20)\nProof. Property (b) and (d) of the definition follow from Lemma 12. Property (a) follows by simple substitution for t = 0 and x = 0.\nBefore verifying property (c), we prove (20). We use an algebraic property of the gamma function that states that \u0393(1 + z) = z\u0393(z) for any positive z. Equation (20) follows from\nFt(x+ 1)\u2212 Ft(x\u2212 1) Ft(x+ 1) + Ft(x\u2212 1) = \u0393( t+\u03b4+22 + x 2 )\u0393( t+\u03b4 2 \u2212 x 2 )\u2212 \u0393( t+\u03b4 2 + x 2 )\u0393( t+\u03b4+2 2 \u2212 x 2 )\n\u0393( t+\u03b4+22 + x 2 )\u0393( t+\u03b4 2 \u2212 x 2 ) + \u0393( t+\u03b4 2 + x 2 )\u0393( t+\u03b4+2 2 \u2212 x 2 )\n= ( t+\u03b42 + x 2 )\u0393( t+\u03b4 2 + x 2 )\u0393( t+\u03b4 2 \u2212 x 2 )\u2212 ( t+\u03b4 2 \u2212 x 2 )\u0393( t+\u03b4 2 + x 2 )\u0393( t+\u03b4 2 \u2212 x 2 )\n( t+\u03b42 + x 2 )\u0393( t+\u03b4 2 + x 2 )\u0393( t+\u03b4 2 \u2212 x 2 ) + ( t+\u03b4 2 \u2212 x 2 )\u0393( t+\u03b4 2 + x 2 )\u0393( t+\u03b4 2 \u2212 x 2 )\n= ( t+\u03b42 + x 2 )\u2212 ( t+\u03b4 2 \u2212 x 2 )\n( t+\u03b42 + x 2 ) + ( t+\u03b4 2 \u2212 x 2 )\n= x\nt+ \u03b4 .\nLet \u03c6(g) = Ft(x+g)Ft\u22121(x) . To verify property (c) of the definition, we need to show that \u03c6(g) \u2264 1 + g x t+\u03b4 for\nany x \u2208 [\u2212t+ 1, t\u2212 1] and any g \u2208 [\u22121, 1]. We can write \u03c6(g) as\n\u03c6(g) = Ft(x+ g)\nFt\u22121(x)\n= 2\u0393( t+\u03b4+12 + x+g 2 )\u0393( t+\u03b4+1 2 \u2212 x+g 2 )\u0393(t+ \u03b4)\n\u0393( t+\u03b42 + x 2 )\u0393( t+\u03b4 2 \u2212 x 2 )\u0393(t+ \u03b4 + 1)\n= 2\nt+ \u03b4 \u00b7\n\u0393( t+\u03b4+12 + x+g 2 )\u0393( t+\u03b4+1 2 \u2212 x+g 2 )\n\u0393( t+\u03b42 + x 2 )\u0393( t+\u03b4 2 \u2212 x 2 )\n.\nFor g = +1, using the formula \u0393(1 + z) = z\u0393(z), we have\n\u03c6(+1) = 2\nt+ \u03b4 \u00b7\n\u0393( t+\u03b42 + x 2 + 1)\u0393( t+\u03b4 2 \u2212 x 2 )\n\u0393( t+\u03b42 + x 2 )\u0393( t+\u03b4 2 \u2212 x 2 )\n= 2\nt+ \u03b4\n( t+ \u03b4\n2 + x 2\n) = 1 + x\nt+ \u03b4 .\nSimilarly, for g = \u22121, using the formula \u0393(1 + z) = z\u0393(z), we have\n\u03c6(\u22121) = 2 t+ \u03b4\n\u00b7 \u0393( t+\u03b42 + x 2 )\u0393( t+\u03b4 2 \u2212 x 2 + 1)\n\u0393( t+\u03b42 + x 2 )\u0393( t+\u03b4 2 \u2212 x 2 )\n= 2\nt+ \u03b4\n( t+ \u03b4\n2 \u2212 x 2\n) = 1\u2212 x\nt+ \u03b4 .\nWe can write any g \u2208 [\u22121, 1] as a convex combination of \u22121 and +1, i.e., g = \u03bb \u00b7 (\u22121) + (1 \u2212 \u03bb) \u00b7 (+1) for some \u03bb \u2208 [0, 1]. Since \u03c6(g) is (logarithmically) convex,\n\u03c6(g) = \u03c6(\u03bb \u00b7 (\u22121) + (1\u2212 \u03bb) \u00b7 (+1)) \u2264 \u03bb\u03c6(\u22121) + (1\u2212 \u03bb)\u03c6(+1)\n= \u03bb ( 1 + x\nt+ \u03b4\n) + (1\u2212 \u03bb) ( 1\u2212 x\nt+ \u03b4 ) = 1 + g x\nt+ \u03b4 ."}, {"heading": "F Proofs of Corollaries 5 and 6", "text": "We state some technical lemmas that will be used in the following proofs. We start with a lower bound on the Krichevsky-Trofimov (KT) potential. It is a generalization of the lower bound proved for integers in Willems et al. [1995] to real numbers.\nLemma 14 (Lower Bound on KT Potential). If c \u2265 1 and a, b are non-negative reals such that a + b = c then\nln\n( \u0393(a+ 1/2) \u00b7 \u0393(b+ 1/2)\n\u03c0 \u00b7 \u0393(c+ 1)\n) \u2265 \u2212 ln(e \u221a \u03c0)\u2212 1\n2 ln(c) + ln ((a c )a(b c )b) .\nProof. From Whittaker and Watson [1962][p. 263 Ex. 45], we have\n\u0393(a+ 1/2)\u0393(b+ 1/2) \u0393(a+ b+ 1) \u2265 \u221a 2\u03c0 (a+ 1/2)a(b+ 1/2)b (a+ b+ 1)a+b+1/2 .\nIt remains to show that\n\u221a 2\u03c0 (a+ 1/2)a(b+ 1/2)b\n(a+ b+ 1)a+b+1/2 >\n\u221a \u03c0\ne 1\u221a a+ b\n( a\na+ b\n)a( b\na+ b\n)b ,\nwhich is equivalent to (1 + 12a ) a(1 + 12b ) b\n(1 + 1a+b ) a+b+1/2\n> 1\ne \u221a 2 .\nFrom the inequality 1 \u2264 (1 + 1/x)x < e valid for any x \u2265 0, it follows that 1 \u2264 (1 + 12a ) a <\n\u221a e and\n1 \u2264 (1 + 12b ) b < \u221a e and 1 \u2264 (1 + 1/(a+ b))a+b < e. Hence,\n(1 + 12a ) a(1 + 12b ) b\n(1 + 1a+b ) a+b+1/2\n> 1 e \u221a\n1 + 1a+b\n\u2265 1 e \u221a 2 .\nLemma 15. Let \u03b4 \u2265 0. Then \u0393(\u03b4 + 1) 2\u03b4\u0393( \u03b4+12 ) 2 \u2265 \u221a \u03b4 + 1 \u03c0 .\nProof. We will prove the equivalent statement that\nln \u0393(\u03b4 + 1)\u03c0\n2\u03b4\u0393( \u03b4+12 ) 2 \u221a \u03b4 + 1 \u2265 0 .\nThe inequality holds with equality in \u03b4 = 0, so it is enough to prove that the derivative of the left-hand side is positive for \u03b4 > 0. The derivative of the left-hand side is equal to\n\u03a8(\u03b4 + 1)\u2212 1 2(\u03b4 + 1)\n\u2212 ln(2)\u2212\u03a8 ( \u03b4 + 1\n2\n) ,\nwhere \u03a8(x) is the digamma function. We will use the upper [Chen, 2005] and lower bound [Batir, 2008] to the digamma function, which state that for any x > 0,\n\u03a8(x) < ln(x)\u2212 1 2x \u2212 1 12x2 + 1 120x4\n\u03a8(x+ 1) > ln ( x+ 1\n2\n) .\nUsing these bounds we have\n\u03a8(\u03b4 + 1)\u2212 1 2(\u03b4 + 1)\n\u2212 ln(2)\u2212\u03a8 ( \u03b4 + 1\n2 ) \u2265 ln ( \u03b4 + 1\n2\n) \u2212 1\n2(\u03b4 + 1) \u2212 ln(2)\u2212 ln\n( \u03b4 + 1\n2\n) + 1\n\u03b4 + 1 +\n1 3(\u03b4 + 1)2 \u2212 2 15(\u03b4 + 1)4\n= ln ( 1\u2212 1\n2(\u03b4 + 1)\n) +\n1\n2(\u03b4 + 1) +\n1 3(\u03b4 + 1)2 \u2212 2 15(\u03b4 + 1)4\n\u2265 \u2212 (4 ln(2)\u2212 2) 4(\u03b4 + 1)2 + 1 3(\u03b4 + 1)2 \u2212 2 15(\u03b4 + 1)4\n= [15(1/2\u2212 ln(2))) + 5](\u03b4 + 1)2 \u2212 2\n15(\u03b4 + 1)4\n\u2265 [15(1/2\u2212 ln(2))) + 5]\u2212 2 15(\u03b4 + 1)4 \u2265 0\nwhere in the second inequality we used the elementary inequality ln(1\u2212 x) \u2265 \u2212x\u2212 (4 ln(2)\u2212 2)x2 valid for x \u2208 [0, .5].\nLemma 16 (Lower Bound on Shifted KT Potential). Let T \u2265 1, \u03b4 \u2265 0, and x \u2208 [\u2212T, T ]. Then\n2T \u00b7 \u0393(\u03b4 + 1)\u0393 ( T+\u03b4+1\n2 + x 2\n) \u00b7 \u0393 ( T+\u03b4+1\n2 \u2212 x 2 ) \u0393( \u03b4+12 ) 2\u0393(T + \u03b4 + 1) \u2265 exp ( x2 2(T + \u03b4) + 1 2 ln ( 1 + \u03b4 T + \u03b4 ) \u2212 ln(e \u221a \u03c0) ) .\nProof. Using Lemma 14, we have\nln 2T \u00b7 \u0393(\u03b4 + 1)\u0393\n( T+\u03b4+1\n2 + x 2\n) \u00b7 \u0393 ( T+\u03b4+1\n2 \u2212 x 2 ) \u0393( \u03b4+12 ) 2\u0393(T + \u03b4 + 1)\n\u2265 ln 2T+\u03b4\n\u221a \u03b4 + 1 \u00b7 \u0393\n( T+\u03b4+1\n2 + x 2\n) \u00b7 \u0393 ( T+\u03b4+1\n2 \u2212 x 2 ) \u03c0\u0393(T + \u03b4 + 1)\n\u2265 \u2212 ln(e \u221a \u03c0) + 1\n2 ln\n( 1 + \u03b4\nT + \u03b4\n) + ln (( 1 + x\nT + \u03b4\n)T+\u03b4+x 2 ( 1 + x\nT + \u03b4\n)T+\u03b4\u2212x 2 )\n= \u2212 ln(e \u221a \u03c0) + 1\n2 ln\n( 1 + \u03b4\nT + \u03b4\n) + (T + \u03b4) D ( 1\n2 +\nx\n2(T + \u03b4) \u2225\u2225\u2225\u222512 )\n\u2265 \u2212 ln(e \u221a \u03c0) + 1\n2 ln\n( 1 + \u03b4\nT + \u03b4\n) +\nx2\n2(T + \u03b4) ,\nwhere in the first inequality we used Lemma 15, in the second one Lemma 14, and in third one the known lower bound to the divergence D (\n1 2 + x 2 \u2225\u2225 1 2 ) \u2265 x 2\n2 . Exponentiating and overapproximating, we get the stated bound.\nF.1 Proof of Corollary 5\nThe Lambert function W (x) : [0,\u221e)\u2192 [0,\u221e) is defined by the equality\nx = W (x) exp (W (x)) for x \u2265 0. (21)\nThe following lemma provides bounds on W (x).\nLemma 17. The Lambert function satisfies 0.6321 log(x+ 1) \u2264W (x) \u2264 log(x+ 1) for x \u2265 0.\nProof. The inequalities are satisfied for x = 0, hence we in the following we assume x > 0. We first prove the lower bound. From (21) we have\nW (x) = log\n( x\nW (x)\n) . (22)\nFrom the first equality, using the elementary inequality ln(x) \u2264 aex 1 a for any a > 0, we get\nW (x) \u2264 1 a e\n( x\nW (x)\n)a \u2200a > 0,\nthat is\nW (x) \u2264 ( 1\na e\n) 1 1+a\nx a 1+a \u2200a > 0. (23)\nUsing (23) in (22), we have\nW (x) \u2265 log  x( 1 a e ) 1 1+a x a 1+a  = 1 1 + a log (a e x) \u2200a > 0 .\nConsider now the function g(x) = xx+1 \u2212 b log(1+b)(b+1) log(x + 1), x \u2265 b. This function has a maximum in x\u2217 = (1 + 1b ) log(1 + b)\u2212 1, the derivative is positive in [0, x\n\u2217] and negative in [x\u2217, b]. Hence the minimum is in x = 0 and in x = b, where it is equal to 0. Using the property just proved on g, setting a = 1x , we have\nW (x) \u2265 x x+ 1 \u2265 b log(1 + b)(b+ 1) log(x+ 1) \u2200x \u2264 b .\nFor x > b, setting a = x+1ex , we have\nW (x) \u2265 e x (e+ 1)x+ 1 log(x+ 1) \u2265 e b (e+ 1)b+ 1 log(x+ 1) (24)\nHence, we set b such that e b\n(e+ 1)b+ 1 =\nb\nlog(1 + b)(b+ 1)\nNumerically, b = 1.71825..., so W (x) \u2265 0.6321 log(x+ 1) .\nFor the upper bound, we use Theorem 2.3 in Hoorfar and Hassani [2008], that says that\nW (x) \u2264 log x+ C 1 + log(C) , \u2200x > \u22121 e , C > 1 e .\nSetting C = 1, we obtain the stated bound.\nLemma 18. Define f(x) = \u03b2 exp x 2\n2\u03b1 , for \u03b1, \u03b2 > 0, x \u2265 0. Then\nf\u2217(y) = y \u221a \u03b1W ( \u03b1y2\n\u03b22\n) \u2212 \u03b2 exp W ( \u03b1y2 \u03b22 ) 2  . Moreover\nf\u2217(y) \u2264 y \u221a \u03b1 log ( \u03b1y2\n\u03b22 + 1\n) \u2212 \u03b2.\nProof. From the definition of Fenchel dual, we have\nf\u2217(y) = max x x y \u2212 f(x) = max x\nx y \u2212 \u03b2 exp x 2\n2\u03b1 \u2264 x\u2217 y \u2212 \u03b2\nwhere x\u2217 = arg maxx x y \u2212 f(x). We now use the fact that x\u2217 satisfies y = f \u2032(x\u2217), to have\nx\u2217 = \u221a \u03b1W ( \u03b1y2\n\u03b22\n) ,\nwhere W (\u00b7) is the Lambert function. Using Lemma 17, we obtain the stated bound.\nProof of Corollary 5. Notice that the KT potential can be written as\nFt(x) = \u00b7 2t \u00b7 \u0393(1)\u0393\n( t+1\n2 + x 2\n) \u00b7 \u0393 ( t+1\n2 \u2212 x 2 ) \u0393( 12 ) 2\u0393(t+ 1) .\nUsing Lemma 16 with \u03b4 = 0 we can lower bound Ft(x) with Ht(x) = \u00b7 exp ( x2\n2t +\n1 2 ln\n( 1\nt\n) \u2212 ln(e \u221a \u03c0) ) .\nSince Ht(x) \u2264 Ft(x), we have F \u2217t (x) \u2264 H\u2217t (x). Using Lemma 18, we have\n\u2200u \u2208 H F \u2217T (\u2016u\u2016) \u2264 H\u2217T (\u2016u\u2016) \u2264 \u221a\u221a\u221a\u221aT log(24T 2 \u2016u\u20162 2 + 1 ) + ( 1\u2212 1\ne \u221a \u03c0T\n) .\nAn application of Theorem 3 completes the proof.\nF.2 Proof of Corollary 6\nProof. Let\nFt(x) = 2t \u00b7 \u0393(\u03b4 + 1)\u0393( t+\u03b4+12 + x 2 )\u0393( t+\u03b4+1 2 \u2212 x 2 )\n\u0393( \u03b4+12 ) 2\u0393(t+ \u03b4 + 1)\n,\nHt(x) = exp\n( x2\n2(t+ \u03b4) +\n1 2 ln\n( 1 + \u03b4\nt+ \u03b4\n) \u2212 ln(e \u221a \u03c0) ) .\nLet ft(x) = ln(Ft(x)) and ht(x) = ln(Ht(x)). By Lemma 16, Ht(x) \u2264 Ft(x) and therefore f\u22121t (x) \u2264 h\u22121t (x) for all x \u2265 0. Theorem 4 implies that\n\u2200u \u2208 \u2206t Regrett(u) \u2264 f\u22121t (D (u\u2016\u03c0)) \u2264 h\u22121t (D (u\u2016\u03c0)) .\nSetting t = T and \u03b4 = T/2, and overapproximating h\u22121t (D (u\u2016\u03c0)) we get the stated bound."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "In the recent years, a number of parameter-free algorithms have been developed for online linear optimization over Hilbert spaces and for learning with expert advice. These algorithms achieve optimal regret bounds that depend on the unknown competitors, without having to tune the learning rates with oracle choices. We present a new intuitive framework to design parameter-free algorithms for both online linear optimization over Hilbert spaces and for learning with expert advice, based on reductions to betting on outcomes of adversarial coins. We instantiate it using a betting algorithm based on the KrichevskyTrofimov estimator. The resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of regret guarantee and per-round complexity.", "creator": "LaTeX with hyperref package"}}}