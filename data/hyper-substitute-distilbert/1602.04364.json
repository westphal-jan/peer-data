{"id": "1602.04364", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2016", "title": "Look, Listen and Learn - A Multimodal LSTM for Speaker Identification", "abstract": "target identification refers to finding experience of localizing certain expression of a person - has the emotional identity accompanying the possible retrieval at a library. this task not currently requires improving perception over both visual senses statistical signals, the robustness resources handle severe quality degradations and larger content variations present nevertheless indispensable. to this paper, we describe a novel automated adaptive chain - term memory ( nfc ) framework thereby seamlessly abstracts spatial visual perspectives auditory modalities from the constraints of each sequence input. the key idea is to extend than conventional methodology ; not only sharing weights correlated time resources, but multiple sharing weights individual modalities. we show that hence the reduced dependency upon face and sounds can gradually stimulate perceived understanding whereas content quality degradations involving variations. we additionally require that our stimulus behaviour is susceptible to distractors, contrasting the false - meaningful variables. \u201d add our multimodal lstm to the big bang theory dataset can suggest that our system enhanced the state - of - the - scene systems or speaker identification with lower false alarm rate relative simultaneous recognition accuracy.", "histories": [["v1", "Sat, 13 Feb 2016 18:49:50 GMT  (8463kb)", "http://arxiv.org/abs/1602.04364v1", "The 30th AAAI Conference on Artificial Intelligence (AAAI-16)"]], "COMMENTS": "The 30th AAAI Conference on Artificial Intelligence (AAAI-16)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jimmy s j ren", "yongtao hu", "yu-wing tai", "chuan wang", "li xu 0001", "wenxiu sun", "qiong yan"], "accepted": true, "id": "1602.04364"}, "pdf": {"name": "1602.04364.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jimmy Ren", "Yongtao Hu", "Yu-Wing Tai", "Chuan Wang", "Li Xu", "Wenxiu Sun Qiong Yan"], "emails": ["yanqiong}@sensetime.com", "wangchuan2400}@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n04 36\n4v 1\n[ cs\n.L G\n] 1\n3 Fe\nb 20\nSpeaker identification is one of the most important building blocks in many intelligent video processing systems such as video conferencing, video summarization and video surveillance, etc. It aims to localize the face of the speaker associated with the ongoing voices. To achieve this task, collective perception over both visual and auditory signals is indispensable.\nIn the past few years, we observe the rapid advances in face recognition and speech recognition respectively by using Convolutional Neural Networks (CNN) (Schroff, Kalenichenko, and Philbin 2015; Sun et al. 2014a) and Recurrent Neural Networks (RNN) (Graves, Mohamed, and Hinton 2013; Hannun et al. 2014). Notwithstanding the recent ground breaking results in processing facial and auditory data, speaker identification (figure 1b) remains challenging for the following reasons. First, severe quality degradations (e.g. blur and occlusion) and unconstrained content variations (e.g. illumination and expression) in real-life videos\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nare not uncommon. These effects significantly degrade the performance of many existing CNN based methods. Second, the state-of-the-art convolutional network based face model is trained with still images. Its application in sequential data as well as its connection to recurrent networks is less explored and it is not straightforward to extend CNN based methods to the multimodal learning setting. Third, a practical system should be robust enough to reject distractors, the faces of non-speaking persons indicated by the red bounding boxes in figure 1b, which adds additional challenges to the task.\nDespite the rich potential of both CNN and RNN for facial and auditory data, it is still unclear if these two\nnetworks can be simultaneously and effectively adopted in the context of multimodal learning. In this paper, we proposed a novel multimodal Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber 1997), a specialized type of Recurrent Neural Network, to address this problem. We show that modeling temporal dependency for facial sequences by LSTM performs much better than CNN based methods in terms of robustness to image degradations. More importantly, by sharing weights not only across time steps but also across different modalities, we can seamlessly unify both visual and auditory modalities from the beginning of each sequence input, which significantly improves the robustness to distractors. This is because the cross-modal shared weights can learn to capture temporal correlation between face and voice sequences. Note that our multimodal LSTM did not assume the the random variables from different modal are correlated. Instead, our multimodal LSTM is capable to learn such correlation if there is any temporal correlation across different modal in their sequences. In the speaker identification task, we believe such temporal correlation exists in the respective face and voice sequences. In this work, we assume the face of the speaker appears in a video when they speak, and there is only one speaker at the same time during the voice over. Multiple speakers in the same video clip can be identified as far as their speaks do not overlap.\nTo our knowledge, our paper is the first attempt in modeling long-term dependencies over multimodal high-level features which demonstrates robustness to both distractors and image degradation. We applied our model to The Big Bang Theory dataset and showed that our system outperformed the state-of-the-art systems in recognition accuracy and with lower false alarm rate.\nThe contributions of this paper are as follows.\n\u2022 We proposed a novel LSTM architecture which enables multimodal learning of sequence data in a unified model. Both temporal dependency within each modality and temporal correlation across modalities can be automatically learned from data.\n\u2022 We empirically showed that cross-modality weight sharing in LSTM simultaneously improves the precision of classification and the robustness to distractors.\n\u2022 We successfully applied our method in a real-world multimodal classification task and the resulting system outperformed the state-of-the-art. The dataset and our implementations are both publicly available."}, {"heading": "Related Work", "text": "Many recent studies have reported the success of using deep CNN in face related tasks. The pioneering work by (Taigman et al. 2014) proposed a very deep CNN architecture together with an alignment technique to perform face verification which achieved near human-level performance. Inspired by GoogLeNet (Szegedy et al. 2015), Sun et al. 2014b used a very deep CNN network with multiple levels of supervision, which surpassed humanlevel face verification performance in the LFW dataset (Huang and Learned-Miller 2013). The recent advance in\nthis field (Schroff, Kalenichenko, and Philbin 2015) pushed the performance even further. In face detection, the stateof-the-art results were also achieved by CNN based models (Yang et al. 2015; Li et al. 2015). For other face related tasks such as face landmark detection and face attribute recognition (Zhang et al. 2015a; Zhang et al. 2015b), CNN based models were also widely adopted.\nThe revived interest on RNN is mainly attributed to its recent success in many practical applications such as language modeling (Kiros et al. 2015), speech recognition (Chorowski et al. 2015; Graves, Mohamed, and Hinton 2013), machine translation (Sutskever, Vinyals, and Le 2014; Jean et al. 2015), conversation modeling (Shang, Lu, and Li 2015) to name a few. Among many variants of RNNs, LSTM is arguably one of the most widely used model. LSTM is a type of RNN in which the memory cells are carefully designed to store useful information to model long term dependency in sequential data (Hochreiter and Schmidhuber 1997). Other than supervised learning, LSTM is also used in recent work in image generation (Theis and Bethge 2015; Gregor et al. 2015), demonstrating its capability of modeling statistical dependencies of imagery data.\nIn terms of the sequence learning problem across multiple modalities, LSTM based models were actively used in recent image caption generation studies (Donahue et al. 2015; Karpathy and Li 2015; Xu et al. 2015). One common characteristic of these techniques is that CNN was used to extract the feature sequences in an image and LSTM was used to generate the corresponding text sequences. Our paper is related to this group of studies in a way that more than one modalities are involved in the learning process. However, our goal is not to generate sequences in an alternative domain but to collectively learn useful knowledge from sequence data of multiple domains. Perhaps the most closely related previous studies to our work are from (Srivastava and Salakhutdinov 2012) and (Ngiam et al. 2011). Unlike these papers, we focused on high-level multimodal learning which explicitly models the temporal correlation of high-level features rather than raw inputs between different modalities. This not only provided a channel to effectively transfer the recent success of deep CNN to the multimodal learning context, the resulting efficient implementation can be directly adopted in video processing as well. We also investigated the robustness to distractors and input quality which is not considered in the previous studies. The closely related papers in multimedia speaker identification are (Bauml, Tapaswi, and Stiefelhagen 2013; Hu et al. 2015), and (Tapaswi, Ba\u0308uml, and Stiefelhagen 2012). However, they did not explicitly model face sequences and the interplay between face and voice sequences."}, {"heading": "LSTM - Single VS. Multi-Modal", "text": "Single Modal LSTM A regular LSTM network contains a number of memory cells within which the multiplicative gate units and the self-recurrent units are the two fundamental building blocks (Hochreiter and Schmidhuber 1997). For a brief revision, equations (1), (3) and (5) formally describe\nthe memory input, the forget gate and the recurrent units of a regular LSTM in the forward pass. The input gate it and the output gate ot in a regular LSTM resemble the forget gate in the forward pass. Figure 2 shows a pictorial illustration of a regular LSTM model.\ngt = \u03d5(Wxg \u2217Xt +Whg \u2217 ht\u22121 + bg), (1)\nit = \u03c3(Wxi \u2217Xt +Whi \u2217 ht\u22121 + bi), (2)\nft = \u03c3(Wxf \u2217Xt +Whf \u2217 ht\u22121 + bf), (3)\not = \u03c3(Wxo \u2217Xt +Who \u2217 ht\u22121 + bo), (4)\nCt = ft \u2299 Ct\u22121 + it \u2299 gt, (5)\nyt = softmax(Wy \u2217 ht). (6)\nIn (1) and (3), X is an input sequence where Xt is an element of the sequence at time t, ht\u22121 is the output of the memory cell at time t\u2212 1. Wxg,Wxf ,Whg,Whf are distinct weight matrices, and bg and bf are bias terms respectively. \u03d5 and \u03c3 are nonlinear functions where \u03d5 denotes a tanh function and \u03c3 denotes a sigmoid function. In (5), \u2299 denotes an element-wise multiplication, it is the input gate at the time step t, and Ct\u22121 is the memory unit at the time step t\u22121. The memory unit at time step t is therefore generated by the collective gating of the input gate and the forget gate. In equation (6), the memory cell output of the current time step is multiplied by Wy and then transformed by the softmax function to compute the model output yt at time t.\nGenerally speaking, the reason that LSTM is able to model long-term dependencies in sequential data is because Ct at each time step can selectively \u201cremember\u201d (store) or \u201cforget\u201d (erase) past information which is modelled by the multiplicative gating operation. More importantly, the strategy to open or to close the gates is data driven which is automatically learned from training data. This information is captured by the trainable weights W, including Whf ,Wxf and so on, rather than hand-crafted. Because W are shared across time steps, this endows LSTM the power to explicitly model temporal relationships over the entire sequence.\nSimple extensions of Single Modal LSTM In order to deal with data from different domains, perhaps the most straightforward method is to incorporate them into a single network by concatenating the data directly to produce\na bigger X . However, this approach is problematic because the multimodal property of the inputs are completely ignored and the model does not have any explicit mechanism to model the correlation across modalites. Though some correlations may be weakly captured by the trained weights, a critical weakness is that it is incapable to handle distractors. In particular, when the face of a person A is combined with the voice of a person B, the model is confused and would fail to generate a meaningful label. Although it may be possible to put all the distractors to a single class and let the model to distinguish the speaker and the distractors automatically, this method performs much worse than our solution in practice. The major difficulty is that distractors share too many features with regular examples when organize the inputs in this way.\nAnother solution is to treat data from different domains completely independent. Namely, we can use multiple LSTMs in parallel and then merge the output labels at the highest layer using a voting mechanism. The advantage of this approach is that the two separate memory units can be trained to store useful information explicitly for each domain. But the weakness is that the interaction across modalities only happens at the highest level during the labelling process. The cross-model correlation is therefore very difficult, if not entirely impossible, to be encoded into the weights through the learning process. Thus, the robustness to distractors relies heavily on the voting stage where some of the temporal correlations may have already been washed out in the independent forward pass.\nMultimodal LSTM Compared with the straightforward solutions, we want to develop a new multimodal LSTM which can explicitly model the long-term dependencies both within the same modality and across modalities in a single multimodal LSTM. Instead of merging input data at preprocessing stage, or merging labels at post-processing stage, our key idea is to selectively share weights across different modalities during the forward pass. This is similar to the weight sharing in time domain in regular LSTM, but we do not share memory units for each modality within the memory cell. The modifications are illustrated in figure 3 and formally expressed in the following equations.\ngst = \u03d5(W s xg \u2217X s t +Whg \u2217 h s t\u22121 + b s g), s = 1 to n, (7) ist = \u03c3(W s xi \u2217X s t +Whi \u2217 h s t\u22121 + b s i ), s = 1 to n, (8) f st = \u03c3(W s xf \u2217X s t +Whf \u2217 h s t\u22121 + b s f ), s = 1 to n, (9) ost = \u03c3(W s xo \u2217X s t +Who \u2217 h s t\u22121 + b s o), s = 1 to n, (10) Cst = f s t \u2299 C s t\u22121 + i s t \u2299 g s t , s = 1 to n, (11)\nhst = o s t \u2299 \u03d5(C s t ), s = 1 to n, (12) yst = softmax(Wy \u2217 h s t ), s = 1 to n. (13)\nKeeping the gating mechanism the same as the regular LSTM, the equations from (7) to (13) describe a new crossmodal weight sharing scheme in the memory cell. The superscript s indexes each modality in the input sequences. n is the total number of modalities in input data, where n = 2 in the speaker identification task. The model is general enough to deal with the tasks with n > 2. Xst is the input sequence\nat time t for modality s. Therefore, the weights with superscript s (e.g. Wsxg) are NOT shared across modalities but only across time steps, the other weights without the superscript (e.g. Whg) are shared across both modalities and time steps. Specifically, the weights associated with the inputs Xst are not shared across modalities. The reasons are twofold. First, we would like to learn a specialized mapping, separately for each modality, from its input space to a new space, where multimodal learning is ensured by the shared weights. Second, specialized weights are preferred to reconcile the dimension difference between different modalities, which avoids a complex implementation.\nAlong with the transform associated with Xst , the output of the memory cell from the previous time step hst\u22121 also need to go through a transform in producing gst as well as all other gates. The weights to perform this transform, Whg,Whi,Whf and Who are shared across the modalities. With these new weight definitions, the separately transformed data by the four Ws is essentially interconnected from gst all the way to the memory cell output h s t .\nThe key insight is that while it is preferable for each modality to have its own information flow because it enables a more focused learning objective with which it is easier to learn the long-term temporal dependency within the modality, we also make such objective correlated and essentially constrained by what happens in the rest of the modalities. More specifically, in forming the forget gate f st for s = 1, it not only relates to hs=1t\u22121 but also constrained by h s=2 t\u22121 , ... , hs=nt\u22121 because Whf is shared among them. Provided that the weights Whg,Whi,Whf and Who are also shared across time steps, they play the vital role of capturing the temporal correlation across different modalities.\nAnother important property of the proposed model is that the memory unit C is NOT shared among modalities. The rationale is that the weights have the job to capture intramodal as well as intermodal relationships, therefore placing them in a single memory unit provides much less flexibility on what can be stored or forgotten. Given all the gates are formed in a multimodal fashion, the insight of such design is that we should not hand-craft the decision on what intramodal/intermodal relationships should be stored or forgotten but to give the model enough flexibility to learn it\nfrom data. The bias terms are not shared across modalities neither to increase this flexibility.\nLikewise, the network output at each time step yst does not relate to its own modality. Whether we should use Wy or Wsy to transform h s t before sending the outputs to the softmax function is not a straightforward decision. We resort to our experiment to address this issue.\nHigh-Level Feature VS. Raw Inputs One of the most important reasons why CNN is attractive is because it is an end to end feature learning process. Previous studies (Razavian et al. 2014; Oquab et al. 2014) have discovered that a successful CNN for a classification task also produces high-level features which are general enough to be used in a variety of tasks. This finding inspired a few recent work on image captioning (Xu et al. 2015; Karpathy and Li 2015) where the high-level CNN features over an image sequence were extracted, and a RNN is learned on top of the extracted CNN features to perform more sophisticated tasks. Such approach is very effective to bridge the effort and success in CNN to the field of sequence modeling. We would like to extend this type of attempt to multimodal sequence learning.\nImplementation In order to maximize the flexibility of our investigation and efficiently work with different variants of network architecture and working environments (e.g. Linux and Windows), we did not implement the multimodal LSTM using any third-party deep learning packages. Instead, we used MATLAB and its GPU functions in the parallel computing toolbox to build our own LSTM from scratch. Our implementation is vectorized (Ren and Xu 2015) and very efficient in training both single-modal and multimodal LSTM described in this paper."}, {"heading": "Experiments", "text": "Three experiments were carefully designed to test the robustness and the applicability of our proposed model.\nDataset overview We chose the TV-series The Big Bang Theory (BBT) as our data source. It has been shown that the speaker identification task over BBT is a very challenging multimodal learning problem due to various kinds of image degradation and the high variations on faces in the videos (Bauml, Tapaswi, and Stiefelhagen 2013; Hu et al. 2015; Tapaswi, Ba\u0308uml, and Stiefelhagen 2012). During data collection, we ran face detection and extracted all the faces in six episodes in the first season and another six episodes in the second season of BBT. We manually annotated the faces for the five leading characters, i.e. Sheldon, Leonard, Howard, Raj and Penny. In total, we have more than 310,000 consecutively annotated face images for the five characters. For audio data, we utilized the pre-annotated subtitles and only extracted the audio segments corresponding to speeches. Data from the second season was used in training and data from the first season was used in testing for all the experiments reported below.\nFeature extraction To ensure the usability of the resulting system, we adopted 0.5 second as the time window of all the\nsequence data including both face and audio. For feature extraction for faces, we adopted a CNN architecture resembles the one in (Krizhevsky, Sutskever, and Hinton 2012) and trained a classifier using the data reported in the next section. The activations of the last fully connected layer was used as the high-level feature for face. We also run principle component analysis (PCA) on all the extracted face features to reduce the dimensionality to the level comparable to audio features. By keeping 85% of the principle components, we obtained a 53-dimension feature vector for each face in the video. The video is 24 frames per second, therefore there are 12 consecutive faces within each face sequence. For audio, we used the mel-frequency cepstral coefficients (MFCC) features (Sahidullah and Saha 2012). Following (Hu et al. 2015) we extracted the 25d MFCC features in a sliding window of 20 milliseconds with stride of 10 milliseconds, thus gives us 25\u00d749 MFCC features."}, {"heading": "LSTM for Face Sequences", "text": "Our first task is to investigate the extend to which modeling temporal dependency of high-level CNN features improves the robustness to quality degradation and face variation. The reasons that we would like to investigate this manner is twofold. First, though it was showed that RNN can be successfully used in speech recognition to improve the robustness to noise (Graves, Mohamed, and Hinton 2013), it was not clear from the literature whether similar principle applies for high-level image features. On the other hand, we would like to clearly measure the extend to which this approach works for faces because this is an important cornerstone for the rest of the experiments.\nData Only face data in the aforementioned data set was used in the experiment. We randomly sampled 40,000 face sequences from the training face images and another 40,000 face sequences in the test face images. Note that each sequence was extracted according to the temporal order in the data, however, we did not guarantee the sequence are strictly from one subtitle segment. This injected more variations in the sequence.\nProcedure and Results Three methods were compared in this experiment. The first method was to use a CNN to directly classify each frame in the sequence. This CNN is the same one as used in the feature extraction for our LSTM. In our setting, the CNN will output 12 labels (12 probability distributions) for each face sequence. Then the output probabilities were averaged to compute the final label for this sequence. The second method used the same CNN to extract features for each frame and reduced the dimensionality using PCA as described in the last section. Then we used a SVM with RBF kernel to classify each feature followed by the same averaging processing before outputting the label. We used the single-modal LSTM (see figure 2) with one hidden memory cell layer to train a sequential classifier. The dimensionality of the hidden layer activation is 512. In our setting, this LSTM contains 12 time steps with 12 identical supervision signals (labels). During the testing, we only look at the last output in the whole output sequence.\nThe results were reported in table 1. We can see that the two CNN alone approaches delivered very similar results, acknowledging the high representative powerful of the CNN features reported in the previous studies. The accuracy of the CNN+SVM approach slightly outperformed the CNN alone approach. This is reasonable because SVM with RBF kernel may classify the data better than the last layer of CNN. The performance of LSTM is significantly higher than the other two. By looking at the correctly classified face sequences which were failed in the other two methods, we can see that the LSTM is more robust to a number of image degradations and variations. This is illustrated in figure 1a."}, {"heading": "Comparison among Multimodal LSTMs", "text": "By the results from the first experiment, there is a reason to believe that performing multimodal learning of face and audio in temporal domain, if do it correctly, has the potential to perform better in speaker identification task. Therefore, the aim of the second experiment was to examine the extend to which the multimodal LSTM benefits the speaker identification performance. Multiple aforementioned multimodal LSTM solutions were tested and compared in this experiment. See the result session for details.\nData In the training process, the face data from the previous experiment was used. One problem is that each face sequence has only 12 time steps which is inconsistent with the 49 time steps in audio sequences. To circumvent this inconsistency, we simply duplicated faces evenly within the 49 time steps. The combinations of face sequences and audio sequences for each identity were randomly paired by the training program during the runtime to maximize the diversity of the training set. For test set of this task, the combinations were however pre-generated. We randomly generated 250,000 correctly paired combinations and 250,000 distractors (ill-paired combinations).\nProcedure and Results During the comparison, one baseline method and three alternative multimodal LSTM methods were used. In the baseline method, we separately trained a single-modal LSTM only for audio using the same audio data in this experiment. We carefully tuned many hyper parameters, making sure it performed as well as we can achieve. We used it to classify the audio sequence. For face sequence, we used the CNN+SVM approach from the last experiment. Therefore, we shall have 49 proposals for audio labels and another 49 labels for face label proposals. We then looked at the number of labels agreed within these two groups of labels. A thresholdm is set to distinguish the sample between distractors and normal samples. For instance, if m = 10 then the whole multimodal sequence will be classified to distractors if there are more than 10 label proposals\ntemporally disagreed with each other. Otherwise, the multimodal sequence will be classified by averaging the proposals. Note that this distractor rejection procedure was used in all the compared methods in this experiment. The threshold m is tuned to generate various dots in the ROC curve.\nThe first multimodal solution does not share any weights across the two modality resulting in two separate singlemodal LSTMs. We called it \u201cno cross-modal weight sharing\u201d. The second solution used the weight sharing scheme introduced previously, but did not share Wy across the modality. Formally, equation (13) should be re-written as\nyst = softmax(W s y \u2217 h s t ), s = 1 to n. (14)\nWe called this solution \u201chalf cross-modal weight sharing\u201d. The third solution completely followed the equation (7) to (13), named \u201cfull cross-modal weight sharing\u201d.\nAs shown in figure 4, the performance difference is clear. It was expected that the baseline method performed less competitive. However, having isolated Wy for each modality performed worse than the naive combination of two single-model LSTMs. On the other hand, with the full weight sharing, multimodal LSTM significantly outperforms all other methods.\nDiscussion The false alarm rate was largely increased by not sharing Wy across modalities. The role of Wy is to transform the memory cell outputs at each time step to the desirable labels. By sharing this transform across the modality, we can generate more consistent labels for normally paired samples and increased the robustness to distractors. Our experiments showed that this behavior can be automatically captured by the shared Wy .\nSpeaker Identification in The Big Bang Theory The last experiment is to apply our method in real-life videos and compare the performance with previous studies.\nData To compare with other speaker identification methods, we evaluated the winning multimodal LSTM from the previous experiment in The Big Bang Theory S01E03, as in (Bauml, Tapaswi, and Stiefelhagen 2013; Tapaswi, Ba\u0308uml, and Stiefelhagen 2012; Hu et al. 2015).\nProcedure and Results We applied our model to video with the time window of 0.5 second and stride of 0.25 second (e.g. 0s-0.5s, 0.25s-0.75s...). Unlike the controlled setting in the second experiment, the number of distractors in videos varies for each scene. In some cases, there are only distractors in a scene. The evaluation criteria should be more sophisticated. We followed (Hu et al. 2015) to calculate the accuracy of speaker identification to ensure a fair comparison. Specifically, speaker identification is considered successful if a) the speaker is in the scene and the system correctly recognized him/her and correctly rejected all the distractors, or b) the speaker is not in the scene and the system correctly rejected all the distractors in the scene.\nTime window more than 0.5 second was also tested to enable a more systematic comparison. We achieved this by further voting within this larger time window. For instance, by having 50% overlapping of 0.5 second windows in the larger window of 2.0 seconds, we will have seven 0.5 second-sized small windows to vote for the final labels.\nOur speaker identification results are reported in table 2. We compared our method against the state-of-the-art systems. Note that, in (Bauml, Tapaswi, and Stiefelhagen 2013; Tapaswi, Ba\u0308uml, and Stiefelhagen 2012), as both of them examined the face tracks within the time window specified by the subtitle/transcript segments, they can be viewed as voting on the range of subtitle/transcript segments. As the average time of subtitle/transcript segments in the evaluation video is 2.5s, they are equivalent to our method when evaluated in the voting window of such size. We applied the same voting strategy as in (Hu et al. 2015) under different time window setup. As can be seen from the results, our method outperformed the previous works by a significant margin."}, {"heading": "Conclusion", "text": "In this paper, we have introduced a new multimodal LSTM and have applied it to the speaker identification task. The key idea is to utilize the cross-modality weight sharing to capture correlation of two or more temporally coherent modalities. As demonstrated in our experiments, our proposed multimodal LSTM is robust against image degradation and distractors, and has outperformed state-of-the-art techniques in speaker identification. To our knowledge, this is the first attempt in modeling long-term dependencies over multimodal high-level features. We believe our multimodal LSTM is also useful to other applications not limited to the speaker identification task."}], "references": [{"title": "Semi-supervised learning with constraints for person identification in multimedia data", "author": ["Tapaswi Bauml", "M. Stiefelhagen 2013] Bauml", "M. Tapaswi", "R. Stiefelhagen"], "venue": null, "citeRegEx": "Bauml et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bauml et al\\.", "year": 2013}, {"title": "Attentionbased models for speech recognition", "author": ["Chorowski"], "venue": null, "citeRegEx": "Chorowski,? \\Q2015\\E", "shortCiteRegEx": "Chorowski", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Donahue"], "venue": null, "citeRegEx": "Donahue,? \\Q2015\\E", "shortCiteRegEx": "Donahue", "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks. In ICASSP", "author": ["Mohamed Graves", "A. Hinton 2013] Graves", "A. Mohamed", "G.E. Hinton"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor"], "venue": null, "citeRegEx": "Gregor,? \\Q2015\\E", "shortCiteRegEx": "Gregor", "year": 2015}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Hannun"], "venue": "Arxiv", "citeRegEx": "Hannun,? \\Q2014\\E", "shortCiteRegEx": "Hannun", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "S. Schmidhuber 1997] Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep Multimodal Speaker Naming", "author": ["Hu"], "venue": "In ACMMM", "citeRegEx": "Hu,? \\Q2015\\E", "shortCiteRegEx": "Hu", "year": 2015}, {"title": "Labeled faces in the wild: Updates and new reporting procedures", "author": ["Huang", "G.B. Learned-Miller 2013] Huang", "E. Learned-Miller"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean"], "venue": null, "citeRegEx": "Jean,? \\Q2015\\E", "shortCiteRegEx": "Jean", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image description", "author": ["Karpathy", "A. Li 2015] Karpathy", "Li", "F.-F"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Sutskever Krizhevsky", "A. Hinton 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A convolutional neural network cascade for face detection", "author": ["Li"], "venue": null, "citeRegEx": "Li,? \\Q2015\\E", "shortCiteRegEx": "Li", "year": 2015}, {"title": "Multimodal deep learning", "author": ["Ngiam"], "venue": null, "citeRegEx": "Ngiam,? \\Q2011\\E", "shortCiteRegEx": "Ngiam", "year": 2011}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["Oquab"], "venue": null, "citeRegEx": "Oquab,? \\Q2014\\E", "shortCiteRegEx": "Oquab", "year": 2014}, {"title": "Cnn features off-the-shelf: An astounding baseline for recognition", "author": ["Razavian"], "venue": "In CVPR Workshop", "citeRegEx": "Razavian,? \\Q2014\\E", "shortCiteRegEx": "Razavian", "year": 2014}, {"title": "On vectorization of deep convolutional neural networks for vision", "author": ["Ren", "J. Xu 2015] Ren", "L. Xu"], "venue": null, "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Design, analysis and experimental evaluation of block based transformation in mfcc computation for speaker recognition", "author": ["Sahidullah", "M. Saha 2012] Sahidullah", "G. Saha"], "venue": "Speech Communication", "citeRegEx": "Sahidullah et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sahidullah et al\\.", "year": 2012}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Kalenichenko Schroff", "F. Philbin 2015] Schroff", "D. Kalenichenko", "J. Philbin"], "venue": null, "citeRegEx": "Schroff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schroff et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lu Shang", "L. Li 2015] Shang", "Z. Lu", "H. Li"], "venue": null, "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Srivastava", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2012}, {"title": "Deep learning face representation by joint identification-verification", "author": ["Sun"], "venue": null, "citeRegEx": "Sun,? \\Q2014\\E", "shortCiteRegEx": "Sun", "year": 2014}, {"title": "Deepid3: Face recognition with very deep neural networks", "author": ["Sun"], "venue": null, "citeRegEx": "Sun,? \\Q2014\\E", "shortCiteRegEx": "Sun", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Vinyals Sutskever", "I. Le 2014] Sutskever", "O. Vinyals", "Q.V. Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy"], "venue": null, "citeRegEx": "Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Szegedy", "year": 2015}, {"title": "Deepface: Closing the gap to humanlevel performance in face verification", "author": ["Taigman"], "venue": null, "citeRegEx": "Taigman,? \\Q2014\\E", "shortCiteRegEx": "Taigman", "year": 2014}, {"title": "knock! knock! who is it? probabilistic person identification in tv-series", "author": ["B\u00e4uml Tapaswi", "M. Stiefelhagen 2012] Tapaswi", "M. B\u00e4uml", "R. Stiefelhagen"], "venue": null, "citeRegEx": "Tapaswi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tapaswi et al\\.", "year": 2012}, {"title": "Generative image modeling using spatial lstms", "author": ["Theis", "L. Bethge 2015] Theis", "M. Bethge"], "venue": null, "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu"], "venue": "In ICML", "citeRegEx": "Xu,? \\Q2015\\E", "shortCiteRegEx": "Xu", "year": 2015}, {"title": "From facial part responses to face detection: A deep learning approach", "author": ["Yang"], "venue": null, "citeRegEx": "Yang,? \\Q2015\\E", "shortCiteRegEx": "Yang", "year": 2015}, {"title": "Facial landmark detection by deep multitask learning", "author": ["Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2015\\E", "shortCiteRegEx": "Zhang", "year": 2015}, {"title": "Learning deep representation for face alignment with auxiliary attributes", "author": ["Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2015\\E", "shortCiteRegEx": "Zhang", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Speaker identification refers to the task of localizing the face of a person who has the same identity as the ongoing voice in a video. This task not only requires collective perception over both visual and auditory signals, the robustness to handle severe quality degradations and unconstrained content variations are also indispensable. In this paper, we describe a novel multimodal Long Short-Term Memory (LSTM) architecture which seamlessly unifies both visual and auditory modalities from the beginning of each sequence input. The key idea is to extend the conventional LSTM by not only sharing weights across time steps, but also sharing weights across modalities. We show that modeling the temporal dependency across face and voice can significantly improve the robustness to content quality degradations and variations. We also found that our multimodal LSTM is robustness to distractors, namely the non-speaking identities. We applied our multimodal LSTM to The Big Bang Theory dataset and showed that our system outperforms the state-of-the-art systems in speaker identification with lower false alarm rate and higher recogni-", "creator": "LaTeX with hyperref package"}}}