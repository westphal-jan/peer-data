{"id": "1602.04511", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2016", "title": "Learning Granger Causality for Hawkes Processes", "abstract": "learning granger causality for general behavioral models is making commonly challenging analysis. considering traditional form, observers propose an auxiliary method, learning granger elimination, for selecting special but significant instance of point processes - - - hawkes analysis. we take the observation by hawkes process's impact function and its granger smoothing graph. theoretically, our model represents impact factor using a logic utilizing regression expressions \u2026 analyzing the granger learning graph requiring dynamic evaluation of common impact functions'coefficients. we imagine an ensemble learning route effectively a maximum likelihood estimator ( mle ) effectively a pure - group - lasso ( soc ) regularizer. additionally, substantial flexibility of our hypothesis allows to incorporate in clustering multiple event types into computational framework. psychologists analyze our learning principles towards propose matching adaptive behavior to enhance intervention strategies. experiments on both autonomous and close - world sensors show that adaptive algorithm can learn the granger causality graph and the feedback patterns as the end processes simultaneously.", "histories": [["v1", "Sun, 14 Feb 2016 21:14:07 GMT  (715kb,D)", "https://arxiv.org/abs/1602.04511v1", null], ["v2", "Sat, 11 Jun 2016 23:47:23 GMT  (720kb,D)", "http://arxiv.org/abs/1602.04511v2", "International Conference on Machine Learning, 2016"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["hongteng xu", "mehrdad farajtabar", "hongyuan zha"], "accepted": true, "id": "1602.04511"}, "pdf": {"name": "1602.04511.pdf", "metadata": {"source": "META", "title": "Learning Granger Causality for Hawkes Processes", "authors": ["Hongteng Xu", "Mehrdad Farajtabar", "Hongyuan Zha"], "emails": ["HXU42@GATECH.EDU", "MEHRDAD@GATECH.EDU", "ZHA@CC.GATECH.EDU"], "sections": [{"heading": "1. Introduction", "text": "In many practical situations, we need to deal with a large amount of irregular and asynchronous sequential data observed in continuous time. The applications include the user viewing records in an IPTV system (when and which TV programs are viewed), and the patient records in hospitals (when and what diagnoses and treatments are given), among many others. All of these data can be viewed\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nas event sequences containing multiple event types and modeled via multi-dimensional point processes. A significant task for a multi-dimensional point process is to learn the so-called Granger causality. From the viewpoint of graphical models, it means to construct a directed graph called Granger causality graph (or local independence graph) (Didelez, 2008) over the dimensions (i.e., the event types) of the process. The arrow connecting two nodes indicates that the event of the dimension corresponding to the destination node is dependent on the historical events of the dimension corresponding to the source node. Learning Granger causality for multi-dimensional point processes is meaningful for many practical applications. Take our previous two examples: the Granger causality among IPTV programs reflects users\u2019 viewing preferences and patterns, which is important for personalized program recommendation and IPTV system simulation; the Granger causality among diseases helps us to construct a disease network, which is beneficial to predict potential diseases for patients and leads to more effective treatments.\nUnfortunately, learning Granger causality for general multi-dimensional point processes is very challenging. Existing works mainly focus on learning Granger causality for time series (Arnold et al., 2007; Eichler, 2012; Basu et al., 2015), where the Granger causality is captured via the socalled vector auto-regressive (VAR) model (Han & Liu, 2013) based on discrete time-lagged variables. For point processes, on the contrary, the event sequence is in continuous time and no fixed time-lagged observation is available. Therefore, it is hard to find a universal and tractable representation of the complicated historical events to describe Granger causality for the process. A potential solution is to construct features for various dimensions from historical events and learn Granger causality via feature selection (Lian et al., 2015). However, this method is highly dependent on the specific feature construction method used, resulting in dubious Granger causality. ar X\niv :1\n60 2.\n04 51\n1v 2\n[ cs\n.L G\n] 1\n1 Ju\nn 20\nTo make concrete progress, we focus on a special class of point processes called Hawkes processes and their Granger causality. Hawkes processes are widely used and are capable of describing the self-and mutually-triggering patterns among different event types. Applications include bioinformatics (Reynaud-Bouret et al., 2010), social network analysis (Zhao et al., 2015), financial analysis (Bacry et al., 2013), etc. Learning Granger causality will further extend applications of Hawkes processes in many other fields.\nTechnically, based on the graphical model of point process (Didelez, 2008), the Granger causality of Hawkes process can be captured by its impact functions. Inspired by this fact, we propose a nonparametric model of Hawkes processes, where the impact functions are represented by a series of basis functions, and we discover the Granger causality via group sparsity of impact functions\u2019 coefficients. Based on the explicit representation of Granger causality, we propose a novel learning algorithm combining the maximum likelihood estimator with the sparsegroup-lasso (SGL) regularizer on impact functions. The pairwise similarity between various impact functions is considered when the clustering structure of event types is available. Introducing these structural constraints enhances the robustness of our method. The learning algorithm applies the EM-based strategy (Lewis & Mohler, 2011; Zhou et al., 2013a) and obtains close-form solutions to update model\u2019s parameters iteratively. Furthermore, we discuss the selection of basis function based on sampling theory, and provide a useful guidance for model selection.\nOur method captures Granger causality from complicated event sequences in continuous time. Compared with existing learning methods for Hawkes processes (Zhou et al., 2013b; Eichler et al., 2015), our model avoids discretized representation of impact functions and conditional intensity, and considers the induced structures across impact functions. These improvements not only reduce the complexity of the learning algorithm but also improve learning performance. We investigate the robustness of our method to the changes of parameters and test our method on both synthetic and real-world data. Experimental results show that our method can indeed reveal the Granger causality of Hawkes processes and obtain superior learning performance compared with other competitors."}, {"heading": "2. Related Work", "text": "Granger causality. Many efforts have been made to learn the Granger causality of point processes (Meek, 2014). For general random processes, a kernel independence test is developed in (Chwialkowski & Gretton, 2014). Focusing on 1-D point process with simple piecewise constant conditional intensity, a model for capturing temporal dependencies between event types is proposed in (Gunawardana\net al., 2011). In (Basu et al., 2015; Song et al., 2013), the inherent grouping structure is considered when learning the Granger casuality on networks from discrete transition process. (Daneshmand et al., 2014) proposed a continuoustime diffusion network inference method based on parametric cascade generative process. In more general cases, a class of graphical models of marked point processes is proposed in (Didelez, 2008) to capture the local independence over various marks. Specializing the work for Hawkes processes, (Eichler et al., 2015) firstly connects Granger causality with impact functions. However, although applying lasso or its variants to capture the intra-structure of nodes (Ahmed & Xing, 2009) is a common strategy, less work has been done on learning causality graph of Hawkes process with sparse-group-lasso as we do, which leads them to be sensitive to noisy and insufficient data.\nHawkes processes. Hawkes processes (Hawkes, 1971) are proposed to model complicated event sequences where historical events have influences on future ones. It is applied to many problems, e.g., seismic analysis (Daley & VereJones, 2007), financial analysis (Bacry et al., 2013), social network modeling (Farajtabar et al., 2015; Zhou et al., 2013a;b) and bioinformatics (Reynaud-Bouret et al., 2010; Carstensen et al., 2010). Most of existing works use predefined impact function with known parameters, e.g., the exponential functions in (Farajtabar et al., 2014; Rasmussen, 2013; Zhou et al., 2013a; Hall & Willett, 2014; Yan et al., 2015) and the power-law functions in (Zhao et al., 2015). For enhancing the flexibility, a nonparametric model of 1- D Hawkes process is first proposed in (Lewis & Mohler, 2011) based on ordinary differential equation (ODE) and extended to multi-dimensional case in (Zhou et al., 2013b; Luo et al., 2015). Similarly, (Bacry et al., 2012) proposes a nonparametric estimation of Hawkes processes via solving the Wiener-Hopf equation. Another nonparametric strategy is the contrast function-based estimation in (ReynaudBouret et al., 2010; Hansen et al., 2015). It minimizes the estimation error of conditional intensity function and leads to a Least-Squares (LS) problem (Eichler et al., 2015). (Du et al., 2012; Lemonnier & Vayatis, 2014) decompose impact functions into basis functions to avoid discretization. The Gaussian process-based methods (Adams et al., 2009; Lloyd et al., 2015; Lian et al., 2015; Samo & Roberts, 2015) have been reported to successfully estimate more general point processes."}, {"heading": "3. Basic Concepts", "text": ""}, {"heading": "3.1. Temporal Point Processes", "text": "A temporal point process is a random process whose realization consists of a list of discrete events in time {ti} with ti \u2208 [0, T ]. Here [0, T ] is the time interval of the process. It can be equivalently represented as a counting\nprocess, N = {N(t)|t \u2208 [0, T ]}, where N(t) records the number of events before time t. A multi-dimensional point process with U types of event is represented by U counting processes {Nu}Uu=1 on a probability space (\u2126,F,P). Nu = {Nu(t)|t \u2208 [0, T ]}, where Nu(t) is the number of type-u events occurring at or before time t. \u2126 = [0, T ]\u00d7U is the sample space. U = {1, ..., U} is the set of event types. F = (F(t))t\u2208R is the filtration representing the set of events sequence the process can realize until time t. P is the probability measure. A way to characterize point processes is via the conditional intensity function capturing the patterns of interests, i.e., self-triggering or self-correcting (Xu et al., 2015). It is defined as the expected instantaneous rate of happening type-u events given the history:\n\u03bbu(t)dt = \u03bbu(t|HUt )dt = E[dNu(t)|F(t)].\nHere HUt = {(ti, ui)|ti < t, ui \u2208 U} collects historical events of all types before time t.\nHawkes Processes. A multi-dimensional Hawkes process is a counting process who has a particular form of intensity:\n\u03bbu(t) = \u00b5u + \u2211U\nu\u2032=1 \u222b t 0 \u03c6uu\u2032(s)dNu\u2032(t\u2212 s), (1)\nwhere \u00b5u is the exogenous base intensity independent of the history while \u2211U u\u2032=1 \u222b t 0 \u03c6uu\u2032(s)dNu\u2032(t \u2212 s) the endogenous intensity capturing the peer influence (Farajtabar et al., 2014). Function \u03c6uu\u2032(t) \u2265 0 is called impact function, which measures decay in the influence of historical type-u\u2032 events on the subsequent type-u events."}, {"heading": "3.2. Granger Causality for Point processes", "text": "We are interested in identifying, if possible, a subset of the event types V \u2282 U for the type-u event, such that \u03bbu(t) only depends on historical events of types in V , denoted as HVt , and not those of the rest types, denoted as H U\\V t . From the viewpoint of graphical model, it is about local independence over the dimensions of the point process \u2014 the occurrence of historical events in V influences the probability of occurrence of type-u events at present and future while the occurrence of historical events in U \\ V does not. In order to proceed formally we introduce some notations. For a subset V \u2282 U , let NV = {Nu(t)|u \u2208 V}. The filtration FVt is defined as \u03c3{Nu(s)|s \u2264 t, u \u2208 V}, i.e., the smallest \u03c3-algebra generated by the random processes. In particular, Fut is the internal filtration of the counting processNu(t) while F\u2212ut is the filtration for the subset U\\{u}. Definition 3.1. (Didelez, 2008). The counting process Nu is locally independent of Nu\u2032 given NU\\{u,u\u2032} if the intensity function \u03bbu(t) is measurable with respect to F\u2212u \u2032\nt for all t \u2208 [0, T ]. Otherwise Nu is locally dependent of Nu\u2032 .\nIntuitively, the above definition says that {Nu\u2032(s)|s < t} does not influence \u03bbu(t), given {Nl(s)|s < t, l 6= u\u2032}. In\n(Eichler et al., 2015), the notion of Granger non-causality is used, and the above definition is equivalent to saying that type-u\u2032 event does not Granger-cause type-u event w.r.t. FUt . Otherwise, we say type-u\n\u2032 event Granger-causes typeu event w.r.t. FUt . With this definition, we can construct the so-called Granger causality graphG = (U , E) with the event types U (the dimensions of the point process) as the nodes and the directed edges indicating the causation, i.e., u\u2032 \u2192 u \u2208 E if type-u\u2032 event Granger-causes type-u one.\nLearning Granger causality for a general multi-dimensional point process is a difficult problem. In the next section we introduce an efficient method for learning the Granger causality of the Hawkes process."}, {"heading": "4. Proposed Model and Learning Algorithm", "text": "In this section, we first generalize a known result for Hawkes process. Then, we propose a model of Hawkes process representing impact functions via a series of basis functions. An efficient learning algorithm combining the MLE with the sparse-group-lasso is applied and analyzed in details. Compared with existing learning algorithms, our algorithm is based on convex optimization and has lower complexity, which learns Granger causality robustly."}, {"heading": "4.1. Granger Causality of Hawkes Process", "text": "The work in (Eichler et al., 2015) reveals the relationship between Hawkes processes\u2019 impact function and its Granger causality graph as follows,\nTheorem 4.1. (Eichler et al., 2015). Assume a Hawkes process with conditional intensity function defined in (17) and Granger causality graph G(U , E). If the condition dNu\u2032(t \u2212 s) > 0 for 0 \u2264 s < t \u2264 T holds, then, u\u2032 \u2192 u /\u2208 E if and only if \u03c6uu\u2032(t) = 0 for t \u2208 [0,\u221e].\nIn practice, Theorem 4.1 can be easily specified in the time interval t \u2208 [0, T ]. It provides an explicit representation of the Granger causality of multi-dimensional Hawkes process \u2014 learning whether type-u\u2032 event Granger-causes type-u event or not is equivalent to detecting whether the impact function \u03c6uu\u2032(t) is all-zero or not. In other words, the group sparsity of impact functions along the time dimension indicates the Granger causality graph over the dimensions of Hawkes process. Therefore, for multidimensional Hawkes process, we can learn its Granger causality via learning its impact functions, which requires tractable and flexible representations of the functions."}, {"heading": "4.2. Learning Task", "text": "When we parameterize \u03c6uu\u2032(t) = auu\u2032\u03ba(t) as (Zhou et al., 2013a) does, where \u03ba(t) models time-decay of event\u2019s influence and auu\u2032 \u2265 0 captures the influence of u\u2032-type\nevents on u-type ones, the binarized infectivity matrix A = [sign(auu\u2032)] is the adjacency matrix of the corresponding Granger causality graph. Although such a parametric model simplifies the representation of impact function and reduces the complexity of the model, this achievement comes with the cost of inflexibility of the model \u2014 the model estimation will be poor if the data does not conform to the assumptions of the model. To address this problem, we propose a nonparametric model of Hawkes processes, representing the impact function in (17) via a linear combination of basis functions as\n\u03c6uu\u2032(t) = \u2211M\nm=1 amuu\u2032\u03bam(t). (2)\nHere \u03bam(t) is the m-th basis function and amuu\u2032 is the coefficient corresponding to \u03bam(t). The selection of bases will be discussed later in the paper.\nSuppose we have a set of event sequences S = {sc}Cc=1. sc = {(tci , uci )} Nc i=1, where t c i is the time stamp of the i-th event of sc and uci \u2208 {1, ..., U} is the type of the event. Thus, the log-likelihood of model parameters \u0398 = {A = [amuu\u2032 ] \u2208 RU\u00d7U\u00d7M ,\u00b5 = [\u00b5u] \u2208 RU} can be expressed as: L\u0398 = C\u2211 c=1 { Nc\u2211 i=1 log \u03bbuci (t c i )\u2212 U\u2211 u=1 \u222b Tc 0 \u03bbu(s)ds }\n= C\u2211 c=1 { Nc\u2211 i=1 log ( \u00b5uci + i\u22121\u2211 j=1 M\u2211 m=1 amuciucj\u03bam(\u03c4 c ij) )\n\u2212 U\u2211 u=1 ( Tc\u00b5u + Nc\u2211 i=1 M\u2211 m=1 amuuciKm(Tc \u2212 t c i ) )} ,\n(3)\nwhere \u03c4 cij = t c i \u2212 tcj , Km(t) = \u222b t 0 \u03bam(s)ds. For constructing Granger causality accurately and robustly, we consider the following three types of regularizers:\nLocal Independence. According to Theorem 4.1, the u\u2032type event has no influence on the u-type one (i.e., directed edge u\u2032 \u2192 u /\u2208 E) if and only if \u03c6uu\u2032(t) = 0 for all t \u2208 R, which requires amuu\u2032 = 0 for all m. Therefore, we use group-lasso (Yang et al., 2010; Simon et al., 2013; Song et al., 2013) to regularize the coefficients of impact functions, denoted as \u2016A\u20161,2 = \u2211 u,u\u2032 \u2016auu\u2032\u20162, where auu\u2032 = [a1uu\u2032 , ..., a M uu\u2032 ] >. It means that along the time dimension the coefficients\u2019 tensor A should yield to the constraint of group sparsity.\nTemporal Sparsity. A necessary condition for the stationarity of Hawkes process is \u222b\u221e 0 \u03c6ij(s)ds < \u221e, which means limt\u2192\u221e \u03c6ij(t) \u2192 0. Therefore, we add sparsity constraints to the coefficients of impact functions, denoted as \u2016A\u20161 = \u2211 u,u\u2032,m |amuu\u2032 |.\nPairwise Similarity. Event types of Hawkes process may exhibit clustering structure. For example, if u and u\u2032 are\nsimilar event types, their influences on other event types should be similar (i.e., \u03c6\u00b7u(t) are close to \u03c6\u00b7u\u2032(t)) and the influences of other event types on them should be similar as well (i.e., \u03c6u\u00b7(t) are close to \u03c6u\u2032\u00b7(t)). When the clustering structure is (partially) available, we add constraints of pairwise similarity on the coefficients of corresponding impact functions as follows E(A) = \u2211U\nu=1 \u2211 u\u2032\u2208Cu \u2016au\u00b7 \u2212 au\u2032\u00b7\u20162F + \u2016a\u00b7u\u2032 \u2212 a\u00b7u\u20162F .\nCu contains the event types within the cluster that the event of u type resides. au\u00b7 \u2208 RU\u00d7M is the slice of A with row index u, and a\u00b7u \u2208 RU\u00d7M is the slice with column index u. In summary, the learning problem of the Hawkes process is\nmin \u0398\u22650 \u2212 L\u0398 + \u03b1S\u2016A\u20161 + \u03b1G\u2016A\u20161,2 + \u03b1PE(A). (4)\nHere \u03b1S , \u03b1G and \u03b1P control the influences of the regularizers. The nonnegative constraint guarantees the model being physically-meaningful."}, {"heading": "4.3. An EM-based Algorithm", "text": "Following (Lewis & Mohler, 2011; Zhou et al., 2013b), we propose an EM-based learning algorithm for solving optimization problem (4) iteratively. Specifically, given current parameters \u0398(k), we first apply the Jensen\u2019s inequality and construct a tight upper-bound of log-likelihood function appeared in (3) as follows:\nQ (k) \u0398 = C\u2211 c=1 { \u2212 U\u2211 u=1 ( Tc\u00b5u + Nc\u2211 i=1 M\u2211 m=1 amuuciKm(Tc \u2212 t c i ) )\n+ Nc\u2211 i=1 ( pii log \u00b5uci pij + i\u22121\u2211 j=1 M\u2211 m=1 pmij log amuciucj\u03bam(\u03c4 c ij) pmij )} ,\npii = \u00b5 (k) uci /\u03bb (k) uci (tci ) and p m ij = a m,(k) uciu c j \u03bam(\u03c4 c ij)/\u03bb (k) uci (tci ). \u03bb (k) u (t) is the conditional intensity function computed with current parameters. When there is pairwise similarity constraint, we rewrite E(A) given current parameters as\nE (k) \u0398 = \u2211U u=1 \u2211 u\u2032\u2208Cu \u2016au\u00b7 \u2212 a(k)u\u2032\u00b7\u2016 2 F + \u2016a\u00b7u\u2032 \u2212 a (k) \u00b7u \u20162F .\nReplacing L\u0398 and E(A) with Q(k)\u0398 and E (k) \u0398 respectively, we decouple parameters and obtain the surrogate objective function F = \u2212Q(k)\u0398 + \u03b1S\u2016A\u20161 + \u03b1G\u2016A\u20161,2 + \u03b1PE (k) \u0398 . Then, we update each individual parameter via solving \u2202F \u2202\u0398 = 0, and obtain the following closed form updates:\n\u00b5(k+1)u = ( \u2211C\nc=1 \u2211 uci=u pii)/( \u2211C c=1 Tc), (5)\na m,(k+1) uu\u2032 = (\u2212B +\n\u221a B2 \u2212 4AC)/(2A), (6)\nA = \u03b1G\n\u2016a(k)uu\u2032\u20162 + 2(|Cu|+ |Cu\u2032 |)\u03b1\u2032P , \u03b1\u2032P =\n{ \u03b1P , u\n\u2032 \u2208 Cu 0, others\nB = \u2211C\nc=1 \u2211 uci=u \u2032 Km(Tc \u2212 tci ) + \u03b1S\n\u2212 2\u03b1\u2032P ( \u2211\nv\u2208Cu a m,(k) vu\u2032 + \u2211 v\u2032\u2208Cu\u2032 a m,(k) uv\u2032 ),\nC = \u2212 \u2211C\nc=1 \u2211 uci=u \u2211 ucj=u \u2032 pmij .\nFurthermore, for solving sparse-group-lasso (SGL), we apply the soft-thresholding method in (Simon et al., 2013) to shrink the updated parameters. Specifically, we set a(k+1)uu\u2032 to all-zero if the following condition is holds:\n\u2016S\u03b7\u03b1S (a (k+1) uu\u2032 \u2212 \u03b7\u2207auu\u2032Q|a(k) uu\u2032 )\u20162 \u2264 \u03b7\u03b1G, (7)\nwhere S\u03b1(z) = sign(z)(|z| \u2212 \u03b1)+ achieves softthresholding for each element of input. \u2207xf |x0 is the subgradient of function f at x0 w.r.t. variable x. We have Q = \u2212Q(k)\u0398 + \u03b1PE(A), and \u03b7 is a small constant. For the a\n(k+1) uu\u2032 unsatisfying (7), we shrink it as\na (k+1) uu\u2032 = 1\u2212 \u03b7\u03b1G \u2016S\u03b7\u03b1S (a (k+1) uu\u2032 \u2212 \u03b7\u2207auu\u2032Q|a(k)\nuu\u2032 )\u20162  +\n\u00d7 S\u03b7\u03b1S (a (k+1) uu\u2032 \u2212 \u03b7\u2207auu\u2032Q|a(k)\nuu\u2032 )\n(8)\nIn summary, Algorithm 1 gives the scheme of our MLEbased algorithm with sparse-group-lasso and pairwise similarity constraints, which is called MLE-SGLP for short. The detailed derivation is given in the appendix.\nAlgorithm 1 Learning Hawkes Processes (MLE-SGLP) 1: Input: Event sequences S = {sc}Cc=1, parameters \u03b1S , \u03b1G, (optional) clustering structure and \u03b1P .\n2: Output: Parameters of model, \u00b5 andA. 3: Initialize \u00b5 = [\u00b5u] andA = [amuu\u2032 ] randomly. 4: repeat 5: repeat 6: Update \u00b5 andA via (5) and (6), respectively. 7: until convergence 8: for u, u\u2032 = 1 : U 9: if (7) holds, auu\u2032 = 0; else, update auu\u2032 via (16).\n10: until convergence"}, {"heading": "4.4. Adaptive Selection of Basis Functions", "text": "Although the nonparametric models in (Lemonnier & Vayatis, 2014; Zhou et al., 2013b) represent impact functions as we do via a set of basis functions, they do not provide guidance for the selection process of basis functions. A\ncontribution of our work is proposing a method of selecting basis functions founded on sampling theory (Alan et al., 1989). Specifically, we focus on the impact functions satisfying following assumptions.\nAssumption 4.1. (i) \u03c6(t) \u2265 0, and \u222b\u221e\n0 \u03c6(t)dt < \u221e.\n(ii) For arbitrary > 0, there always exists a \u03c90, such that\u222b\u221e \u03c90 |\u03c6\u0302(\u03c9)|d\u03c9 \u2264 . \u03c6\u0302(\u03c9) is the Fourier transform of \u03c6(t).\nThe assumption (i) guarantees the existence of \u03c6\u0302(\u03c9), while the assumption (ii) means that we can find a function with a bandlimit, denoted as \u03c902\u03c0 , to approximate the target impact function with bounded residual. Based on these two assumptions, the representation of impact function in (2) can be explained as a sampling process. The {amuu\u2032}Mm=1 can be viewed as the discretized samples of \u03c6uu\u2032(t) in [0, T ] and \u03bam(t) = \u03ba\u03c9(t, tm) is sampling function (i.e., sinc or Gaussian function1) corresponding to a low-pass filter with cutoff frequency \u03c9. tm is the sampling location corresponding to amuu\u2032 and the sampling rate is \u03c9 \u03c0 . The Nyquist-Shannon theorem requires us to have \u03c9 = \u03c90, at least, such that the sampling rate is high enough (i.e., \u03c90\u03c0 , twice bandlimit) to approximate the impact function. Accordingly, the number of samples is M = dT\u03c90\u03c0 e, where dxe returns the smallest integer larger than or equal to x.\nBased on the above argument, the core of selecting basis functions is estimating \u03c90 for impact functions. It is hard because we cannot observe impact functions directly. Fortunately, based on (17) we know that the bandlimits of impact functions cannot be larger than that of conditional intensity functions \u03bb(t) = \u2211U u=1 \u03bbu(t). When sufficient training sequences S = {sc}Cc=1 are available, we can estimate \u03bb(t) via a Gaussian-based kernel density estimator:\n\u03bb(t) = \u2211C\nc=1 \u2211Nc i=1 Gh(t\u2212 tci ). (9)\nHere Gh(\u00b7) is a Gaussian kernel with the bandlimit h. Applying Silverman\u2019s rule of thumb (Silverman, 1986), we set optimal h = ( 4\u03c3\u0302 5 3 \u2211 cNc )0.2, where \u03c3\u0302 is the standard deviation of time stamps {tci}. Therefore, given the upper bound of residual , we can estimate \u03c90 from the Fourier transformation of \u03bb(t), which actually does not require us to compute \u03bb(t) via (19) directly. In summary, we propose Algorithm 2 to select basis functions and more detailed analysis is given in the appendix."}, {"heading": "4.5. Properties of The Proposed Method", "text": "Compared with existing state-of-art methods, e.g., the ODE-based algorithm in (Zhou et al., 2013b) and the LeastSquares (LS) algorithm in (Eichler et al., 2015), our algorithm has following advantages.\n1For Gaussian filter \u03ba\u03c9(t, tm) = exp(\u2212(t\u2212 tm)2/(2\u03c32)), its bandlimit is defined as \u03c9 = \u03c3\u22121.\nAlgorithm 2 Selecting basis functions 1: Input: S = {sc}Cc=1, residual\u2019s upper bound . 2: Output: Basis functions {\u03ba\u03c90(t, tm)}Mm=1. 3: Compute (\u2211C c=1Nc \u221a 2\u03c0h2 ) e\u2212 \u03c92h2 2 to bound |\u03bb\u0302(\u03c9)|.\n4: Find the smallest \u03c90 satisfying \u222b\u221e \u03c90 |\u03bb\u0302(\u03c9)|d\u03c9 \u2264 . 5: The proposed basis functions {\u03ba\u03c90(t, tm)}Mm=1 are selected, where \u03c90 is the cut-off frequency of basis function and tm = (m\u22121)T M , M = d T\u03c90 \u03c0 e.\nComputational complexity: Given a training sequence with N events, the ODE-based algorithm in (Zhou et al., 2013b) represents impact functions by M basis functions, where each basis function is discretized to L points. It learns basis functions and coefficients via alternating optimization \u2014 coefficients are updated via the MLE given basis functions, and then, the basis functions are updated via solving M Euler-Lagrange equations. The complexity of the ODE-based algorithm per iteration is O(MN3U2 + ML(NU + N2)). The LS algorithm in (Eichler et al., 2015) directly discretizes the timeline into L small intervals. In such a situation, impact functions are discretized to L points. The computational complexity of the algorithm is O(NU3L3). In contrast, our algorithm is based on known basis functions and does not estimate impact function via discretized points. The computational complexity of our algorithm per iteration is O(MN3U2). For getting accurate estimation, the ODE-based algorithm sampling basis functions densely. The LS algorithm needs to ensure that there is at most one event in each interval. In other words, both two competitors require L N . On the other hand, our algorithm converges quickly via few iterations. Therefore, the computational complexity of the LS algorithm is the highest among the the three, and our complexity is at least comparable to that of the ODE-based algorithm.\nConvexity: Both LS algorithm and ours are convex and can achieve global optima. The ODE-based algorithm, however, learns basis functions and coefficients alternatively. It is not convex and is prune to a local optima.\nInference of Granger causality: Neither the ODE-based algorithm nor the LS algorithm considers to infer the Granger causality graph of process when learning model. Without suitable regularizers on impact functions, the impact functions learned by these two algorithms are nonzero generally, which cannot indicate the Granger causality graph exactly. What is worse, the LS algorithm even may obtain physically-meaningless impact functions with negative values. To the best of our knowledge, our algorithm is the first attempt to solving this problem via combining MLE of the Hawkes process with sparse-group-lasso, which learns the Granger causality graph robustly, especially in the case having few training sequences."}, {"heading": "5. Experiments", "text": "For demonstrating the feasibility and the efficiency of our algorithm (MLE-SGLP), we compare it with the state-ofart methods, including the ODE-based method in (Zhou et al., 2013b), the Least-Squares (LS) method in (Eichler et al., 2015), on both synthetic and real-world data. We also investigate the influences of regularizers via comparing our algorithm with its variants, including the pure MLE without any regularizer (MLE), the MLE with group-lasso (MLE-GL), and the MLE with sparse regularizer (MLES). For evaluating algorithms comprehensively, given estimate \u0398\u0303 = {\u00b5\u0303, A\u0303}, we apply the following measurements: 1) The log-likelihood of testing data, Loglike; 2) the relative error of \u00b5, e\u00b5 =\n\u2016\u00b5\u0303\u2212\u00b5\u20162 \u2016\u00b5\u20162 ; 3) the relative error of \u03a6(t) = [\u03c6uu\u2032(t)], e\u03c6 = 1U2 \u2211 u,u\u2032 \u222b T 0 |\u03c6\u0303uu\u2032 (t)\u2212\u03c6uu\u2032 (t)|dt\u222b T\n0 \u03c6uu\u2032 (t)dt\n; 4)\nSparsity of impact function \u2014 the Granger causality graph is indicated via all-zero impact functions."}, {"heading": "5.1. Synthetic Data", "text": "We generate two synthetic data sets using sine-like impact functions and piecewise constant impact function respectively. Each of them contains 500 event sequences with time length T = 50 generated via a Hawkes process with U = 5. The exogenous base intensity of each event type is uniformly sampled from [0, 1U ]. The sine-like impact functions are generated as\n\u03c6uv(t) = { buv(1\u2212 cos(\u03c9uvt\u2212 \u03c0suv)), t \u2208 [0, 2\u2212suv4\u03c0\u03c9uv ], 0, otherwise,\nwhere {buv, \u03c9uv, suv} are set as {0.05, 0.6\u03c0, 1} when u, v \u2208 {1, 2, 3}, {0.05, 0.4\u03c0, 0} when u, v \u2208 {4, 5}, {0.02, 0.2\u03c0, 0} when u (or v) = 4, v (or u) \u2208 {1, 2, 3}. The piecewise constant impact functions are the truncated results of above sine-like ones.\nWe test various learning algorithms on each of the two data sets with 10 trials, respectively. In each trial, C = {50, ..., 250} sequences are chosen randomly as training set while the rest 250 sequences are chosen as testing set. In all trials, Gaussian basis functions are used, whose number and bandlimit are decided by Algorithm 2. We test our algorithm with various parameters in a wide range, where \u03b1P , \u03b1S , \u03b1G \u2208 [10\u22122, 104]. According to the Loglike, we set \u03b1S = 10, \u03b1G = 100, \u03b1P = 1000. The Loglike\u2019s curves w.r.t. the parameters are shown in the appendix.\nThe testing results are shown in Fig. 1. We can find that our learning algorithm performs better than other competitors on both data sets, i.e., higher Loglike, lower e\u00b5 and e\u03c6, w.r.t. various C. Especially when having few training sequences, the ODE-based and the LS algorithm need to learn too many parameters from insufficient samples so\nthey are inferior to our MLE-SGLP algorithm and its variants because of the over-fitting problem. By increasing the number of training sequences, the performance of the ODE-based algorithm does not improve a lot \u2014 the nature of non-convexity may lead the ODE-based algorithm to fall into local optimal. All MLE-based algorithms are superior to the ODE-based algorithm and the LS algorithm, and the proposed regularizers indeed help to improve learning results of MLE. Specifically, if the clustering structure is available, our MLE-SGLP algorithm will obtain the best results. Otherwise, our MLE-SGL algorithm will be the best, which is slightly better than MLE-GL and MLE-S.\nFor demonstrating the importance of the sparse-grouplasso regularizer to learning Granger causality graph, Fig. 2 visualizes the estimates of impact functions obtained by various methods. The Granger causality graph of the target Hawkes process is learned by finding those all-zero impact functions (the green subfigures). Our MLE-SGLP algorithm obtains right all-zero impact functions while the pure MLE algorithm sometimes fails because of the lack of sparse-related regularizer. It means that introducing sparse-group-lasso into the framework of MLE is necessary for learning Granger causality. Note that, even if the basis functions we select do not match well with the real case, i.e., the Gaussian basis functions are not suitable for piecewise constant impact functions, our algorithm can still learn the Granger causality graph of the Hawkes process robustly. As Fig. 1(b) shows, although the estimates of nonzero impact functions based on Gaussian basis functions do not fit the ground truth well, the all-zero impact functions are learned exactly via our MLE-SGLP algorithm."}, {"heading": "5.2. Real-world Data", "text": "We test our algorithm on the IPTV viewing record data set (Luo et al., 2014; 2015; 2016). The data set records the viewing behavior of 7100 users, i.e., what and when they watch, in the IPTV system from January to November 2012. U (= 13) categories of TV programs are predefined. Similar to (Luo et al., 2015), we model users\u2019 viewing behavior via a Hawkes process, in which the TV programs\u2019 categories exist self-and mutually-triggering patterns. For example, viewing an episode of a drama would lead to viewing the following episodes (self-triggering) and related news of actors (mutually-triggering). Therefore, the causality among categories is dependent not only on the predetermined displaying schedule but also on users\u2019 viewing preferences.\nWe capture the Granger causality graph of programs\u2019 categories via learning impact functions. In this case, the pairwise sparsity is not applied because the clustering structure is not available. The training data is the viewing behavior in the first 10 months and testing data is the viewing behavior in the last month. Considering the fact that many TV pro-\ngrams are daily or weekly periodic and the time length of most TV programs is about 20-40 minutes, we set the time length of impact function to be 8 days (i.e., the influence of a program will not exist over a week) and the number of samples M = 576 (i.e., one sample per 20 minutes). The cut-off frequency of sampling function is w0 = \u03c0M/T , where T is the number of minutes in 8 days. Table. 1 gives Loglike for various methods w.r.t. different training sequences. We can find that with the increase of training data, all the methods have improvements. Compared with the ODE-based algorithm and pure MLE algorithm, the MLE with regularizers has better Loglike and our MLE-SGL algorithm obtains the best result, especially when the training set is small (i.e., the sequences in one month). Note that here the LS algorithm doesn\u2019t work. Even using a PC with 16GB memory, the LS algorithm runs out-of-memory in this case because it requires to discretize long event sequences with dense samples.\nWe define the infectivity of the u\u2032-th TV program category on the u-th one as \u222b\u221e 0 \u03c6uu\u2032(s)ds, which is shown in Fig. 3(a). It can be viewed as an adjacency matrix of the Granger causality graph. Additionally, by ranking the infectivity from high to low, the top 24 impact functions are selected and shown in Fig. 3(b). We think our algorithm works well because the following reasonable phenomena are observed in our learning results:\n1) All TV program categories have obvious self-triggering patterns because most of TV programs display periodically.\nViewers are likely to watch them daily at the same time. Our learning results reflect these phenomena: the main diagonal elements of the infectivity matrix in Fig. 3(a) are much larger than other ones, and the estimates of impact functions in Fig. 3(b) have clear daily-periodic pattern.\n2) Some popular categories having a large number of viewers and long displaying time, e.g., \u201cdrama\u201d, \u201cmovie\u201d, \u201cnews\u201d and \u201ctalk show\u201d, are likely to be triggered by others, while the other unpopular ones having relative fewer but fixed viewers and short displaying time, e.g., \u201cmusic\u201d, \u201ckids\u2019 program\u201d, \u201cscience\u201d, are mainly triggered by themselves. It is easy to find that the infectivity matrix we learned reflects these patterns \u2014 the non-diagonal elements involving those unpopular categories are very small or zero. In Fig. 3(b) the non-zero impact functions mainly involve popular categories. Additionally, because few viewing events about these categories are observed in the training data, the estimates of the impact functions involving unpopular categories are relatively noisy.\nIn summary, our algorithm performs better on the IPTV data set than other competitors. The learning results are reasonable and interpretable, which prove the rationality and the feasibility of our algorithm to some degree."}, {"heading": "6. Conclusion", "text": "In this paper, we learn the Granger causality of Hawkes processes according to the relationship between the Granger causality and impact functions. Combining the MLE with the sparse-group-lasso, we propose an effective algorithm to learn the Granger causality graph of the target process. We demonstrate the robustness and the rationality of our work on both synthetic and real-world data. In the future, we plan to extend our work and analyze the Granger causality of general point processes."}, {"heading": "7. Acknowledgment", "text": "This work is supported in part by NSF DMS-1317424 and NIH R01 GM108341. Thanks reviewers for providing us with meaningful suggestions."}, {"heading": "8. Appendix", "text": ""}, {"heading": "8.1. Derivation of Surrogate Objective Function", "text": "Using the Jensen\u2019s inequality, we have following inequality for all c and i:\nlog ( \u00b5uci + \u2211M m=1 \u2211i\u22121 j=1 amuciucj\u03ba(\u03c4 c ij) ) \u2265 pii log ( \u00b5uci pii ) + M\u2211 m=1 i\u22121\u2211 j=1 pmij log ( amuciucj\u03ba(\u03c4 c ij) pmij ) .\nThe equation holds if and only if \u00b5u = \u00b5 (k) u and amuu\u2032 = a m,(k) uu\u2032 . Therefore, we have Q\u0398|\u0398(k) \u2265 L\u0398 and Q\u0398(k)|\u0398(k) = L\u0398(k) ."}, {"heading": "8.2. Derivation of Learning Algorithm", "text": "We have surrogate objective function F = \u2212Q\u0398|\u0398(k) + \u03b1S\u2016A\u20161 + \u03b1G\u2016A\u20161,2 + \u03b1PE\u0398|\u0398(k)(A), where Q = \u2212Q\u0398|\u0398(k)++\u03b1PE\u0398|\u0398(k)(A) is the data fidelity term. Similar to (Simon et al., 2013), we choose a group auu\u2032 = [a1uu\u2032 , ..., a M uu\u2032 ] > to minimize and fix other parameters. Given current estimate a(k)uu\u2032 , we majorize Q as\nQ \u2264 Q| a (k) uu\u2032 + (auu\u2032 \u2212 a(k)uu\u2032)\u2207auu\u2032Q|a(k) uu\u2032\n+ 1\n2\u03b7 \u2016auu\u2032 \u2212 a(k)uu\u2032\u2016 2 2.\n(10)\nIntroducing (10) to the surrogate objective function, we rewrite the optimization problem as\nmin auu\u2032\u22650 Q| a (k) uu\u2032 + (auu\u2032 \u2212 a(k)uu\u2032)\u2207auu\u2032Q|a(k) uu\u2032\n+ 1\n2\u03b7 \u2016auu\u2032 \u2212 a(k)uu\u2032\u2016 2 2 + +\u03b1S\u2016auu\u2032\u20161\n+ \u03b1G\u2016auu\u2032\u20162.\n(11)\nBecause both Q| a (k) uu\u2032 and \u2207auu\u2032Q|a(k) uu\u2032 are known, we add \u03b7 2\u2016\u2207auu\u2032Q|a(k)\nuu\u2032 \u201622 to the objective function of (11) and re-\nduce Q| a (k) uu\u2032 from it, and obtain an equivalent optimization problem\nmin auu\u2032\u22650\n1\n2\u03b7 \u2016auu\u2032 \u2212 (a(k)uu\u2032 \u2212 \u03b7\u2207auu\u2032Q|a(k) uu\u2032 )\u201622\n+ \u03b1S\u2016auu\u2032\u20161 + \u03b1G\u2016auu\u2032\u20162. (12)\nThe objective function in (12) is convex, so the optimal solution is characterized by the subgradient equations.\na (k) uu\u2032 \u2212 \u03b7\u2207auu\u2032Q|a(k) uu\u2032 \u2212 auu\u2032 = \u03b7\u03b1S\u03b3 + \u03b7\u03b1G\u03b2. (13)\n\u03b3 = [\u03b31, ..., \u03b3M ] >, where \u03b3m = 1 if amuu\u2032 > 0, and in [0, 1] otherwise. \u03b2 = auu\u2032\u2016auu\u2032\u2016 2 if auu \u2032 6= 0, and in the set {x|\u2016x\u20162 \u2264 1} otherwise. Combining the subgradient equations with the basic algebra in (Simon et al., 2013), we get that auu\u2032 = 0 if \u2016S\u03b7\u03b1S (a (k+1) uu\u2032 \u2212 \u03b7\u2207auu\u2032Q|a(k)\nuu\u2032 )\u20162 \u2264\n\u03b7\u03b1G holds, otherwise auu\u2032 satisfies( 1 +\n\u03b7\u03b1G \u2016auu\u2032\u20162\n) auu\u2032\n= S\u03b7\u03b1S (a (k) uu\u2032 \u2212 \u03b7\u2207auu\u2032Q|a(k)\nuu\u2032 ),\n(14)\nwhere S\u03b1(z) = sign(z)(|z| \u2212 \u03b1)+ achieves softthresholding for each element of input. Taking the norm on both sides, \u2016auu\u2032\u20162 can be replaced by\n(\u2016S\u03b7\u03b1S (a (k) uu\u2032 \u2212 \u03b7\u2207auu\u2032Q|a(k) uu\u2032 )\u20162 \u2212 t\u03b7\u03b1G)+. (15)\nReplacing the \u2016auu\u2032\u20162 in (14) with (15), we obtain the generalized gradient step:\na (k+1) uu\u2032 = 1\u2212 \u03b7\u03b1G \u2016S\u03b7\u03b1S (a (k+1) uu\u2032 \u2212 \u03b7\u2207auu\u2032Q|a(k)\nuu\u2032 )\u20162  +\n\u00d7 S\u03b7\u03b1S (a (k+1) uu\u2032 \u2212 \u03b7\u2207auu\u2032Q|a(k)\nuu\u2032 )\n(16)"}, {"heading": "8.3. Details of Basis Function Selection", "text": "In our model, the intensity function of Hawkes process over all dimensions is:\n\u03bb(t) = U\u2211 u=1 \u03bbu(t)\n= U\u2211 u=1 ( \u00b5u + \u2211U u\u2032=1 \u222b t 0 \u03c6uu\u2032(s)dNu\u2032(t\u2212 s) )\n= U\u2211 u=1 \u00b5u + U\u2211 u=1 \u2211 ti<t \u03c6uui(t\u2212 ti)\n= U\u2211 u=1 \u00b5u + U\u2211 u=1 \u2211 ti<t M\u2211 m=1 amuui\u03bam(t\u2212 ti).\n(17)\nApplying Fourier transform, we have\n\u03bb\u0302(\u03c9) = U\u2211 u=1 \u00b5u \u221a 2\u03c0\u03b4(\u03c9)\n+ U\u2211 u=1 \u2211 ti<t M\u2211 m=1 amuuie \u2212j\u03c9ti \u03ba\u0302m(\u03c9).\n(18)\nIn other words, the spectral of \u03bb(t) is the weighted sum of those of basis functions. Therefore, the cut-off frequency of basis function is bounded by that of intensity function.\nAs we show in our paper, given training sequences S = {sc}Cc=1, , where sc = {(tci , uci )} Nc i=1, we can estimate \u03bb(t) empirically via a Gaussian-based kernel density estimator:\n\u03bb(t) = \u2211C\nc=1 \u2211Nc i=1 Gh(t\u2212 tci ). (19)\nHere tci is the time stamp of the i-th event at the c-th sequence. Gh(t\u2212 tci ) = exp(\u2212 (t\u2212tci ) 2\n2h2 ) is a Gaussian kernel with the bandwidth h.\nBecause we only care about the selection of basis functions, we just need to estimate the spectral of \u03bb(t) rather than compute (19) directly. Specifically, applying Silverman\u2019s rule of thumb (Silverman, 1986), we first set optimal h = ( 4\u03c3\u0302 5 3 \u2211 cNc )0.2, where \u03c3\u0302 is the standard deviation of time stamps {tci}. Applying Fourier transform, we compute an upper bound for the spectral of \u03bb(t) as\n|\u03bb\u0302(\u03c9)| = \u2223\u2223\u2223\u2223\u222b \u221e \u2212\u221e \u03bb(t)e\u2212j\u03c9tdt \u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223 C\u2211 c=1 Nc\u2211 i=1 \u222b \u221e \u2212\u221e e\u2212 (t\u2212tci ) 2 2h2 e\u2212j\u03c9tdt \u2223\u2223\u2223\u2223\u2223 \u2264\nC\u2211 c=1 Nc\u2211 i=1 \u2223\u2223\u2223\u2223\u222b \u221e \u2212\u221e e\u2212 (t\u2212tci ) 2 2hc e\u2212j\u03c9tdt \u2223\u2223\u2223\u2223 =\nC\u2211 c=1 Nc\u2211 i=1 \u2223\u2223\u2223e\u2212j\u03c9tci e\u2212\u03c92h22 \u221a2\u03c0h2\u2223\u2223\u2223 \u2264\nC\u2211 c=1 Nc\u2211 i=1 \u2223\u2223\u2223e\u2212j\u03c9tci \u2223\u2223\u2223 \u2223\u2223\u2223e\u2212\u03c92h22 \u221a2\u03c0h2\u2223\u2223\u2223 =\n( C\u2211 c=1 Nc \u221a 2\u03c0h2 ) e\u2212 \u03c92h2 2 .\n(20)\nFurthermore, we can compute the upper bound of the absolute sum of the spectral higher than \u03c90 as\u222b \u221e\n\u03c90\n|\u03bb\u0302(\u03c9)|d\u03c9\n\u2264 ( C\u2211 c=1 Nc \u221a 2\u03c0h2 )\u222b \u221e \u03c90 e\u2212 \u03c92h2 2 d\u03c9\n=2\u03c0 ( C\u2211 c=1 Nc )\u222b \u221e \u03c90 h\u221a 2\u03c0 e\u2212 \u03c92h2 2 d\u03c9\n=2\u03c0 ( C\u2211 c=1 Nc )( 1 2 \u2212 \u222b \u03c90 0 h\u221a 2\u03c0 e\u2212 \u03c92h2 2 d\u03c9 )\n=2\u03c0 ( C\u2211 c=1 Nc )( 1 2 \u2212 1 2 \u222b \u03c90 \u2212\u03c90 h\u221a 2\u03c0 e\u2212 \u03c92h2 2 d\u03c9 )\n=\u03c0 ( C\u2211 c=1 Nc )( 1\u2212 1\u221a 2 erf(\u03c90h) ) ,\n(21)\nwhere erf(x) = 1\u221a \u03c0 \u222b x \u2212x e \u2212t2dt.\nTherefore, give a bound of residual , we can find an \u03c90 guaranteeing \u222b\u221e \u03c90 |\u03bb\u0302(\u03c9)|d\u03c9 \u2264 , or erf(\u03c90h) \u2265 \u221a 2 \u2212\n\u221a 2 \u03c0 \u2211C c=1Nc . The proposed basis functions {\u03ba\u03c90(t, tm)}Mm=1 are selected, where \u03c90 is the cut-off frequency of basis function and tm = (m\u22121)T M , M = d T\u03c90 \u03c0 e."}, {"heading": "8.4. Configuration of Parameters", "text": "With the help of cross validation, we test our algorithm with various parameters in a wide range, where \u03b1P , \u03b1S , \u03b1G \u2208 [10\u22122, 104]. According to the measure Loglike, we set \u03b1S = 10, \u03b1G = 100, \u03b1P = 1000. The curves of Loglike w.r.t. the three parameters are shown in the following figure. We can find that the learning result is relatively stable when changing the parameters in a wide range."}], "references": [{"title": "Tractable nonparametric bayesian inference in poisson processes with gaussian process intensities", "author": ["Adams", "Ryan Prescott", "Murray", "Iain", "MacKay", "David JC"], "venue": "In ICML,", "citeRegEx": "Adams et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2009}, {"title": "Recovering time-varying networks of dependencies in social and biological studies", "author": ["Ahmed", "Amr", "Xing", "Eric P"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Ahmed et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ahmed et al\\.", "year": 2009}, {"title": "Discrete-time signal processing", "author": ["Alan", "V Oppenheim", "Ronald", "W Schafer", "John", "RB"], "venue": "New Jersey, Printice Hall Inc,", "citeRegEx": "Alan et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Alan et al\\.", "year": 1989}, {"title": "Non-parametric kernel estimation for symmetric hawkes processes. application to high frequency financial data", "author": ["Bacry", "Emmanuel", "Dayri", "Khalil", "Muzy", "Jean-Fran\u00e7ois"], "venue": "The European Physical Journal B,", "citeRegEx": "Bacry et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bacry et al\\.", "year": 2012}, {"title": "Some limit theorems for hawkes processes and application to financial statistics", "author": ["Bacry", "Emmanuel", "Delattre", "Sylvain", "Hoffmann", "Marc", "Muzy", "Jean-Francois"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "Bacry et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bacry et al\\.", "year": 2013}, {"title": "Network granger causality with inherent grouping structure", "author": ["Basu", "Sumanta", "Shojaie", "Ali", "Michailidis", "George"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Basu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Basu et al\\.", "year": 2015}, {"title": "Multivariate hawkes process models of the occurrence of regulatory elements", "author": ["Carstensen", "Lisbeth", "Sandelin", "Albin", "Winther", "Ole", "Hansen", "Niels R"], "venue": "BMC bioinformatics,", "citeRegEx": "Carstensen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carstensen et al\\.", "year": 2010}, {"title": "A kernel independence test for random processes", "author": ["Chwialkowski", "Kacper", "Gretton", "Arthur"], "venue": "In ICML,", "citeRegEx": "Chwialkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chwialkowski et al\\.", "year": 2014}, {"title": "An introduction to the theory of point processes: volume II: general theory and structure, volume 2", "author": ["Daley", "Daryl J", "Vere-Jones", "David"], "venue": "Springer Science & Business Media,", "citeRegEx": "Daley et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Daley et al\\.", "year": 2007}, {"title": "Estimating diffusion network structures: Recovery conditions, sample complexity & soft-thresholding algorithm", "author": ["Daneshmand", "Hadi", "Gomez-Rodriguez", "Manuel", "Song", "Le", "Schoelkopf", "Bernhard"], "venue": "In ICML,", "citeRegEx": "Daneshmand et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Daneshmand et al\\.", "year": 2014}, {"title": "Graphical models for marked point processes based on local independence", "author": ["Didelez", "Vanessa"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Didelez and Vanessa.,? \\Q2008\\E", "shortCiteRegEx": "Didelez and Vanessa.", "year": 2008}, {"title": "Learning networks of heterogeneous influence", "author": ["Du", "Nan", "Song", "Le", "Yuan", "Ming", "Smola", "Alex J"], "venue": "In NIPS,", "citeRegEx": "Du et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Du et al\\.", "year": 2012}, {"title": "Graphical modelling of multivariate time series", "author": ["Eichler", "Michael"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "Eichler and Michael.,? \\Q2012\\E", "shortCiteRegEx": "Eichler and Michael.", "year": 2012}, {"title": "Graphical modeling for multivariate hawkes processes with nonparametric link functions", "author": ["Eichler", "Michael", "Dahlhaus", "Rainer", "Dueck", "Johannes"], "venue": null, "citeRegEx": "Eichler et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Eichler et al\\.", "year": 2015}, {"title": "Coevolve: A joint point process model for information diffusion and network coevolution", "author": ["M. Farajtabar", "Y. Wang", "M. Gomez-Rodriguez", "S. Li", "H. Zha", "L. Song"], "venue": "In NIPS,", "citeRegEx": "Farajtabar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Farajtabar et al\\.", "year": 2015}, {"title": "Shaping social activity by incentivizing users", "author": ["Farajtabar", "Mehrdad", "Du", "Nan", "Gomez-Rodriguez", "Manuel", "Valera", "Isabel", "Zha", "Hongyuan", "Song", "Le"], "venue": "In NIPS,", "citeRegEx": "Farajtabar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Farajtabar et al\\.", "year": 2014}, {"title": "A model for temporal dependencies in event streams", "author": ["Gunawardana", "Asela", "Meek", "Christopher", "Xu", "Puyang"], "venue": "In NIPS,", "citeRegEx": "Gunawardana et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gunawardana et al\\.", "year": 2011}, {"title": "Tracking dynamic point processes on networks", "author": ["Hall", "Eric C", "Willett", "Rebecca M"], "venue": "arXiv preprint arXiv:1409.0031,", "citeRegEx": "Hall et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2014}, {"title": "Transition matrix estimation in high dimensional time series", "author": ["Han", "Fang", "Liu"], "venue": "In ICML,", "citeRegEx": "Han et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Han et al\\.", "year": 2013}, {"title": "Lasso and probabilistic inequalities for multivariate point processes", "author": ["Hansen", "Niels Richard", "Reynaud-Bouret", "Patricia", "Rivoirard", "Vincent"], "venue": null, "citeRegEx": "Hansen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2015}, {"title": "Spectra of some self-exciting and mutually exciting point processes", "author": ["Hawkes", "Alan G"], "venue": null, "citeRegEx": "Hawkes and G.,? \\Q1971\\E", "shortCiteRegEx": "Hawkes and G.", "year": 1971}, {"title": "Nonparametric markovian learning of triggering kernels for mutually exciting and mutually inhibiting multivariate hawkes processes", "author": ["Lemonnier", "Remi", "Vayatis", "Nicolas"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Lemonnier et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lemonnier et al\\.", "year": 2014}, {"title": "A nonparametric em algorithm for multiscale hawkes processes", "author": ["Lewis", "Erik", "Mohler", "George"], "venue": "Journal of Nonparametric Statistics,", "citeRegEx": "Lewis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2011}, {"title": "A multitask point process predictive model", "author": ["Lian", "Wenzhao", "Henao", "Ricardo", "Rao", "Vinayak", "Lucas", "Joseph", "Carin", "Lawrence"], "venue": "In ICML,", "citeRegEx": "Lian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lian et al\\.", "year": 2015}, {"title": "Variational inference for gaussian process modulated poisson processes", "author": ["Lloyd", "Chris", "Gunter", "Tom", "Osborne", "Michael A", "Roberts", "Stephen J"], "venue": "In ICML,", "citeRegEx": "Lloyd et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lloyd et al\\.", "year": 2015}, {"title": "You are what you watch and when you watch: Inferring household structures from iptv viewing data", "author": ["Luo", "Dixin", "Xu", "Hongteng", "Zha", "Hongyuan", "Du", "Jun", "Xie", "Rong", "Yang", "Xiaokang", "Zhang", "Wenjun"], "venue": "Broadcasting, IEEE Transactions on,", "citeRegEx": "Luo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2014}, {"title": "Multitask multi-dimensional hawkes processes for modeling event sequences", "author": ["Luo", "Dixin", "Xu", "Hongteng", "Zhen", "Yi", "Ning", "Xia", "Zha", "Hongyuan", "Yang", "Xiaokang", "Zhang", "Wenjun"], "venue": "In IJCAI,", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Learning mixtures of markov chains from aggregate data with structural constraints", "author": ["Luo", "Dixin", "Xu", "Hongteng", "Zhen", "Yi", "Dilkina", "Bistra", "Zha", "Hongyuan", "Yang", "Xiaokang", "Zhang", "Wenjun"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Luo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2016}, {"title": "Toward learning graphical and causal process models", "author": ["Meek", "Christopher"], "venue": "In UAI Workshop Causal Inference: Learning and Prediction,", "citeRegEx": "Meek and Christopher.,? \\Q2014\\E", "shortCiteRegEx": "Meek and Christopher.", "year": 2014}, {"title": "Bayesian inference for hawkes processes", "author": ["Rasmussen", "Jakob Gulddahl"], "venue": "Methodology and Computing in Applied Probability,", "citeRegEx": "Rasmussen and Gulddahl.,? \\Q2013\\E", "shortCiteRegEx": "Rasmussen and Gulddahl.", "year": 2013}, {"title": "Adaptive estimation for hawkes processes; application to genome analysis", "author": ["Reynaud-Bouret", "Patricia", "Schbath", "Sophie"], "venue": "The Annals of Statistics,", "citeRegEx": "Reynaud.Bouret et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reynaud.Bouret et al\\.", "year": 2010}, {"title": "Scalable nonparametric bayesian inference on point processes with gaussian processes", "author": ["Samo", "Yves-Laurent Kom", "Roberts", "Stephen"], "venue": "In ICML,", "citeRegEx": "Samo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Samo et al\\.", "year": 2015}, {"title": "Density estimation for statistics and data analysis, volume 26", "author": ["Silverman", "Bernard W"], "venue": "CRC press,", "citeRegEx": "Silverman and W.,? \\Q1986\\E", "shortCiteRegEx": "Silverman and W.", "year": 1986}, {"title": "A sparse-group lasso", "author": ["Simon", "Noah", "Friedman", "Jerome", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Simon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simon et al\\.", "year": 2013}, {"title": "Trailer generation via a point process-based visual attractiveness model", "author": ["Xu", "Hongteng", "Zhen", "Yi", "Zha", "Hongyuan"], "venue": "In IJCAI,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "On machine learning towards predictive sales pipeline analytics", "author": ["Yan", "Junchi", "Zhang", "Chao", "Zha", "Hongyuan", "Gong", "Min", "Sun", "Changhua", "Huang", "Jin", "Chu", "Stephen", "Yang", "Xiaokang"], "venue": "In AAAI,", "citeRegEx": "Yan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2015}, {"title": "Online learning for group lasso", "author": ["Yang", "Haiqin", "Xu", "Zenglin", "King", "Irwin", "Lyu", "Michael R"], "venue": "In ICML,", "citeRegEx": "Yang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "Seismic: A selfexciting point process model for predicting tweet popularity", "author": ["Zhao", "Qingyuan", "Erdogdu", "Murat A", "He", "Hera Y", "Rajaraman", "Anand", "Leskovec", "Jure"], "venue": "In KDD,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Learning social infectivity in sparse low-rank networks using multidimensional hawkes processes", "author": ["Zhou", "Ke", "Zha", "Hongyuan", "Song", "Le"], "venue": "In AISTATS,", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}, {"title": "Learning triggering kernels for multi-dimensional hawkes processes", "author": ["Zhou", "Ke", "Zha", "Hongyuan", "Song", "Le"], "venue": "In ICML,", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "Existing works mainly focus on learning Granger causality for time series (Arnold et al., 2007; Eichler, 2012; Basu et al., 2015), where the Granger causality is captured via the socalled vector auto-regressive (VAR) model (Han & Liu, 2013) based on discrete time-lagged variables.", "startOffset": 74, "endOffset": 129}, {"referenceID": 23, "context": "A potential solution is to construct features for various dimensions from historical events and learn Granger causality via feature selection (Lian et al., 2015).", "startOffset": 142, "endOffset": 161}, {"referenceID": 30, "context": "Applications include bioinformatics (Reynaud-Bouret et al., 2010), social network analysis (Zhao et al.", "startOffset": 36, "endOffset": 65}, {"referenceID": 37, "context": ", 2010), social network analysis (Zhao et al., 2015), financial analysis (Bacry et al.", "startOffset": 33, "endOffset": 52}, {"referenceID": 4, "context": ", 2015), financial analysis (Bacry et al., 2013), etc.", "startOffset": 28, "endOffset": 48}, {"referenceID": 13, "context": "Compared with existing learning methods for Hawkes processes (Zhou et al., 2013b; Eichler et al., 2015), our model avoids discretized representation of impact functions and conditional intensity, and considers the induced structures across impact functions.", "startOffset": 61, "endOffset": 103}, {"referenceID": 16, "context": "Focusing on 1-D point process with simple piecewise constant conditional intensity, a model for capturing temporal dependencies between event types is proposed in (Gunawardana et al., 2011).", "startOffset": 163, "endOffset": 189}, {"referenceID": 5, "context": "In (Basu et al., 2015; Song et al., 2013), the inherent grouping structure is considered when learning the Granger casuality on networks from discrete transition process.", "startOffset": 3, "endOffset": 41}, {"referenceID": 9, "context": "(Daneshmand et al., 2014) proposed a continuoustime diffusion network inference method based on parametric cascade generative process.", "startOffset": 0, "endOffset": 25}, {"referenceID": 13, "context": "Specializing the work for Hawkes processes, (Eichler et al., 2015) firstly connects Granger causality with impact functions.", "startOffset": 44, "endOffset": 66}, {"referenceID": 4, "context": ", seismic analysis (Daley & VereJones, 2007), financial analysis (Bacry et al., 2013), social network modeling (Farajtabar et al.", "startOffset": 65, "endOffset": 85}, {"referenceID": 30, "context": ", 2013a;b) and bioinformatics (Reynaud-Bouret et al., 2010; Carstensen et al., 2010).", "startOffset": 30, "endOffset": 84}, {"referenceID": 6, "context": ", 2013a;b) and bioinformatics (Reynaud-Bouret et al., 2010; Carstensen et al., 2010).", "startOffset": 30, "endOffset": 84}, {"referenceID": 15, "context": ", the exponential functions in (Farajtabar et al., 2014; Rasmussen, 2013; Zhou et al., 2013a; Hall & Willett, 2014; Yan et al., 2015) and the power-law functions in (Zhao et al.", "startOffset": 31, "endOffset": 133}, {"referenceID": 35, "context": ", the exponential functions in (Farajtabar et al., 2014; Rasmussen, 2013; Zhou et al., 2013a; Hall & Willett, 2014; Yan et al., 2015) and the power-law functions in (Zhao et al.", "startOffset": 31, "endOffset": 133}, {"referenceID": 37, "context": ", 2015) and the power-law functions in (Zhao et al., 2015).", "startOffset": 39, "endOffset": 58}, {"referenceID": 26, "context": "For enhancing the flexibility, a nonparametric model of 1D Hawkes process is first proposed in (Lewis & Mohler, 2011) based on ordinary differential equation (ODE) and extended to multi-dimensional case in (Zhou et al., 2013b; Luo et al., 2015).", "startOffset": 206, "endOffset": 244}, {"referenceID": 3, "context": "Similarly, (Bacry et al., 2012) proposes a nonparametric estimation of Hawkes processes via solving the Wiener-Hopf equation.", "startOffset": 11, "endOffset": 31}, {"referenceID": 19, "context": "Another nonparametric strategy is the contrast function-based estimation in (ReynaudBouret et al., 2010; Hansen et al., 2015).", "startOffset": 76, "endOffset": 125}, {"referenceID": 13, "context": "It minimizes the estimation error of conditional intensity function and leads to a Least-Squares (LS) problem (Eichler et al., 2015).", "startOffset": 110, "endOffset": 132}, {"referenceID": 11, "context": "(Du et al., 2012; Lemonnier & Vayatis, 2014) decompose impact functions into basis functions to avoid discretization.", "startOffset": 0, "endOffset": 44}, {"referenceID": 0, "context": "The Gaussian process-based methods (Adams et al., 2009; Lloyd et al., 2015; Lian et al., 2015; Samo & Roberts, 2015) have been reported to successfully estimate more general point processes.", "startOffset": 35, "endOffset": 116}, {"referenceID": 24, "context": "The Gaussian process-based methods (Adams et al., 2009; Lloyd et al., 2015; Lian et al., 2015; Samo & Roberts, 2015) have been reported to successfully estimate more general point processes.", "startOffset": 35, "endOffset": 116}, {"referenceID": 23, "context": "The Gaussian process-based methods (Adams et al., 2009; Lloyd et al., 2015; Lian et al., 2015; Samo & Roberts, 2015) have been reported to successfully estimate more general point processes.", "startOffset": 35, "endOffset": 116}, {"referenceID": 34, "context": ", self-triggering or self-correcting (Xu et al., 2015).", "startOffset": 37, "endOffset": 54}, {"referenceID": 15, "context": "0 \u03c6uu\u2032(s)dNu\u2032(t\u2212 s), (1) where \u03bcu is the exogenous base intensity independent of the history while \u2211U u\u2032=1 \u222b t 0 \u03c6uu\u2032(s)dNu\u2032(t \u2212 s) the endogenous intensity capturing the peer influence (Farajtabar et al., 2014).", "startOffset": 186, "endOffset": 211}, {"referenceID": 13, "context": "In (Eichler et al., 2015), the notion of Granger non-causality is used, and the above definition is equivalent to saying that type-u\u2032 event does not Granger-cause type-u event w.", "startOffset": 3, "endOffset": 25}, {"referenceID": 13, "context": "Granger Causality of Hawkes Process The work in (Eichler et al., 2015) reveals the relationship between Hawkes processes\u2019 impact function and its Granger causality graph as follows, Theorem 4.", "startOffset": 48, "endOffset": 70}, {"referenceID": 13, "context": "(Eichler et al., 2015).", "startOffset": 0, "endOffset": 22}, {"referenceID": 36, "context": "Therefore, we use group-lasso (Yang et al., 2010; Simon et al., 2013; Song et al., 2013) to regularize the coefficients of impact functions, denoted as \u2016A\u20161,2 = \u2211 u,u\u2032 \u2016auu\u2032\u20162, where auu\u2032 = [auu\u2032 , .", "startOffset": 30, "endOffset": 88}, {"referenceID": 33, "context": "Therefore, we use group-lasso (Yang et al., 2010; Simon et al., 2013; Song et al., 2013) to regularize the coefficients of impact functions, denoted as \u2016A\u20161,2 = \u2211 u,u\u2032 \u2016auu\u2032\u20162, where auu\u2032 = [auu\u2032 , .", "startOffset": 30, "endOffset": 88}, {"referenceID": 33, "context": "Furthermore, for solving sparse-group-lasso (SGL), we apply the soft-thresholding method in (Simon et al., 2013) to shrink the updated parameters.", "startOffset": 92, "endOffset": 112}, {"referenceID": 2, "context": "A contribution of our work is proposing a method of selecting basis functions founded on sampling theory (Alan et al., 1989).", "startOffset": 105, "endOffset": 124}, {"referenceID": 13, "context": ", 2013b) and the LeastSquares (LS) algorithm in (Eichler et al., 2015), our algorithm has following advantages.", "startOffset": 48, "endOffset": 70}, {"referenceID": 13, "context": "The LS algorithm in (Eichler et al., 2015) directly discretizes the timeline into L small intervals.", "startOffset": 20, "endOffset": 42}, {"referenceID": 13, "context": ", 2013b), the Least-Squares (LS) method in (Eichler et al., 2015), on both synthetic and real-world data.", "startOffset": 43, "endOffset": 65}, {"referenceID": 25, "context": "Real-world Data We test our algorithm on the IPTV viewing record data set (Luo et al., 2014; 2015; 2016).", "startOffset": 74, "endOffset": 104}, {"referenceID": 26, "context": "Similar to (Luo et al., 2015), we model users\u2019 viewing behavior via a Hawkes process, in which the TV programs\u2019 categories exist self-and mutually-triggering patterns.", "startOffset": 11, "endOffset": 29}, {"referenceID": 33, "context": "Similar to (Simon et al., 2013), we choose a group auu\u2032 = [auu\u2032 , .", "startOffset": 11, "endOffset": 31}, {"referenceID": 33, "context": "Combining the subgradient equations with the basic algebra in (Simon et al., 2013), we get that auu\u2032 = 0 if \u2016S\u03b7\u03b1S (a (k+1) uu\u2032 \u2212 \u03b7\u2207auu\u2032Q|a(k) uu\u2032 )\u20162 \u2264 \u03b7\u03b1G holds, otherwise auu\u2032 satisfies ( 1 + \u03b7\u03b1G \u2016auu\u2032\u20162 ) auu\u2032 = S\u03b7\u03b1S (a (k) uu\u2032 \u2212 \u03b7\u2207auu\u2032Q|a(k) uu\u2032 ), (14)", "startOffset": 62, "endOffset": 82}], "year": 2016, "abstractText": "Learning Granger causality for general point processes is a very challenging task. In this paper, we propose an effective method, learning Granger causality, for a special but significant type of point processes \u2014 Hawkes process. According to the relationship between Hawkes process\u2019s impact function and its Granger causality graph, our model represents impact functions using a series of basis functions and recovers the Granger causality graph via group sparsity of the impact functions\u2019 coefficients. We propose an effective learning algorithm combining a maximum likelihood estimator (MLE) with a sparsegroup-lasso (SGL) regularizer. Additionally, the flexibility of our model allows to incorporate the clustering structure event types into learning framework. We analyze our learning algorithm and propose an adaptive procedure to select basis functions. Experiments on both synthetic and real-world data show that our method can learn the Granger causality graph and the triggering patterns of the Hawkes processes simultaneously.", "creator": "LaTeX with hyperref package"}}}