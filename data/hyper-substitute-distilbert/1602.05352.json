{"id": "1602.05352", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2016", "title": "Recommendations as Treatments: Debiasing Learning and Evaluation", "abstract": "quantitative data for evaluating and executing computational systems is targeted to selection sensitivity, either through mis - selection by the developer or through inappropriate actions of the given system itself. through this paper, landau gave incredibly simple approach to handling objective questions, adapting accordingly identifying evolving techniques affecting causal inference. the approach leads to unbiased performance estimators presenting biased data, and derives a reliability factorization metric to provides substantially measured system performance on real - world applications. these theoretically no empirically advocate the robustness of the approach, understanding this it needs highly practical and humane.", "histories": [["v1", "Wed, 17 Feb 2016 09:58:25 GMT  (1111kb,D)", "http://arxiv.org/abs/1602.05352v1", "10 pages"], ["v2", "Fri, 27 May 2016 03:18:59 GMT  (1178kb,D)", "http://arxiv.org/abs/1602.05352v2", "10 pages in ICML 2016"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.IR", "authors": ["tobias schnabel", "adith swaminathan", "ashudeep singh", "navin chandak", "thorsten joachims"], "accepted": true, "id": "1602.05352"}, "pdf": {"name": "1602.05352.pdf", "metadata": {"source": "META", "title": "Recommendations as Treatments: Debiasing Learning and Evaluation ", "authors": ["Tobias Schnabel", "Adith Swaminathan", "Ashudeep Singh {TBS", "Navin Chandak", "Thorsten Joachims"], "emails": ["AS3354}@CORNELL.EDU", "NAVINCHANDAK92@GMAIL.COM", "TJ@CS.CORNELL.EDU"], "sections": [{"heading": "1. Introduction", "text": "Virtually all data for training recommender systems is subject to selection biases. In a movie recommendation system, for example, users typically watch and rate those movies that they like, and rarely rate movies that they do not like (Pradel et al., 2012). Similarly, when an adplacement system recommends ads, it shows ads that it believes to be of interest to the user, but will less frequently display other ads. Having observations be conditioned on the effect we would like to optimize (e.g. the star rating, the probability of a click) leads to data that is Missing Not At Random (MNAR) (Little & Rubin, 2002). This creates a widely-recognized challenge for training and evaluating recommender systems (Marlin & Zemel, 2009; De Myttenaere et al., 2014).\nIn this paper, we develop an approach to evaluating and\nPreliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute.\ntraining recommender systems that remedies selection biases in a principled, practical, and highly effective way. To this effect, we view the recommendation task from a causal inference perspective. In particular, we argue that exposing a user to an item in a recommendation system is an intervention analogous to exposing a patient to a treatment in a medical study. In both cases, the goal is to accurately estimate the effect of new interventions (e.g. a new treatment policy or a new set of recommendations) despite incomplete and biased data due to self-selection or experimenterbias. By connecting recommendation to causal inference from experimental and observational data, the paper derives a principled framework for unbiased evaluation and learning of recommender systems under selection biases.\nThe main contribution of this paper is four-fold. First, we show how estimating the quality of a recommendation system can be approached with propensity-weighting techniques commonly used in causal inference (Imbens & Rubin, 2015), complete-cases analysis (Little & Rubin, 2002), and other problems (Cortes et al., 2008; Bickel et al., 2009; Sugiyama & Kawanabe, 2012). In particular, we derive unbiased estimators for a wide range of performance measure (e.g. MSE, MAE, DCG). Second, with these estimators in hand, we propose an Empirical Risk Minimization (ERM) framework for learning recommendation systems under selection bias, for which we derive generalization error bounds. Third, we use the ERM framework to derive a matrix factorization method that can account for selection bias while remaining conceptually simple and highly scalable. Fourth, we explore methods for estimating propensities in observational settings where selection bias is due to self-selection by the users, and we characterize the robustness of the framework against misspecified propensities.\nOur conceptual and theoretical contributions are validated in an extensive empirical evaluation. For the task of evaluating recommender systems, we show that our performance estimators can be orders-of-magnitude more accurate than\nar X\niv :1\n60 2.\n05 35\n2v 1\n[ cs\n.L G\n] 1\n7 Fe\nb 20\nstandard estimators commonly used in the past (Bell et al., 2007). For the task of learning recommender systems, we show that our new matrix factorization method substantially outperforms methods that ignore selection bias, as well as existing state-of-the-art methods that perform jointlikelihood inference under MNAR data (Herna\u0301ndez-Lobato et al., 2014). This is especially promising given the conceptual simplicity and scalability of our approach compared to joint-likelihood inference. We provide software implementing of our method, as well as a new real-world dataset that can serve as a future benchmark."}, {"heading": "2. Related Work", "text": "Past work that explicitly dealt with the MNAR nature of recommendation data approached the problem as missingdata imputation based on the joint likelihood of the missing data model and the rating model (Marlin et al., 2007; Marlin & Zemel, 2009; Herna\u0301ndez-Lobato et al., 2014). This has lead to sophisticated and highly complex methods. We take a fundamentally different approach that treats both models separately, making our approach modular and scalable. Furthermore, our approach is robust to misspecification of the rating model, and we characterize how the overall learning process degrades gracefully under a misspecified missing-data model. We empirically compare against the state-of-the-art joint likelihood model (Herna\u0301ndez-Lobato et al., 2014) in this paper.\nRelated but different from the problem we consider is recommendation from positive feedback alone (Hu et al., 2008; Dawen Liang & Blei, 2016). Alternative approaches to learning with MNAR data (Steck, 2010; Lim et al., 2015) aim to avoid the problem by considering performance measures less affected by selection bias.\nPropensity-based approaches have been widely used in causal inference from observational studies (Imbens & Rubin, 2015), as well as in complete-case analysis for missing data (Little & Rubin, 2002; Seaman & White, 2013) and in survey sampling (Thompson, 2012). However, their use within recommendation or matrix completion is new to our knowledge. Weighting approaches are also widely used in domain adaptation and covariate shift, where data from one source is used to train for a different problem (e.g., Huang et al., 2006; Bickel et al., 2009; Sugiyama & Kawanabe, 2012). We will draw upon this work, especially the learning theory related to weighting approaches in (Cortes et al., 2008; 2010). Weighted estimators are also used in Monte Carlo estimation via importance sampling (Owen, 2013)."}, {"heading": "3. Unbiased Performance Estimation for Recommendation", "text": "Consider the following toy example adapted from Steck (2010) to illustrate the disastrous effect that selection bias can have on conventional evaluation using a test set of heldout ratings. Denote with u \u2208 {1, ..., U} the users and with i \u2208 {1, ..., I} the items. Figure 1 shows the matrix of true ratings Y \u2208 <U\u00d7I for our toy example, where a subset of users are \u201chorror lovers\u201d who rate all horror movies 5 and all romance movies 1. Similarly, there is a subset of \u201cromance lovers\u201d who rate just the opposite way. However, both groups rate dramas as 3.\nThe binary matrix O \u2208 {0, 1}U\u00d7I in Figure 1 shows for which movies the users provided their rating to the system, [Ou,i = 1] \u21d4 [Yu,i observed]. Our toy example shows a strong correlation between liking and rating a movie, and the matrix P describes the marginal probabilities Pu,i = P (Ou,i = 1) with which each rating is revealed. For this data, consider the following two evaluation tasks."}, {"heading": "3.1. Task 1: Estimating Rating Prediction Accuracy", "text": "For the first task, we want to evaluate how well a predicted rating matrix Y\u0302 reflects the true ratings in Y . Standard evaluation measures like Mean Absolute Error (MAE) or Mean Squared Error (MSE) can be written as:\nR(Y\u0302 ) = U\u2211 u=1 I\u2211 i=1 \u03b4u,i(Y, Y\u0302 ) , (1)\nfor an appropriately chosen \u03b4u,i(Y, Y\u0302 ).\nMAE: \u03b4u,i(Y, Y\u0302 ) = |Yu,i \u2212 Y\u0302u,i|/(U \u00b7 I) , (2) MSE: \u03b4u,i(Y, Y\u0302 ) = (Yu,i \u2212 Y\u0302u,i)2/(U \u00b7 I) , (3)\nAccuracy: \u03b4u,i(Y, Y\u0302 ) = 1{Y\u0302u,i=Yu,i}/(U \u00b7I) . (4)\nSince Y is only partially known, the conventional practice is to estimate R(Y\u0302 ) using the average over only the ob-\nserved entries,\nR\u0302naive(Y\u0302 ) = U \u00b7 I |Ou,i = 1| \u2211\n(u,i):Ou,i=1\n\u03b4u,i(Y, Y\u0302 ) . (5)\nWe call this the naive estimator, and its naivety leads to a gross misjudgment for the Y\u03021 and Y\u03022 given in Figure 1. Even though Y\u03021 is clearly better than Y\u03022 by any reasonable measure of performance, R\u0302naive(Y\u0302 ) will reliably claim that Y\u03022 has better MAE than Y\u03021. This error is due to selection bias, since 1-star ratings are underrepresented in the observed data and \u03b4u,i(Y, Y\u0302 ) is correlated with Yu,i. More generally, under selection bias R\u0302naive(Y\u0302 ) is not an unbiased estimate of the true performance R(Y\u0302 ) of Y\u0302 :\nEO [ R\u0302naive(Y\u0302 ) ] 6= R(Y\u0302 ) . (6)\nBefore tackling the problem of designing an improved estimator to replace R\u0302naive(Y\u0302 ), let\u2019s turn to a closely related evaluation task."}, {"heading": "3.2. Task 2: Estimating Recommendation Quality", "text": "Instead of evaluating the accuracy of predicted ratings, we may want to more directly evaluate the quality of a particular recommendation. To this effect, let\u2019s redefine Y\u0302 to now encode recommendations as a binary matrix analogous to O, where [Y\u0302u,i = 1] \u21d4 [i is recommended to u], limited to a budget of k recommendations per user. An example is Y\u03023 in Figure 1. A reasonable way to measure the quality of a recommendation is the Cumulative Gain (CG) that the user derives from the recommended movies, which we define as the average star-rating of the recommended movies in our toy example1. CG can again be written in the form of Equation (1) with\nCG: \u03b4u,i(Y, Y\u0302 ) = Y\u0302u,i \u00b7 Yu,i/(k \u00b7 U) . (7)\nHowever, unless users have watched all movies in Y\u0302 , we cannot compute CG directly via (1). Hence, we are faced with the following counterfactual question: how well would our users have enjoyed themselves (in terms of CG), if they had followed our recommendations Y\u0302 instead of watching the movies indicated in O? Note that rankings of recommendations are similar to the set-based recommendation described above, and measures like Discounted Cumulative Gain (DCG), DCG@k, Precision at k (PREC@k), and others (Aslam et al., 2006; Yilmaz et al., 2008) also fit in this setting. For those, let the values of Y\u0302 in each row define the predicted ranking, then\nDCG: \u03b4u,i(Y, Y\u0302 )=Yu,i/(log(rank(Y\u0302u,i)) \u00b7 U) , (8) PREC@k: \u03b4u,i(Y, Y\u0302 )=Yu,i \u00b71{rank(Y\u0302u,i)\u2264k}/(k \u00b7U) . (9)\n1More realistically, Y would contain quality scores derived from behavioral indicator like \u201dclicked\u201d and \u201dwatched to the end\u201d.\nOne approach, similar in spirit to condensed DCG (Sakai, 2007), is to use a naive estimator analogous to (5).\nR\u0302naive(Y\u0302 ) = U |Ou,i = 1| \u2211\n(u,i):Ou,i=1\n\u03b4u,i(Y, Y\u0302 ) . (10)\nBut, again, this estimator is generally biased for R(Y\u0302 ).\nThe key to getting unbiased counterfactual estimates of recommendation quality despite missing observations lies in the following connection to estimating average treatment effects of a given policy in causal inference, already explored in the contextual bandit setting (Li et al., 2011; Dud\u0131\u0301k et al., 2011). If we think of making a recommendation as an intervention analogous to treating a patient with a specific drug, in both settings we want to estimate the effect of a new treatment policy (e.g. give drug A to women and drug B to men, or new recommendations Y\u0302 ). The challenge in both cases is that we have only partial knowledge of how much certain patients (users) benefited from certain treatments (movies) (i.e., Yu,i with Ou,i = 1), but the vast majority of potential outcomes in Y is unobserved."}, {"heading": "3.3. Propensity-Scored Performance Estimators", "text": "The key to handling selection bias in both of the abovementioned evaluation tasks lies in understanding the process that generates the observation pattern in O. This process is typically called the Assignment Mechanism in causal inference (Imbens & Rubin, 2015) or the Missing Data Mechanism in missing data analysis (Little & Rubin, 2002). We differentiate the following two settings:\nExperimental Setting. In this setting, the assignment mechanism is under the control of the recommendation system. An example is an ad-placement system that controls which ads to show to which user. Observational Setting. In this setting, the users are part of the assignment mechanism that generates O. An example is an online streaming service for movies, where users self-select the movies they watch and rate.\nIn this paper, we assume that the assignment mechanism is probabilistic, meaning that the marginal probability Pu,i = P (Ou,i = 1) of observing an entry Yu,i is non-zero for all user/item pairs. This ensures that, in principle, every element of Y could be observed, even though any particular O reveals only a small subset. We refer to Pu,i as the propensity of observing Yu,i. In the experimental setting, we know the matrix P of all propensities, since we have implemented the assignment mechanism. In the observational setting, we will need to estimate P from the observed matrixO. We defer the discussion of propensity estimation to Section 5, and focus on the experimental setting for the sake of exposition first.\nIPS Estimator. The following defines the InversePropensity-Scoring (IPS) estimator (Thompson, 2012; Little & Rubin, 2002; Imbens & Rubin, 2015), which applies equally to the task of rating prediction evaluation as to the task of recommendation quality estimation.\nR\u0302IPS(Y\u0302 |P ) = \u2211\n(u,i):Ou,i=1\n\u03b4u,i(Y, Y\u0302 )\nPu,i . (11)\nUnlike the naive estimator R\u0302naive(Y\u0302 ), the IPS estimator is unbiased for any probabilistic assignment mechanism. EO [ R\u0302IPS(Y\u0302 |P )\n] = \u2211 u \u2211 i EOu,i [ \u03b4u,i(Y, Y\u0302 ) Pu,i Ou,i ] = \u2211 u \u2211 i \u03b4u,i(Y, Y\u0302 ) = R(Y\u0302 ) . (12)\nNote that the Pu,i are merely the marginal probabilities of observing an entry, and that unbiasedness is not affected by dependencies within O. To characterize the variability of the IPS estimator, however, we assume that observations are independent given P , which corresponds to a multivariate Bernoulli model where each Ou,i is a biased coin flip with probability Pu,i. The following proposition (proof in appendix) provides some intuition about how the accuracy of the IPS estimator changes as the propensities become more \u201dnon-uniform\u201d. Proposition 3.1 (Tail Bound for IPS Estimator). Let P be the independent Bernoulli probabilities of observing each entry. For any given Y\u0302 , with probability 1 \u2212 \u03b7, the IPS estimator R\u0302(Y\u0302 ;P ) does not deviate from the true R(Y\u0302 ) by more than:\u2223\u2223\u2223R\u0302IPS(Y\u0302 |P )\u2212R(Y\u0302 )\u2223\u2223\u2223 \u2264\u221a\u2212 log 2\u03b7\n2\n\u2211 u,i \u03c12u,i , (13)\nwhere \u03c1u,i = \u03b4u,i(Y,Y\u0302 ) Pu,i if Pu,i < 1, and \u03c1u,i = 0 otherwise.\nTo illustrate this bound, consider the case of uniform propensities Pu,i = p. This means that n = p UI elements of Y are revealed in expectation. In this case, the bound is O( 1\np \u221a U \u00b7I ) for bounded \u03b4u,i(Y, Y\u0302 ). If the Pu,i are nonuniform, the bound will be larger even if \u2211 Pu,i also equals n. This indicates that we are paying for the unbiasedness of IPS in terms of variability, and we will evaluate whether this price is well spent throughout this paper.\nSNIPS Estimator. One technique that can reduce variability is the use of control variates (Owen, 2013). Applied to the IPS estimator, we can use the knowledge that EO [\u2211\n(u,i):Ou,i=1 1 Pu,i\n] = U \u00b7 I . This yields the Self-\nNormalized Inverse Propensity Scoring (SNIPS) estimator (Trotter & Tukey, 1956; Swaminathan & Joachims, 2015)\nR\u0302SNIPS(Y\u0302 |P ) = \u2211 (u,i):Ou,i=1 \u03b4u,i(Y,Y\u0302 ) Pu,i\n1 U \u00b7I \u2211 (u,i):Ou,i=1 1 Pu,i . (14)\nThe SNIPS estimator often has lower variance than the IPS estimator but has a small bias (Hesterberg, 1995)."}, {"heading": "3.4. Empirical Illustration of Estimators", "text": "To illustrate the effectiveness of the proposed estimators in comparison to the naive estimator, we conducted an experiment on the semi-synthetic ML100K dataset described in more detail in Section 6.2. For this dataset, Y is completely known so that we can compute true performance via Equation (1). The probability Pu,i of observing a rating Yu,i was chosen to mimic the observed marginals in ML100K (see Section 6.2) such that 1% of the O matrix was non-zero.\nTable 1 shows the results for estimating rating prediction accuracy via MAE and recommendation quality via DCG@50 for the following five prediction matrices Y\u0302i. Let |Y = r| be the number of r-star ratings in Y . REC ONES: The prediction matrix Y\u0302 is identical to the\ntrue rating matrix Y , except that |Y = 5| randomly selected true ratings of 1 are flipped to 5. This means half of the predicted fives are true fives, and half are true ones. REC FOURS: Same as REC ONES, but flipping 4-star ratings instead. ROTATE: For each predicted rating Y\u0302u,i = Yu,i\u22121 when Yu,i \u2265 2, and Y\u0302u,i = 5 when Yu,i = 1. SKEWED: Predictions Y\u0302u,i are sampled via N (Y\u0302 rawu,i |\u00b5 = Yu,i, \u03c3 = 6\u2212Yu,i2 ) and clipped to the interval [0, 6]. COARSENED: If the true rating Yu,i \u2264 3, then Y\u0302u,i = 3. Otherwise Y\u0302u,i = 4.\nAs we can see in Table 1, the mean IPS estimate averaged over 50 samples of O from P perfectly matches the true\nperformance for both MAE and DCG as expected. The bias of SNIPS is negligible as well. The naive estimator is severely biased and its estimated MAE incorrectly ranks the prediction matrices Y\u0302i (e.g. it ranks the performance of REC ONES higher than REC FOURS). The standard deviation of IPS and SNIPS is larger than that of Naive, but it is still small compared to the bias that Naive incurs. SNIPS manages to reduce the standard deviation of IPS for MAE, but not for DCG. In Section 6, we will further empirically study these estimators."}, {"heading": "4. Propensity-Scored Recommendation Learning", "text": "The previous section derived effective performance estimators for evaluating a prediction Y\u0302 . In the following, we will use these estimators as part of an Empirical Risk Minimization (ERM) framework for learning, prove generalization error bounds for this ERM framework, and derive a matrix factorization method for rating prediction."}, {"heading": "4.1. Empirical Risk Minimization for Recommendation with Propensities", "text": "Empirical Risk Minimization underlies many successful learning algorithms like SVMs (Cortes & Vapnik, 1995), Boosting (Schapire, 1990), and Deep Networks (Bengio, 2009), and weighted ERM approaches have been effective for cost-sensitive classification, domain adaptation and covariate shift (Zadrozny et al., 2003; Bickel et al., 2009; Sugiyama & Kawanabe, 2012). We adapt ERM to our setting by realizing that Equation (1) corresponds to an expected loss (i.e. Risk) over the data generating process P (O|P ). Given a sample from P (O|P ), we can think of the IPS estimator from Equation (11) as the Empirical Risk R\u0302(Y\u0302 ) that estimates R(Y\u0302 ) for any Y\u0302 .\nDefinition 4.1 (Propensity-Scored ERM for Recommendation). Given training observationsO from Y with marginal propensities P , given a hypothesis space H of predictions Y\u0302 , and given a loss function \u03b4u,i(Y, Y\u0302 ). ERM selects the Y\u0302 \u2208 H that optimizes:\nY\u0302 ERM = argmin Y\u0302 \u2208H\n{ R\u0302IPS(Y\u0302 |P ) } . (15)\nNote that using the SNIPS estimator does not change the argmax. To illustrate the validity of the propensity-scored ERM approach, we state the following generalization error bound (proof in appendix) similar to Cortes et al. (2010). We consider only finiteH for the sake of conciseness. Theorem 4.2 (Propensity-Scored ERM Generalization Error Bound). For any finite hypothesis space of predictions H = {Y\u03021, ..., Y\u0302|H|} and loss 0 \u2264 \u03b4u,i(Y, Y\u0302 ) \u2264 \u2206, the true risk R(Y\u0302 ) of the empirical risk minimizer Y\u0302 ERM from H\nusing the IPS estimator, given training observationsO from Y with independent Bernoulli propensities P , is bounded with probability 1\u2212 \u03b7 by:\nR(Y\u0302 ERM ) \u2264 R\u0302IPS(Y\u0302 ERM |P ) +\n\u2206\n\u221a log (2|H|/\u03b7)\n2\n\u221a\u2211 u,i 1 P 2u,i . (16)"}, {"heading": "4.2. Propensity-Scored Matrix Factorization", "text": "We now use propensity-scored ERM to derive a matrix factorization method for the problem of rating prediction. Assume a standard rank d-restricted and L2-regularized matrix factorization model Y\u0302u,i = uTu vi+au+bi+cwith user, item, and global offsets as our hypothesis space H. Under this model, propensity-scored ERM leads to the following training objective:\nargmin U,V,A [ \u2211 Ou,i=1 \u03b4u,i(Y,U TV+A) Pu,i + \u03bb ( ||U ||2F+||V ||2F )] (17)\nwhereA encodes the offset terms and Y\u0302 ERM = UTV +A. Except for the propensities Pu,i that act like weights for each loss term, the training objective is identical to the standard incomplete matrix factorization objective (Koren, 2008; Steck, 2010; Hu et al., 2008) with MSE (3) or MAE (2). This means that we can readily draw upon the existing arsenal of optimization algorithms (i.e., Gemulla et al., 2011; Yu et al., 2012), making the training problem highly scalable. For the experiments reported in this paper, we use Limited-memory BFGS (Byrd et al., 1995). Our implementation is available online at xxxxxx.\nNote that conventional incomplete matrix factorization is a special case of (17) for MCAR data, i.e., all propensities Pu,i are equal. Solving this training objective for other \u03b4u,i(Y, Y\u0302 ) that are non-differentiable is more challenging, but possible avenues exist (Joachims, 2005; Chapelle & Wu, 2010). Finally, note that other recommendation methods (e.g., Weimer et al., 2007; Lin, 2007) can in principle be adapted to propensity scoring as well."}, {"heading": "5. Propensity Estimation for Observational Data", "text": "We now turn to the Observational Setting where propensities need to be estimated, since users make self-directed choices of which observations to reveal. One might be worried that one needs to perfectly reconstruct all propensities. However, as we will show, we merely need estimated propensities that are \u201cbetter\u201d than the naive assumption of observations being revealed uniformly, i.e., Pu,i = |Ou,i = 1|/ (U \u00b7 I) for all users and items. The\nfollowing characterizes what constitutes \u201cbetter\u201d propensities both in terms of the bias they induce, as well as their effect on the variability of the learning process.\nLemma 5.1 (Bias of IPS Estimator under Inaccurate Propensities). Let P be the marginal probabilities of observing an entry of the rating matrix Y , and let P\u0302 be the estimated propensities. The bias of the IPS estimator (11) using P\u0302 is:\nbias ( R\u0302IPS(Y\u0302 |P\u0302 ) ) = \u2211 u,i \u03b4u,i(Y, Y\u0302 ) [ 1\u2212 Pu,i P\u0302u,i ] . (18)\nIn addition to bias, the following generalization error bound (proof in appendix) characterizes the overall impact of the estimated propensities on the learning process.\nTheorem 5.2 (Propensity-Scored ERM Generalization Error Bound under Inaccurate Propensities). For any finite hypothesis space of predictions H = {Y\u03021, ..., Y\u0302|H|}, the prediction error of the empirical risk minimizer Y\u0302 ERM under the IPS estimator with estimated propensities P\u0302 , given training observationsO from Y with independent Bernoulli propensities P , is bounded by:\nR(Y\u0302 ERM ) \u2264 R\u0302IPS(Y\u0302 ERM |P\u0302 ) + \u2206 \u2211 u,i \u2223\u2223\u2223\u2223\u22231\u2212 Pu,iP\u0302u,i \u2223\u2223\u2223\u2223\u2223\n+\u2206\n\u221a log (2|H|/\u03b7)\n2\n\u221a\u2211 u,i 1 P\u0302 2u,i . (19)\nNote that the bound shows a bias-variance tradeoff that does not occur in conventional ERM learning. In particular, the bound suggests that it can be beneficial to overestimate small propensities, if this reduces the variability term more than it increases the bias term."}, {"heading": "5.1. Propensity Estimation via Naive Bayes", "text": "For our assignment model, we need to estimate a collection of Bernoulli random variables Ou,i, where the parameters Pu,i corresponds to the propensities we seek. In principle, the distribution\nP (O | X,Xhid, Y ) (20)\ncan depend on some observable features X (e.g., the predicted rating displayed to the user), unobservable features Xhid (e.g., whether the movie was recommended by a friend), and the ratings Y . It is reasonable to assume that Ou,i is independent of the new predictions Y\u0302 (and therefore independent of \u03b4u,i(Y, Y\u0302 )) once the observable features are taken into account. The following outlines two propensity estimation methods, but there is a wide range of other techniques available (e.g., McCaffrey et al., 2004)\nIn our first approach to estimating P (O|X,Xhid, Y ), we assume that dependencies from covariates X and Xhid and other ratings are negligible, reducing the model to P (Ou,i|Yu,i) similar to Marlin & Zemel (2009). Note that we can treat Yu,i as observed, since we only need to know the propensity for observed entries to compute IPS and SNIPS. This yields the Naive Bayes propensity estimator:\nP (Ou,i | Yu,i) = P (Yu,i | Ou,i)P (Ou,i)\nP (Yu,i) . (21)\nOf these quantities, we can estimate P (Yu,i|Ou,i = 1) and P (Ou,i) even from MNAR data. However, we do need extra information to estimate P (Yu,i), the proportion of different rating values in Y . This can be estimated accurately from even a small experimental (e.g. uniform) sample."}, {"heading": "5.2. Propensity Estimation via Logistic Regression", "text": "The second propensity estimation approach we explore in our experiments is based on logistic regression, as commonly used in statistics (Rosenbaum, 2002). It also starts from (20), but aims to find model parameters \u03c6 such that O becomes independent of unobserved Xhid and Y , i.e., P (O|X,Xhid, Y ) = P (O|X,\u03c6). The main modeling assumption is that there exists a \u03c6 = (w, \u03b2, \u03b3) such that Pu,i = \u03c3 ( wTXu,i + \u03b2i + \u03b3u ) . Here, Xu,i is a vector encoding all observable information about a user-movie pair (e.g., user demographics, whether a movie was promoted), and \u03c3(\u00b7) is the sigmoid function. \u03b2 and \u03b3 are per-movie and per-user offsets."}, {"heading": "6. Empirical Evaluation", "text": "We conduct extensive experiments to explore the empirical performance and robustness of the proposed methods in both the experimental and the observational setting. Furthermore, we compare against the state-of-theart joint-likelihood method for MNAR data (Herna\u0301ndezLobato et al., 2014) on real-world datasets."}, {"heading": "6.1. Experiment Setup", "text": "In all experiments, we perform model selection for the regularization parameter \u03bb and/or the rank of the factorization d via cross-validation as follows. We randomly split the observed MNAR ratings into four folds, training on three and evaluating on the remaining one using the IPS estimator. Note that reflecting this additional split in the propensities requires scaling the propensities in the training folds by k\u22121 k and those in the validation fold by 1 k . The parameters with the best average validation set performance are used to retrain over all MNAR data. We report test performance on the MCAR test samples that come with the real-world datasets, and via Equation (1) for our semi-synthetic data."}, {"heading": "6.2. How is evaluation affected by sampling bias severity?", "text": "In this experiment we evaluate how different observation distributions impact the accuracy of the performance estimators. We compare the Naive estimators for MSE (5), MAE (5) and DCG (10) with their propensity-weighted analogues, IPS (11) and SNIPS (14) respectively. Since this experiment requires experimental control of sampling bias, we created a semi-synthetic dataset and observation model.\nML100K Dataset. The ML100K dataset2 provides 100K MNAR ratings for 1683 movies by 944 users. To allow ground-truth evaluation against a fully known rating matrix, we complete these partial ratings using a standard matrix factorization with d = 100. The completed matrix, however, gives unrealistically high ratings to almost all movies. We therefore adjust ratings for the final Y to match a more realistic rating distribution [p1, p2, p3, p4, p5] for ratings 1 to 5 as given in Marlin & Zemel (2009) as follows: we assign the bottom p1 fraction of the entries by value in the completed matrix a rating of 1 and the next p2 fraction of entries by value a rating of 2 and so on. ML100K Observation Model. If the underlying rating is 4 or 5, the propensity for observing the rating is equal to k. For ratings r < 4, the corresponding propensity is k\u03b14\u2212r. For each \u03b1, k is set so that the expected number of ratings we observe is 1% of the entire matrix. By varying \u03b1 > 0, we vary the MNAR effect: \u03b1 = 1 is missing uniformly at random (MCAR), while \u03b1 \u2192 0 only reveals 4 and 5 rated items. Note that \u03b1 = 0.25 gives a marginal distribution of observed ratings that reasonably matches the observed MNAR rating marginals on ML100K ([0.06, 0.11, 0.27, 0.35, 0.21] in the real data vs. [0.06, 0.10, 0.25, 0.42, 0.17] in our model). Results. Table 1, described in Section 3.4, shows the estimated MAE and DCG@50 when \u03b1 = 0.25. Next, we vary the severity of the sampling bias by changing \u03b1 \u2208 (0, 1]. Figure 2 reports the root mean squared estimation error (RMSE) when estimating MSE, MAE, and DCG respectively. These results are for the Experimental Setting where propensities are known. They are averages over the five prediction matrices Y\u0302i given in Section 3.4 and across 50 trials. Shaded regions indicate a 95% confidence interval.\nOver most of the range of \u03b1, in particular for the realistic value of \u03b1 = 0.25, the IPS and SNIPS estimators are orders-of-magnitude more accurate than the conventional Naive estimator. Even for severely low choices of \u03b1, the gain in bias reduction of IPS and SNIPS still outweighs the added variability compared to Naive. Note that for \u03b1 = 1, SNIPS is algebraically equivalent to Naive, while IPS pays a small penalty due to increased variability from propensity\n2http://grouplens.org/datasets/movielens/\nweighting. For MSE and MAE, SNIPS consistently reduces estimation error over IPS while both are tied for DCG."}, {"heading": "6.3. How is learning affected by sampling bias severity?", "text": "The following experiment explores whether these gains in risk estimation accuracy translate into improved learning via ERM, again in the Experimental Setting. Using the same semi-synthetic ML100K dataset and observation model as above, we compare our matrix factorization MFIPS with the traditional unweighted matrix factorization MF-Naive. Both methods use the same factorization model with separate \u03bb selected via cross-validation and d = 20. The results are plotted in Figure 3 (left), where shaded regions indicate 95% confidence intervals over 40 trials. The propensity-weighted matrix factorization MF-IPS outperforms the conventional matrix factorization by an order of magnitude in terms of MSE. We also conducted experiments for MAE, and the results are similar. Somewhat surprisingly, learning appears even less affected by severe selection bias (i.e., small \u03b1) than evaluation. We conjecture that this is due to many of the critical Y\u0302 sharing the variability of the IPS estimates, such that their ranking stays fairly stable."}, {"heading": "6.4. How robust is evaluation and learning to inaccurately learned propensities?", "text": "We now switch from the Experimental Setting to the Observational Setting, where propensities need to be estimated. To explore robustness to propensity estimates of varying accuracy, we use the ML100K data and observation model with \u03b1 = 0.25. To generate increasingly bad propen-\nsity estimates, we use the Naive Bayes model from Section 5.1, but vary the size of the uniform sample that is available for estimating the marginal ratings P (Yu,i = r) via the Laplace estimator. For zero samples, we impose that P (Yu,i = r) = 1/5 for all r.\nFigure 4 shows how the quality of the propensity estimates impacts evaluation using the same setup as in Section 6.2. Under no condition do the IPS and SNIPS estimator perform worse than Naive. Interestingly, IPS-NB with estimated propensities can perform even better than IPS-KNOWN with known propensities, as can be seen for MSE. This effect is known, partly because the estimated propensities can provide an effect akin to stratification (Hirano et al., 2003; Wooldridge, 2007).\nFigure 3 (right) shows how learning performance is affected by inaccurate propensities. The figure compares the MSE prediction error of MF-IPS-NB with estimated propensities to those of MF-Naive and MF-IPS with known propensities. The shaded area shows the 95% confidence interval over 40 trials. Again, we see that MF-IPS-NB outperforms MF-Naive even for severely degraded propensity estimates, demonstrating the robustness of the approach."}, {"heading": "6.5. Performance on Real-World Data", "text": "Our final experiments concern performance on real-world datasets. We use the following two datasets, which both have a separate test set where users were asked to rate a uniformly drawn sample of items.\nYahoo! R3 Dataset. This dataset3 (Marlin & Zemel, 2009) contains user-song ratings. The MNAR training set provides over 300K ratings for songs that were selfselected by 15400 users. The test set contains ratings by a subset of 5400 users who were asked to rate 10 randomly chosen songs. For this data, we estimate propensities via Naive Bayes. As a uniform sample for eliciting the marginal rating distribution, we set aside 5% of the uniform sample and only report results on the remaining 95% of the test set.\n3http://webscope.sandbox.yahoo.com/\nCoat Shopping Dataset. We collected a new dataset4 simulating MNAR data of customers shopping for a coat in an online store. The training data was generated by giving Amazon Mechanical Turkers a simple web-shop interface with facets and paging. They were asked to find the coat in the store that they wanted to buy the most. Afterwards, they had to rate 24 of the coats they explored (self-selected) and 16 randomly picked ones on a five-point scale. The selfselected ratings are the training set, the uniformly selected ratings are the test set. We learn propensities via logistic regression based on user covariates (gender, age group, location, and fashion-awareness) and item covariates (gender, coat type, color, and was it promoted). A standard regularized logistic regression (Pedregosa et al., 2011) was trained using all pairs of user and item covariates as features and cross-validated to optimize log-likelihood of the self-selected observations.\nResults. Table 2 shows that our propensity-scored matrix factorization MF-IPS with learnt propensities substantially and significantly outperforms the conventional matrix factorization approach, as well as the Bayesian imputation models from (Herna\u0301ndez-Lobato et al., 2014), abbreviated as HL-MNAR and HL-MAR (paired t-test, p < 0.001 for all). This holds for both MAE and MSE. Furthermore, the performance of MF-IPS beats the best published results for Yahoo in terms of MSE (1.115) and is close in terms of MAE (0.770) (the CTP-v model of (Marlin & Zemel, 2009) as reported in the supplementary material of Herna\u0301ndezLobato et al. (2014)). For MF-IPS and MF-Naive all hyperparameters (i.e., \u03bb \u2208 {10\u22126, ..., 1} and d \u2208 {5, 10, 20, 40}) were chosen by cross-validation. For the HL baselines, we explored d \u2208 {5, 10, 20, 40} using software provided by the authors5 and report the best performance on the test set for efficiency reasons. Note that our performance numbers for HL on Yahoo closely match the values reported in (Herna\u0301ndez-Lobato et al., 2014).\nCompared to the complex HL models, we conclude that MF-IPS performs robustly and efficiently on real-world data. We conjecture that this strength is a result of not requiring any generative assumptions about the validity of\n4www.XXXXXXX.xxx/XXXXXX/XXXX/ 5https://bitbucket.org/jmh233/missingdataicml2014\nthe rating model. Furthermore, note that there are several promising directions for further improving performance, like propensity clipping (Strehl et al., 2010), doubly-robust estimation (Dud\u0131\u0301k et al., 2011), and the use of improved methods for propensity estimation (McCaffrey et al., 2004)."}, {"heading": "7. Conclusions", "text": "The paper proposed an effective and robust approach to handling selection bias in the evaluation and learning of recommender systems based on propensity scoring. The modularity of the approach\u2014separating the estimation of the assignment model from the rating model\u2014also makes it very practical. In particular, any conditional probability estimation method can be plugged in as the propensity estimator, and we conjecture that many existing rating models can be retrofit with propensity weighting without sacrificing scalability."}], "references": [{"title": "A statistical method for system evaluation using incomplete judgments", "author": ["Aslam", "Javed A", "Pavlu", "Virgil", "Yilmaz", "Emine"], "venue": "In SIGIR,", "citeRegEx": "Aslam et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Aslam et al\\.", "year": 2006}, {"title": "Chasing $1,000,000. how we won the netflix progress prize", "author": ["R. Bell", "Y. Koren", "C. Volinsky"], "venue": "Statistical Computing and Graphics,", "citeRegEx": "Bell et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bell et al\\.", "year": 2007}, {"title": "Learning deep architectures for ai", "author": ["Bengio", "Yoshua"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio and Yoshua.,? \\Q2009\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2009}, {"title": "Discriminative learning under covariate shift", "author": ["Bickel", "Steffen", "Br\u00fcckner", "Michael", "Scheffer", "Tobias"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "A limited memory algorithm for bound constrained optimization", "author": ["Byrd", "Richard H", "Lu", "Peihuang", "Nocedal", "Jorge", "Zhu", "Ciyou"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Byrd et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Byrd et al\\.", "year": 1995}, {"title": "Gradient descent optimization of smoothed information retrieval metrics", "author": ["Chapelle", "Olivier", "Wu", "Mingrui"], "venue": "Information retrieval,", "citeRegEx": "Chapelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2010}, {"title": "Sample selection bias correction theory", "author": ["Cortes", "Corinna", "Mohri", "Mehryar", "Riley", "Michael", "Rostamizadeh", "Afshin"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "Cortes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2008}, {"title": "Learning bounds for importance weighting", "author": ["Cortes", "Corinna", "Mansour", "Yishay", "Mohri", "Mehryar"], "venue": "In NIPS, pp", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "Modeling user exposure in recommendation", "author": ["Dawen Liang", "Laurent Charlin", "James McInerney", "Blei", "David"], "venue": "In WWW,", "citeRegEx": "Liang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2016}, {"title": "Reducing offline evaluation bias in recommendation systems", "author": ["De Myttenaere", "Arnaud", "Le Grand", "B\u00e9n\u00e9dicte", "Golden", "Boris", "Rossi", "Fabrice"], "venue": "In Benelearn,", "citeRegEx": "Myttenaere et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Myttenaere et al\\.", "year": 2014}, {"title": "Doubly robust policy evaluation and learning", "author": ["Dud\u0131\u0301k", "Miroslav", "Langford", "John", "Li", "Lihong"], "venue": "In ICML, pp", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "Large-scale matrix factorization with distributed stochastic gradient descent", "author": ["Gemulla", "Rainer", "Nijkamp", "Erik", "Haas", "Peter J", "Sismanis", "Yannis"], "venue": "In KDD, pp", "citeRegEx": "Gemulla et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gemulla et al\\.", "year": 2011}, {"title": "Probabilistic matrix factorization with non-random missing data", "author": ["Hern\u00e1ndez-Lobato", "Jose M", "Houlsby", "Neil", "Ghahramani", "Zoubin"], "venue": "In ICML, pp", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2014}, {"title": "Weighted average importance sampling and defensive mixture distributions", "author": ["Hesterberg", "Tim"], "venue": null, "citeRegEx": "Hesterberg and Tim.,? \\Q1995\\E", "shortCiteRegEx": "Hesterberg and Tim.", "year": 1995}, {"title": "Efficient estimation of average treatment effects using the estimated propensity", "author": ["K. Hirano", "G. Imbens", "G. Ridder"], "venue": "score. Econometrica,", "citeRegEx": "Hirano et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hirano et al\\.", "year": 2003}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Hu", "Yifan", "Koren", "Yehuda", "Volinsky", "Chris"], "venue": "In ICDM, pp", "citeRegEx": "Hu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2008}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["Huang", "Jiayuan", "Smola", "Alexander J", "Gretton", "Arthur", "Borgwardt", "Karsten M", "Sch\u00f6lkopf", "Bernhard"], "venue": "In NIPS, pp", "citeRegEx": "Huang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "Causal Inference for Statistics, Social, and Biomedical Sciences", "author": ["G. Imbens", "D. Rubin"], "venue": null, "citeRegEx": "Imbens and Rubin,? \\Q2015\\E", "shortCiteRegEx": "Imbens and Rubin", "year": 2015}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "In ICML, pp", "citeRegEx": "Joachims,? \\Q2005\\E", "shortCiteRegEx": "Joachims", "year": 2005}, {"title": "Factorization meets the neighborhood: A multifaceted collaborative filtering model", "author": ["Koren", "Yehuda"], "venue": "In KDD, pp", "citeRegEx": "Koren and Yehuda.,? \\Q2008\\E", "shortCiteRegEx": "Koren and Yehuda.", "year": 2008}, {"title": "Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms", "author": ["Li", "Lihong", "Chu", "Wei", "Langford", "John", "Wang", "Xuanhui"], "venue": "In WSDM,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Topn recommendation with missing implicit feedback", "author": ["Lim", "Daryl", "McAuley", "Julian", "Lanckriet", "Gert"], "venue": "In RecSys, pp", "citeRegEx": "Lim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2015}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["Lin", "Chih-Jen"], "venue": "Neural computation,", "citeRegEx": "Lin and Chih.Jen.,? \\Q2007\\E", "shortCiteRegEx": "Lin and Chih.Jen.", "year": 2007}, {"title": "Statistical Analysis with Missing Data", "author": ["R.J.A. Little", "D.B. Rubin"], "venue": "John Wiley,", "citeRegEx": "Little and Rubin,? \\Q2002\\E", "shortCiteRegEx": "Little and Rubin", "year": 2002}, {"title": "Collaborative prediction and ranking with non-random missing data", "author": ["Marlin", "Benjamin M", "Zemel", "Richard S"], "venue": "In RecSys, pp", "citeRegEx": "Marlin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Marlin et al\\.", "year": 2009}, {"title": "Collaborative filtering and the missing at random assumption", "author": ["Marlin", "Benjamin M", "Zemel", "Richard S", "Roweis", "Sam", "Slaney", "Malcolm"], "venue": "In UAI,", "citeRegEx": "Marlin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Marlin et al\\.", "year": 2007}, {"title": "Propensity score estimation with boosted regression for evaluating causal effects in observational studies", "author": ["McCaffrey", "Daniel F", "Ridgeway", "Greg", "Morral", "Andrew R"], "venue": "Psychological Methods,", "citeRegEx": "McCaffrey et al\\.,? \\Q2004\\E", "shortCiteRegEx": "McCaffrey et al\\.", "year": 2004}, {"title": "Ranking with non-random missing ratings: influence of popularity and positivity on evaluation metrics", "author": ["Pradel", "Bruno", "Usunier", "Nicolas", "Gallinari", "Patrick"], "venue": "In RecSys, pp", "citeRegEx": "Pradel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pradel et al\\.", "year": 2012}, {"title": "Observational Studies", "author": ["Rosenbaum", "Paul R"], "venue": null, "citeRegEx": "Rosenbaum and R.,? \\Q2002\\E", "shortCiteRegEx": "Rosenbaum and R.", "year": 2002}, {"title": "Alternatives to bpref", "author": ["Sakai", "Tetsuya"], "venue": "In SIGIR, pp. 71\u201378", "citeRegEx": "Sakai and Tetsuya.,? \\Q2007\\E", "shortCiteRegEx": "Sakai and Tetsuya.", "year": 2007}, {"title": "The strength of weak learnability", "author": ["Schapire", "Robert E"], "venue": "Machine Learning,", "citeRegEx": "Schapire and E.,? \\Q1990\\E", "shortCiteRegEx": "Schapire and E.", "year": 1990}, {"title": "Review of inverse probability weighting for dealing with missing data", "author": ["Seaman", "Shaun R", "White", "Ian R"], "venue": "Statistical methods in medical research,", "citeRegEx": "Seaman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Seaman et al\\.", "year": 2013}, {"title": "Training and testing of recommender systems on data missing not at random", "author": ["Steck", "Harald"], "venue": "In KDD,", "citeRegEx": "Steck and Harald.,? \\Q2010\\E", "shortCiteRegEx": "Steck and Harald.", "year": 2010}, {"title": "Learning from logged implicit exploration data", "author": ["Strehl", "Alexander L", "Langford", "John", "Li", "Lihong", "Kakade", "Sham"], "venue": "In NIPS, pp", "citeRegEx": "Strehl et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2010}, {"title": "Machine Learning in Non-Stationary Environments - Introduction to Covariate Shift Adaptation", "author": ["Sugiyama", "Masashi", "Kawanabe", "Motoaki"], "venue": null, "citeRegEx": "Sugiyama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2012}, {"title": "The self-normalized estimator for counterfactual learning", "author": ["A. Swaminathan", "T. Joachims"], "venue": "In NIPS,", "citeRegEx": "Swaminathan and Joachims,? \\Q2015\\E", "shortCiteRegEx": "Swaminathan and Joachims", "year": 2015}, {"title": "Conditional monte carlo for normal samples", "author": ["H.F. Trotter", "J.W. Tukey"], "venue": "In Symposium on Monte Carlo Methods,", "citeRegEx": "Trotter and Tukey,? \\Q1956\\E", "shortCiteRegEx": "Trotter and Tukey", "year": 1956}, {"title": "Maximum margin matrix factorization for collaborative ranking", "author": ["Weimer", "Markus", "Karatzoglou", "Alexandros", "Le", "Quoc Viet", "Smola", "Alex"], "venue": null, "citeRegEx": "Weimer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Weimer et al\\.", "year": 2007}, {"title": "Inverse probability weighted estimation for general missing data problems", "author": ["J. Wooldridge"], "venue": "Journal of Econometrics,", "citeRegEx": "Wooldridge,? \\Q2007\\E", "shortCiteRegEx": "Wooldridge", "year": 2007}, {"title": "A simple and efficient sampling method for estimating AP and NDCG", "author": ["Yilmaz", "Emine", "Kanoulas", "Evangelos", "Aslam", "Javed A"], "venue": "In SIGIR,", "citeRegEx": "Yilmaz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yilmaz et al\\.", "year": 2008}, {"title": "Scalable coordinate descent approaches to parallel matrix factorization for recommender systems", "author": ["Yu", "Hsiang-Fu", "Hsieh", "Cho-Jui", "I Dhillon"], "venue": "In ICDM,", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}, {"title": "Costsensitive learning by cost-proportionate example weighting", "author": ["Zadrozny", "Bianca", "Langford", "John", "Abe", "Naoki"], "venue": "In ICDM, pp", "citeRegEx": "Zadrozny et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zadrozny et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 27, "context": "In a movie recommendation system, for example, users typically watch and rate those movies that they like, and rarely rate movies that they do not like (Pradel et al., 2012).", "startOffset": 152, "endOffset": 173}, {"referenceID": 6, "context": "First, we show how estimating the quality of a recommendation system can be approached with propensity-weighting techniques commonly used in causal inference (Imbens & Rubin, 2015), complete-cases analysis (Little & Rubin, 2002), and other problems (Cortes et al., 2008; Bickel et al., 2009; Sugiyama & Kawanabe, 2012).", "startOffset": 249, "endOffset": 318}, {"referenceID": 3, "context": "First, we show how estimating the quality of a recommendation system can be approached with propensity-weighting techniques commonly used in causal inference (Imbens & Rubin, 2015), complete-cases analysis (Little & Rubin, 2002), and other problems (Cortes et al., 2008; Bickel et al., 2009; Sugiyama & Kawanabe, 2012).", "startOffset": 249, "endOffset": 318}, {"referenceID": 1, "context": "standard estimators commonly used in the past (Bell et al., 2007).", "startOffset": 46, "endOffset": 65}, {"referenceID": 12, "context": "For the task of learning recommender systems, we show that our new matrix factorization method substantially outperforms methods that ignore selection bias, as well as existing state-of-the-art methods that perform jointlikelihood inference under MNAR data (Hern\u00e1ndez-Lobato et al., 2014).", "startOffset": 257, "endOffset": 288}, {"referenceID": 25, "context": "Past work that explicitly dealt with the MNAR nature of recommendation data approached the problem as missingdata imputation based on the joint likelihood of the missing data model and the rating model (Marlin et al., 2007; Marlin & Zemel, 2009; Hern\u00e1ndez-Lobato et al., 2014).", "startOffset": 202, "endOffset": 276}, {"referenceID": 12, "context": "Past work that explicitly dealt with the MNAR nature of recommendation data approached the problem as missingdata imputation based on the joint likelihood of the missing data model and the rating model (Marlin et al., 2007; Marlin & Zemel, 2009; Hern\u00e1ndez-Lobato et al., 2014).", "startOffset": 202, "endOffset": 276}, {"referenceID": 12, "context": "We empirically compare against the state-of-the-art joint likelihood model (Hern\u00e1ndez-Lobato et al., 2014) in this paper.", "startOffset": 75, "endOffset": 106}, {"referenceID": 15, "context": "Related but different from the problem we consider is recommendation from positive feedback alone (Hu et al., 2008; Dawen Liang & Blei, 2016).", "startOffset": 98, "endOffset": 141}, {"referenceID": 21, "context": "Alternative approaches to learning with MNAR data (Steck, 2010; Lim et al., 2015) aim to avoid the problem by considering performance measures less affected by selection bias.", "startOffset": 50, "endOffset": 81}, {"referenceID": 3, "context": "Weighting approaches are also widely used in domain adaptation and covariate shift, where data from one source is used to train for a different problem (e.g., Huang et al., 2006; Bickel et al., 2009; Sugiyama & Kawanabe, 2012).", "startOffset": 152, "endOffset": 226}, {"referenceID": 6, "context": "We will draw upon this work, especially the learning theory related to weighting approaches in (Cortes et al., 2008; 2010).", "startOffset": 95, "endOffset": 122}, {"referenceID": 0, "context": "Hence, we are faced with the following counterfactual question: how well would our users have enjoyed themselves (in terms of CG), if they had followed our recommendations \u0176 instead of watching the movies indicated in O? Note that rankings of recommendations are similar to the set-based recommendation described above, and measures like Discounted Cumulative Gain (DCG), DCG@k, Precision at k (PREC@k), and others (Aslam et al., 2006; Yilmaz et al., 2008) also fit in this setting.", "startOffset": 415, "endOffset": 456}, {"referenceID": 39, "context": "Hence, we are faced with the following counterfactual question: how well would our users have enjoyed themselves (in terms of CG), if they had followed our recommendations \u0176 instead of watching the movies indicated in O? Note that rankings of recommendations are similar to the set-based recommendation described above, and measures like Discounted Cumulative Gain (DCG), DCG@k, Precision at k (PREC@k), and others (Aslam et al., 2006; Yilmaz et al., 2008) also fit in this setting.", "startOffset": 415, "endOffset": 456}, {"referenceID": 20, "context": "The key to getting unbiased counterfactual estimates of recommendation quality despite missing observations lies in the following connection to estimating average treatment effects of a given policy in causal inference, already explored in the contextual bandit setting (Li et al., 2011; Dud\u0131\u0301k et al., 2011).", "startOffset": 270, "endOffset": 308}, {"referenceID": 10, "context": "The key to getting unbiased counterfactual estimates of recommendation quality despite missing observations lies in the following connection to estimating average treatment effects of a given policy in causal inference, already explored in the contextual bandit setting (Li et al., 2011; Dud\u0131\u0301k et al., 2011).", "startOffset": 270, "endOffset": 308}, {"referenceID": 41, "context": "Empirical Risk Minimization underlies many successful learning algorithms like SVMs (Cortes & Vapnik, 1995), Boosting (Schapire, 1990), and Deep Networks (Bengio, 2009), and weighted ERM approaches have been effective for cost-sensitive classification, domain adaptation and covariate shift (Zadrozny et al., 2003; Bickel et al., 2009; Sugiyama & Kawanabe, 2012).", "startOffset": 291, "endOffset": 362}, {"referenceID": 3, "context": "Empirical Risk Minimization underlies many successful learning algorithms like SVMs (Cortes & Vapnik, 1995), Boosting (Schapire, 1990), and Deep Networks (Bengio, 2009), and weighted ERM approaches have been effective for cost-sensitive classification, domain adaptation and covariate shift (Zadrozny et al., 2003; Bickel et al., 2009; Sugiyama & Kawanabe, 2012).", "startOffset": 291, "endOffset": 362}, {"referenceID": 6, "context": "To illustrate the validity of the propensity-scored ERM approach, we state the following generalization error bound (proof in appendix) similar to Cortes et al. (2010). We consider only finiteH for the sake of conciseness.", "startOffset": 147, "endOffset": 168}, {"referenceID": 15, "context": "Except for the propensities Pu,i that act like weights for each loss term, the training objective is identical to the standard incomplete matrix factorization objective (Koren, 2008; Steck, 2010; Hu et al., 2008) with MSE (3) or MAE (2).", "startOffset": 169, "endOffset": 212}, {"referenceID": 40, "context": "This means that we can readily draw upon the existing arsenal of optimization algorithms (i.e., Gemulla et al., 2011; Yu et al., 2012), making the training problem highly scalable.", "startOffset": 89, "endOffset": 134}, {"referenceID": 4, "context": "For the experiments reported in this paper, we use Limited-memory BFGS (Byrd et al., 1995).", "startOffset": 71, "endOffset": 90}, {"referenceID": 18, "context": "Solving this training objective for other \u03b4u,i(Y, \u0176 ) that are non-differentiable is more challenging, but possible avenues exist (Joachims, 2005; Chapelle & Wu, 2010).", "startOffset": 130, "endOffset": 167}, {"referenceID": 26, "context": ", McCaffrey et al., 2004) In our first approach to estimating P (O|X,Xhid, Y ), we assume that dependencies from covariates X and X and other ratings are negligible, reducing the model to P (Ou,i|Yu,i) similar to Marlin & Zemel (2009). Note that we can treat Yu,i as observed, since we only need to know the propensity for observed entries to compute IPS and SNIPS.", "startOffset": 2, "endOffset": 235}, {"referenceID": 14, "context": "This effect is known, partly because the estimated propensities can provide an effect akin to stratification (Hirano et al., 2003; Wooldridge, 2007).", "startOffset": 109, "endOffset": 148}, {"referenceID": 38, "context": "This effect is known, partly because the estimated propensities can provide an effect akin to stratification (Hirano et al., 2003; Wooldridge, 2007).", "startOffset": 109, "endOffset": 148}, {"referenceID": 12, "context": "Table 2 shows that our propensity-scored matrix factorization MF-IPS with learnt propensities substantially and significantly outperforms the conventional matrix factorization approach, as well as the Bayesian imputation models from (Hern\u00e1ndez-Lobato et al., 2014), abbreviated as HL-MNAR and HL-MAR (paired t-test, p < 0.", "startOffset": 233, "endOffset": 264}, {"referenceID": 12, "context": "Note that our performance numbers for HL on Yahoo closely match the values reported in (Hern\u00e1ndez-Lobato et al., 2014).", "startOffset": 87, "endOffset": 118}, {"referenceID": 12, "context": "Table 2 shows that our propensity-scored matrix factorization MF-IPS with learnt propensities substantially and significantly outperforms the conventional matrix factorization approach, as well as the Bayesian imputation models from (Hern\u00e1ndez-Lobato et al., 2014), abbreviated as HL-MNAR and HL-MAR (paired t-test, p < 0.001 for all). This holds for both MAE and MSE. Furthermore, the performance of MF-IPS beats the best published results for Yahoo in terms of MSE (1.115) and is close in terms of MAE (0.770) (the CTP-v model of (Marlin & Zemel, 2009) as reported in the supplementary material of Hern\u00e1ndezLobato et al. (2014)).", "startOffset": 234, "endOffset": 630}, {"referenceID": 33, "context": "Furthermore, note that there are several promising directions for further improving performance, like propensity clipping (Strehl et al., 2010), doubly-robust estimation (Dud\u0131\u0301k et al.", "startOffset": 122, "endOffset": 143}, {"referenceID": 10, "context": ", 2010), doubly-robust estimation (Dud\u0131\u0301k et al., 2011), and the use of improved methods for propensity estimation (McCaffrey et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 26, "context": ", 2011), and the use of improved methods for propensity estimation (McCaffrey et al., 2004).", "startOffset": 67, "endOffset": 91}], "year": 2017, "abstractText": "Most data for evaluating and training recommender systems is subject to selection biases, either through self-selection by the users or through the actions of the recommendation system itself. In this paper, we provide a principled approach to handling selection biases, adapting models and estimation techniques from causal inference. The approach leads to unbiased performance estimators despite biased data, and to a matrix factorization method that provides substantially improved prediction performance on real-world data. We theoretically and empirically characterize the robustness of the approach, finding that it is highly practical and scalable.", "creator": "LaTeX with hyperref package"}}}