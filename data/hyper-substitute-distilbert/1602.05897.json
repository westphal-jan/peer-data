{"id": "1602.05897", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2016", "title": "Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity", "abstract": "we develop simple general difference between differential networks generating composite representations, striving towards a better formulation of deep emotions. we found that adaptive representations signaled by common random signals are sufficiently meaningful to express all messages in the dual kernel space. initially, though the training objective is hard to optimize in the modern application, many initial interpretations form the valid learning condition for optimization. this dual view largely reveals a complex human aesthetic perspective of nonlinear networks significantly underscores their expressive interpretations.", "histories": [["v1", "Thu, 18 Feb 2016 18:14:19 GMT  (52kb,D)", "http://arxiv.org/abs/1602.05897v1", null], ["v2", "Fri, 19 May 2017 18:39:00 GMT  (61kb,D)", "http://arxiv.org/abs/1602.05897v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CC cs.DS stat.ML", "authors": ["amit daniely", "roy frostig", "yoram singer"], "accepted": true, "id": "1602.05897"}, "pdf": {"name": "1602.05897.pdf", "metadata": {"source": "CRF", "title": "Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity", "authors": ["Amit Daniely", "Roy Frostig", "Yoram Singer"], "emails": ["amitdaniely@google.com", "rf@cs.stanford.edu.", "singer@google.com"], "sections": [{"heading": null, "text": "\u2217Email: amitdaniely@google.com \u2020Email: rf@cs.stanford.edu. Work performed at Google. \u2021Email: singer@google.com\nar X\niv :1\n60 2.\n05 89\n7v 1\n[ cs\nContents"}, {"heading": "1 Introduction 1", "text": ""}, {"heading": "2 Related work 2", "text": ""}, {"heading": "3 Setting 3", "text": ""}, {"heading": "4 Computation skeletons 4", "text": "4.1 From computation skeletons to neural networks . . . . . . . . 6 4.2 From computation skeletons to reproducing kernels . . . . . . 7"}, {"heading": "5 Main results 9", "text": ""}, {"heading": "6 Mathematical background 11", "text": ""}, {"heading": "7 Compositional kernel spaces 12", "text": ""}, {"heading": "8 The dual activation function 14", "text": ""}, {"heading": "9 Proofs 19", "text": "9.1 Well-behaved activations . . . . . . . . . . . . . . . . . . . . . 19 9.2 Proofs of Thms. 2 and 3 . . . . . . . . . . . . . . . . . . . . . 22 9.3 Proofs of Thms. 4 and 5 . . . . . . . . . . . . . . . . . . . . . 25\n10 Discussion 28"}, {"heading": "1 Introduction", "text": "Neural network (NN) learning has underpinned state of the art empirical results in numerous applied machine learning tasks (see for instance [31, 33]). Nonetheless, neural network learning remains rather poorly understood in several regards. Notably, it remains unclear why training algorithms find good weights, how learning is impacted by the network architecture and activations, what is the role of random weight initialization, and how to choose a concrete optimization procedure for a given architecture.\nWe start by analyzing the expressive power of NNs subsequent to the random weight initialization. The motivation is the empirical success of training algorithms despite inherent computational intractability, and the fact that they optimize highly non-convex objectives with potentially many local minima. Our key result shows that random initialization already positions learning algorithms at a good starting point. We define an object termed a computation skeleton that describes a distilled structure of feed-forward networks. A skeleton induces a family of network architectures along with a hypothesis class H of functions obtained by certain non-linear compositions according to the skeleton\u2019s structure. We show that the representation generated by random initialization is sufficiently rich to approximately express the functions in H. Concretely, all functions in H can be approximated by tuning the weights of the last layer, which is a convex optimization task.\nIn addition to explaining in part the success in finding good weights, our study provides an appealing perspective on neural network learning. We establish a tight connection between network architectures and their dual kernel spaces. This connection generalizes several previous constructions (see Sec 2). As we demonstrate, our dual view gives rise to design principles for NNs, supporting current practice and suggesting new ideas. We outline below a few points.\n\u2022 Duals of convolutional networks appear a more suitable fit for vision and acoustic tasks than those of fully connected networks.\n\u2022 Our framework surfaces a principled initialization scheme. It is very similar to common practice, but incorporates a small correction.\n\u2022 By modifying the activation functions, two consecutive fully connected layers can be replaced with one while preserving the network\u2019s dual kernel.\n\u2022 The ReLU activation, i.e. x 7\u2192 max(x, 0), possesses favorable properties. Its dual kernel is expressive, and it can be well approximated by random initialization, even when the initialization\u2019s scale is moderately changed.\n\u2022 As the number of layers in a fully connected network becomes very large, its dual kernel converges to a degenerate form for any non-linear activation.\n\u2022 Our result suggests that optimizing the weights of the last layer can serve as a convex proxy for choosing among different architectures prior to training. This idea was advocated and tested empirically in [49]."}, {"heading": "2 Related work", "text": "Current theoretical understanding of NN learning. Understanding neural network learning, particularly its recent successes, commonly decomposes into the following research questions.\n(i) What functions can be efficiently expressed by neural networks?\n(ii) When does a low empirical loss result in a low population loss?\n(iii) Why and when do efficient algorithms, such as stochastic gradient, find good weights?\nThough still far from being complete, previous work provides some understanding of questions (i) and (ii). Standard results from complexity theory [28] imply that essentially all functions of interest (that is, any efficiently computable function) can be expressed by a network of moderate size. Biological phenomena show that many relevant functions can be expressed by even simpler networks, similar to convolutional neural networks [32] that are dominant in ML tasks today. Barron\u2019s theorem [7] states that even two-layer networks can express a very rich set of functions. As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss. In contrast to the first two, question (iii) is rather poorly understood. While learning algorithms succeed in practice, theoretical analysis is overly pessimistic. Direct interpretation of theoretical results suggests that when going slightly deeper beyond single layer networks, e.g. to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16]. Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].\nCompositional kernels and connections to networks. The idea of composing kernels has repeatedly appeared throughout the machine learning literature, for instance in early work by Scho\u0308lkopf et al. [51], Grauman and Darrell [21]. Inspired by deep networks\u2019 success, researchers considered deep composition of kernels [36, 13, 11]. For fully connected two-layer networks, the correspondence between kernels and neural networks with random weights has been examined in [46, 45, 38, 56]. Notably, Rahimi and Recht [46] proved a formal connection (similar to ours) for the RBF kernel. Their work was extended to include polynomial kernels [27, 42] as well as other kernels [6, 5]. Several authors have further explored ways to extend this line of research to deeper, either fully-connected networks [13] or convolutional networks [24, 2, 36]. Our work sets a common foundation for and expands on these ideas. We extend the analysis from fully-connected and convolutional networks to a rather broad family of architectures. In addition, we prove approximation guarantees between a network and its corresponding kernel in our more general setting. We thus extend previous analyses that only applies to fully connected two-layer networks. Finally, we use the connection as an analytical tool to reason about architectural design choices."}, {"heading": "3 Setting", "text": "Notation. We denote vectors by bold-face letters (e.g. x), and matrices by upper case Greek letters (e.g. \u03a3). The 2-norm of x \u2208 Rd is denoted by \u2016x\u2016. For functions \u03c3 : R \u2192 R we let\n\u2016\u03c3\u2016 := \u221a EX\u223cN (0,1) \u03c32(X) = \u221a 1\u221a 2\u03c0 \u222b\u221e \u2212\u221e \u03c3 2(x)e\u2212 x2 2 dx .\nLet G = (V,E) be a directed acyclic graph. The set of neighbors incoming to a vertex v is denoted in(v) := {u \u2208 V | uv \u2208 E}. The d \u2212 1 dimensional sphere is denoted Sd\u22121 = {x \u2208 Rd | \u2016x\u2016 = 1}. We provide a brief overview of reproducing kernel Hilbert spaces in the sequel and merely introduce notation here. In a Hilbert space H, we use a slightly non-standard notation HB for the ball of radius B, {x \u2208 H | \u2016x\u2016H \u2264 B}. We use [x]+ to denote max(x, 0) and 1[b] to denote the indicator function of a binary variable b.\nInput space. Throughout the paper we assume that each example is a sequence of n elements, each of which is represented as a unit vector. Namely, we fix n and take the input space to be X = Xn,d = ( Sd\u22121 )n . Each input example is denoted,\nx = (x1, . . . ,xn), where xi \u2208 Sd\u22121 . (1)\nWe refer to each vector xi as the input\u2019s ith coordinate, and use xij to denote it jth scalar entry. Though this notation is slightly non-standard, it unifies input types seen in various domains. For example, binary features can be encoded by taking d = 1, in which case X = {\u00b11}n. Meanwhile, images and audio signals are often represented as bounded and continuous numerical values\u2014we can assume in full generality that these values lie in [\u22121, 1]. To match the setup above, we embed [\u22121, 1] into the circle S1, e.g. via the map x 7\u2192( sin ( \u03c0x 2 ) , cos ( \u03c0x 2 )) . When each coordinate is categorical\u2014taking one of d values\u2014we can represent category j \u2208 [d] by the unit vector ej \u2208 Sd\u22121. When d may be very large or the basic units exhibits some structure, such as when the input is a sequence of words, a more concise encoding may be useful, e.g. as unit vectors in a low dimension space Sd\u2032 where d\u2032 d (see for instance Mikolov et al. [37], Levy and Goldberg [34]).\nSupervised learning. The goal in supervised learning is to devise a mapping from the input space X to an output space Y based on a sample S = {(x1, y1), . . . , (xm, ym)}, where (xi, yi) \u2208 X \u00d7 Y , drawn i.i.d. from a distribution D over X \u00d7 Y . A supervised learning problem is further specified by an output length k and a loss function ` : Rk \u00d7 Y \u2192 [0,\u221e), and the goal is to find a predictor h : X \u2192 Rk whose loss, LD(h) := E(x,y)\u223cD `(h(x), y), is small. The empirical loss LS(h) := 1m \u2211m i=1 `(h(xi), yi) is commonly used as a proxy for the loss LD. Regression problems correspond to Y = R and, for instance, the squared loss `(y\u0302, y) = (y\u0302 \u2212 y)2. Binary classification is captured by Y = {\u00b11} and, say, the zero-one loss `(y\u0302, y) = 1[y\u0302y \u2264 0] or the hinge loss `(y\u0302, y) = [1 \u2212 y\u0302y]+, with standard extensions to the multiclass case. A loss ` is L-Lipschitz if |`(y1, y)\u2212 `(y2, y)| \u2264 L|y1 \u2212 y2| for all y1, y2 \u2208 Rk, y \u2208 Y , and it is convex if `(\u00b7, y) is convex for every y \u2208 Y .\nNeural network learning. We define a neural network N to be directed acyclic graph (DAG) whose nodes are denoted V (N ) and edges E(N ). Each of its internal units, i.e. nodes with both incoming and outgoing edges, is associated with an activation function \u03c3v : R\u2192 R. In this paper\u2019s context, an activation can be any function that is square integrable with respect to the Gaussian measure on R. We say that \u03c3 is normalized if \u2016\u03c3\u2016 = 1. The set of nodes having only incoming edges are called the output nodes. To match the setup of a supervised learning problem, a network N has nd input nodes and k output nodes, denoted o1, . . . , ok. A network N together with a weight vector w = {wuv | uv \u2208 E} defines a predictor hN ,w : X \u2192 Rk whose prediction is given by \u201cpropagating\u201d x forward through the network. Formally, we define hv,w(\u00b7) to be the output of the subgraph of the node v as follows: for an input node v, hv,w is the identity function, and for all other nodes, we define hv,w recursively as\nhv,w(x) = \u03c3v (\u2211 u\u2208in(v) wuv hu,w(x) ) .\nFinally, we let hN ,w(x) = (ho1,w(x), . . . , hok,w(x)). We also refer to internal nodes as hidden units. The output layer of N is the sub-network consisting of all output neurons of N along with their incoming edges. The representation induced by a network N is the network rep(N ) obtained from N by removing the output layer. The representation function induced by the weights w is RN ,w := hrep(N ),w. Given a sample S, a learning algorithm searches for weights w having small empirical loss LS(w) = 1m \u2211m i=1 `(hN ,w(xi), yi). A popular approach is to randomly initialize the weights and then use a variant of the stochastic gradient method to improve these weights in the direction of lower empirical loss.\nKernel learning. A function \u03ba : X \u00d7 X \u2192 R is a reproducing kernel, or simply a kernel, if for every x1, . . . ,xr \u2208 X , the r \u00d7 r matrix \u0393i,j = {\u03ba(xi,xj)} is positive semi-definite. Each kernel induces a Hilbert space H\u03ba of functions from X to R with a corresponding norm \u2016 \u00b7 \u2016H\u03ba . A kernel and its corresponding space are normalized if \u2200x \u2208 X , \u03ba(x,x) = 1. Given a convex loss function `, a sample S, and a kernel \u03ba, a kernel learning algorithm finds a function f = (f1, . . . , fk) \u2208 Hk\u03ba whose empirical loss, LS(f) = 1m \u2211 i `(f(xi), yi), is minimal\namong all functions with \u2211\ni \u2016fi\u20162\u03ba \u2264 R2 for some R > 0. Alternatively, kernel algorithms minimize the regularized loss,\nLRS (f) = 1\nm m\u2211 i=1 `(f(xi), yi) + 1 R2 k\u2211 i=1 \u2016fi\u20162\u03ba ,\na convex objective that often can be efficiently minimized."}, {"heading": "4 Computation skeletons", "text": "In this section we define a simple structure which we term a computation skeleton. The purpose of a computational skeleton is to compactly describe a feed-forward computation from an input to an output. A single skeleton encompasses a family of neural networks that share the same skeletal structure. Likewise, it defines a corresponding kernel space.\nDefinition. A computation skeleton S is a DAG whose non-input nodes are labeled by activations.\nThough the formal definition of neural networks and skeletons appear identical, we make a conceptual distinction between them as their role in our analysis is rather different. Accompanied by a set of weights, a neural network describes a concrete function, whereas the skeleton stands for a topology common to several networks as well as for a kernel. To further underscore the differences we note that skeletons are naturally more compact than networks. In particular, all examples of skeletons in this paper are irreducible, meaning that for each two nodes v, u \u2208 V (S), in(v) 6= in(u). We further restrict our attention to skeletons with a single output node, showing later that single-output skeletons can capture supervised problems with outputs in Rk. We denote by |S| the number of non-input nodes of S.\nFigure 1 shows four example skeletons, omitting the designation of the activation functions. The skeleton S1 is rather basic as it aggregates all the inputs in a single step. Such topology can be useful in the absence of any prior knowledge of how the output label may be computed from an input example, and it is commonly used in natural language processing where the input is represented as a bag-of-words [23]. The only structure in S1 is a single fully connected layer:\nTerminology (Fully connected layer of a skeleton). An induced subgraph of a skeleton with r + 1 nodes, u1, . . . , ur, v, is called a fully connected layer if its edges are u1v, . . . , urv.\nThe skeleton S2 is slightly more involved: it first processes consecutive (overlapping) parts of the input, and the next layer aggregates the partial results. Altogether, it corresponds to networks with a single one-dimensional convolutional layer, followed by a fully connected layer. The two-dimensional (and deeper) counterparts of such skeletons correspond to networks that are common in visual object recognition.\nTerminology (Convolution layer of a skeleton). Let s, w, q be positive integers and denote n = s(q \u2212 1) + w. A subgraph of a skeleton is a one dimensional convolution layer of width w and stride s if it has n + q nodes, u1, . . . , un, v1, . . . , vq, and qw edges, us(i\u22121)+j vi, for 1 \u2264 i \u2264 q, 1 \u2264 j \u2264 w.\nThe skeleton S3 is a somewhat more sophisticated version of S2: the local computations are first aggregated, then reconsidered with the aggregate, and finally aggregated again. The last skeleton, S4, corresponds to the networks that arise in learning sequenceto-sequence mappings as used in translation, speech recognition, and OCR tasks (see for example Sutskever et al. [55])."}, {"heading": "4.1 From computation skeletons to neural networks", "text": "The following definition shows how a skeleton, accompanied with a replication parameter r \u2265 1 and a number of output nodes k, induces a neural network architecture. Recall that inputs are ordered sets of vectors in Sd\u22121.\nDefinition (Realization of a skeleton). Let S be a computation skeleton and consider input coordinates in Sd\u22121 as in (1). For r, k \u2265 1 we define the following neural network N = N (S, r, k). For each input node in S, N has d corresponding input neurons. For each internal node v \u2208 S labeled by an activation \u03c3, N has r neurons v1, . . . , vr, each with an activation \u03c3. In addition, N has k output neurons o1, . . . , ok with the identity activation \u03c3(x) = x. There is an edge viuj \u2208 E(N ) whenever uv \u2208 E(S). For every output node v in S, each neuron vj is connected to all output neurons o1, . . . , ok. We term N the (r, k)-fold realization of S. We also define the r-fold realization of S as1 N (S, r) = rep (N (S, r, 1)).\nNote that the notion of the replication parameter r corresponds, in the terminology of convolutional networks, to the number of channels taken in a convolutional layer and to the number of hidden units taken in a fully-connected layer.\nFigure 2 illustrates a (5, 4)- and 5-realizations of a skeleton with coordinate dimension d = 2. The (5, 4)-realization is a network with a single (one dimensional) convolutional layer having 5 channels, stride of 2, and width of 4, followed by three fully-connected layers. The global replication parameter r in a realization is used for brevity; it is straightforward to extend results when the different nodes in S are each replicated to a different extent.\nWe next define a scheme for random initialization of the weights of a neural network, that is similar to what is often done in practice. We employ the definition throughout the paper whenever we refer to random weights.\n1Note that for every k, rep (N (S, r, 1)) = rep (N (S, r, k)).\nDefinition (Random weights). A random initialization of a neural network N is a multivariate Gaussian w = (wuv)uv\u2208E(N ) such that each weight wuv is sampled independently from a normal distribution with mean 0 and variance 1/(\u2016\u03c3u\u20162 |in(v)|).\nArchitectures such as convolutional nets have weights that are shared across different edges. Again, it is straightforward to extend our results to these cases and for simplicity we assume no explicit weight sharing."}, {"heading": "4.2 From computation skeletons to reproducing kernels", "text": "In addition to networks\u2019 architectures, a computation skeleton S also defines a normalized kernel \u03baS : X \u00d7X \u2192 [\u22121, 1] and a corresponding norm \u2016 \u00b7 \u2016S on functions f : X \u2192 R. This norm has the property that \u2016f\u2016S is small if and only if f can be obtained by certain simple compositions of functions according to the structure of S. To define the kernel, we introduce a dual activation and dual kernel. For \u03c1 \u2208 [\u22121, 1], we denote by N\u03c1 the multivariate Gaussian distribution on R2 with mean 0 and covariance matrix ( 1 \u03c1 \u03c1 1 ) .\nDefinition (Dual activation and kernel). The dual activation of an activation \u03c3 is the function \u03c3\u0302 : [\u22121, 1]\u2192 R defined as\n\u03c3\u0302(\u03c1) = E (X,Y )\u223cN\u03c1 \u03c3(X)\u03c3(Y ) .\nThe dual kernel w.r.t. to a Hilbert space H is the kernel \u03ba\u03c3 : H1 \u00d7H1 \u2192 R defined as\n\u03ba\u03c3(x,y) = \u03c3\u0302(\u3008x,y\u3009H) .\nSection 7 shows that \u03ba\u03c3 is indeed a kernel for every activation \u03c3 that adheres with the square-integrability requirement. In fact, any continuous \u00b5 : [\u22121, 1]\u2192 R, such that (x,y) 7\u2192 \u00b5(\u3008x,y\u3009H) is a kernel for all H, is the dual of some activation. Note that \u03ba\u03c3 is normalized iff \u03c3 is normalized. We show in Section 8 that dual activations are closely related to Hermite polynomial expansions, and that these can be used to calculate the duals of activation\nfunctions analytically. Table 1 lists a few examples of normalized activations and their corresponding dual (corresponding derivations are in Section 8). The following definition gives the kernel corresponding to a skeleton having normalized activations.2\nDefinition (Compositional kernels). Let S be a computation skeleton with normalized activations and (single) output node o. For every node v, inductively define a kernel \u03bav : X\u00d7X \u2192 R as follows. For an input node v corresponding to the ith coordinate, define \u03bav(x,y) = \u3008xi,yi\u3009. For a non-input node v, define\n\u03bav(x,y) = \u03c3\u0302v\n(\u2211 u\u2208in(v) \u03bau(x,y)\n|in(v)|\n) .\nThe final kernel \u03baS is \u03bao, the kernel associated with the output node o. The resulting Hilbert space and norm are denoted HS and \u2016 \u00b7 \u2016S respectively, and Hv and \u2016 \u00b7 \u2016v denote the space and norm when formed at node v.\nAs we show later, \u03baS is indeed a (normalized) kernel for every skeleton S. To understand the kernel in the context of learning, we need to examine which functions can be expressed as moderate norm functions in HS . As we show in section 7, these are the functions obtained by certain simple compositions according to the feed-forward structure of S. For intuition, the following example contrasts two commonly used skeletons.\nExample 1 (Convolutional vs. fully connected skeletons). Consider a network whose activations are all ReLU, \u03c3(z) = [z]+, and an input space Xn,1 = {\u00b11}n. Say that S1 is a skeleton comprising a single fully connected layer, and that S2 is one comprising a convolutional layer of stride 1 and width q = log0.999(n), followed by a single fully-connected layer. (The skeleton S2 from Figure 1 is a concrete example of the convolutional skeleton with q = 2 and n = 4.) The kernel \u03baS1 takes the form \u03baS1(x,y) = \u03c3\u0302 (\u3008x,y\u3009/n). It is a symmetric kernel and therefore functions with small norm in HS1 are essentially low-degree polynomials. For instance, fix a\n2For a skeleton with unnormalized activations, the corresponding kernel is the kernel of the skeleton S \u2032 obtained by normalizing the activations of S.\nbound R = n1.001 on the norm of the functions. In this case, the space HRS1 contains multiplication of one or two input coordinates. However, multiplication of 3 or more coordinates are no-longer in HRS1 . Moreover, this property holds true regardless of the choice of activation function. On the other hand, HRS2 contains functions whose dependence on adjacent input coordinates is far more complex. It includes, for instance, any function f : X \u2192 {\u00b11} that is symmetric (i.e. f(x) = f(\u2212x)) and that depends on q adjacent coordinates xi, . . . ,xi+q. Furthermore, any sum of n such functions is also in HRS2 ."}, {"heading": "5 Main results", "text": "We review our main results. Let us fix a compositional kernel S. There are a few upshots to underscore upfront. First, our analysis implies that a representation generated by a random initialization of N = N (S, r, k) approximates the kernel \u03baS . The sense in which the result holds is twofold. First, with the proper rescaling we show that \u3008RN ,w(x),RN ,w(x\u2032)\u3009 \u2248 \u03baS(x,x\n\u2032). Then, we also show that the functions obtained by composing bounded linear functions with RN ,w are approximately the bounded-norm functions in HS . In other words, the functions expressed by N under varying the weights of the last layer are approximately bounded-norm functions in HS . For simplicity, we restrict the analysis to the case k = 1. We also confine the analysis to either bounded activations, with bounded first and second derivatives, or the ReLU activation. Extending the results to a broader family of activations is left for future work. Through this and remaining sections we use & to hide universal constants.\nDefinition. An activation \u03c3 : R\u2192 R is C-bounded if it is twice continuously differentiable and \u2016\u03c3\u2016\u221e, \u2016\u03c3\u2032\u2016\u221e, \u2016\u03c3\u2032\u2032\u2016\u221e \u2264 \u2016\u03c3\u2016C.\nNote that many activations are C-bounded for some constant C > 0. In particular, most of the popular sigmoid-like functions such as 1/(1 + e\u2212x), erf(x), x/ \u221a 1 + x2, tanh(x), and tan\u22121(x) satisfy the boundedness requirements. We next introduce terminology that parallels the representation layer of N with a kernel space. Concretely, let N be a network whose representation part has q output neurons. Given weights w, the normalized representation \u03a8w is obtained from the representation RN ,w by dividing each output neuron v by \u2016\u03c3v\u2016 \u221a q. The empirical kernel corresponding to w is defined as \u03baw(x,x \u2032) = \u3008\u03a8w(x),\u03a8w(x\u2032)\u3009. We also define the empirical kernel space corresponding to w as Hw = H\u03baw . Concretely,\nHw = {hv(x) = \u3008v,\u03a8w(x)\u3009 | v \u2208 Rq} ,\nand the norm of Hw is defined as \u2016h\u2016w = inf{\u2016v\u2016 | h = hv}. Our first result shows that the empirical kernel approximates the kernel kS .\nTheorem 2. Let S be a skeleton with C-bounded activations. Let w be a random initialization of N = N (S, r) with\nr \u2265 (4C 4)depth(S)+1 log (8|S|/\u03b4)\n2 .\nThen, for all x,x\u2032, with probability of at least 1\u2212 \u03b4,\n|kw(x,x\u2032)\u2212 kS(x,x\u2032)| \u2264 .\nWe note that if we fix the activation and assume that the depth of S is logarithmic, then the required bound on r is polynomial. For the ReLU activation we get a stronger bound with only quadratic dependence on the depth. However, it requires that \u2264 1/depth(S).\nTheorem 3. Let S be a skeleton with ReLU activations. Let w be a random initialization of N (S, r) with\nr & depth2(S) log (|S|/\u03b4)\n2 .\nThen, for all x,x\u2032 and . 1/depth(S), with probability of at least 1\u2212 \u03b4,\n|\u03baw(x,x\u2032)\u2212 \u03baS(x,x\u2032)| \u2264 .\nFor the remaining theorems, we fix a L-Lipschitz loss ` : R\u00d7Y \u2192 [0,\u221e). For a distribution D on X \u00d7 Y we denote by \u2016D\u20160 the cardinality of the support of the distribution. We note that log (\u2016D\u20160) is bounded by, for instance, the number of bits used to represent an element in X \u00d7 Y . We use the following notion of approximation.\nDefinition. Let D be a distribution on X \u00d7Y. A space H1 \u2282 RX -approximates the space H2 \u2282 RX w.r.t. D if for every h2 \u2208 H2 there is h1 \u2208 H1 such that LD(h1) \u2264 LD(h2) + .\nTheorem 4. Let S be a skeleton with C-bounded activations. Let w be a random initialization of N (S, r) with\nr & L4R4 (4C4)depth(S)+1 log\n( LRC|S| \u03b4 ) 4 .\nThen, with probability of at least 1\u2212\u03b4 over the choices of w we have that H \u221a\n2R w -approximates\nHRS and H \u221a 2R S -approximates HRw.\nTheorem 5. Let S be a skeleton with ReLU activations and . 1/depth(C). Let w be a random initialization of N (S, r) with\nr & L4R4 depth2(S) log\n( \u2016D\u20160|S|\n\u03b4 ) 4 .\nThen, with probability of at least 1\u2212\u03b4 over the choices of w we have that H \u221a\n2R w -approximates\nHRS and H \u221a 2R S -approximates HRw.\nAs in Theorems 2 and 3, for a fixed C-bounded activation and logarithmically deep S, the required bounds on r are polynomial. Analogously, for the ReLU activation the bound is polynomial even without restricting the depth. However, the polynomial growth in Theorems 4 and 5 is rather large. Improving the bounds, or proving their optimality, is left to future work."}, {"heading": "6 Mathematical background", "text": "Reproducing kernel Hilbert spaces (RKHS). The proofs of all the theorems we quote here are well-known and can be found in Chapter 2 of [48] and similar textbooks. Let H be a Hilbert space of functions from X to R. We say that H is a reproducing kernel Hilbert space, abbreviated RKHS or kernel space, if for every x \u2208 X the linear functional f 7\u2192 f(x) is bounded. The following theorem provides a one-to-one correspondence between kernels and kernel spaces.\nTheorem 6. (i) For every kernel \u03ba there exists a unique kernel space H\u03ba such that for every x \u2208 X , \u03ba(\u00b7,x) \u2208 H\u03ba and for all f \u2208 H\u03ba, f(x) = \u3008f(\u00b7), \u03ba(\u00b7,x)\u3009H\u03ba. (ii) A Hilbert space H \u2286 RX is a kernel space if and only if there exists a kernel \u03ba : X \u00d7 X \u2192 R such that H = H\u03ba.\nThe following theorem describes a tight connection between embeddings of X into a Hilbert space and kernel spaces.\nTheorem 7. A function \u03ba : X \u00d7 X \u2192 R is a kernel if and only if there exists a mapping \u03a6 : X \u2192 H to some Hilbert space for which \u03ba(x,x\u2032) = \u3008\u03a6(x),\u03a6(x\u2032)\u3009H. In addition, the following two properties hold,\n\u2022 H\u03ba = {fv : v \u2208 H}, where fv(x) = \u3008v,\u03a6(x)\u3009H.\n\u2022 For every f \u2208 H\u03ba, \u2016f\u2016H\u03ba = inf{\u2016v\u2016H | f = fv}.\nPositive definite functions. A function \u00b5 : [\u22121, 1]\u2192 R is positive definite (PSD) if there are non-negative numbers b0, b1, . . . such that\n\u221e\u2211 i=0 bi <\u221e and \u2200x \u2208 [\u22121, 1], \u00b5(x) = \u221e\u2211 i=0 bix i .\nThe norm of \u00b5 is defined as \u2016\u00b5\u2016 := \u221a\u2211 i bi = \u221a \u00b5(1). We say that \u00b5 is normalized if \u2016\u00b5\u2016 = 1\nTheorem 8 (Schoenberg, [50]). A continuous function \u00b5 : [\u22121, 1] \u2192 R is PSD if and only if for all d = 1, 2, . . . ,\u221e, the function \u03ba : Sd\u22121 \u00d7 Sd\u22121 \u2192 R defined by \u03ba(x,x\u2032) = \u00b5(\u3008x,x\u2032\u3009) is a kernel.\nThe restriction to the unit sphere of many of the kernels used in machine learning applications corresponds to positive definite functions. An example is the Gaussian kernel,\n\u03ba(x,x\u2032) = exp ( \u2212\u2016x\u2212 x \u2032\u20162\n2\u03c32\n) .\nIndeed, note that for unit vectors x,x\u2032 we have\n\u03ba(x,x\u2032) = exp ( \u2212\u2016x\u2016\n2 + \u2016x\u2032\u20162 \u2212 2\u3008x,x\u2032\u3009 2\u03c32\n) = exp ( \u22121\u2212 \u3008x,x\n\u2032\u3009 \u03c32\n) .\nAnother example is the Polynomial kernel \u03ba(x,x\u2032) = \u3008x,x\u2032\u3009d.\nHermite polynomials. The normalized Hermite polynomials is the sequence h0, h1, . . . of orthonormal polynomials obtained by applying the Gram-Schmidt process to the sequence 1, x, x2, . . . w.r.t. the inner-product \u3008f, g\u3009 = 1\u221a 2\u03c0 \u222b\u221e \u2212\u221e f(x)g(x)e \u2212x 2\n2 dx. Recall that we define activations as square integrable functions w.r.t. the Gaussian measure. Thus, Hermite polynomials form an orthonormal basis to the space of activations. In particular, each activation \u03c3 can be uniquely described in the basis of Hermite polynomials,\n\u03c3(x) = a0h0(x) + a1h1(x) + a2h2(x) + . . . , (2)\nwhere the convergence holds in `2 w.r.t. the Gaussian measure. This decomposition is called the Hermite expansion. Finally, we use the following facts (see Chapter 11 in [41] and the relevant entry in Wikipedia):\n\u2200n \u2265 1, hn+1(x) = x\u221a n+ 1\nhn(x)\u2212 \u221a n\nn+ 1 hn\u22121(x) , (3)\n\u2200n \u2265 1, h\u2032n(x) = \u221a nhn\u22121(x) (4)\nE (X,Y )\u223cN\u03c1 hm(X)hn(Y ) =\n{ \u03c1n n = m\n0 n 6= m where n,m \u2265 0, \u03c1 \u2208 [\u22121, 1] , (5)\nhn(0) =\n{ 0, if n is odd\n1\u221a n!\n(\u22121) n 2 (n\u2212 1)!! if n is even\n, (6)\nwhere\nn!! =  1 n \u2264 0 n \u00b7 (n\u2212 2) \u00b7 \u00b7 \u00b7 5 \u00b7 3 \u00b7 1 n > 0 odd n \u00b7 (n\u2212 2) \u00b7 \u00b7 \u00b7 6 \u00b7 4 \u00b7 2 n > 0 even ."}, {"heading": "7 Compositional kernel spaces", "text": "We now describe the details of compositional kernel spaces. Let S be a skeleton with normalized activations and n input nodes associated with the input\u2019s coordinates. Throughout the rest of the section we study the functions in HS and their norm. In particular, we show that \u03baS is indeed a normalized kernel. Recall that \u03baS is defined inductively by the equation,\n\u03bav(x,x \u2032) = \u03c3\u0302v\n(\u2211 u\u2208in(v) \u03bau(x,x \u2032)\n|in(v)|\n) . (7)\nThe recursion (7) describes a means for generating a kernel form another kernel. Since kernels correspond to kernel spaces, it also prescribes an operator that produces a kernel space from other kernel spaces. If Hv is the space corresponding to v, we denote this operator by\nHv = \u03c3\u0302v ( \u2295u\u2208in(v)Hu |in(v)| ) . (8)\nThe reason for using the above notation becomes clear in the sequel. The space HS is obtained by starting with the spacesHv corresponding to the input nodes and propagating them according to the structure of S, where at each node v the operation (8) is applied. Hence, to understand HS we need to understand this operation as well as the spaces corresponding to input nodes. The latter spaces are rather simple: for an input node v corresponding to the variable xi, we have that Hv = {fw | \u2200x, fw(x) = \u3008w,xi\u3009} and \u2016fw\u2016Hv = \u2016w\u2016. To understand (8), it is convenient to decompose it into two operations. The first operation, termed the direct average, is defined through the equation \u03ba\u0303v(x,x \u2032) = \u2211 u\u2208in(v) \u03bau(x,x \u2032)\n|in(v)| , and the\nresulting kernel space is denoted Hv\u0303 = \u2295u\u2208in(v)Hu |in(v)| . The second operation, called the extension according to \u03c3\u0302v, is defined through \u03bav(x,x \u2032) = \u03c3\u0302v (\u03ba\u0303v(x,x\n\u2032)). The resulting kernel space is denoted Hv = \u03c3\u0302v (Hv\u0303). We next analyze these two operations.\nThe direct average of kernel spaces. Let H1, . . . ,Hn be kernel spaces with kernels \u03ba1, . . . , \u03ban : X \u00d7 X \u2192 R. Their direct average, denoted H = H1\u2295\u00b7\u00b7\u00b7\u2295Hnn , is the kernel space corresponding to the kernel \u03ba(x,x\u2032) = 1\nn \u2211n i=1 \u03bai(x,x \u2032).\nLemma 9. The function \u03ba is indeed a kernel. Furthermore, the following properties hold.\n1. If H1, . . . ,Hn are normalized then so is H. 2. H = { f1+...+fn\nn | fi \u2208 Hi } 3. \u2016f\u20162H = inf {\u2016f1\u20162H1+...+\u2016fn\u20162Hn n s.t. f = f1+...+fn n , fi \u2208 Hi }\nProof. (outline) The fact that \u03ba is a kernel follows directly from the definition of a kernel and the fact that an average of PSD matrices is PSD. Also, it is straight forward to verify item 1. We now proceed to items 2 and 3. By Theorem 7 there are Hilbert spaces G1, . . . ,Gn and mappings \u03a6i : X \u2192 Gi such that \u03bai(x,x\u2032) = \u3008\u03a6i(x),\u03a6i(x\u2032)\u3009Gi . Consider now the mapping\n\u03a8(x) = ( \u03a61(x)\u221a\nn , . . . , \u03a6n(x)\u221a n\n) .\nIt holds that \u03ba(x,x\u2032) = \u3008\u03a8(x),\u03a8(x\u2032)\u3009. Properties 2 and 3 now follow directly form Thm. 7 applied to \u03a8.\nThe extension of a kernel space. Let H be a normalized kernel space with a kernel \u03ba. Let \u00b5(x) = \u2211 i bix\ni be a PSD function. As we will see shortly, a function is PSD if and only if it is a dual of an activation function. The extension of H w.r.t. \u00b5, denoted \u00b5 (H), is the kernel space corresponding to the kernel \u03ba\u2032(x,x\u2032) = \u00b5(\u03ba(x,x\u2032)).\nLemma 10. The function \u03ba\u2032 is indeed a kernel. Furthermore, the following properties hold.\n1. \u00b5(H) is normalized if and only if \u00b5 is.\n2. \u00b5(H) = span {\u220f g\u2208A g | A \u2282 H, b|A| > 0 } where span(A) is the closure of the span of A.\n3. \u2016f\u2016\u00b5(H) \u2264 inf {\u2211 A \u220f g\u2208A \u2016g\u2016H\u221a b|A| s.t. f = \u2211 A \u220f g\u2208A g, A \u2282 H }\nProof. (outline) Let \u03a6 : X \u2192 G be a mapping from X to the unit ball of a Hilbert space G such that \u03ba(x,x\u2032) = \u3008\u03a6(x),\u03a6(x\u2032)\u3009. Define\n\u03a8(x) = (\u221a b0, \u221a b1\u03a6(x), \u221a b2\u03a6(x)\u2297 \u03a6(x), \u221a b3\u03a6(x)\u2297 \u03a6(x)\u2297 \u03a6(x), . . . ) It is not difficult to verify that \u3008\u03a8(x),\u03a8(x\u2032)\u3009 = \u00b5(\u03ba(x,x\u2032)). Hence, by Thm. 7, \u03ba\u2032 is indeed a kernel. Verifying property 1 is a straightforward task. Properties 2 and 3 follow by applying Thm. 7 on the mapping \u03a8."}, {"heading": "8 The dual activation function", "text": "The following lemma describes a few basic properties of the dual activation. These properties follow easily from the definition of the dual activation and equations (2), (4), and (5).\nLemma 11. The following properties of the mapping \u03c3 7\u2192 \u03c3\u0302 hold: (a) If \u03c3 = \u2211 i aihi is the Hermite expansion of \u03c3, then \u03c3\u0302(\u03c1) = \u2211 i a 2 i \u03c1 i.\n(b) For every \u03c3, \u03c3\u0302 is positive definite.\n(c) Every positive definite function is a dual of some activation.\n(d) The mapping \u03c3 7\u2192 \u03c3\u0302 preserves norms.\n(e) The mapping \u03c3 7\u2192 \u03c3\u0302 commutes with differentiation.\n(f) For a \u2208 R, a\u0302\u03c3 = a2\u03c3\u0302.\n(g) For every \u03c3, \u03c3\u0302 is continuous in [\u22121, 1] and smooth in (\u22121, 1).\n(h) For every \u03c3, \u03c3\u0302 is non-decreasing and convex in [0, 1].\n(i) For every \u03c3, the range of \u03c3\u0302 is [\u2212\u2016\u03c3\u20162, \u2016\u03c3\u20162].\n(j) For every \u03c3, \u03c3\u0302(0) = ( EX\u223cN(0,1) \u03c3(X) )2 and \u03c3\u0302(1) = \u2016\u03c3\u20162.\nWe next discuss a few examples for activations and calculate their dual activation and kernel. Note that the dual of the exponential activation was calculated in [36] and the duals of the step and the ReLU activations were calculated in [13]. Here, our derivations are different and may prove useful for future calculations of duals for other activations.\nThe exponential activation. Consider the activation function \u03c3(x) = Ceax where C > 0 is a normalization constant such that \u2016\u03c3\u2016 = 1. The actual value of C is e\u22122a2 but it will not be needed for the derivation below. From properties (e) and (f) of Lemma 11 we have that,\n(\u03c3\u0302)\u2032 = \u03c3\u0302\u2032 = a\u0302\u03c3 = a2\u03c3\u0302 .\nThe the solution of ordinary differential equation (\u03c3\u0302)\u2032 = a2\u03c3\u0302 is of the form \u03c3\u0302(\u03c1) = b exp (a2\u03c1). Since \u03c3\u0302(1) = 1 we have b = e\u2212a 2 . We therefore obtain that the dual activation function is\n\u03c3\u0302(\u03c1) = ea 2\u03c1\u2212a2 = ea 2(\u03c1\u22121) .\nNote that the kernel induced by \u03c3 is the RBF kernel, restricted to the d-dimensional sphere,\n\u03ba\u03c3(x,x \u2032) = ea 2(\u3008x,x\u2032\u3009\u22121) = e\u2212 a2\u2016x\u2212x\u2032\u20162 2 .\nThe Sine activation and the Sinh kernel. Consider the activation \u03c3(x) = sin(ax). We can write sin(ax) = e iax\u2212e\u2212iax\n2i . We have\n\u03c3\u0302(\u03c1) = E (X,Y )\u223cN\u03c1\n( eiaX \u2212 e\u2212iaX\n2i\n)( eiaY \u2212 e\u2212iaY\n2i ) = \u22121\n4 E (X,Y )\u223cN\u03c1\n( eiaX \u2212 e\u2212iaX ) ( eiaY \u2212 e\u2212iaY ) = \u22121\n4 E (X,Y )\u223cN\u03c1\n[ eia(X+Y ) \u2212 eia(X\u2212Y ) \u2212 eia(\u2212X+Y ) + eia(\u2212X\u2212Y ) ] .\nRecall that the characteristic function, E[eitX ], when X is distributed N(0, 1) is e\u2212 12 t2 . Since X+Y and \u2212X\u2212Y are normal variables with expectation 0 and variance of 2+2\u03c1, it follows that,\nE (X,Y )\u223cN\u03c1 eia(X+Y ) = E (X,Y )\u223cN\u03c1\ne\u2212ia(X+Y ) = e\u2212 a2(2+2\u03c1) 2 .\nSimilarly, since the variance of X \u2212 Y and Y \u2212X is 2\u2212 2\u03c1, we get\nE (X,Y )\u223cN\u03c1 eia(X\u2212Y ) = E (X,Y )\u223cN\u03c1\neia(\u2212X+Y ) = e\u2212 a2(2\u22122\u03c1) 2 .\nWe therefore obtain that\n\u03c3\u0302(\u03c1) = e\u2212a 2(1\u2212\u03c1) \u2212 e\u2212a2(1+\u03c1)\n2 = e\u2212a 2 sinh(a2\u03c1) .\nHermite activations and polynomial kernels. From Lemma 11 it follows that the dual activation of the Hermite polynomial hn is h\u0302n(\u03c1) = \u03c1\nn. Hence, the corresponding kernel is the polynomial kernel.\nThe normalized step activation. Consider the activation\n\u03c3(x) =\n{\u221a 2 x > 0\n0 x \u2264 0 .\nTo calculate \u03c3\u0302 we compute the Hermite expansion of \u03c3. For n \u2265 0 we let\nan = 1\u221a 2\u03c0 \u222b \u221e \u2212\u221e \u03c3(x)hn(x)e \u2212x 2 2 dx = 1\u221a \u03c0 \u222b \u221e 0 hn(x)e \u2212x 2 2 dx .\nSince h0(x) = 1, h1(x) = x, and h2(x) = x2\u22121\u221a\n2 , we get the corresponding coefficients,\na0 = E X\u223cN(0,1) [\u03c3(X)] = 1\u221a 2\na1 = E X\u223cN(0,1) [\u03c3(X)X] = 1\u221a 2 E X\u223cN(0,1) [|X|] = 1\u221a \u03c0 a2 = 1\u221a 2 E X\u223cN(0,1) [\u03c3(X)(X2 \u2212 1)] = 1 2 E X\u223cN(0,1) [X2 \u2212 1] = 0 .\nFor n \u2265 3 we write gn(x) = hn(x)e\u2212 x2 2 and note that\ng\u2032n(x) = [h \u2032 n(x)\u2212 xhn(x)] e\u2212\nx2\n2 = [\u221a nhn\u22121(x)\u2212 xhn(x) ] e\u2212 x2 2 = \u2212 \u221a n+ 1hn+1(x)e \u2212x 2 2 = \u2212 \u221a n+ 1 gn+1(x) .\nHere, the second equality follows from (4) and the third form (3). We therefore get\nan = 1\u221a \u03c0 \u222b \u221e 0 gn(x)dx\n= \u2212 1\u221a n\u03c0 \u222b \u221e 0 g\u2032n\u22121(x)dx\n= 1\u221a n\u03c0\ngn\u22121(0)\u2212 =0\ufe37 \ufe38\ufe38 \ufe37gn\u22121(\u221e) \n= 1\u221a n\u03c0 hn\u22121(0)\n=  (\u22121) n\u22121 2 (n\u22122)!! \u221a n\u03c0 \u221a (n\u22121)! = (\u22121) n\u22121 2 (n\u22122)!!\u221a \u03c0n! if n is odd\n0 if n is even .\nThe second equality follows from (3) and the last equality follows from (6). Finally, from Lemma 11 we have that \u03c3\u0302(\u03c1) = \u2211\u221e n=0 bn\u03c1 n where\nbn =  ((n\u22122)!!)2 \u03c0n! if n is odd 1 2 if n = 0\n0 if n is even \u2265 2 .\nIn particular, (b0, b1, b2, b3, b4, b5, b6) = ( 1 2 , 1 \u03c0 , 0, 1 6\u03c0 , 0, 3 40\u03c0 , 0 ) . Note that from the Taylor expansion of cos\u22121 it follows that \u03c3\u0302(\u03c1) = 1\u2212 cos \u22121(\u03c1) \u03c0 .\nThe normalized ReLU activation. Consider the activation \u03c3(x) = \u221a\n2 max(0, x). We now write \u03c3\u0302(\u03c1) = \u2211 i bi\u03c1 i. The first coefficient is\nb0 =\n( E\nX\u223cN(0,1) \u03c3(X)\n)2 = 1\n2\n( E X\u223cN(0,1) |X| )2 = 1 \u03c0 .\nTo calculate the remaining coefficients we simply note that the derivative of the ReLU activation is the step activation and the mapping \u03c3 7\u2192 \u03c3\u0302 commutes with differentiation. Hence, from the calculation of the step activation we get,\nbn =  ((n\u22123)!!)2 \u03c0n! if n is even 1 2 if n = 1\n0 if n is odd \u2265 3 .\nIn particular, (b0, b1, b2, b3, b4, b5, b6) = ( 1 \u03c0 , 1 2 , 1 2\u03c0 , 0, 1 24\u03c0 , 0, 1 80\u03c0 ) . We see that the coefficients corresponding to the degrees 0, 1, and 2 sum to 0.9774. The sums up to degrees 4 or 6 are 0.9907 and 0.9947 respectively. That is, we get an excellent approximation of less than 1% error with a dual activation of degree 4.\nThe collapsing tower of fully connected layers. To conclude this section we discuss the case of very deep networks. The setting is taken for illustrative purposes but it might surface when building networks with numerous fully connected layers. Indeed, most deep architectures that we are aware of do not employ more than five consecutive fully connected layers.\nConsider a skeleton Sm consisting of m fully connected layers, each layer associated with the same (normalized) activation \u03c3. We would like to examine the form of the compositional kernel as the number of layers becomes very large. Due to the repeated structure and activation we have\n\u03baSm(x,y) = \u03b1m ( \u3008x,y\u3009 n ) where \u03b1m = \u03c3\u0302 m = m times\ufe37 \ufe38\ufe38 \ufe37 \u03c3\u0302 \u25e6 . . . \u25e6 \u03c3\u0302 .\nHence, the limiting properties of \u03baSm can be understood from the limit of \u03b1m. In the case that \u03c3(x) = x or \u03c3(x) = \u2212x, \u03c3\u0302 is the identity function. Therefore \u03b1m(\u03c1) = \u03c3\u0302(\u03c1) = \u03c1 for all m and \u03baSm is simply the linear kernel. Assume now that \u03c3 is neither the identity nor its negation. The following claim shows that \u03b1m has a point-wise limit corresponding to a degenerate kernel.\nClaim 1. There exists a constant 0 \u2264 \u03b1\u03c3 \u2264 1 such that for all \u22121 < \u03c1 < 1,\nlim m\u2192\u221e \u03b1m(\u03c1) = \u03b1\u03c3\nBefore proving the claim, we note that for \u03c1 = 1, \u03b1m(1) = 1 for all m, and therefore limm\u2192\u221e \u03b1m(1) = 1. For \u03c1 = \u22121, if \u03c3 is anti-symmetric then \u03b1m(\u22121) = \u22121 for all m, and in particular limm\u2192\u221e \u03b1m(\u22121) = \u22121. In any other case, our argument can show that limm\u2192\u221e \u03b1m(\u22121) = \u03b1\u03c3.\nProof. Recall that \u03c3\u0302(\u03c1) = \u2211\u221e\ni=0 bi\u03c1 i where the bi\u2019s are non-negative numbers that sum to 1.\nBy the assumption that \u03c3 is not the identity or its negation, b1 < 1. We first claim that there is a unique \u03b1\u03c3 \u2208 [0, 1] such that\n\u2200x \u2208 (\u22121, \u03b1\u03c3) , \u03c3\u0302(\u03c1) > \u03c1 and \u2200x \u2208 (\u03b1\u03c3, 1) , \u03b1\u03c3 < \u03c3\u0302(\u03c1) < \u03c1 (9)\nTo prove (9) it suffices to prove the following properties.\n(a) \u03c3\u0302(\u03c1) > \u03c1 for \u03c1 \u2208 (\u22121, 0)\n(b) \u03c3\u0302 is non-decreasing and convex in [0, 1]\n(c) \u03c3\u0302(1) = 1\n(d) the graph of \u03c3\u0302 has at most a single intersection in [0, 1) with the graph of f(\u03c1) = \u03c1\nIf the above properties hold we can take \u03b1\u03c3 to be the intersection point or 1 if such a point does not exist. We first show (a). For \u03c1 \u2208 (\u22121, 0) we have that\n\u03c3\u0302(\u03c1) = b0 + \u221e\u2211 i=1 bi\u03c1 i \u2265 b0 \u2212 \u221e\u2211 i=1 bi|\u03c1|i > \u2212 \u221e\u2211 i=1 bi|\u03c1| \u2265 \u2212|\u03c1| = \u03c1 .\nHere, the third inequality follows form the fact that b0 \u2265 0 and for all i, \u2212bi|\u03c1|i \u2265 \u2212bi|\u03c1|. Moreover since b1 < 1, one of these inequalities must be strict. Properties (b) and (c) follows from Lemma 11. Finally, to show (d), we note that the second derivative of \u03c3\u0302(\u03c1) \u2212 \u03c1 is\u2211\ni\u22652 i(i\u2212 1)bi\u03c1i\u22122 which is non-negative in [0, 1). Hence, \u03c3\u0302(\u03c1)\u2212 \u03c1 is convex in [0, 1] and in particular intersects with the x-axis at either 0, 1, 2 or infinitely many times in [0, 1]. As we assume that \u03c3\u0302 is not the identity, we can rule out the option of infinitely many intersections. Also, since \u03c3\u0302(1) = 1, we know that there is at least one intersection in [0, 1]. Hence, there are 1 or 2 intersections in [0, 1] and because one of them is in \u03c1 = 1, we conclude that there is at most one intersection in [0, 1).\nLastly, we derive the conclusion of the claim from equation (9). Fix \u03c1 \u2208 (\u22121, 1). Assume first that \u03c1 \u2265 \u03b1\u03c3. By (9), \u03b1m(\u03c1) is a monotonically non-increasing sequence that is lower bounded by \u03b1\u03c3. Hence, it has a limit \u03b1\u03c3 \u2264 \u03c4 \u2264 \u03c1 < 1. Now, by the continuity of \u03c3\u0302 we have\n\u03c3\u0302(\u03c4) = \u03c3\u0302 (\nlim m\u2192\u221e\n\u03b1m(\u03c1) )\n= lim m\u2192\u221e \u03c3\u0302(\u03b1m(\u03c1)) = lim m\u2192\u221e \u03b1m+1(\u03c1) = \u03c4 .\nSince the only solution to \u03c3\u0302(\u03c1) = \u03c1 in (\u22121, 1) is \u03b1\u03c3, we must have \u03c4 = \u03b1\u03c3. We next deal with the case that \u22121 < \u03c1 < \u03b1\u03c3. If for some m, \u03b1m(\u03c1) \u2208 [\u03b1\u03c3, 1), the argument for \u03b1\u03c3 \u2264 \u03c1 shows that \u03b1\u03c3 = limm\u2192\u221e \u03b1m(\u03c1). If this is not the case, we have that for all m, \u03b1m(\u03c1) \u2264 \u03b1m+1(\u03c1) \u2264 \u03b1\u03c3. As in the case of \u03c1 \u2265 \u03b1\u03c3, this can be used to show that \u03b1m(\u03c1) converges to \u03b1\u03c3."}, {"heading": "9 Proofs", "text": ""}, {"heading": "9.1 Well-behaved activations", "text": "The proof of our main results applies to activations that are decent, i.e. well-behaved, in a sense defined in the sequel. We then show that C-bounded activations as well as the ReLU activation are decent. We first need to extend the definition of the dual activation and kernel to apply to vectors in Rd, rather than just Sd. We denote by M+ the collection of 2 \u00d7 2 positive semi-define matrices and by M++ the collection of positive definite matrices.\nDefinition. Let \u03c3 be an activation. Define the following,\n\u03c3\u0304 :M2+ \u2192 R , \u03c3\u0304(\u03a3) = E (X,Y )\u223cN(0,\u03a3) \u03c3(X)\u03c3(Y ) , k\u03c3(x,y) = \u03c3\u0304 ( \u2016x\u20162 \u3008x,y\u3009 \u3008x,y\u3009 \u2016y\u20162 ) .\nWe underscore the following properties of the extension of a dual activation.\n(a) The following equality holds,\n\u03c3\u0302(\u03c1) = \u03c3\u0304 ( 1 \u03c1 \u03c1 1 ) (b) The restriction of the extended k\u03c3 to the sphere agrees with the restricted definition.\n(c) The extended dual activation and kernel are defined for every activation \u03c3 such that for all a \u2265 0, x 7\u2192 \u03c3(ax) is square integrable with respect to the Gaussian measure.\n(d) For x,y \u2208 Rd, if w \u2208 Rd is a multivariate normal distribution with zero mean vector and identity covariance matrix, then\nk\u03c3(x,y) = E w \u03c3(\u3008w,x\u3009)\u03c3(\u3008w,y\u3009) .\nDenote\nM\u03b3+ := {(\n\u03a311 \u03a312 \u03a312 \u03a322\n) \u2208M+ | 1\u2212 \u03b3 \u2264 \u03a311,\u03a322 \u2264 1 + \u03b3 } .\nDefinition. A normalized activation \u03c3 is (\u03b1, \u03b2, \u03b3)-decent for \u03b1, \u03b2, \u03b3 \u2265 0 if the following conditions hold.\n(i) The dual activation \u03c3\u0304 is \u03b2-Lipschitz in M\u03b3+ with respect to the \u221e-norm.\n(ii) If (X1, Y1), . . . , (Xr, Yr) are independent samples from N (0,\u03a3) for \u03a3 \u2208M\u03b3+ then\nPr (\u2223\u2223\u2223\u2223\u2211ri=1 \u03c3(Xi)\u03c3(Yi)r \u2212 \u03c3\u0304(\u03a3) \u2223\u2223\u2223\u2223 \u2265 ) \u2264 2 exp(\u2212 r 22\u03b12 ) .\nLemma 12 (Bounded activations are decent). Let \u03c3 : R \u2192 R be a C-bounded normalized activation. Then, \u03c3 is (C2, 2C2, \u03b3)-decent for all \u03b3 \u2265 0.\nProof. It is enough to show that the following properties hold.\n1. The (extended) dual activation \u03c3\u0304 is 2C2-Lipschitz in M++ w.r.t. the \u221e-norm.\n2. If (X1, Y1), . . . , (Xr, Yr) are independent samples from N (0,\u03a3) then\nPr (\u2223\u2223\u2223\u2223\u2211ri=1 \u03c3(Xi)\u03c3(Yi)r \u2212 \u03c3\u0304(\u03a3) \u2223\u2223\u2223\u2223 \u2265 ) \u2264 2 exp(\u2212 r 22C4 ) From the boundedness of \u03c3 it holds that |\u03c3(X)\u03c3(Y )| \u2264 C2. Hence, the second property follows directly from Hoeffding\u2019s bound. We next prove the first part. Let z = (x, y) and \u03c6(z) = \u03c3(x)\u03c3(y). Note that for \u03a3 \u2208M++ we have\n\u03c3\u0304(\u03a3) = 1 2\u03c0 \u221a det(\u03a3) \u222b R2 \u03c6(z)e\u2212 z>\u03a3\u22121z 2 dz .\nThus we get that,\n\u2202\u03c3\u0304 \u2202\u03a3 = 1 2\u03c0 \u222b R2 \u03c6(z) [ 1 2 \u221a det(\u03a3)\u03a3\u22121 \u2212 1 2 \u221a det(\u03a3)(\u03a3\u22121zz>\u03a3\u22121) det(\u03a3) ] e\u2212 z>\u03a3\u22121z 2 dz\n= 1 2\u03c0 \u221a det(\u03a3) \u222b R2 \u03c6(z) 1 2 [ \u03a3\u22121 \u2212 \u03a3\u22121zz>\u03a3\u22121 ] e\u2212 z>\u03a3\u22121z 2 dz\nLet g(z) = e\u2212 z>\u03a3\u22121z 2 . Then, the first and second order partial derivatives of g are\n\u2202g \u2202z = \u2212\u03a3\u22121ze\u2212 z>\u03a3\u22121z 2\n\u22022g \u22022z =\n[ \u2212\u03a3\u22121 + \u03a3\u22121zz>\u03a3\u22121 ] e\u2212 z>\u03a3\u22121z 2 .\nWe therefore obtain that,\n\u2202\u03c3\u0304 \u2202\u03a3 = \u2212 1 4\u03c0 \u221a det(\u03a3) \u222b R2 \u03c6 \u22022g \u22022z dz .\nBy the product rule we have\n\u2202\u03c3\u0304 \u2202\u03a3 = \u2212 1 2\u03c0 \u221a det(\u03a3) 1 2 \u222b R2 \u22022\u03c6 \u22022z gdz = \u22121 2 E (X,Y )\u223cN(0,\u03a3) [ \u22022\u03c6 \u22022z (X, Y ) ] We conclude that \u03c3\u0304 is differentiable in M++ with partial derivatives that are point-wise bounded by C 2\n2 . Thus, \u03c3\u0304 is 2C2-Lipschitz in M+ w.r.t. the \u221e-norm.\nWe next show that the ReLU activation is decent.\nLemma 13 (ReLU is decent). There exists a constant \u03b1ReLU \u2265 1 such that for 0 \u2264 \u03b3 \u2264 1, the normalized ReLU activation \u03c3(x) = \u221a 2 max(0, x) is (\u03b1ReLU, 1 + o(\u03b3), \u03b3)-decent.\nProof. The measure concentration property follows from standard concentration bounds for sub-exponential random variables (e.g. [53]). It remains to show that \u03c3\u0304 is (1+o(\u03b3))-Lipschitz in M\u03b3+. We first calculate an exact expression for \u03c3\u0304. The expression was already calculated in [13], yet we give here a derivation for completeness.\nClaim 2. The following equality holds for all \u03a3 \u2208M2+,\n\u03c3\u0304(\u03a3) = \u221a\n\u03a311\u03a322 \u03c3\u0302\n( \u03a312\u221a\n\u03a311\u03a322\n) .\nProof. Let us denote\n\u03a3\u0303 =\n( 1 \u03a312\u221a\n\u03a311\u03a312 \u03a312\u221a\n\u03a311\u03a312 1\n) .\nBy the positive homogeneity of the ReLU activation we have\n\u03c3\u0304 (\u03a3) = E (X,Y )\u223cN(0,\u03a3) \u03c3(X)\u03c3(Y )\n= \u221a\n\u03a311\u03a322 E (X,Y )\u223cN(0,\u03a3) \u03c3 ( X\u221a \u03a311 ) \u03c3 ( Y\u221a \u03a322 ) = \u221a \u03a311\u03a322 E (X\u0303,Y\u0303 )\u223cN(0,\u03a3\u0303) \u03c3 ( X\u0303 ) \u03c3 ( Y\u0303 )\n= \u221a\n\u03a311\u03a322 \u03c3\u0302\n( \u03a312\u221a\n\u03a311\u03a322\n) .\nwhich concludes the proof.\nFor brevity, we henceforth drop the argument from \u03c3\u0304(\u03a3) and use the abbreviation \u03c3\u0304. In order to show that \u03c3\u0304 is (1 + o(\u03b3))-Lipschitz w.r.t. the \u221e-norm it is enough to show that for every \u03a3 \u2208M\u03b3+ we have,\n\u2016\u2207\u03c3\u0304\u20161 = \u2223\u2223\u2223\u2223 \u2202\u03c3\u0304\u2202\u03a312 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202\u03c3\u0304\u2202\u03a311 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202\u03c3\u0304\u2202\u03a322 \u2223\u2223\u2223\u2223 \u2264 1 + o(\u03b3) . (10)\nFirst, Note that \u2202\u03c3\u0304/\u2202\u03a311 and \u2202\u03c3\u0304/\u2202\u03a322 have the same sign, hence, \u2016\u2207\u03c3\u0304\u20161 = \u2223\u2223\u2223\u2223 \u2202\u03c3\u0304\u2202\u03a312 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223 \u2202\u03c3\u0304\u2202\u03a311 + \u2202\u03c3\u0304\u2202\u03a322 \u2223\u2223\u2223\u2223 .\nNext we get that,\n\u2202\u03c3\u0304\n\u2202\u03a311 =\n1\n2 \u221a \u03a322 \u03a311 \u03c3\u0302 ( \u03a312\u221a \u03a311\u03a322 ) \u2212 1 2 \u221a \u03a322 \u03a311 \u03a312\u221a \u03a311\u03a322 \u03c3\u0302\u2032 ( \u03a312\u221a \u03a311\u03a322 ) \u2202\u03c3\u0304\n\u2202\u03a322 =\n1\n2 \u221a \u03a311 \u03a322 \u03c3\u0302 ( \u03a312\u221a \u03a311\u03a322 ) \u2212 1 2 \u221a \u03a311 \u03a322 \u03a312\u221a \u03a311\u03a322 \u03c3\u0302\u2032 ( \u03a312\u221a \u03a311\u03a322 ) \u2202\u03c3\u0304\n\u2202\u03a312 = \u03c3\u0302\u2032\n( \u03a312\u221a\n\u03a311\u03a322\n) .\nWe therefore get that the 1-norm of \u2207\u03c3\u0304 is,\n\u2016\u2207\u03c3\u0304\u20161 = 1\n2 \u03a311 + \u03a322\u221a \u03a311\u03a322 \u2223\u2223\u2223\u2223\u03c3\u0302( \u03a312\u221a\u03a311\u03a322 ) \u2212 \u03a312\u221a \u03a311\u03a322 \u03c3\u0302\u2032 ( \u03a312\u221a \u03a311\u03a322 )\u2223\u2223\u2223\u2223+ \u03c3\u0302\u2032( \u03a312\u221a\u03a311\u03a322 ) .\nThe gradient of 1 2 \u03a311+\u03a322\u221a \u03a311\u03a322 at (\u03a311,\u03a322) = (1, 1) is (0, 0). Therefore, from the mean value theorem we get, 1 2 \u03a311+\u03a322\u221a \u03a311\u03a322 = 1 + o(\u03b3). Furthermore, \u03c3\u0302, \u03c3\u0302\u2032 and \u03a312\u221a \u03a311\u03a322\nare bounded by 1 in absolute value. Hence, we can write,\n\u2016\u2207\u03c3\u0304\u20161 = \u2223\u2223\u2223\u2223\u03c3\u0302( \u03a312\u221a\u03a311\u03a322 ) \u2212 \u03a312\u221a \u03a311\u03a322 \u03c3\u0302\u2032 ( \u03a312\u221a \u03a311\u03a322 )\u2223\u2223\u2223\u2223+ \u03c3\u0302\u2032( \u03a312\u221a\u03a311\u03a322 ) + o(\u03b3) .\nFinally, if we let t = \u03a312\u221a \u03a311\u03a322 , we can further simply the expression for \u2207\u03c3\u0304,\n\u2016\u2207\u03c3\u0304(\u03a3)\u20161 = |\u03c3\u0302(t)\u2212 t\u03c3\u0302\u2032(t)|+ |\u03c3\u0302\u2032(t)|+ o(\u03b3)\n= \u221a 1\u2212 t2 \u03c0 + 1\u2212 cos \u22121(t) \u03c0 + o(\u03b3) .\nFinally, the proof is obtained from the fact that the function f(t) = \u221a\n1\u2212t2 \u03c0 + 1 \u2212 cos \u22121(t) \u03c0\nsatisfies 0 \u2264 f(t) \u2264 1 for every t \u2208 [\u22121, 1]. Indeed, it is simple to verify that f(\u22121) = 0 and f(1) = 1. Hence, it suffices to show that f \u2032 is non-negative in [\u22121, 1] which is indeed the case since,\nf \u2032(t) = 1\n\u03c0 1\u2212 t\u221a 1\u2212 t2 = 1 \u03c0 \u221a 1\u2212 t 1 + t \u2265 0 ."}, {"heading": "9.2 Proofs of Thms. 2 and 3", "text": "We start by an additional theorem which serves as a simple stepping stone for proving the aforementioned main theorems.\nTheorem 14. Let S be a skeleton with (\u03b1, \u03b2, \u03b3)-decent activations, 0 < \u2264 \u03b3, and Bd =\u2211d\u22121 i=0 \u03b2 i. Let w be a random initialization of the network N = N (S, r) with\nr \u2265 2\u03b12B2depth(S) log\n( 8|S| \u03b4 ) 2 .\nThen, for every x,y with probability of at least 1\u2212 \u03b4, it holds that\n|\u03baw(x,y)\u2212 \u03baS(x,y)| \u2264 .\nBefore proving the theorem we show that together with Lemmas 12 and 13, Theorems 2 and 3 follow from Theorem 14. We restate them as corollaries, prove them, and then proceed to the proof of Theorem 14.\nCorollary 15. Let S be a skeleton with C-bounded activations. Let w be a random initialization of N = N (S, r) with\nr \u2265 (4C4)depth(S)+1 log\n( 8|S| \u03b4 ) 2 .\nThen, for every x,y, w.p. \u2265 1\u2212 \u03b4,\n|\u03baw(x,y)\u2212 \u03baS(x,y)| \u2264 .\nProof. From Lemma 12, for all \u03b3 > 0, each activation is (C2, 2C2, \u03b3)-decent. By Theorem 14, it suffices to show that\n2 ( C2 )2depth(S)\u22121\u2211\ni=0\n(2C2)i 2 \u2264 (4C4)depth(S)+1 . The sum of can be bounded above by,\ndepth(S)\u22121\u2211 i=0 (2C2)i = (2C2)depth(S) \u2212 1 2C2 \u2212 1 \u2264 (2C 2)depth(S) C2 .\nTherefore, we get that,\n2 ( C2 )2depth(S)\u22121\u2211\ni=0\n(2C2)i 2 \u2264 2C4(4C4)depth(S) C4 \u2264 (4C4)depth(S)+1 ,\nwhich concludes the proof.\nCorollary 16. Let S be a skeleton with ReLU activations, and w a random initialization of N (S, r) with r \u2265 c1 depth2(S) log( 8|S|\u03b4 ) 2 . For all x,y and \u2264 min(c2, 1depth(S)), w.p. \u2265 1\u2212 \u03b4,\n|\u03baw(x,y)\u2212 \u03baS(x,y)| \u2264\nHere, c1, c2 > 0 are universal constants.\nProof. From Lemma 13, each activation is (\u03b1ReLU, 1 + o( ), )-decent. By Theorem 14, it is enough to show that\ndepth(S)\u22121\u2211 i=0 (1 + o( ))i = O(depth(S)) .\nThis claim follows from the fact that (1 + o( ))i \u2264 eo( )depth(S) as long as i \u2264 depth(S). Since we assume that \u2264 1/depth(S), the expression is bounded by e for sufficiently small . We next prove Theorem 14.\nProof. (Theorem 14) For a node u \u2208 S we denote by \u03a8u,w : X \u2192 Rr the normalized representation of S\u2019s sub-skeleton rooted at u. Analogously, \u03bau,w denotes the empirical kernel of that network. When u is the output node of S we still use \u03a8w and \u03baw for \u03a8u,w and \u03bau,w. Given two fixed x,y \u2208 X and a node u \u2208 S, we denote\nKuw = ( \u03bau,w(x,x) \u03bau,w(x,y) \u03bau,w(x,y) \u03bau,w(y,y) ) , Ku = ( \u03bau(x,x) \u03bau(x,y) \u03bau(x,y) \u03bau(y,y) )\nK\u2190uw = \u2211 v\u2208in(u)Kvw |in(u)| , K\u2190u = \u2211 v\u2208in(u)Kv |in(u)| .\nFor a matrix K \u2208M+ and a function f :M+ \u2192 R, we denote\nfp(K) = f ( K11 K11 K11 K11 ) f(K)\nf(K) f ( K22 K22 K22 K22\n) \nNote that Ku = \u03c3\u0304pu(K\u2190u) and Kuw = \u03c3\u0304pu(K\u2190uw ). We say that a node u \u2208 S, is well-initialized if\n\u2016Kuw \u2212Ku\u2016\u221e \u2264 Bdepth(u) Bdepth(S) . (11)\nHere, we use the convention that B0 = 0. It is enough to show that with probability of at least \u2265 1\u2212 \u03b4 all nodes are well-initialized. We first note that input nodes are well-initialized by construction since Kuw = Ku. Next, we show that given that all incoming nodes for a certain node are well-initialized, then w.h.p. the node is well-initialized as well.\nClaim 3. Assume that all the nodes in in(u) are well-initialized. Then, the node u is wellinitialized with probability of at least 1\u2212 \u03b4|S| .\nProof. It is easy to verify that Kuw is the empirical covariance matrix of r independent variables distributed according to (\u03c3(X), \u03c3(Y )) where (X, Y ) \u223c N (0,K\u2190uw ). Given the assumption that all nodes incoming to u are well-initialized, we have,\n\u2016K\u2190uw \u2212K\u2190u\u2016\u221e = \u2225\u2225\u2225\u2225\u2225 \u2211 v\u2208in(v)Kvw |in(v)| \u2212 \u2211 v\u2208in(v)Kv |in(v)| \u2225\u2225\u2225\u2225\u2225 \u221e\n\u2264 1 |in(v)| \u2211 v\u2208in(v) \u2016Kvw \u2212Kv\u2016\u221e (12)\n\u2264 Bdepth(u)\u22121 Bdepth(S) .\nFurther, since \u2264 \u03b3 then K\u2190uw \u2208 M \u03b3 +. Using the fact that \u03c3u is (\u03b1, \u03b2, \u03b3)-decent and that r \u2265 2\u03b12B2 depth(S) log( 8|S| \u03b4 )\n2 , we get that w.p. of at least 1\u2212 \u03b4|S| ,\n\u2016Kuw \u2212 \u03c3\u0304pu (K\u2190uw )\u2016\u221e \u2264\nBdepth(S) . (13)\nFinally, using (12) and (13) along with the fact that \u03c3\u0304 is \u03b2-Lipschitz, we have\n\u2016Kuw \u2212Ku\u2016\u221e = \u2016Kuw \u2212 \u03c3\u0304pu (K\u2190u)\u2016\u221e \u2264 \u2016Kuw \u2212 \u03c3\u0304pu (K\u2190uw )\u2016\u221e + \u2016\u03c3\u0304 p u (K\u2190uw )\u2212 \u03c3\u0304pu (K\u2190u)\u2016\u221e\n\u2264 Bdepth(S) + \u03b2 \u2016K\u2190uw \u2212K\u2190u\u2016\u221e\n\u2264 Bdepth(S) + \u03b2 Bdepth(u)\u22121 Bdepth(S) = Bdepth(u) Bdepth(S) .\nWe are now ready to conclude the proof. Let u1, . . . , u|S| be an ordered list of the nodes in S in accordance to their depth, starting with the shallowest nodes, and ending with the output node. Denote by Aq the event that u1, . . . , uq are well-initialized. We need to show that Pr(A|S|) \u2265 1\u2212 \u03b4. We do so using an induction on q for the inequality Pr(Aq) \u2265 1\u2212 q\u03b4|S| . Indeed, for q = 1, . . . , n, uq is an input node and Pr(Aq) = 1. Thus, the base of the induction hypothesis holds. Assume that q > n. By Claim (3) we have that Pr(Aq|Aq\u22121) \u2265 1 \u2212 \u03b4|S| . Finally, from the induction hypothesis we have,\nPr(Aq) \u2265 Pr(Aq|Aq\u22121) Pr(Aq\u22121) \u2265 (\n1\u2212 \u03b4 |S|\n)( 1\u2212 (q \u2212 1)\u03b4\n|S|\n) \u2265 1\u2212 q\u03b4\n|S| ."}, {"heading": "9.3 Proofs of Thms. 4 and 5", "text": "Theorems 4 and 5 follow from using the following lemma combined with Theorems 5 and 3. When we apply the lemma, we always focus on the special case where one of the kernels is constant w.p. 1.\nLemma 17. Let D be a distribution on X \u00d7 Y, ` : R \u00d7 Y \u2192 R be an L-Lipschitz loss, \u03b4 > 0, and \u03ba1, \u03ba2 : X \u00d7 X \u2192 R be two independent random kernels sample from arbitrary distributions. Assume that the following properties hold.\n\u2022 For some C > 0, \u2200x \u2208 X , \u03ba1(x,x), \u03ba2(x,x) \u2264 C.\n\u2022 \u2200x,y \u2208 X , Pr\u03ba1,\u03ba2 (|\u03ba1(x,y)\u2212 \u03ba2(x,y)| \u2265 ) \u2264 \u03b4\u0303 for \u03b4\u0303 < c2 2\u03b4\nC2 log2( 1\u03b4 ) where c2 > 0 is a\nuniversal constant.\nThen, w.p. \u2265 1 \u2212 \u03b4 over the choices of \u03ba1, \u03ba2, for every f1 \u2208 HM\u03ba1 there is f2 \u2208 H \u221a 2M \u03ba2\nsuch that LD(f2) \u2264 LD(f1) + \u221a 4LM .\nTo prove the above lemma, we state another lemma below followed by a basic measure concentration result.\nLemma 18. Let x1, . . . ,xm \u2208 Rd, w\u2217 \u2208 Rd and > 0. There are weights \u03b11, . . . , \u03b1m such that for w := \u2211m i=1 \u03b1ixi we have,\n\u2022 L(w) := 1 m \u2211m i=1 |\u3008w,xi\u3009 \u2212 \u3008w\u2217,xi\u3009| \u2264\n\u2022 \u2211\ni |\u03b1i| \u2264 \u2016w\u2217\u20162\n\u2022 \u2016w\u2016 \u2264 \u2016w\u2217\u2016\nProof. Denote M = \u2016w\u2217\u2016, C = maxi \u2016xi\u2016, and yi = \u3008w\u2217,xi\u3009. Suppose that we run stochastic gradient decent on the sample {(x1, y1), . . . , (xm, ym)} w.r.t. the loss L(w), with learning rate \u03b7 =\nC2 , and with projections onto the ball of radius M . Namely, we start with w0 = 0 and\nat each iteration t \u2265 1, we choose at random it \u2208 [m] and perform the update,\nw\u0303t = { wt\u22121 \u2212 \u03b7xit \u3008wt\u22121,xit\u3009 \u2265 yit wt\u22121 + \u03b7xit \u3008wt\u22121,xit\u3009 < yit\nwt = { w\u0303t \u2016w\u0303t\u2016 \u2264M Mw\u0303t \u2016w\u0303t\u2016 \u2016w\u0303t\u2016 > M\nAfter T = M 2C2\n2 iterations the loss in expectation would be at most (see for instance\nChapter 14 in [53]). In particular, there exists a sequence of at most M 2C2\n2 gradient steps\nthat attains a solution w with L(w) \u2264 . Each update adds or subtracts C2 xi from the current solution. Hence w can be written as a weighted sum of xi\u2019s where the sum of each coefficient is at most T\nC2 = M\n2\n.\nTheorem 19 (Bartlett and Mendelson [8]). Let D be a distribution over X\u00d7Y, ` : R\u00d7Y \u2192 R a 1-Lipschitz loss, \u03ba : X \u00d7 X \u2192 R a kernel, and , \u03b4 > 0. Let S = {(x1, y1), . . . , (xm, ym)} be i.i.d. samples from D such that m \u2265 cM 2 maxx\u2208X \u03ba(x,x)+log( 1\u03b4 ) 2\nwhere c is a constant. Then, with probability of at least 1\u2212 \u03b4 we have,\n\u2200f \u2208 HM\u03ba , |LD(f)\u2212 LS(f)| \u2264 .\nProof. (of Lemma 17) By rescaling `, we can assume w.l.o.g that L = 1. Let 1 = \u221a M and S = {(x1, y1), . . . , (xm, ym)} \u223c D be i.i.d. samples which are independent of the choice of \u03ba1, \u03ba2. By Theorem 19, for a large enough constant c, if m = c\nCM2 log( 1\u03b4 ) 21 = c C log( 1\u03b4 ) , then\nw.p. \u2265 1\u2212 \u03b4 2 over the choice of the samples we have,\n\u2200f \u2208 HM\u03ba1 \u222aH \u221a 2M \u03ba2 , |LD(f)\u2212 LS(f)| \u2264 1 (14)\nNow, if we choose c2 = 1 2c2 then w.p. \u2265 1 \u2212m2\u03b4\u0303 \u2265 1 \u2212 \u03b4 2 (over the choice of the examples and the kernel), we have that\n\u2200i, j \u2208 [m], |\u03ba1(xi,xj)\u2212 \u03ba2(xi,xj)| < . (15) In particular, w.p. \u2265 1\u2212\u03b4 (14) and (15) hold and therefore it suffices to prove the conclusion of the theorem under these conditions. Indeed, let \u03a81,\u03a82 : X \u2192 H be two mapping from X to a Hilbert space H so that \u03bai(x,y) = \u3008\u03a8i(x),\u03a8i(y)\u3009. Let f1 \u2208 HM\u03ba1 . By lemma 18 there are \u03b11, . . . , \u03b1m so that for the vector w = \u2211m i=1 \u03b11\u03a81(xi) we have\n1\nm m\u2211 i=1 |\u3008w,\u03a81(xi)\u3009 \u2212 f1(xi)| \u2264 1, \u2016w\u2016 \u2264M , (16)\nand m\u2211 i=1 |\u03b1i| \u2264 M2 1 . (17)\nConsider the function f2 \u2208 H2 defined by f2(x) = \u2211m i=1 \u03b11\u3008\u03a82(xi),\u03a82(x)\u3009. We note that\n\u2016f2\u20162Hk2 \u2264 \u2225\u2225\u2225\u2225\u2225 m\u2211 i=1 \u03b1i\u03a82(xi) \u2225\u2225\u2225\u2225\u2225 2\n= m\u2211\ni,j=1\n\u03b1i\u03b1j\u03ba2(xi,xj)\n\u2264 m\u2211\ni,j=1\n\u03b1i\u03b1j\u03ba1(xi,xj) + m\u2211\ni,j=1\n|\u03b1i\u03b1j|\n= \u2016w\u20162 + ( m\u2211 i=1 |\u03b1i| )2 \u2264 M2 + M 4\n21 = 2M2 .\nDenote by f\u03031(x) = \u3008w,\u03a81(x)\u3009 and note that for every i \u2208 [m] we have,\n|f\u03031(xi)\u2212 f2(xi)| = \u2223\u2223\u2223\u2223\u2223 m\u2211 j=1 \u03b1j (\u03ba1(xi,xj)\u2212 \u03ba2(xi,xj)) \u2223\u2223\u2223\u2223\u2223 \u2264\nm\u2211 i=1 |\u03b1i| \u2264 M2 1 = 1 .\nFinally, we get that,\nLD(f2) \u2264 LS(f2) + 1\n= 1\nm m\u2211 i=1 ` (f2(xi), yi) + 1\n\u2264 1 m m\u2211 i=1 ` ( f\u03031(xi), yi ) + 1 + 1\n\u2264 1 m m\u2211 i=1 ` (f1(xi), yi) + |f\u03031(xi)\u2212 f1(xi)|+ 2 1\n\u2264 1 m m\u2211 i=1 ` (f1(xi), yi) + 3 1 \u2264 LS(f1) + 3 1 \u2264 LD(f1) + 4 1 ,\nwhich concludes the proof."}, {"heading": "10 Discussion", "text": "Role of initialization and training. Our results surface the question of the extent to which random initialization accounts for the success of neural networks. While we mostly leave this question for future research, we would like to point to empirical evidence supporting the important role of initialization. First, numerous researchers and practitioners demonstrated that random initialization, similar to the scheme we analyze, is crucial to the success of neural network learning (see for instance [20]). This suggests that starting from arbitrary weights is unlikely to lead to a good solution. Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15]. For example, competitive accuracy on CIFAR-10, STL-10, MNIST and MONO datasets can be achieved by optimizing merely the last layer [36, 49]. Furthermore, Saxe et al. [49] show that the performance of training the last layer is quite correlated with training the entire network. The effectiveness of optimizing solely the last layer is also manifested by the popularity of the random features paradigm [46]. Finally, other studies show that the metrics induced by the initial and fully trained representations are not substantially different. Indeed, Giryes et al. [19] demonstrated that for the MNIST and CIFAR-10 datasets the distances\u2019 histogram of different examples barely changes when moving from the initial to the trained representation. For the ImageNet dataset the difference is more pronounced yet still moderate.\nThe role of architecture. By using skeletons and compositional kernel spaces, we can reason about functions that the network can actually learn rather than merely express. This may explain in retrospect past architectural choices and potentially guide future choices. Let us consider for example the task of object recognition. It appears intuitive, and is supported by visual processing mechanisms in mammals, that in order to perform object recognition,\nthe first processing stages are confined to local receptive fields. Then, the result of the local computations are applied to detect more complex shapes which are further combined towards a prediction. This processing scheme is naturally expressed by convolutional skeletons. A two dimensional version of Example 1 demonstrates the usefulness of convolutional networks for vision and speech applications.\nThe rationale we described above was pioneered by LeCun and colleagues [32]. Alas, the mere fact that a network can express desired functions does not guarantee that it can actually learn them. Using for example Barron\u2019s theorem [7], one may claim that visionrelated functions are expressed by fully connected two layer networks, but such networks are inferior to convolutional networks in machine vision applications. Our result mitigates this gap. First, it enables use of the original intuition behind convolutional networks in order to design function spaces that are provably learnable. Second, as detailed in Example 1, it also explains why convolutional networks perform better than fully connected networks.\nThe role of other architectural choices. In addition to the general topology of the network, our theory can be useful for understanding and guiding other architectural choices. We give two examples. First, suppose that a skeleton S has a fully connected layer with the dual activation \u03c3\u03021, followed by an additional fully connected layer with dual activation \u03c3\u03022. It is straightforward to verify that if these two layers are replaced by a single layer with dual activation \u03c3\u03022 \u25e6 \u03c3\u03021, the corresponding compositional kernel space remains the same. This simple observation can be useful in potentially saving a whole layer in the corresponding networks.\nThe second example is concerned with the ReLU activation, which is one of the most common activations used in practice. Our theory suggests a somewhat surprising explanation for its usefulness. First, the dual kernel of the ReLU activation enables expression of non-linear functions. However, this property holds true for many activations. Second, Theorem 3 shows that even for quite deep networks with ReLU activations, random initialization approximates the corresponding kernel. While we lack a proof at the time of writing, we conjecture that this property holds true for many other activations. What is then so special about the ReLU? Well, an additional property of the ReLU is being positive homogeneous, i.e. satisfying \u03c3(ax) = a\u03c3(x) for all a \u2265 0. This fact makes the ReLU activation robust to small perturbations in the distribution used for initialization. Concretely, if we multiply the variance of the random weights by a constant, the distribution of the generated representation and the space Hw remain the same up to a scaling. Note moreover that training algorithms are sensitive to the initialization. Our initialization is very similar to approaches used in practice, but encompasses a small \u201ccorrection\u201d, in the form of a multiplication by a small constant which depends on the activation. For most activations, ignoring this correction, especially in deep networks, results in a large change in the generated representation. The ReLU activation is more robust to such changes. We note that similar reasoning applies to the max-pooling operation.\nFuture work. Though our formalism is fairly general, we mostly analyzed fully connected and convolutional layers. Intriguing questions remain, such as the analysis of max-pooling and recursive neural network components from the dual perspective. On the algorithmic side, it is yet to be seen whether our framework can help in understanding procedures such as dropout [54] and batch-normalization [25]. Beside studying existing elements of neural network learning, it would be interesting to devise new architectural components inspired by duality. More concrete questions are concerned with quantitative improvements of the main results. In particular, it remains open whether the dependence on 2O(depth(S)) can be made polynomial and the quartic dependence on 1/ , R, and L can be improved. In addition to being interesting in their own right, improving the bounds may further underscore the effectiveness of random initialization as a way of generating low dimensional embeddings of compositional kernel spaces. Randomly generating such embeddings can be also considered on its own, and we are currently working on design and analysis of random features a la Rahimi and Recht [45]."}, {"heading": "Acknowledgments", "text": "We would like to thank Yossi Arjevani, Elad Eban, Moritz Hardt, Elad Hazan, Percy Liang, Nati Linial, Ben Recht, and Shai Shalev-Shwartz for fruitful discussions, comments, and suggestions."}], "references": [{"title": "Learning polynomials with neural networks", "author": ["A. Andoni", "R. Panigrahy", "G. Valiant", "L. Zhang"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1908\u20131916,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep convolutional networks are hierarchical kernel machines", "author": ["F. Anselmi", "L. Rosasco", "C. Tan", "T. Poggio"], "venue": "arXiv:1508.01084,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Network Learning: Theoretical Foundations", "author": ["M. Anthony", "P. Bartlet"], "venue": "Cambridge University Press,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Provable bounds for learning some deep representations", "author": ["S. Arora", "A. Bhaskara", "R. Ge", "T. Ma"], "venue": "Proceedings of The 31st International Conference on Machine Learning, pages 584\u2013592,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Breaking the curse of dimensionality with convex neural networks", "author": ["F. Bach"], "venue": "arXiv:1412.8690,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "On the equivalence between kernel quadrature rules and random feature expansions", "author": ["F. Bach"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Universal approximation bounds for superposition of a sigmoidal function", "author": ["A.R. Barron"], "venue": "IEEE Transactions on Information Theory, 39(3):930\u2013945,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research, 3:463\u2013482,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network", "author": ["P.L. Bartlett"], "venue": "IEEE Transactions on Information Theory, 44(2):525\u2013536, March", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "What size net gives valid generalization", "author": ["E.B. Baum", "D. Haussler"], "venue": "Neural Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Object recognition with hierarchical kernel descriptors", "author": ["L. Bo", "K. Lai", "X. Ren", "D. Fox"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1729\u20131736. IEEE,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1872\u20131886,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Kernel methods for deep learning", "author": ["Y. Cho", "L.K. Saul"], "venue": "Advances in neural information processing systems, pages 342\u2013350,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "The loss surfaces of multilayer networks", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G. Ben Arous", "Y. LeCun"], "venue": "Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pages 192\u2013204,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Beyond simple features: A large-scale feature search approach to unconstrained face recognition", "author": ["D. Cox", "N. Pinto"], "venue": "Automatic Face & Gesture Recognition and Workshops (FG 2011), 2011 IEEE International Conference on, pages 8\u201315. IEEE,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Complexity theoretic limitations on learning halfspaces", "author": ["A. Daniely"], "venue": "STOC,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Complexity theoretic limitations on learning DNFs", "author": ["A. Daniely", "S. Shalev-Shwartz"], "venue": "arXiv:1404.3378 v1,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "From average case complexity to improper learning complexity", "author": ["A. Daniely", "N. Linial", "S. Shalev-Shwartz"], "venue": "STOC,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks with random gaussian weights: A universal classification strategy", "author": ["R. Giryes", "G. Sapiro", "A.M. Bronstein"], "venue": "arXiv preprint arXiv:1504.08291,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, pages 249\u2013256,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "The pyramid match kernel: Discriminative classification with sets of image features", "author": ["K. Grauman", "T. Darrell"], "venue": "Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, volume 2, pages 1458\u20131465. IEEE,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["M. Hardt", "B. Recht", "Y. Singer"], "venue": "arXiv:1509.01240,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1954}, {"title": "Steps toward deep kernel methods from infinite neural networks", "author": ["T. Hazan", "T. Jaakkola"], "venue": "arXiv:1508.05133,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv:1502.03167,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pages 2146\u20132153", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "IEEE,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Random feature maps for dot product kernels", "author": ["P. Kar", "H. Karnick"], "venue": "arXiv:1201.6530,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Some connections between nonuniform and uniform complexity classes", "author": ["R.M. Karp", "R.J. Lipton"], "venue": "Proceedings of the twelfth annual ACM symposium on Theory of computing, pages 302\u2013309. ACM,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1980}, {"title": "Cryptographic limitations on learning Boolean formulae and finite automata", "author": ["M. Kearns", "L.G. Valiant"], "venue": "STOC, pages 433\u2013444, May", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1989}, {"title": "Cryptographic hardness for learning intersections of halfspaces", "author": ["A.R. Klivans", "A.A. Sherstov"], "venue": "FOCS,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "Advances in Neural Information Processing Systems, pages 2177\u20132185,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "On the computational efficiency of training neural networks", "author": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"], "venue": "Advances in Neural Information Processing Systems, pages 855\u2013863,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional kernel networks", "author": ["J. Mairal", "P. Koniusz", "Z. Harchaoui", "Cordelia Schmid"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian learning for neural networks, volume 118", "author": ["R.M. Neal"], "venue": "Springer Science & Business Media,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Path-SGD: Path-normalized optimization in deep neural networks", "author": ["B. Neyshabur", "R. R Salakhutdinov", "N. Srebro"], "venue": "Advances in Neural Information Processing Systems, pages 2413\u20132421,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Norm-based capacity control in neural networks", "author": ["B. Neyshabur", "N. Srebro", "R. Tomioka"], "venue": "COLT,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of boolean functions", "author": ["R. O\u2019Donnell"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "Spherical random features for polynomial kernels", "author": ["J. Pennington", "F. Yu", "S. Kumar"], "venue": "Advances in Neural Information Processing Systems, pages 1837\u20131845,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "An evaluation of the invariance properties of a biologicallyinspired system for unconstrained face recognition", "author": ["N. Pinto", "D. Cox"], "venue": "Bio-Inspired Models of Network, Information, and Computing Systems, pages 505\u2013518. Springer,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "A high-throughput screening approach to discovering good forms of biologically inspired visual representation", "author": ["N. Pinto", "D. Doukhan", "J.J. DiCarlo", "D.D. Cox"], "venue": "PLoS Computational Biology, 5(11):e1000579,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS, pages 1177\u20131184,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["A. Rahimi", "B. Recht"], "venue": "Advances in neural information processing systems, pages 1313\u20131320,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "On the quality of the initial basin in overspecified neural networks", "author": ["I. Safran", "O. Shamir"], "venue": "arxiv:1511.04210,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Theory of reproducing kernels and its applications", "author": ["S. Saitoh"], "venue": "Longman Scientific & Technical England,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1988}, {"title": "On random weights and unsupervised feature learning", "author": ["A. Saxe", "P.W. Koh", "Z. Chen", "M. Bhand", "B. Suresh", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1089\u20131096,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2011}, {"title": "Positive definite functions on spheres", "author": ["I.J. Schoenberg"], "venue": "Duke Mathematical Journal,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1942}, {"title": "Prior knowledge in support vector kernels", "author": ["B. Sch\u00f6lkopf", "P. Simard", "A. Smola", "Vladimir Vapnik"], "venue": "In Proceedings of the 1997 conference on Advances in neural information processing systems", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1998}, {"title": "Provable methods for training neural networks with sparse connectivity", "author": ["H. Sedghi", "A. Anandkumar"], "venue": "arXiv:1412.2693,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["S. Shalev-Shwartz", "S. Ben-David"], "venue": "Cambridge University Press,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, pages 3104\u20133112,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "Computation with infinite neural networks", "author": ["C.K.I. Williams"], "venue": "pages 295\u2013301,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 30, "context": "Neural network (NN) learning has underpinned state of the art empirical results in numerous applied machine learning tasks (see for instance [31, 33]).", "startOffset": 141, "endOffset": 149}, {"referenceID": 32, "context": "Neural network (NN) learning has underpinned state of the art empirical results in numerous applied machine learning tasks (see for instance [31, 33]).", "startOffset": 141, "endOffset": 149}, {"referenceID": 48, "context": "This idea was advocated and tested empirically in [49].", "startOffset": 50, "endOffset": 54}, {"referenceID": 27, "context": "Standard results from complexity theory [28] imply that essentially all functions of interest (that is, any efficiently computable function) can be expressed by a network of moderate size.", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "Biological phenomena show that many relevant functions can be expressed by even simpler networks, similar to convolutional neural networks [32] that are dominant in ML tasks today.", "startOffset": 139, "endOffset": 143}, {"referenceID": 6, "context": "Barron\u2019s theorem [7] states that even two-layer networks can express a very rich set of functions.", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.", "startOffset": 37, "endOffset": 47}, {"referenceID": 8, "context": "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.", "startOffset": 37, "endOffset": 47}, {"referenceID": 2, "context": "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.", "startOffset": 37, "endOffset": 47}, {"referenceID": 39, "context": "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.", "startOffset": 64, "endOffset": 72}, {"referenceID": 21, "context": "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.", "startOffset": 64, "endOffset": 72}, {"referenceID": 28, "context": "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].", "startOffset": 107, "endOffset": 127}, {"referenceID": 29, "context": "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].", "startOffset": 107, "endOffset": 127}, {"referenceID": 16, "context": "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].", "startOffset": 107, "endOffset": 127}, {"referenceID": 17, "context": "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].", "startOffset": 107, "endOffset": 127}, {"referenceID": 15, "context": "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].", "startOffset": 107, "endOffset": 127}, {"referenceID": 46, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 0, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 3, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 11, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 38, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 34, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 18, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 51, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 13, "context": "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].", "startOffset": 121, "endOffset": 155}, {"referenceID": 50, "context": "[51], Grauman and Darrell [21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[51], Grauman and Darrell [21].", "startOffset": 26, "endOffset": 30}, {"referenceID": 35, "context": "Inspired by deep networks\u2019 success, researchers considered deep composition of kernels [36, 13, 11].", "startOffset": 87, "endOffset": 99}, {"referenceID": 12, "context": "Inspired by deep networks\u2019 success, researchers considered deep composition of kernels [36, 13, 11].", "startOffset": 87, "endOffset": 99}, {"referenceID": 10, "context": "Inspired by deep networks\u2019 success, researchers considered deep composition of kernels [36, 13, 11].", "startOffset": 87, "endOffset": 99}, {"referenceID": 45, "context": "For fully connected two-layer networks, the correspondence between kernels and neural networks with random weights has been examined in [46, 45, 38, 56].", "startOffset": 136, "endOffset": 152}, {"referenceID": 44, "context": "For fully connected two-layer networks, the correspondence between kernels and neural networks with random weights has been examined in [46, 45, 38, 56].", "startOffset": 136, "endOffset": 152}, {"referenceID": 37, "context": "For fully connected two-layer networks, the correspondence between kernels and neural networks with random weights has been examined in [46, 45, 38, 56].", "startOffset": 136, "endOffset": 152}, {"referenceID": 55, "context": "For fully connected two-layer networks, the correspondence between kernels and neural networks with random weights has been examined in [46, 45, 38, 56].", "startOffset": 136, "endOffset": 152}, {"referenceID": 45, "context": "Notably, Rahimi and Recht [46] proved a formal connection (similar to ours) for the RBF kernel.", "startOffset": 26, "endOffset": 30}, {"referenceID": 26, "context": "Their work was extended to include polynomial kernels [27, 42] as well as other kernels [6, 5].", "startOffset": 54, "endOffset": 62}, {"referenceID": 41, "context": "Their work was extended to include polynomial kernels [27, 42] as well as other kernels [6, 5].", "startOffset": 54, "endOffset": 62}, {"referenceID": 5, "context": "Their work was extended to include polynomial kernels [27, 42] as well as other kernels [6, 5].", "startOffset": 88, "endOffset": 94}, {"referenceID": 4, "context": "Their work was extended to include polynomial kernels [27, 42] as well as other kernels [6, 5].", "startOffset": 88, "endOffset": 94}, {"referenceID": 12, "context": "Several authors have further explored ways to extend this line of research to deeper, either fully-connected networks [13] or convolutional networks [24, 2, 36].", "startOffset": 118, "endOffset": 122}, {"referenceID": 23, "context": "Several authors have further explored ways to extend this line of research to deeper, either fully-connected networks [13] or convolutional networks [24, 2, 36].", "startOffset": 149, "endOffset": 160}, {"referenceID": 1, "context": "Several authors have further explored ways to extend this line of research to deeper, either fully-connected networks [13] or convolutional networks [24, 2, 36].", "startOffset": 149, "endOffset": 160}, {"referenceID": 35, "context": "Several authors have further explored ways to extend this line of research to deeper, either fully-connected networks [13] or convolutional networks [24, 2, 36].", "startOffset": 149, "endOffset": 160}, {"referenceID": 36, "context": "[37], Levy and Goldberg [34]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[37], Levy and Goldberg [34]).", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "Such topology can be useful in the absence of any prior knowledge of how the output label may be computed from an input example, and it is commonly used in natural language processing where the input is represented as a bag-of-words [23].", "startOffset": 233, "endOffset": 237}, {"referenceID": 54, "context": "[55]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "= \u221a 1\u2212\u03c12+(\u03c0\u2212cos\u22121(\u03c1))\u03c1 \u03c0 arccos1 [13] Step \u221a 2 \u00b7 1[x \u2265 0] 1 2 + \u03c1 \u03c0 + \u03c1 3 6\u03c0 + 3\u03c1 5 40\u03c0 + .", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "= \u03c0\u2212cos \u22121(\u03c1) \u03c0 arccos0 [13] Exponential ex\u22122 1 e + \u03c1 e + \u03c1 2 2e + \u03c1 3 6e + .", "startOffset": 24, "endOffset": 28}, {"referenceID": 35, "context": "= e\u03c1\u22121 RBF [36]", "startOffset": 11, "endOffset": 15}, {"referenceID": 47, "context": "The proofs of all the theorems we quote here are well-known and can be found in Chapter 2 of [48] and similar textbooks.", "startOffset": 93, "endOffset": 97}, {"referenceID": 49, "context": "Theorem 8 (Schoenberg, [50]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 40, "context": "Finally, we use the following facts (see Chapter 11 in [41] and the relevant entry in Wikipedia):", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "(h) For every \u03c3, \u03c3\u0302 is non-decreasing and convex in [0, 1].", "startOffset": 52, "endOffset": 58}, {"referenceID": 35, "context": "Note that the dual of the exponential activation was calculated in [36] and the duals of the step and the ReLU activations were calculated in [13].", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "Note that the dual of the exponential activation was calculated in [36] and the duals of the step and the ReLU activations were calculated in [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "We first claim that there is a unique \u03b1\u03c3 \u2208 [0, 1] such that \u2200x \u2208 (\u22121, \u03b1\u03c3) , \u03c3\u0302(\u03c1) > \u03c1 and \u2200x \u2208 (\u03b1\u03c3, 1) , \u03b1\u03c3 < \u03c3\u0302(\u03c1) < \u03c1 (9)", "startOffset": 43, "endOffset": 49}, {"referenceID": 0, "context": "(b) \u03c3\u0302 is non-decreasing and convex in [0, 1]", "startOffset": 39, "endOffset": 45}, {"referenceID": 0, "context": "Hence, \u03c3\u0302(\u03c1)\u2212 \u03c1 is convex in [0, 1] and in particular intersects with the x-axis at either 0, 1, 2 or infinitely many times in [0, 1].", "startOffset": 29, "endOffset": 35}, {"referenceID": 0, "context": "Hence, \u03c3\u0302(\u03c1)\u2212 \u03c1 is convex in [0, 1] and in particular intersects with the x-axis at either 0, 1, 2 or infinitely many times in [0, 1].", "startOffset": 127, "endOffset": 133}, {"referenceID": 0, "context": "Also, since \u03c3\u0302(1) = 1, we know that there is at least one intersection in [0, 1].", "startOffset": 74, "endOffset": 80}, {"referenceID": 0, "context": "Hence, there are 1 or 2 intersections in [0, 1] and because one of them is in \u03c1 = 1, we conclude that there is at most one intersection in [0, 1).", "startOffset": 41, "endOffset": 47}, {"referenceID": 52, "context": "[53]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "The expression was already calculated in [13], yet we give here a derivation for completeness.", "startOffset": 41, "endOffset": 45}, {"referenceID": 52, "context": "wt = { w\u0303t \u2016w\u0303t\u2016 \u2264M Mw\u0303t \u2016w\u0303t\u2016 \u2016w\u0303t\u2016 > M After T = M 2C2 2 iterations the loss in expectation would be at most (see for instance Chapter 14 in [53]).", "startOffset": 143, "endOffset": 147}, {"referenceID": 7, "context": "Theorem 19 (Bartlett and Mendelson [8]).", "startOffset": 35, "endOffset": 38}, {"referenceID": 19, "context": "First, numerous researchers and practitioners demonstrated that random initialization, similar to the scheme we analyze, is crucial to the success of neural network learning (see for instance [20]).", "startOffset": 192, "endOffset": 196}, {"referenceID": 48, "context": "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 25, "context": "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 43, "context": "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 42, "context": "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 14, "context": "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].", "startOffset": 111, "endOffset": 131}, {"referenceID": 35, "context": "For example, competitive accuracy on CIFAR-10, STL-10, MNIST and MONO datasets can be achieved by optimizing merely the last layer [36, 49].", "startOffset": 131, "endOffset": 139}, {"referenceID": 48, "context": "For example, competitive accuracy on CIFAR-10, STL-10, MNIST and MONO datasets can be achieved by optimizing merely the last layer [36, 49].", "startOffset": 131, "endOffset": 139}, {"referenceID": 48, "context": "[49] show that the performance of training the last layer is quite correlated with training the entire network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "The effectiveness of optimizing solely the last layer is also manifested by the popularity of the random features paradigm [46].", "startOffset": 123, "endOffset": 127}, {"referenceID": 18, "context": "[19] demonstrated that for the MNIST and CIFAR-10 datasets the distances\u2019 histogram of different examples barely changes when moving from the initial to the trained representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "The rationale we described above was pioneered by LeCun and colleagues [32].", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "Using for example Barron\u2019s theorem [7], one may claim that visionrelated functions are expressed by fully connected two layer networks, but such networks are inferior to convolutional networks in machine vision applications.", "startOffset": 35, "endOffset": 38}, {"referenceID": 53, "context": "On the algorithmic side, it is yet to be seen whether our framework can help in understanding procedures such as dropout [54] and batch-normalization [25].", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "On the algorithmic side, it is yet to be seen whether our framework can help in understanding procedures such as dropout [54] and batch-normalization [25].", "startOffset": 150, "endOffset": 154}, {"referenceID": 44, "context": "Randomly generating such embeddings can be also considered on its own, and we are currently working on design and analysis of random features a la Rahimi and Recht [45].", "startOffset": 164, "endOffset": 168}], "year": 2016, "abstractText": "We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power. \u2217Email: amitdaniely@google.com \u2020Email: rf@cs.stanford.edu. Work performed at Google. \u2021Email: singer@google.com ar X iv :1 60 2. 05 89 7v 1 [ cs .L G ] 1 8 Fe b 20 16", "creator": "LaTeX with hyperref package"}}}