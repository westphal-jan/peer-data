{"id": "1602.06725", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2016", "title": "Variational Inference for Monte Carlo Objectives", "abstract": "formal experimentation making automated latent variable models has largely become achieved for the implementation of flexible and scalable variational decision methods. variational training of this type provided maximizing statistical robust bound on the log - likelihood, using samples from varying variational posterior to computed maximum estimated gradients. currently, burda et lab. ( 1956 ) have derived predicted tighter lower ratio using a multi - window importance sampling estimate of all likelihood profiles showed that optimizing rounding exploits strategies that generate more of local variation and display higher likelihoods. algorithm development showed primary importance of such multi - interval assessments and slowed the success of several related approaches.", "histories": [["v1", "Mon, 22 Feb 2016 11:06:06 GMT  (189kb,D)", "http://arxiv.org/abs/1602.06725v1", null], ["v2", "Wed, 1 Jun 2016 16:36:06 GMT  (198kb,D)", "http://arxiv.org/abs/1602.06725v2", "Appears in Proceedings of the 33rd International Conference on Machine Learning (ICML), New York, NY, USA, 2016. JMLR: W&amp;CP volume 48"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["andriy mnih", "danilo jimenez rezende"], "accepted": true, "id": "1602.06725"}, "pdf": {"name": "1602.06725.pdf", "metadata": {"source": "META", "title": "Variational inference for Monte Carlo objectives", "authors": ["Andriy Mnih", "Danilo J. Rezende"], "emails": ["AMNIH@GOOGLE.COM", "DANILOR@GOOGLE.COM"], "sections": [{"heading": null, "text": "We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator (Mnih & Gregor, 2014) proposed for the single-sample variational objective, and is competitive with the currently used biased estimators."}, {"heading": "1. Introduction", "text": "Directed latent variable models parameterized using neural networks have recently enjoyed a surge in popularity due to the recent advances in variational inference methods that made it possible to train such models efficiently. These methods (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014) approximate the intractable posterior of the model with a variational posterior parameterized using a neural network and maximize a lower bound on the intractable marginal log-likelihood, estimating the\nrequired gradients using samples from the variational posterior. This approach implements an efficient feedforward approximation to the expensive iterative process required by traditional variational inference methods for each data point.\nOne important weakness of variational methods is that training a powerful model using an insufficiently expressive variational posterior can cause the model to use only a small fraction of its capacity. The most direct route to addressing this issue to develop more expressive but still tractable variational posteriors as was done in (Salimans et al., 2015; Rezende & Mohamed, 2015; Gregor et al., 2015).\nHowever, the crippling effect of an excessively simple posterior on the model can alternatively be seen as a consequence of the form of the lower bound optimized by the variational methods (Burda et al., 2015). As the bound is based on a single-sample estimate of the marginal likelihood of the observation, it heavily penalizes samples that explain the observation poorly and thus produce low estimates of the likelihood. As result, the variational posterior learns to cover only the high-probability areas of the true posterior, which in turn assumes a simpler shape which is easier to approximate by the variational posterior. A simple way to minimize this effect is to average over multiple samples when computing the marginal likelihood estimate. The resulting lower bound on the log-likelihood gets tighter as the number of samples increases (Burda et al., 2015), converging to the true value in the limit of infinitely many samples. We will refer to the such objectives derived from likelihood estimates computed by averaging over independent samples as Monte Carlo objectives. When using an objective that averages over multiple samples, the distribution for generating samples no longer explicitly represents the variational posterior and instead is thought of as a proposal distribution due to connections to importance sampling.\nMulti-sample objectives of this type have been used for generative modelling (Bornschein & Bengio, 2015; Burda et al., 2015), structured output prediction (Raiko et al., 2015), and models with hard attention (Ba et al., 2015). As a multi-sample objective is a better proxy for the log-\nar X\niv :1\n60 2.\n06 72\n5v 1\n[ cs\n.L G\n] 2\n2 Fe\nb 20\n16\nlikelihood than a single-sample one, models trained using multi-sample objectives are likely to achieve better loglikelihoods. This has been empirically demonstrated in the context of generative models by Burda et al. (2015) and Bornschein & Bengio (2015), who also showed that using more samples in the objective increased the number of latent variables used in the deeper layers.\nUnfortunately, unless all the latent variables in the model are continuous, learning the proposal distribution with a multi-sample objective is difficult as the gradient estimator obtained by differentiating the objective has very high variance. As a result, with the exception of (Burda et al., 2015) which used an alternative estimator available for continuous latent variables, none of the above methods update the parameters of the proposal distribution by following the gradient of the multi-sample objective. Thus, updates for the proposal distribution and the model parameters in these methods are not optimizing the same objective function, which can lead to suboptimal performance and even prevent convergence.\nIn this paper we develop a new unbiased gradient estimator for multi-sample objectives that replaces the single learning signal of the naive estimator with much lower variance per-sample learning signals. Unlike the NVIL estimator (Mnih & Gregor, 2014) designed for single-sample variational objectives, our estimator does not require learning any additional parameters for variance reduction. We expect that the availability of an effective unbiased gradient estimator will make it easier to integrate models with discrete latent variables into larger systems that can be trained end-to-end."}, {"heading": "2. Multi-sample stochastic lower bounds", "text": ""}, {"heading": "2.1. Estimating the likelihood", "text": "Suppose we would like to fit an intractable latent variable model P (x, h) to data. As the intractability of inference rules out using maximum likelihood estimation, we will proceed by maximizing a lower bound on the loglikelihood. One general way to derive such a lower bound is to start with an unbiased estimator I\u0302 of the marginal likelihood P (x) and then transform it. We will consider estimators of the form I\u0302(h1:K) = 1K \u2211K i=1 f(x, h\ni) where h1, ..., hK are independent samples from some distribution Q(h|x) which can potentially depend on the observation x. Before showing how to transform such an estimator into a bound, let us consider some possible choices for the likelihood estimator.\nPerhaps the simplest estimator of this form can be constructed by sampling hi\u2019s from the prior P (h) and aver-\naging the resulting conditional likelihoods:\nI\u0302(h1:K) = 1\nK K\u2211 i=1 P (x|hi) with hi \u223c P (h). (1)\nWhile this estimator is unbiased, it can have very high variance in models where most latent configurations do not explain a given observation well. For such models, the estimator will greatly underestimate the likelihood for most sets of independent K samples and substantially overestimate it for a small number of such sets. This is a consequence of not taking into account the observation we would like the latent variables to explain when sampling them.\nWe can incorporate the information about the observation we are estimating the likelihood for by sampling the latents from a proposal distribution Q(h|x) conditional on the observation x and using importance sampling:\nI\u0302(h1:K) = 1\nK K\u2211 i=1 P (x, hi) Q(hi|x) with h1:K \u223c Q(h1:K |x),\n(2)\nwhere Q(h1:K |x) \u2261 \u220fK i=1Q(h\ni|x). In addition to also being unbiased this estimator can have dramatically lower variance than the preceding one because it can assign high probability to the latent configurations with high joint probability with the given observation. In fact, if we were able to use the true posterior as the proposal distribution, the estimator would have zero variance. While this is infeasible for the models we are considering, this fact suggests that making the proposal distribution close to the posterior is a sensible strategy."}, {"heading": "2.2. Lower-bounding the log-likelihood", "text": "Having chosen an estimator I\u0302 for the likelihood, we can obtain an estimator L\u0302 of a lower bound on the log-likelihood simply by taking the logarithm of I\u0302 . We can justify this by applying Jensen\u2019s inequality:\nEQ(h1:K |x)\n[ log I\u0302(h1:K) ] \u2264 logEQ(h1:K |x) [ I\u0302(h1:K) ] = logP (x),\nwhere the equality follows from the fact that since I\u0302 is unbiased, EQ(h1:K |x)[I\u0302(h1:K)] = P (x). Therefore, we can think of L\u0302(h1:K) = log I\u0302(h1:K) as a stochastic lower bound on the log-likelihood (Burda et al., 2015).\nWe note that this approach is not specific to the to estimators from Section 2.1 and can be used with any unbiased likelihood estimator based on random sampling. Thus it might be possible to obtain better lower bounds by using methods from the importance sampling literature such as control variates and adaptive importance sampling.\nDespite the potential pitfalls described above, estimators involving sampling from the prior have been used successfully for training models for structured output prediction (Tang & Salakhutdinov, 2013; Dauphin & Grangier, 2015) and models with hard attention (Mnih et al., 2014; Zaremba & Sutskever, 2015).\nThe multi-sample (K > 1) version of above estimator has recently been used in variational training of latent variable models (Burda et al., 2015; Bornschein & Bengio, 2015) as well as models with hard attention (Ba et al., 2015). The single-sample version of the estimator yields the classical variational lower bound (Jordan et al., 1999)\nL(x) = EQ(h|x) [ log P (x, h)\nQ(h|x)\n] , (3)\nwhich is used as the objective by much of the recent work on training generative models (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014).\nThe advantage of using of multi-sample stochastic lower bounds is that increasing the number of samples K is guaranteed to make the bound tighter (Burda et al., 2015), thus making it a better proxy of the log-likelihood. Intuitively, averaging over the K samples inside the log, removes the burden of every sample having to explain the observation well, which leads to the proposal distribution being considerably less concentrated than the variational posterior, which is its single-sample counterpart. Training models by optimizing a multi-sample objective can be seen as a generalization of variational training that does not explicitly represent the variational posterior."}, {"heading": "2.3. Objective", "text": "Thus we will be interested in training models by maximizing objectives of the form\nLK(x) =EQ(h1:K |x)\n[ log 1\nK K\u2211 i=1 f(x, hi)\n] , (4)\nwhich can be seen as lower bounds on the log-likelihood. This class of objectives is a rich one, including the ones used in variational inference, generative modelling, structured prediction, and hard attention."}, {"heading": "2.4. Gradient analysis", "text": "In this section we will analyze the gradient of the objective w.r.t. the parameters of the model and the proposal distribution and explain why developing an effective unbiased estimator for the gradient is difficult in general. In the special case of continuous latent variables an alternative approach to gradient estimation based on reparameterization (Kingma & Welling, 2014; Burda et al., 2015) is likely to be\npreferable to the more general approach we follow in this paper, which is applicable to all types of latent variables.\nAs shown in the supplementary material, differentiating LK(x) w.r.t. the parameters of Q and f gives\n\u2207\u03b8LK(x) =EQ(h1:K |x) \u2211 j L\u0302(h1:K)\u2207\u03b8 logQ(hj |x) + EQ(h1:K |x) \u2211 j w\u0303j\u2207\u03b8 log f(x, hj)\n , (5) where w\u0303j \u2261 f(x,h\nj)\u2211K i=1 f(x,h i) .\nAs our objective LK(x) is an expectation of the stochastic lower bound L\u0302(h1:K) w.r.t. to the proposal distribution, it can depend on any given parameter through the proposal distribution, through the value of the stochastic lower bound as a function of a set of K samples, or both. Intuitively, the first term and the second terms in Eq. 5 capture the effect of \u03b8 on LK(x) though its effect on the proposal distribution and the value of the stochastic lower bound as a function of a set of samples respectively.\nLet us inspect these two terms, both of which are linear combinations of the gradients corresponding to theK samples. The second term is well behaved and is easy to estimate because the weights {w\u0303j} are non-negative and sum to 1, ensuring that the norm of the linear combination of the gradients is at most as large as the norm of the largest of the K gradients. In mixture modelling terms, we can think of w\u0303j as the responsibility of sample j for the observation x \u2014 a measure of how well sample j explains the observation compared to the other K \u2212 1 samples.\nThe first term however is considerably more problematic for two reasons. First, the gradients for all K samples are multiplied by the same scalar L\u0302(h1:K), which can be thought of as the learning signal for the proposal distribution (Mnih & Gregor, 2014). As a result the gradient for a sample that explains the observation well is not given any more weight than the gradient for a sample in the same set of K that explains the observation poorly. This means that the first term does not implement credit assignment within each set ofK samples, unlike the second term which achieves that by weighting the gradients using the responsibilities. As a result, the learning signal for each sample hi will have high variance and make learning slow.1\nAnother important source of variance when estimating the first term is the magnitude of the learning signal. Unlike the responsibilities used in the second term, which are between\n1 Despite not performing credit assignment within sets of K samples, the first term does perform correct credit assignment in expectation over such sets.\n0 and 1, the learning signal can have potentially unbounded magnitude, which means that the norm of the first term can become much larger than the norm of any of the individual sample gradients. This issue can be especially pronounced early in training, when all samples from the proposal Q explain the data poorly, resulting in a very small I\u0302(h1:K) and thus a very negative learning signal. Thus unless special measures are taken, the first term in the gradient will overwhelm the second term and make the overall estimate very noisy."}, {"heading": "2.5. Gradient estimation", "text": "The difficulties described in the previous section affect only the gradient for the parameters the sampling distribution depends on. For all other parameters \u03c8 the first term is identically zero, which leaves only the second, wellbehaved term. As a result, the following naive Monte Carlo estimator based on averaging over m sets of K samples works well:\n\u2207\u03c8LK(x) ' 1\nm m\u2211 l=1 \u2211 j w\u0303j,m\u2207\u03c8 log f(x, hj,m)  , (6) where hi,m \u223c Q(h|x). It is common practice to use a single set of samples (m = 1) for all such estimators, relying on averaging over the training cases in a minibatch to reduce variance to a reasonable level. We will also adopt this approach in this paper and thus will not include a summation over m in the remaining estimator expressions. Now we will now turn out attention to the more challenging problem of estimating gradients for parameters than affect the proposal distribution."}, {"heading": "2.5.1. NAIVE", "text": "We will start with the simplest estimator, also based on naive Monte Carlo:\n\u2207\u03b8LK(x) ' \u2211 j L\u0302(h\n1:K)\u2207\u03b8 logQ(hj |x) + \u2211 j w\u0303 j\u2207\u03b8 log f(x, hj), (7)\nwith hi \u223c Q(h|x). This estimator does not attempt to eliminate either of the two sources of variance described in Section 2.4 and we include it here for completeness only."}, {"heading": "2.5.2. WITH BASELINES (NVIL)", "text": "One simple way to reduce the variance due the large magnitude of the learning signal is to reduce its magnitude by subtracting a quantity, called a baseline, correlated with the learning signal but not dependent on the latent variables. This transformation of the learning signal leaves the gradient estimator unbiased because it amounts to subtracting a term with the expectation of 0 under the proposal distribution. In our use of baselines, we will follow the Neural\nVariational Inference and Learning (NVIL, Mnih & Gregor, 2014) method for training generative models, which is based on optimizing the classical variational lower bound (Eq. 3) The basic idea behind the NVIL estimator is to reduce the magnitude of the learning signal for the parameters of the variational distribution (which is the singlesample counterpart of our proposal distribution) by subtracting two baselines from it: a constant baseline b as well as an input-dependent one b(x).\nThe following estimator is a straightforward adaptation of the same idea to multi-sample objectives:\n\u2207\u03b8LK(x) ' \u2211 j(L\u0302(h\n1:K)\u2212 b(x)\u2212 b)\u2207\u03b8 logQ(hj |x) + \u2211 j w\u0303 j\u2207\u03b8 log f(x, hj), (8)\nwith hi \u223c Q(h|x). The constant baseline b tracks the mean of the learning signal, while the input-dependent is fit to minimize the squared residual of the learning signal L\u0302(h1:K)\u2212 b(x)\u2212 b with the goal of capturing the effect of the observation on the magnitude of the learning signal. We implement the input dependent baseline using a one-hidden layer neural network.\nWhile introducing baselines can addresses the estimator variance due to the large magnitude of the learning signal, it has no effect on the variance due to having the same learning signal for all samples in a set of K."}, {"heading": "2.5.3. LOCAL LEARNING SIGNALS", "text": "We can reduce the effect of the second source of variance by defining a different local learning signal for each sample in a way that minimizes its dependence on the other samples in the set. This can be accomplished by using a separate baseline for each sample that depends on the value of all other samples and eliminates much of the variance due to them. We will now show that this approach does not bias the resulting estimator.\nLet h\u2212j denote the set of K \u2212 1 samples obtained by leaving out sample j from the original set. Since the samples in a set are independent, evaluating the expectations with respect to them in any order produces the same result. Thus the contribution of sample j to the first term in Eq. 5 can be expressed as\nEQ(h1:K |x) [ L\u0302(h1:K)\u2207\u03b8 logQ(hj |x) ] =\nEQ(h\u2212j |x) [ EQ(hj |x) [ L\u0302(h1:K)\u2207\u03b8 logQ(hj |x) \u2223\u2223\u2223h\u2212j]] . Since in the inner-most expectation all samples except for hj are conditioned on, adding any function of them to the learning signal for hj has no effect on the value of the expectation. Thus we can define a baseline that depends on h\u2212j in addition to x. We would like this baseline to be as\ncorrelated with L\u0302(h1:K) as possible without using the value of hj .\nInspecting the global learning signal L\u0302(h1:K) (Eq. 4) suggests that an effective baseline for the local learning signal for sample j can be obtained by replacing f(x, hi) in it by some quantity close f(x, hj) but independent of hj . We can, for example, use some mapping f(x) trained to predict f(x, hi) from the observation x. This gives rise to the following local learning signal for sample j:\nL\u0302(hj |h\u2212j) = L\u0302(h1:K)\u2212 log 1 K \u2211 i 6=j f(x, hi) + f(x)  . (9)\nFor K = 1, this estimator becomes essentially equivalent to the NVIL estimator, with log f(x) corresponding to the input-dependent baseline b(x).\nWe can avoid having to learn an additional mapping by taking advantage of the fact that we have more than one sample in a set. Since the samples in a set are IID, so are the corresponding values f(x, hi), which means that we can get a reasonable estimate of f(x, hj) by taking the mean of the f(x, hi) values for all the other samples in a set. This estimator of f(x, hj) is unbiased, unlike a learned f(x) which is likely to be biased due to the limited capacity of the model used for prediction as well as optimization dynamics. After some simplification, the resulting local learning signals can be written as\nL\u0302(hj |h\u2212j) = L\u0302(h1:K)\u2212 log 1 K \u2212 1 \u2211 i 6=j f(x, hi). (10)\nThis approach to variance reduction, unlike the NVIL one or the one given by Eq. 9, does not require learning any additional parameters for performing variance reduction. Also, as the total cost of computing the per-sample learning signals in Eq. 10 is of the same order as that of computing of the global learning signal, this approach allows us to implement effective variance reduction in the multi-sample case essentially at no cost. This approach relies on having more than one sample for the same observation, however, and so is not applicable in the single-sample setting.\nThe final estimator has the form \u2207\u03b8LK(x) ' \u2211 j L\u0302(hj |h\u2212j)\u2207\u03b8 logQ(hj |x)\n+ \u2211 j w\u0303j\u2207\u03b8 log f(x, hj). (11)\nWe will refer to this estimator as the VIMCO (Variational Inference for Monte Carlo Objectives) estimator.\nWe note that the resulting estimator is a black-box one, in the sense that it can be easily applied to any model\nfor which we can compute the complete log-likelihood logP (x, h) and its parameter gradients exactly. As such it, can be seen as as an alternative to Black Box Variational Inference (Ranganath et al., 2014) and NVIL, specialized for multi-sample objectives."}, {"heading": "3. Structured output prediction", "text": "Structured output prediction (SOP) is a type of supervised learning with high-dimensional outputs with rich structure such as images or text. The particular emphasis of SOP is on capturing the dependencies between the output variables in addition to capturing their dependence on the inputs. Here we will take the approach of viewing SOP as conditional probabilistic modelling with latent variables (Tang & Salakhutdinov, 2013; Sohn et al., 2015).\nTo stay consistent with the terminology for generative models we used so far, we will refer to inputs as contexts and to outputs as observations Thus, given a set of context/observation pairs (c, x), we would like to fit a latent variable model P (x, h|c) to capture the dependencies between the contexts and the observations as well as those between the observed dimensions. Typically such a model factorizes as P (x, h|c) = P (x|h, c)P (h|c), with both the conditional likelihood and the prior terms being conditional on the context. Thus, this is essentially the same setting as for generative modelling, with the only difference being that every distribution now also conditions on the context c, which makes it straightforward to apply the estimators we presented.\nHowever, historically such models have been trained using samples from the prior P (h|c), with the gradients computed using either importance sampling (Tang & Salakhutdinov, 2013) or heuristic rules for backpropagating through binary units (Raiko et al., 2015). As using the prior as the proposal distribution does not allow it to use the information about the observation, such methods tend to require a large number of samples to perform well. Though recently variational training has been applied to SOP models with continuous latent variables (Sohn et al., 2015), we are not aware of any work that used a learned proposal distribution conditional on the observations to train SOP models with multi-sample objectives. We will explore the effectiveness of using this approach in Section 5.2."}, {"heading": "4. Related work", "text": "Multi-sample objectives: The idea of using a multisample objective for a latent variable model was proposed by Raiko et al. (2015), who thought of it not as a lower bound on the log-likelihood but an objective in its own right. They evaluated several gradient estimators at optimizing it for training structured prediction models and\nshowed that a simple biased estimator emulating backpropagation performed best. Tang & Salakhutdinov (2013) proposed an estimator based on importance sampling for an EM-like bound on the log-likelihood using samples from the prior. This is also a biased estimator as it relies on selfnormalized importance sampling to approximate the posterior using a set of weighted samples. Burda et al. (2015) pointed out that the multi-sample objective of Raiko et al. (2015) was a tighter lower bound on the log-likelihood that the single-sample variational lower bound and presented a method for training variational autoencoders by optimizing this multi-sample objective. Their method relies on an unbiased gradient estimator which can be used only for models with continuous latent variables.\nReweighted Wake Sleep: Though the Reweighted Wake Sleep algorithm (RWS, Bornschein & Bengio, 2015) for training generative models has been derived from the perspective of approximating the log-likelihood gradients using importance sampling, it is closely related to the bound optimization approach we follow in this paper. Burda et al. (2015) have shown that the RWS gradient estimator for the model parameters is identical to the one given in by Eq. 6, which means that the RWS model parameter update aims to maximize the lower bound on the log-likelihood based on the multi-sample importance sampling estimator from Eq. 2. The RWS performs two types of updates for the proposal distribution parameters, the first of which, called the wake update is based on the same weights {w\u0303j} as the model parameter update\n\u2206\u03b8 \u221d \u2211 j w\u0303 j\u2207\u03b8 logQ(hj |x) (12)\nand is motivated as a (biased) estimator of the gradient KL(P (h|x)||Q(h|x)). This bias decreases with the increasing number of samples, vanishing in the limit of infinitely many samples.\nThe second update, called the sleep update, is based on a sample (x, h) from the model and has the form\n\u2206\u03b8 \u221d \u2207\u03b8 logQ(h|x), (13)\ncomes from the original Wake-Sleep algorithm (Hinton et al., 1995). The wake update tends to work better than the sleep update, using the two updates together works even better (Bornschein & Bengio, 2015) As neither of these updates appears to be related to the lower bound optimized the model parameter update of RWS, RWS does not seem to optimize a well-defined objective, a feature it shares with the original wake-sleep algorithm. Despite this theoretical weakness RWS works well in practice, outperforming the original wake-sleep and NVIL, which are single-sample algorithms, when using as few as 5 samples per observation.\nBlack Box Methods: As our approach does not assume anything about the structure of the model or the distribution(s) of its latent variables, it can be seen as a black box\nmethod for multi-sample objectives. A number of black box methods have been developed for the classical variational objective, usually based around unbiased gradient estimators for the proposal distribution. Black Box Variational Inference (BBVI Ranganath et al., 2014) and NVIL (Mnih & Gregor, 2014) are two such methods.\nOur method shares some similarities with the black box method of the local expectations (LE) of Titsias & L\u00e1zaroGredilla (2015). The LE method provides a relatively low variance unbiased estimator based on local learning signals derived from computing an exact expectation w.r.t. each variable in the model. Both methods work well without baselines and involve considering multiple values for latent variables. Unlike our method, the LE method optimizes a single-sample objective and requires computing exact expectations for each variable, which makes it much more computationally expensive."}, {"heading": "5. Results", "text": "We evaluate the effectiveness of the proposed approach at training models for generative modelling and structured output prediction. We chose these two tasks because they involve models with hundreds of latent variables, which poses formidable challenges when estimating the gradients for the proposal distributions. In both cases we compare the performance of the VIMCO estimator to that of the NVIL estimator as well as to an effective biased estimator from the literature. We experiment with varying the number of samples in the objective and see how that affects the performance of the resulting models when using different estimators. The details of the training procedure are given in the supplementary material."}, {"heading": "5.1. Generative modelling", "text": "We start by applying the proposed estimator to training generative models, concentrating on sigmoid belief network (SBN) models (Neal, 1992) which consist of layers of binary latent variables. SBNs have been used to evaluate a number of variational training methods for models with discrete latent variables (Mnih & Gregor, 2014; Bornschein & Bengio, 2015; Gu et al., 2015).\nOur first comparison is on the MNIST dataset of 28 \u00d7 28 images of handwritten digits, using the binarization of Salakhutdinov & Murray (2008). We use the standard 50000/10000/10000 split into the training, validation, and test sets. We use an SBN with three hidden layers of 200 binary latent variables (200-200-200-768) as a generative model. The proposal distribution is parameterized as an SBN with the same architecture but going in the opposite direction, from the observation to the deepest hidden layer (768-200-200-200).\nAs our primary goal is here is to see how well the VIMCO estimator performs at optimizing the multisample objective, we train the above model using each of the VIMCO, NVIL, and RWS estimators to optimize the lower bound (Eq. 4) based on 2, 5, 10, and 50 samples (K). To match the computational complexity of the other two estimators, we used only the better-performing wake update (Eq. 12) for the proposal distribution in RWS. To have a single-sample baseline for comparison, we also train the model by optimizing the classical variational objective (K = 1) using NVIL. In all cases, the model parameter gradients were estimated using Eq. 6.\nFigure 1 shows the evolution of the training objective on the validation set as training proceeds. From the left plot, which compares the models trained using VIMCO to those trained NVIL. It is apparent that VIMCO is far more effective than NVIL at optimizing the multi-sample objective and benefits much more from using more samples. NVIL performance improves slightly when using a modest number of samples and gets worse for K = 50. The right plot shows the comparison between VIMCO and RWS. The two methods perform similarly, with VIMCO performing better when using 2 samples and RWS learning slightly when using more samples.\nHaving selected the best model for each method/number of samples combination based on its validation score, we estimated their negative log-likelihoods on the test set using 1000 proposal samples for each data point. The results in Table 1 show that VIMCO and NVIL perform slightly better than RWS for 2 and 5 samples. However, once the number of samples goes from 5 to 10, NVIL performance starts degrading while the performance of VIMCO and RWS continues to improve. Overall, the performance of RWS and VIMCO is similar, though VIMCO seems to have a slight edge over this version of RWS for all numbers\nof samples."}, {"heading": "5.2. Structured output prediction", "text": "In the second set of experiments we evaluated the proposed estimator at training structured output prediction models. We chose a task that has been used as a benchmark for evaluating gradient estimators for models with binary latent variables by Raiko et al. (2015) and Gu et al. (2015), which involves predicting the lower half of an MNIST digit from its top half. We trained two SBN models, one with two and one with three layers of 200 binary latent variables between the 392-dimensional (14\u00d7 28) input and output layers. We use the same binarized MNIST dataset for this task as for the generative modelling experiments in Section 5.1.\nWe consider using two different kinds of proposal distributions for training the model. In the first case, we follow the standard practice for training structured output prediction models and use the model prior as the proposal distribution. However, as the prior does not have access to the observation information which is available during training, most of the resulting samples are unlikely to explain the observation well, potentially leading to inefficient use of samples and unnecessarily noisy learning signal. Hence, in the second case we learn a separate proposal distribution that takes as input both the context and the observation halves of the\nimage. We parameterize the proposal distribution using an SBN with the same structure as the prior except that the last layer of latent variables in addition on being conditioned on the preceding layer is also conditioned on the observation.\nWe train the models using each proposal distribution using VIMCO and NVIL using 2, 5, 20, and 50 sample objectives. As in the previous experiment, we also produce a single-sample baseline by training the models using both types of proposals with NVIL (andK = 1). Figure 2 shows the resulting multi-sample bound values for the three-layer models on the validation set as a function of the number of parameter updates. The left plot, containing the results for models trained by sampling from the prior, shows that the model performance improves dramatically as the number of samples is increased. Though NVIL with 1 or 2 samples, performs better than VIMCO with 2 samples, as the number of samples increases their roles reverse, with VIMCO making much faster progress than NVIL for 20 and 50 samples. The fact that using more samples has such a strong effect on model performance strongly suggests that samples generated from the prior rarely explain the observation well.\nThe right plot on Figure 2 shows the result of training using a learned proposal distribution. It is clear that using a learned proposal leads to drastic improvement for all method / number of samples combinations. In fact, the worst result obtained using a learned proposal distribution is better than the best result obtained by sampling from the prior. In terms of relative performance, the story here is similar to that from the generative modelling experiment: VIMCO performs better than NVIL and benefits much more from increasing the number of samples. The gap between the methods is considerably smaller here, which is likely due to the task being easier. Inspecting the conditional digit completions obtained by sampling from\nthe models shows that the models trained using a learned proposal distribution capture multimodality inherent in the task very well. We show conditional completions from a three-layer model trained using VIMCO with 20 samples in the supplementary material.\nFinally, to compare to the results of Raiko et al. (2015), we followed their evaluation protocol and estimated the negative log-likelihoods for the trained models using 100 latent samples. Their best result on this task was 53.8 nats, obtained using the 2-layer SBN trained using a biased estimator emulating backprop to optimize the 20-sample objective. With VIMCO training, the same model achieves 59.7 nats using the prior as the proposal and 44.2 nats with a learned proposal, which is the first sub-50 nat result on this task."}, {"heading": "6. Discussion", "text": "In this paper we introduced VIMCO, the first unbiased general gradient estimator designed specifically for multisample objectives that generalize the classical variational lower bound. By taking advantage of the structure of the objective function, it implements simple and effective variance reduction at no extra computational cost, eliminating the need for the learned baselines relied on by other general unbiased estimators such as NVIL.\nWe demonstrated the effectiveness of the VIMCO by applying it to variational training of generative and structured output prediction models. VIMCO consistently outperformed NVIL and was competitive with the currently used biased estimators.\nWhile classical variational methods can perform poorly when using an insufficiently expressive variational posterior, multi-sample objectives provide a graceful way of\ntrading computation for quality of fit simply by increasing the number of samples used inside the objective. Combining such objectives with black box variational inference methods could make the latter substantially more effective. We thus hope that the proposed approach will increase the appeal and applicability of black box variational inference."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank Alex Graves, Guillaume Desjardins, Koray Kavukcuoglu, and Volodymyr Mnih for their helpful comments."}], "references": [{"title": "Learning wake-sleep recurrent attention models", "author": ["Ba", "Jimmy", "Salakhutdinov", "Ruslan R", "Grosse", "Roger B", "Frey", "Brendan J"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Importance weighted autoencoders", "author": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1509.00519,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "Predicting distributions with linearizing belief networks", "author": ["Dauphin", "Yann N", "Grangier", "David"], "venue": "arXiv preprint arXiv:1511.05622,", "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "MuProp: Unbiased backpropagation for stochastic neural networks", "author": ["Gu", "Shixiang", "Levine", "Sergey", "Sutskever", "Ilya", "Mnih", "Andriy"], "venue": "arXiv preprint arXiv:1511.05176,", "citeRegEx": "Gu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "The \"wake-sleep\" algorithm for unsupervised neural networks", "author": ["Hinton", "Geoffrey E", "Dayan", "Peter", "Frey", "Brendan J", "Neal", "Radford M"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Connectionist learning of belief networks", "author": ["Neal", "Radford M"], "venue": "Artificial intelligence,", "citeRegEx": "Neal and M.,? \\Q1992\\E", "shortCiteRegEx": "Neal and M.", "year": 1992}, {"title": "Techniques for learning binary stochastic feedforward neural networks", "author": ["Raiko", "Tapani", "Berglund", "Mathias", "Alain", "Guillaume", "Dinh", "Laurent"], "venue": null, "citeRegEx": "Raiko et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2015}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "On the quantitative analysis of Deep Belief Networks", "author": ["Salakhutdinov", "Ruslan", "Murray", "Iain"], "venue": "In Proceedings of the 25th Annual International Conference on Machine Learning", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["Sohn", "Kihyuk", "Lee", "Honglak", "Yan", "Xinchen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sohn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 2015}, {"title": "Learning stochastic feedforward neural networks", "author": ["Tang", "Yichuan", "Salakhutdinov", "Ruslan R"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}, {"title": "Local expectation gradients for black box variational inference", "author": ["Titsias", "Michalis", "L\u00e1zaro-Gredilla", "Miguel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Titsias et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Titsias et al\\.", "year": 2015}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Recently, Burda et al. (2015) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods.", "startOffset": 10, "endOffset": 30}, {"referenceID": 14, "context": "These methods (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014) approximate the intractable posterior of the model with a variational posterior parameterized using a neural network and maximize a lower bound on the intractable marginal log-likelihood, estimating the required gradients using samples from the variational posterior.", "startOffset": 14, "endOffset": 81}, {"referenceID": 16, "context": "The most direct route to addressing this issue to develop more expressive but still tractable variational posteriors as was done in (Salimans et al., 2015; Rezende & Mohamed, 2015; Gregor et al., 2015).", "startOffset": 132, "endOffset": 201}, {"referenceID": 3, "context": "The most direct route to addressing this issue to develop more expressive but still tractable variational posteriors as was done in (Salimans et al., 2015; Rezende & Mohamed, 2015; Gregor et al., 2015).", "startOffset": 132, "endOffset": 201}, {"referenceID": 1, "context": "However, the crippling effect of an excessively simple posterior on the model can alternatively be seen as a consequence of the form of the lower bound optimized by the variational methods (Burda et al., 2015).", "startOffset": 189, "endOffset": 209}, {"referenceID": 1, "context": "The resulting lower bound on the log-likelihood gets tighter as the number of samples increases (Burda et al., 2015), converging to the true value in the limit of infinitely many samples.", "startOffset": 96, "endOffset": 116}, {"referenceID": 1, "context": "Multi-sample objectives of this type have been used for generative modelling (Bornschein & Bengio, 2015; Burda et al., 2015), structured output prediction (Raiko et al.", "startOffset": 77, "endOffset": 124}, {"referenceID": 12, "context": ", 2015), structured output prediction (Raiko et al., 2015), and models with hard attention (Ba et al.", "startOffset": 38, "endOffset": 58}, {"referenceID": 0, "context": ", 2015), and models with hard attention (Ba et al., 2015).", "startOffset": 40, "endOffset": 57}, {"referenceID": 1, "context": "This has been empirically demonstrated in the context of generative models by Burda et al. (2015) and Bornschein & Bengio (2015), who also showed that using more samples in the objective increased the number of latent variables used in the deeper layers.", "startOffset": 78, "endOffset": 98}, {"referenceID": 1, "context": "This has been empirically demonstrated in the context of generative models by Burda et al. (2015) and Bornschein & Bengio (2015), who also showed that using more samples in the objective increased the number of latent variables used in the deeper layers.", "startOffset": 78, "endOffset": 129}, {"referenceID": 1, "context": "As a result, with the exception of (Burda et al., 2015) which used an alternative estimator available for continuous latent variables, none of the above methods update the parameters of the proposal distribution by following the gradient of the multi-sample objective.", "startOffset": 35, "endOffset": 55}, {"referenceID": 1, "context": "Therefore, we can think of L\u0302(h) = log \u00ce(h) as a stochastic lower bound on the log-likelihood (Burda et al., 2015).", "startOffset": 94, "endOffset": 114}, {"referenceID": 9, "context": "Despite the potential pitfalls described above, estimators involving sampling from the prior have been used successfully for training models for structured output prediction (Tang & Salakhutdinov, 2013; Dauphin & Grangier, 2015) and models with hard attention (Mnih et al., 2014; Zaremba & Sutskever, 2015).", "startOffset": 260, "endOffset": 306}, {"referenceID": 1, "context": "The multi-sample (K > 1) version of above estimator has recently been used in variational training of latent variable models (Burda et al., 2015; Bornschein & Bengio, 2015) as well as models with hard attention (Ba et al.", "startOffset": 125, "endOffset": 172}, {"referenceID": 0, "context": ", 2015; Bornschein & Bengio, 2015) as well as models with hard attention (Ba et al., 2015).", "startOffset": 73, "endOffset": 90}, {"referenceID": 6, "context": "The single-sample version of the estimator yields the classical variational lower bound (Jordan et al., 1999)", "startOffset": 88, "endOffset": 109}, {"referenceID": 14, "context": "which is used as the objective by much of the recent work on training generative models (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014).", "startOffset": 88, "endOffset": 155}, {"referenceID": 1, "context": "The advantage of using of multi-sample stochastic lower bounds is that increasing the number of samples K is guaranteed to make the bound tighter (Burda et al., 2015), thus making it a better proxy of the log-likelihood.", "startOffset": 146, "endOffset": 166}, {"referenceID": 1, "context": "In the special case of continuous latent variables an alternative approach to gradient estimation based on reparameterization (Kingma & Welling, 2014; Burda et al., 2015) is likely to be preferable to the more general approach we follow in this paper, which is applicable to all types of latent variables.", "startOffset": 126, "endOffset": 170}, {"referenceID": 17, "context": "Here we will take the approach of viewing SOP as conditional probabilistic modelling with latent variables (Tang & Salakhutdinov, 2013; Sohn et al., 2015).", "startOffset": 107, "endOffset": 154}, {"referenceID": 12, "context": "However, historically such models have been trained using samples from the prior P (h|c), with the gradients computed using either importance sampling (Tang & Salakhutdinov, 2013) or heuristic rules for backpropagating through binary units (Raiko et al., 2015).", "startOffset": 240, "endOffset": 260}, {"referenceID": 17, "context": "Though recently variational training has been applied to SOP models with continuous latent variables (Sohn et al., 2015), we are not aware of any work that used a learned proposal distribution conditional on the observations to train SOP models with multi-sample objectives.", "startOffset": 101, "endOffset": 120}, {"referenceID": 12, "context": "Multi-sample objectives: The idea of using a multisample objective for a latent variable model was proposed by Raiko et al. (2015), who thought of it not as a lower bound on the log-likelihood but an objective in its own right.", "startOffset": 111, "endOffset": 131}, {"referenceID": 1, "context": "Burda et al. (2015) pointed out that the multi-sample objective of Raiko et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Burda et al. (2015) pointed out that the multi-sample objective of Raiko et al. (2015) was a tighter lower bound on the log-likelihood that the single-sample variational lower bound and presented a method for training variational autoencoders by optimizing this multi-sample objective.", "startOffset": 0, "endOffset": 87}, {"referenceID": 1, "context": "Burda et al. (2015) have shown that the RWS gradient estimator for the model parameters is identical to the one given in by Eq.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "comes from the original Wake-Sleep algorithm (Hinton et al., 1995).", "startOffset": 45, "endOffset": 66}, {"referenceID": 4, "context": "SBNs have been used to evaluate a number of variational training methods for models with discrete latent variables (Mnih & Gregor, 2014; Bornschein & Bengio, 2015; Gu et al., 2015).", "startOffset": 115, "endOffset": 180}, {"referenceID": 11, "context": "We chose a task that has been used as a benchmark for evaluating gradient estimators for models with binary latent variables by Raiko et al. (2015) and Gu et al.", "startOffset": 128, "endOffset": 148}, {"referenceID": 4, "context": "(2015) and Gu et al. (2015), which involves predicting the lower half of an MNIST digit from its top half.", "startOffset": 11, "endOffset": 28}, {"referenceID": 12, "context": "Finally, to compare to the results of Raiko et al. (2015), we followed their evaluation protocol and estimated the negative log-likelihoods for the trained models using 100 latent samples.", "startOffset": 38, "endOffset": 58}], "year": 2017, "abstractText": "Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2015) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multisample objectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator (Mnih & Gregor, 2014) proposed for the single-sample variational objective, and is competitive with the currently used biased estimators.", "creator": "LaTeX with hyperref package"}}}