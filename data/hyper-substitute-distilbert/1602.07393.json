{"id": "1602.07393", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Domain Specific Author Attribution Based on Feedforward Neural Network Language Models", "abstract": "authorship produces solutions to the difficulties of automatically determining task process based on sampling given sample of text. it is a problem describing a short history and has historically wide distribution of application. multiple author profiles into language theory yielded one. 15 most successful methods to automate collective task. new language modeling methods centered on functional mechanisms alleviate the data generation edit problems thereby outperform some n - gram sampling. however, there have not been lengthy conversations connecting them investigating authorship attribution. in this presentation, simulations present a static setup modeling artificial robust network language model ( nnlm ) and apply it to a function like text impressions from different authors. we investigate functions manually nnlm performs on a task with moderate author set size and relatively limited training and test data, and how the type of uploaded text samples affect the accuracy. nnlm demonstrates nearly 2. 72 % participation in perplexity, a restriction of persistence of a complex computational model. the matching objectives. after 5 xml test sites, theory also favors product author classification accuracy by 3. 43 % on average, beginning with outdated \u03c9 - linear methods behind xml method. interesting open source product of our methodology that continually demonstrated at", "histories": [["v1", "Wed, 24 Feb 2016 04:32:34 GMT  (597kb,D)", "http://arxiv.org/abs/1602.07393v1", "International Conference on Pattern Recognition Application and Methods (ICPRAM) 2016"]], "COMMENTS": "International Conference on Pattern Recognition Application and Methods (ICPRAM) 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["zhenhao ge", "yufang sun"], "accepted": false, "id": "1602.07393"}, "pdf": {"name": "1602.07393.pdf", "metadata": {"source": "CRF", "title": "Domain Specific Author Attribution Based on Feedforward Neural Network Language Models", "authors": ["Zhenhao Ge", "Yufang Sun"], "emails": ["zhenhao.ge@gmail.com,", "sun361@purdue.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Authorship attribution refers to the task of identifying the text author from a given text sample, by finding the author\u2019s unique textual features. It is possible to do this because the author\u2019s profile or style embodies many characteristics, including personality, cultural and educational background, language origin, life experience and knowledge basis, etc. Every person has his/her own style, and sometimes the author\u2019s identity can be easily recognized. However, most often identifying the author is challenging, because author\u2019s style can vary significantly by topics, mood, environment and experience. Seeking consistency or consistent evolution out of variation is not always an easy task.\nThere has been much research in this area. Juola (Juola, 2006) and Stamatatos (Stamatatos, 2009) for example, have surveyed the state of the art and proposed a set of recommendations to move forward. As more text data become available from the Web and computational linguistic models using statistical methods mature, more opportunities and challenges arise in this area (Koppel et al., 2009). Many statistical models have been successfully applied in this area, such as Latent Dirichlet Allocation (LDA) for topic\nmodeling and dimension reduction (Seroussi et al., 2011), Naive Bayes for text classification (CoyotlMorales et al., 2006), Multiple Discriminant Analysis (MDA) and Support Vector Machines (SVM) for feature selection and classification (Ebrahimpour et al., 2013). Methods based on language modeling are also among the most popular methods for authorship attribution (Kes\u030celj et al., 2003).\nNeural networks with deep learning have been successfully applied in many applications, such as speech recognition (Hinton et al., 2012), object detection (Krizhevsky et al., 2012), natural language processing (Socher et al., 2011), and other pattern recognition and classification tasks (Bishop, 1995), (Ge and Sun, 2015). Neural Network based Language Models (NNLM) have surpassed the performance of traditional N-gram LMs (Bengio et al., 2003), (Mnih and Hinton, 2007) and are purported to generalize better in smaller datasets (Mnih, 2010). In this paper, we propose a similar NNLM setup for authorship attribution. The performance of the proposed method depends highly on the settings of the experiment, in particular the experimental design, author set size and data size (Luyckx, 2011). In this work, we focused on small datasets within one specific text domain, where the sizes of the training and test datasets for ar X iv :1\n60 2.\n07 39\n3v 1\n[ cs\n.C L\n] 2\n4 Fe\nb 20\n16\neach author are limited. This often leads to contextbiased models, where the accuracy of author detection is highly dependent on the degree to which the topics in training and test sets match each other (Luyckx and Daelemans, 2008). The experiments we conceive are based on a closed dataset, i.e. each test author also appears in the training set, so the task is simplified to author classification rather than detection.\nThe paper is organized as follows. Sec. 2 introduces the database used for this project. Sec. 3 explains the methodology of the NNLM, including cost function definition, forward-backward propagation, and weight and bias updates. Sec. 4 describes the implementation of the NNLM, provides the classification metrics, and compares results with conventional baseline N-gram models. Finally, Sec. 5 presents the conclusion and suggests future work."}, {"heading": "2 DATA PREPARATION", "text": "The database is a selection of course transcripts from Coursera, one of the largest Massive Open Online Course (MOOC) platforms. To ensure the author detection less replying on the domain information, 16 courses were selected from one specific text domain of the technical science and engineering fields, covering 8 areas: Algorithm, Data Mining, Information Technologies (IT), Machine Learning, Mathematics, Natural Language Processing (NLP), Programming and Digital Signal Processing (DSP). Table 1 lists more details for each course in the database, such as the number of sentences and words, the number of words per sentence, and vocabulary sizes in multiple stages. For privacy reason, the exact course titles and instructor (author) names are concealed. However, for the purpose of detecting the authors, it is necessary to point out that all courses are taught by different instructors, except for the courses with IDs 7 and 16. This was done intentionally to allow us to investigate how the topic variation affects performance.\nThe transcripts for each course were originally collected in short phrases with various lengths, shown one at a time at the bottom of the video lectures. They were first concatenated and then segmented into sentences, using straight-forward boundary determination by punctuations. The sentence-wise datasets are then stemmed using the Porter Stemming algorithm (Porter, 1980). To further control the vocabulary size, words occurring only once in the entire course or with frequency less than 1/100,000 are considered to have negligible influence on the outcome and are pruned by mapping them to an Out-Of-Vocabulary (OOV) mark \u3008unk\u3009. The first top bar graph in Fig-\nTable 1: Subtitle database from selected Coursera courses\nID Field No. of No. of Words / Vocab. size (original sentences words sentences / stemmed / pruned)\n1 Algorithm 5,672 121,675 21.45 3,972 / 2,702 / 1,809 2 Algorithm 14,902 294055 20.87 6,431 / 4,222 / 2,378"}, {"heading": "3 DSP 8,126 129,665 15.96 3,815 / 2,699 / 1,869", "text": "4 Data Mining 7,392 129,552 17.53 4,531 / 3,140 / 2,141 5 Data Mining 6,906 129,068 18.69 3,008 / 2,041 / 1,475"}, {"heading": "6 DSP 20,271 360,508 17.78 8,878 / 5,820 / 2,687", "text": ""}, {"heading": "7 IT 9,103 164,812 18.11 4,369 / 2,749 / 1,979", "text": "8 Mathematics 5,736 101,012 17.61 3,095 / 2,148 / 1,500 9 Machine Learning 11,090 224,504 20.24 6,293 / 4,071 / 2,259 10 Programming 8,185 160,390 19.60 4,045 / 2,771 / 1,898"}, {"heading": "11 NLP 7,095 111,154 15.67 3,691 / 2,572 / 1,789", "text": ""}, {"heading": "12 NLP 4,395 100,408 22.85 3,973 / 2,605 / 1,789", "text": ""}, {"heading": "13 NLP 4,382 96,948 22.12 4,730 / 3,467 / 2,071", "text": "14 Machine Learning 6,174 116,344 18.84 5,844 / 4,127 / 2,686 15 Mathematics 5,895 152,100 25.80 3,933 / 2,697 / 1,918 16 Programming 6,400 136,549 21.34 4,997 / 3,322 / 2,243\nure 1 shows how the vocabulary size of each course dataset shrinks after stemming and pruning. There are only 0.5 \u223c 1.5% words among all datasets mapped to \u3008unk\u3009, however, the vocabulary sizes are significantly reduced to an average of 2000. The bottom bar graph provides a profile of each instructor in terms of word frequency, i.e. the database coverage of the most frequent k words after stemming and pruning, where k = 500,1000,2000. For example, the most frequent 500 words cover at least 85% of the words in all datasets."}, {"heading": "3 NEURAL NETWORK LANGUAGE MODEL", "text": "The language model is trained using a feed-forward neural network illustrated in Figure 2. Given a sequence of N words W1,W2, . . . ,Wi, . . . ,WN from training text, the network trains weights to predict the word Wt , t \u2208 [1,N] in a designated target word position in sequence, using the information provided from the rest of words, as it is formulated in Eq. (1).\nW \u2217 = argmax t P(Wt |W1W2 \u00b7 \u00b7 \u00b7Wi \u00b7 \u00b7 \u00b7WN), i 6= t (1)\nIt is similar to the classic N-gram language model, where the primary task is to predict the next word given N\u2212 1 previous words. However, here the network can be trained to predict the target word in any position, given the neighboring words.\nThe network contains 4 different types of layers: the word layer, the embedding layer, the hidden layer, and the output (softmax) layer. The weights between adjacent layers, i.e. word-to-embedding weights Wword\u2212emb, embedding-to-hidden weights Wemb\u2212hid, and hidden-to-output weights Whid\u2212out, need to be trained in order to transform the input words to the predicted output word. The following 3 sub-sections briefly introduce the NNLM training procedure, first defining the cost function to be minimized, then describing the forward and backward weight and bias propagation. The implementation details regarding parameter settings and tuning are discussed in Sec. 4."}, {"heading": "3.1 Cost Function", "text": "Given vocabulary size V , it is a multinomial classification problem to predict a single word out of V op-\ntions. So the cost function to be minimized can be formulated as\nC =\u2212\u2211 V t j logy j. (2)\nC is the cross-entropy, and y j, where j \u2208 V and \u2211 j\u2208V y j = 1, is the output of node j in the final output layer of the network, i.e. the probability of selecting the jth word as the predicted word. The parameter t j is the target label and t j \u2208 {0,1}. As a 1-of-V multiclass classification problem, there is only one target value 1, and the rest are 0s."}, {"heading": "3.2 Forward Propagation", "text": "Forward propagation is a process to compute the outputs y j of each layer L j with a) its neural function (i.e. sigmoid, linear, rectified, binary, etc.), and b) the inputs z j, computed using the outputs of the previous layer yi, weights Wi j from layer Li to layer L j, and bias b j of the current layer L j. After weight and bias initialization, the neural network training starts from forward propagating the word inputs to the outputs in the final layer.\nFor the word layer, given word context size N and target word position t, each of the N\u2212 1 input words wi is represented by a binary index column vector xi with length equal to the vocabulary size V . It contains all 0s but only one 1 in a particular position to differentiate it from all other words. The word xi is transformed to its distributed representation in the socalled embedding layer via the equation\nzemb(i) =W Tword\u2212emb \u00b7 xi, (3)\nwhere Wword\u2212emb is the word-to-embedding weights with size [V \u00d7Nemb], which is used in the computation of zemb(i) for different words xi, and Nemb is the dimension of the embedding space. Because zemb(i) is one column in W Tword\u2212emb, representing the word xi, this process is simply a table look up.\nFor the embedding layer, the output yemb is just the concatenation of the representation of the input words zemb(i),\nyemb = [zTemb(1),z T emb(2), \u00b7 \u00b7 \u00b7 ,zTemb(i), \u00b7 \u00b7 \u00b7 ,zTemb(N)]T ,\n(4) where i\u2208V , i 6= t, and t is the index for the target word wt . So yemb is a column vector with length Nemb\u00d7 (N\u22121).\nFor the hidden layer, the input zhid is firstly computed with weights Wemb\u2212hid, embedding output yemb, and hidden bias bhid using\nzhid =W Temb\u2212hid \u00b7 yemb +bhid, (5)\nThen, the logistic function, which is a type of Sigmoid function, is used to compute the output yhid from zhid:\nyhid = 1\n1+ e\u2212zhid . (6)\nFor the output layer, the input zout is given by\nzout =W Thid\u2212out \u00b7 yhid +bout, (7)\nThis output layer is a Softmax layer which incorporates the constraint \u2211V yout = 1 using the Softmax function\nyout = ezout\n\u2211V ezout . (8)"}, {"heading": "3.3 Backward Propagation", "text": "After forward propagating the input words xi to the final output yout of the network, through Eq. (3) to Eq. (8), the next task is to backward propagate error derivatives from the output layer to the input, so that we know the directions and magnitudes to update weights between layers.\nIt starts from the derivative \u2202C\u2202zout(i) of node i in the output layer, i.e.\n\u2202C \u2202zout(i) = \u2211 j\u2208V \u2202C \u2202yout( j) \u2202yout( j) zout(i) = yout(i)\u2212 ti. (9)\nThe further derivation of Eq. (9) requires splitting \u2202yout( j) zout(i)\ninto cases of i = j and i 6= j, i.e. \u2202yout(i)\u2202zout(i) = yout(i)(1\u2212yout(i)) vs. \u2202yout(i)\u2202zout( j) =\u2212yout(i)yout( j) and is omitted here. For simplicity of presentation, the following equations omit the indices i, j.\nTo back-propagate derivatives from the output layer to the hidden layer, we follow the order \u2202C\u2202zout \u2192 \u2202C\u2202Whid\u2212out , \u2202C \u2202bout \u2192 \u2202C \u2202yhid\n\u2192 \u2202C\u2202Zhid . Since Zout = W Thid\u2212out \u00b7yhid, then \u2202zout \u2202whid\u2212out = yhid and \u2202zout\u2202yhid =whid\u2212out. In addition, since Eq. (7), then \u2202zout\u2202bout = 1. Thus,\n\u2202C \u2202whid\u2212out = \u2202zout \u2202whid\u2212out \u00b7 \u2202C \u2202zout = yhid \u2202C \u2202zout , (10)\n\u2202C \u2202bout = \u2202C \u2202zout \u00b7 \u2202zout \u2202bout = \u2202C \u2202zout , (11)\nand\n\u2202C \u2202yhid = \u2211 Nout \u2202zout \u2202yhid \u00b7 \u2202C \u2202zout = \u2211 Nout whid\u2212out \u2202C \u2202zout . (12)\nAlso, \u2202C\n\u2202zhid = \u2202C \u2202yhid \u00b7 dyhid dzhid , (13)\nwhere dyhiddzhid = yhid(1\u2212 yhid), derived using Eq. (6).\nTo back propagate derivatives from the hidden layer to the embedding layer, the derivations of\n\u2202C \u2202wemb\u2212hid , \u2202C\u2202bhid and \u2202C \u2202yemb are very similar to Eq. (10) through Eq. (12), so that\n\u2202C \u2202wemb\u2212hid = \u2202zhid \u2202wemb\u2212hid \u00b7 \u2202C \u2202zhid = yemb \u2202C \u2202zhid , (14)\n\u2202C \u2202bhid = \u2202C \u2202zhid \u00b7 \u2202zhid \u2202bhid = \u2202C \u2202zhid , (15)\nand \u2202C\n\u2202yemb = \u2211\nNhid\n\u2202zhid \u2202yemb \u00b7 \u2202C \u2202zhid = \u2211 Nhid wemb\u2212hid \u2202C \u2202zhid . (16)\nHowever, since the embedding layer is linear rather than sigmoid, then dyembdzemb = 1. Thus,\n\u2202C \u2202zemb = \u2202C \u2202yemb \u00b7 dyemb dzemb = \u2202C \u2202yemb . (17)\nIn the back propagation from the embedding layer to the word layer, since Wword\u2212emb is shared among all words, to obtain \u2202C\u2202Wword\u2212emb ,\n\u2202C \u2202zemb needs to be segmented into \u2202C\u2202zemb(i) , such as [( \u2202C\u2202zemb(1) )T \u00b7 \u00b7 \u00b7( \u2202C\u2202zemb(i) ) T \u00b7 \u00b7 \u00b7 \u2202C\u2202zemb(N) ) T ]T , where i \u2208 N, i 6= t is the index for each input word. From Eq. (3), \u2202zemb\u2202wword\u2212emb = xi, and then\n\u2202C \u2202wword\u2212emb = \u2211 i\u2208N,i 6=t xi \u00b7 \u2202C \u2202zemb(i) . (18)"}, {"heading": "3.4 Weight and Bias Update", "text": "After each iteration of forward-backward propagation, the weights and biases are updated to reduce cost. Denote W as a general form of the weight matrices Wword\u2212emb, Wemb\u2212hid and Whid\u2212out, and \u2206 as an averaged version of the weight gradient, which carries information from previous iterations and is initialized with zeros, the weights are updated with:{\n\u2206i+1 = \u03b1\u2206i + \u2202C\u2202Wi Wi+1 =Wi\u2212 \u03b5\u2206i+1\n(19)\nwhere \u03b1 is the momentum which determines the percentage of weight gradients carried from the previous iteration, and \u03b5 is the learning rate which determine the step size to update weights towards the direction of descent. The biases are updated similarly by just replacing W with b in Eq. (19)."}, {"heading": "3.5 Summary of NNLM", "text": "In the NNLM training, the whole training dataset is segmented into mini-batches with batch size M. The\nneural network in terms of weights and biases gets updated through each iteration of mini-batch training. The gradient \u2202C\u2202Wi in Eq. (19) should be normalized by M. One cycle of feeding all data is called an epoch, and given appropriate training parameters such as learning rate \u03b5 and momentum \u03b1, it normally requires 10 to 20 epochs to get a well-trained network.\nNext we present a procedure for training the NNLM. It includes all the key components described before, has the flexibility to change the training parameters through different epochs, and includes an early termination criterion. 1. Set up general parameters such as the mini-batch\nsize M, the number of epochs and model parameters such as the word context size N, the target word position t, the number of nodes in each layer, etc.;\n2. Split the training data into mini-batches; 3. Initialize networks, such as weights and biases; 4. For each epoch:\na. Set up parameters for current epoch, such as the learning rate \u03b5, the momentum \u03b1, etc.;\nb. For each iteration of mini-batch training: i. Compute weight and bias gradients\nthrough forward-backward propagation; ii. Update weights and biases with current \u03b5\nand \u03b1. c. Check the cost reduction of the validation set,\nand terminate the training early, if it goes up."}, {"heading": "4 IMPLEMENTATION AND RESULTS", "text": "This section covers the implementation details of the authorship attribution system as a N-way classification problem using NNLM. The results are compared with baseline N-gram language models trained using the SRILM toolkit (Stolcke et al., 2002)."}, {"heading": "4.1 NNLM Implementation and Optimization", "text": "The database for each of the 16 courses is randomly split into training, validation and test sets with ratio 8:1:1. To compensate for the model variation due to the limited data size, the segmentation is performed 10 times with different randomization seeds, so the mean and variation of performance can be measured.\nFor each course in this project, we trained a different 4-gram NNLM, i.e. context size N = 4, to predict the 4th word using the 3 preceding words. These\nmodels share the same general parameters, such as a) the number of epochs (15), b) the epoch in which the learning rate decay starts (10), c) the learning rate decay factor (0.9). However, the other model parameters are searched and optimized within certain ranges using a multi-resolutional optimization scheme, with a) the dimension of embedding space Nemb (25 \u223c 200), b) the nodes of the hidden layer Nhid (100 \u223c 800), c) the learning rate \u03b5 (0.05 \u223c 0.3), d) the momentum \u03b1 (0.8\u223c 0.99), and e) mini-batch size M (100\u223c 400). This optimization process is time consuming but worthwhile, since each course has a unique profile, in terms of vocabulary size, word distribution, database size, etc., so a model adapted to its profile can perform better in later classification."}, {"heading": "4.2 Classification with Perplexity Measurement", "text": "Statistical language models provide a tool to compute the probability of the target word Wt given N\u22121 context words W1,W2, . . . ,Wi, . . . ,WN , i\u2208N, i 6= t. Normally, the target word is the Nth word and the context words are the preceding N\u22121 words. Denote W n1 as a word sequence (W1,W2, . . . ,Wn). Using the chain rule of probability, the probability of sequence W n1 can be formulated as\nP(W n1 ) = P(W1)P(W2|W1) . . .P(Wn|W n\u221211 )\n= n\n\u220f k=1\nP(Wk|W k\u221211 ). (20)\nUsing a Markov chain, which approximates the probability of a word sequence with arbitrary length n to the probability of a sequence with the closest N words, the shortened probabilities can be provided by the LM with context size N, i.e. N-gram language model. Eq. (20) can then be simplified to\nP(W n1 )\u2248 P(W nn\u2212N+1) = n\n\u220f k=1 P(Wk|W k\u22121k\u2212N+1) (21)\nPerplexity is an intrinsic measurement to evaluate the fitness of the LM to the test word sequence W N1 , which is defined as\nPP(W n1 ) = P(W n1 )\u2212 1 n (22)\nIn practical use, it normally converts the probability multiplication to the summation of log probabilities. Therefore, using Eq. (21), Eq. (22) can be reformulated as\nPP(W n1 )\u2248\n( n\n\u220f k=1 P(Wk|W k\u22121k\u2212N+1)\n)\u2212 1n\n= 10 \u2212\n\u2211nk=1 log10 P(Wk|W k\u22121 k\u2212N+1)\nn\n(23)\nIn this project, the classification is performed by measuring the perplexity of the test word sequences in terms of sentences, using the trained NNLM of each course. Denote C as the candidate courses/instructors and C \u2217 as the selected one from the classifier. C \u2217 can then be expressed as\nC \u2217 = argmax C PP(W n1 |LMC ) (24)\nThe classification performance with NNLM is also compared with baselines from an SRI N-gram backoff model with Kneser-Ney Smoothing. The perplexities are computed without insertions of start-ofsentence and end-of-sentence tokens in both SRILM and NNLM. To evaluate the LM fitness with different training methods, Table 2 lists the training-to-test perplexities for each of the 16 courses, averaged from 10 different database segmentations. Each line in Table\n2 shows the mean perplexities with standard deviation for the SRI N-gram methods with N from 1 to 4, plus the NNLM 4-gram method. It illustrates that among the SRI N-gram methods, 4-gram is slightly better than the tri-gram, and for the 4-gram NNLM method, it achieves even lower perplexities on average."}, {"heading": "4.3 Classification Accuracy and Confusion Matrix", "text": "To test the classification accuracy for a particular course instructor, the sentence-wise perplexity is computed with the trained NNLMs from different classes. The sentences are randomly selected from the\ntest set. Figure 3(a) shows graphically the accuracy vs. number of sentences for a particular course with ID 3. The accuracies are obtained from 3 different methods, SRI uniqram, 4-gram and NNLM 4-gram. The number of randomly selected sentences is in the range of 1 to 20, and for each particular number of sentences, 100 trials were performed and the mean accuracies with standard deviations are shown in the figure. As mentioned earlier in Sec. 2, courses with ID 7 and 16 were taught by the same instructor, so these two courses are excluded and 14 courses/instructors are used to compute their 16-way classification accuracies. Figure 3(b) demonstrates the mean accuracy over these 14 courses. SRI 4-gram and NNLM 4-gram achieve similar accuracy and variation. However, the NNLM 4-gram is slightly more accurate than the SRI 4-gram.\nFigure 4 again compares the accuracies from these two models. It provides the accuracies of 3 difficulty stages, given 1, 5, or 10 test sentences. Both LMs perform differently along all course/instructor datasets. However, NNLM 4-gram is on average slightly better than SRI 4-gram, especially when the number of sentences is less.\nBesides classification accuracy, the confusion between different course/instructors is also investigated. Figure 5 shows the confusion matrices for all 16 courses/instructors, computed with only one randomly picked test sentence for both methods. The probabilities are all in log scale for better visualization. The confusion value for the ith row and jth column is the log probability of assigning the ith course/instructor as the jth one. Since course 7 and 16 were taught by the same instructor, it is not surprising that the values for (7,16) and (16,7) are larger than the others in the same row. In addition, instruc-\ntors who taught the courses in the same field, such as courses 1,2 (Algorithm) and courses 11,12,13 (NLP) are more likely to be confused with each other. So the topic of the text does play a big role in authorship attribution. Since the NNLM 4-gram assigns higher values than the SRI the 4-gram for (7,16) and (16,7), it is more biased towards the author rather than the content in that sense."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "This paper investigates authorship attribution using NNLM. The experimental setup for NNLM is detailed with mathematical elaboration. The results in terms of LM fitness in perplexity, classification accuracies, and confusion scores are promising, compared with the baseline N-gram methods. The performance is very competitive to the state-of-the-art, in terms of classification accuracy and testing sensitivity, i.e. the length of test text used in order to achieve confident results. From the previous work listed in Sec. 1, the best reported results to date achieved either 95% accuracy on a similar author pool size, or 50% \u223c 60% with 100+ authors and limited training date per author. As it is shown in Figure 4, our work achieves nearly perfect accuracies if more than 10 test sentences are given.\nHowever, since both the SRI baseline and NNLM methods achieves nearly perfect accuracies with only limited test data, the current database may not be sufficiently large and challenging, probably due to the consistency between the training and the test sets and the contribution from the topic distinction. In the future, the algorithm should be tested using datasets with larger author set sizes and greater styling simi-\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 1 \u20100.4 \u20102.6 \u20103.5 \u20104.7 \u20104.2 \u20104.5 \u20103.6 \u20103.9 \u20104.2 \u20103.6 \u20103.8 \u20103.7 \u20103.9 \u20104.3 \u20104.9 \u20103.6 2 \u20103.2 \u20100.3 \u20104.3 \u20104.8 \u20104.6 \u20104.9 \u20103.9 \u20104.1 \u20103.9 \u20103.9 \u20104.5 \u20104.3 \u20104.2 \u20104.5 \u20103.8 \u20103.7 3 \u20104.3 \u20104.5 \u20100.2 \u20105.6 \u20104.4 \u20103.8 \u20104.7 \u20105.1 \u20103.7 \u20104.5 \u20105.2 \u20105.0 \u20104.4 \u20105.3 \u20104.5 \u20103.8 4 \u20104.9 \u20105.5 \u20104.7 \u20100.1 \u20104.4 \u20105.4 \u20106.1 \u20105.5 \u20104.8 \u20104.4 \u20104.7 \u20105.8 \u20104.6 \u20105.6 \u20105.9 \u20103.9 5 \u20104.5 \u20104.8 \u20104.2 \u20104.5 \u20100.2 \u20105.0 \u20105.5 \u20105.1 \u20105.1 \u20104.3 \u20104.5 \u20104.7 \u20104.5 \u20105.0 \u20105.0 \u20103.8 6 \u20105.1 \u20105.1 \u20104.0 \u20105.5 \u20104.9 \u20100.1 \u20105.7 \u20104.4 \u20104.7 \u20104.2 \u20104.7 \u20105.2 \u20104.2 \u20105.3 \u20105.1 \u20104.3 7 \u20103.3 \u20103.0 \u20103.8 \u20104.5 \u20104.5 \u20104.3 \u20100.6 \u20104.4 \u20103.8 \u20103.4 \u20103.8 \u20104.1 \u20104.7 \u20104.3 \u20103.4 \u20102.1 8 \u20105.1 \u20104.7 \u20104.7 \u20105.5 \u20104.8 \u20104.7 \u20105.1 \u20100.1 \u20104.4 \u20104.2 \u20105.1 \u20104.8 \u20104.7 \u20104.9 \u20105.3 \u20104.2 9 \u20105.4 \u20105.3 \u20104.6 \u20105.5 \u20105.4 \u20105.7 \u20105.7 \u20104.5 \u20100.1 \u20104.4 \u20104.8 \u20105.2 \u20104.3 \u20105.4 \u20105.1 \u20103.6\n10 \u20104.1 \u20104.7 \u20104.0 \u20104.5 \u20105.4 \u20105.4 \u20104.4 \u20104.2 \u20104.3 \u20100.2 \u20104.9 \u20105.2 \u20104.6 \u20104.8 \u20104.3 \u20103.3 11 \u20104.4 \u20105.0 \u20104.5 \u20104.3 \u20104.0 \u20105.8 \u20104.9 \u20105.8 \u20104.2 \u20104.5 \u20100.2 \u20104.7 \u20104.2 \u20104.5 \u20105.8 \u20104.0 12 \u20104.0 \u20105.0 \u20103.9 \u20105.1 \u20104.0 \u20104.7 \u20105.4 \u20104.4 \u20104.1 \u20104.0 \u20104.0 \u20100.2 \u20104.0 \u20104.7 \u20104.5 \u20103.7 13 \u20105.4 \u20104.7 \u20104.4 \u20104.2 \u20104.8 \u20104.9 \u20105.5 \u20104.7 \u20104.8 \u20104.8 \u20104.7 \u20105.5 \u20100.1 \u20105.3 \u20105.1 \u20103.9 14 \u20104.5 \u20105.0 \u20104.7 \u20106.2 \u20104.7 \u20105.5 \u20105.6 \u20104.9 \u20104.6 \u20104.0 \u20104.2 \u20104.7 \u20104.7 \u20100.1 \u20105.6 \u20104.1 15 \u20104.5 \u20104.6 \u20103.9 \u20104.8 \u20104.8 \u20105.1 \u20104.7 \u20104.3 \u20104.7 \u20104.1 \u20104.7 \u20104.7 \u20104.5 \u20105.1 \u20100.2 \u20103.9 16 \u20103.4 \u20103.3 \u20103.4 \u20105.0 \u20104.6 \u20105.6 \u20102.5 \u20104.9 \u20103.6 \u20102.7 \u20103.8 \u20104.2 \u20103.8 \u20104.6 \u20103.9 \u20100.5\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 1 \u20100.4 \u20102.3 \u20104.3 \u20105.2 \u20103.7 \u20104.5 \u20103.8 \u20104.2 \u20104.0 \u20103.7 \u20104.5 \u20104.1 \u20104.0 \u20104.2 \u20104.4 \u20103.3 2 \u20103.1 \u20100.3 \u20103.8 \u20104.9 \u20104.3 \u20104.5 \u20104.0 \u20104.0 \u20104.0 \u20103.7 \u20104.0 \u20104.5 \u20104.0 \u20104.4 \u20103.7 \u20103.5 3 \u20104.2 \u20104.1 \u20100.4 \u20103.5 \u20103.9 \u20104.1 \u20104.1 \u20104.2 \u20104.1 \u20103.4 \u20104.3 \u20104.4 \u20104.0 \u20104.1 \u20104.8 \u20102.9 4 \u20104.0 \u20103.9 \u20103.9 \u20100.3 \u20103.5 \u20104.3 \u20104.4 \u20104.7 \u20103.8 \u20104.2 \u20104.5 \u20104.6 \u20103.9 \u20104.1 \u20104.2 \u20103.8 5 \u20103.9 \u20103.9 \u20104.0 \u20103.8 \u20100.3 \u20104.0 \u20103.8 \u20104.7 \u20104.0 \u20104.5 \u20103.4 \u20103.9 \u20103.6 \u20104.2 \u20104.0 \u20104.3 6 \u20104.4 \u20104.0 \u20103.8 \u20104.2 \u20104.2 \u20100.3 \u20104.5 \u20103.9 \u20103.8 \u20104.7 \u20104.4 \u20104.9 \u20104.1 \u20104.3 \u20103.9 \u20104.9 7 \u20104.2 \u20103.9 \u20103.5 \u20104.2 \u20104.6 \u20104.9 \u20100.3 \u20104.7 \u20104.4 \u20103.6 \u20103.9 \u20105.0 \u20104.3 \u20104.3 \u20104.8 \u20102.8 8 \u20103.7 \u20103.0 \u20104.4 \u20105.1 \u20104.6 \u20103.7 \u20103.9 \u20100.4 \u20103.3 \u20103.6 \u20103.9 \u20104.6 \u20104.1 \u20103.8 \u20103.7 \u20104.0 9 \u20104.5 \u20103.4 \u20104.4 \u20105.5 \u20104.8 \u20104.4 \u20104.1 \u20104.3 \u20100.3 \u20103.5 \u20104.4 \u20104.7 \u20103.9 \u20104.1 \u20104.5 \u20103.8\n10 \u20103.3 \u20103.1 \u20103.7 \u20104.7 \u20104.3 \u20104.7 \u20103.5 \u20103.8 \u20103.4 \u20100.5 \u20104.2 \u20104.1 \u20104.5 \u20104.0 \u20103.5 \u20102.6 11 \u20103.9 \u20103.2 \u20104.1 \u20104.8 \u20103.0 \u20104.7 \u20103.8 \u20104.9 \u20103.4 \u20104.5 \u20100.5 \u20103.0 \u20103.0 \u20103.4 \u20104.3 \u20103.8 12 \u20103.5 \u20103.2 \u20104.2 \u20104.4 \u20103.3 \u20104.3 \u20104.0 \u20104.5 \u20103.5 \u20104.1 \u20102.8 \u20100.5 \u20103.4 \u20103.6 \u20103.7 \u20103.7 13 \u20104.3 \u20103.5 \u20104.4 \u20105.0 \u20103.8 \u20104.2 \u20104.7 \u20105.0 \u20103.6 \u20104.9 \u20103.4 \u20104.1 \u20100.3 \u20104.3 \u20103.8 \u20104.2 14 \u20103.9 \u20103.0 \u20104.8 \u20104.7 \u20104.0 \u20104.0 \u20103.8 \u20104.6 \u20103.3 \u20104.0 \u20103.7 \u20103.8 \u20103.8 \u20100.4 \u20104.0 \u20104.4 15 \u20104.2 \u20103.4 \u20104.5 \u20104.6 \u20104.3 \u20104.4 \u20103.7 \u20104.5 \u20104.1 \u20104.1 \u20104.2 \u20104.2 \u20104.2 \u20104.5 \u20100.3 \u20104.2 16 \u20104.4 \u20104.5 \u20104.1 \u20104.2 \u20104.5 \u20106.0 \u20103.7 \u20104.9 \u20104.7 \u20103.7 \u20104.4 \u20105.2 \u20104.8 \u20105.2 \u20105.2 \u20100.2\n(a)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 1 \u20100.4 \u20102.6 \u20103.5 \u20104.7 \u20104.2 \u20104.5 \u20103.6 \u20103.9 \u20104.2 \u20103.6 \u20103.8 \u20103.7 \u20103.9 \u20104.3 \u20104.9 \u20103.6 2 \u20103.2 \u20100.3 \u20104.3 \u20104.8 \u20104.6 \u20104.9 \u20103.9 \u20104.1 \u20103.9 \u20103.9 \u20104.5 \u20104.3 \u20104.2 \u20104.5 \u20103.8 \u20103.7 3 \u20104.3 \u20104.5 \u20100.2 \u20105.6 \u20104.4 \u20103.8 \u20104.7 \u20105.1 \u20103.7 \u20104.5 \u20105.2 \u20105.0 \u20104.4 \u20105.3 \u20104.5 \u20103.8 4 \u20104.9 \u20105.5 \u20104.7 \u20100.1 \u20104.4 \u20105.4 \u20106.1 \u20105.5 \u20104.8 \u20104.4 \u20104.7 \u20105.8 \u20104.6 \u20105.6 \u20105.9 \u20103.9 5 \u20104.5 \u20104.8 \u20104.2 \u20104.5 \u20100.2 \u20105.0 \u20105.5 \u20105.1 \u20105.1 \u20104.3 \u20104.5 \u20104.7 \u20104.5 \u20105.0 \u20105.0 \u20103.8 6 \u20105.1 \u20105.1 \u20104.0 \u20105.5 \u20104.9 \u20100.1 \u20105.7 \u20104.4 \u20104.7 \u20104.2 \u20104.7 \u20105.2 \u20104.2 \u20105.3 \u20105.1 \u20104.3 7 \u20103.3 \u20103.0 \u20103.8 \u20104.5 \u20104.5 \u20104.3 \u20100.6 \u20104.4 \u20103.8 \u20103.4 \u20103.8 \u20104.1 \u20104.7 \u20104.3 \u20103.4 \u20102.1 8 \u20105.1 \u20104.7 \u20104.7 \u20105.5 \u20104.8 \u20104.7 \u20105.1 \u20100.1 \u20104.4 \u20104.2 \u20105.1 \u20104.8 \u20104.7 \u20104.9 \u20105.3 \u20104.2 9 \u20105.4 \u20105.3 \u20104.6 \u20105.5 \u20105.4 \u20105.7 \u20105.7 \u20104.5 \u20100.1 \u20104.4 \u20104.8 \u20105.2 \u20104.3 \u20105.4 \u20105.1 \u20103.6\n10 \u20104.1 \u20104.7 \u20104.0 \u20104.5 \u20105.4 \u20105.4 \u20104.4 \u20104.2 \u20104.3 \u20100.2 \u20104.9 \u20105.2 \u20104.6 \u20104.8 \u20104.3 \u20103.3 11 \u20104.4 \u20105.0 \u20104.5 \u20104.3 \u20104.0 \u20105.8 \u20104.9 \u20105.8 \u20104.2 \u20104.5 \u20100.2 \u20104.7 \u20104.2 \u20104.5 \u20105.8 \u20104.0 12 \u20104.0 \u20105.0 \u20103.9 \u20105.1 \u20104.0 \u20104.7 \u20105.4 \u20104.4 \u20104.1 \u20104.0 \u20104.0 \u20100.2 \u20104.0 \u20104.7 \u20104.5 \u20103.7 13 \u20105.4 \u20104.7 \u20104.4 \u20104.2 \u20104.8 \u20104.9 \u20105.5 \u20104.7 \u20104.8 \u20104.8 \u20104.7 \u20105.5 \u20100.1 \u20105.3 \u20105.1 \u20103.9 14 \u20104.5 \u20105.0 \u20104.7 \u20106.2 \u20104.7 \u20105.5 \u20105.6 \u20104.9 \u20104.6 \u20104.0 \u20104.2 \u20104.7 \u20104.7 \u20100.1 \u20105.6 \u20104.1 15 \u20104.5 \u20104.6 \u20103.9 \u20104.8 \u20104.8 \u20105.1 \u20104.7 \u20104.3 \u20104.7 \u20104.1 \u20104.7 \u20104.7 \u20104.5 \u20105.1 \u20100.2 \u20103.9 16 \u20103.4 \u20103.3 \u20103.4 \u20105.0 \u20104.6 \u20105.6 \u20102.5 \u20104.9 \u20103.6 \u20102.7 \u20103.8 \u20104.2 \u20103.8 \u20104.6 \u20103.9 \u20100.5\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n1 \u20100.4 \u20102.3 \u20104.3 \u20105.2 \u20103.7 \u20104.5 \u20103.8 \u20104.2 \u20104.0 \u20103.7 \u20104.5 \u20104.1 \u20104.0 \u20104.2 \u20104.4 \u20103.3\n2 \u20103.1 \u20100.3 \u20103.8 \u20104.9 \u20104.3 \u20104.5 \u20104.0 \u20104.0 \u20104.0 \u20103.7 \u20104.0 \u20104.5 \u20104.0 \u20104.4 \u20103.7 \u20103.5\n3 \u20104.2 \u20104.1 \u20100.4 \u20103.5 \u20103.9 \u20104.1 \u20104.1 \u20104.2 \u20104.1 \u20103.4 \u20104.3 \u20104.4 \u20104.0 \u20104.1 \u20104.8 \u20102.9\n4 \u20104.0 \u20103.9 \u20103.9 \u20100.3 \u20103.5 \u20104.3 \u20104.4 \u20104.7 \u20103.8 \u20104.2 \u20104.5 \u20104.6 \u20103.9 \u20104.1 \u20104.2 \u20103.8 5 \u20103.9 \u20103.9 \u20104.0 \u20103.8 \u20100.3 \u20104.0 \u20103.8 \u20104.7 \u20104.0 \u20104.5 \u20103.4 \u20103.9 \u20103.6 \u20104.2 \u20104.0 \u20104.3 6 \u20104.4 \u20104.0 \u20103.8 \u20104.2 \u20104.2 \u20100.3 \u20104.5 \u20103.9 \u20103.8 \u20104.7 \u20104.4 \u20104.9 \u20104.1 \u20104.3 \u20103.9 \u20104.9 7 \u20104.2 \u20103.9 \u20103.5 \u20104.2 \u20104.6 \u20104.9 \u20100.3 \u20104.7 \u20104.4 \u20103.6 \u20103.9 \u20105.0 \u20104.3 \u20104.3 \u20104.8 \u20102.8 8 \u20103.7 \u20103.0 \u20104.4 \u20105.1 \u20104.6 \u20103.7 \u20103.9 \u20100.4 \u20103.3 \u20103.6 \u20103.9 \u20104.6 \u20104.1 \u20103.8 \u20103.7 \u20104.0 9 \u20104.5 \u20103.4 \u20104.4 \u20105.5 \u20104.8 \u20104.4 \u20104.1 \u20104.3 \u20100.3 \u20103.5 \u20104.4 \u20104.7 \u20103.9 \u20104.1 \u20104.5 \u20103.8\n10 \u20103.3 \u20103.1 \u20103.7 \u20104.7 \u20104.3 \u20104.7 \u20103.5 \u20103.8 \u20103.4 \u20100.5 \u20104.2 \u20104.1 \u20104.5 \u20104.0 \u20103.5 \u20102.6 11 \u20103.9 \u20103.2 \u20104.1 \u20104.8 \u20103.0 \u20104.7 \u20103.8 \u20104.9 \u20103.4 \u20104.5 \u20100.5 \u20103.0 \u20103.0 \u20103.4 \u20104.3 \u20103.8 12 \u20103.5 \u20103.2 \u20104.2 \u20104.4 \u20103.3 \u20104.3 \u20104.0 \u20104.5 \u20103.5 \u20104.1 \u20102.8 \u20100.5 \u20103.4 \u20103.6 \u20103.7 \u20103.7 13 \u20104.3 \u20103.5 \u20104.4 \u20105.0 \u20103.8 \u20104.2 \u20104.7 \u20105.0 \u20103.6 \u20104.9 \u20103.4 \u20104.1 \u20100.3 \u20104.3 \u20103.8 \u20104.2 14 \u20103.9 \u20103.0 \u20104.8 \u20104.7 \u20104.0 \u20104.0 \u20103.8 \u20104.6 \u20103.3 \u20104.0 \u20103.7 \u20103.8 \u20103.8 \u20100.4 \u20104.0 \u20104.4 15 \u20104.2 \u20103.4 \u20104.5 \u20104.6 \u20104.3 \u20104.4 \u20103.7 \u20104.5 \u20104.1 \u20104.1 \u20104.2 \u20104.2 \u20104.2 \u20104.5 \u20100.3 \u20104.2 16 \u20104.4 \u20104.5 \u20104.1 \u20104.2 \u20104.5 \u20106.0 \u20103.7 \u20104.9 \u20104.7 \u20103.7 \u20104.4 \u20105.2 \u20104.8 \u20105.2 \u20105.2 \u20100.2\n(b)\nFigure 5: Course/instructor confusion matrices (16\u00d716) for SRI 4-gram (a) and NNLM 4-gram (b).\nlarities. Since purely topic-neutral text data may not even exist (Luyckx, 2011), developing general author LMs with mixed-topic data, and then adapting them to particular topics may also be desirable. It could be particularly helpful when the topics of text data is available. To compensate the relatively small size of the training set, LMs may also be trained with a group of authors and then adapt to the individuals.\nBecause the NNLM assigns a unique representation for a single word, it is difficult to model words with multiple meanings (Mnih, 2010). Thus, combining the NNLM and N-gram models might be beneficial. The recurrent NNLM, which captures more context size than the current feed-forward model (Mikolov et al., 2010), may also be worth exploring."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors would like to thank Coursera Incorporation for providing the course transcript datasets for the research in this paper."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Neural networks for pattern recognition", "author": ["C.M. Bishop"], "venue": "Oxford university press.", "citeRegEx": "Bishop,? 1995", "shortCiteRegEx": "Bishop", "year": 1995}, {"title": "Authorship attribution using word sequences", "author": ["R.M. Coyotl-Morales", "L. Villase\u00f1or-Pineda", "M. Montes-y G\u00f3mez", "P. Rosso"], "venue": "Progress in Pattern Recognition, Image Analysis and Applications, pages 844\u2013853. Springer.", "citeRegEx": "Coyotl.Morales et al\\.,? 2006", "shortCiteRegEx": "Coyotl.Morales et al\\.", "year": 2006}, {"title": "Automated authorship attribution using advanced signal classification techniques", "author": ["M. Ebrahimpour", "T.J. Putni\u0146\u0161", "M.J. Berryman", "A. Allison", "Ng", "B.W.-H.", "D. Abbott"], "venue": "PloS one, 8(2):e54998.", "citeRegEx": "Ebrahimpour et al\\.,? 2013", "shortCiteRegEx": "Ebrahimpour et al\\.", "year": 2013}, {"title": "Sleep stages classification using neural networks with multi-channel neural data", "author": ["Z. Ge", "Y. Sun"], "venue": "Brain Informatics and Health, pages 306\u2013316. Springer.", "citeRegEx": "Ge and Sun,? 2015", "shortCiteRegEx": "Ge and Sun", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "Mohamed", "A.r", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "Sainath", "T. N"], "venue": "Signal Processing", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Authorship attribution", "author": ["P. Juola"], "venue": "Foundations and Trends in information Retrieval, 1(3):233\u2013334.", "citeRegEx": "Juola,? 2006", "shortCiteRegEx": "Juola", "year": 2006}, {"title": "N-gram-based author profiles for authorship attribution", "author": ["V. Ke\u0161elj", "F. Peng", "N. Cercone", "C. Thomas"], "venue": "Proceedings of the conference pacific association for computational linguistics, PACLING, volume 3, pages 255\u2013264.", "citeRegEx": "Ke\u0161elj et al\\.,? 2003", "shortCiteRegEx": "Ke\u0161elj et al\\.", "year": 2003}, {"title": "Computational methods in authorship attribution", "author": ["M. Koppel", "J. Schler", "S. Argamon"], "venue": "Journal of the American Society for information Science and Technology, 60(1):9\u201326.", "citeRegEx": "Koppel et al\\.,? 2009", "shortCiteRegEx": "Koppel et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Scalability issues in authorship attribution", "author": ["K. Luyckx"], "venue": "ASP/VUBPRESS/UPA.", "citeRegEx": "Luyckx,? 2011", "shortCiteRegEx": "Luyckx", "year": 2011}, {"title": "Authorship attribution and verification with many authors and limited data", "author": ["K. Luyckx", "W. Daelemans"], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 513\u2013520. Association for Computational Linguistics.", "citeRegEx": "Luyckx and Daelemans,? 2008", "shortCiteRegEx": "Luyckx and Daelemans", "year": 2008}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "INTERSPEECH 2010, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Learning Distributed Representations for Statistical Language Modelling and Collaborative Filtering", "author": ["A. Mnih"], "venue": "PhD thesis, University of Toronto.", "citeRegEx": "Mnih,? 2010", "shortCiteRegEx": "Mnih", "year": 2010}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th international conference on Machine learning, pages 641\u2013648. ACM.", "citeRegEx": "Mnih and Hinton,? 2007", "shortCiteRegEx": "Mnih and Hinton", "year": 2007}, {"title": "An algorithm for suffix stripping", "author": ["M.F. Porter"], "venue": "Program, 14(3):130\u2013137.", "citeRegEx": "Porter,? 1980", "shortCiteRegEx": "Porter", "year": 1980}, {"title": "Authorship attribution with latent dirichlet allocation", "author": ["Y. Seroussi", "I. Zukerman", "F. Bohnert"], "venue": "Proceedings of the fifteenth conference on computational natural language learning, pages 181\u2013189. Association for Computational Linguistics.", "citeRegEx": "Seroussi et al\\.,? 2011", "shortCiteRegEx": "Seroussi et al\\.", "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C. Lin", "C. Manning", "A.Y. Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML11), pages 129\u2013136.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "A survey of modern authorship attribution methods", "author": ["E. Stamatatos"], "venue": "Journal of the American Society for information Science and Technology, 60(3):538\u2013556.", "citeRegEx": "Stamatatos,? 2009", "shortCiteRegEx": "Stamatatos", "year": 2009}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["A Stolcke"], "venue": "INTERSPEECH.", "citeRegEx": "Stolcke,? 2002", "shortCiteRegEx": "Stolcke", "year": 2002}], "referenceMentions": [{"referenceID": 6, "context": "Juola (Juola, 2006) and Stamatatos (Stamatatos, 2009) for example, have surveyed the state of the art and proposed a set of recommendations to move forward.", "startOffset": 6, "endOffset": 19}, {"referenceID": 18, "context": "Juola (Juola, 2006) and Stamatatos (Stamatatos, 2009) for example, have surveyed the state of the art and proposed a set of recommendations to move forward.", "startOffset": 35, "endOffset": 53}, {"referenceID": 8, "context": "As more text data become available from the Web and computational linguistic models using statistical methods mature, more opportunities and challenges arise in this area (Koppel et al., 2009).", "startOffset": 171, "endOffset": 192}, {"referenceID": 16, "context": "Many statistical models have been successfully applied in this area, such as Latent Dirichlet Allocation (LDA) for topic modeling and dimension reduction (Seroussi et al., 2011), Naive Bayes for text classification (CoyotlMorales et al.", "startOffset": 154, "endOffset": 177}, {"referenceID": 3, "context": ", 2006), Multiple Discriminant Analysis (MDA) and Support Vector Machines (SVM) for feature selection and classification (Ebrahimpour et al., 2013).", "startOffset": 121, "endOffset": 147}, {"referenceID": 7, "context": "Methods based on language modeling are also among the most popular methods for authorship attribution (Ke\u0161elj et al., 2003).", "startOffset": 102, "endOffset": 123}, {"referenceID": 5, "context": "Neural networks with deep learning have been successfully applied in many applications, such as speech recognition (Hinton et al., 2012), object detection (Krizhevsky et al.", "startOffset": 115, "endOffset": 136}, {"referenceID": 9, "context": ", 2012), object detection (Krizhevsky et al., 2012), natural language processing (Socher et al.", "startOffset": 26, "endOffset": 51}, {"referenceID": 17, "context": ", 2012), natural language processing (Socher et al., 2011), and other pattern recognition and classification tasks (Bishop, 1995), (Ge and Sun, 2015).", "startOffset": 37, "endOffset": 58}, {"referenceID": 1, "context": ", 2011), and other pattern recognition and classification tasks (Bishop, 1995), (Ge and Sun, 2015).", "startOffset": 64, "endOffset": 78}, {"referenceID": 4, "context": ", 2011), and other pattern recognition and classification tasks (Bishop, 1995), (Ge and Sun, 2015).", "startOffset": 80, "endOffset": 98}, {"referenceID": 0, "context": "Neural Network based Language Models (NNLM) have surpassed the performance of traditional N-gram LMs (Bengio et al., 2003), (Mnih and Hinton, 2007) and are purported to generalize better in smaller datasets (Mnih, 2010).", "startOffset": 101, "endOffset": 122}, {"referenceID": 14, "context": ", 2003), (Mnih and Hinton, 2007) and are purported to generalize better in smaller datasets (Mnih, 2010).", "startOffset": 9, "endOffset": 32}, {"referenceID": 13, "context": ", 2003), (Mnih and Hinton, 2007) and are purported to generalize better in smaller datasets (Mnih, 2010).", "startOffset": 92, "endOffset": 104}, {"referenceID": 10, "context": "The performance of the proposed method depends highly on the settings of the experiment, in particular the experimental design, author set size and data size (Luyckx, 2011).", "startOffset": 158, "endOffset": 172}, {"referenceID": 11, "context": "This often leads to contextbiased models, where the accuracy of author detection is highly dependent on the degree to which the topics in training and test sets match each other (Luyckx and Daelemans, 2008).", "startOffset": 178, "endOffset": 206}, {"referenceID": 15, "context": "The sentence-wise datasets are then stemmed using the Porter Stemming algorithm (Porter, 1980).", "startOffset": 80, "endOffset": 94}, {"referenceID": 10, "context": "Since purely topic-neutral text data may not even exist (Luyckx, 2011), developing general author LMs with mixed-topic data, and then adapting them to particular topics may also be desirable.", "startOffset": 56, "endOffset": 70}, {"referenceID": 13, "context": "Because the NNLM assigns a unique representation for a single word, it is difficult to model words with multiple meanings (Mnih, 2010).", "startOffset": 122, "endOffset": 134}, {"referenceID": 12, "context": "The recurrent NNLM, which captures more context size than the current feed-forward model (Mikolov et al., 2010), may also be worth exploring.", "startOffset": 89, "endOffset": 111}], "year": 2016, "abstractText": "Authorship attribution refers to the task of automatically determining the author based on a given sample of text. It is a problem with a long history and has a wide range of application. Building author profiles using language models is one of the most successful methods to automate this task. New language modeling methods based on neural networks alleviate the curse of dimensionality and usually outperform conventional N-gram methods. However, there have not been much research applying them to authorship attribution. In this paper, we present a novel setup of a Neural Network Language Model (NNLM) and apply it to a database of text samples from different authors. We investigate how the NNLM performs on a task with moderate author set size and relatively limited training and test data, and how the topics of the text samples affect the accuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement of fitness of a trained language model to the test data. Given 5 random test sentences, it also increases the author classification accuracy by 3.43% on average, compared with the N-gram methods using SRILM tools. An open source implementation of our methodology is freely available at https://github.com/zge/authorship-attribution/.", "creator": "LaTeX with hyperref package"}}}