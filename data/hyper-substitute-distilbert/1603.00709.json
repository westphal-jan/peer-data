{"id": "1603.00709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2016", "title": "Probabilistic Relational Model Benchmark Generation", "abstract": "the validation technique any statistical mining resource goes through frequent elimination proceeding where benchmarks before proving essential. completing this paper, producers drive down randomly type relational database segments that allow critically check probabilistic beliefs among customer attributes. we are particularly interested improving probabilistic relational models ( spp ), in extend sparse networks ( bns ) to a rigorous data mining context able conduct effective and relevant reasoning over relational data. even dealing with minimum of technologies not focused, separately, on the generation without generalized bayesian networks featuring relational databases, no work has be sought for prms components that track.. paper provides an adaptive approach why generating intelligent information algorithms scratch they highlight this gap. s proposed method allows to generate elements as versatile as incorporate dynamic data adding large randomly generated relational schema including a random packet of depth estimates. this can be his interest suited only for machine learning researchers to examine multiple proposals in a common framework, but mainly for similar institutions to evaluate the effectiveness of the components with a particular management system.", "histories": [["v1", "Wed, 2 Mar 2016 13:46:31 GMT  (367kb,D)", "http://arxiv.org/abs/1603.00709v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["mouna ben ishak", "rajani chulyadyo", "philippe leray"], "accepted": false, "id": "1603.00709"}, "pdf": {"name": "1603.00709.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Relational Model Benchmark Generation", "authors": ["Mouna Ben Ishak", "Rajani Chulyadyo"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Probabilistic Relational Model, Relational data representation, Benchmark generation"}, {"heading": "1 Introduction", "text": "Data mining is the central step in knowledge discovery in databases. It relies on several research areas including statistics and machine learning. Usually, machine learning techniques are developed around flat data representation (i.e., matrix form) and are known as propositional learning approaches. However, due to the development of communication and storage technologies, data management practices have taken further aspects. Data\nar X\niv :1\n60 3.\n00 70\n9v 1\n[ cs\n.L G\n] 2\nM ar\ncan present a very large number of dimensions, with several different types of entities. With the growing interest in extracting patterns from such data representation, relational data mining approaches have emerged with the interest of finding patterns in a given relational database [11] and Statistical Relational Learning (SRL) has emerged as an area of machine learning that enables effective and robust reasoning about relational data structures [18]. In this paper, we are particularly interested in Probabilistic Relational Models (PRMs)1 [22, 28], which represent a relational extension of Bayesian networks [27], where the probability model specification concerns classes of objects rather than simple attributes. PRMs present a probabilistic graphical formalism that enables flexible modeling of complex relational interactions.\nPRMs have proved their applicability in several areas (e.g., risk analysis, web page classification, recommender systems) [7, 12, 32] as they allow to minimize data preprocessing and the loss of significant information [30]. The use of PRMs implies their construction either by experts or by applying learning algorithms in order to learn the model from some existing observational relational data. PRMs learning involves finding a graphical structure as well as a set of conditional probability distributions that best fit to the relational training data. The evaluation of the learning approaches is usually done using randomly generated data coming from either real known networks or randomly generated ones. However, neither the first nor the second are available in the literature. Moreover, there is a growing interest from the database community to produce database benchmarks to support and illustrate decision support systems (DSSs). For real-world business tasks, uncertainty is an unmissable aspect. So, benchmarks designed to support DSSs should consider this task.\nIn this paper, we propose an algorithmic approach that allows to generate random PRMs from scratch, and then populate a database instance. The originality of this process is that it allows to generate synthetic relational data from a randomly generated relational schema and a random set of probabilistic dependencies. Since PRMs bring together two neighboring subfields of computer science, namely machine learning and database management, our process can be useful for both domains. It is imperative for statistical relational learning researchers to evaluate the effectiveness of their learning approaches. On the other hand, it can be of interest for database designers to evaluate the effectiveness of a database management system (DBMS) components. It allows to generate various relational schemas, from simple to complex ones, and to populate database tables with huge number of tuples derived from underlying probability distributions defined by the\n1Neville and Jensen [26] use the term \u2018Relational Bayesian Network\u2019 to refer to Bayesian networks that have been extended to model relational databases [22, 28] and use the term \u2018PRM\u2019 in its more general sense to distinguish the family of probabilistic graphical models that are interested in extracting statistical patterns from relational models. In this paper, we preserve the term PRM as used by [22, 28].\ngenerated PRMs. This paper presents an extended version of a preliminary work published in [1, 2]."}, {"heading": "2 Background", "text": "This section first provides a brief recall on Bayesian networks and relational model, and then introduces PRMs."}, {"heading": "2.1 Bayesian networks", "text": "Bayesian networks (BNs) [27] are directed acyclic graphs allowing to efficiently encode and manipulate probability distributions over high-dimensional spaces. Formally, they are defined as follows:\nDefinition 1 A Bayesian network B = \u3008G,\u0398\u3009 is defined by:\n1) A graphical component (structure): a directed acyclic graph (DAG) G = (V,E), where V is the set of vertices representing n discrete random variables A = {A1, . . . , An}, and E is the set of directed edges corresponding to conditional dependence relationships among these variables.\n2) A numerical component (parameters): \u0398 = {\u03981, . . . ,\u0398n} where each \u0398i = P (Ai|Pa (Ai)) denotes the conditional probability distribution (CPD) of each node Ai given its parents in G denoted by Pa (Ai).\nSeveral approaches have been proposed to learn BNs from data [9]. The evaluation of these learning algorithms requires either the use of known networks or the use of a random generation process. The former allows to sample data and perform learning using this data in order to recover the initial gold standard net. The latter allows to generate synthetic BNs and data in order to provide a large number of possible models and to carry out experimentation while varying models from simple to complex ones.\nRandom Bayesian networks generation comes to provide a graph structure and parameters. Statnikov et al. [33] proposed an algorithmic approach to generate arbitrarily large BNs by tiling smaller real-world known networks. The complexity of the final model is controlled by the number of tiling and a connectivity parameter which determines the maximum number of connections between one node and the next tile. Some works have been devoted to the generation of synthetic networks but without any guarantee that every allowed graph is produced with the same uniform probability [21]. In [20], the authors have proposed an approach, called PMMixed algorithm, that allows the generation of uniformly distributed Bayesian networks using Markov chains. Using this algorithm, constraints on generated nets can be added with relative ease such as constraints on nodes degree, maximum\nnumber of dependencies in the graph, etc. Once the DAG structure is generated, it is easy to construct a complete Bayesian network by randomly generating associated probability distributions by sampling either Uniform or Dirichlet distributions. Having the final BN, standard sampling method, such as forward sampling [19], can be used to generate observational data."}, {"heading": "2.2 Relational model", "text": "The manner how the data is organized in a database depends on the chosen database model. The relational model is the most commonly used one and it represents the basis for the most large scale knowledge representation systems [11]. Formally, the relational representation can be defined as follows:\nDefinition 2 The relational representation consists of\n\u2022 A set of relations (or tables or classes) X = {X1, . . . , Xn}. Each relation Xi has two parts:\n\u2013 The heading (relation schema): a fixed set of attributes A(X) = {A1, . . . , Ak}. Each attribute Ai is characterized by a name and a domain denoted Di.\n\u2013 The body: a set of tuples (or records). Each tuple associates for each attribute Ai in the heading a value from its domain Di.\n\u2022 Each relation has a key (i.e., a unique identifier, a subset of the heading of a relation Xi.) and, possibly, a set of foreign key attributes (or reference slots \u03c1). A foreign key attribute is a field that points to a key field in another relation, called the referenced relation. The associated constraint is a referential constraint. A chain of such constraints constitutes a referential path. If a referential path from some relation to itself is found then it is called a referential cycle2. Relation headings and constraints are described by a relational schema R.\nUsually the interaction with a relational database is ensured by specifying queries using structured query language (SQL), which on their part use some specific operators to extract significant meaning such as aggregators. An aggregation function \u03b3 takes a multi-set of values of some ground type, and returns a summary of it. Some requests need to cross long reference paths, with some possible back and forth. They use composed slots to define functions from some objects to other ones to which they are indirectly related. We call this composition of slots a slot chain K. We call a slot chain single-valued when all the crossed reference slots end with a cardinality\n2Database designs involving referential cycles are usually contraindicated [10].\nequal to 1. A slot chain is multi-valued if it contains at least one reference slot ending with cardinality equal to many. Multi-valued slot chains imply the use of aggregators.\nGenerally, database benchmarks are used to measure the performance of a database system. A database benchmark includes several subtasks (e.g., generating the transaction workload, defining transaction logic, generating the database) [16].\nRandom database generation consists on creating the database schema, determining data distribution, generating it and loading all these components to the database system under test. Several propositions have been developed in this context. The main issue was how to provide a large number of records using some known distributions in order to be able to evaluate the system results [4, 6]. In some research, known benchmarks 3 are used and the ultimate goal is only to generate a large dataset [17]. Nowadays, several software tools are available (e.g., DbSchema4, DataFiller5) to populate database instances knowing the relational schema structure. Records are then generated on the basis of this input by considering that the attributes are probabilistically independent which is not relevant when these benchmarks are used to evaluate decision support systems. The Transaction Processing Performance Council (TPC)6 organization provides the TPC-DS7 benchmark which has been designed to be suitable with real-world business tasks which are characterized by the analysis of huge amount of data. The TPC-DS schema models sales and the sales returns process for an organization. TPC-DS provides tools to generate either data sets or query sets for the benchmark. Nevertheless, uncertainty management stays a prominent challenge to provide better rational decision making."}, {"heading": "2.3 Probabilistic relational models", "text": "Probabilistic relational models [15, 22, 28] are an extension of BNs in the relational context. They bring together the strengths of probabilistic graphical models and the relational presentation. Formally, they are defined as follows [15]:\nDefinition 3 A Probabilistic Relational Model \u03a0 for a relational schema R is defined by:\n1) A qualitative dependency structure S : for each class (relation) X \u2208 X and each descriptive attribute A \u2208 A(X), there is a set of parents\n3http://www.tpc.org 4http://www.dbschema.com/ 5https://www.cri.ensmp.fr/people/coelho/datafiller.html 6http://www.tpc.org 7http://www.tpc.org/tpcds\nPa(X.A) = {U1, . . . , Ul} that describes probabilistic dependencies. Each Ui has the form X.B if it is a simple attribute in the same relation or \u03b3(X.K.B), where K is a slot chain and \u03b3 is an aggregation function.\n2) A quantitative component, a set of conditional probability distributions (CPDs), representing P (X.A|Pa(X.A)).\nThe PRM \u03a0 is a meta-model used to describe the overall behavior of a system. To perform probabilistic inference, this model has to be instantiated. A PRM instance contains, for each class of \u03a0, the set of objects involved in the system and the relations that hold between them (i.e., tuples from the database instance which are interlinked). This structure is known as a relational skeleton \u03c3r [15].\nDefinition 4 A relational skeleton \u03c3r of a relational schema is a partial specification of an instance of the schema. It specifies the set of objects \u03c3r(Xi) for each class and the relations that hold between the objects. However, it leaves the values of the attributes unspecified.\nGiven a relational skeleton, the PRM \u03a0 defines a distribution over the possible worlds consistent with \u03c3r through a ground Bayesian network [15].\nDefinition 5 A Ground Bayesian Network (GBN) is defined given a PRM \u03a0 together with a relational skeleton \u03c3r. A GBN consists of:\n1) A qualitative component:\n\u2022 A node for every attribute of every object x \u2208 \u03c3r(X), x.A. \u2022 Each x.A depends probabilistically on a set of parents Pa(x.A) = u1, . . . ul of the form x.B or x.K.B, where each ui is an instance of the Ui defined in the PRM. If K is not single-valued, then the parent is an aggregate computed from the set of random variables {y|y \u2208 x.K}, \u03b3(x.K.B).\n2) A quantitative component, the CPD for x.A is P (X.A|Pa(X.A)) .\nExample 1 An example of a relational schema is depicted in Figure 1, with three classes X = {Movie, V ote, User}. The relation V ote has a descriptive attribute V ote.Rating and two reference slots V ote.User and V ote.Movie. V ote.User relates the objects of class V ote with the objects of class User. Dotted links presents reference slots. An example of a slot chain would be V ote.User.User\u22121.Movie which could be interpreted as all the votes of movies cast by a particular user. V ote.Movie.genre \u2192 V ote.rating is an example of a probabilistic dependency derived from a slot chain of length 1 where V ote.Movie.genre is a parent of V ote.rating as shown in Figure 2. Also, varying the slot chain length\nmay give rise to other dependencies. For instance, using a slot chain of length 3, we can have a probabilistic dependency from \u03b3(V ote.User.User\u22121.Movie.genre) to V ote.rating. In this case, V ote.rating depends probabilistically on an aggregate value of all the genres of movies voted by a particular user.\nFigure 3 is an example of a relational skeleton of the relational schema of Figure 1. This relational skeleton contains 3 users, 5 movies and 9 votes.\nAlso it specifies the relations between these objects, e.g., the user U1 voted for two movies m1 and m2.\nFigure 4 presents the ground Bayesian network constructed from the relational skeleton of Figure 3 and the PRM of Figure 2. It resumes the same\ndependencies as well as CPDs of the PRM at the level of objects. Here, we have not reproduced the CPDs to not overload the figure.\nPRM structure learning has not been well studied in the literature. Only few works have been proposed to learn PRMs [13] or almost similar models [23, 24] from relational data.\nFriedman et al. [13] proposed Relational Greedy Hill-Climbing Search (RGS) algorithm. They applied a greedy search procedure to explore the space of PRM structures while allowing increasingly large slot chains. PRM structures are generated using the add edge, delete edge and reverse edge operators and aggregation functions if needed (cf. Section 2.3). As for score function, they used a relational extension of the Bayesian Dirichlet (BD) [8] score expressed as follows:\nRBDscore = \u2211 i \u2211 A\u2208A(Xi) \u2211 u\u2208V (Pa(Xi.A)) log[DM({CXi.A[v, u]}, {\u03b1Xi.A[v, u]})]\n\u2212 \u2211 i \u2211 A\u2208A(Xi) \u2211 u\u2208V (Pa(Xi.A)) lengthK(Xi.A, Pa(Xi.A)) (1)\nWhere\nDM({CXi.A[v, u]}, {\u03b1Xi.A[v, u]}) = \u0393(\n\u2211 v \u03b1[v])\n\u0393( \u2211\nv(\u03b1[v] + C[v])) \u220f v \u0393(\u03b1[v] + C[v]) \u0393(\u03b1[v]) ,\nand\n\u0393(x) = \u222b \u221e 0 tx\u22121e\u2212tdt\nis the Gamma function.\nAs for standard BNs, evaluating the effectiveness of the proposed approaches is needed. However, neither relational benchmarks nor general random generation process are available.\nRandom probabilistic relational models generation has to be established in order to evaluate proposed learning approaches in a common framework. [24] used a predefined schema and have only generated a number of dependencies varying from 5 to 15 and the conditional probability tables for attributes from a Dirichlet distribution. In [23], the authors have generated relational synthetic data to perform experimentation. Their generation process is based only on a particular family of relational schemas, with N classes (nodes) and N \u2212 1 referential constraints (edges). Referential constraints are then expressed using relationship classes. This gives rise to a final relational schema containing 2N \u2212 1 relations whereas in real\nworld cases, relational schemas may have more than N \u2212 1 referential constraints. If the schema is fully connected (as described in [25]), it will have a tree structure. Torti et al. [34] proposed a slightly different representation of PRMs, developed in the basis of the object-oriented framework and expert knowledge. Their main issue is probabilistic inference rather than learning. In their experimental studies [35], they have randomly generated PRMs using the layer pattern. The use of this architecture pattern imposes a particular order when searching for connections between classes, generating reference slots of the relational schema and also when creating the relational skeleton. No indication has been made about the generation of probabilistic dependencies between attributes. In addition, they are interested neither in populating a relational database nor in communicating with a database management system."}, {"heading": "3 PRM Benchmark Generation", "text": "Due to the lack of famous PRMs in the literature, this paper proposes a synthetic approach to randomly generate probabilistic relational models from scratch and to randomly instantiate them and populate relational databases. To the best of our knowledge, this has not yet been addressed."}, {"heading": "3.1 Principle", "text": "As we are working with a relational variety of Bayesian networks, our generation process is inspired from classical methods of generation of random BNs while respecting the relational domain representation.\nThe overall process is outlined in Algorithm 1. Roughly, the proposed generation process is divided into three main steps:\n\u2022 The first step generates a random PRM. For this, a relational schema is generated using Generate Relational Schema function(Section 3.2). Then, a graph dependency structure is generated using Generate Dependency Structure and Determine Slot Chains functions (Section 3.3). And finally, conditional probability tables are generated by the Generate CPD function in the same way than Bayesian networks (cf. Section 2.1).\n\u2022 The second step instantiates the model generated in the first step. First, a relational skeleton is generated usingGenerate Relational Skeleton function (Section 3.4). Then, using Create GBN function, a ground Bayesian Network is generated from both the generated PRM and the generated relational skeleton.\n\u2022 The third step presents the Sampling function. It involves database instance population and can be performed using a standard sampling\nAlgorithm 1: Generate Random PRM-DB\nInput: N : the number of relations, Kmax : The maximum slot chain length allowed Output: \u03a0 :< R,S, CPD >, DB Instance begin\nStep 1: Generate the PRM\n\u03a0.R \u2190 Generate Relational Schema(N) \u03a0.S \u2190 Generate Dependency Structure(\u03a0.R) \u03a0.S \u2190 Determine Slot Chains(\u03a0.R,\u03a0.S,Kmax) \u03a0.CPD \u2190 Generate CPD(\u03a0.S) Step 2: Instantiate the PRM\n\u03c3r \u2190 Generate Relational Skeleton(\u03a0.R) GBN \u2190 Create GBN(\u03a0, \u03c3r) Step 3: Database population\nDB Instance\u2190 Sampling(GBN)\nmethod over the GBN (Section 3.5)."}, {"heading": "3.2 Generation of a random relational schema", "text": "The relational schema generation process is depicted in Algorithm 2. Our aim is to generate a relational schema with a given number of classes such that it does not contain any referential cycles and also respects the relational model definition presented in section 2.2. We apply concepts from the graph theory for random schema generation. We associate this issue to a DAG structure generation process, where nodes represent relations and edges represent referential constraints definition. Xi \u2192 Xj means that Xi is the referencing relation and Xj is the referenced one. Besides, we aim to construct schemas where \u2200Xi, Xi \u2208 X there exists a referential path from Xi to Xj . This assumption allows to browse all classes in order to discover probabilistic dependencies later and it is traduced by searching DAG structures containing a single connected component (i.e., connected DAG).\nHaving a fixed number of relations N , the Generate DAG function constructs a DAG structure G with N nodes, where each node ni \u2208 G corresponds to a relation Xi \u2208 R following various possible implementation policies (cf. Section 5.2). For each class, we generate a primary key attribute using Generate Primary Key function. Then, we randomly generate the number of attributes and their associated domains usingGenerate Attributes\nAlgorithm 2: Generate Relational Schema\nInput: N : the required number of classes Output: R : The generated relational schema begin\nrepeat G \u2190 Generate DAG(Policy) until G is a connected DAG ; for each relation Xi \u2208 R do Pk Xi \u2190 Generate Primary Key(Xi) A(Xi)\u2190 Generate Attributes(Policy) V(Xi.A)\u2190 Generate States(Policy)\nfor each ni \u2192 nj \u2208 G do Fk Xi \u2190 Generate Foreign Key(Xi, Xj , Pk Xj)\nand Generate States functions respectively. Note that the generated domains do not take into account possible probabilistic dependencies between attributes. For each ni \u2192 nj \u2208 G, we generate a foreign key attribute in Xi using the Generate Foreign Key function. Foreign key generation is limited to one attribute as foreign keys reference simple primary keys (i.e., primary keys generated from only one attribute)."}, {"heading": "3.3 Generation of a random PRM", "text": "Relational schemas are not sufficient to generate databases when the attributes are not independent. We need to randomly generate probabilistic dependencies between the attributes of the classes in the schema. These dependencies have to provide the DAG of the dependency structure S and a set of CPDs which define a PRM (cf. Definition 3).\nWe especially focus on the random generation of the dependency structure. Once this latter is identified, conditional probability distributions may be sampled in a similar way as standard BNs parameter generation.\nThe dependency structure S should be a DAG to guarantee that each generated ground network is also a DAG [14]. S has the specificity that one descriptive attribute may be connected to another with different possible slot chains. Theoretically, the number of slot chains may be infinite. In practice a user-defined maximum slot chain length Kmax, is specified to identify the horizon of all possible slot chains. In addition, the Kmax value should be at least equal to N \u2212 1 in order to not neglect potential dependencies between attributes of classes connected via a long path. Each edge in the DAG has to\nAlgorithm 3: Generate Dependency Structure\nInput: R : The relational schema Output: S : The generated relational dependency structure begin\nfor each class Xi \u2208 R do Gi \u2190 Generate Sub DAG(Policy)\nS \u2190 \u22c3 Gi\nS \u2190 Generate Super DAG(Policy)\nbe annotated to express from which slot chain this dependency is detected. We add dependencies following two steps. First we add oriented edges to the dependency structure while keeping a DAG structure. Then we identify the variable from which the dependency has been drawn by a random choice of a legal slot chain related to this dependency."}, {"heading": "3.3.1 Constructing the DAG structure", "text": "The DAG structure identification is presented in Algorithm 3. The idea here is to find, for each node X.A, a set of parents from the same class or from further classes while promoting intra-class dependencies in order to control the final model complexity as discussed in [14]. This condition promotes the discovery of intra-class dependencies or those coming from short slot chains. The longer the slot chain, the lower is the chance of finding a probabilistic dependency through the slot chain. To follow this condition, havingN classes, we propose to constructN separated sub-DAGs, each of which is built over attributes of its corresponding class using the Generate Sub DAG function. Then, we construct a super-DAG over all the previously constructed sub-DAGs. At this stage, the super-DAG contains N disconnected components: The idea is to add inter-class dependencies in such a manner that we connect these disconnected components while keeping a global DAG structure.\nTo add inter-class dependencies, we constrain the choice of adding dependencies among only variables that do not belong to the same class. For an attribute X.A, the Generate Super DAG function chooses randomly an attribute Y.B, where X 6= Y , then verifies whether the super-DAG structure augmented by a new dependency from X.A to Y.B remains a DAG. If so, it keeps the dependency otherwise it rejects it and searches for a new one. The policies that are used are discussed in Section 5.2.\nAlgorithm 4: Determine Slot Chains\nInput: R : The relational schema, S : The dependency structure, Kmax : The maximum slot chain length Output: S : The generated relational dependency structure with generated slot chains\nbegin Kmax \u2190 max(Kmax, card(XR)\u2212 1) for each X.A\u2192 Y.B \u2208 S do\nPot Slot Chains List\u2190 Generate Potential Slot chains(X,Y,R,Kmax) for each slot Chain \u2208 Pot Slot Chains List do\nl\u2190 length(slot Chain) W [i]\u2190 exp \u2212l nb Occ(l,Pot Slot Chains List)\nSlot Chain\u2217 \u2190 Draw(Pot Slot Chains List,W ) if Needs Aggregator(Slot Chain\u2217) then\n\u03b3 \u2190 Random Choice Agg(list Aggregators)\nif Slot Chain\u2217 = 0 then S.Pa(X.A)\u2190 S.Pa(X.A) \u222a Y.B % here X = Y else S.Pa(X.A)\u2190 S.Pa(X.A) \u222a \u03b3(Y.Slot Chain\u2217.B)"}, {"heading": "3.3.2 Determining slot chains", "text": "During this step, we have to take into consideration that one variable may be reached through different slot chains and the dependency between two descriptive attributes will depend on the chosen one. Following [14], the generation process has to give more priority to shorter slot chains for selection. Consequently, we have used the penalization term discussed in [14]. Longer indirect slot chains are penalized by having the probability of occurrence of a probabilistic dependency from a slot chain length l inversely proportional to expl.\nHaving a dependency X.A \u2192 Y.B between two descriptive attributes X.A and Y.B, we start by generating the list of all possible slot chains (Pot Slot Chains List) of length \u2264 Kmax from which X can reach Y in the relational schema using the Generate Potential Slot chains function. Then, we create a vector W of the probability of occurrence for each of the slot chains found, with log(W [i]) \u221d \u2212lnb Occ(l,Pot Slot Chains List) , where l is the slot chain length and nb Occ is the number of slot chains of length l \u2208 Pot Slot Chains List. This value will rapidly decrease when the value\nof l increases, which allows to reduce the probability of selecting long slot chains. We then sample a slot chain from Pot Slot Chains List following W using the Draw function. If the chosen slot chain implies an aggregator, then we choose it randomly from the list of existing ones using the Random Choice Agg function. The slot chain determination is depicted in Algorithm 4.\nSimplifying slot chains. While finding slot chains, duplicate slot chains might be encountered. By \u2019duplicate\u2019, we mean the slot chains which produce the same result. For example, in the schema of figure 1, V ote.User and V ote.User.User\u22121.User are equivalent because traversing through the slot chains, we obtain the same set of User objects. Similarly, V ote.Movie\u22121.Movie is the same as an empty slot chain because this slot chain results in the target Movie object. When such duplicates are found, we pick the shorter one to avoid redundant, unnecessary computations. This is an improvement to our previous work [1, 2], where simplification of slot chains had not been considered. We apply the following rule to simplify slot chains.\nA slot chain is represented as a sequence of reference slots and inverse slots as \u03c11.\u03c12. . . . .\u03c1n\u22121.\u03c1n. If \u03c1n\u22121 is an inverse slot and \u03c1 \u22121 n = \u03c1n\u22121, then the slot chain can be simplified by eliminating the last two slots. So, the simplified slot chain would, then, be \u03c11.\u03c12. . . . .\u03c1n\u22123.\u03c1n\u22122. This can be done repetitively until no simplification is possible."}, {"heading": "3.4 GBN generation", "text": "The generated schema together with the added probabilistic dependencies and generated parameters results in a probabilistic relational model. To instantiate this latter, we need to generate a relational skeleton. The GBN is, then, fully determined with this relational skeleton and the CPDs already present at the meta-level.\nA relational skeleton can be imagined as a DAG where nodes are objects of different classes present in the associated relational schema and edges are directed from one object to another conforming to the reference slots present in the relational schema. This graph is, in fact, a special case of k-partite graph8 of definition 6.\nDefinition 6 Relational skeleton as a k-partite graph A relational skeleton is a special case of k-partite graph, Gk = (Vk, Ek), with the following properties:\n1. The graph is acyclic,\n8A k-partite graph is a graph whose vertices can be partitioned into k disjoint sets so that there is no edge between any two vertices within the same set.\n2. All edges are directed (an edge u\u2192 v indicates that the object u refers to v, i.e., u has a foreign key which refers to the primary key of v),\n3. Edges between two different types of objects are always oriented in the same direction, i.e. for all edges (u \u2014 v) between objects of U and V where u \u2208 U , and v \u2208 V , the direction of all edges must be either u\u2192 v or u\u2190 v and not both\n4. For all edges u \u2192 v between the objects of U and V , out degree of u = 1 but indegree of v can be greater than 1.\nIn this regard, relational skeleton generation process can be considered as a problem of generating objects and assigning links (or foreign keys) between them such that the resulting graph is a k-partite graph of definition 6. In our previous work [1, 2], we presented an algorithm to generate relational skeleton, where it generates nearly same number of objects of each class and iteratively adds random edges between objects of a pair of classes such that the direction of the edges conform to the underlying schema. This approach, in fact, does not create realistic skeleton because in real world, relational skeleton tends to be scale-free, i.e., degree of the vertices of the graph follows power law. Hence, in real datasets, the number of objects for classes with foreign keys tend to be very high compared to that for classes which do not have foreign keys and are referenced by other classes. Thus, we took a different approach to generate relational skeleton. Our new and improved approach to generating such k-partite graph is presented as Algorithm 5. We adapt [5]\u2019s directed scale-free graph generation algorithm for our special k-partite graph and use Chinese Restaurant Process[29] to apply preferential attachment.\nThe basic idea here is to iteratively generate an object of a class with no parents in the relational schema DAG and then recursively add an edge from this object to objects of its children classes. This process is essentially a depth first search (DFS), where we begin by generating an object of the root node of the graph and then at each encounter of a node in DFS, we add an edge from the object of the parent node to either a new or an existing object of the encountered node. The object of the parent node gets connected to a new object with probability p = \u03b1/(np \u2212 1 + \u03b1), where np is the total of objects of the parent node generated so far, and \u03b1 is a scalar parameter for the process. When it gets attached to an existing object, an object of the correct type is picked from the set of existing objects with probability nk/(np \u2212 1 + \u03b1), where nk is the indegree of the object to be selected and n is the total number of objects generated so far. Thus, as the skeleton graph grows, probability of getting connected to new objects will decrease and the objects with higher indegree will be preferred for adding new edges. At each iteration, a DFS is performed starting from one of the nodes without parents in the relational schema DAG. Thus, if there is only\nAlgorithm 5: Generate Relational Skeleton\nInput: Relational Schema as a DAG, G = (Vg, Eg); Total number of objects in the resulting skeleton, Ntotal; Scalar parameter for CRP, \u03b1 Output: A relational skeleton, I = (V,E) begin\nfor node \u2208 Vg do N(node)\u2190 0 %Total number of objects of each type generated so far\nV \u2190 {} %Set of objects E \u2190 {} %Set of directed edges between objects m\u2190 Number of nodes without any parents in G %number of roots if m > 1 then\nDivide G into m subgraphs such that each subgraph contains a root and all of its descendants.\nrepeat if m > 1 then\ng \u2190 one of the m subgraphs picked randomly else\ng \u2190 G %i.e., if G has only one root objroot \u2190 A new object of the root of g nroot \u2190 nroot + 1 children\u2190 Children of the root in g ((V \u2032, E\u2032), N \u2032)\u2190 Generate SubSkeleton(objroot, g, children,N, \u03b1) %Perform depth-first search over g and add edges recursively\nV \u2190 V \u222a V \u2032\nE \u2190 E \u222a E\u2032\nN \u2190 N \u2032 %Update the set of number of generated objects of each type\nn\u2190 cardinality(V ) %Total number of objects generated so far.\nuntil n >= Ntotal;\nI \u2190 (V,E)\none node that does not have any parent, then each iteration will visit all classes in the relational schema resulting in a complete set of objects and relations for all classes, otherwise only a subset of classes will be visited\nAlgorithm 6: Generate SubSkeleton\nInput: Parent object objp; Graph g; Parent node, parent; Children nodes, children; Set of the number of objects of each class generated so far, N ; Scalar parameter \u03b1 Output: Relational skeleton, I = (V,E); Set of the number of objects of each class generated so far, N\nbegin V \u2190 {objp} E \u2190 {} np \u2190 N(parent) %Total number of parents generated so far for C \u2208 children do\nnc \u2190 N(C) %Total number of the child C generated so far p\u2190 \u03b1/(np \u2212 1 + \u03b1) r \u2190 A random value between 0 and 1 %r \u2208 [0, 1] if r <= p then\nobjc \u2190 Create a new object of type C N(C)\u2190 nc + 1 epc \u2190 (objp, objc) %Add an edge from objp to objc E \u2190 E \u222a {epc} childrenc \u2190 Children of C in the graph g ((V \u2032, E\u2032), N \u2032)\u2190 Generate SubSkeleton(objc, g, C, childrenc, N, \u03b1) V \u2190 V \u222a V \u2032\nE \u2190 E \u222a E\u2032\nN \u2190 N \u2032 else\nobjc \u2190 An existing object of type C picked randomly with probability nk/(np \u2212 1) where nk = indegree of objc epc \u2190 (objp, objc) %Add an edge from objp to objc E \u2190 E \u222a {epc}\nI \u2190 (V,E)\nin each iteration. So, at the beginning of each iteration, one of the nodes without parents is picked randomly in the latter case. The iteration process is continued until the skeleton contains the required number of objects."}, {"heading": "3.5 Database population", "text": "This process is equivalent to generating data from a Bayesian network. We can generate as many relational database instances as needed by sampling from the constructed GBN. The specificity of the generated tuples is that they are sampled not only from functional dependencies but also from probabilistic dependencies provided by the randomly generated PRM."}, {"heading": "4 Toy example", "text": "In this section, we illustrate our proposal through a toy example.\nRelational schema generation. Figure 5 presents the result of running Algorithm 2, with N = 4 classes. For each class, a primary key has been added (clazz0id, clazz1id, clazz2id and clazz3id). Then a number of attributes has been generated randomly together with a set of possible states for each attribute using the policies described in Section 5.2 (e.g., clazz0 has 3 descriptive attributes att0, att1 and att2. att0 is a binary variable). Finally, foreign key attributes have been specified following the DAG structure of the graph G (e.g., clazz2 references class clazz1 using foreign key attribute clazz1fkatt12).\nPRM generation. We recall that this process is performed in two steps: randomly generate the dependency structure S (Algorithm 3), then randomly generate the conditional probability distributions which is similar to parameter generation of a standard BN. The random generation of the S is performed in two phases. We start by constructing the DAG structure, the result of this phase is in Figure 6. Then, we fix a maximum slot chain length Kmax to randomly determine from which slot chain the dependency has been detected. We use Kmax = 3, the result of this phase gives rise to the graph dependency structure of Figure 7. S contains 5 intra-class and 5 inter-class probabilistic dependencies. Three of the inter-class dependencies have been generated from slot chains of length 1: Clazz0.clazz1fkatt10.att1\u2192 Clazz0.att2; MODE(Clazz2.clazz2fkatt23\u22121.att0)\u2192 Clazz2.att3 and; Clazz2.clazz1fkatt12.att1\u2192 Clazz2.att3 One from slot chain of length 2: MODE(Clazz2.clazz1fkatt12.clazz1fkatt12\u22121.att0)\u2192 Clazz2.att3 One from slot chain of length 3: MODE(Clazz2.clazz2fkatt23\u22121.claszz1fkatt13.clazz1fkatt10\u22121.att2) \u2192 Clazz2.att3\nGBN creation. Once the PRM is generated, we follow the two steps presented in Section 3.4 to create a GBN and to populate the DB instance. We create a relational skeleton for the relational schema by performing depth first search on the schema DAG (cf. Algorithm 5). The first three iterations of the DFS are shown in figures 8 and 9. As the schema has only one node without any parent (i.e., a class without any foreign key), one complete DFS returns a set of objects of each class as shown in figure 8. At each iteration, we obtain different number of objects. As we can see in figure 9, the first iteration created five objects whereas the second and third iteration resulted in four and two objects respectively. We continue the iteration until we obtain the required number of objects in the skeleton. We then instantiate the probabilistic model generated in the previous step with the generation skeleton to obtain a ground Bayesian network. Sampling this GBN enables us to populate values for all attributes of all objects in the relational skeleton. For this example, we generated a random dataset with 2500 objects. The corresponding schema diagram is shown in figure 10, which also shows the number of objects of each class. The diagram is generated using SchemaSpy9.\n9http://schemaspy.sourceforge.net/\n(a )\n(b )\nF ig\nu re\n9: N\nex t\ntw o\nit er\na ti\non s\no f\nD F\nS on\nth e\nre la\nti on\nal sc\nh em\na of\nfi gu\nre 5\nfo ll\now in\ng th\ne fi\nrs t\nit er\na ti\non of\nfi gu\nre 8\nto g en\ner a te\nre la\nti o n\na l\nsk el\net on\ngr a p\nh .\nA t\nea ch\nit er\nat io\nn ,\na n\new ob\nje ct\nof \u2018C\nla ss\n3\u2019 w\nil l\nal w\nay s\nb e\ng en\ner a te\nd a s\nit d\no es\nn o t\nh av\ne an\ny p\na re\nn t.\nT h\ne o b\nje ct\nw il\nl th\nen b\ne li\nn ke\nd to\na n\nex is\nti n\ng ob\nje ct\nor a\nn ew\non e,\nan d\nth e\nsa m\ne th\nin g\ngo es\non fo\nr th\ne n\new ob\nje ct\ns. H\ner e,\nth e\nsk el\net o n\na ft\ner th\ne fi\nrs t\nit er\na ti\non h\nas fi\nve ob\nje ct\ns. T\nh e\nse co\nn d\nit er\nat io\nn cr\nea te\ns fo\nu r\nn ew\nob je\nct s,\nw h\ner ea\ns th\ne th\nir d\nit er\na ti\no n\ncr ea\nte s\no n\nly tw\no ob\nje ct\ns."}, {"heading": "5 Implementation", "text": "This section explains the implementation strategy of our generator, identifies the chosen policies and discusses the complexity of the algorithms."}, {"heading": "5.1 Software implementation", "text": "The proposed algorithms have been implemented in PILGRIM10 API, a software platform that our lab is actively developing to provide an efficient tool to deal with several probabilistic graphical models (e.g., BNs, Dynamic BNs, PRMs). Developed in C++, PILGRIM uses Boost graph library11 to manage graphs, ProBT API12 to manipulate BNs objects and Database Template Library (DTL)13 to communicate with databases. Currently, only PostgreSQL RDBMS is supported in this platform.\nBesides the algorithms, we have also implemented serialization of PRMs. Because there is currently no formalization of PRMs, we propose an enhanced version of the XML syntax of the ProbModelXML specification14 to serialize our generated models. We have added new tags to specify\n10http://pilgrim.univ-nantes.fr/ 11http://www.boost.org/ 12http://www.probayes.com/fr/Bayesian-Programming-Book/downloads/ 13http://dtemplatelib.sourceforge.net/dtl introduction.htm 14http://www.cisiad.uned.es/techreports/ProbModelXML.pdf\nnotions related to relational schema definition and we used the standard <AdditionalProperties> tags to add further notions related to PRMs (e.g., aggregators associated with dependencies, classes associated with nodes)."}, {"heading": "5.2 Implemented policies", "text": "Policy for generating the relational schema DAG structure. To randomly generate the relational schema DAG structure, we use PMMixed algorithm (cf. Section 2.1), which generates uniformly distributed DAGs in the DAGs space. The structure generated by this algorithm may be a disconnected graph whereas we are in need of a DAG structure containing a single connected component. To preserve this condition together with the interest of generating uniformly distributed examples, we follow the rejection sampling technique. The idea is to generate a DAG following PMMixed principle, if this DAG contains just one connected component, then it is accepted, otherwise it is rejected. We repeat these steps until generating a DAG structure satisfying our condition.\nPolicies for generating attributes and their cardinalities. Having the graphical structure, we continue by generating, for each relation R, a primary key attribute, a set of attributes A, where card(A)\u22121 \u223c Poisson(\u03bb = 1), to avoid empty sets, and for each attribute A \u2208 A, we specify a set of possible states V(A), where card(V(A))\u2212 2 \u223c Poisson(\u03bb = 1).\nPolicies for generating the dependency structure. We follow the PMMixed principle to construct a DAG structure inside each class. Then, in order to add inter-class dependencies, we use a modified version of the PMMixed algorithm where we constrain the choice of adding dependencies among only variables that do not belong to the same class."}, {"heading": "5.3 Complexity of the generation process", "text": "We have reported this work to this stage as it is closely related to the choice of the implementation policies. Let N be the number of relations (classes), we report the average complexity of each step of the generation process.\nComplexity of the relational schema generation process. Algorithm 2 is structured of three loops. Namely, the most expensive one is the first loop dedicated for the DAG structure construction and uses the PMMixed algorithm. Time complexity of the PMMixed algorithm is O(N \u2217 lgN). This algorithm is called until reaching the stop condition (i.e., a connected DAG). Let T be the average number of calls of the PMMixed algorithm. T is the ratio of the number of all connected DAG constructed from N nodes [31] to the number of all DAGs constructed from N nodes [3]. Time complexity of Algorithm 2 is is O(T \u2217N \u2217 lgN).\nComplexity of the dependency structure generation process. As for Algorithm 2, the most expensive operation of Algorithm 3 is the\ngeneration of the DAG structure inside each class Xi\u2208{1...N} \u2208 X . Through Algorithm 2, a set of attributes A(Xi) has been generated for each Xi. As card(A(Xi))\u2212 1 \u223c Poisson(\u03bb = 1), following Section 5.2, Then the average number of generated attributes for each class is lambda = 1 + 1 = 2. Then time complexity of the algorithm is O(N \u2217 2 \u2217 lg 2).\nComplexity of the slot chains determination process. The most expensive operation of Algorithm 4 is the Generate Potential Slot chains method. This latter explores recursively the relational schema graph in order to find all paths (i.e., slot chains) of length k \u2208 {0 . . .Kmax}. Time complexity of this method is O(NKmax).\nComplexity of the relational skeleton generation process. The relational skeleton generation algorithm is basically an iteration of depth first search over a relational schema. Thus, complexity of the algorithm would be the same as that of a DFS, i.e. O(V + E) where V and E are respectively the number of vertices and the number of edges in the graph."}, {"heading": "6 Conclusion and perspectives", "text": "We have developed a process that allows to randomly generate probabilistic relational models and instantiate them to populate a relational database. The generated relational data is sampled from not only the functional dependencies of the relational schema but also from the probabilistic dependencies present in the PRM.\nOur process can more generally be used by other data mining methods as a probabilistic generative model allowing to randomly generated relational data. Moreover, it can be enriched by test query components to help database designers to evaluate the effectiveness of their RDBMS components."}], "references": [{"title": "Random generation and population of probabilistic relational models and databases", "author": ["M. Ben Ishak", "P. Leray", "N. Ben Amor"], "venue": "Proceedings of the 26th IEEE International Conference on Tools with Artificial Intelligence ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic relational model benchmark generation: Principle and application", "author": ["M. Ben Ishak", "P. Leray", "N. Ben Amor"], "venue": "Intelligent Data Analysis International Journal (to appear), pages ?\u2013?", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "The asymptotic number of acyclic digraphs", "author": ["E.A. Bender", "R.W. Robinson"], "venue": "ii. J. Comb. Theory, Ser. B, 44(3):363\u2013369", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1988}, {"title": "Benchmarking database systems: A systematic approach", "author": ["D. Bitton", "C. Turbyfill", "D.J. Dewitt"], "venue": "Proceedings of the 9th International Conference on Very Large Data Bases, pages 8\u201319. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1983}, {"title": "Directed scale-free graphs", "author": ["B. Bollob\u00e1s", "C. Borgs", "J. Chayes", "O. Riordan"], "venue": "Proceedings of the fourteenth annual ACM-SIAM symposium on Discrete algorithms, pages 132\u2013139. Society for Industrial and Applied Mathematics", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Flexible database generators", "author": ["N. Bruno", "S. Chaudhuri"], "venue": "Proceedings of the 31st International Conference on Very Large Data Bases, pages 1097\u20131107. ACM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "A personalized recommender system from probabilistic relational model and users\u2019 preferences", "author": ["R. Chulyadyo", "P. Leray"], "venue": "Proceedings of the 18th Annual Conference on Knowledge-Based and Intelligent Information & Engineering Systems, pages 1063\u20131072", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["G.F. Cooper", "E. Herskovits"], "venue": "Machine Learning, 9:309\u2013347", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "Learning Bayesian networks: approaches and issues", "author": ["R. Daly", "Q. Shen", "S. Aitken"], "venue": "The Knowledge Engineering Review, 26:99\u2013157", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "The Relational Database Dictionary", "author": ["C.J. Date"], "venue": "Extended Edition. Apress, New York", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "editors", "author": ["S. Dzeroski", "N. Lavrac"], "venue": "Relational Data Mining. Springer New York Inc., New York, NY, USA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Probabilistic relational models with relational uncertainty: An early study in web page classification", "author": ["E. Fersini", "E. Messina", "F. Archetti"], "venue": "Proceedings of the International Joint Conference on Web Intelligence and Intelligent Agent Technology, pages 139\u2013142. IEEE Computer Society", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning probabilistic relational models", "author": ["N. Friedman", "L. Getoor", "D. Koller", "A. Pfeffer"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence, pages 1300\u20131309", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning statistical models from relational data", "author": ["L. Getoor"], "venue": "PhD thesis, Stanford University", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Probabilistic Relational Models", "author": ["L. Getoor", "D. Koller", "N. Friedman", "A. Pfeffer", "B. Taskar"], "venue": "Getoor, L., and Taskar, B., eds., Introduction to Statistical Relational Learning. MA: MIT Press, Cambridge", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Benchmark Handbook: For Database and Transaction Processing Systems", "author": ["J. Gray"], "venue": "Morgan Kaufmann Publishers Inc., San Francisco, CA, USA", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Quickly generating billion-record synthetic databases", "author": ["J. Gray", "P. Sundaresan", "S. Englert", "K. Baclawski", "P.J. Weinberger"], "venue": "Proceedings of the 1994 ACM SIGMOD international conference on Management of data, pages 243\u2013252. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1994}, {"title": "Probabilistic entity-relationship models", "author": ["D. Heckerman", "C. Meek", "D. Koller"], "venue": "PRMs, and plate models, In Getoor, L., and Taskar, B., eds., Introduction to Statistical Relational Learning. MA: MIT Press, Cambridge", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Propagating uncertainty in Bayesian networks by probabilistic logic sampling", "author": ["M. Henrion"], "venue": "Proceedings of Uncertainty in Artificial Intelligence 2 Annual Conference on Uncertainty in Artificial Intelligence (UAI-86), pages 149\u2013163, Amsterdam, NL", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1986}, {"title": "Random generation of Bayesian networks", "author": ["J.S. Ide", "F.G. Cozman"], "venue": "Brazilian symp.on artificial intelligence, pages 366\u2013375. Springer- Verlag", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Generating random Bayesian networks with constraints on induced width", "author": ["J.S. Ide", "F.G. Cozman", "F.T. Ramos"], "venue": "Proceedings of the 16th Eureopean Conference on Artificial Intelligence, pages 323\u2013327", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Probabilistic frame-based systems", "author": ["D. Koller", "A. Pfeffer"], "venue": "Proc. AAAI, pages 580\u2013587. AAAI Press", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "A sound and complete algorithm for learning causal models from relational data", "author": ["M. Maier", "K. Marazopoulou", "D. Arbour", "D. Jensen"], "venue": "Proceedings of the Twenty-ninth Conference on Uncertainty in Artificial Intelligence, pages 371\u2013380", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning causal models of relational domains", "author": ["M. Maier", "B. Taylor", "H. Oktay", "D. Jensen"], "venue": "Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, pages 531\u2013538", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Reasoning about independence in probabilistic models of relational data", "author": ["M.E. Maier", "K. Marazopoulou", "D. Jensen"], "venue": "CoRR, abs/1302.4381", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Relational dependency networks", "author": ["J. Neville", "D. Jensen"], "venue": "Journal of Machine Learning Research, 8:653\u2013692", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Probabilistic reasoning in intelligent systems", "author": ["J. Pearl"], "venue": "Morgan Kaufmann, San Franciscos", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1988}, {"title": "Probabilistic Reasoning for Complex Systems", "author": ["A.J. Pfeffer"], "venue": "PhD thesis, Stanford University", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2000}, {"title": "Combinatorial stochastic processes", "author": ["J. Pitman"], "venue": "Lecture Notes for St. Flour Summer School", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Attribute-value learning versus inductive logic programming: the missing links", "author": ["L. De Raedt"], "venue": "Proceedings of the Eighth International Conference on Inductive Logic Programming, pages 1\u20138", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Counting unlabeled acyclic digraphs", "author": ["R.W. Robinson"], "venue": "C. H. C. LITTLE, Ed., Combinatorial Mathematics V, volume 622 of Lecture Notes in Mathematics. Springer, Berlin / Heidelberg", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1977}, {"title": "A probabilistic relational model for security risk analysis", "author": ["T. Sommestad", "M. Ekstedt", "P. Johnson"], "venue": "Computers & Security, 29:659\u2013679", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "An algorithm for generation of large Bayesian networks", "author": ["A.R. Statnikov", "I. Tsamardinos", "C. Aliferis"], "venue": "Technical report, Department of Biomedical Informatics, Discovery Systems Laboratory, Vanderbilt University", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Reinforcing the objectoriented aspect of probabilistic relational models", "author": ["L. Torti", "P.H. Wuillemin", "C. Gonzales"], "venue": "Proceedings of the 5th Probabilistic Graphical Models, pages 273\u2013280", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Structured probabilistic inference", "author": ["P.H. Wuillemin", "L. Torti"], "venue": "Int. J. Approx. Reasoning, 53(7):946\u2013968", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "With the growing interest in extracting patterns from such data representation, relational data mining approaches have emerged with the interest of finding patterns in a given relational database [11] and Statistical Relational Learning (SRL) has emerged as an area of machine learning that enables effective and robust reasoning about relational data structures [18].", "startOffset": 196, "endOffset": 200}, {"referenceID": 17, "context": "With the growing interest in extracting patterns from such data representation, relational data mining approaches have emerged with the interest of finding patterns in a given relational database [11] and Statistical Relational Learning (SRL) has emerged as an area of machine learning that enables effective and robust reasoning about relational data structures [18].", "startOffset": 363, "endOffset": 367}, {"referenceID": 21, "context": "In this paper, we are particularly interested in Probabilistic Relational Models (PRMs)1 [22, 28], which represent a relational extension of Bayesian networks [27], where the probability model specification concerns classes of objects rather than simple attributes.", "startOffset": 89, "endOffset": 97}, {"referenceID": 27, "context": "In this paper, we are particularly interested in Probabilistic Relational Models (PRMs)1 [22, 28], which represent a relational extension of Bayesian networks [27], where the probability model specification concerns classes of objects rather than simple attributes.", "startOffset": 89, "endOffset": 97}, {"referenceID": 26, "context": "In this paper, we are particularly interested in Probabilistic Relational Models (PRMs)1 [22, 28], which represent a relational extension of Bayesian networks [27], where the probability model specification concerns classes of objects rather than simple attributes.", "startOffset": 159, "endOffset": 163}, {"referenceID": 6, "context": ", risk analysis, web page classification, recommender systems) [7, 12, 32] as they allow to minimize data preprocessing and the loss of significant information [30].", "startOffset": 63, "endOffset": 74}, {"referenceID": 11, "context": ", risk analysis, web page classification, recommender systems) [7, 12, 32] as they allow to minimize data preprocessing and the loss of significant information [30].", "startOffset": 63, "endOffset": 74}, {"referenceID": 31, "context": ", risk analysis, web page classification, recommender systems) [7, 12, 32] as they allow to minimize data preprocessing and the loss of significant information [30].", "startOffset": 63, "endOffset": 74}, {"referenceID": 29, "context": ", risk analysis, web page classification, recommender systems) [7, 12, 32] as they allow to minimize data preprocessing and the loss of significant information [30].", "startOffset": 160, "endOffset": 164}, {"referenceID": 25, "context": "It allows to generate various relational schemas, from simple to complex ones, and to populate database tables with huge number of tuples derived from underlying probability distributions defined by the Neville and Jensen [26] use the term \u2018Relational Bayesian Network\u2019 to refer to Bayesian networks that have been extended to model relational databases [22, 28] and use the term \u2018PRM\u2019 in its more general sense to distinguish the family of probabilistic graphical models that are interested in extracting statistical patterns from relational models.", "startOffset": 222, "endOffset": 226}, {"referenceID": 21, "context": "It allows to generate various relational schemas, from simple to complex ones, and to populate database tables with huge number of tuples derived from underlying probability distributions defined by the Neville and Jensen [26] use the term \u2018Relational Bayesian Network\u2019 to refer to Bayesian networks that have been extended to model relational databases [22, 28] and use the term \u2018PRM\u2019 in its more general sense to distinguish the family of probabilistic graphical models that are interested in extracting statistical patterns from relational models.", "startOffset": 354, "endOffset": 362}, {"referenceID": 27, "context": "It allows to generate various relational schemas, from simple to complex ones, and to populate database tables with huge number of tuples derived from underlying probability distributions defined by the Neville and Jensen [26] use the term \u2018Relational Bayesian Network\u2019 to refer to Bayesian networks that have been extended to model relational databases [22, 28] and use the term \u2018PRM\u2019 in its more general sense to distinguish the family of probabilistic graphical models that are interested in extracting statistical patterns from relational models.", "startOffset": 354, "endOffset": 362}, {"referenceID": 21, "context": "In this paper, we preserve the term PRM as used by [22, 28].", "startOffset": 51, "endOffset": 59}, {"referenceID": 27, "context": "In this paper, we preserve the term PRM as used by [22, 28].", "startOffset": 51, "endOffset": 59}, {"referenceID": 0, "context": "This paper presents an extended version of a preliminary work published in [1, 2].", "startOffset": 75, "endOffset": 81}, {"referenceID": 1, "context": "This paper presents an extended version of a preliminary work published in [1, 2].", "startOffset": 75, "endOffset": 81}, {"referenceID": 26, "context": "1 Bayesian networks Bayesian networks (BNs) [27] are directed acyclic graphs allowing to efficiently encode and manipulate probability distributions over high-dimensional spaces.", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "Several approaches have been proposed to learn BNs from data [9].", "startOffset": 61, "endOffset": 64}, {"referenceID": 32, "context": "[33] proposed an algorithmic approach to generate arbitrarily large BNs by tiling smaller real-world known networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Some works have been devoted to the generation of synthetic networks but without any guarantee that every allowed graph is produced with the same uniform probability [21].", "startOffset": 166, "endOffset": 170}, {"referenceID": 19, "context": "In [20], the authors have proposed an approach, called PMMixed algorithm, that allows the generation of uniformly distributed Bayesian networks using Markov chains.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "Having the final BN, standard sampling method, such as forward sampling [19], can be used to generate observational data.", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "The relational model is the most commonly used one and it represents the basis for the most large scale knowledge representation systems [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 9, "context": "We call a slot chain single-valued when all the crossed reference slots end with a cardinality Database designs involving referential cycles are usually contraindicated [10].", "startOffset": 169, "endOffset": 173}, {"referenceID": 15, "context": ", generating the transaction workload, defining transaction logic, generating the database) [16].", "startOffset": 92, "endOffset": 96}, {"referenceID": 3, "context": "The main issue was how to provide a large number of records using some known distributions in order to be able to evaluate the system results [4, 6].", "startOffset": 142, "endOffset": 148}, {"referenceID": 5, "context": "The main issue was how to provide a large number of records using some known distributions in order to be able to evaluate the system results [4, 6].", "startOffset": 142, "endOffset": 148}, {"referenceID": 16, "context": "In some research, known benchmarks 3 are used and the ultimate goal is only to generate a large dataset [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "3 Probabilistic relational models Probabilistic relational models [15, 22, 28] are an extension of BNs in the relational context.", "startOffset": 66, "endOffset": 78}, {"referenceID": 21, "context": "3 Probabilistic relational models Probabilistic relational models [15, 22, 28] are an extension of BNs in the relational context.", "startOffset": 66, "endOffset": 78}, {"referenceID": 27, "context": "3 Probabilistic relational models Probabilistic relational models [15, 22, 28] are an extension of BNs in the relational context.", "startOffset": 66, "endOffset": 78}, {"referenceID": 14, "context": "Formally, they are defined as follows [15]:", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "This structure is known as a relational skeleton \u03c3r [15].", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Given a relational skeleton, the PRM \u03a0 defines a distribution over the possible worlds consistent with \u03c3r through a ground Bayesian network [15].", "startOffset": 140, "endOffset": 144}, {"referenceID": 12, "context": "Only few works have been proposed to learn PRMs [13] or almost similar models [23, 24] from relational data.", "startOffset": 48, "endOffset": 52}, {"referenceID": 22, "context": "Only few works have been proposed to learn PRMs [13] or almost similar models [23, 24] from relational data.", "startOffset": 78, "endOffset": 86}, {"referenceID": 23, "context": "Only few works have been proposed to learn PRMs [13] or almost similar models [23, 24] from relational data.", "startOffset": 78, "endOffset": 86}, {"referenceID": 12, "context": "[13] proposed Relational Greedy Hill-Climbing Search (RGS) algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "As for score function, they used a relational extension of the Bayesian Dirichlet (BD) [8] score expressed as follows:", "startOffset": 87, "endOffset": 90}, {"referenceID": 23, "context": "[24] used a predefined schema and have only generated a number of dependencies varying from 5 to 15 and the conditional probability tables for attributes from a Dirichlet distribution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "In [23], the authors have generated relational synthetic data to perform experimentation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "If the schema is fully connected (as described in [25]), it will have a tree structure.", "startOffset": 50, "endOffset": 54}, {"referenceID": 33, "context": "[34] proposed a slightly different representation of PRMs, developed in the basis of the object-oriented framework and expert knowledge.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "In their experimental studies [35], they have randomly generated PRMs using the layer pattern.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "The dependency structure S should be a DAG to guarantee that each generated ground network is also a DAG [14].", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "A, a set of parents from the same class or from further classes while promoting intra-class dependencies in order to control the final model complexity as discussed in [14].", "startOffset": 168, "endOffset": 172}, {"referenceID": 13, "context": "Following [14], the generation process has to give more priority to shorter slot chains for selection.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "Consequently, we have used the penalization term discussed in [14].", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "This is an improvement to our previous work [1, 2], where simplification of slot chains had not been considered.", "startOffset": 44, "endOffset": 50}, {"referenceID": 1, "context": "This is an improvement to our previous work [1, 2], where simplification of slot chains had not been considered.", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": "In our previous work [1, 2], we presented an algorithm to generate relational skeleton, where it generates nearly same number of objects of each class and iteratively adds random edges between objects of a pair of classes such that the direction of the edges conform to the underlying schema.", "startOffset": 21, "endOffset": 27}, {"referenceID": 1, "context": "In our previous work [1, 2], we presented an algorithm to generate relational skeleton, where it generates nearly same number of objects of each class and iteratively adds random edges between objects of a pair of classes such that the direction of the edges conform to the underlying schema.", "startOffset": 21, "endOffset": 27}, {"referenceID": 4, "context": "We adapt [5]\u2019s directed scale-free graph generation algorithm for our special k-partite graph and use Chinese Restaurant Process[29] to apply preferential attachment.", "startOffset": 9, "endOffset": 12}, {"referenceID": 28, "context": "We adapt [5]\u2019s directed scale-free graph generation algorithm for our special k-partite graph and use Chinese Restaurant Process[29] to apply preferential attachment.", "startOffset": 128, "endOffset": 132}, {"referenceID": 0, "context": "Algorithm 6: Generate SubSkeleton Input: Parent object objp; Graph g; Parent node, parent; Children nodes, children; Set of the number of objects of each class generated so far, N ; Scalar parameter \u03b1 Output: Relational skeleton, I = (V,E); Set of the number of objects of each class generated so far, N begin V \u2190 {objp} E \u2190 {} np \u2190 N(parent) %Total number of parents generated so far for C \u2208 children do nc \u2190 N(C) %Total number of the child C generated so far p\u2190 \u03b1/(np \u2212 1 + \u03b1) r \u2190 A random value between 0 and 1 %r \u2208 [0, 1] if r <= p then objc \u2190 Create a new object of type C N(C)\u2190 nc + 1 epc \u2190 (objp, objc) %Add an edge from objp to objc E \u2190 E \u222a {epc} childrenc \u2190 Children of C in the graph g ((V \u2032, E\u2032), N \u2032)\u2190 Generate SubSkeleton(objc, g, C, childrenc, N, \u03b1) V \u2190 V \u222a V \u2032 E \u2190 E \u222a E\u2032 N \u2190 N \u2032 else objc \u2190 An existing object of type C picked randomly with probability nk/(np \u2212 1) where nk = indegree of objc epc \u2190 (objp, objc) %Add an edge from objp to objc E \u2190 E \u222a {epc} I \u2190 (V,E)", "startOffset": 519, "endOffset": 525}, {"referenceID": 30, "context": "T is the ratio of the number of all connected DAG constructed from N nodes [31] to the number of all DAGs constructed from N nodes [3].", "startOffset": 75, "endOffset": 79}, {"referenceID": 2, "context": "T is the ratio of the number of all connected DAG constructed from N nodes [31] to the number of all DAGs constructed from N nodes [3].", "startOffset": 131, "endOffset": 134}], "year": 2016, "abstractText": "The validation of any database mining methodology goes through an evaluation process where benchmarks availability is essential. In this paper, we aim to randomly generate relational database benchmarks that allow to check probabilistic dependencies among the attributes. We are particularly interested in Probabilistic Relational Models (PRMs), which extend Bayesian Networks (BNs) to a relational data mining context and enable effective and robust reasoning over relational data. Even though a panoply of works have focused, separately, on the generation of random Bayesian networks and relational databases, no work has been identified for PRMs on that track. This paper provides an algorithmic approach for generating random PRMs from scratch to fill this gap. The proposed method allows to generate PRMs as well as synthetic relational data from a randomly generated relational schema and a random set of probabilistic dependencies. This can be of interest not only for machine learning researchers to evaluate their proposals in a common framework, but also for databases designers to evaluate the effectiveness of the components of a database management system.", "creator": "LaTeX with hyperref package"}}}