{"id": "1603.01860", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2016", "title": "Generalization error bounds for learning to rank: Does the length of document lists matter?", "abstract": ", consider the generalization capability of algorithms for samples to rank exceed a query level, effectively construct developing about linear sampling. different generalization allocation schemes necessarily degrade as the size of adjacent document list associated with a query increases. problems show once such a degradation is continuously intrinsic to the problem. for several objective functions, including powerful shannon - entropy leak cipher in the well known listnet method, hash lies \\ emph { \\ } degradation affecting generalization ability as long lists become problematic. we continuously provide novel optimal error rules under $ \\ ell _ { $ * and faster status checks if the wiener effect is smooth.", "histories": [["v1", "Sun, 6 Mar 2016 19:01:53 GMT  (34kb)", "http://arxiv.org/abs/1603.01860v1", "Appeared in ICML 2015. arXiv admin note: substantial text overlap witharXiv:1405.0586"]], "COMMENTS": "Appeared in ICML 2015. arXiv admin note: substantial text overlap witharXiv:1405.0586", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ambuj tewari", "sougata chaudhuri"], "accepted": true, "id": "1603.01860"}, "pdf": {"name": "1603.01860.pdf", "metadata": {"source": "META", "title": "Generalization error bounds for learning to rank:  Does the length of document lists matter?", "authors": ["Ambuj Tewari"], "emails": ["TEWARIA@UMICH.EDU", "SOUGATA@UMICH.EDU"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n01 86\n0v 1\n[ cs\n.L G\n] 6\nM ar"}, {"heading": "1. Introduction", "text": "Learning to rank at the query level has emerged as an exciting research area at the intersection of information retrieval and machine learning. Training data in learning to rank consists of queries along with associated documents, where documents are represented as feature vectors. For each query, the documents are labeled with human relevance judgements. The goal at training time is to learn a ranking function that can, for a future query, rank its associated documents in order of their relevance to the query. The performance of ranking functions on test sets is evaluated using a variety of performance measures such as NDCG (Ja\u0308rvelin & Keka\u0308la\u0308inen, 2002), ERR (Chapelle et al., 2009) or Average Precision (Yue et al., 2007).\nThe performance measures used for testing ranking methods cannot be directly optimized during training time as they lead to discontinuous optimization problems. As a re-\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\nsult, researchers often minimize surrogate loss functions that are easier to optimize. For example, one might consider smoothed versions of, or convex upper bounds on, the target performance measure. However, as soon as one optimizes a surrogate loss, one has to deal with two questions (Chapelle et al., 2011). First, does minimizing the surrogate on finite training data imply small expected surrogate loss on infinite unseen data? Second, does small expected surrogate loss on infinite unseen data imply small target loss on infinite unseen data? The first issue is one of generalization error bounds for empirical risk minimization (ERM) algorithms that minimize surrogate loss on training data. The second issue is one of calibration: does consistency in the surrogate loss imply consistency in the target loss?\nThis paper deals with the former issue, viz. that of generalization error bounds for surrogate loss minimization. In pioneering works, Lan et al. (2008; 2009) gave generalization error bounds for learning to rank algorithms. However, while the former paper was restricted to analysis of pairwise approach to learning to rank, the later paper was limited to results on just three surrogates: ListMLE, ListNet and RankCosine. To the best of our knowledge, the most generally applicable bound on the generalization error of query-level learning to rank algorithms has been obtained by Chapelle & Wu (2010).\nThe bound of Chapelle & Wu (2010), while generally applicable, does have an explicit dependence on the length of the document list associated with a query. Our investigations begin with this simple question: is an explicit dependence on the length of document lists unavoidable in generalization error bounds for query-level learning to rank algorithms? We focus on the prevalent technique in literature where learning to rank algorithms learn linear scoring functions and obtain ranking by sorting scores in descending order. Our first contribution (Theorem 3) is to show that dimension of linear scoring functions that are permutation invariant (a necessary condition for being valid scoring functions for learning to rank) has no dependence on the length\nof document lists. Our second contribution (Theorems 5, 6, 9) is to show that as long as one uses the \u201cright\u201d norm in defining the Lipschitz constant of the surrogate loss, we can derive generalization error bounds that have no explicit dependence on the length of document lists. The reason that the second contribution involves three bounds is that they all have different strengths and scopes of application (See Table 1 for a comparison). Our final contribution is to provide novel generalization error bounds for learning to rank in two previously unexplored settings: almost dimension independent bounds when using high dimensional features with \u21131 regularization (Theorem 12) and \u201coptimistic\u201d rates (that can be as fast as O(1/n)) when the loss function is smooth (Theorem 17). We also apply our results on popular convex and non-convex surrogates. All omitted proofs can be found in the appendix (see supplementary material)."}, {"heading": "2. Preliminaries", "text": "In learning to rank (also called subset ranking to distinguish it from other related problems, e.g., bipartite ranking), a training example is of the form ((q, d1, . . . , dm), y). Here q is a search query and d1, . . . , dm are m documents with varying degrees of relevance to the query. Human labelers provide the relevance vector y \u2208 Rm where the entries in y contain the relevance labels for the m individual documents. Typically, y has integer-valued entries in the range {0, . . . , Ymax} where Ymax is often less than 5. For our theoretical analysis, we get rid of some of these details by assuming that some feature map \u03a8 exists to map a query document pair (q, d) to Rd. As a result, the training example ((q, d1, . . . , dm), y) gets converted into (X, y) where X = [\u03a8(q, d1), . . . ,\u03a8(q, dm)]\n\u22a4 is an m \u00d7 d matrix with the m query-document feature vector as rows. With this abstraction, we have an input space X \u2286 Rm\u00d7d and a label space Y \u2286 Rm. A training set consists of iid examples (X(1), y(1)), . . . , (X(n), y(n)) drawn from some underlying distribution D. To rank the documents in an instance X \u2208 X , often a score vector s \u2208 Rm is computed. A ranking of the documents can then be obtained from s by sorting its entries in decreasing order. A common choice for the scoring function is to make it linear in the input X and consider the following class of vector-valued\nfunctions:\nFlin = {X 7\u2192 Xw : X \u2208 Rm\u00d7d, w \u2208 Rd}. (1)\nDepending upon the regularization, we also consider the following two subclasses of Flin :\nF2 := {X 7\u2192 Xw : X \u2208 Rm\u00d7d, w \u2208 Rd, \u2016w\u20162 \u2264 W2}, F1 := {X 7\u2192 Xw : X \u2208 Rm\u00d7d, w \u2208 Rd, \u2016w\u20161 \u2264 W1}.\nIn the input space X , it is natural for the rows of X to have a bound on the appropriate dual norm. Accordingly, whenever we use F2, the input space is set to X = {X \u2208 Rm\u00d7d : \u2200j \u2208 [m], \u2016Xj\u20162 \u2264 RX} where Xj denotes jth row of X and [m] := {1, . . . ,m}. Similarly, when we use F1, we set X = {X \u2208 Rm\u00d7d : \u2200j \u2208 [m], \u2016Xj\u2016\u221e \u2264 R\u0304X}. These are natural counterparts to the following function classes studied in binary classification and regression:\nG2 := {x 7\u2192 \u3008x,w\u3009 : \u2016x\u20162 \u2264 RX , w \u2208 Rd, \u2016w\u20162 \u2264 W2}, G1 := {x 7\u2192 \u3008x,w\u3009 : \u2016x\u2016\u221e \u2264 R\u0304X , w \u2208 Rd, \u2016w\u20161 \u2264 W1}.\nA key ingredient in the basic setup of the learning to rank problem is a loss function \u03c6 : Rm \u00d7 Y \u2192 R+ where R+ denotes the set of non-negative real numbers. Given a class F of vector-valued functions, a loss \u03c6 yields a natural loss class: namely the class of real-valued functions that one gets by composing \u03c6 with functions in F :\n\u03c6 \u25e6 F := {(X, y) 7\u2192 \u03c6(f(X), y) : X \u2208 Rm\u00d7d, f \u2208 F}.\nFor vector valued scores, the Lipschitz constant of \u03c6 depends on the norm ||| \u00b7 ||| that we decide to use in the score space (||| \u00b7 |||\u22c6 is dual of ||| \u00b7 |||):\n\u2200y \u2208 Y, s, s\u2032 \u2208 Rm, |\u03c6(s1, y)\u2212\u03c6(s2, y)| \u2264 G\u03c6|||s1\u2212s2|||.\nIf \u03c6 is differentiable, this is equivalent to: \u2200y \u2208 Y, s \u2208 R\nm, |||\u2207s\u03c6(s, y)|||\u22c6 \u2264 G\u03c6. Similarly, the smoothness constant H\u03c6 of \u03c6 defined as: \u2200y \u2208 Y, s, s\u2032 \u2208 Rm,\n|||\u2207s\u03c6(s1, y)\u2212\u2207s\u03c6(s2, y)|||\u22c6 \u2264 H\u03c6|||s1 \u2212 s2|||.\nalso depends on the norm used in the score space. If \u03c6 is twice differentiable, the above inequality is equivalent to\n\u2200y \u2208 Y, s \u2208 Rm, |||\u22072s\u03c6(s, y)|||op \u2264 H\u03c6\nwhere ||| \u00b7 |||op is the operator norm induced by the pair ||| \u00b7 |||, ||| \u00b7 |||\u22c6 and defined as |||M |||op := supv 6=0 |||Mv|||\u22c6|||v||| . Define the expected loss of w under the distribution D L\u03c6(w) := E(X,y)\u223cD [\u03c6(Xw, y)] and its empirical loss on the sample as L\u0302\u03c6(w) := 1n \u2211n i=1 \u03c6(X (i)w, y(i)). The minimizer of L\u03c6(w) (resp. L\u0302\u03c6(w)) over some function class (parameterized by w) will be denoted by w\u22c6 (resp. w\u0302). We may refer to expectations w.r.t. the sample using E\u0302 [\u00b7]. To reduce notational clutter, we often refer to (X, y) jointly by Z and X \u00d7 Y by Z . For vectors, \u3008u, v\u3009 denotes the standard inner product \u2211 i uivi and for matrices U, V of the\nsame shape, \u3008U, V \u3009 means Tr(U\u22a4V ) = \u2211ij UijVij . The set of m! permutation \u03c0 of degree m is denoted by Sm. A vector of ones is denoted by 1."}, {"heading": "3. Application to Specific Losses", "text": "To whet the reader\u2019s appetite for the technical presentation that follows, we will consider two loss functions, one convex and one non-convex, to illustrate the concrete improvements offered by our new generalization bounds. A generalization bound is of the form: L\u03c6(w\u0302) \u2264 L\u03c6(w \u22c6)+\u201ccomplexity term\u201d. It should be noted that w\u22c6 is not available to the learning algorithm as it needs knowledge of underlying distribution of the data. The complexity term of Chapelle & Wu (2010) is O(GCW\u03c6 W2RX \u221a m/n). The constant GCW\u03c6 is the Lipschitz constant of the surrogate \u03c6 (viewed as a function of the score vector s) w.r.t. \u21132 norm. Our bounds will instead be of the form O(G\u03c6W2RX \u221a 1/n), where G\u03c6 is the Lipschitz constant of \u03c6 w.r.t. \u2113\u221e norm. Note that our bounds are free of any explicit m dependence. Also, by definition, G\u03c6 \u2264 GCW\u03c6 \u221a m but the former can be much smaller as the two examples below illustrate. In benchmark datasets (Liu et al., 2007), m can easily be in the 100-1000 range."}, {"heading": "3.1. Application to ListNet", "text": "The ListNet ranking method (Cao et al., 2007) uses a convex surrogate, that is defined in the following way1. Define m maps from Rm to R as: Pj(v) = exp(vj)/ \u2211m i=1 exp(vi) for j \u2208 [m]. Then, we have, for s \u2208 Rm and y \u2208 Rm,\n\u03c6LN(s, y) = \u2212 m\u2211\nj=1\nPj(y) logPj(s).\nAn easy calculation shows that the Lipschitz (as well as smoothness) constant of \u03c6LN is m independent.\n1The ListNet paper actually defines a family of losses based on probability models for top k documents. We use k = 1 in our definition since that is the version implemented in their experimental results.\nProposition 1. The Lipschitz (resp. smoothness) constant of \u03c6LN w.r.t. \u2016 \u00b7 \u2016\u221e satisfies G\u03c6LN \u2264 2 (resp. H\u03c6LN \u2264 2) for any m \u2265 1. Since the bounds above are independent of m, so the generalization bounds resulting from their use in Theorem 9 and Theorem 17 will also be independent of m (up to logarithmic factors). We are not aware of prior generalization bounds for ListNet that do not scale with m. In particular, the results of Lan et al. (2009) have an m! dependence since they consider the top-m version of ListNet. However, even if the top-1 variant above is considered, their proof technique will result in at least a linear dependence on m and does not result in as tight a bound as we get from our general results. It is also easy to see that the Lipschitz constant GCW\u03c6LN of ListNet loss w.r.t. \u21132 norm is also 2 and hence the bound of Chapelle & Wu (2010) necessarily has a \u221a m dependence in it. Moreover, generalization error bounds for ListNet exploiting its smoothness will interpolate between the pessimistic 1/ \u221a n and optimistic 1/n rates. These have never been provided before."}, {"heading": "3.2. Application to Smoothed DCG@1", "text": "This example is from the work of Chapelle & Wu (2010). Smoothed DCG@1, a non-convex surrogate, is defined as:\n\u03c6SD(s, y) = D(1)\nm\u2211\ni=1\nG(yi) exp(si/\u03c3)\u2211 j exp(sj/\u03c3) ,\nwhere D(i) = 1/ log2(1 + i) is the \u201cdiscount\u201d function and G(i) = 2i \u2212 1 is the \u201cgain\u201d function. The amount of smoothing is controlled by the parameter \u03c3 > 0 and the smoothed version approaches DCG@1 as \u03c3 \u2192 0 (DCG stands for Discounted Cumulative Gain (Ja\u0308rvelin & Keka\u0308la\u0308inen, 2002)).\nProposition 2. The Lipschitz constant of \u03c6SD w.r.t. \u2016 \u00b7 \u2016\u221e satisfies G\u03c6SD \u2264 2D(1)G(Ymax)/\u03c3 for any m \u2265 1. Here Ymax is maximum possible relevance score of a document (usually less than 5).\nAs in the ListNet loss case we previously considered, the generalization bound resulting from Theorem 9 will be independent of m. This is intuitively satisfying: DCG@1, whose smoothing we are considering, only depends on the document that is put in the top position by the score vector s (and not on the entire sorted order of s). Our generalization bound does not deteriorate as the total list size m grows. In contrast, the bound of Chapelle & Wu (2010) will necessarily deteriorate as\n\u221a m since the constant GCW\u03c6SD\nis the same as G\u03c6SD . Moreover, it should be noted that even in the original SmoothedDCG paper, \u03c3 is present in the denominator of GCW\u03c6SD , so our results are directly comparable. Also note that this example can easily be extended to consider DCG@k for case when document list length m \u226b k (a very common scenario in practice)."}, {"heading": "3.3. Application to RankSVM", "text": "RankSVM (Joachims, 2002) is another well established ranking method, which minimizes a convex surrogate based on pairwise comparisons of documents. A number of studies have shown that ListNet has better empirical performance than RankSVM. One possible reason for the better performance of ListNet over RankSVM is that the Lipschitz constant of RankSVM surrogate w.r.t \u2016 \u00b7 \u2016\u221e doe scale with document list size as O(m2). Due to lack of space, we give the details in the supplement."}, {"heading": "4. Does The Length of Document Lists Matter?", "text": "Our work is directly motivated by a very interesting generalization bound for learning to rank due to Chapelle & Wu (2010, Theorem 1). They considered a Lipschitz continuous loss \u03c6 with Lipschitz constant GCW\u03c6 w.r.t. the \u21132 norm. They show that, with probability at least 1\u2212 \u03b4,\n\u2200w \u2208 F2, L\u03c6(w) \u2264 L\u0302\u03c6(w) + 3GCW\u03c6 W2RX \u221a m\nn\n+\n\u221a 8 log(1/\u03b4)\nn .\nThe dominant term on the right is O(GCW\u03c6 W2RX \u221a m/n). In the next three sections, we will derive improved bounds of the form O\u0303(G\u03c6W2RX \u221a 1/n) where G\u03c6 \u2264 GCW\u03c6 \u221a m but can be much smaller. Before we do that, let us examine the dimensionality reduction in linear scoring function that is caused by a natural permutation invariance requirement."}, {"heading": "4.1. Permutation invariance removes m dependence in dimensionality of linear scoring functions", "text": "As stated in Section 2, a ranking is obtained by sorting a score vector obtained via a linear scoring function f . Consider the space of linear scoring function that consists of all linear maps f that map Rm\u00d7d to Rm:\nFfull := { X 7\u2192 [\u3008X,W1\u3009 , . . . , \u3008X,Wm\u3009]\u22a4 : Wi \u2208 Rm\u00d7d } .\nThese linear maps are fully parameterized by matrices W1, . . . ,Wm. Thus, a full parameterization of the linear scoring function is of dimension m2d. Note that the popularly used class of linear scoring functions Flin defined in Eq. 1 is actually a low d-dimensional subspace of the full m2d dimensional space of all linear maps. It is important to note that the dimension of Flin is independent of m. In learning theory, one of the factors influencing the generalization error bound is the richness of the class of hypothesis functions. Since the linear function class Flin has dimension independent of m, we intuitively expect that, at least under some conditions, algorithms that minimize ranking losses using linear scoring functions should have\nan m independent complexity term in the generalization bound. The reader might wonder whether the dimension reduction from m2d to d in going from Ffull to Flin is arbitrary. To dispel this doubt, we prove the lower dimensional class Flin is the only sensible choice of linear scoring functions in the learning to rank setting. This is because scoring functions should satisfy a permutation invariance property. That is, if we apply a permutation \u03c0 \u2208 Sm to the rows of X to get a matrix \u03c0X then the scores should also simply get permuted by \u03c0. That is, we should only consider scoring functions in the following class:\nFperminv = {f : \u2200\u03c0 \u2208 Sm, \u2200X \u2208 Rm\u00d7d, \u03c0f(X) = f(\u03c0X)}.\nThe permutation invariance requirement, in turn, forces a reduction from dimension m2d to just 2d (which has no dependence on m).\nTheorem 3. The intersection of the function classes Ffull and Fperminv is the 2d-dimensional class:\nF \u2032lin = {X 7\u2192 Xw + (1\u22a4Xv)1 : w, v \u2208 Rd}. (2)\nNote that the extra degree of freedom provided by the v parameter in Eq. 2 is useless for ranking purposes since adding a constant vector (i.e., a multiple of 1) to a score vector has no effect on the sorted order. This is why we said that Flin is the only sensible choice of linear scoring functions."}, {"heading": "5. Online to Batch Conversion", "text": "In this section, we build some intuition as to why it is natural to use \u2016 \u00b7 \u2016\u221e in defining the Lipschitz constant of the loss \u03c6. To this end, consider the following well known online gradient descent (OGD) regret guarantee. Recall that OGD refers to the simple online algorithm that makes the update wi+1 \u2190 wi\u2212\u03b7\u2207wifi(wi) at time i. If we run OGD to generate wi\u2019s, we have, for all \u2016w\u20162 \u2264 W2:\nn\u2211\ni=1\nfi(wi)\u2212 n\u2211\ni=1\nfi(w) \u2264 W 22 2\u03b7 + \u03b7G2n\nwhere G is a bound on the maximum \u21132-norm of the gradients \u2207wifi(wi) and fi\u2019s have to be convex. If (X(1), y(1)), . . . , (X(n), y(n)) are iid then by setting fi(w) = \u03c6(X\n(i)w, y(i)), 1 \u2264 i \u2264 n we can do an \u201conline to batch conversion\u201d. That is, we optimize over \u03b7, take expectations and use Jensen\u2019s inequality to get the following excess risk bound:\n\u2200\u2016w\u20162 \u2264 W2, E [L\u03c6(w\u0302OGD)]\u2212 L\u03c6(w) \u2264 W2G \u221a 2\nn\nwhere w\u0302OGD = 1n \u2211n\ni=1 wi and G has to satisfy (noting that s = X(i)wi)\nG \u2265 \u2016\u2207wifi(wi)\u20162 = \u2016(X(i))\u22a4\u2207s\u03c6(X(i)wi, y(i))\u20162\nwhere we use the chain rule to express \u2207w in terms of \u2207s. Finally, we can upper bound\n\u2016(X(i))\u22a4\u2207s\u03c6(X(i)wi, y(i))\u20162 \u2264 \u2016(X(i))\u22a4\u20161\u21922 \u00b7 \u2016\u2207s\u03c6(X(i)wi, y(i))\u20161 \u2264 RX\u2016\u2207s\u03c6(X(i)wi, y(i))\u20161\nas RX \u2265 maxmj=1 \u2016Xj\u20162 and because of the following lemma. Lemma 4. For any 1 \u2264 p \u2264 \u221e,\n\u2016X\u2016p\u2192q = sup v 6=0 \u2016Xv\u2016q \u2016v\u2016p\n\u2016X\u22a4\u20161\u2192p = \u2016X\u2016q\u2192\u221e = mmax j=1 \u2016Xj\u2016p ,\nwhere q is the dual exponent of p (i.e., 1 q + 1 p = 1).\nThus, we have shown the following result.\nTheorem 5. Let \u03c6 be convex and have Lipschitz constant G\u03c6 w.r.t. \u2016 \u00b7 \u2016\u221e. Suppose we run online gradient descent (with appropriate step size \u03b7) on fi(w) = \u03c6(X(i)w, y(i)) and return w\u0302OGD = 1T \u2211n i=1 wi. Then we have, \u2200\u2016w\u20162 \u2264 W2, E [L\u03c6(w\u0302OGD)]\u2212L\u03c6(w) \u2264 G\u03c6 W2 RX \u221a 2\nn .\nThe above excess risk bound has no explicit m dependence. This is encouraging but there are two deficiencies of this approach based on online regret bounds. First, the result applies to the output of a specific algorithm that may not be the method of choice for practitioners. For example, the above argument does not yield uniform convergence bounds that could lead to excess risk bounds for ERM (or regularized versions of it). Second, there is no way to generalize the result to Lipschitz, but non-convex loss functions. It may noted here that the original motivation for Chapelle & Wu (2010) to prove their generalization bound was to consider the non-convex loss used in their SmoothRank method. We will address these issues in the next two sections."}, {"heading": "6. Stochastic Convex Optimization", "text": "We first define the regularized empirical risk minimizer:\nw\u0302\u03bb = argmin \u2016w\u20162\u2264W2\n\u03bb 2 \u2016w\u201622 + L\u0302\u03c6(w). (3)\nWe now state the main result of this section.\nTheorem 6. Let the loss function \u03c6 be convex and have Lipschitz constantG\u03c6 w.r.t. \u2016\u00b7\u2016\u221e. Then, for an appropriate choice of \u03bb = O(1/ \u221a n), we have\nE [L\u03c6(w\u0302\u03bb)] \u2264 L\u03c6(w\u22c6) + 2G\u03c6 RX W2 ( 8\nn +\n\u221a 2\nn\n) .\nThis result applies to a batch algorithm (regularized ERM) but unfortunately requires the regularization parameter \u03bb to be set in a particular way. Also, it does not apply to non-convex losses and does not yield uniform convergence bounds. In the next section, we will address these deficiencies. However, we will incur some extra logarithmic factors that are absent in the clean bound above."}, {"heading": "7. Bounds for Non-convex Losses", "text": "The above discussion suggests that we have a possibility of deriving tighter, possibly m-independent, generalization error bounds by assuming that \u03c6 is Lipschitz continuous w.r.t. \u2016 \u00b7 \u2016\u221e. The standard approach in binary classification is to appeal to the Ledoux-Talagrand contraction principle for establishing Rademacher complexity (Bartlett & Mendelson, 2003). It gets rid of the loss function and incurs a factor equal to the Lipschitz constant of the loss in the Rademacher complexity bound. Since the loss function takes scalar argument, the Lipschitz constant is defined for only one norm, i.e., the absolute value norm. It is not immediately clear how such an approach would work when the loss takes vector valued arguments and is Lipschitz w.r.t. \u2016 \u00b7 \u2016\u221e. We are not aware of an appropriate extension of the Ledoux-Talagrand contraction principle. Note that Lipschitz continuity w.r.t. the Euclidean norm \u2016 \u00b7 \u20162 does not pose a significant challenge since Slepian\u2019s lemma can be applied to get rid of the loss. Several authors have already exploited Slepian\u2019s lemma in this context (Bartlett & Mendelson, 2003; Chapelle & Wu, 2010). We take a route involving covering numbers and define the data-dependent (pseudo-)metric:\ndZ (1:n) \u221e (w,w \u2032) := n max i=1\n\u2223\u2223\u2223\u03c6(X(i)w, y(i))\u2212 \u03c6(X(i)w\u2032, y(i)) \u2223\u2223\u2223\nLet N\u221e(\u01eb, \u03c6 \u25e6 F , Z(1:n)) be the covering number at scale \u01eb of the composite class \u03c6 \u25e6F = \u03c6 \u25e6F1 or \u03c6 \u25e6F2 w.r.t. the above metric. Also define\nN\u221e(\u01eb, \u03c6 \u25e6 F , n) := max Z(1:n) N\u221e(\u01eb, \u03c6 \u25e6 F , Z(1:n)).\nWith these definitions in place, we can state our first result on covering numbers. Proposition 7. Let the loss \u03c6 be Lipschitz w.r.t. \u2016 \u00b7 \u2016\u221e with constant G\u03c6. Then following covering number bound holds: log2 N\u221e(\u01eb, \u03c6 \u25e6 F2, n) \u2264 \u2308 G2\u03c6 W 2 2 R 2 X\n\u01eb2\n\u2309 log2(2mn+ 1).\nProof. Note that\nn max i=1\n\u2223\u2223\u2223\u03c6(X(i)w, y(i))\u2212 \u03c6(X(i)w\u2032, y(i)) \u2223\u2223\u2223\n\u2264 G\u03c6 \u00b7 nmax i=1 m max j=1\n\u2223\u2223\u2223 \u2329 X (i) j , w \u232a \u2212 \u2329 X (i) j , w \u2032 \u232a\u2223\u2223\u2223 .\nThis immediately implies that if we have a cover of the class G2 (Sec.2) at scale \u01eb/G\u03c6 w.r.t. the metric\nn max i=1 m max j=1\n\u2223\u2223\u2223 \u2329 X (i) j , w \u232a \u2212 \u2329 X (i) j , w \u2032 \u232a\u2223\u2223\u2223\nthen it is also a cover of \u03c6\u25e6F2 w.r.t. dZ (1:n)\n\u221e , at scale \u01eb. Now comes a simple, but crucial observation: from the point of view of the scalar valued function class G2, the vectors (X\n(i) j ) i=1:n j=1:m constitute a data set of size mn. Therefore,\nN\u221e(\u01eb, \u03c6 \u25e6 F2, n) \u2264 N\u221e(\u01eb/G\u03c6,G2,mn). (4)\nNow we appeal to the following bound due to Zhang (2002, Corollary 3) (and plug the result into (4)):\nlog2 N\u221e(\u01eb/G\u03c6,G2,mn) \u2264 \u2308 G2\u03c6 W 2 2 R 2 X\n\u01eb2\n\u2309 log2(2mn+1)\nCovering number N2(\u01eb, \u03c6\u25e6F , Z(1:n)) uses pseudo-metric:\nd Z(1:n) 2 (w,w \u2032) :=\n( n\u2211\ni=1\n1\nn\n( \u03c6(X(i)w, y(i))\u2212 \u03c6(X(i)w\u2032, y(i))\n)2 )1/2\nIt is well known that a control on N2(\u01eb, \u03c6 \u25e6 F , Z(1:n)) provides control on the empirical Rademacher complexity and that N2 covering numbers are smaller than N\u221e ones. For us, it will be convenient to use a more refined version2 due to Mendelson (2002). Let H be a class of functions, with H : Z 7\u2192 R, uniformly bounded by B. Then, we have following bound on empirical Rademacher complexity\nR\u0302n (H)\n\u2264 inf \u03b1>0\n 4\u03b1+ 10 \u222b suph\u2208H \u221a E\u0302[h2]\n\u03b1\n\u221a log2 N2(\u01eb,H, Z(1:n))\nn d\u01eb\n\n\n(5)\n\u2264 inf \u03b1>0\n(\n4\u03b1+ 10\n\u222b B\n\u03b1\n\u221a log2 N2(\u01eb,H, Z(1:n))\nn d\u01eb\n)\n. (6)\nHere R\u0302n (H) is the empirical Rademacher complexity of the class H defined as\nR\u0302n (H) := E\u03c31:n [ sup h\u2208H 1 n n\u2211\ni=1\n\u03c3ih(Zi)\n] ,\nwhere \u03c31:n = (\u03c31, . . . , \u03c3n) are iid Rademacher (symmetric Bernoulli) random variables.\n2We use a further refinement due to Srebro and Sridharan available at http://ttic.uchicago.edu/\u02dckarthik/dudley.pdf\nCorollary 8. Let \u03c6 be Lipschitz w.r.t. \u2016 \u00b7 \u2016\u221e and uniformly bounded3 by B for w \u2208 F2. Then the empirical Rademacher complexities of the class \u03c6 \u25e6F2 is bounded as\nR\u0302n (\u03c6 \u25e6 F2) \u2264 10G\u03c6W2RX \u221a log2(3mn)\nn\n\u00d7 log 6B \u221a n\n5G\u03c6W2RX \u221a log2(3mn) .\nProof. This follows by simply plugging in estimates from Proposition 7 into (6) and choosing \u03b1 optimally.\nControl on the Rademacher complexity immediately leads to uniform convergence bounds and generalization error bounds for ERM. The informal O\u0303 notation hides factors logarithmic in m,n,B,G\u03c6, RX ,W1. Note that all hidden factors are small and computable from the results above.\nTheorem 9. Suppose \u03c6 is Lipschitz w.r.t. \u2016 \u00b7 \u2016\u221e with constant G\u03c6 and is uniformly bounded by B as w varies over F2. With probability at least 1\u2212 \u03b4,\n\u2200w \u2208 F2, L\u03c6(w) \u2264 L\u0302\u03c6(w)\n+ O\u0303 ( G\u03c6W2RX \u221a 1\nn +B\n\u221a log(1/\u03b4)\nn\n)\nand therefore with probability at least 1\u2212 2\u03b4, L\u03c6(w\u0302) \u2264 L\u03c6(w\u22c6)+O\u0303 ( G\u03c6W2RX \u221a 1\nn +B\n\u221a log(1/\u03b4)\nn\n) .\nwhere w\u0302 is an empirical risk minimizer over F2.\nProof. Follows from standard bounds using Rademacher complexity. See, for example, Bartlett & Mendelson (2003).\nAs we said before, ignoring logarithmic factors, the bound forF2 is an improvement over the bound of Chapelle & Wu (2010)."}, {"heading": "8. Extensions", "text": "We extend the generalization bounds above to two settings: a) high dimensional features and b) smooth losses."}, {"heading": "8.1. High-dimensional features", "text": "In learning to rank situations involving high dimensional features, it may not be appropriate to use the class F2 of \u21132 bounded predictors. Instead, we would like to consider the class F1 of \u21131 bounded predictors. In this case, it is\n3A uniform bound on the loss easily follows under the (very reasonable) assumption that \u2200y,\u2203sy s.t. \u03c6(sy, y) = 0. Then \u03c6(Xw, y) \u2264 G\u03c6\u2016Xw \u2212 sy\u2016\u221e \u2264 G\u03c6(W2RX + maxy\u2208Y \u2016sy\u2016\u221e) \u2264 G\u03c6(2W2RX).\nnatural to measure size of the input matrix X in terms of a bound R\u0304X on the maximum \u2113\u221e norm of each of its row. The following analogue of Proposition 7 can be shown.\nProposition 10. Let the loss \u03c6 be Lipschitz w.r.t. \u2016\u00b7\u2016\u221e with constant G\u03c6. Then the following covering number bound holds: log2 N\u221e(\u01eb, \u03c6 \u25e6 F1, n) \u2264 \u2308 288G2\u03c6W 2 1 R\u0304 2 X (2 + log d)\n\u01eb2\n\u2309\n\u00d7 log2 ( 2 \u2308 8G\u03c6W1R\u0304X\n\u01eb\n\u2309 mn+ 1 ) .\nUsing the above result to control the Rademacher complexity of \u03c6 \u25e6 F1 gives the following bound. Corollary 11. Let \u03c6 be Lipschitz w.r.t. \u2016\u00b7\u2016\u221e and uniformly bounded byB for w \u2208 F1. Then the empirical Rademacher complexities of the class \u03c6 \u25e6 F1 is bounded as\nR\u0302n (\u03c6 \u25e6 F1) \u2264 120 \u221a 2G\u03c6W1R\u0304X\n\u221a log(d) log2(24mnG\u03c6W1R\u0304X)\nn\n\u00d7 log2 B+24mnG\u03c6W1R\u0304X 40 \u221a 2G\u03c6W1R\u0304X \u221a log(d) log2(24mnG\u03c6W1R\u0304X ) .\nAs in the previous section, control of Rademacher complexity immediately yields uniform convergence and ERM generalization error bounds.\nTheorem 12. Suppose \u03c6 is Lipschitz w.r.t. \u2016 \u00b7 \u2016\u221e with constant G\u03c6 and is uniformly bounded by B as w varies over F1. With probability at least 1\u2212 \u03b4,\n\u2200w \u2208 F1, L\u03c6(w) \u2264 L\u0302\u03c6(w)\n+ O\u0303 ( G\u03c6W1R\u0304X \u221a log d\nn +B\n\u221a log(1/\u03b4)\nn\n)\nand therefore with probability at least 1\u2212 2\u03b4,\nL\u03c6(w\u0302) \u2264 L\u03c6(w\u22c6)+O\u0303 ( G\u03c6W1R\u0304X \u221a log d\nn +B\n\u221a log(1/\u03b4)\nn\n)\nwhere w\u0302 is an empirical risk minimizer over F1.\nAs can be easily seen from Theorem. 12, the generalization bound is almost independent of the dimension of the document feature vectors. We are not aware of existence of such a result in learning to rank literature."}, {"heading": "8.2. Smooth losses", "text": "We will again use online regret bounds to explain why we should expect \u201coptimistic\u201d rates for smooth losses before giving more general results for smooth but possibly nonconvex losses."}, {"heading": "8.3. Online regret bounds under smoothness", "text": "Let us go back to OGD guarantee, this time presented in a slightly more refined version. If we run OGD with learning rate \u03b7 then, for all \u2016w\u20162 \u2264 W2:\nn\u2211\ni=1\nfi(wi)\u2212 n\u2211\ni=1\nfi(w) \u2264 W 22 2\u03b7 + \u03b7\nn\u2211\ni=1\n\u2016gi\u201622\nwhere gi = \u2207wifi(wi) (if fi is not differentiable at wi then we can set gi to be an arbitrary subgradient of fi at wi). Now assume that all fi\u2019s are non-negative functions and are smooth w.r.t. \u2016 \u00b7 \u20162 with constant H . Lemma 3.1 of Srebro et al. (2010) tells us that any non-negative, smooth function f(w) enjoy an important self-bounding property for the gradient:\n\u2016\u2207wfi(w)\u20162 \u2264 \u221a 4Hfi(w)\nwhich bounds the magnitude of the gradient of f at a point in terms of the value of the function itself at that point. This means that \u2016gi\u201622 \u2264 4Hfi(wi) which, when plugged into the OGD guarantee, gives:\nn\u2211\ni=1\nfi(wi)\u2212 n\u2211\ni=1\nfi(w) \u2264 W 22 2\u03b7 + 4\u03b7H n\u2211\ni=1\nfi(wi)\nAgain, setting fi(w) = \u03c6(X(i)w, y(i)), 1 \u2264 t \u2264 n, and using the online to batch conversion technique, we can arrive at the bound: for all \u2016w\u20162 \u2264 W2:\nE [L\u03c6(w\u0302)] \u2264 L\u03c6(w) (1\u2212 4\u03b7H) + W 22 2\u03b7(1\u2212 4\u03b7H)n\nAt this stage, we can fix w = w\u22c6, the optimal \u21132-norm bounded predictor and get optimal \u03b7 as:\n\u03b7 = W2\n4HW2 + 2 \u221a 4H2W 22 + 2HL\u03c6(w \u22c6)n . (7)\nAfter plugging this value of \u03b7 in the bound above and some algebra (see Section H), we get the upper bound\nE [L\u03c6(w\u0302)] \u2264 L\u03c6(w\u22c6) + 2 \u221a 2HW 22L\u03c6(w \u22c6)\nn + 8HW 22 n .\n(8) Such a rate interpolates between a 1/ \u221a n rate in the \u201cpessimistic\u201d case (L\u03c6(w\u22c6) > 0) and the 1/n rate in the \u201coptimistic\u201d case (L\u03c6(w\u22c6) = 0) (this terminology is due to Panchenko (2002)).\nNow, assuming \u03c6 to be twice differentiable, we need H such that\nH \u2265 \u2016\u22072w\u03c6(X(i)w, y(i))\u20162\u21922 = \u2016X\u22a4\u22072s\u03c6(X(i)w, y(i))X\u20162\u21922\nwhere we used the chain rule to express \u22072w in terms of \u22072s. Note that, for OGD, we need smoothness in w w.r.t.\n\u2016 \u00b7 \u20162 which is why the matrix norm above is the operator norm corresponding to the pair \u2016\u00b7\u20162, \u2016\u00b7\u20162. In fact, when we say \u201coperator norm\u201d without mentioning the pair of norms involved, it is this norm that is usually meant. It is well known that this norm is equal to the largest singular value of the matrix. But, just as before, we can bound this in terms of the smoothness constant of \u03c6 w.r.t. \u2016 \u00b7 \u2016\u221e (see Section I in the appendix):\n\u2016(X(i))\u22a4\u22072s\u03c6(X(i)w, y(i))X(i)\u20162\u21922 \u2264 R2X\u2016\u22072s\u03c6(X(i)w, y(i))\u2016\u221e\u21921.\nwhere we used Lemma 4 once again. This result using online regret bounds is great for building intuition but suffers from the two defects we mentioned at the end of Section 5. In the smoothness case, it additionally suffers from a more serious defect: the correct choice of the learning rate \u03b7 requires knowledge of L\u03c6(w\u22c6) which is seldom available."}, {"heading": "8.4. Generalization error bounds under smoothness", "text": "Once again, to prove a general result for possibly nonconvex smooth losses, we will adopt an approach based on covering numbers. To begin, we will need a useful lemma from Srebro et al. (2010, Lemma A.1 in the Supplementary Material). Note that, for functions over real valued predictions, we do not need to talk about the norm when dealing with smoothness since essentially the only norm available is the absolute value.\nLemma 13. For any h-smooth non-negative function f : R \u2192 R+ and any t, r \u2208 R we have\n(f(t)\u2212 f(r))2 \u2264 6h(f(t) + f(r))(t \u2212 r)2.\nWe first provide an extension of this lemma to the vector case.\nLemma 14. If \u03c6 : Rm \u2192 R+ is a non-negative function with smoothness constant H\u03c6 w.r.t. a norm ||| \u00b7 ||| then for any s1, s2 \u2208 Rm we have\n(\u03c6(s1)\u2212 \u03c6(s2))2 \u2264 6H\u03c6 \u00b7 (\u03c6(s1) + \u03c6(s2)) \u00b7 |||s1 \u2212 s2|||2.\nUsing the basic idea behind local Rademacher complexity analysis, we define the following loss class:\nF\u03c6,2(r) := {(X, y) 7\u2192 \u03c6(Xw, y) : \u2016w\u20162 \u2264 W2, L\u0302\u03c6(w) \u2264 r}.\nNote that this is a random subclass of functions since L\u0302\u03c6(w) is a random variable. Proposition 15. Let \u03c6 be smooth w.r.t. \u2016\u00b7\u2016\u221e with constant H\u03c6. The covering numbers of F\u03c6,2(r) in the dZ (1:n)\n2 metric defined above are bounded as follows: log2 N2(\u01eb,F\u03c6,2(r), Z(1:n)) \u2264 \u2308 12H\u03c6 W 2 2 R 2 X r\n\u01eb2\n\u2309 log2(2mn+1).\nControl of covering numbers easily gives a control on the Rademacher complexity of the random subclass F\u03c6,2(r). Corollary 16. Let \u03c6 be smooth w.r.t. \u2016 \u00b7 \u2016\u221e with constant H\u03c6 and uniformly bounded by B for w \u2208 F2. Then the empirical Rademacher complexity of the class F\u03c6,2(r) is bounded as\nR\u0302n (F\u03c6,2(r)) \u2264 4 \u221a rC log\n3 \u221a B\nC\nwhere C = 5 \u221a 3W2RX\n\u221a H\u03c6 log2(3mn)\nn .\nWith the above corollary in place we can now prove our second key result.\nTheorem 17. Suppose \u03c6 is smooth w.r.t. \u2016 \u00b7 \u2016\u221e with constant H\u03c6 and is uniformly bounded by B over F2. With probability at least 1\u2212 \u03b4,\n\u2200w \u2208 F2, L\u03c6(w) \u2264 L\u0302\u03c6(w) + O\u0303 (\u221a\nL\u03c6(w)D0 n + D0 n\n)\nwhere D0 = B log(1/\u03b4) + W 22R 2 XH\u03c6. Moreover, with probability at least 1\u2212 2\u03b4,\nL\u03c6(w\u0302) \u2264 L\u03c6(w\u22c6) + O\u0303 (\u221a\nL\u03c6(w\u22c6)D0 n + D0 n\n)\nwhere w\u0302, w\u22c6 are minimizers of L\u0302\u03c6(w) and L\u03c6(w) respectively (over w \u2208 F2)."}, {"heading": "9. Conclusion", "text": "We showed that it is not necessary for generalization error bounds for query-level learning to rank algorithms to deteriorate with increasing length of document lists associated with queries. The key idea behind our improved bounds was defining Lipschitz constants w.r.t. \u2113\u221e norm instead of the \u201cstandard\u201d \u21132 norm. As a result, we were able to derive much tighter guarantees for popular loss functions such as ListNet and Smoothed DCG@1 than previously available.\nOur generalization analysis of learning to rank algorithms paves the way for further interesting work. One possibility is to use these bounds to design active learning algorithms for learning to rank with formal label complexity guarantees. Another interesting possibility is to consider other problems, such as multi-label learning, where functions with vector-valued outputs are learned by optimizing a joint function of those outputs."}, {"heading": "Acknowledgement", "text": "We gratefully acknowledge the support of NSF under grant IIS-1319810. Thanks to Prateek Jain for discussions that led us to Theorem 3."}, {"heading": "A. Proof of Proposition 1", "text": "Proof. Let ej\u2019s denote standard basis vectors. We have\n\u2207s\u03c6LN(s, y) = \u2212 m\u2211\nj=1\nPj(y)ej + m\u2211\nj=1\nexp(sj)\u2211m j\u2032=1 exp(sj\u2032 ) ej\nTherefore,\n\u2016\u2207s\u03c6LN(s, y)\u20161 \u2264 m\u2211\nj=1\nPj(y)\u2016ej\u20161 + m\u2211\nj=1\nexp(sj)\u2211m j\u2032=1 exp(sj\u2032 ) \u2016ej\u20161\n= 2.\nWe also have\n[\u22072s\u03c6LN(s, y)]j,k =    \u2212 exp(2sj)(\u2211m j\u2032=1 exp(sj\u2032 )) 2 + exp(sj)\u2211 m j\u2032=1 exp(sj\u2032 ) if j = k\n\u2212 exp(sj+sk)(\u2211m j\u2032=1 exp(sj\u2032 )) 2 if j 6= k .\nMoreover,\n\u2016\u22072s\u03c6LN(s, y)\u2016\u221e\u21921 \u2264 m\u2211\nj=1\nm\u2211\nk=1\n|[\u22072s\u03c6LN(s, y)]j,k|\n\u2264 m\u2211\nj=1\nm\u2211\nk=1\nexp(sj + sk)\n( \u2211m j\u2032=1 exp(sj\u2032)) 2 +\nm\u2211\nj=1\nexp(sj)\u2211m j\u2032=1 exp(sj\u2032 )\n= ( \u2211m j=1 exp(sj)) 2\n( \u2211m\nj\u2032=1 exp(sj\u2032 )) 2 + \u2211m j=1 exp(sj)\u2211m j\u2032=1 exp(sj\u2032 )\n= 2"}, {"heading": "B. Proof of Proposition 2", "text": "Proof. Let 1(condition) denote an indicator variable. We have\n[\u2207s\u03c6SD(s, y)]j = D(1) ( m\u2211\ni=1\nG(ri)\n[ 1\n\u03c3 exp(si/\u03c3)\u2211 j\u2032 exp(sj\u2032/\u03c3) 1(i=j) \u2212 1 \u03c3 exp((si + sj)/\u03c3) ( \u2211 j\u2032 exp(sj\u2032/\u03c3)) 2\n])\nTherefore,\n\u2016\u2207s\u03c6SD(s, y)\u20161 D(1)G(Ymax) \u2264 m\u2211\nj=1\n( m\u2211\ni=1\n[ 1\n\u03c3 exp(si/\u03c3)\u2211 j\u2032 exp(sj\u2032/\u03c3) 1(i=j) + 1 \u03c3 exp((si + sj)/\u03c3) ( \u2211 j\u2032 exp(sj\u2032/\u03c3)) 2\n])\n= 1\n\u03c3 ( \u2211 j exp(sj/\u03c3)\u2211 j\u2032 exp(sj\u2032/\u03c3) + ( \u2211 j exp(sj/\u03c3)) 2 ( \u2211 j\u2032 exp(sj\u2032/\u03c3)) 2 )\n= 2\n\u03c3 ."}, {"heading": "C. RankSVM", "text": "The RankSVM surrogate is defined as:\n\u03c6RS(s, y) =\nm\u2211\ni=1\nm\u2211\nj=1\nmax(0, 1(yi>yj)(1 + sj \u2212 si))\nIt is easy to see that \u2207s\u03c6RS(s, y) = \u2211m\ni=1 \u2211m j=1 max(0, 1(yi>yj)(1 + sj \u2212 si))(ej \u2212 ei). Thus, the \u21131 norm of gradient\nis O(m2) ."}, {"heading": "D. Proof of Theorem 3", "text": "Proof. It is straightforward to check that F \u2032lin is contained in both Ffull as well as Fperminv. So, we just need to prove that any f that is in both Ffull and Fperminv has to be in F \u2032lin as well. Let P\u03c0 denote the m \u00d7 m permutation matrix corresponding to a permutation \u03c0. Consider the full linear class Ffull. In matrix notation, the permutation invariance property means that, for any \u03c0,X , we have P\u03c0[\u3008X,W1\u3009 , . . . , \u3008X,Wm\u3009\u3009]\u22a4 = [\u3008P\u03c0X,W1\u3009 , . . . , \u3008P\u03c0X,Wm\u3009]\u22a4. Let \u03c11 = {P\u03c0 : \u03c0(1) = 1}, where \u03c0(i) denotes the index of the element in the ith position according to permutation \u03c0. Fix any P \u2208 \u03c11. Then, for any X , \u3008X,W1\u3009 = \u3008PX,W1\u3009. This implies that, for all X , Tr(W1\u22a4X) = Tr(W1\u22a4PX). Using the fact that Tr(A\u22a4X) = Tr(B\u22a4X), \u2200X implies A = B, we have that W1\u22a4 = W1\u22a4P . Because P\u22a4 = P\u22121, this means PW1 = W1. This shows that all rows of W1, other than 1st row, are the same but perhaps different from 1st row. By considering \u03c1i = {P\u03c0 : \u03c0(i) = i} for i > 1, the same reasoning shows that, for each i, all rows of Wi, other than ith row, are the same but possibly different from ith row.\nLet \u03c11\u21942 = {P\u03c0 : \u03c0(1) = 2, \u03c0(2) = 1}. Fix any P \u2208 \u03c11\u21942. Then, for any X , \u3008X,W2\u3009 = \u3008PX,W1\u3009 and \u3008X,W1\u3009 = \u3008PX,W2\u3009. Thus, we have W\u22a42 = W\u22a41 P as well as W\u22a41 = W\u22a42 P which means PW2 = W1, PW1 = W2. This shows that row 1 of W1 and row 2 of W2 are the same. Moreover, row 2 of W1 and row 1 of W2 are the same. Thus, for some u, u\u2032 \u2208 Rd, W1 is of the form [u|u\u2032|u\u2032| . . . |u\u2032]\u22a4 and W2 is of the form [u\u2032|u|u\u2032| . . . |u\u2032]\u22a4. Repeating this argument by considering \u03c11\u2194i for i > 2 shows that Wi is of the same form (u in row i and u\u2032 elsewhere).\nTherefore, we have proved that any linear map that is permutation invariant has to be of the form:\nX 7\u2192  u\u22a4Xi + (u\u2032)\u22a4 \u2211\nj 6=i Xj\n  m\ni=1\n.\nWe can reparameterize above using w = u\u2212 u\u2032 and v = u\u2032 which proves the result."}, {"heading": "E. Proof of Lemma 4", "text": "Proof. The first equality is true because\n\u2016X\u22a4\u20161\u2192p = sup v 6=0 \u2016X\u22a4v\u2016p \u2016v\u20161 = sup v 6=0 sup u6=0\n\u2329 X\u22a4v, u \u232a \u2016v\u20161\u2016u\u2016q\n= sup u6=0 sup v 6=0 \u3008v,Xu\u3009 \u2016v\u20161\u2016u\u2016q = sup u6=0 \u2016Xu\u2016\u221e \u2016u\u2016q = \u2016X\u2016q\u2192\u221e.\nThe second is true because\n\u2016X\u2016q\u2192\u221e = sup u6=0 \u2016Xu\u2016\u221e \u2016u\u2016q = sup u6=0 m max j=1 | \u3008Xj , u\u3009 | \u2016u\u2016q\n= m\nmax j=1 sup u6=0 | \u3008Xj, u\u3009 | \u2016u\u2016q = m max j=1 \u2016Xj\u2016p."}, {"heading": "F. Proof of Theorem 6", "text": "Our theorem is developed from the \u201cexpectation version\u201d of Theorem 6 of Shalev-Shwartz et al. (2009) that was originally given in probabilistic form. The expected version is as follows.\nLet Z be a space endowed with a probability distribution generating iid draws Z1, . . . , Zn. Let W \u2286 Rd and f : W\u00d7Z \u2192\nR be \u03bb-strongly convex4 and G-Lipschitz (w.r.t. \u2016 \u00b7 \u20162) in w for every z. We define F (w) = E [f(w,Z)] and let w\u22c6 = argmin\nw\u2208W F (w),\nw\u0302 = argmin w\u2208W\n1\nn\nn\u2211\ni=1\nf(w,Zi).\nThen E [F (w\u0302)\u2212 F (w\u22c6)] \u2264 4G2 \u03bbn\n, where the expectation is taken over the sample. The above inequality can be proved by carefully going through the proof of Theorem 6 proved by Shalev-Shwartz et al. (2009).\nWe now derive the \u201cexpectation version\u201d of Theorem 7 of Shalev-Shwartz et al. (2009). Define the regularized empirical risk minimizer as follows:\nw\u0302\u03bb = argmin w\u2208W\n\u03bb 2 \u2016w\u201622 + 1 n\nn\u2211\ni=1\nf(w,Zi). (9)\nThe following result gives optimality guarantees for the regularized empirical risk minimizer. Theorem 18. Let W = {w : \u2016w\u20162 \u2264 W2} and let f(w, z) be convex and G-Lipschitz (w.r.t. \u2016 \u00b7 \u20162) in w for every z. Let Z1, ..., Zn be iid samples and let \u03bb = \u221a 4G2 n\nW2 2 2 + 4W2 2 n\n. Then for w\u0302\u03bb and w\u22c6 as defined above, we have\nE [F (w\u0302\u03bb)\u2212 F (w\u22c6)] \u2264 2GW2 ( 8\nn +\n\u221a 2\nn\n) . (10)\nProof. Let r\u03bb(w, z) = \u03bb2 \u2016w\u201622 + f(w, z). Then r\u03bb is \u03bb-strongly convex with Lipschitz constant \u03bbW2 + G in \u2016 \u00b7 \u20162. Applying \u201cexpectation version\u201d of Theorem 6 of Shalev-Shwartz et al. (2009) to r\u03bb, we get\nE\n[ \u03bb\n2 \u2016w\u0302\u03bb\u201622 + F (w\u0302\u03bb)\n] \u2264 min\nw\u2208W\n{ \u03bb\n2 \u2016w\u201622 + F (w)\n} + 4(\u03bbW2 +G) 2\n\u03bbn \u2264 \u03bb 2 \u2016w\u22c6\u201622 + F (w\u2217) +\n4(\u03bbW2 +G) 2\n\u03bbn .\nThus, we get\nE [F (w\u0302\u03bb)\u2212 F (w\u22c6)] \u2264 \u03bbW 22 2 + 4(\u03bbW2 +G) 2 \u03bbn .\nMinimizing the upper bound w.r.t. \u03bb, we get \u03bb = \u221a 4G2\nn\n\u221a 1\nW2 2 2 + 4W2 2 n\n. Plugging this choice back in the equation above and\nusing the fact that \u221a a+ b \u2264 \u221aa+ \u221a b finishes the proof of Theorem 18.\nWe now have all ingredients to prove Theorem 6.\nProof of Theorem 6. Let Z = X \u00d7 Y and f(w, z) = \u03c6(Xw, y) and apply Theorem 18. Finally note that if \u03c6 is G\u03c6Lipschitz w.r.t. \u2016 \u00b7 \u2016\u221e and every row of X \u2208 Rm\u00d7d has Euclidean norm bounded by RX then f(\u00b7, z) is G\u03c6RX -Lipschitz w.r.t. \u2016 \u00b7 \u20162 in w."}, {"heading": "G. Proof of Theorem 12", "text": "Proof. Following exactly the same line of reasoning (reducing a sample of size n, where each prediction is Rm-valued, to an sample of size mn, where each prediction is real valued) as in the beginning of proof of Proposition 7, we have\nN\u221e(\u01eb, \u03c6 \u25e6 F1, n) \u2264 N\u221e(\u01eb/G\u03c6,G1,mn). (11) Plugging in the following bound due to Zhang (2002, Corollary 5):\nlog2 N\u221e(\u01eb/G\u03c6,G1,mn) \u2264 \u2308 288G2\u03c6W 2 1 R\u0304 2 X (2 + ln d)\n\u01eb2\n\u2309\n\u00d7 log2 ( 2\u23088G\u03c6W1R\u0304X/\u01eb\u2309mn+ 1 )\ninto (11) respectively proves the result.\n4Recall that a function is called \u03bb-strongly convex (w.r.t. \u2016 \u00b7 \u20162) iff f \u2212 \u03bb2 \u2016 \u00b7 \u201622 is convex.\nH. Calculations involved in deriving Equation (8)\nPlugging in the value of \u03b7 from (7) into the expression\nL\u03c6(w \u22c6) (1\u2212 4\u03b7H) + W 22 2\u03b7(1\u2212 4\u03b7H)n\nyields (using the shorthand L\u22c6 for L\u03c6(w\u22c6))\nL\u22c6 + 2HW2L\n\u22c6\n\u221a 4H2W 22 + 2HL \u22c6n +\nW2 n\n[ 4H2W 22\u221a\n4H2W 22 + 2HL \u22c6n\n+ \u221a 4H2W 22 + 2HL \u22c6n+ 4HW2\n]\nDenoting HW 22 /n by x, this simplifies to\nL\u22c6 + 2 \u221a xL\u22c6 + 4x \u221a x\u221a\n4x+ 2L\u22c6 + \u221a x \u221a 4x+ 2L\u22c6 + 4x.\nUsing the arithmetic mean-geometric mean inequality to upper bound the middle two terms gives\nL\u22c6 + 2 \u221a 2xL\u22c6 + 4x2 + 4x.\nFinally, using \u221a a+ b \u2264 \u221aa+ \u221a b, we get our final upper bound\nL\u22c6 + 2 \u221a 2xL\u22c6 + 8x."}, {"heading": "I. Calculation of smoothness constant", "text": "\u2016(X(i))\u22a4\u22072s\u03c6(X(i)w, y(i))X(i)\u20162\u21922 = sup v 6=0 \u2016(X(i))\u22a4\u22072s\u03c6(X(i)w, y(i))X(i)v\u20162 \u2016v\u20162\n\u2264 sup v 6=0 \u2016(X(i))\u22a4\u20161\u21922\u2016\u22072s\u03c6(X(i)w, y(i))X(i)v\u20161 \u2016v\u20162 \u2264 sup v 6=0 \u2016(X(i))\u22a4\u20161\u21922 \u00b7 \u2016\u22072s\u03c6(X(i)w, y(i))\u2016\u221e\u21921 \u00b7 \u2016X(i)v\u2016\u221e \u2016v\u20162\n\u2264 sup v 6=0 \u2016(X(i))\u22a4\u20161\u21922 \u00b7 \u2016\u22072s\u03c6(X(i)w, y(i))\u2016\u221e\u21921 \u00b7 \u2016X(i)\u20162\u2192\u221e \u00b7 \u2016v\u20162 \u2016v\u20162 \u2264 (\nm max j=1\n\u2016X(i)j \u2016 )2 \u00b7 \u2016\u22072s\u03c6(X(i)w, y(i))\u2016\u221e\u21921\n\u2264 R2X\u2016\u22072s\u03c6(X(i)w, y(i))\u2016\u221e\u21921."}, {"heading": "J. Proof of Lemma 14", "text": "Proof. Consider the function f(t) = \u03c6((1 \u2212 t)s1 + ts2).\nIt is clearly non-negative. Moreover\n|f \u2032(t1)\u2212 f \u2032(t2)| = | \u3008\u2207s\u03c6(s1 + t1(s2 \u2212 s1))\u2212\u2207s\u03c6(s1 + t2(s2 \u2212 s1)), s2 \u2212 s1\u3009 | \u2264 |||\u2207s\u03c6(s1 + t1(s2 \u2212 s1))\u2212\u2207s\u03c6(s1 + t2(s2 \u2212 s1))|||\u22c6 \u00b7 |||s2 \u2212 s1||| \u2264 H\u03c6 |t1 \u2212 t2| |||s2 \u2212 s1|||2\nand therefore it is smooth with constant h = H\u03c6|||s2 \u2212 s1|||2. Appealing to Lemma 13 now gives\n(f(1)\u2212 f(0))2 \u2264 6H\u03c6|||s2 \u2212 s1|||2(f(1) + f(0))(1\u2212 0)2\nwhich proves the lemma since f(0) = \u03c6(s1) and f(1) = \u03c6(s2)."}, {"heading": "K. Proof of Proposition 15", "text": "Proof. Let w,w\u2032 \u2208 F\u03c6,2(r). Using Lemma 14 n\u2211\ni=1\n1\nn\n( \u03c6(X(i)w, y(i))\u2212 \u03c6(X(i)w\u2032, y(i)) )2\n\u2264 6H\u03c6 n\u2211\ni=1\n1\nn\n( \u03c6(X(i)w, y(i)) + \u03c6(X(i)w\u2032, y(i)) )\n\u00b7 \u2016X(i)w \u2212X(i)w\u2032\u20162\u221e \u2264 6H\u03c6 \u00b7 nmax\ni=1 \u2016X(i)w \u2212X(i)w\u2032\u20162\u221e\n\u00b7 n\u2211\ni=1\n1\nn\n( \u03c6(X(i)w, y(i)) + \u03c6(X(i)w\u2032, y(i)) )\n= 6H\u03c6 \u00b7 nmax i=1\n\u2016X(i)w \u2212X(i)w\u2032\u20162\u221e \u00b7 ( L\u0302\u03c6(w) + L\u0302\u03c6(w \u2032) )\n\u2264 12H\u03c6r \u00b7 n\nmax i=1\n\u2016X(i)w \u2212X(i)w\u2032\u20162\u221e.\nwhere the last inequality follows because L\u0302\u03c6(w) + L\u0302\u03c6(w\u2032) \u2264 2r. This immediately implies that if we have a cover of the class G2 at scale \u01eb/ \u221a 12H\u03c6r w.r.t. the metric\nn max i=1 m max j=1\n\u2223\u2223\u2223 \u2329 X (i) j , w \u232a \u2212 \u2329 X (i) j , w \u2032 \u232a\u2223\u2223\u2223\nthen it is also a cover of F\u03c6,2(r) w.r.t. dZ (1:n) 2 . Therefore, we have\nN2(\u01eb,F\u03c6,2(r), Z(1:n)) \u2264 N\u221e(\u01eb/ \u221a 12H\u03c6r,G2,mn). (12)\nAppealing once again to a result by Zhang (2002, Corollary 3), we get\nlog2 N\u221e(\u01eb/ \u221a 12H\u03c6r,G2,mn) \u2264\n\u2308 12H\u03c6W 2 2 R 2 X r\n\u01eb2\n\u2309\n\u00d7 log2(2mn+ 1)\nwhich finishes the proof."}, {"heading": "L. Proof of Corollary 16", "text": "Proof. We plug in Proposition 15\u2019s estimate into (5):\nR\u0302n (F\u03c6,2(r)) \u2264 inf \u03b1>0  4\u03b1+ 10 \u222b \u221aBr\n\u03b1\n\u221a\u221a\u221a\u221a \u2308 12H\u03c6 W 22 R 2 X r\n\u01eb2\n\u2309 log2(2mn+ 1) n d\u01eb  \n\u2264 inf \u03b1>0\n( 4\u03b1+ 20 \u221a 3W2RX \u221a rH\u03c6 log2(3mn)\nn\n\u222b \u221aBr\n\u03b1\n1 \u01eb d\u01eb\n) .\nNow choosing \u03b1 = C \u221a r where C = 5 \u221a 3W2RX\n\u221a H\u03c6 log2(3mn)\nn gives us the upper bound\nR\u0302n (F\u03c6,2(r)) \u2264 4 \u221a rC ( 1 + log \u221a B\nC\n) \u2264 4\u221arC log 3 \u221a B\nC ."}, {"heading": "M. Proof of Theorem 17", "text": "Proof. We appeal to Theorem 6.1 of Bousquet (2002) that assumes there exists an upper bound\nR\u0302n (F2,\u03c6(r)) \u2264 \u03c8n(r)\nwhere \u03c8n : [0,\u221e) \u2192 R+ is a non-negative, non-decreasing, non-zero function such that \u03c8n(r)/ \u221a r is non-increasing. The upper bound in Corollary 16 above satisfies these conditions and therefore we set \u03c8n(r) = 4 \u221a rC log 3 \u221a B\nC with C as\ndefined in Corollary 16. From Bousquet\u2019s result, we know that, with probability at least 1\u2212 \u03b4,\n\u2200w \u2208 F2, L\u03c6(w) \u2264 L\u0302\u03c6(w) + 45r\u22c6n + \u221a 8r\u22c6nL\u03c6(w)\n+ \u221a 4r0L\u03c6(w) + 20r0\nwhere r0 = B(log(1/\u03b4) + log logn)/n and r\u22c6n is the largest solution to the equation r = \u03c8n(r). In our case, r \u22c6 n =(\n4C log 3 \u221a B\nC\n)2 . This proves the first inequality.\nNow, using the above inequality with w = w\u0302, the empirical risk minimizer and noting that L\u0302\u03c6(w\u0302) \u2264 L\u0302\u03c6(w\u22c6), we get\nL\u03c6(w\u0302) \u2264 L\u0302\u03c6(w\u22c6) + 45r\u22c6n + \u221a 8r\u22c6nL\u03c6(w\u0302)\n+ \u221a 4r0L\u03c6(w\u0302) + 20r0\nThe second inequality now follows after some elementary calculations detailed below.\nM.1. Details of some calculations in the proof of Theorem 17\nUsing Bernstein\u2019s inequality, we have, with probability at least 1\u2212 \u03b4,\nL\u0302\u03c6(w \u22c6) \u2264 L\u03c6(w\u22c6) +\n\u221a 4Var[\u03c6(Xw\u22c6, y)] log(1/\u03b4)\nn +\n4B log(1/\u03b4)\nn\n\u2264 L\u03c6(w\u22c6) + \u221a 4BL\u03c6(w\u22c6) log(1/\u03b4)\nn +\n4B log(1/\u03b4)\nn\n\u2264 L\u03c6(w\u22c6) + \u221a 4r0L\u03c6(w\u22c6) + 4r0.\nSet D0 = 45r\u22c6n + 20r0. Putting the two bounds together and using some simple upper bounds, we have, with probability at least 1\u2212 2\u03b4,\nL\u03c6(w\u0302) \u2264 \u221a D0L\u0302\u03c6(w\u22c6) +D0,\nL\u0302\u03c6(w \u22c6) \u2264 \u221a D0L\u03c6(w\u22c6) +D0.\nwhich implies that\nL\u03c6(w\u0302) \u2264 \u221a D0 \u221a\u221a D0L\u03c6(w\u22c6) +D0 +D0.\nUsing \u221a ab \u2264 (a+ b)/2 to simplify the first term on the right gives us\nL\u03c6(w\u0302) \u2264 D0 2 +\n\u221a D0L\u03c6(w\u22c6) +D0\n2 +D0 =\n\u221a D0L\u03c6(w\u22c6)\n2 + 2D0 ."}], "references": [{"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["Bartlett", "Peter L", "Mendelson", "Shahar"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bartlett et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2003}, {"title": "Concentration inequalities and empirical processes theory applied to the analysis of learning algorithms", "author": ["Bousquet", "Olivier"], "venue": "PhD thesis, Ecole Polytechnique,", "citeRegEx": "Bousquet and Olivier.,? \\Q2002\\E", "shortCiteRegEx": "Bousquet and Olivier.", "year": 2002}, {"title": "Learning to rank: from pairwise approach to listwise approach", "author": ["Cao", "Zhe", "Qin", "Tao", "Liu", "Tie-Yan", "Tsai", "Ming-Feng", "Li", "Hang"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Cao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2007}, {"title": "Gradient descent optimization of smoothed information retrieval metrics", "author": ["Chapelle", "Olivier", "Wu", "Mingrui"], "venue": "Information retrieval,", "citeRegEx": "Chapelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2010}, {"title": "Expected reciprocal rank for graded relevance", "author": ["Chapelle", "Olivier", "Metlzer", "Donald", "Zhang", "Ya", "Grinspan", "Pierre"], "venue": "In Proceedings of the 18th ACM Conference on Information and Knowledge Management,", "citeRegEx": "Chapelle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2009}, {"title": "Future directions in learning to rank", "author": ["Chapelle", "Olivier", "Chang", "Yi", "Liu", "Tie-Yan"], "venue": "In Proceedings of the Yahoo! Learning to Rank Challenge June", "citeRegEx": "Chapelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2010}, {"title": "Cumulated gainbased evaluation of IR techniques", "author": ["J\u00e4rvelin", "Kalervo", "Kek\u00e4l\u00e4inen", "Jaana"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "J\u00e4rvelin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin et al\\.", "year": 2002}, {"title": "Optimizing search engines using clickthrough data", "author": ["Joachims", "Thorsten"], "venue": "In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Joachims and Thorsten.,? \\Q2002\\E", "shortCiteRegEx": "Joachims and Thorsten.", "year": 2002}, {"title": "Query-level stability and generalization in learning to rank", "author": ["Lan", "Yanyan", "Liu", "Tie-Yan", "Qin", "Tao", "Ma", "Zhiming", "Li", "Hang"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Lan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2008}, {"title": "Generalization analysis of listwise learning-to-rank algorithms", "author": ["Lan", "Yanyan", "Liu", "Tie-Yan", "Ma", "Zhiming", "Li", "Hang"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Lan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2009}, {"title": "LETOR: Benchmark dataset for research on learning to rank for information retrieval", "author": ["Liu", "Tie-yan", "Xu", "Jun", "Qin", "Tao", "Xiong", "Wenying", "Li", "Hang"], "venue": "In Proceedings of SIGIR 2007 Workshop on Learning to Rank for Information Retrieval,", "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Rademacher averages and phase transitions in Glivenko-Cantelli classes", "author": ["Mendelson", "Shahar"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Mendelson and Shahar.,? \\Q2002\\E", "shortCiteRegEx": "Mendelson and Shahar.", "year": 2002}, {"title": "Some extensions of an inequality of Vapnik and Chervonenkis", "author": ["Panchenko", "Dmitriy"], "venue": "Electronic Communications in Probability,", "citeRegEx": "Panchenko and Dmitriy.,? \\Q2002\\E", "shortCiteRegEx": "Panchenko and Dmitriy.", "year": 2002}, {"title": "Stochastic convex optimization", "author": ["Shalev-Shwartz", "Shai", "Shamir", "Ohad", "Srebro", "Nathan", "Sridharan", "Karthik"], "venue": "In Proceedings of the 22nd Annual Conference on Learning Theory,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2009}, {"title": "Smoothness, low noise, and fast rates", "author": ["Srebro", "Nathan", "Sridharan", "Karthik", "Tewari", "Ambuj"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "Covering number bounds of certain regularized linear function classes", "author": ["Zhang", "Tong"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Zhang and Tong.,? \\Q2002\\E", "shortCiteRegEx": "Zhang and Tong.", "year": 2002}, {"title": "Proof of Theorem 6 Our theorem is developed from the \u201cexpectation version\u201d of Theorem 6 of Shalev-Shwartz et al. (2009) that was originally given in probabilistic form. The expected version is as follows. Let Z be a space endowed with a probability distribution generating iid draws Z1", "author": ["F. \u2016Xj\u2016p"], "venue": null, "citeRegEx": ".Xj.p.,? \\Q2009\\E", "shortCiteRegEx": ".Xj.p.", "year": 2009}], "referenceMentions": [{"referenceID": 4, "context": "The performance of ranking functions on test sets is evaluated using a variety of performance measures such as NDCG (J\u00e4rvelin & Kek\u00e4l\u00e4inen, 2002), ERR (Chapelle et al., 2009) or Average Precision (Yue et al.", "startOffset": 151, "endOffset": 174}, {"referenceID": 3, "context": "However, as soon as one optimizes a surrogate loss, one has to deal with two questions (Chapelle et al., 2011). First, does minimizing the surrogate on finite training data imply small expected surrogate loss on infinite unseen data? Second, does small expected surrogate loss on infinite unseen data imply small target loss on infinite unseen data? The first issue is one of generalization error bounds for empirical risk minimization (ERM) algorithms that minimize surrogate loss on training data. The second issue is one of calibration: does consistency in the surrogate loss imply consistency in the target loss? This paper deals with the former issue, viz. that of generalization error bounds for surrogate loss minimization. In pioneering works, Lan et al. (2008; 2009) gave generalization error bounds for learning to rank algorithms. However, while the former paper was restricted to analysis of pairwise approach to learning to rank, the later paper was limited to results on just three surrogates: ListMLE, ListNet and RankCosine. To the best of our knowledge, the most generally applicable bound on the generalization error of query-level learning to rank algorithms has been obtained by Chapelle & Wu (2010). The bound of Chapelle & Wu (2010), while generally applicable, does have an explicit dependence on the length of the document list associated with a query.", "startOffset": 88, "endOffset": 1220}, {"referenceID": 3, "context": "However, as soon as one optimizes a surrogate loss, one has to deal with two questions (Chapelle et al., 2011). First, does minimizing the surrogate on finite training data imply small expected surrogate loss on infinite unseen data? Second, does small expected surrogate loss on infinite unseen data imply small target loss on infinite unseen data? The first issue is one of generalization error bounds for empirical risk minimization (ERM) algorithms that minimize surrogate loss on training data. The second issue is one of calibration: does consistency in the surrogate loss imply consistency in the target loss? This paper deals with the former issue, viz. that of generalization error bounds for surrogate loss minimization. In pioneering works, Lan et al. (2008; 2009) gave generalization error bounds for learning to rank algorithms. However, while the former paper was restricted to analysis of pairwise approach to learning to rank, the later paper was limited to results on just three surrogates: ListMLE, ListNet and RankCosine. To the best of our knowledge, the most generally applicable bound on the generalization error of query-level learning to rank algorithms has been obtained by Chapelle & Wu (2010). The bound of Chapelle & Wu (2010), while generally applicable, does have an explicit dependence on the length of the document list associated with a query.", "startOffset": 88, "endOffset": 1255}, {"referenceID": 10, "context": "In benchmark datasets (Liu et al., 2007), m can easily be in the 100-1000 range.", "startOffset": 22, "endOffset": 40}, {"referenceID": 2, "context": "Application to ListNet The ListNet ranking method (Cao et al., 2007) uses a convex surrogate, that is defined in the following way1.", "startOffset": 50, "endOffset": 68}, {"referenceID": 8, "context": "In particular, the results of Lan et al. (2009) have an m! dependence since they consider the top-m version of ListNet.", "startOffset": 30, "endOffset": 48}, {"referenceID": 8, "context": "In particular, the results of Lan et al. (2009) have an m! dependence since they consider the top-m version of ListNet. However, even if the top-1 variant above is considered, their proof technique will result in at least a linear dependence on m and does not result in as tight a bound as we get from our general results. It is also easy to see that the Lipschitz constant G \u03c6LN of ListNet loss w.r.t. l2 norm is also 2 and hence the bound of Chapelle & Wu (2010) necessarily has a \u221a m dependence in it.", "startOffset": 30, "endOffset": 465}, {"referenceID": 14, "context": "1 of Srebro et al. (2010) tells us that any non-negative, smooth function f(w) enjoy an important self-bounding property for the gradient:", "startOffset": 5, "endOffset": 26}], "year": 2016, "abstractText": "We consider the generalization ability of algorithms for learning to rank at a query level, a problem also called subset ranking. Existing generalization error bounds necessarily degrade as the size of the document list associated with a query increases. We show that such a degradation is not intrinsic to the problem. For several loss functions, including the cross-entropy loss used in the well known ListNet method, there is no degradation in generalization ability as document lists become longer. We also provide novel generalization error bounds under l1 regularization and faster convergence rates if the loss function is smooth.", "creator": "LaTeX with hyperref package"}}}