{"id": "1603.02597", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2016", "title": "Prediction of Infinite Words with Automata", "abstract": "in the classic automated solving sequence averaging, a predictor considers periodic positive correction results from an emitter and tries against guess one next value before output appears. their predictor displays an result if probability is expected period after which all of the system's intentions are correct. in conceptual framework put consider third case into which coherent predictor asks an example and how emitted responses are drawn by a complete set ; i. da., what emitted sequence is an extended word. we examine the predictive stability of finite automata, pushdown automata, stack problems ( including discipline titled pushdown automata ), and multihead row automata. we relate our new formulation using purely periodic ideas, pure periodic words, also multilinear words, describing adaptive prediction algorithms for mastering variable sequences.", "histories": [["v1", "Tue, 8 Mar 2016 17:12:09 GMT  (442kb,D)", "http://arxiv.org/abs/1603.02597v1", null]], "reviews": [], "SUBJECTS": "cs.FL cs.LG", "authors": ["tim smith"], "accepted": false, "id": "1603.02597"}, "pdf": {"name": "1603.02597.pdf", "metadata": {"source": "META", "title": "Prediction of Infinite Words with Automata", "authors": ["Tim Smith"], "emails": ["tim.smith@u-pem.fr"], "sections": [{"heading": "1 Introduction", "text": "One motivation for studying prediction of infinite words comes from its position as a kind of underlying \u201csimplest case\u201d of other prediction tasks. For example, take the problem of designing an intelligent agent, a purposeful autonomous entity able to explore and interact with its environment. At each moment, it receives data from its sensors, which it stores in its memory. We would like the agent to analyze the data it is receiving, so that it can make predictions about future data and carry out actions in the world on the basis of those predictions. That is, we would like the agent to discover the laws of nature governing its environment.\nWithout any constraints on the problem, this is a formidable task. The data being received by the agent might be present in multiple channels, corresponding to sight, hearing, touch, and other senses, and in each channel the data given at each instant could have a complex structure, e.g. a visual field or tactile array. The data source could be nondeterministic or probabilistic, and furthermore could be sensitive to actions taken by the agent, leading to a feedback loop between the agent and its environment. The laws governing the environment could be mathematical in nature or arise from intensive computational processing.\n? This is the full version of a paper accepted for publication at CSR 2016. It contains an appendix with proofs which were sketched in the body.\nar X\niv :1\n60 3.\n02 59\n7v 1\n[ cs\n.F L\n] 8\nM ar\n2 01\n6\nA natural approach to tackling such a complex problem is to start with the easiest case. How, then, can we simplify the above scenario? First, say that instead of receiving data through multiple channels, the agent has only a single channel of data. And say that instead of the data having a complex structure like a visual field, it simply consists of a succession of symbols, and that the set of possible symbols is finite. Say that the data source is completely deterministic, and moreover that the data is not sensitive to the actions or predictions of the agent, but is simply output one symbol at a time without depending on any input.\nUnder these simplifying assumptions, the problem we are left with is that of predicting an infinite word. That is, the agent\u2019s environment now consists of some infinite word, which it is the agent\u2019s task to predict on the basis of the symbols it has seen so far. We hope that by exploring and making progress in this simple setting, we can develop techniques which may help with the more general prediction problems encountered in the original scenario."}, {"heading": "1.1 Our contributions", "text": "In this paper, we consider the case in which the predictor in the above setting is an automaton. In our model, a predicting automaton M takes as input an infinite word \u03b1 and produces as output an infinite word M(\u03b1), with the restriction that for each i \u2265 1, M must output the ith symbol of M(\u03b1) before it can read beyond the i \u2212 1th symbol of \u03b1. If there is an n \u2265 1 such that for every i \u2265 n, the ith symbol of M(\u03b1) equals the ith symbol of \u03b1, then we say that M masters \u03b1.\nWe consider three classes of infinite words. The first are the purely periodic words, those of the form xxx \u00b7 \u00b7 \u00b7 for some string x. Next are the ultimately periodic words, those of the form xyyy \u00b7 \u00b7 \u00b7 for strings x, y. Finally we consider the multilinear words [21], which consist of an initial string followed by strings that repeat in a way governed by linear polynomials, for example abaabaaab \u00b7 \u00b7 \u00b7 .\nAll of the automata we consider are deterministic automata with a one-way input tape. We first examine DFAs (deterministic finite automata), showing that no DFA predictor masters every purely periodic word. We then consider DPDAs (deterministic pushdown automata), showing that no DPDA predictor masters every purely periodic word. We next turn to DSAs (deterministic stack automata). Stack automata are a generalization of pushdown automata whose stack head, in addition to pushing and popping when at the top of the stack, can move up and down the stack in read-only mode [10]. We show that there is a DSA predictor which masters every purely periodic word, and we provide an algorithm by which it can do so.\nNext, we consider multi-DFAs (multihead deterministic finite automata), finite automata with one or more input heads [13]. We show that there is a multiDFA predictor which masters every ultimately periodic word, and we provide an algorithm by which it can do so. Finally, we consider sensing multi-DFAs, multihead DFAs extended with the ability to sense, for each pair of heads, whether those two heads are at the same position on the input tape [14]. We show that there is a sensing multi-DFA predictor which masters every multilinear word,\nand we provide an algorithm by which it can do so. Our results are depicted in Table 1."}, {"heading": "1.2 Related work", "text": "A classic survey of inductive inference, including the problem of sequence prediction, can be found in [2]. The concept of \u201cmastering\u201d an infinite word is a form of \u201clearning in the limit\u201d, a concept which originates with the seminal paper of Gold [11], where it is applied to language learnability. Turing machines are considered as sequence extrapolators in [4]. An early work on prediction of periodic sequences is [20], where these sequences appear in the setting of two-player emission-prediction games. Inference of ultimately periodic sequences is treated in [15] in an \u201coffline\u201d setting, where the input is a finite string and the output is a description of an ultimately periodic sequence. An algorithm is presented which computes the shortest possible description of an ultimately periodic sequence when given a long enough prefix of that sequence, and can be implemented in time and space linear in the size of the input, using techniques from string matching. The algorithm works by finding the LRS (longest repeated suffix) of the input and predicting the symbol which followed that suffix on its previous occurrence.\nIn [18], finite-state automata are considered as predicting machines and the question of which sequences appear \u201crandom\u201d to these machines is answered. A binary sequence is said to appear random to a predicting machine if no more than half of the predictions made of the sequence\u2019s terms by that machine are correct. Further work on this concept appears in [5]. In [9] the finite-state predictability of an infinite sequence is defined as the minimum fraction of prediction errors that can be made by an finite-state predictor, and it is proved that finite-state predictability can be obtained by an efficient prediction procedure using techniques from data compression. In [3] a random prediction method for binary sequences is given which ensures that the proportion of correct predictions approaches the frequency of the more common symbol (0 or 1) in the sequence. In [16], \u201cinverse\nproblems\u201d for D0L systems are discussed (in the title and throughout the paper, the term \u201cfinite automata\u201d refers to morphisms). These problems ask, given a word, to find a morphism and initial string which generate that word (bounds are assumed on the size of the morphism and initial string). An approach is given for solving this problem by trying different string lengths for the righthand side of the morphism until a combination is found which is compatible with the input. A genetic algorithm is described to search the space of word lengths. In [6], an evolutionary algorithm is used to search for the finite-state machine with the highest prediction ratio for a given purely periodic word, in the space of all automata with a fixed number of states. In [7], the problem of successfully predicting a single 0 in an infinite binary word being revealed sequentially to the predictor is considered; only one prediction may be made, but at a time of the predictor\u2019s choosing. Learning of languages consisting of infinite words has also been studied; see [1] for recent work.\nAn early and influential approach to predicting infinite sequences is that of program-size complexity [22]. Unfortunately this model is incomputable, and in [17] it is shown furthermore that some sequences can only be predicted by very complex predictors which cannot be discovered mathematically due to problems of Go\u0308del incompleteness. [17] concludes that \u201cperhaps the only reasonable solution would be to add additional restrictions to both the algorithms which generate the sequences to be predicted, and to the predictors.\u201d This suggestion is akin to the approach followed in the present paper, where the automata and infinite words considered are of various restricted classes. Following on from [17], in [12] the formalism of sequence prediction is extended to a competition between two agents, which is shown to be a computational resources arms race."}, {"heading": "1.3 Outline of paper", "text": "The rest of the paper is organized as follows. Section 2 gives definitions for infinite words and predicting automata. Section 3 studies prediction of purely periodic and ultimately periodic words. Section 4 studies prediction of multilinear words. Section 5 gives our conclusions."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Words", "text": "Where X is a set, we denote the cardinality of X by |X|. For a list or tuple v, v[i] denotes the ith element of v; indexing starts at 1. An alphabet A is a finite set of symbols. A word is a concatenation of symbols from A. We denote the set of finite words by A\u2217 and the set of infinite words by A\u03c9. We call finite words strings and infinite words streams or \u03c9-words. The length of x is denoted by |x|. We denote the empty string by \u03bb. A language is a subset of A\u2217. A (symbolic) sequence S is an element of A\u2217 \u222a A\u03c9. A prefix of S is a string x such that S = xS\u2032 for some sequence S\u2032. The ith symbol of S is denoted by\nS[i]; indexing starts at 1. For a non-empty string x, x\u03c9 denotes the infinite word xxx \u00b7 \u00b7 \u00b7 . Such a word is called purely periodic. An infinite word of the form xy\u03c9, where x and y are strings and y 6= \u03bb, is called ultimately periodic. An infinite word is multilinear if it has the form\nq \u220f\nn\u22650 ra1n+b11 r a2n+b2 2 \u00b7 \u00b7 \u00b7 ramn+bmm ,\nwhere \u220f\ndenotes concatenation, q is a string, m is a positive integer, and for each 1 \u2264 i \u2264 m, ri is a non-empty string and ai and bi are nonnegative integers such that ai + bi > 0. For example,\n\u220f n\u22650 an+1b = abaabaaab \u00b7 \u00b7 \u00b7 is a multilinear word.\nThe class of multilinear words appears in [21] and also in [8] (as the reducts of the \u201cprime\u201d stream \u03a0). Clearly the multilinear words properly include the ultimately periodic words. Any multilinear word which is not ultimately periodic we call properly multilinear."}, {"heading": "2.2 Predictors", "text": "We now define predictors based on various types of automata. (See [23] for results on the original automata, which are language recognizers rather than predictors.) Each predictor M takes as input an infinite word \u03b1 and produces as output an infinite word M(\u03b1), with the restriction that for each i \u2265 1, M must output the ith symbol of M(\u03b1) before it can read beyond the i\u2212 1th symbol of \u03b1. We call M(\u03b1)[i] M \u2019s guess about position i of \u03b1. If M(\u03b1)[i] = \u03b1[i] then we say that the guess is correct; otherwise we say that it is incorrect. If there is an n \u2265 1 such that for every i \u2265 n, M(\u03b1)[i] = \u03b1[i], then we say that M masters \u03b1. (If M outputs only a finite number of symbols when given \u03b1, then we say that M(\u03b1) is undefined and M does not master \u03b1.)\nDFA predictors A DFA predictor is a tuple M = (Q,A, T, ., qs), where Q is the set of states, A is the input alphabet, . is the start-of-input marker, qs \u2208 Q is the initial state, and T is a transition function of the form [Q\u00d7 (A \u222a {.})]\u2192 [Q\u00d7A].\nTo perform a computation, M is given an input consisting of the symbol . followed by an infinite word \u03b1. M starts in state qs with its input head positioned at .. M then makes transitions based on its current state and input symbol. At each transition, M changes state, moves its head to the right, and makes a guess about what the next symbol will be. The sequence of these guesses constitutes M(\u03b1). More formally, let C = [C1, C2, C3, . . . ] where Ci = {[qi, ci, gi] with qi \u2208 Q, ci \u2208 (A \u222a {.}), gi \u2208 A such that q1 = qs and for each i \u2265 1, ci = (.\u03b1)[i] and T (qi, ci) = [qi+1, gi]. Notice that there is only one possible C, given M and \u03b1. Now for i \u2265 1, set M(\u03b1)[i] = gi.\nDPDA predictors A DPDA predictor is a tuple M = (Q,A, F, T, .,M, qs), where Q is the set of states, A is the input alphabet, F is the stack alphabet,\n. is the start-of-input marker, M is the bottom-of-stack marker, qs \u2208 Q is the initial state, and T is a transition function of the form\n[Q\u00d7 (A \u222a {.})\u00d7 (F \u222a {M})]\u2192 [Q\u00d7 (A \u222a {stay})\u00d7 (F \u222a {pop, keep})].\nTo perform a computation, M is given an input consisting of the symbol . followed by an infinite word \u03b1. M starts in state qs with stack M and with its input head positioned at .. M then makes transitions based on its current state, input symbol, and stack symbol. At each transition, M (1) changes state, (2) either moves its input head to the right and guesses what the next symbol will be, or else keeps it in place (using stay), and (3) either pushes a symbol to the stack, pops the stack, or leaves it alone (using keep). It is illegal for M to pop M. The sequence of guesses made by M constitutes M(\u03b1).\nDSA predictors A DSA predictor is a tuple M = (Q,A, F, T, .,M, qs), where Q is the set of states, A is the input alphabet, F is the stack alphabet, . is the start-of-input marker, M is the bottom-of-stack marker, qs \u2208 Q is the initial state, and T is a transition function of the form\n[Q\u00d7 (A \u222a {.})\u00d7 (F \u222a {M})\u00d7 {top, inside}]\u2192 [Q\u00d7 (A \u222a {stay})\u00d7 (F \u222a {pop, keep, up, down})].\nTo perform a computation, M is given an input consisting of the symbol . followed by an infinite word \u03b1. M starts in state qs with stack M and with its input head positioned at .. M then makes transitions based on its current state, input symbol, stack symbol, and whether or not the stack head is at the top of the stack (top means the stack head is at the top; inside means it is not). At each transition, M (1) changes state, (2) either moves its input head to the right and guesses what the next symbol will be, or else keeps it in place (using stay), and (3) either pushes a symbol to the stack, pops the stack, leaves it alone (using keep), or moves its stack head up or down. It is illegal for M to push or pop the stack when the stack head is not at the top of the stack, or to move it up when it is already at the top or down when it is already at the bottom. The sequence of guesses made by M constitutes M(\u03b1).\nMulti-DFA predictors A multi-DFA predictor is a tuple of the form M = (Q,A, k, T, ., qs), where Q is the set of states, A is the input alphabet, k \u2265 1 is the number of input heads, . is the start-of-input marker, qs \u2208 Q is the initial state, and T is a transition function of the form\n[Q\u00d7 (A \u222a {.})k]\u2192 [Q\u00d7 {stay, right}k \u00d7A].\nTo perform a computation, M is given an input consisting of the symbol . followed by an infinite word \u03b1. M starts in state qs with its k input heads all positioned at .. M then makes transitions based on its current state and the input symbols it sees under each of its heads. At each transition, M (1) changes\nstate, (2) for each head either moves it to the right or keeps it in place (using stay), and (3) makes a guess about what the next symbol will be. If in a given transition, M does not reach a new input position (one which had not previously been reached by any head), M \u2019s guess at that transition is disregarded (i.e., it is not included in M(\u03b1)). That is, M(\u03b1)[i] is the guess of the first transition which moves any head to \u03b1[i].\nA sensing multi-DFA predictor is a multi-DFA predictor extended so that its transition function takes an additional argument indicating, for each pair of heads, whether those two heads are at the same input position."}, {"heading": "3 Prediction of periodic words", "text": "In this section we study finite automata, pushdown automata, stack automata, and multihead finite automata as predictors of purely periodic and ultimately periodic words."}, {"heading": "3.1 Prediction by DFAs", "text": "Theorem 1. Let A be an alphabet such that |A| \u2265 2. Then no DFA predictor masters every purely periodic word over A.\nProof. Suppose some DFA predictor M masters every purely periodic word over A. M has some number of states p. Take any a, b \u2208 A such that a 6= b. Let \u03b1 be the purely periodic word (ap+1b)\u03c9. Then there is an n \u2265 1 such that for every i \u2265 n, M(\u03b1)[i] = \u03b1[i]. Take the first segment of p + 1 consecutive as after the position n. At two of these as, M is in the same state. Then M will repeat the guesses it made between those two as for as long as it keeps reading as. But then M will guess a for the next b, a contradiction. So M does not master \u03b1. ut"}, {"heading": "3.2 Prediction by DPDAs", "text": "Theorem 2. Let A be an alphabet such that |A| \u2265 2. Then no DPDA predictor masters every purely periodic word over A.\nProof (Sketch). Suppose some DPDA predictor M = (Q,A, F, T, .,M, qs) masters every purely periodic word over A. We set p to be very large with respect to |Q| and |F |. Take any a, b \u2208 A such that a 6= b. Let \u03b1 be the purely periodic word (apb)\u03c9. Then there is some position m \u2265 0 after which all of M \u2019s guesses about \u03b1 are correct. Now, between each two segments of p consecutive a\u2019s, there is only one symbol (a single b), so the stack can grow by at most |Q| \u00b7 |F | between each two segments. It follows that in some segment of p consecutive a\u2019s occurring after m, the stack height does not decrease by more than |Q|\u00b7|F |, since otherwise it would eventually become negative. We show that in such a segment, because p is so large with respect to |Q| and |F |, there are two configurations Ci and Cj of M occurring at different input positions with the same state and stack symbol,\nsuch that the stack below the top symbol at Ci is not accessed between Ci and Cj . Then since all of M \u2019s guesses between Ci and Cj are a\u2019s, M will continue to guess a\u2019s for as long as it continues to read a\u2019s. But then M will guess a for the b at the end of the segment, contradicting the supposition that all of M \u2019s guesses about \u03b1 after m are correct. Therefore M does not master every purely periodic word over A. ut"}, {"heading": "3.3 Prediction by DSAs", "text": "We give two results about the predictive capabilities of DSAs: first, that some DSA predictor masters every purely periodic word, and second, that no DSA predictor can master any infinite word which is not multilinear.\nAlgorithm 1 A DSA predictor which masters every purely periodic word. The input head is denoted by hi and the stack head is denoted by hs. The input consists of the symbol . followed by an infinite word \u03b1. Wherever a guess is not specified, it may be taken to be arbitrary.\n1: loop 2: move hi 3: push \u03b1[hi] 4: recovering \u2190 false 5: loop 6: move hs down until stack[hs] = M 7: matched\u2190 true 8: loop 9: move hs up\n10: move hi, guessing stack[hs] 11: matched\u2190 false if \u03b1[hi] 6= stack[hs] 12: break if top 13: recovering \u2190 true if not matched 14: break if recovering and matched\nTheorem 3. Let A be an alphabet. Then some DSA predictor masters every purely periodic word over A.\nProof. Let M be a DSA predictor which implements Algorithm 1. (The boolean variables recovering and matched can be accommodated using M \u2019s finite state control.) The idea is that M will gradually build up its stack until the stack consists of the period (or a cyclic shift thereof) of the purely periodic word to be mastered. Following Algorithm 1, M begins by pushing the first symbol of the input after . onto its stack, and then enters the loop spanning lines 5\u201314. This loop moves the stack head to the bottom of the stack and then moves it up symbol by symbol, predicting that the input will match the stack. Call each iteration of the loop spanning lines 5\u201314 a \u201cpass\u201d, and call a pass successful if\nmatched is true at line 14 and unsuccessful otherwise. Observe that if a pass is successful, then all of the guesses made during it (on line 10) are correct, and that if eventually there are no more unsuccessful passes, then M masters its input.\nNow take any purely periodic word \u03b1 = x\u03c9. To show that M masters \u03b1, we first show that every unsuccessful pass will eventually be followed by a successful pass. Observe that there must be at least one successful pass, since M begins the passes with only one symbol on the stack, and that symbol will eventually reappear in the input. So take any unsuccessful pass after the first successful pass. Now take the most recent successful pass prior to that unsuccessful pass. Let i be the position of the input head in x (counting from zero, so 0 \u2264 i < |x|) at the beginning of this most recent successful pass and let h be the height of the stack. Then the position of the input head in x after the successful pass is (i + h) mod |x|. Then after |x| \u2212 1 unsuccessful passes, the position of the input head in x will be (i + h|x|) mod |x| = i. So the next pass after that will be successful. Hence every unsuccessful pass will eventually be followed by a successful pass.\nSince each unsuccessful pass sets recovering to true, the next successful pass after it will break at line 14, causing M to push another symbol onto the stack. If the height of the stack never reaches |x|, then after some point, every pass is successful and M masters \u03b1. So say the height of the stack eventually reaches |x|. Then since the last pass before the stack reached that height was successful, and the input symbol following that pass is now at the top of the stack, the previous |x| symbols of the input match the stack. Then every subsequent pass will be successful, and M masters \u03b1. ut Theorem 4. Every infinite word mastered by a DSA predictor is multilinear.\nProof. Let M be a DSA predictor and let \u03b1 be any infinite word mastered by M . We will show that there is a DSA recognizer for Prefix(\u03b1), the set of all prefixes of \u03b1. Since M masters \u03b1, there is an n \u2265 1 such that for every i \u2265 n, M(\u03b1)[i] = \u03b1[i]. Take any such n. Let C = (q, s, i) be the configuration of M upon reaching position n of \u03b1, where q is the state of M , s is the stack, and i is the position of the stack head within s. Let M\u03b1 be a DSA recognizer which operates as follows. First M\u03b1 uses its finite control to check that the first n symbols of its input match the first n symbols of \u03b1. Then M\u03b1 uses its finite control to push s onto its stack and move its stack head to position i within s. Next M\u03b1 simulates M , starting from C. Whenever M would make a guess, M\u03b1 instead checks that the next symbol of the input matches M \u2019s guess. If any check fails, then M\u03b1 rejects its input; otherwise, when M\u03b1 reaches end-of-input, it accepts. Since all of M \u2019s guesses after n are correct, M\u03b1 now recognizes Prefix(\u03b1), and hence M\u03b1 determines \u03b1 in the sense of [21]. Then by Theorem 8 of [21], \u03b1 is multilinear. ut"}, {"heading": "3.4 Prediction by multi-DFAs", "text": "We next consider multi-DFA predictors. We leave their more powerful cousins, sensing multi-DFA predictors, to Section 4.\nAlgorithm 2 A 2-head DFA predictor which masters every ultimately periodic word. The heads are denoted by t and h. The input consists of the symbol . followed by an infinite word \u03b1. Wherever a guess is not specified, it may be taken to be arbitrary.\nmove h loop\nmove t move h, guessing \u03b1[t] move h if \u03b1[h] 6= \u03b1[t]\nTheorem 5. Let A be an alphabet. Then some multi-DFA predictor masters every ultimately periodic word over A.\nProof. We employ a variation of the \u201ctortoise and hare\u201d cycle detection algorithm [19], adapted to our setting. Let M be a 2-head DFA predictor which implements Algorithm 2. Take any ultimately periodic word \u03b1 = xy\u03c9. Following the algorithm, the two heads t (for \u201ctortoise\u201d) and h (for \u201chare\u201d) begin at the start of the input. M moves h one square to the right (making an arbitrary guess) and then enters the loop. In the loop, M guesses that h will match t. After each missed guess, h moves ahead an extra square (making an arbitrary guess), so the distance between the two heads increases by 1. If this distance stops growing, then there are no more missed guesses, so M masters \u03b1. Otherwise, both heads will reach the periodic part y\u03c9 of \u03b1 and the distance between them will reach a multiple of |y|. Then each head will point to the same position in y as the other, so all guesses will be correct from that point on. So again M masters \u03b1. ut"}, {"heading": "4 Prediction of multilinear words", "text": "We turn now to prediction of the class of multilinear words. We give an algorithm by which a sensing multi-DFA can master every multilinear word.\nTheorem 6. Let A be an alphabet. Then some sensing multi-DFA predictor masters every multilinear word over A.\nProof (Sketch). Let M be a sensing 10-head DFA predictor which implements Algorithm 3. The idea of the algorithm is as follows. Any properly multilinear word \u03b1 can be written as q \u220f n\u22651 m\u220f i\u22651 pis n i for some m \u2265 1 and strings q, pi, si subject to certain conditions. That is, \u03b1 can be broken into \u201cblocks\u201d, each block consisting of m \u201csegments\u201d of the form pis n i . To master \u03b1, M will alternate between two procedures, Correction and Matching. Correction attempts to position h1, h2, h3, and h4 so that each head is at the beginning of a segment, h2 is ahead of h1 by a given number of segments, h3 is ahead of h2 by the same number of segments, and h4 is ahead of h3 by the same number of segments. Each time Correction is entered, the given number of segments used to separate the\nheads is increased by one. Matching attempts to master \u03b1 on the assumption that Correction has successfully positioned h1, h2, h3, and h4 at the beginning of segments and that the number of segments separating the heads is a multiple of m (meaning that the segments share the same pi and si). If any problem is detected, Matching is exited and Correction is entered again.\nThe number of segments used to separate the heads is given by r \u2212 l. Before each call to Correction, r is moved forward, increasing this number by one. Correction works by first moving h1 forward to h4 and then calling AdvanceOne(1), which tries to move h1 to the beginning of the next segment. Then Correction moves h2 to h1 and calls AdvanceMany(2), which tries to move h2 forward by r\u2212 l segments. Correction then moves h3 to h2 and calls AdvanceMany(3), which tries to move h3 forward by r \u2212 l segments. Finally, Correction moves h4 to h3 and calls AdvanceMany(4), which tries to move h4 forward by r \u2212 l segments. If everything worked as intended, the four heads are now at the beginning of segments and each pair of heads hi and hi+1 are separated by the same number of segments, r \u2212 l.\nMatching works by using h1, h2, and h3 to predict h4. If the four heads are separated by the same number of segments, and if this number is a multiple of m, then the heads share the same pi and si. In this case, the later heads have extra copies of si: for some d \u2265 1, in each segment i, h4 will see d more copies of si than h3, which will see d more than h2, which will see d more than h1. Matching moves the heads together, using the earlier heads to predict h4 and detecting when each head passes its last copy of si by comparing the heads with each other. By use of a normal form for properly multilinear words, we guarantee that the first symbol of pi+1 differs from the first symbol of si, ensuring that the next segment can be detected. The supplemental head h3a is used to predict h4\u2019s last d copies of si by using h3\u2019s last d copies a second time. Once all heads are at the beginning of the next segment, Matching repeats from the start. If any guess is incorrect, then the heads were not separated by a multiple of m segments when Matching was entered. Upon making an incorrect guess, Matching exits, r \u2212 l is increased, and Correction is entered again.\nThe fact that M is sensing allows it to perform operations a designated number of times, a technique used in the procedures AdvanceMany and AdvanceOne called by Correction. This technique works in the following way. Let n be the distance between the heads l and r at a given point in the computation. To perform an operation n times, we first move another head, say inner, to r. Then we move l and r together until l reaches inner, performing the operation after each step. Now the operation has been performed n times, and we can repeat this process to perform it another n times. Further, by increasing the distance between l and r, we can increase n. It is also possible to nest this process, by moving another head, say outer, to r, keeping outer\u2019s position constant relative to l and r during the inner process, and moving l and r, but not outer, each time the inner process is completed. When l reaches outer, the inner process has been executed n times, each time performing its operation n times. In AdvanceMany and AdvanceOne, this technique is used to advance\na given hi by n segments, using within each segment a threshold based on n to detect the beginning of the next segment.\nAlgorithm 3 A sensing 10-head DFA predictor which masters every multilinear word. The heads are denoted by h1, h2, h3a, h3, h4, t, l, r, inner, and outer. The input consists of the symbol . followed by an infinite word \u03b1. Wherever a guess is not specified, it may be taken to be arbitrary.\nloop move r Correction Matching\nprocedure Matching loop\nmove h3a until h3a = h3\nwhile \u03b1[h1] = \u03b1[h2] = \u03b1[h3] = \u03b1[h4] do move h1, h2, h3a, h3 move h4, guessing \u03b1[h2]\nbreak unless \u03b1[h2] = \u03b1[h4]\nwhile \u03b1[h2] = \u03b1[h3] = \u03b1[h4] do move h2, h3 move h4, guessing \u03b1[h3]\nbreak unless \u03b1[h3] = \u03b1[h4]\nwhile \u03b1[h3a] = \u03b1[h3] = \u03b1[h4] do move h3a, h3 move h4, guessing \u03b1[h3a]\nbreak unless \u03b1[h3a] = \u03b1[h4]\nwhile h3a 6= h3 and \u03b1[h3a] = \u03b1[h4] do move h3a move h4, guessing \u03b1[h3a]\nbreak unless \u03b1[h3a] = \u03b1[h4]\nprocedure Correction move h1 until h1 = h4 AdvanceOne(1)\nmove h2 until h2 = h1 AdvanceMany(2)\nmove h3 until h3 = h2 AdvanceMany(3)\nmove h4 until h4 = h3 AdvanceMany(4)\nprocedure AdvanceMany(i) move outer until outer = r while l 6= outer do\nAdvanceOne(i) move l, r\nprocedure AdvanceOne(i) move t until t = hi move hi move inner until inner = r while l 6= inner do\nif \u03b1[t] = \u03b1[hi] then move l, r, outer else move inner until inner = r move hi move t move hi\nwhile \u03b1[t] = \u03b1[hi] do move t move hi, guessing \u03b1[t]\nTo show that M masters every multilinear word \u03b1, we first show that if either Matching or Correction gets \u201cstuck\u201d, i.e. is entered and does not end, then in its stuck state it will continue to make guesses, all of which are correct, and so M masters \u03b1. In particular, we show that the first while loop of AdvanceOne will\nalways end. This loop implements the \u201ctortoise and hare\u201d routine of Algorithm 2 on \u03b1, waiting for a streak of r \u2212 l consecutive matches. Such a streak will eventually be obtained, because if \u03b1 is ultimately periodic, then by the proof of Theorem 5, the \u201ctortoise and hare\u201d algorithm masters \u03b1, and if \u03b1 is properly multilinear, then we show that the \u201ctortoise and hare\u201d algorithm will eventually achieve k consecutive matches on \u03b1 for any k \u2265 1, and so the loop will end.\nSo we are left with the case in which Matching and Correction always end. Since r is moved at the beginning of each iteration of the main loop, and since Correction and Matching leave r\u2212 l unchanged, r\u2212 l will grow. If \u03b1 is ultimately periodic, then eventually r\u2212 l will be large enough for AdvanceOne to \u201cline up\u201d the heads hi and t with respect to the periodic part of \u03b1, so that M masters \u03b1. If \u03b1 is properly multilinear, then eventually r \u2212 l will be large enough for AdvanceOne to always advance hi by at least one segment. We show further that r \u2212 l will grow slowly enough with respect to the segment length that eventually whenever hi is at the beginning of a segment, AdvanceOne will move it to the beginning of the next segment and not farther. As a result, eventually Correction will always end with the four heads h1, h2, h3, and h4 at the beginning of segments, with the heads separated by r \u2212 l segments as desired. When r\u2212 l next reaches a multiple of m, the segments of the four heads will share the same pi and si. We show that then Matching can make use of h1, h2, and h3 to correctly predict h4 as intended. Thus M masters \u03b1. ut"}, {"heading": "5 Conclusion", "text": "In this paper, we studied the classic problem of sequence prediction from the angle of automata and infinite words. We examined several types of automata and sought to find out which classes of infinite words they could master. In doing so we described novel prediction algorithms for the classes of purely periodic, ultimately periodic, and multilinear words. Open questions in our investigation include whether there is a DSA predictor which masters every ultimately periodic word, and whether there is a multi-DFA predictor without sensing which masters every multilinear word. Other directions for further research would be to consider other types of automata as predictors, e.g. automata with two-way input tapes, and to attempt prediction of other classes of infinite words, e.g. morphic words. It would also be interesting to consider questions of computational tractability, e.g. how many guesses and how much time is required to achieve mastery.\nAcknowledgments. I would like to thank my Ph.D. advisor at Northeastern, Rajmohan Rajaraman, for his helpful comments and suggestions. The continuation of this work at Marne-la-Valle\u0301e was supported by the Agence Nationale de la Recherche (ANR) under the project EQINOCS (ANR-11-BS02-004)."}, {"heading": "10. Ginsburg, S., Greibach, S.A., Harrison, M.A.: One-way stack automata. J. ACM", "text": "14(2), 389\u2013418 (Apr 1967)"}, {"heading": "11. Gold, E.M.: Language identification in the limit. Information and Control", "text": "10(5), 447\u2013474 (1967), http://groups.lis.illinois.edu/amag/langev/paper/ gold67limit.html 12. Hibbard, B.: Adversarial sequence prediction. In: Proceedings of the 2008 Conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference. pp. 399\u2013403. IOS Press, Amsterdam, The Netherlands, The Netherlands (2008), http://dl.acm.org/citation.cfm?id=1566174.1566212"}, {"heading": "13. Holzer, M., Kutrib, M., Malcher, A.: Complexity of multi-head finite automata: Origins and directions. Theor. Comput. Sci. 412(1-2), 83\u201396 (Jan 2011)", "text": ""}, {"heading": "14. Hromkovic\u030c, J.: One-way multihead deterministic finite automata. Acta Informatica", "text": "19(4), 377\u2013384 (1983), http://dx.doi.org/10.1007/BF00290734"}, {"heading": "15. Johansen, P.: Inductive inference of ultimately periodic sequences. BIT Numerical", "text": "Mathematics 28(3), 573\u2013580 (1988), http://dx.doi.org/10.1007/BF01941135"}, {"heading": "16. Leblanc, B., Lutton, E., Allouche, J.P.: Inverse problems for finite automata:", "text": "A solution based on genetic algorithms. In: Hao, J.K., Lutton, E., Ronald, E., Schoenauer, M., Snyers, D. (eds.) Artificial Evolution, Lecture Notes in Computer Science, vol. 1363, pp. 157\u2013166. Springer Berlin Heidelberg (1998), http: //dx.doi.org/10.1007/BFb0026598 17. Legg, S.: Is there an elegant universal theory of prediction? In: Algorithmic Learning Theory, 17th International Conference, ALT 2006, Barcelona, Spain, October 7-10, 2006, Proceedings. pp. 274\u2013287 (2006), http://dx.doi.org/10.1007/ 11894841_23"}, {"heading": "18. O\u2019Connor, M.G.: An unpredictability approach to finite-state randomness. J.", "text": "Comput. Syst. Sci. 37(3), 324\u2013336 (Dec 1988), http://dx.doi.org/10.1016/ 0022-0000(88)90011-6 19. Sedgewick, R., Szymanski, T.G., Yao, A.C.: The complexity of finding cycles in periodic functions. SIAM Journal on Computing 11(2), 376\u2013390 (1982) 20. Shubert, B.: Games of prediction of periodic sequences. Tech. rep., United States Naval Postgraduate School (1971) 21. Smith, T.: On Infinite Words Determined by Stack Automata. In: FSTTCS 2013. Leibniz International Proceedings in Informatics (LIPIcs), vol. 24, pp. 413\u2013424. Schloss Dagstuhl\u2013Leibniz-Zentrum fuer Informatik, Dagstuhl, Germany (2013) 22. Solomonoff, R.: A formal theory of inductive inference. part i. Information and Control 7(1), 1 \u2013 22 (1964), http://www.sciencedirect.com/science/article/ pii/S0019995864902232 23. Wagner, K., Wechsung, G.: Computational Complexity. Mathematics and its Applications, Springer (1986)"}, {"heading": "A Prediction by DPDAs", "text": "In this appendix, we give a full proof of Theorem 2, filling out the sketch given in the body. This theorem states that no DPDA predictor masters every purely periodic word. We start with a lemma.\nLemma 1. Take any integer n \u2265 1 and let L = m1, . . . ,mn be a list of integers such that for all 1 \u2264 i < n, |mi\u2212mi+1| \u2264 1. Let d = mn\u2212m1. Take any integer k \u2265 1. Suppose n \u2265 (2k\u2212d)k2k\u2212d. Then there are integers 1 \u2264 p1 < \u00b7 \u00b7 \u00b7 < pk \u2264 n such that for each pi, for all j such that pi \u2264 j \u2264 pk, mj \u2265 mpi .\nProof. Suppose there are 1 \u2264 a < b \u2264 n such that mb \u2212ma \u2265 k \u2212 1. Then for 1 \u2264 i \u2264 k, set pi to the highest j such that j \u2264 b and mj = ma + i\u2212 1. Then we are done.\nSo say there are no such a, b. Then we have d < k and mn\u2212k < mi < m1 +k for all mi. Then there are at most (m1+k)\u2212(mn\u2212k) = 2k\u2212d distinct values in L. Then some value appears in L at least n2k\u2212d \u2265 k2k\u2212d times. For any integer j, let |L|j be the number of occurrences of j in L. Take the lowest mn\u2212k < j < m1+k such that |L|j \u2265 kj+k\u2212mn . If j = mn \u2212 k + 1 then j is the lowest value in L and appears at least k times, so choose pi from those appearances and we are done. Otherwise, |L|j\u22121 < kj\u22121+k\u2212mn , so |L|j \u2265 k|L|j\u22121. Then there are k appearances of j in L uninterrupted by j\u22121, so choose pi from those appearances and we are done. ut\nTheorem 2. Let A be an alphabet such that |A| \u2265 2. Then no DPDA predictor masters every purely periodic word over A.\nProof. Let M = (Q,A, F, T, .,M, qs) be a DPDA predictor. Suppose M masters every purely periodic word over A. Let k = |Q| \u00b7 |F | + 1 and let p = (3k)k3k. Take any a, b \u2208 A such that a 6= b. Let \u03b1 be the purely periodic word (apb)\u03c9. Then there is some position m \u2265 0 after which all of M \u2019s guesses about \u03b1 are correct.\nNow, if the stack height increased by more than |Q|\u00b7|F | at one input position, there would be two configurations C1 and C2 of M at that position with the same state and stack symbol, with C1 occurring prior to C2, such that the stack below the top symbol at C1 is not accessed between C1 and C2. Then M would loop and never reach the next input position. So the most that the stack height can increase at one position is |Q| \u00b7 |F |.\nLet the stack difference of a segment of p consecutive a\u2019s be the height of the stack at the end of the segment minus the height of the stack at the beginning of the segment. Because there is only one symbol between each two segments (a single b), the stack height can increase by at most |Q| \u00b7 |F | between segments. Then there must be a segment of p consecutive a\u2019s starting after position m with a stack difference of at least \u2212|Q| \u00b7 |F |, since otherwise the stack height after m would eventually become negative.\nSo take any segment of p consecutive a\u2019s starting after m with a stack difference d \u2265 \u2212|Q| \u00b7 |F |. Let C1, . . . , Cn be the successive configurations of M during this segment, where each configuration Ci has the form (qi, si), with qi being the current state and si the current stack. We have k \u2265 \u2212d and n \u2265 p. Hence n \u2265 (3k)k3k \u2265 (2k \u2212 d)k2k\u2212d. Then by Lemma 1 there is a list P of integers 1 \u2264 p1 < \u00b7 \u00b7 \u00b7 < pk \u2264 n such that for each pi, for all j such that pi \u2264 j \u2264 pk, |sj | \u2265 |spi |. So since k > |Q| \u00b7 |F |, two of the P -indexed configurations Ci and Cj have the same state and stack symbol, with i < j. If Ci and Cj occurred at the same input position, then since the stack below the top symbol at Ci is not accessed between Ci and Cj , M would loop and never reach the next input position. So Ci and Cj occur at distinct input positions i1 < i2 within the segment of p consecutive a\u2019s.\nNow, all of the input symbols from i1 to i2 are a\u2019s. Therefore as long as M continues to read a\u2019s it will repeat the computation between i1 and i2, since the stack below the top symbol at i1 is not accessed between i1 and i2. So since all of M \u2019s guesses from i1 to i2 are a\u2019s, M will continue to guess a\u2019s for as long as it continues to read a\u2019s. But then M will guess a for the b at the end of the segment, contradicting the supposition that all of M \u2019s guesses about \u03b1 after m are correct. Therefore M does not master every purely periodic word over A. ut"}, {"heading": "B Prediction of multilinear words", "text": "In this appendix, we give a full proof of Theorem 6, filling out the sketch given in the body. This theorem states that some sensing multi-DFA predictor masters every multilinear word. We begin by providing a normal form for properly multilinear words, together with some definitions to be used in the proofs. Then we prove a result about the behavior of Algorithm 2 (\u201ctortoise and hare\u201d) when applied to multilinear words. With this groundwork laid, we show that by implementing Algorithm 3, a sensing multi-DFA can master every multilinear word.\nB.1 Normal form for properly multilinear words\nIn the theorem below we give a convenient form for properly multilinear words, resembling Proposition 32 of [8], but with a tighter constraint.\nTheorem 7. Let \u03b1 be a properly multilinear word. Then \u03b1 can be written as\nq \u220f\nn\u22651\nm\u220f i\u22651 pis n i\nfor some m \u2265 1, string q, and strings pi and si such that\n\u2013 for every i from 1 to m, pi 6= \u03bb and si 6= \u03bb, \u2013 for every i from 1 to m\u2212 1, si[1] 6= pi+1[1], and \u2013 sm[1] 6= p1[1].\nProof. By Theorem 15 of [21], \u03b1 can be written as\nq \u220f\nn\u22650 ra1n+b11 r a2n+b2 2 \u00b7 \u00b7 \u00b7 ramn+bmm\nfor some m \u2265 1, string q, non-empty strings ri, and nonnegative integers ai, bi where ai + bi > 0, such that\n\u2013 for every i from 1 to m, bi \u2265 1, \u2013 for every i from 1 to m\u2212 1, ri[1] 6= ri+1[1], and \u2013 if m \u2265 2, r1[1] 6= rm[1].\nWe transform this form into the desired one in four steps. Following [21], we view each rain+bii as a triple [ri, ai, bi]. First, rotate the terms as described in Section 5 of [21] until am is greater than 0. Second, split every triple [r, a, b] such that a > 0 into two triples [rb, 0, 1] and [ra, 1, 0]. Third, replace every triple [r, 0, b] with [rb, 0, 1]. Fourth, merge all adjacent triples [r, 0, 1],[t, 0, 1] into [rt, 0, 1] repeatedly until there are no more such adjacent triples. It is readily verified that the resulting list of triples consists of pairs [p, 0, 1],[s, 1, 0] subject to the desired constraints. ut\nDefinitions for properly multilinear words We now have that any properly multilinear word can be written as\nq \u220f\nn\u22651\nm\u220f\ni=1\npis n i\nsubject to the conditions of Theorem 7. In the context of a properly multilinear word \u03b1 written subject to those conditions, we make the following definitions. Strings pi and si are already defined for 1 \u2264 i \u2264 m. Let \u03c1 = max{|pi| | 1 \u2264 i \u2264 m}. Let \u03c3 = max{|si| | 1 \u2264 i \u2264 m}. For each n > m, let pn = p((n\u22121) mod m)+1\nand sn = s((n\u22121) mod m)+1. For each n \u2265 1, let blockn = m\u220f i=1 pis n i and segn = pns d nm e n . We have \u03b1 = q\n\u220f n\u22651 blockn = q \u220f n\u22651 segn. For j, k \u2265 1, we say that position\nj of \u03b1 occurs in block k of \u03b1, and write block(j) = k, iff |q k\u22121\u220f n=1 blockn| < j \u2264 |q k\u220f\nn=1 blockn|. (For j \u2264 |q|, we say that position j does not occur in any block, and block(j) is undefined.) For j, k \u2265 1, we say that position j of \u03b1 occurs in segment k of \u03b1, and write seg(j) = k, iff |q\nk\u22121\u220f n=1 segn| < j \u2264 |q k\u220f n=1 segn|. (For\nj \u2264 |q|, we say that position j does not occur in any segment, and seg(j) is undefined.) Notice that for all i > |q|, block(i) = d seg(i)m e.\nB.2 \u201cTortoise and hare\u201d applied to multilinear words\nIn this subsection we show that if a multi-DFA predictor M implements Algorithm 2 on a multilinear word, then for every k \u2265 1, M will at some point make k consecutive correct guesses. We will make use of this result in the next subsection in proving that there is a sensing multi-DFA predictor which masters every multilinear word. We start with some lemmas.\nLemma 2. Let M be a multi-DFA predictor implementing Algorithm 2 on a properly multilinear word \u03b1. Write \u03b1 in the form of Theorem 7. Let b = 2\u03c1+\u03c32. Suppose that while h is in a segment jh and t is in a segment jt such that jh mod m = jt mod m, h moves b symbols. Then h and t will agree afterward until h leaves jh or t leaves jt, and if one leaves before the other, then at that point they will disagree.\nProof. Consider the point at which h begins to move the b symbols. Since jh mod m = jt mod m, for some 1 \u2264 j \u2264 m, segment jh has the form pjsblock(h)j and segment jt has the form pjs block(t) j . After h moves 2|pj | symbols, both heads are past pj , so each head is inside some occurrence of sj . Let 1 \u2264 dt \u2264 |sj | be the position of t within its occurrence of sj and let 1 \u2264 dh \u2264 |sj | be the position of h within its occurrence of sj . Let d = (dt \u2212 dh) mod |sj |; d indicates how many times dh must be incremented with respect to dt before dh mod |sj | = dt mod |sj |, at which point we say h and t have \u201clined up\u201d with respect to sj . Since h is moved an extra symbol with respect to t for each missed guess, if the two heads mismatch d more times, they will be lined up. So after h moves another |sj |2 symbols (making at most b symbols in total), if the heads are not lined up, there were less than d mismatches, hence at most |sj | \u2212 2 mismatches. Hence there were at least |sj |2 \u2212 (|sj | \u2212 2) = |sj |(|sj | \u2212 1) + 2 guesses. Then by the pigeonhole principle, M must have made |sj | consecutive correct guesses. So the heads are lined up or else M has made |sj | consecutive correct guesses. Either way, since the same |sj | symbols will keep repeating under the two heads, h and\nt will now agree until h leaves jh or t leaves jt . If one leaves before the other, then at that point they will disagree, since sj [1] 6= pj+1[1]. ut\nLemma 3. Let M be a multi-DFA predictor implementing Algorithm 2 on a properly multilinear word \u03b1. Write \u03b1 in the form of Theorem 7. Suppose for some k \u2265 1, M never gets k consecutive guesses correct. Then for every d, there is some point after which always seg(h)\u2212 seg(t) \u2265 d.\nProof. Let p = m\u2211 i=1 |pi| and s = m\u2211 i=1\n|si|. For each n \u2265 1, let sumblock1to(n) = n\u2211 i=1 |block i|. We have for all n \u2265 1, sumblock1to(n) = n\u2211 i=1 |block i| = n\u2211 i=1 p + is = np+ ns(n+1)2 . Now, since M never gets k consecutive guesses correct, and since h moves an extra symbol ahead of t on each missed guess, we have always h \u2265 t+ tk \u2212 2 \u2265 t(1 + 1k )\u2212 2. Now take any b \u2265 1. Eventually t will pass block 3bk. Consider any point after that. There are n \u2265 3bk and 1 \u2264 c \u2264 |blockn+1| such that t is on the cth symbol of block n+ 1. So t = |q|+ sumblock1to(n) + c and h \u2265 (|q|+ sumblock1to(n) + c)(1 + 1k )\u2212 2. We have\n(1 + 1\nk ) sumblock1to(n) = (1 +\n1 k )(np+ ns(n+ 1) 2 )\n= (1 + 1\nk )np+ (1 +\n1 k )n(n+ 1) s 2\n= (1 + 1\nk )np+ ((1 +\n1 k )n2 + (1 + 1 k )n) s 2\n= p(n+ n\nk ) + (n2 +\nn2\nk + n+\nn k ) s 2\n\u2265 p(n+ 3b) + (n2 + n+ 3bn+ 3b)s 2 > 2bp+ p(n+ b) + (n2 + n+ 2bn+ bn+ b) s\n2\n> 1 + p(n+ b) + (n2 + n+ 2bn+ b2 + b) s\n2\n= 1 + p(n+ b) + (n+ b)(n+ b+ 1) s 2 = 1 + sumblock1to(n+ b),\ngiving us (1 + 1k ) sumblock1to(n) > sumblock1to(n+ b) + 1. Then we have\nh \u2265 (|q|+ sumblock1to(n) + c)(1 + 1 k )\u2212 2\n= |q|(1 + 1 k ) + (1 + 1 k ) sumblock1to(n) + c(1 + 1 k )\u2212 2 > |q|(1 + 1 k ) + sumblock1to(n+ b) + 1 + c(1 + 1 k )\u2212 2 > |q|+ sumblock1to(n+ b),\ngiving us h > |q| + sumblock1to(n + b). Therefore block(h) \u2265 n + b + 1, so since block(t) = n + 1, we have block(h) \u2212 block(t) \u2265 b. So for every b, there is some point after which always block(h) \u2212 block(t) \u2265 b. So now take any d. Let b = d\u22121m + 1. As shown above, there is some point after which always block(h)\u2212 block(t) \u2265 b. From that point onward, from the fact that each block contains exactly m segments, we have always seg(h)\u2212 seg(t) \u2265 m(b\u22121)+ 1 \u2265 d, which was to be shown. ut\nLemma 4. Let M be a multi-DFA predictor implementing Algorithm 2 on a properly multilinear word \u03b1. Write \u03b1 in the form of Theorem 7. Suppose for some k \u2265 1, M never gets k consecutive guesses correct. Then for every n \u2265 1, there are segments jh, jt \u2265 n of \u03b1 such that jh mod m = jt mod m, t enters jt before h enters jh, and h leaves jh before t leaves jt.\nProof. Take any n \u2265 1. Take any segments jh \u2032, jt \u2032 \u2265 n such that at some point, h is in jh \u2032 and t is in jt \u2032. Take any d > jh \u2032 \u2212 jt \u2032 such that d mod m = 0. By Lemma 3, there is some point after which always seg(h) \u2212 seg(t) \u2265 d. So there is a last point at which seg(h)\u2212 seg(t) < d. At this point, h is in some segment jh \u2032\u2032 and t is in some segment jt such that jh \u2032\u2032 \u2212 jt = d \u2212 1 and jh \u2032\u2032, jt \u2265 n. If t leaves jt before h leaves jh \u2032\u2032, then seg(h) \u2212 seg(t) would still be less than d, a contradiction. So h leaves jh \u2032\u2032 and enters jh \u2032\u2032 + 1 before t leaves jt . Now seg(h)\u2212seg(t) = d. Now if t leaves jt before h leaves jh \u2032\u2032+1, then seg(h)\u2212seg(t) would again be less than d, a contradiction. So h leaves jh \u2032\u2032+1 before t leaves jt . Letting jh = jh \u2032\u2032 + 1, we therefore have that t enters jt before h enters jh, and h leaves jh before t leaves jt . Further, we have jh, jt \u2265 n, and since jh \u2212 jt = d and d mod m = 0, jh mod m = jt mod m, completing the proof. ut\nTheorem 8. Let M be a multi-DFA predictor implementing Algorithm 2 on a multilinear word \u03b1. Then for every k \u2265 1, M will at some point make k consecutive correct guesses.\nProof. If \u03b1 is ultimately periodic, then by the proof of Theorem 5, M masters \u03b1, so the statement holds. So say \u03b1 is properly multilinear. Write \u03b1 in the form of Theorem 7. Suppose for contradiction that for some k \u2265 1, M never gets k consecutive guesses correct. Let b = 2\u03c1+ \u03c32. There is some n \u2265 1 such that for every n\u2032 \u2265 n, |segn\u2032 | \u2265 b + k. Then by Lemma 4, there are segments jh, jt \u2265 n such that jh mod m = jt mod m, t enters jt before h enters jh, and h leaves jh before t leaves jt . So t is in jt for the whole time that h is in jh. Then by Lemma 2, once h has moved b symbols into jh, h and t will agree until h reaches the beginning of segment jh + 1. Since |seg jh | \u2265 b + k, M therefore makes k consecutive correct guesses, contradicting the supposition that M never does so. So for every k \u2265 1, M will at some point make k consecutive correct guesses. ut\nB.3 Prediction of multilinear words by sensing multi-DFAs\nWe now give a full proof of Theorem 6, filling out the sketch given in the body. We prove lemmas about the matching and correction procedures, and then prove\nthe main result. Algorithm 3 calls upon four procedures: Matching, Correction, AdvanceMany, and AdvanceOne. (The procedure AdvanceOne takes a parameter i \u2208 {1, 2, 3, 4}, and so is really four separate procedures; likewise for AdvanceMany.) All of the procedures have access to all of the heads of the predicting automaton. Below we prove lemmas about the behavior of these procedures when they are entered in certain \u201cready\u201d configurations. Let M be a sensing multi-DFA predictor with heads h1, h2, h3, h3a, h4, t, l, r, inner, and outer, and let \u03b1 be an infinite word. We say that M is in a Matching-ready configuration on \u03b1 if its heads are positioned on \u03b1 such that h1 \u2264 h2 \u2264 h3 \u2264 h4 and h3a \u2264 h3. For each 1 \u2264 i \u2264 4, we say that M is in an Advance(i)-ready configuration on \u03b1 if its heads are positioned on \u03b1 such that t \u2264 hi, l \u2264 r, inner \u2264 r, and outer \u2264 r. We say that M is in a Correction-ready configuration on \u03b1 if M is in an Advance(4)-ready configuration on \u03b1 and h1 \u2264 h2 \u2264 h3 \u2264 h4.\nMatching procedure We prove two lemmas about the matching procedure Matching.\nLemma 5. Let M be a sensing multi-DFA predictor in a Matching-ready configuration on an infinite word \u03b1. If M enters Matching and Matching does not end, then M masters \u03b1.\nProof. Matching consists of an outer loop and four inner loops. If the first inner loop does not end, then h1, h2, h3, and h4 match, so guessing that h4 matches h2 is correct, and M masters \u03b1. If the second loop is entered and does not end, then h2, h3, and h4 all match, so guessing that h4 matches h3 is correct, and Mmasters \u03b1. If the third loop does not end, then h3a, h3, and h4 all match, so guessing that h4 matches h3a is correct, and M masters \u03b1. If the fourth loop does not end, then h3a and h4 match, so guessing that h4 matches h3a is correct, and M masters \u03b1. So say the four inner loops always end. Now, each time the body of an inner loop is entered, at least one guess is made, and if any guess is missed in an inner loop, the outer loop ends immediately thereafter. So if the outer loop does not end and M does not master \u03b1, then at some point M ceases entering the bodies of the inner loops. After that point, if the outer loop does not end immediately after skipping the first inner loop, then h2 and h4 match. Next, the second inner loop is skipped, so h2, h3, and h4 do not all match, hence h3 is different from h2 and h4. But then the outer loop ends immediately after the second inner loop. So Matching ends or M masters \u03b1. ut\nLemma 6. Let M be a sensing multi-DFA predictor in a Matching-ready configuration on a properly multilinear word \u03b1. Write \u03b1 in the form of Theorem 7. Suppose that h1, h2, h3, and h4 are all at the beginning of segments, and for some d \u2265 1, seg(h2) \u2212 seg(h1) = seg(h3) \u2212 seg(h2) = seg(h4) \u2212 seg(h3) = dm. Then if M enters Matching, M masters \u03b1.\nProof. We have that for some i, j \u2265 1, for each k \u2208 {1, 2, 3, 4}, hk is at the beginning of a string of the form pjs i+d(k\u22121) j pj+1. Recall that from Theorem 7,\nsj [1] 6= pj+1[1]. In Matching, M first moves h3a to the same position as h3. We depict the positions of the heads below. By h s we mean that head h is at the first symbol of string s.\n\u00b7 \u00b7 \u00b7h1 pj sij pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7h2 pj sijsdj pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7h3a h3 pj sijsdjsdj pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7h4 pj sijsdjsdjsdj pj+1 \u00b7 \u00b7 \u00b7\nFollowing Matching, M moves the heads until they disagree, which will happen after |pjsij | symbols, when h1 reaches pj+1. In doing so M guesses h2 for h4, and since h2 and h4 do not disagree, all of the guesses will be correct.\n\u00b7 \u00b7 \u00b7 pj sij h1 pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 pj sij h2 sdj pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 pj sij h3a h3 sdjsdj pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 pj sij h4 sdjsdjsdj pj+1 \u00b7 \u00b7 \u00b7\nNext, M moves h2, h3, and h4 together until they disagree, which will happen after |sdj | symbols, when h2 reaches pj+1. In doing so M guesses h3 for h4, and since h3 and h4 do not disagree, all of the guesses will be correct.\n\u00b7 \u00b7 \u00b7 pj sij h1 pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 pj sijsdj h2 pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 pj sij h3a sdj h3 sdj pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 pj sijsdj h4 sdjsdj pj+1 \u00b7 \u00b7 \u00b7\nNext, M moves h3a, h3, and h4 together until they disagree, which will happen after |sdj | symbols, when h3 reaches pj+1. In doing so M guesses h3a for h4, and since h3a and h4 do not disagree, all of the guesses will be correct.\n\u00b7 \u00b7 \u00b7 pj sij h1 pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 pj sijsdj h2 pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 pj sijsdj h3a sdj h3 pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 pj sijsdjsdj h4 sdj pj+1 \u00b7 \u00b7 \u00b7\nFinally, M moves h3a and h4 together until h3a reaches h3 or h3a and h4 disagree. (Here M uses its sensing ability to detect coincidence of h3a and h3.) Since h3a\nand h4 agree for the next |sdj | symbols, h3a will reach h3.\n\u00b7 \u00b7 \u00b7 pj sij h1 pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 pj sijsdj h2 pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 pj sijsdjsdj h3a h3 pj+1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 pj sijsdjsdjsdj h4 pj+1 \u00b7 \u00b7 \u00b7\nNow all of the heads are at pj+1, and the above process will repeat. Because no guesses were missed during this process, Matching will run perpetually without missing another guess, and so M masters \u03b1. ut\nCorrection procedure The correction procedure consists of Correction and its helper procedures AdvanceOne and AdvanceMany. We give lemmas for these procedures first for ultimately periodic words, and then for properly multilinear words.\nLemmas for the correction procedure (ultimately periodic case)\nLemma 7. Let M be a sensing multi-DFA predictor in an Advance(i)-ready configuration on an ultimately periodic word \u03b1 for some 1 \u2264 i \u2264 4. Write \u03b1 as ps\u03c9 for strings p, s. If M enters AdvanceOne(i) and AdvanceOne(i) does not end, then M masters \u03b1. Further, if r \u2212 l \u2265 |ps| when AdvanceOne(i) is entered, then M masters \u03b1.\nProof. When AdvanceOne is entered, it moves t until t reaches hi. At this point, t and hi are at the beginning of an infinite word \u03b2, where \u03b1 = \u03b1[1..t]\u03b2. Clearly the ultimately periodic words are closed under shifts, so \u03b2 is ultimately periodic. AdvanceOne then implements the \u201ctortoise and hare\u201d routine of Algorithm 2 on \u03b2, waiting for a streak of r\u2212 l consecutive matches of t and hi. By the proof of Theorem 5, this algorithm masters every ultimately periodic word, so such a streak will eventually be obtained. Finally, AdvanceOne moves t and hi together until they mismatch. If this happens, AdvanceOne ends; if this never happens, then all of the guesses during this loop will be correct, so M masters \u03b1. If r \u2212 l \u2265 |ps|, the last |s| guesses in the streak of r \u2212 l consecutive correct guesses were made while both heads were past p. The last |s| symbols of the streak will therefore keep repeating under both heads. So the two heads will continue to agree, and M masters \u03b1. ut Lemma 8. Let M be a sensing multi-DFA predictor in an Advance(i)-ready configuration on an ultimately periodic word \u03b1 for some 1 \u2264 i \u2264 4. If M enters AdvanceMany(i) and AdvanceMany(i) does not end, then M masters \u03b1.\nProof. AdvanceMany first moves outer until outer = r, and then repeatedly calls AdvanceOne on hi and moves l and r together. On each call to AdvanceOne, by Lemma 7, AdvanceOne will end, or M masters \u03b1. So if M does not master \u03b1, then after r \u2212 l iterations of the loop, l will catch up with outer, and AdvanceMany will end. ut\nLemma 9. Let M be a sensing multi-DFA predictor in a Correction-ready configuration on an ultimately periodic word \u03b1. Write \u03b1 as ps\u03c9 for strings p, s. If M enters Correction and Correction does not end, then M masters \u03b1. Further, if r \u2212 l \u2265 |ps| when Correction is entered, then M masters \u03b1.\nProof. By Lemmas 7 and 8, each call to AdvanceOne and AdvanceMany will end, or M masters \u03b1. So Correction will end, or M masters \u03b1. If r\u2212 l \u2265 |ps| when Correction is entered, then r\u2212 l \u2265 |ps| when AdvanceOne is entered, so by Lemma 7, M masters \u03b1. ut\nLemmas for the correction procedure (properly multilinear case)\nLemma 10. Let M be a sensing multi-DFA predictor in an Advance(i)-ready configuration on a properly multilinear word \u03b1 for some 1 \u2264 i \u2264 4. If M enters AdvanceOne(i), AdvanceOne(i) will end, and it will move hi at least once.\nProof. When AdvanceOne is entered, it moves t until t reaches hi. At this point, t and hi are at the beginning of an infinite word \u03b2, where \u03b1 = \u03b1[1..t]\u03b2. Clearly the properly multilinear words are closed under shifts, so \u03b2 is properly multilinear. AdvanceOne then implements the \u201ctortoise and hare\u201d routine of Algorithm 2 on \u03b2, waiting for a streak of r \u2212 l consecutive matches of t and hi. By Theorem 8, such a streak will eventually be obtained. Finally, AdvanceOne moves t and hi together until they mismatch, which must eventually happen, since \u03b2 is not ultimately periodic. So AdvanceOne will end, and clearly it will have moved hi at least once. ut\nLemma 11. Let M be a sensing multi-DFA predictor in an Advance(i)-ready configuration on a properly multilinear word \u03b1 for some 1 \u2264 i \u2264 4. Write \u03b1 in the form of Theorem 7. Suppose \u03c1+ 2\u03c3 \u2264 r\u2212 l. Then if M enters AdvanceOne(i) with hi in some segment, AdvanceOne(i) will end with hi in a subsequent segment.\nProof. Let h = hi. When AdvanceOne is entered, h is in some segment j, so h is at the beginning of a string of the form wsnj pj+1, where |w| \u2264 max(|pj |, |sj |) and 0 \u2264 n \u2264 block(h). Suppose AdvanceOne ends before h reaches pj+1. Since the required streak is r\u2212 l, h and t must each have moved at least r\u2212 l symbols. Then since r \u2212 l \u2265 |pj | + 2|sj |, we have n \u2265 2, and h and t are both in the snj part of segment j, past the first sj . Let c be the position of t within sj and let d be the position of h within sj . t and h agreed on the last |sj | symbols, so when t was last at position c within sj , h was at position d within sj , and t and h agreed on those positions. But then sj [c] = sj [d], so t and h agree now, a contradiction, since they must disagree for AdvanceOne to end. Therefore AdvanceOne will not end before h reaches pj+1. But by Lemma 10, AdvanceOne will end. So AdvanceOne will end with h in a subsequent segment. ut\nLemma 12. Let M be a sensing multi-DFA predictor in an Advance(i)-ready configuration on a properly multilinear word \u03b1 for some 1 \u2264 i \u2264 4. Write \u03b1 in\nthe form of Theorem 7. Suppose that M enters AdvanceOne(i) with hi at the beginning of some segment j, and that \u03c1 + 2\u03c3 \u2264 r \u2212 l \u2264 |segj | \u2212 2\u03c1 \u2212 \u03c32. Then AdvanceOne(i) will end with hi at the beginning of segment j + 1.\nProof. Let h = hi. By Lemma 11, AdvanceOne will not end before h reaches segment j + 1. Now when AdvanceOne is entered, it moves t until t reaches h and then implements the \u201ctortoise and hare\u201d routine of Algorithm 2, waiting for a streak of r \u2212 l consecutive matches of t and h. Let b = 2\u03c1 + \u03c32. Then by Lemma 2, once h has moved b symbols into segment j, h and t will agree until h reaches the beginning of segment j + 1, at which point they will disagree. So since |segj | \u2265 b+ r\u2212 l, h and t will achieve a streak of r\u2212 l consecutive matches while in segment j. Then AdvanceOne will enter the second while loop and move t and h together until they mismatch, which will happen when h reaches the beginning of segment j + 1. ut\nLemma 13. Let M be a sensing multi-DFA predictor in an Advance(i)-ready configuration on a properly multilinear word \u03b1 for some 1 \u2264 i \u2264 4. Write \u03b1 in the form of Theorem 7. Suppose that M enters AdvanceOne(i) with hi in some segment j, and that 4(\u03c1 + \u03c3) \u2264 r \u2212 l \u2264 segj+1 \u2212 \u03c32 \u2212 4(\u03c1 + \u03c3). Then AdvanceOne(i) will end with hi at the beginning of segment j+ 1 or the beginning of segment j + 2.\nProof. Let h = hi. When AdvanceOne is entered, it moves t until t reaches h and then implements the \u201ctortoise and hare\u201d routine of Algorithm 2, waiting for a streak of r\u2212 l consecutive matches of t and h. Initially, h is at the beginning of a string of the form wsn \u2032 j pj+1s n\u2032\u2032 j+1pj+2 where |w| \u2264 max(|pj |, |sj |), n\u2032 \u2265 0, and n\u2032\u2032 = block(h) or block(h)+1. By Lemma 11, AdvanceOne will not end with h in segment j. So h will reach the beginning of pj+1. If AdvanceOne ends now, then the lemma is satisfied. So say AdvanceOne does not end at this point.\nThen consider the situation with h at the beginning of the string pj+1s n\u2032\u2032 j+1pj+2.\nWe have h\u2212 t \u2264 |w|+ |sj |, since if t has not reached sn \u2032 j , then h\u2212 t \u2264 |w|, and if t reached sn \u2032 j , then h was at most |w| ahead of it, and with both of them in sn \u2032 j , they could separate by at most another |sj | before reaching identical positions in sj , after which they would not separate further. Now let s be the current streak. Suppose s > |pj |+ 2|sj |. Then since t has moved at least s symbols, t is in sn \u2032 j , past the first sj . Let c be the position of t within sj . t and h agreed on the last |sj | symbols, so when t was last at position c within sj , h was at position 1 within sj , since now h is at a position following the last position of sj . t and h agreed on those positions, so sj [c] = sj [1]. But since the streak was not reset when h reached pj+1, t and h are still in agreement, so sj [c] = pj+1[1], giving sj [1] = pj+1[1], a contradiction. So s \u2264 |pj |+ 2|sj |.\nNow, t is at most |w| + |sj | symbols behind h, and therefore at most |w| + |sj |+ |pj+1| symbols behind the start of sn \u2032\u2032 j+1. t will reach the start of the second sj+1, since at that point the streak is at most |pj |+ 2|sj |+ |w|+ |sj |+ 2|pj+1|, which is less than r \u2212 l. Then the procedure will not end before h reaches pj+2, since if it did, t and h would disagree while both in sn \u2032\u2032 j+1, after an |sj+1| streak\nwith both in sn \u2032\u2032 j+1, which is impossible. So given that AdvanceOne did not end with h at the beginning of pj+1, h will reach the beginning of pj+2. Now when h reached pj+1, t was at most |w|+ |sj | symbols behind h, so when t reaches pj+1, t is at most 2(|w|+ |sj |) symbols behind h. Let b = 2\u03c1+\u03c32. Then the number of symbols remaining ahead of h in segment j + 1 is at least\n|segj+1| \u2212 2(|w|+ |sj |) \u2265 r \u2212 l + \u03c32 + 4(\u03c1+ \u03c3)\u2212 2(|w|+ |sj |) = r \u2212 l + b+ 2\u03c1+ 4\u03c3 \u2212 2(|w|+ |sj |) \u2265 r \u2212 l + b.\nSo by Lemma 2, once h has moved another b symbols, h and t will agree until h reaches pj+2, at which point they will disagree. So h and t will achieve a streak of r\u2212 l consecutive correct guesses while in segment j + 1. Then AdvanceOne will enter the second while loop and move t and h together until they mismatch, which happens when h reaches the beginning of segment j + 2. ut\nLemma 14. Let M be a sensing multi-DFA predictor in an Advance(i)-ready configuration on a properly multilinear word \u03b1 for some 1 \u2264 i \u2264 4. If M enters AdvanceMany(i), then AdvanceMany(i) will end, and it will move hi at least once.\nProof. AdvanceMany first moves outer until outer = r, and then repeatedly calls AdvanceOne(i) and moves l and r together. Since r\u2212 l \u2265 1, there will be at least one call to AdvanceOne. On each call to AdvanceOne, by Lemma 10, AdvanceOne will end, and it will move hi at least once. So after r\u2212 l iterations of the loop, l will catch up with outer, AdvanceMany will end, and it will have moved hi at least once. ut\nLemma 15. Let M be a sensing multi-DFA predictor in an Advance(i)-ready configuration on a properly multilinear word \u03b1 for some 1 \u2264 i \u2264 4. Write \u03b1 in the form of Theorem 7. Suppose \u03c1+2\u03c3 \u2264 r\u2212l. Then if M enters AdvanceMany(i) with hi in some segment j, AdvanceMany(i) will end with seg(hi) \u2265 j+ r\u2212 l.\nProof. AdvanceMany first moves outer until outer = r, and then repeatedly calls AdvanceOne(i) and moves l and r together. On the first call to AdvanceOne, by Lemma 11, hi will be advanced from its current segment to some subsequent segment. Since AdvanceOne leaves r\u2212 l unchanged, the same will be true for each subsequent call to AdvanceOne. So after r \u2212 l iterations of the loop, l will catch up with outer and AdvanceMany will end with seg(hi) \u2265 j + r \u2212 l. ut\nLemma 16. Let M be a sensing multi-DFA predictor in an Advance(i)-ready configuration on a properly multilinear word \u03b1 for some 1 \u2264 i \u2264 4. Write \u03b1 in the form of Theorem 7. Suppose that M enters AdvanceMany(i) with hi at the beginning of some segment j and \u03c1+ 2\u03c3 \u2264 r\u2212 l \u2264 block(hi)\u2212 2\u03c1\u2212\u03c32. Then AdvanceMany(i) will end with hi at the beginning of segment j + r \u2212 l.\nProof. Let h = hi. We have seg(h) = j, so for every segment k \u2265 j, |segk| = |pksd k m e k | \u2265 d kme \u2265 d seg(h) m e = block(h). Hence for every segment k \u2265 j, we have |segk| \u2265 block(h) \u2265 r \u2212 l + 2\u03c1 + \u03c32. Therefore we can make use of Lemma 12 whenever h is at the beginning of segment j or any subsequent segment. Now, AdvanceMany first moves outer until outer = r, and then repeatedly calls AdvanceOne(i) and moves l and r together. When AdvanceOne is first called, h is at the beginning of a segment, so by Lemma 12, AdvanceOne will end with h at the beginning of the next segment. Since AdvanceOne leaves r\u2212 l unchanged, the same will be true for each subsequent call to AdvanceOne. After r\u2212 l iterations of the loop, l will catch up with outer and AdvanceMany will end, leaving h at the beginning of segment j + r \u2212 l. ut\nLemma 17. Let M be a sensing multi-DFA predictor in a Correction-ready configuration on a properly multilinear word \u03b1. If M enters Correction, then Correction will end, and it will move h4 at least once.\nProof. By Lemmas 10 and 14, each call to AdvanceOne and AdvanceMany will end, and h4 will be moved at least once. So Correction will end, and it will have moved h4 at least once. ut\nLemma 18. Let M be a sensing multi-DFA predictor in a Correction-ready configuration on a properly multilinear word \u03b1. Write \u03b1 in the form of Theorem 7. Suppose \u03c1 + 2\u03c3 \u2264 r \u2212 l. Then if M enters Correction with h4 in some segment j, Correction will end with seg(h4) \u2265 j + 3(r \u2212 l) + 1.\nProof. Correction begins by moving h1 until h1 = h4, and then runs AdvanceOne(1). By Lemma 11, AdvanceOne(1) will end with seg(h1) \u2265 j + 1. Next, h2 is moved until h2 = h1 and then AdvanceMany(2) is called. Since r \u2212 l is unchanged, by Lemma 15, AdvanceMany(2) will end with seg(h2) \u2265 j + 1 + r \u2212 l. Next, h3 is moved until h3 = h2 and then AdvanceMany(3) is called. Again since r \u2212 l is unchanged, by Lemma 15, AdvanceMany(3) will end with seg(h3) \u2265 j + 1 + 2(r \u2212 l). Finally, h4 is moved until h4 = h3 and AdvanceMany(4) is called. Again since r\u2212 l is unchanged, by Lemma 15, AdvanceMany(4) will end with seg(h4) \u2265 j+1+3(r\u2212l), completing the proof. ut\nLemma 19. Let M be a sensing multi-DFA predictor in a Correction-ready configuration on a properly multilinear word \u03b1. Write \u03b1 in the form of Theorem 7. Suppose that M enters Correction with h4 in some segment j and 4(\u03c1+\u03c3) \u2264 r \u2212 l \u2264 block(h4) \u2212 \u03c32 \u2212 4(\u03c1 + \u03c3). Then Correction will end with h1 at the beginning of some segment i > j, h2 at the beginning of segment i + r \u2212 l, h3 at the beginning of segment i + 2(r \u2212 l), and h4 at the beginning of segment i+ 3(r \u2212 l).\nProof. Correction begins by moving h1 until h1 = h4, and then runs AdvanceOne(1). Then seg(h1) = j, so we have |segj+1| = |pj+1s d j+1m e j+1 | \u2265 d j+1m e \u2265 d seg(h1)m e = block(h1). Therefore |segj+1| \u2265 block(h1), and hence we can make use of Lemma 13. So by Lemma 13, AdvanceOne(1) will end with h1 at the\nbeginning of either the next segment or of the one after it. So now h1 is at the beginning of some segment i > j. Next, h2 is moved until h2 = h1. Now h2 is at the beginning of segment i, so since r \u2212 l is unchanged, by Lemma 16, AdvanceMany(2) will end with h2 at the beginning of segment i+ r\u2212 l. Next, h3 is moved until h3 = h2. Now h3 is at the beginning of segment i+ r\u2212 l, so again since r \u2212 l is unchanged, by Lemma 16, AdvanceMany(3) will end with h3 at the beginning of segment i + 2(r \u2212 l). Finally, h4 is moved until h4 = h3. Now h4 is at the beginning of segment i+ 2(r\u2212 l), so again since r\u2212 l is unchanged, by Lemma 16, AdvanceMany(4) will end with h4 at the beginning of segment i+ 3(r \u2212 l), completing the proof. ut\nMain loop With lemmas for the matching and correction procedures in place, we are ready to prove the main result. We first give a lemma to establish that the procedures will always be entered in the \u201cready\u201d configurations defined above.\nLemma 20. Let M be a sensing multi-DFA predictor which implements Algorithm 3 on an infinite word \u03b1. Then whenever M enters Matching, it is in a Matching-ready configuration, whenever M enters AdvanceOne(i) or AdvanceMany(i) for any 1 \u2264 i \u2264 4, it is in an Advance(i)-ready configuration, and whenever M enters Correction, it is in a Correction-ready configuration.\nProof. Let us say that M is CM-ready if it is in a configuration on \u03b1 which is both Correction-ready and Matching-ready. At the beginning of Algorithm 3, all the heads are at the beginning of the input, so M is CM-ready. In the main loop, M moves r, then calls Correction, and then calls Matching. If M is CM-ready when it moves r, then it remains CM-ready after moving r.\nNow, suppose M is CM-ready when it calls Correction. Correction first moves h1 until it reaches h4. Since M is CM-ready, it is in an Advance(4)-ready configuration on \u03b1, so t \u2264 h4. Hence now t \u2264 h1, so M is in an Advance(1)ready configuration on \u03b1. Now M enters AdvanceOne(1). Notice that AdvanceMany(i) and AdvanceOne(i) never move t past hi, l past r, inner past r, or outer past r. So whenever M enters these procedures in an Advance(i)ready configuration, it remains in an Advance(i)-ready configuration upon exiting them. Next, Correction moves h2 until it reaches h1. Since t \u2264 h1, we have now t \u2264 h2, so M is in an Advance(2)-ready configuration on \u03b1 when it enters AdvanceMany(2). Next, Correction moves h3 until it reaches h2. Since t \u2264 h2, we have now t \u2264 h3, so M is in an Advance(3)-ready configuration on \u03b1 when it enters AdvanceMany(3). Finally, Correction moves h4 until it reaches h3. Since t \u2264 h3, we have now t \u2264 h4, so M is in an Advance(4)-ready configuration on \u03b1 when it enters AdvanceMany(4). So if M is CM-ready when it enters Correction, then it is again CM-ready upon exiting Correction.\nFinally, notice that Matching never moves h1 past h2, h2 past h3, h3 past h4, or h3a past h3. So if M is CM-ready when it enters Matching, then it is again CM-ready upon exiting Matching. So Correction and Matching are only entered when M is CM-ready, completing the proof. ut\nTheorem 6. Let A be an alphabet. Then some sensing multi-DFA predictor masters every multilinear word over A.\nProof. Let M be a sensing 10-head DFA predictor which implements Algorithm 3. By Lemma 20, whenever M enters Matching, it is in a Matching-ready configuration, whenever M enters AdvanceOne(i) or AdvanceMany(i) for any 1 \u2264 i \u2264 4, it is in an Advance(i)-ready configuration, and whenever M enters Correction, it is in a Correction-ready configuration. We can therefore make use of the lemmas proved above for the matching and correction procedures. To see that M masters every multilinear word, take any such word \u03b1. Suppose for contradiction that M does not master \u03b1.\nFirst, suppose \u03b1 is ultimately periodic. Then \u03b1 = ps\u03c9 for some strings p, s such that s 6= \u03bb. By Lemma 5, if Matching is entered and does not end, then M masters \u03b1. By Lemma 9, if Correction is entered and does not end, then M masters \u03b1. So since we supposed that M does not master \u03b1, both procedures always end. Then since r is moved at the beginning of each iteration of the loop, and since Correction and Matching leave r \u2212 l unchanged, eventually Correction will be entered with r \u2212 l \u2265 |ps|. Then by Lemma 9, M masters \u03b1, contradicting the supposition that it does not. So M masters \u03b1.\nSo say \u03b1 is properly multilinear. Then \u03b1 can be written as\nq \u220f\nn\u22650\nm\u220f i\u22651 pis n i\nsubject to the conditions of Theorem 7 and the definitions of Section B.1. By Lemma 5, if Matching is entered and does not end, then M masters \u03b1. So since we supposed that M does not master \u03b1, Matching always ends. Now by Lemma 17, each time Correction is entered, it will end, and it will move h4 at least once. For each i \u2265 1, let point i be the point of the computation during the ith iteration of the loop of Algorithm 3, after r has been moved but before Correction has been entered. Since r is moved at the beginning of each iteration of the loop, and since Correction and Matching leave r \u2212 l unchanged, we have for all i \u2265 1, at point i, r \u2212 l = i. Let j = 4(\u03c1 + \u03c3) + |q|. Then for all i \u2265 j, at point i, r \u2212 l \u2265 4(\u03c1 + \u03c3) and h4 > |q|. For each i \u2265 j, denote by f(i) the value of seg(h4) at point i. We have f(j) \u2265 1 and by Lemma 18, for all i > j, f(i) \u2265 f(i \u2212 1) + 3i + 1. Solving the recurrence, we get for all i \u2265 j, f(i) \u2265 (i\u2212j+1)(3(i\u2212j)+2)2 \u2265 (i\u2212j)2 2 . Then for all i \u2265 j, at point i, block(h4) \u2265 (i\u2212j) 2 2m . Let k = 2m(2j + \u03c3 2 + 4\u03c1+ 4\u03c3) + j. For all i \u2265 k, at point i, we have\nblock(h4) \u2265 (i\u2212 j)2\n2m\n\u2265 (i\u2212 j)(k \u2212 j) 2m = (i\u2212 j)(2j + \u03c32 + 4\u03c1+ 4\u03c3) = 2ij + i\u03c32 + 4i\u03c1+ 4i\u03c3 \u2212 j(2j + \u03c32 + 4\u03c1+ 4\u03c3)\n= ij + i\u03c32 + 4i\u03c1+ 4i\u03c3 + ij \u2212 j(2j + \u03c32 + 4\u03c1+ 4\u03c3) \u2265 ij + i\u03c32 + 4i\u03c1+ 4i\u03c3 \u2265 i+ \u03c32 + 4(\u03c1+ \u03c3) = r \u2212 l + \u03c32 + 4(\u03c1+ \u03c3).\nThen for all i \u2265 k, at point i, we have 4(\u03c1+\u03c3) \u2264 r\u2212l \u2264 block(h4)\u2212\u03c32\u22124(\u03c1+\u03c3). So we can make use of Lemma 19 at any point i \u2265 k. Since r \u2212 l increases by 1 with each iteration of the loop, for some i \u2265 k, at point i, r\u2212 l is a multiple of m. Take any such i. Then when Correction is entered on the ith iteration of the loop, by Lemma 19, it will exit with h1 at the beginning of some segment d, h2 at the beginning of segment d+r\u2212 l, h3 at the beginning of segment d+2(r\u2212 l), and h4 at the beginning of segment d+ 3(r \u2212 l). Then the number of segments between h1 and h2 equals the number of segments between h2 and h3 equals the number of segments between h3 and h4 equals r \u2212 l, which is a multiple of m. So in the next call to Matching, by Lemma 6, M masters \u03b1, contradicting the supposition that it does not. So M masters \u03b1. ut"}], "references": [{"title": "Learning regular omega languages", "author": ["D. Angluin", "D. Fisman"], "venue": "Tim Smith in Computer Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Inductive inference: Theory and methods", "author": ["D. Angluin", "C.H. Smith"], "venue": "ACM Comput. Surv", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1983}, {"title": "Minimax vs. Bayes prediction", "author": ["D. Blackwell"], "venue": "Probability in the Engineering and Informational Sciences", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Toward a mathematical theory of inductive inference", "author": ["L. Blum", "M. Blum"], "venue": "Information and Control 28(2),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1975}, {"title": "Predictions with automata", "author": ["A. Broglio", "P. Liardet"], "venue": "Symbolic Dynamics and its Applications. Contemporary Mathematics, vol. 135, pp. 111\u2013124. American Mathematical Society", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "Prediction of binary sequences by evolving finite state machines", "author": ["U. Cerruti", "M. Giacobini", "P. Liardet"], "venue": "Artificial Evolution, Lecture Notes in Computer Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "High-confidence predictions under adversarial uncertainty", "author": ["A. Drucker"], "venue": "TOCT 5(3),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Degrees of Streams. Integers", "author": ["J. Endrullis", "D. Hendriks", "J.W. Klop"], "venue": "Electronic Journal of Combinatorial Number Theory 11B(A6),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Universal prediction of individual sequences", "author": ["M. Feder", "N. Merhav", "M. Gutman"], "venue": "IEEE Transactions on Information Theory 38, 1258\u20131270", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "One-way stack automata", "author": ["S. Ginsburg", "S.A. Greibach", "M.A. Harrison"], "venue": "J. ACM 14(2), 389\u2013418", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1967}, {"title": "Language identification in the limit", "author": ["E.M. Gold"], "venue": "Information and Control 10(5),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1967}, {"title": "Adversarial sequence prediction", "author": ["B. Hibbard"], "venue": "Proceedings of the 2008 Conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Complexity of multi-head finite automata: Origins and directions", "author": ["M. Holzer", "M. Kutrib", "A. Malcher"], "venue": "Theor. Comput. Sci. 412(1-2), 83\u201396", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "One-way multihead deterministic finite automata", "author": ["J. Hromkovi\u010d"], "venue": "Acta Informatica 19(4),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1983}, {"title": "Inductive inference of ultimately periodic sequences", "author": ["P. Johansen"], "venue": "BIT Numerical Mathematics", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1988}, {"title": "Inverse problems for finite automata: A solution based on genetic algorithms", "author": ["B. Leblanc", "E. Lutton", "J.P. Allouche"], "venue": "Artificial Evolution, Lecture Notes in Computer Science,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Is there an elegant universal theory of prediction", "author": ["S. Legg"], "venue": "Algorithmic Learning Theory, 17th International Conference,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "An unpredictability approach to finite-state randomness", "author": ["M.G. O\u2019Connor"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1988}, {"title": "The complexity of finding cycles in periodic functions", "author": ["R. Sedgewick", "T.G. Szymanski", "A.C. Yao"], "venue": "SIAM Journal on Computing 11(2), 376\u2013390", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1982}, {"title": "Games of prediction of periodic sequences", "author": ["B. Shubert"], "venue": "Tech. rep., United States Naval Postgraduate School", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1971}, {"title": "On Infinite Words Determined by Stack Automata", "author": ["T. Smith"], "venue": "FSTTCS 2013. Leibniz International Proceedings in Informatics (LIPIcs), vol. 24, pp. 413\u2013424. Schloss Dagstuhl\u2013Leibniz-Zentrum fuer Informatik, Dagstuhl, Germany", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "A formal theory of inductive inference. part i. Information and Control", "author": ["R. Solomonoff"], "venue": "http://www.sciencedirect.com/science/article/", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1964}, {"title": "Computational Complexity", "author": ["K. Wagner", "G. Wechsung"], "venue": "Mathematics and its Applications, Springer", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1986}], "referenceMentions": [{"referenceID": 20, "context": "Finally we consider the multilinear words [21], which consist of an initial string followed by strings that repeat in a way governed by linear polynomials, for example abaabaaab \u00b7 \u00b7 \u00b7 .", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "Stack automata are a generalization of pushdown automata whose stack head, in addition to pushing and popping when at the top of the stack, can move up and down the stack in read-only mode [10].", "startOffset": 189, "endOffset": 193}, {"referenceID": 12, "context": "Next, we consider multi-DFAs (multihead deterministic finite automata), finite automata with one or more input heads [13].", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "Finally, we consider sensing multi-DFAs, multihead DFAs extended with the ability to sense, for each pair of heads, whether those two heads are at the same position on the input tape [14].", "startOffset": 183, "endOffset": 187}, {"referenceID": 1, "context": "A classic survey of inductive inference, including the problem of sequence prediction, can be found in [2].", "startOffset": 103, "endOffset": 106}, {"referenceID": 10, "context": "The concept of \u201cmastering\u201d an infinite word is a form of \u201clearning in the limit\u201d, a concept which originates with the seminal paper of Gold [11], where it is applied to language learnability.", "startOffset": 140, "endOffset": 144}, {"referenceID": 3, "context": "Turing machines are considered as sequence extrapolators in [4].", "startOffset": 60, "endOffset": 63}, {"referenceID": 19, "context": "An early work on prediction of periodic sequences is [20], where these sequences appear in the setting of two-player emission-prediction games.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "Inference of ultimately periodic sequences is treated in [15] in an \u201coffline\u201d setting, where the input is a finite string and the output is a description of an ultimately periodic sequence.", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "In [18], finite-state automata are considered as predicting machines and the question of which sequences appear \u201crandom\u201d to these machines is answered.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "Further work on this concept appears in [5].", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "In [9] the finite-state predictability of an infinite sequence is defined as the minimum fraction of prediction errors that can be made by an finite-state predictor, and it is proved that finite-state predictability can be obtained by an efficient prediction procedure using techniques from data compression.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "In [3] a random prediction method for binary sequences is given which ensures that the proportion of correct predictions approaches the frequency of the more common symbol (0 or 1) in the sequence.", "startOffset": 3, "endOffset": 6}, {"referenceID": 15, "context": "In [16], \u201cinverse", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "In [6], an evolutionary algorithm is used to search for the finite-state machine with the highest prediction ratio for a given purely periodic word, in the space of all automata with a fixed number of states.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [7], the problem of successfully predicting a single 0 in an infinite binary word being revealed sequentially to the predictor is considered; only one prediction may be made, but at a time of the predictor\u2019s choosing.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Learning of languages consisting of infinite words has also been studied; see [1] for recent work.", "startOffset": 78, "endOffset": 81}, {"referenceID": 21, "context": "An early and influential approach to predicting infinite sequences is that of program-size complexity [22].", "startOffset": 102, "endOffset": 106}, {"referenceID": 16, "context": "Unfortunately this model is incomputable, and in [17] it is shown furthermore that some sequences can only be predicted by very complex predictors which cannot be discovered mathematically due to problems of G\u00f6del incompleteness.", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "[17] concludes that \u201cperhaps the only reasonable solution would be to add additional restrictions to both the algorithms which generate the sequences to be predicted, and to the predictors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Following on from [17], in [12] the formalism of sequence prediction is extended to a competition between two agents, which is shown to be a computational resources arms race.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "Following on from [17], in [12] the formalism of sequence prediction is extended to a competition between two agents, which is shown to be a computational resources arms race.", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "The class of multilinear words appears in [21] and also in [8] (as the reducts of the \u201cprime\u201d stream \u03a0).", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "The class of multilinear words appears in [21] and also in [8] (as the reducts of the \u201cprime\u201d stream \u03a0).", "startOffset": 59, "endOffset": 62}, {"referenceID": 22, "context": "(See [23] for results on the original automata, which are language recognizers rather than predictors.", "startOffset": 5, "endOffset": 9}, {"referenceID": 20, "context": "Since all of M \u2019s guesses after n are correct, M\u03b1 now recognizes Prefix(\u03b1), and hence M\u03b1 determines \u03b1 in the sense of [21].", "startOffset": 118, "endOffset": 122}, {"referenceID": 20, "context": "Then by Theorem 8 of [21], \u03b1 is multilinear.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "We employ a variation of the \u201ctortoise and hare\u201d cycle detection algorithm [19], adapted to our setting.", "startOffset": 75, "endOffset": 79}], "year": 2016, "abstractText": "In the classic problem of sequence prediction, a predictor receives a sequence of values from an emitter and tries to guess the next value before it appears. The predictor masters the emitter if there is a point after which all of the predictor\u2019s guesses are correct. In this paper we consider the case in which the predictor is an automaton and the emitted values are drawn from a finite set; i.e., the emitted sequence is an infinite word. We examine the predictive capabilities of finite automata, pushdown automata, stack automata (a generalization of pushdown automata), and multihead finite automata. We relate our predicting automata to purely periodic words, ultimately periodic words, and multilinear words, describing novel prediction algorithms for mastering these sequences.", "creator": "LaTeX with hyperref package"}}}