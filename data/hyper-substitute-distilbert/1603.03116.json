{"id": "1603.03116", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Mar-2016", "title": "Low-rank passthrough neural networks", "abstract": "deep learning consists in shaping neural networks to perform techniques that sequentially unfold in linear steps provide a time dimension or enhance intrinsic depth effect. effective learning within formal setting is usually emphasized through specialized institutional architectures including where unable to mitigate the vanishing critical collapse of naive deep networks. techniques around these architecture, such including lstms, msc, intrusion filtering and passive packet network, are working on a single structural principle : aggregate state transition.", "histories": [["v1", "Thu, 10 Mar 2016 01:04:07 GMT  (244kb,D)", "https://arxiv.org/abs/1603.03116v1", "16 pages, 7 figures"], ["v2", "Thu, 19 May 2016 19:38:30 GMT  (333kb,D)", "http://arxiv.org/abs/1603.03116v2", "17 pages, 8 figures"]], "COMMENTS": "16 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["antonio valerio miceli barone"], "accepted": false, "id": "1603.03116"}, "pdf": {"name": "1603.03116.pdf", "metadata": {"source": "CRF", "title": "LOW-RANK PASSTHROUGH NEURAL NETWORKS", "authors": ["Antonio Valerio Miceli Barone"], "emails": ["amiceli@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. Effective learning in this setting is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem of naive deep networks. Many of these architectures, such as LSTMs, GRUs, Highway Networks and Deep Residual Network, are based on a single structural principle: the state passthrough. We observe that these architectures, hereby characterized as Passthrough Networks, in addition to the mitigation of the vanishing gradient problem, enable the decoupling of the network state size from the number of parameters of the network, a possibility that is exploited in some recent works but not thoroughly explored. In this work we propose simple, yet effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. We present competitive experimental results on synthetic tasks and a near state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data."}, {"heading": "1 OVERVIEW", "text": "Deep neural networks can perform non-trivial computation by the repeated the application of parametric non-linear transformation layers to vectorial (or, more generally, tensorial) data. This staging of many computation steps can be done over a time dimension for tasks involving sequential inputs or outputs of varying length, yielding a recurrent neural network, or over an intrinsic circuit depth dimension, yielding a deep feed-forward neural network, or both. Training these deep models is complicated by the exploding and vanishing gradient problems (Hochreiter, 1991; Bengio et al., 1994).\nStarting from the original LSTM of Hochreiter & Schmidhuber (1997), various network architectures have been proposed to ameliorate the vanishing gradient problem in the recurrent neural network setting, such as the modern LSTM (Graves & Schmidhuber, 2005), the GRU (Cho et al., 2014b) and other variants (Greff et al., 2015; Jo\u0301zefowicz et al., 2015). These architectures led to a number of breakthroughs in different tasks such as speech recognition (Graves et al., 2013), machine translation (Cho et al., 2014a; Bahdanau et al., 2014), natural language parsing (Vinyals et al., 2014), question answering (Iyyer et al., 2014) and many others. More recently, similar methods have been applied in the feed-forward neural network setting yielding state of the art results with architectures such as Highway Networks (Srivastava et al., 2015), Deep Residual Networks (He et al., 2015) and Grid LSTM1 (Kalchbrenner et al., 2015). All these architectures are based on a single structural principle which, in this work, we will refer to as the state passthrough. We will thus refer to these architectures as Passthrough Networks.\n\u2217Work partially done while affiliated with University of Pisa. 1which also generalize to networks which are deep in both an intrinsic dimension and a time dimension, or\neven in multiple additional dimensions.\nar X\niv :1\n60 3.\n03 11\n6v 2\n[ cs\n.L G\n] 1\n9 M\nay 2\n01 6\nAnother difficulty in training neural networks is the trade-off between the network representation power and its number of trainable parameters, which affects its data complexity during training in addition to its implementation memory requirements. More specifically, the number of parameters influences the representation power in two ways: on one hand, it can be thought as the number of tunable \u201dknobs\u201d or \u201dswitches\u201d that need to be set to represent a given computable function. On the other hand, however, the number of parameters constrains, in most neural architectures, the size of the partial results that are propagated inside the network: its internal memory capacity.\nIn typical \u201dfully connected\u201d neural architectures, a layer acting on a n-dimensional state vector has O(n2) parameters stored in one or more matrices. Since a sufficiently complex function requires a large number of bits to be represented regardless of architectural details, we can\u2019t hope to find low-dimensional representation for really hard learning tasks, but there can be many functions of practical interest that are simple enough to be represented by a relatively small number of bits while still requiring some sizable amount of memory to be computed. Therefore, representing these functions on a fully connected neural network can be wasteful in terms of number of parameters. For some tasks, this quadratic dependency between state size and parameter number can cause a model going from underfitting the training set to overfitting it just by the addition of a single state component. For this reason, a number of neural low-dimensional layer parametrization have been proposed, such as convolutional layers (LeCun et al., 2004; Krizhevsky et al., 2012) which impose a sparse, local, periodic structure on the parameter matrices, or multiplicative matrix decompositions, notably the Unitary Evolution RNNs (Arjovsky et al., 2015) (which also addresses the vanishing gradient problem) and others (Le et al., 2013; Moczulski et al., 2015).\nIn this work we observe that the state passthrough allows for a systematic decoupling of the network state size from the number of parameters: since by default the state vector passes mostly unaltered through the layers, each layer can be made simple enough to be described only by a small number of parameters without affecting the overall memory capacity of the network. This effectively spreads the computation over the depth or time dimension of the network, but without making the network \u201dthin\u201d (as proposed, for instance, by Srivastava et al. (2015)).\nTo the best of our knowledge, this systematic decoupling has not been described in a systematic way, although it has been exploited by some convolutional passthrough architectures for image recognition (Srivastava et al., 2015; He et al., 2015) or algorithmic tasks (Kaiser & Sutskever, 2015), or architectures with addressable read-write memory (Graves et al., 2014; Gregor et al., 2015; Neelakantan et al., 2015; Kurach et al., 2015; Danihelka et al., 2016).\nIn this work we introduce an unified view of passthrough architectures, describe their state sizeparameter size decoupling property, propose simple but effective low-dimensional parametrizations that exploit this decoupling based on low-rank or low-rank plus diagonal matrix decompositions. Our approach extends the LSTM architecture with a single projection layer by Sak et al. (2014) which has been applied to speech recognition, natural language modeling (Jo\u0301zefowicz et al., 2016), video analysis (Sun et al., 2015) et cetera. We provide experimental evaluation of our approach on GRU and Highway Network architectures on various machine learning tasks, including a near state of the art result for the hard task of sequential randomly-permuted MNIST image recognition (Le et al., 2015)."}, {"heading": "2 MODEL", "text": "In this section we will introduce a notation to describe various neural network architectures, then we will formally describe passthrough architectures and finally will introduce our low-dimensional parametrizations for these architectures.\nA neural network can be described as a dynamical system that transforms an input u into an output y over multiple time steps T . At each step t the network has a n-dimensional state vector x(t) \u2208 Rn defined as\nx(t) = { in(u, \u03b8) if t = 0 f(x(t\u2212 1), t, u, \u03b8) if t \u2265 1 (1)\nwhere in is a state initialization function, f is a state transition function and \u03b8 \u2208 Rk is vector of trainable parameters. The output\ny = out(x(0 : T ), \u03b8) (2)\nis generated by an output function out, where x(0 : T ) denotes the whole sequence of states visited during the execution.\nIn a feed-forward neural network with constant hidden layer width n, the input u \u2208 Rm and the output y \u2208 Rl are vectors of fixed dimension m and l respectively, T is a model hyperparameter and the functions above can be simplified as\nin(u, \u03b8) = in(u, \u03b8in)\nf(x(t\u2212 1), t, u, \u03b8) = f(x(t\u2212 1), \u03b8t) out(x(0 : T ), \u03b8) = out(x(T ), \u03b8out)\n(3)\nhighlighting the dependence of the different layers on different subsets of parameters.\nIn a recurrent neural network the input u is typically a list of T m-dimensional vectors u(t) \u2208 Rm for t \u2208 1, . . . , T where T is variable, the output y is either a single l-dimensional vector or a list of T such vectors. The model functions can be written as\nin(u, \u03b8) = \u03b8in\nf(x(t\u2212 1), t, u, \u03b8) = f(x(t\u2212 1), u(t), \u03b8f ) out(x(0 : T ), \u03b8) = [out(x(1), \u03b8out), . . . , out(x(T ), \u03b8out)]\n(4)\nwhere for a fixed-dimensional output we assume that only y(T ) is meaningful.\nOther neural architectures, such as \u201dseq2seq\u201d transducers without attention (Cho et al., 2014a), can be also described with this framework."}, {"heading": "2.1 PASSTHROUGH NETWORKS", "text": "Passthough networks can be defined as networks where the state transition function f has a special form such that, at each step t the state vector x(t) (or a sub-vector x\u0302(t)) is propagated to the next step modified only by some (nearly) linear, element-wise transformations.\nLet the state vector x(t) \u2261 (x\u0302(t), x\u0303(t)) be the concatenation of x\u0302(t) \u2208 Rn\u0302 and x\u0303(t) \u2208 Rn\u0303 with n\u0302+ n\u0303 = n (where n\u0303 can be equal to zero). We define a network to have a state passthrough on x\u0302 if x\u0302 evolves as\nx\u0302(t) = f\u03c0(x(t\u2212 1), t, u, \u03b8) f\u03c4 (x(t\u2212 1), t, u, \u03b8) + x\u0302(t\u2212 1) f\u03b3(x(t\u2212 1), t, u, \u03b8) (5) where f\u03c0 is the next state proposal function, f\u03c4 is the transform function, f\u03b3 is the carry function and denotes element-wise vector multiplication.\nThe rest of the state vector x\u0303(t), if present, evolves according to some other function f\u0303 . In practice x\u0303(t) is only used in LSTM variants, while in other passthrough architectures x\u0302(t) = x(t).\nWe denote the state passthrough as additive if f\u03c4 (x(t \u2212 1), t, u, \u03b8) = f\u03b3(x(t \u2212 1), t, u, \u03b8) = 1\u2297n\u0302. This choice is used in the original LSTM of Hochreiter & Schmidhuber (1997) and in the Deep Residual Network2 of He et al. (2015).\nWe denote the state passthrough as convex if f\u03c4 (x(t \u2212 1), t, u, \u03b8) = 1\u2297n\u0302 \u2212 f\u03b3(x(t \u2212 1), t, u, \u03b8). This choice is used in GRUs (Cho et al., 2014b) and Highway Networks (Srivastava et al., 2015). Modern LSTM variants (Greff et al., 2015) typically use a transform function (\u201dforget gate\u201d) f\u03c4 and carry function (\u201dinput gate\u201d) f\u03c4 independent of each other.\nAs concrete example, we can describe a fully connected Highway Network as\nf\u03c0(x(t\u2212 1), t, u, \u03b8) = g(\u03b8(W\u03c0)t \u00b7 x(t\u2212 1) + \u03b8 (b\u03c0) t )\nf\u03c4 (x(t\u2212 1), t, u, \u03b8) = \u03c3(\u03b8(W\u03c4 )t \u00b7 x(t\u2212 1) + \u03b8 (b\u03c4 ) t ) f\u03b3(x(t\u2212 1), t, u, \u03b8) = 1\u2297n \u2212 f\u03c4 (x(t\u2212 1), t, u, \u03b8)\n(6)\nwhere g is an element-wise activation function, usually the ReLU (Glorot et al., 2011) or the hyperbolic tangent, \u03c3 is the element-wise logistic sigmoid, and \u2200t \u2208 1, . . . , T , the parameters \u03b8(W\u03c0)t\n2the Deep Residual Network does not exactly fit this definition of passthrough network due to the ReLU non-linearities applied between the layers, but it is similar enough that it can be considered to be based on the same principle\nx\u0302 (t\u22121)\nf \u03b3\nf \u03c4\nf \u03c0\n+ x\u0302 (t)\nFigure 1: Generic state passthrough hidden layer. Optional non-passthrough state x\u0303(t) and pertimestep input u(t) are not shown.\nand \u03b8(W\u03c4 )t are matrices in Rn\u00d7n and \u03b8 (b\u03c0) t and \u03b8 (b\u03c0) t are vectors in Rn. Dependence on the input u occurs only though the initialization function, which is model-specific and is omitted here, as is the output function."}, {"heading": "2.2 LOW-RANK PASSTHROUGH NETWORKS", "text": "In fully connected architectures there are n \u00d7 n matrices that act on the state vector, such as the \u03b8 (W\u03c0) t and \u03b8 (W\u03c4 ) t matrices of the Highway Network of eq. 6. Each of these matrices has n\n2 entries, thus for large n, the entries of these matrices can make up the majority of independently trainable parameters of the model.\nAs discussed in the previous section, this parametrization can be wasteful. Specifically, this parameterization implies that, at each step, all the information in each state component can affect all the information in any state component at the next step. That is, the computation performed at each step is essentially fully global. Classical physical systems, however, consist of spatially separated parts with primarily local interactions, long-distance interactions are possible but they tend to be limited by propagation delays, bandwidth and noise. Therefore it may be beneficial to bias our model class towards models that tend to adhere to these physical constraints by using a parametrization which reduces the number of parameters required to represent them.\nWe can accomplish this low-dimensional parametrization by imposing some constraints on the n\u00d7n matrices that parametrize the state transitions. One way of doing this is to impose a convolutional structure on these matrices, which corresponds to strict locality and periodicity constraints as in a cellular automaton. These constraints may work well in certain domains such as vision, but may be overly restrictive in other domains.\nWe propose instead to impose a low-rank constraint on these matrices. This is easily accomplished by rewriting each of these matrices as the product of two matrices where the inner dimension d is a model hyperparameter. For instance, in the case of the Highway Network of eq. 6 we can redefine \u2200t \u2208 1, . . . , T\n\u03b8 (W\u03c0) t = \u03b8 (L\u03c0) t \u00b7 \u03b8 (R\u03c0) t\n\u03b8 (W\u03c4 ) t = \u03b8 (L\u03c4 ) t \u00b7 \u03b8 (R\u03c4 ) t\n(7)\nwhere \u03b8(L\u03c0)t , \u03b8 (L\u03c4 ) t \u2208 Rn\u00d7d and \u03b8 (R\u03c0) t , \u03b8 (R\u03c4 ) t \u2208 Rd\u00d7n. When d < n/2 this result in a reduction of the number of independent parameters of the model.\nThis low-rank constraint can be thought as a bandwidth constraint on the computation performed at each step: the R matrices first project the state into a smaller subspace, extracting the information\nneeded for that specific step, then the Lmatrices project it back to the original state space, spreading the selected information to all the state components that need to be updated.\nNote that if we were to apply this constraint to a non-passthrough architecture, such as a MultiLayer Perceptron or a Elman\u2019s Recurrent Neural Network, it would create an information bottleneck within each layer, effectively reducing the memory capacity of the model. But in a passthrough architecture the memory capacity is unaffected since the state passthrough takes care of propagating all the information that does not need to be updated during one step to the next step. Therefore we exploit the decoupling property of the state passthrough. A similar approach has been proposed for the LSTM architecture by Sak et al. (2014), although they force the the R matrices to be the same for all the functions of the state transition, while we allow each parameter matrix to be parametrized independently by a pair of R and L matrices.\nLow-rank passthrough architectures are universal in that they retain the same representation classes of their parent architectures. This is trivially true if the inner dimension d is allowed to be O(n) in the worst case, and for some architectures even if d is held constant. For instance, it is easily shown that for any Highway Network with state size n and T hidden layers and for any > 0, there exist a Low-rank Highway Network with d = 1, state size at most 2n and at most nT layers that computes the same function within an margin of error."}, {"heading": "2.3 LOW-RANK PLUS DIAGONAL PASSTHROUGH NETWORKS", "text": "As we show in the experimental section, on some tasks the low-rank constraint may prove to be excessively restrictive if the goal is to train a model with fewer parameters than one with arbitrary matrices. A simple extension is to add to each low-rank parameter matrix a diagonal parameter matrix, yielding a matrix that is full-rank but still parametrized in a low-dimensional space. For instance, for the Highway Network architecture we modify eq. 7 to\n\u03b8 (W\u03c0) t = \u03b8 (L\u03c0) t \u00b7 \u03b8 (R\u03c0) t + \u03b8 (D\u03c0) t\n\u03b8 (W\u03c4 ) t = \u03b8 (L\u03c4 ) t \u00b7 \u03b8 (R\u03c4 ) t + \u03b8 (D\u03c4 ) t\n(8)\nwhere \u03b8(D\u03c0)t , \u03b8 (D\u03c4 ) t \u2208 Rn\u00d7n are trainable diagonal parameter matrices.\nLow-rank plus diagonal decompositions have been used for over a century in factor analysis in statistics (Spearman, 1904), system identification (Kalman, 1982) and other applications. They arise naturally in the estimation of linear relationships between variables from noisy measurements, under certain independence assumptions on the measurement noise. Refer to Saunderson et al. (2012) and Ning et al. (2015) for a review.\nAt first, it may seem that adding diagonal parameter matrices is redundant in passthrough networks. After all, the state passthrough itself can be considered as a diagonal matrix applied to the state vector, which is then additively combined to the new proposed state computed by the f\u03c0 function. However, since the state passthrough completely skips over all non-linear activation functions (except in the Residual Network architecture where it only skips over some of them), these formulations are not equivalent. In particular, the low-rank plus diagonal parametrization may help in recurrent neural networks which receive input at each time step, since they allow each component of the state vector to directly control how much input signal is inserted into it at each step. We demonstrate the effectiveness of this model in the sequence copy tasks described in the experiments section."}, {"heading": "3 EXPERIMENTS", "text": "In this section we report a preliminary experiment on Low-rank Highway Networks on the MNIST dataset and several experiments on Low-rank GRUs."}, {"heading": "3.1 LOW-RANK HIGHWAY NETWORKS", "text": "We applied the low-rank and low-rank plus diagonal Highway Network architecture to the classic benchmark task of handwritten digit classification on the MNIST dataset.\nWe used the low-rank architecture described by equations 6 and 7, with T = 5 hidden layers, ReLU activation function, state dimension n = 1024 and maximum rank (internal dimension) d = 256.\nThe input-to-state layer is a dense 784 \u00d7 1024 matrix followed by a (biased) ReLU activation and the state-to-output layer is a dense 1024\u00d7 10 matrix followed by a (biased) identity activation. We did not use any convolution layer, pooling layer or data augmentation technique.\nWe used dropout (Srivastava et al., 2014) in order to achieve regularization. We applied standard dropout layers with dropout probability p = 0.2 just before the input-to-state layer and p = 0.5 just before the state-to-output layer. We also applied dropout inside each hidden layer in the following way: we inserted dropout layers with p = 0.3 inside both the proposal function and the transform function, immediately before both the R matrices and the L matrices, totaling to four dropout layers per hidden layer, although the random dropout matrices are shared between proposal and transform functions. Dropout applied this way does not disrupt the state passthrough, thus it does not cause a reduction of memory capacity during training. We further applied L2-regularization with coefficient \u03bb = 1\u00d7 10\u22123 per example on the hidden-to-output parameter matrix. We also used batch normalization (Ioffe & Szegedy, 2015) after the input-to-state matrix and after each parameter matrix in the hidden layers.\nParameter matrices are randomly initialized using an uniform distribution with scale equal to \u221a 6/a where a is the input dimension. Initial bias vectors are all initialized at zero except for those of the transform functions in the hidden layers, which are initialized at \u22121.0. We trained to minimize the sum of the per-class L2-hinge loss plus the L2-regularization cost (Tang, 2013). Optimization was performed using Adam (Kingma & Ba, 2014) with standard hyperparameters, learning rate starting at 3\u00d7 10\u22123 halving every three epochs without validation improvements. Mini-batch size was equal to 100. Code is available online3.\nWe ran our experiments on a machine with a 24 core Intel(R) Xeon(R) CPU X5670 2.93GHz, 24 GB of RAM. We did not use a GPU. Training took approximately 4 hours .\nWe obtained perfect training accuracy and 98.83% test accuracy. While this result does not reach the state of the art for this task (99.13% test accuracy with unsupervised dimensionality reduction reported by Tang (2013)), it is still relatively close.\nWe also tested the low-rank plus diagonal Highway Network architecture of eq. 8 with the same settings as above, obtaining a test accuracy of 98.64%. The inclusion of diagonal parameter matrices does not seem to help in this particular task."}, {"heading": "3.2 LOW-RANK GRUS", "text": "We applied the Low-rank and Low-rank plus diagonal GRU architectures to a subset of sequential benchmarks described in the Unitary Evolution Recurrent Neural Networks article by Arjovsky et al. (2015), specifically the memory task, the addition task and the sequential randomly permuted MNIST task. For the memory tasks, we also considered two different variants proposed by Danihelka et al. (2016) and Henaff et al. (2016) which are hard for the uRNN architecture.\nWe chose to compare against the uRNN architecture because it set state of the art results in terms of both data complexity and accuracy and because it is an architecture with similar design objectives as low-rank passthrough architectures, namely a low-dimensional parametrization and the mitigation of the vanishing gradient problem, but it is based on quite different principles (it does not use a state passthrough as defined in this work, instead it relies on the reversibility and norm-preservation properties of unitary matrices in order preserve state information between time steps, and uses a multiplicative unitary decomposition in order to achieve low-dimensional parametrization).\nThe GRU architecture (Cho et al., 2014b) is a passthrough recurrent neural network defined as in(u, \u03b8) = \u03b8in\nf\u03c9(x(t\u2212 1), t, u, \u03b8) = \u03c3(\u03b8U\u03c9 \u00b7 u(t) + \u03b8(W\u03c9) \u00b7 x(t\u2212 1) + \u03b8(b\u03c9)) f\u03b3(x(t\u2212 1), t, u, \u03b8) = \u03c3(\u03b8U\u03b3 \u00b7 u(t) + \u03b8(W\u03b3) \u00b7 x(t\u2212 1) + \u03b8(b\u03b3)) f\u03c4 (x(t\u2212 1), t, u, \u03b8) = 1\u2297n \u2212 f\u03b3(x(t\u2212 1), t, u, \u03b8) f\u03c0(x(t\u2212 1), t, u, \u03b8) = g(\u03b8U\u03c0 \u00b7 u(t) + \u03b8(W\u03c0) \u00b7 (x(t\u2212 1) f\u03c9(x(t\u2212 1), t, u, \u03b8)) + \u03b8(b\u03c0)) (9)\n3https://github.com/Avmb/lowrank-highwaynetwork\nNote that with respect of the definition of the Highway Network architecture of eq. 6, the initial state \u03b8in is a model parameter, there is an additional function f\u03c9 (the \u201dreset\u201d gate), parameters don\u2019t depend on time t and input u(t) is included in the computation at each step though the \u03b8U matrices. We have also defined the transform function f\u03c4 in terms of the carry function f\u03b3 rather than vice versa for consistency with the literature, although the two formulations are isomorphic.\nWe turn this architecture into the Low-rank GRU architecture by redefining each of the \u03b8W matrices as the product of two matrices with inner dimension d. For the memory tasks, which turned out to be difficult for the low-rank parametrization, we also consider the low-rank plus diagonal parametrization. We also applied the low-rank plus diagonal parametrization for the sequential permuted MNIST task.\nIn our experiments we optimized using RMSProp (Tieleman & Hinton, 2012) with gradient component clipping at 1. Code is available online4. Our code is based on the published uRNN code5 (specifically, on the LSTM implementation) by the original authors for the sake of a fair comparison. In order to achieve convergence on the memory task however, we had to slightly modify the optimization procedure, specifically we changed gradient component clipping with gradient norm clipping (with NaN detection and recovery), and we added a small = 1\u00d7 10\u22128 term in the parameter update formula. No modifications of the original optimizer implementation were required for the other tasks.\nWe ran our experiments on the same machine as the experiments described in the previous section, with the exception of the largest sequential permuted MNIST experiment (low-rank plus diagonal GRU with n = 256, d = 24 which was run on a machine with a Geforce GTX TITAN X GPU).\nWe will now present a short description of each task, the experimental details and results."}, {"heading": "3.2.1 MEMORY TASK", "text": "The input of an instance of this task is a sequence of T = N + 20 discrete symbols in a ten symbol alphabet ai : i \u2208 0, . . . 9, encoded as one-hot vectors. The first 10 symbols in the sequence are \u201ddata\u201d symbols i.i.d. sampled from a0, . . . , a7, followed byN\u22121 \u201dblank\u201d a8 symbols, then a distinguished \u201drun\u201d symbol a9, followed by 10 more \u201dblank\u201d a8 symbols. The desired output sequence consists of N + 10 \u201dblank\u201d a8 symbols followed by the 10 \u201ddata\u201d symbols as they appeared in the input sequence. Therefore the model has to remember the 10 \u201ddata\u201d symbol string over the temporal gap of size N , which is challenging for a recurrent neural network when N is large. In our experiment we set N = 500, which is the hardest setting explored in the uRNN work. The training set consists of 100, 000 training examples and 10, 000 validation/test examples.\nThe architecture is described by eq. (9), with an additional output layer with a dense n\u00d7 10 matrix followed a (biased) softmax. We train to minimize the cross-entropy loss.\nWe were able to solve this task using a GRU with full recurrent matrices with state size n = 128, learning rate 1\u00d7 10\u22123, mini-batch size 20, initial bias of the carry functions (the \u201dupdate\u201d gates) 4.0, however this model has many more parameters, nearly 50, 000 in the recurrent layer only, than the uRNN work which has about 6, 500, and it converges much more slowly than the uRNN.\nWe were not able to achieve convergence with a pure low-rank model without exceeding the number of parameters of the fully connected model, but we achieved fast convergence with a low-rank plus diagonal model with d = 50, with other hyperparameters set as above. This model has still more parameters (39, 168 in the recurrent layer, 41, 738 total) than the uRNN model and converges more slowly but still reasonably fast, reaching test cross-entropy < 1\u00d7 10\u22123 nats and almost perfect classification accuracy in less than 35, 000 updates.\nWe also consider two variants of this task which are difficult for the uRNN model. For both these tasks we used the same settings as above except that the task size parameter is set at N = 100 for consistency with the works that introduced these variants.\nIn the variant of Danihelka et al. (2016), the length of the sequence to be remembered is randomly sampled between 1 and 10 for each sequence. They manage to achieve fast convergence with\n4https://github.com/Avmb/lowrank-gru 5https://github.com/amarshah/complex_RNN\ntheir Associative LSTM architecture with 65, 505 parameters, and slower convergence with standard LSTM models. Our low-rank plus diagonal GRU architecture, which has less parameters than their Associative LSTM, performs comparably or better, reaching test cross-entropy < 1\u00d7 10\u22123 nats and almost perfect classification accuracy in less than 30, 000 updates.\nIn the variant of Henaff et al. (2016), the length of the sequence to be remembered is fixed at 10 but the model is expected to copy it after a variable number of time steps randomly chosen, for each sequence, between 1 and N = 100. The authors achieve slow convergence with a standard LSTM model, while our low-rank plus diagonal GRU architecture achieves fast convergence, reaching test cross-entropy < 1\u00d7 10\u22123 nats and almost perfect classification accuracy in less than 38, 000 updates, and perfect test accuracy in 87, 000 updates."}, {"heading": "3.2.2 ADDITION TASK", "text": "For each instance of this task, the input sequence has length T and consists of two real-valued components, at each step the first component is independently sampled from the interval [0, 1] with uniform probability, the second component is equal to zero everywhere except at two randomly chosen time step, one in each half of the sequence, where it is equal to one. The result is a single real value computed from the final state which we want to be equal to the sum of the two elements of the first component of the sequence at the positions where the second component was set at one. In our experiment we set T = 750. The training set consists of 100, 000 training examples and 10, 000 validation/test examples.\nWe use a Low-rank GRU with 2\u00d7 n input matrix, n\u00d7 1 output matrix and (biased) identity output activation. We train to minimize the mean squared error loss. We use the following hyperparameter configuration: State size n = 128, maximum rank d = 24. This results in approximately 6, 140\nparameters in the recurrent hidden layer. Learning rate was set at 1\u00d7 10\u22123, mini-batch size 20, initial bias of the carry functions (the \u201dupdate\u201d gates) was set to 4.\nWe trained on 14, 500 mini-batches, obtaining a mean squared error on the test set of 0.003, which is a better result than the one reported in the uRNN article, in terms of training time and final accuracy."}, {"heading": "3.2.3 SEQUENTIAL MNIST TASK", "text": "This task consists of handwritten digit classification on the MNIST dataset with the caveat that the input is presented to the model one pixel value at time, over T = 784 time steps. To further increase the difficulty of the task, the inputs are reordered according to a random permutation (fixed for all the task instances).\nWe use a Low-rank GRU with 1\u00d7n input matrix, n\u00d7 10 output matrix and (biased) softmax output activation.\nLearning rate was set at 5\u00d7 10\u22124, mini-batch size 20, initial bias of the carry functions (the \u201dupdate\u201d gates) was set to 5.\nWe considered two hyperparameter configurations:\n1. State size n = 128, maximum rank d = 24.\n2. State size n = 512, maximum rank d = 4.\nConfiguration 1 reaches a validation accuracy of 93.4% in 320, 400 iterations. Final test accuracy is 91.8%. The reported uRNN accuracy is 91.4%. Our model however takes 100, 500 to reach a validation accuracy comparable to the final accuracy of the uRNN model, which is instead reached in about 20, 000 iterations.\nConfiguration 2 reaches a validation accuracy of 92.5% in 464, 700 iterations, with test accuracy of 91.3%. Note that even with the rather extreme bottleneck of d = 4, this model performs well.\nFor this task, we also consider three low-rank plus diagonal parametrizations. We report the best validation accuracy and test accuracy results, in addition to the results for a full-rank baseline GRU:\n1. State size n = 64, maximum rank d = 24. Validation accuracy: 93.1%, test accuracy: 91.9%.\n2. State size n = 128, maximum rank d = 24. Validation accuracy: 94.1%, test accuracy: 93.5%.\n3. State size n = 128, full-rank. Validation accuracy: 93.0%, test accuracy: 92.8%.\n4. State size n = 256, maximum rank d = 24. Validation accuracy: 95.1%, test accuracy: 94.7%.\nNote that the low-rank plus diagonal GRU is more accurate than the full rank GRU with the same state size, while the low-rank GRU is slightly less accurate, indicating the utility of the diagonal component of the parametrization for this task.\nThese results surpass the uRNN and are on par with more complex architectures with time-skip connections (Zhang et al., 2016) (reported test set accuracy 94.0%). To our knowledge, at the time of this writing, the best result on this task is the LSTM with recurrent batch normalization by Cooijmans et al. (2016) (reported test set accuracy 95.2%). The architectural innovations of these works are orthogonal to our own and in principle they can be combined to it."}, {"heading": "4 CONCLUSIONS AND FUTURE WORK", "text": "We presented a framework that unifies the description various types of recurrent and feed-forward neural networks as passthrough neural networks.\nWe proposed low-dimensional parametrizations for passthrough neural networks based on low-rank or low-rank plus diagonal decompositions of the n\u00d7 n matrices that occur in the hidden layers. We experimentally compared our models with state of the art models, obtaining competitive results including a state of the art for the randomly-permuted sequential MNIST task.\nOur parametrizations are alternative to convolutional parametrizations explored by Srivastava et al. (2015); He et al. (2015); Kaiser & Sutskever (2015). We note that the two approaches can be combined in at least two ways:\n\u2022 A low-rank (plus diagonal) decompostion (with a suitable axis reshaping) can be applied to convolutional filter banks when the number of channels is large.\n\u2022 The \u201dlocal\u201d state acted on by the convolutional passthrough filters can be paired with a \u201dglobal\u201d state acted on by low-rank (plus diagonal) passthrough matrices. The global state is replicated on additional channels to update the local state and the local state is pooled to update the global state. This arrangement may be useful in particular in the Neural GPU (Kaiser & Sutskever, 2015) in order to augment the cellular automaton with \u201dglobal variables\u201d, which would otherwise need to be replicated on the cell states and threaded over the computation.\nLow-rank and low-rank plus diagonal parametrizations are linear, alternative parametrizations could include non-linear activation functions, effectively replacing each hidden parameter matrix with a MLP, similar to the network-in-network approach of Lin et al. (2013).\nWe leave the exploration of these extensions to future work."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Giuseppe Attardi and the Department of Computer Science of University of Pisa for letting us use their machines to run the experiments presented in this paper."}], "references": [{"title": "Unitary evolution recurrent neural networks", "author": ["Arjovsky", "Martin", "Shah", "Amar", "Bengio", "Yoshua"], "venue": "CoRR, abs/1511.06464,", "citeRegEx": "Arjovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho", "Kyunghyun", "van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho", "Kyunghyun", "van Merrienboer", "Bart", "Gulcehre", "Caglar", "Bougares", "Fethi", "Schwenk", "Holger", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Associative Long Short-Term Memory", "author": ["I. Danihelka", "G. Wayne", "B. Uria", "N. Kalchbrenner", "A. Graves"], "venue": "ArXiv e-prints,", "citeRegEx": "Danihelka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Danihelka et al\\.", "year": 2016}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-rahman", "Hinton", "Geoffrey E"], "venue": "CoRR, abs/1303.5778,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Lstm: A search space odyssey", "author": ["Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Koutn\u0131\u0301k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "J\u00fcrgen"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Orthogonal RNNs and Long-Memory Tasks", "author": ["M. Henaff", "A. Szlam", "Y. LeCun"], "venue": "ArXiv e-prints,", "citeRegEx": "Henaff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["Hochreiter", "Sepp"], "venue": "Diploma, Technische Universita\u0308t Mu\u0308nchen,", "citeRegEx": "Hochreiter and Sepp.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter and Sepp.", "year": 1991}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Iyyer", "Mohit", "Boyd-Graber", "Jordan", "Claudino", "Leonardo", "Socher", "Richard", "Daum\u00e9 III", "Hal"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "An empirical exploration of recurrent network architectures", "author": ["J\u00f3zefowicz", "Rafal", "Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["J\u00f3zefowicz", "Rafal", "Vinyals", "Oriol", "Schuster", "Mike", "Shazeer", "Noam", "Wu", "Yonghui"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2016}, {"title": "Neural gpus learn algorithms", "author": ["Kaiser", "Lukasz", "Sutskever", "Ilya"], "venue": "CoRR, abs/1511.08228,", "citeRegEx": "Kaiser et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Kalchbrenner", "Nal", "Danihelka", "Ivo", "Graves", "Alex"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "System Identification from Noisy Data", "author": ["Kalman", "R.E"], "venue": "Defense Technical Information Center,", "citeRegEx": "Kalman and R.E,? \\Q1982\\E", "shortCiteRegEx": "Kalman and R.E", "year": 1982}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fastfood-approximating kernel expansions in loglinear time", "author": ["Le", "Quoc", "Sarl\u00f3s", "Tam\u00e1s", "Smola", "Alex"], "venue": "In Proceedings of the international conference on machine learning,", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Le", "Quoc V", "Jaitly", "Navdeep", "Hinton", "Geoffrey E"], "venue": "arXiv preprint arXiv:1504.00941,", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["LeCun", "Yann", "Huang", "Fu Jie", "Bottou", "Leon"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "LeCun et al\\.,? \\Q2004\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2004}, {"title": "ACDC: A structured efficient linear layer", "author": ["Moczulski", "Marcin", "Denil", "Misha", "Appleyard", "Jeremy", "de Freitas", "Nando"], "venue": "CoRR, abs/1511.05946,", "citeRegEx": "Moczulski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moczulski et al\\.", "year": 2015}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Neelakantan", "Arvind", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "CoRR, abs/1511.04834,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Linear models based on noisy data and the frisch scheme", "author": ["Ning", "Lipeng", "Georgiou", "Tryphon T", "Tannenbaum", "Allen", "Boyd", "Stephen P"], "venue": "SIAM Review,", "citeRegEx": "Ning et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ning et al\\.", "year": 2015}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Sak", "Hasim", "Senior", "Andrew W", "Beaufays", "Fran\u00e7oise"], "venue": "In INTERSPEECH,", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Diagonal and low-rank matrix decompositions, correlation matrices, and ellipsoid fitting", "author": ["Saunderson", "James", "Chandrasekaran", "Venkat", "Parrilo", "Pablo A", "Willsky", "Alan S"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Saunderson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Saunderson et al\\.", "year": 2012}, {"title": "intelligence,\u201d objectively determined and measured", "author": ["Spearman", "Charles"], "venue": "The American Journal of Psychology,", "citeRegEx": "Spearman and Charles.,? \\Q1904\\E", "shortCiteRegEx": "Spearman and Charles.", "year": 1904}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Temporal localization of finegrained actions in videos by domain transfer from web images", "author": ["Sun", "Chen", "Shetty", "Sanketh", "Sukthankar", "Rahul", "Nevatia", "Ram"], "venue": "In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference,", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "Deep learning using linear support vector machines", "author": ["Tang", "Yichuan"], "venue": "arXiv preprint arXiv:1306.0239,", "citeRegEx": "Tang and Yichuan.,? \\Q2013\\E", "shortCiteRegEx": "Tang and Yichuan.", "year": 2013}, {"title": "Grammar as a foreign language", "author": ["Vinyals", "Oriol", "Kaiser", "Lukasz", "Koo", "Terry", "Petrov", "Slav", "Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "arXiv preprint arXiv:1412.7449,", "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Architectural complexity measures of recurrent neural networks", "author": ["Zhang", "Saizheng", "Wu", "Yuhuai", "Che", "Tong", "Lin", "Zhouhan", "Memisevic", "Roland", "Salakhutdinov", "Ruslan", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1602.08210,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Training these deep models is complicated by the exploding and vanishing gradient problems (Hochreiter, 1991; Bengio et al., 1994).", "startOffset": 91, "endOffset": 130}, {"referenceID": 10, "context": ", 2014b) and other variants (Greff et al., 2015; J\u00f3zefowicz et al., 2015).", "startOffset": 28, "endOffset": 73}, {"referenceID": 18, "context": ", 2014b) and other variants (Greff et al., 2015; J\u00f3zefowicz et al., 2015).", "startOffset": 28, "endOffset": 73}, {"referenceID": 8, "context": "These architectures led to a number of breakthroughs in different tasks such as speech recognition (Graves et al., 2013), machine translation (Cho et al.", "startOffset": 99, "endOffset": 120}, {"referenceID": 1, "context": ", 2013), machine translation (Cho et al., 2014a; Bahdanau et al., 2014), natural language parsing (Vinyals et al.", "startOffset": 29, "endOffset": 71}, {"referenceID": 37, "context": ", 2014), natural language parsing (Vinyals et al., 2014), question answering (Iyyer et al.", "startOffset": 34, "endOffset": 56}, {"referenceID": 17, "context": ", 2014), question answering (Iyyer et al., 2014) and many others.", "startOffset": 28, "endOffset": 48}, {"referenceID": 12, "context": ", 2015), Deep Residual Networks (He et al., 2015) and Grid LSTM1 (Kalchbrenner et al.", "startOffset": 32, "endOffset": 49}, {"referenceID": 21, "context": ", 2015) and Grid LSTM1 (Kalchbrenner et al., 2015).", "startOffset": 23, "endOffset": 50}, {"referenceID": 1, "context": "Training these deep models is complicated by the exploding and vanishing gradient problems (Hochreiter, 1991; Bengio et al., 1994). Starting from the original LSTM of Hochreiter & Schmidhuber (1997), various network architectures have been proposed to ameliorate the vanishing gradient problem in the recurrent neural network setting, such as the modern LSTM (Graves & Schmidhuber, 2005), the GRU (Cho et al.", "startOffset": 110, "endOffset": 199}, {"referenceID": 27, "context": "For this reason, a number of neural low-dimensional layer parametrization have been proposed, such as convolutional layers (LeCun et al., 2004; Krizhevsky et al., 2012) which impose a sparse, local, periodic structure on the parameter matrices, or multiplicative matrix decompositions, notably the Unitary Evolution RNNs (Arjovsky et al.", "startOffset": 123, "endOffset": 168}, {"referenceID": 24, "context": "For this reason, a number of neural low-dimensional layer parametrization have been proposed, such as convolutional layers (LeCun et al., 2004; Krizhevsky et al., 2012) which impose a sparse, local, periodic structure on the parameter matrices, or multiplicative matrix decompositions, notably the Unitary Evolution RNNs (Arjovsky et al.", "startOffset": 123, "endOffset": 168}, {"referenceID": 0, "context": ", 2012) which impose a sparse, local, periodic structure on the parameter matrices, or multiplicative matrix decompositions, notably the Unitary Evolution RNNs (Arjovsky et al., 2015) (which also addresses the vanishing gradient problem) and others (Le et al.", "startOffset": 160, "endOffset": 183}, {"referenceID": 25, "context": ", 2015) (which also addresses the vanishing gradient problem) and others (Le et al., 2013; Moczulski et al., 2015).", "startOffset": 73, "endOffset": 114}, {"referenceID": 28, "context": ", 2015) (which also addresses the vanishing gradient problem) and others (Le et al., 2013; Moczulski et al., 2015).", "startOffset": 73, "endOffset": 114}, {"referenceID": 12, "context": "To the best of our knowledge, this systematic decoupling has not been described in a systematic way, although it has been exploited by some convolutional passthrough architectures for image recognition (Srivastava et al., 2015; He et al., 2015) or algorithmic tasks (Kaiser & Sutskever, 2015), or architectures with addressable read-write memory (Graves et al.", "startOffset": 202, "endOffset": 244}, {"referenceID": 9, "context": ", 2015) or algorithmic tasks (Kaiser & Sutskever, 2015), or architectures with addressable read-write memory (Graves et al., 2014; Gregor et al., 2015; Neelakantan et al., 2015; Kurach et al., 2015; Danihelka et al., 2016).", "startOffset": 109, "endOffset": 222}, {"referenceID": 11, "context": ", 2015) or algorithmic tasks (Kaiser & Sutskever, 2015), or architectures with addressable read-write memory (Graves et al., 2014; Gregor et al., 2015; Neelakantan et al., 2015; Kurach et al., 2015; Danihelka et al., 2016).", "startOffset": 109, "endOffset": 222}, {"referenceID": 29, "context": ", 2015) or algorithmic tasks (Kaiser & Sutskever, 2015), or architectures with addressable read-write memory (Graves et al., 2014; Gregor et al., 2015; Neelakantan et al., 2015; Kurach et al., 2015; Danihelka et al., 2016).", "startOffset": 109, "endOffset": 222}, {"referenceID": 5, "context": ", 2015) or algorithmic tasks (Kaiser & Sutskever, 2015), or architectures with addressable read-write memory (Graves et al., 2014; Gregor et al., 2015; Neelakantan et al., 2015; Kurach et al., 2015; Danihelka et al., 2016).", "startOffset": 109, "endOffset": 222}, {"referenceID": 19, "context": "(2014) which has been applied to speech recognition, natural language modeling (J\u00f3zefowicz et al., 2016), video analysis (Sun et al.", "startOffset": 79, "endOffset": 104}, {"referenceID": 35, "context": ", 2016), video analysis (Sun et al., 2015) et cetera.", "startOffset": 24, "endOffset": 42}, {"referenceID": 26, "context": "We provide experimental evaluation of our approach on GRU and Highway Network architectures on various machine learning tasks, including a near state of the art result for the hard task of sequential randomly-permuted MNIST image recognition (Le et al., 2015).", "startOffset": 242, "endOffset": 259}, {"referenceID": 0, "context": ", 2012) which impose a sparse, local, periodic structure on the parameter matrices, or multiplicative matrix decompositions, notably the Unitary Evolution RNNs (Arjovsky et al., 2015) (which also addresses the vanishing gradient problem) and others (Le et al., 2013; Moczulski et al., 2015). In this work we observe that the state passthrough allows for a systematic decoupling of the network state size from the number of parameters: since by default the state vector passes mostly unaltered through the layers, each layer can be made simple enough to be described only by a small number of parameters without affecting the overall memory capacity of the network. This effectively spreads the computation over the depth or time dimension of the network, but without making the network \u201dthin\u201d (as proposed, for instance, by Srivastava et al. (2015)).", "startOffset": 161, "endOffset": 849}, {"referenceID": 0, "context": ", 2012) which impose a sparse, local, periodic structure on the parameter matrices, or multiplicative matrix decompositions, notably the Unitary Evolution RNNs (Arjovsky et al., 2015) (which also addresses the vanishing gradient problem) and others (Le et al., 2013; Moczulski et al., 2015). In this work we observe that the state passthrough allows for a systematic decoupling of the network state size from the number of parameters: since by default the state vector passes mostly unaltered through the layers, each layer can be made simple enough to be described only by a small number of parameters without affecting the overall memory capacity of the network. This effectively spreads the computation over the depth or time dimension of the network, but without making the network \u201dthin\u201d (as proposed, for instance, by Srivastava et al. (2015)). To the best of our knowledge, this systematic decoupling has not been described in a systematic way, although it has been exploited by some convolutional passthrough architectures for image recognition (Srivastava et al., 2015; He et al., 2015) or algorithmic tasks (Kaiser & Sutskever, 2015), or architectures with addressable read-write memory (Graves et al., 2014; Gregor et al., 2015; Neelakantan et al., 2015; Kurach et al., 2015; Danihelka et al., 2016). In this work we introduce an unified view of passthrough architectures, describe their state sizeparameter size decoupling property, propose simple but effective low-dimensional parametrizations that exploit this decoupling based on low-rank or low-rank plus diagonal matrix decompositions. Our approach extends the LSTM architecture with a single projection layer by Sak et al. (2014) which has been applied to speech recognition, natural language modeling (J\u00f3zefowicz et al.", "startOffset": 161, "endOffset": 1698}, {"referenceID": 10, "context": "Modern LSTM variants (Greff et al., 2015) typically use a transform function (\u201dforget gate\u201d) f\u03c4 and carry function (\u201dinput gate\u201d) f\u03c4 independent of each other.", "startOffset": 21, "endOffset": 41}, {"referenceID": 9, "context": "This choice is used in the original LSTM of Hochreiter & Schmidhuber (1997) and in the Deep Residual Network2 of He et al. (2015). We denote the state passthrough as convex if f\u03c4 (x(t \u2212 1), t, u, \u03b8) = 1\u2297n\u0302 \u2212 f\u03b3(x(t \u2212 1), t, u, \u03b8).", "startOffset": 113, "endOffset": 130}, {"referenceID": 6, "context": "where g is an element-wise activation function, usually the ReLU (Glorot et al., 2011) or the hyperbolic tangent, \u03c3 is the element-wise logistic sigmoid, and \u2200t \u2208 1, .", "startOffset": 65, "endOffset": 86}, {"referenceID": 31, "context": "A similar approach has been proposed for the LSTM architecture by Sak et al. (2014), although they force the the R matrices to be the same for all the functions of the state transition, while we allow each parameter matrix to be parametrized independently by a pair of R and L matrices.", "startOffset": 66, "endOffset": 84}, {"referenceID": 31, "context": "Refer to Saunderson et al. (2012) and Ning et al.", "startOffset": 9, "endOffset": 34}, {"referenceID": 30, "context": "(2012) and Ning et al. (2015) for a review.", "startOffset": 11, "endOffset": 30}, {"referenceID": 34, "context": "We used dropout (Srivastava et al., 2014) in order to achieve regularization. We applied standard dropout layers with dropout probability p = 0.2 just before the input-to-state layer and p = 0.5 just before the state-to-output layer. We also applied dropout inside each hidden layer in the following way: we inserted dropout layers with p = 0.3 inside both the proposal function and the transform function, immediately before both the R matrices and the L matrices, totaling to four dropout layers per hidden layer, although the random dropout matrices are shared between proposal and transform functions. Dropout applied this way does not disrupt the state passthrough, thus it does not cause a reduction of memory capacity during training. We further applied L2-regularization with coefficient \u03bb = 1\u00d7 10\u22123 per example on the hidden-to-output parameter matrix. We also used batch normalization (Ioffe & Szegedy, 2015) after the input-to-state matrix and after each parameter matrix in the hidden layers. Parameter matrices are randomly initialized using an uniform distribution with scale equal to \u221a 6/a where a is the input dimension. Initial bias vectors are all initialized at zero except for those of the transform functions in the hidden layers, which are initialized at \u22121.0. We trained to minimize the sum of the per-class L2-hinge loss plus the L2-regularization cost (Tang, 2013). Optimization was performed using Adam (Kingma & Ba, 2014) with standard hyperparameters, learning rate starting at 3\u00d7 10\u22123 halving every three epochs without validation improvements. Mini-batch size was equal to 100. Code is available online3. We ran our experiments on a machine with a 24 core Intel(R) Xeon(R) CPU X5670 2.93GHz, 24 GB of RAM. We did not use a GPU. Training took approximately 4 hours . We obtained perfect training accuracy and 98.83% test accuracy. While this result does not reach the state of the art for this task (99.13% test accuracy with unsupervised dimensionality reduction reported by Tang (2013)), it is still relatively close.", "startOffset": 17, "endOffset": 2017}, {"referenceID": 0, "context": "We applied the Low-rank and Low-rank plus diagonal GRU architectures to a subset of sequential benchmarks described in the Unitary Evolution Recurrent Neural Networks article by Arjovsky et al. (2015), specifically the memory task, the addition task and the sequential randomly permuted MNIST task.", "startOffset": 178, "endOffset": 201}, {"referenceID": 0, "context": "We applied the Low-rank and Low-rank plus diagonal GRU architectures to a subset of sequential benchmarks described in the Unitary Evolution Recurrent Neural Networks article by Arjovsky et al. (2015), specifically the memory task, the addition task and the sequential randomly permuted MNIST task. For the memory tasks, we also considered two different variants proposed by Danihelka et al. (2016) and Henaff et al.", "startOffset": 178, "endOffset": 399}, {"referenceID": 0, "context": "We applied the Low-rank and Low-rank plus diagonal GRU architectures to a subset of sequential benchmarks described in the Unitary Evolution Recurrent Neural Networks article by Arjovsky et al. (2015), specifically the memory task, the addition task and the sequential randomly permuted MNIST task. For the memory tasks, we also considered two different variants proposed by Danihelka et al. (2016) and Henaff et al. (2016) which are hard for the uRNN architecture.", "startOffset": 178, "endOffset": 424}, {"referenceID": 5, "context": "In the variant of Danihelka et al. (2016), the length of the sequence to be remembered is randomly sampled between 1 and 10 for each sequence.", "startOffset": 18, "endOffset": 42}, {"referenceID": 13, "context": "In the variant of Henaff et al. (2016), the length of the sequence to be remembered is fixed at 10 but the model is expected to copy it after a variable number of time steps randomly chosen, for each sequence, between 1 and N = 100.", "startOffset": 18, "endOffset": 39}, {"referenceID": 38, "context": "These results surpass the uRNN and are on par with more complex architectures with time-skip connections (Zhang et al., 2016) (reported test set accuracy 94.", "startOffset": 105, "endOffset": 125}, {"referenceID": 38, "context": "These results surpass the uRNN and are on par with more complex architectures with time-skip connections (Zhang et al., 2016) (reported test set accuracy 94.0%). To our knowledge, at the time of this writing, the best result on this task is the LSTM with recurrent batch normalization by Cooijmans et al. (2016) (reported test set accuracy 95.", "startOffset": 106, "endOffset": 312}, {"referenceID": 33, "context": "Our parametrizations are alternative to convolutional parametrizations explored by Srivastava et al. (2015); He et al.", "startOffset": 83, "endOffset": 108}, {"referenceID": 12, "context": "(2015); He et al. (2015); Kaiser & Sutskever (2015).", "startOffset": 8, "endOffset": 25}, {"referenceID": 12, "context": "(2015); He et al. (2015); Kaiser & Sutskever (2015). We note that the two approaches can be combined in at least two ways:", "startOffset": 8, "endOffset": 52}], "year": 2016, "abstractText": "Deep learning consists in training neural networks to perform computations that sequentially unfold in many steps over a time dimension or an intrinsic depth dimension. Effective learning in this setting is usually accomplished by specialized network architectures that are designed to mitigate the vanishing gradient problem of naive deep networks. Many of these architectures, such as LSTMs, GRUs, Highway Networks and Deep Residual Network, are based on a single structural principle: the state passthrough. We observe that these architectures, hereby characterized as Passthrough Networks, in addition to the mitigation of the vanishing gradient problem, enable the decoupling of the network state size from the number of parameters of the network, a possibility that is exploited in some recent works but not thoroughly explored. In this work we propose simple, yet effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. We present competitive experimental results on synthetic tasks and a near state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data.", "creator": "LaTeX with hyperref package"}}}