{"id": "1603.06060", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "DASA: Domain Adaptation in Stacked Autoencoders using Systematic Dropout", "abstract": "domain adaptation occurs using adapting behaviour of continuous learning using approach we generate differences in source q when their presentation in query domain where the spatial matching samples serving both processes are dissimilar. the task constraint typically addressing or firing a learner in chosen target domain is restricted by shortage of abundant initial samples. within newer paper we write adaptation technique for domain reconstruction addressing stacked autoencoder ( ga ) providing deep object clusters ( dnn ) functioning in simplified stages : ( or ) unsupervised structure adaptation using systematic dropouts in light - cycle applications, ( ii ) knowledge co - correlated with limited quasi \u2010 labeled items in vector domain. trials experimentally examine procedures in the place of retinal nerve segmentation where both sae - dnn is processed using fixed number of labeled samples this biased source domain ( drive method ) instead identifies existing visually complex already distributed samples in target context ( stare data ). operational expense of self - eh measured using $ logloss $ in source domain is $ 0. 19 $, without and with discount towards $ 0. 30 $ and $ 0. 08 $, and $ 0. 34 $ when trained exclusively with limited samples visited home domain. the area under roc curve alignment observed respectively respectively $ 0. 90 $, $ 74. 68 $, $ 0. 92 $ and $ 0. 87 $. 91 detection efficiency of vessel replacement with dasa strongly follows our claim.", "histories": [["v1", "Sat, 19 Mar 2016 07:27:56 GMT  (3661kb,D)", "http://arxiv.org/abs/1603.06060v1", "Accepted at Asian Conference on Pattern Recognition 2015"]], "COMMENTS": "Accepted at Asian Conference on Pattern Recognition 2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["abhijit guha roy", "debdoot sheet"], "accepted": false, "id": "1603.06060"}, "pdf": {"name": "1603.06060.pdf", "metadata": {"source": "CRF", "title": "DASA: Domain Adaptation in Stacked Autoencoders using Systematic Dropout", "authors": ["Abhijit Guha Roy"], "emails": ["abhi4ssj@gmail.com,", "debdoot@ee.iitkgp.ernet.in"], "sections": [{"heading": "1. Introduction", "text": "The under-performance of learning based systems during deployment stage can be attributed to dissimilarity in distribution of samples between the source domain on which the system is initially trained and the target domain on which it is deployed. Transfer learning is an active field of research which deals with transfer of knowledge between the source and target domains for addressing this challenge and enhancing performance of learning based systems [6], when it is challenging to train a system exclusively in the target domain due to unavailability of sufficient labeled samples. While domain adaptation (DA) have been primarily developed for simple reasoning and shallow network architectures, there exist few techniques for adapting deep networks\nwith complex reasoning [4]. In this paper we propose a systematic dropout based technique for adapting a stacked autoencoder (SAE) based deep neural network (DNN) [2] for the purpose of vessel segmentation in retinal images [1]. Here the SAE-DNN is initially trained using ample number of samples in the source domain (DRIVE dataset1) to evaluate efficacy of DA during deployment in the target domain (STARE dataset2) where an insufficient number of labeled samples are available for reliable training exclusively in the target domain.\nRelated Work: Autoencoder (AE) is a type of neural network which learns compressed representations inherent in the training samples without labels. Stacked AE (SAE) is created by hierarchically connecting hidden layers to learn hierarchical embedding in compressed representations. An SAE-DNN consists of encoding layers of an SAE followed by a target prediction layer for the purpose of regression or classification. With increase in demand for DA in SAEDNNs different techniques have been proposed including marginalized training [3], via graph regularization [7] and structured dropouts [10], across applications including recognizing speech emotion [4] to fonts [9].\nChallenge: The challenge of DA is to retain nodes common across source and target domains, while adapting the domain specific nodes using fewer number of labeled samples. Earlier methods [3, 7, 10] are primarily challenged by their inability to re-tune nodes specific to the source domain to nodes specific for target domain for achieving desired performance, while they are able to only retain nodes or a thinned network which encode domain invariant hierarchical embeddings.\nApproach: Here we propose a method for DA in SAE (DASA) using systematic dropout. The two stage method adapts a SAE-DNN trained in the source domain following (i) unsupervised weight adaptation using systematic dropouts in mini-batch training with abundant unlabeled samples in target domain, and (ii) supervised fine-tuning with limited number of labeled samples in target domain. The systematic dropout per mini-batch is introduced only\n1http://www.isi.uu.nl/Research/Databases/DRIVE 2http://www.ces.clemson.edu/ ahoover/stare\n1\nar X\niv :1\n60 3.\n06 06\n0v 1\n[ cs\n.C V\n] 1\n9 M\nar 2\n01 6\nin the representation encoding (hidden) layers and is guided by a saliency map defined by response of the neurons in the mini-batch under consideration. Error backpropagation and weight updates are however across all nodes and not only restricted to the post dropout activated nodes, contrary to classical randomized dropout approaches [8]. Thus having different dropout nodes across different mini-batches and weight updates across all nodes in the network, ascertains refinement of domain specific hierarchical embeddings while preserving domain invariant ones.\nThe problem statement is formally introduced in Sec. 2. The methodology is explained in Sec. 3. The experiments are detailed in Sec. 4, results are presented and discussed in Sec. 5 with conclusion in Sec. 6."}, {"heading": "2. Problem Statement", "text": "Let us consider a retinal image represented in the RGB color space as I, such that the pixel location x \u2208 I has the color vector c(x) = {r(x), g(x), b(x)}. N(x) is a neighborhood of pixels centered at x. The task of retinal vessel segmentation can be formally defined as assigning a class label y \u2208 {vessel,background} using a hypothesis model H(I,x, N(x); {I}train). When the statistics of samples in I is significantly dissimilar from Itrain, the performance of H(\u00b7) is severely affected. Generally {I}train is referred to\nas the source domain and I or the set of samples used during deployment belong to the target domain. The hypothesisH(\u00b7) which optimally defines source and target domains are also referred to as Hsource and Htarget . DA is formally defined as a transformationHsource\nDA\u2212\u2192 Htarget as detailed in Fig. 1."}, {"heading": "3. Exposition to the Solution", "text": "Let us consider the source domain as Dsource with abundant labeled samples to train an SAE-DNN (Hsource) for the task of retinal vessel segmentation, and a target domain Dtarget with limited number of labeled samples and ample unlabeled samples, insufficient to learn Htarget reliably as illustrated in Fig. 1. Dsource and Dtarget are closely related, but exhibiting distribution shifts between samples of the source and target domains, thus resulting in underperformance of Hsource in Dtarget as also illustrated in Fig. 1. The technique of generating Hsource using Dsource , and subsequently adapting Hsource to Htarget via systematic dropout usingDtarget is explained in the following sections."}, {"heading": "3.1. SAE-DNN learning in the source domain", "text": "AE is a single layer neural network that encodes the cardinal representations of a pattern p = {pk} onto a trans-\nformed spaces y = {yj} with w = {wjk} denoting the connection weights between neurons, such that\ny = fNL([w b].[p ; 1]) (1)\nwhere the cardinality of y denoted as |y| = J \u00d7 1, |p| = K\u00d71, |w| = J\u00d7K, and b is termed as the bias connection with |b| = J \u00d7 1. We choose fNL(\u00b7) to be a sigmoid function defined as fNL(z) = 1/(1 + exp(\u2212z)). AE is characteristic with another associated function which is generally termed as the decoder unit such that\np\u0302 = fNL([w \u2032 b\u2032].[y ; 1]) (2)\nwhere |p\u0302| = |p| = K\u00d71, |w\u2032| = K\u00d7J and |b\u2032| = K\u00d71. When |y| << |{pn}|, this network acts to store compressed representations of the pattern p encoded through the weights W = {w,b,w\u2032,b\u2032}. However the values of elements of these weight matrices are achieved through learning, and without the need of having class labels of the patterns p, it follows unsupervised learning using some optimization algorithm [5], viz. stochastic gradient descent.\n{w,b,w\u2032,b\u2032} = arg min {w,b,w\u2032,b\u2032} (J(W)) (3)\nsuch that J(\u00b7) is the cost function used for optimization over all available patterns pn \u2208 {p(x),x \u2208 I}\nJ(W) = \u2211 n \u2016pn \u2212 p\u0302n\u2016+ \u03b2|\u03c1\u2212 \u03c1\u0302n| (4)\nwhere \u03b2 regularizes the sparsity penalty, \u03c1 is the imposed sparsity and \u03c1\u0302n is the sparsity observed with the nth pattern in the mini-batch.\nThe SAE-DNN consists of L = 2 cascade connected AEs followed by a softmax regression layer known as the target layer with t as its output. The number of output nodes in this layer is equal to the number of class labels such that |t| = |\u2126| and the complete DNN is represented as\nt = fNL ([w3 b3]. [fNL ([w2 b2]. [fNL ([w1 b1]. [p 1] T ) 1 ]T) 1 ]T) (5) where {W1 = {w1,b1},W2 = {w2,b2}} are the pretrained weights of the network obtained from the earlier section. The weights W3 = {w3,b3} are randomly initialized and convergence of the DNN is achieved through supervised learning with the cost function\nJ(W) = \u2211 m \u2016tm \u2212 \u2126m\u2016 (6)\nduring which all the weights W = {W1 = {w1,b1},W2 = {w2,b2},W3 = {w3,b3}} are updated to completely tune the DNN."}, {"heading": "3.2. SAE-DNN adaptation in the target domain", "text": "Unupervised adaptation of SAE weights using systematic dropouts: The first stage of DA utilizes abundant unlabeled samples available in target domain to retain nodes which encode domain invariant hierarchical embeddings while re-tuning the nodes specific in source domain to those specific in target domain. We follow the concept of systematic node drop-outs during training [8]. The number of layers and number of nodes in the SAE-DNN however remains unchanged during domain adaptation. Fig. 2 illustrates the concept.\nWeights connecting each of the hidden layers is imported from the SAE-DNN trained in Dsource are updated in this stage using an auto-encoding mechanism. When each minibatch in Dtarget is fed to this AE with one of the hidden layers from the SAE-DNN; some of the nodes in the hidden layer exhibit high response with most of the samples in the mini-batch, while some of the nodes exhibit low response. The nodes which exhibit high-response in the mini-batch are representative of domain invariant embeddings which need to be preserved, while the ones which exhibit lowresponse are specific to Dsource and need to be adapted to Dtarget . We set a free parameter \u03c4 \u2208 [0, 1] defined as the transfer coefficient used for defining saliency metric ({slj} \u2208 s) for the jth node in the lth layer as\nslj = { 1, if ylj \u2265 \u03c4 . 0, otherwise.\n(7)\nwhere ylj \u2208 y as in (1), and we redefine (2) while preserving (4) and the original learning rules.\np\u0302 = fNL([w \u2032 b\u2032].[y.s ; 1]) (8)\nSupervised fine tuning with limited number of labeled samples: The SAE-DNN with weight embeddings updated in the previous stage is now fine tuned using limited number of labeled samples in Dtarget following procedures in (5) and (6)."}, {"heading": "4. Experiments", "text": "SAE-DNN architecture: We have a two-layered architecture with L = 2 where AE1 consists of 400 nodes and AE2 consists of 100 nodes. The number of nodes at input is 15\u00d7 15\u00d7 3 corresponding to the input with patch size of 15 \u00d7 15 in the color retinal images in RGB space. AEs are unsupervised pre-trained with learning rate of 0.3, over 50 epochs, \u03b2 = 0.1 and \u03c1 = 0.04. Supervised weight refinement of the SAE-DNN is performed with a learning rate of 0.1 over 200 epochs. The training configuration of learning rate and epochs were same in the source and target domains, with \u03c4 = 0.1.\nSource and target domains: The SAE-DNN is trained in Dsource using 4% of the available patches from the 20 images in the training set in DRIVE dataset. DA is performed in Dtarget using (i) 4% of the available patches in 10 unlabeled images for unsupervised adaptation using systematic dropout and (ii) 4% of the available patches in 3 labeled images for fine tuning.\nBaselines and comparison: We have experimented with the following SAE-DNN baseline (BL) configurations and training mechanisms for comparatively evaluating efficacy of DA: BL1: SAE-DNN trained in source domain and deployed in target domain without DA; BL2: SAE-DNN trained in target domain with limited samples and deployed in target domain."}, {"heading": "5. Results and Discussion", "text": "The results comparing performance of the SAE-DNN are reported in terms of logloss and area under ROC curve as presented in Table 1, and DA aspects in Fig. 3.\nHierarchical embedding in representations learned across domains: AEs are typically characteristic of learning hierarchical embedded representations. The first level of embedding represented in terms of w1 in Fig. 3(g) is over-complete in nature, exhibiting substantial similarity between multiple sets of weights which promotes sparsity in the nature of w2 in Fig. 3(h). Some of these weight kernels are domain invariant, and as such remain preserved after DA as observed for w1 in Fig. 3(i) and for w2 in Fig. 3(j). Some of the kernels which are domain specific, exhibit significant dissimilarity in w1 and w2 between source domain in Figs. 3(g) and 3(h) vs. target domain in Figs. 3(i) and 3(j). These are on account of dissimilarity of sample statistics in\nthe domains as illustrated earlier in Fig. 1 and substantiates DASA of being able to retain nodes common across source and target domains, while re-tuning domain specific nodes.\nAccelerated learning with DA: The advantage with DA is the ability to transfer knowledge from source domain to learn with fewer number of labeled samples and ample number of unlabeled samples available in the target domain when directly learning in the target domain does not yield desired performance. Figs. 3(k) and 3(l) compare the learning of w1 and w2 using ample unlabeled data in source and target domain exclusively vs. DA. Fig. 3(m) presents the acceleration of learning with DA in target domain vs. learning exclusively with insufficient number of labeled samples.\nImportance of transfer coefficient: The transfer coefficient \u03c4 drives quantum of knowledge transfer from the source to target domains by deciding on the amount of nodes to be dropped while adapting with ample unlabeled samples. This makes it a critical parameter to be set in DASA to avoid over-fitting and negative transfers as illustrated in Table. 2 where optimal \u03c4 = 0.1. Generally \u03c4 \u2208 [0, 1] with \u03c4 \u2192 0 being associated with large margin transfer between domains when they are not very dissimilar, and \u03c4 \u2192 1 being associated otherwise."}, {"heading": "6. Conclusion", "text": "We have presented DASA, a method for knowledge transfer in an SAE-DNN trained with ample labeled samples in source domain for application in target domain with less number of labeled samples insufficient to directly train to solve the task in hand. DASA is based on systematic droupout for adaptation being able to utilize (i) ample unlabeled samples and (ii) limited amount of labeled samples in target domain. We experimentally provide its efficacy to solve the problem of vessel segmentation when trained with DRIVE dataset (source domain) and adapted to deploy on STARE dataset (target domain). It is observed that DASA outperforms the different baselines and also exhibits accelerated learning due to knowledge transfer. While systematic drouput is demonstrated on an SAE-DNN in DASA, it can be extended to other deep architectures as well."}, {"heading": "Acknowledgement", "text": "We acknowledge NVIDIA for partially supporting this work through GPU Education Center at IIT Kharagpur."}], "references": [{"title": "Retinal imaging and image analysis", "author": ["M.D. Abr\u00e0moff", "M.K. Garvin", "M. Sonka"], "venue": "IEEE Rev. Biomed. Engg.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Found., Trends, Mach. Learn.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K. Weinberger", "F. Sha"], "venue": "In Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Autoencoderbased unsupervised domain adaptation for speech emotion recognition", "author": ["J. Deng", "Z. Zhang", "F. Eyben", "B. Schuller"], "venue": "IEEE Signal Process. Let.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Neural Networks and Learning Machines", "author": ["S. Haykin"], "venue": "Pearson Education,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. Knowledge., Data Engg.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Marginalized denoising autoencoder via graph regularization for domain adaptation", "author": ["Y. Peng", "S. Wang", "B.-L. Lu"], "venue": "In Proc. Neural Inf. Process. Sys.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1958}, {"title": "Real-world font recognition using deep network and domain adaptation", "author": ["Z. Wang", "J. Yang", "H. Jin", "E. Shechtman", "A. Agarwala", "J. Brandt", "T.S. Huang"], "venue": "In Proc. Int. Conf. Learning Representations, page arXiv:1504.00028,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Fast easy unsupervised domain adaptation with marginalized structured dropout", "author": ["Y. Yang", "J. Eisenstein"], "venue": "Proc. Assoc., Comput. Linguistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "Transfer learning is an active field of research which deals with transfer of knowledge between the source and target domains for addressing this challenge and enhancing performance of learning based systems [6], when it is challenging to train a system exclusively in the target domain due to unavailability of sufficient labeled samples.", "startOffset": 208, "endOffset": 211}, {"referenceID": 3, "context": "While domain adaptation (DA) have been primarily developed for simple reasoning and shallow network architectures, there exist few techniques for adapting deep networks with complex reasoning [4].", "startOffset": 192, "endOffset": 195}, {"referenceID": 1, "context": "In this paper we propose a systematic dropout based technique for adapting a stacked autoencoder (SAE) based deep neural network (DNN) [2] for the purpose of vessel segmentation in retinal images [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "In this paper we propose a systematic dropout based technique for adapting a stacked autoencoder (SAE) based deep neural network (DNN) [2] for the purpose of vessel segmentation in retinal images [1].", "startOffset": 196, "endOffset": 199}, {"referenceID": 2, "context": "With increase in demand for DA in SAEDNNs different techniques have been proposed including marginalized training [3], via graph regularization [7] and structured dropouts [10], across applications including recognizing speech emotion [4] to fonts [9].", "startOffset": 114, "endOffset": 117}, {"referenceID": 6, "context": "With increase in demand for DA in SAEDNNs different techniques have been proposed including marginalized training [3], via graph regularization [7] and structured dropouts [10], across applications including recognizing speech emotion [4] to fonts [9].", "startOffset": 144, "endOffset": 147}, {"referenceID": 9, "context": "With increase in demand for DA in SAEDNNs different techniques have been proposed including marginalized training [3], via graph regularization [7] and structured dropouts [10], across applications including recognizing speech emotion [4] to fonts [9].", "startOffset": 172, "endOffset": 176}, {"referenceID": 3, "context": "With increase in demand for DA in SAEDNNs different techniques have been proposed including marginalized training [3], via graph regularization [7] and structured dropouts [10], across applications including recognizing speech emotion [4] to fonts [9].", "startOffset": 235, "endOffset": 238}, {"referenceID": 8, "context": "With increase in demand for DA in SAEDNNs different techniques have been proposed including marginalized training [3], via graph regularization [7] and structured dropouts [10], across applications including recognizing speech emotion [4] to fonts [9].", "startOffset": 248, "endOffset": 251}, {"referenceID": 2, "context": "Earlier methods [3, 7, 10] are primarily challenged by their inability to re-tune nodes specific to the source domain to nodes specific for target domain for achieving desired performance, while they are able to only retain nodes or a thinned network which encode domain invariant hierarchical embeddings.", "startOffset": 16, "endOffset": 26}, {"referenceID": 6, "context": "Earlier methods [3, 7, 10] are primarily challenged by their inability to re-tune nodes specific to the source domain to nodes specific for target domain for achieving desired performance, while they are able to only retain nodes or a thinned network which encode domain invariant hierarchical embeddings.", "startOffset": 16, "endOffset": 26}, {"referenceID": 9, "context": "Earlier methods [3, 7, 10] are primarily challenged by their inability to re-tune nodes specific to the source domain to nodes specific for target domain for achieving desired performance, while they are able to only retain nodes or a thinned network which encode domain invariant hierarchical embeddings.", "startOffset": 16, "endOffset": 26}, {"referenceID": 7, "context": "Error backpropagation and weight updates are however across all nodes and not only restricted to the post dropout activated nodes, contrary to classical randomized dropout approaches [8].", "startOffset": 183, "endOffset": 186}, {"referenceID": 4, "context": "However the values of elements of these weight matrices are achieved through learning, and without the need of having class labels of the patterns p, it follows unsupervised learning using some optimization algorithm [5], viz.", "startOffset": 217, "endOffset": 220}, {"referenceID": 7, "context": "We follow the concept of systematic node drop-outs during training [8].", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "We set a free parameter \u03c4 \u2208 [0, 1] defined as the transfer coefficient used for defining saliency metric ({sj} \u2208 s) for the j node in the l layer as", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "Generally \u03c4 \u2208 [0, 1] with \u03c4 \u2192 0 being associated with large margin transfer between domains when they are not very dissimilar, and \u03c4 \u2192 1 being associated otherwise.", "startOffset": 14, "endOffset": 20}], "year": 2016, "abstractText": "Domain adaptation deals with adapting behaviour of machine learning based systems trained using samples in source domain to their deployment in target domain where the statistics of samples in both domains are dissimilar. The task of directly training or adapting a learner in the target domain is challenged by lack of abundant labeled samples. In this paper we propose a technique for domain adaptation in stacked autoencoder (SAE) based deep neural networks (DNN) performed in two stages: (i) unsupervised weight adaptation using systematic dropouts in mini-batch training, (ii) supervised fine-tuning with limited number of labeled samples in target domain. We experimentally evaluate performance in the problem of retinal vessel segmentation where the SAE-DNN is trained using large number of labeled samples in the source domain (DRIVE dataset) and adapted using less number of labeled samples in target domain (STARE dataset). The performance of SAE-DNN measured using logloss in source domain is 0.19, without and with adaptation are 0.40 and 0.18, and 0.39 when trained exclusively with limited samples in target domain. The area under ROC curve is observed respectively as 0.90, 0.86, 0.92 and 0.87. The high efficiency of vessel segmentation with DASA strongly substantiates our claim.", "creator": "LaTeX with hyperref package"}}}