{"id": "1603.06127", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Sentence Pair Scoring: Towards Unified Framework for Text Comprehension", "abstract": "text review - task is sentence letter scoring, list ing the literature in various forms - - - slanted as answer number selection, inclusion, a page scoring, next utterance pairing, then every entailment or ir. g. a ranking of verb networks.", "histories": [["v1", "Sat, 19 Mar 2016 18:35:26 GMT  (29kb)", "http://arxiv.org/abs/1603.06127v1", "submitted as long paper to ACL 2016"], ["v2", "Thu, 28 Apr 2016 03:10:26 GMT  (29kb)", "http://arxiv.org/abs/1603.06127v2", "submitted as long paper to ACL 2016"], ["v3", "Fri, 6 May 2016 22:17:36 GMT  (29kb)", "http://arxiv.org/abs/1603.06127v3", "submitted as paper to CoNLL 2016"], ["v4", "Tue, 17 May 2016 14:08:38 GMT  (30kb)", "http://arxiv.org/abs/1603.06127v4", "submitted as paper to CoNLL 2016"]], "COMMENTS": "submitted as long paper to ACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["petr baudi\\v{s}", "jan pichl", "tom\\'a\\v{s} vysko\\v{c}il", "jan \\v{s}ediv\\'y"], "accepted": false, "id": "1603.06127"}, "pdf": {"name": "1603.06127.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["baudipet@fel.cvut.cz", "sedivjan@fel.cvut.cz"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n06 12\n7v 1\n[ cs\n.C L\n] 1\n9 M\nar 2\nWe argue that such tasks are similar from the model perspective (especially in the context of high-capacity deep neural models) and propose new baselines by comparing the performance of popular convolutional, recurrent and attentionbased neural models across many Sentence Pair Scoring tasks and datasets. We discuss the problem of evaluating randomized models, propose a statistically grounded methodology, and attempt to improve comparisons by releasing new datasets that are much harder than some of the currently used well explored benchmarks.\nTo address the current research fragmentation in a future-proof way, we introduce a unified open source software framework with easily pluggable models, allowing easy evaluation on a wide range of semantic natural language tasks. This allows us to outline a path towards a universal machine learned semantic model for machine reading tasks. We support this plan by experiments that demonstrate reusability of models trained on different tasks, even across corpora of very different nature."}, {"heading": "1 Introduction", "text": "A typical NLP machine learning task involves classifying a sequence of tokens such as a sen-\ntence or a document, i.e. approximating a function f1(s) \u2208 [0, 1] (where f1 may determine a domain, sentiment, etc.). But there is a large class of problems that involve classifying a pair of sentences, f2(s0, s1) \u2208 R (where s0, s1 are sequences of tokens, typically sentences).\nTypically, the function f2 represents some sort of semantic similarity, that is whether (or how much) the two sequences are semantically related. This formulation allows f2 to be a measure for tasks as wide as topic relatedness, paraphrasing, degree of entailment, a pointwise ranking task for answer-bearing sentences or next utterance classification.\nIn this work, we adopt the working assumption that there exist certain universal f2 type measures that may be successfuly applied to a wide variety of semantic similarity tasks \u2014 in the case of fully differentiable models, both architecture-wise and weight-wise, trained to represent universal semantic comprehension of sentences and adapted to the given task by just fine-tuning or adapting the final dense layer. Our argument for preferring f2 to f1 in this pursuit is the fact that the other sentence in the pair is essentially a very complex label when training the sequence model, which can therefore discern semantically rich structures and dependencies.\nDetermining and demonstrating such universal semantic comprehension models across multiple tasks remains a few steps ahead, since the research landscape is fragmented in this regard, with model research typically reported within the context of just a single f2-type task, each dataset requiring sometimes substantial engineering work before measurements are possible, and results reported in ways that make meaningful model comparisons problematic. The primary purpose of this work is to unify research within a single framework that employs task-independent mod-\nels, task-specific adaptation modules and unified statistically appropriate methodology for reportings. To demonstrate the feasibility of pursuing universal, task-independent f2 models, we show that even simple neural models learn universal semantic comprehension as we improve their performance (even on relatively large datasets) by employing cross-task transfer learning.\nThe paper is structured as follows. In Sec. 2, we outline possible specific f2 tasks and available datasets; in Sec. 3, we survey the popular basic baselines and the simpler currently employed neural models on these tasks; finally, in Sec. 4, we present model-task performances within a unified framework to establish the watermark for future research as well as gain insight into the suitability of models across a variety of tasks. In Sec. 5, we demonstrate that transfer learning across tasks is helpful to powerfully seed models. We conclude with Sec. 6, summarizing our findings and outlining several future research directions."}, {"heading": "2 Tasks and Datasets", "text": "The tasks we are aware of that can be phrased as f2-type problems are listed below. In general, we have decided to primarily focus on tasks that have reasonably large and realistically complex datasets freely available. On the contrary, we have explicitly avoided datasets that have licence restrictions on availability or commercial usage."}, {"heading": "2.1 Answer Sentence Selection", "text": "Given a question and a set of candidate answerbearing sentences, the task here is to rank higher sentences that are more likely to contain the answer to the question. As it is fundamentally an Information Retrival task in nature, the model performance is commonly evaluated in terms of Mean Average Precision (MAP) and Mean Reciprocial Rank (MRR).\nThis task is popular in the NLP research community thanks to the dataset introduced in (Wang et al., 2007) which we refer to as wang, with six papers published between February 2015 and 2016 alone and neural models substantially improving over classical approaches based primarily on parse tree edits.1 We can certainly call it the main research testbed for f2-style task models.\n1 http://aclweb.org/aclwiki/index.php? title=Question_Answering_(State_of_the_ art)\nThis task has also immediate applications e.g. in Question Answering systems.\nIn the context of practical applications, the sofar standard wang dataset has several downsides we observed when tuning and evaluating our models, illustrated numerically in Fig. 1 \u2014 the set of candidate sentences is often very small and quite uneven (which also makes rank-based measures unstable) and the total number of questions as well as individual sentence pairs is relatively small. Furthermore, the validation and test set are very small which makes for noisy performance measurements; the splits also seem quite different in the nature of questions since we see minimum correlation between performance on the validation and set tests, which calls the parameter tuning procedures and epoch selection for early stopping into question.\nAlternative datasets WikiQA (Yang et al., 2015) and InsuranceQA (Tan et al., 2015) were proposed, but are encumbered by licence restrictions. Furthermore, we speculate that they may suffer from many of the problems above2 (even if they are somewhat larger) and they did not gain much traction in the research community.\nTo alleviate the problems listed above, we are introducing a new dataset yodaqa/curatedv2 based on the curatedv2 question dataset (introduced in (Baudis\u030c and S\u030cedivy\u0301, 2015), further denoisified by Mechanical Turkers) with candidate sentences as retrieved by the YodaQA question answering system (Baudis\u030c, 2015) from English Wikipedia. For models that can make substantial use of more data, we also include an even larger dataset yodaqa/large2470 (though it has seen less gold standard denoisification), with its splits as supersets of the smaller splits.3\nFig. 1 compares the critical characteristics of the datasets. Furthermore, as apparent below, the baseline performances on the newly proposed datasets are much lower, which suggests that future improvements will be more apparent in evaluation.\n2For example, InsuranceQA is effectively a classification task rather than a ranking task, which we do not find as appealing in the context of practical applications.\n3Note that the wang and yodaqa datasets however share a common ancestry regarding the set of questions and there may be some overlaps, even across train and test splits. Therefore, mixing training and evaluation on wang and yodaqa datasets within a single model instance is not advisable."}, {"heading": "2.2 Next Utterance Ranking", "text": "(Lowe et al., 2015) proposed a new large-scale and realistic dataset for an f2-style task of ranking candidates for the next utterance in a chat dialog, given the dialog context. The technical formulation of the task is the same as for Answer Sentence Selection, but semantically, choosing the best followup involves different concerns than choosing an answer-bearing sentence.\nThe newly proposed Ubuntu Dialogue dataset is based on IRC chat logs of the Ubuntu community technical support channels and contains casually typed interactions regarding computer-related problems.4 While the training set consists of individual labelled pairs of token sequences, evaluation is done by ranking 10 followups to given message(s) that are potentially relatively long (even more than 200 tokens).\nOur primary motivation for using this dataset is its size. The numerical characteristics of this dataset are shown in Table 1.5 We use the v2 version of the dataset.6 Research published on this dataset so far relies on simple neural models. (Lowe et al., 2015) (Kadlec et al., 2015)"}, {"heading": "2.3 Semantic Textual Similarity", "text": "One of the canonical problems for f2-type tasks is the STS track of the SemEval conferences (Agirrea et al., 2015). This is an annual competition of scoring pairs of sentences from 0 to 5 with the objective of maximizing correlation (Pearson\u2019s r) with manually annotated gold standard; the data is composed from diverse per-source splits that range from almost word-by-word paraphrases of newspaper titles to loosely related user forum\n4In a manner, they resemble tweet data, but without the length description and with heavily technical jargon, command sequences etc.\n5As in past papers, we use only the first 1M pairs (10%) of the training set.\n6 https://github.com/rkadlec/ ubuntu-ranking-dataset-creator\nquestions. Every year, the past years are used as the training set.\nSince 2016 results weren\u2019t released at the time of writing this paper, we use 2012 to 2014 as the training set, 2014 tweet-news split as the validation set, and 2015 splits (and the mean score across splits) as the testing set, making our results comparable to 2015 competition entrants. This means 3000 training pairs, 750 validation pairs and 8500 testing pairs. Contrary to Answer Sentence Selection, state-of-art methods are based on parse tree alignments (Sultan et al., 2015) and weren\u2019t beaten by neural models yet.\nWe also report results for this task on another dataset from the SemEval conferences, SICK2014 (Marelli et al., 2014). In contrast to the STS track, it is geared at specifically benchmarking semantic compositional methods, aiming to capture only similarities on purely language and common knowledge level, without relying on domain knowledge, and there are no named entities or multi-word idioms. It consists of 4500 training pairs, 500 validation pairs and 4927 testing pairs."}, {"heading": "2.4 Paraphrase Identification", "text": "Finally, a popular separately considered task is Paraphrase Identification, which can be thought of as a special case of Semantic Textual Scoring, but the task goal is binary classification rather than regression of a score on continuous scale.\nThe canonical dataset is the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005), consisting of 4076 training and 1725 test pairs; 2/3 of the samples are labelled as 1 (is-a-paraphrase). State-of-art model uses an ensemble of handcrafted overlap features (Ji and Eisenstein, 2013) and weren\u2019t beaten by neural models (Cheng and Kartsaklis, 2015) (He et al., 2015) yet.7\n7 http://aclweb.org/aclwiki/index.php? title=Paraphrase_Identification_(State_ of_the_art)"}, {"heading": "2.5 Other", "text": "Due to the very wide scope of the field, we leave some popular tasks and datasets as future work. In particular, this concerns the Recognizing Textual Entailment task (supported by the SNLI dataset) (Bowman et al., 2015) and the problem of memory selection in Memory Networks (supported by the baBi dataset) (Weston et al., 2015). A more realistic large paraphrasing dataset based on the AskUbuntu Stack Overflow forum had been recently proposed (Lei et al., 2015)."}, {"heading": "3 Models", "text": "As our goal is a universal text comprehension model, architecture-wise we focus on neural network models. We assume that the sequence is transformed using N -dimensional word embeddings on input, and employ models that produce a pair of sentence embeddings E0, E1 from the sequences of word embeddings e0, e1. Unless noted otherwise, a Siamese architecture is used that shares weights among both sentenes.\nA scorer module that compares the E0, E1 sentence embeddings to produce a scalar result is connected to the model; for specific task-model configurations, we use either the dot-product module E0 \u00b7 ET1 (representing non-normalized vector angle, as in e.g. (Yu et al., 2014) or (Weston et al., 2014)) or the MLP module that takes elementwise product and sum of the embeddings and feeds them to a two-layer perceptron with hidden layer of width 2N (as in e.g. (Tai et al., 2015)).8 For the STS task, we follow this by score regression using class interpolation as in (Tai et al., 2015).\nWhen training for a ranking task (Answer Sentence Selection), we use the bipartite ranking version of Ranknet (Burges et al., 2005) as the objective; when training for STS task, we use Pearson\u2019s r formula as the objective; for binary classification tasks, we use the binary crossentropy objective."}, {"heading": "3.1 Baselines", "text": "In order to anchor the reported performance, we report several basic methods. Weighed word overlaps metrics TF-IDF and BM25 (Robertson et al., 1995) are inspired by IR research and provide strong baselines for many tasks. We treat s0\n8The motivation is to capture both angle and euclid distance in multiple weighed sums. Past literature uses absolute difference rather than sum, but both performed equally in our experiments and we adopted sum for technical reasons.\nas the query and s1 as the document, counting the number of common words and weighing them appropriately. IDF is determined on the training set.\nThe avg metric represents the baseline method when using word embeddings that proved successful e.g. in (Yu et al., 2014) or (Weston et al., 2014), simply taking the mean vector of the word embedding sequence and training an U weight matrix N\u00d72N that projects both embeddings to the same vector space, Ei = tanh(U \u00b7 e\u0304i), where the MLP scorer compares them. During training, p = 1/3 standard (elementwise) dropout is applied on the input embeddings.\nA simple extension of the above are the DAN Deep Averaging Networks (Iyyer et al., 2015), which were shown to adequately replace much more complex models in some tasks. Two dense perceptron layers are stacked between the mean and projection, relu is used instead of tanh as the non-linearity, and word-level dropout is used instead of elementwise dropout."}, {"heading": "3.2 Recurrent Neural Networks", "text": "RNN with memory units are popular models for processing sentenes (Tan et al., 2015) (Lowe et al., 2015) (Bowman et al., 2015). We use a bidirectional network with 2N GRU memory units9 (Cho et al., 2014) in each direction; the final unit states are summed across the two per-direction GRUs to yield a 2N vector representation of the sentence. Like in the avg baseline, a projection matrix is applied on this representation and final vectors compared by an MLP scorer. We have found that applying massive dropout p = 4/5 both on the input and output of the network helps to avoid overfitting even early in the training."}, {"heading": "3.3 Convolutional Neural Networks", "text": "CNN with sentence-wide pooling layer are also popular models for processing sentences (Yu et al., 2014) (Tan et al., 2015) (Severyn and Moschitti, 2015) (He et al., 2015) (Kadlec et al., 2015). We apply a multi-channel convolution (Kim, 2014) with single-token channel of N convolutions and 2, 3, 4 and 5-token channels of N/2 convolutions each, relu transfer function, max-pooling over the whole sentence, and as above a projection to shared space and an MLP scorer. Dropout\n9While the LSTM architecture is more popular, we have found the GRU results are equivalent while the number of parameters is reduced.\np = 1/2 is applied both on the input and output of the convolution network."}, {"heading": "3.4 RNN-CNN Model", "text": "The RNN-CNN model aims to combine both recurrent and convolutional networks by using the memory unit states in each token as the new representation of the token which is then fed to the convolutional network. Inspired by (Tan et al., 2015), the aim of this model is to allow the RNN to model long-term dependencies and model contextual representations of words, while taking advantage of the CNN and pooling operation for crisp selection of the gist of the sentence. We use the same parameters as for the individual models, except that we find applying dropout detrimental and need to reduce the number of parameters by using only N memory units per direction."}, {"heading": "3.5 Attention-Based Models", "text": "The idea of attention models is to attend preferrentially to some parts of the sentence when building its representation (Hermann et al., 2015) (Tan et al., 2015) (dos Santos et al., 2016) (Rockta\u0308schel et al., 2015). There are many ways to model attention, as the initial proof of concept we adopt the (Tan et al., 2015) model attn1511 which is conceptually simple and easy to implement. It asymmetrically extends the RNN-CNN model by extra links from s0 CNN output to the post-recurrent representation of each s1 token, determining an attention level for each token by weighed sum of the token vectors, and focusing on the relevant s1 segment by transforming the attention levels using softmax and multiplying the token representations by the attention levels before they are fed to the convolutional network.\nConvolutional network weights are not shared between the two sentences and the convolutional network output is not projected before applying the MLP scorer. The CNN used here is singlechannel with 2N convolution filters 3 tokens wide.\n4 Model Performance\n4.1 dataset-sts framework\nTo easily implement models, dataset loaders and task adapters in a modular fashion so that any model can be easily run on any f2-type task, we have created a new software package dataset-sts that integrates a variety of datasets, a Python dataset adapter PySTS and a\nPython library for easy construction of deep neural NLP models for semantic sentence pair scoring KeraSTS that uses the Keras machine learning library (Chollet, 2015). The framework is available for other researchers as open source on GitHub.10"}, {"heading": "4.2 Experimental Setting", "text": "We use N = 300 dimensional GloVe embeddings matrix pretrained on Wikipedia 2014 + Gigaword 5 (Pennington et al., 2014) that we keep adaptable during training; words in the training set not included in the pretrained model are initialized by random vectors uniformly sampled from [\u22120.25,+0.25] to match the embedding standard deviation.\nWord overlap is an important feature in many f2-type tasks (Yu et al., 2014) (Severyn and Moschitti, 2015), especially when the sentences may contain named entities, numeric or other data for which no embedding is available. As a workaround, ensemble of world overlap count and neural model score is typically used to produce the final score. We try to adopt a more flexible approach, extending the embedding of each input token by several extra dimensions carrying boolean flags \u2014 bigram overlap, unigram overlap (except stopwords and interpunction), and whether the token starts with a capital letter or is a number.\nParticular hyperparameters are tuned primarily on the yodaqa/curatedv2 dataset unless noted otherwise in the respective results table caption. We apply 10\u22124 L2 regularization and use Adam optimization with standard parameters (Kingma and Ba, 2014). In the answer selection tasks, we train on 1/4 of the dataset in each epoch. After training, we use the epoch with best validation performance; sadly, we typically observe heavy overfitting as training progresses and rarely use a model from later than a couple of epochs."}, {"heading": "4.3 Evaluation Methodology", "text": "We report model performance averaged across 16 training runs (with different seeds). A consideration we must emphasize is that randomness plays a large role in neural models both in terms of randomized weight initialization and stochastic dropout. For example, the typical methodology for reporting results on the wang dataset is to evaluate and report a single test run after tuning on the\n10Link redacted. Src snapshot included in submission.\ndev set,11 but wang test MRR has empirical standard deviation of 0.025 across repeated runs of our attn1511 model, which is more than twice the gap between every two successive papers pushing the state-of-art on this dataset! See the \u2217-marked sample in Fig. 2 for a practical example of this phenomenon.\nFurthermore, on more complex tasks (Answer Sentence Selection in particular, see Fig. 1) the validation set performance is not a great approximator for test set performance and a strategy like picking training run with best validation performance would lead just to overfitting on the validation set.\nTo allow comparison between models (and with future models), we therefore report also 95% confidence intervals for each model performance estimate, as determined from the empirical standard deviation using Student\u2019s t-distribution.12"}, {"heading": "4.4 Results", "text": "In Fig. 2 to 5, we show the cross-task performance of our models. We can observe an effect analogous to what has been described in (Kadlec et al., 2015) \u2014 when the dataset is smaller, CNN models are preferrable, while larger dataset allows RNN models to capture the text comprehension task bet-\n11Confirmed by personal communication with paper authors.\n12Over larger number of samples, this estimate converges to the normal distribution confidence levels. Note that the confidence interval covers the range of the true expected performance, not performance individually measured samples.\nter. IR baselines provide strong competition and finding new ways to ensemble them with models should prove beneficial in the future.13 This is especially apparent in the new Answer Sentence Selection datasets that have very large number of sentence candidates per question. The attention mechanism also has the highest impact in this kind of Information Retrieval task.\nWhile our models clearly yet lag behind the state-of-art on the paraphrasing and STS tasks, it establishes the new baseline on the Ubuntu Dialogue dataset and it is not possible to statistically determine its relation to state-of-art on the wang Answer Sentence Selection dataset.\nNote to readers: We apologize for the fact that the numbers listed here are not entirely final. The Ubuntu dataset shows single val samples rather than 16-way test results and some other measurements are with fewer than 16 runs and confidence intervals not included. We expect to catch up with the measurements in just a couple of days. and do not expect paper conclusions to be affected in any way."}, {"heading": "5 Model Reusability", "text": "To confirm the hypothesis that our models learn a generic task akin to some form of text comprehension, we tried to train a model on the large Ubuntu Dialogue dataset (Next Utterance Ranking task) and then transfer the weights and retrain the model instance on the smaller curatedv2 dataset (Answer Sentence Selection task).\nWe used the RNN model for the experiment in a configuration with dot-product scorer (which works much better on the Ubuntu dataset). The configuration, when trained from scratch on curatedv2, achieves MRR 0.371\u00b10.023, while with Ubuntu Dialogue pre-training it achieves MRR 0.493 \u00b1 0.021. This represents a jump of about 0.12 points and the resulting performance is comparable to a much more sophisticated attention model that is trained exclusively on the curatedv2 dataset.\nDuring our experiments, we have noticed that it is important not to apply dropout during retraining if it wasn\u2019t applied during the original training. We have also tried freezing the weights of some layers, but this never yielded a significant\n13We have tried simple averaging of predictions (as per (Kadlec et al., 2015)), but the benefit was small and inconsistent.\nimprovement."}, {"heading": "6 Conclusion", "text": "We have unified a variety of tasks in a single scientific framework of sentence pair scoring, and demonstrated a platform for general modelling of this problem and aggregate benchmarking of these models across many datasets. Promising initial transfer learning results suggest that a quest for generic neural model capable of task-independent text comprehension is becoming a meaningful pursuit. The open source nature of our framework and the implementation choice of a popular and extensible deep learning library allows for high reusability of our research and easy extensions with further more advanced models."}, {"heading": "6.1 Future Work", "text": "Due to a very wide breadth of the f2-problem scope, we were not able to cover all major tasks at once. Developing adapters, models and benchmarks for the task of Recognizing Textual Entailment remains the major next step, as well as for the problem of Hypothesis Evidencing where we would like to study the task of producing a binary classification of a hypothesis sentence s0 based on a number of memory sentences s1. We are in the process of developing realistic, hard datasets\nbased on real-world event yes/no questions and newspaper snippets, as well as solving school test exams based on textbook and encyclopedia snippets.\nWe also did not include several major classes of models in our initial evaluation. Most notably, this includes serial RNNs with attention as used e.g. for the RTE task (Rockta\u0308schel et al., 2015), and the skip-thoughts method of sentence embedding. (Kiros et al., 2015)\nWe believe that the Ubuntu Dialogue Dataset results demonstrate that the time is ripe to push the research models further towards the real world by allowing for wider sentence variability and less explicit supervision. But in particular, we believe that new models should be developed and tested on tasks with long sentences and wide vocabulary. Pushing the models to their limits will allow better differentiation regarding how much semantic text comprehension they are able to exhibit, which in turn should improve reusability, but datasets with low baseline performance will also better emphasize relative performance difference of individual models and make statistical significance more attainable.\nIn terms of models, recent work in many NLP domains (dos Santos et al., 2016) (Cheng et al., 2016) (Kumar et al., 2015) clearly points towards\nvarious forms of attention modelling to remove the bottleneck of having to compress the full spectrum of semantics into a single vector of fixed dimensionality. Another promising approach might be giving the network more flexibility regarding the final representation, for example by allowing it to remember a set of \u201cfacts\u201d derived from each sentence; related work has been done on end-to-end differentiable shift-reduce parsers with LSTM as stack cells (Dyer et al., 2015).\nIn this paper, we have shown the benefit of training a model on a single dataset and then applying it on another dataset. One open question is whether we could jointly train a model on multiple tasks simultaneously (even if they do not share some output layers), for example by task-by-task batch interleaving during training. Another way to\nimprove training would be to include extra supervision similar to the token overlap features that we already employ; for example, in the new Answer Sentence Selection task datasets, we can explicitly mark the actual tokens representing the answer."}, {"heading": "Acknowledgments", "text": "This work was financially supported by the Grant Agency of the Czech Technical University in Prague, grant No. SGS16/ 084/OHK3/1T/13, and the Forecast Foundation. Computational resources were provided by the CESNET LM2015042 and the CERIT Scientific Cloud LM2015085, provided under the programme \u201cProjects of Large Research, Development, and Innovations Infrastructures.\u201d\nWe\u2019d like to thank Toma\u0301s\u030c Tunys, Rudolf Kadlec, Ryan\nLowe, Cicero Nogueira dos santos and Bowen Zhou for help-\nful discussions and their insights, and Silvestr Stanko and Jir\u030c\u0131\u0301\nNa\u0301dvorn\u0131\u0301k for their software contributions."}], "references": [{"title": "Modeling of the question answering task in the YodaQA system", "author": ["Petr Baudi\u0161", "Jan \u0160ediv\u00fd."], "venue": "Experimental IR Meets Multilinguality, Multimodality, and Interaction, pages 222\u2013228. Springer.", "citeRegEx": "Baudi\u0161 and \u0160ediv\u00fd.,? 2015", "shortCiteRegEx": "Baudi\u0161 and \u0160ediv\u00fd.", "year": 2015}, {"title": "YodaQA: A Modular Question Answering System Pipeline", "author": ["Petr Baudi\u0161."], "venue": "POSTER 2015 - 19th International Student Conference on Electrical Engineering.", "citeRegEx": "Baudi\u0161.,? 2015", "shortCiteRegEx": "Baudi\u0161.", "year": 2015}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Learning to rank using gradient descent", "author": ["Chris Burges", "Tal Shaked", "Erin Renshaw", "Ari Lazier", "Matt Deeds", "Nicole Hamilton", "Greg Hullender."], "venue": "Proceedings of the 22nd international conference on Machine learning, pages 89\u201396. ACM.", "citeRegEx": "Burges et al\\.,? 2005", "shortCiteRegEx": "Burges et al\\.", "year": 2005}, {"title": "Syntaxaware multi-sense word embeddings for deep compositional models of meaning", "author": ["Jianpeng Cheng", "Dimitri Kartsaklis."], "venue": "arXiv preprint arXiv:1508.02354.", "citeRegEx": "Cheng and Kartsaklis.,? 2015", "shortCiteRegEx": "Cheng and Kartsaklis.", "year": 2015}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."], "venue": "CoRR, abs/1601.06733.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "CoRR, abs/1409.1259.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Automatically constructing a corpus of sentential paraphrases", "author": ["William B Dolan", "Chris Brockett"], "venue": null, "citeRegEx": "Dolan and Brockett.,? \\Q2005\\E", "shortCiteRegEx": "Dolan and Brockett.", "year": 2005}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "CoRR, abs/1505.08075.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Multiperspective sentence similarity modeling with convolutional neural networks", "author": ["Hua He", "Kevin Gimpel", "Jimmy Lin"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1684\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": null, "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Discriminative improvements to distributional sentence similarity", "author": ["Yangfeng Ji", "Jacob Eisenstein."], "venue": "EMNLP.", "citeRegEx": "Ji and Eisenstein.,? 2013", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2013}, {"title": "Improved deep learning baselines for ubuntu corpus dialogs", "author": ["Rudolf Kadlec", "Martin Schmid", "Jan Kleindienst."], "venue": "arXiv preprint arXiv:1510.03753.", "citeRegEx": "Kadlec et al\\.,? 2015", "shortCiteRegEx": "Kadlec et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in Neural Information Processing Systems, pages 3276\u20133284.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher."], "venue": "CoRR, abs/1506.07285.", "citeRegEx": "Kumar et al\\.,? 2015", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Denoising bodies to titles: Retrieving similar questions with recurrent convolutional models", "author": ["Tao Lei", "Hrishikesh Joshi", "Regina Barzilay", "Tommi S. Jaakkola", "Kateryna Tymoshenko", "Alessandro Moschitti", "Llu\u0131\u0301s M\u00e0rquez i Villodre"], "venue": null, "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."], "venue": "CoRR, abs/1506.08909.", "citeRegEx": "Lowe et al\\.,? 2015", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Okapi at trec-3", "author": ["Stephen E Robertson", "Steve Walker", "Susan Jones"], "venue": null, "citeRegEx": "Robertson et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Robertson et al\\.", "year": 1995}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Phil Blunsom."], "venue": "CoRR, abs/1509.06664.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2015", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["Aliaksei Severyn", "Alessandro Moschitti."], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages", "citeRegEx": "Severyn and Moschitti.,? 2015", "shortCiteRegEx": "Severyn and Moschitti.", "year": 2015}, {"title": "Dls@ cu: Sentence similarity from word alignment and semantic vector composition", "author": ["Md Arafat Sultan", "Steven Bethard", "Tamara Sumner."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation, pages 148\u2013153.", "citeRegEx": "Sultan et al\\.,? 2015", "shortCiteRegEx": "Sultan et al\\.", "year": 2015}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "CoRR, abs/1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Lstmbased deep learning models for non-factoid answer selection", "author": ["Ming Tan", "Bing Xiang", "Bowen Zhou."], "venue": "CoRR, abs/1511.04108.", "citeRegEx": "Tan et al\\.,? 2015", "shortCiteRegEx": "Tan et al\\.", "year": 2015}, {"title": "What is the jeopardy model? a quasisynchronous grammar for qa", "author": ["Mengqiu Wang", "Noah A Smith", "Teruko Mitamura."], "venue": "EMNLP-CoNLL, volume 7, pages 22\u201332.", "citeRegEx": "Wang et al\\.,? 2007", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "CoRR, abs/1410.3916.", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "CoRR, abs/1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Deep learning for answer sentence selection", "author": ["Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman."], "venue": "CoRR, abs/1412.1632.", "citeRegEx": "Yu et al\\.,? 2014", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "This task is popular in the NLP research community thanks to the dataset introduced in (Wang et al., 2007) which we refer to as wang, with six papers published between February 2015 and 2016 alone and neural models substantially improving over classical approaches based primarily on parse tree edits.", "startOffset": 87, "endOffset": 106}, {"referenceID": 32, "context": "Alternative datasets WikiQA (Yang et al., 2015) and InsuranceQA (Tan et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 28, "context": ", 2015) and InsuranceQA (Tan et al., 2015) were proposed, but are encumbered by licence restrictions.", "startOffset": 24, "endOffset": 42}, {"referenceID": 0, "context": "To alleviate the problems listed above, we are introducing a new dataset yodaqa/curatedv2 based on the curatedv2 question dataset (introduced in (Baudi\u0161 and \u0160ediv\u00fd, 2015), further denoisified by Mechanical Turkers) with candidate sentences as retrieved by the YodaQA question answering system (Baudi\u0161, 2015) from English Wikipedia.", "startOffset": 145, "endOffset": 170}, {"referenceID": 1, "context": "To alleviate the problems listed above, we are introducing a new dataset yodaqa/curatedv2 based on the curatedv2 question dataset (introduced in (Baudi\u0161 and \u0160ediv\u00fd, 2015), further denoisified by Mechanical Turkers) with candidate sentences as retrieved by the YodaQA question answering system (Baudi\u0161, 2015) from English Wikipedia.", "startOffset": 293, "endOffset": 307}, {"referenceID": 20, "context": "(Lowe et al., 2015) proposed a new large-scale and realistic dataset for an f2-style task of ranking candidates for the next utterance in a chat dialog, given the dialog context.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "(Lowe et al., 2015) (Kadlec et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": ", 2015) (Kadlec et al., 2015)", "startOffset": 8, "endOffset": 29}, {"referenceID": 26, "context": "Contrary to Answer Sentence Selection, state-of-art methods are based on parse tree alignments (Sultan et al., 2015) and weren\u2019t beaten by neural models yet.", "startOffset": 95, "endOffset": 116}, {"referenceID": 21, "context": "We also report results for this task on another dataset from the SemEval conferences, SICK2014 (Marelli et al., 2014).", "startOffset": 95, "endOffset": 117}, {"referenceID": 8, "context": "The canonical dataset is the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005), consisting of 4076 training and 1725 test pairs; 2/3 of the samples are labelled as 1 (is-a-paraphrase).", "startOffset": 66, "endOffset": 92}, {"referenceID": 13, "context": "State-of-art model uses an ensemble of handcrafted overlap features (Ji and Eisenstein, 2013) and weren\u2019t beaten by neural models (Cheng and Kartsaklis, 2015) (He et al.", "startOffset": 68, "endOffset": 93}, {"referenceID": 4, "context": "State-of-art model uses an ensemble of handcrafted overlap features (Ji and Eisenstein, 2013) and weren\u2019t beaten by neural models (Cheng and Kartsaklis, 2015) (He et al.", "startOffset": 130, "endOffset": 158}, {"referenceID": 10, "context": "State-of-art model uses an ensemble of handcrafted overlap features (Ji and Eisenstein, 2013) and weren\u2019t beaten by neural models (Cheng and Kartsaklis, 2015) (He et al., 2015) yet.", "startOffset": 159, "endOffset": 176}, {"referenceID": 2, "context": "In particular, this concerns the Recognizing Textual Entailment task (supported by the SNLI dataset) (Bowman et al., 2015) and the problem of memory selection in Memory Networks (supported by the baBi dataset) (Weston et al.", "startOffset": 101, "endOffset": 122}, {"referenceID": 31, "context": ", 2015) and the problem of memory selection in Memory Networks (supported by the baBi dataset) (Weston et al., 2015).", "startOffset": 95, "endOffset": 116}, {"referenceID": 19, "context": "A more realistic large paraphrasing dataset based on the AskUbuntu Stack Overflow forum had been recently proposed (Lei et al., 2015).", "startOffset": 115, "endOffset": 133}, {"referenceID": 33, "context": "(Yu et al., 2014) or (Weston et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 30, "context": ", 2014) or (Weston et al., 2014)) or the MLP module that takes elementwise product and sum of the embeddings and feeds them to a two-layer perceptron with hidden layer of width 2N (as in e.", "startOffset": 11, "endOffset": 32}, {"referenceID": 27, "context": "(Tai et al., 2015)).", "startOffset": 0, "endOffset": 18}, {"referenceID": 27, "context": "8 For the STS task, we follow this by score regression using class interpolation as in (Tai et al., 2015).", "startOffset": 87, "endOffset": 105}, {"referenceID": 3, "context": "When training for a ranking task (Answer Sentence Selection), we use the bipartite ranking version of Ranknet (Burges et al., 2005) as the objective; when training for STS task, we use Pearson\u2019s r formula as the objective; for binary classification tasks, we use the binary crossentropy objective.", "startOffset": 110, "endOffset": 131}, {"referenceID": 23, "context": "Weighed word overlaps metrics TF-IDF and BM25 (Robertson et al., 1995) are inspired by IR research and provide strong baselines for many tasks.", "startOffset": 46, "endOffset": 70}, {"referenceID": 33, "context": "in (Yu et al., 2014) or (Weston et al.", "startOffset": 3, "endOffset": 20}, {"referenceID": 30, "context": ", 2014) or (Weston et al., 2014), simply taking the mean vector of the word embedding sequence and training an U weight matrix N\u00d72N that projects both embeddings to the same vector space, Ei = tanh(U \u00b7 \u0113i), where the MLP scorer compares them.", "startOffset": 11, "endOffset": 32}, {"referenceID": 12, "context": "A simple extension of the above are the DAN Deep Averaging Networks (Iyyer et al., 2015), which were shown to adequately replace much more complex models in some tasks.", "startOffset": 68, "endOffset": 88}, {"referenceID": 28, "context": "RNN with memory units are popular models for processing sentenes (Tan et al., 2015) (Lowe et al.", "startOffset": 65, "endOffset": 83}, {"referenceID": 20, "context": ", 2015) (Lowe et al., 2015) (Bowman et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 2, "context": ", 2015) (Bowman et al., 2015).", "startOffset": 8, "endOffset": 29}, {"referenceID": 6, "context": "We use a bidirectional network with 2N GRU memory units9 (Cho et al., 2014) in each direction; the final unit states are summed across the two per-direction GRUs to yield a 2N vector representation of the sentence.", "startOffset": 57, "endOffset": 75}, {"referenceID": 33, "context": "CNN with sentence-wide pooling layer are also popular models for processing sentences (Yu et al., 2014) (Tan et al.", "startOffset": 86, "endOffset": 103}, {"referenceID": 28, "context": ", 2014) (Tan et al., 2015) (Severyn and Moschitti, 2015) (He et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 25, "context": ", 2015) (Severyn and Moschitti, 2015) (He et al.", "startOffset": 8, "endOffset": 37}, {"referenceID": 10, "context": ", 2015) (Severyn and Moschitti, 2015) (He et al., 2015) (Kadlec et al.", "startOffset": 38, "endOffset": 55}, {"referenceID": 14, "context": ", 2015) (Kadlec et al., 2015).", "startOffset": 8, "endOffset": 29}, {"referenceID": 15, "context": "We apply a multi-channel convolution (Kim, 2014) with single-token channel of N convolutions and 2, 3, 4 and 5-token channels of N/2 convolutions each, relu transfer function, max-pooling over the whole sentence, and as above a projection to shared space and an MLP scorer.", "startOffset": 37, "endOffset": 48}, {"referenceID": 28, "context": "Inspired by (Tan et al., 2015), the aim of this model is to allow the RNN to model long-term dependencies and model contextual representations of words, while taking advantage of the CNN and pooling operation for crisp selection of the gist of the sentence.", "startOffset": 12, "endOffset": 30}, {"referenceID": 11, "context": "The idea of attention models is to attend preferrentially to some parts of the sentence when building its representation (Hermann et al., 2015) (Tan et al.", "startOffset": 121, "endOffset": 143}, {"referenceID": 28, "context": ", 2015) (Tan et al., 2015) (dos Santos et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 24, "context": ", 2016) (Rockt\u00e4schel et al., 2015).", "startOffset": 8, "endOffset": 34}, {"referenceID": 28, "context": "There are many ways to model attention, as the initial proof of concept we adopt the (Tan et al., 2015) model attn1511 which is conceptually simple and easy to implement.", "startOffset": 85, "endOffset": 103}, {"referenceID": 7, "context": "To easily implement models, dataset loaders and task adapters in a modular fashion so that any model can be easily run on any f2-type task, we have created a new software package dataset-sts that integrates a variety of datasets, a Python dataset adapter PySTS and a Python library for easy construction of deep neural NLP models for semantic sentence pair scoring KeraSTS that uses the Keras machine learning library (Chollet, 2015).", "startOffset": 418, "endOffset": 433}, {"referenceID": 22, "context": "We use N = 300 dimensional GloVe embeddings matrix pretrained on Wikipedia 2014 + Gigaword 5 (Pennington et al., 2014) that we keep adaptable during training; words in the training set not included in the pretrained model are initialized by random vectors uniformly sampled from [\u22120.", "startOffset": 93, "endOffset": 118}, {"referenceID": 33, "context": "Word overlap is an important feature in many f2-type tasks (Yu et al., 2014) (Severyn and Moschitti, 2015), especially when the sentences may contain named entities, numeric or other data for which no embedding is available.", "startOffset": 59, "endOffset": 76}, {"referenceID": 25, "context": ", 2014) (Severyn and Moschitti, 2015), especially when the sentences may contain named entities, numeric or other data for which no embedding is available.", "startOffset": 8, "endOffset": 37}, {"referenceID": 16, "context": "We apply 10 L2 regularization and use Adam optimization with standard parameters (Kingma and Ba, 2014).", "startOffset": 81, "endOffset": 102}, {"referenceID": 13, "context": "(Ji and Eisenstein, 2013) 0.", "startOffset": 0, "endOffset": 25}, {"referenceID": 10, "context": "859 (He et al., 2015) 0.", "startOffset": 4, "endOffset": 21}, {"referenceID": 14, "context": "We can observe an effect analogous to what has been described in (Kadlec et al., 2015) \u2014 when the dataset is smaller, CNN models are preferrable, while larger dataset allows RNN models to capture the text comprehension task bet-", "startOffset": 65, "endOffset": 86}, {"referenceID": 14, "context": "We have tried simple averaging of predictions (as per (Kadlec et al., 2015)), but the benefit was small and inconsistent.", "startOffset": 54, "endOffset": 75}, {"referenceID": 28, "context": "(Tan et al., 2015) 0.", "startOffset": 0, "endOffset": 18}, {"referenceID": 28, "context": "5%) scored better than (Tan et al., 2015).", "startOffset": 23, "endOffset": 41}, {"referenceID": 24, "context": "for the RTE task (Rockt\u00e4schel et al., 2015), and the skip-thoughts method of sentence embedding.", "startOffset": 17, "endOffset": 43}, {"referenceID": 17, "context": "(Kiros et al., 2015)", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": ", 2016) (Cheng et al., 2016) (Kumar et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 18, "context": ", 2016) (Kumar et al., 2015) clearly points towards", "startOffset": 8, "endOffset": 28}, {"referenceID": 20, "context": "\u2217 Exact models from (Lowe et al., 2015) reran on the v2 version of the dataset (by personal communication with Ryan Lowe) \u2014 note that the results in (Lowe et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 20, "context": ", 2015) reran on the v2 version of the dataset (by personal communication with Ryan Lowe) \u2014 note that the results in (Lowe et al., 2015) and (Kadlec et al.", "startOffset": 117, "endOffset": 136}, {"referenceID": 14, "context": ", 2015) and (Kadlec et al., 2015) are on v1 and not directly comparable.", "startOffset": 12, "endOffset": 33}, {"referenceID": 27, "context": "841 (Tai et al., 2015) 0.", "startOffset": 4, "endOffset": 22}, {"referenceID": 9, "context": "Another promising approach might be giving the network more flexibility regarding the final representation, for example by allowing it to remember a set of \u201cfacts\u201d derived from each sentence; related work has been done on end-to-end differentiable shift-reduce parsers with LSTM as stack cells (Dyer et al., 2015).", "startOffset": 294, "endOffset": 313}], "year": 2017, "abstractText": "We review the task of Sentence Pair Scoring, popular in the literature in various forms \u2014 slanted as Answer Sentence Selection, Paraphrasing, Semantic Text Scoring, Next Utterance Ranking, Recognizing Textual Entailment or e.g. a component of Memory Networks. We argue that such tasks are similar from the model perspective (especially in the context of high-capacity deep neural models) and propose new baselines by comparing the performance of popular convolutional, recurrent and attentionbased neural models across many Sentence Pair Scoring tasks and datasets. We discuss the problem of evaluating randomized models, propose a statistically grounded methodology, and attempt to improve comparisons by releasing new datasets that are much harder than some of the currently used well explored benchmarks. To address the current research fragmentation in a future-proof way, we introduce a unified open source software framework with easily pluggable models, allowing easy evaluation on a wide range of semantic natural language tasks. This allows us to outline a path towards a universal machine learned semantic model for machine reading tasks. We support this plan by experiments that demonstrate reusability of models trained on different tasks, even across corpora of very different nature.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}