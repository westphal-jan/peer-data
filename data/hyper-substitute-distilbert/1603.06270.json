{"id": "1603.06270", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2016", "title": "Multi-Task Cross-Lingual Sequence Tagging from Scratch", "abstract": "then present a spatial linear recurrent neural node for sequence allocation. given global range of words, our approach employs additional link communication units on both trait & attribute levels to derive morphology and context information, and applies for sparse random learning layer using predict code input. our model handles functionality independent, language independent, and feature engineering free. let further restrict our field to per - task matching cross - dependency joint usage by sharing the architecture naming tasks. our tree achieves state - be - the - art practices in multiple performance on several benchmark tasks including pattern prediction, chunking, and ner. we also demonstrate that separate - task and bi - lingual joint training can improve user language in difficult cases.", "histories": [["v1", "Sun, 20 Mar 2016 21:15:56 GMT  (470kb,D)", "http://arxiv.org/abs/1603.06270v1", null], ["v2", "Tue, 9 Aug 2016 15:07:39 GMT  (470kb,D)", "http://arxiv.org/abs/1603.06270v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zhilin yang", "ruslan salakhutdinov", "william cohen"], "accepted": false, "id": "1603.06270"}, "pdf": {"name": "1603.06270.pdf", "metadata": {"source": "CRF", "title": "Multi-Task Cross-Lingual Sequence Tagging from Scratch", "authors": ["Zhilin Yang", "Ruslan Salakhutdinov", "William Cohen"], "emails": ["zhiliny@cs.cmu.edu", "rsalakhu@cs.cmu.edu", "wcohen@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Sequence tagging is a fundamental problem in natural language processing which has many wide applications, including part-of-speech (POS) tagging, chunking, and named entity recognition (NER). Given a sequence of words, sequence tagging aims to predict a linguistic tag for each word such as the POS tag. Recently progress has been made on neural sequence-tagging models which make only minimal assumptions about the language, task, and feature set (Collobert et al., 2011)\nThis paper explores an important potential advantage of these task-independent, languageindependent and feature-engineering free models: their ability to be jointly trained on multiple tasks. In particular, we explore two types of joint training. In multi-task joint training, a model is jointly trained to perform multiple sequence-\ntagging tasks in the same language\u2014e.g., POS tagging and NER for English. In cross-lingual joint training, a model is trained to perform the same task in multiple languages\u2014e.g., NER in English and Spanish.\nMulti-task joint training can exploit the fact that different sequence tagging tasks in one language share language-specific regularities. For example, models of English POS tagging and English NER might benefit from using similar underlying representations for words, and in past work, certain sequence-tagging tasks have benefitted by leveraging the underlying similarity of related tasks (Ando and Zhang, 2005). Currently, however, the best results on specific sequence-tagging tasks are usually achieved by approaches that target only one specific task, either POS tagging (S\u00f8gaard, 2011; Toutanova et al., 2003), chunking (Shen and Sarkar, 2005), or NER (Luo et al., 2015; Passos et al., 2014). Such approaches employ separate model development for each individual task, which makes joint training difficult. In other work, some recent neural approaches have been proposed to address multiple sequence tagging problems in a unified framework (Huang et al., 2015). Though gains have been shown using multi-task joint training, the prior models that benefit from multi-task joint training did not achieve state-ofthe-art performance (Collobert et al., 2011); thus the question of whether joint training can improve over strong baseline methods is still unresolved.\nCross-lingual joint training typically uses word alignments or parallel corpora to improve the performance on different languages (Kiros et al., 2014; Gouws et al., 2014). However, many successful approaches in sequence tagging rely heavily on feature engineering to handcraft languagedependent features, such as character-level morphological features and word-level N-gram patterns (Huang et al., 2015; Toutanova et al., 2003;\nar X\niv :1\n60 3.\n06 27\n0v 1\n[ cs\n.C L\n] 2\n0 M\nar 2\n01 6\nSun et al., 2008), making it difficult to share latent representations between different languages. Some multilingual taggers that do not rely on feature engineering have also been presented (Lample et al., 2016; dos Santos et al., 2015), but while these methods are language-independent, they do not study the effect of cross-lingual joint training.\nIn this work, we focus on developing a general model that can be applied in both multi-task and cross-lingual settings by learning from scratch, i.e., without feature engineering or pipelines. Given a sequence of words, our model employs deep gated recurrent units on both character and word levels, and applies a conditional random field layer to make the structured prediction. On the character level, the gated recurrent units capture the morphological information; on the word level, the gated recurrent units learn N-gram patterns and word semantics.\nOur model can handle both multi-task and cross-lingual joint training in a unified manner by simply sharing the network architecture and model parameters between tasks and languages. For multi-task joint training, we share both character and word level parameters between tasks to learn language-specific regularities. For crosslingual joint training, we share the character-level parameters to capture the morphological similarity between languages without use of parallel corpora or word alignments.\nWe evaluate our model on five datasets of different tasks and languages, including POS tagging, chunking and NER in English; and NER in Dutch and Spanish. We achieve state-of-the-art results on several standard benchmarks: CoNLL 2000 chunking (95.41%), CoNLL 2002 Dutch NER (85.19%), CoNLL 2003 Spanish NER (85.77%), and CoNLL 2003 English NER (91.20%). We also achieve very competitive results on Penn Treebank POS tagging (97.55%, the second best result in the literature). Finally, we conduct experiments to systematically explore the effectiveness of multi-task and cross-lingual joint training on several tasks."}, {"heading": "2 Related Work", "text": "Ando and Zhang (2005) proposed a multi-task joint training framework that shares structural parameters among multiple tasks, and improved the performance on various tasks including NER. Collobert et al. (2011) presented a task indepen-\ndent convolutional network and employed multitask joint training to improve the performance of chunking. However, there is still a gap between these multi-task approaches and the state-of-theart results on individual tasks. Furthermore, it is unclear whether these approaches can be effective in a cross-lingual setting.\nMultilingual resources were extensively used for cross-lingual sequence tagging through various ways, such as cross-lingual feature extraction (Darwish, 2013), text categorization (Virga and Khudanpur, 2003), and Bayesian parallel data prediction (Snyder et al., 2008). Parallel corpora and word alignments are also used for training crosslingual distributed word representations (Kiros et al., 2014; Gouws et al., 2014; Zhou et al., 2015). Unlike these approaches, our method mainly focuses on using morphological similarity for crosslingual joint training.\nSeveral neural architectures based on recurrent networks were proposed for sequence tagging. Huang et al. (2015) used word-level Long ShortTerm Memory (LSTM) units based on handcrafted features; dos Santos et al. (2015) employed convolutional layers on both character and word levels; Chiu and Nichols (2015) applied convolutional layers on the character level and LSTM units on the word level; Gillick et al. (2015) employed a sequence-to-sequence LSTM with a novel tagging scheme. We show that our architecture gives better performance experimentally than these approaches in Section 5.\nMost similar to our work is the recent approach independently developed by Lample et al. (2016) (published two weeks before our submission), which employs LSTM on both character and word levels. However, there are several crucial differences. First, we study cross-lingual joint training and show improvement over their approach in various cases. Second, while they mainly focus on NER, we generalize our model to other sequence tagging tasks, and also demonstrate the effectiveness of multi-task joint training. There are also differences in the technical aspect, such as the cost-sensitive loss function and gated recurrent units used in our work."}, {"heading": "3 Model", "text": "In this section, we present our model for sequence tagging based on deep hierarchical gated recurrent units and conditional random fields. Our recurrent\nnetworks are hierarchical since we have multiple layers on both word and character levels in a hierarchy."}, {"heading": "3.1 Gated Recurrent Unit", "text": "A gated recurrent unit (GRU) network is a type of recurrent neural networks first introduced for machine translation (Cho et al., 2014). A recurrent network can be represented as a sequence of units, corresponding to the input sequence (x1,x2, \u00b7 \u00b7 \u00b7 ,xT ), which can be either a word sequence in a sentence or a character sequence in a word. The unit at position t takes xt and the previous hidden state ht\u22121 as input, and outputs the current hidden state ht. The model parameters are shared between different units in the sequence.\nA gated recurrent unit at position t has two gates, an update gate zt and a reset gate rt. More specifically, each gated recurrent unit can be expressed as follows\nrt = \u03c3(Wrxxt +Wrhht\u22121)\nzt = \u03c3(Wzxxt +Wzhht\u22121)\nh\u0303t = tanh(Whxxt +Whh(rt ht\u22121)) ht = zt ht\u22121 + (1\u2212 zt) h\u0303t,\nwhere W \u2019s are model parameters of each unit, h\u0303t is a candidate hidden state that is used to compute ht, \u03c3 is an element-wise sigmoid logistic function defined as \u03c3(x) = 1/(1 + e\u2212x), and denotes element-wise multiplication of two vectors. Intuitively, the update gate zt controls how much the unit updates its hidden state, and the reset gate rt determines how much information from the previous hidden state needs to be reset.\nSince a recurrent neural network only models the information flow in one direction, it is usually helpful to use an additional recurrent network that goes in the reverse direction. More specifically, we use bidirectional gated recurrent units, where given a sequence of length T , we have one GRU going from 1 to T and the other from T to 1. Let\u2212\u2192 h t and \u2190\u2212 h t denote the hidden states at position t of the forward and backward GRUs respectively. We concatenate the two hidden states to form the final hidden state ht = [ \u2212\u2192 h t, \u2190\u2212 h t].\nWe stack multiple recurrent layers together to form a deep recurrent network (Sutskever et al., 2014). Each layer learns a more effective representation taking the hidden states of the previous layer as input. Let hl,t denote the hidden state at position t in layer l. The forward GRU at position t in layer l computes \u2212\u2192 h l,t using \u2212\u2192 h l,t\u22121 and hl\u22121,t as input, and the backward GRU performs similar operations but in a reverse direction."}, {"heading": "3.2 Hierarchical GRU", "text": "Our model employs a hierarchical GRU that encodes both word-level and character-level sequential information.\nThe input of our model is a sequence of words (x1,x2, \u00b7 \u00b7 \u00b7 ,xT ) of length T , where xt is a oneof-K embedding of the t-th word. The word at each position t also has a character-level representation, denoted as a sequence of length St, (ct,1, ct,2, \u00b7 \u00b7 \u00b7 , ct,St) where ct,s is the one-of-K embedding of the s-th character in the t-th word."}, {"heading": "3.2.1 Character-Level GRU", "text": "Given a word, we first employ a deep bidirectional GRU to learn useful morphological representation from the character sequence of the word. Suppose the character-level GRU has Lc layers, we then obtain forward and backward hidden states \u2190\u2212 h Lc,s and \u2212\u2192 h Lc,s at each position s in the character sequence. Since recurrent networks usually tend to memorize more short-term patterns, we concatenate the first hidden state of the backward GRU and the last hidden state of the forward GRU to encode character-level morphology in both prefixes and suffixes. We further concatenate the characterlevel representation with the one-of-K word embedding xt to form the final representation hwt for the t-th word. More specifically, we have\nhwt = [ \u2212\u2192 h Lc,St , \u2190\u2212 h Lc,1,xt],\nwhere hwt is a representation of the t-th word, which encodes both character-level morphology and word-level semantics, as shown in Figure 1."}, {"heading": "3.2.2 Word-Level GRU", "text": "The character-level GRU outputs a sequence of word representations hw = (hw1 ,h w 2 , \u00b7 \u00b7 \u00b7 ,hwT ). We employ a word-level deep bidirectional GRU with Lw layers on top of these word representations. The word-level GRU takes the sequence hw as input, and computes a sequence of hidden states h = (h1,h2, \u00b7 \u00b7 \u00b7 ,hT ).\nDifferent from the character-level GRU, the word-level GRU aims to extract the context information in the word sequence, such as N-gram patterns and neighbor word dependencies. Such information is usually encoded using handcrafted features. However, as we show in our experimental results, the word-level GRU can learn the relevant information without being language-specific or task-specific. The hidden states h output by the word-level GRU will be used as input features for the next layers."}, {"heading": "3.3 Conditional Random Field", "text": "The goal of sequence tagging is to predict a sequence of tags y = (y1, y2, \u00b7 \u00b7 \u00b7 , yT ). To model the dependencies between tags in a sequence, we apply a conditional random field (Lafferty et al., 2001) layer on top of the hidden states h output by the word-level GRU (Huang et al., 2015). Let Y(h) denote the space of tag sequences for h. The conditional log probability of a tag sequence y,\ngiven the hidden state sequence h, can be written as\nlog p(y|h) = f(h,y)\u2212 log \u2211\ny\u2032\u2208Y(h)\nexp f(h,y\u2032),\n(1) where f is a function that assigns a score for each pair of h and y.\nTo define the function f(h,y), for each position t, we multiply the hidden state hwt with a parameter vector wyt that is indexed by the the tag yt, to obtain the score for assigning yt at position t. Since we also need to consider the correlation between tags, we impose first order dependency by adding a score Ayt\u22121,yt at position t, where A is a parameter matrix defining the similarity scores between different tag pairs. Formally, the function f can be written as\nf(h,y) = T\u2211 t=1 wTyth w t + T\u2211 t=1 Ayt\u22121,yt ,\nwhere we set y0 to be a START token. It is possible to directly maximize the conditional log likelihood based on Eq. (1). However, this training objective is usually not optimal since each possible y\u2032 contributes equally to the objective function. Therefore, we add a cost function between y and y\u2032 based on the max-margin principle that high-cost tags y\u2032 should be penalized more heavily (Gimpel and Smith, 2010). More specifically, the objective function to maximize for each training instance y and h is written as\nf(h,y)\u2212 log \u2211\ny\u2032\u2208Y(h)\nexp(f(h,y\u2032)+ cost(y,y\u2032)).\n(2) In our work, the cost function is defined as the tag-wise Hamming loss between two tag sequences multiplied by a constant. The objective function on the training set is the sum of Eq. (2) over all the training instances. The full architecture of our model is illustrated in Figure 1."}, {"heading": "3.4 Training", "text": "We employ mini-batch AdaGrad (Duchi et al., 2011) to train our neural network in an end-toend manner with backpropagation. Both the character embeddings and word embeddings are finetuned during training. We use dynamic programming to compute the normalizer of the CRF layer in Eq. (2). When making prediction, we again use dynamic programming in the CRF layer to decode the most probable tag sequence."}, {"heading": "4 Multi-Task and Cross-Lingual Joint Training", "text": "In this section we study joint training of multiple tasks and multiple languages. On one hand, different sequence tagging tasks in the same language share language-specific regularities. For example, POS tagging and NER in English should learn similar underlying representation since they are in the same language. On the other hand, some languages share character-level morphologies, such as English and Spanish. Therefore, it is desirable to leverage multi-task and cross-lingual joint training to boost model performance.\nSince our model is generally applicable to different tasks in different languages, it can be naturally extended to multi-task and cross-lingual joint training. The basic idea is to share part of the architecture and parameters between tasks and languages, and to jointly train multiple objective functions with respect to different tasks and languages.\nWe now discuss the details of our joint training\nalgorithm in the multi-task setting. Suppose we have D tasks, with the training instances of each task being (X1, X2, \u00b7 \u00b7 \u00b7 , XD). Each task d has a set of model parameters Wd, which is divided into two sets, task specific parameters and shared parameters, i.e.,\nWd =Wd,spec \u222aWshared,\nwhere shared parameters Wshared are a set of parameters that are shared among the D tasks, while task specific parametersWd,spec are the rest of the parameters that are trained for each task d separately.\nDuring joint training, we are optimizing the average over all objective functions of D tasks. We iterate over each task d, sample a batch of training instances fromXd, and perform a gradient descent step to update model parameters Wd. Similarly, we can derive a cross-lingual joint training algorithm by replacing D tasks with D languages.\nThe network architectures we employ for joint training are illustrated in Figure 2. For multi-task joint training, we share all the parameters below the CRF layer including word embeddings to learn language-specific regularities shared by the tasks. For cross-lingual joint training, we share the parameters of the character-level GRU to capture the morphological similarity between languages. Note that since we do not consider using parallel corpus in this work, we mainly focus on joint training between languages with similar morphology. We leave the study of cross-lingual joint training by sharing word semantics based on parallel corpora to future work."}, {"heading": "5 Experiments", "text": "In this section, we use several benchmark datasets for multiple tasks in multiple languages to evaluate our model as well as the joint training algorithm."}, {"heading": "5.1 Datasets and Settings", "text": "We use the following benchmark datasets in our experiments: Penn Treebank (PTB) POS tagging, CoNLL 2000 chunking, CoNLL 2003 English NER, CoNLL 2002 Dutch NER and CoNLL 2002 Spanish NER. The statistics of the datasets are described in Table 1.\nWe construct the POS tagging dataset with the instructions described in Toutanova et al. (2003). Note that as a standard practice, the POS tags are extracted from the parsed trees.\nFor the task of CoNLL 2003 English NER, we follow previous works (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015) to append one-hot gazetteer features to the input of the CRF layer for fair comparison.1\nWe set the hidden state dimensions to be 300 for the word-level GRU. We set the number of GRU layers to Lc = Lw = 2 (two layers for the wordlevel and character-level GRUs respectively). The learning rate is fixed at 0.01. We use the development set to tune the other hyperparameters of our model. Since the CoNLL 2000 chunking dataset does not have a development set, we hold out one fifth of the training set for parameter tuning.\nWe truncate all words whose character sequence length is longer than a threshold (17 for English, 35 for Dutch, and 20 for Spanish). We replace all numeric characters with \u201c0\u201d. We also use the BIOES (Begin, Inside, Outside, End, Single) tagging scheme (Ratinov and Roth, 2009)."}, {"heading": "5.2 Pre-Trained Word Embeddings", "text": "Since the training corpus for a sequence tagging task is relatively small, it is difficult to train ran-\n1Although gazetteers are arguably a type of feature engineering, we note that unlike most feature engineering techniques they are straightforward to include in a model. We use only the gazetteer file provided by the CoNLL 2003 shared task, and do not use gazetteers for any other tasks or languages described here.\ndomly initialized word embeddings to accurately capture the word semantics. Therefore, we leverage word embeddings pre-trained on large-scale corpora. All the pre-trained embeddings we use are publicly available.\nOn the English datasets, following previous works that are based on neural networks (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015), we use the 50-dimensional SENNA embeddings2 trained on Wikipedia. For Spanish and Dutch, we use the 64-dimensional Polyglot embeddings3 (Al-Rfou et al., 2013), which are trained on Wikipedia articles of the corresponding languages. We use pre-trained word embeddings as initialization, and fine-tune the embeddings during training."}, {"heading": "5.3 Performance", "text": "In this section, we report the results of our model on the benchmark datasets and compare to the previously-reported state-of-the-art results.\n2http://ronan.collobert.com/senna/ 3https://sites.google.com/site/rmyeid/\nprojects/polyglot 4We note that this number is often mistakenly cited as 95.23, which is actually the score on base NP chunking rather than CoNLL 2000.\nFor English NER, there are two evaluation methods used in the literature. Some models are trained with both the training and development set, while others are trained with the training set only. We report our results in both cases. In the first case, we tune the hyperparameters by training on the training set and testing on the development set.\nBesides our standalone model, we experimented with multi-task and cross-lingual joint training as well, using the architecture described in Section 4. For multi-task joint training, we jointly train all tasks in English, including POS tagging, chunking and NER. For cross-lingual joint training, we jointly train NER in English, Dutch and Spanish. We also remove the word embeddings and the character-level GRU respectively to analyze the contribution of different components.\nThe results are shown in Tables 2, 3, 4, 5, 6 and 7. We achieve state-of-the-art results on English NER, Dutch NER, Spanish NER and English chunking. Our model outperforms the best previously-reported results on Dutch NER and English chunking by 2.35 points and 0.95 points respectively. We also achieve the second best re-\nsult on English POS tagging, which is 0.23 points worse than the current state-of-the-art.\nJoint training improves the performance on Spanish NER, Dutch NER and English chunking by 1.08 points, 0.19 points and 0.75 points respectively, and has no significant improvement on English POS tagging and English NER.\nOn POS tagging, the best result is 97.78% reported by Ling et al. (2015). However, the embeddings they used are not publicly available. To demonstrate the effectiveness of our model, we slightly revise our model to reimplement their model with the same parameter settings described in their original paper. We use SENNA embeddings to initialize the reimplemented model for fair comparison, and obtain an accuracy of 97.41% that is 0.14 points worse than our result, which indicates that our model is more effective and the main difference lies in using different pre-trained embeddings.\nBy comparing the results without the character-\nlevel GRU and without word embeddings, we can observe that both components contribute to the final results. It is also clear that word embeddings have significantly more contribution than the character-level GRU, which indicates that our model largely depends on memorizing the word semantics. Character-level morphology, on the other hand, has relatively smaller but still critical contribution."}, {"heading": "5.4 Joint Training", "text": "In this section, we analyze the effectiveness of multi-task and cross-lingual joint training in more detail. In order to explore possible gains in performance of joint training for resource-poor languages or tasks, we consider joint training of various task pairs and language pairs where differentsized subsets of the actual labeled corpora are made available. Given a pair of tasks of languages, we jointly train one task with full labels and the other with partial labels. In particular, we introduce a labeling rate r, and sample a fraction r of the sentences in the training set, discarding the rest. Evaluation is based on the partially-labeled task. The results are reported in Table 8.\nWe observe that the performance of a specific task with relatively lower labeling rates (0.1 and 0.3) can usually benefit from other tasks with full labels through multi-task or cross-lingual joint training. The performance gain can be up to 1.99 points when the labeling rate of the target task is 0.1. The improvement with 0.1 labeling rate is on average 0.37 points larger than with 0.3 labeling rate, which indicates that the improvement of joint training is more significant when the target\ntask has less labeled data. We also use t-SNE (Van der Maaten and Hinton, 2008) to obtain a 2-dimensional visualization of the character-level GRU output for the country names in English and Spanish, shown in Figure 3. We can clearly see that our model captures the morphological similarity between two languages through joint training, since all corresponding pairs are nearest neighbors in the original embedding space."}, {"heading": "6 Conclusion", "text": "We presented a new model for sequence tagging based on gated recurrent units and conditional random fields. We explored multi-task and crosslingual joint training through sharing part of the network architecture and model parameters. We achieved state-of-the-art results on various tasks including POS tagging, chunking, and NER, in multiple languages. We also demonstrated that joint training can improve model performance in various cases.\nIn this work, we mainly focus on leveraging morphological similarities for cross-lingual joint training. In the future, an important problem will be joint training based on cross-lingual word semantics with the help of parallel data. Furthermore, it will be interesting to apply our joint training approach to low-resource tasks and languages."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Al-Rfou et al.2013] Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": null, "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005] Rie Kubota Ando", "Tong Zhang"], "venue": null, "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Named entity extraction using adaboost", "author": ["Lluis Marquez", "Llu\u0131\u0301s Padr\u00f3"], "venue": "In CoNLL,", "citeRegEx": "Carreras et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2002}, {"title": "Named entity recognition: a maximum entropy approach using global information", "author": ["Chieu", "Ng2002] Hai Leong Chieu", "Hwee Tou Ng"], "venue": "In COLING,", "citeRegEx": "Chieu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chieu et al\\.", "year": 2002}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Chiu", "Nichols2015] Jason PC Chiu", "Eric Nichols"], "venue": "arXiv preprint arXiv:1511.08308", "citeRegEx": "Chiu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Named entity recognition using cross-lingual resources: Arabic as an example", "author": ["Kareem Darwish"], "venue": "In ACL,", "citeRegEx": "Darwish.,? \\Q2013\\E", "shortCiteRegEx": "Darwish.", "year": 2013}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Victor Guimaraes", "RJ Niter\u00f3i", "Rio de Janeiro"], "venue": "In Proceedings of NEWS 2015 The Fifth Named Entities Workshop,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "In HLT-NAACL,", "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Multilingual language processing from bytes", "author": ["Gillick et al.2015] Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": "arXiv preprint arXiv:1512.00103", "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Softmax-margin crfs: Training loglinear models with cost functions", "author": ["Gimpel", "Smith2010] Kevin Gimpel", "Noah A Smith"], "venue": "In NAACL,", "citeRegEx": "Gimpel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2010}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Gouws et al.2014] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": null, "citeRegEx": "Gouws et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2014}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "A multiplicative model for learning distributed text-based attribute representations", "author": ["Kiros et al.2014] Ryan Kiros", "Richard Zemel", "Ruslan R Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Chunking with support vector machines", "author": ["Kudo", "Matsumoto2001] Taku Kudo", "Yuji Matsumoto"], "venue": "In NAACL,", "citeRegEx": "Kudo et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kudo et al\\.", "year": 2001}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando CN Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural architectures for named entity recognition", "author": ["Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "arXiv preprint arXiv:1603.01360", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Phrase clustering for discriminative learning", "author": ["Lin", "Wu2009] Dekang Lin", "Xiaoyun Wu"], "venue": "In ACL,", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Joint named entity recognition and disambiguation", "author": ["Luo et al.2015] Gang Luo", "Xiaojiang Huang", "ChinYew Lin", "Zaiqing Nie"], "venue": null, "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Learning multilingual named entity recognition from wikipedia", "author": ["Nothman et al.2013] Joel Nothman", "Nicky Ringland", "Will Radford", "Tara Murphy", "James R Curran"], "venue": "Artificial Intelligence,", "citeRegEx": "Nothman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nothman et al\\.", "year": 2013}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": null, "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In CoNLL,", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Voting between multiple data representations for text", "author": ["Shen", "Sarkar2005] Hong Shen", "Anoop Sarkar"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2005}, {"title": "Guided learning for bidirectional sequence classification", "author": ["Shen et al.2007] Libin Shen", "Giorgio Satta", "Aravind Joshi"], "venue": "In ACL,", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Unsupervised multilingual learning for pos tagging", "author": ["Tahira Naseem", "Jacob Eisenstein", "Regina Barzilay"], "venue": "In EMNLP,", "citeRegEx": "Snyder et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snyder et al\\.", "year": 2008}, {"title": "Semisupervised condensed nearest neighbor for part-of-speech tagging", "author": ["Anders S\u00f8gaard"], "venue": "In ACL,", "citeRegEx": "S\u00f8gaard.,? \\Q2011\\E", "shortCiteRegEx": "S\u00f8gaard.", "year": 2011}, {"title": "Modeling latent-dynamic in shallow parsing: a latent conditional model with improved inference", "author": ["Sun et al.2008] Xu Sun", "Louis-Philippe Morency", "Daisuke Okanohara", "Jun\u2019ichi Tsujii"], "venue": "In COLING,", "citeRegEx": "Sun et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2008}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In NAACL,", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Visualizing data using t-sne", "author": ["Van der Maaten", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Transliteration of proper names in cross-lingual information retrieval", "author": ["Virga", "Khudanpur2003] Paola Virga", "Sanjeev Khudanpur"], "venue": "In Proceedings of the ACL 2003 workshop on Multilingual and mixed-language named entity recognition-Volume", "citeRegEx": "Virga et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Virga et al\\.", "year": 2003}, {"title": "Learning bilingual sentiment word embeddings for cross-language sentiment classification", "author": ["Zhou et al.2015] Huiwei Zhou", "Long Chen", "Fulin Shi", "Degen Huang"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Recently progress has been made on neural sequence-tagging models which make only minimal assumptions about the language, task, and feature set (Collobert et al., 2011)", "startOffset": 144, "endOffset": 168}, {"referenceID": 21, "context": ", 2003), chunking (Shen and Sarkar, 2005), or NER (Luo et al., 2015; Passos et al., 2014).", "startOffset": 50, "endOffset": 89}, {"referenceID": 23, "context": ", 2003), chunking (Shen and Sarkar, 2005), or NER (Luo et al., 2015; Passos et al., 2014).", "startOffset": 50, "endOffset": 89}, {"referenceID": 14, "context": "In other work, some recent neural approaches have been proposed to address multiple sequence tagging problems in a unified framework (Huang et al., 2015).", "startOffset": 133, "endOffset": 153}, {"referenceID": 6, "context": "Though gains have been shown using multi-task joint training, the prior models that benefit from multi-task joint training did not achieve state-ofthe-art performance (Collobert et al., 2011); thus the question of whether joint training can improve over strong baseline methods is still unresolved.", "startOffset": 167, "endOffset": 191}, {"referenceID": 15, "context": "Cross-lingual joint training typically uses word alignments or parallel corpora to improve the performance on different languages (Kiros et al., 2014; Gouws et al., 2014).", "startOffset": 130, "endOffset": 170}, {"referenceID": 13, "context": "Cross-lingual joint training typically uses word alignments or parallel corpora to improve the performance on different languages (Kiros et al., 2014; Gouws et al., 2014).", "startOffset": 130, "endOffset": 170}, {"referenceID": 18, "context": "Some multilingual taggers that do not rely on feature engineering have also been presented (Lample et al., 2016; dos Santos et al., 2015), but while these methods are language-independent, they do not study the effect of cross-lingual joint training.", "startOffset": 91, "endOffset": 137}, {"referenceID": 6, "context": "Collobert et al. (2011) presented a task independent convolutional network and employed multitask joint training to improve the performance of chunking.", "startOffset": 0, "endOffset": 24}, {"referenceID": 7, "context": "Multilingual resources were extensively used for cross-lingual sequence tagging through various ways, such as cross-lingual feature extraction (Darwish, 2013), text categorization (Virga and Khudanpur, 2003), and Bayesian parallel data prediction (Snyder et al.", "startOffset": 143, "endOffset": 158}, {"referenceID": 27, "context": "Multilingual resources were extensively used for cross-lingual sequence tagging through various ways, such as cross-lingual feature extraction (Darwish, 2013), text categorization (Virga and Khudanpur, 2003), and Bayesian parallel data prediction (Snyder et al., 2008).", "startOffset": 247, "endOffset": 268}, {"referenceID": 15, "context": "Parallel corpora and word alignments are also used for training crosslingual distributed word representations (Kiros et al., 2014; Gouws et al., 2014; Zhou et al., 2015).", "startOffset": 110, "endOffset": 169}, {"referenceID": 13, "context": "Parallel corpora and word alignments are also used for training crosslingual distributed word representations (Kiros et al., 2014; Gouws et al., 2014; Zhou et al., 2015).", "startOffset": 110, "endOffset": 169}, {"referenceID": 34, "context": "Parallel corpora and word alignments are also used for training crosslingual distributed word representations (Kiros et al., 2014; Gouws et al., 2014; Zhou et al., 2015).", "startOffset": 110, "endOffset": 169}, {"referenceID": 13, "context": "Huang et al. (2015) used word-level Long ShortTerm Memory (LSTM) units based on handcrafted features; dos Santos et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "(2015) used word-level Long ShortTerm Memory (LSTM) units based on handcrafted features; dos Santos et al. (2015) employed convo-", "startOffset": 93, "endOffset": 114}, {"referenceID": 11, "context": "lutional layers on both character and word levels; Chiu and Nichols (2015) applied convolutional layers on the character level and LSTM units on the word level; Gillick et al. (2015) employed a sequence-to-sequence LSTM with a novel tag-", "startOffset": 161, "endOffset": 183}, {"referenceID": 18, "context": "Most similar to our work is the recent approach independently developed by Lample et al. (2016) (published two weeks before our submission), which employs LSTM on both character and word levels.", "startOffset": 75, "endOffset": 96}, {"referenceID": 5, "context": "A gated recurrent unit (GRU) network is a type of recurrent neural networks first introduced for machine translation (Cho et al., 2014).", "startOffset": 117, "endOffset": 135}, {"referenceID": 30, "context": "form a deep recurrent network (Sutskever et al., 2014).", "startOffset": 30, "endOffset": 54}, {"referenceID": 17, "context": "To model the dependencies between tags in a sequence, we apply a conditional random field (Lafferty et al., 2001) layer on top of the hidden states h output by the word-level GRU (Huang et al.", "startOffset": 90, "endOffset": 113}, {"referenceID": 14, "context": ", 2001) layer on top of the hidden states h output by the word-level GRU (Huang et al., 2015).", "startOffset": 73, "endOffset": 93}, {"referenceID": 9, "context": "We employ mini-batch AdaGrad (Duchi et al., 2011) to train our neural network in an end-toend manner with backpropagation.", "startOffset": 29, "endOffset": 49}, {"referenceID": 31, "context": "We construct the POS tagging dataset with the instructions described in Toutanova et al. (2003). Note that as a standard practice, the POS tags are extracted from the parsed trees.", "startOffset": 72, "endOffset": 96}, {"referenceID": 6, "context": "For the task of CoNLL 2003 English NER, we follow previous works (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015) to append one-hot gazetteer features to the input of the CRF layer for fair comparison.", "startOffset": 65, "endOffset": 133}, {"referenceID": 14, "context": "For the task of CoNLL 2003 English NER, we follow previous works (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015) to append one-hot gazetteer features to the input of the CRF layer for fair comparison.", "startOffset": 65, "endOffset": 133}, {"referenceID": 23, "context": "80 Passos et al. (2014)\u2020\u2021 90.", "startOffset": 3, "endOffset": 24}, {"referenceID": 20, "context": "77 Luo et al. (2015)\u2020\u2021 91.", "startOffset": 3, "endOffset": 21}, {"referenceID": 18, "context": "2 Lample et al. (2016)\u2217 90.", "startOffset": 2, "endOffset": 23}, {"referenceID": 6, "context": "On the English datasets, following previous works that are based on neural networks (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015), we use the 50-dimensional SENNA embeddings2 trained on Wikipedia.", "startOffset": 84, "endOffset": 152}, {"referenceID": 14, "context": "On the English datasets, following previous works that are based on neural networks (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2015), we use the 50-dimensional SENNA embeddings2 trained on Wikipedia.", "startOffset": 84, "endOffset": 152}, {"referenceID": 0, "context": "For Spanish and Dutch, we use the 64-dimensional Polyglot embeddings3 (Al-Rfou et al., 2013), which are trained on Wikipedia articles of the corresponding languages.", "startOffset": 70, "endOffset": 92}, {"referenceID": 27, "context": "014 Sun et al. (2008)\u2020\u2021 94.", "startOffset": 4, "endOffset": 22}, {"referenceID": 6, "context": "34 Collobert et al. (2011) 94.", "startOffset": 3, "endOffset": 27}, {"referenceID": 6, "context": "34 Collobert et al. (2011) 94.32 Huang et al. (2015)\u2020 94.", "startOffset": 3, "endOffset": 53}, {"referenceID": 20, "context": "78% reported by Ling et al. (2015). However, the embeddings they used are not publicly available.", "startOffset": 16, "endOffset": 35}, {"referenceID": 22, "context": "24 Shen et al. (2007)\u2020\u2021 97.", "startOffset": 3, "endOffset": 22}, {"referenceID": 22, "context": "24 Shen et al. (2007)\u2020\u2021 97.33 S\u00f8gaard et al. (2011)\u2020\u2021 97.", "startOffset": 3, "endOffset": 52}, {"referenceID": 6, "context": "50 Collobert et al. (2011) 97.", "startOffset": 3, "endOffset": 27}, {"referenceID": 6, "context": "50 Collobert et al. (2011) 97.29 Huang et al. (2015)\u2020 97.", "startOffset": 3, "endOffset": 53}, {"referenceID": 6, "context": "50 Collobert et al. (2011) 97.29 Huang et al. (2015)\u2020 97.55 Ling et al. (2015) 97.", "startOffset": 3, "endOffset": 79}, {"referenceID": 6, "context": "50 Collobert et al. (2011) 97.29 Huang et al. (2015)\u2020 97.55 Ling et al. (2015) 97.78 Ling et al. (2015) (SENNA)\u2217 97.", "startOffset": 3, "endOffset": 104}], "year": 2016, "abstractText": "We present a deep hierarchical recurrent neural network for sequence tagging. Given a sequence of words, our model employs deep gated recurrent units on both character and word levels to encode morphology and context information, and applies a conditional random field layer to predict the tags. Our model is task independent, language independent, and feature engineering free. We further extend our model to multi-task and crosslingual joint training by sharing the architecture and parameters. Our model achieves state-of-the-art results in multiple languages on several benchmark tasks including POS tagging, chunking, and NER. We also demonstrate that multi-task and cross-lingual joint training can improve the performance in various cases.", "creator": "LaTeX with hyperref package"}}}