{"id": "1603.06503", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Static and Dynamic Feature Selection in Morphosyntactic Analyzers", "abstract": "we study the paradigm involves potential feature fitting methods extending morphosyntactic models under a number across evolutionary models. we compare a static type targeting features to a dynamic ordering based locally constrained constraint statistics, especially we test the techniques to standalone taggers as well like collaboration systems via tagging and parsing. experiments on comb languages show that object selection can result beyond infinitely diverse combinations as well as higher priority and those conditions, could easily determine those dynamic situation works better given a static ordering prove that joint codes benefit positively than standalone trees. we also show that the same techniques that be used to assign which business categories to predict in order they maximize syntactic accuracy satisfying a group system. our final stages represent a substantial background covering the methodology of the strategy for agile ways, while eliminating the same range lowers both the number of implementations and the running time by up to 80 % in some cases.", "histories": [["v1", "Mon, 21 Mar 2016 17:20:34 GMT  (34kb)", "http://arxiv.org/abs/1603.06503v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bernd bohnet", "miguel ballesteros", "ryan mcdonald", "joakim nivre"], "accepted": false, "id": "1603.06503"}, "pdf": {"name": "1603.06503.pdf", "metadata": {"source": "CRF", "title": "Static and Dynamic Feature Selection in Morphosyntactic Analyzers", "authors": ["Bernd Bohnet", "Miguel Ballesteros", "Ryan McDonald", "Joakim Nivre"], "emails": ["bohnetbd@google.com,", "ryanmcd@google.com,", "miguel.ballesteros@upf.edu,", "joakim.nivre@lingfil.uu.se"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n06 50\n3v 1\n[ cs\n.C L\n] 2\n1 M\nar 2\n01 6"}, {"heading": "1 Introduction", "text": "Morphosyntactic tagging, whether limited to basic part-of-speech tags or using rich morphosyntactic features, is a fundamental task in natural language processing, used in a variety of applications from machine translation (Habash and Sadat, 2006) to information extraction (Banko et al., 2007). In addition, tagging can be the first step of a syntactic analysis, providing a shallow, non-hierarchical representation of syntactic structure.\nMorphosyntactic taggers tend to belong to one of two different paradigms: standalone taggers or joint taggers. Standalone taggers use narrow contextual representations, typically an n-gram window of fixed size.\nTo achieve state-of-the-art results, they employ sophisticated optimization techniques in combination with rich feature representations (Brants, 2000; Toutanova and Manning, 2000; Gime\u0301nez and Ma\u0300rquez, 2004; Mu\u0308ller et al., 2013). Joint taggers, on the other hand, combine morphosyntactic tagging with deeper syntactic processing. The most common case is parsers that predict constituency structures jointly with part-ofspeech tags (Charniak and Johnson, 2005; Petrov et al., 2006) or richer word morphology Goldberg and Tsarfaty (2008).\nIn dependency parsing, pipeline models have traditionally been the norm, but recent studies have shown that joint tagging and dependency parsing can improve accuracy of both (Lee et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Bohnet et al., 2013). Unfortunately, joint models typically increase the search space, making them more cumbersome than their pipeline equivalents. For instance, in the joint morphosyntactic transition-based parser of Bohnet et al. (2013), the number of parser actions increases linearly by the size of the part-of-speech and/or morphological label sets. For some languages this can be quite large. For example, Mu\u0308ller et al. (2013) report morphological tag sets of size 1,000 or more.\nThe promise of joint tagging and parsing is that by trading-off surface morphosyntactic predictions with longer distance dependency predictions, accuracy can be improved. However, it is unlikely that every decision will benefit from this trade-off. Local n-gram context is sufficient for many tagging decisions, and parsing decisions likely only benefit from morphological attributes that correlate with syntactic functions, like case, or those that constrain agreement, like gender or number. At the same time, while standalone morphosyntactic taggers require large feature sets in order to make accurate predictions, it may be\nthe case that fewer features are needed in a joint model, where these predictions are made in tandem with dependency decisions of larger scope. This naturally raises the question as to whether we can advantageously optimize feature sets at the tagger and parser levels in joint parsing systems to alleviate their inherent complexity.\nWe investigate this question in the context of the joint morphosyntactic parser of Bohnet et al. (2013), focusing on optimizing and compressing feature sets via greedy feature selection techniques, and explicitly contrasting joint systems with standalone taggers. The main findings emerging from our investigations are:\n\u2022 Feature selection works for standalone taggers but is more effective in a joint system. This holds for model size as well as tagging accuracy (and parsing accuracy as a result).\n\u2022 Dynamic feature selection strategies that take feature redundancy into account often lead to more compact models than static selection strategies with little loss in accuracy.\n\u2022 Similar selection techniques can also reduce the set of morphological attributes to be predicted jointly with parsing, reducing the size of the output space at no cost in accuracy.\nThe key to all our findings is that these techniques simultaneously compress model size and/or decrease the search space while increasing the underlying accuracy of tagging and parsing, even surpassing the state of the art in a variety of languages. With respect to the former, we observe empirical speed-ups upwards of 5x. With respect to the latter, we show that the resulting morphosyntactic taggers consistently beat stateof-the-art taggers across a number of languages."}, {"heading": "2 Related Work", "text": "Since morphosyntactc tagging interacts with other tasks such as word segmentation and syntactic parsing, there has been an increasing interest in joint models that integrate tagging with these other tasks. This line of work includes joint tagging and word segmentation (Zhang and Clark, 2008a), joint tagging and named entity recognition (Mo\u0301ra and Vincze, 2012), joint tagging and parsing (Lee et al., 2011; Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012;\nBohnet et al., 2013), and even joint word segmentation, tagging and parsing (Hatori et al., 2012). These studies often show improved accuracy from joint inference in one or all of the tasks involved.\nFeature selection has been a staple of statistical NLP since its beginnings, notably selection via frequency cut-offs in part-of-speech tagging (Ratnaparkhi, 1996). Since then efforts have been made to tie feature selection with model optimization. For instance, McCallum and Li (2003) used greedy forward selection with respect to model log-likelihood to select features for named entity recognition. Sparse priors, such as L1 regularization, are a common feature selection technique that trades off feature sparsity with the model\u2019s objective (Gao et al., 2007). Martins et al. (2011) extended such sparse regularization techniques to allow a model to deselect entire feature templates, potentially saving entire blocks of feature extraction computation. However, current systems still tend to employ millions of features without selection, relying primarily on model regularization to combat overfitting. Selection of morphological attributes has been carried out previously in Ballesteros (2013) and selection of features under similar constraints was carried out by Ballesteros and Bohnet (2014)."}, {"heading": "3 Feature Selection", "text": "The feature selection methods we investigate can all be viewed as greedy forward selection, shown in Figure 1. This paradigm starts from an empty set and considers features one by one. In each iteration, a model is generated from a training set and tested on a development set relative to some accuracy metric of interest. The feature under consideration is added if it increases this metric beyond some threshold and discarded otherwise.\nThis strategy is similar to the one implemented in MaltOptimizer (Ballesteros and Nivre, 2014). It differs from classic forward feature selection (Della Pietra et al., 1997) in that it does not test all features in parallel, but instead relies on an ordering of features as input. This is primarily for efficiency, as training models in parallel for a large number of feature templates is cumbersome.\nThe set of features, F , can be defined as fully instantiated input features, e.g., suffix=ing, or as feature templates, e.g., prefix, suffix, form, etc. Here we always focus on the latter. By reducing the number of feature templates, we are more likely to\npositively affect the runtime of feature extraction, as many computations can simply be removed."}, {"heading": "3.1 Static Feature Ordering", "text": "Our feature selection algorithm assumes a given order of the features to be evaluated against the objective function. One simple strategy is for a human to provide a static ordering on features, that is fixed for traversal. This means that we are testing feature templates in a predefined order and keeping those that improve the accuracy. Those that do not are discarded and never visited again. In Figure 1, this means that the Order(F ) function is fixed throughout the procedure. In our experiments, this fixed order is the same as in Table 1."}, {"heading": "3.2 Dynamic Feature Ordering", "text": "In text categorization, static feature selection based on correlation statistics is a popular technique (Yang and Pedersen, 1997). The typical strategy in such offline selectors is to rank each feature by its correlation to the output space, and to select the top K features. This strategy is often called max relevance, since it aims to optimize the features based solely to their predictive power.\nUnfortunately, the n best features selected by these algorithms might not provide the best result (Peng et al., 2005). Redundancy among the features is the primary reason for this, and Peng et al. develop the minimal redundancy maximal relevance (MRMR) technique to address this problem. The MRMR method tries to keep the redundancy minimal among the features. The approach is based on mutual information to compute the relevance of features and the redundancy of a feature in relation to a set of already selected features.\nThe mutual information of two discrete random variables X and Y is defined as follows\nI(X;Y ) = \u2211 x\u2208X \u2211 y\u2208Y p(x, y)log2 p(x,y) p(x)p(y)\nMax relevance selects the feature set X that maximizes the mutual information of feature templates Xi \u2208 X and the output classes c \u2208 C .\nmax D(X,C), D(X,C) = 1\n|X|\n\u2211\nXi\u2208X\nI(Xi;C).\nTo account for cases when features are highly redundant, and thus would not change much the discriminative power of a classifier, the following criterion can be added to minimize mutual information between selected feature templates:\nmin R(X), R(X) = 1\n|X2|\n\u2211\nXi,Xj\u2208X\nI(Xi;Xj)\nMinimal redundancy maximal relevance (MRMR) combines both objectives:\nmax \u03a6(D,R), \u03a6 = D(X,C)\u2212R(X)\nFor the greedy feature selection method outlined Figure 1, we can use the MRMR criteria to define the Order(F ) function. This leads to a dynamic feature selection technique as we update the order of features considered dynamically at each iteration, taking into account redundancy amongst already selected features.\nThis technique can be seen in the same light as greedy document summarization (Carbonell and Goldstein, 1998), where sentences are selected for a summary if they are both relevant and minimally redundant with sentences previously selected."}, {"heading": "4 Morphosyntactic Tagging", "text": "In this section, we describe the two systems for morphosyntactic tagging we use to compare feature selection techniques."}, {"heading": "4.1 Standalone Tagger", "text": "The first tagger is a standalone SVM tagger, whose training regime is shown in Figure 2. The tagger iterates up to k times (typically twice) over a sentence from left to right (line 2). This iteration is performed to allow the final assignment of tags to benefit from tag features on both sides of the target token. For each token of the sentence, the tagger initializes an n-best list and extracts features for the token in question (line 4- 7). In the innermost loop (line 9-11), the algo-\nrithm computes the score for each morphosyntactic tag and inserts a pair consisting of the morphosyntactic tag and its score into the n-best list. The algorithm returns a two dimensional array, where the first dimension contains the tokens and the second dimension contains the sorted lists of tag-score pairs. The tagger is trained online using MIRA (Crammer et al., 2006). When evaluating this system as a standalone tagger, we select the 1-best tag. This can be viewed as a multi-pass version of standard SVM-based tagging (Ma\u0300rquez and Gime\u0301nez, 2004)."}, {"heading": "4.2 Joint Dependency-Based Tagger", "text": "The joint tagger-parser follows the design of Bohnet et al. (2013), who augment an arcstandard transition-based dependency parser with the capability to select a part-of-speech tag and/or morphological tag for each input word from an n-best list of tags for that word. The tag selection is carried out when an input word is shifted onto the stack. Only the k highest-scoring tokens from each n-best list are considered, and only tags whose score is at most \u03b1 below the score of the best tag. In all experiments of this paper, we set k to 2 and \u03b1 to 0.25. The tagger-parser uses beam search to find the highest scoring combined tagging and dependency tree. When pruning the beam, it first extracts the 40 highest scoring distinct dependency trees and then up to 8 variants that differ only with respect to the tagging, a technique that was found by Bohnet et al. (2013) to give a good balance between tagging and parsing ambiguity in the beam. The tagger-parser is trained using the same online learning algorithm as the standalone tagger. When evaluating this system as a part-of-speech tagger, we consider only the finally selected tag sequence in the dependency tree output by the parser."}, {"heading": "5 Part-of-Speech Tagging Experiments", "text": "To simplify matters, we start by investigating feature selection for part-of-speech taggers, both in the context of standalone and joint systems. The main hypotheses we are testing is whether feature selection techniques are more powerful in joint morphosyntactic systems as opposed to standalone taggers. That is, the resulting models are both more compact and accurate. Additionally, we wish to empirically compare the impact of static versus dynamic feature selection techniques."}, {"heading": "5.1 Data Sets", "text": "We experiment with corpora from five different languages: Chinese, English, German, Hungarian and Russian. For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules, conversion tools and with the same split as in Zhang and Clark (2008b).1 For English, we use the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006).2 For German, we use the Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012). For Hungarian, we use the Szeged Dependency Treebank (Farkas et al., 2012). For Russian we use the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002)."}, {"heading": "5.2 Feature Templates", "text": "Table 1 presents the feature templates that we employed in our experiments (second column). The name of the functor indicates the purpose of the feature template. For instance, the functor form defines the word form. The argument specifies the location of the token, for instance, form(w+1) denotes the token to the right of the current token w.\nWhen more than one argument is given, the functor is applied to each defined position and the results are concatenated. Thus, form(w,w+1) expands to form(w)+form(w+1). The functor formlc denotes the form with all letters converted to lowercase and lem denotes the lemma of a word. The functors suffix1, suffix2,... and prefix1,... denote suffixes and prefixes of length 1, 2, ..., 5. The suffix1+uc, ... functors concatenates a suffix with a\n1Training: 001\u2013815, 1001\u20131136. Development: 886\u2013 931, 1148\u20131151. Test: 816\u2013885, 1137\u20131147.\n2Training: 02-21. Development: 24. Test: 23.\nvalue that indicates uppercase or lowercase word. The functors pos and mor denote part-of-speech tags and morphological tags, respectively. The tags to the right of the current position are available as features in the second iteration of the standalone tagger as well as in the final resolution stage in the joint system. Patterns of the form ci denote the ith character. Finally, the functor number denotes a sequence of numbers, with optional periods and commas."}, {"heading": "5.3 Main Results", "text": "In our experiments we make a division of the training corpora into 80% for training and 20% for development. Therefore, in each iteration a model is trained over 80% of the training corpus and tested on 20%.3 For feature selection, if the outcome of the newly trained model is better than the best result so far, then the feature is added to the feature model; otherwise, it is not. A model has to show improvement of at least 0.02 on part-ofspeech tagging accuracy to count as better.4\nTable 1 (columns under Part-of-Speech) shows the features that the algorithms selected for each language and each system, and Table 2 shows the performance on the development set. We primarily report part-of-speech tagging accuracy (POS), but also report unlabeled (UAS) and labeled (LAS) attachment scores (Buchholz and Marsi, 2006) to show the effect of improved taggers on parsing quality. Additionally, Table 2 contains the number of features selected (#).\nThe first conclusion to draw is that the feature selection algorithms work for both standalone and joint systems. The number of features selected is drastically reduced. The dynamic MRMR feature selection technique for the joint system compresses the model by as much as 78%. This implies faster inference (smaller dot products and less feature extraction) and a smaller memory footprint. In general, joint systems compress more\n3There is also a held-out test set for evaluation, which is the standard test set provided and depicted in Section 5.1.\n4All the experiments were carried out on a CPU Intel Xeon 3.4 Ghz with 6 cores. Since the feature selection experiments require us to train a large number of parsing and/or tagging models, we needed to find a realistic training setup that gives us a sufficient accuracy level while maintaining a reasonable speed. After some preliminary experiments, we selected a beam size of 8 and 12 training iterations for the feature selection experiments while the final models are tested with a beam size of 40 and 25 training iterations. The size k of the second beam for alternative tag sequences is kept at 8 for all experiments and the threshold \u03b1 at 0.25.\nover their standalone counterpart, by about 20%. Furthermore, the dynamic technique tends to have slightly more compression.\nThe accuracies of the joint tagger-parser are in general superior to the ones obtained by the standalone tagger, as noted by Bohnet and Nivre (2012). In terms of tagging accuracy, static selection works slightly better for Chinese, German and Hungarian while dynamic MRMR works best for English and Russian (Table 2). Moreover, the standalone tagger selects several feature templates that requires iterating over the sentence, such as pos(w+1), pos(w+2), whereas the feature templates selected by the joint system contain significantly fewer of these features. This shows that a joint system is less reliant on context features to resolve many ambiguities that previously needed a wider context. This is almost certainly due to the global contextual information pushed to the tagger via parsing decisions. As a consequence, the preprocessing tagger can be simplified and we need to conduct only one iteration over the sentence while maintaining the same accuracy level. Interestingly, the dynamic MRMR technique tends to select less form features, which have the largest number of realizable values and thus model parameters.\nTable 3 compares the performance of our two taggers with two state-of-the-art taggers. Except for English, the joint tagger consistently outperforms the Stanford tagger and MarMot: for Chinese by 0.3, for German by 0.38, for Hungarian by 0.25 and for Russian by 0.75. Table 4 compares the resulting parsing accuracies to state-ofthe-art dependency parsers for English and Chinese, showing that the results are in line with or higher than the state of the art."}, {"heading": "6 Morphological Tagging Experiments", "text": "The joint morphology and syntactic inference requires the selection of morphological attributes (case, number, etc.) and the selection of features to predict the morphological attributes. In past work on joint morphosyntactic parsing, all morphological attributes are predicted jointly with syntactic dependencies (Bohnet et al., 2013). However, this could lead to unnecessary complexity as only a subset of the attributes are likely to influence parsing decisions, and vice versa.\nIn this section we investigate whether feature selection methods can also be used to reduce the\nset of morphological attributes that are predicted as part of a joint system. For instance, consider a language that has the following attributes: case, gender, number, animacy. And let us say that language does not have gender agreement. Then likely only case and number will be useful in a joint system, and the gender and animacy attributes can be predicted independently. This could substantially improve the speed of the joint model \u2013 on top of standard feature selection \u2013 as the size of the morphosyntactic tag set will be reduced significantly."}, {"heading": "6.1 Data Sets", "text": "We use the data sets listed in subsection 5.1 for the languages that provide morphological annotation, which are German, Hungarian and Russian."}, {"heading": "6.2 Main Results: Attribute Selection", "text": "For the selection of morphological attributes (e.g. case, number, tense), we explore a simple method\nthat departs slightly from those in Section 3. In particular, we do not run greedy forward selection. Instead, we compute accuracy improvements for each attribute offline. We then independently select attributes based on these values. Our initial design was a greedy forward attribute selection, but we found experimentally that independent attribute selection worked best.\nWe run 10-fold cross-validation experiments on the training set where 90% of training set is used for training and 10% for testing. Here we simply test for each attribute independently whether its inclusion in a joint morphosyntactic parsing system increases parsing accuracy (LAS/UAS) by a statistically significant amount. If it does, then it is included. We applied cross validation to obtain more reliable results than with the development sets as some improvements where small, e.g., gender and number in German are within the range of the standard deviation results on the development set. We use parsing accuracy as we are primarily testing whether a subset of attributes can be used in place of the full set in joint morphosyntactic parsing.\nEven though this method only tests an attribute\u2019s contribution independently of other attributes, we found experimentally that this was never a problem. For instance, in German, without any morphologic attribute, we get a baseline of 89.18 LAS; when we include the attribute case, we get 89.45 LAS; and when we include number, we get 89.32 LAS. When we include both case and number, we get 89.60 LAS.\nTable 5 shows which attributes were selected. We include an attribute when the cross-validation experiment shows an improvement of at least 0.1 with a statistical significance of 0.01 or better (indicated in the table by **). Some borderline cases remain such as for Russian passive where we observed an accuracy gain of 0.2 but only a low statistical significance."}, {"heading": "6.3 Main Results: Feature Selection", "text": "Having fixed the set of attributes to be predicted jointly with the parser, we can turn our attention to optimizing the feature sets for morphosyntactic tagging. To this end, we again consider greedy forward selection with the static and dynamic strategies. Table 1 shows the selected features for the different languages where the grey boxes again\nmean that the feature was selected. Table 6 shows the performance on the development set. For German, the full template set performs best but only 0.04 better than static selection which performs nearly as well while reducing the template set by 68%. For Hungarian, all sets perform similarly while dynamic selection needs 86% less features. The top performing feature set for Russian is dynamic selection in a joint system which needs 81% less features. We observe again that dynamic selection tends to select less feature templates compared to static selection, but here both the full set of features and the set selected by static selection appear to have better accuracy on average.\nThe feature selection methods obtain significant speed-ups for the joint system. On the development sets we observed a speedup from 0.015 to 0.003 sec/sentence for Hungarian, from 0.014 to 0.004 sec/sentence for German, and from 0.015 to 0.006 sec/sentence for Russian. This represents a reduction in running time between 50 and 80%.\nTable 7 compares our system to other state-ofthe-art morphosyntactic parsers. We can see that on average the accuracies of our attribute/feature selection models are competitive or above the state-of-the art. The key result is that state of the art accuracy can be achieved with much leaner and faster models."}, {"heading": "7 Conclusions", "text": "There are several methodological lessons to learn from this paper. First, feature selection is generally useful as it leads to fewer features and faster tagging while maintaining state-of-the-art results. Second, feature selection is even more effective\nfor joint tagging-parsing, where it leads to even better results and smaller feature sets. In some cases, the number of feature templates is reduced by up to 80% with a correponding reduction in running time. Third, dynamic feature selection strategies (Peng et al., 2005) lead to more compact models than static feature selection, without significantly impacting accuracy. Finally, similar methods can be applied to morphological attribute selection leading to even leaner and faster models."}, {"heading": "Acknowledgement", "text": "Miguel Ballesteros is supported by the European Commission under the contract numbers FP7-ICT610411 (project MULTISENSOR) and H2020RIA-645012 (project KRISTINA)"}], "references": [{"title": "Automatic feature selection for agenda-based dependency parsing", "author": ["Ballesteros", "Bohnet2014] Miguel Ballesteros", "Bernd Bohnet"], "venue": "In Proceedings of the 25th International Conference on Computational Linguistics (COLING)", "citeRegEx": "Ballesteros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2014}, {"title": "MaltOptimizer: Fast and Effective Parser Optimization", "author": ["Ballesteros", "Nivre2014] Miguel Ballesteros", "Joakim Nivre"], "venue": "Natural Language Engineering", "citeRegEx": "Ballesteros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2014}, {"title": "Effective morphological feature selection with maltoptimizer at the spmrl 2013 shared task", "author": ["Miguel Ballesteros"], "venue": "In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages,", "citeRegEx": "Ballesteros.,? \\Q2013\\E", "shortCiteRegEx": "Ballesteros.", "year": 2013}, {"title": "Open information extraction for the web", "author": ["Banko et al.2007] Michele Banko", "Michael J Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni"], "venue": "In IJCAI,", "citeRegEx": "Banko et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Dependency treebank for Russian: Concept, tools, types of information", "author": ["Svetlana Grigorieva", "Nikolai Grigoriev", "Leonid Kreidlin", "Nadezhda Frid"], "venue": "In COLING,", "citeRegEx": "Boguslavsky et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Boguslavsky et al\\.", "year": 2000}, {"title": "Development of a dependency treebank for Russian and its possible applications in NLP", "author": ["Ivan Chardin", "Svetlana Grigorieva", "Nikolai Grigoriev", "Leonid Iomdin", "Leonid Kreidlin", "Nadezhda Frid"], "venue": null, "citeRegEx": "Boguslavsky et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Boguslavsky et al\\.", "year": 2002}, {"title": "Rule-based dependency parser refined by empirical and corpus statistics", "author": ["Leonid Iomdin", "Victor Sizov", "Leonid Tsinman", "Vadim Petrochenkov"], "venue": "In Proceedings of the International Conference", "citeRegEx": "Boguslavsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boguslavsky et al\\.", "year": 2011}, {"title": "A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["Bohnet", "Nivre2012] Bernd Bohnet", "Joakim Nivre"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Bohnet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bohnet et al\\.", "year": 2012}, {"title": "Joint morphological and syntactic analysis for richly inflected languages", "author": ["Bohnet et al.2013] Bernd Bohnet", "Joakim Nivre", "Igor Boguslavsky", "Richard Farkas", "Filip Ginter", "Jan Hajia"], "venue": null, "citeRegEx": "Bohnet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bohnet et al\\.", "year": 2013}, {"title": "TnT \u2013 a statistical part-of-speech tagger. In ANLP", "author": ["Thorsten Brants"], "venue": null, "citeRegEx": "Brants.,? \\Q2000\\E", "shortCiteRegEx": "Brants.", "year": 2000}, {"title": "CoNLL-X shared task on multilingual dependency parsing", "author": ["Buchholz", "Marsi2006] Sabine Buchholz", "Erwin Marsi"], "venue": "In CoNLL,", "citeRegEx": "Buchholz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Buchholz et al\\.", "year": 2006}, {"title": "The use of mmr, diversitybased reranking for reordering documents and producing summaries", "author": ["Carbonell", "Goldstein1998] Jaime Carbonell", "Jade Goldstein"], "venue": "In Proceedings of the 21st annual international ACM SIGIR conference on Re-", "citeRegEx": "Carbonell et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Carbonell et al\\.", "year": 1998}, {"title": "Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing", "author": ["Michael Collins", "Terry Koo"], "venue": "In CoNLL,", "citeRegEx": "Carreras et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "Coarse-to-fine n-best parsing and MaxEnt discriminative reranking", "author": ["Charniak", "Johnson2005] Eugene Charniak", "Mark Johnson"], "venue": "In ACL,", "citeRegEx": "Charniak et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Charniak et al\\.", "year": 2005}, {"title": "Online passive-aggressive algorithms", "author": ["Crammer et al.2006] Koby Crammer", "Ofer Dekel", "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Crammer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2006}, {"title": "Inducing features of random fields", "author": ["Vincent Della Pietra", "John Lafferty"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Pietra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1997}, {"title": "Dependency parsing of hungarian: Baseline results and challenges", "author": ["Veronika Vincze", "Helmut Schmid"], "venue": "In EACL,", "citeRegEx": "Farkas et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Farkas et al\\.", "year": 2012}, {"title": "A comparative study of parameter estimation methods for statistical natural language processing", "author": ["Gao et al.2007] Jianfeng Gao", "Galen Andrew", "Mark Johnson", "Kristina Toutanova"], "venue": "In ACL,", "citeRegEx": "Gao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2007}, {"title": "SVMTool: A general POS tagger generator based on support vector machines", "author": ["Gim\u00e9nez", "M\u00e0rquez2004] Jes\u00fas Gim\u00e9nez", "Llu\u0131\u0301s M\u00e0rquez"], "venue": null, "citeRegEx": "Gim\u00e9nez et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gim\u00e9nez et al\\.", "year": 2004}, {"title": "A single generative model for joint morphological segmentation and syntactic parsing", "author": ["Goldberg", "Tsarfaty2008] Yoav Goldberg", "Reut Tsarfaty"], "venue": "In ACL,", "citeRegEx": "Goldberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2008}, {"title": "Arabic preprocessing schemes for statistical machine translation", "author": ["Habash", "Sadat2006] Nizar Habash", "Fatiha Sadat"], "venue": null, "citeRegEx": "Habash et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Habash et al\\.", "year": 2006}, {"title": "Incremental joint pos tagging and dependency parsing in chinese", "author": ["Hatori et al.2011] Jun Hatori", "Takuya Matsuzaki", "Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "In IJCNLP,", "citeRegEx": "Hatori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hatori et al\\.", "year": 2011}, {"title": "Incremental joint approach to word segmentation, pos tagging, and dependency parsing in chinese", "author": ["Hatori et al.2012] Jun Hatori", "Takuya Matsuzaki", "Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "In ACL,", "citeRegEx": "Hatori et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hatori et al\\.", "year": 2012}, {"title": "Dynamic programming for linear-time incremental parsing", "author": ["Huang", "Sagae2010] Liang Huang", "Kenji Sagae"], "venue": "In ACL,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Efficient third-order dependency parsers", "author": ["Koo", "Collins2010] Terry Koo", "Michael Collins"], "venue": "In ACL,", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Simple semi-supervised dependency parsing", "author": ["Koo et al.2008] Terry Koo", "Xavier Carreras", "Michael Collins"], "venue": "In ACL,", "citeRegEx": "Koo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "A discriminative model for joint morphological disambiguation and dependency parsing", "author": ["Lee et al.2011] John Lee", "Jason Naradowsky", "David A. Smith"], "venue": "In ACL,", "citeRegEx": "Lee et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2011}, {"title": "Joint models for chinese pos tagging and dependency parsing", "author": ["Li et al.2011] Zhenghua Li", "Min Zhang", "Wanxiang Che", "Ting Liu", "Wenliang Chen", "Haizhou Li"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "A general pos tagger generator based on support vector machines. JMLR", "author": ["M\u00e0rquez", "Gim\u00e9nez2004] L M\u00e0rquez", "J Gim\u00e9nez"], "venue": null, "citeRegEx": "M\u00e0rquez et al\\.,? \\Q2004\\E", "shortCiteRegEx": "M\u00e0rquez et al\\.", "year": 2004}, {"title": "Turbo parsers: Dependency parsing by approximate variational inference", "author": ["Noah Smith", "Eric Xing", "Pedro Aguiar", "Mario Figueiredo"], "venue": "In EMNLP,", "citeRegEx": "Martins et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2010}, {"title": "Structured sparsity in structured prediction", "author": ["Noah A Smith", "Pedro MQ Aguiar", "M\u00e1rio AT Figueiredo"], "venue": "In EMNLP,", "citeRegEx": "Martins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2011}, {"title": "Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons", "author": ["McCallum", "Li2003] Andrew McCallum", "Wei Li"], "venue": "In Proceedings of the seventh conference on Natural language learn-", "citeRegEx": "McCallum et al\\.,? \\Q2003\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2003}, {"title": "Online learning of approximate dependency parsing algorithms", "author": ["McDonald", "Pereira2006] Ryan McDonald", "Fernando Pereira"], "venue": "In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics", "citeRegEx": "McDonald et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2006}, {"title": "Online largemargin training of dependency parsers", "author": ["Koby Crammer", "Fernando Pereira"], "venue": "In ACL,", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Joint part-of-speech tagging and named entity recognition using factor graphs", "author": ["M\u00f3ra", "Vincze2012] Gy\u00f6rgy M\u00f3ra", "Veronika Vincze"], "venue": null, "citeRegEx": "M\u00f3ra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "M\u00f3ra et al\\.", "year": 2012}, {"title": "Efficient higher-order crfs for morphological tagging", "author": ["M\u00fcller et al.2013] Thomas M\u00fcller", "Helmut Schmid", "Hinrich Sch\u00fctze"], "venue": "Proceedings of EMNLP", "citeRegEx": "M\u00fcller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2013}, {"title": "Feature Selection Based on Mutual Information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy", "author": ["Peng et al.2005] Hanchuan Peng", "Fuhui Long", "Chris Ding"], "venue": "In IEEE Transactions On Pattern Analysis And Machine In-", "citeRegEx": "Peng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Petrov et al.2006] Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein"], "venue": "In ACL,", "citeRegEx": "Petrov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "A maximum entropy part-of-speech tagger", "author": ["Adwait Ratnaparkhi"], "venue": "In EMNLP,", "citeRegEx": "Ratnaparkhi.,? \\Q1996\\E", "shortCiteRegEx": "Ratnaparkhi.", "year": 1996}, {"title": "Making ellipses explicit in dependency conversion for a german treebank", "author": ["Seeker", "Kuhn2012] Wolfgang Seeker", "Jonas Kuhn"], "venue": "In LREC,", "citeRegEx": "Seeker et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Seeker et al\\.", "year": 2012}, {"title": "Morphological and syntactic case in statistical dependency parsing", "author": ["Seeker", "Kuhn2013] Wolfgang Seeker", "Jonas Kuhn"], "venue": "Computational Linguistics,", "citeRegEx": "Seeker et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Seeker et al\\.", "year": 2013}, {"title": "An empirical study of semi-supervised structured conditional models for dependency parsing", "author": ["Suzuki et al.2009] Jun Suzuki", "Hideki Isozaki", "Xavier Carreras", "Michael Collins"], "venue": null, "citeRegEx": "Suzuki et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Suzuki et al\\.", "year": 2009}, {"title": "Enriching the knowledge sources used in a maximum entropy partof-speech tagger", "author": ["Toutanova", "Manning2000] Kristina Toutanova", "Christopher D. Manning"], "venue": "Joint SIGDAT Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Toutanova et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2000}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Yamada", "Matsumoto2003] Hiroyasu Yamada", "Yuji Matsumoto"], "venue": "In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT),", "citeRegEx": "Yamada et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2003}, {"title": "A comparative study on feature selection in text categorization", "author": ["Yang", "Pedersen1997] Yiming Yang", "Jan O Pedersen"], "venue": "In ICML,", "citeRegEx": "Yang et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Yang et al\\.", "year": 1997}, {"title": "Joint word segmentation and POS tagging using a single perceptron", "author": ["Zhang", "Clark2008a] Yue Zhang", "Stephen Clark"], "venue": "In ACL,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing", "author": ["Zhang", "Clark2008b] Yue Zhang", "Stephen Clark"], "venue": "In EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Transition-based parsing with rich non-local features", "author": ["Zhang", "Nivre2011] Yue Zhang", "Joakim Nivre"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "Online learning for inexact hypergraph search", "author": ["Zhang et al.2013] Hao Zhang", "Liang Huang", "Kai Zhao", "Ryan McDonald"], "venue": "In EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "machine translation (Habash and Sadat, 2006) to information extraction (Banko et al., 2007).", "startOffset": 71, "endOffset": 91}, {"referenceID": 9, "context": "To achieve state-of-the-art results, they employ sophisticated optimization techniques in combination with rich feature representations (Brants, 2000; Toutanova and Manning, 2000; Gim\u00e9nez and M\u00e0rquez, 2004; M\u00fcller et al., 2013).", "startOffset": 136, "endOffset": 227}, {"referenceID": 35, "context": "To achieve state-of-the-art results, they employ sophisticated optimization techniques in combination with rich feature representations (Brants, 2000; Toutanova and Manning, 2000; Gim\u00e9nez and M\u00e0rquez, 2004; M\u00fcller et al., 2013).", "startOffset": 136, "endOffset": 227}, {"referenceID": 37, "context": "The most common case is parsers that predict constituency structures jointly with part-ofspeech tags (Charniak and Johnson, 2005; Petrov et al., 2006) or richer word morphology Goldberg and Tsarfaty (2008).", "startOffset": 101, "endOffset": 150}, {"referenceID": 9, "context": "To achieve state-of-the-art results, they employ sophisticated optimization techniques in combination with rich feature representations (Brants, 2000; Toutanova and Manning, 2000; Gim\u00e9nez and M\u00e0rquez, 2004; M\u00fcller et al., 2013). Joint taggers, on the other hand, combine morphosyntactic tagging with deeper syntactic processing. The most common case is parsers that predict constituency structures jointly with part-ofspeech tags (Charniak and Johnson, 2005; Petrov et al., 2006) or richer word morphology Goldberg and Tsarfaty (2008).", "startOffset": 137, "endOffset": 535}, {"referenceID": 26, "context": "In dependency parsing, pipeline models have traditionally been the norm, but recent studies have shown that joint tagging and dependency parsing can improve accuracy of both (Lee et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Bohnet et al., 2013).", "startOffset": 174, "endOffset": 258}, {"referenceID": 21, "context": "In dependency parsing, pipeline models have traditionally been the norm, but recent studies have shown that joint tagging and dependency parsing can improve accuracy of both (Lee et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Bohnet et al., 2013).", "startOffset": 174, "endOffset": 258}, {"referenceID": 8, "context": "In dependency parsing, pipeline models have traditionally been the norm, but recent studies have shown that joint tagging and dependency parsing can improve accuracy of both (Lee et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Bohnet et al., 2013).", "startOffset": 174, "endOffset": 258}, {"referenceID": 7, "context": ", 2011; Bohnet and Nivre, 2012; Bohnet et al., 2013). Unfortunately, joint models typically increase the search space, making them more cumbersome than their pipeline equivalents. For instance, in the joint morphosyntactic transition-based parser of Bohnet et al. (2013), the number of parser actions", "startOffset": 32, "endOffset": 271}, {"referenceID": 7, "context": "We investigate this question in the context of the joint morphosyntactic parser of Bohnet et al. (2013), focusing on optimizing and compressing feature sets via greedy feature selection techniques, and explicitly contrasting joint systems with standalone taggers.", "startOffset": 83, "endOffset": 104}, {"referenceID": 22, "context": ", 2013), and even joint word segmentation, tagging and parsing (Hatori et al., 2012).", "startOffset": 63, "endOffset": 84}, {"referenceID": 38, "context": "Feature selection has been a staple of statistical NLP since its beginnings, notably selection via frequency cut-offs in part-of-speech tagging (Ratnaparkhi, 1996).", "startOffset": 144, "endOffset": 163}, {"referenceID": 17, "context": "Sparse priors, such as L1 regularization, are a common feature selection technique that trades off feature sparsity with the model\u2019s objective (Gao et al., 2007).", "startOffset": 143, "endOffset": 161}, {"referenceID": 34, "context": "Feature selection has been a staple of statistical NLP since its beginnings, notably selection via frequency cut-offs in part-of-speech tagging (Ratnaparkhi, 1996). Since then efforts have been made to tie feature selection with model optimization. For instance, McCallum and Li (2003) used greedy forward selection with respect to model log-likelihood to select features for named entity recognition.", "startOffset": 145, "endOffset": 286}, {"referenceID": 16, "context": "Sparse priors, such as L1 regularization, are a common feature selection technique that trades off feature sparsity with the model\u2019s objective (Gao et al., 2007). Martins et al. (2011) extended such sparse regularization techniques to allow a model to deselect entire feature templates, potentially saving entire blocks of feature extraction computation.", "startOffset": 144, "endOffset": 185}, {"referenceID": 2, "context": "Selection of morphological attributes has been carried out previously in Ballesteros (2013) and selection of features under similar constraints was carried out by Ballesteros and Bohnet (2014).", "startOffset": 73, "endOffset": 92}, {"referenceID": 2, "context": "Selection of morphological attributes has been carried out previously in Ballesteros (2013) and selection of features under similar constraints was carried out by Ballesteros and Bohnet (2014).", "startOffset": 73, "endOffset": 193}, {"referenceID": 36, "context": "(Peng et al., 2005).", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "The tagger is trained online using MIRA (Crammer et al., 2006).", "startOffset": 40, "endOffset": 62}, {"referenceID": 7, "context": "The joint tagger-parser follows the design of Bohnet et al. (2013), who augment an arcstandard transition-based dependency parser with the capability to select a part-of-speech tag and/or morphological tag for each input word from an n-best list of tags for that word.", "startOffset": 46, "endOffset": 67}, {"referenceID": 7, "context": "When pruning the beam, it first extracts the 40 highest scoring distinct dependency trees and then up to 8 variants that differ only with respect to the tagging, a technique that was found by Bohnet et al. (2013) to give a good balance between tagging and parsing ambiguity in the beam.", "startOffset": 192, "endOffset": 213}, {"referenceID": 16, "context": "For Hungarian, we use the Szeged Dependency Treebank (Farkas et al., 2012).", "startOffset": 53, "endOffset": 74}, {"referenceID": 4, "context": "For Russian we use the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002).", "startOffset": 42, "endOffset": 94}, {"referenceID": 5, "context": "For Russian we use the SynTagRus Treebank (Boguslavsky et al., 2000; Boguslavsky et al., 2002).", "startOffset": 42, "endOffset": 94}, {"referenceID": 6, "context": "2 For German, we use the Tiger Treebank (Brants et al., 2002) in the improved dependency conversion by Seeker and Kuhn (2012). For Hungarian, we use the Szeged Dependency Treebank (Farkas et al.", "startOffset": 41, "endOffset": 126}, {"referenceID": 8, "context": "ical attributes are predicted jointly with syntactic dependencies (Bohnet et al., 2013).", "startOffset": 66, "endOffset": 87}, {"referenceID": 22, "context": "case, number, tense), we explore a simple method System POS LAS UAS # Chinese Li et al. (2011) 93.", "startOffset": 78, "endOffset": 95}, {"referenceID": 20, "context": "55 Hatori et al. (2012) 93.", "startOffset": 3, "endOffset": 24}, {"referenceID": 20, "context": "55 Hatori et al. (2012) 93.94 81.20 Bohnet and Nivre (2012) 93.", "startOffset": 3, "endOffset": 60}, {"referenceID": 20, "context": "55 Hatori et al. (2012) 93.94 81.20 Bohnet and Nivre (2012) 93.24 77.91 81.42 joint-static 94.14 78.77 81.91 20 English McDonald et al. (2005) 90.", "startOffset": 3, "endOffset": 143}, {"referenceID": 20, "context": "55 Hatori et al. (2012) 93.94 81.20 Bohnet and Nivre (2012) 93.24 77.91 81.42 joint-static 94.14 78.77 81.91 20 English McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.", "startOffset": 3, "endOffset": 176}, {"referenceID": 20, "context": "55 Hatori et al. (2012) 93.94 81.20 Bohnet and Nivre (2012) 93.24 77.91 81.42 joint-static 94.14 78.77 81.91 20 English McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.", "startOffset": 3, "endOffset": 204}, {"referenceID": 20, "context": "55 Hatori et al. (2012) 93.94 81.20 Bohnet and Nivre (2012) 93.24 77.91 81.42 joint-static 94.14 78.77 81.91 20 English McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Koo and Collins (2010) 93.", "startOffset": 3, "endOffset": 232}, {"referenceID": 20, "context": "55 Hatori et al. (2012) 93.94 81.20 Bohnet and Nivre (2012) 93.24 77.91 81.42 joint-static 94.14 78.77 81.91 20 English McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Koo and Collins (2010) 93.04 Zhang and Nivre (2011) 92.", "startOffset": 3, "endOffset": 261}, {"referenceID": 20, "context": "55 Hatori et al. (2012) 93.94 81.20 Bohnet and Nivre (2012) 93.24 77.91 81.42 joint-static 94.14 78.77 81.91 20 English McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Koo and Collins (2010) 93.04 Zhang and Nivre (2011) 92.9 Martins et al. (2010) 93.", "startOffset": 3, "endOffset": 288}, {"referenceID": 20, "context": "55 Hatori et al. (2012) 93.94 81.20 Bohnet and Nivre (2012) 93.24 77.91 81.42 joint-static 94.14 78.77 81.91 20 English McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Koo and Collins (2010) 93.04 Zhang and Nivre (2011) 92.9 Martins et al. (2010) 93.26 Bohnet and Nivre (2012) 97.", "startOffset": 3, "endOffset": 318}, {"referenceID": 20, "context": "55 Hatori et al. (2012) 93.94 81.20 Bohnet and Nivre (2012) 93.24 77.91 81.42 joint-static 94.14 78.77 81.91 20 English McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Koo and Collins (2010) 93.04 Zhang and Nivre (2011) 92.9 Martins et al. (2010) 93.26 Bohnet and Nivre (2012) 97.33 92.44 93.38 Zhang et al. (2013) 93.", "startOffset": 3, "endOffset": 356}, {"referenceID": 20, "context": "55 Hatori et al. (2012) 93.94 81.20 Bohnet and Nivre (2012) 93.24 77.91 81.42 joint-static 94.14 78.77 81.91 20 English McDonald et al. (2005) 90.9 McDonald and Pereira (2006) 91.5 Huang and Sagae (2010) 92.1 Koo and Collins (2010) 93.04 Zhang and Nivre (2011) 92.9 Martins et al. (2010) 93.26 Bohnet and Nivre (2012) 97.33 92.44 93.38 Zhang et al. (2013) 93.50 Koo et al. (2008) \u2020 93.", "startOffset": 3, "endOffset": 380}, {"referenceID": 12, "context": "16 Carreras et al. (2008) \u2020 93.", "startOffset": 3, "endOffset": 26}, {"referenceID": 12, "context": "16 Carreras et al. (2008) \u2020 93.5 Suzuki et al. (2009) \u2020 93.", "startOffset": 3, "endOffset": 54}, {"referenceID": 11, "context": "81 Hungarian Farkas et al. (2012) 87.", "startOffset": 13, "endOffset": 34}, {"referenceID": 4, "context": "1 Bohnet et al. (2013) 97.", "startOffset": 2, "endOffset": 23}, {"referenceID": 4, "context": "32 Russian Boguslavsky et al. (2011) 86.", "startOffset": 11, "endOffset": 37}, {"referenceID": 4, "context": "32 Russian Boguslavsky et al. (2011) 86.0 90.0 Bohnet et al. (2013) 98.", "startOffset": 11, "endOffset": 68}, {"referenceID": 36, "context": "Third, dynamic feature selection strategies (Peng et al., 2005) lead to more compact models than static feature selection, without significantly impacting accuracy.", "startOffset": 44, "endOffset": 63}], "year": 2016, "abstractText": "We study the use of greedy feature selection methods for morphosyntactic tagging under a number of different conditions. We compare a static ordering of features to a dynamic ordering based on mutual information statistics, and we apply the techniques to standalone taggers as well as joint systems for tagging and parsing. Experiments on five languages show that feature selection can result in more compact models as well as higher accuracy under all conditions, but also that a dynamic ordering works better than a static ordering and that joint systems benefit more than standalone taggers. We also show that the same techniques can be used to select which morphosyntactic categories to predict in order to maximize syntactic accuracy in a joint system. Our final results represent a substantial improvement of the state of the art for several languages, while at the same time reducing both the number of features and the running time by up to 80% in some cases.", "creator": "LaTeX with hyperref package"}}}