{"id": "1603.06554", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Action-Affect Classification and Morphing using Multi-Task Representation Learning", "abstract": "social brain study carries on affect from traditional expressions, and comes as specialized on cognitive. this discipline focuses on reflex affect analysis. function does not replicate in groups. humans usually couple affect with context element in natural interactions ; for example, deaf woman could start talking and flirting. recognizing body structure in sequences relies efficient algorithms to indicate how subtle micro movements that differentiate between anger and sad reducing the psychological variations between acting actions. we approach from traditional classification for time - series data acquisition by executing a multi - task learning algorithms utilizing learns a shared representation that is well - adjusted for state - induced classification as well experience generation. for adaptive knowledge both require conditional delayed autonomous nets to be weak developmental block. we propose a new network that dominates the crbm model whilst fewer dedicated multi - task component cannot become 12 - task conditional restricted virtual clocks ( mtcrbms ). we evaluate our approach on merging publicly available datasets, primary body index dataset underlying the tower game setting, and show what classification score positively over the balance - of - the - elements, as useful considering the generative abilities of our model.", "histories": [["v1", "Mon, 21 Mar 2016 19:38:07 GMT  (3237kb,D)", "http://arxiv.org/abs/1603.06554v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.HC cs.LG", "authors": ["timothy j shields", "mohamed r amer", "max ehrlich", "amir tamrakar"], "accepted": false, "id": "1603.06554"}, "pdf": {"name": "1603.06554.pdf", "metadata": {"source": "CRF", "title": "Action-Affect Classification and Morphing using Multi-Task Representation Learning", "authors": ["Timothy J. Shields", "Mohamed R. Amer", "Max Ehrlich", "Amir Tamrakar"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Body Affect; Multi-Task Learning; Conditional Restricted Boltzmann Machines; Deep Learning;"}, {"heading": "1 Introduction", "text": "There has been so much activity in the field of affective computing that it already contributed to the creation of new research directions in affect analysis [1]. There are multiple research directions for analyzing human affect, including face data [2], audiovisual data [3], and body data [4]. One of the main challenges of affect analysis is that it does not occur in isolation. Humans usually couple affect with an action in natural interactions; for example, a person could be talking and smiling, or knocking on a door angrily as shown in Fig. 1. To be able to recognize body action-affect pairs, efficient temporal algorithms are needed to capture the micro movements that differentiate between happy and sad as well as capture the macro variations between the different actions. The focus of our work is on single-view, multi-task action-affect recognition from skeleton data\n? Both authors equally contributed to this work\nar X\niv :1\n60 3.\n06 55\n4v 1\n[ cs\n.C V\ncaptured by motion capture or Kinect sensors. Our work leverages the knowledge and work done by the graphics and animation community [5,6,7] and uses machine learning to enhance it and make it accessible for a wide variety of applications. We use the Body Affect dataset produced by [7] and the Tower Game [8] dataset as the test cases for our novel multi-task approach. Time series analysis is a difficult problem that requires efficient modeling, because of the large amounts of data it introduces. There are multiple approaches that designed features to reduce the data dimensionality, using mid-level features, and then use a simpler model to do classification [9,10]. We depart from these methods by proposing a model that learns a shared representation using multi-task learning. For this paper we choose Conditional Restricted Boltzmann Machines, which are non-linear generative models for modeling time series data, as our building block. They use an undirected model with binary latent variables connected to a number of visible variables. A CRBM-based generative model enables modeling short-term phenomenon. We propose a new hybrid model that enhances the CRBM model with multi-task, discriminative, components based on the work of [11]. This work leads to a superior classification performance, while also allowing us to model temporal dynamics efficiently. We evaluate our approach on the Body Affect [7] and Tower Game [8] datasets and show how our results are superior to the state-of-the-art.\nOur contributions:\n\u2013 Multi-task learning model for unimodal and multimodal time-series data. \u2013 Method for applying affect to a neutral skeleton (Sequence Morphing).\n\u2013 Evaluations on two multi-task public datasets [7,8].\nPaper organization: In sec. 2 we discuss prior work. In sec. 3 we give a brief background of similar models that motivate our approach, followed by a description of our model. In sec. 4 we describe the inference algorithm. In sec. 5 we specify our learning algorithm. In sec. 6 we show quantitative results of our approach, followed by the conclusion in sec. 7."}, {"heading": "2 Prior work", "text": "In this section we first review literature on activity recognition in RGB-D and Motion Capture Sequences; second we review Multi-Task Learning approaches; finally we review temporal, energy-based, representation learning.\nBody Affect Analysis: Initial work on activity recognition in RGB-D sequences has been popular in recent years with the availability of cheap depth sensors. Since initial work [12], there have been an increasing number of approaches addressing the problem of activity recognition using skeletal data [9]. Prior to activity recognition in RGB-D sequences, datasets were captured using motion capture sensors. During that time, research focused on graphics applications such as generating animation and transitions between animations using signal processing techniques rather than machine learning or computer vision. Their main goal was to generate natural looking skeletons for animation. Some methods used knowledge of signal processing to transform a neutral skeleton pose to reflect a certain emotion [5]. These methods were very constrained to the type of motion and were engineered to reproduce the same motions. Other work used a language based modeling of affect [6] where they modeled actions (verbs) and affect (adverbs) using a graph. They were able to produce results using a combination of low level functions to interpolate between example motions. More recent work [13] modeling non-stylized motion for affect communication used segmentation techniques which divided complex motions into a set of motion primitives that they used as dynamic features. Unlike our approach, their mid-level features were hand engineered rather than learned, which is very limited, does not scale and is prone to feature design flaws. More recent work such as [7] collected natural body affect datasets where they have varied identity, gender, emotion, and actions of the actors but not used it for classification.\nMulti-Task Learning: Multi-task learning is a natural approach for problems that require simultaneous solutions of several related problems [14]. Multi-task learning approaches can be grouped into two main sets. The first set focuses on regularizing the parameter space. The main assumption is that there is an optimal shared parameter space for all tasks. These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21]. The second set focuses on correlating relevant features jointly [22,23,24,25]. Other work focused on the schedule\nof which tasks should be learned [26]. Multi-task learning achieved good results on vision problems such as: person re-identification [27], multiple attribute recognition [28], and tracking [29]. Recently, Deep Multi-Task Learning (DMTL) emerged with the rise of deep learning. Deep Neural Networks (DNNs) were used to address multi-task learning and were applied successfully to facial landmark detection [30], scene classification [31], object localization and segmentation [32] and attribute prediction [33]. Other work used multi-task autoencoders [34] for object recognition in a generalized domain [35], where the tasks were the different domains. Other work used multi-task RNNs for interaction prediction in still images [36]. Most of the Deep Multi-task Learning approaches only focused on using DNN-based models applied to still images. Our approach is the first DMTL for temporal and multimodal sequence analysis.\nRepresentation Learning: Deep learning has been successfully applied to many problems [37]. Restricted Boltzmann Machines (RBMs) form the building blocks in energy-based deep networks [38,39]. In [38,39], the networks are trained using the Contrastive Divergence (CD) algorithm [40], which demonstrated the ability of deep networks to capture the distributions over the features efficiently and to learn complex representations. RBMs can be stacked together to form deeper networks known as Deep Boltzmann Machines (DBMs), which capture more complex representations. Recently, temporal models based on deep networks have been proposed, capable of modeling a rich set of time series analysis problems. These include Conditional RBMs (CRBMs) [41] and Temporal RBMs (TRBMs) [42,43,44]. CRBMs have been successfully used in both visual and audio domains. They have been used for modeling human motion [41], tracking 3D human pose [45], and phone recognition [46]. TRBMs have been applied for transferring 2D and 3D point clouds [47], and polyphonic music generation [48]."}, {"heading": "3 Model", "text": "Rather than immediately defining our Multi-Task CRBM (MT-CRBM) model, we discuss a sequence of models, gradually increasing in complexity, such that the different components of our final model can be understood in isolation. We start with the basic RBM model (sec. 3.1), then we extend the RBM to the CRBM model (sec. 3.2), then we further extend the CRBM to a new discriminative (DCRBM) model (sec. 3.3), then we extend the D-CRBM to our main multi-task model (MT-CRBM) (sec. 3.4), and finally we define a multi-task multimodal model (MTM-CRBM) (sec. 3.5)."}, {"heading": "3.1 Restricted Boltzmann Machines", "text": "RBMs [39], shown in Figure 2(a), define a probability distribution pR as a Gibbs distribution (1), where v is a vector of visible nodes, h is a vector of hidden nodes, ER is the energy function, and Z is the partition function. The parameters \u03b8 to be learned are a and b, the biases for v and h respectively, and the weights W .\nThe RBM is fully connected between layers, with no lateral connections. This architecture implies that v and h are factorial given one of the two vectors. This allows for the exact computation of pR(v|h) and pR(h|v).\npR(h,v) = exp[\u2212ER(h,v)]\nZ(\u03b8) ,\nZ(\u03b8) = \u2211 h,v exp[\u2212ER(h,v)], \u03b8 =\n[ {a,b} -bias, {W } -fully connected. ] (1)\nIn case of binary valued data vi is defined as a logistic function. In case of real valued data, vi is defined as a multivariate Gaussian distribution with a unit covariance. A binary valued hidden layer hj is defined as a logistic function such that the hidden layer becomes sparse [41,49]. The probability distributions over v and h are defined as in (2).\npR(vi = 1|h) = \u03c3(ai + \u2211 j hjwij), Binary,\npR(vi|h) = N (ai + \u2211 j hjwij , 1), Real,\npR(hj = 1|v) = \u03c3(bj + \u2211 i viwij), Binary.\n(2)\nThe energy function ER for the real valued v is defined as in (3).\nER(h,v) = \u2211 i (ai \u2212 vi)2 2 \u2212 \u2211 j bjhj \u2212 \u2211 i,j viwijhj (3)"}, {"heading": "3.2 Conditional Restricted Boltzmann Machines", "text": "CRBMs [41] are a natural extension of RBMs for modeling short term temporal dependencies. A CRBM, shown in Figure 2(b), is an RBM which takes into account history from the previous N time instances, t\u2212N, . . . , t\u2212 1, when considering time t. This is done by treating the previous time instances as additional inputs. Doing so does not complicate inference. Some approximations have been made to facilitate efficient training and inference, more details are available in\n[41]. A CRBM defines a probability distribution pC as a Gibbs distribution (4).\npC(ht,vt|v<t) = exp[\u2212EC(vt,ht|v<t)]Z(\u03b8) , Z(\u03b8) = \u2211 h,v exp[\u2212EC(ht,vt|v<t)], \u03b8 = [ {a,b} -bias, {A,B} -auto regressive, {W } -fully connected. ] (4)\nThe visible vectors from the previous N time instances, denoted as v<t, influence the current visible and hidden vectors. The probability distributions are defined in (5).\npC(vi|h,v<t) = N (ci + \u2211 j hjwij , 1),\npC(hj = 1|v,v<t) = \u03c3(dj + \u2211 i viwij),\nci = ai + \u2211 pApivp,<t , dj = bj + \u2211 pBpjvp,<t.\n(5)\nThe new energy function EC(ht,vt|v<t) in (6) is defined in a manner similar to that of the RBM (3).\nEC(ht,vt|v<t) = \u2211 i(ci \u2212 vi,t)2/2\u2212 \u2211 j djhj,t \u2212 \u2211 i,j vi,twijhj,t, (6)\nNote that A and B are matrices defining dynamic biases for vt and ht, consisting of concatenated vectors of previous time instances of a and b."}, {"heading": "3.3 Discriminative Conditional Restricted Boltzmann Machines", "text": "We extend the CRBMs to the D-CRBMs shown in Figure 2(c). D-CRBMs are based on the D-RBM model presented in in [11], generalized to account for temporal phenomenon using CRBMs. D-CRBMs define the probability distribution pDC as a Gibbs distribution (7).\npDC(yt,ht,vt|v<t) = exp[\u2212EDC(yt,ht,vt|v<t)]Z(\u03b8) ,\nZ(\u03b8) = \u2211 y,h,v exp[\u2212EDC(yt,ht,vt|v<t)], \u03b8 = [{a,b, s} -bias, {A,B} -auto regressive, {W ,U } -fully connected. ] (7) The probability distribution over the visible layer will follow the same distributions as in (5). The hidden layer h is defined as a function of the labels y and the visible nodes v. A new probability distribution for the classifier is defined to relate the label y to the hidden nodes h (8).\npDC(vi,t|ht,v<t) = N (ci + \u2211 j hjwij , 1),\npDC(hj,t = 1|yt,vt,v<t) = \u03c3(dj + \u2211 k yk,tujk + \u2211 i vi,twij),\npDC(yk,t|h) = exp[sk+\n\u2211 j ujkhj ]\u2211\nk\u2217 exp[sk\u2217+ \u2211 j ujk\u2217hj ] .\n(8)\nThe new energy function EDC is defined as in (9).\nEDC(yt,ht,vt|v<t) = EC(ht,vt|v<t)\ufe38 \ufe37\ufe37 \ufe38 Generative \u2212 \u2211 k skyk,t \u2212 \u2211 j,k\nhj,tujkyk,t\ufe38 \ufe37\ufe37 \ufe38 Discriminative\n(9)"}, {"heading": "3.4 Multi-Task Conditional Restricted Boltzmann Machines", "text": "In the same way the CRBMs can be extended to the DC-RBMs by adding a discriminative term to the model, we can extend the CRBMs to be multi-task MT-CRBMs Figure 3(a). MTCRBMs define the probability distribution pMT as a Gibbs distribution (10). The MT-CRBMs learn a shared representation layer for all tasks.\npMT(y L t ,ht,vt|v<t) = exp[\u2212EDC(yLt ,ht,vt|v<t)] Z(\u03b8) , Z(\u03b8) = \u2211 y,h,v exp[\u2212EMT(yLt ,ht,vt|v<t)], \u03b8 = [{a,b, sL} -bias, {A,B} -auto regressive, {W ,UL} -fully connected. ] (10) The probability distribution over the visible layer will follow the same distributions as in (8). The hidden layer h is defined as a function of the multi-task labels yL and the visible nodes v. A new probability distribution for the multitask classifier is defined to relate the multi-task labels yL to the hidden nodes h\nas shown in (11).\npMT(vi,t|ht,v<t) = N (ci + \u2211 j hjwij , 1),\npMT(hj,t = 1|yLt ,vt,v<t) = \u03c3(dj + \u2211 l,k y l k,tu l jk + \u2211 i vi,twij),\npMT(y l k,t|h) =\nexp[slk+ \u2211\nj u l jkhj ]\u2211\nk\u2217 exp[s l k\u2217+ \u2211 j u l jk\u2217hj ] .\n(11)\nThe energy for the model shown in Figure 3(a), EMT, is defined as in (12).\nEMT(y L t ,ht,vt|v<t) = EC(vt,ht|v<t)\ufe38 \ufe37\ufe37 \ufe38\nGenerative\n\u2212 \u2211 k,l slky l k,t \u2212 \u2211 j,k,l\nhj,tujky l k,t\ufe38 \ufe37\ufe37 \ufe38\nMulti-Task\n(12)"}, {"heading": "3.5 Multi-Task Multimodal Conditional Restricted Boltzmann Machines", "text": "We can naturally extend MT-CRBMs to MTM-CRBMs. A MTM-CRBMs combines a collection of unimodal MT-CRBMs, one for each visible modality. The hidden representations produced by the unimodal MT-CRBMs are then treated as the visible vector of a single fusion MT-CRBMs. The result is a MTMCRBM model that relates multiple temporal modalities to multi-task classification labels. MTMCRBMs define the probability distribution pMTM as a Gibbs distribution (13). The MTM-CRBMs learn an extra representation layer for each of the modalities, which learns a modality specific representation as well as the shared layer for all the tasks.\npMTM(y L t ,ht,h 1:M t ,v 1:M t |v1:M<t ) = exp[\u2212EMTM(yLt ,ht,h1:Mt ,v1:Mt |v1:M<t )]/Z(\u03b8),\nZ(\u03b8) = \u2211\ny,v,h exp[\u2212EMTM(yLt ,ht,h1:Mt ,v1:Mt |v1:M<t ),\n\u03b8 = [ {a1:M ,b1:M , e, sL} -bias, {A1:M ,B1:M ,C 1:M} -auto regressive, {W 1:M ,U 1:M ,W ,UL} -fully connected. ] (13)\nSimilar to the MT-CRBMs(11), the hidden layer h is defined as a function of the labels yL and the visible nodes v. A new probability distribution for the classifier is defined to relate the label yL to the hidden nodes h is defined as in\n(14).\npMTM(v m i,t|hmt ,vm<t) = N (cmi + \u2211 j h m j w m ij , 1),\npMTM(h m j,t = 1|yLt ,vmt ,vm<t) = \u03c3(dmj + \u2211 l,k y l k,tu l jk + \u2211 i v m i,tw m ij ),\npMTM(y l k,t|hmt ) =\nexp[slk+ \u2211 j u m,l jk h\nm j,t]\u2211\nl\u2217 exp[s l k\u2217+ \u2211 j u m,l jk\u2217h m j,t] ,\npMTM(hn,t = 1|yLt ,h1:Mt ,h1:M<t ) = \u03c3(fn + \u2211 l,k y l k,tu l nk + \u2211 m,j h m j,tw m jn),\npMTM(y l k,t|h) =\nexp[slk+ \u2211\nj u l nkhn]\u2211\nk\u2217 exp[s l k\u2217+ \u2211 n u l nk\u2217hn] .\n(14)\nwhere,\ncmi = a m i + \u2211 pA m p,iv m p,<t,\ndmj = b m j + \u2211 pB m p,jvp,<t,\nfn = en + \u2211 m,r C m r,nh m r,<t.\n(15)\nThe new energy function EMTM is defined in (16) similar to that of the MTCRBMs (10).\nEMTM(y L t ,ht,h 1:M t ,v 1:M t |v1:M<t ) = \u2211 m EMT(y L t ,h m t ,v\nm t |vm<t)\ufe38 \ufe37\ufe37 \ufe38\nUnimodal \u2212 \u2211 j fnhn,t \u2212 \u2211 j,k,m\nhmj,twjnhn,t\ufe38 \ufe37\ufe37 \ufe38 Fusion\n\u2212 \u2211 k,l slky l k,t \u2212 \u2211 n,k,l hn,tu l nky\nl k,t\ufe38 \ufe37\ufe37 \ufe38\nMulti-Task\n(16)"}, {"heading": "4 Inference", "text": "We first discuss inference for the MTM-CRBM since it is the most general case. To perform classification at time t in the MTM-CRBM given v1:M<t and v 1:M t we use a bottom-up approach, computing the mean of each node given the activation coming from the nodes below it; that is, we compute the mean of hmt using v m <t and vmt for each modality, then we compute the mean of ht using h 1:M <t , then we compute the mean of yLt for each task using ht, obtaining the classification probabilities for each task. Figure 4 illustrates our inference approach. Inference in the MT-CRBM is the same as the MTM-CRBM, except there is only one modality, and inference in the D-CRBM is the same as the MT-CRBM, except there is only one task."}, {"heading": "5 Learning", "text": "Learning our model is done using Contrastive Divergence (CD) [40], where \u3008\u00b7\u3009data is the expectation with respect to the data and \u3008\u00b7\u3009recon is the expectation with respect to the reconstruction. The learning is done using two steps: a bottom-up pass and a top-down pass using sampling equations from (8) for D-CRBM, (11) for MT-CRBM, and (14) for MTM-CRBM. In the bottomup pass the reconstruction is generated by first sampling the unimodal layers p(hmt,j = 1|vmt ,vm<t, yl) for all the hidden nodes in parallel. This is followed by sampling the fusion layer p(ht,n = 1|yLk,t,h1:Mt ,h1:M<t ). In the top-down pass the unimodal layer is generated using the activated fusion layer p(hmt,j = 1|ht, yLk,t). This is followed by sampling the visible nodes p(vmt,i|hmt ,vm<t) for all the visible nodes in parallel. The gradient updates are described in (17). Similarly learning of D-CRBM and MT-CRBM could be done.\n\u2206ai \u221d \u3008vmi \u3009data \u2212 \u3008vmi \u3009recon, \u2206bj \u221d \u3008hmj \u3009data \u2212 \u3008hmj \u3009recon, \u2206en \u221d \u3008hn\u3009data \u2212 \u3008hn\u3009recon, \u2206slk \u221d \u3008ylk\u3009data \u2212 \u3008ylk\u3009recon, \u2206Amp,i,<t \u221d vmk,<t(\u3008vmi,t\u3009data \u2212 \u3008vmi,t\u3009recon), \u2206Bmp,j,<t \u221d vmi,<t(\u3008hmj,t\u3009data \u2212 \u3008hmj,t\u3009recon), \u2206Cmr,n,<t \u221d hmj,<t(\u3008hn,t\u3009data \u2212 \u3008hn,t\u3009recon), \u2206wmi,j \u221d \u3008vmi hmj \u3009data \u2212 \u3008vmi hmj \u3009recon, \u2206wj,k \u221d \u3008hmj hn\u3009data \u2212 \u3008hmj hn\u3009recon, \u2206ul,mjk \u221d \u3008ylkhmj \u3009data \u2212 \u3008ylkhmj \u3009recon, \u2206uLnk \u221d \u3008ylkhn\u3009data \u2212 \u3008ylkhn\u3009recon.\n(17)"}, {"heading": "6 Experiments", "text": "We now describe the datasets in (sec 6.1), specify the implementation details in (sec 6.2), and present our quantitative results in (sec 6.3)."}, {"heading": "6.1 Datasets", "text": "Our problem is very particular in that we focus on multi-task learning for body affect. In the literature [4,9] most of the datasets were either single task for activity recognition, not publicly available, too few instances, or only RGB-D without skeleton. We found two available datasets to evaluate our approach that are multi-task. The first dataset is the Body Affect dataset [7], collected using a motion capture sensor, which consists of a set of actors performing several actions with different affects. The second dataset is the Tower Game [8], collected using a Kinect sensor, which consists of an interaction between two humans performing a cooperative task, with the goal of classifying different components of entrainment. In the following subsections we describe the datasets.\nBody Affect Dataset: This dataset [7] consists of a library of human movements captured using a motion capture sensor, annotated with actor, action, affect, and gender. The dataset was collected for studying human behavior and personality properties from human movement. The data consists of 30 actors (15 female and 15 male) each performing four actions (walking, knocking, lifting, and throwing) with each of four affect styles (angry, happy, neutral, and sad). For each actor, there are 40 data instances: 8 instances of walking (2 directions x 4 affects), 8 instances of knocking (2 repetitions x 4 affects), 8 instances of lifting (2 repetitions x 4 affects), 8 instances of throwing (2 repetitions x 4 affects), and 8 instances of the sequences (2 repetitions x 4 affects). For knocking and lifting and throwing there were 5 repetitions per data instances. Thus, the 24 records of knocking, lifting, and throwing contain 120 separate instances, yielding a total of 136 instances per actor and a total of 4,080 instances. We split dataset into 50% training using 15 actors and 50% testing using the other 15 actors.\nTower Game Dataset: This dataset [8] is a simple game of tower building often used in social psychology to elicit different kinds of interactive behaviors from the participants. It is typically played between two people working with a small fixed number of simple toy blocks that can be stacked to form various kinds of towers. The data consists of 112 videos which were divided into 1213 10-second segments indicating the presence or absence of these behaviors in each segment. Entrainment is the alignment in the behavior of two individuals and it involves simultaneous movement, tempo similarity, and coordination. Each measure was rated low, medium, or high for the entire 10 seconds segment. 50% of that data was used for training and 50% were used for testing. In this dataset we call each person\u2019s skeletal data a modality, where our goal is to model mocap-mocap representations."}, {"heading": "6.2 Implementation Details", "text": "For pre-processing the Tower Game dataset, we followed the same approach as [50] by forming a body centric transformation of the skeletons generated by the Kinect sensors. We use the 11 joints from the upper body of the two players since the tower game almost entirely involves only upper body actions and gestures are done using the upper body. We used the raw joint locations normalized with respect to a selected origin point. We use the same descriptor provided by [51,52]. The descriptor consists of 84 dimensions based on the normalized joints location, inclination angles formed by all triples of anatomically connected joints, azimuth angles between projections of the second bone and the vector on the plane perpendicular to the orientation of the first bone, bending angles between a basis vector, perpendicular to the torso, and joint positions. As for the Body Affect dataset we decided to use the full body centric representation [53] for motion capture sensors resulting in 42 dimensions per frame.\nFor the Body Affect dataset we trained a three-task model for the following three tasks: Action (AC) \u2208 {Walking, Knocking, Lifting, Throwing}, Affect (AF) \u2208 {Neutral, Happy, Sad, Angry}, Gender (G) \u2208 {Male, Female}. The data is split into a training set consisting of 50% of the instances, and a test set consisting of the remaining 50%. For the Tower Game dataset we trained a three-task model for the following tasks,: Tempo Similarity (TS), Coordination (C), and Simultaneous Movement (SM), each in {Low, Medium, High}. The data is split into a training set consisting of 50% of the instances, and a test set consisting of the remaining 50%.\nWe tuned our model parameters. For selecting the model parameters we used a grid search. We varied the number of hidden nodes per layer in the range of {10, 20, 30, 50, 70, 100, 200}, as well as the auto-regressive nodes in the range of {5, 10}, resulting a total of 2744 trained models. The best performing model on the Body Affect dataset has the following configuration v = 42, h = 30, v<t = 42 \u00d7 10 and the best performing model on the Tower Game dataset has the following configuration vm = 84, hm = 30, vm<t = 10\u00d7 84 for each of the modalities and for the fusion layer in the Tower Game dataset h1:M = 60, h = 60, h1:M<t = 10\u00d7 60.\nNote that in our MT-CRBM model, the tasks are assumed conditionally independent given the hidden representation. Thus the number of parameters needed for the hidden-label edges is H \u00b7 \u2211L k=1 Yk, where H is the dimensionality of the hidden layer and Yk is the number of classes for task k. Contrast this to the number of parameters needed if instead the tasks are flattened as a Cartesian product, H \u00b7 \u220fL k=1 Yk. Our factored representation of the multiple tasks uses only linearly many parameters instead of the exponentially many parameters needed for the flattened representation."}, {"heading": "6.3 Quantitative Results", "text": "We first define baselines and variants of the model, followed by the average classification accuracy results on the two datasets, and finally we provide some\ngenerative results, which we call Morphing, on the Body Affect dataset.\nBaselines and Variants: Since we compare our approach against the results presented in [8] we decided to use the same baselines they used. They used SVM classifiers on a combination of features. SVM+RAW: The first set of features consisted of first order static and dynamic handcrafted skeleton features. The static features are computed per frame. The features consist of relationships between all pairs of joints of a single actor, and the relationships between all pairs of joints of both the actors. The dynamic features are extracted per window (a set of 300 frames). In each window, they compute first and second order dynamics of each joint, as well as relative velocities and accelerations of pairs of joints per actor, and across actors. The dimensionality of their static and dynamic features is (257400 D). SVM+BoW100 and SVM+BoW300: To reduce their dimensionality they used, Bag-of-Words (BoW) (100 and 300 D) [54,52]. We also evaluate our approach using HCRF [55]. We define our own model\u2019s variants, D-CRBMs which is our single-task model presented in Section 3.3, MT-CRBMs which is our multi-task model presented in Section 3.4, MTM-CRBMs the multi-modal multi-task model presented in Section 3.5 and DM-CRBMs an extension to the D-CRBMs to be multimodal similar to MTM-CRBMs. We also add two new variants1 MT-CRBMs-Deep and MTM-CRBMs-Deep shown in Fig.5, which are\n1 This model is initially prototyped by [56] in the deep learning book.\na deeper version of the original models, by adding a task specific representation layer.\nClassification: For the Body Affect dataset, Table 1 shows the results of the baselines as well as our model and its variants. For the Tower Game dataset, Table 2 shows our average classification accuracy using different features and baselines combinations as well as the results from our models. We can see that the MT-CRBMs-Deep model outperforms all the other models for both cases, thereby demonstrating its effectiveness on predicting multi-task labels correctly. Furthermore, the MTM-CRBMs-Deep model outperforms all the SVM variants which used high dimensional handcrafted features, demonstrating its ability to learn a rich representation starting from the raw skeleton features. Note that only the MTM-CRBMs and MTM-CRBMs-Deep performed well on predicting the different tasks simultaneously with a relatively large margin better than the other models, using a shared representation that uses less parameters than our D-CRBMs model that treats all the labels flat.\nMorphing: Besides classifying action and affect, the MT-CRBMs model trained on the body affect dataset is also capable of generation. We demonstrate this by morphing a motion capture sequence of one affect to the same sequence with a different affect. For example, we could morph a Neutral Walk into a Happy Walk. We morph a sequence by sweeping through its frames, updating the vt vector of each frame in order. To update vt, we first compute the expected value of ht given v<t, vt and y L t , then compute the expected value of vt given v<t and ht. The v<t used is a linear blend of the original sequence and the newly generated sequence, so that the generated sequence retains the general shape of the original sequence. To evaluate the morphing process, we take a Neutral sequence for each action and each actor; morph it to a Happy, Sad, or Angry sequence of the same action type; and then compare the classifier probability of the target affect for the original Neutral sequence and the generated Happy, Sad, or Angry sequence. The average classifier probabilities before and after the morphing process for each action and target affect are shown in Table 3. Most of the classification probabilities for target affects increased as a result of morphing the Neutral sequences toward them, which means that our model was able to morph the sequences successfully."}, {"heading": "7 Conclusion and Future Work", "text": "We have proposed a collection of hybrid models, both discriminative and generative, that model the relationships in and distributions of temporal, multimodal, multi-task data. An extensive experimental evaluation of these models on two different datasets demonstrates the superiority of our approach over the stateof-the-art for multi-task classification of temporal data. This improvement in classification performance is accompanied by new generative capabilities and an efficient use of model parameters via factorization across tasks.\nThe generative capabilities of our approach enable new and interesting applications, such as the demonstrated sequence morphing. A future direction of work is to further explore and improve these generative applications of the models.\nThe factorization of tasks used in our approach means the number of parameters grows only linearly with the number of tasks and classes. This is seen to be significant when contrasted with a single-task model that uses a flattened Cartesian product of tasks, where the number of parameters grows exponentially with the number of tasks. Our factorized approach makes adding additional tasks a trivial matter."}, {"heading": "Acknowledgments", "text": "This research was partially developed with funding from the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laborotory\n(AFRL). The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government"}], "references": [{"title": "Affective Computing", "author": ["R.W. Picard"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "F.D.L.: Auto- mated face analysis for affective computing", "author": ["R. Calvo", "S. D\u2019Mello", "J. Gratch", "A. Kappas", "J.F. Cohn", "Torre"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions", "author": ["Z. Zeng", "M. Pantic", "G.I. Roisman", "T.S. Huang"], "venue": "IEEE Transactions on Pat- tern Analysis and Machine Intelligence", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Affective body expression perception and recognition: A survey", "author": ["A. Kleinsmith", "N. Bianchi-Berthouze"], "venue": "IEEE Transactions on Affective Computing", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Emotion from motion. In: GI", "author": ["K. Amaya", "A. Bruderlin", "T. Calvert"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Verbs and adverbs: Multidimensional motion interpolation using radial basis functions", "author": ["C. Rose", "B. Bodenheimer", "M.F. Cohen"], "venue": "Computer Graphics and Applications", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "A motion capture library for the study of identity, gender, and emotion perception from biological motion", "author": ["Y. MA", "H.M. PATERSON", "F.E. POLLICK"], "venue": "BMR", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "The tower game dataset: A multimodal dataset for analyzing social interaction predicates", "author": ["D.A. Salter", "A. Tamrakar", "M.R.A. Behjat Siddiquie", "A. Divakaran", "B. Lande", "D. Mehri"], "venue": "ACII", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Rgb-d-based action recog- nition datasets: A survey", "author": ["J. Zhang", "W. Li", "P.O. Ogunbona", "P. Wang", "C. Tang"], "venue": "arxiv", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "A survey of video datasets for human action and activity recognition", "author": ["J.M. Chaquet", "E.J. Carmona", "A. Fern\u00e1ndez-Caballero"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["H. Larochelle", "Y. Bengio"], "venue": "In: ICML", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Action recognition based on a bag of 3d points", "author": ["W. Li", "Z. Zhang", "Z. Liu"], "venue": "Com- puter Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Detecting affect from non-stylised body motions", "author": ["D. Bernhardt", "P. Robinson"], "venue": "ACII", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Regularized multi?task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "In: KDD", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C.A. Micchelli", "M. Pontil"], "venue": "JMLR", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Learning multiple visual tasks while discovering their structure", "author": ["C. Ciliberto", "L. Rosasco", "S. Villa"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "The benefit of multitask representa- tion learning", "author": ["A. Maurer", "M. Pontil", "B. Romera-Paredes"], "venue": "ArXiv", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Sparse coding for multitask and transfer learning", "author": ["A. Maurer", "M. Pontil", "B. Romera-Paredes"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H.D. III"], "venue": "ICML", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Clustered multi-task learning via alternating structure optimization", "author": ["J. Zhou", "J. Chen", "J. Ye"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Learning with whom to share in multi-task feature learn- ing", "author": ["Z. Kang", "K. Grauman"], "venue": "Timothy J. Shields", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Multilinear multitask learning", "author": ["B. Romera-Paredes", "H. Aung", "N. Bianchi-Berthouze", "M. Pontil"], "venue": "ICML", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "A unified perspective on multi-domain and multi-task learning", "author": ["Y. Yang", "T.M. Hospedales"], "venue": "ICLR", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Curriculum learning of multiple tasks", "author": ["A. Pentina", "V. Sharmanska", "C.H. Lampert"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Multi-task learning with low rank attribute embedding for person re-identification", "author": ["C. Su", "F. Yang", "S. Zhang", "Q. Tian", "L.S. Davis", "W. Gao"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Predicting multiple attributes via relative multi-task learning", "author": ["L. Chen", "Q. Zhang", "B. Li"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Robust visual tracking via structured multi-task sparse learning", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "IJCV", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Facial landmark detection by deep multi- task learning", "author": ["Z. Zhang", "P. Luo", "C.C. Loy", "X. Tang"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Scalable multitask representation learning for scene classification", "author": ["M. Lapin", "B. Schiele", "M. Hein"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Deep joint task learning for generic object extraction", "author": ["X. Wang", "L. Zhang", "L. Lin", "Z. Liang", "W. Zuo"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Multi-task cnn model for attribute prediction", "author": ["A.H. Abdulnabi", "G. Wang", "J. Lu"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Representation learning via semi-supervised autoencoder for multi-task learning", "author": ["F. Zhuang", "D. Luo", "X. Jin", "H. Xiong", "P. Luo", "Q. He"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Domain generalization for object recognition with multi-task autoencoders", "author": ["M. Ghifary", "W.B. Kleijn", "M. Zhang", "D. Balduzzi"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Multi-task recurrent neural network for immediacy prediction", "author": ["X. Chu", "W. Ouyang", "W. Yang", "X. Wang"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "FTML", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In: Science", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "In: NC", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2002}, {"title": "Two distributed-state models for gen- erating high-dimensional time series", "author": ["G.W. Taylor", "G.E. Hinton", "S.T. Roweis"], "venue": "Journal of Machine Learning Research", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Learning multilevel distributed representations for high-dimensional sequences", "author": ["I. Sutskever", "G.E. Hinton"], "venue": "AISTATS", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "The recurrent temporal restricted boltzmann machine", "author": ["I. Sutskever", "G. Hinton", "G. Taylor"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2008}, {"title": "Temporal autoencoding restricted boltzmann machine", "author": ["C. Hausler", "A. Susemihl"], "venue": "CoRR", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Dynamical binary latent variable models for 3d human pose tracking", "author": ["G.W. Taylor", "et. al"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2010}, {"title": "Phone recognition using restricted boltzmann ma- chines", "author": ["A.R. Mohamed", "G.E. Hinton"], "venue": "In: ICASSP", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2009}, {"title": "Facial expression transfer with input-output temporal restricted boltzmann machines", "author": ["M.D. Zeiler", "L.S.G.W. Taylor", "I. Matthews", "R. Fergus"], "venue": "Action-Affect Classification and Morphing", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2011}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N.B. Lewandowski", "Y. Bengio", "P. Vincent"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "Deep boltzmann machine", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "AISTATS", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2009}, {"title": "Moddrop: adaptive multi-modal gesture recognition", "author": ["N. Neverova", "C. Wolf", "G. Taylor", "F. Nebout"], "venue": "In: PAMI", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Multi-scale deep learning for gesture detection and localization", "author": ["N. Neverova", "C. Wolf", "G.W. Taylor", "F. Nebout"], "venue": "ECCV-W", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2014}, {"title": "The moving pose: An efficient 3d kinematics descriptor for low-latency action recognition and detection", "author": ["M. Zanfir", "M. Leordeanu", "C. Sminchisescu"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}, {"title": "Skeleton-intrinsic symmetrization of shapes", "author": ["Q. Zheng", "Z. Hao", "H. Huang", "K. Xu", "H. Zhang", "D. Cohen-Or", "B. Chen"], "venue": "Computer Graphics Forum. Vol- ume", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2015}, {"title": "Unsupervised learning of human action categories using spatial-temporal words", "author": ["J. Niebles", "H. Wang", "L. Fei-Fei"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2008}, {"title": "Hidden conditional random fields for gesture recognition", "author": ["S.B. Wang", "A. Quattoni", "L.P. Morency", "D. Demirdjian", "T. Darrell"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2006}, {"title": "Deep learning. Book in preparation for MIT Press (2016", "author": ["Y.B. Ian Goodfellow", "A. Courville"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "There has been so much activity in the field of affective computing that it already contributed to the creation of new research directions in affect analysis [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "There are multiple research directions for analyzing human affect, including face data [2], audiovisual data [3], and body data [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "There are multiple research directions for analyzing human affect, including face data [2], audiovisual data [3], and body data [4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "There are multiple research directions for analyzing human affect, including face data [2], audiovisual data [3], and body data [4].", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "Examples from the Body Affect dataset [7] of a person Knocking with various affects: Neutral, Angry, Happy, and Sad.", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "Our work leverages the knowledge and work done by the graphics and animation community [5,6,7] and uses machine learning to enhance it and make it accessible for a wide variety of applications.", "startOffset": 87, "endOffset": 94}, {"referenceID": 5, "context": "Our work leverages the knowledge and work done by the graphics and animation community [5,6,7] and uses machine learning to enhance it and make it accessible for a wide variety of applications.", "startOffset": 87, "endOffset": 94}, {"referenceID": 6, "context": "Our work leverages the knowledge and work done by the graphics and animation community [5,6,7] and uses machine learning to enhance it and make it accessible for a wide variety of applications.", "startOffset": 87, "endOffset": 94}, {"referenceID": 6, "context": "We use the Body Affect dataset produced by [7] and the Tower Game [8] dataset as the test cases for our novel multi-task approach.", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "We use the Body Affect dataset produced by [7] and the Tower Game [8] dataset as the test cases for our novel multi-task approach.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "There are multiple approaches that designed features to reduce the data dimensionality, using mid-level features, and then use a simpler model to do classification [9,10].", "startOffset": 164, "endOffset": 170}, {"referenceID": 9, "context": "There are multiple approaches that designed features to reduce the data dimensionality, using mid-level features, and then use a simpler model to do classification [9,10].", "startOffset": 164, "endOffset": 170}, {"referenceID": 10, "context": "We propose a new hybrid model that enhances the CRBM model with multi-task, discriminative, components based on the work of [11].", "startOffset": 124, "endOffset": 128}, {"referenceID": 6, "context": "We evaluate our approach on the Body Affect [7] and Tower Game [8] datasets and show how our results are superior to the state-of-the-art.", "startOffset": 44, "endOffset": 47}, {"referenceID": 7, "context": "We evaluate our approach on the Body Affect [7] and Tower Game [8] datasets and show how our results are superior to the state-of-the-art.", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "\u2013 Evaluations on two multi-task public datasets [7,8].", "startOffset": 48, "endOffset": 53}, {"referenceID": 7, "context": "\u2013 Evaluations on two multi-task public datasets [7,8].", "startOffset": 48, "endOffset": 53}, {"referenceID": 11, "context": "Since initial work [12], there have been an increasing number of approaches addressing the problem of activity recognition using skeletal data [9].", "startOffset": 19, "endOffset": 23}, {"referenceID": 8, "context": "Since initial work [12], there have been an increasing number of approaches addressing the problem of activity recognition using skeletal data [9].", "startOffset": 143, "endOffset": 146}, {"referenceID": 4, "context": "Some methods used knowledge of signal processing to transform a neutral skeleton pose to reflect a certain emotion [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 5, "context": "Other work used a language based modeling of affect [6] where they modeled actions (verbs) and affect (adverbs) using a graph.", "startOffset": 52, "endOffset": 55}, {"referenceID": 12, "context": "More recent work [13] modeling non-stylized motion for affect communication used segmentation techniques which divided complex motions into a set of motion primitives that they used as dynamic features.", "startOffset": 17, "endOffset": 21}, {"referenceID": 6, "context": "More recent work such as [7] collected natural body affect datasets where they have varied identity, gender, emotion, and actions of the actors but not used it for classification.", "startOffset": 25, "endOffset": 28}, {"referenceID": 13, "context": "Multi-Task Learning: Multi-task learning is a natural approach for problems that require simultaneous solutions of several related problems [14].", "startOffset": 140, "endOffset": 144}, {"referenceID": 14, "context": "These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21].", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21].", "startOffset": 122, "endOffset": 126}, {"referenceID": 16, "context": "These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21].", "startOffset": 217, "endOffset": 233}, {"referenceID": 17, "context": "These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21].", "startOffset": 217, "endOffset": 233}, {"referenceID": 18, "context": "These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21].", "startOffset": 217, "endOffset": 233}, {"referenceID": 19, "context": "These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21].", "startOffset": 217, "endOffset": 233}, {"referenceID": 20, "context": "These approaches regularize the parameter space by using a specific loss [15], methods that manually define relationships [16], or more automatic ways that estimate the latent structure of relationships between tasks [17,18,19,20,21].", "startOffset": 217, "endOffset": 233}, {"referenceID": 21, "context": "The second set focuses on correlating relevant features jointly [22,23,24,25].", "startOffset": 64, "endOffset": 77}, {"referenceID": 22, "context": "The second set focuses on correlating relevant features jointly [22,23,24,25].", "startOffset": 64, "endOffset": 77}, {"referenceID": 23, "context": "The second set focuses on correlating relevant features jointly [22,23,24,25].", "startOffset": 64, "endOffset": 77}, {"referenceID": 24, "context": "The second set focuses on correlating relevant features jointly [22,23,24,25].", "startOffset": 64, "endOffset": 77}, {"referenceID": 25, "context": "of which tasks should be learned [26].", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "Multi-task learning achieved good results on vision problems such as: person re-identification [27], multiple attribute recognition [28], and tracking [29].", "startOffset": 95, "endOffset": 99}, {"referenceID": 27, "context": "Multi-task learning achieved good results on vision problems such as: person re-identification [27], multiple attribute recognition [28], and tracking [29].", "startOffset": 132, "endOffset": 136}, {"referenceID": 28, "context": "Multi-task learning achieved good results on vision problems such as: person re-identification [27], multiple attribute recognition [28], and tracking [29].", "startOffset": 151, "endOffset": 155}, {"referenceID": 29, "context": "Deep Neural Networks (DNNs) were used to address multi-task learning and were applied successfully to facial landmark detection [30], scene classification [31], object localization and segmentation [32] and attribute prediction [33].", "startOffset": 128, "endOffset": 132}, {"referenceID": 30, "context": "Deep Neural Networks (DNNs) were used to address multi-task learning and were applied successfully to facial landmark detection [30], scene classification [31], object localization and segmentation [32] and attribute prediction [33].", "startOffset": 155, "endOffset": 159}, {"referenceID": 31, "context": "Deep Neural Networks (DNNs) were used to address multi-task learning and were applied successfully to facial landmark detection [30], scene classification [31], object localization and segmentation [32] and attribute prediction [33].", "startOffset": 198, "endOffset": 202}, {"referenceID": 32, "context": "Deep Neural Networks (DNNs) were used to address multi-task learning and were applied successfully to facial landmark detection [30], scene classification [31], object localization and segmentation [32] and attribute prediction [33].", "startOffset": 228, "endOffset": 232}, {"referenceID": 33, "context": "Other work used multi-task autoencoders [34] for object recognition in a generalized domain [35], where the tasks were the different domains.", "startOffset": 40, "endOffset": 44}, {"referenceID": 34, "context": "Other work used multi-task autoencoders [34] for object recognition in a generalized domain [35], where the tasks were the different domains.", "startOffset": 92, "endOffset": 96}, {"referenceID": 35, "context": "Other work used multi-task RNNs for interaction prediction in still images [36].", "startOffset": 75, "endOffset": 79}, {"referenceID": 36, "context": "Representation Learning: Deep learning has been successfully applied to many problems [37].", "startOffset": 86, "endOffset": 90}, {"referenceID": 37, "context": "Restricted Boltzmann Machines (RBMs) form the building blocks in energy-based deep networks [38,39].", "startOffset": 92, "endOffset": 99}, {"referenceID": 38, "context": "Restricted Boltzmann Machines (RBMs) form the building blocks in energy-based deep networks [38,39].", "startOffset": 92, "endOffset": 99}, {"referenceID": 37, "context": "In [38,39], the networks are trained using the Contrastive Divergence (CD) algorithm [40], which demonstrated the ability of deep networks to capture the distributions over the features efficiently and to learn complex representations.", "startOffset": 3, "endOffset": 10}, {"referenceID": 38, "context": "In [38,39], the networks are trained using the Contrastive Divergence (CD) algorithm [40], which demonstrated the ability of deep networks to capture the distributions over the features efficiently and to learn complex representations.", "startOffset": 3, "endOffset": 10}, {"referenceID": 39, "context": "In [38,39], the networks are trained using the Contrastive Divergence (CD) algorithm [40], which demonstrated the ability of deep networks to capture the distributions over the features efficiently and to learn complex representations.", "startOffset": 85, "endOffset": 89}, {"referenceID": 40, "context": "These include Conditional RBMs (CRBMs) [41] and Temporal RBMs (TRBMs) [42,43,44].", "startOffset": 39, "endOffset": 43}, {"referenceID": 41, "context": "These include Conditional RBMs (CRBMs) [41] and Temporal RBMs (TRBMs) [42,43,44].", "startOffset": 70, "endOffset": 80}, {"referenceID": 42, "context": "These include Conditional RBMs (CRBMs) [41] and Temporal RBMs (TRBMs) [42,43,44].", "startOffset": 70, "endOffset": 80}, {"referenceID": 43, "context": "These include Conditional RBMs (CRBMs) [41] and Temporal RBMs (TRBMs) [42,43,44].", "startOffset": 70, "endOffset": 80}, {"referenceID": 40, "context": "They have been used for modeling human motion [41], tracking 3D human pose [45], and phone recognition [46].", "startOffset": 46, "endOffset": 50}, {"referenceID": 44, "context": "They have been used for modeling human motion [41], tracking 3D human pose [45], and phone recognition [46].", "startOffset": 75, "endOffset": 79}, {"referenceID": 45, "context": "They have been used for modeling human motion [41], tracking 3D human pose [45], and phone recognition [46].", "startOffset": 103, "endOffset": 107}, {"referenceID": 46, "context": "TRBMs have been applied for transferring 2D and 3D point clouds [47], and polyphonic music generation [48].", "startOffset": 64, "endOffset": 68}, {"referenceID": 47, "context": "TRBMs have been applied for transferring 2D and 3D point clouds [47], and polyphonic music generation [48].", "startOffset": 102, "endOffset": 106}, {"referenceID": 38, "context": "RBMs [39], shown in Figure 2(a), define a probability distribution pR as a Gibbs distribution (1), where v is a vector of visible nodes, h is a vector of hidden nodes, ER is the energy function, and Z is the partition function.", "startOffset": 5, "endOffset": 9}, {"referenceID": 40, "context": "A binary valued hidden layer hj is defined as a logistic function such that the hidden layer becomes sparse [41,49].", "startOffset": 108, "endOffset": 115}, {"referenceID": 48, "context": "A binary valued hidden layer hj is defined as a logistic function such that the hidden layer becomes sparse [41,49].", "startOffset": 108, "endOffset": 115}, {"referenceID": 40, "context": "CRBMs [41] are a natural extension of RBMs for modeling short term temporal dependencies.", "startOffset": 6, "endOffset": 10}, {"referenceID": 40, "context": "[41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "D-CRBMs are based on the D-RBM model presented in in [11], generalized to account for temporal phenomenon using CRBMs.", "startOffset": 53, "endOffset": 57}, {"referenceID": 39, "context": "Learning our model is done using Contrastive Divergence (CD) [40], where \u3008\u00b7\u3009data is the expectation with respect to the data and \u3008\u00b7\u3009recon is the expectation with respect to the reconstruction.", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "In the literature [4,9] most of the datasets were either single task for activity recognition, not publicly available, too few instances, or only RGB-D without skeleton.", "startOffset": 18, "endOffset": 23}, {"referenceID": 8, "context": "In the literature [4,9] most of the datasets were either single task for activity recognition, not publicly available, too few instances, or only RGB-D without skeleton.", "startOffset": 18, "endOffset": 23}, {"referenceID": 6, "context": "The first dataset is the Body Affect dataset [7], collected using a motion capture sensor, which consists of a set of actors performing several actions with different affects.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "The second dataset is the Tower Game [8], collected using a Kinect sensor, which consists of an interaction between two humans performing a cooperative task, with the goal of classifying different components of entrainment.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "Body Affect Dataset: This dataset [7] consists of a library of human movements captured using a motion capture sensor, annotated with actor, action, affect, and gender.", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "Tower Game Dataset: This dataset [8] is a simple game of tower building often used in social psychology to elicit different kinds of interactive behaviors from the participants.", "startOffset": 33, "endOffset": 36}, {"referenceID": 49, "context": "For pre-processing the Tower Game dataset, we followed the same approach as [50] by forming a body centric transformation of the skeletons generated by the Kinect sensors.", "startOffset": 76, "endOffset": 80}, {"referenceID": 50, "context": "We use the same descriptor provided by [51,52].", "startOffset": 39, "endOffset": 46}, {"referenceID": 51, "context": "We use the same descriptor provided by [51,52].", "startOffset": 39, "endOffset": 46}, {"referenceID": 52, "context": "As for the Body Affect dataset we decided to use the full body centric representation [53] for motion capture sensors resulting in 42 dimensions per frame.", "startOffset": 86, "endOffset": 90}, {"referenceID": 7, "context": "Baselines and Variants: Since we compare our approach against the results presented in [8] we decided to use the same baselines they used.", "startOffset": 87, "endOffset": 90}, {"referenceID": 53, "context": "SVM+BoW100 and SVM+BoW300: To reduce their dimensionality they used, Bag-of-Words (BoW) (100 and 300 D) [54,52].", "startOffset": 104, "endOffset": 111}, {"referenceID": 51, "context": "SVM+BoW100 and SVM+BoW300: To reduce their dimensionality they used, Bag-of-Words (BoW) (100 and 300 D) [54,52].", "startOffset": 104, "endOffset": 111}, {"referenceID": 54, "context": "We also evaluate our approach using HCRF [55].", "startOffset": 41, "endOffset": 45}, {"referenceID": 55, "context": "1 This model is initially prototyped by [56] in the deep learning book.", "startOffset": 40, "endOffset": 44}, {"referenceID": 51, "context": "4 SVM+BoW300[52] 39.", "startOffset": 12, "endOffset": 16}, {"referenceID": 54, "context": "5 HCRF[55] 44.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "3 SVM+Raw [8] 59.", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "5 SVM+BoW100 [8] 65.", "startOffset": 13, "endOffset": 16}, {"referenceID": 51, "context": "3 SVM+BoW300 [52] 54.", "startOffset": 13, "endOffset": 17}, {"referenceID": 54, "context": "8 HCRF[55] 67.", "startOffset": 6, "endOffset": 10}], "year": 2016, "abstractText": "Most recent work focused on affect from facial expressions, and not as much on body. This work focuses on body affect analysis. Affect does not occur in isolation. Humans usually couple affect with an action in natural interactions; for example, a person could be talking and smiling. Recognizing body affect in sequences requires efficient algorithms to capture both the micro movements that differentiate between happy and sad and the macro variations between different actions. We depart from traditional approaches for time-series data analytics by proposing a multi-task learning model that learns a shared representation that is well-suited for action-affect classification as well as generation. For this paper we choose Conditional Restricted Boltzmann Machines to be our building block. We propose a new model that enhances the CRBM model with a factored multi-task component to become Multi-Task Conditional Restricted Boltzmann Machines (MTCRBMs). We evaluate our approach on two publicly available datasets, the Body Affect dataset and the Tower Game dataset, and show superior classification performance improvement over the state-of-the-art, as well as the generative abilities of our model.", "creator": "LaTeX with hyperref package"}}}