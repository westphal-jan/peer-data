{"id": "1603.06598", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Stack-propagation: Improved Representation Learning for Syntax", "abstract": "traditional queue models typically resemble part - truth - speech ( pos ) coding by constructing features from coin - tuned signals. we emphasized that no static approach linguist must utilize pos logic given a regularizer of learned items. we demonstrate a preferred method for storing using stacked grammar of concepts which collectively described \" stack - propagation \". stack compared this to dependency parsing node tagging, where messages use the hash property of the tagger, toward a representation of any input tokens whose identification function. at proper speed, our parser does thus detect arbitrary pos tags. on 19 languages from the universal dependencies, tally score is 1. 3 % ( 104 ) shorter accurate than a state - of - the - land word - alignment approach and 89. 7 % more agile than her most comparable greedy model.", "histories": [["v1", "Mon, 21 Mar 2016 20:12:44 GMT  (709kb,D)", "http://arxiv.org/abs/1603.06598v1", null], ["v2", "Wed, 8 Jun 2016 01:39:25 GMT  (729kb,D)", "http://arxiv.org/abs/1603.06598v2", "10 pages"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yuan zhang", "david weiss"], "accepted": true, "id": "1603.06598"}, "pdf": {"name": "1603.06598.pdf", "metadata": {"source": "CRF", "title": "Stack-propagation: Improved Representation Learning for Syntax", "authors": ["Yuan Zhang", "David Weiss"], "emails": ["yuanzh@csail.mit.edu", "djweiss@google.com"], "sections": [{"heading": "1 Introduction", "text": "In recent years, transition-based dependency parsers powered by neural network scoring functions have dramatically increased the state-of-theart in terms of both speed and accuracy (Chen and Manning, 2014; Alberti et al., 2015; Weiss et al., 2015). Similar approaches also achieve state-ofthe-art in other NLP tasks, such as constituency parsing (Durrett and Klein, 2015) or semantic role labeling (FitzGerald et al., 2015). These approaches all share a common principle: replace hand-tuned conjunctions of traditional NLP feature templates with continuous approximations learned by the hidden layer of a feed-forward network.\n\u2217Research conducted at Google.\nHowever, state-of-the-art dependency parsers depend crucially on the use of predicted part-ofspeech (POS) tags. In the pipeline or stacking (Wolpert, 1992) method, these are predicted from an independently trained tagger and used as features in the parser. However, there are two main disadvantages of a pipeline: (1) errors from the POS tagger cascade into parsing errors, and (2) POS taggers often make mistakes precisely because they cannot take into account the syntactic context of a parse tree. The POS tags may also contain only coarse information, such as when using the universal tagset of Petrov et al. (2011).\nOne approach to solve these issues has been to avoid using POS tags during parsing, e.g. either using semi-supervised clustering instead of POS tags (Koo et al., 2008) or building recurrent representations of words using neural networks (Dyer et al., 2015; Ballesteros et al., 2015). However, the best accuracy for these approaches is still achieved by running a POS tagger over the data first and combining the predicted POS tags with additional representations. As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015). However, these approaches typically require sacrificing either efficiency or accuracy compared to the best pipeline model, and often they simply re-rank the predictions of a pipelined POS tagger.\nIn this work, we show how to improve accuracy for both POS tagging and parsing by incorporating stacking into the architecture of a feed-forward network. We propose a continuous form of stacking that allows for easy backpropagation down the pipeline across multiple tasks, a process we call\nar X\niv :1\n60 3.\n06 59\n8v 1\n[ cs\n.C L\n] 2\n1 M\nar 2\n01 6\n\u201cstack-propagation\u201d (Figure 1). At the core of this idea is that we use POS tags as regularization instead of features.\nOur model design for parsing is very simple: we use the hidden layer of a window-based POS tagging network as the representation of tokens in a greedy, transition-based neural network parser. Both networks are implemented with a refined version of the feed-forward network (Figure 3) from Chen and Manning (2014), as described in Weiss et al. (2015). We link the tagger network to the parser by translating traditional feature templates for parsing into feed-forward connections from the tagger to the parser (Figure 2). At training time, we unroll the parser decisions and apply stackpropagation by alternating between stochastic updates to the parsing or tagging objectives (Figure 4). The parser\u2019s representations of tokens are thus regularized to be individually predictive of POS tags, even as they are trained to be useful for parsing when concatenated and fed into the parser network.\nThe key advantage of our approach is that at test time, we do not require predicted POS tags for parsing. Instead, we run the tagger network over the entire sentence, and then dynamically connect the parser network to the tagger network based upon the discrete parser configurations as parsing unfolds. In this way, we avoid cascading POS tagging errors to the parser. In addition, because the parser re-uses the representation from the tagger, we can drop all lexicalized features from the parser network, leading to a compact, faster model.\nThe rest of the paper is organized as follows. In Section 2, we describe the layout of our combined\narchitecture. In Section 3, we introduce stackpropagation and show how we train our model. We evaluate our approach on 19 languages from the Universal Dependencies treebank in Section 4. We observe a >2% absolute gain in labeled accuracy compared to state-of-the-art, LSTM-based greedy parsers (Ballesteros et al., 2015) and a >1% gain compared to a state-of-the-art, graphbased method (Lei et al., 2014). We also evaluate our method on the Wall Street Journal, where we find that our architecture outperforms other greedy models, especially when only coarse POS tags from the universal tagset are provided during training. In Section 5, we systematically evaluate the different components of our approach to demonstrate the effectiveness of stack-propagation compared to traditional types of joint modeling. We also show that our approach leads to large reductions in cascaded errors from the POS tagger.\nWe hope that this work will motivate further research in combining traditional pipelined structured prediction models with deep neural architectures that learn intermediate representations in a task-driven manner. One important finding of this work is that, even without POS tags, our architecture outperforms recurrent approaches that build custom word representations using character-based LSTMs (Ballesteros et al., 2015). These results suggest that learning rich embeddings of words may not be as important as building an intermediate representation that takes multiple features of the surrounding context into account. Our results also suggest that deep models for dependency parsing may not discover POS classes when trained solely for parsing, even when it is fully within the capacity of the model. Designing architectures to apply stack-propagation in other coupled NLP tasks might yield significant accuracy improvements for deep learning."}, {"heading": "2 Continuous Stacking Model", "text": "In this section, we introduce a novel neural network model for parsing and tagging that incorporates POS tags as a regularization of learned implicit representations. The basic unit of our model (Figure 3) is a simple, feed-forward network that has been shown to work very well for parsing tasks (Chen and Manning, 2014; Weiss et al., 2015). The inputs to this unit are feature matrices which are embedded and passed as input to a hidden layer. The final layer is a softmax prediction.\n\u2026\u2026\nI ate a sandwich with cheese I ate asandwich with cheese PRON VERB DET NOUN ADP NOUN Parsing loss {\n{Window-based classifier PRON VERB DET NOUN NOUN\nI ate a sandwich with cheese I ate asandwich with cheese PRON VERB DET NOUN ADP NOUN Parsing loss { { I ate a sandwich with cheese I ate asandwich with cheese PRON VERB DET NOUN ADP NOUN Parsing loss { { I ate a sandwich with cheese I ate asandwich with cheese PRON VERB DET NOUN ADP NOUN Parsing loss { { {Transition-based parser I ate a snack today I ate ate a ate snack ate Transition-based parser\nI ate a snack today\n\u2026\u2026\nI ate a sandwich with cheese I ate asandwich with cheese PRON VERB DET NOUN ADP NOUN Parsing loss { {Window-based classifier PRON VERB DET NOUN NOUN I ate a sandwich with cheese I ate asandwich with cheese PRON VERB DET NOUN ADP NOUN Parsing loss { { I ate a sandwich with cheese I ate asandwich with cheese PRON VERB DET NOUN A P NOUN Parsing loss { { I ate a sandwich with cheese I ate asandwich with cheese PRON VERB DET NOUN ADP NOUN Parsing loss { { {Transition-based parser I ate a snack today I ate ate a ate snack ate Transition-based parser\nI ate a snack today \u2026\u2026\nI ate a sandwich with cheese I ate asandwich with cheese PRON VERB DET NOUN ADP NOUN Parsing loss { {Window-based classifier PRON VERB DET NOUN NOUN I ate a sandwich with cheese I ate asandwich with cheese PRON VERB DET NOUN ADP NOUN Parsing loss { { I ate a sandwich with cheese I ate asandwich with cheese PRON VERB DET NOUN ADP NOUN Parsing loss { { I ate a sandwich with cheese I ate asandwich with cheese PRON VERB DET NOUN ADP NOUN Parsing loss { { {Transition-based parser I ate a snack today I ate ate a ate snack ate Transition-based parser\nWe use two such networks in this work: a window-based version for tagging and a transition-based version for dependency parsing. In a traditional stacking (pipeline) approach, we would use the discrete predicted POS tags from the tagger as features in the parser (Chen and Manning, 2014). In our model, we instead feed the continuous hidden layer activations of the tagger network as input to the parser. The primary strength of our approach is that the parser has access to all of the features and information used by the POS tagger during training time, but it is allowed to make its own decisions at test time.\nTo implement this, we show how we can reuse feature templates from Chen and Manning (2014) to specify the feed-forward connections from the tagger network to the parser network. An interesting consequence is that because this structure is a function of the derivation produced by the parser, the final feed-forward structure of the stacked model is not known until run-time. However, because the connections for any specific parsing decision are fixed given the derivation, we can still extract examples for training off-line by unrolling the network structure from gold derivations. In other words, we can utilize our approach with the same simple stochastic optimization techniques used in prior works. Figure 2 shows a fully unrolled architecture on a simple example."}, {"heading": "2.1 The Tagger Network", "text": "As described above, our POS tagger follows the basic structure from prior work with embedding,\nW ords Feature extraction\nSuffixes\nC lusters\nEmbedding\nHidden (Relu)\nSoftmax\nhidden, and softmax layers. Like the \u201cwindowapproach\u201d network of Collobert et al. (2011), the tagger is evaluated per-token, with features extracted from a window of tokens surrounding the target. The input consists of a rich set of features for POS tagging that are deterministically extracted from the training data. As in prior work, the features are divided into groups of different sizes that share an embedding matrix E. Features for each group g are represented as a sparse matrix Xg with dimension F g \u00d7 V g, where F g is the number of feature templates in the group, and V g is the vocabulary size of the feature templates. Each row of Xg is a one-hot vector indicating the appearance of each feature.\nThe network first looks up the learned embedding vectors for each feature and then concate-\nThe fox jumps . stack buffer\ndet\n(a) Parser configuration c.\nnates them to form the embedding layer. This embedding layer can be written as:\nh0 = [X gEg | \u2200g] (1)\nwhere Eg is a learned V g \u00d7 Dg embedding matrix for feature group. Thus, the final size |h0| =\u2211\ng F gDg is the sum of all embedded feature sizes. The specific features and their dimensions used in the tagger are listed in Table 1. Note that for all features, we create additional null value that triggers when features are extracted outside the scope of the sentence. We use a single hidden layer in our model and apply rectified linear unit (ReLU) activation function over the hidden layer outputs. A final softmax layer reads in the activations and outputs probabilities for each possible POS tag."}, {"heading": "2.2 The Parser Network", "text": "The parser component follows the same design as the POS tagger with the exception of the features and the output space. Instead of a windowbased classifier, features are extracted from an arcstandard parser configuration1 c consisting of the\n1Note that the \u201cstack\u201d in the parse configuration is separate from the \u201cstacking\u201d of the POS tagging network and the parser network (Figure 1).\nstack s, the buffer b and the so far constructed dependencies. (Nivre, 2004). Prior implementations of this model used up to four groups of discrete features: words, labels (from previous decisions), POS tags, and morphological attributes (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015).\nIn this work, we apply the same design principle but we use an implicitly learned intermediate representation in the parser to replace traditional discrete features. We only retain discrete features over the labels in the incrementally constructed tree (Figure 4). Specifically, for any token of interest, we feed the hidden layer of the tagger network evaluated for that token as input to the parser. We implement this idea by re-using the feature templates from prior work as indexing functions.\nWe define this process formally as follows. Let fi(c) be a function mapping from parser configurations c to indices in the sentence, where i denotes each of our feature templates. For example, in Figure 4(a), when i =stack0, fi(c) is the index of \u201cfox\u201d in the sentence. Let htagger1 (j) be the hidden layer activation of the tagger network evaluated at token j. We define the input Ximplicit by concatenating these tagger activations according to our feature templates:\nximpliciti , h tagger 1 (fi(c)). (2)\nThus, the feature group Ximplicit is the rowconcatenation of the hidden layer activations of the tagger, as indexed by the feature templates. We have that F implicit is the number of feature templates, and V implicit = Htagger, the number of possible values is the number of hidden\nunits in the tagger. Just as for other features, we learn an embedding matrix Eimplicit of size H implicit\u00d7F implicit. Note that as in the POS tagger network, we reserve an additional null value for out of scope feature templates. A full example of this lookup process, and the resulting feedforward network connections created, is shown for a simple three-feature template consisting of the top two tokens on the stack and the first on the buffer in Figure 2. See Table 1 for the full list of 20 tokens that we extract for each state."}, {"heading": "3 Learning with Stack-propagation", "text": "In this section we describe how we train our stacking architecture. At a high level, we simply apply backpropagation to our proposed continuous form of stacking (hence \u201cstack-propagation.\u201d) There are two major issues to address: (1) how to handle the dynamic many-to-many connections between the tagger network and the parser network, and (2) how to incorporate the POS tag labels during training.\nAddressing the first point turns out to be fairly easy in practice: we simply unroll the gold trees into a derivation of (state, action) pairs that produce the tree. The key property of our parsing model is that the connections of the feedforward network are constructed incrementally as the parser state is updated. This is different than a generic recurrent model such as an LSTM, which passes activation vectors from one step to the next. The important implication at training time is that, unlike a recurrent network, the parser decisions are conditionally independent given a fixed history. In other words, if we unroll the network structure ahead of time given the gold derivation, we do not need to perform inference when training with respect to these examples. Thus, the overall training procedure is similar to that introduced in Chen and Manning (2014).\nTo incorporate the POS tags as a regularization during learning, we take a fairly standard approach from multi-task learning. The objective of learning is to find parameters \u0398 that maximize the data log-likelihood with a regularization on \u0398 for both parsing and tagging:\nmax \u0398\n\u03bb \u2211\nx,y\u2208T log(P\u0398(y | x))+\u2211\nc,a\u2208P log (P\u0398(a | c)) , (3)\nwhere {x, y} are POS tagging examples extracted from individual tokens and {c, a} are parser (configuration, action) pairs extracted from the unrolled gold parse tree derivations, and \u03bb is a tradeoff parameter.\nWe optimize this objective stochastically by alternating between two updates:\n\u2022 TAGGER: Pick a POS tagging example and update the tagger network with backpropagation. \u2022 PARSER: (Figure 4) Given a parser configuration c from the set of gold contexts, compute both tagger and parser activations. Backpropagate the parsing loss through the stacked architecture to update both parser and tagger, ignoring the tagger\u2019s softmax layer parameters.\nWhile the learning procedure is inspired from multi-task learning\u2014we only update each step with regards one of the two likelihoods\u2014there are subtle differences that are important. While a traditional multi-task learning approach would use the final layer of the parser network to predict both POS tags and parse trees, we predict POS tags from the first hidden layer of our model (the \u201ctagger\u201d network) only. We treat the POS labels as regularization of our parser and simply discard the softmax layer of the tagger network at test time. As we will show in Section 4, this regularization leads to dramatic gains in parsing accuracy. Note that in Section 5, we also show experimentally that stack-propagation is more powerful than the traditional multi-task approach, and by combining them together, we can achieve better accuracy on both POS and parsing tasks."}, {"heading": "3.1 Implementation details", "text": "Following Weiss et al. (2015), we use minibatched averaged stochastic gradient descent (ASGD) (Bottou, 2010) with momentum (Hinton, 2012) to learn the parameters \u0398 of the network. We use a separate learning rate, moving average, and velocity for the tagger network and the parser; the PARSER updates all averages, velocities, and learning rates, while the TAGGER updates only the tagging factors. We tuned the hyperparameters of momentum rate \u00b5, the initial learning rate \u03b70 and the learning rate decay step \u03b3 using held-out data. The training data for parsing and tagging can be extracted from either the same corpus or different\ncorpora; in our experiments they were always the same.\nTo trade-off the two objectives, we used a random sampling scheme to perform 10 epochs of PARSER updates and 5 epochs of TAGGER updates. In our experiments, we found that pretraining with TAGGER updates for one epoch before interleaving PARSER updates yielded faster training with better results. We also experimented using the TAGGER updates solely for initializing the parser and found that interleaving updates was crucial to obtain improvements over the baseline."}, {"heading": "4 Experiments", "text": "In this section, we evaluate our approach on several dependency parsing tasks across a wide variety of languages."}, {"heading": "4.1 Experimental Setup", "text": "We first investigated our model on 19 languages from the Universal Dependencies Treebanks v1.2.2 We selected the 19 largest currently spoken languages for which the full data was freely available. We used the coarse universal tagset in our experiments with no explicit morphological annotations. To measure parsing accuracy, we report unlabeled attachment score (UAS) and labeled attachment score (LAS) computed on all tokens (including punctuation), as is standard for non-English datasets.\nFor simplicity, we use the arc-standard (Nivre, 2004) transition system with greedy decoding. Because this transition system only produces projective trees, we first apply a projectivization step to all treebanks before unrolling the gold derivations during training. We make an exception for Dutch, where we observed a significant gain on development data by introducing the SWAP action (Nivre,\n2http://universaldependencies.org\n2009) and allowing non-projective trees. For models that required predicted POS tags, we trained a window-based tagger using the same features as the tagger component of our stacking model. We used 5-fold jackknifing to produce predicted tags on the training set. We found that the window-based tagger was comparable to a stateof-the-art CRF tagger for most languages. For every network we trained, we used the development data to evaluate a small range of hyperparameters, stopping training early when UAS no longer improved on the held-out data. We use H = 1024 hidden units in the parser, and H = 128 hidden units in the tagger. The parser embeds the tagger activations with D = 64. Note that following Ballesteros et al. (2015), we did not use any auxiliary data beyond that in the treebanks, such as pre-trained word embeddings.\nFor a final set of experiments, we evaluated on the standard Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993)), dependencies generated from version 3.3.0 of the Stanford converter (De Marneffe et al., 2006). We followed standard practice and used sections 2-21 for training, section 22 for development, and section 23 for testing. Following Weiss et al. (2015), we used section 24 to tune any hyperparameters of the model to avoid overfitting to the development set. As is common practice, we use pretrained word embeddings from the word2vec package when training on this dataset."}, {"heading": "4.2 Results", "text": "We present our main results on the Universal Treebanks in Table 2. We directly compare our approach to other baselines in two primary ways. First, we compare the effectiveness of our learned continuous representations with those of Alberti et al. (2015), who use the predicted distribution over\nPOS tags concatenated with word embeddings as input to the parser. Because they also incorporate beam search into training, we re-implement a greedy version of their method to allow for direct comparisons of token representations. We refer to this as the \u201cPipeline (Ptag)\u201d baseline. Second, we also compare our architecture trained without POS tags as regularization, which we refer to as \u201cOurs (window-based)\u201d. This model has the same architecture as our full model but with no POS supervision and updates. Since this model never observes POS tags in any way, we compare against a recurrent character-based parser (Ballesteros et al., 2015) which is state-of-the-art when no POS tags are provided.3 Finally, we compare to RGBParser (Lei et al., 2014), a state-of-the art graph-based (non-greedy) approach.\nOur greedy stackprop model outperforms all other methods, including the graph-based RBGParser, by a significant margin on the test set (78.9% vs 77.6%). This is despite the limitations of greedy parsing. Stackprop also yields a 2.3% absolute improvement in accuracy compared to using POS tag confidences as features (Pipeline Ptag). Finally, we also note that adding stackprop to our window-based model improves accuracy in every language, while incorporating predicted POS tags into the LSTM baseline leads to\n3We thank Ballesteros et al. (2015) for their assistance running their code on the treebanks.\nModel Variant UAS LAS POS\nArc-standard transition system Pipeline (Ptag) 81.56 76.55 95.14 Ours (window-based) 82.08 77.08 - Ours (Stackprop) 83.38 78.78 -\nJoint parsing & tagging transition system Pipeline (Ptag) 81.61 76.57 95.30 Ours (window-based) 82.58 77.76 94.92 Ours (Stackprop) 83.21 78.64 95.43\nTable 4: Averaged parsing and POS tagging results on the UD treebanks for joint variants of stackprop. Given the windowbased architecture, stackprop leads to higher parsing accuracies than joint modeling (83.38% vs. 82.58%).\noccasional drops in accuracy (most likely due to cascaded errors.)"}, {"heading": "5 Discussion", "text": "Stackprop vs. other representations. One unexpected result was that, even without the POS tag labels at training time, our stackprop architecture achieves better accuracy than either the character-based LSTM or the pipelined baselines (Table 2). This suggests that adding windowbased representations\u2013which aggregate over many features of the word and surrounding context\u2013 is more effective than increasing the expressiveness of individual word representations by using character-based recurrent models. In future work we will explore combining these two complementary approaches.\nWe hypothesized that stackprop might provide larger gains over the pipelined model when the POS tags are very coarse. We tested this latter hypothesis on the WSJ corpus by training our model using the coarse universal tagsets instead of the fine tagset (Table 3). We found that stackprop achieves similar accuracy using coarse tagsets as the fine tagset, while the pipelined baseline\u2019s performance drops dramatically. And while stackprop doesn\u2019t achieve the highest reported accuracies on the WSJ, it does achieve competitive accuracies and outperforms prior state-of-the-art for greedy methods (Dyer et al., 2015).\nStackprop vs. joint modeling. An alternative to stackprop would be to train the final layer of our architecture to predict both POS tags and dependency arcs. To evaluate this, we trained our window-based architecture with the integrated\ntransition system of Bohnet and Nivre (2012), which augments the SHIFT transition to predict POS tags. Note that if we also apply stackprop, the network learns from POS annotations twice: once in the TAGGER updates, and again the PARSER updates. We therefore evaluated our window-based model both with and without stack-propagation, and with and without the joint transition system.\nWe compare these variants along with our reimplementation of the pipelined model of Alberti et al. (2015) in Table 4. We find that stackprop is always better, even when it leads to \u201cdouble counting\u201d the POS annotations; in this case, the result is a model that is significantly better at POS tagging while marginally worse at parsing than stackprop alone.\nReducing cascaded errors. As expected, we observe a significant reduction in cascaded POS tagging errors. An example from the English UD treebank is given in Figure 5. Across the 19 languages in our test set, we observed a 10.9% gain (34.1% vs. 45.0%) in LAS on tokens where the pipelined POS tagger makes a mistake, compared to a 1.8% gain on the rest of the corpora.\nDecreased model size. Previous neural parsers that use POS tags require learning embeddings for words and other features on top of the parameters used in the POS tagger (Chen and Manning, 2014; Weiss et al., 2015). In contrast, the parameters for our parser are essentially free given the POS tagger. In practice, most of the memory of the parsing model is contained in the word embeddings matrix, so we halve the memory footprint of the parser. Compared to our implementation of the pipeline model, we observed that this more compact parser model was also roughly twice as fast.\nContextual embeddings. Finally, we also explored the significance of the representations learned by the tagger. Unlike word embedding models, the representations used in our parser are\nHeterosexuals increasingly back gay marriage root advmod advmod amod dobj\nconstructed for each token based on its surrounding context. We demonstrate a few interesting trends we observed in Table 5, where we show the nearest neighbors to sample tokens in this contextual embedding space. These representations tend to represent syntactic patterns rather than individual words, distinguishing between the form (e.g. \u201cjudge\u201d as a noun vs. a verb\u2019) and context of tokens (e.g. preceded by a personal pronoun)."}, {"heading": "6 Conclusions", "text": "We present a stacking neural network model for dependency parsing and tagging. Through a simple learning method we call \u201cstack-propagation,\u201d our model learns effective intermediate representations for parsing by using POS tags as regularization of implicit representations. Our model outperforms all state-of-the-art parsers when evaluated on 19 languages of the Universal Dependencies treebank and outperforms other greedy models on the Wall Street Journal.\nWe observe that the ideas presented in this work can also be as a principled way to optimize upstream NLP components for down-stream applications. In future work, we will extend this idea\nbeyond sequence modeling to improve models in NLP that utilize parse trees as features. The basic tenet of stack-propagation is that the hidden layers of neural models used to generate annotations can be used instead of the annotations themselves. This suggests a new methodology to building deep neural models for NLP: we can design them from the ground up to incorporate multiple sources of annotation and learn far more effective intermediate representations."}, {"heading": "Acknowledgments", "text": "We would like to thank Ryan McDonald, Emily Pitler, Chris Alberti, Michael Collins, and Slav Petrov for their repeated discussions, suggestions, and feedback, as well all members of the Google NLP Parsing Team. We would also like to thank Miguel Ballesteros for assistance running the character-based LSTM."}], "references": [{"title": "Improved transition-based parsing and tagging with neural networks", "author": ["Chris Alberti", "David Weiss", "Greg Coppola", "Slav Petrov."], "venue": "Proceedings of EMNLP 2015.", "citeRegEx": "Alberti et al\\.,? 2015", "shortCiteRegEx": "Alberti et al\\.", "year": 2015}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proceddings of EMNLP.", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["Bernd Bohnet", "Joakim Nivre."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and", "citeRegEx": "Bohnet and Nivre.,? 2012", "shortCiteRegEx": "Bohnet and Nivre.", "year": 2012}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L\u00e9on Bottou."], "venue": "Proceedings of COMPSTAT\u20192010, pages 177\u2013186. Springer.", "citeRegEx": "Bottou.,? 2010", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), volume 1, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa."], "venue": "JMLR.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine De Marneffe", "Bill MacCartney", "Christopher D. Manning."], "venue": "In", "citeRegEx": "Marneffe et al\\.,? 2006", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Neural crf parsing", "author": ["Greg Durrett", "Dan Klein."], "venue": "Proceedings of the Association for Computational Linguistics. Association for Computational Linguistics.", "citeRegEx": "Durrett and Klein.,? 2015", "shortCiteRegEx": "Durrett and Klein.", "year": 2015}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith."], "venue": "ACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Semantic role labeling with neural network factors", "author": ["Nicholas FitzGerald", "Oscar Tckstrm", "Kuzman Ganchev", "Dipanjan Das."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP \u201915).", "citeRegEx": "FitzGerald et al\\.,? 2015", "shortCiteRegEx": "FitzGerald et al\\.", "year": 2015}, {"title": "Incremental joint pos tagging and dependency parsing in chinese", "author": ["Jun Hatori", "Takuya Matsuzaki", "Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "In IJCNLP,", "citeRegEx": "Hatori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hatori et al\\.", "year": 2011}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["Geoffrey E Hinton."], "venue": "Neural Networks: Tricks of the Trade, pages 599\u2013619. Springer.", "citeRegEx": "Hinton.,? 2012", "shortCiteRegEx": "Hinton.", "year": 2012}, {"title": "Simple semi-supervised dependency parsing", "author": ["Terry Koo", "Xavier Carreras P\u00e9rez", "Michael Collins."], "venue": "46th Annual Meeting of the Association for Computational Linguistics, pages 595\u2013603.", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1381\u20131391.", "citeRegEx": "Lei et al\\.,? 2014", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Joint models for chinese pos tagging and dependency parsing", "author": ["Zhenghua Li", "Min Zhang", "Wanxiang Che", "Ting Liu", "Wenliang Chen", "Haizhou Li."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Li et al\\.,? 2011", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Joint optimization for chinese POS tagging and dependency parsing", "author": ["Zhenghua Li", "Min Zhang", "Wanxiang Che", "Ting Liu", "Wenliang Chen."], "venue": "IEEE/ACM Transactions on Audio, Speech & Language Processing, pages 274\u2013286.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre."], "venue": "Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, IncrementParsing \u201904, pages 50\u201357, Stroudsburg, PA, USA. Association", "citeRegEx": "Nivre.,? 2004", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Non-projective dependency parsing in expected linear time", "author": ["Joakim Nivre."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages", "citeRegEx": "Nivre.,? 2009", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "arXiv preprint arXiv:1104.2086.", "citeRegEx": "Petrov et al\\.,? 2011", "shortCiteRegEx": "Petrov et al\\.", "year": 2011}, {"title": "Joint chinese word segmentation, pos tagging and parsing", "author": ["Xian Qian", "Yang Liu."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 501\u2013", "citeRegEx": "Qian and Liu.,? 2012", "shortCiteRegEx": "Qian and Liu.", "year": 2012}, {"title": "Joint pos tagging and transition-based constituent parsing in chinese with non-local features", "author": ["Zhiguo Wang", "Nianwen Xue."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 733\u2013742.", "citeRegEx": "Wang and Xue.,? 2014", "shortCiteRegEx": "Wang and Xue.", "year": 2014}, {"title": "Structured training for neural network transition-based parsing", "author": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of ACL 2015, pages 323\u2013333.", "citeRegEx": "Weiss et al\\.,? 2015", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Stacked generalization", "author": ["David H Wolpert."], "venue": "Neural networks.", "citeRegEx": "Wolpert.,? 1992", "shortCiteRegEx": "Wolpert.", "year": 1992}, {"title": "Randomized greedy inference for joint segmentation, POS tagging and dependency parsing", "author": ["Yuan Zhang", "Chengtao Li", "Regina Barzilay", "Kareem Darwish."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "In recent years, transition-based dependency parsers powered by neural network scoring functions have dramatically increased the state-of-theart in terms of both speed and accuracy (Chen and Manning, 2014; Alberti et al., 2015; Weiss et al., 2015).", "startOffset": 181, "endOffset": 247}, {"referenceID": 0, "context": "In recent years, transition-based dependency parsers powered by neural network scoring functions have dramatically increased the state-of-theart in terms of both speed and accuracy (Chen and Manning, 2014; Alberti et al., 2015; Weiss et al., 2015).", "startOffset": 181, "endOffset": 247}, {"referenceID": 22, "context": "In recent years, transition-based dependency parsers powered by neural network scoring functions have dramatically increased the state-of-theart in terms of both speed and accuracy (Chen and Manning, 2014; Alberti et al., 2015; Weiss et al., 2015).", "startOffset": 181, "endOffset": 247}, {"referenceID": 7, "context": "Similar approaches also achieve state-ofthe-art in other NLP tasks, such as constituency parsing (Durrett and Klein, 2015) or semantic role labeling (FitzGerald et al.", "startOffset": 97, "endOffset": 122}, {"referenceID": 9, "context": "Similar approaches also achieve state-ofthe-art in other NLP tasks, such as constituency parsing (Durrett and Klein, 2015) or semantic role labeling (FitzGerald et al., 2015).", "startOffset": 149, "endOffset": 174}, {"referenceID": 23, "context": "In the pipeline or stacking (Wolpert, 1992) method, these are predicted from an independently trained tagger and used as features in the parser.", "startOffset": 28, "endOffset": 43}, {"referenceID": 19, "context": "The POS tags may also contain only coarse information, such as when using the universal tagset of Petrov et al. (2011).", "startOffset": 98, "endOffset": 119}, {"referenceID": 12, "context": "either using semi-supervised clustering instead of POS tags (Koo et al., 2008) or building recurrent representations of words using neural networks (Dyer et al.", "startOffset": 60, "endOffset": 78}, {"referenceID": 8, "context": ", 2008) or building recurrent representations of words using neural networks (Dyer et al., 2015; Ballesteros et al., 2015).", "startOffset": 77, "endOffset": 122}, {"referenceID": 1, "context": ", 2008) or building recurrent representations of words using neural networks (Dyer et al., 2015; Ballesteros et al., 2015).", "startOffset": 77, "endOffset": 122}, {"referenceID": 14, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 10, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 2, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 20, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 21, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 15, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 24, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 0, "context": "As an alternative, a wide range of prior work has investigated jointly modeling both POS and parse trees (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012; Qian and Liu, 2012; Wang and Xue, 2014; Li et al., 2014; Zhang et al., 2015; Alberti et al., 2015).", "startOffset": 105, "endOffset": 266}, {"referenceID": 4, "context": "Both networks are implemented with a refined version of the feed-forward network (Figure 3) from Chen and Manning (2014), as described in Weiss et al.", "startOffset": 97, "endOffset": 121}, {"referenceID": 4, "context": "Both networks are implemented with a refined version of the feed-forward network (Figure 3) from Chen and Manning (2014), as described in Weiss et al. (2015). We link the tagger network to the parser by translating traditional feature templates for parsing into feed-forward connections from the tagger to the parser (Figure 2).", "startOffset": 97, "endOffset": 158}, {"referenceID": 1, "context": "We observe a >2% absolute gain in labeled accuracy compared to state-of-the-art, LSTM-based greedy parsers (Ballesteros et al., 2015) and a >1% gain compared to a state-of-the-art, graphbased method (Lei et al.", "startOffset": 107, "endOffset": 133}, {"referenceID": 13, "context": ", 2015) and a >1% gain compared to a state-of-the-art, graphbased method (Lei et al., 2014).", "startOffset": 73, "endOffset": 91}, {"referenceID": 1, "context": "One important finding of this work is that, even without POS tags, our architecture outperforms recurrent approaches that build custom word representations using character-based LSTMs (Ballesteros et al., 2015).", "startOffset": 184, "endOffset": 210}, {"referenceID": 4, "context": "The basic unit of our model (Figure 3) is a simple, feed-forward network that has been shown to work very well for parsing tasks (Chen and Manning, 2014; Weiss et al., 2015).", "startOffset": 129, "endOffset": 173}, {"referenceID": 22, "context": "The basic unit of our model (Figure 3) is a simple, feed-forward network that has been shown to work very well for parsing tasks (Chen and Manning, 2014; Weiss et al., 2015).", "startOffset": 129, "endOffset": 173}, {"referenceID": 4, "context": "In a traditional stacking (pipeline) approach, we would use the discrete predicted POS tags from the tagger as features in the parser (Chen and Manning, 2014).", "startOffset": 134, "endOffset": 158}, {"referenceID": 4, "context": "To implement this, we show how we can reuse feature templates from Chen and Manning (2014) to specify the feed-forward connections from the tagger network to the parser network.", "startOffset": 67, "endOffset": 91}, {"referenceID": 5, "context": "Like the \u201cwindowapproach\u201d network of Collobert et al. (2011), the tagger is evaluated per-token, with features extracted from a window of tokens surrounding the target.", "startOffset": 37, "endOffset": 61}, {"referenceID": 17, "context": "(Nivre, 2004).", "startOffset": 0, "endOffset": 13}, {"referenceID": 4, "context": "Prior implementations of this model used up to four groups of discrete features: words, labels (from previous decisions), POS tags, and morphological attributes (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015).", "startOffset": 161, "endOffset": 227}, {"referenceID": 22, "context": "Prior implementations of this model used up to four groups of discrete features: words, labels (from previous decisions), POS tags, and morphological attributes (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015).", "startOffset": 161, "endOffset": 227}, {"referenceID": 0, "context": "Prior implementations of this model used up to four groups of discrete features: words, labels (from previous decisions), POS tags, and morphological attributes (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015).", "startOffset": 161, "endOffset": 227}, {"referenceID": 4, "context": "Thus, the overall training procedure is similar to that introduced in Chen and Manning (2014). To incorporate the POS tags as a regularization during learning, we take a fairly standard approach from multi-task learning.", "startOffset": 70, "endOffset": 94}, {"referenceID": 3, "context": "(2015), we use minibatched averaged stochastic gradient descent (ASGD) (Bottou, 2010) with momentum (Hinton, 2012) to learn the parameters \u0398 of the network.", "startOffset": 71, "endOffset": 85}, {"referenceID": 11, "context": "(2015), we use minibatched averaged stochastic gradient descent (ASGD) (Bottou, 2010) with momentum (Hinton, 2012) to learn the parameters \u0398 of the network.", "startOffset": 100, "endOffset": 114}, {"referenceID": 20, "context": "Following Weiss et al. (2015), we use minibatched averaged stochastic gradient descent (ASGD) (Bottou, 2010) with momentum (Hinton, 2012) to learn the parameters \u0398 of the network.", "startOffset": 10, "endOffset": 30}, {"referenceID": 1, "context": "\u201cB\u201915 LSTM\u201d is the character-based LSTM model (Ballesteros et al., 2015), while \u201cOurs (window)\u201d is our window-based architecture variant without stackprop.", "startOffset": 46, "endOffset": 72}, {"referenceID": 17, "context": "For simplicity, we use the arc-standard (Nivre, 2004) transition system with greedy decoding.", "startOffset": 40, "endOffset": 53}, {"referenceID": 1, "context": "Note that following Ballesteros et al. (2015), we did not use any auxiliary data beyond that in the treebanks, such as pre-trained word embeddings.", "startOffset": 20, "endOffset": 46}, {"referenceID": 16, "context": "For a final set of experiments, we evaluated on the standard Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993)), dependencies generated from version 3.", "startOffset": 113, "endOffset": 134}, {"referenceID": 6, "context": "0 of the Stanford converter (De Marneffe et al., 2006). We followed standard practice and used sections 2-21 for training, section 22 for development, and section 23 for testing. Following Weiss et al. (2015), we used section 24 to tune any hyperparameters of the model to avoid overfitting to the development set.", "startOffset": 32, "endOffset": 209}, {"referenceID": 0, "context": "First, we compare the effectiveness of our learned continuous representations with those of Alberti et al. (2015), who use the predicted distribution over", "startOffset": 92, "endOffset": 114}, {"referenceID": 8, "context": "NO TAGS Dyer et al. (2015) 92.", "startOffset": 8, "endOffset": 27}, {"referenceID": 8, "context": "60 Dyer et al. (2015) 93.", "startOffset": 3, "endOffset": 22}, {"referenceID": 0, "context": "05 Alberti et al. (2015) 94.", "startOffset": 3, "endOffset": 25}, {"referenceID": 0, "context": "For reference, we show the most accurate models from Alberti et al. (2015) and Weiss et al.", "startOffset": 53, "endOffset": 75}, {"referenceID": 0, "context": "For reference, we show the most accurate models from Alberti et al. (2015) and Weiss et al. (2015), which use a deeper model and beam search for inference.", "startOffset": 53, "endOffset": 99}, {"referenceID": 1, "context": "Since this model never observes POS tags in any way, we compare against a recurrent character-based parser (Ballesteros et al., 2015) which is state-of-the-art when no POS tags are provided.", "startOffset": 107, "endOffset": 133}, {"referenceID": 13, "context": "3 Finally, we compare to RGBParser (Lei et al., 2014), a state-of-the art graph-based (non-greedy) approach.", "startOffset": 35, "endOffset": 53}, {"referenceID": 1, "context": "We thank Ballesteros et al. (2015) for their assistance running their code on the treebanks.", "startOffset": 9, "endOffset": 35}, {"referenceID": 8, "context": "And while stackprop doesn\u2019t achieve the highest reported accuracies on the WSJ, it does achieve competitive accuracies and outperforms prior state-of-the-art for greedy methods (Dyer et al., 2015).", "startOffset": 177, "endOffset": 196}, {"referenceID": 2, "context": "transition system of Bohnet and Nivre (2012), which augments the SHIFT transition to predict POS tags.", "startOffset": 21, "endOffset": 45}, {"referenceID": 0, "context": "We compare these variants along with our reimplementation of the pipelined model of Alberti et al. (2015) in Table 4.", "startOffset": 84, "endOffset": 106}, {"referenceID": 4, "context": "Previous neural parsers that use POS tags require learning embeddings for words and other features on top of the parameters used in the POS tagger (Chen and Manning, 2014; Weiss et al., 2015).", "startOffset": 147, "endOffset": 191}, {"referenceID": 22, "context": "Previous neural parsers that use POS tags require learning embeddings for words and other features on top of the parameters used in the POS tagger (Chen and Manning, 2014; Weiss et al., 2015).", "startOffset": 147, "endOffset": 191}], "year": 2017, "abstractText": "Traditional syntax models typically leverage part-of-speech (POS) information by constructing features from hand-tuned templates. We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations. We propose a simple method for learning a stacked pipeline of models which we call \u201cstack-propagation.\u201d We apply this to dependency parsing and tagging, where we use the hidden layer of the tagger network as a representation of the input tokens for the parser. At test time, our parser does not require predicted POS tags. On 19 languages from the Universal Dependencies, our method is 1.3% (absolute) more accurate than a state-of-the-art graph-based approach and 2.7% more accurate than the most comparable greedy model.", "creator": "TeX"}}}