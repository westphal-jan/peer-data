{"id": "1603.06807", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2016", "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus", "abstract": "over any past implementation, large - scale supervised learning threshold validation enabled machine error researchers continually submit substantial estimates. however, to our date, there are no massive - scale criterion - answer corpora surveys. particularly this paper archaeologists explore the 30m factoid question - answer approach, an integral question - answered pair corpus augmented by applying using digital neural filtering architecture on the knowledge base freebase fields indicate facts amongst natural language questions. traditional original error - winner pairs signal valued both by human evaluators independently via abstract evaluation metrics, including well - established machine logic and clause formation metrics. across all rigorous criteria : identity - generation gap outperforms the competing y - selection baseline. finally, when presented to human evaluators, the source knowledge proved to be distinct from usable human - generated data.", "histories": [["v1", "Tue, 22 Mar 2016 14:25:16 GMT  (452kb,D)", "http://arxiv.org/abs/1603.06807v1", "13 pages, 1 figure, 7 tables"], ["v2", "Sun, 29 May 2016 20:00:20 GMT  (452kb,D)", "http://arxiv.org/abs/1603.06807v2", "13 pages, 1 figure, 7 tables"]], "COMMENTS": "13 pages, 1 figure, 7 tables", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["iulian vlad serban", "alberto garc\u00eda-dur\u00e1n", "\u00e7aglar g\u00fcl\u00e7ehre", "sungjin ahn", "sarath chandar", "aaron c courville", "yoshua bengio"], "accepted": true, "id": "1603.06807"}, "pdf": {"name": "1603.06807.pdf", "metadata": {"source": "CRF", "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus", "authors": ["Iulian Vlad Serban", "Alberto Garc\u0131\u0301a-Dur\u00e1n", "Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Aaron Courville", "Yoshua Bengio"], "emails": ["sarath.chandar@umontreal.ca", "aaron.courville@umontreal.ca", "yoshua.bengio@umontreal.ca", "alberto.garcia-duran@utc.fr"], "sections": [{"heading": "1 Introduction", "text": "A major obstacle for training question-answering (QA) systems has been due to the lack of labeled data. The question answering field has focused on building QA systems based on traditional information retrieval procedures (Lopez et al., 2011;\n* First authors. \u25e6 University of Montreal, Canada\nEmail: {iulian.vlad.serban,caglar.gulcehre,sungjin.ahn, sarath.chandar,aaron.courville,yoshua.bengio}@umontreal.ca Universite\u0301 de Technologie de Compie\u0300gne - CNRS, France Email: alberto.garcia-duran@utc.fr\n\u2020 CIFAR Senior Fellow\nDumais et al., 2002; Voorhees and Tice, 2000). More recently, researchers have started to utilize large-scale knowledge bases (KBs) (Lopez et al., 2011), such as Freebase (Bollacker et al., 2008), WikiData (Vrandec\u030cic\u0301 and Kro\u0308tzsch, 2014) and Cyc (Lenat and Guha, 1989).1 Bootstrapping QA systems with such structured knowledge is clearly beneficial, but it is unlikely alone to overcome the lack of labeled data. To take into account the rich and complex nature of human language, such as paraphrases and ambiguity, it would appear that labeled question and answer pairs are necessary. The need for such labeled pairs is even more critical for training neural network-based QA systems, where researchers until now have relied mainly on hand-crafted rules and heuristics to synthesize artificial QA corpora (Bordes et al., 2014; Bordes et al., 2015).\nMotivated by these recent developments, in this paper we focus on generating questions based on the Freebase KB. We frame question generation as a transduction problem starting from a Freebase fact, represented by a triple consisting of a subject, a relationship and an object, which is transduced into a question about the subject, where the object is the correct answer (Bordes et al., 2015). We propose several models, largely inspired by recent neural machine translation models (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), and we use an approach similar to Luong et al. (2015) for dealing with the problem of rare-words. We evaluate the produced questions in a human-based experiment as well as with respect to automatic evaluation metrics, including the well-established machine translation metrics BLEU and METEOR and a sentence similarity metric. We find that the question-generation model outperforms the competing template-based\n1Freebase is now a part of WikiData.\nar X\niv :1\n60 3.\n06 80\n7v 1\n[ cs\n.C L\n] 2\n2 M\nar 2\n01 6\nbaseline, and, when presented to untrained human evaluators, the produced questions appear to be indistinguishable from real human-generated questions. This suggests that the produced questionanswer pairs are of high quality and therefore that they will be useful for training QA systems. Finally, we use the best performing model to construct a new factoid question answer corpus \u2013 The 30M Factoid Question-Answer Corpus \u2013 which is made freely available to the research community.2"}, {"heading": "2 Related Work", "text": "Question generation has attracted interest in recent years with notable work by Rus et al. (2010), followed by the increasing interest from the Natural Language Generation (NLG) community. A simple rule-based approach was proposed in different studies as wh-fronting or wh-inversion (Kalady et al., 2010; Ali et al., 2010). This comes at the disadvantage of not making use of the semantic content of words apart from their syntactic role. The problem of determining the question type (e.g. that a Where-question should be triggered for locations), which requires knowledge of the category type of the elements involved in the sentence, has been addressed in two different ways: by using named entity recognizers (Mannem et al., 2010; Yao and Zhang, 2010) or semantic role labelers (Chen et al., 2009). In Curto et al. (2012) questions are split into classes according to their syntactic structure, prefix of the question and the category of the answer, and then a pattern is learned to generate questions for that class of questions. After the identification of key points, Chen et al. (2009) apply handcrafted-templates to generate questions framed in the right target expression by following the analysis of Graesser et al. (1992), who classify questions according to a taxonomy consisting of 18 categories.\nThe works discussed so far propose ways to map unstructured text to questions. This implies a two-step process: first, transform a text into a symbolic representation (e.g. a syntactic representation of the sentence), and second, transform the symbolic representation of the text into the question (Yao et al., 2012). On the other hand, going from a symbolic representation (structured information) to a question, as we will describe in the next section, only involves the second step. Closer to our approach is the work by Olney et\n2www.agarciaduran.org\nal. (2012). They take triples as input, where the edge relation defines the question template and the head of the triple replaces the placeholder token in the selected question template. In the same spirit, Duma et al. (2013) generate short descriptions from triples by using templates defined by the relationship and replacing accordingly the placeholder tokens for the subject and object.\nOur baseline is similar to that of Olney et al. (2012), where a set of relationship-specific templates are defined. These templates include placeholders to replace the string of the subject. The main difference with respect to their work is that our baseline does not explicitly define these templates. Instead, each relationship has as many templates as there are different ways of framing a question with that relationship in the training set. This yields more diverse and semantically richer questions by effectively taking advantage of the fact-question pairs, which Olney et al. did not have access to in their experiments.\nUnlike the work by Berant and Liang (2014), which addresses the problem of deterministically generating a set of candidate logical forms with a canonical realization in natural language for each, our work addresses the inverse problem: given a logical form (fact) it outputs the associated question.\nIt should also be noted that recent work in question answering have used simpler rule-based and template-based approaches to generate synthetic questions to address the lack of question-answer pairs to train their models (Bordes et al., 2014; Bordes et al., 2015)."}, {"heading": "3 Task Definition", "text": ""}, {"heading": "3.1 Knowledge Bases", "text": "In general, a KB can be viewed as a multirelational graph, which consists of a set of nodes (entities) and a set of edges (relationships) linking nodes together. In Freebase (Bollacker et al., 2008) these relationships are directed and always connect exactly two entities. For example, in Freebase the two entities fires creek and nantahala national forest are linked together by the relationship contained by. Since the triple {fires creek, contained by, nantahala national forest} represents a complete and self-contained piece of information, it is also called a fact where fires creek is the subject (head of the edge), contained by is the relationship and\nnantahala national forest is the object (tail of the edge)."}, {"heading": "3.2 Transducing Facts to Questions", "text": "We aim to transduce a fact into a question, such that:\n1. The question is concerned with the subject and relationship of the fact, and\n2. The object of the fact represents a valid answer to the generated question.\nWe model this in a probabilistic framework as a directed graphical model:\nP (Q|F ) = N\u220f\nn=1\nP (wn|w<n, F ), (1)\nwhere F = (subject, relationship, object) represents the fact, Q = (w1, . . . , wN ) represents the question as a sequence of tokens w1, . . . , wN , and w<n represents all the tokens generated before token wn. In particular, wN represents the question mark symbol \u2019?\u2019."}, {"heading": "3.3 Dataset", "text": "We use the SimpleQuestions dataset (Bordes et al., 2015) in order to train our models. This is by far the largest dataset of question-answer pairs created by humans based on a KB. It contains over 100K question-answer pairs created by users on Amazon Mechanical Turk3 in English based on the Freebase KB. In order to create the questions, human participants were shown one whole Freebase fact at a time and they were asked to phrase a question such that the object of the presented fact becomes the answer of the question.4 Consequently, both the subject and the relationship are explicitly given in each question. But indirectly characteristics of the object may also be given since the humans have an access to it as well. Often when phrasing a question the annotators tend to be more informative about the target object by giving specific information about it in the question produced. For example, in the question What city is the American actress X from? the city name given in the object informs the human participant that it was in America - information, which was not provided by either the subject or relationship of the\n3www.mturk.com 4It is not necessary for the object to be the only answer,\nbut it is required to be one of the possible answers.\nfact. We have also observed that the questions are often ambiguous: that is, one can easily come up with several possible answers that may fit the specifications of the question. Table 1 shows statistics of the dataset."}, {"heading": "4 Model", "text": "We propose to attack the problem with the models inspired by the recent success of neural machine translation models (Sutskever et al., 2014; Bahdanau et al., 2015). Intuitively, one can think of the transduction task as a \u201clossy translation\u201d from structured knowledge (facts) to human language (questions in natural language), where certain aspects of the structured knowledge is intentionally left out (e.g. the name of the object). These models typically consist of two components: an encoder, which encodes the source phrase into one or several fixed-size vectors, and a decoder, which decodes the target phrase based on the results of the encoder."}, {"heading": "4.1 Encoder", "text": "In contrast to the neural machine translation framework, our source language is not a proper language but instead a sequence of three variables making up a fact. We propose an encoder sub-model, which encodes each atom of the fact into an embedding. Each atom {s, r, o}, may stand for subject, relationship and object, respectively, of a fact F = (s, r, o) is represented as a 1-of-K vector xatom, whose embedding is obtained as eatom = Einxatom, where Ein \u2208 RDEnc\u00d7K is the embedding matrix of the input vocabulary and K is the size of that vocabulary. The encoder transforms this embedding into Enc(F )atom \u2208 RHDec as Enc(F )atom =WEnceatom, where WEnc \u2208 RHDec\u00d7DEnc .\nThis embedding matrix, Ein, could be another parameter of the model to be learned, however, as discussed later (see Section 4.3), we have learned it separately and beforehand with TransE (Bordes et al., 2013), a model aimed at modeling this kind of multi-relational data. We fix it and do not allow the encoder to tune it during training.\nWe call fact embedding Enc(F ) \u2208 R3HDec the concatenation [Enc(F )s, Enc(F )r, Enc(F )o] of\nthe atom embeddings, which is the input for the next module."}, {"heading": "4.2 Decoder", "text": "For the decoder, we use a GRU recurrent neural network (RNN) (Cho et al., 2014) with an attention-mechanism (Bahdanau et al., 2015) on the encoder representation to generate the associated question Q to that fact F . Recently, it has been shown that the GRU RNN performs equally well across a range of tasks compared to other RNN architectures, such as the LSTM RNN (Greff et al., 2015). The hidden state of the decoder RNN is computed at each time step n as:\ngrn = \u03c3(WrEoutwn\u22121 + Crc(F, hn\u22121) + Urhn\u22121) (2)\ngun = \u03c3(WuEoutwn\u22121 + Cuc(F, hn\u22121) + Uuhn\u22121) (3)\nh\u0303 = tanh(WEoutwn\u22121 + Cc(F, hn\u22121) (4) + U(grn \u25e6 hn\u22121)) hn = g u n \u25e6 hn\u22121 + (1\u2212 gun) \u25e6 h\u0303, (5)\nwhere \u03c3 is the sigmoid function, s.t. \u03c3(x) \u2208 [0, 1], and the circle, \u25e6, represents element-wise multiplication. The initial state h0 of this RNN is given by the output of a feedforward neural network fed with the fact embedding. The product Eoutwn \u2208 RDDec is the decoder embedding vector corresponding to the word wn (coded as a 1- of-V vector, with V being the size of the output vocabulary), the variables Ur, Uu, U, Cr, Cu, C \u2208 RHDec\u00d7HDec , Wr,Wu,W \u2208 RHDec\u00d7DDec are the parameters of the GRU and c(F, hn\u22121) is the context vector (defined below Eq. 6). The vector gr is called the reset gate, gu as the update gate and h\u0303 the candidate activation. By adjusting gr and gu appropriately, the model is able to create linear skip-connections between distant hidden states, which in turn makes the credit assignment problem easier and the gradient signal stronger to earlier hidden states. Then, at each time step n the set of probabilities of word tokens is given by applying a softmax layer over Votanh(Vhhn + VwEoutwn\u22121 + Vcc(F, hn\u22121)), where Vo \u2208 RV\u00d7HDec , Vh, Vc \u2208 RHDec\u00d7HDec and Vw \u2208 RHDec\u00d7DDec . Lastly, the function c(F, hn\u22121) is computed using an attention-mechanism:\nc(F, hn\u22121) = \u03b1s,n\u22121Enc(F )s + \u03b1r,n\u22121Enc(F )r + \u03b1o,n\u22121Enc(F )o, (6)\nwhere \u03b1s,n\u22121, \u03b1r,n\u22121, \u03b1r,n\u22121 are real-valued scalars, which weigh the contribution of the subject, relationship and object representations. They correspond to the attention of the model, and are computed by applying a one-layer neural network with tanh-activation function on the encoder representations of the fact, Enc(F ), and the previous hidden state of the RNN, hn\u22121, followed by the sigmoid function to restrict the attention values to be between zero and one. The need for the attention-mechanism is motivated by the intuition that the model needs to attend to the subject only once during the generation process while attending to the relationship at all other times during the generation process. The model is illustrated in Figure 1."}, {"heading": "4.3 Modeling the Source Language", "text": "A particular problem with the model presented above is related to the embeddings for the entities, relationships and tokens, which all have to be learned in one way or another. If we learn these naively on the SimpleQuestions training set,\nthe model will perform poorly when it encounters previously unseen entities, relationships or tokens. Furthermore, the multi-relational graph defined by the facts in SimpleQuestions is extremely sparse, i.e. each node has very few edges to other nodes, as can be expected due to high ratio of unique entities over number of examples. Therefore, even for many of the entities in SimpleQuestions, the model may perform poorly if the embedding is learned solely based on the SimpleQuestions dataset alone.\nOn the source side, we can resolve this issue by initializing the subject, relationship and object embeddings to those learned by applying multi-relational embedding-based models to the knowledge base. Multi-relational embeddingbased models (Bordes et al., 2011) have recently become popular to learn distributed vector embeddings for knowledge bases, and have shown to scale well and yield good performance. Due to its simplicity and good performance, we choose to use TransE (Bordes et al., 2013) to learn such embeddings. TransE is a translation-based model, whose energy function is trained to output low values when the fact expresses true information, i.e. a fact which exists in the knowledge base, and otherwise high values. Formally, the energy function is defined as f(s, r, o) = ||es + er \u2212 eo||2, where es, er and eo are the real-valued embedding vectors for the subject, relationship and object of a fact. Further details are given by Bordes et al. (2013).\nEmbeddings for entities with few connections are easy to learn, yet the quality of these embeddings depends on how inter-connected they are. In the extreme case where the subject and object of a triple only appears once in the dataset, the learned embeddings of the subject and object will be semantically meaningless. This happens very often in SimpleQuestions, since only around 5% of the entities have more than 2 connections in the graph. Thus, by applying TransE directly over this set of triples, we would eventually end up with a layout of entities that does not contain clusters of semantically close concepts. In order to guarantee an effective semantic representation of the embeddings, we have to learn them together with additional triples extracted from the whole Freebase graph to complement the SimpleQuestions graph with relevant information for this task.\nWe need a coarse representation for the entities contained in SimpleQuestions, capturing the ba-\nsic information, like the profession or nationality, the annotators tend to use when phrasing the questions, and accordingly we have ensured the embeddings contain this information by taking triples coming from the Freebase graph5 regarding:\n1. Category information: given by the type/instance relationship, this ensures that all the entities of the same semantic category are close to each other. Although one might think that the expected category of the subject/object could be inferred directly from the relationship, there are fine-grained differences in the expected types that be extracted only directly by observing this category information.\n2. Geographical information: sometimes the annotators have included information about nationality (e.g. Which French president. . . ?) or location (e.g. Where in Germany. . . ?) of the subject and/or object. This information is given by the relationships person/nationality and location/contained by. By including these facts in the learning, we ensure the existence of a fine-grained layout of the embeddings regarding this information within a same category.\n3. Gender: similarly, sometimes annotators have included information about gender (e.g. Which male audio engineer. . . ?). This information is given by the relationship person/gender.\nTo this end, we have included more than 300, 000 facts from Freebase in addition to the facts in SimpleQuestions for training. Table 2 shows the differences in the embeddings before and after adding additional facts for training the TransE representations."}, {"heading": "4.4 Generating Questions", "text": "To resolve the problem of data sparsity and previously unseen words on the target side, we draw inspiration from the placeholders proposed for handling rare words in neural machine translation by Luong et al. (2015). For every question and answer pair, we search for words in the question\n5Extracted from one of the latest Freebase dumps (downloaded by mid-August 2015) https://developers. google.com/freebase/data\nwhich overlap with words in the subject string of the fact.6 We heuristically estimate the sequence of most likely words in the question, which correspond to the subject string. These words are then replaced by the placeholder token <placeholder>. For example, given the fact {fires creek, contained by, nantahala national forest} the original question Which forest is Fires Creek in? is transformed into the question Which forest is <placeholder>in?. The model is trained on these modified questions, which means that model only has to learn decoder embeddings for tokens which are not in the subject string. At test time, after outputting a question, all placeholder tokens are replaced by the subject string and then the outputs are evaluated. We call this the Single-Placeholder (SP) model. The main difference with respect to that of Luong et al. (2015) is that we do not use placeholder tokens in the input language, because then the entities and relationships in the input would not be able to transmit semantic (e.g. topical) information to the decoder. If we had included placeholder tokens in the input language, the model would not be able to generate informative words regarding the subject in the question (e.g. it would be impossible for the model to learn that the subject Paris may be accompanied by the words French city when generating a question, because it would not see Paris but only a placeholder token).\nA single placeholder token for all question types could unnecessarily limit the model. We therefore also experiment with another model, called the Multi-Placeholder (MP) model, which uses 60 different placeholder tokens such that the placeholder for a given question is chosen based on the subject category extracted from the relationship (e.g. contained by is classified in the category location, and so the transformed question would\n6We use the tool difflib: https://docs.python. org/2/library/difflib.html\nbe Which forest is <location placeholder> in?). This could make it easier for the model to learn to phrase questions about a diverse set of entities, but it also introduces additional parameters, since there are now 60 placeholder embeddings to be learned, and therefore the model may suffer from overfitting. This way of addressing the sparsity in the output reduces the vocabulary size to less than 7000 words."}, {"heading": "4.5 Template-based Baseline", "text": "To compare our neural network models, we propose a (non-parametric) template-based baseline model, which makes use of the entire training set when generating a question. The baseline operates on questions modified with the placeholder as in the preceding section. Given a fact F as input, the baseline picks a candidate fact Fc in the training set at uniformly random, where Fc has the same relationship as F . Then the baseline considers the questions corresponding to Fc and as in the SP model, in the final step the placeholder token in the question is replaced by the subject string of the fact F ."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Training Procedure", "text": "To train the neural network models, we optimized the log-likelihood using the first-order gradientbased optimization algorithm Adam (Kingma and Ba, 2015). To decide when to stop training we used early stopping with patience (Bengio, 2012) on the METEOR score obtained for the validation set. In all experiments, we use the default split of the SimpleQuestions dataset into training, validation and test sets.\nWe trained TransE embeddings with embedding dimensionality 200 for each subject, relationship and object. Based on preliminary experiments, for all neural network models we fixed the learning\nrate to 0.00025 and clipped parameter gradients with norms larger than 0.1. We further fixed the embedding dimensionality of words to be 200, and the hidden state of the decoder RNN to have dimensionality 600."}, {"heading": "5.2 Evaluation", "text": "To investigate the performance of our models, we make use of both automatic evaluation metrics and human evaluators."}, {"heading": "5.2.1 Automatic Evaluation Metrics", "text": "BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) are two widely used evaluation metrics in statistical machine translation and automatic image-caption generation (Chen et al., 2015). Similar to statistical machine translation, where a phrase in the source language is mapped to a phrase in the target language, in this task a KB fact is mapped to a natural language question. Both tasks are highly constrained, e.g. the set of valid outputs is limited. This is true in particular for short phrases, such as one sentence questions. Furthermore, both in both tasks, the majority of valid outputs are paraphrases of each other, which BLEU and METEOR have been designed to capture. We therefore believe that BLEU and METEOR constitute reasonable performance metrics for evaluating the generated questions.\nAlthough we believe that METEOR and BLEU are reasonable evaluation metrics, they may have not recognize certain paraphrases, in particular paraphrases of entities. We therefore also make use of a sentence similarity metric, as proposed by Rus and Lintean (2012), which we will denote Embedding Greedy (Emb. Greedy). The metric makes use of a word similarity score, which in our experiments is the cosine similarity between two Word2Vec word embeddings (Mikolov et al., 2013).7 The metric finds a (non-exclusive) alignment between words in the two questions, which maximizes the similarity between aligned words, and computes the sentence similarity as the mean over the word similarities between aligned words.\nThe results are shown in Table 3. Example questions produced by the model with multiple placeholders are shown in Table 4. The neural network models outperform the template-\n7We use the Word2Vec embeddings pretrained on the Google News Corpus: https://code.google.com/ p/word2vec/.\nbased baseline by a clear margin across all metrics. Template-based baseline is already a relatively strong model, because it makes use of a separate template for each relationship. Given enough training data this suggests that neural networks are generally better at the question generation task compared to hand-crafted template-based procedures, and therefore that they may be useful for generating question answering corpora. Furthermore, it appears that the best performing models are the models where TransE are trained on the largest set of triples (TransE++). This set contains, apart from the supporting triples described in Section 4.3, triples involving entities which are highly connected to the entities found in the SimpleQuestions facts. In total, around 30 millions of facts, which have been used to generate the 30M Factoid Question-Answer Corpus. Lastly, it is not clear whether the model with a single placeholder or the model with multiple placeholders performs best. This motivates the following human evaluation study."}, {"heading": "5.2.2 Human Evaluation Study", "text": "We carry out pairwise preference experiments on Amazon Mechanical Turk.\nInitially, we considered carrying out separate experiments for measuring relevancy and fluency respectively, since this is common practice in machine translation. However, the relevancy of a question is determined solely by a single factor, i.e. the relationship, since by construction the subject is always in the question. Measuring relevancy is therefore not very useful in our task. To verify this we carried out an internal pairwise preference experiment with human subjects, who were repeatedly shown a fact and two questions and asked to select the most relevant question. We found that 93% of the questions generated by the MP Triples TransE++ model were either judged better or at least as good as the human generated questions w.r.t. relevancy. The remaining 7% questions of the MP Triples TransE++ model questions were also judged relevant questions, although less so compared to the human generated questions. In the next experiment, we therefore measure the holistic quality of the questions.\nWe setup experiments comparing: HumanBaseline (human and baseline questions), HumanMP (human and MP Triples TransE++ questions) and Baseline-MP (baseline and MP Triples TransE++ questions). We show human evaluators\na fact along with two questions, one question from each model for the corresponding fact, and ask the them to choose the question which is most relevant to the fact and most natural. The human evaluator also has the option of not choosing either question. This is important if both questions are equally good or if neither of the questions make sense. At the beginning of each experiment, we show the human evaluators two examples of statements and a corresponding pair of questions, where we briefly explain the form of the statements and how questions relate to those statements. Following the introductory examples, we present the facts and corresponding pair of questions one by one. To avoid presentation bias, we randomly shuffle the order of the examples and the order in which questions are shown by each model. During each experiment, we also show four check facts and corresponding check questions at random, which any attentive human annotator should be able to answer easily. We discard responses of human evaluators who fail any of these four checks.\nThe preference of each example is defined as the question which is preferred by the majority of the evaluators. Examples where neither of the two questions are preferred by the majority of the evaluators, i.e. when there is an equal number of eval-\nuators who prefer each question, are assigned to a separate preference class called \u201ccomparable\u201d.8\nThe results are shown in Table 5. In total, 3, 810 preferences were recorded by 63 independent human evaluators. The questions produced by each model model pair were evaluated in 5 batches (HITs). Each human evaluated 44-75 examples (facts and corresponding question pairs) in each batch and each example was evaluated by 3-5 evaluators. In agreement with the automatic evaluation metrics, the human evaluators strongly prefer either the human or the neural network model over the template-based baseline. Furthermore, it appears that humans cannot distinguish between the human-generated questions and the neural network questions, on average showing a preference towards the later over the former ones. We hypothesize this is because our model penalizes uncommon and unnatural ways to frame questions9 and, sometimes, includes specific information about the target object that the humans do not (see last example of Table 4). This confirms our earlier as-\n8The probabilities for the \u201ccomparable\u201d class in Table 5 can be computed in each row as 100 minus the third and fourth column in the table.\n9Upon further inspection, we believe that some questions of the SimpleQuestions dataset have been produced by nonnative English speakers.\nsertion, that the neural network questions can be used for building question answering systems."}, {"heading": "6 Conclusion", "text": "We propose new neural network models for mapping knowledge base facts into corresponding natural language questions. The neural networks combine ideas from recent neural network architectures for statistical machine translation, as well as multi-relational knowledge base embeddings for overcoming sparsity issues and placeholder techniques for handling rare words. The produced question and answer pairs are evaluated using automatic evaluation metrics, including BLEU, METEOR and sentence similarity, and are found to outperform a template-based baseline model. When evaluated by untrained human subjects, the question and answer pairs produced by our best performing neural network appears to be indistinguishable from real human-generated questions. Finally, we use our best performing neural network model to generate a corpus of 30M question and answer pairs, which we hope will enable future researchers to improve their question answering systems."}, {"heading": "Acknowledgments", "text": "The authors acknowledge IBM Research, NSERC, Canada Research Chairs and CIFAR for funding. The authors thank Yang Yu, Bing Xiang, Bowen Zhou and Gerald Tesauro for constructive feedback, and Antoine Bordes, Nicolas Usunier, Sumit Chopra and Jason Weston for providing the SimpleQuestions dataset. This research was enabled in part by support provided by Calcul Qubec (www.calculquebec.ca) and Compute Canada (www.computecanada.ca)."}, {"heading": "A Supplemental Material: Generated Questions", "text": ""}], "references": [{"title": "Automation of question generation from sentences", "author": ["Ali et al.2010] Husam Ali", "Yllias Chali", "Sadid A Hasan"], "venue": "In Proceedings of QG2010: The Third Workshop on Question Generation,", "citeRegEx": "Ali et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ali et al\\.", "year": 2010}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "METEOR: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures", "author": ["Banerjee", "Lavie2005] Satanjeev Banerjee", "Alon Lavie"], "venue": null, "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Yoshua Bengio"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bengio.", "year": 2012}, {"title": "Semantic parsing via paraphrasing", "author": ["Berant", "Liang2014] Jonathan Berant", "Percy Liang"], "venue": "In Proceedings of ACL,", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD international", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Open question answering with weakly supervised embedding models. In Machine Learning and Knowledge Discovery in Databases - European Conference, (ECML PKDD)", "author": ["Jason Weston", "Nicolas Usunier"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Largescale simple question answering with memory", "author": ["Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Generating questions automatically from informational text", "author": ["Chen et al.2009] Wei Chen", "Gregory Aist", "Jack Mostow"], "venue": "In Proceedings of the 2nd Workshop on Question Generation (AIED", "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Microsoft COCO captions: Data collection and evaluation", "author": ["Chen et al.2015] Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Dollar", "C. Lawrence Zitnick"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Question generation based on lexicosyntactic patterns learned from the web", "author": ["Curto et al.2012] Sergio Curto", "A Mendes", "Luisa Coheur"], "venue": "Dialogue and Discourse,", "citeRegEx": "Curto et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Curto et al\\.", "year": 2012}, {"title": "Generating natural language from linked data: Unsupervised template extraction", "author": ["Duma", "Klein2013] Daniel Duma", "Ewan Klein"], "venue": null, "citeRegEx": "Duma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duma et al\\.", "year": 2013}, {"title": "Web question answering: Is more always better", "author": ["Dumais et al.2002] Susan Dumais", "Michele Banko", "Eric Brill", "Jimmy Lin", "Andrew Ng"], "venue": "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in in-", "citeRegEx": "Dumais et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Dumais et al\\.", "year": 2002}, {"title": "QUEST: A model of question answering", "author": ["Sallie E Gordon", "Lawrence E Brainerd"], "venue": "Computers and Mathematics with Applications,", "citeRegEx": "Graesser et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Graesser et al\\.", "year": 1992}, {"title": "LSTM: A search space odyssey", "author": ["Greff et al.2015] Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Natural language question generation using syntax and keywords", "author": ["Ajeesh Elikkottil", "Rajarshi Das"], "venue": "In Proceedings of QG2010: The Third Workshop on Question Generation,", "citeRegEx": "Kalady et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kalady et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "In The International Conference on Learning Representations", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Building large knowledge-based systems; representation and inference in the Cyc project", "author": ["Lenat", "Guha1989] Douglas B. Lenat", "Ramanathan V. Guha"], "venue": null, "citeRegEx": "Lenat et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Lenat et al\\.", "year": 1989}, {"title": "Is question answering fit for the semantic web? a survey", "author": ["Lopez et al.2011] Vanessa Lopez", "Victoria Uren", "Marta Sabou", "Enrico Motta"], "venue": "Semantic Web,", "citeRegEx": "Lopez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lopez et al\\.", "year": 2011}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proceedings of ACL,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Question generation from paragraphs at upenn: Qgstec system description", "author": ["Rashmi Prasad", "Aravind Joshi"], "venue": "In Proceedings of QG2010: The Third Workshop on Question Generation,", "citeRegEx": "Mannem et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mannem et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Question generation from concept maps", "author": ["Olney et al.2012] Andrew M Olney", "Arthur C Graesser", "Natalie K Person"], "venue": "Dialogue and Discourse,", "citeRegEx": "Olney et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Olney et al\\.", "year": 2012}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A comparison of greedy and optimal assessment of natural language student input using wordto-word similarity metrics", "author": ["Rus", "Lintean2012] Vasile Rus", "Mihai Lintean"], "venue": "In Proceedings of the Seventh Workshop on Building Educational Appli-", "citeRegEx": "Rus et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rus et al\\.", "year": 2012}, {"title": "The first question generation shared task evaluation challenge", "author": ["Rus et al.2010] Vasile Rus", "Brendan Wyse", "Paul Piwek", "Mihai Lintean", "Svetlana Stoyanchev", "Cristian Moldovan"], "venue": "In Proceedings of the 6th International Natural Language Generation", "citeRegEx": "Rus et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rus et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Overview of the trec-9 question answering track", "author": ["Voorhees", "Tice2000] Ellen M Voorhees", "DM Tice"], "venue": null, "citeRegEx": "Voorhees et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Voorhees et al\\.", "year": 2000}, {"title": "Wikidata: a free collaborative knowledgebase", "author": ["Vrande\u010di\u0107", "Kr\u00f6tzsch2014] Denny Vrande\u010di\u0107", "Markus Kr\u00f6tzsch"], "venue": "Communications of the ACM,", "citeRegEx": "Vrande\u010di\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vrande\u010di\u0107 et al\\.", "year": 2014}, {"title": "Question generation with minimal recursion semantics", "author": ["Yao", "Zhang2010] Xuchen Yao", "Yi Zhang"], "venue": "In Proceedings of QG2010: The Third Workshop on Question Generation,", "citeRegEx": "Yao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2010}, {"title": "Semantics-based question generation and implementation", "author": ["Yao et al.2012] Xuchen Yao", "Gosse Bouma", "Yi Zhang"], "venue": "Dialogue and Discourse,", "citeRegEx": "Yao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 21, "context": "More recently, researchers have started to utilize large-scale knowledge bases (KBs) (Lopez et al., 2011), such as Freebase (Bollacker et al.", "startOffset": 85, "endOffset": 105}, {"referenceID": 5, "context": ", 2011), such as Freebase (Bollacker et al., 2008), WikiData (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) and Cyc (Lenat and Guha, 1989).", "startOffset": 26, "endOffset": 50}, {"referenceID": 8, "context": "hand-crafted rules and heuristics to synthesize artificial QA corpora (Bordes et al., 2014; Bordes et al., 2015).", "startOffset": 70, "endOffset": 112}, {"referenceID": 9, "context": "hand-crafted rules and heuristics to synthesize artificial QA corpora (Bordes et al., 2014; Bordes et al., 2015).", "startOffset": 70, "endOffset": 112}, {"referenceID": 9, "context": "We frame question generation as a transduction problem starting from a Freebase fact, represented by a triple consisting of a subject, a relationship and an object, which is transduced into a question about the subject, where the object is the correct answer (Bordes et al., 2015).", "startOffset": 259, "endOffset": 280}, {"referenceID": 12, "context": "els (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), and we use an approach similar to Luong et al.", "startOffset": 4, "endOffset": 69}, {"referenceID": 29, "context": "els (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), and we use an approach similar to Luong et al.", "startOffset": 4, "endOffset": 69}, {"referenceID": 1, "context": "els (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), and we use an approach similar to Luong et al.", "startOffset": 4, "endOffset": 69}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015), and we use an approach similar to Luong et al. (2015) for dealing with the problem of rare-words.", "startOffset": 8, "endOffset": 86}, {"referenceID": 27, "context": "Question generation has attracted interest in recent years with notable work by Rus et al. (2010), fol-", "startOffset": 80, "endOffset": 98}, {"referenceID": 18, "context": "A simple rule-based approach was proposed in different studies as wh-fronting or wh-inversion (Kalady et al., 2010; Ali et al., 2010).", "startOffset": 94, "endOffset": 133}, {"referenceID": 0, "context": "A simple rule-based approach was proposed in different studies as wh-fronting or wh-inversion (Kalady et al., 2010; Ali et al., 2010).", "startOffset": 94, "endOffset": 133}, {"referenceID": 23, "context": "been addressed in two different ways: by using named entity recognizers (Mannem et al., 2010; Yao and Zhang, 2010) or semantic role labelers (Chen et al.", "startOffset": 72, "endOffset": 114}, {"referenceID": 10, "context": ", 2010; Yao and Zhang, 2010) or semantic role labelers (Chen et al., 2009).", "startOffset": 55, "endOffset": 74}, {"referenceID": 10, "context": ", 2010; Yao and Zhang, 2010) or semantic role labelers (Chen et al., 2009). In Curto et al. (2012) questions are split into classes according to their syn-", "startOffset": 56, "endOffset": 99}, {"referenceID": 10, "context": "After the identification of key points, Chen et al. (2009) apply handcrafted-templates to generate questions framed in the right target expression by following the analysis of Graesser et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 10, "context": "After the identification of key points, Chen et al. (2009) apply handcrafted-templates to generate questions framed in the right target expression by following the analysis of Graesser et al. (1992), who classify questions according to a taxonomy consisting of 18 categories.", "startOffset": 40, "endOffset": 199}, {"referenceID": 33, "context": "a syntactic representation of the sentence), and second, transform the symbolic representation of the text into the question (Yao et al., 2012).", "startOffset": 125, "endOffset": 143}, {"referenceID": 14, "context": "In the same spirit, Duma et al. (2013) generate short descriptions from triples by using templates defined by the rela-", "startOffset": 20, "endOffset": 39}, {"referenceID": 25, "context": "Our baseline is similar to that of Olney et al. (2012), where a set of relationship-specific templates are defined.", "startOffset": 35, "endOffset": 55}, {"referenceID": 5, "context": "In Freebase (Bollacker et al., 2008) these relationships are directed and always connect exactly two entities.", "startOffset": 12, "endOffset": 36}, {"referenceID": 9, "context": "We use the SimpleQuestions dataset (Bordes et al., 2015) in order to train our models.", "startOffset": 35, "endOffset": 56}, {"referenceID": 29, "context": "translation models (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 19, "endOffset": 66}, {"referenceID": 1, "context": "translation models (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 19, "endOffset": 66}, {"referenceID": 7, "context": "3), we have learned it separately and beforehand with TransE (Bordes et al., 2013), a model aimed at modeling this kind of multi-relational data.", "startOffset": 61, "endOffset": 82}, {"referenceID": 12, "context": "For the decoder, we use a GRU recurrent neural network (RNN) (Cho et al., 2014) with an", "startOffset": 61, "endOffset": 79}, {"referenceID": 1, "context": "attention-mechanism (Bahdanau et al., 2015) on the encoder representation to generate the associated question Q to that fact F .", "startOffset": 20, "endOffset": 43}, {"referenceID": 17, "context": "RNN architectures, such as the LSTM RNN (Greff et al., 2015).", "startOffset": 40, "endOffset": 60}, {"referenceID": 6, "context": "Multi-relational embeddingbased models (Bordes et al., 2011) have recently become popular to learn distributed vector embed-", "startOffset": 39, "endOffset": 60}, {"referenceID": 7, "context": "Due to its simplicity and good performance, we choose to use TransE (Bordes et al., 2013) to learn such embeddings.", "startOffset": 68, "endOffset": 89}, {"referenceID": 6, "context": "Further details are given by Bordes et al. (2013).", "startOffset": 29, "endOffset": 50}, {"referenceID": 22, "context": "dling rare words in neural machine translation by Luong et al. (2015). For every question and answer pair, we search for words in the question", "startOffset": 50, "endOffset": 70}, {"referenceID": 22, "context": "The main difference with respect to that of Luong et al. (2015) is that we do not use placeholder tokens in the input language, because then the entities and relationships in the in-", "startOffset": 44, "endOffset": 64}, {"referenceID": 3, "context": "To decide when to stop training we used early stopping with patience (Bengio, 2012) on the METEOR score obtained for the validation set.", "startOffset": 69, "endOffset": 83}, {"referenceID": 26, "context": "BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) are two", "startOffset": 5, "endOffset": 28}, {"referenceID": 11, "context": "widely used evaluation metrics in statistical machine translation and automatic image-caption generation (Chen et al., 2015).", "startOffset": 105, "endOffset": 124}, {"referenceID": 24, "context": "The metric makes use of a word similarity score, which in our experiments is the cosine similarity between two Word2Vec word embeddings (Mikolov et al., 2013).", "startOffset": 136, "endOffset": 158}], "year": 2016, "abstractText": "Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale questionanswer corpora available. In this paper we present the 30M Factoid QuestionAnswer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the questiongeneration model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear to be indistinguishable from real human-generated questions.", "creator": "LaTeX with hyperref package"}}}