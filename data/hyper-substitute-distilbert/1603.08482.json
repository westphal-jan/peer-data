{"id": "1603.08482", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2016", "title": "Estimating Mixture Models via Mixtures of Polynomials", "abstract": "mixture modeling yielded some general technique upon making naive simple model more popular involving explicit combination. this generality gains novelty in part explains the success of complex expectation vector ( md ) formulas, in that components are generated statistics derive for sample wide model called mixture theories. however, the likelihood of a mixture model is non - convex, so em yield no reliable global distribution guarantees. recently, method defined fitting theorists offer improved guidelines for compact population models, finding they don't extend easily because the range of specified constraints that exist. in informal work, analysts present polymom, our unifying equation based on method of moments in which integration procedures might easily derivable, just as in em. polymom extends applicable when the moments of both particular mixture data are polynomials of the value. any key solution is because the arguments of the mixture modelling are any mixture of elementary polynomials, which allows us practically cast estimation as a generalized moment problem. we obtained generic relaxations have semidefinite bounds, hence could get parameters using ideas from computer algebra. simulations help allows you to draw insights and apply framework from program intelligence, computer algebra and set theory of moments to study concepts in statistical estimation.", "histories": [["v1", "Mon, 28 Mar 2016 18:55:02 GMT  (271kb,D)", "http://arxiv.org/abs/1603.08482v1", "NIPS 2015"]], "COMMENTS": "NIPS 2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["sida i wang", "arun tejasvi chaganty", "percy liang"], "accepted": true, "id": "1603.08482"}, "pdf": {"name": "1603.08482.pdf", "metadata": {"source": "CRF", "title": "Estimating Mixture Models via Mixtures of Polynomials", "authors": ["Sida I. Wang", "Arun Tejasvi Chaganty", "Percy Liang"], "emails": ["sidaw@cs.stanford.edu", "chaganty@cs.stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Mixture models play a central role in machine learning and statistics, with diverse applications including bioinformatics, speech, natural language, and computer vision. The idea of mixture modeling is to explain data through a weighted combination of simple parametrized distributions [52, 38]. In practice, maximum likelihood estimation via Expectation Maximization (EM) has been the workhorse for these models, as the parameter updates are often easily derivable. However, EM is well-known to suffer from local optima. The method of moments, dating back to Pearson [47] in 1894, is enjoying a recent revival [3, 2, 5, 27, 26, 11, 28, 24, 22, 8] due to its strong global theoretical guarantees. However, current methods depend strongly on the specific distributions and are not easily extensible to new ones.\nIn this paper, we present a method of moments approach, which we call Polymom, for estimating a wider class of mixture models in which the moment equations are polynomial equations (Section 2). Solving general polynomial equations is NP-hard, but our key insight is that for mixture models, the moments equations are mixtures of polynomials equations and we can hope to solve them if the moment equations for each mixture component are simple polynomials equations that we can solve. Polymom proceeds as follows: First, we recover mixtures of monomials of the parameters from the data moments by solving an instance of the Generalized Moment Problem (GMP) [34, 33] (Section 3). We show that for many mixture models, the GMP can be solved with basic linear algebra and in the general case, can be approximated by an SDP in which the moment\nar X\niv :1\n60 3.\n08 48\n2v 1\n[ st\nat .M\nL ]\n2 8\nM ar\n2 01\nequations are linear constraints. Second, we extend multiplication matrix ideas from the computer algebra literature [48, 39, 50, 25] to extract the parameters by solving a generalized eigenvalue problem (Section 4).\nPolymom improves on previous method of moments approaches in both generality and flexibility. First, while tensor factorization has been the main driver for many of the method of moments approaches for many types of mixture models, [5, 4, 11, 26, 6, 22], each model required specific adaptations which are non-trivial even for experts. In contrast, Polymom provides a unified principle for tackling new models that is as turnkey as computing gradients or EM updates. To use Polymom (Figure 1), one only needs to provide a list of observation functions (\u03c6n) and derive their expected values expressed symbolically as polynomials in the parameters of the specified model (fn). Polymom then estimates expectations of \u03c6n and outputs parameter estimates of the specified model. Since Polymom works in an optimization framework, we can easily incorporate constraints such as non-negativity and parameter tying which is difficult to do in the tensor factorization paradigm. In simulations, we compared Polymom with EM and tensor factorization and found that Polymom performs similarly or better on some models (Section 5)."}, {"heading": "2 Problem formulation", "text": ""}, {"heading": "2.1 The method of moments estimator", "text": "In a mixture model, each data point x \u2208 RD is associated with a latent component z \u2208 [K]:\nz \u223c Multinomial(\u03c0), x | z \u223c p(x;\u03b8\u2217z), (1)\nwhere \u03c0 = (\u03c01, . . . , \u03c0K) are the mixing coefficients, \u03b8 \u2217 k \u2208 RP are the true model parameters for the kth mixture component, and x \u2208 RD is the random variable representing data. We restrict\nour attention to mixtures where each component distribution comes from the same parameterized family. For example, for a mixture of Gaussians, \u03b8\u2217k = (\u03be \u2217 k \u2208 RD,\u03a3\u2217k \u2208 RD\u00d7D) consists of the mean and covariance of component k. We define N observation functions \u03c6n : RD \u2192 R for n \u2208 [N ] and define fn(\u03b8) to be the expectation of \u03c6n over a single component with parameters \u03b8, which we assume is a simple polynomial:\nfn(\u03b8) := Ex\u223cp(x;\u03b8)[\u03c6n(x)] = \u2211\n\u03b1\nan\u03b1\u03b8 \u03b1, (2)\nwhere \u03b8\u03b1 = \u220fP p=1 \u03b8 \u03b1p p . The expectation of each observation function E[\u03c6n(x)] can then be expressed as a mixture of polynomials of the true parameters\nE[\u03c6n(x)] = K\u2211\nk=1\n\u03c0kE[\u03c6n(x)|z = k] = K\u2211\nk=1\n\u03c0kfn(\u03b8 \u2217 k). (3)\nThe method of moments for mixtures seeks parameters [\u03b8k] K k=1 that satisfy the moment condi-\ntions expressed as the following polynomial equations:\nE[\u03c6n(x)] = K\u2211\nk=1\n\u03c0kfn(\u03b8k). (4)\nwhere E[\u03c6n(x)] can be estimated from the data: 1T \u2211T t=1 \u03c6n(xt)\np\u2192 E[\u03c6n(x)]. Clearly, the true parameters [\u03b8\u2217k] K k=1 satisfy these conditions as in (3). The goal of this work is to find parameters satisfying moment conditions that can be written in the mixture of polynomial form (4). We assume that the N given observations functions \u03c61, . . . , \u03c6N uniquely identify the model parameters (up to permutation of the components).\nExample 2.1 (1-dimensional Gaussian mixture). Consider a K-mixture of 1D Gaussians with parameters \u03b8k = [\u03bek, \u03c3 2 k] corresponding to the mean and variance, respectively, of the k-th component (Figure 1: steps 1 and 2). We choose the observation functions, \u03c6(x) = [x1, . . . , x6], which have corresponding moment polynomials,\nf(\u03b8) = [\u03be, \u03be2 + \u03c32, \u03be3 + 3\u03be\u03c32, . . . ].\nFor example, instantiating (4), E[x2] = \u2211K k=1 \u03c0k(\u03be 2 k + \u03c3 2 k). Given \u03c6(x) and f(\u03b8\n\u2217), and data, the Polymom framework can recover the parameters. Note that the 6 moments we use have been shown by Pearson [47] to be sufficient for a mixture of two Gaussians.\nExample 2.2 (Mixture of linear regressions). Consider a mixture of linear regressions [54, 11], where each data point x = [x, y] is drawn from component k by sampling x from an unknown distribution independent of k and setting y = wkx + , where \u223c N (0, \u03c32k). The parameters \u03b8k = (wk, \u03c3 2 k) are the slope and noise variance for each component k. Let us take our observation functions to be \u03c6(x) = [x, xy, xy2, x2, . . . , x3y2],\nfor which the moment polynomials are\nf(\u03b8) = [E[x],E[x2]w,E[x3]w2 + E[x]\u03c32,E[x2], . . .].\nIn Example 2.1, the coefficients an\u03b1 in the polynomial fn(\u03b8) are just constants determined by integration. For the conditional model in Example 2.2, the coefficients depends on the data. While Example 2.2 works, Polymom cannot handle arbitrary data dependence, see Appendix B for sufficient conditions and counterexamples."}, {"heading": "2.2 Solving the moment conditions", "text": "Our goal is to recover model parameters \u03b8\u22171, . . . ,\u03b8 \u2217 K \u2208 RP for each of the K components of the mixture model that generated the data as well as their respective mixing proportions \u03c01, . . . , \u03c0K \u2208 R. To start, let\u2019s ignore sampling noise and identifiability issues and suppose that we are given exact moment conditions as defined in (4). Each condition fn \u2208 R[\u03b8] is a polynomial of the parameters \u03b8, for n = 1, . . . , N .\nEquation 4 is a polynomial system of N equations in the K + K \u00d7 P variables [\u03c01, . . . , \u03c0K ] and [\u03b81, . . . ,\u03b8K ] \u2208 RP\u00d7K . It is natural to ask if standard polynomial solving methods can solve (4) in the case where each fn(\u03b8) is simple. Unfortunately, the complexity of general polynomial equation solving is lower bounded by the number of solutions, and each of the K! permutations of the mixture components corresponds to a distinct solution of (4) under this polynomial system representation. While several methods can take advantage of symmetries in polynomial systems [51, 14], they still cannot be adapted to tractably solve (4) to the best of our knowledge.\nThe key idea of Polymom is to exploit the mixture representation of the moment equations (4). One idea is to seek a equivalent representation of the moment conditions expressed as polynomial equations (4) that is invariant to permutations of the K components. Specifically, let \u00b5\u2217 be a particular \u201cmixture\u201d over the component parameters \u03b8\u22171, . . . ,\u03b8 \u2217 k (i.e. \u00b5\n\u2217 is a probability measure). Then we can express the moment conditions (4) in terms of \u00b5\u2217:\nE[\u03c6n(x)] = \u222b fn(\u03b8) \u00b5 \u2217(d\u03b8), where \u00b5\u2217(\u03b8) = K\u2211\nk=1\n\u03c0k\u03b4(\u03b8 \u2212 \u03b8\u2217k). (5)\nConceptually, we no longer have any permutation invariance because the variable is \u00b5. While permuted solutions of (4) are not equal to each other in the parameter space, \u00b5 remains the same measure regardless of the \u201corder in summing delta functions\u201d. As a result, solving the original moment conditions (4) is equivalent to solving the following feasibility problem over \u00b5, but where we deliberately \u201cforget\u201d the permutation of the components by using \u00b5 to represent the problem:\nfind \u00b5 \u2208M+(RP ), the set of probability measures over RP s.t. \u222b fn(\u03b8) \u00b5(d\u03b8) = E[\u03c6n(x)], n = 1, . . . , N\n\u00b5 is K-atomic (i.e. sum of K deltas). (6)\nIf the true model parameters [\u03b8\u2217k] K k=1 can be identified by the N observed moments up to permu-\ntation, then the measure \u00b5\u2217(\u03b8) = \u2211K k=1 \u03c0k\u03b4(\u03b8 \u2212 \u03b8\u2217k) solving Problem 6 is also unique.\nPolymom solves Problem 6 in two steps:\n1. Moment completion (Section 3): We show that Problem 6 over the measure \u00b5 can be relaxed to an SDP over a certain (parameter) moment matrix Mr(y) whose optimal solution is\nMr(y \u2217) = \u2211K k=1 \u03c0kvr(\u03b8 \u2217 k)vr(\u03b8 \u2217 k) >, where vr(\u03b8 \u2217 k) is the vector of all monomials of degree at most r.\n2. Solution extraction (Section 4): We then take Mr(y) and construct a series of generalized eigendecomposition problems, whose eigenvalues yield [\u03b8\u2217k] K k=1.\nRemark. From this point on, distributions and moments refer to \u00b5\u2217 which is over parameters, not over the data. All the structure about the data is captured in the moment conditions (4)."}, {"heading": "3 Moment completion", "text": "The first step is to reformulate Problem 6 as an instance of the Generalized Moment Problem (GMP) introduced by [33]. A reference on the GMP, algorithms for solving GMPs, and its various extensions is [34]. We start by observing that Problem 6 only depends on the integrals of monomials under the measure \u00b5: for example, if fn(\u03b8) = 2\u03b8 3 1 \u2212 \u03b821\u03b82, then we only need to know the integrals\nover the constituent monomials (y3,0 := \u222b \u03b831\u00b5(d\u03b8) and y2,1 := \u222b \u03b821\u03b82\u00b5(d\u03b8)) in order to evaluate the integral over fn. This suggests that we can optimize over the (parameter) moment sequence y = (y\u03b1)\u03b1\u2208NP , rather than the measure \u00b5 itself. We say that the moment sequence y has a representing measure \u00b5 if y\u03b1 = \u222b \u03b8\u03b1 \u00b5(d\u03b8) for all \u03b1, but we do not assume that such a \u00b5 exists. The Riesz linear functional Ly : R[\u03b8]\u2192 R is defined to be the linear map such that Ly(\u03b8\u03b1) := y\u03b1 and Ly(1) = 1. For example, Ly(2\u03b831\u2212\u03b821\u03b82+3) = 2y3,0\u2212y2,1+3. If y has a representing measure \u00b5, then Ly simply maps polynomials f to integrals of f against \u00b5.\nThe key idea of the GMP approach is to convexify the problem by treating y as free variables and then introduce constraints to guarantee that y has a representing measure. First, let vr(\u03b8) := [\u03b8\u03b1 : |\u03b1| \u2264 r] \u2208 R[\u03b8]s(r) be the vector of all s(r) monomials of degree no greater than r. Then, define the truncated moment matrix as\nMr(y) := Ly(vr(\u03b8)vr(\u03b8) T),\nwhere the linear functional Ly is applied elementwise (see Example 3.1 below). If y has a representing measure \u00b5, then Mr(y) is simply a (positive) integral over rank 1 matrices vr(\u03b8)vr(\u03b8) T with respect to \u00b5, so necessarily Mr(y) 0 holds. Furthermore, by Theorem 1 [16], for y to have a K-atomic representing measure, it is sufficient that rank(Mr(y)) = rank(Mr\u22121(y)) = K. So Problem 6 is equivalent to\nfind y \u2208 RN (or equivalently, find M(y)) s.t. \u2211 \u03b1 an\u03b1y\u03b1 = E[\u03c6n(x)], n = 1, . . . , N\nMr(y) 0, y0 = 1 rank(Mr(y)) = K and rank(Mr\u22121(y)) = K.\n(7)\nUnfortunately, the rank constraints in Problem 7 are not tractable. We use the following relaxation to obtain our final (convex) optimization problem\nminimize y tr(CMr(y))\ns.t. \u2211\n\u03b1 an\u03b1y\u03b1 = E[\u03c6n(x)], n = 1, . . . , N Mr(y) 0, y0 = 1\n(8)\nwhere C 0 is a chosen scaling matrix. A common choice is C = Is(r) corresponding to minimizing the nuclear norm of the moment matrix, the usual convex relaxation for rank. Appendix C discusses some other choices of C and more theory on Problem 8. However, for special cases like three-view mixture models, mixture of linear regressions, etc. Problem 7 can also be solved with basic linear algebra, and there is no need to solve Problem 8 (see Section 5).\nExample 3.1 (moment matrix for a 1-dimensional Gaussian mixture). Recall that the parameters \u03b8 = [\u03be, \u03c32] are the mean and variance of a one dimensional Gaussian. Let us choose the monomials v2(\u03b8) = [1, \u03be, \u03be\n2, \u03c32]. Step 4 for Figure 1 shows the moment matrix when using r = 2. The moment matrix for r = 2 is then:\nMr=2(y) =\n \n1 \u03be \u03be2 \u03c32 \u03be3 \u03bec\n1 y0,0 y1,0 y2,0 y0,1 y3,0 y1,1 \u03be y1,0 y2,0 y3,0 y1,1 y4,0 y2,1 \u03be2 y2,0 y3,0 y4,0 y2,1 y5,0 y3,1 c y0,1 y1,1 y2,1 y0,2 y3,1 y1,2 \u03be3 y3,0 y4,0 y5,0 y3,1 y6,0 y4,1 \u03bec y1,1 y2,1 y3,1 y1,2 y4,1 y2,2  \n(9)\nEach row and column of the moment matrix is labeled with a monomial and entry (i, j) is subscripted by the product of the monomials in row i and column j. For \u03c62(x) = x\n2, we have f2(\u03b8) = \u03be\n2 + c, which leads to the linear constraint y2,0 + y0,1 \u2212 E[x2] = 0. For \u03c63(x) = x3, f3(\u03b8) = \u03be 3 + 3\u03bec, leading to the constraint y3,0 + 3y1,1 \u2212 E[x3] = 0.\nRelated work. Readers familiar with the sum of squares and polynomial optimization literature [32, 36, 46, 45] will note that Problem 8 is similar to the SDP relaxation of a polynomial optimization problem. However, in typical polynomial optimization, we are only interested in solutions \u03b8\u2217 that actually satisfy the given constraints, whereas here we are interested in K solutions [\u03b8\u2217k] K k=1, whose mixture satisfies constraints corresponding to the moment conditions (4). Within machine learning, generalized PCA has been formulated as a moment problem [44] and the Hankel matrix (basically the moment matrix) has been used to learn weighted automata [8]. While similar tools are used, the conceptual approach and the problems considered are different. For example, the moment matrix of this paper consists of unknown moments of the model parameters, whereas exisiting works considered moments of the data that are always directly observable.\nConstraints. Constraints such as non-negativity (for parameters which represent probabilities or variances) and parameter tying [29] are quite common in graphical models and are not easily addressed with existing method of moments approaches. The GMP framework allows us to incorporate some constraints using localizing matrices [15]. Consider the case of a 2D mixture of Gaussians where the mean parameters \u03be1, \u03be2 lies on the parabola \u03be1 \u2212 \u03be22 = 0 for all components. In this case, we just need to add constraints to Problem 8: y(1,0)+\u03b2 \u2212 y(0,2)+\u03b2 = 0 for all \u03b2 \u2208 N2 up to degree |\u03b2| \u2264 2r \u2212 2. Thus, we can handle constraints during the estimation procedure rather than projecting back onto the constraint set as a post-processing step. This is necessary for models that only become identifiable by the observed moments after constraints are taken into account. By incorporating these constraints into parameter estimation, we can possibly identify the model parameters with fewer moments. We describe this method and its learning implications in Appendix D.1.\nGuarantees and statistical efficiency. In some circumstances, e.g. in three-view mixture models or the mixture of linear regressions, the constraints fully determine the moment matrix; we consider these cases in Section 5 and Appendix A. While there are no general guarantee on\nProblem 8, the flat extension theorem tells us when the moment matrix corresponds to a unique solution (more discussions in Appendix C):\nTheorem 1 (Flat extension theorem [16]). Let y be the solution to Problem 8 for a particular r. If Mr(y) 0 and rank(Mr\u22121(y)) = rank(Mr(y)) then y is the optimal solution to Problem 7 for K = rank(Mr(y)) and there exists a unique K-atomic supporting measure \u00b5 of Mr(y).\nRecovering Mr(y) is linearly dependent on small perturbations of the input [21], suggesting that the method has polynomial sample complexity for most models where the moments concentrate at a polynomially rate. In Appendix D, we discuss a few other important considerations like noise robustness, making Problem 8 more statistically efficient, and some open problems."}, {"heading": "4 Solution extraction", "text": "Having completed the (parameter) moment matrix Mr(y) (Section 3), we now turn to the problem of extracting the model parameters [\u03b8\u2217k] K k=1. The solution extraction method we present is based on ideas from solving multivariate polynomial systems where the solutions are eigenvalues of certain multiplication matrices [48, 39, 13, 49].1 The main advantage of the solution extraction view is that higher-order moments and structure in parameters are handled in the framework without model-specific effort.\nRecall that the true moment matrix is\nMr(y \u2217) =\nK\u2211\nk=1\n\u03c0kv(\u03b8 \u2217 k)v(\u03b8 \u2217 k) T ,\nwhere v(\u03b8) := [\u03b8\u03b11 , . . . ,\u03b8\u03b1s(r) ] \u2208 R[\u03b8]s(r) contains all the monomials up to degree r. We use \u03b8 = [\u03b81, . . . , \u03b8P ] for variables and [\u03b8 \u2217 k] K k=1 for the true solutions to these variables (note the boldface). For example, \u03b8\u2217k,p := (\u03b8 \u2217 k)p denotes the p\nth value of the kth component, which corresponds to a solution for the variable \u03b8p. Typically, s(r) K,P and the elements of v(\u03b8) are arranged in a degree ordering so that ||\u03b1i||1 \u2264 ||\u03b1j ||1 for i \u2264 j. We can also write Mr(y\u2217) = VPV>, where V := [v(\u03b8\u22171), . . . ,v(\u03b8 \u2217 K)] \u2208 Rs(r)\u00d7K is the canonical basis and P := diag(\u03c01, . . . , \u03c0K) contains the mixing proportions. At the high level, we want to factorize Mr(y \u2217) to get V, however we cannot simply eigen-decompose Mr(y \u2217) since V is not orthogonal. To overcome this challenge, we will exploit the internal structure of V to construct several other matrices that share the same factors and perform simultaneous diagonalization.\nSpecifically, let V[\u03b21; . . . ;\u03b2K ] \u2208 RK\u00d7K be a sub-matrix of V with only the rows corresponding to monomials with exponents \u03b21, . . . ,\u03b2K \u2208 NP . Typically, \u03b21, . . . ,\u03b2K are just the first K monomials in v. Now consider the exponent \u03b3p \u2208 NP which is 1 in position p and 0 elsewhere, corresponding to the monomial \u03b8\u03b3p = \u03b8p. The key property of the canonical basis is that multiplying each column k by a monomial \u03b8\u2217k,p just performs a \u201cshift\u201d to another set of rows:\nV[\u03b21; . . . ;\u03b2K ] Dp = V [ \u03b21 + \u03b3p; . . . ;\u03b2K + \u03b3p ] , where Dp := diag(\u03b8 \u2217 1,p, . . . , \u03b8 \u2217 K,p). (10)\nNote that Dp contains the p th parameter for all K mixture components.\nExample 4.1 (Shifting the canonical basis). Let \u03b8 = [\u03b81, \u03b82] and the true solutions be \u03b8 \u2217 1 = [2, 3] and \u03b8\u22172 = [\u22122, 5]. To extract the solution for \u03b81 (which are (\u03b8\u22171,1, \u03b8\u22172,1)), let \u03b21 = (1, 0),\u03b22 = (1, 1), 1 Dreesen et al. [20] is a short overview and Stetter [49] is a comprehensive treatment including numerical issues.\nand \u03b31 = (1, 0).\nV =\n  v(\u03b81) v(\u03b82) 1 1 1 \u03b81 2 \u22122 \u03b82 3 5 \u03b821 4 4 \u03b81\u03b82 6 \u221210 \u03b822 9 25 \u03b821\u03b82 12 20  \n[ v1 v2 \u03b81 2 \u22122 \u03b81\u03b82 6 \u221210 ] \ufe38 \ufe37\ufe37 \ufe38 V[\u03b21;\u03b22] [ 2 0 0 \u22122 ] \ufe38 \ufe37\ufe37 \ufe38 diag(\u03b81,1,\u03b82,1) = [ v1 v2 \u03b821 4 4 \u03b821\u03b82 12 20 ] \ufe38 \ufe37\ufe37 \ufe38 V[\u03b21+\u03b31;\u03b22+\u03b31]\n(11)\nWhile (10) reveals the structure of V, we don\u2019t know V. However, we recover its column space U \u2208 Rs(r)\u00d7K from the moment matrix Mr(y\u2217), for example with an SVD. Thus, we can relate U and V by a linear transformation: V = UQ, where Q \u2208 RK\u00d7K is some unknown invertible matrix. (10) can now be rewritten as:\nU[\u03b21; . . . ;\u03b2K ]Q Dp = U [ \u03b21 + \u03b3p; . . . ;\u03b2K + \u03b3p ] Q, p = 1, . . . , P, (12)\nwhich is a generalized eigenvalue problem where Dp are the eigenvalues and Q are the eigenvectors. Crucially, the eigenvalues, Dp = diag(\u03b8 \u2217 1,p, . . . , \u03b8 \u2217 K,p) give us solutions to our parameters. Note that for any choice of \u03b21, . . . ,\u03b2K and p \u2208 [P ], we have generalized eigenvalue problems that share eigenvectors Q, though their eigenvectors Dp may differ. Corresponding eigenvalues (and hence solutions) can be obtained by solving a simultaneous generalized eigenvalue problem, e.g., by using random projections like Algorithm B of [3] or more robust [30] simutaneous diagonalization algorithms [10, 9, 1].\nAlgorithm 1 Basic solution extraction Input: column space basis U \u2208 Rs(r)\u00d7K , \u03b21, . . . ,\u03b2K \u2208 NP so that rank (U[\u03b21; . . . ;\u03b2K ]) = K Output: Estimated solutions \u03b8\u03021, . . . , \u03b8\u0302K \u2208 RP\nfor parameter dimensions p = 1, . . . , P do Bp \u2190 U [ \u03b3p + [\u03b21, . . . ,\u03b2K ] ]\n\u03b3q \u2190 [1p=q]p=1,...,P end for\nFind Q: solve the simultaneous eigenvalue problems: BpQ = U[\u03b21; . . . ;\u03b2K ]QDp for p = 1, . . . , P Find \u03b8\u0302k: Let [q1, . . . ,qK ] := Q for qk \u2208 RK\u00d71 \u03b8\u0302k,p \u2190 \u03c1\nTBpqk \u03c1TU[\u03b21;...;\u03b2K ]qk for p = 1, . . . , P , k = 1, . . . ,K, and arbitrary \u03c1\nWe describe one approach to solve (12) (Algorithm 1), which is similar to Algorithm B of [3]. The idea is to take P random weighted combinations of the equations (12) and solve the resulting (generalized) eigendecomposition problems. Let R \u2208 RP\u00d7P be a random matrix whose entries are drawn from N (0, 1). A simple approach to find Q is solving\nU[\u03b21; . . . ;\u03b2K ] \u22121\n( P\u2211\np=1\nRq,pU [ \u03b21 + \u03b3p; . . . ;\u03b2K + \u03b3p\n] )\nQ = QDq\nfor each q = 1, . . . , P . The resulting eigenvalues can be collected in \u039b \u2208 RP\u00d7K , where \u039bq,k = Dq,k,k. Note that by definition \u039bq,k = \u2211P p=1Rq,p\u03b8 \u2217 k,p, so we can simply invert to obtain [\u03b8 \u2217 1, . . . ,\u03b8 \u2217 K ] =\nR\u22121\u039b. Although this simple approach does not have great numerical properties, these eigenvalue problems are solvable if the eigenvalues [\u03bbq,1, . . . , \u03bbq,K ] are distinct for all q, which happens with probability 1 as long as the parameters \u03b8\u2217k are different from each other. In Appendix A.1, we show how the tensor decomposition algorithm from [3] can be seen as solving (12) for a particular instantiation of \u03b21, . . .\u03b2K ."}, {"heading": "5 Applications", "text": "Let us now look at some applications of Polymom. Table 2 presents several models with corresponding observation functions and moment polynomials. It is fairly straightforward to write down observation functions for a given model. The moment polynomials can then be derived by computing expectations under the model, a computation comparable to deriving updates for EM.\n2 h\u03b1(\u03be, c) = \u2211b\u03b1/2c i=0 a\u03b1,\u03b1\u22122i\u03be \u03b1\u22122ici and a\u03b1,i be the absolute value of the coefficient of the degree i term of the \u03b1th (univariate) Hermite polynomial. For example, the first few are h1(\u03be, c) = \u03be, h2(\u03be, c) = \u03be2+c, h3(\u03be, c) = \u03be3+3\u03bec, h4(\u03be, c) = \u03be4 + 6\u03be2c+ 3c2.\nWe implemented Polymom for several mixture models in Python and the code can be found at https://github.com/sidaw/polymom. A simpler and cleaner demostration of solving a mixture of Gaussian in the noiseless case can be found at https://github.com/sidaw/mompy in the form of an IPython Notebook (extra examples.ipynb). We used CVXOPT to handle the SDP and the random projections algorithm to extract solutions. In Table 3, we show the relative error maxk ||\u03b8k \u2212 \u03b8\u2217k||2/||\u03b8\u2217k||2 averaged over 10 random models of each class.\nGuarantees. In the rest of this section, we will discuss guarantees on parameter recovery for each of these models. In summary, we match many of the existing results in the literature for the mixture of linear regressions and multiview mixtures when K \u2264 D. In these case the moment matrix is fully determined by the linear constraints and Problem 8 is just a linear solve. More discussions can be found in Appendix A.2.\nIn addition, we can obtain per-instance guarantees in the following sense. Recall that Polymom involves solving an SDP relaxation and performing solution extraction. If the SDP solution has a flat extension (Theorem 1) at the true number of components K (a checkable assumption), then we have solved the moment completion problem exactly, and since solution extraction always works, we are guaranteed to obtain the true parameters. On the other hand, if the SDP solution has a higher rank K \u2032 > K, then as a consolation prize, we have found a K \u2032-mixture model that matches the moments (that we observed) of the true K-mixture model."}, {"heading": "6 Conclusion", "text": "We presented an unifying framework for learning many types of mixture models via the method of moments. For example, for the mixture of Gaussians, we can apply the same algorithm to both mixtures in 1D needing higher-order moments [47, 24] and mixtures in high dimensions where lower-order moments suffice [5]. The Generalized Moment Problem [33, 34] and its semidefinite relaxation hierarchies is what gives us the generality, although we rely heavily on the ability of nuclear norm minimization to recover the underlying rank. As a result, while we always obtain parameters satisfying the moment conditions, we do not have formal guarantees on consistent estimation in general, although we do have guarantees for several model families. The second main tool is solution extraction, which characterizes a more general structure of mixture models compared the tensor structure observed by [5, 3]. This view draws connections to the literature on\nsolving polynomial systems, where many techniques might be useful [49, 50, 25]. Finally, through the connections we\u2019ve drawn, it is our hope that Polymom can make the method of moments as turnkey as EM on more latent-variable models, and provide a way to improve the statistical efficiency of method of moments procedures.\nAcknowledgments. This work was supported by a Microsoft Faculty Research Fellowship to the third author and a NSERC PGS-D fellowship for the first author."}, {"heading": "A Examples", "text": "In this section, we first describe how undercomplete tensor factorization can be seen as a special case of the solution extraction framework, and elaborate on the mixture of Gaussians, the mixture of linear regressions and the multiview mixture model.\nA.1 Tensor factorization as solution extraction\nExample A.1 (Tensor decomposition as solution extraction). Many latent variable models have been tackled via tensor decomposition [5], and symmetric, undercomplete tensor decomposition can be framed as a solution extraction problem. Suppose we observe the tensor T := \u2211K k=1 \u03b8 \u2217\u22973 k \u2208 RP\u00d7P\u00d7P . We would like to recover the components \u03b8\u2217k. For us, the inputs are constraints \u03b8r\u03b8s\u03b8t\u2212 Trst = 0 for all r, s, t = 1, . . . , P . Choose v(\u03b8) = [1, \u03b81, . . . , \u03b8P , \u03b8 2 1, \u03b81\u03b82, . . . , \u03b8 2 P ] = [1,\u03b8, vecs(\u03b8\u2297\u03b8)], where vecs : RP\u00d7P \u2192 RP 2 just flattens the matrix. In the simplest case, suppose P = K and rank(U) = K. Then the fully observed U is\nU =\n  size P\n1 U1 P U2 P 2 U3\n  =\n  terms \u03b8 1 Ly(\u03b8) \u03b8 Ly(\u03b8 \u2297 \u03b8)\nvecs(\u03b8\u2297\u03b8) Ly(vecs(\u03b8 \u2297 \u03b8)\u2297 \u03b8)\n  (13)\nwhere the linear functional Ly applies elementwise. One choice of basis is just all the variables U[\u03b21; . . . ;\u03b2K ] = U2 and the eigenvalue problem we are required to solve is the generalized Hermi-\ntian eigenvalue problem U2QD = (\u2211P p=1 \u03b7pLy(\u03b8p\u03b8 \u2297 \u03b8) ) Q. [3] proposed an algorithm that is procedurally identical, where, in their notation Pairs := U2 and Triples(\u03b7) := (\u2211P p=1 \u03b7pLy(\u03b8p\u03b8 \u2297 \u03b8) ) , and the algorithm proposed needed to solve the eigenvalue problem B(\u03b7) = Pairs\u22121 Triples(\u03b7).\nTypically, \u03b21, . . . ,\u03b2K are just the first K monomials in v (i.e. the K monomials of the smallest degree).\nUnder this formulation, generalization to the fully-observed overcomplete tensor decomposition case K \u2265 D = P is clear if we observe enough moments to have enough basis vectors such that rank(U[\u03b21; . . . ;\u03b2K ]) = K:\nProposition A.2. If K \u2264 1+P+P 2+\u00b7 \u00b7 \u00b7+P r = P r+1\u22121P\u22121 , then solution extraction succeeds if we observe moments up to order 2r+1 and monomials vectors of the true parameters vr(\u03b81), . . . ,vr(\u03b8K) are linearly independent.\nProof. To get the theoretical result, it suffices to consider higher-order moments:\nU =\n[terms vecs(\u03b8 \u2297r)\nvecs(\u03b8\u2297r) Ly(vecs(\u03b8 \u2297r)\u2297 vecs(\u03b8\u2297r))\nvecs(\u03b8\u2297r+1) Ly(vecs(\u03b8 \u2297r+1)\u2297 vecs(\u03b8\u2297r))\n] (14)\nwhere we can take the U[\u03b21; . . . ;\u03b2K ] from the top block, and U [ \u03b21 + \u03b3q; . . . ;\u03b2K + \u03b3q ] belongs to the bottom block for all q. So 2r + 1 order moments is needed if K \u2264 P r and this result is comparable to [7]. In practice, we would take all moments vecs(\u03b8\u22971), . . . , vecs(\u03b8\u2297r+1). We may use lower order moments as well:\nU =\n \nterms vecs(\u03b8\u22971) vecs(\u03b8\u22972) \u00b7\u00b7\u00b7 vecs(\u03b8\u2297r)\nvecs(\u03b8\u22971) ... vecs(\u03b8\u22972)\n... \u00b7 \u00b7 \u00b7 Ly(vecs(\u03b8\u2297l)\u2297 vecs(\u03b8\u2297m)) \u00b7 \u00b7 \u00b7 vecs(\u03b8\u2297r+1) ...\n \n(15)\nwhere the entry of this matrix at block l,m is Ly(vecs(\u03b8 \u2297l)\u2297 vecs(\u03b8\u2297m)) as expected. While this still requires observing 2r + 1th order moments, lower order moments are more accurate and can result in better parameter estimates.\nA.2 Moment completion for specific models\nFor several mixture models, we work out the polynomial constraints, and then discuss the moment completion problem."}, {"heading": "A.2.1 Mixture of Linear Regressions", "text": "In Example 2.2, we described the mixture of linear regressions model in 1-dimension with parameters \u03b8\u2217k = (wk, \u03c3 2 k). Let us now consider the D-dimensional extension: we observe x = [x, \u03c5]\n3 where x := [x1, . . . , xD] is drawn from an unspecified distribution and \u03c5 = w \u00b7x+ with \u223c N (0, \u03c32) for a known \u03c3. The parameters are \u03b8\u2217k = (wk) for 1 \u2264 k \u2264 K. Next, we choose observation functions \u03c6\u03b1,b(x) = x\n\u03b1\u03c5b for \u03b1 : 0 \u2264 |\u03b1| \u2264 3 and 0 \u2264 b \u2264 3, with corresponding moment polynomials: f\u03b1,b(\u03b8,x) = x \u03b1E \u223cN (0,\u03c32) [ (w \u00b7 x+ )b ] . These polynomials can be expressed in closed form using Hermite polynomials (see Section A.2.2). For example, f0,2(\u03b8,x) = ( (w \u00b7 x)2 + \u03c32 ) .\nGiven these observation functions and moment polynomials, and data, the Polymom framework solves the moment completion problem (Problem 7) followed by solution extraction (Section 4) to recover the parameters. Further, we can guarantee that Polymom can recover parameters for this model when K \u2264 D by showing that Problem 7 can be solved exactly. Note that while no entry of the moment matrix is directly observed, each observation gives us a linear constraint on the entries of the moment matrix. Let \u03b3p \u2208 NP be the vector with value 1 at position p and 0 elsewhere, then Ly(f\u03b1,1(\u03b8)) = \u2211P p=1 E[x \u03b1+\u03b3p ]y\u03b3p , and Ly(f\u03b1,2(\u03b8)) = ( E[x\u03b1]\u03c32 + \u2211P p,q=1 E[x \u03b1+\u03b3p+\u03b3q ]y\u03b3p+\u03b3q ) , etc. When K \u2264 D, there are enough equations that this system admits an unique solution for y. Note that [11] recover parameters for this model by solving a series of low-rank tensor recovery problems, which ultimately requires the computation of the same moments described above. In contrast, the Polymom framework makes the dependence on moments upfront and takes care of the heavy-lifting in a problem-agnostic manner. Furthermore, we can even obtain parameters outside the regime of [11]: with the above observation functions and moment polynomials, we can recover parameters (with a certificate) ."}, {"heading": "A.2.2 Mixture of Gaussians", "text": "We now look at D-dimensional extensions to Example 2.1. Let the data be drawn from Gaussians with diagonal covariance, x|z \u223c N (\u03bez,diag(cz)). The parameters of this model are \u03b8\u2217k = (\u03bek, ck) \u2208\n3 We use \u03c5 here since y is reserved for the parameter moments.\nR2D. The observable functions are \u03c6\u03b1(x) := x\u03b1, and the moment polynomials are f\u03b1(\u03b8) = E[x\u03b1] = \u220fD d=1 h\u03b1[d](\u03be[d], c[d]), where h\u03b1(\u03be, c) = \u2211b\u03b1/2c i=0 a\u03b1,\u03b1\u22122i\u03be\n\u03b1\u22122ici and a\u03b1,i be the absolute value of the coefficient of the degree i term of the \u03b1th (univariate) Hermite polynomial. The first few are h1(\u03be, c) = \u03be, h2(\u03be, c) = \u03be 2 + c, h3(\u03be, c) = \u03be 3 + 3\u03bec, h4(\u03be, c) = \u03be\n4 + 6\u03be2c+ 3c2. Using this set of \u03c6\u03b1 and f\u03b1, Polymom will attempt to solve the SDP in Problem 8 and recover the parameters. In this case however, the moment conditions are non-trivial and we cannot guarantee recovery of the true parameters. However, Polymom is guaranteed to recover parameters that match the moments and that minimizes nuclear norm.\nThis full covariance case poses no conceptual trouble for Polymom. In the case of full covariance, Isserlis theorem (or Wicks theorem) allows us to derive these polynomials and [53] provides an algorithm for computing these polynomials. Toeplitz covariance or other structured covariances with parameter sharing or constraints are also conceptually handled under Polymom.\nWe can modify this model by introducing constraints: consider the case of 2D mixture where the mean parameters for all components lies on a parabola \u03be1 \u2212 \u03be22 = 0. In this case, we just need to add constraints to Problem 8: y(1,0)+\u03b2 \u2212 y(0,2)+\u03b2 = 0 for all \u03b2 \u2208 N2 up to degree |\u03b2| \u2264 2r\u2212 2.\nBy incorporating these contraints at estimation time, we can possibly identify the model parameters with less moments. See Section D for more details."}, {"heading": "A.2.3 Mixture of Binomials", "text": "We include a quick example on the mixture of binomials in 1 dimension to illustrate how Polymom can be applied to a discrete model. In this model, x \u2208 N and 0 \u2264 x \u2264 m and each component is a binomial distribution for m trials each with probabiliy p of success. The probability mass function for the entire mixture model is p(x) = \u2211K k=1 \u03c0k ( m x ) pxk(1 \u2212 pk)m\u2212x. There are only K scalar parameters p1, . . . , pK and the observation function is just the empirical probabilities \u03c6i(x) = 1x=i for x, i \u2208 N, 0 \u2264 x, i \u2264 n, with corresponding polynomials fi(p) = ( m i ) pi(1\u2212 p)m\u2212i, which can be expanded to become linear constraints in Problem 8."}, {"heading": "A.2.4 Multiview Mixtures", "text": "Here we consider the three-view mixture model which has been well studied in [5, section 3.3]. We will show that we can solve the model without explicit whitening, a transformation that has been shown to introduce noise[31]. The model is a mixture of three conditionally independent arbitrary distributions parameterized by their conditional means: we have z \u223c Multinomial\u03c0,xl|z \u223c pl(\u03be(l)z ) where pl(\u03be (l) z ) is such that Exl|z[xl] = \u03be. The parameters are \u03b8k = [\u03be (1), \u03be(2), \u03be(3)]. Using the observation functions \u03c6 = [x(1), x(2), x(3), x(1) \u2297 x(2), . . . , x(1) \u2297 x(2) \u2297 x(3)], we have the following moment polynomials, f = [\u03be(1), \u03be(2), \u03be(3), \u03be(1) \u2297 \u03be(2), . . . , \u03be(1) \u2297 \u03be(2) \u2297 \u03be3].\nThe multiview mixture model is another model for which we can guarantee parameter recovery when K \u2264 D. To prove this is the case, we will again show that Problem 8 can be solved exactly. It suffices to consider just the first P columns of the moment matrix M2, which are almost directly observable. As before, vecs(\u00b7) just flattens a matrix into a vector.\nMT2 =\n  \u03be1 \u03be2 \u03be3 vecs(\u03be1\u2297\u03be2) vecs(\u03be1\u2297\u03be3) vecs(\u03be2\u2297\u03be3) \u03be1 Z2,0,0 Y1,1,0 Y1,0,1 Z2,1,0 Z2,0,1 Y1,1,1 \u03be2 Y1,1,0 Z0,2,0 Y0,1,1 Z1,2,0 Y1,1,1 Z0,2,1 \u03be3 Y1,0,1 Y0,1,1 Z0,0,2 Y1,1,1 Z1,0,2 Z0,1,2   (16)\nwhere Y\u03b11,\u03b13,\u03b13 and Z\u03b11,\u03b13,\u03b13 are both equal to Ly(\u03be \u2297\u03b11 1 \u2297\u03be\u2297\u03b122 \u2297\u03be\u2297\u03b133 ), but are used to respectively denote observed and unknown variables. However, this equation is only partially true as both sides contain the same set of values but the precise arrangements depends on where the minor matrix\nappears in the moment matrix. We ignore this problem as it should be clear from the row and column labels. In the undercomplete case, it is assumed that rank(U) = K \u2264 min(P1, P2, P3), thus we can easily complete this matrix using simple linear algebra in the exact case by repeatedly applying Lemma A.3 below. Generally, we may try to complete the moment matrix by solving Problem 8 from these partial observations, provided that optimizing with the nuclear norm recovers the true rank.\nLemma A.3 (low rank completion of missing corner). For any matrix \u0393 = [ A B C X ] with a missing block X, where rank(\u0393) = rank(A) = rank(B) = K and A \u2208 RK\u00d7K , X = CA\u22121B uniquely completes \u0393. Proof. Because A contains the entire K elements basis, there exists unique Y,Z \u2208 RK\u00d7K so that B = AY and C = ZA. Similarly, X = ZB = CA \u22121 B."}, {"heading": "B Separability", "text": "For conditional models, the coefficients of the moment polynomials can depend on the data but such dependence can sometimes break the process of converting from component moment constraints to mixture moment constraints. In this section, we define separability, which is a sufficient condition on what dependence is allowed under Polymom and then we give some counterexamples.\nConsider a mixture of linear regressions [54, 11], where the parameters \u03b8k = (wk, \u03c3 2 k) are the slope and noise variance for each component k. Then each data point x = [x, y] is drawn from component k by sampling x from an unknown distribution independent of k and setting y = wkx+ , where \u223c N (0, \u03c32k). If we take observation function \u03c6b,c(x) = xbyc, then the corresponding fb,c(\u03b8) depends on the unknown distribution of x: for example, f1,2(\u03b8) = E[x3]w + E[x]\u03c32. In contrast, for the mixture of Gaussians, we had f2(\u03b8) = \u00b5\n2 + \u03c32, which only depends on the parameters. However, not all is lost, since the key thing is that f1,2(\u03b8) depends only on the distribution of x, which is independent of the component k and furthermore can be estimated from data. More generally, we allow fn to depend on x but in a restricted way. We say that fn(\u03b8,x) is separable if E[fn(\u03b8,x)] does not depend on the parameters [\u03b8k]Kk=1 of the mixture generating x. In other words,\nE[\u03c6n(x)] = E[fn(\u03b8,x)] where for all k : E[fn(\u03b8,x) | z = k] = E[fn(\u03b8,x)] \u2208 R[\u03b8]. (17)\nIn this case, we can define fn(\u03b8) := E[fn(\u03b8,x)], and (4) is still valid. For the mixture of linear regressions, we would define fb,c(\u03b8,x) = x\nb E \u223cN (0,\u03c32)[(wx+ )c]. In this more general setup, the approximate moment equations on T data points is 1T \u2211T t=1[fn(\u03b8,xt)] = 1 T \u2211T t=1 \u03c6n(xt).\nAn example of non-separability is a mixture of linear regressions where the variance is not a parameter and is different across mixture components: \u03b8 = (w) and x = (x, y). Recall that E[xy2] = \u2211K k=1 \u03c0k(E[x3]w2k + E[x]\u03c32k), but E[x3]w2k + E[x]\u03c32k cannot be written as E[fn(wk,x)] for any fn, since it depends on \u03c3 2 k. Thus, this example falls outside our framework. In the simplest case, we can make fn(w,x) separable by introducing \u03c3k as a parameter, but this is not always possible if the noise distribution is unknown or if \u03c3k(x) depends on x. For example, if we have heteroskedastic noise, E[xa(y\u2212w \u00b7x)] = 0 are valid moment constraints for individual components, but it is not clear how to convert this to the mixture case."}, {"heading": "C Theory of the moment completion problem", "text": "For solution extraction, we assumed that moments of all monomials are observed but for many models only polynomials of parameters can be estimated from the data. For example, in a Gaussian\nmixture the 2nd moment observable function \u03c6(x) = \u03be2 +c is a polynomial, but solution extraction requires moments of monomials like \u03be2 and c. Furthermore, we assumed in Section 4 that there exists underlying true parameters [\u03b8\u2217k] K k=1 while an arbitrary moment sequence of the parameters y and its corresponding moment matrix M(y) may not correspond to any parameters (i.e. no representing measure). In Section 5, we showed how moment completion can be done with just linear algebra for multiview models, and we now focus on the harder case of having to solve the SDP Problem 8.\nWhile we do not have a complete answer since the rank constrained Problem 7 cannot be solved, we point to the relevant literature and give some sufficient conditions for solution extraction and sufficient conditions for parameter recovery."}, {"heading": "C.1 Conditions for solution extraction", "text": "In Section 4, we showed that simple conditions based only on the column space basis is sufficient for solution extraction to be successful. However, to further investigate consistency and noise, we need to address a few more important issues. First consider the noiseless setting, we may not have enough moment contraints to guarantee a unique solution (identifiability). Even if we assume that we have enough constraints for identifying a K mixture, we still do not know if solving the relaxed Problem 8 that relaxed the rank = K constraint can recover the true parameters. Second, under noise, there may not exist a rank K basis of the moment matrix and even when a rank K basis exists, it may not correspond to any true parameters.\nIn the case when some moment matching parameters can be extracted, the moment matrix satisfies the flat extension condition, which is the same as conditions in Section 4 where \u201cBp := U [ \u03b3p + [\u03b21, . . . ,\u03b2K ] ] is observed\u201d and U[\u03b21; . . . ;\u03b2K ] is a column space basis of Mr(y). Let the highest degree monomial of U[\u03b21; . . . ;\u03b2K ] be of degree r\u2212 1 = deg(\u03b8\u03b2K ) = |\u03b2K |, and the highest degree monomial of Bp := U [ \u03b3p + [\u03b21, . . . ,\u03b2K ] ] be of degree r = \u2223\u2223\u03b3p + \u03b2K \u2223\u2223 = |deg(\u03b2K)| + 1. Since U[\u03b21; . . . ;\u03b2K ] is a basis of col(Mr(y))\nrank (Mr\u22121(y)) = rank (U[\u03b21; . . . ;\u03b2K ]) = K (18)\n= rank (Mr(y)) \u2265 rank ( U [ \u03b3p + [\u03b21, . . . ,\u03b2K ] ]) . (19)\nIf we got this basis from the moment matrix, then we say that the moment matrix Mr\u22121(y) corresponding to U[\u03b21; . . . ;\u03b2K ] has a flat extension, because Mr\u22121(y) can be extended to a moment matrix Mr(y) with higher degree monomials without an increase in rank. The concept of flat extension and its consequences are of central importance for the truncated moment problem, which is quite relevant to our problem and studied by [16, 17, 15, 18]. Next, we reproduce the simplest flat extension theorem:\nTheorem C.1 ([16]: flat extension theorem). Suppose Mr\u22121(y) 0 and there exists Mr(y) so that rank(Mr(y)) = rank(Mr\u22121(y)) (i.e. a flat extension), then there exists an unique rank(Mr(y))atomic representing measure \u00b5 of Mr(y).\nHere the first column of Mr(y) contains every monomial of degree up to r so that deg(vr(\u03b8)) = r. However, several generalizations of the flat extension theorem are also useful for estimation of mixture models where sparse monomials are handled [35, 37] or where constraints are handled [18].\nThe conceptual importance is that Theorem C.1 allows us to work with just the moment matrix satisfying constraints from possibly noisy observations, without assuming the moment matrix is generated by some true parameters. Of course, it also provides a checkable criterion for when solutions can be extracted [40]. We still do not know if solving Problem 8 provides a flat extension in a finite number of steps. [42, 43, 41] investigated this issue very recently and showed that linear optimization over the cone of moments have finite convergence under generic conditions (theorem 4.2 of [41]).\nStill, our issue is not fully resolved as representing measures under linear constraints may not be unique, and as a result even a flat moment matrix may not correspond to the true parameters. For parameter fitting, we\u2019d like to find the solution with minimal rank or otherwise optimal in some way. We explore this issue next but unfortunately we can only give some partial answers.\nProposition C.2 (existence of C). In the noiseless setting, there exist C so that minimizing C \u2022Mr(y)) = c \u00b7 y will give the right solution. Proof. Let Mr(y) = U\u03a3U\nT be the SVD with U \u2208 Rs(r)\u00d7K and \u03a3 \u2208 RK\u00d7K . Let U\u22a5 \u2208 Rs(r)\u00d7(s(r)\u2212K) be the orthogonal compliment of U, then any C = U\u22a5DUT\u22a5 suffices and D \u2208 R(s(r)\u2212K)\u00d7(s(r)\u2212K) is an arbitrary diagonal matrix with positive diagonal elements.\nThe convex iteration algorithm [19] is one way to reduce rank that sometimes works for us empirically, where if the convex iteration algorithm converges to 0, then the moment matrix has rank K."}, {"heading": "D Extensions", "text": ""}, {"heading": "D.1 Constraints on parameters", "text": "Constraints on parameters is a common and important consideration in applications. While constraints can often be addressed in maximum likelihood or maximum a prioterior learning using EM [29, see shared parameters], it is less clear how to address constraints under the tensor decomposition approach because of its reliance on special tensor structure and it is well-known that MME generally can give us parameters outside of the parameter space even in the well-specified case.\nExample D.1. Examples of constraints on parameters Some parameters are known: Gaussian with sparse covariance matrix where we already know that some dimensions are uncorrelated; to solve a substitution cipher using an HMM, the transitions matrix is a language model that is given.\nParameters are tied: transitions in an HMM might only depend on the relatively difference between states if the states are ordered i.e. the transition matrix is Toeplitz.\nPolytope constraints: some of the parameters might be probabilities (e.g. multinomial distribution):\n\u03b8 = [\u03c01, . . . , \u03c0P , \u03be1, . . .], \u03c0p \u2265 0, P\u2211\np=1\n\u03c0p = 1\nSemialgebraic constraints: For some polynomial g \u2208 R[\u03b8], gi(\u03b8\u2217k) \u2265 0, i = 1, . . . , I. This includes discrete sets \u03b8i \u2208 {0, 1} and ellipsoids.\nThe obvious attempt is to project to the feasible set after computing an unconstrained estimation with MME. But this approach has several serious issues. First, some constrained models are only identifiable after the constraints are taken into account, which happens when the model has a lot of parameters and we cannot observe correspondingly more moments. In this case, unconstrained estimation is useful only if we can characterize the entire subset of the parameters space satisfying moment conditions, which is generally not possible in the tensor decomposition approach. Second, we need to determine what projection to use. In the case of two equal parameters, if one estimate is much more noisy than the other, it can be better to just ignore the more noisy estimate than to project under the wrong metric (see Example D.3). Third and strangely, even in the case when the first two issues are handled, it was observed by [12] for probablities parameters, that clipping to 0 is empirically inferior compared to heuristics like taking the absolute value, which is not a projection.\nUnder the Polymom formulation, we can take constraints into account during estimation. The technique of localizing matrix [15] in moment theory allows us to deal with semialgebraic constraints. Of course, the computational complexity increases if the constraints are themselves complicated and high degree. Next, we define the localization matrix, give an example, and then give a constrained version of the flat extension theorem.\nExample D.2 (localizing matrix for an inequality constraint). Let \u03b8 = [c, \u03be], so that \u03b8\u03b1 = c\u03b11\u03be\u03b12 and Ly(\u03b8 \u03b1) = y\u03b1, and chose the monomials v2(\u03b8) = [1, c, \u03be, c\n2, c\u03be, \u03be2]. Suppose that c is the variance and we want to have constraint that c\u2212 1 \u2265 0, then\nM1((c\u2212 1)y) =\n \n1 c \u03be\n1 y1,0 \u2212 1 y2,0 \u2212 y1,0 y1,1 \u2212 y1,0 c y2,0 \u2212 y1,0 y3,0 \u2212 y2,0 y2,1 \u2212 y2,0 \u03be y1,1 \u2212 y0,1 y2,1 \u2212 y1,1 y1,2 \u2212 y0,2\n  (20)\nit is clear that a necessary condition for extracted solutions to satisfy the constraint c \u2212 1 \u2265 0 is that M1((c\u2212 1)y) 0 since fTM1((c\u2212 1)y)f = Ly(f(\u03b8)2(c\u2212 1)) \u2265 0."}, {"heading": "D.2 Noise and statistical efficiency", "text": "In the presense of noise Problem 8 may not be feasible and even if it was, it may not be ideal to exactly match noisy moments. Furthermore, it is argued that higher order moments are too noisy to be useful, but there are also more of them and they do contain more information about the model parameters as long as we can model how noisy they are. We consider the problem with slack and a weighting matrix W 0 \u2208 RN\u00d7N modelling how much noise is present in each constraint function. This effect is fairly well-known, and here is a very simple example which shows that even much more noisy measurements can improve efficiency.\nExample D.3 (efficient estimation). Suppose X \u223c N ([\u03be, \u03be],diag[\u03c32, c\u03c32]) and we would like to estimate the mean parameter \u03be by matching moments. Any estimators of the form \u03be\u0302 = 1T \u2211T t=1(\u03b3xt,1+ (1\u2212 \u03b3)xt,2) are consistent and has risk\nR = E [ (\u03be\u0302 \u2212 \u03be)2 ] = E   ( \u03b3 T\u2211\nt=1\nxt,2 \u2212 \u03b3\u03be + (1\u2212 \u03b3) T\u2211\nt=1\nxt,2 \u2212 (1\u2212 \u03b3)\u03be )2  (21)\n= E  \u03b32 ( \u03be \u2212 1\nT\nT\u2211\nt=1\nxt,1\n)2 + (1\u2212 \u03b3)2 ( \u03be \u2212 1\nT\nT\u2211\nt=1\nxt,2\n)2  (22)\n= 1\nT (\u03b32\u03c32 + (1\u2212 \u03b3)2c\u03c32) (23)\nunder the squared loss, and the efficient estimator would have \u03b3 = c\u22121c and a risk of \u03c32 T c2\u2212c+1 c2 . For c = 10, the risk for efficient estimation is 0.91\u03c3 2\nT whereas for \u03b3 = 0.5, the risk is 2.75 \u03c32 T .\nThis example suggests that a weighting matrix W has the potential to make use of higher order moments and also give better estimates. Consider\nminimize g,y C \u2022M(y) s.t. gn = \u2211 \u03b1 an\u03b1y\u03b1 \u2212 E[\u03c6n(x)\ngTWg \u2264 M(y) 0.\n(24)\nIn the simplest case when W = IN , and = 0, Problem 24 is the same as Problem 8.\nminimize g,y C \u2022M(y) s.t. gn = \u2211 \u03b1 an\u03b1y\u03b1 \u2212 E[\u03c6n(x)]\nW \u2022 F \u2264 M(y) 0[ 1 gT\ng F\n] 0\n(25)\nA good weighting matrix W should put more weights on moment conditions that can be estimated more precisely. The asymptotically efficient weighting matrix suggested by the Generalized Method of Moments [23] is\nW \u22121 = E [ g([\u03b8k] K k=1,x)g([\u03b8k] K k=1,x) T ] \u2248 1 T T\u2211\nt=1\ng([\u03b8k] K k=1,x)g([\u03b8k] K k=1,x) T (26)\nTheorem 2 (Gen.MM is asymptotically efficient [23]). Let gn(\u03b8,X) := \u2211 k fn(\u03b8k) \u2212 hn(X) so that E[hn(X)] = E[\u03c6n(x)]. Let W \u22121 = E[g(\u03b8,X)g(\u03b8,X)T] \u2248 1T \u2211T t=1 g(\u03b8,Xt)g(\u03b8,Xt)\nT Iterative Gen.MM is efficient with this weighting matrix W."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Mixture modeling is a general technique for making any simple model more expressive<lb>through weighted combination. This generality and simplicity in part explains the<lb>success of the Expectation Maximization (EM) algorithm, in which updates are easy<lb>to derive for a wide class of mixture models. However, the likelihood of a mixture<lb>model is non-convex, so EM has no known global convergence guarantees. Recently,<lb>method of moments approaches offer global guarantees for some mixture models, but<lb>they do not extend easily to the range of mixture models that exist. In this work, we<lb>present Polymom, an unifying framework based on method of moments in which es-<lb>timation procedures are easily derivable, just as in EM. Polymom is applicable when<lb>the moments of a single mixture component are polynomials of the parameters. Our<lb>key observation is that the moments of the mixture model are a mixture of these<lb>polynomials, which allows us to cast estimation as a Generalized Moment Problem.<lb>We solve its relaxations using semidefinite optimization, and then extract parame-<lb>ters using ideas from computer algebra. This framework allows us to draw insights<lb>and apply tools from convex optimization, computer algebra and the theory of mo-<lb>ments to study problems in statistical estimation. Simulations show good empirical<lb>performance on several models.", "creator": "LaTeX with hyperref package"}}}