{"id": "1603.08636", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "Towards an Automated Requirements-driven Development of Smart Cyber-Physical Systems", "abstract": "the invariant refinement method for self adaptation ( pcs - sa ) constitutes a scientific method targeting public agency smart audio - physical systems ( scps ). api allows for executing simulated translation merging the system commands into the complementary architecture blocks underlying each ensemble - based prototype compiler ( nas ). however, since the requirements are captured using linear language, there constitutes implicit separation between their misinterpretation due may initial interface requirements'misunderstanding, which too eventually lead to coding errors. thus, thereby enables isolation of the design process is desirable. in their paper, step ( i ) explain the translation process regarding logical library requirements regarding defining ip - sa compiler, ( ii ) identify individual specification both can be automated and / or achieved using biological language processing method, and ( iii ) propose suitable methods.", "histories": [["v1", "Tue, 29 Mar 2016 04:34:39 GMT  (258kb,D)", "http://arxiv.org/abs/1603.08636v1", "In Proceedings FESCA 2016,arXiv:1603.08371"]], "COMMENTS": "In Proceedings FESCA 2016,arXiv:1603.08371", "reviews": [], "SUBJECTS": "cs.SE cs.CL", "authors": ["jiri vinarek", "petr hnetynka"], "accepted": false, "id": "1603.08636"}, "pdf": {"name": "1603.08636.pdf", "metadata": {"source": "CRF", "title": "Towards an Automated Requirements-driven Development of Smart Cyber-Physical Systems", "authors": ["Jiri Vinarek", "Petr Hnetynka"], "emails": ["vinarek@d3s.mff.cuni.cz", "hnetynka@d3s.mff.cuni.cz"], "sections": [{"heading": null, "text": "J. Kofron\u030c, J. Tumova, B. Buhnova (Eds.): Formal Engineering Approaches to Software Components and Architectures (FESCA\u201916) EPTCS 205, 2016, pp. 59\u201368, doi:10.4204/EPTCS.205.5\nc\u00a9 J. Vinarek, P. Hnetynka This work is licensed under the Creative Commons Attribution License.\nTowards an Automated Requirements-driven Development of Smart Cyber-Physical Systems\nPosition paper\nJiri Vinarek Petr Hnetynka Charles University in Prague, Faculty of Mathematics and Physics,\nDepartment of Distributed and Dependable Systems, Malostranske namesti 25, Prague, Czech Republic\nvinarek@d3s.mff.cuni.cz hnetynka@d3s.mff.cuni.cz\nThe Invariant Refinement Method for Self Adaptation (IRM-SA) is a design method targeting development of smart Cyber-Physical Systems (sCPS). It allows for a systematic translation of the system requirements into the system architecture expressed as an ensemble-based component system (EBCS). However, since the requirements are captured using natural language, there exists the danger of their misinterpretation due to natural language requirements\u2019 ambiguity, which could eventually lead to design errors. Thus, automation and validation of the design process is desirable. In this paper, we (i) analyze the translation process of natural language requirements into the IRM-SA model, (ii) identify individual steps that can be automated and/or validated using natural language processing techniques, and (iii) propose suitable methods."}, {"heading": "1 Introduction", "text": "Smart Cyber-Physical Systems (sCPS) are complex distributed decentralized systems of cooperating mobile and stationary devices closely interacting with the physical environment. Examples of sCPS include systems like smart home, smart traffic management, etc.\nDesigning and developing such a system is a quite complex task with many challenges. Mobility and distribution bring the high level of dynamism to the system, which has to be aware of changes in its environment. Openness and open-endness are other challenging issues resulting in needs that the designed system has to be able to tackle unanticipated changes and participants unknown at design time.\nThe traditional software design and development techniques have been shown unsuitable for such systems and novel approaches [8, 13, 14] have been proposed to tackle with the challenges. One of these promising approaches is Ensemble-Based Component Systems (EBCS) [1]. Using EBCS, the system is modeled and developed as a set of ensembles, i.e., dynamic cooperation groups of software components. Components are specified by their knowledge (i.e., component\u2019s attributes) and by a set of processes manipulating the knowledge.\nThe Invariant Refinement Method for Self Adaptation (IRM-SA) [2] is a design method targeting development of sCPS using EBCS. IRM-SA allows for a systematic translation of the system requirements written in natural language into the system architecture expressed as components and ensembles. Using IRM-SA, a designer gradually refines the initial requirements and iteratively builds a model that consists of so-called invariants. Invariants are then hierarchically decomposed and at the lowest level, they directly correspond to an implementation (in the DEECo component model [1], with which IRM-SA is currently tied).\nTo speedup and ease the design process with IRM-SA, the guide 1 and graphical editor 2 have been created. The editor allows for editing of the constraints and performing several basic validations of the designed IRM-SA model. Additionally, skeletons of the implementation can be generated directly from the designed model. Even though the guide and editor exist, the whole process, i.e., translation of requirements into the IRM-SA model, is manual and it can be time-consuming and laborious. Additionally, as the requirements are expressed as a text in natural language, there is a danger of ambiguity and misinterpretation of them, which can result in a suboptimal design. Even more, designers can unintentionally miss important requirements.\nThe goal of this paper is to analyze the IRM-SA design process and identify particular steps, which can be, fully or at least partially, automated with the help of natural language processing tools. To achieve the goal, we use our experience gained with automated processing of textual use-cases, their verification and transformation into an implementation ([15, 16, 18]).\nThe paper is structured as follows: Section 2 explains the IRM-SA method and its inputs and outputs. Section 3 discusses steps of the IRM-SA method from the perspective of their automation and proposes solutions for them. Finally, Section 4 discusses related work while Section 5 concludes the paper."}, {"heading": "2 IRM-SA explained by example", "text": "In this section, we briefly describe the IRM-SA method and its usage on an example (for a detailed description, please see the IRM-SA guide).\nThe experiment described in [6] proved that usage of IRM-SA represents a significant help in EBCS design and development. Participants of this experiment designed using IRM-SA an EBCS architecture with less errors than participants using another design method. Even though, the resulting architectures were not completely without errors, especially thank to different understanding of the input requirements provided as a text in natural language and thus, there is still space for improvements.\nThis is even more important, as one of the outputs of the IRM-SA method \u2013 the IRM-SA model \u2013 can be used not only at design time, but also during development and maintenance of the developed system. In particular, the IRM-SA model allows for traceability between purpose of each invariant (requirement) and its realization and therefore it is ideal for documentation and maintenance. Plus, as stated in [10], it is a mistake to understand requirements specifications as final and unchangeable and thus keeping up-to-date traceability links to requirements is quite important.\nAdditionally, the IRM-SA model is in the DEECo implementation employed for controlling selfadaptation of the system, i.e., the model captures multiple alternatives of the system architecture and the appropriate one is chosen based on actual situation.\nTo sum up, the IRM-SA model is one of the key artifacts of the developed system and its correctness is essential. Therefore, designers/developers would benefit from a tool which not only allows for easy creation of the model (the currently available editor allows for this) but also which would be able to (semi)automatically parse the textual requirements, generate parts of the model from the requirements, and validate individual actions performed by the designer/developer. To provide such a tool, natural language processing methods and tools have to be incorporated in the process.\n1http://svn.pst.ifi.lmu.de/ascens/guide/irm/ 2https://github.com/d3scomp/IRM-SA"}, {"heading": "2.1 IRM-SA method and model", "text": "The IRM-SA design method is an iterative top-down design approach. A designer has to perform the following steps in order to built the IRM-SA model from the requirement specification:\n1. Find the top-level goals of the system and specify the top-level (abstract) invariants.\n2. Find the components of the system (and their fields) by asking \u201cwhich knowledge does each invariant involve and where is this knowledge obtained from?\u201d\n3. Decompose each invariant by asking \u201chow can this invariant be satisfied?\u201d\n4. Separate the concerns of the abstract invariants into sub-invariants that correspond to (abstract) activities that can be done in isolation.\n5. Compose invariants together by asking \u201cwhy do I need to satisfy these invariants?\u201d\n6. In case of situation-specific requirements, try first to accurately capture the condition of being in one situation or another. Use the assumptions to do that. Then use OR decomposition to specify which invariants to satisfy in each situation.\nFigure 1 shows the IRM-SA model created for the electric car (e-car) navigation and parking system example; its requirement specification is in Figure 2. Both the specification and model are overtaken\nfrom the IRM-SA guide (and they were also employed in the experiment mentioned above). The model was created manually with the help of the IRM-SA model editor.\nThe highlighting and underlining in the requirements specification is included solely for the purpose of this paper and would not appear in an actual specification. Meaning of the highlighting/underlining is as follows:\n\u2022 Underlined text refers to components and their attributes (double line for components, single line for attributes). Red color is used for the \u201cE-car\u201d component, blue one for the \u201cParking\u201d component)\n\u2022 Highlighted text is related to a particular invariant; colors correspond with colors in figure 1.\n\u2022 Purple numbers in the requirements specification refer to corresponding invariants in the model."}, {"heading": "3 Automation of IRM-SA", "text": "In this section, we analyze the IRM-SA method from the perspective of its automation. We identify individual steps that can be automated using natural language processing techniques, and propose suitable methods.\nThe individual goals of such an automation are: (i) (semi)automatically generate invariants in the IRM-SA model from the requirements document (and thus make synchronization and traceability between the requirements and IRM-SA model more robust and faster to obtain), (ii) (semi)automatically validate the resulting IRM-SA model."}, {"heading": "3.1 Component identification", "text": "Components in EBCS design represent \u201csmart\u201d entities of the system. In our example, there are two types of components \u2013 E-Car and Parking. Both components and also their attributes are several times mentioned in the requirements as well as in the summary. Based on our experience with derivation of the domain model from textual specification [17], it seems possible to obtain a list of potential components in an automated fashion. Also, a similar approach is employed in [5], where authors retrieve UML class models from test cases.\nBoth names of the components and their attributes are in the requirement texts almost always represented as noun phrases, which in simple sentences appear as subjects or objects (either direct or indirect). To parse a sentence and identify its elements, the Stanford CoreNLP toolkit [11] is an ideal tool. For example, with the usage of the Stanford dependency parser on the sentence \u201dEvery car needs to continuously monitor its energy level (battery).\u201d, we get the dependency graph showed in Figure 3. The parser returns a part-of-speech (POS) tag for each word (e.g., VB for verb in base form, NN for singular noun) and dependency relations between the words (e.g., nsubj for nominal subject or dobj for direct object). The resulting list of potential names of components and attributes contains car (nominal subject) and energy level (direct object). Unfortunately, this might not be sufficient (not all of the subjects/objects are components/attributes). A possible way to overcome this issue is to employ statistical classification techniques to learn the patterns from training data. Similarly, we have employed these techniques in [18].\nEvery/DT\ncar/NN\nneeds/VBZ\nto/TO\nmonitor/VB\ncontinuously/RB level/NN\nenergy/NNits/PRP$\ndet\nnsubj xcomp\nmark advmod dobj\nnmod:poss compound"}, {"heading": "3.2 Component disambiguation", "text": "Another issue with the list of candidates for component/attribute names is that it may be ambiguous. Multiple noun phrases may refer to the same component, e.g., in our e-cars system specification the words \u201ce-car\u201d and \u201ccar\u201d refer to the same component. Similarly, \u201cparking station\u201d and \u201cparking place\u201d or \u201cenergy level\u201d and \u201cbattery\u201d. These ambiguities can be a sign of a poorly written specification and their replacement with the same word or phrase is advisable. On the other hand, such a situation may happen (especially if the specification is prepared by multiple authors and/or it evolves over time) and even more, in some cases, the use of different words for the same entity may be intentional, e.g., in a case of abbreviations and noun phrase shortenings (\u201cplace of interest\u201d and \u201cPOI\u201d or \u201ctrip plan\u201d and \u201cplan\u201d, etc.).\nA distinction of these cases is not always clear and we expect that user will have to be involved in a decision about these ambiguities.\nFor example, the requirement in Figure 3 is written in a way which suggest that the phrase \u201cenergy level\u201d and \u201cbattery\u201d can be used interchangeably. This relation can be deduced automatically, as the\nword \u201cbattery\u201d has been marked as appositional modifier (appos) of the word \u201clevel\u201d. Another option for disambiguation might be employment of string distance metrics [4]3 to identify corresponding entities (e.g., \u201ccar\u201d and \u201ce-car\u201d)."}, {"heading": "3.3 Invariant type identification", "text": "During applying the IRM-SA method, sentences in the Requirements section of the specification are rather directly translated into invariants of the IRM-SA model. However, the issue is to identify whether the particular sentence relates to the abstract, process, exchange or assumption invariant. It would be helpful, if the IRM-SA editor could automatically propose the invariant type.\nAssumption invariants should be included only in the situation-specific section of the requirements specification and thus is easier to locate them (see the yellow highlighting in Figure 2 and the yellow invariants in Figure 1). Additionally, the particular sentences express a condition, which is necessary to detect and extract. To extract it, the dependency parser can be again utilized. To support it, tools for information extraction like Ollie4 or OpenIE5 can also be used as they are able to detect enabling conditions.\nTo distinguish between process and exchange invariants, it is necessary to analyze the main verb of the sentence (see the blue and green highlighting in Figure 2 and the blue and green invariants in Figure 1). With verbs such as \u201cexchange\u201d or \u201cpropagate\u201d, there is a high chance that the sentence corresponds to exchange invariant, while verbs \u201chave\u201d, \u201cmonitor\u201d, \u201cassess\u201d, \u201cobtain\u201d, \u201cacquire\u201d or \u201cdetermine\u201d usually denote a process invariant. A direct solution would be a simple comparison of the particular verb with a predefined set of verbs but it would be rather limiting. Instead, a suitable approach is to classify verbs according to their meaning, which is taken from WordNet[12]. WordNet is a large lexical database of English nouns, verbs, adjectives and adverbs. In the database, synonyms are grouped together forming so-called synsets. The synsets are interlinked according to their relations, forming network of related words and concepts. Multiple WordNet similarity measures were proposed and their implementation is available6 and can be used for the process and exchange invariants identification.\nFinally, sentences containing additional sub-requirements can be marked as abstract invariants (the gray highlighting and the gray invariants in Figures 1 and 2)."}, {"heading": "3.4 Knowledge flow recognition", "text": "One of the key IRM-SA ideas is that each invariant (with the exception of assumption ones) represents a computation that produces output knowledge given a particular input knowledge such that the invariant is satisfied (as stated in [2]).\nFor example, in the invariant deduced from the requirement 1(d) in Figure 2, the energy level and POI of the vehicle and the available parking slots from the parking places serve as the input parameters for computation of the vehicle\u2019s plan (all possible parameters, i.e., component attributes, are already known as they were identified in the previous phases). Schematically, it can be written\nV::energy, V::POI, P::availability -> V::plan.\n3an implementation available at http://secondstring.sourceforge.net/ 4https://github.com/knowitall/ollie 5https://github.com/knowitall/openie 6http://search.cpan.org/dist/WordNet-Similarity/\nSuch an abstraction of the requirements to input and output parameters allows for easier reasoning about the invariants and it is employed in subsequent sections.\nHowever, an issue is how to identify which parameters are input and which output. A straightforward automatic approach for distinguishing input parameters from the output ones is an iteration starting from the simple invariants and taking into account types of parameters already distinguished from the previous iterations. The approach starts with the process invariants having only single parameter. Such a parameter must be an output one (otherwise the invariant would not produce any knowledge and the above mentioned IRM-SA idea would not hold). Examples of such invariants are requirements 1(a) and 1(b). Next, if a single attribute is present in multiple invariants, it can be assumed that it serves as an input parameter. Nevertheless, this is only an assumption and thus a fully automated approach is hard to achieve. A possible solution is to use an assisted iterative approach, in which a tool identifies input and output parameters and the human designer confirms/reverts the decisions.\nIn some cases, computation associated with an abstract invariant may not have all the parameters precisely specified, as the particular parameters are unknown yet. They may be specified in the child invariants and from the view of the higher-level invariant, they can be seen as an implementation detail. In such cases, we use the notation V::? to mark that the component V participates in the invariant, but the specific attribute is specified later in a child invariant. The final assignment of the parameter is up to the designer."}, {"heading": "3.5 Invariant refinement and composition", "text": "In EBCS, communication between components is implicit via their knowledge sharing, which is conveyed via ensembles. An ensemble is thus specified via a condition determining when components are part of the particular ensemble and via knowledge that has to be interchanged.\nLet us again assume the requirement 1(d) with the parameter abstraction\nV::energy, V::POI, P::availability -> V::plan.\nAs the parameters come from different components (V and P) but the computation can be performed only in a single component, it is clear that the invariant has to be refined as a composition of several invariants, from which at least one is an exchange invariant (in the implementation, the exchange invariants results in the ensemble definition). Such situations can be rather easily detected automatically based on the parameters\u2019 owners.\nNevertheless, refinement and composition of the abstract invariants is more difficult as they represent high-level goals and can intentionally abstract from some \u201cimplementation details\u201d, i.e., omit some attributes. For example, in Figure 1, composition of the invariants 5 and 6 means that the trip plan is computed first without any knowledge about availability of the parking places (the output parameter V::planFeasibility in the invariant 5) and then it is made more specific with information about the availability (the invariant 6). Different composition of the lower-level invariants would lead to a completely different behavior.\nAnother issue in the automatic composition of invariants is a reasoning about situation-specific requirements, which have to be grouped together according to a requirement they belong to. The grouping is performed based on their output parameters and can result in duplication of invariant subtrees. However, identification of the right subtree to be duplicated is not straightforward. Both these issues are very hard to solve automatically and we plan to further investigate possibilities of their automation in more detail."}, {"heading": "3.6 Model validation", "text": "With the abstraction of invariants described in 3.4, the IRM-SA model can be automatically validated according to the knowledge flow. In particular, following checks can be performed:\n\u2022 Configurations with missing input parameters can be discovered (i.e., an invariant producing the particular attribute is not included in the configuration due to a missing dependency relation).\n\u2022 Configurations with multiple invariants writing to the same attribute can be detected.\n\u2022 Also, detection of unused output parameters or unused attributes can be performed.\nAll of these checks may point to flaws in the model and/or specification and discover them early."}, {"heading": "4 Related work", "text": "As far as we know, there are no attempts to automatize requirements processing for EBCS design. Nevertheless, a related approach is described in [3], in which authors propose an approach called NPL-KAOS that can automatically obtain a KAOS model from large volume of literature (KAOS [9] is a goal-oriented requirement engineering method and it was one of inspirations for the IRM-SA method). With the use of natural language processing tools and text mining techniques they process abstracts of scientific publications. First, they detect goal-oriented keywords and then they use the Stanford parser to tag semantic structures. From obtained semantic trees they extract goals and finally organize them into taxonomies. The taxonomies are used to define relations between goals and this way they simulate the process of refinement. Similarly to our approach, the authors try to automatically derive a model from textual data which would serve for purposes of requirements engineering. However, their problem is different. Their main goal is to help requirements engineer during the early stages of goal elicitation by extraction of main concepts from the large body of research abstracts. Although the extraction process may miss some goals in the single abstract, with large number of abstracts they can count on the fact that the goal will be at the end noticed. Contrary, our method is intended to process a single specification and therefore cannot reckon on this effect.\nIn [5], the authors (semi-)automatically derive a UML model and OCL constraints from a specification and test cases, which are both written in natural language. They employ formal methods to verify correctness of the derived design. First, grammatical analysis is used to derive UML class diagrams from the test cases. Then, the behavior of test cases is inspected and the UML sequence diagrams are derived. In the next step, OCL constraints are deduced from the requirements and test cases. Finally, verification of static aspects (UML class diagrams and OCL invariants) and dynamic aspects (satisfaction of specified method pre-conditions and post-conditions) is checked. As in our approach, authors use the Stanford parser to get dependencies from the sentences. They also employ WordNet (in their case, to distinguish components of the system and actors). The main difference is that we directly target sCPS design and EBCS and therefore include an identification of process and exchange invariants, adaptability, etc."}, {"heading": "5 Conclusion", "text": "In the paper, we have analyzed the IRM-SA design method with respect to its possible automation with the help of natural language processing methods and tools. We have identified steps that can be automated and sketched solutions. As the automated natural language understanding is generally still a challenging\nissue, the full automation is hard (and in many cases impossible) to achieve. Thus, we target a semiautomated system that guides the human designer, recommends solutions and validates the designers actions.\nCurrently, we plan to implement all the identified proposed solutions, to integrate them to the existing IRM-SA editor and to validate the resulting system on a real-life case-study.\nEven though the proposed approaches are tailored to IRM-SA (which is currently tied with the DEECo component model), they can be reused in different contexts. The IRM-SA method itself can be without changes applied to another ensemble-based component model (e.g., Helena [7]) and the approaches proposed in this paper can be applied in tools for the KAOS method or similar ones."}, {"heading": "6 Acknowledgement", "text": "This work was partially supported by the project no. LD15051 from COST CZ (LD) programme by the Ministry of Education, Youth and Sports of the Czech Republic and partially supported by Charles University institutional funding SVV-2016-260331."}], "references": [{"title": "DEECO: An Ensemble-based Component System", "author": ["Tomas Bures", "Ilias Gerostathopoulos", "Petr Hnetynka", "Jaroslav Keznikl", "Michal Kit", "Frantisek Plasil"], "venue": "Proceedings of CBSE", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "The Invariant Refinement Method", "author": ["Tomas Bures", "Ilias Gerostathopoulos", "Petr Hnetynka", "Jaroslav Keznikl", "Michal Kit", "Frantisek Plasil"], "venue": "In: Software Engineering for Collective Autonomic Systems, LNCS 8998,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "NLP-KAOS for Systems Goal Elicitation: Smart Metering System Case Study", "author": ["E. Casagrande", "S. Woldeamlak", "W.L. Woon", "H.H. Zeineldin", "D. Svetinovic"], "venue": "IEEE Transactions on Software Engineering", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A comparison of string distance metrics for name-matching tasks", "author": ["William W. Cohen", "Pradeep Ravikumar", "Stephen E. Fienberg"], "venue": "Proceedings of IIWeb-03,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Formal Specification Level: Towards verificationdriven design based on natural language processing", "author": ["Rolf Drechsler", "Mathias Soeken", "Robert Wille"], "venue": "Proceedings of FDL", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Foundations for Ensemble Modeling \u2014 The Helena Approach. In: Specification, Algebra, and Software, LNCS 8373", "author": ["Rolf Hennicker", "Annabelle Klarl"], "venue": "Springer, pp. 359\u2013381,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Software Engineering for Ensembles", "author": ["Matthias H\u00f6lzl", "Axel Rauschmayer", "Martin Wirsing"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Requirements Engineering: From Craft to Discipline", "author": ["Axel van Lamsweerde"], "venue": "Proceedings of SIGSOFT\u201908/FSE-16,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Applying UML and patterns: an introduction to object-oriented analysis and design and the unified proces, 3rd edition", "author": ["Craig Larman"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "The Stanford CoreNLP Natural Language Processing Toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "WordNet: A Lexical Database for English", "author": ["George A. Miller"], "venue": "Communications of the ACM 38(11),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "Taming Heterogeneity and Distribution in sCPS", "author": ["Brice Morin", "Franck Fleurey", "Olivier Barais"], "venue": "Proceedings of SEsCPS", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Architectural Abstractions for Hybrid Programs", "author": ["Ivan Ruchkin", "Bradley Schmerl", "David Garlan"], "venue": "Proceedings of CBSE", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Formal Verification of Annotated Textual Use-Cases", "author": ["Viliam Simko", "David Hauzar", "Petr Hnetynka", "Tomas Bures", "Frantisek Plasil"], "venue": "The Computer Journal 58(7),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "From Textual Use-Cases to Component-Based Applications", "author": ["Viliam Simko", "Petr Hnetynka", "Tomas Bures"], "venue": "SNPD", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Implemented Domain Model Generation", "author": ["Viliam Simko", "Petr Kroha", "Petr Hnetynka"], "venue": "Technical Report D3S-TR-2013-03, Charles University in Prague, Faculty of Mathematics and Physics, Department of Distributed and Dependable Systems", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Recovering Traceability Links Between Code and Specification Through Domain Model Extraction", "author": ["Jiri Vinarek", "Petr Hnetynka", "Viliam Simko", "Petr Kroha"], "venue": "Proceedings of EOMAS", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "The traditional software design and development techniques have been shown unsuitable for such systems and novel approaches [8, 13, 14] have been proposed to tackle with the challenges.", "startOffset": 124, "endOffset": 135}, {"referenceID": 11, "context": "The traditional software design and development techniques have been shown unsuitable for such systems and novel approaches [8, 13, 14] have been proposed to tackle with the challenges.", "startOffset": 124, "endOffset": 135}, {"referenceID": 12, "context": "The traditional software design and development techniques have been shown unsuitable for such systems and novel approaches [8, 13, 14] have been proposed to tackle with the challenges.", "startOffset": 124, "endOffset": 135}, {"referenceID": 0, "context": "One of these promising approaches is Ensemble-Based Component Systems (EBCS) [1].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "The Invariant Refinement Method for Self Adaptation (IRM-SA) [2] is a design method targeting development of sCPS using EBCS.", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "Invariants are then hierarchically decomposed and at the lowest level, they directly correspond to an implementation (in the DEECo component model [1], with which IRM-SA is currently tied).", "startOffset": 147, "endOffset": 150}, {"referenceID": 13, "context": "To achieve the goal, we use our experience gained with automated processing of textual use-cases, their verification and transformation into an implementation ([15, 16, 18]).", "startOffset": 160, "endOffset": 172}, {"referenceID": 14, "context": "To achieve the goal, we use our experience gained with automated processing of textual use-cases, their verification and transformation into an implementation ([15, 16, 18]).", "startOffset": 160, "endOffset": 172}, {"referenceID": 16, "context": "To achieve the goal, we use our experience gained with automated processing of textual use-cases, their verification and transformation into an implementation ([15, 16, 18]).", "startOffset": 160, "endOffset": 172}, {"referenceID": 8, "context": "Plus, as stated in [10], it is a mistake to understand requirements specifications as final and unchangeable and thus keeping up-to-date traceability links to requirements is quite important.", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "[2] In order to do that, every car needs to :", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "(a) Continuously monitor its energy level (battery) ;[9]", "startOffset": 53, "endOffset": 56}, {"referenceID": 5, "context": "(b) Continuously monitor its position) ;[7]", "startOffset": 40, "endOffset": 43}, {"referenceID": 6, "context": "(c) Continuously assess whether its energy level would be enough to complete the trip based on the distance left to cover; [8] (d) Have a plan to follow, which is based on its energy level and on the available parking slots in the parking places near the POI .", "startOffset": 123, "endOffset": 126}, {"referenceID": 9, "context": "[11, 14]", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "[11, 14]", "startOffset": 0, "endOffset": 8}, {"referenceID": 8, "context": "[10]", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12, 15]", "startOffset": 0, "endOffset": 8}, {"referenceID": 13, "context": "[12, 15]", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "When an e-car is more than 5km far from the POI [16], it should update its plan at least once per 60 seconds .", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "[14]", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "When an e-car is equal to or less than 5km far from the POI [13], it should update its plan at least every 10 seconds [11].", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "When an e-car is equal to or less than 5km far from the POI [13], it should update its plan at least every 10 seconds [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 15, "context": "Based on our experience with derivation of the domain model from textual specification [17], it seems possible to obtain a list of potential components in an automated fashion.", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "Also, a similar approach is employed in [5], where authors retrieve UML class models from test cases.", "startOffset": 40, "endOffset": 43}, {"referenceID": 9, "context": "To parse a sentence and identify its elements, the Stanford CoreNLP toolkit [11] is an ideal tool.", "startOffset": 76, "endOffset": 80}, {"referenceID": 16, "context": "Similarly, we have employed these techniques in [18].", "startOffset": 48, "endOffset": 52}, {"referenceID": 3, "context": "Another option for disambiguation might be employment of string distance metrics [4]3 to identify corresponding entities (e.", "startOffset": 81, "endOffset": 84}, {"referenceID": 10, "context": "Instead, a suitable approach is to classify verbs according to their meaning, which is taken from WordNet[12].", "startOffset": 105, "endOffset": 109}, {"referenceID": 1, "context": "One of the key IRM-SA ideas is that each invariant (with the exception of assumption ones) represents a computation that produces output knowledge given a particular input knowledge such that the invariant is satisfied (as stated in [2]).", "startOffset": 233, "endOffset": 236}, {"referenceID": 2, "context": "Nevertheless, a related approach is described in [3], in which authors propose an approach called NPL-KAOS that can automatically obtain a KAOS model from large volume of literature (KAOS [9] is a goal-oriented requirement engineering method and it was one of inspirations for the IRM-SA method).", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "Nevertheless, a related approach is described in [3], in which authors propose an approach called NPL-KAOS that can automatically obtain a KAOS model from large volume of literature (KAOS [9] is a goal-oriented requirement engineering method and it was one of inspirations for the IRM-SA method).", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "In [5], the authors (semi-)automatically derive a UML model and OCL constraints from a specification and test cases, which are both written in natural language.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": ", Helena [7]) and the approaches proposed in this paper can be applied in tools for the KAOS method or similar ones.", "startOffset": 9, "endOffset": 12}], "year": 2016, "abstractText": "The Invariant Refinement Method for Self Adaptation (IRM-SA) is a design method targeting development of smart Cyber-Physical Systems (sCPS). It allows for a systematic translation of the system requirements into the system architecture expressed as an ensemble-based component system (EBCS). However, since the requirements are captured using natural language, there exists the danger of their misinterpretation due to natural language requirements\u2019 ambiguity, which could eventually lead to design errors. Thus, automation and validation of the design process is desirable. In this paper, we (i) analyze the translation process of natural language requirements into the IRM-SA model, (ii) identify individual steps that can be automated and/or validated using natural language processing techniques, and (iii) propose suitable methods.", "creator": "LaTeX with hyperref package"}}}