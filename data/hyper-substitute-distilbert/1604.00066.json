{"id": "1604.00066", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "To Fall Or Not To Fall: A Visual Approach to Physical Stability Prediction", "abstract": "understanding physical function is a key competence that pushes humans ; animals to act and maneuver atop uncertain perception exploring previously unseen environments containing novel object and human configurations. mathematical psychology has shown that management skills he acquired beneath infants from observations attained near very early standard.", "histories": [["v1", "Thu, 31 Mar 2016 21:53:32 GMT  (1496kb,D)", "http://arxiv.org/abs/1604.00066v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.RO", "authors": ["wenbin li", "seyedmajid azimi", "ale\\v{s} leonardis", "mario fritz"], "accepted": false, "id": "1604.00066"}, "pdf": {"name": "1604.00066.pdf", "metadata": {"source": "CRF", "title": "To Fall Or Not To Fall: A Visual Approach to Physical Stability Prediction", "authors": ["Wenbin Li", "Seyedmajid Azimi", "Ale\u0161 Leonardis", "Mario Fritz"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Intuitive Physics, Physics-based simulation, Stability, Visual Learning and Inference"}, {"heading": "1 Introduction", "text": "Scene understanding requires \u2013 among others \u2013 understanding of relations between and among the objects. Many of these relations are governed by the Newtonian laws and thereby rule out unlikely or even implausible configurations for the observer. They are part of \u201cdark matter\u201d [30] in our everyday visual data which helps us interpret the configurations of objects correctly and accurately. Although objects simply obey these elementary laws of Newtonian mechanics, which can very well be captured in simulators, uncertainty in perception makes exploiting these relations challenging in artificial systems.\nIn contrast, humans understand such physical relations naturally, which e.g. enables them to manipulate and interact with novel objects in unseen conditions with ease. We build on a rich set of prior experiences that allow us to employ a type of commonsense understanding that does not \u2013 most likely \u2013 involve symbolic representation of 3D geometry that is processed by a physics simulation\nar X\niv :1\n60 4.\n00 06\n6v 1\n[ cs\n.C V\nengine. We rather seem to build on what has been coined as \u201cna\u0308\u0131ve physics\u201d [28] or \u201cintuitive physics\u201d [20], which is a good enough proxy to make us operate successfully in the real-world.\nIt has not been shown yet how to equip machines with a similar set of physics commonsense \u2013 and thereby bypassing a model\u2013based representation and a physical simulation. In fact, it has been argued that such an approach is unlikely due to e.g. the complexity of the problem [5]. Only recently, several approach have revived this idea and reattempted a fully data drive approach to capturing the essence of physical events via machine learning methods [21,29,10].\nIn contrast, studies in developmental psychology [1] have shown that infants acquire the knowledge of physical events by observation at a very early age, including support, collision and unveiling. According to their research, the infant with some innate basic core knowledge [4] gradually builds its internal model of the physical event by observing its various outcomes. Amazingly, such basic knowledge of physical event, for example the understanding of support phenomenon can make its way into relatively complex operations as shown in Figure 1. Such structures are generated by stacking up an element or removing one while retaining the structure\u2019s stability primarily relying on effective knowledge of support events in such toy constructions. In our work, we focus on exactly this support event and construct a model for machines to predict object stability.\nHence, we revisit the classic setup of Tenenbaum and colleagues [5] and explore to which extend machines can predict physical stability events directly from appearance cues. We approach this problem by synthetically generating a large set of wood block towers with a range of conditions, including varying number of blocks, varying block sizes, more planar vs. multi-layered configurations. We run those configurations through a simulator (only at training time! ) in order to generate labels if the tower would fall. We show for the first time that aforementioned stability test can be learned and predicted in a purely data driven way \u2013 bypassing traditional model-based simulation approaches. In order to shed\nmore light on the capabilities and limitations of our model, we accompany our experimental study with human judgments on the same stimuli."}, {"heading": "2 Related Work", "text": "As human, we possesses the ability to judge from vision alone if an object is physically stable or not and predict the objects\u2019 physical behaviors. Yet it is unclear: (1) how do we make such decision and (2) how do we acquire this capability. Research in development psychology [2,1,3] suggests that infants acquire the knowledge of physical events at very young age by observing those events, including support events and others. This partly answers to the question (2), however there seems no consensus on how the internal mechanism for interpreting external physical events to address question (1). [5] proposed an intuitive physics simulation engine for such mechanism and found it resemble to human subjects\u2019 behaviors pattern in several psychological tasks. Historically, intuitive physics is connected to the case where people often hold erroneous physical intuitions [20], such as people tend to expect an object dropped from a moving subject will fall vertically straight down. It is rather counter-intuitive how the proposed simulation engine in [5] can explain such erroneous intuitions.\nWhile it is probably illusive to fully reveal human\u2019s inner mechanism for physical modeling and inference, it is feasible to build up models based on observation, in particular the visual information. In fact, looking back to history, physical laws are discovered through the observation of physical events [19]. Our work is in this direction. By observing a large number of support event instances in simulation, we want to gain deeper insight into the prediction paradigm.\nIn our work, we use a game engine to render scene images and a built-in physics simulator to simulate the scenes\u2019 stability behavior. The data generation procedure is based on the platform used in [5], however as discussed before, their work hypothesized a simulation engine as an internal mechanism for human to understand the physics in the external world while we are interested in finding an image-based model to directly predict the physical behavior from visual channel. Learning from synthetic data has a long tradition in computer vision and recently has gained increasing interest [18,25,22,24] due to data hungry deep learning approach.\nUnderstanding physical events also plays an important role in scene understanding in computer vision. By including the additional clue from physical constraints into the inference mechanism, mostly from the support event, it has further improved results in segmentation of surfaces [12], scenes [26] from image data, and object segmentation in 3D point cloud data [31].\nOnly very recently, learning physical concepts from data has been attempted. [21] aims at understanding dynamic events governed by laws of Newtonian physics, but uses proto-typical motion scenarios as exemplars. [10] analyze a billiard table scenarios and aim at learning the dynamics from observation. While learning is largely data-driven, the object notion is predefined as location of the\nballs are provided to the system. [29] aims to understand physical properties of objects. They again rely on a explicit physical simulation.\nIn contrast, we only use simulation at training time and predict for the first time visual stability directly from visual inputs of towers with a large number of degrees of freedom.\nA recent paper [17] that appeared a few days before this work is a related research thread that was conducted in parallel without our knowledge. The focus of their work is different from ours, namely predicting outcome and falling trajectories for simple 4 block scenes. In our work, we significantly vary the scene parameters, investigate if and how the prediction performance from image trained model changes according to such changes, and further we examine how the human\u2019s prediction adapt to the variation in the generated scenes and compare it to our model."}, {"heading": "3 Towards Modeling a Visual Stability Test", "text": "In order to tackle a visual stability test, we require a data generation process that allows us to control various degrees of freedom induced by the problem as well as generation of large quantities of data in a repeatable setup. Therefore, we follow the pioneer work on this topic [5] and use a simulator to setup and predict physical outcomes of wood block towers. Afterwards, we describe the method that we investigate for visual stability prediction. We employ state of the art deep learning techniques, which are the de facto standard in today\u2019s recognition systems. Lastly, we describe the setup of the human study that we conduct to complement the machine predictions with a human reference."}, {"heading": "3.1 Synthetic Data", "text": "Based on the scene simulation framework used in [13,5], we generate synthetic data in our experiment with rectangular cuboid blocks as basic elements. Number of blocks, block size, stacking depth are varied in different scenes, to which we will refer as scene parameters.\nBlock Numbers We expect that varying size of towers and involved blocks will influence the difficulty and challenge the competence of eye-balling the stability of a tower in humans and machine. While evidently the appearance becomes more complex, with increasing number of blocks, the number of contact surfaces and interactions equally make the problem richer. Therefore, we include scenes with four different numbers of blocks, 4 blocks, 6 blocks, 10 blocks and 14 blocks as {4B, 6B, 10B, 14B}.\nStacking Depth As we focus our investigations on judging stability from monocular input, we vary the depth of the tower from a one layer setting which we call 2D to a multi-layer setting which we call 3D. The first one only allows a single block along the image plane at all height levels while the other does not enforce\nsuch constraint and can expand in the image plane. Visually, the former results in a single-layer stacking similar to Tetris while the latter ends in a multiple-layer structure as shown in Figure 2. The latter most likely requires the observer to pick up on more subtle visual cues, as its layers are heavily occluded.\nBlock Size We include two groups of block size settings. In the first one, the towers are constructed of blocks that have all the same size of 0.2m\u00d70.2m\u00d70.6m as in the [5]. The second one introduces varying block sizes where two of the three dimensions are randomly scaled with respect to a truncated Normal distribution N(1, \u03c32) around [1 \u2212 \u03b4, 1 + \u03b4], \u03c3 and \u03b4 are small values. These two settings are referred to as {Uni,NonUni}. The setting with non uniform blocks introduces small visual cues where stability hinges on small gaps between differently sized blocks that are challenging even for human observers.\nScenes Combining these three scene parameters, we define 16 different scene groups. For example, group 10B-2D-Uni is for scenes stacked with 10 Blocks of same size, stacked within a single layer. For each group, 1000 candidate scenes are generated where each scene is constructed with non-overlapping geometrical constraint in a bottom-up manner. There are 16K scenes in total. For prediction experiments, half of the images in each group are for training and the other half for test, the split is fixed across the experiments.\nRendering While we keep the rendering basic, we like to point out that we deliberately decided against colored bricks as in [5] in order to challenge perception and make identifying brick outlines and configurations more challenging. The lighting is fixed across scenes and the camera is automatically adjusted so that the whole tower is centered in the captured image. Images are rendered at resolution of 800\u00d7 800 in color.\nPhysics Engine We use Bullet [8] physics engine in Panda3D [11] to perform physics-based simulation for 2000ms at 1000Hz for each scene. Surface friction and gravity are enabled in the simulation. The system records the configuration of a scene of N blocks at time t as (p1, p2, ..., pN )t, where pi is the location for block i. The stability is then automatically decided as a Boolean variable:\nS = N\u2227 i=1 (\u2206((pi)t=T \u2212 (pi)t=0) > \u03c4)\nwhere T is the end time of simulation, \u03b4 measures the displacement for the blocks between the starting point and end time, \u03c4 is the displacement threshold, \u2227 denotes the logical and operator, that is to say it counts as unstable S = True if any block in the scene moved in simulation, otherwise as stable S = False."}, {"heading": "3.2 Stability Prediction from Still Images", "text": "Inspiration from Human Research in [13,5] suggests the combinations of the most salient features in the scenes are insufficient to capture people\u2019s judgments, however, contemporary study reveals human\u2019s perception of visual information, in particular some geometric feature, like critical angle [6,7] plays an important role in the process. Regardless of the actual inner mechanism for human to parse the visual input, it is clear there is a mapping f involving visual input I to the stability prediction P .\nf : I, \u2217 \u2192 P\nThe mapping can be inclusive, as in [13] using it along with other aspects, like physical constraint to make judgment or exclusive, as in [6] using visual cues alone to decide.\nImage Classifier for Stability Prediction In our work, we are interested in the mapping f exclusive to visual input and directly predicts the physical stability. We use deep convolutional neural networks to learn the mapping as it has shown great success on image classification task [15]. Such networks have been shown to be able to adapt to a wide range of classification and prediction task [23] through re-training or adaptation by fine-tuning. Therefore, these approaches seem adequate method to study visual prediction on this challenging task with the motivation that by changing conventional image classes labels to stability labels the network can learn \u201cphysical stability salient\u201d features.\nIn a pilot study, we tested on a subset of the generated data with LeNet [16], a relatively small network designed for digit recognition, AlexNet [15], a large network and VGG Net[27], a even larger network than AlexNet. We trained from scratch for the LeNet and fine-tuned for the large network pre-trained on ImageNet [9]. VGG Net consistently outperforms the other two, hence we use it across our experiment. We use the Caffe framework [14] in all our experiments."}, {"heading": "3.3 Human Subject Study", "text": "We recruit human subjects to predict stability for give scene images. Due to large number of test data, we sample images from different scene groups for human subject test. 8 subjects are recruited for the test. Each subject is presented with a set of captured images from the test split. Each set includes 96 images where images cover all 16 scene groups with 6 scene instances per group. For each scene image, subject is required to rate the stability on a scale from 1\u2212 5 without any constraint for response time:\n1. Definitely unstable: definitely at least one block will move/fall 2. Probably unstable: probably at least one block will move/fall 3. Cannot tell: the subject is not sure about the stability 4. Probably stable: probably no block will move/fall 5. Definitely stable: definitely no block will move/fall\nThe main questions that we aim at answering with our study are as follows:\n1. How well do humans perform on the task? This provides a basic reference of the task difficulty. High human performance may indicate the task is too simple while low performance for too difficulty for the setup. In addition, human performance is also a baseline to our image-based model. 2. How does human performance vary with respect to scene variations? This provides an additional reference to the task difficulty concerning the scene parameter. If certain scene parameter does not influence too much on the human performance, it can be less relevant to human perception for the task. If the human performance changes along with the variation of the scene parameter, then we can further distinguish between more dominated factors and less dominated ones. Moreover, it serves as a parallel baseline to compare with the image-based model to see if both are affected by the presented challenges equally. 3. How does human performance compare to image-based model? This answers if the data-driven image-based approach can match or even possible to outperform human. 4. How do human vs. machine confidences relate? The setup of our human study provides as with confidence values 1-5, that reflect the certainty in the human judgment. We would like to investigate how these confidences are mimicked by our data-drive approach. 5. How do failure cases differ? Are they plausible? In order to gain more insights in our model and analyze to what extend a scene understanding of real physics or a certain human notion of commonsense physics was achieved, we can compare failure cases and modes between human and machine prediction.\nSince we want to compare the performance between the image-based model and human, we use the same data for both in test. More details will be discussed in the following section."}, {"heading": "4 Experiment", "text": "Our experimental analysis is composed of two main part. The first will study if and to what extend a largely model free visual stability test can be learned directly from data only. The second part, will put these finding in relation to human judgments on the same stimuli."}, {"heading": "4.1 Visual Stability Prediction", "text": "In this part of experiments, image are captured before the physics engine is enabled, and the stability labels are recorded from the simulation engine as described before. At training time, the model has access to image and the stability labels. At test time, the learned model predicts stability results against the results generated by the simulator.\nWe divide the experiment design into 3 sets: the intra-group, cross-group and generalization. The first set investigates influence on the model\u2019s performance from individual scene parameter, the other two sets explore generalization properties under different settings.\nIntra-Group Experiment In this set of experiments, we train and test on the scenes with the same scene parameters in order to assess the feasibility of our task.\nNumber of Blocks In this group of experiment, we fix the stacking depth and keep the all blocks in the same size but vary the number of blocks in the scene to observe how it affects the prediction rates from the image trained model, which approximates the relative recognition difficulty from this scene parameter alone. The results have been shown in Table 1. Consistent drop of performance can be observed with increasing number of blocks in the scene under various block sizes and stacking depth conditions. More blocks in the scene generally leads to higher scene structure and hence higher difficulty in perception.\nBlock Size In this group of experiment, we aim to explore how same size and varied blocks sizes affect the prediction rates from the image trained model. We compare the results at different number of blocks to the previous group, in the most obvious case, scenes happened to have similar stacking patterns and same number of blocks can result in changes visual appearance. To further eliminate the influence from the stacking depth, we fix all the scenes in this group to be 2D stacking only. As can be seen from Table 1, the performance decreases when moving from 2D stacking to 3D. The additional variety introduced by the block size indeed makes the task more challenging.\nStacking Depth In this group of experiment, we want to investigate how stacking depth affects the prediction rates. With increasing stacking depth, it naturally introduces ambiguity in the perception of the scene structure, namely some parts of the scene can be occluded or partially occluded by other parts. Similar to the experiments in previous groups, we want to minimize the influences from other scene parameters, we fix the block size to be the same and only observe the performance across different number of blocks. The results in Table 1 show a little inconsistent behaviors between relative simple scenes (4 blocks and 6 blocks) and difficult scenes (10 blocks and 14 blocks). For simple scenes, prediction accuracy increases when moving from 2D stacking to 3D while it is the other way around for the complex scene. Naturally relaxing the constraint in stacking depth can introduce additional challenge for perception of depth information, yet given a fixed number of blocks in the scene, the condition change is also more likely to make the scene structure lower which reduces the difficulty in perception. A combination of these two factors decides the final difficulty of the task, for simple scenes, the height factor has stronger influence and hence exhibits better prediction accuracy for 3D over 2D stacking while for complex scenes, the stacking depth dominates the influence as the significant higher number of blocks can retain a reasonable height of the structure, hence receives decreased performance when moving from 2D stacking to 3D.\nCross-Group Experiment In this set of experiment, we want to see how the learned model transfers across scenes with different complexity, so we further divide the scene groups into two large groups by the number of blocks, where a simple scene group for all the scenes with 4 and 6 blocks and a complex scene for the rest of scenes with 10 and 14 blocks. We investigate in two-direction classification, shown in Figure 6, namely:\n1. Train on simple scenes and predict on complex scenes: Train on 4 and 6 blocks and test on 10 and 14 blocks\n2. Train on complex scenes and predict on simple scenes: Train on 10 and 14 blocks and test on 4 and 6 blocks\nThe result is shown in Table 2. When trained on simple scenes and predicting on complex scenes, it gets 69.9%, which is significantly better than random guess at 50%. This is understandable as the learned visual feature can transfer across different scene. Further we observe significant performance boost when trained on complex scenes and tested on simple scene. This can be explained by the richer feature learned from the complex scenes with better generalization.\nGeneralization Experiment In this set of experiment, we want to explore if we can train a general model to predict stability for scenes with any scene parameters, which is very similar to human\u2019s prediction in the task. We use\ntraining images from all different scene groups and test on any groups. The Result is shown in Table 3. While the performance exhibits similar trend to the one in the intra-group with respect to the complexity of the scenes, namely increasing recognition rate for simpler settings and decreasing rate for more complex settings, there is a consistent improvement over the intra-group experiment for individual groups. Together with the result in the cross-group experiment, it suggests a strong generalization capability of the image trained model.\nDiscussion Overall, we can conclude that direct stability prediction is possible and in fact fairly accurate at recognition rates over 80% for moderate difficulty levels. As expected, the 3D setting adds difficulties to the prediction from\nappearance due to significant occlusion for towers of more than 10 blocks. Surprisingly, little effect was observed for small tower sizes switching from uniform to non-uniform blocks - although the appearance difference can be quite small."}, {"heading": "4.2 Human Judgment on Synthetic Data", "text": "For human subject test, the predictions are binarized, namely \u201cdefinitely unstable\u201d and \u201cprobably unstable\u201d are treated as unstable prediction and \u201cprobably stable\u201d and \u201cdefinitely stable\u201d as stable prediction regardless of the certainty quantifiers. The results are shown in Table 4 and Figure 11.\nHow well do humans perform on this task? For very simple scenes with few blocks, human can reach close to perfect performance while for complex scenes, the performance drops significantly to around 60%.\nHow does human performance vary with respect to scene variations As we discuss before, the number of blocks indicates the scene\u2019s complexity, given the same block size and stacking depth condition, the human\u2019s prediction degrades with increasing number of blocks in the scene in general. Given the same number block size condition and the number of blocks in the scene, the human\u2019s predictions in 3D stacking are better than the counterpart in 2D. This can partially be explained by the factor that the structure can have larger chance to be lower than the scene with stacking constraints, and the decreased height in return reduces the scene\u2019s complexity for human\u2019s judgment. The varied blocks size consistently shows higher difficulty than the fixed blocks size as in most cases, when one scene group changes the block condition from \u201cUni\u201d to \u201cNonUni\u201d, the performance decreases.\nHow does human performance compare to image-based model? Compared to the human prediction in the same part of test data, the image-based model outperforms human in most scene groups. While showing similar trends in performance with respect to different scene parameters, the image-based model is less affected by a more difficult scene parameter setting, for example, given the same block size and stacking depth condition, the prediction accuracy decreases more slowly than the counter part in human prediction. We interpret this as image-based model possesses better generalization capability than human in the very task.\nTo gain further insight into the results, we plot the average accuracy against each scene parameter alone. The results are shown in Figure 7. Both human and the image-based model decrease consistently with respect to the number of blocks in the scene. However for both stacking depth and block size, human and the image-based model exhibit different trends, while the image-based model always outperforms the human, the human performance catches up in more complex scene parameter settings.\nHow do failure cases differ? Are they plausible? In our test, it shows that human prone to make mistake for scenes in significant height while the machine is less affected by the factor. This is also consistent with the observation in [5] that height plays an important role in human\u2019s judgment for stability. In contrast, the machine is trained across different heights, and hence can adapt to more variation. Top row in Figure 8 shows some examples of such scenes. On the other hand, the machine makes more mistake when the scenes are constructed multiple layers than human. This is understandable as our model is only trained on monocular images while the human has the prior knowledge for perception of depth information. Examples are shown in the bottom row in Figure 8. Figure 10 provides further examples of some false predictions of stable from unstable for both human and machine. While the occlusion condition can both affect human and machine for the judgment, it affects the machine more than human which is consistent with the false prediction of unstable. Similarly, height affects human more than machine as in false unstable predictions.\nFurther, we count the histogram of human\u2019s rating and the prediction confidence from the image based model (for visualization purpose, we quantized the prediction confidence into 5 bins). The result is shown in Figure 9. It\u2019s interesting to see the two distributions are relatively similar.\nCorrelation between human performance and machine Different from [5], our work does not aim to reconstruct human\u2019s inner mechanism hence the correlation between the human\u2019s prediction and the model\u2019s is not our priority. Yet we list such statistics to provide a more comprehensive image of the results\nto the reader. Here we shown the scatter plots for the pair (x, y) of human\u2019s prediction x and machine\u2019s prediction y by different scene parameters, namely number of blocks (Figure 12), stacking depth (Figure 13) and block sizes (Figure 14). We computed the Pearson correlation coefficient for each group. The detailed values are shown in Table 5 6 7. Interestingly, human prediction and human prediction are moderately positive correlated."}, {"heading": "5 Conclusion", "text": "In this work, we answer to the question if and how well we can build up a mechanism to predict physical stability directly from visual input. In contrast to existing approaches we bypass explicit 3D representation and physical simulation and learn a model for visual stability prediction from data. We evaluate our model on a range of conditions including variations in number of blocks, size of blocks and 3D structure of the overall tower. The results reflect the challenges of an increasing complex inference with increasing size of the structure as well as challenges due to small features the stability hinges on due to occlusions or block size variations.\nBased on these encouraging results we envision systems that exploit such data driven notions of physics to arrive at advanced methods for scene understanding that reason on physical plausible state during visual inference. We also will investigate richer output spaces than binary labels that shed more light on the quality of physical understanding that was acquired by the learning based approach."}], "references": [{"title": "A model of physical reasoning in infancy", "author": ["R. Baillargeon"], "venue": "Advances in infancy research", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "How do infants learn about the physical world", "author": ["R. Baillargeon"], "venue": "Current Directions in Psychological Science", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "The acquisition of physical knowledge in infancy: A summary in eight lessons", "author": ["R. Baillargeon"], "venue": "Blackwell handbook of childhood cognitive development", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Innate ideas revisited: For a principle of persistence in infants\u2019 physical reasoning", "author": ["R. Baillargeon"], "venue": "Perspectives on Psychological Science", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Simulation as an engine of physical scene understanding", "author": ["P.W. Battaglia", "J.B. Hamrick", "J.B. Tenenbaum"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Visual perception of the physical stability of asymmetric three-dimensional objects", "author": ["S.A. Cholewiak", "R.W. Fleming", "M. Singh"], "venue": "Journal of vision", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Perception of physical stability and center of mass of 3-d objects", "author": ["S.A. Cholewiak", "R.W. Fleming", "M. Singh"], "venue": "Journal of vision", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Bullet physics engine", "author": ["E. Coumans"], "venue": "Open Source Software: http://bulletphysics. org 1", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning visual predictive models of physics for playing billiards", "author": ["K. Fragkiadaki", "P. Agrawal", "S. Levine", "J. Malik"], "venue": "arXiv preprint arXiv:1511.07404", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "The panda3d graphics engine", "author": ["M. Goslin", "M.R. Mine"], "venue": "Computer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Blocks world revisited: Image understanding using qualitative geometry and mechanics", "author": ["A. Gupta", "A.A. Efros", "M. Hebert"], "venue": "ECCV", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Internal physics models guide probabilistic judgments about object dynamics", "author": ["J. Hamrick", "P. Battaglia", "J.B. Tenenbaum"], "venue": "Proceedings of the 33rd annual conference of the cognitive science society. Cognitive Science Society Austin, TX", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proceedings of the ACM International Conference on Multimedia. ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Comparison of learning algorithms for handwritten digit recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. Muller", "E Sackinger"], "venue": "International conference on artificial neural networks", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning physical intuition of block towers by example", "author": ["A. Lerer", "S. Gross", "R. Fergus"], "venue": "arXiv preprint arXiv:1603.01312", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Recognizing materials from virtual examples", "author": ["W. Li", "M. Fritz"], "venue": "ECCV", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Galileos great discovery: How things fall", "author": ["D.W. MacDougal"], "venue": "Newton\u2019s Gravity. Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Intuitive physics", "author": ["M. McCloskey"], "venue": "Scientific american", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1983}, {"title": "Newtonian image understanding: Unfolding the dynamics of objects in static images", "author": ["R. Mottaghi", "H. Bagherinezhad", "M. Rastegari", "A. Farhadi"], "venue": "arXiv preprint arXiv:1511.04048", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning deep object detectors from 3d models", "author": ["X. Peng", "B. Sun", "K. Ali", "K. Saenko"], "venue": "ICCV", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "CVPR workshops", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep reflectance maps", "author": ["K. Rematas", "T. Ritschel", "M. Fritz", "E. Gavves", "T. Tuytelaars"], "venue": "CVPR", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Image-based synthesis and re-synthesis of viewpoints guided by 3d models", "author": ["K. Rematas", "T. Ritschel", "M. Fritz", "T. Tuytelaars"], "venue": "CVPR", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "ECCV", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Naive Physics: An Essay in Ontology", "author": ["B. Smith", "R. Casati"], "venue": "Philosophical Psychology", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1994}, {"title": "Galileo: Perceiving physical object properties by integrating a physics engine with deep learning", "author": ["J. Wu", "I. Yildirim", "J.J. Lim", "B. Freeman", "J. Tenenbaum"], "venue": "NIPS", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Inferring \u201ddark matter\u201d and \u201ddark energy\u201d from videos", "author": ["D. Xie", "S. Todorovic", "S.C. Zhu"], "venue": "ICCV", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Beyond point clouds: Scene understanding by reasoning geometry and physics", "author": ["B. Zheng", "Y. Zhao", "J. Yu", "K. Ikeuchi", "S.C. Zhu"], "venue": "CVPR", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 29, "context": "They are part of \u201cdark matter\u201d [30] in our everyday visual data which helps us interpret the configurations of objects correctly and accurately.", "startOffset": 31, "endOffset": 35}, {"referenceID": 27, "context": "We rather seem to build on what has been coined as \u201cn\u00e4\u0131ve physics\u201d [28] or \u201cintuitive physics\u201d [20], which is a good enough proxy to make us operate successfully in the real-world.", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "We rather seem to build on what has been coined as \u201cn\u00e4\u0131ve physics\u201d [28] or \u201cintuitive physics\u201d [20], which is a good enough proxy to make us operate successfully in the real-world.", "startOffset": 95, "endOffset": 99}, {"referenceID": 4, "context": "the complexity of the problem [5].", "startOffset": 30, "endOffset": 33}, {"referenceID": 20, "context": "Only recently, several approach have revived this idea and reattempted a fully data drive approach to capturing the essence of physical events via machine learning methods [21,29,10].", "startOffset": 172, "endOffset": 182}, {"referenceID": 28, "context": "Only recently, several approach have revived this idea and reattempted a fully data drive approach to capturing the essence of physical events via machine learning methods [21,29,10].", "startOffset": 172, "endOffset": 182}, {"referenceID": 9, "context": "Only recently, several approach have revived this idea and reattempted a fully data drive approach to capturing the essence of physical events via machine learning methods [21,29,10].", "startOffset": 172, "endOffset": 182}, {"referenceID": 0, "context": "In contrast, studies in developmental psychology [1] have shown that infants acquire the knowledge of physical events by observation at a very early age, including support, collision and unveiling.", "startOffset": 49, "endOffset": 52}, {"referenceID": 3, "context": "According to their research, the infant with some innate basic core knowledge [4] gradually builds its internal model of the physical event by observing its various outcomes.", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "Hence, we revisit the classic setup of Tenenbaum and colleagues [5] and explore to which extend machines can predict physical stability events directly from appearance cues.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "Research in development psychology [2,1,3] suggests that infants acquire the knowledge of physical events at very young age by observing those events, including support events and others.", "startOffset": 35, "endOffset": 42}, {"referenceID": 0, "context": "Research in development psychology [2,1,3] suggests that infants acquire the knowledge of physical events at very young age by observing those events, including support events and others.", "startOffset": 35, "endOffset": 42}, {"referenceID": 2, "context": "Research in development psychology [2,1,3] suggests that infants acquire the knowledge of physical events at very young age by observing those events, including support events and others.", "startOffset": 35, "endOffset": 42}, {"referenceID": 4, "context": "[5] proposed an intuitive physics simulation engine for such mechanism and found it resemble to human subjects\u2019 behaviors pattern in several psychological tasks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Historically, intuitive physics is connected to the case where people often hold erroneous physical intuitions [20], such as people tend to expect an object dropped from a moving subject will fall vertically straight down.", "startOffset": 111, "endOffset": 115}, {"referenceID": 4, "context": "It is rather counter-intuitive how the proposed simulation engine in [5] can explain such erroneous intuitions.", "startOffset": 69, "endOffset": 72}, {"referenceID": 18, "context": "In fact, looking back to history, physical laws are discovered through the observation of physical events [19].", "startOffset": 106, "endOffset": 110}, {"referenceID": 4, "context": "The data generation procedure is based on the platform used in [5], however as discussed before, their work hypothesized a simulation engine as an internal mechanism for human to understand the physics in the external world while we are interested in finding an image-based model to directly predict the physical behavior from visual channel.", "startOffset": 63, "endOffset": 66}, {"referenceID": 17, "context": "Learning from synthetic data has a long tradition in computer vision and recently has gained increasing interest [18,25,22,24] due to data hungry deep learning approach.", "startOffset": 113, "endOffset": 126}, {"referenceID": 24, "context": "Learning from synthetic data has a long tradition in computer vision and recently has gained increasing interest [18,25,22,24] due to data hungry deep learning approach.", "startOffset": 113, "endOffset": 126}, {"referenceID": 21, "context": "Learning from synthetic data has a long tradition in computer vision and recently has gained increasing interest [18,25,22,24] due to data hungry deep learning approach.", "startOffset": 113, "endOffset": 126}, {"referenceID": 23, "context": "Learning from synthetic data has a long tradition in computer vision and recently has gained increasing interest [18,25,22,24] due to data hungry deep learning approach.", "startOffset": 113, "endOffset": 126}, {"referenceID": 11, "context": "By including the additional clue from physical constraints into the inference mechanism, mostly from the support event, it has further improved results in segmentation of surfaces [12], scenes [26] from image data, and object segmentation in 3D point cloud data [31].", "startOffset": 180, "endOffset": 184}, {"referenceID": 25, "context": "By including the additional clue from physical constraints into the inference mechanism, mostly from the support event, it has further improved results in segmentation of surfaces [12], scenes [26] from image data, and object segmentation in 3D point cloud data [31].", "startOffset": 193, "endOffset": 197}, {"referenceID": 30, "context": "By including the additional clue from physical constraints into the inference mechanism, mostly from the support event, it has further improved results in segmentation of surfaces [12], scenes [26] from image data, and object segmentation in 3D point cloud data [31].", "startOffset": 262, "endOffset": 266}, {"referenceID": 20, "context": "[21] aims at understanding dynamic events governed by laws of Newtonian physics, but uses proto-typical motion scenarios as exemplars.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] analyze a billiard table scenarios and aim at learning the dynamics from observation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] aims to understand physical properties of objects.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "A recent paper [17] that appeared a few days before this work is a related research thread that was conducted in parallel without our knowledge.", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "Therefore, we follow the pioneer work on this topic [5] and use a simulator to setup and predict physical outcomes of wood block towers.", "startOffset": 52, "endOffset": 55}, {"referenceID": 12, "context": "Based on the scene simulation framework used in [13,5], we generate synthetic data in our experiment with rectangular cuboid blocks as basic elements.", "startOffset": 48, "endOffset": 54}, {"referenceID": 4, "context": "Based on the scene simulation framework used in [13,5], we generate synthetic data in our experiment with rectangular cuboid blocks as basic elements.", "startOffset": 48, "endOffset": 54}, {"referenceID": 4, "context": "6m as in the [5].", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "Rendering While we keep the rendering basic, we like to point out that we deliberately decided against colored bricks as in [5] in order to challenge perception and make identifying brick outlines and configurations more challenging.", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "Physics Engine We use Bullet [8] physics engine in Panda3D [11] to perform physics-based simulation for 2000ms at 1000Hz for each scene.", "startOffset": 29, "endOffset": 32}, {"referenceID": 10, "context": "Physics Engine We use Bullet [8] physics engine in Panda3D [11] to perform physics-based simulation for 2000ms at 1000Hz for each scene.", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "Inspiration from Human Research in [13,5] suggests the combinations of the most salient features in the scenes are insufficient to capture people\u2019s judgments, however, contemporary study reveals human\u2019s perception of visual information, in particular some geometric feature, like critical angle [6,7] plays an important role in the process.", "startOffset": 35, "endOffset": 41}, {"referenceID": 4, "context": "Inspiration from Human Research in [13,5] suggests the combinations of the most salient features in the scenes are insufficient to capture people\u2019s judgments, however, contemporary study reveals human\u2019s perception of visual information, in particular some geometric feature, like critical angle [6,7] plays an important role in the process.", "startOffset": 35, "endOffset": 41}, {"referenceID": 5, "context": "Inspiration from Human Research in [13,5] suggests the combinations of the most salient features in the scenes are insufficient to capture people\u2019s judgments, however, contemporary study reveals human\u2019s perception of visual information, in particular some geometric feature, like critical angle [6,7] plays an important role in the process.", "startOffset": 295, "endOffset": 300}, {"referenceID": 6, "context": "Inspiration from Human Research in [13,5] suggests the combinations of the most salient features in the scenes are insufficient to capture people\u2019s judgments, however, contemporary study reveals human\u2019s perception of visual information, in particular some geometric feature, like critical angle [6,7] plays an important role in the process.", "startOffset": 295, "endOffset": 300}, {"referenceID": 12, "context": "The mapping can be inclusive, as in [13] using it along with other aspects, like physical constraint to make judgment or exclusive, as in [6] using visual cues alone to decide.", "startOffset": 36, "endOffset": 40}, {"referenceID": 5, "context": "The mapping can be inclusive, as in [13] using it along with other aspects, like physical constraint to make judgment or exclusive, as in [6] using visual cues alone to decide.", "startOffset": 138, "endOffset": 141}, {"referenceID": 14, "context": "We use deep convolutional neural networks to learn the mapping as it has shown great success on image classification task [15].", "startOffset": 122, "endOffset": 126}, {"referenceID": 22, "context": "Such networks have been shown to be able to adapt to a wide range of classification and prediction task [23] through re-training or adaptation by fine-tuning.", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "In a pilot study, we tested on a subset of the generated data with LeNet [16], a relatively small network designed for digit recognition, AlexNet [15], a large network and VGG Net[27], a even larger network than AlexNet.", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "In a pilot study, we tested on a subset of the generated data with LeNet [16], a relatively small network designed for digit recognition, AlexNet [15], a large network and VGG Net[27], a even larger network than AlexNet.", "startOffset": 146, "endOffset": 150}, {"referenceID": 26, "context": "In a pilot study, we tested on a subset of the generated data with LeNet [16], a relatively small network designed for digit recognition, AlexNet [15], a large network and VGG Net[27], a even larger network than AlexNet.", "startOffset": 179, "endOffset": 183}, {"referenceID": 8, "context": "We trained from scratch for the LeNet and fine-tuned for the large network pre-trained on ImageNet [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 13, "context": "We use the Caffe framework [14] in all our experiments.", "startOffset": 27, "endOffset": 31}, {"referenceID": 4, "context": "This is also consistent with the observation in [5] that height plays an important role in human\u2019s judgment for stability.", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "Correlation between human performance and machine Different from [5], our work does not aim to reconstruct human\u2019s inner mechanism hence the correlation between the human\u2019s prediction and the model\u2019s is not our priority.", "startOffset": 65, "endOffset": 68}], "year": 2016, "abstractText": "Understanding physical phenomena is a key competence that enables humans and animals to act and interact under uncertain perception in previously unseen environments containing novel object and their configurations. Developmental psychology has shown that such skills are acquired by infants from observations at a very early stage. In this paper, we contrast a more traditional approach of taking a modelbased route with explicit 3D representations and physical simulation by an end-to-end approach that directly predicts stability and related quantities from appearance. We ask the question if and to what extent and quality such a skill can directly be acquired in a data-driven way\u2014 bypassing the need for an explicit simulation. We present a learning-based approach based on simulated data that predicts stability of towers comprised of wooden blocks under different conditions and quantities related to the potential fall of the towers. The evaluation is carried out on synthetic data and compared to human judgments on the same stimuli.", "creator": "LaTeX with hyperref package"}}}