{"id": "1604.00119", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "Semi-supervised and Unsupervised Methods for Categorizing Posts in Web Discussion Forums", "abstract": "web content forums are located among millions grown people research or share information geared to a specialization outside organizations such several automotive associations, relationships, sports, etc. they typically implement posts that fall into different heading such as \\ console { complaints }, \\ keyboard { problems }, \\ log { feedback }, \\ textit { spam }, c. automatic diagnosis of these categories can aid information retrieval that is tailored but changing intelligence requirements. previously, a number of supervised experiments have attempted toward assign system problem ; eventually, these depend on the availability of documented supporting evidence.. few existing substantive and semi - operational problems are either justified without identifying a single category or don't report meaning - rich performance.. contrast, this analysis proposes isolated and semi - supervised solutions that evaluate no ordinary minimal training data whereas achieve this objective rather compromising on performance. a fine - coarse analysis is routinely carried across to discuss adaptive reliability. typically analytic methods mainly based as sequence distributions ( equations, perfect markov models ) have can distinguish competence for each category using word and audio - wide - speech proficiency distributions, and frequently learn features. empirical evaluations per domains demonstrate where the proposed methods are finer suited within this task than existing ones.", "histories": [["v1", "Fri, 1 Apr 2016 03:32:03 GMT  (540kb,D)", "https://arxiv.org/abs/1604.00119v1", null], ["v2", "Tue, 5 Apr 2016 16:06:13 GMT  (540kb,D)", "http://arxiv.org/abs/1604.00119v2", null], ["v3", "Sun, 24 Apr 2016 21:27:17 GMT  (540kb,D)", "http://arxiv.org/abs/1604.00119v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG cs.SI", "authors": ["krish perumal"], "accepted": false, "id": "1604.00119"}, "pdf": {"name": "1604.00119.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Krish Perumal"], "emails": [], "sections": [{"heading": null, "text": "Semi-supervised and unsupervised methods for categorizing posts in Web discussion\nforums\nKrish Perumal\nMaster of Science\nGraduate Department of Computer Science\nUniversity of Toronto\n2016\nWeb discussion forums are used by millions of people worldwide to share information belonging to a variety of domains such as automotive vehicles, pets, sports, etc. They typically contain posts that fall into different categories such as problem, solution, feedback, spam, etc. Automatic identification of these categories can aid information retrieval that is tailored for specific user requirements. Previously, a number of supervised methods have attempted to solve this problem; however, these depend on the availability of abundant training data. A few existing unsupervised and semi-supervised approaches are either focused on identifying a single category or do not report category-specific performance. In contrast, this work proposes unsupervised and semi-supervised methods that require no or minimal training data to achieve this objective without compromising on performance. A fine-grained analysis is also carried out to discuss their limitations. The proposed methods are based on sequence models (specifically, Hidden Markov Models) that can model language for each category using word and part-of-speech probability distributions, and manually specified features. Empirical evaluations across domains demonstrate that the proposed methods are better suited for this task than existing ones.\nii"}, {"heading": "Acknowledgements", "text": "I thank my advisor, Prof. Graeme Hirst, whose valuable guidance, feedback and attention to detail was pivotal for the completion of this work. I also thank my second reader, Prof. Gerald Penn, for assessing my work and providing me valuable comments.\nI am grateful to Dr. Afsaneh Fazly who first introduced me to this work, and advised me on possible research directions to explore at every step of this work. I also thank Brandon Seibel and Alex Minnaar for assisting me during my visits to Verticalscope Inc. I sincerely thank Verticalscope Inc. for allowing me to access their data for this research.\nI sincerely thank the Natural Sciences and Engineering Research Council of Canada for supporting this research through the NSERC-ENGAGE grant (no. EGP 477227-14). I am also grateful to Mohamed Abdalla for assisting me with some of the implementations mentioned in this work.\nI thank my parents who worked tirelessly to provide me a great education. I also thank my friends and well-wishers at the Department of Computer Science who assisted me (both academically and otherwise) during my time here \u2013 Kaustav Kundu, Dave Fernig, Ivan Vendrov, Nona Naderi, Jamie Kiros, Aida Nematzadeh, Aditya Bhargava, Sean Robertson, Patricia Thaine, Katie Fraser and Tong Wang. I also specially thank my best friends back in India \u2014 Sidhant Sharma, Kartik Arunachalam, Nishant Gupta, and Vishi Jhalani.\niii\nContents"}, {"heading": "1 Introduction 1", "text": ""}, {"heading": "2 Related Work 5", "text": "2.1 Supervised Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.2 Unsupervised Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.3 Semi-supervised Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.4 Methods Applied to Other Tasks . . . . . . . . . . . . . . . . . . . . . . 10"}, {"heading": "3 Description of Implemented Methods 12", "text": "3.1 Existing Methods with Minor Enhancements . . . . . . . . . . . . . . . . 12\n3.1.1 Conversation Model . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n3.1.2 Conversation Model with Gaussian Mixtures . . . . . . . . . . . . 17\n3.1.3 Fully Supervised Methods . . . . . . . . . . . . . . . . . . . . . . 18\n3.2 Proposed Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n3.2.1 Conversation Model with Part-of-Speech Tags . . . . . . . . . . . 21\n3.2.2 Conversation Model with Features . . . . . . . . . . . . . . . . . . 23\n3.2.3 Conversation Model with Post Embeddings . . . . . . . . . . . . . 23\n3.2.4 Semi-supervised Conversation Model . . . . . . . . . . . . . . . . 24\n3.2.5 Other Enhancements . . . . . . . . . . . . . . . . . . . . . . . . . 24\n3.3 Mapping of Clusters to Categories . . . . . . . . . . . . . . . . . . . . . . 26\niv"}, {"heading": "4 Data Collection and Annotation 27", "text": ""}, {"heading": "5 Experiments 32", "text": "5.1 Evaluation Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 5.2 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n5.2.1 Preprocessing and Configuration Parameters . . . . . . . . . . . . 33 5.2.2 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n5.3 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.4 Performance Comparison with State-of-the-Art . . . . . . . . . . . . . . . 38\n5.4.1 Unsupervised HMM+Mix Model . . . . . . . . . . . . . . . . . . 38 5.4.2 Semi-supervised Answer Extraction . . . . . . . . . . . . . . . . . 39\n5.5 Category-wise Performance and Error Analysis . . . . . . . . . . . . . . . 39 5.6 Effect of the Amount of Training Data . . . . . . . . . . . . . . . . . . . 41 5.7 Summary of Experimental Results . . . . . . . . . . . . . . . . . . . . . . 42"}, {"heading": "6 Conclusions and Future Work 44", "text": "6.1 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 6.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\nBibliography 47\nv\nChapter 1\nIntroduction\nThe Internet contains a wide range of user-generated content in the form of blogs, discussion forums, social media posts, digital media, etc. These enable users to exchange information in a manner less formal and more personalized than centralized information sources such as government agencies, media houses, and educational and research institutes. Among these, Web discussion forums are platforms where people converse with one another to collaboratively solve problems and discuss issues. These forums might encompass a wide range of topics (e.g., Yahoo Answers1) or be limited to a narrow domain (e.g., JeepForum2). The former kind of forums are typically organized into topic hierarchies, essentially reducing them to forums of the latter kind. For example, Yahoo Answers consists of topics such as Arts and Humanities, Health, Family and Relationships, etc. Further, Family and Relationships contains sub-topics such as Family, Friends, Marriage and Divorce, etc. Such a hierarchy enables easier navigation for users who wish to seek or provide information about a specific topic of their interest. Within each topic, forums consist of individual conversations, called threads, each containing multiple user messages, called posts.\n1http://answers.yahoo.com 2http://www.jeepforum.com/\n1\nChapter 1. Introduction 3\nAn example discussion forum thread is shown in Table 1.1. Here, user 15JKU (called original poster from here onward) initially asks for advice on whether 35\u2019s model tires on 18-20-inch wheels are good for daily driving as well as mudding (a hobby of driving jeeps on muddy off-road surfaces). The user also does not want the wheels to make much noise. User mschi772 responds that the original poster is expecting too much from a single tire, and requests clarification on the user\u2019s priorities. The original poster clarifies that he/she wants to know how loud the models of wheels and tires are, and that the priority is suitability for mudding. User mschi772 proposes another model called Nitto Trail Grapplers which are better for mudding, but would make noise and wear out quickly on streets. User JcArnold responds that he/she has 37-inch trails which work well in rock and snow. User Pedro7 joins the conversation by asking the original poster to not go for mud tires if noise is a concern. The user recommends Nittos as the best possible solution, but warns against expecting a perfect tire for every situation. Finally, the original poster provides feedback by thanking everyone and announcing that he/she is choosing Nittos.\nTable 1.1 also contains a column (which is not part of the original forum) mentioning the manually identified purpose of each post in the thread. With this information, a user seeking a solution to a similar problem need only read three out of six posts replying to the first post. Without such information, the user must read the entire thread. This problem becomes much more pronounced in cases where threads contain tens or hundreds of posts, and reading the entire thread becomes impractical (unless one participates in the thread conversation from the beginning). For example, http://www.jeepforum. com/forum/f15/mud-tires-119948/ contains more than 500 posts discussing popular brands of tires. Most of these posts involve off-topic personalized discussions. In such cases, the purpose of each post can guide the user towards useful posts (i.e., containing solutions) and away from trivial posts (i.e., containing feedback or off-topic discussions). Moreover, current information retrieval techniques return entire threads as results to search queries. But by being sensitized to these annotations, they can return targeted\nChapter 1. Introduction 4\nresults containing only relevant posts instead of entire threads. Further, user-contributed information contained in these forums can be better structured and contribute towards the development of domain-specific knowledge bases. With these motivations in mind, this work aims to automatically annotate each post in a discussion forum with its purpose in the conversation thread.\nThe problem described is neither a novel nor a neglected one in the field of computational linguistics (as will be demonstrated in the discussion of related work in the following chapter). It is closely related to the problem of dialogue act tagging, which is defined as the identification of the meaning of an utterance at the level of illocutionary force (Stolcke et al., 2000), i.e., an utterance could be identified as falling into one or more categories such as problem, solution, clarification, feedback, command, request, etc. Most of the previous work has concentrated on supervised machine learning methods which make use of manually annotated data in order to predict the annotations of unseen data. In contrast, this paper discusses novel approaches using minimal (semi-supervised) or no manually annotated data (unsupervised). Some previous work on semi-supervised and unsupervised methods exists; however, this research paper will empirically demonstrate (in section 5) that the proposed methods perform better.\nThe main contributions of this work are the following.\n\u2022 Summarizing existing work on categorizing discussion forum posts and discussing\ntheir limitations.\n\u2022 Proposing novel methods based on sequence models for categorizing discussion\nforum posts with minimal or no annotated data.\n\u2022 Developing an annotated dataset of discussion forums from a hereto neglected au-\ntomotive domain.\n\u2022 Conducting experiments to analyze the performance of existing and proposed meth-\nods on datasets belonging to different domains.\nChapter 2\nRelated Work\nThe problem of identifying the purpose or intention of each post in a discussion forum thread has been extensively tackled in previous literature. However, there is no unanimously agreed-upon set of tags to identify, because they depend on the final objective of the tagging process. For example, the objective of an answer retrieval system is better achieved by concentrating on identifying Question and Answer posts alone, whereas the objective of an answer quality assessment system is fulfilled by additionally identifying Positive/Negative Feedback posts. Most of the previous work has concentrated on tackling these kind of dialogue categories, and uses the term dialogue act tagging. Also, some previous work has named categories specific to the target domain. For example, a forum on the medical domain may typically consist of posts explaining medical conditions and those providing treatment options, hence identifying categories such as Medical Problem and Treatment, whereas a forum on the computer-related technical domain may consist of categories such as Problem: Hardware, Problem: Software, Solution: Install and Solution: Search. This research paper does not restrict itself solely to dialogue act tagging ; neither does it address the classification of categories for only a specific domain. Hence, it uses the general term forum post categorization.\n5\nChapter 2. Related Work 7"}, {"heading": "2.1 Supervised Methods", "text": "Supervised machine learning methods use previously labeled data for training, in order to predict the categories assigned to unseen data. Related previous work on classification of categories of discussion forum posts has largely focused on the application of these methods. In particular, most of the work has concentrated on the computer-related technical domain. Catherine et al. (2012) employed Support Vector Machines (SVMs) to extract Answer posts in a thread (assuming that the first post in the thread is a Question). They used a number of structural and syntactic features, in addition to forum-specific features such as author authority1 and post ratings. Their methods were evaluated on a corpus of Apple discussion forums2. Bhatia et al. (2012) used supervised machine learning algorithms (i.e., SVMs, logit model classifier, naive Bayes, etc.) to classify forum posts into eight categories \u2014 Question, Repeat Question, Clarification, Further Details, Solution, Positive Feedback, Negative Feedback, and Junk. They evaluated their methods on a dataset of the Ubuntu forums3. Qu and Liu (2011) used Hidden Markov Models (HMMs) to classify forum posts into four categories \u2014 Problem, Solution, Good Feedback and Bad Feedback. They evaluated their methods on the Oracle database support forums4. Similarly, Wang et al. (2010) attempted to identify Problem and Solution posts in the CNET forums5 dataset, but with more fine-grained categories based on the types of Problem posts (i.e., Hardware, Software, Media, OS, Network, and Programming) and Solution posts (i.e., Documentation, Install, Search, and Support). Kim et al. (2010) worked on the same dataset, and attempted to classify posts into 12 categories that are similar to the ones used by Bhatia et al. (2012). Additionally, they tagged the links between posts, i.e., identifying which post is a reply to which other post. For both tasks,\n1According to Catherine et al. (2012), author authority is a numerical or categorical value that is indicative of an author\u2019s level of expertise in the context of the forum.\n2https://discussions.apple.com 3http://ubuntuforums.org 4https://community.oracle.com/community/database/ 5http://forums.cnet.com\nChapter 2. Related Work 8\nthey reported the best performance using Conditional Random Fields (CRFs). Wang et al. (2011) went one step further by jointly classifying both posts and the links between them. They used two different methods: (1) composition of results from both tasks done separately, and (2) combination of post and link tag sets in a single task. Two other papers reported work on forums on the travel and medical domains. Ding et al. (2008) used CRFs to identify Answer posts and the context in which they answered the Question post. However, they did not attempt to identify Question posts, because they were assumed to be known beforehand. Their techniques were evaluated on a corpus of the TripAdvisor forums6. Sondhi et al. (2010) used CRFs and SVMs with various semantic and structural features to identify Medical Problem and Treatment in the HealthBoards forums7. A summary of all these methods is presented in Table 2.1. A major drawback of these approaches is that they are constrained by the requirement of manually annotated data for training, and are limited in applicability to the domains they are trained on."}, {"heading": "2.2 Unsupervised Methods", "text": "Unsupervised methods identify unlabeled clusters of data, each of which could potentially be mapped to a target category that one wants to identify. These methods use a taskdependent measure of similarity to identify whether two input units should belong to the same cluster or not, and in some cases, also model the interactions between the clusters. In contrast to supervised techniques, they require no labeled data; hence, they are not limited in applicability to a specific domain. To the best of our knowledge, three unsupervised techniques have been previously proposed for categorization of posts in Web forums. Cong et al. (2008) used labeled sequential patterns to identify Question posts, followed by a graph-based propagation method to extract corresponding Answer posts. The question detection phase was supervised, whereas answer extraction was\n6http://www.tripadvisor.com/ForumHome 7http://www.healthboards.com\nChapter 2. Related Work 9\nunsupervised. The graph-based propagation used language models and author authority in order to assign scores to the links (edges) between posts (nodes). The method was evaluated on a corpus of forum threads on the travel domain. Deepak and Visweswariah (2014) identified Solution posts using a translation-based model that leverages lexical correlations between Problem and Solution posts. Joty et al. (2011) used a combination of HMMs and Gaussian Mixture Models (GMMs) in order to classify forum posts into 12 dialogue act categories. In addition to word n-grams, they used some structural features such as the chronological position of a post in the thread, the number of tokens in the post, and author identity. Both these papers reported results on corpora of forums on the computer-related technical domain (i.e., Apple discussion forums and Ubuntu forums). Other unsupervised techniques have been employed for the related tasks of dialogue act classification in spoken dialogue systems (Crook et al., 2009) and Twitter conversations (Ritter et al., 2010). Although they worked specifically on genres of text that are very different from Web forums, they can potentially inspire future approaches tailored for Web forums. All these unsupervised approaches ignored the evaluation of category-wise classification. Instead, they reported overall accuracy measures which do not adequately reflect the technique\u2019s performance (as will be shown in chapter 5). One major drawback of unsupervised methods is that they often generate clusters that are undesired or have no meaning in the real world. For example, clustering of forum posts on the travel domain might lead to a cluster containing posts pertaining to New York City sightseeing alone. This is a meaningful cluster in general, but it has no meaning when one aims to find clusters of post categories such as Question, Answer, Feedback, etc. Moreover, because the clusters are unlabeled, post-processing is necessary to map the clusters to the categories that are desired as the output.\nChapter 2. Related Work 10"}, {"heading": "2.3 Semi-supervised Methods", "text": "Semi-supervised methods can overcome the drawbacks of both unsupervised and supervised methods by using a minimal amount of labeled data (that is costly to obtain) and a large amount of unlabeled data (that is easily available). To the best of our knowledge, there exist only two semi-supervised methods for categorization of posts in Web forums. One employed domain adaptation from labeled spoken dialogue datasets by means of a sub-tree pattern mining algorithm (Jeong et al., 2009). Another method extracted Answer posts in forum threads using a co-training framework (Catherine et al., 2013). However, it focused only on extracting Answer posts, with the assumption that the first post in a thread is a Question. Both methods used features such as the chronological position of a post in the thread, and post and author ratings."}, {"heading": "2.4 Methods Applied to Other Tasks", "text": "There exists other previous work that is applied to tasks unrelated to forum post categorization but which inspires the development of techniques discussed in this research paper. Barzilay and Lee (2004) proposed a content model for multi-document summarization based on sentence extraction. This model consists of an HMM at the sentence level that is tailored towards identifying sentence clusters belonging to different topics. Inspired by this model, Ritter et al. (2010) suggested a \u2018conversation model\u2019 for the modeling of dialogue acts in Twitter conversations. Their model replicates Barzilay\u2019s model but replaces sentences in a document with tweets in a Twitter conversation as units of the HMM. They used Topic Modeling (using Latent Dirichlet Allocation) along with the conversation model and reported better performance; but the evaluation was done only qualitatively. Similarly, Joty et al. (2011) applied conversation models to email and forum threads where a single post is considered an HMM unit. They further enriched this technique by using structural features from emails and forums, in addition to language\nChapter 2. Related Work 11\nmodels. They used GMMs along with their feature-enhanced conversation models, and reported better performance than using conversation models alone. The motivation for these techniques is that HMMs can model the sequential nature of dialogue acts well. For example, the fact that a Solution is more likely to follow a Problem, as opposed to any other category, can be implicitly encoded in the HMMs.\nChapter 3\nDescription of Implemented\nMethods\nThe code for existing methods (that are relevant to this work) is not available to other researchers. Also, a number of technical details that are necessary for reproduction are omitted in literature. Hence, it is important to describe the implementations of previous methods that inspire or form the basis of the proposed methods. In the process, a few enhancements are also proposed. These are described in the following section."}, {"heading": "3.1 Existing Methods with Minor Enhancements", "text": ""}, {"heading": "3.1.1 Conversation Model", "text": "The conversation model that was introduced in the previous chapter is described here. While there are three different variants of this model (as described in the previous chapter), this work implements the originally proposed model by Barzilay and Lee (2004), while making necessary modifications for applying it to forum post categorization. The conversation model is a Hidden Markov Model (HMM), in which hidden (unobserved) states correspond to post categories, and emissions (observed) correspond to bags of post\n12\nChapter 3. Description of Implemented Methods 13\nn-grams. A plate notation of the equivalent graphical model is shown in Figure 3.1 (derived from Ritter et al. (2010) and Joty et al. (2011)). Here, a thread Tk consists of a sequence of category labels, and each category label Ci emits a bag of word n-grams Ni of the ith chronological post in the thread.\nThe priors for this model are derived from a two-step process: (i) every post is represented as a vector of word n-gram frequency counts, and (ii) the vectors are clustered using hierarchical clustering. The resultant cluster labels are used to calculate the frequency counts of initial HMM states and state transitions, and hence, the corresponding probabilities. The priors are optionally calculated using an additional concept of insertion states. These are the states which contain a number of posts fewer than a fixed threshold, called state size threshold. This concept is used to account for small noise states that pertain to no meaningful target category. If used, all insertion states are merged into a single state, representing a noise state.\nThe learning algorithm (Algorithm 1) of the conversation model uses iterative Expectation Maximization (EM) to maximize the expected probability of a post given a state, repeating until convergence of the sum of all observation probabilities. During the expectation step (E-step), a word n-gram language model is constructed for each state. Using this state-specific language model, the emission probability of an observation (or\nChapter 3. Description of Implemented Methods 14\npost) can be calculated. During the maximization step (M-step), the most likely state sequence is calculated using Viterbi algorithm. All configuration parameters used in this algorithm are described in Table 3.1. Each function used in the algorithm is described below.\n\u2022 vectorize: Given a post, it outputs a vector using frequency counts of word n-grams\nin the post. The number of dimensions of the vector is equal to the word vocabulary size of all posts.\n\u2022 cluster : Given a set of vectors, it clusters them using the complete linkage hierar-\nchical clustering algorithm with cosine distance metric, and outputs a cluster label for each vector.\n\u2022 merge small states : Given a list of states (one for each post), it merges all states\nwith fewer than stateSizeThreshold number of posts into a single state, and outputs the updated states as well as the updated number of states. This is applicable only if the mergeInsertionStates parameter is set to true.\n\u2022 language model : Constructs a word n-gram language model for the posts belonging\nto a given state. A smoothing parameter \u03b41 is used to account for unseen word n-grams when calculating the probability of a post.\n\u2022 Viterbi algorithm: Runs Viterbi algorithm to output the most likely state sequence,\ngiven the HMM parameters (i.e., initial state probabilities, state transition probabilities, and state-specific language models).\nChapter 3. Description of Implemented Methods 15\nIn the HMM, the probability of a post Pi, given a state Sk, is calculated as a categorical\nprobability of its word n-grams, as shown in Equation 3.1.\np(Pi|Sk) = \u220f j p(Wi,j|Lk) (3.1)\nwhere:\nWi,j is the j th (in no particular order) word n-gram in post Pi, and Lk is the language model for state Sk.\nParameter Name Description Data Type initialNumClusters The initial number of clusters to be output using agglomerative clustering Integer mergeInsertionStates ; stateSizeThreshold\nStates with a number of posts fewer than stateSizeThreshold are merged into a single state if mergeInsertionStates is set to true Boolean; Integer\nlmType The type of language model to be used for calculating the emission probability of a post given a state \u2018unigram\u2019 or \u2018bigram\u2019 \u03b41 Smoothing parameter for language modeling (to account for unseen n-grams) Float \u03b42 Smoothing parameter for calculation of HMM state transition probabilities (to account for unseen state transitions) Float maxNumIterations Maximum number of iterations of Expectation Maximization Integer numMixtureComponents Number of mixture components to be used for conversation model with Gaussian mixtures Integer\nTable 3.1: Configuration parameters used in conversation models\nChapter 3. Description of Implemented Methods 16\nAlgorithm 1 Conversation model Input: A list of threads T, each containing a list of posts P (in chronological order) Parameters: initialNumClusters, mergeInsertionStates, stateSizeThreshold, maxNumIterations, lmType, \u03b41, \u03b42 Output: A list of cluster labels CL for each post in each thread (in the order of the input)\n1: for all thread Tx do 2: for all post Px,y \u2208 Tx do 3: Vx,y := vectorize(Px,y) // Vx,y is the vector of post Px,y 4: end for 5: end for 6: ICL := cluster(V, initialNumClusters) // ICL is the list of initial cluster labels for each post\n(ICLx,y is the initial cluster label for post Px,y in thread Tx) 7: S := ICL // S is the list of states for all posts; at this step, it is the same as the initial cluster\nlabels 8: for n = 1\u2192 maxNumIterations do 9: if mergeInsertionStates is true then\n10: [S, numStates] := merge small states(S, stateSizeThreshold) 11: end if 12: for i = 1\u2192 numStates do 13: SPi = \u2205 14: for all state Sx,y do 15: if Sx,y = i then 16: SPi := SPi \u222a Px,y // SPi is the set of all posts that belong to state i 17: end if 18: end for 19: Li := language model(SPi, lmType, \u03b41) 20: end for 21: for i = 1\u2192 numStates do 22: init countsi := \u03a3Tx1Sx,1 = i // Sx,1 is the state of the first post in thread Tx 23: end for 24: for i = 1\u2192 numStates do 25: \u03c0i := (init countsi + \u03b42)/(\u03a3k(init countsk) + \u03b42 \u00d7 numStates) // \u03c0i is the probability that initial state is i 26: end for 27: for i = 1\u2192 numStates do 28: for j = 1\u2192 numStates do\n29: trans countsi,j := \u2211\nTx |Tx|\u22121\u2211 a=1 1Sx,a = i, Sx,a+1 = j\n30: end for 31: end for 32: for i = 1\u2192 numStates do 33: for j = 1\u2192 numStates do 34: \u03c6i,j := (trans countsi,j + \u03b42)/(\u03a3k,l(trans countsk,l) + \u03b42 \u00d7 numStates2) // \u03c6i,j is the probability of transitioning from state i to state j 35: end for 36: end for 37: S := V iterbi algorithm(\u03c0, \u03c6, L) 38: if sum of observation probabilities converged then 39: break 40: end if 41: end for 42: CL := S\nChapter 3. Description of Implemented Methods 17"}, {"heading": "3.1.2 Conversation Model with Gaussian Mixtures", "text": "The previous model used standard HMM emission probabilities that were based on ngram frequency counts, which can suffer from the drawback of producing topical clusters. To counter this, Joty et al. (2011) proposed a method which models the HMM emissions as a mixture of Gaussians, i.e., a Gaussian Mixture Model (GMM). A plate notation of the resultant model is shown in Figure 3.2. Here a thread Tk consists of a sequence of category labels, and each category label Ci and Gaussian mixture Mi emit a bag of word n-grams Ni, which corresponds to the i th chronological post in the thread. Apart from preventing topical clusters, the authors argue that this can define finer and hence, richer emission distributions. Also, in contrast to the Topic Model-based approach (Ritter et al., 2010), learning and inference can be done using the EM algorithm without approximate inference techniques.\nIn addition to the steps in the simple conversation model, the learning algorithm (Algorithm 2) of the current model uses Gaussian mixture components as input to the Viterbi algorithm to calculate the most likely state sequence. Each function used in the algorithm is described below.\nChapter 3. Description of Implemented Methods 18\n\u2022 fit GMM \u2013 It fits the given vector to the GMM corresponding to the vector\u2019s state.\nThe initial values of mean and variance of each mixture component are initialized randomly. The value of the numMixtureComponents parameter decides the number of mixture components to be used.\n\u2022 V iterbi algorithm \u2013 Runs Viterbi algorithm in order to output the most likely state\nsequence given the parameters of the HMM and GMMs (i.e., initial state probabilities, state transition probabilities, and state-specific Gaussian mixture components).\nHere, the probability of a post Pi, given a state Sk, is calculated as shown in Equation\n3.2.\np(Pi|Sk) = \u2211 j p(Mk,j|Sk)p(Pi|Mk,j) (3.2)\nwhere:\nMk,j refers to the j th (in no particular order) mixture model component for state Sk."}, {"heading": "3.1.3 Fully Supervised Methods", "text": "Accumulating all features used by existing supervised methods and modifying them to suit specific datasets (where necessary), a fully supervised method is implemented using Support Vector Machines (SVM)1. Table 3.2 lists the most representative features that were used.\n1The weka.classifiers.functions.SMO classifier from the Weka toolkit (Hall et al., 2009) is used for implementing SVM.\nChapter 3. Description of Implemented Methods 19\nAlgorithm 2 Conversation model with Gaussian Mixtures Input: A list of threads T, each containing a list of posts P (in chronological order) Parameters: initialNumClusters, mergeInsertionStates, stateSizeThreshold, maxNumIterations, lmType, \u03b41, \u03b42, numMixtureComponents Output: A list of cluster labels CL for each post in each thread (in the order of the input)\n1: for all thread Tx do 2: for all post Px,y \u2208 Tx do 3: Vx,y := vectorize(Px,y) // Vx,y is the vector of post Px,y 4: end for 5: end for 6: ICL := cluster(V, initialNumClusters) // ICL is the list of initial cluster labels for each post\n(ICLx,y is the initial cluster label for post Px,y in thread Tx). 7: S := ICL // S is the list of states for all posts; at this step, it is the same as the initial cluster\nlabels. 8: for n = 1\u2192 maxNumIterations do 9: if mergeInsertionStates is true then\n10: [S, numStates] := merge small states(S, stateSizeThreshold) 11: end if 12: for all thread Tx do 13: for all post Px,y \u2208 Tx do 14: fit GMM(GSx,y , Px,y, numMixtureComponents) // G is the set of GMMs; Gi is the GMM for state i 15: end for 16: end for 17: for i = 1\u2192 numStates do 18: init countsi := \u03a3Tx1Sx,1 = i // Sx,1 is the state of the first post in thread Tx 19: end for 20: for i = 1\u2192 numStates do 21: \u03c0i := (init countsi + \u03b42)/(\u03a3k(init countsk) + \u03b42 \u00d7 numStates) // \u03c0i is the probability that initial state is i 22: end for 23: for i = 1\u2192 numStates do 24: for j = 1\u2192 numStates do\n25: trans countsi,j := \u2211\nTx |Tx|\u22121\u2211 a=1 1Sx,a = i, Sx,a+1 = j\n26: end for 27: end for 28: for i = 1\u2192 numStates do 29: for j = 1\u2192 numStates do 30: \u03c6i,j := (trans countsi,j + \u03b42)/(\u03a3k,l(trans countsk,l) + \u03b42 \u00d7 numStates2) // \u03c6i,j is the probability of transitioning from state i to state j 31: end for 32: end for 33: S := V iterbi algorithm(\u03c0, \u03c6,G) 34: if sum of observation probabilities converged then 35: break 36: end if 37: end for 38: CL := S\nChapter 3. Description of Implemented Methods 21"}, {"heading": "3.2 Proposed Methods", "text": ""}, {"heading": "3.2.1 Conversation Model with Part-of-Speech Tags", "text": "Since conversation models take only word n-gram language models into account, it is likely that they output clusters of posts that are topically related, without reflecting the posts\u2019 purpose or intention. To overcome this limitation, the conversation model is enhanced by modeling emissions as arising partially from part-of-speech (POS) tags of words. This might better characterize the syntactic nature of the post. This is based on the assumption that posts belonging to the same category are likely to be syntactically similar. The proposed model uses POS n-gram language models in addition to word n-gram language models, and calculates the HMM emission probability of a post given its state using a linear combination of both. Here, the probability of a post Pi, given a state Sk, is calculated as shown in Equation 3.3.\np(Pi|Sk) = \u220f\nj [\u03bb\u00d7 p(Wi,j|Lk) + (1\u2212 \u03bb)\u00d7 p(POSi,j|PLk)] Z\n0 \u2264 \u03bb \u2264 1\nZ = \u2211 i,k [\u220f j [\u03bb\u00d7 p(Wi,j|Lk) + (1\u2212 \u03bb)\u00d7 p(POSi,j|PLk)] ] (3.3)\nwhere:\nPOSi,j is the j th (in no particular order) POS n-gram in post Pi, PLk is the POS n-gram language model for state Sk, \u03bb is the parameter that controls the proportion of probability arising from the word and POS language models (using \u03bb = 1 is equivalent to the conversation model), and Z is the normalizing constant.\nChapter 3. Description of Implemented Methods 23"}, {"heading": "3.2.2 Conversation Model with Features", "text": "This model allows for the incorporation of discriminative features that might be useful for generating clusters that better represent the desired categories. For example, the chronological position of a post in a thread might be a useful feature, because a post is more likely to be a Problem if it is the first post in a thread as opposed to any other position. Here, the probability of a post Pi, given a state Sk, is calculated as shown in equation 3.4.\np(Pi|Sk) = \u220f j p(Wi,j|Lk) \u220f f p(Fi,f |FLk) (3.4)\nwhere:\nFi,f is the f th (in no particular order) discrete-valued feature in post Pi, and FLk is the feature model for state Sk.\nTable 3.3 lists the features used in this model. All feature values are discretized. These features comprise a small subset of those used in the fully supervised setup, and are relatively simpler and easier to obtain."}, {"heading": "3.2.3 Conversation Model with Post Embeddings", "text": "In the conversation models, the clustering of posts is performed as a first step using vectors of word n-grams in the post. This step may suffer from issues of sparsity and high vector dimensionality. To avoid this, it is proposed to use embeddings that are low-dimensional semantic representations of posts. Word2Vec2, with enhancements as proposed by Le and Mikolov (2014), can be used to generate embeddings of variable lengths of text. This technique uses a recurrent neural network that predicts a word given its surrounding context. For the current task, this technique is used to generate\n2http://code.google.com/p/word2vec/\nChapter 3. Description of Implemented Methods 24\none embedding per post, which can then be used for clustering. The rest of the model remains unchanged."}, {"heading": "3.2.4 Semi-supervised Conversation Model", "text": "As discussed before, semi-supervised techniques can make use of a minimal amount of labeled data in order to better guide the prediction of labels (as opposed to unlabeled clusters in case of unsupervised techniques). A modification can be made to the previous models to achieve this \u2014 the priors can be constructed from a small amount of labeled data instead of clustering all posts using vectors of post n-grams. More concretely, labeled data can be used to initialize the language models and the HMM parameters (initial state and state transition probabilities) for the first iteration of the EM algorithm. The rest of the model remains unaffected."}, {"heading": "3.2.5 Other Enhancements", "text": "All the models discussed above can be combined with one another, except in the case of semi-supervised models with post embeddings. This is because the semi-supervised models calculate priors from labeled data, whereas those with post embeddings use hierarchical clustering of unlabeled data.\nAlso, the following modifications can be made in an attempt to simplify the models and improve performance. The conversation models with POS tags require the setting of a configuration parameter which decides the proportion of probability that comes from language and POS models in the linear combination. Also, this parameter value (when fixed) is used uniformly across all word and POS n-grams. However, one could estimate a parameter value that is specific to a word and POS tag pair by using frequency counts from predicted labels during the previous iteration of the EM algorithm. In case of the first iteration of the unsupervised models, the frequency counts can be calculated using the initial cluster labels; and in case of semi-supervised models, this can be done using\nChapter 3. Description of Implemented Methods 25\nthe labels of the training data. Equation 3.5 can be used to calculate the fractional contribution of a word in the language model Lk for state Sk, and equation 3.6 can be used analogously for calculating the fractional contribution of a POS tag. Equation 3.7 can be used to determine the value of \u03bb, which can then be used in the conversation model with POS tags, as shown in equation 3.8.\nDiscussion forum posts often contain informal text with misspellings and spelling variations, which cannot be modeled by word n-gram language modeling. However, character n-grams could potentially overcome this limitation. Also, they have been a very useful discriminative feature in the area of authorship attribution, because they seem to account for lexical, syntactic, and stylistic information (Sapkota et al., 2015). Hence, character n-gram language models can be used in isolation or in addition to word n-gram language models in each of the models discussed in previous sub-sections.\nWordFrac(Lk, w) = Frequency of w in posts from state Sk\nTotal frequency of w (3.5)\nPosFrac(PLk, pos) = Frequency of pos in posts from state Sk\nTotal frequency of pos (3.6)\n\u03bb(w, pos, k) = WordFrac(Lk, w)\nWordFrac(Lk, w) + PosFrac(PLk, pos) (3.7)\np(Pi|Sk) = \u220f\nj [\u03bb(Wi,j, POSi,j, k)\u00d7 p(Wi,j|Lk) + (1\u2212 \u03bb(Wi,j, POSi,j, k))\u00d7 p(POSi,j|PLk)] Z\nZ = \u2211 i,k [\u220f j [\u03bb(Wi,j, POSi,j, k)\u00d7 p(Wi,j|Lk) + (1\u2212 \u03bb(Wi,j, POSi,j, k))\u00d7 p(POSi,j|PLk)] ] (3.8)\nChapter 3. Description of Implemented Methods 26"}, {"heading": "3.3 Mapping of Clusters to Categories", "text": "Unsupervised methods output cluster labels for each post (and not a specific category label). In order to match them with an observed category label, a one-to-one mapping is obtained using Kuhn-Munkres algorithm for maximal weighting in a bipartite graph (Kuhn, 1955; Munkres, 1957). In this procedure, one set of disjoint nodes of the bipartite graph corresponds to the set of predicted cluster labels, and the other set corresponds to the set of manually obtained gold labels. The weight of an edge from cluster label c to gold label g is calculated as the number of posts which are predicted as c and also have a gold label g. Joty et al. (2011) follow the same procedure.\nChapter 4\nData Collection and Annotation\nPrevious work has used forum datasets belonging to the travel and computer-related technical domains (listed in Table 4.1).\nIn addition to these, the current work attempts to observe the performance of post categorization on forums belonging to the automotive domain. For this purpose, forums that discuss Jeep and Mercedes-Benz vehicles were obtained from Verticalscope\n27\nInc.1 Around 150 threads each were randomly picked from JeepForum2 and BenzWorld3. Threads whose first posts contained advertisements or spam posts (as identified by Topic Modeling done previously) were filtered out. Also, threads which had only one post or more than 30 posts, were discarded. This resulted in a total of 93 threads in the JeepForum dataset, and 108 threads in the Benzworld dataset.\nNext, previous literature was studied in order to decide the tagset of categories to annotate the forum posts in the dataset. Kim et al. (2010) use a tagset of 12 categories \u2014 Question, Question-Add, Question-Confirmation, Question-Correction, Answer, Answer-Add, Answer-Confirmation, Answer-Correction, Answer-Objection, Resolution, Reproduction, and Other. Since this is the most fine-grained set of categories, a pilot annotation study was conducted using these. Five annotators annotated posts from six randomly picked threads in the automotive domain. Based on the quantitative results of the annotation and the feedback from annotators, it was observed that using a more\n1Verticalscope Inc. (http://www.verticalscope.com) is a privately held corporation that specializes in the acquisition and development of websites and online communities for the Automotive, Powersports, Power Equipment, Pets, Sports and Technology vertical markets.\n2jeepforum.com 3benzworld.org\nChapter 4. Data Collection and Annotation 29\ncoarse-grained set of six categories would be simpler and more meaningful. These categories and their description are shown in Table 4.2.\nThe main annotation task was set up on CrowdFlower4, a Web platform for obtaining crowdsourced annotations. In each thread, the posts were displayed to the annotators in chronological order. Some posts contain quoted text, i.e., a span of text from a previously posted answer. These were enclosed within \u2018[QUOTE]\u2019 tags along with the username of the post which is quoted. Some posts contain URLs or images, which were displayed to the annotators using the tags \u2018[URL]\u2019 and \u2018[IMG]\u2019 respectively. In addition to providing the target set of annotation categories, the following instructions were provided to the annotators.\n4http://www.crowdflower.com/\nChapter 4. Data Collection and Annotation 30\nThe top three trusted annotators were picked from each annotation task, and gold labels were assigned to each post if at least two out of three annotators agreed. However, if there was disagreement among all annotators, the post was left unlabeled. Details of the resulting datasets, including the inter-annotator agreements and quantity of data, are shown in Table 4.3. Since the annotations were crowdsourced, there is no common set of annotators for each post. Hence, instead of using standard annotation quality measures like Scott\u2019s \u03c0 and Fleiss\u2019s \u03ba, Krippendorf\u2019s \u03b1 is reported, which can account for missing values (Artstein and Poesio, 2008). The values obtained (i.e., 0.62 and 0.47) are reflective of moderate to substantial agreement. In order to further confirm the validity of the annotations, two annotators randomly sampled 10% of the threads and\nChapter 4. Data Collection and Annotation 31\nmanually analyzed the annotations for correctness. They found 98.8% and 91.8% of posts to be correctly annotated in the JeepForum and BenzWorld datasets respectively.\nThe category-wise distribution of posts for both datasets are shown in Figures 4.1 and 4.2. Solution and Other are the most prevalent categories, whereas Clarification-Request and Clarification form only 8-9% of the posts. Consequently, the former two categories are expected to be easier to classify (i.e., achieve better accuracy in classification) in comparison to the latter two.\nChapter 5\nExperiments"}, {"heading": "5.1 Evaluation Measures", "text": "The predicted labels for all posts can be evaluated against the corresponding gold labels using metrics like precision, recall and F1-measure. Moreover, micro-averaged and macroaveraged values of these metrics can indicate overall performance across categories. All evaluation metrics are calculated as shown in Equations 5.1 to 5.12. In all cases, c is a single category, and CS is the set of all categories. The values of micro-averaged precision, recall and F1-measure are all equal if the number of predictions is the same as the number of posts (i.e., every post is predicted as belonging to some category). All methods implemented in the current work make some category prediction for every post; hence, this condition holds true.\nAccuracy, A(c) = # actual c posts predicted as c + # actual non-c posts predicted as non-c\n# predictions\n(5.1)\nPrecision, P (c) = # actual c posts predicted as c\n# posts predicted as c (5.2)\nRecall, R(c) = # c posts predicted as c\n# actual c posts (5.3)\nF1-Measure, F (c) = 2\u00d7 P \u00d7R P +R\n(5.4)\n32\nChapter 5. Experiments 33\nMicro-Averaged-Accuracy, MicroA = \u03a3c\u2208CS [# actual c posts predicted as c]\n# predictions (5.5)\nMicro-Averaged-Precision, MicroP = MicroA (5.6)\nMicro-Averaged-Recall, MicroR = \u03a3c\u2208CS [# c posts predicted as c]\n# posts (5.7)\nMicro-Averaged-F1-Measure, MicroF = 2\u00d7MicroP \u00d7MicroR MicroP +MicroR\n(5.8)\nMacro-Averaged-Accuracy, MacroA = \u03a3c\u2208CS [Accuracy(c)]\n|CS| (5.9)\nMacro-Averaged-Precision, MacroP = \u03a3c\u2208CS [Precision(c)]\n|CS| (5.10)\nMacro-Averaged-Recall, MacroR = \u03a3c\u2208CS [Recall(c)]\n|CS| (5.11)\nMacro-Averaged-F1-Measure, MacroF = 2\u00d7MacroP \u00d7MacroR MacroP +MacroR\n(5.12)"}, {"heading": "5.2 Experimental Setup", "text": ""}, {"heading": "5.2.1 Preprocessing and Configuration Parameters", "text": "Initially, all forum posts were tokenized by sentence and word, followed by POS tagging and stemming \u2014 all using Stanford CoreNLP Toolkit (Manning et al., 2014). Stopword removal was found to degrade performance; hence, it was not used. It is important to note that forum conversations often consist of informal English language text, along with the use of domain-specific abbreviations, and non-standard special characters, such as ellipses and emoticons. Hence, some errors are introduced in all the previous steps. However, no effort was made to overcome them, and this is accepted as a limitation of the current work.\nAll methods, except those using post embeddings, require the conversion of posts to vectors of n-grams. For this purpose, both unigrams and bigrams were tried, and the former was found to produce better performance. The use of TF-IDF term weighting did\nChapter 5. Experiments 34\nnot improve performance; hence, it was ignored. The maximum number of iterations of Expectation Maximization was set to 100, which was sufficient because all experimental runs were completed in fewer than 100 iterations. The values of both smoothing parameters (i.e., delta1 and delta2 ) were varied in the range of 10\u22121 to 10\u22129. Subsequently, 10\u22122 and 10\u22129 were found to be the best values for delta1 and delta2 respectively. The value of the POS model\u2019s \u03bb was varied between 10\u22126 and 1\u2212 10\u22126, and the value of 0.999 was found to be the best. Since the unigram/bigram vocabulary size is much larger than the POS tag vocabulary size, the former probability distribution is much more fine-grained. For example, each word unigram\u2019s probability value in the Benzworld dataset is of the order of 10\u22124 (since the unigram vocabulary size is 5000), whereas each POS unigram\u2019s probability value is of the order of 10\u22122 (since the POS vocabulary size is 42). So, the value of 0.999 for word unigrams and 0.001 for POS unigrams can be viewed as a scaling factor to ensure that both contribute almost equally towards discriminating between post categories. To provide further clarity, using a \u03bb value of 0.5 gives rise to a predominantly POS-based model because unigram probability values are too low to make a significant difference towards identifying one category over another. The parameters, initialNumClusters and stateSizeThreshold, directly affect the resulting number of clusters. In all experimental runs, both these parameters were varied in the range of 1 to 100, and those which did not output the desired number of clusters (i.e., number of distinct gold labels) were ignored. In each case, different parameter values were best suited; however, only the best performing results are reported. For GMM-based methods, the number of Gaussian mixture components was varied from 2 to 8, and 3 was found to be the best value. Parameters specific to GMM, such as initial mixture component means and variances, were initialized randomly by sampling from the Gaussian distribution.\nFor semi-supervised methods, experiments were carried out in a randomized n-fold cross-validation setup. The dataset was randomly (by sampling from the uniform distribution) divided into n equal-sized folds, and the experiment was run n times. In each\nChapter 5. Experiments 35\nrun, one fold was used for initializing the priors of the models, and the remaining n\u2212 1 folds were used for evaluation. This is in contrast to a traditional fully supervised setting, where n\u2212 1 folds are used for training and the remaining fold is used for evaluation.\nIn the case of language models, it was observed that accuracy values differ by more than two percentage points when using unigram and bigram language models. Also, different datasets benefited from different models. Hence, experiments were run using both, and results are reported for the better performing alternative.\nA number of enhancements were proposed in section 3.2.5 with the objective of further enhancing the performance of the conversation models. However, in all cases, these led to deteriorating performance. Specifically, the use of character or skip-gram language models in isolation or in conjunction with word language and POS models lowered performance by around 2 percentage points with respect to the best performing method. The use of fractional contributions of language and POS modeling led to performance deterioration of up to 10 percentage points. Hence, these enhancements are ignored when reporting results."}, {"heading": "5.2.2 Baselines", "text": "The random baseline randomly (by sampling from the uniform distribution) assigns category labels to every post. The majority baseline assigns the most commonly occurring gold category label to every post. In all datasets on which results are reported, Solution is the most commonly occurring gold category.\nTwo other baselines are heuristic in nature, and are both based on the assumption that the first post in the thread is very likely to be a Problem. The first of these, called Problem-Solution Heuristic 1, assigns Problem to the first post in the thread, Other to the last post, and Solution to the rest. It assumes that the last post in the thread is very likely to be unrelated to the main thread topic and that many of the preceding posts are likely to be Solution. The second heuristic baseline, called Problem-Solution Heuristic 2,\nChapter 5. Experiments 36\nassigns Problem to the first post in the thread, Solution to the second post, and Other to the rest. It assumes that the second post is very likely to be a Solution in direct response to the first Problem post, and many of the following posts are likely to be Other.\nChapter 5. Experiments 37"}, {"heading": "5.3 Main Results", "text": "Table 5.1 lists the micro and macro-averaged accuracy values when experiments were run using all possible combinations of the implemented models.\nFor reported results of unsupervised methods, different values of parameters, initialNumClusters and stateSizeThreshold, were used in each case. This is because the same values did not lead to the desired number of clusters. For example, for the JeepForum dataset, the conversation model\u2019s parameters were: initialNumClusters = 30 and stateSizeThreshold = 25. This resulted in six clusters, the same as the number of gold label categories. However, the same parameters yielded a very large number of clusters (15) when used with the conversation model with post embeddings. Only the best performing results are reported. In case of methods using GMM, since parameters were randomly initialized, fluctuations in performance are expected across different runs. Hence the reported accuracy values are averages over 10 runs. For the JeepForum dataset, unsupervised methods reached maximum micro-averaged and macro-averaged accuracy values using conversation models with post embeddings, POS tags, and features. However, for the BenzWorld dataset, the performance was the best using conversation models with GMM and features. All unsupervised methods outperformed the random baseline. But they performed worse than the majority baseline in many cases, and the problem-solution heuristic baselines in all cases.\nFor semi-supervised methods, the reported accuracy values are averages over 10 runs of 5-fold cross-validation. This setup entails the use of only around 20 labeled threads for setting the model priors, because both datasets contain approximately 100 threads. The GMM-based semi-supervised methods performed only as well as their unsupervised counterparts. For the JeepForum dataset, semi-supervised methods which used POS tags and/or features in the absence of GMM, outperformed all baselines. For the BenzWorld dataset, the same is true, except in case of the conversation model with POS tags, which performed worse than problem-solution heuristic 1. Overall, the methods using both\nPOS tags and features performed the best. For the JeepForum dataset, the best microaveraged and macro-averaged accuracy values are 0.54 and 0.85 respectively. In case of the BenzWorld dataset, the same accuracy values are 0.52 and 0.84 respectively."}, {"heading": "5.4 Performance Comparison with State-of-the-Art", "text": ""}, {"heading": "5.4.1 Unsupervised HMM+Mix Model", "text": "Joty et al. (2011) reported results of their best performing HMM+Mix model for dialogue act classification on email and forum thread datasets, neither of which are available to other researchers. Their forum thread dataset contains 200 threads sourced from TripAdvisor (for which they report a macro-accuracy value of 78.35%). Hence, for performance comparison, the current work also used a dataset of nearly 200 threads from TripAdvisor (made available by Bhatia et al. (2012)). As a caveat, it is important to note that this dataset has eight dialogue act categories, whereas Joty et al. (2011) consider 12. Also, the current work\u2019s conversation model with GMM (called HMM+Mix++) was used for performance comparison, since it is an improved adaptation of the HMM+Mix model. Table 5.2 shows that the proposed conversation model with POS tags and features outperformed HMM+Mix++ in terms of macro-accuracy values. Also, the semi-supervised conversation model with POS tags and features performed much better (0.92 on NYC and 0.90 on Ubuntu); but this is not directly comparable since the other methods are unsupervised.\nChapter 5. Experiments 39"}, {"heading": "5.4.2 Semi-supervised Answer Extraction", "text": "Catherine et al. (2013) reported the performance of their semi-supervised answer extraction approach on 300 labeled threads of the Apple discussion forums dataset. They trained using only three training threads; however, these three are not available to other researchers. The code is also unavailable. Hence, for the sake of simplicity, the methods are indirectly compared as follows. For their method, values reported in their paper are used as is. For the best proposed method (i.e., the semi-supervised conversation models with POS tags and features), a 100-fold cross-validation setup was used (i.e., out of 300 labeled threads, 3 were used for training, and 297 were used for testing, in each fold). Table 5.3 shows that the values obtained for the proposed method are better in terms of F1-measure and precision."}, {"heading": "5.5 Category-wise Performance and Error Analysis", "text": "Table 5.4 shows the category-wise performance of one of the runs of 5-fold cross-validation for both the JeepForum and Benzworld datasets using the semi-supervised conversation model with POS tags and features. This method outperformed the problem-solution heuristic baseline for every category except Problem. Table 5.5 shows the confusion matrix of the same experimental fold using the JeepForum dataset. The confusion matrix for the BenzWorld dataset is similar. The most common error was the prediction of a non-Solution category as Solution, indicating a bias of the method towards predicting\nChapter 5. Experiments 40\nthe majority category. This also happened in the case of Other, but to a lesser extent. In addition, Clarification-Request was often predicted as Problem, and Clarification was predicted as Solution. This seems to occur because Clarification-Request and Clarification can be understood as specific types of Problem and Solution posts. Overall, the predictions of minority categories are not practically useful, because they were less accurate than the predictions using the random baseline. Since previous literature ignores the analysis of category-wise performance altogether, a direct comparison is not possible.\nIn order to analyze the performance of only the Problem and Solution categories, another setup was used where all other categories were coalesced into Other. Results of the coarse-grained classification setup are shown in Table 5.6. As compared to the fine-grained classification setup, there is no significant change in F1-measure values for Problem and Solution. Also, this setup performed only as well as or slightly worse than the corresponding best problem-solution heuristic. However, the performance was much better than the baseline for Solution.\nChapter 5. Experiments 41\nPredicted"}, {"heading": "5.6 Effect of the Amount of Training Data", "text": "One important measure of the quality of a learning algorithm is whether its performance increases with increasing amount of training data. To evaluate this, multiple n-fold cross-validation experiments were conducted with decreasing value of n, i.e., increasing number of training threads. Figure 5.1 demonstrates that the performance of the semisupervised conversation model with POS tags and features on the JeepForum dataset increased as the number of folds decreased. The micro-accuracy value is 0.50 using 10-\nChapter 5. Experiments 42\nfold cross-validation, which demonstrates that around 9 labeled threads1 are enough to reasonably predict categories for unseen threads. Similar effects are seen in the case of other datasets; but to avoid redundancy, they are not shown here. The same method in a fully supervised setup performed even better. Table 5.7 shows its performance in a 10-fold cross-validation setup as compared to an equivalent setup that used SVMs. Despite the fact that the latter used many more features as well as feature selection2, its performance was similar. The features used in both cases were previously described in section 3."}, {"heading": "5.7 Summary of Experimental Results", "text": "Experimental results indicate that purely unsupervised methods are not adequate for tackling a task as complex as forum post categorization. However, they are able to capture some sequential dependencies, as observed from the fact that they outperformed two trivial baselines (i.e., the random and majority baselines). Using post embeddings (which\n1The JeepForum dataset contains 93 threads, only 1/10th of which were used in a single fold of 10-fold cross-validation.\n2Feature selection was done using information gain based attribute evaluation.\nis still purely unsupervised), the performance did not conclusively improve. But knowledge of POS tags and simple textual features provided more context for classification, and thus, enabled the technique to classify more accurately.\nThe novel proposal of incorporating a few labeled examples for initializing the model priors led to better performance than the problem-solution heuristic baselines in most cases. Direct comparison with existing methods is not possible due to limitations in availability of data and code. However, approximate comparison setups demonstrate the better performance of proposed methods. Prediction of Problem and Solution categories were the most accurate, followed by Other and Feedback. However, predictions of the minority categories, Clarification-Request and Clarification, were not accurate enough to be practically useful, since the maximum accuracy value is 0.30.\nChapter 6\nConclusions and Future Work"}, {"heading": "6.1 Conclusions", "text": "This paper described the problem of forum post categorization, and discussed the need for automatic methods to solve it. The relevant previous work was presented and an argument was made for the need for unsupervised and semi-supervised methods to solve the problem. Subsequently, methods were proposed for categorizing forum posts using sequence models, which distinguish between categories, using language models based on word and part-of-speech probability distributions, in addition to manually specified features. The unsupervised methods include the novel application of conversation models that were previously proposed for other tasks. Although the experimental results demonstrate that they are not practically useful, they are shown to perform better than previously proposed methods. Hence, it can be safely concluded that the current unsupervised methods are not robust enough to capture the complexity of forum post categorization. Next, it was proposed to use a novel semi-supervised version of the earlier methods by employing a few labeled threads to guide the process. Experimental results demonstrate that these methods outperformed all the baselines. Also, an indirect comparison with a semi-supervised method proposed in previous work, demonstrates better performance.\n44\nChapter 6. Conclusions and Future Work 45"}, {"heading": "6.2 Future Work", "text": "Discussion forum posts often contain multiple dialogue categories , i.e., a post could start with a Solution to a previous Problem, and end with a new Problem posed for users to discuss in future posts. In such cases, the post is annotated with a single representative category. Although this might be straightforward for human annotators, the proposed methods have no intuition about this. Hence, it might be useful to employ summarization, so as to retain the overall meaning of the post, and cut out the parts that are not representative. Such methods need only classify the relevant text in the post and might perform better. This problem could also be tackled by classifying individual sentences in posts, rather than the post as a whole. This could be done in a two-tier HMM setup where the first level comprises sentence classification, and the second level comprises of post classification. However, this proposal is dependent on the availability of datasets that are annotated by category at the sentence level. Instead, majority voting or other heuristics could be employed to pool the predicted categories of individual sentences into a single post category.\nSince all the proposed methods employ first-order HMMs, they lack the knowledge of long-range dependencies between different categories. Consequently, they are unable to learn that a post can not be classified as Solution, without a Problem post before it. This problem can be addressed by using higher-order Markov chains, but it would lead to much greater run-time and space complexity. Instead, the use of heuristics to flag certain categories, based on prior post categories in the thread, could resolve this problem more efficiently.\nComparison of fine-grained and coarse-grained classification results indicates that Clarification-Request and Clarification categories are not easy to identify. Since interannotator agreement values for these two categories are also the least among all categories, it seems that manual identification is also not easy. Hence, future work should\nChapter 6. Conclusions and Future Work 46\neither discard these categories or use them for annotation in a controlled setting with trained expert annotators, as opposed to crowdsourcing."}], "references": [{"title": "Inter-coder agreement for computational linguistics", "author": ["Ron Artstein", "Massimo Poesio"], "venue": "Computational Linguistics,", "citeRegEx": "Artstein and Poesio.,? \\Q2008\\E", "shortCiteRegEx": "Artstein and Poesio.", "year": 2008}, {"title": "Catching the drift: Probabilistic content models, with applications to generation and summarization", "author": ["Regina Barzilay", "Lillian Lee"], "venue": "In Proceedings of the 2nd Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Barzilay and Lee.,? \\Q2004\\E", "shortCiteRegEx": "Barzilay and Lee.", "year": 2004}, {"title": "Classifying user messages for managing web forum data", "author": ["Sumit Bhatia", "Prakhar Biyani", "Prasenjit Mitra"], "venue": "In Proceedings of the 15th International Workshop on the Web and Databases", "citeRegEx": "Bhatia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bhatia et al\\.", "year": 2012}, {"title": "Does similarity matter? The case of answer extraction from technical discussion forums", "author": ["Rose Catherine", "Amit Singh", "Rashmi Gangadharaiah", "Dinesh Raghu", "Karthik Visweswariah"], "venue": "In Proceedings of the 24th International Conference on Computational Linguistics (COLING),", "citeRegEx": "Catherine et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Catherine et al\\.", "year": 2012}, {"title": "Semi-supervised answer extraction from discussion forums", "author": ["Rose Catherine", "Rashmi Gangadharaiah", "Karthik Visweswariah", "Dinesh Raghu"], "venue": "In Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP),", "citeRegEx": "Catherine et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Catherine et al\\.", "year": 2013}, {"title": "Finding question-answer pairs from online forums", "author": ["Gao Cong", "Long Wang", "Chin-Yew Lin", "Young-In Song", "Yueheng Sun"], "venue": "In Proceedings of the 31st Annual International ACM SIGIR Conference,", "citeRegEx": "Cong et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cong et al\\.", "year": 2008}, {"title": "Unsupervised classification of dialogue acts using a Dirichlet process mixture model", "author": ["Nigel Crook", "Ramon Granell", "Stephen Pulman"], "venue": "In Proceedings of the 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Crook et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Crook et al\\.", "year": 2009}, {"title": "Unsupervised solution post identification from discussion forums", "author": ["P Deepak", "Karthik Visweswariah"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Deepak and Visweswariah.,? \\Q2014\\E", "shortCiteRegEx": "Deepak and Visweswariah.", "year": 2014}, {"title": "Using conditional random fields to extract contexts and answers of questions from online forums", "author": ["Shilin Ding", "Gao Cong", "Chin-yew Lin Xiaoyan"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Ding et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2008}, {"title": "The WEKA data mining software: an update", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H Witten"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Semi-supervised speech act recognition in emails and forums", "author": ["Minwoo Jeong", "CY Lin", "GG Lee"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Jeong et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jeong et al\\.", "year": 2009}, {"title": "Unsupervised modeling of dialog acts in asynchronous conversations", "author": ["Shafiq Joty", "Giuseppe Carenini", "Chin Yew Lin"], "venue": "In Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Joty et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Joty et al\\.", "year": 2011}, {"title": "Tagging and linking web forum posts", "author": ["Su Nam Kim", "Li Wang", "Timothy Baldwin"], "venue": "In Proceedings of the 14th Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Kim et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2010}, {"title": "The Hungarian method for the assignment problem", "author": ["Harold W Kuhn"], "venue": "Naval Research Logistics Quarterly,", "citeRegEx": "Kuhn.,? \\Q1955\\E", "shortCiteRegEx": "Kuhn.", "year": 1955}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Algorithms for the assignment and transportation problems", "author": ["James Munkres"], "venue": "Journal of the Society for Industrial and Applied Mathematics,", "citeRegEx": "Munkres.,? \\Q1957\\E", "shortCiteRegEx": "Munkres.", "year": 1957}, {"title": "Finding problem solving threads in online forum", "author": ["Zhonghua Qu", "Yang Liu"], "venue": "In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP),", "citeRegEx": "Qu and Liu.,? \\Q2011\\E", "shortCiteRegEx": "Qu and Liu.", "year": 2011}, {"title": "Unsupervised modeling of Twitter conversations", "author": ["Alan Ritter", "Colin Cherry", "Bill Dolan"], "venue": "In Proceedings of the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Ritter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Not all character ngrams are created equal: a study in authorship attribution", "author": ["Upendra Sapkota", "Steven Bethard", "Manuel Montes-y G\u00f3mez"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Sapkota et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sapkota et al\\.", "year": 2015}, {"title": "Shallow information extraction from medical forum data", "author": ["Parikshit Sondhi", "Manish Gupta", "Chengxiang Zhai", "Julia Hockenmaier"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),", "citeRegEx": "Sondhi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sondhi et al\\.", "year": 2010}, {"title": "Dialogue act modeling for automatic tagging and recognition of conversational speech", "author": ["Andreas Stolcke", "Klaus Ries", "Noah Coccaro", "Elizabeth Shriberg", "Rebecca Bates", "Daniel Jurafsky", "Paul Taylor", "Rachel Martin", "Carol Van Ess-Dykema", "Marie Meteer"], "venue": "Computational Linguistics,", "citeRegEx": "Stolcke et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Stolcke et al\\.", "year": 2000}, {"title": "Thread-level analysis over technical user forum data", "author": ["Li Wang", "Su Nam Kim", "Timothy Baldwin"], "venue": "In Proceedings of the Australasian Language Technology Association Workshop,", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Predicting thread discourse structure over technical web forums", "author": ["Li Wang", "Marco Lui", "Su Nam Kim", "Joakim Nivre", "Timothy Baldwin"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 21, "context": "It is closely related to the problem of dialogue act tagging, which is defined as the identification of the meaning of an utterance at the level of illocutionary force (Stolcke et al., 2000), i.", "startOffset": 168, "endOffset": 190}, {"referenceID": 2, "context": "(2010) Tagset: Question, Question-Add, Question-Confirmation, Question-Correction, Answer, Answer-Add, Answer-Confirmation, Answer-Correction, Answer-Objection, Resolution, Reproduction, Other Classifier: CRF Features: Lexical, structural, post context, semantic Dataset: CNET (computer-related technical domain) Qu and Liu (2011) Tagset: Problem, Solution, Good Feedback, Bad Feedback Classifier: HMM Features: Bag-of-words Dataset: Oracle database (computer-related technical domain) Catherine et al. (2012) Tagset: Answer Classifier: SVM Features: Structural, syntactic, author authority, post ratings Dataset: Apple (computer-related technical domain) Bhatia et al.", "startOffset": 486, "endOffset": 510}, {"referenceID": 2, "context": "(2012) Tagset: Answer Classifier: SVM Features: Structural, syntactic, author authority, post ratings Dataset: Apple (computer-related technical domain) Bhatia et al. (2012) Tagset: Question, Repeat Question, Clarification, Solution, Further Details, Positive Feedback, Negative Feedback, Spam Classifiers: SVM, logit model Features: Structural, content, sentiment, number of posts by user, user authority Datasets: Ubuntu (computer-related technical domain), TripAdvisor-NYC (travel domain)", "startOffset": 153, "endOffset": 174}, {"referenceID": 2, "context": "Catherine et al. (2012) employed Support Vector Machines (SVMs) to extract Answer posts in a thread (assuming that the first post in the thread is a Question).", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Bhatia et al. (2012) used supervised machine learning algorithms (i.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Bhatia et al. (2012) used supervised machine learning algorithms (i.e., SVMs, logit model classifier, naive Bayes, etc.) to classify forum posts into eight categories \u2014 Question, Repeat Question, Clarification, Further Details, Solution, Positive Feedback, Negative Feedback, and Junk. They evaluated their methods on a dataset of the Ubuntu forums. Qu and Liu (2011) used Hidden Markov Models (HMMs) to classify forum posts into four categories \u2014 Problem, Solution, Good Feedback and Bad Feedback.", "startOffset": 0, "endOffset": 368}, {"referenceID": 2, "context": "Bhatia et al. (2012) used supervised machine learning algorithms (i.e., SVMs, logit model classifier, naive Bayes, etc.) to classify forum posts into eight categories \u2014 Question, Repeat Question, Clarification, Further Details, Solution, Positive Feedback, Negative Feedback, and Junk. They evaluated their methods on a dataset of the Ubuntu forums. Qu and Liu (2011) used Hidden Markov Models (HMMs) to classify forum posts into four categories \u2014 Problem, Solution, Good Feedback and Bad Feedback. They evaluated their methods on the Oracle database support forums. Similarly, Wang et al. (2010) attempted to identify Problem and Solution posts in the CNET forums dataset, but with more fine-grained categories based on the types of Problem posts (i.", "startOffset": 0, "endOffset": 597}, {"referenceID": 2, "context": "Bhatia et al. (2012) used supervised machine learning algorithms (i.e., SVMs, logit model classifier, naive Bayes, etc.) to classify forum posts into eight categories \u2014 Question, Repeat Question, Clarification, Further Details, Solution, Positive Feedback, Negative Feedback, and Junk. They evaluated their methods on a dataset of the Ubuntu forums. Qu and Liu (2011) used Hidden Markov Models (HMMs) to classify forum posts into four categories \u2014 Problem, Solution, Good Feedback and Bad Feedback. They evaluated their methods on the Oracle database support forums. Similarly, Wang et al. (2010) attempted to identify Problem and Solution posts in the CNET forums dataset, but with more fine-grained categories based on the types of Problem posts (i.e., Hardware, Software, Media, OS, Network, and Programming) and Solution posts (i.e., Documentation, Install, Search, and Support). Kim et al. (2010) worked on the same dataset, and attempted to classify posts into 12 categories that are similar to the ones used by Bhatia et al.", "startOffset": 0, "endOffset": 902}, {"referenceID": 2, "context": "Bhatia et al. (2012) used supervised machine learning algorithms (i.e., SVMs, logit model classifier, naive Bayes, etc.) to classify forum posts into eight categories \u2014 Question, Repeat Question, Clarification, Further Details, Solution, Positive Feedback, Negative Feedback, and Junk. They evaluated their methods on a dataset of the Ubuntu forums. Qu and Liu (2011) used Hidden Markov Models (HMMs) to classify forum posts into four categories \u2014 Problem, Solution, Good Feedback and Bad Feedback. They evaluated their methods on the Oracle database support forums. Similarly, Wang et al. (2010) attempted to identify Problem and Solution posts in the CNET forums dataset, but with more fine-grained categories based on the types of Problem posts (i.e., Hardware, Software, Media, OS, Network, and Programming) and Solution posts (i.e., Documentation, Install, Search, and Support). Kim et al. (2010) worked on the same dataset, and attempted to classify posts into 12 categories that are similar to the ones used by Bhatia et al. (2012). Additionally, they tagged the links between posts, i.", "startOffset": 0, "endOffset": 1039}, {"referenceID": 3, "context": "According to Catherine et al. (2012), author authority is a numerical or categorical value that is indicative of an author\u2019s level of expertise in the context of the forum.", "startOffset": 13, "endOffset": 37}, {"referenceID": 20, "context": "Wang et al. (2011) went one step further by jointly classifying both posts and the links between them.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "Ding et al. (2008) used CRFs to identify Answer posts and the context in which they answered the Question post.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "Ding et al. (2008) used CRFs to identify Answer posts and the context in which they answered the Question post. However, they did not attempt to identify Question posts, because they were assumed to be known beforehand. Their techniques were evaluated on a corpus of the TripAdvisor forums. Sondhi et al. (2010) used CRFs and SVMs with various semantic and structural features to identify Medical Problem and Treatment in the HealthBoards forums.", "startOffset": 0, "endOffset": 312}, {"referenceID": 5, "context": "Cong et al. (2008) used labeled sequential patterns to identify Question posts, followed by a graph-based propagation method to extract corresponding Answer posts.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "Other unsupervised techniques have been employed for the related tasks of dialogue act classification in spoken dialogue systems (Crook et al., 2009) and Twitter conversations (Ritter et al.", "startOffset": 129, "endOffset": 149}, {"referenceID": 18, "context": ", 2009) and Twitter conversations (Ritter et al., 2010).", "startOffset": 34, "endOffset": 55}, {"referenceID": 6, "context": "Deepak and Visweswariah (2014) identified Solution posts using a translation-based model that leverages lexical correlations between Problem and Solution posts.", "startOffset": 0, "endOffset": 31}, {"referenceID": 6, "context": "Deepak and Visweswariah (2014) identified Solution posts using a translation-based model that leverages lexical correlations between Problem and Solution posts. Joty et al. (2011) used a combination of HMMs and Gaussian Mixture Models (GMMs) in order to classify forum posts into 12 dialogue act categories.", "startOffset": 0, "endOffset": 180}, {"referenceID": 10, "context": "One employed domain adaptation from labeled spoken dialogue datasets by means of a sub-tree pattern mining algorithm (Jeong et al., 2009).", "startOffset": 117, "endOffset": 137}, {"referenceID": 4, "context": "Another method extracted Answer posts in forum threads using a co-training framework (Catherine et al., 2013).", "startOffset": 85, "endOffset": 109}, {"referenceID": 1, "context": "Barzilay and Lee (2004) proposed a content model for multi-document summarization based on sentence extraction.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Barzilay and Lee (2004) proposed a content model for multi-document summarization based on sentence extraction. This model consists of an HMM at the sentence level that is tailored towards identifying sentence clusters belonging to different topics. Inspired by this model, Ritter et al. (2010) suggested a \u2018conversation model\u2019 for the modeling of dialogue acts in Twitter conversations.", "startOffset": 0, "endOffset": 295}, {"referenceID": 1, "context": "Barzilay and Lee (2004) proposed a content model for multi-document summarization based on sentence extraction. This model consists of an HMM at the sentence level that is tailored towards identifying sentence clusters belonging to different topics. Inspired by this model, Ritter et al. (2010) suggested a \u2018conversation model\u2019 for the modeling of dialogue acts in Twitter conversations. Their model replicates Barzilay\u2019s model but replaces sentences in a document with tweets in a Twitter conversation as units of the HMM. They used Topic Modeling (using Latent Dirichlet Allocation) along with the conversation model and reported better performance; but the evaluation was done only qualitatively. Similarly, Joty et al. (2011) applied conversation models to email and forum threads where a single post is considered an HMM unit.", "startOffset": 0, "endOffset": 730}, {"referenceID": 1, "context": "While there are three different variants of this model (as described in the previous chapter), this work implements the originally proposed model by Barzilay and Lee (2004), while making necessary modifications for applying it to forum post categorization.", "startOffset": 149, "endOffset": 173}, {"referenceID": 17, "context": "1 (derived from Ritter et al. (2010) and Joty et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 11, "context": "(2010) and Joty et al. (2011)).", "startOffset": 11, "endOffset": 30}, {"referenceID": 18, "context": "Also, in contrast to the Topic Model-based approach (Ritter et al., 2010), learning and inference can be done using the EM algorithm without approximate inference techniques.", "startOffset": 52, "endOffset": 73}, {"referenceID": 11, "context": "To counter this, Joty et al. (2011) proposed a method which models the HMM emissions as a mixture of Gaussians, i.", "startOffset": 17, "endOffset": 36}, {"referenceID": 9, "context": "SMO classifier from the Weka toolkit (Hall et al., 2009) is used for implementing SVM.", "startOffset": 37, "endOffset": 56}, {"referenceID": 14, "context": "Word2Vec, with enhancements as proposed by Le and Mikolov (2014), can be used to generate embeddings of variable lengths of text.", "startOffset": 43, "endOffset": 65}, {"referenceID": 19, "context": "Also, they have been a very useful discriminative feature in the area of authorship attribution, because they seem to account for lexical, syntactic, and stylistic information (Sapkota et al., 2015).", "startOffset": 176, "endOffset": 198}, {"referenceID": 13, "context": "In order to match them with an observed category label, a one-to-one mapping is obtained using Kuhn-Munkres algorithm for maximal weighting in a bipartite graph (Kuhn, 1955; Munkres, 1957).", "startOffset": 161, "endOffset": 188}, {"referenceID": 16, "context": "In order to match them with an observed category label, a one-to-one mapping is obtained using Kuhn-Munkres algorithm for maximal weighting in a bipartite graph (Kuhn, 1955; Munkres, 1957).", "startOffset": 161, "endOffset": 188}, {"referenceID": 11, "context": "Joty et al. (2011) follow the same procedure.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Ubuntu (Bhatia et al., 2012) Domain: Computer technical Tagset: Question, Repeat Question, Clarification, Solution, Further Details, Positive Feedback, Negative Feedback, Spam Number of threads: 100 TripAdvisor-NYC (Bhatia et al.", "startOffset": 7, "endOffset": 28}, {"referenceID": 2, "context": ", 2012) Domain: Computer technical Tagset: Question, Repeat Question, Clarification, Solution, Further Details, Positive Feedback, Negative Feedback, Spam Number of threads: 100 TripAdvisor-NYC (Bhatia et al., 2012) Domain: Travel Tagset: Same as Ubuntu Number of threads: 100 Apple (Catherine et al.", "startOffset": 194, "endOffset": 215}, {"referenceID": 3, "context": ", 2012) Domain: Travel Tagset: Same as Ubuntu Number of threads: 100 Apple (Catherine et al., 2012) Domain: Computer technical Tagset: Answer Number of threads: 300 labeled and 140,000 unlabeled", "startOffset": 75, "endOffset": 99}, {"referenceID": 12, "context": "Kim et al. (2010) use a tagset of 12 categories \u2014 Question, Question-Add, Question-Confirmation, Question-Correction, Answer, Answer-Add, Answer-Confirmation, Answer-Correction, Answer-Objection, Resolution, Reproduction, and Other.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "Hence, instead of using standard annotation quality measures like Scott\u2019s \u03c0 and Fleiss\u2019s \u03ba, Krippendorf\u2019s \u03b1 is reported, which can account for missing values (Artstein and Poesio, 2008).", "startOffset": 158, "endOffset": 185}, {"referenceID": 15, "context": "Initially, all forum posts were tokenized by sentence and word, followed by POS tagging and stemming \u2014 all using Stanford CoreNLP Toolkit (Manning et al., 2014).", "startOffset": 138, "endOffset": 160}, {"referenceID": 2, "context": "Hence, for performance comparison, the current work also used a dataset of nearly 200 threads from TripAdvisor (made available by Bhatia et al. (2012)).", "startOffset": 130, "endOffset": 151}, {"referenceID": 2, "context": "Hence, for performance comparison, the current work also used a dataset of nearly 200 threads from TripAdvisor (made available by Bhatia et al. (2012)). As a caveat, it is important to note that this dataset has eight dialogue act categories, whereas Joty et al. (2011) consider 12.", "startOffset": 130, "endOffset": 270}, {"referenceID": 3, "context": "Precision Recall F1-measure Catherine et al. (2013) 0.", "startOffset": 28, "endOffset": 52}], "year": 2016, "abstractText": "Semi-supervised and unsupervised methods for categorizing posts in Web discussion forums Krish Perumal Master of Science Graduate Department of Computer Science University of Toronto 2016 Web discussion forums are used by millions of people worldwide to share information belonging to a variety of domains such as automotive vehicles, pets, sports, etc. They typically contain posts that fall into different categories such as problem, solution, feedback, spam, etc. Automatic identification of these categories can aid information retrieval that is tailored for specific user requirements. Previously, a number of supervised methods have attempted to solve this problem; however, these depend on the availability of abundant training data. A few existing unsupervised and semi-supervised approaches are either focused on identifying a single category or do not report category-specific performance. In contrast, this work proposes unsupervised and semi-supervised methods that require no or minimal training data to achieve this objective without compromising on performance. A fine-grained analysis is also carried out to discuss their limitations. The proposed methods are based on sequence models (specifically, Hidden Markov Models) that can model language for each category using word and part-of-speech probability distributions, and manually specified features. Empirical evaluations across domains demonstrate that the proposed methods are better suited for this task than existing ones.", "creator": "LaTeX with hyperref package"}}}