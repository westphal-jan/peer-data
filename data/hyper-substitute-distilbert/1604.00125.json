{"id": "1604.00125", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "AttSum: Joint Learning of Focusing and Summarization with Neural Attention", "abstract": "attention pair ranking whereas human saliency ranking combines simply two main cues creating human query - mediated summarization. recently supervised descriptive systems often perform the two tasks in isolation. however, since hypothesis seekers follow the sell - off examining fairness and saliency, accepting resources as metadata, observers of the intervening mechanisms could be trained mentally. the result combines a novel summarization array - attsum, systematically tackles the two tasks jointly. cognition automatically learns vector representations utilizing sentences as well as a document cluster. meanwhile, it incorporates the attention enhancement to reinforce the attentive reading of human letters when a query appears given. extensive experiments indicate conducted on different array - focused summarization benchmark datasets. without using minimal random - crafted features, attsum achieves minimal responses. it sees particularly possible that the sentences recognized previously execute on a query stimulus meet the query probability.", "histories": [["v1", "Fri, 1 Apr 2016 04:18:39 GMT  (104kb,D)", "http://arxiv.org/abs/1604.00125v1", "7 pages, 2 figures"], ["v2", "Tue, 27 Sep 2016 02:22:33 GMT  (81kb,D)", "http://arxiv.org/abs/1604.00125v2", "10 pages, 1 figure"]], "COMMENTS": "7 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["ziqiang cao", "wenjie li", "sujian li", "furu wei", "yanran li"], "accepted": false, "id": "1604.00125"}, "pdf": {"name": "1604.00125.pdf", "metadata": {"source": "CRF", "title": "AttSum: Joint Learning of Focusing and Summarization with Neural Attention", "authors": ["Ziqiang Cao", "Wenjie Li", "Sujian Li", "Furu Wei"], "emails": ["cswjli}@comp.polyu.edu.hk", "lisujian@pku.edu.cn", "furu@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Query-focused summarization [Dang, 2005] aims to create a brief, well-organized and fluent summary that answers the need of the query. It is useful in many scenarios like news services and search engines, etc. Nowadays, most summarization systems are under the extractive framework which directly selects existing sentences to form the summary. Basically, there are two major tasks in extractive query-focused summarization, i.e., to measure the saliency of a sentence and its relevance to a user\u2019s query. A summarization system should select the sentences which both reflect the main ideas of the document cluster and meet the query need to form the summary.\nAfter a long period of research, learning-based models like Logistic Regression [Li et al., 2013] etc. have become growingly popular in this area. However, most current supervised summarization systems often perform the two tasks in isolation. Usually, they design query-dependent features (e.g., query word overlap) to learn the relevance ranking, and query-independent features (e.g., term frequency) to learn the\nsaliency ranking. Then, the two types of features are combined to train an overall ranking model. Note that the only supervision available is the reference summaries. Humans write summaries with the trade-off between relevance and saliency. Some salient content may not appear in reference summaries if it fails to respond to the query. Likewise, the content relevant to the query but not representative of documents will be excluded either. Therefore, reference summaries just act as an intersection of relevant and salient content. As a result, weights for neither query-dependent nor query-independent features could be learned well from reference summaries.\nIn addition, when measuring the query relevance, most summarization systems merely make use of surface features like the TF-IDF cosine similarity between a sentence and the query [Wan and Xiao, 2009]. However, relevance is not similarity. Take the document cluster \u201cd360f\u201d in DUC1 2005 as an example. It has the following query:\nWhat are the benefits of drug legalization?\nHere, \u201cDrug legalization\u201d are the key words with high TFIDF scores. And yet the main intent of the query is to look for \u201cbenefit\u201d, which is a very general word and does not present in the source text at all. It is not surprising that when measured by the TF-IDF cosine similarity, the sentences with top scores all contain the words \u201cdrug\u201d or \u201clegalization\u201d. Nevertheless, none of them provides advantages of drug legalization. See Section 4.6 for reference. Apparently, even if a sentence is exactly the same as the query, it is still totally useless in the summary because it is unable to answer the query need. Therefore, the surface features are inadequate to measure the query relevance, which further augments the error of the whole summarization system. This drawback partially explains why it might achieve acceptable performance to adopt generic summarization models in the query-focused summarization task (e.g., [Gillick and Favre, 2009]).\nIntuitively, the isolation problem can be solved with a joint model. Meanwhile, neural networks have shown to generate better representations than surface features in the summarization task [Cao et al., 2015b; Yin and Pei, 2015]. Thus, a joint neural network model should be a nice solution to extractive query-focused summarization. To this end, we propose a novel summarization system called AttSum, which\n1http://www-nlpir.nist.gov/projects/duc/\nar X\niv :1\n60 4.\n00 12\n5v 1\n[ cs\n.I R\n] 1\nA pr\n2 01\n6\njoints query relevance ranking and sentence saliency ranking with a neural attention model. The attention mechanism has been successfully applied to learn alignment between various modalities, e.g., between speech frames and text in the speech recognition task [Chorowski et al., 2014], between visual features of a picture and its text description in the image caption generation task [Xu et al., 2015], and between source and target words in the machine translation task [Bahdanau et al., 2014], etc. Besides, the work of [Kobayashi et al., 2015] demonstrates that it is reasonably good to use the similarity between the sentence embedding and document embedding for saliency measurement, where the document embedding is derived from the sum pooling of sentence embeddings. In order to consider the relevance and saliency simultaneously, we introduce the weighted-sum pooling over sentence embeddings to represent the document, where the weight is the automatically learned query relevance of a sentence. In this way, the document representation will be biased to the sentence embeddings which match the meaning of both query and documents. The working mechanism of AttSum is consistent with the way how humans read when having a particular query in their minds. Naturally, they pay more attention to the sentences that meet the query need.\nWe verify AttSum on the widely-used DUC 2005 \u223c 2007 query-focused summarization benchmark datasets. Without using any hand-crafted features, AttSum is still able to achieve competitive summarization performance. We also conduct qualitative analysis for those sentences with large relevance scores to the query. The result reveals that AttSum indeed focuses on highly query relevant content.\nThe contributions of our work are as follows:\n\u2022 We apply the attention mechanism to simulate human attentive reading behavior for query-focused summarization;\n\u2022 We propose a joint neural network model to learn query relevance ranking and sentence saliency ranking simultaneously."}, {"heading": "2 Query-Focused Sentence Ranking", "text": "For generic summarization, people read all the sentences with almost equal attention. However, given a query, people will naturally pay more attention to the query relevant sentences\nand summarize the main ideas from them. Similar to human attentive reading behavior, AttSum, the system to be illustrated in this section, ranks the sentences with its focus on the query. The overall framework is shown in Fig. 1 and 2. From the bottom to up, AttSum is composed of three major layers. CNN Layer Use Convolutional Neural Networks to project\nthe sentences and queries onto the embeddings. Pooling Layer With the attention mechanism, combine the\nsentence embeddings to form the document embedding in the same latent space.\nRanking Layer Rank a sentence according to the similarity between its embedding and the embedding of the document cluster.\nThe rest of this section describes the details of the three layers."}, {"heading": "2.1 CNN Layer", "text": "Convolutional Neural Networks (CNNs) have been widely used in various Natural Language Processing (NLP) areas including summarization [Cao et al., 2015b; Yin and Pei, 2015]. They are able to learn the compressed representations of n-grams effectively and tackle the sentences with variable lengths naturally. We use CNNs to project both sentences and the query onto distributed representations, i.e.,\nv(s) = CNN(s) v(q) = CNN(q)\nA basic CNN contains a convolution operation on the top of word embeddings, which is followed by a pooling operation. Let v(wi) \u2208 Rk refer to the k-dimensional word embedding corresponding to the ith word in the sentence. Assume v(wi : wi+j) to be the concatenation of word embeddings [v(wi), \u00b7 \u00b7 \u00b7 ,v(wi+j)]. A convolution operation involves a filter Wht \u2208 Rl\u00d7hk, which is applied to a window of h words to produce the abstract features chi \u2208 Rl:\nchi = f(W h t \u00d7 v(wi : wi+j)), (1)\nwhere f(\u00b7) is a non-linear function and the use of tanh is the common practice. To simplify, the bias term is left out. This filter is applied to each possible window of words in the sentence to produce a feature map. Subsequently, a pooling operation is applied over the feature map to obtain the final features c\u0302h \u2208 Rl of the filter. Here we use the max-over-time pooling [Collobert et al., 2011].\nc\u0302h = max{ch1 , ch2 , \u00b7 \u00b7 \u00b7 } (2)\nThe idea behind it is to capture the most important features in a feature map. c\u0302h is the output of CNN Layer, i.e., the embeddings of sentences and queries."}, {"heading": "2.2 Pooling Layer", "text": "With the attention mechanism, AttSum uses the weightedsum pooling over the sentence embeddings to represent the document cluster. To achieve this, AttSum firstly learns the query relevance of a sentence automatically:\nr(s, q) = \u03c3(v(s)Mv(q)T ), (3)\nwhere v(s)Mv(q)T ,M \u2208 Rl\u00d7l is a tensor function, and \u03c3 stands for the sigmoid function. The tensor function has the power to measure the interaction between any two elements of sentence and query embeddings. Therefore, two identical embeddings will have a low score. This characteristic is exactly what we need. To reiterate, relevance is not equivalent to similarity. Then with r(s, q) as weights, we introduce the weighted-sum pooling to calculate the document embedding v(d|q):\nv(d|q) = \u2211\ns\u2208d r(s, q)v(s) (4)\nNotably, a sentence embedding plays two roles, both the pooling item and the pooling weight. On the one hand, if a sentence is highly related to the query, its pooling weight is large. On the other hand, if a sentence is salient in the document cluster, its embedding should be representative. As a result, the weighted-sum pooling generates the document representation which is automatically biased to embeddings of sentences match both documents and the query.\nAttSum simulates human attentive reading behavior. The experiments to be presented in Section 4.6 will demonstrate its strong ability to catch query relevant sentences. Actually, the attention mechanism has been applied in onesentence summary generation before [Rush et al., 2015; Hu et al., 2015]. The success of these works, however, heavily depends on the hand-crafted features. We believe that the attention mechanism may not be able to play its anticipated role if it is not used appropriately."}, {"heading": "2.3 Ranking Layer", "text": "Since the semantics directly lies in sentence and document embeddings, we rank a sentence according to its embedding similarity to the document cluster, following the work of [Kobayashi et al., 2015]. Here we adopt cosine similarity:\ncos(d, s|q) = v(s) \u2022 v(d|q) T\n||v(s)|| \u2022 ||v(d|q)|| (5)\nCompared with Euclidean distance, one advantage of cosine similarity is that it is automatically scaled. According\nto [Ka\u030ageba\u0308ck et al., 2014], cosine similarity is the best metrics to measure the embedding similarity for summarization.\nIn the training process, we apply the pairwise ranking strategy [Collobert et al., 2011] to tune model parameters, as shown in Fig. 2. Specifically, we calculate the ROUGE-2 scores [Lin, 2004] of all the sentences in the training dataset. Those sentences with high ROUGE-2 scores are regarded as positive samples, and the rest as negative samples. Afterwards, we randomly choose a pair of positive and negative sentences which are denoted as s+ and s\u2212, respectively. Through the CNN Layer and Pooling Layer, we generate the embeddings of v(s+), v(s\u2212) and v(d|q). We can then obtain the ranking scores of s+ and s\u2212 according to Eq. 5. With the pairwise ranking criterion, AttSum should give a positive sample a higher score in comparison with a negative sample. The cost function is defined as follows:\n(d, s+, s\u2212|q) (6) = max(0,\u2126\u2212 cos(d, s+|q) + cos(d, s\u2212|q)),\nwhere \u2126 is a margin threshold. With this cost function, we can use the gradient descent algorithm to update model parameters. In this paper, we apply the diagonal variant of AdaGrad with mini-batches [Duchi et al., 2011]. AdaGrad adapts the learning rate for different parameters at different steps. Thus it is less sensitive to initial parameters than the stochastic gradient descent."}, {"heading": "3 Sentence Selection", "text": "A summary is obliged to offer both informative and nonredundant content. While AttSum focuses on sentence ranking, it employs a simple greedy algorithm, similar to the MMR strategy [Carbonell and Goldstein, 1998], to select summary sentences. The whole process is shown in Algorithm 1. At first, we discard sentences less than 8 words since too short sentences are often incomplete. Then we sort the rest in descending order according to the derived ranking scores. Finally, we iteratively dequeue the top-ranked sentence, and append it to the current summary if it is non-\nredundant. A sentence is considered non-redundant if it contains significantly new bi-grams compared with the current summary content. We empirically set the cut-off of the new bi-gram ratio to 0.5.\nAlgorithm 1 Greedy Sentence Selection Process Input:\nSorted sentence array: s1, s2, \u00b7 \u00b7 \u00b7 , sN ; Output:\nSummary: S 1: Initialization: S = \u03c6 2: for i = 1; i \u2264 N ; i+ + do 3: if length of si \u2264 8 OR bi-gram overlap between si and S \u2265 0.5 then 4: continue; 5: end if 6: S = S \u222a {si}; 7: if length of S reaches the bound then 8: break; 9: end if\n10: end for"}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset", "text": "In this work, we focus on the query-focused multi-document summarization task. The experiments are conducted on the DUC 2005\u223c 2007 datasets. All the documents are from news websites and grouped into various thematic clusters. In each cluster, there are four reference summaries created by NIST assessors. We use Stanford CoreNLP2 to process the datasets, including sentence splitting, tokenization and lemmatization. Our summarization model compiles the documents in a cluster into a single document. Table 1 shows the basic information of the three datasets. We can find that the data sizes of DUC are quite different. The sentence number of DUC 2007 is only about a half of DUC 2005\u2019s. For each cluster, a summarization system is requested to generate a summary with the length limit of 250 words. We conduct a 3-fold crossvalidation on DUC datasets, with two years of data as the training set and one year of data as the test set."}, {"heading": "4.2 Model Setting", "text": "For the CNN layer, we introduce a 50-dimensional word embedding set. This word embedding set is trained on a large English news corpus with the word2vec model [Mikolov et al., 2013]. In this paper, we adopt the Python implement of\n2http://stanfordnlp.github.io/CoreNLP/\nword2vec, i.e., gensim3. Since the summarization dataset is quite limited, we do not update these word embeddings in the training process, which greatly reduces the model parameters to be learned. There are two hyper-parameters in our model, i.e., the word window size h and the CNN layer dimension l. We set h = 2, which is consistent with the ROUGE-2 evaluation. As for l, we explore the change of model performance with l \u2208 [5, 100]. Finally, we choose l = 50 for all the rest experiments. It is the same dimension as the word embeddings. During the training of pairwise ranking, we set the margin \u2126 = 0.5. The initial learning rate is 0.1 and batch size is 100."}, {"heading": "4.3 Evaluation Metric", "text": "For evaluation, we adopt the widely-used automatic evaluation metric ROUGE [Lin, 2004] 4. It measures the summary quality by counting the overlapping units such as the n-grams, word sequences and word pairs between the peer summary and reference summaries. We take ROUGE-2 as the main measures due to its high capability of evaluating automatic summarization systems [Owczarzak et al., 2012]. Its recall score is computed as follows:\nROUGE \u2212 2recall =\n\u2211 b\u2208{References}\nNmatch(b)\u2211 b\u2208{References} N(b) (7)\nwhere b stands for a bi-gram, and Nmatch(b) is the maximum number of bi-grams co-occurring in the peer summary and a set of reference summaries. N(b) is the total number of bi-grams in reference summaries. During the training data of pairwise ranking, we also rank the sentences according to ROUGE-2 scores."}, {"heading": "4.4 Baselines", "text": "To evaluate the summarization performance of AttSum, we compare it with the best peer systems participating DUC evaluations. We name these participants \u201dPeer\u201d plus their IDs. We also choose as baselines two popular extractive queryfocused summarization methods, called MultiMR [Wan and Xiao, 2009] and SVR [Ouyang et al., 2011]. MultiMR is a graph-based manifold ranking method which makes uniform use of the sentence-to-sentence relationships and the sentence-to-query relationships. SVR extracts both querydependent and query-independent features and applies Support Vector Regression to learn feature weights. Note that MultiMR is unsupervised while SVR is supervised.\nTo verify the effectiveness of the joint model, we design a baseline called ISOLATION, which performs saliency ranking and relevance ranking in isolation. Specifically, it directly uses the sum pooling over sentence embeddings to represent the document cluster. Therefore the embedding similarity between a sentence and the document cluster could only measure the sentence saliency. To include the query information, we supplement the common hand-crafted feature TF-IDF cosine similarity to the query. This query-dependent feature,\n3http://rare-technologies.com/ deep-learning-with-word2vec-and-gensim/\n4ROUGE-1.5.5 with options: -n 2 -m -u -c 95 -l 250 -x -r 1000 -f A -p 0.5 -t 0\ntogether with the embedding similarity, are used in sentence ranking. ISOLATION removes the attention mechanism, and mixtures hand-crafted and automatically learned features."}, {"heading": "4.5 Summarization Performance", "text": "The ROUGE scores of the different summarization methods are presented in Table 2. We consider ROUGE-2 as the main evaluation metrics, and also provide the ROUGE-1 results as the common practice. As can be seen, AttSum always enjoys a reasonable increase over ISOLATION, indicating that the joint model indeed takes effects. With respect to other methods, AttSum outperforms MultiMR on all the three years, and achieves a competitive performance to the widely-used supervised summarization system SVR. SVR heavily depends on hand-crafted features while AttSum learns all the features automatically. Nevertheless, AttSum works better than SVR on DUC 2006 and 2007. On DUC 2005, the performance of AttSum is inferior to SVR. Over-fitting is a possible reason. Table 1 demonstrates the data size of DUC 2005 is highly larger than the other two. As a result, when using the 3-fold cross-validation, the number of training data for DUC 2005 is the smallest among the three years. The lack of training data impedes the learning of sentence and document embeddings.\nNotably, top performing DUC participants surpass our model in certain cases, especially on DUC 2007. However, DUC participants may apply abstractive summarization. For example, Peer 15 [Prasad Pingali and Varma, 2007] on DUC 2007 defines about 100 rules to compress sentences. The linguistic quality evaluation reveals that the high ROUGE scores of this summarization system are at the cost of readability loss. By contrast, AttSum applies extractive summarization which is able to ensure the sentence-level readability."}, {"heading": "4.6 Query Relevance Performance", "text": "We also perform the qualitative analysis to exam the query relevance performance of AttSum. We randomly choose\nsome queries in the test datasets and calculate the relevance scores of sentences according to Eq. 3. We then extract the top ranked sentences and check whether they are able to meet the query need. Examples for both one-sentence queries and multiple-sentence queries are shown in Table 3. We also give the sentences with top TF-IDF cosine similarity to the query for comparison.\nWith manual inspection, we find that most query-focused sentences in AttSum can answer the query to a large extent. For instance, when asked to tell the advantages of drug legalization, AttSum catches the sentences about drug trafficking prevention, the control of marijuana use, and the economic effectiveness, etc. All these aspects are mentioned in reference summaries. The sentences with the high TF-IDF similarity, however, are usually short and simply repeat the key words in the query. The advantage of AttSum over TF-IDF similarity is apparent in query relevance ranking.\nWhen there are multiple sentences in a query, AttSum may only focus on a part of them. Take the second query in Table 3 as an example. Although the responses to all the four query sentences are involved more or less, we can see that AttSum tends to describe the steps of wetland preservation more. Actually, by inspection, the reference summaries do not treat the query sentences equally either. For this query, they only tell a little about frustrations during wetland preservation. Since AttSum projects a query onto a single embedding, it may augment the bias in reference summaries. It seems to be hard even for humans to read attentively when there are a number of needs in a query. Because only a small part of DUC datasets contains such a kind of complex queries, we do not purposely design a special model to handle them in our current work."}, {"heading": "5 Related Work", "text": ""}, {"heading": "5.1 Extractive Summarization", "text": "Work on extractive summarization spans a large range of approaches. Starting with unsupervised methods, one of the widely known approaches is Maximum Marginal Relevance (MMR) [Carbonell and Goldstein, 1998]. It used a greedy approach to select sentences and considered the trade-off between saliency and redundancy. Good results could be achieved by reformulating this as an Integer Linear Programming (ILP) problem which was able to find the optimal solution [McDonald, 2007; Gillick and Favre, 2009]. Graphbased models played a leading role in the extractive summarization area, due to its ability to reflect various sentence relationships. For example, [Wan and Xiao, 2009] adopted manifold ranking to make use of the within-document sentence relationships, the cross-document sentence relationships and the sentence-to-query relationships. In contrast to these unsupervised approaches, there are also various learning-based summarization systems. Different classifiers have been explored, e.g., conditional random field (CRF) [Galley, 2006], Support Vector Regression (SVR) [Ouyang et al., 2011], and Logistic Regression [Li et al., 2013], etc.\nMany query-focused summarizers are heuristic extensions of generic summarization methods by incorporating the information of the given query. A variety of query-\ndependent features were defined to measure the relevance, including TF-IDF cosine similarity [Wan and Xiao, 2009], WordNet similarity [Ouyang et al., 2011], and word cooccurrence [Prasad Pingali and Varma, 2007], etc. However, these features usually reward sentences similar to the query, which fail to meet the query need."}, {"heading": "5.2 Deep Learning in Summarization", "text": "In the summarization area, the application of deep learning techniques has attracted more and more interest. [Genest et al., 2011] used unsupervised auto-encoders to represent both manual and system summaries for the task of summary evaluation. Their method , however, did not surpass ROUGE. Recently, some works [Cao et al., 2015a; Cao et al., 2015b] have tried to use neural networks to complement sentence ranking features. Although these models achieved the state-of-the-art performance, they still heavily relied on hand-crafted features. A few researches explored to directly measure similarity based on distributed representations. [Yin and Pei, 2015] trained a language model based on convolutional neural networks to project sentences onto distributed representations. Others like [Kobayashi et al., 2015; Ka\u030ageba\u0308ck et al., 2014] just used the sum of trained word embeddings to represent sentences or documents.\nIn addition to extractive summarization, deep learning technologies have also been applied to compressive and abstractive summarization. [Filippova et al., 2015] used word embeddings and Long Short Term Memory models (LSTMs) to output readable and informative sentence compressions. [Rush et al., 2015; Hu et al., 2015] leveraged the neural atten-\ntion model [Bahdanau et al., 2014] in the machine translation area to generate one-sentence summaries. We have described these methods in Section 2.2."}, {"heading": "6 Conclusion and Future Work", "text": "This paper proposes a novel query-focused summarization system called AttSum which jointly handles saliency ranking and relevance ranking. It automatically generates distributed representations for sentences as well as the document cluster. Meanwhile, it applies the attention mechanism to simulate human attentive reading behavior when a query is given. We conduct extensive experiments on DUC query-focused summarization datasets. Using no hand-crafted features, AttSum achieves competitive performance. It is also observed that the sentences recognized to focus on the query indeed meet the query need.\nSince we have obtained the semantic representations for the document cluster, we believe our system can be easily extended into abstractive summarization. The only additional step is to integrate a neural language model after document embeddings. We leave this as our future work."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Proceedings of ACL: Short Papers", "author": ["Ziqiang Cao", "Furu Wei", "Sujian Li", "Wenjie Li", "Ming Zhou", "Houfeng Wang. Learning summary prior representation for extractive summarization"], "venue": "pages 829\u2013833,", "citeRegEx": "Cao et al.. 2015b", "shortCiteRegEx": null, "year": 2015}, {"title": "diversity-based reranking for reordering documents and producing summaries", "author": ["Jaime Carbonell", "Jade Goldstein. The use of mmr"], "venue": "Proceedings of SIGIR, pages 335\u2013336,", "citeRegEx": "Carbonell and Goldstein. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: First results", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.1602,", "citeRegEx": "Chorowski et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "Lon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research, 12:2493\u2013 2537,", "citeRegEx": "Collobert et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Overview of duc 2005", "author": ["Hoa Trang Dang"], "venue": "Proceedings of DUC, pages 1\u201312,", "citeRegEx": "Dang. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "The Journal of Machine Learning Research", "author": ["John Duchi", "Elad Hazan", "Yoram Singer. Adaptive subgradient methods for online learning", "stochastic optimization"], "venue": "12:2121\u20132159,", "citeRegEx": "Duchi et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Proceedings of EMNLP", "author": ["Katja Filippova", "Enrique Alfonseca", "Carlos A. Colmenares", "Lukasz Kaiser", "Oriol Vinyals. Sentence compression by deletion with lstms"], "venue": "pages 360\u2013368,", "citeRegEx": "Filippova et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of EMNLP", "author": ["Michel Galley. A skip-chain conditional random field for ranking meeting utterances by importance"], "venue": "pages 364\u2013372,", "citeRegEx": "Galley. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "In Proceedings of the Workshop on Automatic Text Summarization", "author": ["Pierre-Etienne Genest", "Fabrizio Gotti", "Yoshua Bengio. Deep learning for automatic summary scoring"], "venue": "pages 17\u201328,", "citeRegEx": "Genest et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Proceedings of the Workshop on ILP for NLP", "author": ["Dan Gillick", "Benoit Favre. A scalable global model for summarization"], "venue": "pages 10\u201318,", "citeRegEx": "Gillick and Favre. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Lcsts: A large scale chinese short text summarization dataset", "author": ["Baotian Hu", "Qingcai Chen", "Fangze Zhu"], "venue": "Proceedings of EMNLP, pages 1967\u20131972,", "citeRegEx": "Hu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of EACL Workshop", "author": ["Mikael K\u00e5geb\u00e4ck", "Olof Mogren", "Nina Tahmasebi", "Devdatt Dubhashi. Extractive summarization using continuous vector space models"], "venue": "pages 31\u201339,", "citeRegEx": "K\u00e5geb\u00e4ck et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of EMNLP", "author": ["Hayato Kobayashi", "Masaki Noguchi", "Taichi Yatsuka. Summarization based on embedding distributions"], "venue": "pages 1984\u20131989,", "citeRegEx": "Kobayashi et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of ACL", "author": ["Chen Li", "Xian Qian", "Yang Liu. Using supervised bigram-based ilp for extractive summarization"], "venue": "pages 1004\u20131013,", "citeRegEx": "Li et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "Proceedings of the ACL Workshop, pages 74\u201381,", "citeRegEx": "Lin. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "A study of global inference algorithms in multi-document summarization", "author": ["Ryan McDonald"], "venue": "Springer,", "citeRegEx": "McDonald. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Information Processing & Management", "author": ["You Ouyang", "Wenjie Li", "Sujian Li", "Qin Lu. Applying regression models to query-focused multi-document summarization"], "venue": "47(2):227\u2013237,", "citeRegEx": "Ouyang et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "An assessment of the accuracy of automatic evaluation in summarization", "author": ["Owczarzak et al", "2012] Karolina Owczarzak", "John M Conroy", "Hoa Trang Dang", "Ani Nenkova"], "venue": "In Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Iiit hyderabad at duc 2007", "author": ["Rahul K Prasad Pingali", "Vasudeva Varma"], "venue": "Proceedings of DUC 2007,", "citeRegEx": "Prasad Pingali and Varma. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "In Proceedings of EMNLP", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston. A neural attention model for abstractive sentence summarization"], "venue": "pages 379\u2013389,", "citeRegEx": "Rush et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In IJCAI", "author": ["Xiaojun Wan", "Jianguo Xiao. Graph-based multi-modality learning for topic-focused multi-document summarization"], "venue": "pages 1586\u2013 1591,", "citeRegEx": "Wan and Xiao. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio. Show"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "Xu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of IJCAI", "author": ["Wenpeng Yin", "Yulong Pei. Optimizing sentence modeling", "selection for document summarization"], "venue": "pages 1383\u20131389,", "citeRegEx": "Yin and Pei. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Query-focused summarization [Dang, 2005] aims to create a brief, well-organized and fluent summary that answers the need of the query.", "startOffset": 28, "endOffset": 40}, {"referenceID": 14, "context": "After a long period of research, learning-based models like Logistic Regression [Li et al., 2013] etc.", "startOffset": 80, "endOffset": 97}, {"referenceID": 22, "context": "In addition, when measuring the query relevance, most summarization systems merely make use of surface features like the TF-IDF cosine similarity between a sentence and the query [Wan and Xiao, 2009].", "startOffset": 179, "endOffset": 199}, {"referenceID": 10, "context": ", [Gillick and Favre, 2009]).", "startOffset": 2, "endOffset": 27}, {"referenceID": 1, "context": "Meanwhile, neural networks have shown to generate better representations than surface features in the summarization task [Cao et al., 2015b; Yin and Pei, 2015].", "startOffset": 121, "endOffset": 159}, {"referenceID": 24, "context": "Meanwhile, neural networks have shown to generate better representations than surface features in the summarization task [Cao et al., 2015b; Yin and Pei, 2015].", "startOffset": 121, "endOffset": 159}, {"referenceID": 3, "context": ", between speech frames and text in the speech recognition task [Chorowski et al., 2014], between visual features of a picture and its text description in the image caption generation task [Xu et al.", "startOffset": 64, "endOffset": 88}, {"referenceID": 23, "context": ", 2014], between visual features of a picture and its text description in the image caption generation task [Xu et al., 2015], and between source and target words in the machine translation task [Bahdanau et al.", "startOffset": 108, "endOffset": 125}, {"referenceID": 0, "context": ", 2015], and between source and target words in the machine translation task [Bahdanau et al., 2014], etc.", "startOffset": 77, "endOffset": 100}, {"referenceID": 13, "context": "Besides, the work of [Kobayashi et al., 2015] demonstrates that it is reasonably good to use the similarity between the sentence embedding and document embedding for saliency measurement, where the document embedding is derived from the sum pooling of sentence embeddings.", "startOffset": 21, "endOffset": 45}, {"referenceID": 1, "context": "Convolutional Neural Networks (CNNs) have been widely used in various Natural Language Processing (NLP) areas including summarization [Cao et al., 2015b; Yin and Pei, 2015].", "startOffset": 134, "endOffset": 172}, {"referenceID": 24, "context": "Convolutional Neural Networks (CNNs) have been widely used in various Natural Language Processing (NLP) areas including summarization [Cao et al., 2015b; Yin and Pei, 2015].", "startOffset": 134, "endOffset": 172}, {"referenceID": 4, "context": "Here we use the max-over-time pooling [Collobert et al., 2011].", "startOffset": 38, "endOffset": 62}, {"referenceID": 21, "context": "Actually, the attention mechanism has been applied in onesentence summary generation before [Rush et al., 2015; Hu et al., 2015].", "startOffset": 92, "endOffset": 128}, {"referenceID": 11, "context": "Actually, the attention mechanism has been applied in onesentence summary generation before [Rush et al., 2015; Hu et al., 2015].", "startOffset": 92, "endOffset": 128}, {"referenceID": 13, "context": "Since the semantics directly lies in sentence and document embeddings, we rank a sentence according to its embedding similarity to the document cluster, following the work of [Kobayashi et al., 2015].", "startOffset": 175, "endOffset": 199}, {"referenceID": 12, "context": "According to [K\u00e5geb\u00e4ck et al., 2014], cosine similarity is the best metrics to measure the embedding similarity for summarization.", "startOffset": 13, "endOffset": 36}, {"referenceID": 4, "context": "In the training process, we apply the pairwise ranking strategy [Collobert et al., 2011] to tune model parameters, as shown in Fig.", "startOffset": 64, "endOffset": 88}, {"referenceID": 15, "context": "Specifically, we calculate the ROUGE-2 scores [Lin, 2004] of all the sentences in the training dataset.", "startOffset": 46, "endOffset": 57}, {"referenceID": 6, "context": "In this paper, we apply the diagonal variant of AdaGrad with mini-batches [Duchi et al., 2011].", "startOffset": 74, "endOffset": 94}, {"referenceID": 2, "context": "While AttSum focuses on sentence ranking, it employs a simple greedy algorithm, similar to the MMR strategy [Carbonell and Goldstein, 1998], to select summary sentences.", "startOffset": 108, "endOffset": 139}, {"referenceID": 17, "context": "This word embedding set is trained on a large English news corpus with the word2vec model [Mikolov et al., 2013].", "startOffset": 90, "endOffset": 112}, {"referenceID": 15, "context": "For evaluation, we adopt the widely-used automatic evaluation metric ROUGE [Lin, 2004] 4.", "startOffset": 75, "endOffset": 86}, {"referenceID": 22, "context": "We also choose as baselines two popular extractive queryfocused summarization methods, called MultiMR [Wan and Xiao, 2009] and SVR [Ouyang et al.", "startOffset": 102, "endOffset": 122}, {"referenceID": 18, "context": "We also choose as baselines two popular extractive queryfocused summarization methods, called MultiMR [Wan and Xiao, 2009] and SVR [Ouyang et al., 2011].", "startOffset": 131, "endOffset": 152}, {"referenceID": 20, "context": "For example, Peer 15 [Prasad Pingali and Varma, 2007] on DUC 2007 defines about 100 rules to compress sentences.", "startOffset": 21, "endOffset": 53}, {"referenceID": 2, "context": "Starting with unsupervised methods, one of the widely known approaches is Maximum Marginal Relevance (MMR) [Carbonell and Goldstein, 1998].", "startOffset": 107, "endOffset": 138}, {"referenceID": 16, "context": "Good results could be achieved by reformulating this as an Integer Linear Programming (ILP) problem which was able to find the optimal solution [McDonald, 2007; Gillick and Favre, 2009].", "startOffset": 144, "endOffset": 185}, {"referenceID": 10, "context": "Good results could be achieved by reformulating this as an Integer Linear Programming (ILP) problem which was able to find the optimal solution [McDonald, 2007; Gillick and Favre, 2009].", "startOffset": 144, "endOffset": 185}, {"referenceID": 22, "context": "For example, [Wan and Xiao, 2009] adopted manifold ranking to make use of the within-document sentence relationships, the cross-document sentence relationships and the sentence-to-query relationships.", "startOffset": 13, "endOffset": 33}, {"referenceID": 8, "context": ", conditional random field (CRF) [Galley, 2006], Support Vector Regression (SVR) [Ouyang et al.", "startOffset": 33, "endOffset": 47}, {"referenceID": 18, "context": ", conditional random field (CRF) [Galley, 2006], Support Vector Regression (SVR) [Ouyang et al., 2011], and Logistic Regression [Li et al.", "startOffset": 81, "endOffset": 102}, {"referenceID": 14, "context": ", 2011], and Logistic Regression [Li et al., 2013], etc.", "startOffset": 33, "endOffset": 50}, {"referenceID": 22, "context": "dependent features were defined to measure the relevance, including TF-IDF cosine similarity [Wan and Xiao, 2009], WordNet similarity [Ouyang et al.", "startOffset": 93, "endOffset": 113}, {"referenceID": 18, "context": "dependent features were defined to measure the relevance, including TF-IDF cosine similarity [Wan and Xiao, 2009], WordNet similarity [Ouyang et al., 2011], and word cooccurrence [Prasad Pingali and Varma, 2007], etc.", "startOffset": 134, "endOffset": 155}, {"referenceID": 20, "context": ", 2011], and word cooccurrence [Prasad Pingali and Varma, 2007], etc.", "startOffset": 31, "endOffset": 63}, {"referenceID": 9, "context": "[Genest et al., 2011] used unsupervised auto-encoders to represent both manual and system summaries for the task of summary evaluation.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Recently, some works [Cao et al., 2015a; Cao et al., 2015b] have tried to use neural networks to complement sentence ranking features.", "startOffset": 21, "endOffset": 59}, {"referenceID": 24, "context": "[Yin and Pei, 2015] trained a language model based on convolutional neural networks to project sentences onto distributed representations.", "startOffset": 0, "endOffset": 19}, {"referenceID": 13, "context": "Others like [Kobayashi et al., 2015; K\u00e5geb\u00e4ck et al., 2014] just used the sum of trained word embeddings to represent sentences or documents.", "startOffset": 12, "endOffset": 59}, {"referenceID": 12, "context": "Others like [Kobayashi et al., 2015; K\u00e5geb\u00e4ck et al., 2014] just used the sum of trained word embeddings to represent sentences or documents.", "startOffset": 12, "endOffset": 59}, {"referenceID": 7, "context": "[Filippova et al., 2015] used word embeddings and Long Short Term Memory models (LSTMs) to output readable and informative sentence compressions.", "startOffset": 0, "endOffset": 24}, {"referenceID": 21, "context": "[Rush et al., 2015; Hu et al., 2015] leveraged the neural attention model [Bahdanau et al.", "startOffset": 0, "endOffset": 36}, {"referenceID": 11, "context": "[Rush et al., 2015; Hu et al., 2015] leveraged the neural attention model [Bahdanau et al.", "startOffset": 0, "endOffset": 36}, {"referenceID": 0, "context": ", 2015] leveraged the neural attention model [Bahdanau et al., 2014] in the machine translation area to generate one-sentence summaries.", "startOffset": 45, "endOffset": 68}], "year": 2016, "abstractText": "Query relevance ranking and sentence saliency ranking are the two main tasks in extractive queryfocused summarization. Previous supervised summarization systems often perform the two tasks in isolation. However, since reference summaries are the trade-off between relevance and saliency, using them as supervision, neither of the two rankers could be trained well. This paper proposes a novel summarization system called AttSum, which tackles the two tasks jointly. It automatically learns distributed representations for sentences as well as the document cluster. Meanwhile, it applies the attention mechanism to simulate the attentive reading of human behavior when a query is given. Extensive experiments are conducted on DUC query-focused summarization benchmark datasets. Without using any hand-crafted features, AttSum achieves competitive performance. It is also observed that the sentences recognized to focus on the query indeed meet the query need.", "creator": "LaTeX with hyperref package"}}}