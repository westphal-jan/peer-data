{"id": "1604.00377", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "Reinforcement learning based local search for grouping problems: A case study on graph coloring", "abstract": "grouping problems aim to partition a group of solutions into maximal maximal disjoint subsets according to specific specific techniques and constraints. grouping problems cover a useful class beyond important combinatorial taxonomy problems that are generally computationally difficult. in this paper, peers propose a descriptive causal model for nonlinear problems, i. e., performing learning based local search ( rls ), which combines reinforcement management techniques with resource - based local routes. routing logic representing the gradient alternatives is ambiguous : evaluating mostly - known selective nonlinear problem ( static filtering ) where a very simple descent - level coloring algorithm performs accomplished. experimental projects on adaptive dimacs underlying adaptive benchmark applications indicate that rls produce competitive optimal compared beneath a several other worst - known coloring algorithms.", "histories": [["v1", "Fri, 1 Apr 2016 19:38:35 GMT  (340kb)", "http://arxiv.org/abs/1604.00377v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yangming zhou", "jin-kao hao", "b\\'eatrice duval"], "accepted": false, "id": "1604.00377"}, "pdf": {"name": "1604.00377.pdf", "metadata": {"source": "CRF", "title": "Reinforcement learning based local search for grouping problems: A case study on graph coloring", "authors": ["Yangming Zhou", "Jin-Kao Hao", "B\u00e9atrice Duval"], "emails": ["zhou.yangming@yahoo.com", "hao@info.univ-angers.fr", "bd@info.univ-angers.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 4.\n00 37\n7v 1\n[ cs\n.A I]\nGrouping problems aim to partition a set of items into multiple mutually disjoint subsets according to some specific criterion and constraints. Grouping problems cover a large class of important combinatorial optimization problems that are generally computationally difficult. In this paper, we propose a general solution approach for grouping problems, i.e., reinforcement learning based local search (RLS), which combines reinforcement learning techniques with descent-based local search. The viability of the proposed approach is verified on a well-known representative grouping problem (graph coloring) where a very simple descent-based coloring algorithm is applied. Experimental studies on popular DIMACS and COLOR02 benchmark graphs indicate that RLS achieves competitive performances compared to a number of well-known coloring algorithms.\nKey words: Grouping problems and graph coloring; Reinforcement learning and heuristics; Combinatorial optimization."}, {"heading": "1 Introduction", "text": "Grouping problems aim to partition a set of items into a collection of mutually disjoint subsets according to some specific criterion and constraints. Grouping problems naturally arise in numerous domains. Well-known grouping problems\n\u2217 Corresponding author. Email addresses: zhou.yangming@yahoo.com (Yangming Zhou), hao@info.univ-angers.fr (Jin-Kao Hao), bd@info.univ-angers.fr (Be\u0301atrice Duval).\nPreprint submitted to Elsevier 4 April 2016\ninclude, for instance, graph coloring (GCP) [12,15,17,29], timetabling [12,30], bin packing [13, 38], scheduling [28] and clustering [1]. Formally, given a set V of n distinct items, the task of a grouping problem is to partition the items of set V into k different groups gi (i = 1, . . . , k) (k can be fixed or variable), such that \u222aki=1gi = V and gi \u2229 gj = \u2205, i 6= j while taking into account some specific constraints and optimization objective. For instance, the graph coloring problem is to partition the vertices of a given graph into a minimum number of k color classes such that adjacent vertices must be put into different color classes.\nAccording to whether the number of groups k is fixed in advance, grouping problems can be divided into constant grouping problems or variable grouping problems [28]. In some contexts, the number of groups k is a fixed value of the problem, such as identical or non-identical parallel-machines scheduling problem, while in other settings, k is variable and the goal is to find a feasible grouping with a minimum number of groups, such as the bin packing problem and graph coloring problem. Grouping problems can also be classified according to the types of the groups. A grouping problem with identical groups means that all groups have similar characteristics, thus naming of the groups is irrelevant. Aforementioned examples such as identical parallel-machines scheduling, bin-packing and graph coloring belong to this category. Another category of grouping problems have non-identical groups where the groups are of different characteristics. Hence, swapping items between two groups will result in a new grouping, such as the non-identical parallel-machines scheduling problem.\nMany grouping problems, including the examples mentioned above are NPhard, thus computationally challenging. Due to the high computational complexity of these problems, exponential times are expected for any algorithm to solve such a problem exactly. On the other hand, heuristic and meta-heuristic methods are often employed to find satisfactory sub-optimal solutions in acceptable computing time, but without provable optimal guarantee of the attained solutions. A number of heuristic approaches for grouping problems, in particular based on genetic algorithms, have been proposed in the literature with varying degrees of success [13, 15, 38]. These approaches are rather complex since they are population-based and often hybridized with other search methods like local optimization.\nIn this work, we are interested in investigating a general purpose local search methodology for grouping problems which employs machine learning techniques to process information collected from the search process with the purpose of improving the performance of heuristic algorithms. Indeed, previous work has demonstrated that machine learning can contribute to improve optimization methods [3, 4, 20]. Existing research in these areas has pursued different objectives.\n\u2022 Algorithm selection and analysis. For instance, Hutter et al. used machine learning techniques such as random forests and approximate Gaussian process to model algorithm\u2019s runtime as a function of problem-specific instance features. This model can predict algorithm runtime for the propositional satisfiability problem, travelling salesperson problem and mixed integer programming problem [24]. \u2022 Learning generative models of solutions. For example, Ceberio et al. introduced the Plackett-Luce probability model to the framework of estimation of distribution algorithms and applied it to solve the linear order problem and the flow-shop scheduling problem [9]. \u2022 Learning evaluation functions. For instance, Boyan and Moore proposed the STAGE algorithm to learn an evaluation function which predicts the outcome of a local search algorithm such as hill-climbing or Walk-SAT, as a function of state features along its search trajectories. The learned evaluation function is used to bias future search trajectories towards better solutions [5]. \u2022 Understanding the search space. For example, Porumbel et al. used multidimensional scaling techniques to explore the spatial distribution of the local optimal solutions visited by tabu search, thus improving local search algorithms for the graph coloring problem [36].For the same problem, the authors of [21] used the results of an analysis of legal k-colorings to help finding solutions with fewer colors.\nIn this paper, we present the reinforcement learning based local search (RLS) approach for grouping problems, which combines reinforcement learning techniques with a descent-based local search procedure. Our proposed RLS approach belongs to the above-mentioned category of learning generative models of solutions. For a grouping problem with its k groups, we associate to an item a probability vector with respect to each possible group and determine the group of the item according to the probability vector. Once all items are assigned to their groups, a grouping solution is generated. Then, the descentbased local search procedure is invoked to improve this solution until a local optimum is attained. Afterward, the probability vector of each item is updated by comparing the group of the item in the starting solution and in the attained local optimum solution. If an item stays in its original group, then we reward the selected group of the item, otherwise we penalize the original group and compensate the new group (i.e., expected group). There are two key issues that need to be considered, i.e., how do we select a suitable group for each item according to the probability vector, and how do we smooth the probabilities to avoid potential search traps. To handle these issues, we design two strategies: a hybrid group selection strategy that uses a noise probability to switch between random selection and greedy selection; and a probability smoothing mechanism able to forget old decisions.\nTo evaluate the viability of the proposed RLS method, we use the well-\nknown graph coloring problem (GCP) as a case study. GCP is one representative grouping problem which has been object of intensive studies in the past decades. We show computational experiments on both DIMACS and COLOR02 benchmark graphs. Computational results demonstrate that the proposed approach, despite its simplicity, achieves competitive performances on most tested instances compared to many existing algorithms. With an analysis of three important issues of RLS, we show the effectiveness of combining reinforcement learning and descent-based local search. We also assess the contribution of the probability smoothing technique to the performance of RLS.\nThe rest of the paper is organized as follows. Section 2 provides an introduction of reinforcement learning and its applications to enhance heuristic search. The proposed RLS method is described in Section 3. Section 4 is dedicated to computational assessments and comparisons of RLS applied to the graph coloring problem. Concluding comments and future research directions are discussed in Section 5."}, {"heading": "2 Reinforcement learning and heuristic search", "text": "In this section, we briefly introduce the principles of reinforcement learning (RL) and provide a review of some representative examples of using reinforcement learning to solve combinatorial optimization problems."}, {"heading": "2.1 Reinforcement learning", "text": "Reinforcement learning is a learning pattern, which aims to learn optimal actions from a finite set of available actions through continuously interacting with an unknown environment. In contrast to supervised learning techniques, reinforcement learning does not need an experienced agent to show the correct way, but adjusts its future actions based on the obtained feedback signal from the environment [18].\nThere are three key elements in a RL agent, i.e., states, actions and rewards. At each instant a RL agent observes the current state, and takes an action from the set of its available actions for the current state. Once an action is performed, the RL agent changes to a new state, based on transition probabilities. Correspondingly, a feedback signal is returned to the RL agent to inform it about the quality of its performed action."}, {"heading": "2.2 Reinforcement learning and heuristic search", "text": "There are a number of studies in the literature where reinforcement learning techniques are put at the service of heuristic algorithms for solving combinatorial problems. Reinforcement learning techniques in these studies have been explored at three different levels.\nHeuristic level where RL is directly used as a heuristic to solve optimization problems. In this case, RL techniques are used to learn and directly assign values to the variables. For example, the authors of [35] proposed to solve combinatorial optimization problems based on a population of RL agents. Pairs of variable and value are considered as the RL states, and the branching strategies as the actions. Each RL agent is assigned a specific area of the search space where it has to learn and find good local solutions.\nMeta-heuristic level where RL is integrated into a meta-heuristic. There are two types of these algorithms. Firstly, RL is used to learn properties of good initial solutions or an evaluation function that guides a meta-heuristic toward high quality solutions. For example, RL is employed to learn a new evaluation function over multiple search trajectories of the same problem instance and alternates between using the learned and the original evaluation function [5]. Secondly, RL learns the best neighborhoods or heuristics to build or change a solution during the search, so that a good solution can be obtained at the end. For instance, Xu et al. [45] proposed a formulation of constraint satisfaction problems as a RL task. A number of different variable ordering heuristics are available, and RL learns which one to use, and when to use it.\nHyper-heuristic level where RL is used as a component of a hyper-heuristic. Specifically, RL is integrated into selection mechanisms and acceptance mechanisms in order to select a suitable low-level heuristic and determine when to accept a move respectively. For example, Burke et al. [7] presented a hyperheuristic in which the selection of low-level heuristics makes use of basic reinforcement learning principles combined with a tabu search mechanism. The algorithm increases or decreases the rank of the low-level heuristics when the objective function value is improving or deteriorating. Two other examples can be found in [19,40] where RL is used to schedule several search operators (crossovers, local search...) under the genetic and multi-agent based optimization frameworks.\nBoth meta-heuristic level and hyper-heuristic level approaches attempt to replace the random component of an algorithm with a RL component to obtain an informed decision mechanism. Based on the above-classification, our proposed RLS approach belongs to first type of the meta-heuristic level category. Specifically, RLS combines reinforcement learning techniques with descent-\nbased local search with the purpose of learning properties of good initial solutions."}, {"heading": "3 Reinforcement learning based local search for grouping problems", "text": "Grouping problems aim to partition a set of items into k disjoint groups according to some imperative constraints and an optimization criterion. For our RLS approach, we suppose that the number of groups k is given in advance. Note that such a assumption is not necessarily restrictive. In fact, to handle a grouping problem with variable k, one can repetitively run RLS with different k values. We will illustrate this approach on the graph coloring problem in Section 4."}, {"heading": "3.1 Main scheme", "text": "By combining reinforcement learning techniques with a solution improvement procedure, our proposed RLS approach is composed of four keys components: a descent-based local search procedure, a group selection strategy, a probability updating mechanism (i.e., reinforcement learning mechanism), and a probability smoothing technique.\nWe define a probability matrix P of size n \u00d7 k (n is the number of items and k is the number of groups, see Figure 1 for an example). An element pij denotes the probability that the i-th item vi selects the j-th group gj as its group. Therefore, the i-th row of the probability matrix defines the probability vector of the i-th item and is denoted by pi. At the beginning, all the probability values in the probability matrix are set as 1/k. It means that all items select a group from the available k groups with equal probability.\nAt instant t, each item vi, i \u2208 {1, 2, ..., n} selects one suitable group gj , j \u2208 {1, 2, ..., k} by applying a group selection strategy (Section 3.2) based on its probability vector pi(t). Once all the items are assigned to their groups, a grouping solution St is obtained. Then, this solution is improved by a descentbased local search procedure to attain a local optimum denoted by S\u0302t (Section 3.3). By comparing the solution St and the improved solution S\u0302t, we update the probability vector of each item based on the following rules (Section 3.4):\n(a) If the item stays in its original group, then we reward the selected group. (b) If the item is moved to a new group, then we penalize the selected group\nand compensate its new group (i.e., expected group).\nNext, we apply a probability smoothing technique to smooth each item\u2019s probability vector (Section 3.5). Hereafter, RLS iteratively runs until a predefined stop condition is reached (e.g., a legal solution is found or the number of iterations without improvement exceeds a maximum allowable value). The schematic diagram of RLS for grouping problems is depicted in Figure 2 while its algorithmic pseudo-code is provided in Algorithm 1. In the following subsections, the four key components of our RLS approach are presented in detail."}, {"heading": "3.2 Group selection", "text": "At each iteration of RLS, each item vi needs to select a group gj from the k available groups according to its probability vector pi. We consider four possible group selection strategies:\n\u2022 Random selection: the item selects its group at random (regardless of its\nAlgorithm 1 Pseudo-code of our RLS for grouping problems.\n1: Input: G: a grouping problem instance; k: the number of available groups;\n2: Output: the best solution S\u2217 found so far; 3: for all vi, i = 1, 2, ..., n do 4: P0 = [pij = 1/k]j=1,2,...,k; 5: end for 6: repeat 7: St \u2190 groupSelecting(Pt\u22121, \u03c9); /\u2217 Section 3.2 \u2217/ 8: S\u0302t \u2190 DB \u2212 LS(St); /\u2217 Section 3.3 \u2217/ 9: Pt \u2190 probabilityUpdating(Pt\u22121, St, S\u0302t, \u03b1, \u03b2, \u03b3); /\u2217 Section 3.4 \u2217/ 10: Pt \u2190 probabilitySmoothing(Pt, p0, \u03c1); /\u2217 Section 3.5 \u2217/ 11: until Stop condition met\nprobability vector). As this selection strategy does not use any useful information collected from the search history, it is expected that this strategy would not perform well. \u2022 Greedy selection: the item always selects the group gj such that the associated probability pij has the maximum value. This strategy is intuitively reasonable, but may cause the algorithm to be trapped rapidly. \u2022 Roulette wheel selection: the item selects its group based on its probability vector and the chance for the item to select group gj is proportional to the probability pij . Thus a group with a large (small) probability has more (less) chance to be selected. \u2022 Hybrid selection: this strategy combines the random selection and greedy selection strategies in a probabilistic way; with a noise probability \u03c9, random selection is applied; with probability 1\u2212 \u03c9, greedy selection is applied.\nAs we show in Section 4.3.3, the group selection strategy greatly affects the performance of the RLS approach. After experimenting the above strategies, we adopted the hybrid selection strategy which combines randomness and greediness which are controlled by the noise probability \u03c9. The purpose of selecting a group with maximum probability (greedy selection) is to make an attempt to correctly select the group for an item that is most often falsified at a local optimum. Selecting such a group for this item may help the search to escape from the current trap. On the other hand, using the noise probability has the advantage of flexibility by switching back and forth between greediness and randomness. Also, this allows the algorithm to occasionally move away from being too greedy. This hybrid group selection strategy proves to be better than the roulette wheel selection strategy, as confirmed by the experiments of Section 4.3.3."}, {"heading": "3.3 Descent-based local search for solution improvement", "text": "Even if any optimization procedure can be used to improve a given starting grouping solution. For the reason of simplicity, we employ a simple and fast descent-based local search (DB-LS) procedure in this work. To explore the search space, DB-LS iteratively makes transitions from the incumbent solution to a neighboring solution according to a given neighborhood relation such that each transition leads to a better solution. This iterative improvement process continues until no improved solution exists in the neighborhood in which case the incumbent solution corresponds to a local optimum with respect to the neighborhood.\nLet \u2126 denote the search space of the given grouping problem. Let N : \u2126 \u2192 2\u2126 be the neighborhood relation which associates to each solution S \u2208 \u2126 a subset of solutions N(S) \u2282 \u2126 (i.e., N(S) is the set of neighboring solutions of S). Typically, given a solution S, a neighboring solution can be obtained by moving an item of S from its current group to another group. Let f : \u2126 \u2192 R be the evaluation (or cost) function which measures the quality or cost of each grouping solution. The pseudo code of Algorithm 2 displays the general DB-LS procedure.\nAlgorithm 2 Pseudo-code of descent-based local search procedure\n1: Input: S - an initial candidate grouping solution; 2: Output: S\u2217 - the local optimum solution attained; 3: f(S\u2217) = f(S); 4: repeat 5: choose a best neighbor S \u2032\u2032\nof S such that 6: S \u2032\u2032\n= argminS\u2032\u2208N(S) f(S);\n7: S\u2217 = S \u2032\u2032\n; 8: f(S\u2217) = f(S \u2032\u2032\n) 9: S = S\u2217; 10: until f(S \u2032\u2032 ) > f(S\u2217)\nDescent-based local search can find a local optimum quickly. However, the local optimal solution discovered is generally of poor quality. It is fully possible to improve the performance of RLS by replacing the descent-based local search with a more powerful improvement algorithm. In RLS, we make the assumption that, if the item stays in its original group after the descent-based local search, then the item has selected the right group in the original solution, otherwise its new group in the improved solution would be the right group. This assumption can be considered to be reasonable because the descent-based local search procedure is driven by its cost function and each transition from the current solution to a new (neighboring) solution is performed only when the transition leads to an improvement."}, {"heading": "3.4 Reinforcement learning - probability updating", "text": "Reinforcement learning is defined as how an agent should take actions in an environment so to maximize some notion of cumulative reward. Reinforcement learning acts optimally through trial-and-error interactions with an unknown environment. Actions may affect not only the immediate reward but also the next situation and all subsequent rewards. The intuition underlying reinforcement learning is that actions that lead to large rewards should be made more likely to recur. In RLS, the problem of selecting the most appropriate group for each item is viewed as a reinforcement learning problem. Through the interactions with the unknown environment, RLS evolves and gradually finds the optimal or a suboptimal solution of the problem.\nAt instant t, we firstly generate a grouping solution St based on the current probability matrix Pt (see Section 3.1). In other words, each item selects one suitable group from the k available groups based on its probability vector (with the group selection strategy of Sect. 3.2). Then solution St is improved by the descent-based local search procedure, leading to an improved solution S\u0302t. Now, for each item vi, we compare its groups in St and S\u0302t. If the item stays in its original group (say gu), we reward the selected group gu (called correct group) and update its probability vector pi according to Eq. (1):\npij(t+ 1) =\n \n\n\u03b1 + (1\u2212 \u03b1)pij(t) j = u (1\u2212 \u03b1)pij(t) otherwise. (1)\nwhere \u03b1 (0 < \u03b1 < 1) is a reward factor. When item vi moves from its original group gu of solution St to a new group (say gv, v 6= u) of the improved solution S\u0302t, we penalize the discarded group gu (called incorrect group), compensate the new group gv (called expected group) and finally update its probability vector pi according to Eq. (2):\npij(t + 1) =\n   \n \n(1\u2212 \u03b3)(1\u2212 \u03b2)pij(t) j = u \u03b3 + (1\u2212 \u03b3) \u03b2 k\u22121 + (1\u2212 \u03b3)(1\u2212 \u03b2)pij(t) j = v (1\u2212 \u03b3) \u03b2 k\u22121 + (1\u2212 \u03b3)(1\u2212 \u03b2)pij(t) otherwise.\n(2)\nwhere \u03b2 (0 < \u03b2 < 1) and \u03b3 (0 < \u03b3 < 1) are a penalization factor and compensation factor respectively. This process is repeated until each item can select its group correctly. The update of the complete probability matrix P is bounded by O(n\u00d7 k) in terms of time complexity.\nIt is necessary to note that our learning scheme is different from general reinforcement learning schemes such as linear reward-penalty, linear rewardinaction and linear reward-\u01eb-penalty. The philosophy of these schemes is to increase the probability of selecting an action in the event of success and\ndecrease it when receives a failed signal. Unlike these general schemes, our learning scheme not only rewards the correct group and penalizes the incorrect group, but also compensates the expected group."}, {"heading": "3.5 Reinforcement learning - probability smoothing", "text": "The intuition behind the probability smoothing technique is that old decisions that were made long ago are no longer helpful and may mislead the current search. Therefore, these aged decisions should be considered less important than the recent ones. In addition, all items are required to correctly select their suitable groups in order to produce a legal grouping solution. It is not enough that only a part of items can correctly select their groups. Based on these two reasons, we introduce a probability smoothing technique to reduce the group probabilities periodically.\nOur probability smoothing strategy is inspired by forgetting mechanisms in smoothing techniques in clause weighting local search algorithms for satisfiability (SAT) [23,25]. Based on the way that weights are smoothed or forgotten, there are four available forgetting or smoothing techniques for MVC and SAT:\n\u2022 Decrease one from all clause weights which are greater than one such as PAWS [41]. \u2022 Pull all clause weights to their mean value using the formula wi = \u03c1 \u00b7 wi + (1\u2212 \u03c1) \u00b7 wi like ESG [39] and SAPS [23]. \u2022 Transfer weights from neighboring satisfied clauses to unsatisfied ones like DDWF [25]. \u2022 Reduce all edge weights using the formula wi = \u230a\u03c1 \u00b7 wi\u230b when the average weight achieves a threshold like NuMVC [8].\nThe probability smoothing strategy adopted in our RLS approach works as follows (see Algorithm 3). For an item, each possible group is associated with a value between 0 and 1 as its probability, and each group probability is initialized as 1/k. At each iteration, we adjust the probability vector based on the obtained feedback information (i.e., reward, penalize or compensate a group). Once the probability of a group in a probability vector achieves a given threshold (i.e., p0), it is reduced by multiplying a smoothing coefficient (i.e., \u03c1 < 1) to forget some earlier decisions. It is obvious that the smoothing technique used in RLS is different from the above-mentioned four techniques. To the best of our knowledge, this is the first time a smoothing technique is introduced into local search algorithms for grouping problems.\nAlgorithm 3 Pseudo-code of the probability smoothing procedure\n1: Input: Pt: probability matrix at instant t; p0: smoothing probability; \u03c1: smoothing coefficient;\n2: Output: new probability matrix Pt after smoothing; 3: for i = 1 to n do 4: piw = max{pij, j = 1, 2, ..., k}; 5: if piw > p0 then 6: for j = 1 to k do 7: if j = w then 8: pij(t) = \u03c1 \u00b7 pij(t\u2212 1); 9: else 10: pij(t) = 1\u2212\u03c1 k\u22121 \u00b7 piw(t\u2212 1) + pij(t\u2212 1); 11: end if 12: end for 13: end if 14: end for"}, {"heading": "4 RLS applied to graph coloring: a case study", "text": "This section presents an application of the proposed RLS method to the wellknown graph coloring problem which is a typical grouping problem. After presenting the descent-based local search procedure for the problem, we first conduct an experimental analysis of the RLS approach by investigating the influence of its three important components, i.e., the reinforcement learning mechanism, the probability smoothing technique and the group selection strategy. Then we present computational results attained by the proposed RLS method in comparison with a number of existing local search algorithms over well-known DIMACS and COLOR02 benchmark instances."}, {"heading": "4.1 Graph coloring and local search coloring algorithm", "text": "GCP is one of the most studied combinatorial optimization problems [17]. GCP is also a nice representative of grouping problems. Given an undirected graph G = (V,E), where V is the set of |V | = n vertices and E is the set of |E| = m edges, a legal k-coloring of G is a partition of V into k mutually disjoint groups or color classes such that two vertices linked by an edge must belong to two different color classes. GCP is to determine the smallest k for a graph G such that a legal k-coloring exists. This minimum number of groups (i.e., colors) required for a legal coloring is the chromatic number \u03c7(G). When the number of color classes k is fixed, the problem is called k-coloring problem (k-GCP for short). As a grouping problem, items correspond to vertices and\ngroups correspond to color classes.\nNotice that GCP can be approximated by solving a series of k-GCP (with decreasing k) as follows [16]. For a given G and a given k, we use our RLS approach to solve k-GCP by seeking a legal k-coloring. If such a coloring is successfully found, we decrease k and solve the new k-GCP again. We repeat this process until no legal k-coloring can be reached. In this case, the last k for which a legal k-coloring has been found represents an approximation (upper bound) of the chromatic number of G. This general solution approach has been used in many coloring algorithms including most of those reviewed below, and is adopted in our work.\nGiven the theoretical and practical interest of GCP, a huge number of coloring algorithms have been proposed in the past decades [2, 16, 27, 33]. Among them, algorithms based on local search are certainly the most popular approaches, like simulated annealing (SA) [26], tabu search (TS) [15,22], guided local search (GLS) [10], iterated local search (ILS) [11], quantum annealing algorithms [42] and focused walk based local search (FWLS) [44]. Populationbased hybrid algorithms represent another class of complex approaches which typically combine local search and dedicated recombination crossover operators [14, 15, 31, 32, 37]. Recent surveys of algorithms for GCP can be found in [16, 33].\nTo apply the proposed RLS approach to k-GCP, we need to specify three important ingredients of the descent-based local search in RLS, i.e., the search space, the neighborhood and the evaluation function. First, a legal or illegal k-coloring can be represented by S = {g1, g2, ..., gk} such that gi is the group of vertices receiving color i. Therefore, the search space \u2126 is composed of all possible legal and illegal k-colorings. The evaluation function f(S) counts the number of conflicting edges inducted by S such that:\nf(S) = \u2211\n{u,v}\u2208E\n\u03b4(u, v) (3)\nwhere \u03b4(u, v) = 1, if u \u2208 gi, v \u2208 gj and i = j, and otherwise \u03b4(u, v) = 0. Accordingly, a candidate solution S is a legal k-coloring S if f(S) = 0.\nThe neighborhood of a given k-coloring is constructed by moving a conflicting vertex v from its original group gi to another group gj(i 6= j) [15]. Therefore, for a k-coloring S with cost f(S), the size of the neighborhood is bounded by O(f(S) \u00d7 k). To evaluate each neighboring solution efficiently, our descentbased local search adopts the fast incremental evaluation technique introduced in [14,15]. The principle is to maintain a gain matrix which records the variation \u2206 = f(S \u2032)\u2212 f(S) between the incumbent solution S and every neighboring solution S \u2032. After each solution transition from S to S \u2032, only the affected elements of the gain matrix are updated accordingly.\nThe descent-based local search procedure starts then with a random solution taken from the search space \u2126 and iteratively improves this solution by a neighboring solution of better quality according to the evaluation function f . This process stops either when a legal k-coloring is found (i.e., a solution with f(S) = 0, or no better solution exists among the neighboring solutions (in this later case, a local optimum is reached)."}, {"heading": "4.2 Benchmark instances and experimental settings", "text": "We show extensive computational results on two sets of the well-known DIMACS 1 and COLOR02 2 coloring benchmark instances. These instances are the most widely used benchmark instances for assessing the performance of graph coloring algorithms.\nThe used DIMACS graphs can be divided into six types:\n\u2022 Standard random graphs are denoted as DSJCn.x, where n is the number of vertices of the graph. The chromatic number \u03c7 of these graphs are unknown. \u2022 Random geometric graphs are composed of R125.x, R250.x, DSJR500.x and R1000.x, graphs with letter c being complements of geometric graphs. \u2022 Flat graphs are structured graphs produced based on an equi-partitioning of vertices into k sets. This kind of graphs are denoted as flatn k 0, where n and k are the number of vertices and chromatic number respectively. \u2022 Leighton graphs are random graphs of density below 0.25. This kind of graphs are denoted as le450 kx, where 450 is the number of vertices, k \u2208 {15, 25} is the chromatic number of the graph, x \u2208 {a, b, c, d} is a letter to indicate different graphs with the same characteristics. \u2022 Scheduling graphs, i.e., school1 and school1 nsh. \u2022 Latin square graph, i.e., latin square 10.\nThe used COLOR02 graphs are of three types:\n\u2022 Queen graphs are highly structured instances and their edge density decreases with their size. The graphs are denoted as queenx x, where x \u2208 {5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, with an exception, i.e., queen8 12. \u2022 Mycile graphs are denoted as mycilek, where k \u2208 {3, 4, 5, 6, 7}. These graphs are based on the Mycielski transformation. \u2022 Miles Graphs (milesx, with x \u2208 {250, 500, 750, 1000, 1500}) are similar to geometric graphs in that nodes are placed in space with two nodes connected if they are close enough.\n1 Publicly available at: ftp://dimacs.rutgers.edu/pub/challenge/graph/benchmarks/color/ 2 Publicly available at: http://mat.gsia.cmu.edu/COLOR02/\nOur RLS algorithm was coded in C and compiled using GNU g++ on a machine with an Intel E5-2760 processor (2.8GHz and 2G RAM) under Linux. To obtain our experimental results, each instance was solved 20 times independently with different random seeds. Each execution was terminated when a legal k-coloring is found or the number of iterations without improvement reaches its maximum allowable value (Imax = 10\n6). In our experiments, all parameters were fixed except for the penalization factor \u03b2 that varies between 0 and 0.45. Table 1 gives the descriptions and settings of the parameters used for our experiments."}, {"heading": "4.3 Analysis of key components of the RLS approach", "text": "We first show an analysis of the main ingredients of the RLS approach: reinforcement learning mechanism, probability smoothing technique and group selection strategies. This study allows us to better understand the behavior of the proposed RLS approach and shed lights on its inner functioning."}, {"heading": "4.3.1 Effectiveness of the reinforcement learning mechanism", "text": "To verify the effectiveness of the reinforcement learning mechanism used in RLS, we make a comparison between RLS and its variant RLS0 where we removed the reinforcement learning mechanism from RLS and randomly restart the search when the DB-LS procedure attains a local optimum.\nThe investigation was conducted on the 32 DIMACS instances and each algorithm was run 20 times to solve each instance. The comparative results of RLS and RLS0 are provided in Table 2. For each graph, we list the known chromatic number \u03c7 or the best k\u2217 reported in the literature when \u03c7 is still unknown. For each algorithm, we indicate the number of the best (the smallest) k value for which the algorithm attains a legal k-coloring and the number of such successful runs over 20 executions (#hit). The differences between the best k of RLS0 and the best k of RLS are provided in the last column. The results show\nthat RLS significantly outperforms RLS0 in terms of the best k value for 29 out of 32 instances (indicated in bold). For example, on instance flat 26 0, RLS attains the chromatic number k (i.e., \u03c7 = 26) while RLS0 needs 45 colors to color it legally. Specially, we observe that RLS has a larger improvement on hard instances than on easy instances. For instance, latin square 10 and flat 76 0 are two well-known hard instances, RLS achieves two largest improvements, i.e., using 70 and 46 fewer colors than RLS0. In summary, RLS\nattains better results on 29 out of 32 instances compared to its variant with the reinforcement learning mechanism disabled. This experiment confirms the effectiveness of the reinforcement learning mechanism to help the descent-based local search to attain much better results."}, {"heading": "4.3.2 Effectiveness of the probability smoothing technique", "text": "To study the effectiveness of the probability smoothing technique used in RLS, we compare RLS with its alternative algorithm RLS1, which is obtained from RLS by adjusting the probability updating scheme. More specifically, RLS1 works in the same way as RLS, but it does not use the probability smoothing strategy, that is, line 12 in Algorithm 1 is removed. For this experiment, by following [15], we use running profiles to observe the change of evaluation function f over the number of iterations. Running profiles provide interesting information about the convergence of the studied algorithms.\nThe running profiles of RLS and RLS1 are shown in Figure 3 on two selected instances: Fig. 3(a) for flat300 28 0 (k = 32), and Fig.3(b) for latin square 10 (k = 101). We observe that though both algorithms successfully obtain a legal k-coloring, RLS converges to the best solution more quickly than RLS1, i.e., the objective value f of RLS decreases more quickly than that of RLS1. Consequently, RLS needs less iterations to attain a legal solution. This experiment\ndemonstrated the benefit of using probability smoothing technique in RLS."}, {"heading": "4.3.3 Comparison of different group selection strategies", "text": "The group selection strategy plays an important role in RLS. At each iteration, each vertex selects a suitable group based on the group selection strategy to produce a new solution for the next round of the descent-based local search optimization. In this section, we show an analysis of the group selection strategies to confirm the interest of the adopted hybrid strategy which combines random and greedy strategies.\nThe investigation was carried out between RLS and its variant RLS2, which is obtained from RLS by means of replacing the hybrid group selection strategy with the roulette wheel selection strategy. In the experiment, each instance was tested 20 times independently with different random seeds. The number of successful runs, the average number of iterations and the average running time of successful runs are reported.\nTable 3 show comparative results of RLS with RLS2 for the chosen instances. The results indicate that RLS significantly outperforms RLS2 in terms of the best k value and the number of successful running times. For example, on instance DSJR500.1c, RLS colors this graph with 85 colors, while RLS2 needs more colors (k = 87) to color it. A similar observation can be found on instance le450 25c, for which RLS obtains a legal 26-coloring, while RLS2 only obtains a 27-coloring. Furthermore, when they need the same number of colors to color a graph DSJC1000.1, RLS achieves it with a higher success rate compared to RLS2. This experiment confirms the interest of the adopted hybrid selection strategy."}, {"heading": "4.4 Computational results of RLS and comparisons", "text": "We turn now our attention to a comparative study of the proposed RLS approach with respect to some well-known coloring algorithms in the literature. This study focuses on five algorithms based on advanced local search methods including the prominent simulating annealing (SA) algorithm [26], the improved tabu search (TS) algorithm [15], the guided local search (GLS) algorithm [10], the iterative local search (ILS) algorithm [11] and the focused walk based local search (FWLS) algorithm [44]. This comparison is not exhaustive, yet it allows us to assess the effectiveness of using the learning mechanism to boost a very simple descent procedure.\nWe present in Tables 4 and 5 the results of RLS together with the best solutions of these algorithms. We list the number of vertices (n) and edges (m) of each graph, the known chromatic number \u03c7 or the best k\u2217 reported in the literature when \u03c7 is still unknown. For each algorithm, we list the best (the smallest) k for which a legal k-coloring is attained. A summary of the comparisons between our RLS algorithm and each reference algorithm is provided at the bottom of these tables. The rows \u2018better\u2019, \u2018equal\u2019, and \u2018worse\u2019 respectively represent the number of instances for which our RLS algorithm achieves a better, an equal, and a worse solution than the corresponding reference algorithm over the total number of instances for which the algorithm is tested. The results of the reference algorithms are extracted from the literature except for TS which was run on the same computing platform as RLS. In these tables, \u2018\u2212\u2019 indicate that the result of the algorithm on this instance is unavailable in the literature. When a result of RLS is no worse than any result of the competing algorithms, this result is marked in bold.\nFrom Table 4 which concerns the DIMACS graphs, we observe that our RLS algorithm achieves a competitive and even better performance on some graphs. For instance, compared to SA, RLS finds 16 better best solutions out of the 23 instances tested by SA. The comparison with TS is more informative and meaningful given that RLS and TS share the same data structures, both were programmed in C and were run on the same computer. We observe that despite its simple descent local search procedure, RLS attains better results than TS thanks to its learning mechanism. Additionally, we observe that RLS achieves much more \u2018better\u2019 results than \u2018worse\u2019 results compared to the reference algorithms except for ILS. Nevertheless, since the results of ILS for 9 instances are unavailable, it is difficult to draw a clear conclusion.\nWhen we compare our RLS algorithm with the five reference algorithms on the COLOR02 graph instances (Table 5), we observe that RLS dominates these algorithms. Specifically, RLS achieves no worse results on these instances than any of the reference algorithms, and obtains 4 better solutions than SA and\nGLS, 1 better solution than TS, ILS, and FWLS respectively.\nWe also find that our proposed RLS method even achieves competitive performances compared to some complex population algorithms proposed in recent years, such as ant-based algorithm [6] (2008), and modified cuckoo optimiza-\ntion algorithm [34] (2015). However, given the very simplicity of its underlying local search procedure, it is no surprise that RLS alone cannot compete with the most powerful coloring algorithms like [15,31,32,37,42,43]. Indeed, these algorithm are typically complex hybrid algorithms mixing several approaches like genetic computing and local optimization. On the other hand, given the way the proposed RLS approach is composed, it would be interesting to replace the simple descent-based local search by any of these advanced coloring algorithms and investigate the proposed reinforcement learning mechanism in comparison with these advanced coloring algorithms."}, {"heading": "5 Conclusion and discussion", "text": "In this paper, we proposed a reinforcement learning based optimization approach for solving the class of grouping problems. The proposed RLS approach combines reinforcement learning techniques with a descent-based local search procedure. Reinforcement learning is used to maintain and update a set of probability vectors, each probability vector specifying the probability that an item belongs to a particular group. At each iteration, RLS builds a starting grouping solution according to the probability vectors and with the help of a group selection strategy. RLS then applies a descent-based local search procedure to improve the given grouping solution until a local optimum is reached. At this point, the starting solution and the ending local optimum solution are compared to update the probability vector of each item according to the situation of the item. Specifically, RLS rewards the selected group of the item if the item stays in the original group, otherwise RLS penalizes the selected group and compensates the new group.\nExperimental analyses and performance assessments of the RLS approach were carried out on the graph coloring problem which is a well-known grouping problem. Based on experimental results on popular DIMACS and COLOR02 benchmark graphs, we showed that 1) reinforcement learning is highly valuable to increase the performance of the descent-based local search procedure; 2) the probability smoothing technique which forgets old decisions is very useful to avoid search traps; and 3) the hybrid group selection strategy combining randomness and greediness is more suitable than other selection strategies.\nIn terms of computational results, RLS, despite the simplicity of its basic coloring procedure, proved to be competitive compared to five advanced local search algorithms. It performs even better than some recent and complex optimization algorithms like the ant-based algorithm [6] and modified cuckoo optimization algorithm [34]. On the other hand, given the competitiveness of the graph coloring problems, RLS cannot really competes with the most advanced coloring algorithms which are often based on complex hybrid schemes. Fortunately, given the way of reinforcement learning being used in RLS, it is reasonable to believe that the proposed reinforcement learning techniques could be combined with these advanced coloring approaches, e.g., by replacing the descent procedure with a more powerful algorithm within the RLS approach. Such a possibility constitutes one of our future research.\nFinally, another future work is to apply the proposed approach to solve other grouping problems. For this purpose, it is necessary to devise a descent local search procedure (or any other solution improvement procedure) for the studied problem while the other ingredients of the RLS approach can be kept unchanged."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the PGMO project (2014-2015, Jacques Hadamard Mathematical Foundation, Paris). The financial support for Yangming Zhou from the China Scholarship Council (CSC, 2014-2018) is acknowledged."}], "references": [{"title": "A new grouping genetic algorithm for clustering problems", "author": ["L. Agustn-Blas", "S. Salcedo-Sanz", "S. Jim\u00e9nez-Fern\u00e1ndez", "L. Carro-Calvo", "J.D. Ser", "J. Portilla-Figueras"], "venue": "Expert Systems with Applications,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "A cellular learning automatabased algorithm for solving the vertex coloring problem", "author": ["J.A. Torkestani", "M.R. Meybodi"], "venue": "Expert Systems with Applications,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Statistical machine learning for large-scale optimization", "author": ["S. Baluja", "A. Barto", "K. Boese", "J. Boyan", "W. Buntine", "T. Carson", "R. Caruana", "D. Cook", "S. Davies", "T. Dean"], "venue": "Neural Computing Surveys,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "The LION way. Machine Learning plus Intelligent Optimization, LIONlab", "author": ["R. Battiti", "M. Brunato"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Learning evaluation functions to improve optimization by local search", "author": ["J.A. Boyan", "A.W. Moore"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "An ant-based algorithm for coloring graphs", "author": ["T.N. Bui", "T.H. Nguyen", "C.M. Patel", "K.T. Phan"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "A Tabu-Search Hyper-heuristic for Timetabling and Rostering", "author": ["E. Burke", "G. Kendall", "E. Soubeiga"], "venue": "Journal of Heuristics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "NuMVC: An efficient local search algorithm for minimum vertex cover", "author": ["S. Cai", "K. Su", "C. Luo", "A. Sattar"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "The Plackett-Luce ranking model on permutation-based optimization problems", "author": ["J. Ceberio", "A. Mendiburu", "J.A. Lozano"], "venue": "Proceedings of the IEEE Congress on Evolutionary Computation", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Stochastic local search methods for highly constrained combinatorial optimisation problems, Ph.D", "author": ["M. Chiarandini"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "An application of iterated local search to graph coloring problem", "author": ["M. Chiarandini", "T. St\u00fctzle"], "venue": "Proceedings of the Computational Symposium on Graph Coloring and its Generalizations,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "A grouping hyper-heuristic framework: Application on graph coloring", "author": ["A. Elhag", "E. \u00d6zcan"], "venue": "Expert Systems with Applications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Genetic Algorithms and Grouping Problems", "author": ["E. Falkenauer"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Genetic and hybrid algorithms for graph coloring", "author": ["C. Fleurent", "J. Ferland"], "venue": "Annals of Operations Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "Hybrid evolutionary algorithms for graph coloring", "author": ["P. Galinier", "J.K. Hao"], "venue": "Journal of Combinatorial Optimization,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Recent advances in graph vertex coloring", "author": ["P. Galinier", "J.P. Hamiez", "J.K. Hao", "D. Porumbel"], "venue": "In Handbook of optimization,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Computers and Intractability: A Guide to the Theory of NP-Completness", "author": ["M. Garey", "D. Johnson"], "venue": "W.H. Freeman and Co.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1979}, {"title": "Reinforcement learning: A tutorial survey and recent advances", "author": ["A. Gosavi"], "venue": "INFORMS Journal on Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "A multi-agent based selfadaptive genetic algorithm for the long-term car pooling problem", "author": ["Y. Guo", "G. Goncalves", "T. Hsu"], "venue": "Journal of Mathematical Modelling and Algorithms in Operations Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Particle Swarm Algorithm variants for the Quadratic Assignment Problems - A probabilistic learning approach", "author": ["F. Hafiz", "A. Abdennour"], "venue": "Expert Systems with Applications,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "An analysis of solution properties of the graph coloring problem. Metaheuristics: Computer Decision-Making, Chapter 15, pp325-346", "author": ["J.P. Hamiez", "J.K. Hao"], "venue": "Resende M.G.C. and de Sousa J.P. (Eds.), Kluwer", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1993}, {"title": "Using tabu search techniques for graph", "author": ["A. Hertz", "D. de Werra"], "venue": "coloring, Computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1987}, {"title": "Scaling and probabilistic smoothing: efficient dynamic local search for SAT, Principles and Practice of Constraint Programming", "author": ["F. Hutter", "D.A.D. Tompkins", "H.H. Hoos"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Algorithm runtime prediction: methods & evaluation", "author": ["F. Hutter", "L. Xu", "H.H. Hoos", "K. Leyton-Brown"], "venue": "Artificial Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Neighbourhood clause weight redistribution in local search for SAT, Principles and Practice of Constraint Programming", "author": ["A. Ishtaiwi", "J. Thornton", "A. Sattar", "D.N. Pham"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Optimization by simulated annealing: an experimental evaluation; part II, graph coloring and number partitioning", "author": ["D.S. Johnson", "C.R. Aragon", "L.A. McGeoch", "C. Schevon"], "venue": "Operations Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1991}, {"title": "Cliques, Coloring, and Satisfiability", "author": ["D.S. Johnson", "Trick M"], "venue": "DIMACS Series in Discrete Math. and Theor. Comput. Sci. ,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1996}, {"title": "A particle swarm optimizer for grouping problems", "author": ["A.H. Kashan", "M.H. Kashan", "S. Karimiyan"], "venue": "Information Sciences,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "A general-purpose hill-climbing method for order independent minimum grouping problems: A case study in graph colouring and bin packing", "author": ["R. Lewis"], "venue": "Computers & Operations Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Finding feasible timetables using group-based operators, Evolutionary Computation", "author": ["R. Lewis", "B. Paechter"], "venue": "IEEE Transactions on,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "A memetic algorithm for graph coloring", "author": ["Z. L\u00fc", "J. Hao"], "venue": "European Journal of Operational Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "A metaheuristic approach for the vertex coloring problem", "author": ["E. Malaguti", "M. Monaci", "P. Toth"], "venue": "INFORMS Journal on Computing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "A survey on vertex coloring problems", "author": ["E. Malaguti", "P. Toth"], "venue": "International Transactions in Operational Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Modified cuckoo optimization algorithm (MCOA) to solve graph coloring problem", "author": ["S. Mahmoudi", "S. Lotfi"], "venue": "Applied Soft Computing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Global search in combinatorial optimization using reinforcement learning algorithms, Proceedings of the Congress on Evolutionary Computation (CEC)", "author": ["V.V. Miagkikh", "W.F. Punch III"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1999}, {"title": "A search space \u201ccartography\u201d for guiding graph coloring heuristics", "author": ["D.C. Porumbel", "J.K. Hao", "P. Kuntz"], "venue": "Computers & Operations Research,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "An Evolutionary Approach with Diversity Guarantee and Well-Informed Grouping Recombination for Graph Coloring", "author": ["D.C. Porumbel", "Hao", "J.-K", "P. Kuntz"], "venue": "Computers & Operations Research", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "A grouping genetic algorithm with controlled gene transmission for the bin packing problem", "author": ["M. Quiroz-Castellanos", "L. Cruz-Reyes", "J. Torres-Jim\u00e9nez", "C. G\u00f3mez", "H.J. Huacuja", "Alvim", "A.C. F"], "venue": "Computers & Operations Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "The exponentiated subgradient algorithm for heuristic boolean programming", "author": ["D. Schuurmans", "F. Southey", "R.C. Holte"], "venue": "In Proceedings of the 7th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2001}, {"title": "A multi-agent based optimization method applied to the quadratic assignment problem", "author": ["I. Sghir", "J.K. Hao", "I.B. Jaafar", "K. Ghdira"], "venue": "Expert Systems with Applications,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Additive versus multiplicative clause weighting for SAT", "author": ["J. Thornton", "N.P. Duc", "Stuart B", "Valnir F.Jr."], "venue": "In Proceedings of the Conference of the American Association for Artificial Intelligence (AAAI),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2004}, {"title": "Quantum annealing of the graph coloring problem", "author": ["O. Titiloye", "A. Crispin"], "venue": "Discrete Optimization,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2011}, {"title": "Coloring large graphs based on independent set extraction", "author": ["Q. Wu", "J.K. Hao"], "venue": "Computers & Operations Research,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "FWLS: A Local Search for Graph Coloring, Frontiers in Algorithmics and Algorithmic Aspects in Information and Management", "author": ["W. Wu", "C. Luo", "K. Su"], "venue": "Third Joint International Conference,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "Learning adaptation to solve constraint satisfaction problems, Proceedings of Learning and Intelligent Optimization (LION", "author": ["Y. Xu", "D. Stern", "H. Samulowitz"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2009}], "referenceMentions": [{"referenceID": 11, "context": "include, for instance, graph coloring (GCP) [12,15,17,29], timetabling [12,30], bin packing [13, 38], scheduling [28] and clustering [1].", "startOffset": 44, "endOffset": 57}, {"referenceID": 14, "context": "include, for instance, graph coloring (GCP) [12,15,17,29], timetabling [12,30], bin packing [13, 38], scheduling [28] and clustering [1].", "startOffset": 44, "endOffset": 57}, {"referenceID": 16, "context": "include, for instance, graph coloring (GCP) [12,15,17,29], timetabling [12,30], bin packing [13, 38], scheduling [28] and clustering [1].", "startOffset": 44, "endOffset": 57}, {"referenceID": 28, "context": "include, for instance, graph coloring (GCP) [12,15,17,29], timetabling [12,30], bin packing [13, 38], scheduling [28] and clustering [1].", "startOffset": 44, "endOffset": 57}, {"referenceID": 11, "context": "include, for instance, graph coloring (GCP) [12,15,17,29], timetabling [12,30], bin packing [13, 38], scheduling [28] and clustering [1].", "startOffset": 71, "endOffset": 78}, {"referenceID": 29, "context": "include, for instance, graph coloring (GCP) [12,15,17,29], timetabling [12,30], bin packing [13, 38], scheduling [28] and clustering [1].", "startOffset": 71, "endOffset": 78}, {"referenceID": 12, "context": "include, for instance, graph coloring (GCP) [12,15,17,29], timetabling [12,30], bin packing [13, 38], scheduling [28] and clustering [1].", "startOffset": 92, "endOffset": 100}, {"referenceID": 37, "context": "include, for instance, graph coloring (GCP) [12,15,17,29], timetabling [12,30], bin packing [13, 38], scheduling [28] and clustering [1].", "startOffset": 92, "endOffset": 100}, {"referenceID": 27, "context": "include, for instance, graph coloring (GCP) [12,15,17,29], timetabling [12,30], bin packing [13, 38], scheduling [28] and clustering [1].", "startOffset": 113, "endOffset": 117}, {"referenceID": 0, "context": "include, for instance, graph coloring (GCP) [12,15,17,29], timetabling [12,30], bin packing [13, 38], scheduling [28] and clustering [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 27, "context": "According to whether the number of groups k is fixed in advance, grouping problems can be divided into constant grouping problems or variable grouping problems [28].", "startOffset": 160, "endOffset": 164}, {"referenceID": 12, "context": "A number of heuristic approaches for grouping problems, in particular based on genetic algorithms, have been proposed in the literature with varying degrees of success [13, 15, 38].", "startOffset": 168, "endOffset": 180}, {"referenceID": 14, "context": "A number of heuristic approaches for grouping problems, in particular based on genetic algorithms, have been proposed in the literature with varying degrees of success [13, 15, 38].", "startOffset": 168, "endOffset": 180}, {"referenceID": 37, "context": "A number of heuristic approaches for grouping problems, in particular based on genetic algorithms, have been proposed in the literature with varying degrees of success [13, 15, 38].", "startOffset": 168, "endOffset": 180}, {"referenceID": 2, "context": "Indeed, previous work has demonstrated that machine learning can contribute to improve optimization methods [3, 4, 20].", "startOffset": 108, "endOffset": 118}, {"referenceID": 3, "context": "Indeed, previous work has demonstrated that machine learning can contribute to improve optimization methods [3, 4, 20].", "startOffset": 108, "endOffset": 118}, {"referenceID": 19, "context": "Indeed, previous work has demonstrated that machine learning can contribute to improve optimization methods [3, 4, 20].", "startOffset": 108, "endOffset": 118}, {"referenceID": 23, "context": "This model can predict algorithm runtime for the propositional satisfiability problem, travelling salesperson problem and mixed integer programming problem [24].", "startOffset": 156, "endOffset": 160}, {"referenceID": 8, "context": "introduced the Plackett-Luce probability model to the framework of estimation of distribution algorithms and applied it to solve the linear order problem and the flow-shop scheduling problem [9].", "startOffset": 191, "endOffset": 194}, {"referenceID": 4, "context": "The learned evaluation function is used to bias future search trajectories towards better solutions [5].", "startOffset": 100, "endOffset": 103}, {"referenceID": 35, "context": "used multidimensional scaling techniques to explore the spatial distribution of the local optimal solutions visited by tabu search, thus improving local search algorithms for the graph coloring problem [36].", "startOffset": 202, "endOffset": 206}, {"referenceID": 20, "context": "For the same problem, the authors of [21] used the results of an analysis of legal k-colorings to help finding solutions with fewer colors.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "In contrast to supervised learning techniques, reinforcement learning does not need an experienced agent to show the correct way, but adjusts its future actions based on the obtained feedback signal from the environment [18].", "startOffset": 220, "endOffset": 224}, {"referenceID": 34, "context": "For example, the authors of [35] proposed to solve combinatorial optimization problems based on a population of RL agents.", "startOffset": 28, "endOffset": 32}, {"referenceID": 4, "context": "For example, RL is employed to learn a new evaluation function over multiple search trajectories of the same problem instance and alternates between using the learned and the original evaluation function [5].", "startOffset": 204, "endOffset": 207}, {"referenceID": 44, "context": "[45] proposed a formulation of constraint satisfaction problems as a RL task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] presented a hyperheuristic in which the selection of low-level heuristics makes use of basic reinforcement learning principles combined with a tabu search mechanism.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "Two other examples can be found in [19,40] where RL is used to schedule several search operators (crossovers, local search.", "startOffset": 35, "endOffset": 42}, {"referenceID": 39, "context": "Two other examples can be found in [19,40] where RL is used to schedule several search operators (crossovers, local search.", "startOffset": 35, "endOffset": 42}, {"referenceID": 22, "context": "Our probability smoothing strategy is inspired by forgetting mechanisms in smoothing techniques in clause weighting local search algorithms for satisfiability (SAT) [23,25].", "startOffset": 165, "endOffset": 172}, {"referenceID": 24, "context": "Our probability smoothing strategy is inspired by forgetting mechanisms in smoothing techniques in clause weighting local search algorithms for satisfiability (SAT) [23,25].", "startOffset": 165, "endOffset": 172}, {"referenceID": 40, "context": "\u2022 Decrease one from all clause weights which are greater than one such as PAWS [41].", "startOffset": 79, "endOffset": 83}, {"referenceID": 38, "context": "\u2022 Pull all clause weights to their mean value using the formula wi = \u03c1 \u00b7 wi + (1\u2212 \u03c1) \u00b7 wi like ESG [39] and SAPS [23].", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "\u2022 Pull all clause weights to their mean value using the formula wi = \u03c1 \u00b7 wi + (1\u2212 \u03c1) \u00b7 wi like ESG [39] and SAPS [23].", "startOffset": 113, "endOffset": 117}, {"referenceID": 24, "context": "\u2022 Transfer weights from neighboring satisfied clauses to unsatisfied ones like DDWF [25].", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "\u2022 Reduce all edge weights using the formula wi = \u230a\u03c1 \u00b7 wi\u230b when the average weight achieves a threshold like NuMVC [8].", "startOffset": 114, "endOffset": 117}, {"referenceID": 16, "context": "GCP is one of the most studied combinatorial optimization problems [17].", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "Notice that GCP can be approximated by solving a series of k-GCP (with decreasing k) as follows [16].", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "Given the theoretical and practical interest of GCP, a huge number of coloring algorithms have been proposed in the past decades [2, 16, 27, 33].", "startOffset": 129, "endOffset": 144}, {"referenceID": 15, "context": "Given the theoretical and practical interest of GCP, a huge number of coloring algorithms have been proposed in the past decades [2, 16, 27, 33].", "startOffset": 129, "endOffset": 144}, {"referenceID": 26, "context": "Given the theoretical and practical interest of GCP, a huge number of coloring algorithms have been proposed in the past decades [2, 16, 27, 33].", "startOffset": 129, "endOffset": 144}, {"referenceID": 32, "context": "Given the theoretical and practical interest of GCP, a huge number of coloring algorithms have been proposed in the past decades [2, 16, 27, 33].", "startOffset": 129, "endOffset": 144}, {"referenceID": 25, "context": "Among them, algorithms based on local search are certainly the most popular approaches, like simulated annealing (SA) [26], tabu search (TS) [15,22], guided local search (GLS) [10], iterated local search (ILS) [11], quantum annealing algorithms [42] and focused walk based local search (FWLS) [44].", "startOffset": 118, "endOffset": 122}, {"referenceID": 14, "context": "Among them, algorithms based on local search are certainly the most popular approaches, like simulated annealing (SA) [26], tabu search (TS) [15,22], guided local search (GLS) [10], iterated local search (ILS) [11], quantum annealing algorithms [42] and focused walk based local search (FWLS) [44].", "startOffset": 141, "endOffset": 148}, {"referenceID": 21, "context": "Among them, algorithms based on local search are certainly the most popular approaches, like simulated annealing (SA) [26], tabu search (TS) [15,22], guided local search (GLS) [10], iterated local search (ILS) [11], quantum annealing algorithms [42] and focused walk based local search (FWLS) [44].", "startOffset": 141, "endOffset": 148}, {"referenceID": 9, "context": "Among them, algorithms based on local search are certainly the most popular approaches, like simulated annealing (SA) [26], tabu search (TS) [15,22], guided local search (GLS) [10], iterated local search (ILS) [11], quantum annealing algorithms [42] and focused walk based local search (FWLS) [44].", "startOffset": 176, "endOffset": 180}, {"referenceID": 10, "context": "Among them, algorithms based on local search are certainly the most popular approaches, like simulated annealing (SA) [26], tabu search (TS) [15,22], guided local search (GLS) [10], iterated local search (ILS) [11], quantum annealing algorithms [42] and focused walk based local search (FWLS) [44].", "startOffset": 210, "endOffset": 214}, {"referenceID": 41, "context": "Among them, algorithms based on local search are certainly the most popular approaches, like simulated annealing (SA) [26], tabu search (TS) [15,22], guided local search (GLS) [10], iterated local search (ILS) [11], quantum annealing algorithms [42] and focused walk based local search (FWLS) [44].", "startOffset": 245, "endOffset": 249}, {"referenceID": 43, "context": "Among them, algorithms based on local search are certainly the most popular approaches, like simulated annealing (SA) [26], tabu search (TS) [15,22], guided local search (GLS) [10], iterated local search (ILS) [11], quantum annealing algorithms [42] and focused walk based local search (FWLS) [44].", "startOffset": 293, "endOffset": 297}, {"referenceID": 13, "context": "Populationbased hybrid algorithms represent another class of complex approaches which typically combine local search and dedicated recombination crossover operators [14, 15, 31, 32, 37].", "startOffset": 165, "endOffset": 185}, {"referenceID": 14, "context": "Populationbased hybrid algorithms represent another class of complex approaches which typically combine local search and dedicated recombination crossover operators [14, 15, 31, 32, 37].", "startOffset": 165, "endOffset": 185}, {"referenceID": 30, "context": "Populationbased hybrid algorithms represent another class of complex approaches which typically combine local search and dedicated recombination crossover operators [14, 15, 31, 32, 37].", "startOffset": 165, "endOffset": 185}, {"referenceID": 31, "context": "Populationbased hybrid algorithms represent another class of complex approaches which typically combine local search and dedicated recombination crossover operators [14, 15, 31, 32, 37].", "startOffset": 165, "endOffset": 185}, {"referenceID": 36, "context": "Populationbased hybrid algorithms represent another class of complex approaches which typically combine local search and dedicated recombination crossover operators [14, 15, 31, 32, 37].", "startOffset": 165, "endOffset": 185}, {"referenceID": 15, "context": "Recent surveys of algorithms for GCP can be found in [16, 33].", "startOffset": 53, "endOffset": 61}, {"referenceID": 32, "context": "Recent surveys of algorithms for GCP can be found in [16, 33].", "startOffset": 53, "endOffset": 61}, {"referenceID": 14, "context": "The neighborhood of a given k-coloring is constructed by moving a conflicting vertex v from its original group gi to another group gj(i 6= j) [15].", "startOffset": 142, "endOffset": 146}, {"referenceID": 13, "context": "To evaluate each neighboring solution efficiently, our descentbased local search adopts the fast incremental evaluation technique introduced in [14,15].", "startOffset": 144, "endOffset": 151}, {"referenceID": 14, "context": "To evaluate each neighboring solution efficiently, our descentbased local search adopts the fast incremental evaluation technique introduced in [14,15].", "startOffset": 144, "endOffset": 151}, {"referenceID": 14, "context": "For this experiment, by following [15], we use running profiles to observe the change of evaluation function f over the number of iterations.", "startOffset": 34, "endOffset": 38}, {"referenceID": 25, "context": "This study focuses on five algorithms based on advanced local search methods including the prominent simulating annealing (SA) algorithm [26], the improved tabu search (TS) algorithm [15], the guided local search (GLS) algorithm [10], the iterative local search (ILS) algorithm [11] and the focused walk based local search (FWLS) algorithm [44].", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "This study focuses on five algorithms based on advanced local search methods including the prominent simulating annealing (SA) algorithm [26], the improved tabu search (TS) algorithm [15], the guided local search (GLS) algorithm [10], the iterative local search (ILS) algorithm [11] and the focused walk based local search (FWLS) algorithm [44].", "startOffset": 183, "endOffset": 187}, {"referenceID": 9, "context": "This study focuses on five algorithms based on advanced local search methods including the prominent simulating annealing (SA) algorithm [26], the improved tabu search (TS) algorithm [15], the guided local search (GLS) algorithm [10], the iterative local search (ILS) algorithm [11] and the focused walk based local search (FWLS) algorithm [44].", "startOffset": 229, "endOffset": 233}, {"referenceID": 10, "context": "This study focuses on five algorithms based on advanced local search methods including the prominent simulating annealing (SA) algorithm [26], the improved tabu search (TS) algorithm [15], the guided local search (GLS) algorithm [10], the iterative local search (ILS) algorithm [11] and the focused walk based local search (FWLS) algorithm [44].", "startOffset": 278, "endOffset": 282}, {"referenceID": 43, "context": "This study focuses on five algorithms based on advanced local search methods including the prominent simulating annealing (SA) algorithm [26], the improved tabu search (TS) algorithm [15], the guided local search (GLS) algorithm [10], the iterative local search (ILS) algorithm [11] and the focused walk based local search (FWLS) algorithm [44].", "startOffset": 340, "endOffset": 344}, {"referenceID": 14, "context": "Instance n m \u03c7/k RLS TS [15] SA [26] GLS [10] ILS [11] FWLS [44]", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "Instance n m \u03c7/k RLS TS [15] SA [26] GLS [10] ILS [11] FWLS [44]", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "Instance n m \u03c7/k RLS TS [15] SA [26] GLS [10] ILS [11] FWLS [44]", "startOffset": 41, "endOffset": 45}, {"referenceID": 10, "context": "Instance n m \u03c7/k RLS TS [15] SA [26] GLS [10] ILS [11] FWLS [44]", "startOffset": 50, "endOffset": 54}, {"referenceID": 43, "context": "Instance n m \u03c7/k RLS TS [15] SA [26] GLS [10] ILS [11] FWLS [44]", "startOffset": 60, "endOffset": 64}, {"referenceID": 5, "context": "We also find that our proposed RLS method even achieves competitive performances compared to some complex population algorithms proposed in recent years, such as ant-based algorithm [6] (2008), and modified cuckoo optimiza-", "startOffset": 182, "endOffset": 185}, {"referenceID": 14, "context": "Instance n m \u03c7/k RLS TS [15] SA [26] GLS [10] ILS [11] FWLS [44]", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "Instance n m \u03c7/k RLS TS [15] SA [26] GLS [10] ILS [11] FWLS [44]", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "Instance n m \u03c7/k RLS TS [15] SA [26] GLS [10] ILS [11] FWLS [44]", "startOffset": 41, "endOffset": 45}, {"referenceID": 10, "context": "Instance n m \u03c7/k RLS TS [15] SA [26] GLS [10] ILS [11] FWLS [44]", "startOffset": 50, "endOffset": 54}, {"referenceID": 43, "context": "Instance n m \u03c7/k RLS TS [15] SA [26] GLS [10] ILS [11] FWLS [44]", "startOffset": 60, "endOffset": 64}, {"referenceID": 33, "context": "tion algorithm [34] (2015).", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "However, given the very simplicity of its underlying local search procedure, it is no surprise that RLS alone cannot compete with the most powerful coloring algorithms like [15,31,32,37,42,43].", "startOffset": 173, "endOffset": 192}, {"referenceID": 30, "context": "However, given the very simplicity of its underlying local search procedure, it is no surprise that RLS alone cannot compete with the most powerful coloring algorithms like [15,31,32,37,42,43].", "startOffset": 173, "endOffset": 192}, {"referenceID": 31, "context": "However, given the very simplicity of its underlying local search procedure, it is no surprise that RLS alone cannot compete with the most powerful coloring algorithms like [15,31,32,37,42,43].", "startOffset": 173, "endOffset": 192}, {"referenceID": 36, "context": "However, given the very simplicity of its underlying local search procedure, it is no surprise that RLS alone cannot compete with the most powerful coloring algorithms like [15,31,32,37,42,43].", "startOffset": 173, "endOffset": 192}, {"referenceID": 41, "context": "However, given the very simplicity of its underlying local search procedure, it is no surprise that RLS alone cannot compete with the most powerful coloring algorithms like [15,31,32,37,42,43].", "startOffset": 173, "endOffset": 192}, {"referenceID": 42, "context": "However, given the very simplicity of its underlying local search procedure, it is no surprise that RLS alone cannot compete with the most powerful coloring algorithms like [15,31,32,37,42,43].", "startOffset": 173, "endOffset": 192}, {"referenceID": 5, "context": "It performs even better than some recent and complex optimization algorithms like the ant-based algorithm [6] and modified cuckoo optimization algorithm [34].", "startOffset": 106, "endOffset": 109}, {"referenceID": 33, "context": "It performs even better than some recent and complex optimization algorithms like the ant-based algorithm [6] and modified cuckoo optimization algorithm [34].", "startOffset": 153, "endOffset": 157}], "year": 2015, "abstractText": "Grouping problems aim to partition a set of items into multiple mutually disjoint subsets according to some specific criterion and constraints. Grouping problems cover a large class of important combinatorial optimization problems that are generally computationally difficult. In this paper, we propose a general solution approach for grouping problems, i.e., reinforcement learning based local search (RLS), which combines reinforcement learning techniques with descent-based local search. The viability of the proposed approach is verified on a well-known representative grouping problem (graph coloring) where a very simple descent-based coloring algorithm is applied. Experimental studies on popular DIMACS and COLOR02 benchmark graphs indicate that RLS achieves competitive performances compared to a number of well-known coloring algorithms.", "creator": "gnuplot 4.6 patchlevel 4"}}}