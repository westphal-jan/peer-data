{"id": "1604.00461", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2016", "title": "Embedding Lexical Features via Low-Rank Tensors", "abstract": "complex nlp procedures rely heavily on engineered architecture, which often combine word and contextual factors into complex lexical features. hardware components may in large consolidation of processors, eventually can lead automated signal - processing. we present each new model deliberately avoids specific lexical features - - - integration of tasks / words, contextual information and labels - - - in a geometric solution captures modeling behavior among these parts. we apply low - abstraction database loading to the corresponding parameter tensors to reduce predicted parameter error and improve detection speed. furthermore, we include quantitative behaviors in handling features that express $ n $ - grams of relation lengths. layered programming achieves state - of - the - shell functionality which application in functional search, pp - attachment, and sql validation.", "histories": [["v1", "Sat, 2 Apr 2016 04:59:21 GMT  (1859kb,D)", "http://arxiv.org/abs/1604.00461v1", "Accepted by NAACL 2016"]], "COMMENTS": "Accepted by NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["mo yu", "mark dredze", "raman arora", "matthew r gormley"], "accepted": true, "id": "1604.00461"}, "pdf": {"name": "1604.00461.pdf", "metadata": {"source": "CRF", "title": "Embedding Lexical Features via Low-Rank Tensors", "authors": ["Mo Yu", "Mark Dredze", "Matthew R. Gormley"], "emails": ["yum@us.ibm.com", "mdredze@cs.jhu.edu", "arora@cs.jhu.edu", "mgormley@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Statistical NLP models usually rely on handdesigned features, customized for each task. These features typically combine lexical and contextual information with the label to be scored. In relation extraction, for example, there is a parameter for the presence of a specific relation occurring with a feature conjoining a word type (lexical) with dependency path information (contextual). In measuring phrase semantic similarity, a word type is conjoined with its position in the phrase to signal its role. Figure 1b shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information.\n\u2217Paper submitted during Mo Yu\u2019s PhD study at HIT.\nTo avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014).\nHowever, using only word embeddings is not sufficient to represent complex lexical features (e.g. \u03c6 in Figure 1c). In these features, the same word embedding conjoined with different non-lexical properties may result in features indicating different labels; the corresponding lexical feature representations should take the above interactions into consideration. Such important interactions also increase the risk of over-fitting as feature space grows exponentially, yet how to capture these interactions in representation learning remains an open question.\nTo address the above problems,1 we propose a general and unified approach to reduce the feature space by constructing low-dimensional feature representations, which provides a new way of combining word embeddings, traditional non-lexical properties, and label information. Our model exploits the inner structure of features by breaking the feature into multiple parts: lexical, non-lexical and (optional) label. We demonstrate that the full feature is an outer product among these parts. Thus, a parameter tensor scores each feature to produce a prediction. Our model then reduces the number of param-\n1Our paper only focuses on lexical features, as non-lexical features usually suffer less from over-fitting.\nar X\niv :1\n60 4.\n00 46\n1v 1\n[ cs\n.C L\n] 2\nA pr\n2 01\n1\n0 1\n0\n\u03d5 =wg \u2227wc \u2227 u \u2227 y\n\u201csee\u201d \u201cPMOD\u201d \u201ctelescope\u201d\npostag(g+1) =\u201cDT\u201d\ntelescope see with\nPMOD?\na a girl\nword(c)\u2227word(g) word(c)\u2227postag(g) word(p)\u2227word(g)\nword(c)\u2227postag(g+1) word(c)\u2227word(g)\u2227postag(g+1) word(c)\u2227word(g)\u2227distance(g, p) \u2026\nc p g\n0\nbc cts wl Model P R F1 P R F1 P R F1 HeadEmb CNN (wsize=1) + local features CNN (wsize=3) + local features FCT local only FCT global 60.69 42.39 49.92 56.41 34.45 42.78 41.95 31.77 36.16 FCT global (Brown) 63.15 39.58 48.66 62.45 36.47 46.05 54.95 29.93 38.75 FCT global (WordNet) 59.00 44.79 50.92 60.20 39.60 47.77 50.95 34.18 40.92 PET (Plank and Moschitti, 2013) 51.2 40.6 45.3 51.0 37.8 43.4 35.4 32.8 34.0 BOW (Plank and Moschitti, 2013) 57.2 37.1 45.0 57.5 31.8 41.0 41.1 27.2 32.7 Best (Plank and Moschitti, 2013) 55.3 43.1 48.5 54.1 38.1 44.7 39.9 35.8 37.8 Table 7: Performance on ACE2005 test sets. The first part of the table shows the performance of different models on different sources of entity types, where \u201dG\u201d means that the gold types are used and \u201dP\u201d means that we are using the predicted types. The second part of the table shows the results under the low-resource setting, where the entity types are unknown. Dev MRR Test MRR Model Fine-tuning 1,000 10,000 100,000 1,000 10,000 100,000 SUM - 46.95 35.29 30.69 52.63 41.19 37.32 SUM Y 50.81 36.81 32.92 57.23 45.01 41.23 Best Recursive NN (d=50) Y 45.67 30.86 27.05 54.84 39.25 35.49 Best Recursive NN (d=200) Y 48.97 33.50 31.13 53.59 40.50 38.57 FCT N 47.53 35.58 31.31 54.33 41.96 39.10 FCT Y 51.22 36.76 33.59 61.11 46.99 44.31 FCT + LM - 49.43 37.46 32.22 53.56 42.63 39.44 FCT + LM +supervised Y 53.82 37.48 34.43 65.47 49.44 45.65 joint 56.53 41.41 36.45 68.52 51.65 46.53 Table 8: Performance on the semantic similarity task with PPDB data. Appendix 1: Features Used in FCT 7.1 Overall performances on ACE 2005 SUM(AB) 6= SUM(BA) (7) 2n 2 |V |n (8)\nA A0 of B0 B (9)\nA B A0 of B0 (10)\nT f e) Relations (11) f \u2326 e [f : e] FCT CNN\n@`\n@R\n@` @T = @` @R @R @T L1, L2\n@L @R = @L1 @R + @L2 @R\ns(l, e1, e2, S; T ) = nX\ni=1\ns(l, ewi , fwi)\n= nX\ni=1\nTl fwi ewi (12)\n@` @T =\nnX\ni=1\n@`\n@R \u2326 fwi \u2326 ewi , (13)\nv2(wc)=1 v3(u)=1\nv4(y)=1\n0\nv1(wg)=1\n1\n0\n0 1\n0\n0\n(a) (b) (c) (d)\nFigure 1: An example of lexical features used in dependency parsing. To predict the \u201cPMOD\u201d arc (the dashed one) between \u201csee\u201d and \u201cwith\u201d in (a), we may rely on lexical features in (b). Here p, c, g are indices of the word \u201cwith\u201d, its child (\u201ctelescope\u201d) and a candidate head. Figure (c) shows what the fifth feature (\u03c6) is like, when the candidate is \u201csee\u201d. As is common in multi-class classification tasks, each template generates a different feature for each label y. Thus a feature \u03c6 = wg \u2227 wc \u2227 u \u2227 y is the conjunction of the four parts. Figure (d) is the one-hot representation of \u03c6, which is equivalent to the outer product (i.e. a 4-way tensor) among the four one-hot vectors. v(x) = 1 means the vector v has a single non-zero element in the x position.\neters by approximating the parameter tensor with a low-rank tensor: the Tucker approximation of Yu et al. (2015) but applied to each embedding type (view), or the Canonical/Parallel-Factors Decomposition (CP). Our models use fewer parameters than previous work that learns a separate representation for each feature (Ando and Zhang, 2005; Yang and Eisenstein, 2015). CP approximation also allows for much faster prediction, going from a method that is cubic in rank and exponential in the number of lexical parts, to a method linear in both. Furthermore, we consider two methods for handling features that rely on n-grams of mixed lengths.\nOur model makes the following contributions when contrasted with prior work:\nLei et al. (2014) applied CP to combine different views of features. Compared to their work, our usage of CP-decomposition is different in the application to feature learning: (1) We focus on dimensionality reduction of existing, well-verified features, while Lei et al. (2014) generates new features (usually different from ours) by combining some \u201catom\u201d features. Thus their work may ignore some useful features; it relies on binary features as supplementary but our model needs not. (2) Lei et al. (2014)\u2019s factorization relies on views with explicit meanings, e.g. head/modifier/arc in dependency parsing, making it less general. Therefore its applications to tasks like relation extraction are less obvious.\nCompared to our previous work (Gormley et al., 2015; Yu et al., 2015), this work allows for higherorder interactions, mixed-length n-gram features,\nlower-rank representations. We also demonstrate the strength of our new model via applications to new tasks.\nThe resulting method learns smoothed feature representations combining lexical, non-lexical and label information, achieving state-of-the-art performance on several tasks: relation extraction, preposition semantics and PP-attachment."}, {"heading": "2 Notation and Definitions", "text": "We begin with some background on notation and definitions. Let T \u2208 Rd1\u00d7\u00b7\u00b7\u00b7\u00d7dK be a K-way tensor (i.e., a tensor with K views). In this paper, we consider the tensor k-mode product, i.e. multiplying a tensor T \u2208 Rd1\u00d7\u00b7\u00b7\u00b7\u00d7dK by a matrix x \u2208 Rdk\u00d7J (or a vector if J = 1) in mode (view) k. The product is denoted by T \u00d7k x and is of size d1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 dk\u22121 \u00d7 J \u00d7 dk+1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 dK . Elementwise, we have\n(T \u00d7k x)i1...ik\u22121 j ik+1...iK = dk\u2211\nik=1\nTi1...ik...iKxikj ,\nfor j = 1, . . . , J . A mode-k fiber Ti1...ik\u22121\u2022ik+1...iK of T is the dk dimensional vector obtained by fixing all but the kth index. The mode-k unfolding T(k) of T is the dk \u00d7 \u220f i 6=k di matrix obtained by concate-\nnating all the \u220f i 6=k di mode-k fibers along columns.\nGiven two matrices W1 \u2208 Rd1\u00d7r1 ,W2 \u2208 Rd2\u00d7r2 , we write W1 \u2297W2 to denote the Kronecker product between W1 and W2 (outer product for vectors). We define the Frobenius product (matrix dot product) A B = \u2211i,j AijBij between two matrices with\nthe same sizes; and define element-wise (Hadamard) multiplication a \u25e6 b between vectors with the same sizes.\nTucker Decomposition: Tucker Decomposition represents a d1 \u00d7 d2 \u00d7 . . .\u00d7 dK tensor T as:\nT = g \u00d71 W1 \u00d72 W2 . . .\u00d7K WK (1)\nwhere each \u00d7i is the tensor i-mode product and each Wi is a ri \u00d7 di matrix. Tensor g with size r1 \u00d7 r2 \u00d7 . . . \u00d7 rK is called the core tensor. We say that T has a Tucker rank (r(1), r(2), . . . , r(K)), where r(i) = rank(T(i)) is the rank of mode-i unfolding. To simplify learning, we define the Tucker rank as r(i)=rank(g(i)), which can be bounded simply by the dimensions of g, i.e. r(i) \u2264 ri; this allows us to enforce a rank constraint on T simply by restricting the dimensions ri of g, as described in \u00a76. CP Decomposition: CP decomposition represents a d1\u00d7d2\u00d7. . .\u00d7dK tensor T as a sum of rank-one tensors (i.e. a sum of outer products of K vectors):\nT = r\u2211\nj=1\nW1[j, :]\u2297W2[j, :]\u2297 . . .\u2297WK [j, :] (2)\nwhere each Wi is an r \u00d7 di matrix and Wi[j, :] is the vector of its j-th row. For CP decomposition, the rank r of a tensor T is defined to be the number of rank-one tensors in the decomposition. CP decomposition can be viewed as a special case of Tucker decomposition in which r1 = r2 = . . . = rK = r and g is a superdiagonal tensor."}, {"heading": "3 Factorization of Lexical Features", "text": "Suppose we have feature \u03c6 that includes information from a label y, multiple lexical items w1, . . . ,wn and non-lexical property u. This feature can be factorized as a conjunction of each part: \u03c6 = y \u2227 u \u2227 w1\u2227. . .\u2227wn. The feature fires when all (n+2) parts fire in the instance (reflected by the \u2227 symbol in \u03c6). The one-hot representation of \u03c6 can then be viewed as a tensor e\u03c6 = y\u2297 u\u2297w1\u2297 \u00b7 \u00b7 \u00b7 \u2297wn, where each feature part is also represented as a one-hot vector.2 Figure 1d illustrates this case with two lexical parts. Given an input instance x and its associated label y, we can extract a set of features S(x, y). In 2u, y,wi denote one-hot vectors instead of symbols.\na traditional log-linear model, we view the instance x as a bag-of-features, i.e. a feature vector F (x, y). Each dimension corresponds to a feature \u03c6, and has value 1 if \u03c6 \u2208 S(x, y). Then the log-linear model scores the instance as s(x, y;w) = wTF (x, y) =\u2211\n\u03c6\u2208S(x,y) s(\u03c6;w), where w is the parameter vector. We can re-write s(x, y;w) based on the factorization of the features using tensor multiplication; in which w becomes a parameter tensor T :\ns(x, y;w) = s(x, y; T ) = \u2211\n\u03c6\u2208S(x,y) s(\u03c6; T ) (3)\nHere each \u03c6 has the form (y, u,w1, . . . ,wn), and\ns(\u03c6; T ) = T \u00d7l y\u00d7f u\u00d7w1 w1...\u00d7wn wn. (4)\nNote that one-hot vectors wi of words themselves are large (|wi| > 500k), thus the above formulation with parameter tensor T can be very large, making parameter estimation difficult. Instead of estimating only the values of the dimensions which appear in training data as in traditional methods, we will reduce the size of tensor T via a low-rank approximation. With different approximation methods, (4) will have different equivalent forms, e.g. (6), (7) in \u00a74.1. Optimization objective: The loss function ` for training the log-linear model uses (3) for scores, e.g., the log-loss `(x, y; T ) = \u2212 log exp{s(x,y;T )}\u2211\ny\u2032\u2208L exp{s(x,y\u2032;T )} .\nLearning can be formulated as the following optimization problem:\nminimize: T\n\u2211\n(x,y)\u2208D `(x, y; T )\nsubject to:    rank(T ) \u2264 (r1, r2, ..., rn+2) (Tucker-form)\nrank(T ) \u2264 r (CP-form)\n(5)\nwhere the constraints on rank(T ) depend on the chosen tensor approximation method (\u00a72).\nThe above framework has some advantages: First, as discussed in \u00a71 and here, we hope the representations capture rich interactions between different parts of the lexical features; the low-rank tensor approximation methods keep the most important interaction information of the original tensor, while significantly reducing its size. Second, the low-rank structure will encourage weight-sharing among lexical features with similar decomposed parts, leading\nto better model generalization. Note that there are examples where features have different numbers of multiple lexical parts, such as both unigram and bigram features in PP-attachment. We will use two different methods to handle these features (\u00a75). Remarks (advantages of our factorization) Compared to prior work, e.g. (Lei et al., 2014; Lei et al., 2015), the proposed factorization has the following advantages:\n1. Parameter explosion when mapping a view with lexical properties to its representation vector (as will be discussed in 4.3): Our factorization allows the model to treat word embeddings as inputs to the views of lexical parts, dramatically reducing the parameters. Prior work cannot do this since its views are mixtures of lexical and non-lexical properties. Note that Lei et al. (2014) uses embeddings by concatenating them to specific views, which increases dimensionality, but the improvement is limited.\n2. No weight-sharing among conjunctions with same lexical property, like the child-word \u201cword(c)\u201d and its conjunction with head-postag \u201cword(c) \u2227 word(g)\u201d in Figure 1(b). The factorization in prior work treats them as independent features, greatly increasing the dimensionality. Our factorization builds representations of both features based on the embedding of \u201cword(c)\u201d, thus utilizing their connections and reducing the dimensionality.\nThe above advantages are also key to overcome the problems of prior work mentioned at the end of \u00a71."}, {"heading": "4 Feature Representations via Low-rank Tensor Approximations", "text": "Using one-hot encodings for each of the parts of feature \u03c6 results in a very large tensor. This section shows how to compute the score in (4) without constructing the full feature tensor using two tensor approximation methods (\u00a74.1 and \u00a74.2).\nWe begin with some intuition. To score the original (full rank) tensor representation of \u03c6, we need a parameter tensor T of size d1 \u00d7 d2 \u00d7 . . . \u00d7 dn+2, where d3 = \u00b7 \u00b7 \u00b7 = dn+2 = |V | is the vocabulary size, n is the number of lexical parts in the feature\nand d1 = |L| and d2 = |F | are the number of different labels and non-lexical properties, respectively. (\u00a75 will handle n varying across features.) Our methods reduce the tensor size by embedding each part of \u03c6 into a lower dimensional space, where we represent each label, non-lexical property and words with an r1, r2, r3, . . . , rn+2 dimensional vector respectively (ri di, \u2200i). These embedded features can then be scored by much smaller tensors. We denote the above transformations as matrices Wl \u2208 Rr1\u00d7d1 , Wf \u2208 Rr2\u00d7d2 , Wi \u2208 Rri+2\u00d7di+2 for i = 1, . . . , n, and write corresponding lowdimensional hidden representations as h(l)y = Wly, h (f) u = Wfu and h (i) w = Wiw.\nIn our methods, the above transformations of embeddings are parts of low-rank tensors as in (5), so the embeddings of non-lexical properties and labels can be trained simultaneously with the low-rank tensors. Note that for one-hot input encodings the transformation matrices are essentially lookup tables, making the computation of these transformations sufficiently fast."}, {"heading": "4.1 Tucker Form", "text": "For our first approximation, we assume that tensor T has a low-rank Tucker decomposition: T = g \u00d7l Wl \u00d7f Wf \u00d7w1 W1 \u00d7w2 \u00b7 \u00b7 \u00b7 \u00d7wn Wn. We can then express the scoring function (4) for a feature \u03c6 = (y, u,w1, . . .wn) with n-lexical parts, as:\ns(y, u,w1, \u00b7 \u00b7 \u00b7 ,wn; g,Wl,Wf , {Wi}ni=1) = g \u00d7l h(l)y \u00d7f h(f)u \u00d7w1 h (1) w1 \u00b7 \u00b7 \u00b7 \u00d7wn h (n) wn , (6)\nwhich amounts to first projecting u, y, and wi (for all i) to lower dimensional vectors h(f)u ,h (l) y ,h (i) wi , and then weighting these hidden representations using the flattened core tensor g. The low-dimensional representations and the corresponding weights are learned jointly using a discriminative (supervised) criterion. We call the model based on this representation the Low-Rank Feature Representation with Tucker form, or LRFRn-TUCKER."}, {"heading": "4.2 CP Form", "text": "For the Tucker approximation the number of parameters in (6) scale exponentially with the number of lexical parts. For instance, suppose each h(i)wi has di-\nmensionality r, then |g| \u221d rn. To address scalability and further control the complexity of our tensor based model, we approximate the parameter tensor using CP decomposition as in (2), resulting in the following scoring function:\ns(y, u,w1, \u00b7 \u00b7 \u00b7 ,wn;Wl,Wf , {Wi}ni=1) = r\u2211\nj=1\n( h (l) y \u25e6 h(f)u \u25e6 h(1)w1 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 h (n) wn ) j . (7)\nWe call this model Low-Rank Feature Representation with CP form (LRFRn-CP)."}, {"heading": "4.3 Pre-trained Word Embeddings", "text": "One of the computational and statistical bottlenecks in learning these LRFRn models is the vocabulary size; the number of parameters to learn in each matrix Wi scales linearly with |V | and would require very large sets of labeled training data. To alleviate this problem, we use pre-trained continuous word embeddings (Mikolov et al., 2013) as input embeddings rather than the one-hot word encodings. We denote the m-dimensional word embeddings by ew; so the transformation matrices Wi for the lexical parts are of size ri \u00d7m where m |V |.\nWe note that when sufficiently large labeled data is available, our model allows for fine-tuning the pre-trained word embeddings to improve the expressive strength of the model, as is common with deep network models.\nRemarks Our LRFRs introduce embeddings for non-lexical properties and labels, making them better suit the common setting in NLP: rich linguistic properties; and large label sets such as open-domain tasks (Hoffmann et al., 2010). The LRFR-CP better suits n-gram features, since when n increases 1, the only new parameters are the corresponding Wi. It is also very efficient during prediction (O(nr)), since the cost of transformations can be ignored with the help of look-up tables and pre-computing.\n5 Learning Representations for n-gram Lexical Features of Mixed Lengths\nFor features with n lexical parts, we can train an LRFRn model to obtain their representations. However, we often have features of varying n (e.g. both unigrams (n=1) and bigrams (n=2) as in Figure 1).\nWe require representations for features with arbitrary different n simultaneously.\nWe propose two solutions. The first is a straightforward solution based on our framework, which handles each nwith a (n+2)-way tensor. This strategy is commonly used in NLP, e.g. Taub-Tabib et al. (2015) have different kernel functions for different order of dependency features. The second is an approximation method which aims to use a single tensor to handle all ns.\nMultiple Low-Rank Tensors Suppose that we can divide the feature set S(x, y) into subsets S1(x, y), S2(x, y), . . . , Sn(x, y) which correspond to features with one lexical part (unigram features), two lexical parts (bigram features), . . . and n lexical parts (n-gram features), respectively. To handle these types of features, we modify the training objective as follows:\nminimize T1,T2,\u00b7\u00b7\u00b7 ,Tn\n\u2211\n(x,y)\u2208D `(x, y; T1, T2, . . . , ...Tn), (8)\nwhere the score of a training instance (x, y) is defined as s(x, y; T ) =\u2211ni=1 \u2211 \u03c6\u2208Si(x,y) s(\u03c6; Ti). We use the Tucker form low-rank tensor for T1, and the CP form for Ti (\u2200i > 1). We refer to this method as LRFR1-TUCKER & LRFR2-CP.\nWord Clusters Alternatively, to handle different numbers of lexical parts, we replace some lexical parts with discrete word clusters. Let c(w) denote the word cluster (e.g. from Brown clustering) for word w. For bigram features we have:\ns(y, u,w1,w2; T ) = s(y, u\u2227c(w1),w2; T ) + s(y, u\u2227c(w2),w1; T ) = T \u00d7l y\u00d7f (u \u2227 c(w1))\u00d7w ew2\n+ T \u00d7l y\u00d7f (u \u2227 c(w2))\u00d7w ew1 (9) where for each word we have introduced an additional set of non-lexical properties that are conjunctions of word clusters and the original non-lexical properties. This allows us to reduce an n-gram feature representation to a unigram representation. The advantage of this method is that it uses a single low-rank tensor to score features with different numbers of lexical parts. This is particularly helpful when we have very limited labeled data. We denote this method as LRFR1-BROWN, since we use Brown clusters in practice. In the experiments we use the\nTucker form for LRFR1-BROWN."}, {"heading": "6 Parameter Estimation", "text": "The goal of learning is to find a tensor T that solves problem (5). Note that this is a non-convex objective, so compared to the convex objective in a traditional log-linear model, we are trading better feature representations with the cost of a harder optimization problem. While stochastic gradient descent (SGD) is a natural choice for learning representations in large data settings, problem (5) involves rank constraints, which require an expensive proximal operation to enforce the constraints at each iteration of SGD. We seek a more efficient learning algorithm. Note that we fixed the size of each transformation matrix Wi \u2208 Rri\u00d7di so that the smaller dimension (ri < di) matches the upper bound on the rank. Therefore, the rank constants are always satisfied through a run of SGD and we in essence have an unconstrained optimization problem. Note that in this way we do not guarantee orthogonality and fullrank of the learned transformation matrices. These properties are assumed in general, but are not necessary according to (Kolda and Bader, 2009).\nThe gradients are computed via the chain-rule. We use AdaGrad (Duchi et al., 2011) and apply L2 regularization on all Wis and g, except for the case of ri=di, where we will start with Wi = I and regularize with \u2016Wi - I\u20162. We use early-stopping on a development set."}, {"heading": "7 Experimental Settings", "text": "We evaluate LRFR on three tasks: relation extraction, PP attachment and preposition disambiguation (see Table 1 for a task summary). We include detailed feature templates in Table 2.\nPP-attachment and relation extraction are two fundamental NLP tasks, and we test our models on the largest English data sets. The preposition disambiguation task was designed for compositional semantics, which is an important application of deep learning and distributed representations. On all these tasks, we compare to the state-of-the-art.\nWe use the same word embeddings in Belinkov et al. (2014) on PP-attachment for a fair comparison. For the other experiments, we use the same 200-d word embeddings in Yu et al. (2015).\nRelation Extraction We use the English portion of the ACE 2005 relation extraction dataset (Walker et al., 2006). Following Yu et al. (2015), we use both gold entity spans and types, train the model on the news domain and test on the broadcast conversation domain. To highlight the impact of training data size we evaluate with all 43,518 relations (entity mention pairs) and a reduced training set of the first 10,000 relations. We report precision, recall, and F1.\nWe compare to two baseline methods: 1) a loglinear model with a rich binary feature set from Sun et al. (2011) and Zhou et al. (2005) as described in Yu et al. (2015) (BASELINE); 2) the embedding model (FCM) of Gormley et al. (2015), which uses rich linguistic features for relation extraction. We use the same feature templates and evaluate on finegrained relations (sub-types, 32 labels) (Yu et al., 2015). This will evaluate how LRFR can utilize nonlexical linguistic features.\nPP-attachment We consider the prepositional phrase (PP) attachment task of Belinkov et al. (2014),3 where for each PP the correct head (verbs or nouns) must be selected from content words before the PP (within a 10-word window). We formulate the task as a ranking problem, where we optimize the score of the correct head from a list of candidates with varying sizes.\nPP-attachment suffers from data sparsity because of bi-lexical features, which we will model with methods in \u00a75. Belikov et al. show that rich features \u2013 POS, WordNet and VerbNet \u2013 help this task. The combination of these features give a large number of non-lexical properties, for which embeddings of non-lexical properties in LRFR should be useful.\nWe extract a dev set from section 22 of the PTB following the description in Belinkov et al. (2014).\nPreposition Disambiguation We consider the preposition disambiguation task proposed by Ritter et al. (2014). The task is to determine the spatial relationship a preposition indicates based on the two objects connected by the preposition. For example, \u201cthe apple on the refrigerator\u201d indicates the \u201csupport by Horizontal Surface\u201d relation, while \u201cthe apple on the branch\u201d indicates the \u201cSupport from Above\u201d relation. Since the meaning of a preposition depends\n3 http://groups.csail.mit.edu/rbg/code/pp\non the combination of both its head and child word, we expect conjunctions between these word embeddings to help, i.e. features with two lexical parts.\nWe include three baselines: point-wise addition (SUM) (Mitchell and Lapata, 2010), concatenation (Ritter et al., 2014), and an SVM based on handcrafted features in Table 2. Ritter et al. show that the first two methods beat other compositional models.\nHyperparameters are all tuned on the dev set. The chosen values are learning rate \u03b7 = 0.05 and the weight of L2 regularizer \u03bb = 0.005 for LRFR, except for the third LRFR in Table 3 which has \u03bb = 0.05. We select the rank of LRFR-TUCKER with a grid search from the following values: r1 = {10, 20, d1}, r2 = {20, 50, d2} and r3 = {50, 100, 200}. For LRFR-CP, we select r = {50, 100, 200}. For the PP-attachement task there is no r1 since it uses a ranking model. For the Preposition Disambiguation we do not choose r1 since the number of labels is small."}, {"heading": "8 Results", "text": "Relation Extraction All LRFR-TUCKER models improve over BASELINE and FCM (Table 3), making\nthese the best reported numbers for this task. However, LRFR-CP does not work as well on the features with only one lexical part. The Tucker-form does a better job of capturing interactions between different views. In the limited training setting, we find that LRFR-CP does best.\nAdditionally, the primary advantage of the CP approximation is its reduction in the number of model parameters and running time. We report each model\u2019s running time for a single pass on the development set. The LRFR-CP is by far the fastest. The first three LRFR-TUCKER models are slightly slower than FCM, because they work on dense nonlexical property embeddings while FCM benefits from sparse vectors.\nPP-attachment Table 4 shows that LRFR (89.6 and 90.3) improves over the previous best standalone system HPCD (88.7) by a large margin, with exactly the same resources. Belinkov et al. (2014) also reported results of parsers and parser re-rankers, which can access to additional resources (complete parses for training and complete sentences as inputs) so it is unfair to compare them with the standalone systems like HPCD and our LRFR. Nonethe-\nless LRFR1-TUCKER & LRFR2-CP (90.3) still outperforms the state-of-the-art parser RBG (88.4), reranker Charniak-RS (88.6), and the combination of the state-of-the-art parser and compositional model RBG + HPCD (90.1). Thus, even with fewer resources, LRFR becomes the new best system.\nNot shown in the table: we also tried LRFR1TUCKER & LRFR2-CP with postag features only (89.7), and with grand-head-modifier conjunctions removed (89.3) . Note that compared to LRFR, RBG benefits from binary features, which also exploit grand-head-modifier structures. Yet the above reduced models still work better than RBG (88.4) without using additional resources.4 Moreover, the results of LRFR can still be potentially improved by combining with binary features. The above results show the advantage of our factorization method, which allows for utilizing pre-trained word embeddings, and thus can benefit from semi-supervised learning.\nPreposition Disambiguation LRFR improves (Table 5) over the best methods (SUM and Concatenation) in Ritter et al. (2014) as well as the SVM\n4Still this is not a fair comparison since we have different training objectives. Using RBG\u2019s factorization and training with our objective will give a fair comparison and we leave it to future work.\nbased on the original lexical features (85.1). In this task LRFR1-BROWN better represents the unigram and bigram lexical features, compared to the usage of two low-rank tensors (LRFR1-TUCKER & LRFR2CP). This may be because LRFR1-BROWN has fewer parameters, which is better for smaller training sets.\nWe also include a control setting (LRFR1-BROWN - Control), which has a full rank parameter tensor with the same inputs on each view as LRFR1BROWN, but represented as one hot vectors without transforming to the hidden representations hs. This is equivalent to an SVM with the compound cluster features as in Koo et al. (2008). It performs much worse than LRFR1-BROWN, showing the advantage of using word embeddings and low-rank tensors.\nSummary For unigram lexical features, LRFRnTUCKER achieves better results than LRFRn-CP. However, in settings with fewer training examples,\nfeatures with more lexical parts (n-grams), or when faster predictions are advantageous, LRFRn-CP does best as it has fewer parameters to estimate. For ngrams of variable length, LRFR1-TUCKER & LRFR2CP does best. In settings with fewer training examples, LRFR1-BROWN does best as it has only one parameter tensor to estimate."}, {"heading": "9 Related Work", "text": "Dimensionality Reduction for Complex Features is a standard technique to address high-dimensional features, including PCA, alternating structural optimization (Ando and Zhang, 2005), denoising autoencoders (Vincent et al., 2008), and feature embeddings (Yang and Eisenstein, 2015). These methods treat features as atomic elements and ignore the inner structure of features, so they learn separate embedding for each feature without shared parameters. As a result, they still suffer from large parameter spaces when the feature space is very huge.5\nAnother line of research studies the inner structures of lexical features: e.g. Koo et al. (2008), Turian et al. (2010), Sun et al. (2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), and Hermann et al. (2014) used pre-trained word embeddings to replace the lexical parts of features ; Srikumar and Manning (2014), Gormley et al. (2015) and Yu et al. (2015) propose splitting lexical features into different parts and employing tensors to perform classification. The above can therefore be seen as special cases of our model that only embed a certain part (view) of the complex features. This restriction also makes their model parameters form a full rank tensor, resulting in data sparsity and high computational costs when the tensors are large.\nComposition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b). When using only word embeddings, these models achieved successes on several NLP tasks, but sometimes fail to learn useful syntactic or semantic patterns beyond the strength of combinations of word\n5For example, a state-of-the-art dependency parser (Zhang and McDonald, 2014) extracts about 10 million features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters.\nembeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a specific kind of linguistic patterns, e.g. dependency paths (Ma et al., 2015; Liu et al., 2015), while a recent trend enhances compositional models with linguistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types). These models are similar to ours in the sense of learning representations based on linguistic features and embeddings.\nLow-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Lei et al., 2014; Chen and Manning, 2014). Yu and Dredze (2015) proposed a model to compose phrase embeddings from words, which has an equivalent form of our CPbased method under certain restrictions. Our work applies a similar idea to exploiting the inner structure of complex features, and can handle n-gram features with different ns. Our factorization (\u00a73) is general and easy to adapt to new tasks. More importantly, it makes the model benefit from pre-trained word embeddings as shown by the PP-attachment results."}, {"heading": "10 Conclusion", "text": "We have presented LRFR, a feature representation model that exploits the inner structure of complex lexical features and applies a low-rank tensor to efficiently score features with this representation. LRFR attains the state-of-the-art on several tasks, including relation extraction, PP-attachment, and preposition disambiguation. We make our implementation available for general use.6"}, {"heading": "Acknowledgements", "text": "A major portion of this work was done when MY was visiting MD and RA at JHU. This research was supported in part by NSF grant IIS-1546482.\n6https://github.com/Gorov/LowRankFCM"}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005] Rie Kubota Ando", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Exploring compositional architectures and word vector representations for prepositional phrase attachment", "author": ["Tao Lei", "Regina Barzilay", "Amir Globerson"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Belinkov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Belinkov et al\\.", "year": 2014}, {"title": "Neural probabilistic language models", "author": ["Bengio et al.2006] Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain"], "venue": "In Innovations in Machine Learning", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "A semantic matching energy function for learning with multirelational data", "author": ["Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "Machine Learning", "citeRegEx": "Bordes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Online learning in tensor space. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Cao", "Khudanpur2014] Yuan Cao", "Sanjeev Khudanpur"], "venue": null, "citeRegEx": "Cao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2014}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Improved relation extraction with feature-rich compositional embedding models", "author": ["Mo Yu", "Mark Dredze"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Gormley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gormley et al\\.", "year": 2015}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["Hermann", "Blunsom2013] Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Association for Computational Linguistics", "citeRegEx": "Hermann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2013}, {"title": "Semantic frame identification with distributed word representations", "author": ["Dipanjan Das", "Jason Weston", "Kuzman Ganchev"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Tensor decompositions and applications", "author": ["Kolda", "Bader2009] Tamara G Kolda", "Brett W Bader"], "venue": "SIAM review,", "citeRegEx": "Kolda et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolda et al\\.", "year": 2009}, {"title": "Simple semi-supervised dependency parsing", "author": ["Koo et al.2008] Terry Koo", "Xavier Carreras", "Michael Collins"], "venue": "In Proceedings of ACL", "citeRegEx": "Koo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Low-rank tensors for scoring dependency structures. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Lei et al.2014] Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola"], "venue": null, "citeRegEx": "Lei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "High-order low-rank tensors for semantic role labeling", "author": ["Lei et al.2015] Tao Lei", "Yuan Zhang", "Llu\u0131\u0301s M\u00e0rquez", "Alessandro Moschitti", "Regina Barzilay"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "A dependency-based neural network for relation classification", "author": ["Liu et al.2015] Yang Liu", "Furu Wei", "Sujian Li", "Heng Ji", "Ming Zhou", "Houfeng WANG"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Ma et al.2015] Mingbo Ma", "Liang Huang", "Bowen Zhou", "Bing Xiang"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Effective self-training for parsing", "author": ["Eugene Charniak", "Mark Johnson"], "venue": "In Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computa-", "citeRegEx": "McClosky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Name tagging with word clusters and discriminative training", "author": ["Miller et al.2004] Scott Miller", "Jethran Guinness", "Alex Zamanian"], "venue": "In Proceedings of HLT-NAACL", "citeRegEx": "Miller et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2004}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Lapata2010] Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "Employing word representations and regularization for domain adaptation of relation extraction", "author": ["Nguyen", "Grishman2014] Thien Huu Nguyen", "Ralph Grishman"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Leveraging preposition ambiguity to assess representation of semantic interaction in cdsm", "author": ["Ritter et al.2014] Samuel Ritter", "Cotie Long", "Denis Paperno", "Marco Baroni", "Matthew Botvinick", "Adele Goldberg"], "venue": "In NIPS Workshop on Learning Semantics", "citeRegEx": "Ritter et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2014}, {"title": "Composition of word representations improves semantic role labelling", "author": ["Roth", "Woodsend2014] Michael Roth", "Kristian Woodsend"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Roth et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2014}, {"title": "Semantic compositionality through recursive matrixvector spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of EMNLP-CoNLL", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Parsing with compositional vector grammars", "author": ["John Bauer", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of ACL", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts"], "venue": "Proceedings of EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Learning distributed representations for structured output prediction", "author": ["Srikumar", "Manning2014] Vivek Srikumar", "Christopher D Manning"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Srikumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srikumar et al\\.", "year": 2014}, {"title": "Semi-supervised relation extraction with large-scale word clustering", "author": ["Sun et al.2011] Ang Sun", "Ralph Grishman", "Satoshi Sekine"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "Template kernels for dependency parsing", "author": ["Yoav Goldberg", "Amir Globerson"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Taub.Tabib et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Taub.Tabib et al\\.", "year": 2015}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Association for Computational Linguistics", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Unsupervised multi-domain adaptation with feature embeddings", "author": ["Yang", "Eisenstein2015] Yi Yang", "Jacob Eisenstein"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Learning composition models for phrase embeddings", "author": ["Yu", "Dredze2015] Mo Yu", "Mark Dredze"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Combining word embeddings and feature embeddings for fine-grained relation extraction. In North American Chapter of the Association for Computational Linguistics (NAACL)", "author": ["Yu et al.2015] Mo Yu", "Matthew R. Gormley", "Mark Dredze"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Enforcing structural diversity in cubepruned dependency parsing", "author": ["Zhang", "McDonald2014] Hao Zhang", "Ryan McDonald"], "venue": "In Proceedings of ACL", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou et al.2005] GuoDong Zhou", "Jian Su", "Jie Zhang", "Min Zhang"], "venue": "In Proceedings of ACL", "citeRegEx": "Zhou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al.", "startOffset": 231, "endOffset": 252}, {"referenceID": 19, "context": ", 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014).", "startOffset": 72, "endOffset": 197}, {"referenceID": 12, "context": ", 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014).", "startOffset": 72, "endOffset": 197}, {"referenceID": 30, "context": ", 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014).", "startOffset": 72, "endOffset": 197}, {"referenceID": 28, "context": ", 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014).", "startOffset": 72, "endOffset": 197}, {"referenceID": 10, "context": ", 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014).", "startOffset": 72, "endOffset": 197}, {"referenceID": 33, "context": "eters by approximating the parameter tensor with a low-rank tensor: the Tucker approximation of Yu et al. (2015) but applied to each embedding type (view), or the Canonical/Parallel-Factors Decomposition (CP).", "startOffset": 96, "endOffset": 113}, {"referenceID": 8, "context": "Compared to our previous work (Gormley et al., 2015; Yu et al., 2015), this work allows for higherorder interactions, mixed-length n-gram features, lower-rank representations.", "startOffset": 30, "endOffset": 69}, {"referenceID": 33, "context": "Compared to our previous work (Gormley et al., 2015; Yu et al., 2015), this work allows for higherorder interactions, mixed-length n-gram features, lower-rank representations.", "startOffset": 30, "endOffset": 69}, {"referenceID": 13, "context": "(Lei et al., 2014; Lei et al., 2015), the proposed factorization has the following advantages:", "startOffset": 0, "endOffset": 36}, {"referenceID": 14, "context": "(Lei et al., 2014; Lei et al., 2015), the proposed factorization has the following advantages:", "startOffset": 0, "endOffset": 36}, {"referenceID": 13, "context": "Note that Lei et al. (2014) uses embeddings by concatenating them to specific views, which increases dimensionality, but the improvement is limited.", "startOffset": 10, "endOffset": 28}, {"referenceID": 18, "context": "To alleviate this problem, we use pre-trained continuous word embeddings (Mikolov et al., 2013) as input embeddings rather than the one-hot word encodings.", "startOffset": 73, "endOffset": 95}, {"referenceID": 29, "context": "Taub-Tabib et al. (2015) have different kernel functions for different order of dependency features.", "startOffset": 0, "endOffset": 25}, {"referenceID": 7, "context": "We use AdaGrad (Duchi et al., 2011) and apply L2 regularization on all Wis and g, except for the case of ri=di, where we will start with Wi = I and regularize with \u2016Wi I\u20162.", "startOffset": 15, "endOffset": 35}, {"referenceID": 1, "context": "We use the same word embeddings in Belinkov et al. (2014) on PP-attachment for a fair comparison.", "startOffset": 35, "endOffset": 58}, {"referenceID": 1, "context": "We use the same word embeddings in Belinkov et al. (2014) on PP-attachment for a fair comparison. For the other experiments, we use the same 200-d word embeddings in Yu et al. (2015). Relation Extraction We use the English portion of the ACE 2005 relation extraction dataset (Walker et al.", "startOffset": 35, "endOffset": 183}, {"referenceID": 1, "context": "We use the same word embeddings in Belinkov et al. (2014) on PP-attachment for a fair comparison. For the other experiments, we use the same 200-d word embeddings in Yu et al. (2015). Relation Extraction We use the English portion of the ACE 2005 relation extraction dataset (Walker et al., 2006). Following Yu et al. (2015), we use both gold entity spans and types, train the model on the news domain and test on the broadcast conversation domain.", "startOffset": 35, "endOffset": 325}, {"referenceID": 33, "context": "We use the same feature templates and evaluate on finegrained relations (sub-types, 32 labels) (Yu et al., 2015).", "startOffset": 95, "endOffset": 112}, {"referenceID": 27, "context": "We compare to two baseline methods: 1) a loglinear model with a rich binary feature set from Sun et al. (2011) and Zhou et al.", "startOffset": 93, "endOffset": 111}, {"referenceID": 27, "context": "We compare to two baseline methods: 1) a loglinear model with a rich binary feature set from Sun et al. (2011) and Zhou et al. (2005) as described in Yu et al.", "startOffset": 93, "endOffset": 134}, {"referenceID": 27, "context": "We compare to two baseline methods: 1) a loglinear model with a rich binary feature set from Sun et al. (2011) and Zhou et al. (2005) as described in Yu et al. (2015) (BASELINE); 2) the embedding model (FCM) of Gormley et al.", "startOffset": 93, "endOffset": 167}, {"referenceID": 8, "context": "(2015) (BASELINE); 2) the embedding model (FCM) of Gormley et al. (2015), which uses rich linguistic features for relation extraction.", "startOffset": 51, "endOffset": 73}, {"referenceID": 1, "context": "PP-attachment We consider the prepositional phrase (PP) attachment task of Belinkov et al. (2014),3 where for each PP the correct head (verbs or nouns) must be selected from content words before the PP (within a 10-word window).", "startOffset": 75, "endOffset": 98}, {"referenceID": 1, "context": "We extract a dev set from section 22 of the PTB following the description in Belinkov et al. (2014).", "startOffset": 77, "endOffset": 100}, {"referenceID": 22, "context": "Preposition Disambiguation We consider the preposition disambiguation task proposed by Ritter et al. (2014). The task is to determine the spatial relationship a preposition indicates based on the two objects connected by the preposition.", "startOffset": 87, "endOffset": 108}, {"referenceID": 31, "context": "Task Benchmark Dataset Numbers on Each View #Labels (d1) #Non-lexical Features (d2) Relation Extraction Yu et al. (2015) ACE 2005 32 264 PP-attachment Belinkov et al.", "startOffset": 104, "endOffset": 121}, {"referenceID": 1, "context": "(2015) ACE 2005 32 264 PP-attachment Belinkov et al. (2014) WSJ 1,213 / 607 Preposition Disambiguation Ritter et al.", "startOffset": 37, "endOffset": 60}, {"referenceID": 1, "context": "(2015) ACE 2005 32 264 PP-attachment Belinkov et al. (2014) WSJ 1,213 / 607 Preposition Disambiguation Ritter et al. (2014) Ritter et al.", "startOffset": 37, "endOffset": 124}, {"referenceID": 1, "context": "(2015) ACE 2005 32 264 PP-attachment Belinkov et al. (2014) WSJ 1,213 / 607 Preposition Disambiguation Ritter et al. (2014) Ritter et al. (2014) 6 9/3", "startOffset": 37, "endOffset": 145}, {"referenceID": 33, "context": "Table 2: Up-left: Unigram lexical features (only showing non-lexical parts) for relation extraction (from Yu et al. (2014)).", "startOffset": 106, "endOffset": 123}, {"referenceID": 22, "context": "We include three baselines: point-wise addition (SUM) (Mitchell and Lapata, 2010), concatenation (Ritter et al., 2014), and an SVM based on handcrafted features in Table 2.", "startOffset": 97, "endOffset": 118}, {"referenceID": 1, "context": "Belinkov et al. (2014) also reported results of parsers and parser re-rankers, which can access to additional resources (complete parses for training and complete sentences as inputs) so it is unfair to compare them with the standalone systems like HPCD and our LRFR.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "System Resources Used Acc SVM (Belinkov et al., 2014) distance, word, embedding, clusters, POS, WordNet, VerbNet 86.", "startOffset": 30, "endOffset": 53}, {"referenceID": 1, "context": "0 HPCD (Belinkov et al., 2014) distance, embedding, POS, WordNet, VerbNet 88.", "startOffset": 7, "endOffset": 30}, {"referenceID": 13, "context": "RBG (Lei et al., 2014) dependency parser 88.", "startOffset": 4, "endOffset": 22}, {"referenceID": 17, "context": "4 Charniak-RS (McClosky et al., 2006) dependency parser + re-ranker 88.", "startOffset": 14, "endOffset": 37}, {"referenceID": 1, "context": "The baseline results are from Belinkov et al. (2014).", "startOffset": 30, "endOffset": 53}, {"referenceID": 22, "context": "Preposition Disambiguation LRFR improves (Table 5) over the best methods (SUM and Concatenation) in Ritter et al. (2014) as well as the SVM", "startOffset": 100, "endOffset": 121}, {"referenceID": 12, "context": "This is equivalent to an SVM with the compound cluster features as in Koo et al. (2008). It performs much worse than LRFR1-BROWN, showing the advantage of using word embeddings and low-rank tensors.", "startOffset": 70, "endOffset": 88}, {"referenceID": 31, "context": "Dimensionality Reduction for Complex Features is a standard technique to address high-dimensional features, including PCA, alternating structural optimization (Ando and Zhang, 2005), denoising autoencoders (Vincent et al., 2008), and feature embeddings (Yang and Eisenstein, 2015).", "startOffset": 206, "endOffset": 228}, {"referenceID": 9, "context": "Koo et al. (2008), Turian et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "Koo et al. (2008), Turian et al. (2010), Sun et al.", "startOffset": 0, "endOffset": 40}, {"referenceID": 9, "context": "Koo et al. (2008), Turian et al. (2010), Sun et al. (2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), and Hermann et al.", "startOffset": 0, "endOffset": 59}, {"referenceID": 9, "context": "Koo et al. (2008), Turian et al. (2010), Sun et al. (2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), and Hermann et al.", "startOffset": 0, "endOffset": 87}, {"referenceID": 9, "context": "Koo et al. (2008), Turian et al. (2010), Sun et al. (2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), and Hermann et al.", "startOffset": 0, "endOffset": 113}, {"referenceID": 8, "context": "(2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), and Hermann et al. (2014) used pre-trained word embeddings to replace the lexical parts of features ; Srikumar and Manning (2014), Gormley et al.", "startOffset": 66, "endOffset": 88}, {"referenceID": 8, "context": "(2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), and Hermann et al. (2014) used pre-trained word embeddings to replace the lexical parts of features ; Srikumar and Manning (2014), Gormley et al.", "startOffset": 66, "endOffset": 192}, {"referenceID": 8, "context": "(2014) used pre-trained word embeddings to replace the lexical parts of features ; Srikumar and Manning (2014), Gormley et al. (2015) and Yu et al.", "startOffset": 112, "endOffset": 134}, {"referenceID": 8, "context": "(2014) used pre-trained word embeddings to replace the lexical parts of features ; Srikumar and Manning (2014), Gormley et al. (2015) and Yu et al. (2015) propose splitting lexical features into different parts and employing tensors to perform classification.", "startOffset": 112, "endOffset": 155}, {"referenceID": 6, "context": "Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b).", "startOffset": 113, "endOffset": 201}, {"referenceID": 3, "context": "Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b).", "startOffset": 113, "endOffset": 201}, {"referenceID": 24, "context": "Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b).", "startOffset": 113, "endOffset": 201}, {"referenceID": 16, "context": "dependency paths (Ma et al., 2015; Liu et al., 2015), while a recent trend enhances compositional models with linguistic features.", "startOffset": 17, "endOffset": 52}, {"referenceID": 15, "context": "dependency paths (Ma et al., 2015; Liu et al., 2015), while a recent trend enhances compositional models with linguistic features.", "startOffset": 17, "endOffset": 52}, {"referenceID": 1, "context": "For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 1, "context": "For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.", "startOffset": 13, "endOffset": 147}, {"referenceID": 1, "context": "For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.", "startOffset": 13, "endOffset": 178}, {"referenceID": 13, "context": "Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Lei et al., 2014; Chen and Manning, 2014).", "startOffset": 95, "endOffset": 162}, {"referenceID": 13, "context": "Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Lei et al., 2014; Chen and Manning, 2014). Yu and Dredze (2015) proposed a model to compose phrase embeddings from words, which has an equivalent form of our CPbased method under certain restrictions.", "startOffset": 121, "endOffset": 185}], "year": 2016, "abstractText": "Modern NLP models rely heavily on engineered features, which often combine word and contextual information into complex lexical features. Such combination results in large numbers of features, which can lead to overfitting. We present a new model that represents complex lexical features\u2014comprised of parts for words, contextual information and labels\u2014in a tensor that captures conjunction information among these parts. We apply lowrank tensor approximations to the corresponding parameter tensors to reduce the parameter space and improve prediction speed. Furthermore, we investigate two methods for handling features that include n-grams of mixed lengths. Our model achieves state-of-the-art results on tasks in relation extraction, PPattachment, and preposition disambiguation.", "creator": "LaTeX with hyperref package"}}}