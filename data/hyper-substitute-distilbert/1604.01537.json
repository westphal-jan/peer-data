{"id": "1604.01537", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "Generating Chinese Classical Poems with RNN Encoder-Decoder", "abstract": "both allow teams now correctly reconstructed legacy poem lines by your sequence - to - sequence learning process, & understand a novel system anchored on the rnn encoder - decoder structure to generate markers ( jueju in cantonese ), enable semantic topic and sequence input. our system can jointly learn semantic meaning within a single source, semantic relevance among lines in a poem, especially the use thereof horizontal, rhythmical and tonal variations, without utilizing any constraint templates. experimental results show that our system confronts other competitive systems. teams just find that the high points generally dissolve the word boundaries : chinese dynasty poetry potentially cross target lines into latin dramatically improve achievement.", "histories": [["v1", "Wed, 6 Apr 2016 08:26:31 GMT  (501kb)", "http://arxiv.org/abs/1604.01537v1", "12 pages, 8 figures, 4 tables"]], "COMMENTS": "12 pages, 8 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["xiaoyuan yi", "ruoyu li", "maosong sun"], "accepted": false, "id": "1604.01537"}, "pdf": {"name": "1604.01537.pdf", "metadata": {"source": "CRF", "title": "Generating Chinese Classical Poems with RNN Encoder-Decoder", "authors": ["Xiaoyuan Yi", "Ruoyu Li", "Maosong Sun"], "emails": ["yxyz2012yxy@163.com", "liruoyusince1995@gmail.com", "sms@mail.tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Chinese classical poetry is undoubtedly the largest and most bright pearl, if Chinese classical literature is compared to a crown. As a kind of literary form starting from the Pre-Qin Period, classical poetry stretches more than two thousand years, having a farreaching influence on the development of Chinese history. Poets write poems to record important events, express feelings and make comments. There are different kinds of Chinese classical poetry, in which the quatrain with huge quantity and high quality must be considered as a quite important one. In the most famous anthology of classical\npoetry, Three Hundred of Tang Poems (Sun, 1764), quatrains cover more than 25%, whose amount is the largest. The quatrain is a kind of classical poetry with rules and forms which mean that besides the necessary requirements on grammars and semantics for general poetry, quatrains must obey the rules of structure and tone. Figure 1 shows a quatrain generated by our system. A quatrain contains four lines, each line consists of seven or five characters. In Archaic Chinese, characters are divided into two kinds according to the tone, namely Ping(level tone) and Ze(oblique tone). Characters of particular tone must be in\nparticular positions, which makes the poetry cadenced and full of rhythmic beauty. Meanwhile, according to the vowels, characters are divided into different rhyme categories. The last character of the first(optional), second and last line in a quatrain must belong to the same rhyme category, which enhances the coherence of poetry. In this paper, we mainly focus on the automatic generation of quatrains. Actually, the automatic generation of poetry has been a hot issue for a long time, and Deep Learning opens a new door to it now, which makes computer no longer rely on prepared templates, and try to learn the composition method automatically from a large number of excellent poems. Poetry composition by machine is not only a beautiful wish. Based on the poem generation system, interesting applications can be developed, which can be used for education of Chinese classical poetry and the literary researches. Different from the semantically similar pairs in machine translation tasks, the pair of two adjacent sentences in a quatrain is semantically relevant. We take poem generation as a sequence-to-sequence learning problem, and use RNN EncoderDecoder to learn the semantic meaning within a single line, the semantic relevance among lines and the use of rhythm jointly. Furthermore, we use attention mechanism to capture character associations to improve the relevance between input lines and output lines. Consisting of three independent line generation blocks (word-to-line, line-to-line and context-to-line), our system can generate a quatrain with a user keyword. Our system is evaluated based on the task of quatrain generation with two kinds of methods, the automatic and the manual. Experimental results show that our system outperforms other generation systems.\nThe rest of this paper is organized as follows. In section 2 we will introduce the related methods and systems. In section 3 we will give the details of our system and analyze the advantages of the model. Then Section 4 will give the evaluation experiments design and results. In section 5 we draw a conclusion and point out future work."}, {"heading": "2 Related Work", "text": "The research about poetry generation started in 1960s, and is a focus in recent decades. Manurung(2003) proposed three criteria for automatically generated poetry: grammaticality(the generated lines must obey grammar rules and be readable), meaningfulness(the lines should express something related to the theme) and poeticness(generated poems must have poetic features, such as the rhythm, cadence and the special use of words). The early methods are based on rules and templates. For example, ASPERA (Gerv\u00e1s, 2001) uses the changes of accent as the templates to fill with words. Haiku generation system (Wu et al., 2009) expands the input queries to haiku sentences according to the rules extracted from the corpus. Such methods are mechanical, which match the requirements of grammaticality, but perform poorly on meaningfulness and poeticness. One important approach is to generate poems with evolutionary algorithms. The process of poetry generation is described as natural selection. Then through genetic heredity and variation, good results are selected by the evaluation functions (Manurung, 2003; Levy, 2001). However, the methods depend on the quality of the evaluation functions which are hard to be designed well. Generally, the sentences perform better on meaningfulness but can\nhardly satisfy the poeticness. Another approach is based on the methods for the generation of other kinds of texts. Yan et al.(2013) generate poems with the method of automatic summarization. While SMT is first applied on the task of couplets generation by (Jiang and Zhou, 2008). They treat the generation of the couplets as a kind of machine translation tasks. He et al.(2012) apply the method on quatrain generation, translating the input sentence into the second sentence, the second one into the third one, and so on. Sentences generated with such method is good at the relevance, but cannot obey the rules and forms. With the cross field of Deep Learning and Natural Language Process becoming focused, neural network has been applied on poetry generation. Zhang and Lapata (2014) compress all the previous information into a vector with RNN to produce the probability distribution of the next character to be generated. Our work differs from the previous work mainly as follows. Firstly, we use RNN Encoder-Decoder as the basic structure of our system compared with the method of (He et al., 2012). Moreover, in He's system the rhythm is controlled externally and the results perform poorly on tonal patterns. While our system can learn all these things jointly. Secondly, compared with (Zhang and Lapata, 2014), our model is based on bidirectional RNN with gated units instead of the simple RNN. Besides, Zhang (2014) compress all context information into a small vector, losing much of the useful information. They need two more translation models and another rhythm template to control semantic relevance and tones. While the results of our system can obey these constraints naturally. Finally, we use attention mechanism to\ncapture character associations and invert target sentences in training."}, {"heading": "3 The Poem Generator", "text": "The RNN Encoder-Decoder structure is suitable for sequence-to-sequence learning tasks. In machine translation tasks, the sentence pairs are semantically similar, from which the model learns the corresponding relations. In Chinese classical quatrains, there is a close semantic relevance between two adjacent lines. Such two poem lines are semantically relevant sequence pairs. We use RNN Encoder-Decoder to learn the relevance which is then used to generate a poem line given the previous line. For utilizing context information in different levels, we build three poem line generation blocks (word-to-line, line-to-line and context-to-line) to generate a whole quatrain. As illustrated in Figure 2, the user inputs a keyword as the topic to show the main content and emotion the poem should convey. Firstly, WPB generates a line relevant to the keyword as the first line. Then SPB takes the first line as input and generates the relevant second line. CPB generates the third line with the first two lines as input. Finally, CPB takes the second and third lines as input and generates the last line. We train the model and generate poems based on Chinese characters, since there are no effective segmentation tools for Archaic\nChinese. Fortunately, the length of Chinese classical poem lines is fixed five or seven characters. And most words in Chinese classical poetry consist of one or two Chinese characters. Therefore, this method is feasible."}, {"heading": "3.1 Sentence Poem Block (SPB)", "text": "SPB (Sentence Poem Block) is used for line-to-line generation. When generating the second line, the only available context information is the first line. Therefore, we use SPB to generate the second line, taking the first line as input. As shown in figure 3, we use bidirectional RNN with attention mechanism proposed by (Bahdanau et al., 2015) to build SPB. Let us denote an input poem line by X , , \u2026 , , and an output line by Y , , \u2026 , . e is the wordembedding of the t-th character . and\nrepresent the forward and backward hidden states in Encoder respectively.\nIn Encoder:\n\u2219 1 2 3\n1 \u2219 \u2219 4 ; 5\n(1)-(5) are formulas for the computation\nof forward hidden states. The computation of backward hidden states is similar. \u2022 is element-wise multiplication. is the final hidden state of t-th character in Encoder. and are the reset gate and update gate respectively in (Cho et al., 2014). The formulas in decoder are similar. The difference is that a context vector of attention mechanism is used to calculate the hidden state of t-th character in Decoder.\nis computed as:\n, 6\n, exp ,\n\u2211 exp , 7\n, tanh 8 Some poems in our corpus are not of\nhigh quality. Therefore, we add a neural language model (Mikolov et al., 2010) into the system to improve the meaningfulness and make the poem more idiomatic for modern people. We combine the probability distributions of decoder and language model by:\n1 \u2217 , , | , \u2026 , 9\nWhere lm is the probability distribution\nof language model, and is its weight. Larger lead to more common lines, and smaller lead to more novel lines. Furthermore, as shown in figure 3, when training SPB we invert the target lines for two reasons. First, the final character of second line must be level-tone. While the tail character of third line is oblique-tone. If the final character of input line is level-tone, SPB can't determine the tone of the final character\nof output line because of the pairs <line2, line3> in training data. We add a tone controller into SPB. Obviously, this control will do harm to meaningfulness of the outputs. Therefore we invert the target sentences in training so that SPB can generate the tail character first, which will minimize the damage on meaningfulness as possible. Furthermore, we find inverting target lines can improve the performance, as the way of inverting the source lines mentioned in (Sutskever et al., 2014)."}, {"heading": "3.2 Context Poem Block (CPB)", "text": "To utilize more context information, we build another Encoder-Decoder called CPB (Context Poem Block). The structure of CPB is similar with that of SPB. The difference is that we concatenate two adjacent lines in a quatrain as a long input sequence, and use the third line as the target sequence in training. By this means, the model can utilize information of last two lines when generating current line. The final characters of the second and the fourth line must rhyme and the final character of the third line must not rhyme. When generating the fourth line, SPB can't determine the rhyme. By taking the second line into consideration, CPB can learn the rhyme. Thus we use CPB to generate the third and the fourth lines. Zhang (2014) utilizes context by compressing all lines into a 200-dimensional vector, which causes a severe loss to semantic information. Whereas our method can save all information. When generating current line, the model can learn focus points with attention, rather than use all context indiscriminately, which will improve semantic relevance between the inputs and the outputs. We don't concatenate more lines for two reasons. Firstly, too long sequences result in low performance. Secondly, relevance between the fourth line and the\nfirst line is relatively weak, there is no need to make the system more complicated."}, {"heading": "3.3 Word Poem Block (WPB)", "text": "A big shortcoming of SMT-based methods is that they need another model to generate the first line. For example, He (2012) expands user keywords, then use constraint templates and a language model to search for a line. For RNN Encoder-Decoder, words and sentences will be mapped into the same vector space. Since our system is based on characters, words can be considered as short sequences. Ideally, SPB will generate a relevant line taking a word as input. But the training pairs are all long sequences, it won't work well when the input is a short word. Therefore, we train the third EncoderDecoder, called Word Poem Block (WPB). Based on the model parameters of trained SPB, we use some <word, line> pairs to train it more to improve WPB's ability of generating long sequences with short sequences."}, {"heading": "3.4 Qualitative Analysis", "text": "Our poetry generation task is on sequence pairs with semantic relevance. We\nconducted several qualitative experiments. The results show that RNN Encoder-Decoder can capture the semantic meanings and relevance of pairs on such relevance learning tasks well, which is the reason why we use the model to build our system."}, {"heading": "3.4.1 Poem Line Representations", "text": "We used the average of in formula (5) as the representation of a poem line. We selected three types of classical poetry: frontier-style poetry (poetry about the wars), boudoir-plaint poetry (poetry about women's sadness) and history-nostalgia poetry (poetry about history). For each type, we obtained ten lines and used Barnes-Hut-SNE (van der Maaten, 2013) to map their representations into two-dimensional space. As shown in figure 4, lines with the same type gather together. The representations can capture the semantic meanings of poem lines well. We also generated representations for 20,000 lines then calculated their cos distance and calculated their lcs (longest common sequences) as the comparison. As shown in table 1, the results of lcs are simple string matchings, but the results of knn have the same meaning as the input's. Though there are no explicit boundaries in input sequences for SPB, the vector representations can contain the information of whole words. For example, \u201c\u767d\u5934\u201d(white hair, a symbol of senility) is a word. In results\nof knn, the two character \u201c\u767d\u201d and \u201c\u5934\u201d appear as a entirety(or not appear). But for lcs, character \"\u767d\" appears alone in line \u201c\u6101 \u7ea2\u6028\u767d\u6ee1\u6c5f\u6ee8\u201d, in which \u201c\u767d\u201d means white flowers."}, {"heading": "3.4.2 Gated Units in Word Boundary Recognition", "text": "The training and generation are based on characters without any explicit word boundaries in the sequences, but gated units can recognize the word boundaries roughly. As we can see in formula (2) and formula (3), when tends to be zero and\ntends to be one, the gated units tend to use current input to update the hidden state. Whereas gated units tend to continue previous hidden states. We used the average value of every elements in as the reset value of t-th character. Along the direction of hidden states propagation, we calculated the difference between reset values of the latter and the previous characters to get the reset tendency. Similarly, we got the update tendency. As shown in figure 5, higher reset tendency and smaller update tendency mean\nthat the two characters tend to be an entirety. Whereas they tend to be separated. In line \u201c\u4e00\u58f0\u79cb\u96c1\u8fde\u5929\u8fdc\u201d, \u201c\u4e00\u58f0\u201d and \u201c\u79cb \u96c1\u201d are both words. We can see the reset tendency and update tendency reflect the word boundaries roughly. Furthermore, the tendency of gated units in decoder is similar with that in encoder. This nature makes the vector representations contain information of whole words and makes output lines keep the same structures as input lines."}, {"heading": "3.4.3 Attention Mechanism in Capturing Associations", "text": "Different from word alignments in translation task, attention mechanism can capture the implicit associations between two characters. Figure 6 is the visualizations of\n, in formula (11). The top two plots show the associations between input and output lines generated by a SPB trained with inverted target lines. The bottom two plots are the results of SPB trained with normal target lines. And in the left two plots, outputs are syntactically similar with inputs. While in the right two ones, output are syntactically different from\nthe inputs. In the top left plot, when SPB generates the character \u201c\u5e06\u201d(sail), there is a strong dependence on the character \u201c\u96c1\u201d(wild goose). In Chinese classical poetry, \u201c\u96c1\u201d is a symbol of homesickness and \u201c\u5e06\u201d is a symbol of travelers. There is an association between these two characters. Also, in the top right plot, besides the first character, there are dependencies on the second character \u201c\u5f52\u201d(return), since the input line is about home-bound ships and the output line is about the travelers. But the associations are more obvious on pairs with similar syntactic structures. It is worth mentioning that some researchers (Wang et al., 2015) made an attempt to generate Songci (a kind of Chinese poetry in Song dynasty) with LSTM and drew a conclusion that the attention mechanism is ineffective. While we got a great improvement by inverting the target lines in training. There are no obvious associations in the bottom two plots compared with results in the top two plots. We think the improvement by inverting may be related to attributive structures in Archaic Chinese. The quantitative evaluation results in section 4 also show that inverting target lines leads to higher scores. Figure 7 shows an attention result of CPB. As we can see, because the input line is\na description of a beautiful woman, attention mechanism focused on two characters, \u201c\u773c\u201d (eyes) and \u201c\u5507\u201d(lips). Though there is a color word \u201c\u4e39\u201d(red) in the input line, attention mechanism chose to focus on \u201c\u773c\u201d and \u201c\u5507\u201d instead of \u201c\u4e39\u201d for generating the character \u201c\u7ea2\u201d(also means red color) since in Chinese classical poetry, \u201c\u7ea2\u201d is often used to describe the beauty of women. Compared with the simple alignments of words with same semantic meanings in translation task, attention mechanism can learn the associations and helps the system to focus on the most relevant information instead of all context, which results in a stronger relevance between input line and output line."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Data and Settings", "text": "Our corpus contains 398,391 poems from Tang Dynasty to the contemporary. We didn't use earlier poetry because regular tonal patterns were formed in Tang dynasty. We used 10,000 poems as testing set and other as training set. We extracted three pairs from each quatrain. We used 999,442 pairs to train SPB, and 596,610 pairs to train CPB. For training WPB, we selected 3000 words, and for each words we got 150 lines which the word appears in. Finally we obtained 450,000 word-to-line pairs (half are 5-char and the other half are 7-char). We used the whole training set to train neural language model. We built our system based on GroundHog1."}, {"heading": "4.2 Evaluation Design", "text": "BLEU Score Evaluation Referring to He (2012) and Zhang(2014), we used \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1\u00a0 https://github.com/lisa-groundhog/GroundHog.\u00a0 2\u00a0 http://duilian.msra.cn/jueju/.\u00a0 3\u00a0 http://www.poeming.com/web/index.htm.\u00a0 4\u00a0 Because\u00a0the\u00a0keywords\u00a0is\u00a0limited\u00a0in\u00a0SMT,\u00a0we\u00a0used\u00a0\nBLEU-2 score to evaluate the output line given the previous line as input. Since most words in Chinese classical poetry consist of one or two characters, BLEU-2 is effective. It's hard to obtain human-authored references for poem lines so we used the method in (He et al., 2012) to extract references automatically. We selected 4,400 quatrains from testing set (2,200 of them are 5-char and other 2,200 are 7-char) and extracted 20 references for each line in a quatrain(except the last line). We compared our system with the system in (He et al., 2012). Human Evaluation Since poetry is a kind of creative text, human evaluation is necessary. Referring to the three criteria in (Manurung, 2003), we designed five criteria: Fluency (are the lines fluent and wellformed?), Coherence(does the quatrain has consistent topic across four lines?), Meaningfulness(does the poem convey some certain messages?), Poeticness(does the poem have poetic features such as the poetic images?), Entirety(the reader's general impression on the poem). Each criterion was scored from 0 to 5. We compared four comparison systems. PG, our system. SMT, He (2012)'s system2. DX, the DaoXiang Poem Creator 3 . This system is the pioneer for Chinese classical poetry generation. It has been developed for 15 years and been used over one hundred million times. Human, the poems of famous ancient poets containing the given keywords. We selected 24 typical keywords and generated two quatrains (5-char and 7-char) for each keyword using the four systems4. By this means, we obtained 192 quatrain (24*4*2) in total. We invited 16 experts5 on Chinese classical poetry to evaluate these quatrains. Each expert evaluated 24 quatrains.\nlines\u00a0of\u00a0ancient\u00a0poets\u00a0for\u00a0the\u00a0keywords\u00a0not\u00a0included\u00a0 in\u00a0their\u00a0list\u00a0as\u00a0the\u00a0first\u00a0lines.\u00a0 5\u00a0 All\u00a0the\u00a0experts\u00a0have\u00a0the\u00a0ability\u00a0to\u00a0assess\u00a0and\u00a0create\u00a0 Chinese\u00a0classical\u00a0poetry.\u00a0\nand each group completed the assessments of the 192 poems. Thus we got two scores for each quatrain and we used the average score."}, {"heading": "4.3 Evaluation Results", "text": "Table 2 shows the BLEU-2 scores. Because DX system generates poetry as a whole, we only compared our system with SMT on single line generation task. Given an input line, the output of SMT is often the original adjacent next line of ancient poets. To be fair, we removed the lines of ancient poets from the top-n lists of our system and SMT, then we used the best ones. In Chinese classical poetry, the relevance between two lines in a pair is related to the position. Therefore He et al.(2012) use pairs in different positions to train corresponding position-sensitive models. Because of the \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 6\u00a0 We\u00a0got\u00a0lower\u00a0BLEU\u20101\u00a0scores\u00a0of\u00a0SMT\u00a0system\u00a0than\u00a0 those\u00a0reported\u00a0in (He\u00a0et\u00a0al.,\u00a02012),\u00a0because\u00a0we\u00a0\nlimited training data, we used pairs in all positions to train SPB. Even so, we got much higher BLEU scores than SMT in all positions 6 . Moreover, 95% of the outputs generated by our system observe tonal constraints, but only 31% of SMT's outputs observe the constraints. We can also see that simply adding the attention mechanism to system made against the performance, but the combination of attention and inverting led to a great improvement. In (Sutskever et al., 2014), they find reversing source sentences can improve the LSTM\u2019s performance and conclude that this preprocessing can introduce short term dependencies between the source and the target sentences which will make the optimization problem easier. As their explanation, inverting source sentences and inverting target sentences should be equivalent. But as shown in table 2, inverting\nremoved\u00a0the\u00a0lines\u00a0of\u00a0ancient\u00a0poets\u00a0from\u00a0candidate\u00a0 lists.\u00a0\nsource sentences made little improvement in our task. We think this is because of the attributive structure in Chinese classical poetry. There are many words with the structure attributive + central word in Chinese poetry. For example, \u201c\u9752\u8349\u201d(green grass), the character \u201c \u9752 \u201d (green) is attributive and the character \u201c\u8349\u201d(grass) is central word. In normal generation order, \u201c\u9752\u201d will be generated earlier than \u201c\u8349\u201d. But there are many other characters can be the central word of \u201c\u9752\u201d, such as \u201c\u9752\u5c71\u201d (green mountain), \"\u9752\u4e91\"(green cloud), \u201c\u9752 \u70df\u201d (green smoke), which increases the uncertainty in the generation of \u201c\u8349\u201d . Whereas inverting target sentence can reduce this uncertainty since the attributive of \u201c\u8349\u201d is often \u201c\u9752\u201d in Chinese classical poetry. When generating t-th character, the only information can be used to determine the attention weights , by Decoder is , thus reducing uncertainty will lead to better attention weights. This is why simply adding the attention mechanism didn't work. As shown in table 3, our system got higher scores than other systems, expect the Human. For SMT and DX, the scores of 7- char poems are lower than that of 5-char poems in all criteria (both in Human evaluation and BLEU evaluation) because the composition of 7-char quatrains is more difficult. But the poems generated by PG got higher scores on 7-char poems, benefiting from gated units and attention mechanism. Scores of PG is closed to scores of Human, though there is still a gap. We also asked the experts to select a best line from the quatrains evaluated. 37% of the selected 5-char lines are of our system and 42% are of poets. And 45% of the selected 7-char lines are of our system, 45% are of poets. This indicates that our system has little difference with poets, at least in generating meaningful sentences.\nThough the qualitative experiments have shown the feasibility of our system, we also conducted a quantitative experiment to compare SPB and CPB. We trained another Encoder-Decoder called A3, which concatenates three adjacent lines as input and takes the fourth line as target. BLEU is used to evaluate the line-to-line generation task which focuses on the semantic relevance between two lines, but CPB and A3 both concentrate on the relevance between the generated line and all context. Thus BLEU is not suitable for comparing these three Blocks. Therefore, we designed another comparison method, called generation probability ranking score (GPRS). Taking a sequence as input, we can get the generation probability of a specified line by Decoder. We used the testing data in section 4.2. By inputting a testing line to a Block, we got the generation probabilities of all references (including the original next line of ancient poet). We sorted all the references descending on generation probability. Then we calculated the GPRS of the input line by\n, r is the rank of the original next line of ancient poet (0 r N ) and N is the number of references. Higher GPRS indicates that the Block has a greater chance to generate the original lines, which means the Block can capture the relevance between generated lines and context better. As shown in table 4, we can see that CPB got the highest scores. And A3 is better than SPB but worse than CPB, for the\nreasons we mentioned in section 3.2."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we take the generation of poem lines as a sequence-to-sequence learning problem, and build a novel system to generate quatrains based on RNN Encoder-Decoder. Compared with other methods, our system can jointly learn semantic meanings, semantic relevance, and the use of rhythmical and tonal patterns, without utilizing any constraint templates. Both automatic evaluation and human evaluation show that our system outperforms other systems, but there is still a gap between our system and ancient poets. We show that RNN Encoder-Decoder is also suitable for the learning tasks on semantically relevant sequences. The attention mechanism can captures character associations, and gated units can recognize word boundaries roughly. Moreover, inverting target lines in training will lead to better performance. There are lots to do for our system in the future. Based on different blocks, our system is extensible. We will improve our system to utilize more context information and generate other types of Chinese poetry, such\nas Songci and Yuefu. We also hope our work could be helpful to other related work, such as the building of poetry retrieval system, word segmentation of poems, and literature researches."}, {"heading": "In Proceedings of the 22nd International", "text": "Conference on Computational Linguistics, pages 377-384, Manchester, UK. Robert P. Levy. 2001. A computational model of poetic creativity with neural network as measure of adaptive fitness. In Proceedings of the ICCBR-01 Workshop on Creative Systems. Hisar Maruli Manurung. 2003. An evolutionary algorithm approach to poetry generation. Ph.D. thesis, University of Edinburgh. T. Mikolov, M. Karafi\u00e1t, L. Burget, J. \u010cernock\u00fd, and S. Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of INTERSPEECH, pages 1045-1048, Makuhari, Japan. Zhu Sun. 1764. Three hundred Poems of the Tang Dynasty(\u300a\u5510\u8bd7\u4e09\u767e\u9996\u300b). Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. Advances in Neural Information Processing Systems, 4:3104-3112. Laurens van der Maaten. 2013. Barnes-hut-sne. In Proceedings of the First International Conference on Learning Representations(ICLR 2013), Scottsdale, Arizona. Qixin Wang, Tianyi Luo, Dong Wang, and Chao Xing. 2015. Chinese song iambics generation with neural attention-based model.\nXiaofeng Wu, Naoko Tosa, and Ryohei Nakatsu. 2009. New hitch haiku: An interactive renku poem composition supporting tool applied for sightseeing navigation system. In Proceedings of the 8th International Conference on Entertainment Computing, pages 191-196, Paris, France. Rui Yan, Han Jiang, Mirella Lapata, Shou-De Lin, Xueqiang Lv, and Xiaoming Li. 2013. I, poet:automatic Chinese poetry composition through a generative summarization framework under constrained optimization. In Proceedings of the 23rd International Joint Conference on Artificial Intelligence, pages 2197-2203, Beijing, China. Xingxing Zhang and Mirella Lapata. 2014. Chinese poetry generation with recurrent neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 670-680, Doha, Qatar."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyungHyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 2015 International Conference on Learning Representations, San", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine", "author": ["Diego", "CA.K. Cho", "B. Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "Diego et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Diego et al\\.", "year": 2014}, {"title": "Generating Chinese classical poems with statistical machine translation models", "author": ["Jing He", "Ming Zhou", "Long Jiang."], "venue": "Proceedings of the 26th AAAI Conference on Artificial Intelligence, pages 1650 \u2013 1656,", "citeRegEx": "He et al\\.,? 2012", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Generating Chinese couplets using a statistical mt approach", "author": ["Toronto", "Canada. Long Jiang", "Ming Zhou."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics, pages", "citeRegEx": "Toronto et al\\.,? 2008", "shortCiteRegEx": "Toronto et al\\.", "year": 2008}, {"title": "A computational model of poetic creativity with neural network as measure of adaptive fitness", "author": ["Manchester", "UK. Robert P. Levy"], "venue": "In Proceedings of the ICCBR-01 Workshop on Creative Systems", "citeRegEx": "Manchester and Levy.,? \\Q2001\\E", "shortCiteRegEx": "Manchester and Levy.", "year": 2001}, {"title": "An evolutionary algorithm approach to poetry generation", "author": ["Hisar Maruli Manurung."], "venue": "Ph.D. thesis, University of Edinburgh. T. Mikolov, M. Karafi\u00e1t, L. Burget, J. \u010cernock\u00fd, and S. Khudanpur. 2010. Recurrent neural", "citeRegEx": "Manurung.,? 2003", "shortCiteRegEx": "Manurung.", "year": 2003}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems, 4:3104-3112. Laurens van der Maaten. 2013. Barnes-hut-sne.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "New hitch haiku: An interactive renku poem composition supporting tool applied for sightseeing navigation system", "author": ["Xiaofeng Wu", "Naoko Tosa", "Ryohei Nakatsu."], "venue": "Proceedings of the 8th International Conference on", "citeRegEx": "Wu et al\\.,? 2009", "shortCiteRegEx": "Wu et al\\.", "year": 2009}, {"title": "I, poet:automatic Chinese poetry composition", "author": ["Paris", "France. Rui Yan", "Han Jiang", "Mirella Lapata", "Shou-De Lin", "Xueqiang Lv", "Xiaoming Li"], "venue": "Entertainment Computing,", "citeRegEx": "Paris et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paris et al\\.", "year": 2013}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Xingxing Zhang", "Mirella Lapata."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 670-680, Doha, Qatar.", "citeRegEx": "Zhang and Lapata.,? 2014", "shortCiteRegEx": "Zhang and Lapata.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "Haiku generation system (Wu et al., 2009) expands the input queries to haiku sentences according to the rules extracted from the corpus.", "startOffset": 24, "endOffset": 41}, {"referenceID": 5, "context": "Then through genetic heredity and variation, good results are selected by the evaluation functions (Manurung, 2003; Levy, 2001).", "startOffset": 99, "endOffset": 127}, {"referenceID": 5, "context": "Manurung(2003) proposed three criteria for automatically generated poetry: grammaticality(the generated lines must obey grammar rules and be readable), meaningfulness(the lines should express something related to the theme) and poeticness(generated poems must have poetic features, such as the rhythm, cadence and the special use of words).", "startOffset": 0, "endOffset": 15}, {"referenceID": 2, "context": "Firstly, we use RNN Encoder-Decoder as the basic structure of our system compared with the method of (He et al., 2012).", "startOffset": 101, "endOffset": 118}, {"referenceID": 9, "context": "Secondly, compared with (Zhang and Lapata, 2014), our model is based on bidirectional RNN with gated units instead of the simple RNN.", "startOffset": 24, "endOffset": 48}, {"referenceID": 2, "context": "He et al.(2012) apply the method on quatrain generation, translating the input sentence into the second sentence, the second one into the third one, and so on.", "startOffset": 0, "endOffset": 16}, {"referenceID": 2, "context": "He et al.(2012) apply the method on quatrain generation, translating the input sentence into the second sentence, the second one into the third one, and so on. Sentences generated with such method is good at the relevance, but cannot obey the rules and forms. With the cross field of Deep Learning and Natural Language Process becoming focused, neural network has been applied on poetry generation. Zhang and Lapata (2014) compress all the previous information into a vector with RNN to produce the probability distribution of the next character to be generated.", "startOffset": 0, "endOffset": 423}, {"referenceID": 2, "context": "He et al.(2012) apply the method on quatrain generation, translating the input sentence into the second sentence, the second one into the third one, and so on. Sentences generated with such method is good at the relevance, but cannot obey the rules and forms. With the cross field of Deep Learning and Natural Language Process becoming focused, neural network has been applied on poetry generation. Zhang and Lapata (2014) compress all the previous information into a vector with RNN to produce the probability distribution of the next character to be generated. Our work differs from the previous work mainly as follows. Firstly, we use RNN Encoder-Decoder as the basic structure of our system compared with the method of (He et al., 2012). Moreover, in He's system the rhythm is controlled externally and the results perform poorly on tonal patterns. While our system can learn all these things jointly. Secondly, compared with (Zhang and Lapata, 2014), our model is based on bidirectional RNN with gated units instead of the simple RNN. Besides, Zhang (2014) compress all context information into a small vector, losing much of the useful information.", "startOffset": 0, "endOffset": 1062}, {"referenceID": 0, "context": "As shown in figure 3, we use bidirectional RNN with attention mechanism proposed by (Bahdanau et al., 2015) to build SPB.", "startOffset": 84, "endOffset": 107}, {"referenceID": 6, "context": "Furthermore, we find inverting target lines can improve the performance, as the way of inverting the source lines mentioned in (Sutskever et al., 2014).", "startOffset": 127, "endOffset": 151}, {"referenceID": 2, "context": "It's hard to obtain human-authored references for poem lines so we used the method in (He et al., 2012) to extract references automatically.", "startOffset": 86, "endOffset": 103}, {"referenceID": 2, "context": "We compared our system with the system in (He et al., 2012).", "startOffset": 42, "endOffset": 59}, {"referenceID": 5, "context": "Referring to the three criteria in (Manurung, 2003), we designed five criteria: Fluency (are the lines fluent and wellformed?), Coherence(does the quatrain has consistent topic across four lines?), Meaningfulness(does the poem convey some certain messages?), Poeticness(does the poem have poetic features such as the poetic images?), Entirety(the reader's general impression on the poem).", "startOffset": 35, "endOffset": 51}, {"referenceID": 2, "context": "It's hard to obtain human-authored references for poem lines so we used the method in (He et al., 2012) to extract references automatically. We selected 4,400 quatrains from testing set (2,200 of them are 5-char and other 2,200 are 7-char) and extracted 20 references for each line in a quatrain(except the last line). We compared our system with the system in (He et al., 2012). Human Evaluation Since poetry is a kind of creative text, human evaluation is necessary. Referring to the three criteria in (Manurung, 2003), we designed five criteria: Fluency (are the lines fluent and wellformed?), Coherence(does the quatrain has consistent topic across four lines?), Meaningfulness(does the poem convey some certain messages?), Poeticness(does the poem have poetic features such as the poetic images?), Entirety(the reader's general impression on the poem). Each criterion was scored from 0 to 5. We compared four comparison systems. PG, our system. SMT, He (2012)'s system2.", "startOffset": 87, "endOffset": 965}, {"referenceID": 2, "context": "Because of the 6  We got lower BLEU\u20101 scores of SMT system than those reported in (He et al., 2012), because we limited training data, we used pairs in all positions to train SPB.", "startOffset": 82, "endOffset": 99}, {"referenceID": 6, "context": "In (Sutskever et al., 2014), they find reversing source sentences can improve the LSTM\u2019s performance and conclude that this preprocessing can introduce short term dependencies between the source and the target sentences which will make the optimization problem easier.", "startOffset": 3, "endOffset": 27}, {"referenceID": 2, "context": "Therefore He et al.(2012) use pairs in different positions to train corresponding position-sensitive models.", "startOffset": 10, "endOffset": 26}], "year": 2016, "abstractText": "We take the generation of Chinese classical poem lines as a sequence-to-sequence learning problem, and build a novel system based on the RNN Encoder-Decoder structure to generate quatrains (Jueju in Chinese), with a topic word as input. Our system can jointly learn semantic meaning within a single line, semantic relevance among lines in a poem, and the use of structural, rhythmical and tonal patterns, without utilizing any constraint templates. Experimental results show that our system outperforms other competitive systems. We also find that the attention mechanism can capture the word associations in Chinese classical poetry and inverting target lines in training can improve", "creator": "PScript5.dll Version 5.2.2"}}}