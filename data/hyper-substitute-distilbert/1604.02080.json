{"id": "1604.02080", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2016", "title": "Planning with Information-Processing Constraints and Model Uncertainty in Markov Decision Processes", "abstract": "information - theoretic tools toward performing numerical acting have been proposed to solve particular classes besides markov decision problems. nevertheless, mathematical techniques well governed uses basic variational free development principle and allow designing strategic planning mechanisms lacking information - processing constraints modeled in terms above a hermann - leibler divergence with corresponds to a reference distribution. together we consider a generalization of functional mdp planners by taking model uncertainty to account. as model uncertainty need also be formalized as an information - driven constraint, we commonly derive sigma functional equation. applying single generalized variational principle. we get a generalized forward iteration scheme and obtaining a convergence proof. overcome system complexity, this stability simulation reduces standard value iteration, their specialized model, bayesian mdp planning, as robust planning. experiments develop the benefits of this simulation in a closed world simulation.", "histories": [["v1", "Thu, 7 Apr 2016 17:12:07 GMT  (276kb,D)", "http://arxiv.org/abs/1604.02080v1", "16 pages, 3 figures"]], "COMMENTS": "16 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.AI cs.SY", "authors": ["jordi grau-moya", "felix leibfried", "tim genewein", "daniel a braun"], "accepted": false, "id": "1604.02080"}, "pdf": {"name": "1604.02080.pdf", "metadata": {"source": "CRF", "title": "Planning with Information-Processing Constraints and Model Uncertainty in Markov Decision Processes", "authors": ["Jordi Grau-Moya", "Felix Leibfried", "Tim Genewein", "Daniel A. Braun"], "emails": ["jordi.grau@tuebingen.mpg.de,"], "sections": [{"heading": null, "text": "Keywords: bounded rationality, model uncertainty, robustness, planning, Markov Decision Processes"}, {"heading": "1 Introduction", "text": "The problem of planning in Markov Decision Processes was famously addressed by Bellman who developed the eponymous principle in 1957 [1]. Since then numerous variants of this principle have flourished in the literature. Here we are particularly interested in a generalization of the Bellman principle that takes information-theoretic constraints into account. In the recent past there has been a special interest in the KullbackLeibler divergence as a constraint to limit deviations of the action policy from a prior. This can be interesting in a number of ways. Todorov [2,3], for example, has transformed the general MDP problem into a restricted problem class without explicit action variables, where control directly changes the dynamics of the environment and control costs are measured by the Kullback-Leibler divergence between controlled and uncontrolled dynamics. This simplification allows mapping the Bellman recursion to a linear algebra problem. This approach can also be be generalized to continuous state spaces leading to path integral control [4,5]. The same equations can also be interpreted in terms of bounded rational decision-making where the decision-maker has limited computational resources that allow only limited deviations from a prior decision strategy ar X iv :1 60 4.\n02 08\n0v 1\n[ cs\n.A I]\n7 A\npr 2\n01 6\n(measured by the Kullback-Leiber divergence in bits) [6]. Such a decision-maker can also be instantiated by a sampling process that has restrictions in the number of samples it can afford [7]. Disregarding the possibility of a sampling-based interpretation, the Kullback-Leibler divergence introduces a control information cost that is interesting in its own right when formalizing the perception action cycle [8].\nWhile the above frameworks have led to interesting computational advances, so far they have neglected the possibility of model misspecification in the MDP setting. Model misspecification or model uncertainty does not refer to the uncertainty arising due to the stochastic nature of the environment (usually called risk-uncertainty in the economic literature), but refers to the uncertainty with respect to the latent variables that specify the MDP. In Bayes-Adaptive MDPs [9], for example, the uncertainty over the latent parameters of the MDP is explicitly represented, such that new information can be incorporated with Bayesian inference. However, Bayes-Adaptive MDPs are not robust with respect to model misspecification and have no performance guarantees when planning with wrong models [10]. Accordingly, there has been substantial interest in developing robust MDP planners [11,12,13]. One way to take model uncertainty into account is to bias an agent\u2019s belief model from a reference Bayesian model towards worst-case scenarios; thus avoiding disastrous outcomes by not visiting states where the transition probabilities are not known. Conversely, the belief model can also be biased towards best-case scenarios as a measure to drive exploration\u2014also referred in the literature as optimism in face of uncertainty [14,15].\nWhen comparing the literature on information-theoretic control and model uncertainty, it is interesting to see that some notions of model uncertainty follow exactly the same mathematical principles as the principles of relative entropy control [3]. In this paper we therefore formulate a unified and combined optimization problem for MDP planning that takes both, model uncertainty and bounded rationality into account. This new optimization problem can be solved by a generalized value iteration algorithm. We provide a theoretical analysis of its convergence properties and simulations in a grid world."}, {"heading": "2 Background and Notation", "text": "In the MDP setting the agent at time t interacts with the environment by taking action at \u2208 A while in state st \u2208 S . Then the environment updates the state of the agent to st+1 \u2208 S according to the transition probabilities T (st+1|at, st). After each transition the agent receives a reward Rst+1st,at \u2208 R that is bounded. For our purposes we will consider A and S to be finite. The aim of the agent is to choose its policy \u03c0(a|s) in order to maximize the total discounted expected reward or value function for any s \u2208 S\nV \u2217(s) = max \u03c0 lim T\u2192\u221e E [ T\u22121\u2211 t=0 \u03b3tRst+1st,at ]\nwith discount factor 0 \u2264 \u03b3 < 1. The expectation is over all possible trajectories \u03be = s0, a0, s1 . . . of state and action pairs distributed according to p(\u03be) = \u220fT\u22121 t=0 \u03c0(at|st) T (st+1|at, st). It can be shown that the optimal value function satisfies the following\nrecursion V \u2217(s) = max\n\u03c0 \u2211 a,s\u2032 \u03c0(a|s)T (s\u2032|a, s) [ Rs \u2032 s,a + \u03b3V \u2217(s\u2032) ] . (1)\nAt this point there are two important implicit assumptions. The first is that the policy \u03c0 can be chosen arbitrarily without any constraints which, for example, might not be true for a bounded rational agent with limited information-processing capabilities. The second is that the agent needs to know the transition-model T (s\u2032|a, s), but this model is in practice unknown or even misspecified with respect to the environment\u2019s true transition-probabilities, specially at initial stages of learning. In the following, we explain how to incorporate both bounded rationality and model uncertainty into agents."}, {"heading": "2.1 Information-Theoretic Constraints for Acting", "text": "Consider a one-step decision-making problem where the agent is in state s and has to choose a single action a from the set A to maximize the reward Rs\u2032s,a, where s\u2032 is the next the state. A perfectly rational agent selects the optimal action a\u2217(s) = argmaxa \u2211 s\u2032 T (s\n\u2032|a, s)Rs\u2032s,a. However, a bounded rational agent has only limited resources to find the maximum of the function \u2211 s\u2032 T (s\n\u2032|a, s)Rs\u2032s,a. One way to model such an agent is to assume that the agent has a prior choice strategy \u03c1(a|s) in state s before a deliberation process sets in that refines the choice strategy to a posterior distribution \u03c0(a|s) that reflects the strategy after deliberation. Intuitively, because the deliberation resources are limited, the agent can only afford to deviate from the prior strategy by a certain amount of information bits. This can be quantified by the relative entropy DKL(\u03c0||\u03c1) = \u2211 a \u03c0(a|s) log \u03c0(a|s) \u03c1(a|s) that measures the average information cost of the policy \u03c0(a|s) using the source distribution \u03c1(a|s). For a bounded rational agent this relative entropy is bounded by some upper limit K. Thus, a bounded rational agent has to solve a constrained optimization problem that can be written as\nmax \u03c0 \u2211 a \u03c0(a|s) \u2211 s\u2032 T (s\u2032|a, s)Rs \u2032 s,a s.t. DKL(\u03c0||\u03c1) \u2264 K\nThis problem can be rewritten as an unconstrained optimization problem\nF \u2217(s) = max \u03c0 \u2211 a \u03c0(a|s) \u2211 s\u2032 T (s\u2032|a, s)Rs \u2032 s,a \u2212 1 \u03b1 DKL(\u03c0||\u03c1) (2)\n= 1 \u03b1 log \u2211 a \u03c1(a|s)e\u03b1 \u2211 s\u2032 T (s \u2032|a,s)Rs \u2032 s,a . (3)\nwhere F \u2217 is a free energy that quantifies the value of the policy \u03c0 by trading off the average reward against the information cost. The optimal strategy can be expressed analytically in closed-form as\n\u03c0\u2217(a|s) = \u03c1(a|s)e \u03b1 \u2211 s\u2032 T (s \u2032|a,s)Rs \u2032 s,a\nZ\u03b1(s)\nwith partition sum Z\u03b1(s) = \u2211 a \u03c1(a|s) exp ( \u03b1 \u2211 s\u2032 T (s \u2032|a, s)Rs\u2032s,a )\n. Therefore, the maximum operator in (2) can be eliminated and the free energy can be rewritten as in (3). The Lagrange multiplier \u03b1 quantifies the boundedness of the agent. By setting \u03b1\u2192 \u221e we recover a perfectly rational agent with optimal policy \u03c0\u2217(a|s) = \u03b4(a \u2212 a\u2217(s)). For \u03b1 = 0 the agent has no computational resources and the agent\u2019s optimal policy is to act according to the prior \u03c0\u2217(a|s) = \u03c1(a|s). Intermediate values of \u03b1 lead to a spectrum of bounded rational agents."}, {"heading": "2.2 Information-Theoretic Constraints for Model Uncertainty", "text": "In the following we assume that the agent has a model of the environment T\u03b8(s\u2032|a, s) that depends on some latent variables \u03b8 \u2208 \u0398. In the MDP setting, the agent holds a belief \u00b5(\u03b8|a, s) regarding the environmental dynamics where \u03b8 is a unit vector of transition probabilities into all possible states s\u2032. While interacting with the environment the agent can incorporate new data by forming the Bayesian posterior \u00b5(\u03b8|a, s,D), where D is the observed data. When the agent has observed an infinite amount of data (and assuming \u03b8\u2217(a, s) \u2208 \u0398) the belief will converge to the delta distribution \u00b5(\u03b8|s, a,D) = \u03b4(\u03b8\u2212\u03b8\u2217(a, s)) and the agent will act optimally according to the true transition probabilities, exactly as in ordinary optimal choice strategies with known models. When acting under a limited amount of data the agent cannot determine the value of an action a with the true transition model according to \u2211 s\u2032 T (s\n\u2032|a, s)Rs\u2032s,a, but it can only determine an expected value according to its beliefs \u222b \u03b8 \u00b5(\u03b8|a, s) \u2211 s\u2032 T\u03b8(s\n\u2032|a, s)Rs\u2032s,a. The Bayesian model \u00b5 can be subject to model misspecification (e.g. by having a wrong likelihood or a bad prior) and thus the agent might want to allow deviations from its model towards best-case (optimistic agent) or worst-case (pessimistic agent) scenarios up to a certain extent, in order to act more robustly or to enhance its performance in a friendly environment [16]. Such deviations can be measured by the relative entropy DKL(\u03c8|\u00b5) between the Bayesian posterior \u00b5 and a new biased model \u03c8. Effectively, this allows for mathematically formalizing model uncertainty, by not only considering the specified model but all models within a neighborhood of the specified model that deviate no more than a restricted number of bits. Then, the effective expected value of an action a while having limited trust in the Bayesian posterior \u00b5 can be determined for the case of optimistic deviations as\nF \u2217(a, s) = max \u03c8 \u222b \u03b8 \u03c8(\u03b8|a, s) \u2211 s\u2032 T\u03b8(s \u2032|a, s)Rs \u2032 s,a \u2212 1 \u03b2 DKL(\u03c8||\u00b5) (4)\nfor \u03b2 > 0, and for the case of pessimistic deviations as\nF \u2217(a, s) = min \u03c8 \u222b \u03b8 \u03c8(\u03b8|a, s) \u2211 s\u2032 T\u03b8(s \u2032|a, s)Rs \u2032 s,a \u2212 1 \u03b2 DKL(\u03c8||\u00b5) (5)\nfor \u03b2 < 0. Conveniently, both equations can be expressed as a single equation\nF \u2217(a, s) = 1\n\u03b2 logZ\u03b2(a, s)\nwith \u03b2 \u2208 R and Z\u03b2(s, a) = \u222b \u03b8 \u00b5(\u03b8|a, s) exp ( \u03b2 \u2211 s\u2032 T\u03b8(s \u2032|a, s)Rs\u2032s,a )\nwhen inserting the optimal biased belief\n\u03c8\u2217(\u03b8|a, s) = 1 Z\u03b2(a, s) \u00b5(\u03b8|a, s) exp ( \u03b2 \u2211 s\u2032 T\u03b8(s \u2032|a, s)Rs \u2032 s,a )\ninto either equation (4) or (5). By adopting this formulation we can model any degree of trust in the belief \u00b5 allowing deviation towards worst-case or best-case with \u2212\u221e \u2264 \u03b2 \u2264 \u221e. For the case of \u03b2 \u2192 \u2212\u221e we recover an infinitely pessimistic agent that considers only worst-case scenarios, for \u03b2 \u2192 \u221e an agent that is infinitely optimistic and for \u03b2 \u2192 0 the Bayesian agent that fully trusts its model."}, {"heading": "3 Model Uncertainty and Bounded Rationality in MDPs", "text": "In this section, we consider a bounded rational agent with model uncertainty in the infinite horizon setting of an MDP. In this case the agent must take into account all future rewards and information costs, thereby optimizing the following free energy objective\nF \u2217(s) = max \u03c0 ext \u03c8 lim T\u2192\u221e E T\u22121\u2211 t=0 \u03b3t ( Rst+1st,at\u2212 1 \u03b2 log \u03c8(\u03b8t|at, st) \u00b5(\u03b8t|at, st) \u2212 1 \u03b1 log \u03c0(at|st) \u03c1(at|st) ) (6)\nwhere the extremum operator ext can be either max for \u03b2 > 0 or min for \u03b2 < 0, 0 < \u03b3 < 1 is the discount factor and the expectation E is over all trajectories \u03be = s0, a0, \u03b80, s1, a1, . . . aT\u22121, \u03b8T\u22121, sT with distribution p(\u03be) = \u220fT\u22121 t=0 \u03c0(at|st)\u03c8(\u03b8t|at, st) T\u03b8t(st+1|at, st). Importantly, this free energy objective satisfies a recursive relation and thereby generalizes Bellman\u2019s optimality principle to the case of model uncertainty and bounded rationality. In particular, equation (6) fulfills the recursion\nF \u2217(s) = max \u03c0 ext \u03c8 E\u03c0(a|s) [ \u2212 1 \u03b1 log \u03c0(a|s) \u03c1(a|s) +\nE\u03c8(\u03b8|a,s) [ \u2212 1 \u03b2 log \u03c8(\u03b8|a, s) \u00b5(\u03b8|a, s) +\nET\u03b8(s\u2032|a,s) [ Rs \u2032 s,a + \u03b3F \u2217(s\u2032) ]]] . (7)\nApplying variational calculus and following the same rationale as in the previous sections [6], the extremum operators can be eliminated and equation (7) can be reexpressed as\nF \u2217(s) = 1\n\u03b1 logE\u03c1(a|s)\n[ E\u00b5(\u03b8|a,s) [ exp ( \u03b2ET\u03b8(s\u2032|a,s) [ Rs \u2032 s,a + \u03b3F \u2217(s\u2032) ])]\u03b1 \u03b2 ] (8)\nbecause\nF \u2217(s) = max \u03c0\nE\u03c0(a|s) [ 1\n\u03b2 logZ\u03b2(a, s)\u2212\n1 \u03b1 log \u03c0(a|s) \u03c1(a|s)\n] (9)\n= 1\n\u03b1 logE\u03c1(a|s)\n[ exp ( \u03b1\n\u03b2 logZ\u03b2(a, s)\n)] , (10)\nwhere\nZ\u03b2(a, s) = ext \u03c8\nE\u03c8(\u03b8|a,s) [ ET\u03b8(s\u2032|a,s) [ Rs \u2032 s,a + \u03b3F \u2217(s\u2032) ] \u2212 1 \u03b2 log \u03c8(\u03b8|a, s) \u00b5(\u03b8|a, s) ] (11)\n= E\u00b5(\u03b8|a,s) exp ( \u03b2ET\u03b8(s\u2032|a,s) [ Rs \u2032 s,a + \u03b3F \u2217(s\u2032) ]) with the optimizing arguments\n\u03c8\u2217(\u03b8|a, s) = 1 Z\u03b2(a, s)\n\u00b5(\u03b8|a, s) exp ( \u03b2ET\u03b8(s\u2032|a,s) [ Rs \u2032 s,a + \u03b3F (s \u2032) ])\n\u03c0\u2217(a|s) = 1 Z\u03b1(s)\n\u03c1(a|s) exp ( \u03b1\n\u03b2 logZ\u03b2(a, s)\n) (12)\nand partition sum\nZ\u03b1(s) = E\u03c1(a|s) [ exp ( \u03b1\n\u03b2 logZ\u03b2(a, s)\n)] .\nWith this free energy we can model a range of different agents for different \u03b1 and \u03b2. For example, by setting \u03b1\u2192\u221e and \u03b2 \u2192 0 we can recover a Bayesian MDP planner and by setting \u03b1 \u2192 \u221e and \u03b2 \u2192 \u2212\u221e we recover a robust planner. Additionally, for \u03b1\u2192\u221e and when \u00b5(\u03b8|a, s) = \u03b4(\u03b8\u2212 \u03b8\u2217(a, s)) we recover an agent with standard value function with known state transition model from equation (1)."}, {"heading": "3.1 Free Energy Iteration Algorithm", "text": "Solving the self-consistency equation (8) can be achieved by a generalized version of value iteration. Accordingly, the optimal solution can be obtained by initializing the free energy at some arbitrary value F and applying a value iteration scheme Bi+1F = BBiF where we define the operator\nBF (s) = max \u03c0 ext \u03c8 E\u03c0(a|s) [ \u2212 1 \u03b1 log \u03c0(a|s) \u03c1(a|s) +\nE\u03c8(\u03b8|a,s) [ \u2212 1 \u03b2 log \u03c8(\u03b8|a, s) \u00b5(\u03b8|a, s) +\nET\u03b8(s\u2032|a,s) [ Rs \u2032 s,a + \u03b3F (s \u2032) ]]] (13)\nwith B1F = BF , which can be simplified to\nBF (s) = 1\n\u03b1 logE\u03c1(a|s)\n[ E\u00b5(\u03b8|a,s) [ exp ( \u03b2ET\u03b8(s\u2032|a,s) [ Rs \u2032 s,a + \u03b3F (s \u2032) ])]\u03b1 \u03b2 ] In Algorithm (1) we show the pseudo-code of this generalized value iteration scheme. Given state-dependent prior policies \u03c1(a|s) and the Bayesian posterior beliefs \u00b5(\u03b8|a, s) and the values of \u03b1 and \u03b2, the algorithm outputs the equilibrium distributions for the action probabilities \u03c0(a|s), the biased beliefs \u03c8(\u03b8|a, s) and estimates of the free energy value function F \u2217(s). The iteration is run until a convergence criterion is met. The convergence proof is shown in the next section.\nAlgorithm 1: Iterative algorithm solving the self-consistency equation (8) Input: \u03c1(a|s), \u00b5(\u03b8|a, s), \u03b1, \u03b2 Initialize: F \u2190 0, Fold \u2190 0 while not converged do\nforall the s \u2208 S do\nF (s)\u2190 1 \u03b1 logE\u03c1(a|s) [ E\u00b5(\u03b8|a,s) [ exp ( \u03b2ET\u03b8(s\u2032|a,s) [ Rs \u2032 s,a + \u03b3Fold(s \u2032) ])]\u03b1 \u03b2 ] end Fold \u2190 F\nend\n\u03c0(a|s)\u2190 1 Z\u03b1(s) \u03c1(a|s) exp ( \u03b1 \u03b2 logZ\u03b2(a, s) ) \u03c8(\u03b8|a, s)\u2190 1\nZ\u03b2(a,s) \u00b5(\u03b8|a, s) exp ( \u03b2ET\u03b8(s\u2032|a,s) [ Rs \u2032 s,a + \u03b3F (s \u2032) ])\nreturn \u03c0(a|s), \u03c8(\u03b8|a, s), F (s)"}, {"heading": "4 Convergence", "text": "Here, we show that the value iteration scheme described through Algorithm 1 converges to a unique fixed point satisfying Equation (8). To this end, we first prove the existence of a unique fixed point (Theorem 1) following [17,18], and subsequently prove the convergence of the value iteration scheme presupposing that a unique fixed point exists (Theorem 2) following [19].\nTheorem 1. Assuming a bounded reward functionRs \u2032\ns,a, the optimal free-energy vector F \u2217(s) is a unique fixed point of Bellman\u2019s equation F \u2217 = BF \u2217, where the mapping B : R|S| \u2192 R|S| is defined as in equation (13)\nProof. Theorem 1 is proven through Proposition 1 and 2 in the following.\nProposition 1. The mapping T\u03c0,\u03c8 : R|S| \u2192 R|S|\nT\u03c0,\u03c8F (s) = E\u03c0(a|s) [ \u2212 1 \u03b1 log \u03c0(a|s) \u03c1(a|s) +\nE\u03c8(\u03b8|a,s) [ \u2212 1 \u03b2 log \u03c8(\u03b8|a, s) \u00b5(\u03b8|a, s) +\nET\u03b8(s\u2032|a,s) [ Rs \u2032 s,a + \u03b3F (s \u2032) ]]] . (14)\nconverges to a unique solution for every policy-belief-pair (\u03c0, \u03c8) independent of the initial free-energy vector F (s).\nProof. By introducing the matrix P\u03c0,\u03c8(s, s\u2032) and the vector g\u03c0,\u03c8(s) as\nP\u03c0,\u03c8(s, s \u2032) := E\u03c0(a|s) [ E\u03c8(\u03b8|a,s) [T\u03b8(s\u2032|a, s)] ] ,\ng\u03c0,\u03c8(s) := E\u03c0(a|s) [ E\u03c8(\u03b8|a,s) [ ET\u03b8(s\u2032|a,s) [ Rs \u2032 s,a ] \u2212 1 \u03b2 log \u03c8(\u03b8|a, s) \u00b5(\u03b8|a, s) ] \u2212 1 \u03b1 log \u03c0(a|s) \u03c1(a|s) ] ,\nEquation (14) may be expressed in compact form: T\u03c0,\u03c8F = g\u03c0,\u03c8 + \u03b3P\u03c0,\u03c8F . By applying the mapping T\u03c0,\u03c8 an infinite number of times on an initial free-energy vector F , the free-energy vector F\u03c0,\u03c8 of the policy-belief-pair (\u03c0, \u03c8) is obtained:\nF\u03c0,\u03c8 := lim i\u2192\u221e T i\u03c0,\u03c8F = lim i\u2192\u221e i\u22121\u2211 t=0 \u03b3tP t\u03c0,\u03c8g\u03c0,\u03c8 + lim i\u2192\u221e\n\u03b3iP i\u03c0,\u03c8F\ufe38 \ufe37\ufe37 \ufe38 \u21920 ,\nwhich does no longer depend on the initial F . It is straightforward to show that the quantity F\u03c0,\u03c8 is a fixed point of the operator T\u03c0,\u03c8:\nT\u03c0,\u03c8F\u03c0,\u03c8 = g\u03c0,\u03c8 + \u03b3P\u03c0,\u03c8 lim i\u2192\u221e i\u22121\u2211 t=0 \u03b3tP t\u03c0,\u03c8g\u03c0,\u03c8\n= \u03b30P 0\u03c0,\u03c8g\u03c0,\u03c8 + lim i\u2192\u221e i\u2211 t=1 \u03b3tP t\u03c0,\u03c8g\u03c0,\u03c8\n= lim i\u2192\u221e i\u22121\u2211 t=0 \u03b3tP t\u03c0,\u03c8g\u03c0,\u03c8 + lim i\u2192\u221e\n\u03b3iP i\u03c0,\u03c8g\u03c0,\u03c8\ufe38 \ufe37\ufe37 \ufe38 \u21920 = F\u03c0,\u03c8.\nFurthermore, F\u03c0,\u03c8 is unique. Assume for this purpose an arbitrary fixed point F \u2032 such that T\u03c0,\u03c8F \u2032 = F \u2032, then F \u2032 = limi\u2192\u221e T i\u03c0,\u03c8F \u2032 = F\u03c0,\u03c8.\nProposition 2. The optimal free-energy vector F \u2217 = max\u03c0 ext\u03c8 F\u03c0,\u03c8 is a unique fixed point of Bellman\u2019s equation F \u2217 = BF \u2217.\nProof. The proof consists of two parts where we assume ext = max in the first part and ext = min in the second part respectively. Let ext = max and F \u2217 = F\u03c0\u2217,\u03c8\u2217 , where (\u03c0\u2217, \u03c8\u2217) denotes the optimal policy-belief-pair. Then\nF \u2217 = T\u03c0\u2217,\u03c8\u2217F \u2217 \u2264 max\n\u03c0 max \u03c8 T\u03c0,\u03c8F \u2217\ufe38 \ufe37\ufe37 \ufe38\n=BF\u2217\n=: T\u03c0\u2032,\u03c8\u2032F \u2217 Induction\u2264 F\u03c0\u2032,\u03c8\u2032 ,\nwhere the last inequality can be straightforwardly proven by induction and exploiting the fact that P\u03c0,\u03c8(s, s\u2032) \u2208 [0; 1]. But by definition F \u2217 = max\u03c0max\u03c8 F\u03c0,\u03c8 \u2265 F\u03c0\u2032,\u03c8\u2032 , hence F \u2217 = F\u03c0\u2032,\u03c8\u2032 and therefore F \u2217 = BF \u2217. Furthermore, F \u2217 is unique. Assume for this purpose an arbitrary fixed point F \u2032 = F\u03c0\u2032,\u03c8\u2032 such that F \u2032 = BF \u2032 with the corresponding policy-belief-pair (\u03c0\u2032, \u03c8\u2032). Then\nF \u2217 = T\u03c0\u2217,\u03c8\u2217F \u2217 \u2265 T\u03c0\u2032,\u03c8\u2032F \u2217 Induction \u2265 F\u03c0\u2032,\u03c8\u2032 = F \u2032,\nand similarly F \u2032 \u2265 F \u2217, hence F \u2032 = F \u2217. Let ext = min and F \u2217 = F\u03c0\u2217,\u03c8\u2217 . By taking a closer look at Equation (13), it can be seen that the optimization over \u03c8 does not depend on \u03c0. Then\nF \u2217 = T\u03c0\u2217,\u03c8\u2217F \u2217 \u2265 min\n\u03c8 T\u03c0\u2217,\u03c8F\n\u2217 =: T\u03c0\u2217,\u03c8\u2032F \u2217 Induction\u2265 F\u03c0\u2217,\u03c8\u2032 .\nBut by definition F \u2217 = min\u03c8 F\u03c0\u2217,\u03c8 \u2264 F\u03c0\u2217,\u03c8\u2032 , hence F \u2217 = F\u03c0\u2217,\u03c8\u2032 . Therefore it holds that BF \u2217 = max\u03c0min\u03c8 T\u03c0,\u03c8F \u2217 = max\u03c0 T\u03c0,\u03c8\u2217F \u2217 and similar to the first part of the proof we obtain\nF \u2217 = T\u03c0\u2217,\u03c8\u2217F \u2217 \u2264 max\n\u03c0 T\u03c0,\u03c8\u2217F \u2217\ufe38 \ufe37\ufe37 \ufe38 =BF\u2217\n=: T\u03c0\u2032,\u03c8\u2217F \u2217 Induction\u2264 F\u03c0\u2032,\u03c8\u2217.\nBut by definition F \u2217 = max\u03c0 F\u03c0,\u03c8\u2217 \u2265 F\u03c0\u2032,\u03c8\u2217, hence F \u2217 = F\u03c0\u2032,\u03c8\u2217 and therefore F \u2217 = BF \u2217. Furthermore, F\u03c0\u2217,\u03c8\u2217 is unique. Assume for this purpose an arbitrary fixed point F \u2032 = F\u03c0\u2032,\u03c8\u2032 such that F \u2032 = BF \u2032. Then\nF \u2032 = T\u03c0\u2032,\u03c8\u2032F \u2032 \u2264 T\u03c0\u2032,\u03c8\u2217F \u2032 Induction \u2264 F\u03c0\u2032,\u03c8\u2217 Induction \u2264 T\u03c0\u2032,\u03c8\u2217F \u2217 \u2264 T\u03c0\u2217,\u03c8\u2217F \u2217 = F \u2217,\nand similarly F \u2217 \u2264 F \u2032, hence F \u2217 = F \u2032.\nTheorem 2. Let be a positive number satisfying < \u03b71\u2212\u03b3 where \u03b3 \u2208 (0; 1) is the discount factor and where u and l are the bounds of the reward function Rs \u2032\ns,a such that l \u2264 Rs\u2032s,a \u2264 u and \u03b7 = max{|u|, |l|}. Suppose that the value iteration scheme from Algorithm 1 is run for i = dlog\u03b3 (1\u2212\u03b3) \u03b7 e iterations with an initial free-energy vector F (s) = 0 for all s. Then, it holds that maxs |F \u2217(s)\u2212BiF (s)| \u2264 , where F \u2217 refers to the unique fixed point from Theorem 1.\nProof. We start the proof by showing that the L\u221e-norm of the difference vector between the optimal free-energy F \u2217 andBiF exponentially decreases with the number of iterations i:\nmax s \u2223\u2223F \u2217(s)\u2212BiF (s)\u2223\u2223 =: \u2223\u2223F \u2217(s\u2217)\u2212BiF (s\u2217)\u2223\u2223 Eq. (9) = \u2223\u2223\u2223\u2223max\u03c0 E\u03c0(a|s\u2217) [ 1 \u03b2 logZ\u03b2(a, s \u2217)\u2212 1 \u03b1 log \u03c0(a|s\u2217) \u03c1(a|s\u2217)\n] \u2212max\n\u03c0 E\u03c0(a|s\u2217)\n[ 1\n\u03b2 logZi\u03b2(a, s \u2217)\u2212 1 \u03b1 log \u03c0(a|s\u2217) \u03c1(a|s\u2217) ]\u2223\u2223\u2223\u2223 \u2264 max\n\u03c0 \u2223\u2223\u2223\u2223E\u03c0(a|s\u2217)[ 1\u03b2 logZ\u03b2(a, s\u2217)\u2212 1\u03b2 logZi\u03b2(a, s\u2217) ]\u2223\u2223\u2223\u2223\n\u2264 max a \u2223\u2223\u2223\u2223 1\u03b2 logZ\u03b2(a, s\u2217)\u2212 1\u03b2 logZi\u03b2(a, s\u2217) \u2223\u2223\u2223\u2223\n=: \u2223\u2223\u2223\u2223 1\u03b2 logZ\u03b2(a\u2217, s\u2217)\u2212 1\u03b2 logZi\u03b2(a\u2217, s\u2217) \u2223\u2223\u2223\u2223\nEq. (11) = \u2223\u2223\u2223\u2223ext\u03c8 E\u03c8(\u03b8|a\u2217,s\u2217) [ ET\u03b8(s\u2032|a\u2217,s\u2217) [ Rs \u2032 s,a + \u03b3F \u2217(s\u2032) ] \u2212 1 \u03b2 log \u03c8(\u03b8|a\u2217, s\u2217) \u00b5(\u03b8|a\u2217, s\u2217) ] \u2212 ext\n\u03c8 E\u03c8(\u03b8|a\u2217,s\u2217)\n[ ET\u03b8(s\u2032|a\u2217,s\u2217) [ Rs \u2032 s,a + \u03b3B i\u22121F (s\u2032) ] \u2212 1 \u03b2 log \u03c8(\u03b8|a\u2217, s\u2217) \u00b5(\u03b8|a\u2217, s\u2217) ]\u2223\u2223\u2223\u2223 \u2264 max\n\u03c8 \u2223\u2223\u2223\u2223E\u03c8(\u03b8|a\u2217,s\u2217)[ET\u03b8(s\u2032|a\u2217,s\u2217)[\u03b3F \u2217(s\u2032)\u2212 \u03b3Bi\u22121F (s\u2032)]]\u2223\u2223\u2223\u2223 \u2264 \u03b3max\ns \u2223\u2223F \u2217(s)\u2212Bi\u22121F (s)\u2223\u2223 Recur.\u2264 \u03b3imax s |F \u2217(s)\u2212 F (s)| \u2264 \u03b3i \u03b7 1\u2212 \u03b3 ,\nwhere we exploit the fact that |extx f(x)\u2212 extx g(x)| \u2264 maxx |f(x)\u2212 g(x)| and that the free-energy is bounded through the reward bounds l and u with \u03b7 = max{|u|, |l|}. For a convergence criterion > 0 such that \u2265 \u03b3i \u03b71\u2212\u03b3 , it then holds that i \u2265 log\u03b3 (1\u2212\u03b3) \u03b7 presupposing that < \u03b7 1\u2212\u03b3 ."}, {"heading": "5 Experiments: Grid World", "text": "This section illustrates the proposed value iteration scheme with an intuitive example where an agent has to navigate through a grid-world. The agent starts at position S \u2208 S with the objective to reach the goal state G \u2208 S and can choose one out of maximally four possible actions a \u2208 {\u2191,\u2192, \u2193,\u2190} in each time-step. Along the way, the agent can encounter regular tiles (actions move the agent deterministically one step in the desired direction), walls that are represented as gray tiles (actions that move the agent towards the wall are not possible), holes that are represented as black tiles (moving into the hole causes a negative reward) and chance tiles that are illustrated as white tiles with a question mark (the transition probabilities of the chance tiles are unknown to the agent). Reaching the goal G yields a reward R = +1 whereas stepping into a hole results in a negative reward R = \u22121. In both cases the agent is subsequently teleported back\nto the starting position S. Transitions to regular tiles have a small negative reward of R = \u22120.01. When stepping onto a chance tile, the agent is pushed stochastically to an adjacent tile giving a reward as mentioned above. The true state-transition probabilities of the chance tiles are not known by the agent, but the agent holds the Bayesian belief\n\u00b5(\u03b8s,a|a, s) = Dirichlet ( \u03a6 s\u20321 s,a, . . . , \u03a6 s\u2032N(s) s,a ) = N(s)\u220f i=1 (\u03b8 s\u2032i s,a) \u03a6 s\u2032i s,a\u22121\nwhere transition model is denoted as T\u03b8s,a(s \u2032|s, a) = \u03b8s\u2032s,a and \u03b8s,a = ( \u03b8 s\u20321 s,a . . . \u03b8 s\u2032N(s) s,a ) and N(s) is the number of possible actions in state s. The data is incorporated into the\nmodel as a count vector ( \u03a6 s\u20321 s,a, . . . , \u03a6 s\u2032N(s) s,a ) where \u03a6s \u2032\ns,a represents the number of times that the transition (s, a, s\u2032) has occurred. The prior \u03c1(a|s) for the actions at every state is set to be uniform. An important aspect of the model is that in the case of unlimited observational data, the agent will plan with the correct transition probabilities.\nWe conducted two experiments with discount factor \u03b3 = 0.9 and uniform priors \u03c1(a|s) for the action variables. In the first experiment, we explore and illustrate the agent\u2019s planning behavior under different degrees of computational limitations (by varying \u03b1) and under different model uncertainty attitudes (by varying \u03b2) with fixed uniform beliefs \u00b5(\u03b8|a, s). In the second experiment, the agent is allowed to update its beliefs \u00b5(\u03b8|a, s) and use the updated model to re-plan its strategy."}, {"heading": "5.1 The Role of the Parameters \u03b1 and \u03b2 on Planning", "text": "Figure 1 shows the solution to the variational free energy problem that is obtained by iteration until convergence according to Algorithm 1 under different values of \u03b1 and \u03b2. In particular, the first row shows the free energy function F \u2217(s) (Eq. (8)). The second, third and fourth row show heat maps of the position of an agent that follows the optimal policy (Eq. (12)) according to the agent\u2019s biased beliefs (plan) and to the actual transition probabilities in a friendly and unfriendly environment, respectively. In chance tiles, the most likely transitions in these two environments are indicated by arrows where the agent is teleported with a probability of 0.999 into the tile indicated by the arrow and with a probability of 0.001 to a random other adjacent tile.\nIn the first column of Fig. 1 it can be seen that a stochastic agent (\u03b1 = 3.0) with high model uncertainty and optimistic attitude (\u03b2 = 400) has a strong preference for the broad corridor in the bottom by assuming favorable transitions for the unknown chance tiles. This way the agent also avoids the narrow corridors that are unsafe due to the stochasticity of the low-\u03b1 policy. In the second column of Fig. 1 with low \u03b1 = 3 and high model uncertainty with pessimistic attitude \u03b2 = \u2212400, the agent strongly prefers the upper broad corridor because unfavorable transitions are assumed for the chance tiles. The third column of Fig. 1 shows a very pessimistic agent (\u03b2 = \u2212400) with high precision (\u03b1 = 11) that allow the agent to safely choose the shortest distance by selecting the upper narrow corridor without risking any tiles with unknown transitions. The fourth column of Fig. 1 shows a very optimistic agent (\u03b2 = 400) with high precision. In this case the agent chooses the shortest distance by selecting the bottom narrow corridor that includes two chance tiles with unknown transition."}, {"heading": "5.2 Updating the Bayesian Posterior \u00b5 with Observations from the Environment", "text": "Similar to model identification adaptive controllers that perform system identification while the system is running [20], we can use the proposed planning algorithm also in a reinforcement learning setup by updating the Bayesian beliefs about the MDP while executing always the first action and replanning in the next time step. During the learning phase, the exploration is governed by both factors \u03b1 and \u03b2, but each factor has a different influence. In particular, lower \u03b1-values will cause more exploration due to the inherent stochasticity in the agent\u2019s action selection, similar to an -greedy policy. If \u03b1 is kept fixed through time, this will of course also imply a \u201csuboptimal\u201d (i.e. bounded optimal) policy in the long run. In contrast, the parameter \u03b2 governs exploration of states with unknown transition-probabilities more directly and will not have an impact on the agent\u2019s performance in the limit, where sufficient data has eliminated model uncertainty. We illustrate this with simulations in a grid-world environment where the agent is allowed to update its beliefs \u00b5(\u03b8|a, s) over the state-transitions every time it enters a chance tile and receives observation data acquired through interaction with the environment\u2014compare left panels in Figure 2. In each step, the agent can then use the updated belief-models for planning the next action.\nFigure 2 (right panels) shows the number of data points acquired (each time a chance tile is visited) and the average reward depending on the number of steps that the agent has interacted with the environment. The panels show several different cases: while keeping \u03b1 = 12.0 fixed we test \u03b2 = (0.2, 5.0, 20.0) and while keeping \u03b2 = 0.2 fixed we test \u03b1 = (5.0, 8.0, 12.0). It can be seen that lower \u03b1 leads to better exploration, but it can also lead to lower performance in the long run\u2014see for example rightmost bottom panel. In contrast, optimistic \u03b2 values can also induce high levels of exploration with the added advantage that in the limit no performance detriment is introduced. However, high \u03b2 values can in general also lead to a detrimental persistence with bad policies, as can be seen for example in the superiority of the low-\u03b2 agent at the very beginning of the learning process."}, {"heading": "6 Discussion and Conclusions", "text": "In this paper we are bringing two strands of research together, namely research on information-theoretic principles of control and decision-making and robustness principles for planning under model uncertainty. We have devised a unified recursion principle that extends previous generalizations of Bellman\u2019s optimality equation and we have shown how to solve this recursion with an iterative scheme that is guaranteed to converge to a unique optimum. In simulations we could demonstrate how such a combination of information-theoretic policy and belief constraints that reflect model uncertainty can be beneficial for agents that act in partially unknown environments.\nMost of the research on robust MDPs does not consider information-processing constraints on the policy, but only considers the uncertainty in the transition probabilities by specifying a set of permissible models such that worst-case scenarios can be computed in order to obtain a robust policy [11,12]. Recent extensions of these approaches include more general assumptions regarding the set properties of the permissible models and assumptions regarding the data generation process [13]. Our approach falls inside this class of robustness methods that use a restricted set of permissible models, because we extremize the biased belief \u03c8(\u03b8|a, s) under the constraint that it has to be within some information bounds measured by the Kullback-Leibler divergence from a reference Bayesian posterior. Contrary to these previous methods, our approach additionally considers robustness arising from the stochasticity in the policy.\nInformation-processing constraints on the policy in MDPs have been previously considered in a number of studies [3,21,22,17], however not in the context of model uncertainty. In these studies a free energy value recursion is derived when restricting the class of policies through the Kullback-Leibler divergence and when disregarding separate information-processing constraints on observations. However, a small number of studies has considered information-processing constraints both for actions and observations. For example, Polani and Tishby [8] and Ortega and Braun [6] combine both kinds of information costs. The first cost formalizes an information-processing cost in the policy and the second cost constrains uncertainty arising from the state transitions directly (but crucially not the uncertainty in the latent variables). In both informationprocessing constraints the cost is determined as a Kullback-Leibler divergence with respect to a reference distribution. Specifically, the reference distribution in [8] is given\nby the marginal distributions (which is equivalent to a rate distortion problem) and in [6] is given by fixed priors. The Kullback-Leibler divergence costs for the observations in these cases essentially correspond to a risk-sensitive objective. While there is a relation between risk-sensitive and robust MDPs [23,24,25], the innovation in our approach is at least twofold. First, it allows combining information-processing constraints on the policy with model uncertainty (as formalized by a latent variable). Second, it provides a natural setup to study learning.\nThe algorithm presented here and Bayesian models in general [9] are computationally expensive as they have to compute possibly high-dimensional integrals depending on the number of allowed transitions for action-state pairs. However, there have been tremendous efforts in solving unknown MDPs efficiently, especially by sampling methods [26,27,28]. An interesting future direction to extend our methodology would therefore be to develop a sampling-based version of Algorithm 1 to increase the range of applicability and scalability [29]. Moreover, such sampling methods might allow for reinforcement learning applications, for example by estimating free energies through TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic methods for adaptive control [20].\nAcknowledgments This study was supported by the DFG, Emmy Noether grant BR4164/11. The code was developed on top of the RLPy library [33]."}], "references": [{"title": "Dynamic Programming", "author": ["Richard Bellman"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1957}, {"title": "Linearly-solvable markov decision problems", "author": ["Emanuel Todorov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Efficient computation of optimal actions", "author": ["Emanuel Todorov"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Path integral control and bounded rationality", "author": ["Daniel A Braun", "Pedro A Ortega", "Evangelos Theodorou", "Stefan Schaal"], "venue": "In Adaptive Dynamic Programming And Reinforcement Learning (ADPRL),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Risk sensitive path integral control", "author": ["Bart van den Broek", "Wim Wiegerinck", "Hilbert J. Kappen"], "venue": "In UAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Thermodynamics as a theory of decision-making with information-processing costs", "author": ["Pedro A Ortega", "Daniel A Braun"], "venue": "In Proc. R. Soc. A,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Generalized thompson sampling for sequential decision-making and causal inference", "author": ["Pedro A Ortega", "Daniel A Braun"], "venue": "Complex Adaptive Systems Modeling,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Information theory of decisions and actions", "author": ["Naftali Tishby", "Daniel Polani"], "venue": "In Perceptionaction cycle,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes", "author": ["Michael O\u2019Gordon Duff"], "venue": "PhD thesis, University of Massachusetts Amherst,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Bias and variance approximation in value function estimates", "author": ["Shie Mannor", "Duncan Simester", "Peng Sun", "John N Tsitsiklis"], "venue": "Management Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Robust control of markov decision processes with uncertain transition matrices", "author": ["Arnab Nilim", "Laurent El Ghaoui"], "venue": "Operations Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Robust dynamic programming", "author": ["Garud N Iyengar"], "venue": "Mathematics of Operations Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Robust markov decision processes", "author": ["Wolfram Wiesemann", "Daniel Kuhn", "Ber\u00e7 Rustem"], "venue": "Mathematics of Operations Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "The many faces of optimism: a unifying approach", "author": ["Istv\u00e1n Szita", "Andr\u00e1s L\u0151rincz"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["Istv\u00e1n Szita", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Trading value and information in mdps. In Decision Making with Imperfect Decision Makers", "author": ["Jonathan Rubin", "Ohad Shamir", "Naftali Tishby"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "and JN Tsitsiklis", "author": ["DP Bertseka"], "venue": "Neuro-dynamic programming.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Reinforcement learning in finite mdps: Pac analysis", "author": ["Alexander L Strehl", "Lihong Li", "Michael L Littman"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Adaptive control", "author": ["Karl J \u00c5str\u00f6m", "Bj\u00f6rn Wittenmark"], "venue": "Courier Corporation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Linear theory for control of nonlinear stochastic systems", "author": ["Hilbert J Kappen"], "venue": "Physical review letters,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Fox D Poole, et al", "author": ["J Peters", "K M\u00fclling", "Y Altun"], "venue": "Relative entropy policy search. In TwentyFourth National Conference on Artificial Intelligence (AAAI-10), pages 1607\u20131612. AAAI Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Risk-sensitive reinforcement learning", "author": ["Yun Shen", "Michael J Tobia", "Tobias Sommer", "Klaus Obermayer"], "venue": "Neural computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Robustness and risk-sensitivity in markov decision processes", "author": ["Takayuki Osogami"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Risk-sensitive and robust decision-making: a cvar optimization approach", "author": ["Yinlam Chow", "Aviv Tamar", "Shie Mannor", "Marco Pavone"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "A bayesian approach for learning and planning in partially observable markov decision processes", "author": ["St\u00e9phane Ross", "Joelle Pineau", "Brahim Chaib-draa", "Pierre Kreitmann"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Efficient bayes-adaptive reinforcement learning using sample-based search", "author": ["Arthur Guez", "David Silver", "Peter Dayan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Scalable and efficient bayes-adaptive reinforcement learning based on monte-carlo tree search", "author": ["Arthur Guez", "David Silver", "Peter Dayan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Monte carlo methods for exact & efficient solution of the generalized optimality equations", "author": ["Pedro A Ortega", "Daniel A Braun", "Naftali Tishby"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "G-learning: Taming the noise in reinforcement learning via soft updates", "author": ["Roy Fox", "Ari Pakman", "Naftali Tishby"], "venue": "arXiv preprint arXiv:1512.08562,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "A minimum relative entropy principle for learning and acting", "author": ["Pedro A Ortega", "Daniel A Braun"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "A bayesian rule for adaptive control based on causal interventions", "author": ["Pedro A Ortega", "Daniel A Braun"], "venue": "In 3d Conference on Artificial General Intelligence", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Rlpy: A value-function-based reinforcement learning framework for education and research", "author": ["Alborz Geramifard", "Christoph Dann", "Robert H Klein", "William Dabney", "Jonathan P How"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The problem of planning in Markov Decision Processes was famously addressed by Bellman who developed the eponymous principle in 1957 [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "Todorov [2,3], for example, has transformed the general MDP problem into a restricted problem class without explicit action variables, where control directly changes the dynamics of the environment and control costs are measured by the Kullback-Leibler divergence between controlled and uncontrolled dynamics.", "startOffset": 8, "endOffset": 13}, {"referenceID": 2, "context": "Todorov [2,3], for example, has transformed the general MDP problem into a restricted problem class without explicit action variables, where control directly changes the dynamics of the environment and control costs are measured by the Kullback-Leibler divergence between controlled and uncontrolled dynamics.", "startOffset": 8, "endOffset": 13}, {"referenceID": 3, "context": "This approach can also be be generalized to continuous state spaces leading to path integral control [4,5].", "startOffset": 101, "endOffset": 106}, {"referenceID": 4, "context": "This approach can also be be generalized to continuous state spaces leading to path integral control [4,5].", "startOffset": 101, "endOffset": 106}, {"referenceID": 5, "context": "(measured by the Kullback-Leiber divergence in bits) [6].", "startOffset": 53, "endOffset": 56}, {"referenceID": 6, "context": "Such a decision-maker can also be instantiated by a sampling process that has restrictions in the number of samples it can afford [7].", "startOffset": 130, "endOffset": 133}, {"referenceID": 7, "context": "Disregarding the possibility of a sampling-based interpretation, the Kullback-Leibler divergence introduces a control information cost that is interesting in its own right when formalizing the perception action cycle [8].", "startOffset": 217, "endOffset": 220}, {"referenceID": 8, "context": "In Bayes-Adaptive MDPs [9], for example, the uncertainty over the latent parameters of the MDP is explicitly represented, such that new information can be incorporated with Bayesian inference.", "startOffset": 23, "endOffset": 26}, {"referenceID": 9, "context": "However, Bayes-Adaptive MDPs are not robust with respect to model misspecification and have no performance guarantees when planning with wrong models [10].", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "Accordingly, there has been substantial interest in developing robust MDP planners [11,12,13].", "startOffset": 83, "endOffset": 93}, {"referenceID": 11, "context": "Accordingly, there has been substantial interest in developing robust MDP planners [11,12,13].", "startOffset": 83, "endOffset": 93}, {"referenceID": 12, "context": "Accordingly, there has been substantial interest in developing robust MDP planners [11,12,13].", "startOffset": 83, "endOffset": 93}, {"referenceID": 13, "context": "Conversely, the belief model can also be biased towards best-case scenarios as a measure to drive exploration\u2014also referred in the literature as optimism in face of uncertainty [14,15].", "startOffset": 177, "endOffset": 184}, {"referenceID": 14, "context": "Conversely, the belief model can also be biased towards best-case scenarios as a measure to drive exploration\u2014also referred in the literature as optimism in face of uncertainty [14,15].", "startOffset": 177, "endOffset": 184}, {"referenceID": 2, "context": "When comparing the literature on information-theoretic control and model uncertainty, it is interesting to see that some notions of model uncertainty follow exactly the same mathematical principles as the principles of relative entropy control [3].", "startOffset": 244, "endOffset": 247}, {"referenceID": 5, "context": "Applying variational calculus and following the same rationale as in the previous sections [6], the extremum operators can be eliminated and equation (7) can be reexpressed as", "startOffset": 91, "endOffset": 94}, {"referenceID": 15, "context": "To this end, we first prove the existence of a unique fixed point (Theorem 1) following [17,18], and subsequently prove the convergence of the value iteration scheme presupposing that a unique fixed point exists (Theorem 2) following [19].", "startOffset": 88, "endOffset": 95}, {"referenceID": 16, "context": "To this end, we first prove the existence of a unique fixed point (Theorem 1) following [17,18], and subsequently prove the convergence of the value iteration scheme presupposing that a unique fixed point exists (Theorem 2) following [19].", "startOffset": 88, "endOffset": 95}, {"referenceID": 17, "context": "To this end, we first prove the existence of a unique fixed point (Theorem 1) following [17,18], and subsequently prove the convergence of the value iteration scheme presupposing that a unique fixed point exists (Theorem 2) following [19].", "startOffset": 234, "endOffset": 238}, {"referenceID": 18, "context": "2 Updating the Bayesian Posterior \u03bc with Observations from the Environment Similar to model identification adaptive controllers that perform system identification while the system is running [20], we can use the proposed planning algorithm also in a reinforcement learning setup by updating the Bayesian beliefs about the MDP while executing always the first action and replanning in the next time step.", "startOffset": 191, "endOffset": 195}, {"referenceID": 10, "context": "Most of the research on robust MDPs does not consider information-processing constraints on the policy, but only considers the uncertainty in the transition probabilities by specifying a set of permissible models such that worst-case scenarios can be computed in order to obtain a robust policy [11,12].", "startOffset": 295, "endOffset": 302}, {"referenceID": 11, "context": "Most of the research on robust MDPs does not consider information-processing constraints on the policy, but only considers the uncertainty in the transition probabilities by specifying a set of permissible models such that worst-case scenarios can be computed in order to obtain a robust policy [11,12].", "startOffset": 295, "endOffset": 302}, {"referenceID": 12, "context": "Recent extensions of these approaches include more general assumptions regarding the set properties of the permissible models and assumptions regarding the data generation process [13].", "startOffset": 180, "endOffset": 184}, {"referenceID": 2, "context": "Information-processing constraints on the policy in MDPs have been previously considered in a number of studies [3,21,22,17], however not in the context of model uncertainty.", "startOffset": 112, "endOffset": 124}, {"referenceID": 19, "context": "Information-processing constraints on the policy in MDPs have been previously considered in a number of studies [3,21,22,17], however not in the context of model uncertainty.", "startOffset": 112, "endOffset": 124}, {"referenceID": 20, "context": "Information-processing constraints on the policy in MDPs have been previously considered in a number of studies [3,21,22,17], however not in the context of model uncertainty.", "startOffset": 112, "endOffset": 124}, {"referenceID": 15, "context": "Information-processing constraints on the policy in MDPs have been previously considered in a number of studies [3,21,22,17], however not in the context of model uncertainty.", "startOffset": 112, "endOffset": 124}, {"referenceID": 7, "context": "For example, Polani and Tishby [8] and Ortega and Braun [6] combine both kinds of information costs.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "For example, Polani and Tishby [8] and Ortega and Braun [6] combine both kinds of information costs.", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "Specifically, the reference distribution in [8] is given", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "by the marginal distributions (which is equivalent to a rate distortion problem) and in [6] is given by fixed priors.", "startOffset": 88, "endOffset": 91}, {"referenceID": 21, "context": "While there is a relation between risk-sensitive and robust MDPs [23,24,25], the innovation in our approach is at least twofold.", "startOffset": 65, "endOffset": 75}, {"referenceID": 22, "context": "While there is a relation between risk-sensitive and robust MDPs [23,24,25], the innovation in our approach is at least twofold.", "startOffset": 65, "endOffset": 75}, {"referenceID": 23, "context": "While there is a relation between risk-sensitive and robust MDPs [23,24,25], the innovation in our approach is at least twofold.", "startOffset": 65, "endOffset": 75}, {"referenceID": 8, "context": "The algorithm presented here and Bayesian models in general [9] are computationally expensive as they have to compute possibly high-dimensional integrals depending on the number of allowed transitions for action-state pairs.", "startOffset": 60, "endOffset": 63}, {"referenceID": 24, "context": "However, there have been tremendous efforts in solving unknown MDPs efficiently, especially by sampling methods [26,27,28].", "startOffset": 112, "endOffset": 122}, {"referenceID": 25, "context": "However, there have been tremendous efforts in solving unknown MDPs efficiently, especially by sampling methods [26,27,28].", "startOffset": 112, "endOffset": 122}, {"referenceID": 26, "context": "However, there have been tremendous efforts in solving unknown MDPs efficiently, especially by sampling methods [26,27,28].", "startOffset": 112, "endOffset": 122}, {"referenceID": 27, "context": "An interesting future direction to extend our methodology would therefore be to develop a sampling-based version of Algorithm 1 to increase the range of applicability and scalability [29].", "startOffset": 183, "endOffset": 187}, {"referenceID": 28, "context": "Moreover, such sampling methods might allow for reinforcement learning applications, for example by estimating free energies through TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic methods for adaptive control [20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 29, "context": "Moreover, such sampling methods might allow for reinforcement learning applications, for example by estimating free energies through TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic methods for adaptive control [20].", "startOffset": 186, "endOffset": 193}, {"referenceID": 30, "context": "Moreover, such sampling methods might allow for reinforcement learning applications, for example by estimating free energies through TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic methods for adaptive control [20].", "startOffset": 186, "endOffset": 193}, {"referenceID": 18, "context": "Moreover, such sampling methods might allow for reinforcement learning applications, for example by estimating free energies through TD-learning [30], or by Thompson sampling approaches [31,32] or other stochastic methods for adaptive control [20].", "startOffset": 243, "endOffset": 247}, {"referenceID": 31, "context": "The code was developed on top of the RLPy library [33].", "startOffset": 50, "endOffset": 54}], "year": 2016, "abstractText": "Information-theoretic principles for learning and acting have been proposed to solve particular classes of Markov Decision Problems. Mathematically, such approaches are governed by a variational free energy principle and allow solving MDP planning problems with information-processing constraints expressed in terms of a Kullback-Leibler divergence with respect to a reference distribution. Here we consider a generalization of such MDP planners by taking model uncertainty into account. As model uncertainty can also be formalized as an information-processing constraint, we can derive a unified solution from a single generalized variational principle. We provide a generalized value iteration scheme together with a convergence proof. As limit cases, this generalized scheme includes standard value iteration with a known model, Bayesian MDP planning, and robust planning. We demonstrate the benefits of this approach in a grid world simulation.", "creator": "LaTeX with hyperref package"}}}