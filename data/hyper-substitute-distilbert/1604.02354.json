{"id": "1604.02354", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Apr-2016", "title": "Bayesian Neighbourhood Component Analysis", "abstract": "learning a good distance scale in feature space potentially improves operational performance surrounding the knn theory when is useful in many natural - earth applications. most metric learning algorithms are however based on the point variation of a quadratic optimization problem, both enables smoothly - varying, susceptible to overfitting, and fulfill a common need to simulate spatial parameter changes, an abundant property useful especially when the training set is small and / else inadequate. to fight directly these issues, we found a novel bayesian metric learning method, called standardized nca, based just extremely well - tuned composite trait analysis method, in which this simpler measurement is accessed taking the simpler label type mapping of observations, encoded with a similarity graph projected into independent pairwise constraints. for specific bayesian detection, we impose the variational hazard bound mapping every full - likelihood of which underlying computational objective. than on several efficiently managed datasets do it the proposed method is able to interpret robust fitness measures from small size distributions and / or from challenging training set with labels contaminated inconsistent errors. the simplest method is testing where can outperform a previous pairwise improved bayesian metric detection step.", "histories": [["v1", "Fri, 8 Apr 2016 13:35:03 GMT  (2249kb,D)", "http://arxiv.org/abs/1604.02354v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["dong wang", "xiaoyang tan"], "accepted": false, "id": "1604.02354"}, "pdf": {"name": "1604.02354.pdf", "metadata": {"source": "CRF", "title": "Bayesian Neighbourhood Component Analysis", "authors": ["Dong Wang", "Xiaoyang Tan"], "emails": ["(x.tan@nuaa.edu.cn)."], "sections": [{"heading": null, "text": "I. INTRODUCTION\nLearning a good distance metric in feature space is crucial in many real-world applications. It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7]. Most of distance metric learning (DML) methods aim to learn a linear transformation which pulls together samples from the same class while pushing away those from different classes.\nThere has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14]. Although metric learning algorithms have achieved great success, there are several challenges remaining to be addressed: Firstly, most algorithms are based on point estimation, which is sensitive to the choice of training examples; Secondly, they tend to be over-fitting especially when training set is small, partly due to the large parameter space; Last but not least, learning a Mahalanobis metric usually involves very big computational complexity ( i.e., o(N3), N is the number of training data). The recently proposed pairwise constrained Bayesian metric learning method (BML) [15] overcomes some of the above issues by taking the prior distribution of the transformation matrix into account. However, it treats each sample independently and ignores the different importance of each sample, which limits its efficiency in learning.\nGraph constraints can be regarded as the extension to pairwise constraints ([15], [10]) by simultaneously building the\nDong Wang and Xiaoyang Tan are with the Department of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, P.R. China. Corresponding author: Xiaoyang Tan (x.tan@nuaa.edu.cn).\nsimilarity constraints between one sample with many others. One typical metric learning method using graph constraints is the well-known Neighbourhood Component Analysis (NCA) [16], which is conceptually simple and is developed under a well-formulated probabilistic framework. There are several extensions of this method in literatures, such as the maximummargin nearest neighbor (LMNN) [17], Nearest Class Mean [1], label noise robust NCA [18], and so on. However, all of them are based on the point estimation.\nAnother problem with many existing metric learning methods is that they seldom take the influence of label noise into consideration. The errors in data labels may result in unnecessary complex model which in turn leads to overfitting. This problem has been previously studied under the umbrella of agnostic learning [19], in which the relationship between the label and the data is largely relaxed. Since then many label noise robust methods have been proposed [20] [21] [22] [23] [24] [25], but they usually have high computation cost and do not focus on metric learning.\nIn this paper we present a graph constrained Bayesian metric learning method to address all the above issues. The method is based on the NCA method but for the first time, extends it under the Bayesian framework, hence called Bayesian Neighbourhood Component Analysis (BNCA). Unlike previous studies on metric learning methods, our method naturally takes account the uncertainty due to the unreliable parameter estimation under difficult conditions and is less susceptible to overfitting by exploiting the prior knowledge. Compared to other methods, it also provides more robust estimation even when there are errors in data labels, with the help of effective group label consistency constraints. Furthermore, we develop a variational lower bound of the log-likelihood of the objective, which significantly reduces the computational cost while preserving the effectiveness of Bayesian estimation. Finally, we give a throughout analysis of the proposed method from the angle of sample selection, robustness property, and computational complexity.\nTo verify the effectiveness of the proposed method, we have conducted a comprehensive study that compares the BNCA with other DML techniques on several real-world applications, including image classification, digital recognition, and face recognition. Our experiments demonstrate that the BNCA method is able to learn robust metric measures from small size dataset and/or from unperfect training set with its data labels contaminated by errors. It is also shown to outperform a previous pairwise constrained Bayesian metric learning method and several other state of the art DML methods.\nThe remaining parts of this paper are organized as follows: In Section II, preliminaries are provided regarding the closely related metric learning algorithms, then we detail our proposed Beyesian Neighbourhood Component Analysis in Section III.\nar X\niv :1\n60 4.\n02 35\n4v 1\n[ cs\n.C V\n] 8\nA pr\n2 01\n6\n2 In section IV we give a thorough analysis of this method. In section V, we investigate the performance of our method empirically over several popular datasets. We conclude this paper in Section VI."}, {"heading": "II. PRELIMINARY", "text": "Assuming that we have a dataset D of N data points, denoted as D = {xi, yi}, i = 1, 2, 3, ..., N , where yi is the label of the i-th data point xi. In distance metric learning, we aim to learn a Mahalanobis matrix\u2014A using some form of supervision information. Mahalanobis distance metric measures the squared distance between two data points xi and xj as follows,\nd2A(xi, xj) = (xi \u2212 xj)TA(xi \u2212 xj) (1)\nwhere A \u2265 0 is a positive semidefinite matrix and xi, xj \u2208 Rd is a pair of samples (i, j). For simplicity we denote d2A(xi, xj) as d2Aij . With these notations, in what follows, we give a brief overview on two state-of-the-art works closely related to ours in learning a Mahalanobis metric, i.e., NCA [16] and pairwise constrained Bayesian metric learning [15]."}, {"heading": "A. Neighbourhood Component Analysis", "text": "The NCA algorithm [16] begins by constructing a complete graph with each data point as its node. Let the weight of each edge between any two nodes denoted as pij . It is interpreted as the probability that data point xi selects xj as its neighbor and can be calculated as follows,\npij = exp(\u2212d2Aij )\u2211 t\u2208Ni exp(\u2212d 2 Ait)\n(2)\nwhere Ni denotes the set of neighbors of xi. It can be checked that pij \u2265 0 and \u2211 j\u2208Ni pij = 1, and hence pij is a valid probability measure. The object of NCA is then to learn a linear transformation A which maximizes the log likelihood that after transformation each data point selects the points with the same labels as itself as neighbors, i.e.,\nmax L(A) = \u2211 i log( \u2211 j\u2208Ni 1{yi = yj} \u00b7 pij) (3)"}, {"heading": "B. Pairwise Constrained Bayesian Metric Learning", "text": "Yang et al. [15] proposed a Bayesian metric learning (BML) method that estimates the posterior distribution for the distance metric from labeled pairwise constraints. It defines the probability for two data points xi and xj to form an equivalence or inequivalence constraint under a given distance metric A:\nP (yij |xi, xj , A, \u00b5) = 1\n1 + exp(yij(d2Aij \u2212 \u00b5)) (4)\nwhere yij = { +1 (xi, xj) \u2208 S \u22121 (xi, xj) \u2208 D\n(5)\nIn the above, S and D respectively denote the sets of equivalence or inequivalence constraints. Given this, the posterior\ndistribution of metric A and the threshold \u00b5 can be estimated by maximizing the following objective:\nL(A,\u00b5) = \u220f (i,j) P (yij |xi, xj , A, \u00b5)p(A)p(\u00b5) (6)\nThis method effectively overcomes some of the limitations of traditional metric learning methods. However, it does not take the structure of data into consideration and does not scale well. Particularly, since its objective just requires that the distance between similar pairs of points should be lower than that between dissimilar ones, all pairs (i, j) (o(N2)) need to be calculated for training. This not only increases the computational cost, but also ignores the importance weight of each sample regards to model training, which significantly decreases the learning efficiency because ideally we should focus more on those data whose labels are not consistent with most of its neighbors, instead of treating them indifferently. This problem is partially addressed later by Yang et al. with an active learning method for data pair selection [15], but the computational cost remains high."}, {"heading": "III. BAYESIAN NEIGHBOURHOOD COMPONENT ANALYSIS", "text": ""}, {"heading": "A. The Proposed Method", "text": "We start our derivation by considering the three components of a general Bayesian model, i.e., prior, likelihood, posterior. Since the original NCA is a discriminant model, we write its likelihood as P (Y |X,A) in our Bayesian NCA, where A is the linear transformation matrix to be learnt. We follow the same assumption as that of NCA, i.e., the sample labels are conditionally independent given the labels of their nearest neighbors, hence the conditional model can be written as\nP (Y |X,A) = 1 Z(A) \u220f i P (yi|xi, YNi , XNi , A) (7)\nwhere Z(A) is a normalizing constant known as the partition function. To be consistent with NCA, we define\nP (yi = k|xi, YNi , XNi , A) = \u2211 j\u2208Ni 1{yj = k} \u00b7 exp(\u2212d 2 Aij )\u2211\nt\u2208Ni exp(\u2212d 2 Ait)\n(8) Comparing eq. (8) with eq. (4), we see that one of the major differences between our model and the BML lies in that the local Neighbourhood structure Ni is naturally embedded into the model in our method.\nTo compute the posterior of the distance metric A, a prior for it should be specified, and a convenient choice for this could be the Wishart prior. Unfortunately, it is well-known that combining the Gaussian prior with a non-Gaussian likelihood is difficult to compute. In addition, the integration of A is untractable as well.\nTo bypass the above issues, we first approximate the distance metric A as a linear combination of the top eigenvectors of the observed data, and then estimate the posterior distribution of the combination weights using variational method.\n3 1) Eigen Approximation: Let X = (x1, x2, ..., xN ) denote all the examples, and vl, (l = 1, 2, ..., d) be the top d eigenvectors of XXT . Inspired by [15], we approximate A using the first d eigenvectors, i.e., A= \u2211d l=1 \u03b3lvlv T l , where \u03b3l, (l = 1, 2, ..., d) are the combination coefficients. With this, the likelihood P (yi = k|xi, YNi , XNi , A) in eq. (8) reduces to its equivalent form P (yi = k|xi, YNi , XNi , \u03b3):\nP (yi = k|xi, YNi , XNi , \u03b3) = \u2211 j\u2208Ni 1{yj = k} \u00b7 exp(\u2212d 2 \u03b3ij )\u2211\nt\u2208Ni exp(\u2212d 2 \u03b3it))\n(9)\nwhere we define:\nwlij = (v T l (xi \u2212 xj))2 wij = [w 1 ij , w 2 ij , ..., w d ij ] T\n\u03b3 = [\u03b31, \u03b32, ..., \u03b3d] T\n(10)\nthen d2\u03b3ij = d 2 Aij = \u03b3 Twij . Our task then boils down to compute the posterior distribution of \u03b3. For simplicity, we assume that the prior distribution of \u03b3 to be Gaussian:\np(\u03b3) = N(\u03b3|m0, V0) (11)\nwhere m0 and V0 are respectively mean and covariance. 2) Variational Approximation: At the second step we employ the variational method to estimate the posterior distribution of \u03b3. The main idea is to introduce variational distributions for \u03b3 to construct the lower bound and then maximize the the lower bound to obtain the approximate estimation for the posterior distribution. We begin with the unnormalized logarithm likelihood log{Z(A)P (Y |X, \u03b3)} . Note that maximizing this objective directly regarding to A leads to the standard NCA algorithm, but our goal here is for local variational approximation, hence the partition function Z(A) is simply treated as a constant. Particularly,\nL = log{Z(A)P (Y |X, \u03b3)} = \u2211 i \u2211 k 1{yi = k} log{p(yi = k|xi, YNi , XNi , \u03b3)}\n= \u2211 i \u2211 k 1{yi = k} log{ \u2211 j\u2208Ni 1{yj = k} \u00b7 exp(\u2212d 2 \u03b3ij )\u2211 t\u2208Ni exp(\u2212d 2 \u03b3it) }\n(12)\nSince log(a+ b) > log(a) + log(b) if 0 < a, b < 1, we have,\nL > \u2211 i \u2211 j\u2208Ni yij log{ exp(\u2212d2\u03b3ij )\u2211 t\u2208Ni exp(\u2212d 2 \u03b3it) }\n> \u2211 i \u2211 j\u2208Ni yij log{ 1 1 + \u2211 t\u2208Ni exp(d 2 \u03b3ij \u2212 d 2 \u03b3it) }\n(13)\nLet xNi1 , xNi2 , ..., xNiK be respectively the K nearest neighbors of xi. For convenience, we introduce the following notations:\n\u03b7tij = d 2 \u03b3ij \u2212 d 2 \u03b3it = (wij \u2212 wit) T \u03b3 W ji = [wij \u2212 wiNi1 , wij \u2212 wiNi2 , ..., wij \u2212 wiNiK ] \u03b7ij = [\u03b7 Ni1 ij , \u03b7 Ni2 ij , ..., \u03b7 NiK ij ] T = (W ji ) T \u03b3 (14)\nRecall the definition of log-sum-exp function: lse(\u03b7ij) , log(1 + \u2211 t\u2208Ni exp(\u03b7tij)) (15)\nThen eq. (13) can be rewritten as: L > \u2212 \u2211 i \u2211 j\u2208Ni yij lse(\u03b7ij) (16)\nUsing Bohning\u2019s quadratic bound (see [26] page 758, section. 21.8.2 for details) we have:\nL > \u2211 i \u2211 j\u2208Ni yij{\u2212 1 2 \u03b7TijH\u03b7ij + b T ij\u03b7ij \u2212 cij}\n= \u2211 i \u2211 j\u2208Ni yij{\u2212 1 2 \u03b3TW ji H(W j i ) T \u03b3 + bTij(W j i ) T \u03b3 \u2212 cij}\n(17)\nwhere cij is a constant and the remaining notations are defined as,\nH = 1\n2 [IK \u2212\n1\nK + 1 1K1\nT K ] (18)\nbij = H\u03c8ij \u2212 g(\u03c8ij) (19)\ng(\u03c8ij) = exp(\u03c8ij \u2212 lse(\u03c8ij)) (20)\nNote that \u03c8ij is the variational parameter. Now we proceed to compute the posterior distribution of \u03b3, which we model as a Gaussian, denoted as N(\u03b3|mT , VT ). We write the unconstrained posterior distribution,\np(\u03b3|X,Y ) \u221d p(Y |X, \u03b3)p(\u03b3), (21)\nand plug in the approximated likelihood (17) and the prior distribution N(\u03b3|m0, V0) to get,\nVT = [V \u22121 0 + \u2211 i \u2211 j\u2208Ni yijW j i H(W j i ) T ]\u22121 (22)\nmT = VT (V \u22121 0 m0 + \u2211 i \u2211 j\u2208Ni yijW j i bij) (23)\nFinally the variational parameter \u03c8ij is updated as,\n\u03c8ij = (W j i ) TmT (24)\nWe summarize the proposed method in Algorithm. 1."}, {"heading": "B. Distance Estimation", "text": "For inference we are interested in the expectation of the point-to-point distance d2\u03b3ij for a new couple of data (i, j) according to the posterior distribution of \u03b3, which is a Gaussian distribution as shown above. Particularly, we have,\nd2\u03b3ij \u223c N(d 2 \u03b3ij |mij , \u03c3 2 ij) (25)\nwhere\nmij = (wij) TmT\n\u03c32ij = (wij) TVTwij\n(26)\nIt is worthwhile to mention that this mechanism of outputting model uncertainty in distance metric calculation is potentially\n4 Algorithm 1 Bayesian Neighbourhood Component Analysis. Input:\nInput: Training set {(xi, yi)| i = 1, 2, ..., N}, prior distribution N(\u03b3|m0, V0) ;\nOutput: posterior distribution N(\u03b3|mT , VT ) \u2014\u2014 Training Stage\n1: Define W ji , H according to (14) and (18) respectively. 2: Compute VT with eq. (22). 3: Repeat 4: compute \u03c8ij for all (i, j) with eq. (24) 5: compute bij for all (i, j) with eq. (19) 6: compute mT with Eq. (23). 7: Until converged. 8: Return N(\u03b3|mT , VT ).\nbeneficial to many real world applications but unfortunately is largely ignored in the field. For example, in the application of image retrieval rather than ranking the results purely based on the estimated similarity, we could now construct a more robust ranking scheme by taking the value of the related similarity uncertainty (i.e., \u03c32ij) into account. We would not pursue this issue any further as it is out of the range of this paper, but it will be the focus of our future work.\nInstead, one could simply use the MAP value\u2014mij to estimate each d2\u03b3ij . To see the difference between this with the traditional NCA method, we decompose its expectation as follows,\nE(d2\u03b3ij ) = \u2211 l wlijm l T\n= \u2211 l (xi \u2212 xj)T vlmlT vTl (xi \u2212 xj) (27)\nwhere wlij and m l T are the l-th element of wij and mT respectively. Now defining the new coordinate axes as [v\u20321, v \u2032 2, ..., v \u2032 d] with v\u2032l = (m l T ) 1 2 \u00b7 vl, we see that the inference equation (26) essentially calculates the distance in a feature space spanned by the top d the eigenvectors of XXT but scaled by (mT ) 1 2 , according to the distribution of the corresponding eigenvalues (c.f., Fig. 1)."}, {"heading": "C. Prediction under Parameter Uncertainty", "text": "Under the difficult condition of small size training samples or samples with label noise, a single estimate of parameter A tends to be unreliable and the traditional DML methods that are based on it may cause overconfidence in the future predictions. In other words, they just make predictions but cannot tell whether these predictions make sense. By contrast, for Bayesian methods this is really not a problem because no errors would be introduced due to the inaccurate estimation of A. Particularly, the prediction for a never-seen sample xi can be obtained from p(yi|xi, YNi , X). Recall that the variational posterior of metric parameter \u03b3 is a Gaussian distribution, i.e.,\nq(\u03b3) = N(\u03b3|mT , V T ) (c.f., eq. (23) & eq. (22)), we have, p(yi|xi, YNi , XNi) = \u222b \u03b3 p(yi|xi, YNi , XNi , \u03b3)q(\u03b3)d\u03b3 (28)\nThe difficulty here is that this integration of \u03b3 is untractable, because p(yi|xi, YNi , X, \u03b3) is a multinomial distribution while q(\u03b3) is a Gaussian one. Instead we adopt a MCMC method [27] to approximate this expectation:\np(yi|xi, YNi , XNi) \u2248 1\nT T\u2211 l=1 p(yi|xi, YNi , XNi , \u03b3l) (29)\nwhere \u03b3l(l = 1, 2, ..., T ) are sampled i.i.d. from q(\u03b3)."}, {"heading": "IV. ANALYSIS OF THE PROPOSED METHOD", "text": ""}, {"heading": "A. Adaptive Sample Selection in Learning", "text": "In the process of metric learning, it is beneficial to exploit the local property of samples in the input space to improve the learning efficiency. Take the NCA algorithm as an example. Its gradient is calculated as follows,\n\u2202L \u2202A = 2A \u2211 i ( \u2211 j\u2208Ni pijxijx T ij \u2212\n\u2211 j\u2208Ni yijpijxijx T ij\u2211\nj\u2208Ni yijpij ) (30)\nThat is, for any point xi, if and only if all its K nearest neighbors have the same labels as that of xi, then \u2211 j\u2208Ni yijpij = 1, which means that the gradient equals to ~0. Hence the NCA algorithm would pay more attention on those points whose labels are inconsistent with its K nearest neighbors. In other words, not all pairs (xi, xj) are active in constraining the search space of the transformation matrix in the same way.\nSimilar observations can be made in other NCA extensions, such as the Large Margin NN (LMNN [17]) method:\nmin L(A) = \u2211 i \u2211 j\u2208Ni (d2\u03b3ij + \u00b5 \u2211 l\u2208Ni (1\u2212 yil)\u03beijl(A)) (31)\nwhere the first term can be seen as a regularization while the second one penalizes those data points that violate the large margin condition.\nBut the situation is different for pairwise constrained models, e.g., [15], in the sense that they usually lack an automatic sample selection mechanism in the metric learning objective by itself (c.f., eq. (6)). In our opinion, it is important to give different importance weights to different points during training, because doing this properly potentially allows us to significantly reduce the computational costs and to lessen the likelihood of overfitting. Actually to avoid computing distance for all possible data pairs, Yang et al. [15] designed an effective active learning method to select the most uncertainty pairs in training process, but the algorithm still needs to compute and store all the pairs\u2019 uncertainty scores.\nLet\u2019s come to our results of BNCA shown in eq. (22) and eq. (23). For simplicity here we only care about the diagonal elements V llT (l = 1, 2, ..., d) of VT . Let us define W jl\ni as the l-th row of W ji (W j i = [W j1 i ,W j2 i , ...,W jd i ] T ):\nW j l\ni = [w l ij \u2212 wliNi1 , w l ij \u2212 wliNi2 , ..., w l ij \u2212 wliNiK ] (32)\n5\nxi xNi1 xNi2 xNi3\nxi\nxi xNi1 xNi2 xNi3\nxNi1 xNi2 xNi3\nxi\nxj\nxk xi\nxj\nxk\nFrom eq. (22) we get:\nV llT = ((V ll 0 ) \u22121 + \u2211 i \u2211 j\u2208Ni \u2211 t\u2208Ni yijW jl i Hll(W jl i ) T )\u22121 (33)\nAssume that K 1, H can be approximated by 12I , such that:\nV llT = ((V ll 0 ) \u22121 +\n1\n2 \u2211 i \u2211 j\u2208Ni yij \u2211 t\u2208Ni (wlij \u2212 wlit)2)\u22121 (34)\nIf we simply throw away the non-diag elements of VT from Eq. (23), we see that V llT is in proportion to m l T . In other words, in BNCA we scale the axis vl by reducing the variance of \u03b3l such that all xi\u2019s neighbors in the same class will be closer in that direction, as illustrated in Fig. 1.\nTo see how our proposed BNCA handles different data points adaptively, we consider the following two extreme circumstances: 1) all xi\u2019s nearest neighbors have the same labels as that of xi and have the same distance to xi; 2) none of xi\u2019s nearest neighbors belongs to the same class of xi. In both cases the term \u2211 j\u2208Ni yij \u2211 t\u2208Ni(w l ij \u2212 wlit)2 (c.f., eq. (34)) equals to 0, such that the variance won\u2019t be changed by xi. In the first circumstance those xi can be thought of as perfect points that needn\u2019t to be adjusted, while the second case illustrates how our method handles the data in a robust way when some of them lie on the decision boundary or when their labels are too noisy to be learnt from."}, {"heading": "B. Robustness against Label Noise", "text": "To reveal the influence of label noise on the training of a DML model, we start the analysis with the NCA method. First let us denote the two major components of its gradient (eq. (30)) as CE and CI , respectively,\nCE = \u2211 i \u2211 j\u2208Ni pijxijx T ij (35)\nCI = \u2211 i\n\u2211 j\u2208Ni yijpijxijx T ij\u2211\nj\u2208Ni yijpij (36)\n= \u2211 i \u2211 j pijxijx T ij \u2211 k 1(yi = k) \u00b7 1(yj = k)\u2211 j 1(yj = k) \u00b7 pij\n(37)\nWe see that, \u2202L\n\u2202A = 2A(CE \u2212 CI) (38)\nIntuitively, the CE term denotes the total scatter matrix of the data points lying on the manifold induced by A and CI is the corresponding intra-class scatter matrix eq. (38) reveals that, up to a constant matrix, in each step the NCA algorithm tries to seek a better linear transformation such that after projection the total covariance becomes \u2019larger\u2019 while the intra-class covariance becomes \u2019smaller\u2019. However, when the class labels are inaccurate or noisy, the estimation of CI tends to be inaccurate (the CE will be not influenced by this).\nThe same situation occurs in LMNN. As can be seen from eq. (31), label noise would possibly result in a lot of incorrect training triples (ijl), which pull together samples from different class while pushing away those from the same classes. This issue becomes more and more troublesome with the increase in the noise level, and actually all the DML techniques trained in a supervised way would suffer from this if not properly taken care of. This is witnessed by our experiments given later, showing that under some high noise level, many traditional start-of-the-art DML methods such as NCA and LMNN will even be inferior to the unsupervised baseline, i.e., PCA.\nFinally let us come back to the proposed BNCA. From eq. (22) and eq. (23) we see that the estimated posterior distribution is influenced by the label noise as well, which may lead to inaccurate scaling of the projection axis in each training iteration (c.f., Algorithm. 1). But in our BNCA method there are two built-in mechanisms that help to alleviate this problem: the first one is due to the introducing of prior knowledge, which works like a regularization to the model explanation of the data. The other one can be seen from the variational inference result of eq. (34), where the influence of data points with label noise is effectively reduced because these erroneous data tend to have quite different labels with their neighbors. It is worth mentioning that in BNCA there is no any preprocessing of label correction and hence no extra time cost would be involved for this."}, {"heading": "C. Computational Cost", "text": "To analyze the computational cost of the proposed method, first note that usually the most time-consuming step in Laplace approximation or a conjugate gradient method is related to the calculation of Hessian matrix. In each iteration, it needs o(N3) computational cost (where N is the number of training data, e.g., if N is 103, the computational cost could be as large as 109). While in our case, thanks to the Bohning\u2019s approximation the Hessian matrix becomes a constant matrix (c.f., eq. (18)), calculated only once.\nFurthermore, in NCA and other point based metric learning methods their objective are commonly optimized using some gradient descent based method, such as stochastic gradient descent [28] or conjugate gradient descend [29], etc.. Those methods usually need a number of iterations to perform local search. Even worse, when encountering non-convex problems, they usually need more time to converge and may get lost in the local minimums. By contrast our BNCA method avoids the time-consuming gradient iterations completely - the lower bound of eq. (17)) actually gives us an analytic solution (c.f.,\n6 eq. (22) and eq. (23)). As a consequence, our computational cost is significantly reduced compared to others."}, {"heading": "V. EXPERIMENTS", "text": "To verify the effectiveness of the proposed method, in this section we first compare the robustness performance of our method with several related DML methods on the datasets either with small sample size or with label noise, then we turn to investigate in depth the behavior of the proposed method."}, {"heading": "A. Experimental Settings", "text": "We compare the performance of the proposed method with several other closely related DML methods, including NCA [16], LMNN [17], metric learning for Nearest Class Mean [1] (NCM), and pairwise constrained Bayesian metric learning [15] (BML). Both the NCA and the LMNN are metric learning methods with graph constraints, and the NCA is the method our method based on, hence chosen to be the baseline algorithm. Like ours, the BML method proposed by Yang et al. [15] is a Bayesian method as well, but with pairwise constraints for learning. Finally, we also adopted an unsupervised latent feature learning method, i.e., Principal Component Analysis [30] (PCA) for comparison, since it is completely irrelevant to the issue of label noise.\nFor all the methods except the NCM (which has its own classifier), we used the K-NN method equipped with the corresponding learnt metric for classification. In addition, the performance of all the compared methods are based on the original implementation kindly provided by the corresponding authors, and the related hyper-parameters are fine-tuned through cross validation. Each experiment is repeated for ten times, and both the mean and the standard deviation of the classification accuracy are reported. To evaluate the performance of the compared methods, we also conducted pairwise one-tail statistical test under significance level 0.05.\nImplementation Details In Bayesian NCA there are a few parameters need to be initialized, mainly including the parameters of the prior distribution N(\u03b3|m0, V0) and local variational approximation parameters , including bij (eq. (19)) and \u03c8ij (eq. (24)). In this work we set m0 to ~1 where ~1 is all 1\u2019s vector and is a small scalar (e.g. 0.1). From section III-B we see that this choice of m0 is equivalent to initialize BNCA with PCA, which is commonly used in metric learning for initialization and will not be affected by label noise. Besides, we set V0 to \u03c3I , where \u03c3 is a vary small value (e.g. 0.001). This helps to preserve the stability of VT (eq. (22)), one important property related to overfitting. Then we compute bij and \u03c8ij according to eq. (19) and eq. (24) respectively."}, {"heading": "B. Learning from Small Size Training Set", "text": "First we investigate the performance of our method with small sample size on three UCI datasets (\u201dBalance\u201d, \u201dIonosphere\u201d, and \u201dSpambase\u201d). In each dataset we randomly sample 3 subsets as training set with the size of 10\u00d7C, 20\u00d7C and 30\u00d7C (C is the number of categaries) respectively, and use an extra subset containing 100 data points as test set.\nTable I gives the classification performance. One can see that when the training set is small, point estimation-based methods tend to be unreliable. With only 10 training samples the standard NCA performs even worse than the baseline approach (PCA) on two of the three datasets tested. By contrast the proposed BNCA performs the best among the compared methods, partly due to the advantage of the Bayesian framework which potentially provides reliable estimation even when the size of the training data is small.\nTable I also shows that with increasing number of training points, the performance of all the methods considered here improves a lot. As expected, when we sample 30 data from each class, the performance gap between the Bayesian approaches and the point estimation based methods (such as LMNN) becomes small. Furthermore, one can see that our BNCA consistently outperforms BML. That illustrates the benefits of graph constraints for metric learning, which allows our method to effectively exploit the local structure of data while BML does not."}, {"heading": "C. Learning Under Random Label Noise", "text": "To test the performance of our method under label noise, we tested our method on several real-world applications, including image classification (on the Caltech-10 dataset [31]), digital recognition (on the MNIST [32]), and face recognition (on the FRGC-2 [33]). The datasets adopted are popular benchmark on each of the task respectively: 1) Caltech-10 is a subset sampled from Caltech-256 image dataset [34] with 10 most popular categories. The training set contains 300 images (30 from each class) and the test set is another randomly sampled 300 images; 2) The dataset of MNIST we used contains 600 digit images sampled from the full dataset (60 from each class; training/test: 300/300); 3) The dataset of FRGC we used contains 400 face images from 20 subjects (20 images per subject, training/test: 200/200). On all these datasets we inject random label noise on 3 levels (10%, 20%, 30%) and the test sets are kept clean.\nFig. 2 illustrates some of the noisy data of Caltech-10. One can see that in each category there exist some portion of images that are not belonging to this category, possibly due to the errors introduced in the labelling procedure, and very few work investigate the consequence of this.\nTable II shows how the metric learning algorithms perform under random label noise. Label noise could mislead metric learning algorithms in a way that it pulls data from different class together while keeps those from the same class away. When there is no label noise, almost all metric learning methods help to make an improvement in accuracy. However, it can be seen that the performance of all the methods declines with the increasing of noise level. Particularly, as the noise level increases to 30% some of the metric learning methods do not work (such as NCA, NCM and LMNN) in the sense that they even perform worse than the original baseline PCA. Our BNCA works significantly better than traditional metric learning methods even under this challenging case - even when the noise level reaches 30%, the p-value is smaller than 0.001 when comparing our method with the second best performer in terms of accuracy.\n7\nTABLE I COMPARATIVE PERFORMANCE (%) ON UCI DATASETS WITH VARYING SIZES OF TRAINING SET. (THE ASTERISKS INDICATE A STATISTICALLY SIGNIFICANT DIFFERENCE BETWEEN THE SECOND BEST PERFORMER AND THE PROPOSED METHOD AT A SIGNIFICANCE LEVEL OF 0.05.)\nDataset training Size(#) PCA NCA NCM LMNN BML BNCA Balance 10 69.42\u00b1 0.20 67.85\u00b1 0.35 66.28\u00b1 0.20 70.38\u00b1 0.33 71.38\u00b1 0.33\u2217 75.42\u00b1 0.31 20 74.53\u00b1 0.12 74.38\u00b1 0.29 72.62\u00b1 0.13 77.38\u00b1 0.28\u2217 76.74\u00b1 0.26 80.76\u00b1 0.26 30 77.23\u00b1 0.10 79.38\u00b1 0.18 76.16\u00b1 0.10 81.18\u00b1 0.16\u2217 79.74\u00b1 0.14 83.26\u00b1 0.14 Ionosphere 10 68.91\u00b1 0.17 70.97\u00b1 0.28 68.91\u00b1 0.17 73.54\u00b1 0.30 73.68\u00b1 0.28\u2217 76.75\u00b1 0.28 20 73.12\u00b1 0.15 75.75\u00b1 0.21 73.19\u00b1 0.15 78.64\u00b1 0.23\u2217 76.52\u00b1 0.20 81.22\u00b1 0.21 30 76.14\u00b1 0.11 80.96\u00b1 0.17 77.82\u00b1 0.10 83.88\u00b1 0.16\u2217 80.86\u00b1 0.16 85.96\u00b1 0.16 Spambase 10 73.42\u00b1 0.23 70.38\u00b1 0.37 70.08\u00b1 0.24 72.85\u00b1 0.35 75.38\u00b1 0.35\u2217 79.42\u00b1 0.34 20 77.53\u00b1 0.20 76.74\u00b1 0.24 75.56\u00b1 0.20 79.38\u00b1 0.25\u2217 78.52\u00b1 0.24 83.76\u00b1 0.24 30 79.53\u00b1 0.14 81.74\u00b1 0.23 79.74\u00b1 0.14 84.38\u00b1 0.23\u2217 80.97\u00b1 0.22 86.76\u00b1 0.22\nTABLE II COMPARATIVE PERFORMANCE (%) ON DIFFERENT DATASETS WITH VARYING DEGREE OF LABEL NOISE.(THE ASTERISKS INDICATE A STATISTICALLY SIGNIFICANT DIFFERENCE BETWEEN THE SECOND BEST PERFORMER AND THE PROPOSED METHOD AT A SIGNIFICANCE LEVEL OF 0.05.)\nTable III compares the corresponding running time of the methods in Table II under the situation of no label noise, with our (unoptimized Matlab) implementation. The table shows that on the average the proposed BNCA runs more 61.2% faster than the NCA algorithm and more than 41.0% faster than the BML method. This is consistent with our analysis described section. IV-C. That is, in each iteration of variational inference, the introduction of fixed curvature Bohning bound effectively avoids computing the Hessian matrix, resulting in significant reduction of running time.\nA B"}, {"heading": "D. Predictive Performance under Difficult Conditions", "text": "In the previous two sections we have shown the benefits of the proposed BNCA method that learns either from a small number of training examples or from examples with label noise. In this section, we investigate empirically the robustness performance of the BNCA method under difficult conditions by comparing it with the baseline NCA method. The motivation for this is that since the difficult samples are commonly those lying either in the uncertain region, or those lying far away from the normal distribution, making prediction under these conditions would impose great challenge for a traditional DML method based on point estimation, due to its lack of accounting for parameter uncertainty.\nWe conducted this series of experiments using MNIST [32]. Firstly 300 \u201dnormal\u201d data points are sampled (by \u201dnormal\u201d we mean that those digital images are not difficult for a human to recognize), and are used to train two models, i.e., a NCA and a BNCA model. For test, we collect two different test sets: One is normal while the other is most difficult in the sense that all digital images in this set are hard to recognize even by human. Since it is both time-consuming and error prone to select those difficult samples manually, we adopt one state of the art model on the MNIST dataset, i.e, the C-SVDDNet [35], as the expert to choose samples, and those samples close to the decision boundary of C-SVDDNet would be regarded as difficult samples otherwise as normal samples. In this way, we collect 300 random normal samples and 300 most difficult samples respectively as test sets. There is no overlapping\nbetween these two test sets. Some of the samples are illustrated in Fig. 3. To evaluate the performance of NCA and Bayesian NCA in the two cases, we compute the predictive probability P (yi|xi, YNi , XNi , A) (using eq. (8)) and P (yi|xi, YNi , XNi) (using eq. (29)) on the test data.\nFig. 4(a) visualizes the probability mass assigned to the normal test samples by the two models, respectively. We can see that both NCA and BNCA have a single peak probability mass, indicating that both of them are quite certain about their predictions. However, on the difficult sets their behaviors are largely different. Fig. 4(b) gives the results on this harder test set, and the ranking list of predictions according to their assigned probability.\nIt is obvious that the NCA is overconfident in its prediction. For example, the leftmost column of Fig. 4(b) shows that NCA incorrectly classifies the image of \u201d9\u201d as \u201d7\u201d with a high predictive probability of over 0.99 (those predictions with posterior mass less than 0.01 are canceled), indicating that this type of approximation to the posterior with a point mass is inadequate. On the other hand, the predictions of the BNCA are more moderate. One can see that although the true labels may not be ranked the highest, they are correctly appeared among the first few high ranking candidates. Under the previous example, although there is over 60% probability is assigned to the digital of \u201d7\u201d by our method, a significantly higher amount of predictive mass (\u2265 25%) than that of NCA is correctly assigned to the number of \u201d9\u201d. This reveals that under difficult conditions BNCA provides a much better approximation to the posterior than the point estimation method of NCA, by considering the uncertainty of parameters.\nMore precisely, we compare the performance of NCA and BNCA on the two test sets. For this a new measurement, i.e., modified mean average precision (MAP), is introduced as our performance metric, which is defined as\nMAP = N\u2211 i=1 K\u2211 k=1 pi(k) \u00b7 1{yi = k} \u00b7 1{p(yi = k|xi) > \u03c4}\n(39)\nwhere pi(k) is the precision at cut-off k in i\u2019s ranking list, and \u03c4 is a truncating threshold. The threshold is usually set to be a very small number (e.g., 0.01), since if the corresponding\nmissing\nrank = 1\nmissing\nrank = 3\nmissing\nrank = 3\nmissing\nrank = 2\nmissing\nrank = 2\nmissing\nrank = 1\nmissing\nrank = 3\nmissing\nrank = 3\nmissing\nrank = 2\nmissing\nrank = 2\nresponse p(yi = k|xi) of the model to the input xi is too small, there is no point to count it in performance measuring.\nFig. 5 gives the results. One can see that while on the normal test data, the MAP accuracy of BNCA is slightly better than that of NCA (about 1.0% higher), the BNCA significantly outperforms NCA by more than 5.0% on the difficult set. This reveals that taking the prediction uncert i ty into account indeed helps to improve the mean average precision. Also note that since the pairwise constrained BML method [15] only estimates whether a date pair belongs to the same class or\nnot, while not being able to give the predictive distribution p(yi|xi), it is not included for comparison here."}, {"heading": "E. Discussions", "text": "1) Robustness against Overfitting: To further investigate the behavior of the proposed BNCA method, we plot in Fig. 6 the learning curves of both BNCA and NCA as the function of the number of iterations. Three datasets (Caltech10, MNIST, and FRGC) are used for this, with the same experimental setting as before, and for each dataset there are\n10\n30.0% random label noise injected. For NCA training, we used the conjugate gradient method [29], which seeks the steepest gradient direction with proper step size in each training step. The figure shows that with the iterations going, on all of the three datasets the training errors of NCA keep decreases but their test errors tend to rise at the same time, indicating that the method is easy to be overfitting under the condition of label noise. Although some empirical tricks such as early stopping can be adopted, the figure clearly shows that this is not an issue for our Bayesian extension to the NCA. Actually the figure reveals that it only takes a few iterations before the learning converges.\n2) The Importance of Prior in Metric Learning: As described in Section III, our method explores the usefulness of prior within a Bayesian framework, whose parameters are efficiently estimated through variational inference. It would be interesting to investigate in more details on the role played by the prior. For this we conducted a series of experiments on the Caltech-10 dataset by varying the value of V0 (\u03c3I) from 10\u22124 to 104 but keeping the mean value m0 fixed at the same time. Note that a large value of V0 indicates that the prior tends to be more noninformative (i.e., higher uncertain) about the \u03b3 value. Fig. 7 shows how the performance changes as a function of the degree of uncertainty in prior. One can see that the prior is beneficial when the value of V0 in the range between 10\u22124 and 100, but further increasing this tends to be useless as the prior becomes more noninformative.\nWe note that as it is generally difficult to estimate the full matrix A (c.f., eq. (11)), most traditional metric learning\nmethods exploit various low-rank approximation [16] [1] to it without using any prior. This may encounter difficulty especially when the size of samples is small or when the labels are noisy, and Bayesian learning provides a unique way to address this issue by incorporating the prior information."}, {"heading": "VI. CONCLUSION", "text": "We present a new Bayesian metric learning method\u2014 Bayesian Neighbourhood component analysis (BNCA) that effectively improves the performance of KNN classifier under the condition of small sample size and/or when data labels\n11\nare noisy. The method is based on the classical NCA method with point estimation, and for the first time extends it under the Bayesian framework. The major advantages of BNCA over NCA in distance metric learning are three folds: 1) it is easy to train without worrying about overfitting; 2) it performs more robust compared to NCA under difficult conditions; 3) it naturally handles label noise by reducing the influence of data points with possible labelling errors. In addition, to improve the efficiency of Bayesian learning, we introduce a new variational lower bound of the log-likelihood of the objective. Extensive experiments conducted on several challenging real-world applications show that the performance of the proposed BNCA method significantly improves upon the baseline NCA method and it outperforms several other state of the art distance metric learning methods as well. We are currently investigating more applications of the proposed BNCA method, such as image retrieval with model uncertainty."}], "references": [{"title": "Metric learning for large scale image classification: Generalizing to new classes at near-zero cost", "author": ["T. Mensink", "J. Verbeek", "F. Perronnin", "G. Csurka"], "venue": "Computer Vision\u2013ECCV 2012. Springer, 2012, pp. 488\u2013501.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning distance metrics with contextual constraints for image retrieval", "author": ["S.C. Hoi", "W. Liu", "M.R. Lyu", "W.-Y. Ma"], "venue": "Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, vol. 2. IEEE, 2006, pp. 2072\u20132078.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Ordinal distance metric learning for image ranking", "author": ["C. Li", "Q. Liu", "J. Liu", "H. Lu"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 26, no. 7, pp. 1551\u20131559, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiple instance metric learning from automatically labeled bags of faces", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "Computer Vision\u2013 ECCV 2010. Springer, 2010, pp. 634\u2013647.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Neighborhood repulsed metric learning for kinship verification", "author": ["J. Lu", "X. Zhou", "Y.-P. Tan", "Y. Shang", "J. Zhou"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 36, no. 2, pp. 331\u2013345, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive distance metric learning for clustering", "author": ["J. Ye", "Z. Zhao", "H. Liu"], "venue": "Computer Vision and Pattern Recognition, 2007. CVPR\u201907. IEEE Conference on. IEEE, 2007, pp. 1\u20137.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Pedestrian recognition with a learned metric", "author": ["M. Dikmen", "E. Akbas", "T.S. Huang", "N. Ahuja"], "venue": "Computer Vision\u2013ACCV 2010. Springer, 2011, pp. 501\u2013512.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Is that you? metric learning approaches for face identification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on. IEEE, 2009, pp. 498\u2013505.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation", "author": ["M. Guillaumin", "T. Mensink", "J. Verbeek", "C. Schmid"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on. IEEE, 2009, pp. 309\u2013316.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Large scale metric learning from equivalence constraints", "author": ["M. Koestinger", "M. Hirzer", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 2288\u20132295.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Constrained empirical risk minimization framework for distance metric learning", "author": ["W. Bian", "D. Tao"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 23, no. 8, pp. 1194\u20131205, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Incorporating privileged information through metric learning", "author": ["S. Fouad", "P. Tino", "S. Raychaudhury", "P. Schneider"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 24, no. 7, pp. 1086\u20131098, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient dual approach to distance metric learning", "author": ["C. Shen", "J. Kim", "F. Liu", "L. Wang", "A. Van Den Hengel"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 25, no. 2, pp. 394\u2013406, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A distributed approach toward discriminative distance metric learning", "author": ["J. Li", "X. Lin", "X. Rui", "Y. Rui", "D. Tao"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 26, no. 9, pp. 2111\u20132122, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian active distance metric learning", "author": ["L. Yang", "R. Jin", "R. Sukthankar"], "venue": "Proceedings of the Twenty-Third Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-07). Corvallis, Oregon: AUAI Press, 2007, pp. 442\u2013449.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "G.E. Hinton", "S.T. Roweis", "R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 2004, pp. 513\u2013520.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "Advances in neural information processing systems, 2005, pp. 1473\u20131480.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust distance metric learning in the presence of label noise", "author": ["D. Wang", "X. Tan"], "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward efficient agnostic learning", "author": ["M.J. Kearns", "R.E. Schapire", "L.M. Sellie"], "venue": "Machine Learning, vol. 17, no. 2-3, pp. 115\u2013141, 1994.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "Estimating a kernel fisher discriminant in the presence of label noise", "author": ["N.D. Lawrence", "B. Sch\u00f6lkopf"], "venue": "ICML. Citeseer, 2001, pp. 306\u2013313.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Boosting parallel perceptrons for label noise reduction in classification problems", "author": ["I. Cantador", "J.R. Dorronsoro"], "venue": "Artificial Intelligence and Knowledge Engineering Applications: A Bioinspired Approach. Springer, 2005, pp. 586\u2013593.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "A novel noise filtering algorithm for imbalanced data", "author": ["J. Van Hulse", "T.M. Khoshgoftaar", "A. Napolitano"], "venue": "Machine Learning and Applications (ICMLA), 2010 Ninth International Conference on. IEEE, 2010, pp. 9\u201314.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Handling label noise in video classification via multiple instance learning", "author": ["T. Leung", "Y. Song", "J. Zhang"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE, 2011, pp. 2056\u20132063.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Label-noise reduction with support vector machines", "author": ["S. Fefilatyev", "M. Shreve", "K. Kramer", "L. Hall", "D. Goldgof", "R. Kasturi", "K. Daly", "A. Remsen", "H. Bunke"], "venue": "Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012, pp. 3504\u20133508.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification in the presence of label noise: a survey", "author": ["B. Fr\u00e9nay", "M. Verleysen"], "venue": "2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": "MIT press,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "An introduction to mcmc for machine learning", "author": ["C. Andrieu", "N. De Freitas", "A. Doucet", "M.I. Jordan"], "venue": "Machine learning, vol. 50, no. 1-2, pp. 5\u201343, 2003.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010. Springer, 2010, pp. 177\u2013 186.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "author": ["J.R. Shewchuk"], "venue": "1994.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1994}, {"title": "Principal component analysis", "author": ["S. Wold", "K. Esbensen", "P. Geladi"], "venue": "Chemometrics and intelligent laboratory systems, vol. 2, no. 1-3, pp. 37\u201352, 1987.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1987}, {"title": "Distance metric learning using dropout: a structured regularization approach", "author": ["Q. Qian", "J. Hu", "R. Jin", "J. Pei", "S. Zhu"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 323\u2013332.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Overview of the face recognition grand challenge", "author": ["P.J. Phillips", "P.J. Flynn", "T. Scruggs", "K.W. Bowyer", "J. Chang", "K. Hoffman", "J. Marques", "J. Min", "W. Worek"], "venue": "Computer vision and pattern recognition, 2005. CVPR 2005. IEEE computer society conference on, vol. 1. IEEE, 2005, pp. 947\u2013954.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Caltech-256 object category dataset", "author": ["G. Griffin", "A. Holub", "P. Perona"], "venue": "2007.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Unsupervised feature learning with c-svddnet", "author": ["D. Wang", "X. Tan"], "venue": "Eprint Arxiv, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7].", "startOffset": 149, "endOffset": 152}, {"referenceID": 4, "context": "It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7].", "startOffset": 175, "endOffset": 178}, {"referenceID": 5, "context": "It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7].", "startOffset": 191, "endOffset": 194}, {"referenceID": 6, "context": "It has been shown to significantly improve the performance of object classification [1], image retrieval [2], image ranking [3], face identification [4], kinship verification [5], clustering [6], or person reidentification [7].", "startOffset": 223, "endOffset": 226}, {"referenceID": 7, "context": "There has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14].", "startOffset": 89, "endOffset": 92}, {"referenceID": 8, "context": "There has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14].", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "There has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14].", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "There has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "There has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "There has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14].", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "There has been considerable research on distance metric learning over the past few years [8] [9] [10] [11] [12] [13] [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 14, "context": "The recently proposed pairwise constrained Bayesian metric learning method (BML) [15] overcomes some of the above issues by taking the prior distribution of the transformation matrix into account.", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "Graph constraints can be regarded as the extension to pairwise constraints ([15], [10]) by simultaneously building the", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "Graph constraints can be regarded as the extension to pairwise constraints ([15], [10]) by simultaneously building the", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "One typical metric learning method using graph constraints is the well-known Neighbourhood Component Analysis (NCA) [16], which is conceptually simple and is developed under a well-formulated probabilistic framework.", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "There are several extensions of this method in literatures, such as the maximummargin nearest neighbor (LMNN) [17], Nearest Class Mean [1], label noise robust NCA [18], and so on.", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "There are several extensions of this method in literatures, such as the maximummargin nearest neighbor (LMNN) [17], Nearest Class Mean [1], label noise robust NCA [18], and so on.", "startOffset": 135, "endOffset": 138}, {"referenceID": 17, "context": "There are several extensions of this method in literatures, such as the maximummargin nearest neighbor (LMNN) [17], Nearest Class Mean [1], label noise robust NCA [18], and so on.", "startOffset": 163, "endOffset": 167}, {"referenceID": 18, "context": "This problem has been previously studied under the umbrella of agnostic learning [19], in which the relationship between the label and the data is largely relaxed.", "startOffset": 81, "endOffset": 85}, {"referenceID": 19, "context": "Since then many label noise robust methods have been proposed [20] [21] [22] [23] [24] [25], but they usually have high computation cost and do not focus on metric learning.", "startOffset": 62, "endOffset": 66}, {"referenceID": 20, "context": "Since then many label noise robust methods have been proposed [20] [21] [22] [23] [24] [25], but they usually have high computation cost and do not focus on metric learning.", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "Since then many label noise robust methods have been proposed [20] [21] [22] [23] [24] [25], but they usually have high computation cost and do not focus on metric learning.", "startOffset": 72, "endOffset": 76}, {"referenceID": 22, "context": "Since then many label noise robust methods have been proposed [20] [21] [22] [23] [24] [25], but they usually have high computation cost and do not focus on metric learning.", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": "Since then many label noise robust methods have been proposed [20] [21] [22] [23] [24] [25], but they usually have high computation cost and do not focus on metric learning.", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "Since then many label noise robust methods have been proposed [20] [21] [22] [23] [24] [25], but they usually have high computation cost and do not focus on metric learning.", "startOffset": 87, "endOffset": 91}, {"referenceID": 15, "context": ", NCA [16] and pairwise constrained Bayesian metric learning [15].", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": ", NCA [16] and pairwise constrained Bayesian metric learning [15].", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "Neighbourhood Component Analysis The NCA algorithm [16] begins by constructing a complete graph with each data point as its node.", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "[15] proposed a Bayesian metric learning (BML) method that estimates the posterior distribution for the distance metric from labeled pairwise constraints.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "with an active learning method for data pair selection [15], but the computational cost remains high.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "Inspired by [15], we approximate A using the first d eigenvectors, i.", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": "Using Bohning\u2019s quadratic bound (see [26] page 758, section.", "startOffset": 37, "endOffset": 41}, {"referenceID": 26, "context": "Instead we adopt a MCMC method [27] to approximate this expectation:", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "Similar observations can be made in other NCA extensions, such as the Large Margin NN (LMNN [17]) method:", "startOffset": 92, "endOffset": 96}, {"referenceID": 14, "context": ", [15], in the sense that they usually lack an automatic sample selection mechanism in the metric learning objective by itself (c.", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": "[15] designed an effective active learning method to select the most uncertainty pairs in training process, but the algorithm still needs to compute and store all the pairs\u2019 uncertainty scores.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Furthermore, in NCA and other point based metric learning methods their objective are commonly optimized using some gradient descent based method, such as stochastic gradient descent [28] or conjugate gradient descend [29], etc.", "startOffset": 183, "endOffset": 187}, {"referenceID": 28, "context": "Furthermore, in NCA and other point based metric learning methods their objective are commonly optimized using some gradient descent based method, such as stochastic gradient descent [28] or conjugate gradient descend [29], etc.", "startOffset": 218, "endOffset": 222}, {"referenceID": 15, "context": "Experimental Settings We compare the performance of the proposed method with several other closely related DML methods, including NCA [16], LMNN [17], metric learning for Nearest Class Mean [1] (NCM), and pairwise constrained Bayesian metric learning [15] (BML).", "startOffset": 134, "endOffset": 138}, {"referenceID": 16, "context": "Experimental Settings We compare the performance of the proposed method with several other closely related DML methods, including NCA [16], LMNN [17], metric learning for Nearest Class Mean [1] (NCM), and pairwise constrained Bayesian metric learning [15] (BML).", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "Experimental Settings We compare the performance of the proposed method with several other closely related DML methods, including NCA [16], LMNN [17], metric learning for Nearest Class Mean [1] (NCM), and pairwise constrained Bayesian metric learning [15] (BML).", "startOffset": 190, "endOffset": 193}, {"referenceID": 14, "context": "Experimental Settings We compare the performance of the proposed method with several other closely related DML methods, including NCA [16], LMNN [17], metric learning for Nearest Class Mean [1] (NCM), and pairwise constrained Bayesian metric learning [15] (BML).", "startOffset": 251, "endOffset": 255}, {"referenceID": 14, "context": "[15] is a Bayesian method as well, but with pairwise constraints for learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": ", Principal Component Analysis [30] (PCA) for comparison, since it is completely irrelevant to the issue of label noise.", "startOffset": 31, "endOffset": 35}, {"referenceID": 30, "context": "Learning Under Random Label Noise To test the performance of our method under label noise, we tested our method on several real-world applications, including image classification (on the Caltech-10 dataset [31]), digital recognition (on the MNIST [32]), and face recognition (on the FRGC-2 [33]).", "startOffset": 206, "endOffset": 210}, {"referenceID": 31, "context": "Learning Under Random Label Noise To test the performance of our method under label noise, we tested our method on several real-world applications, including image classification (on the Caltech-10 dataset [31]), digital recognition (on the MNIST [32]), and face recognition (on the FRGC-2 [33]).", "startOffset": 247, "endOffset": 251}, {"referenceID": 32, "context": "Learning Under Random Label Noise To test the performance of our method under label noise, we tested our method on several real-world applications, including image classification (on the Caltech-10 dataset [31]), digital recognition (on the MNIST [32]), and face recognition (on the FRGC-2 [33]).", "startOffset": 290, "endOffset": 294}, {"referenceID": 33, "context": "The datasets adopted are popular benchmark on each of the task respectively: 1) Caltech-10 is a subset sampled from Caltech-256 image dataset [34] with 10 most popular categories.", "startOffset": 142, "endOffset": 146}, {"referenceID": 31, "context": "We conducted this series of experiments using MNIST [32].", "startOffset": 52, "endOffset": 56}, {"referenceID": 34, "context": "e, the C-SVDDNet [35], as the expert to choose samples, and those samples close to the decision boundary of C-SVDDNet would be regarded as difficult samples otherwise as normal samples.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "Also note that since the pairwise constrained BML method [15] only estimates whether a date pair belongs to the same class or not, while not being able to give the predictive distribution p(yi|xi), it is not included for comparison here.", "startOffset": 57, "endOffset": 61}, {"referenceID": 28, "context": "For NCA training, we used the conjugate gradient method [29], which seeks the steepest gradient direction with proper step size in each training step.", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "methods exploit various low-rank approximation [16] [1] to it without using any prior.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "methods exploit various low-rank approximation [16] [1] to it without using any prior.", "startOffset": 52, "endOffset": 55}], "year": 2016, "abstractText": "Learning a good distance metric in feature space potentially improves the performance of the KNN classifier and is useful in many real-world applications. Many metric learning algorithms are however based on the point estimation of a quadratic optimization problem, which is time-consuming, susceptible to overfitting, and lack a natural mechanism to reason with parameter uncertainty, an important property useful especially when the training set is small and/or noisy. To deal with these issues, we present a novel Bayesian metric learning method, called Bayesian NCA, based on the well-known Neighbourhood Component Analysis method, in which the metric posterior is characterized by the local label consistency constraints of observations, encoded with a similarity graph instead of independent pairwise constraints. For efficient Bayesian optimization, we explore the variational lower bound over the log-likelihood of the original NCA objective. Experiments on several publicly available datasets demonstrate that the proposed method is able to learn robust metric measures from small size dataset and/or from challenging training set with labels contaminated by errors. The proposed method is also shown to outperform a previous pairwise constrained Bayesian metric learning method.", "creator": "LaTeX with hyperref package"}}}