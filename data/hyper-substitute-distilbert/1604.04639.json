{"id": "1604.04639", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2016", "title": "ModelWizard: Toward Interactive Model Construction", "abstract": "data scientists leading in model construction routinely discover machine learning models that might support a nonlinear, strictly defiance of predictiveness, but in generalization across domains. questions characterized as \" what often we model common cause factors \" the \" damned if drivers'same dependence on x reverses \" inspire many computational models to consider and recognize, yet current leaders emphasize maintaining a general correlation roughly at once.", "histories": [["v1", "Fri, 15 Apr 2016 20:43:20 GMT  (1957kb,D)", "http://arxiv.org/abs/1604.04639v1", "Master's Thesis"]], "COMMENTS": "Master's Thesis", "reviews": [], "SUBJECTS": "cs.PL cs.LG", "authors": ["dylan hutchison"], "accepted": false, "id": "1604.04639"}, "pdf": {"name": "1604.04639.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "MODELWIZARD: TOWARD INTERACTIVE MODEL CONSTRUCTION\nby\nDylan Hutchison"}, {"heading": "A THESIS", "text": "Submitted to the Faculty of the Stevens Institute of Technology in partial fulfillment of the requirements for the degree of\nMASTER OF SCIENCE - COMPUTER SCIENCE\nDylan Hutchison, Candidate ADVISORY COMMITTEE\nDavid Naumann, Advisor Date\nPhilippos Mordohai, Reader Date\nSTEVENS INSTITUTE OF TECHNOLOGY Castle Point on Hudson\nHoboken, NJ 07030 2015\nar X\niv :1\n60 4.\n04 63\n9v 1\n[ cs\n.P L\n] 1\n5 A\npr 2\n01 6\nc\u00a92015, Dylan Hutchison. All rights reserved.\niii\nMODELWIZARD: TOWARD INTERACTIVE MODEL CONSTRUCTION\nABSTRACT\nData scientists engage in model construction to discover machine learning models that well explain a dataset, in terms of predictiveness, understandability and generalization across domains. Questions such as \u201cwhat if we model common cause Z\u201d and \u201cwhat if Y\u2019s dependence on X reverses\u201d inspire many candidate models to consider and compare, yet current tools emphasize constructing a final model all at once.\nTo more naturally reflect exploration when debating numerous models, we\npropose an interactive model construction framework grounded in composable operations. Primitive operations capture core steps refining data and model that, when verified, form an inductive basis to prove model validity. Derived, composite operations enable advanced model families, both generic and specialized, abstracted away from low-level details.\nWe prototype our envisioned framework in ModelWizard, a domain-specific\nlanguage embedded in F# to construct Tabular models. We enumerate language design and demonstrate its use through several applications, emphasizing how language may facilitate creation of complex models. To future engineers designing data science languages and tools, we offer ModelWizard\u2019s design as a new model construction paradigm, speeding discovery of our universe\u2019s structure.\nAuthor: Dylan Hutchison Advisor: David Naumann Date: 6 May 2015 Department: Computer Science Degree: Master of Science - Computer Science\niv"}, {"heading": "Acknowledgments", "text": "I am grateful to David Naumann and Philippos Mordohai, for travel support and discussions stimulating ModelWizard\u2019s growth between programming languages and machine learning; to Claudio Russo and Nicolas Rolland, for dives into bittersweet mathematical theory and pragmatic engineering; to Matthew Smith, for validation on real world ecology data and enthusiastic patience using development-stage software.\nTo Andy Gordon, from our internship enterprise and beyond,\nMay we open our office whiteboards, nay, the whiteboards of our minds,\nso timidly advertised yet so richly energized and ripe to publicize.\nOur whims combined will change the world one puzzle piece at a time.\nv Table of Contents\nAbstract iii\nAcknowledgments iv\nList of Tables viii\nList of Figures ix\nList of Code Listings x"}, {"heading": "1 Introduction 1", "text": "1.1 Data Science as Model Construction 1 1.2 Prelude Example: Freefall in ModelWizard 4\n1.2.1 Model Selection Matters: Overfitting and Consistency 9 1.2.2 Bayesian Inference 10 1.2.3 Interactive Model Construction 13\n1.3 Probabilistic Programming 14\n1.3.1 Tabular and Infer.NET 15\n1.4 ModelWizard: Interactive Model Construction for Tabular 15\n1.4.1 Concurrently Refining Data and Model 16 1.4.2 Composable Model Primitives 17 1.4.3 Safety in Model Construction 18\n1.5 Thesis Outline 18"}, {"heading": "2 ModelWizard Language 19", "text": "2.1 Types: State, Operation and Safety 19\nvi\n2.1.1 Defining \u201cValid\u201d States 20 2.1.2 ValidState and ValidOp Types 23 2.1.3 OpMonad: ValidOp Computation Expression 24\n2.2 Escape to F#: GetState 26 2.3 Machine Learning Base Models 26 2.4 Typing Data, Tables as Nominal Column Domains 28\n2.4.1 Type Inference 30\n2.5 Coupling Columns 31\n2.5.1 Continuous Coupling: Noisy Regression 31 2.5.2 Discrete Coupling: Index 32 2.5.3 Bookkeeping: Reordering Tables and Columns 35\n2.6 Latent Columns 35 2.7 Excel, Inference and Advanced Operations 36"}, {"heading": "3 Applications 38", "text": "3.1 Small Model Search: Discrete Bayesian Networks 38 3.2 Derived Model Family: Naive Bayes 42 3.3 Hybrid Model: Internet Plant Sales 45\n3.3.1 ExactInfer and Exact: Functional Dependencies 46 3.3.2 \u201cPre-processing\u201d during Modeling 49\n3.4 User-Movie-Rating Recommendation 51\n3.4.1 InfernoDB: Schema-Derived Clustering Models 51 3.4.2 Inferno Operation with Cluster Setting 53 3.4.3 Hyperparameter Sweeps over Number of Clusters 55 3.4.4 Performance on Missing Data 56 3.4.5 Alternatives 58\nvii"}, {"heading": "4 Discussion 61", "text": "4.1 Addressing Big Data\u2019s Challenges 61 4.2 Extending Model Safety to Model Inferability 64 4.3 Extending to Usable Interactivity 65 4.4 Interactive Theorem Proving Analogy 67 4.5 Conclusion 68\nAppendices 69\nA OpMonad Translation 69 B ModelWizard API of Primitive and Derived Operations 70\nBibliography 72\nviii\nList of Tables\n1.1 Apple in Freefall dataset 2 1.2 Apple in Freefall dataset, with \u201cmissing value\u201d rows 5\nix\nList of Figures\n1.1 Apple Freefall Linear and Quadratic Regression 7\n2.1 Tabular Conjugate Distributions 27 2.2 Factor graph of CDiscrete(N=2)[cloudy] on column rain 33\n3.1 Uncertain Bayesian Network 38 3.2 Intermediate states during execution of Figure 3.1 39 3.3 Initial State: Data and Schema before Naive Bayes 43 3.4 Naive Bayes result state after Code Listing 3.2 runs on Figure 3.3 43 3.5 Internet Plant Sales data 45 3.6 EFD-augmented Naive Bayes model classifying Wild Propagate 46 3.7 Synthetic movie recommendation data snippet, from [SG12, SG13] 51 3.8 Cross-validation mean scores for locally varying numbers of clusters 57 3.9 Tabular model after Inferno and sweeping numbers of clusters 57 3.10 Inferno model performance comparison varying missing cell count 59 3.11 Denormalized version of Figure 3.7\u2019s movie data 60\n4.1 ModelWizard GUI Sketch 66\nx List of Code Listings\n2.1 Selected OpMonad type signatures and implementations 25 3.1 Script to build and select Figure 3.1\u2019s most likely model 40 3.2 Naive Bayes script expressed in primitives 43 3.3 NaiveBayes derived operation 44 3.4 ModelWizard script to construct plant sales model 47 3.5 Inferno ModelWizard Operation 54\n1 Chapter 1 Introduction\nIn his 2013 visionary article, Dr. Chris Mattmann issued a call for data scientists: \u201cTo get the most out of big data, funding agencies should develop shared tools for optimizing discovery and train a new breed of researchers\u201d [Mat13]. This thesis is an answer to Dr. Mattmann\u2019s call, laying groundwork for new tools to help data scientists gain insight faster."}, {"heading": "1.1 Data Science as Model Construction", "text": "We address three scientific goals:\n\u2022 Prediction of unseen states.1\nE.g., an apple dropped from a tall building will fall about 122 meters in 5 seconds.\n\u2022 Understanding of the structure that naturally generates states.\nE.g., the force of gravity decreases apple elevation quadratically with time. Equivalently, gravity linearly decreases a hidden variable, apple velocity, which in turn linearly decreases apple elevation with time.\n\u2022 Generalization of common structure to other problems and domains.\nE.g., quadratic regression generalizes to polynomial regression, a relationship we may apply to light dispersion, roller coasters and plenty other datasets.\n1State is a domain-specific term whose definition is clear in the context of a problem. A chemical engineer working on oil refinery design may use state to refer to the chemical composition of fluid in a reactor; a statistical physicist to refer to a specific configuration of a thermodynamic system; a computer architect to refer to the contents of each register on a CPU; a stockbroker to refer to the price of equities at a given time. More examples abound.\nIn the context of data science, we address these goals by analyzing datasets:\npredicting unseen data values, understanding the structure that generates data, and generalizing that structure to other datasets. We apply these principles to the apple freefall example by imagining a dataset of apple elevations over time in the form of Table 1.1. Each row consisting of a (time, elevation) pair is a data point. We aim to predict unseen rows with missing time or elevation, to understand the structure that generates these pairs, and to generalize that structure to other contexts.\nBy structure we refer to some mechanism that generates data points. As-\nsuming the standpoint of epistemic uncertainty,2 there exists a true mechanism that generates data points in reality that we do not fully know. We call the unknown true mechanism true structure, and we call a guess at the true structure a candidate structure, shortened to structure for brevity. We also refer to structure as a model. We use the language of probability to describe uncertain knowledge of true structure.\nTrue structure includes everything up to recording a data point. Including how\nwe collect data points in models can therefore be helpful, in addition to how \u201cnature\u201d generates data points. In the Apple Freefall data for example, if we know our method of recording elevation is particularly inaccurate or imprecise, we ought to reflect\n2in contrast to aleatory uncertainty, which holds that no true structure may ever be known due to intrinsic randomness. Distinguishing the two types of uncertainty may be useful for determining what uncertainty is reducible by further data collection [KD09], but for our purposes we consider true structure completely discoverable in the limit of infinite data.\n3 that knowledge in our model.3 We may also introduce noise to account for model imprecision: that a model may partly account for the true structure mechanism. The magnitude of noise reflects how well that model accounts.\nModels represent a (joint) probability distribution over data points. Impossible\ndata points that a model cannot generate have zero probability mass. Possible data points receive probability mass proportional to likelihood that the model will generate them. We create candidate models using the subset of possible data points present in a dataset\u2019s non-missing values.\nWe are specifically interested in structure taking the form of machine learning\nmodels. These are models with a computable sampling procedure to output a data point. Sampling algorithms such as rejection sampling compute samples that satisfy conditions. Inference algorithms solve the reverse problem: computing the probability that the model will generate a data point.\nSome models are able to sample and infer data point probabilities for every\ncolumn in their dataset. We call those models generative, since they specify a distribution that can be used to generate samples over any column. We may also consider discriminative models that do not define distributions over every column.4 We cannot sample or infer data point probabilities of \u201cunmodeled\u201d columns in a discriminative model, instead using them as known inputs to sample or infer other columns. Data scientists may find discriminative models relieving because they do not have to specify distributions behind unmodeled columns, saving work required to justify why a certain distribution makes sense for a column.\nDefine model construction as the process of creating machine learning models\nfor a dataset. A well-built model solves all three goals: predicting unseen states\n3See how the Apple Freefall model noise term reflects data imprecision in Figure 1.1a and 1.1b. 4See [BBB+07] for an overview of generative and discriminative models.\n4 by (conditional) sampling, understanding structure by examining model components, and generalizing structure by applying components to machine learning models for other datasets. Prediction is a key link, because high predictive power on unseen data indicates that a machine learning model is close to the true structure generating data. Simplicity is another guide that increases understanding and generalizability; see [Fre14] for further reading on scoring machine learning models for comprehensibility.\nIn constructing our final model we may consider alternative models, comparing\nthem as we imagine them and choosing models that are simpler and fit our dataset better. We call this process model exploration.\nThe goal of the ModelWizard project is to develop a tool that supports the\nmodel exploration workflow. We aim to facilitate creativity, increase interactivity, introduce safety and support abstraction when writing a ModelWizard script that ultimately constructs a Tabular machine learning model."}, {"heading": "1.2 Prelude Example: Freefall in ModelWizard", "text": "We now introduce ModelWizard\u2019s exploration workflow through a small example, focusing on how a user guides exploration through questions and fitness metrics. Subsections address issues to consider before basing judgements on ModelWizard\u2019s output, and then on the underlying Bayesian methematics.\nAssume the perspective of a data scientist, unaware of gravity, presented with\nthe apple-in-freefall data of Table 1.1, collected on a windy day or by an imprecise instrument to add non-determinism. We may ask several questions: where is the apple at 4 or 5 seconds? What happens past 6 seconds when we observe the apple at elevation zero? At what time does the apple reach an elevation of 50m? We incorporate our questions into the dataset in Table 1.2.\nAn initial model we may posit for this dataset is a linear relationship between\ntime and apple elevation. To create this model, we use the ModelWizard operations\nLinReg \"Elevation\" \"Time\" (1.1)\nModel \"tmain\" \"Time\" (1.2)\nOperation 1.1 places a noisy linear regression model on elevation with explanatory variable time. Operation 1.2 places an uninformative Gaussian distribution on time.\nFigure 1.1a shows the resulting linear model. The rows in the bottom table\nlabelled \u2018tmain\u2019 list the name, type and model of corresponding columns in the original dataset of Table 1.2. The giant LinReg function block implements linear regression in Tabular syntax such that the Elevation column model may call linear regression as a macro. Detail-oriented readers may skip to Chapter 2 for deeper Tabular syntax and semantics, but for now we recommend accepting linear regression as is, focusing instead on ModelWizard\u2019s workflow. The linear function block\u2019s one addition to usual regression is the noise term, a probabilistic analogue to what statisticians call regression error. Larger inferred noise precision (equivalently, smaller inferred noise variance) tends to indicate less error and greater model fit, though we should take\n6 caution to avoid too high precision which is prone to overfit.\nOperation 1.2 converts the model from discriminative to generative. Because\nthe last row of our dataset in Table 1.2 contains a missing value for time, we must use a generative model in order to have a way to predict time. A generative model also allows us to answer questions such as \u201cwhat is the apple elevation\u201d without specifying time, equivalent to using the posterior distribution (see Section 1.2.2) on time to determine a distribution over elevation. Alternatively, if we omit the last row of Table 1.2 and only care about predicting elevation given time, then we could omit Operation 1.2, leaving time as an input and disallowing missing time values in our dataset. The advantage of the alternative discriminative model is not needing to specify and justify a distribution on time, which we questionably list as a Gaussian distribution. In summary, ModelWizard may construct both generative and discriminative versions of the linear model, and both have advantages in different situations.\nFigure 1.1c shows results of inference5 on the linear model. Inference consists of\ntraining a given model on observed (non-missing) values in the input dataset to form a posterior model, outputting posterior model parameters in the left of Figure 1.1c, and outputting marginal distributions from the posterior model on missing values, each conditioned on the observed values in the same row, in the right of Figure 1.1c. Greybackground \u201cPointMass\u201d distributions correspond to observed values in the original dataset of Table 1.2, whereas green-background cells are more meaningful posterior distributions for missing values in the dataset.\nIt seems suspicious that the linear model predicts positive elevation at 7 sec-\nonds, one second after the apple is known to reach 0m elevation. To better evaluate the linear model\u2019s performance using a method less prone to overfitting, we use leave-one-out cross-validation on the known rows of Table 1.1 with the operation\n5We use the Expectation Propagation inference algorithm throughout the Apple Freefall example.\n8 CrossValidate kFold RMSE \"tmain\" \"Elevation\" 5 and similarly for Time. Resulting root mean square errors (not shown in Figure 1.1c) are 14.996m for elevation and 1.046s for time. Viewing these numbers alone, we do not know whether the linear model is optimal or whether other models could deliver lower error.\nCould a more complicated model like quadratic regression between time and\napple elevation add enough predictive power to merit its increase in model complexity? An answer to this question in either direction\u2014whether quadratic regression is a viable or ill-suited model\u2014is an important clue to understanding the dataset\u2019s structure. Ruling out a model family like the family of quadratic models can be just as informative to a data scientist as showing a model\u2019s plausibility.\nWe build the quadratic regression model shown in Figure 1.1b by replacing\nOperation 1.1 with\nQuadReg \"Elevation\" \"Time\"\nFigure 1.1d tables inference results.\nIt is heartening to see that the quadratic model predicts negative elevation at\n7 seconds and that it has higher noise precision. Running the same cross-validation operation returns root mean square errors of 13.546m for elevation and 0.759s for time, both lower than the linear model. Comparing cross-validation numbers, we see evidence that a quadratic relationship more likely governs the structure behind our dataset than a linear relationship.\nOf course, bringing in knowledge of gravity, the quadratic model parameters\nmake more sense than those of the linear model; the inferred parameter a = \u22128.625 is reasonably close to Earth\u2019s \u22129.8m/s2 true force of gravity. The error is a result of the noisy data set, as the skeptical reader may verify by calculating elevation = 200\u2212 9.8 2 t2 for observed times t in Table 1.1 and comparing them to the means of the quadratic\n9 model\u2019s posterior distributions.\nModel evidence, the probability of observing the dataset under a model, does\nnot function as well as cross-validation for evaluating model fitness, as seen from the quadratic model evidence in Figure 1.1d being lower than the linear model evidence in Figure 1.1c. We demonstrate inference using model evidence in Section 3.1 and then switch to (5-fold, i.e. leave-n/5-out) cross-validation for other applications."}, {"heading": "1.2.1 Model Selection Matters: Overfitting and Consistency", "text": "Overfit models perform well on training datasets but do not perform well on other datasets generated from the same true structure. Formally, we say a model overfits if it performs better on a training set of data but worse on the distribution of all possible data sets than an alternative model [Mit97]. The Apple Freefall models, for example, would clearly overfit if their noise precisions are excessively high. They would predict tight distributions accurately on their training set that do not predict well on non-training set data.6\nModel comparison methods can be another source of overfitting. We strive\nto compare models in a way that does not favor specialized, overfit models. Luckily for the Apple Freefall example, model comparison by leave-one-out cross-validation is equivalent to model comparison by the Akaike information criterion (AIC) [Sto77], which has a desirable model complexity penalty [Boz87] that penalizes models with larger degrees of freedom. The complexity penalty enables realistic comparison between the simpler linear model and the more adaptive and thereby more likely to overfit quadratic model.\nWe also strive to compare models consistently, that is, to compare models\n6We ignore the case of modeling a perfectly deterministic true structure, in which case a (zero variance) Dirac delta distribution may perfectly capture a deterministic relationship between columns.\n10\nwith a guarantee that the probability of choosing the \u201ccorrect\u201d model closer to the true structure converges to 1 as the number of data points approaches infinity. Unfortunately AIC is an inconsistent model comparison method [Sha93], which means that AIC may not choose the best model even after training on infinite data observations. We employ leave-one-out cross-validation anyway for convenience in the Apple Freefall example. Consistent model comparison methods include leave-nv-out cross-validation, Bayesian information criterion (BIC) and Bayes factors for models with a finite number of parameters, and consistent methods exist for comparing nonparametric models too [JWC+10]."}, {"heading": "1.2.2 Bayesian Inference", "text": "We obtain cross-validation scores used to compare models via inference, characterized by its use of Bayes\u2019 Rule. Beginning with a prior predictive model7 that specifies Pr(row) for any row of data, we use Bayes\u2019 Rule to iteratively condition on each observed row of a dataset and form a posterior predictive distribution:\nPr(row | obsRow)\ufe38 \ufe37\ufe37 \ufe38 posterior pred. model := \u03b1Pr(obsRow | row)\ufe38 \ufe37\ufe37 \ufe38 obsRow likelihood Pr(row)\ufe38 \ufe37\ufe37 \ufe38 prior pred. model\nwhere obsRow is a row of observed (non-missing) data points and row is a row of data for which we wish to calculate its probability via a model that incorporates the data evidence in obsRow. For example, obsRow may be (time = 1, elevation = 196) and row may be (time = 2, elevation = 0). We hope our model assigns Pr(row | obsRow) infinitesimal probability.\n\u03b1 is a normalizing constant that ensures the posterior predictive model is a\n7Vocabulary note: a (prior or posterior) predictive model specifies a distribution over a row of data, whereas a (prior or posterior) model specifies a distribution over model parameters. We refer to the latter as parameter models to disambiguate. See [GvD07] for examples of each.\n11\nvalid probability distribution, i.e., that \u222b Pr(row) drow = 1, integrating over all possible rows:\n\u03b1 = 1\nPr(obsRow) = 1\u2211 row Pr(obsRow, row) = 1\u2211 row Pr(obsRow | row) Pr(row)\nThe reasoning is that if Pr(obsRow | row) > Pr(obsRow), then row increases obsRow\u2019s likelihood, which indicates we should raise Pr(row | obsRow) since obsRow is in fact observed. The \u2018<\u2019 version similarly decreases Pr(row | obsRow).\nIn the context of the apple freefall example, we tune our model from prior\nto posterior by adjusting the value of regression coefficients\u2014free parameters in the model for which we specify (usually uninformative) prior distributions:\nPr(param | obsRow)\ufe38 \ufe37\ufe37 \ufe38 posterior param dist. := \u03b1Pr(obsRow | param)\ufe38 \ufe37\ufe37 \ufe38 param likelihood Pr(param)\ufe38 \ufe37\ufe37 \ufe38 prior param dist.\nWe use parameters in our model to determine a distribution over possible rows of data by Pr(row) = Pr(row | param) Pr(param).\nThe inference algorithms we use in Tabular and ModelWizard are forms of\nvariational inference,8 a process that fits an easier-to-analyze probability distribution as close as possible to an arbitrary probability distribution like the one our apple freefall model represents. In exchange for approximation error in using the easierto-analyze distribution in place of the original distribution, we gain computational speedup in estimating the posterior distribution. Other forms of inference are exact inference,9 explicitly calculating an exact posterior distribution, and sampling-based inference,10 sampling from a posterior distribution without explicitly calculating it.\n8e.g., variational message passing, expectation propagation, ... 9e.g., variable elimination, junction tree, conjugate distributions, ...\n10e.g., rejection sampling, likelihood weighting, MCMC and its variants, ...\n12\nCross-validation calls inference as a subroutine by using the resulting probabil-\nity distribution from inference to make predictions on held out data. Well-performing models recover held out data accurately as indicated by low cross-validation error. Cross-validation performs inference in several rounds, holding out different sets of data randomly at each round. Thus, a model with high cross-validation score generally maintains performance when trained on different subsets of data, which is an excellent guard against overfitting as discussed in Section 1.2.\nIt is useful to imagine comparing models by cross-validation (or other model\nscoring measures such as model evidence) as an extension of Bayes\u2019 Rule where we select the best fitting model at the same time as the best fitting parameters for each model:\nPr(model, param | obsRow)\ufe38 \ufe37\ufe37 \ufe38 posterior model and param dist. := \u03b1Pr(obsRow | param)\ufe38 \ufe37\ufe37 \ufe38 param likelihood Pr(param | model)\ufe38 \ufe37\ufe37 \ufe38 prior param dist. Pr(model)\ufe38 \ufe37\ufe37 \ufe38 prior model dist.\nWe select model and its parameters with highest probability. Using the cross-validation method, this is the model (with parameters set by usual inference) that scores the least prediction error. Using the model evidence method, this is the model with greatest likelihood. Methods such as BIC, Bayes\u2019 factors and others discussed in Section 1.2 follow the Bayes\u2019 rule pattern similarly. The extension of the distribution over possible rows of data is Pr(row) = Pr(row | param) Pr(param | model) Pr(model), though after selection Pr(model) is a point mass distribution on the selected model.\nAn alternative to model selection is model mixing: taking a sum over possible\nmodels weighted by likelihood, instead of selecting one \u201cmode\u201d model with highest likelihood. Model mixing has more robustness to change in data when two models both explain the data well, since a small shift in data could change a selected model abruptly while a small shift in data will smoothly change a mixture of models\n13\n[HMRV99]. We use the model selection approach because understanding is one of our goals. It is easier to interpret a single model than a mixture of models, especially when we use our model as a proxy for true structure in nature.\nThere is a potential vocabulary conflict with statisticians, who define inference\nas \u201cthe process of deducing properties of an underlying distribution by analysis of data\u201d [UC08]. Recalling that true structure is a probability distribution over possible data, there is no conflict; true structure is the underlying distribution, whose properties we capture in models we create through model construction. \u201cStructure learning\u201d is also equivalent, as seen from the similar breakdown of a model into \u201cstructure\u201d and \u201cform\u201d in [KT08].\nIn this thesis, \u201cmodel construction\u201d refers to the entire process of finding a\nmodel close to true structure, \u201cinference\u201d refers to Bayesian inference procedures to answer queries and calculate posterior distributions by conditioning on data using Bayes\u2019 rule, and \u201cmodel exploration\u201d refers to determining model family/form, using the term \u201clearning\u201d whenever a choice is guided by dataset predictive power."}, {"heading": "1.2.3 Interactive Model Construction", "text": "In Section 1.2 we build our quadratic regression model interactively, creating other models like linear regression and comparing them as we naturally posit them through questions about the apple dataset. In this manner, model construction naturally fits into the scientific method. To illustrate the benefits of interactivity, imagine two other extremes of model construction: manual and automated.\nScientists adopting a manual model construction strategy take years to study\na scenario well enough to become expert and build a good model. The approach works but at high labor cost, whose fruits tend to be one-off, special purpose models applicable to a single problem.\n14\nScientists adopting an automated exploration strategy, using algorithms that\noutput good families, face tractability problems trying to search through the space of all models. For example, [CG01a] has O(n5) complexity searching within the family of discrete Bayesian networks, a small subspace of all possible models. Further, many automated algorithms such as multilayer neural network training generate \u201cblack box\u201d models, which predict accurately but offer no insight into why the models work.\nInteractivity delivers a happy medium between the two extremes. Data scien-\ntists will work hand-in-hand with computers to navigate the space of models, leveraging scientists\u2019 creative insight to guide model search and computers\u2019 automation to present statistics, model performance results and model suggestions as feedback. Scientist and computer together will craft understandable models on multiple data sources faster than either can alone."}, {"heading": "1.3 Probabilistic Programming", "text": "In probabilistic programming, machine learning models take the form of programs. Users write code that samples data points, effectively defining a probability distribution over possible data points. The model specified as a program gains power from programming language constructs, such as functional abstraction and control flow by iteration and conditionals. Compilers use top level probabilistic program code to synthesize inference code. See [GHNR14], [DRK13] and [Goo13] for different flavors of introduction to probabilistic programming.\nOne success story demonstrating potential benefits for probabilistic program-\nming is on creating a seismic monitoring model to help detect nuclear tests for the UN Preparatory Commission for the Comprehensive Nuclear-Test-Ban Treaty (CTBTO) One of the CTBTO\u2019s models has on the order of 28,000 lines of code in C [Fis13].\n15\nThat model was rewritten in the probabilistic programming language BLOG, delivering similar accuracy at 25 lines of code [Rus14]. Reduction in development time and expertise barriers added considerable value."}, {"heading": "1.3.1 Tabular and Infer.NET", "text": "Tabular is a probabilistic programming language where models consist of probabilistic annotations on database schemas written inside spreadsheets [GGR+14, GRS+15]. Tabular\u2019s motivation is the idea that data scientists would find it easier and more intuitive to specify their model by marking up the schema of their dataset, as opposed to specifying their model in a standalone language separate from their dataset. Data scientists do need an understanding of Tabular\u2019s primitives, which are designed to be easily readable given background knowledge in statistics and probability.\nThe Tabular compiler compiles Tabular models into Infer.NET probabilistic\nprograms [MWGK12], which can then be run in an Infer.NET inference engine to infer posterior distributions and answer queries. Infer.NET supports variational message passing, expectation propagation, and Gibbs sampling, all of which are accessible from Tabular."}, {"heading": "1.4 ModelWizard: Interactive Model Construction for Tabular", "text": "ModelWizard is a domain-specific language, embedded in F#, for interactively constructing Tabular models. Users write scripts in ModelWizard that incrementally construct Tabular models, progressing from an initial \u201cdo-nothing\u201d Tabular model toward an inference-ready Tabular model, one operation at a time.\nModelWizard is inspired by the difficulty of writing Tabular models, just as\nTabular is inspired by the difficulty of writing Infer.NET models. ModelWizard\u2019s goal\n16\nis to reduce the time it takes users to build a Tabular model from scratch. We are especially interested in the case where the user does not know what the final model is and is still exploring the space of models.\nIn this thesis we present the design and implementation of ModelWizard as\nan embedded DSL, together with case studies in ModelWizard\u2019s use. The following subsections highlight key features in ModelWizard\u2019s design:"}, {"heading": "1.4.1 Concurrently Refining Data and Model", "text": "ModelWizard\u2019s operations modify both a dataset\u2019s machine learning model under construction and the dataset itself. This is different than the traditional \u201cpre-processing phase\u201d approach in machine learning and data mining literature, to perform all dataset-modifying operations prior to constructing a model.\nOperations in ModelWizard that traditionally occur during pre-processing are\nchanging types of columns, creating new tables of unique values, remapping values to point to another table with a foreign key relationship, and capturing functional dependencies by moving data between tables, a key step in table normalization. Other operations we imagine as extensions are numeric transformations like taking square roots, full table normalization, data compression by taking principal components and outlier detection and cleaning, to name a few. We also imagine post-processing extensions such as model validation and visualization.\nWhile refining data and model at the same time adds flexibility to the model\nconstruction process, we are not yet certain whether it is always helpful. Data scientists may find performing pre-processing activities prior to modeling conceptually easier. User case studies will help illuminate an answer.\n17"}, {"heading": "1.4.2 Composable Model Primitives", "text": "Rather than construct complicated machine learning models all at once, we construct models incrementally via sequences of primitive operations. Think of primitives as bricks and F# code as glue. Composing primitives and F# together cements higherlevel, derived operations we think of as pillars and walls. The beauty of derived operations is that we may abstract them, treating them as a reusable single piece by forgetting the bricks and glue that compose them. We construct the mightiest models, machine learning castles indeed, composing all available operations in the blueprint of F#.\nWe find ModelWizard\u2019s primitive operations naturally capture common ma-\nchine learning paradigms that when composed into derived models, offer easy access to many model families, such as clustering and Naive Bayes. Creating models as compositions lends understanding into how the models work, since we can \u201copen the box\u201d of a derived operation and inspect its component operations and code to see how it works. At the same time, abstraction allows analysts to consider derived models without worrying about their low-level details by using the models as \u201cclosed black boxes.\u201d\nFor example, operations 1.1 and 1.2 that construct the Apple Freefall model\nare derived operations, abstracting details of regression and automating choice of model by deducing that time is a real-valued variable. Users who want to \u201ddig in,\u201d understanding and modifying the derived operations, may do so by inspecting their source code.\n18"}, {"heading": "1.4.3 Safety in Model Construction", "text": "Tabular has typing rules in terms of the domains of data columns and distributions to ensure a schema\u2019s probabilistic annotations make sense and that data actually conforms to the annotated schema. A state, consisting of a dataset plus a Tabular model, is valid provided it contains no naming conflicts, is well-typed, that its schema and data satisfy asserted relational properties like primary/foreign key, and that no cyclic references are present.\nUsers developing models directly can easily construct invalid states. In con-\ntrast, ModelWizard ensures only valid states are constructable, throwing exceptions when an operation cannot construct a valid state."}, {"heading": "1.5 Thesis Outline", "text": "Chapter 2 introduces ModelWizard language syntax and design. We cover core types and OpMonad, ModelWizard\u2019s operation builder, followed by a walk through operations grouped by topic. See Appendix B for a concise ModelWizard operation API.\nChapter 3 opens with simple model construction on small datasets and pro-\ngresses to advanced model construction on real-world data. We cover Bayesian networks, Naive Bayes classifiers, a hybrid model with functional dependencies and clustering for collaborative filtering. Readers seeking further intuition may skip Chapter 2 to view Chapter 3\u2019s examples.\nChapter 4 concludes by presenting desirable extensions and discussing Model-\nWizard\u2019s fit in the data science world.\n19\nChapter 2 ModelWizard Language\nWe illustrate in this chapter the language syntax and design of ModelWizard, a domain-specific language embedded in F#. We cover core types, detail how to build operations safely with OpMonad, survey our current API of operations and show how to use them in model construction. Readers unfamiliar with F# syntax may find [SGC12] a useful reference."}, {"heading": "2.1 Types: State, Operation and Safety", "text": "ModelWizard\u2019s core data type is State: a pair consisting of a dataset and a Tabular model annotating that dataset. We call the dataset Data and the Tabular model Schema. type TableName = string type ColumnName = string type DataTable =\n{tablename:TableName; colnames:ColumnName []; data:System.IComparable [,]}\ntype Data = DataTable list type Schema = //... type State = Schema * Data type StateOp <\u2019R> = (State -> \u2019R * State)\nData is a relatively thin wrapper around an in-memory array of tables. A table\nhas a name, an ordered array of column names and a two-dimensional data array,1 of\n1The System.IComparable[,] type on Data indicates Data values have a total order. Do not think deeply into the total ordering; it is a pragmatic requirement that makes working with F# libraries easier.\nThe total order may be unrelated to the actual meaning of data values. For example, string values are compared lexicographically such that \u2018blue\u2019 \u201cis less than\u201d \u2018red.\u2019 For modeling purposes we may still treat color as a nominal column, that is, one whose data values have no semantic total order\n20\nwhich the first dimension indicates row number and the second dimension indicates column. Missing values have value null.\nSchema is analogous to the Tabular Schema type [GGR+14]. Its first component\nis a representation of a classical relational database schema, composed of a list of named tables, each of which has a list of named and typed columns. Its second component is annotations on columns with probabilistic model expressions that define distributions over the data entries corresponding to the schema columns. The Tabular compiler compiles Schemas into code in a probabilistic programming language, with which we run inference using the language\u2019s inference engine. Thus, by manipulating Schema in ModelWizard, we manipulate the machine learning model that we use in inference.\nStateOp<\u2019R> is an operation on State, returning a new State plus additional\ninformation of type \u2019R. \u2019R could include information about the new state such as outlier data points or oddly fitting model components, information on other States worth considering, or anything else the user writes in the StateOp function. The new State may be the same as the original State."}, {"heading": "2.1.1 Defining \u201cValid\u201d States", "text": "The State type by itself offers no guarantee that a State is valid. Before we may provide any notion of validity, let alone the safe construction of valid states, we must define what we mean by \u201cvalid\u201d and \u201csafe.\u201d\nWe call a State valid when its component Schema is valid on its own and when\nthe component Schema conforms to the State\u2019s component Data in the categories listed below. Safety refers to a process of constructing a state that preserves state validity, or informally, that \u201cvalid states in yield valid states out; garbage states in and can be semantically compared only for equality.\n21\nyield garbage.\u201d\nThe following categories outline the main kinds of Schema validity and Schema-\nData conformance that we check in the ModelWizard implementation:\n1. Naming conflicts. Table names must be unique, column names must be unique\nwithin each table, and table and column names present in a Schema must correspond to tables and columns in the Data it is paired with. ModelWizard always checks for naming conflicts, especially when parsing arguments that create an operation and executing operations that create new columns or tables.\n2. Type checking. Modeled columns in a Schema (that is, columns with Tabular\nmodel expressions) must have a type that can be generated by the model on that column. It would not make sense, for example, that a column with a real type have a Discrete model expression. Data values must in turn match column types in the Schema they are paired with. ModelWizard performs type checking at runtime when there is a chance that data may not match a new type, explained further in Section 2.4.\n3. Primary and foreign key correctness. ModelWizard guarantees columns with the\nannotation \u2018pk\u2019 are unmodeled and have no duplicate values, such that each value in a primary key column is unique. Further, (foreign key) columns whose domain is that of another (primary key) column may only assume values that are a subset of those in the primary key domain. Section 2.4 and 3.3.1 introduce the primitive operations CreateTableUniques, Link and Exact that carefully check these requirements.\n4. Cyclic dependencies. Tabular requires that columns and tables only refer to pre-\nviously declared columns and tables. We enforce this requirement in Section 2.5.3\n22\nvia Schema-reordering operations.\nChecking state validity at runtime ensures that states are actually valid, at\nthe sacrifice of scalability to big data or big models. While unimplemented, we lower barriers to scalability by running checks only when necessary, running checks lazily in background threads or using other techniques discussed in Section 4.1.\nThe following notions of state validity are outside the scope of ModelWizard:\n1. Detecting data valid according to type and syntax but questionable ac-\ncording to data likelihood. For example, an int-type column with values {1, 3, 1, 1, 2, 4, 1, 9001, 2, 3, 1} is valid syntactically and type-wise, but the value 9001 is a huge outlier worth questioning. However, it would be straightforward to write an operation that returns unlikely Data values.\n2. Suggesting corrections to data values with syntax or type errors. For example,\nsuppose we have the data value \u201815B\u2019. If present in a column containing with many other integers, we could propose that the \u2018B\u2019 is a typo, accidentally inserted. We may also interpret the value as hexadecimal and propose replacing \u201815B\u2019 with the decimal \u2018347\u2019. With supporting evidence from other columns, a third plausible alternative is that \u201815B\u2019 is the concatenation of two columns, such as an airline seat reservation with row number 15 and seat position B for \u201caisle.\u201d\nWe refrain from heroically suggesting remedies for invalid states. More extensive data cleaning is the subject of active research, as seen in the DataTamer [SBI+13] and Potter\u2019s Wheel [RH01] systems among others. We would welcome integrating data cleaning techniques with ModelWizard in order to create a better end-to-end data analytics and model construction platform.\n3. Detecting models valid according to type and syntax but questionable according\n23\nto data likelihood. For example, nothing in our system prevents a user from placing a Gaussian distribution with mean 10,000 and variance 1 on a column with values {1, 5, 2, 2}. However, we do provide features that make model scoring and comparison easier, such as running inference, computing data likelihood scores and running cross-validation. We welcome contributions that perform model scoring in the background, alerting the user if he performs an operation that leads to a very unlikely model.\n4. Guaranteeing models are inferable by a particular back-end inference engine. See\nSection 4.2 for a discussion on model inferability.\nDefining and enforcing validity as outlined above allows us to make the guaran-\ntee that every model constructed by ModelWizard has a valid Tabular counterpart and will pass Tabular\u2019s type checking, a strong enough guarantee to be useful in practice but not so strong as to result in intractable model checking."}, {"heading": "2.1.2 ValidState and ValidOp Types", "text": "We use the ValidState type, presented below, to indicate a State ModelWizard guarantees valid. The below code is part of an F# signature file, whose purpose is to declare the types an F# library implementation file exports. Because ValidState\u2019s constructor is missing from the signature file, there is no direct way to construct ValidStates except through other methods that handle ValidStates in a controlled manner, to guarantee preservation of State validity. The function unwrapVS enables de-construction: retrieving encapsulated State from a ValidState. type ValidState // Constructor hidden !! val unwrapVS : ValidState -> State val UNSAFE_ValidState : Schema * Data -> ValidState\n24\nFor development convenience and expert users, we include an extra function\nUNSAFE ValidState which escapes ValidState\u2019s safety guarantee and allows direct construction of ValidStates. Writing UNSAFE in all capitals flags users to be especially careful to only construct truly valid ValidStates when calling this function.\nValidOp is a type, presented below, for a StateOp guaranteed to preserve state\nvalidity. In fact, ValidOps throw exceptions when run in a way that would create an invalid state and possibly corrupt the user\u2019s data. The user may run ValidOps on ValidStates by runValidOp and on plain old States by unwrapVOP. UNSAFE ValidOp is similar to UNSAFE ValidState. type ValidOp <\u2019R> // Constructor hidden !! val runValidOp : ValidOp <\u2019R> -> ValidState -> \u2019R * ValidState val unwrapVOP : ValidOp <\u2019R> -> StateOp <\u2019R> val UNSAFE_ValidOp : StateOp <\u2019R> -> ValidOp <\u2019R>\nModelWizard\u2019s principal workflow is to load a ValidState from an original\nData source (such as an Excel workbook) with a default \u201cdo-nothing\u201d Schema, perform a series of ValidOps that refine ValidState incrementally, and end with a final ValidState guaranteed valid. We expect users will create many intermediary ModelWizard scripts before settling on a final script that creates a final model. Intermediary scripts may return extra information \u2019R, such as inference performance or the names of columns in the range of a functional dependency. See Section 4.3 for a sketch of how we envision users editing scripts interactively."}, {"heading": "2.1.3 OpMonad: ValidOp Computation Expression", "text": "OpMonad is an F# computation expression class to create an intuitive syntax for chaining together ValidOps into a compound, derived ValidOp. As required of a proper monad, OpMonad respects the monad laws of left and right identity and associativity [Wad92, Section 2.10]. Code Listing 2.1 lists selected type signatures\n25\ntype OpMonad =\nmember Bind: ValidOp <\u2019R> * (\u2019R -> ValidOp <\u2019N>)->ValidOp <\u2019N> member Return: \u2019R -> ValidOp <\u2019R> member Zero: unit -> ValidOp <unit > member Combine: ValidOp <unit > * ValidOp <\u2019R> -> ValidOp <\u2019R> member Delay: (unit -> ValidOp <\u2019R>) -> ValidOp <\u2019R> member ReturnFrom:ValidOp <\u2019R> -> ValidOp <\u2019R> member For: seq <\u2019a> * (\u2019a -> ValidOp <unit >)->ValidOp <unit >\nval OPM : OpMonad // shorthand for an OpMonad instance // Implementation: member __.Bind(vop , f) = VOP <| fun s ->\nlet res ,newst = (unwrapVOP vop) s in unwrapVOP (f res) newst\nmember __.Zero() = VOP <| fun s -> ((),s) member __.Return(x) = VOP <| fun s -> (x,s) member __.ReturnFrom(x) = x member this.Combine(vop1 , vop2) = this.Bind(vop1 , fun () -> vop2) member this.Delay(f) = this.Bind(this.Return (), f)\nCode Listing 2.1: Selected OpMonad type signatures and implementations\nand implementations.\nThe key notion to understand OpMonad is how let! statements translate\nto monadic bind. The following guidelines, in which vop is a placeholder for any ValidOp, outline the translation:\n\u2022 let! r = vop translates to let r,nextState = vop curState\n\u2022 do! vop is a synonym for let! () = vop for a ValidOp encapsulating StateOp <unit>.\n\u2022 return x creates a ValidOp that returns x without modifying state.\n\u2022 return! vop evaluates a ValidOp directly.\n\u2022 |> and <| are F# operators for function application.\nPlease see [PS12] for a deeper review of monads as F# computation expressions.\n26\nRefer also to Appendix A for an example translation of an OpMonad expression into an F# quotation calling OpMonad\u2019s computation expression components."}, {"heading": "2.2 Escape to F#: GetState", "text": "GetState links ModelWizard operations to F# code by returning a copy of an executing operation\u2019s current schema and data, exposing them to F# code. Operations may subsequently inspect column names, types and models as well as their underlying data. Nearly every derived operation includes a call to GetState.\nThere is no corresponding WriteState operation except via UNSAFE operations.\nIncluding one would allow a user writing derived operations to easily create an invalid operation, destroying OpMonad\u2019s guarantees. Thus, user code may inspect state but not alter state, except through ModelWizard\u2019s suite of primitive operations."}, {"heading": "2.3 Machine Learning Base Models", "text": "ModelWizard includes two base machine learning models: Gaussian and Discrete2 distributions. Both have conjugate priors: Gamma on the precision of a Gaussian, another Gaussian on the mean of a Gaussian, and Dirichlet on the probability vector of a Discrete. Conjugate priors allow inference of a Gaussian and Discrete distribution\u2019s parameters.\nTabular represents Gaussian and Discrete conjugate distributions by the col-\numn markup CGaussian and CDiscrete. Figure 2.1 presents the markup\u2019s meaning as a procedure to draw from their represented probability distributions. N and MeanPrec are hyperparameters that may be specified in the Tabular model referencing\n2Throughout this thesis, \u201cDiscrete\u201d refers to a Categorical distribution, also known as a generalized Bernoulli distribution. The range of a Discrete distribution are the integers 1 to N for fixed distribution parameter N. An integer is sampled from a Discrete distribution with probability determined by an N-dimensional probability vector, stored as a distribution parameter.\n27\nCDiscrete(N= ):\n1. Draw table-level ~p from an N-dimensional Dirichlet distribution with concentration ~\u03b1 = ~1. 2. For each row, draw from a discrete distribution with probability vector ~p.\nCGaussian(MeanPrec= ):\n1. Draw table-level \u00b5 from a Gaussian distribution with mean 0 and precision (inverse of variance) MeanPrec. 2. Draw table-level \u03c4 from a Gamma distribution with shape and scale 1.0. 3. For each row, draw from a Gaussian distribution with mean \u00b5 and precision \u03c4 .\nFigure 2.1: Tabular Conjugate Distributions\nthe conjugate models, which affect the prior distribution from which conjugate prior distribution itself is drawn. We typically assign uninformative priors to reduce bias by choice of prior in inference, though one can assign informative priors just as easily. CGaussian has in fact, another hyperparameter MeanMean that is the table-level \u00b5 in Figure 2.1 which comes into play when MeanPrec is sufficiently large.\nWe create these distributions on a column, independent of all other columns,\nin ModelWizard by the primitive operations ModelDiscrete and ModelGaussian. ModelDiscrete may be performed on any column of type upto(N) or link(T). Link(T) columns are essentially upto columns with N set to the number of rows in table T. ModelGaussian may be performed on columns of type real or int, converting the column to type real in the int case. We enforce type constraints by throwing an exception if one operates on a mismatching type.\nTo make it easier to independently model a column, we use the derived oper-\nation Model to place a \u201cdefault\u201d distribution appropriate for the column\u2019s type by means of a call to ModelDiscrete or ModelGaussian. Columns of type string, the raw text type, are handled by a TypeNominal operation described in the next section.\nFuture work may extend the \u201ctwo-level\u201d Gaussian and Discrete conjugate dis-\ntributions to higher levels of hierarchical models. This can be done partially for the\n28\nprior on the mean of a Gaussian distribution, since the prior is itself another Gaussian distribution. A Gaussian distribution\u2019s precision (alternatively, variance) is more tricky. As for the Dirichlet distribution, the prior of a Discrete distribution, placing a prior on a Dirichlet requires the nonparametric Dirichlet process [Teh10], which is historically hard to perform variational inference on because the number of variational parameters is nonconstant. Recent work has had more success [BJ04].\nWe also leave integrating other base distributions into ModelWizard as future\nwork. Good candidates are Gamma, Wishart, Poisson and Binomial, as these distributions all have existing representations in Tabular. Multivariate Gaussian is a particularly excellent candidate as it creates a new way to couple continuous columns alongside polynomial regression. We do not include the Beta and Bernoulli distributions as they are special cases of the Dirichlet and Discrete distributions, respectively. It may also be worth investigating other prior distributions; for example, Gelman argues that a Gaussian distribution truncated to only place probability mass on positive values tends to perform better as a prior on the variance of a Gaussian than the inverse-Gamma we use in CGaussian [Gel06].\nA challenge to additional base distributions is their level of inference support\nin Infer.Net. Using other back-end inference engines such as Stan [Sta14] or R2 [NHRS14] may add diversity to other base distributions\u2019 level of inference support."}, {"heading": "2.4 Typing Data, Tables as Nominal Column Domains", "text": "Columns in Tabular can have type int, real, link(T), upto(N) and string. The domain of upto(N) is the integers [0, N \u2212 1]. One can imagine a bool type as upto(2). The domain of link(T) is all rows of table T.\nWe change column types by the operations TypeUpto, TypeReal and\n29\nTypeNominal. These operations are checked such that they will not place an improper type on a column, and they are only allowed to act on unmodeled input columns. For example, TypeReal will check the data to ensure that the target column\u2019s data really are real-valued when there is a chance the data may not be, say when converting from type string as opposed to type int. TypeUpto and TypeLink act similarly. TypeInt is also implemented but not discussed here or in the API as no machine learning models use the int type, taking upto or real in its place.\nString types have special handling. One could use the former three type oper-\nations to convert a string column that contains numeric data, but it is more common that a string column has nominal data, that is, data whose values are only comparable by equality like {red, blue, red, green}.\nWe handle a nominal column C in table T with the primitive operation\nCreateTableUniques T [C]. It creates a new Tabular table (call it NT) whose rows contain the unique values of column C. Thus, NT\u2019s rows form the domain of C. We bind the domain of C in T to NT by the operation Link NT [C,C] T C, which changes column C to type link. Read the Link operation as \u201cmap column C in foreign table T to column C in primary table NT and store the corresponding row matches in foreign table T\u2019s column C.\u201d\nComposing CreateTableUniques with Link forms the derived operation\nTypeNominal, an operation that effectively types a string column as a nominal column. It is also possible to run TypeNominal on int and real columns, effectively treating the numbers in those columns as nominal labels instead of numeric values. Running TypeNominal on an upto column is redundant and disallowed.\nA promising future direction is handling ordinal columns. Ordinal data have\nranking in addition to equality comparison, like the data {first, second, third}. One way to represent ordinal data is by logistic regression, supposing that ordinal values\n30\nare delineated by continuous thresholds and then more easily inferring a continuous number and seeing which \u201cbucket\u201d between thresholds the continuous number lies in. \u201cIndicator\u201d columns perform the logistic regression in Tabular to indicate when a column is \u201cgreater than\u201d its threshold value. While we have built this model in Tabular, we have not yet represented it as an operation due to inference troubles related to algorithm support in Infer.NET."}, {"heading": "2.4.1 Type Inference", "text": "Having access to the data at the time of modeling and interleaving F# code with operations enables typing automation. We can create TypeInfer, a derived operation that infers the type of a column from its data.\nWe design TypeInfer to first apply a heuristic that columns with less than\n5% unique values are likely to be nominal columns, because they contain significantly high redundancy. For example the data {1, 1, 1, 0, 1, 0, 0} is more likely nominal than continuous, with 0 and 1 indicating presence or absence of some label or property. Users may call TypeReal to override this heuristic.\nIf a column contains greater than 5% unique values, then TypeInfer attempts\nto type as an int, then to type as a real if the data contains a non-integer, then to type as a nominal as a last resort.\nWe extend TypeInfer to automatically type all of a table\u2019s input columns\nby the derived operation TypeInferTable. We show a simplified version of TypeInferTable\u2019s implementation in the OpMonad expression of Appendix A.\n31"}, {"heading": "2.5 Coupling Columns", "text": "The previous two sections introduce how to type columns and place base machine learning models on a column independent of all other columns. We now consider coupling models of columns such that one column depends on another."}, {"heading": "2.5.1 Continuous Coupling: Noisy Regression", "text": "Our prelude apple freefall example in Section 1.2 introduces LinReg and QuadReg as two operations to couple continuous columns by polynomial regression. The user specifies the domain and the range of the regression as input to the operations. Polynomial regression is a generic, \u201cdefault\u201d way to couple continuous columns without special knowledge of the structure of a dataset.\nWe add noise to polynomial regression in order to place a non-point-mass\ndistribution over the range column when the domain column is deterministic (e.g., when the domain column is unmodeled with markup input). The noise has fixed mean 0 so that we may infer the constant term properly (\u2018b\u2019 in linear regression, \u2018c\u2019 in quadratic), or else the noise and constant term would conflict in explaining regression bias. Inferred noise variance corresponds to regression error magnitude.\nLinReg and QuadReg rely on the function tables in Figure 1.1a and Figure 1.1b\ndefined above the main tables to implement regression in an abstracted manner. Function tables use a special syntax in Tabular to implement a generic function that involves table-level inputs with default parameters (\u201chyper\u201d columns), additional table-level columns determined by the inputs (\u201cparam\u201d columns), concrete row-level inputs from a table (\u201cinput\u201d columns), additional row-level columns that depend on the inputs (\u201clatent\u201d columns), and a row-level \u201coutput\u201d column that returns a function\u2019s result to the table that calls it. \u201cTable-level\u201d refers to columns that have\n32\nthe same value for every row in a table, whereas \u201crow-level\u201d refers to columns whose value may differ for each row of data. Tabular\u2019s compiler inserts function tables inline into the non-function tables that call them, a process similar to macro expansion. The insertion should occur in a way that does not conflict with names in the calling table.\nUnfortunately, Tabular syntax forbids function tables that reference other func-\ntion tables, recursively or not. If we had the ability to conduct recursive calls, then we could implement polynomial regression in a recursive, more concise manner. Imagine the operation PolyReg N rngcol domcol to do N-ary polynomial regression between domain column domcol and range column rngcol. We may implement PolyReg N by adding an Nth degree term in the function and then recursively calling PolyReg (N-1) for the rest of the regression, down to PolyReg 0 base case. Even without the recursive implementation, it is possible to implement PolyReg N without individually using the operations LinReg, QuadReg, and so on by using a symbolic algebra algorithm to create an Nth degree regression as a function table in F# code, though we have not presently implemented this in ModelWizard.\nThe regression operations place a distribution over the range column deter-\nmined by the domain column, effectively meaning that the domain column value determines the range column. If the domain column is modeled (i.e., has a probability distribution as opposed to an input column), then the regression on the range column distribution is a function of the domain column distribution."}, {"heading": "2.5.2 Discrete Coupling: Index", "text": "Indexed models are a natural way to couple Discrete machine learning models. We Index a (discrete or continuous) modeled range column\u2019s distribution by a discrete (modeled or unmodeled) dimension D domain column by creating an array of copies of the range column\u2019s distribution, one for each of the D domain values, and selecting the\n33\ncopy according to the domain column\u2019s corresponding value. The copy distributions are identical to the original range column distribution, except that the copies have their own parameters.\nThe indexed range column may have any model. We use the notation\nModel[DomCol] to indicate indexing the range column\u2019s model by discrete domain column DomCol. Section 3.1 and 3.2 show models where a Discrete and Gaussian distribution, respectively, are indexed by a discrete variable. It is also possible to index a LinReg or QuadReg model by a discrete variable. Such an indexing would make sense in the context of Section 1.2\u2019s apple freefall example if we had data for apples on different planets (each with their own force of gravity). We could index the regression on apple elevation by the planet the apple is on.\nFigure 2.2a graphically depicts one of the indexed distributions from Sec-\ntion 3.1\u2019s Bayesian network in terms of gates [MW09] and plates [Bun94]. PVec is the probability vector governing sampled outcomes of rain on each row. Alpha is the\n34\nconcentration vector for the Dirichlet distribution used as the prior for the Discrete distribution. Alpha is a vector of 1s when left unspecified. N=2 sets the dimension of the vectors at 2, which is equal to the number of states of rain (a boolean-valued column, typed as upto(2)).\nThe dashed box is a plate, which means that its contents are replicated; there is\na distinct PVec for every value of cloudy (also of type upto(2), though the dimension of the domain column need not equal the dimension of the range column in general). Inputs to the plate (Alpha) are sent to each replicated PVec. The square inside a box is a gate, which determines the output of the plate (rain) by the indexing variable cloudy. Figure 2.2b shows the \u201cunfolded\u201d version of the gate and plate.\nA helpful way to think about the notation is that there are distinct PVec\nparameters for the Discrete distribution governing rain. The distribution of cloudy determines the weights with which we use each parameter. Thus, we say that cloudy determines the distribution over rain.\nIn terms of operations, Index is a simple primitive operation. Index verifies\nthat the domain column is discrete and the range column is modeled, throwing an exception if either condition does not hold, and replaces the range column\u2019s distribution by an indexed version. Syntactically, however, Index is more complicated due to the occasional need to index distributions across table links. The user may specify a list of table links to traverse. See Figure 3.9 in the Applications chapter for an example where movie ratings are indexed across a table link to a cluster column in a user table and a title table.\nIndex and LinReg/QuadReg offer venues for a discrete column to determine\na discrete column, for a discrete column to determine a continuous column, and for a continuous column to determine a continuous column. We do not have a way for a continuous column to determine a discrete column. As mentioned in Section 2.4\n35\nabove, we may create a way in the future by implementing a logistic regression operation. With that future addition, we would have \u201cdefault\u201d ways to couple columns of any type, a valuable ability for users experimenting with dependencies in a dataset."}, {"heading": "2.5.3 Bookkeeping: Reordering Tables and Columns", "text": "With the power of the coupling operations LinReg, QuadReg and Index to create dependencies between columns, we run the risk of creating cyclic dependencies: one column depending on another that transitively depends on the first. Cyclic dependencies are inexpressible in Tabular; columns may only refer to columns declared above them in a Tabular model. In other words, the overall model must fit into a directed acyclic graph.\nWe guard against cyclic dependencies by checking the set of columns that a\ncolumn depends on at operation-runtime. There are three scenarios. If a dependency can be created without changing column order, it is done. If a dependency can be created that requires changing column order, then a ReorderColumns operation is invoked internally. If a dependency cannot be created due to cyclic dependencies (sometimes involving many columns), then an exception is thrown.\nThe same caution applies to tables. The Link operation, for example, creates\na table dependence that may require reordering tables. Cyclic table references are not allowed."}, {"heading": "2.6 Latent Columns", "text": "A common modeling paradigm is to create latent columns not present in the original dataset in addition to the concrete columns present with data values. Derived from concrete columns, latent columns expose intermediary states of probabilistic modeling\n36\nto the user, often increasing model understandability. For example, latent columns inside the function tables of LinReg and QuadReg increase their interpretability by delineating the addition of noise from the regression.\nNewColumn is an operation that may create an UptoColumn or a LinkColumn.\nSection 3.4 demonstrates UptoColumn by creating new cluster columns for tables. The number of clusters is given as an argument to UptoColumn. We see these columns as output in Figure 3.9, since we can treat new columns as concrete columns with all missing data. Section 3.3 demonstrates LinkColumn in the ExactInfer operation, where links are created from tables with range columns to tables with domain columns that exactly determine the range columns."}, {"heading": "2.7 Excel, Inference and Advanced Operations", "text": "Operations interfacing with Excel are essential to every application so that we can load and save data and models. ModelWizard reads data from an Excel workbook data model using bindings in the .NET framework API. Luckily, Excel\u2019s data model holds types on columns that are enforced by Excel\u2019s processing model. ModelWizard therefore trusts the type of a column indicated by Excel by using an UNSAFE ValidState operation when constructing a state directly from an Excel workbook. Writing data is similar, except that data is first written to a user-named Excel spreadsheet and then added to the containing workbook\u2019s data model. Tabular models are read from and written to spreadsheets directly.\nInference operations call the Tabular compiler. We implement them by calling\nGetState, converting ModelWizard Data and Schema into Tabular Data and Schema and executing the Tabular compiler to create an Infer.NET probabilistic program. The Infer.NET engine returns inference results, which we parse and return to the\n37\ncaller.\nMore advanced machine learning operations including NaiveBayes, Exact,\nExactInfer and Inferno are described in the next chapter: Applications.\n38\nChapter 3 Applications\nIn this chapter we illustrate the model construction process with ModelWizard through progressively more complex examples."}, {"heading": "3.1 Small Model Search: Discrete Bayesian Networks", "text": "In this section we demonstrate how one constructs an acyclic discrete Bayesian network in ModelWizard, and how to search over and select different models using inference to score them. Our model search example is small, selecting between two Bayesian networks that differ in one arrow. One may extend it to larger search spaces bearing in mind computational feasibility.\nSuppose we want to model Figure 3.1\u2019s Bayesian network, modified from\n[Mur01] adding uncertainty in the arrow between wet grass and rain. Does wet grass determine rain\u2019s distribution, rain determine wet grass\u2019s distribution, or are wet grass and rain independent? We answer by building and comparing each candidate model, using data likelihood as selection criterion.\nWe begin with a default all-input model and dataset that performs no predic-\n39\ntion, shown in Figure 3.1a, and walk through the operations in Figure 3.1 to construct a predictive model. Figure 3.2 tables intermediate models.\nA natural first step is to model each column independently, in this case using\nModelDiscrete since all columns are nominal. ModelDiscrete allows inference of each nominal value\u2019s probability (see Section 2.3 for details) and prediction of missing values, as with rain\u2019s missing value in Figure 3.1a. Recall that Tabular inference consists of training a given model on observed (non-missing) values in the input dataset to form a posterior model, outputting posterior model parameters that in this case form conditional probability tables, and outputting marginal distributions from the posterior model on each missing value, conditioned on the observed values in the same row of that missing value.\nIndex correlates two columns. Taking Index \"tmain\" \"rain\" \"cloudy\" (read\n\u201cIndex the distribution of rain by cloudy\u201d), for example, Index creates separate distributions on rain for each value of cloudy, effectively creating a conditional probability table where rain depends on cloudy. Rain\u2019s Tabular model suffix \u2018[cloudy]\u2019 indicates the new dependence.\n40\nlet s0 = readValidState inputFile let le1 ,s1 =\ns0 |> runValidOp (OPM { do! ModelDiscrete \"tmain\" \"cloudy\" do! ModelDiscrete \"tmain\" \"rain\" do! ModelDiscrete \"tmain\" \"sprinkler\" do! ModelDiscrete \"tmain\" \"wetGrass\" do! Index \"tmain\" \"rain\" \"cloudy\" [] do! Index \"tmain\" \"sprinkler\" \"cloudy\" [] do! Index \"tmain\" \"wetGrass\" \"sprinkler\" [] return! ScoreLogEvidence }) //Ok: -2286.50\nlet le2 ,s2 =\ns1 |> runValidOp (OPM { do! Index \"tmain\" \"rain\" \"wetGrass\" [] return! ScoreLogEvidence }) // Better: -2128.71\nlet le3 ,s3 =\ns1 |> runValidOp (OPM { do! Index \"tmain\" \"wetGrass\" \"rain\" [] return! ScoreLogEvidence }) //Best: -1963.57\nlet candidates= List.zip [le1;le2;le3] [s1;s2;s3] let bestle ,bestmodel = List.maxBy fst candidates\nCode Listing 3.1: Script to build and select Figure 3.1\u2019s most likely model\n41\nModel candidates s1, s2 and s3 follow from further indexing. Rain and wet-\nGrass are independent in model s1, rain indexes the distribution of wetGrass in model s2, and wetGrass indexes the distribution of rain in model s3. We use the ScoreLogEvidence procedure to compile the current Tabular model into Infer.NET inference code, execute inference and return model log evidence scores under 1000 rows of input data, storing scores in le1 through le3.\nAn unusual paradigm in the example is the reuse of State s1 in the construc-\ntion of s2 and s3. Many monads are \u201csingle-track\u201d in the sense that they disallow storing and reusing internal state because it could lead to undesirable operations, such as saving a pointer to a file, then deleting the file, then attempting to access the file through the pointer. This kind of behavior is also disallowed in OpMonad since OpMonad expressions evaluate to a single ValidState. We had to jump outside the OpMonad construct into general F# let bindings in order to save state s1 and use it in the construction of two different states bound at the top level. We retain \u201csingle-track\u201d enforcement in OpMonad for efficiency, since copying State in the current implementation entails copying Data within that State eagerly. Section 4.1 discusses delayed approaches to updating Data.\nWe find model s3 is most likely because le3 is greatest, fulfilling our intuition\nthat rain more likely causes wet grass (assuming conditions necessary for causal interpretation; see [KTHO05]). We also recover the same conditional probability tables (not shown) from Tabular inference as the Infer.NET program, proof that Tabular succeeded in learning the true model for this simple scenario. In fact, we generated the 1000 rows of input data from an Infer.NET probabilistic program with model s3 as ground truth and fixed conditional probability tables,1 and so model s3 is the true\n1Adapted from http://research.microsoft.com/en-us/um/cambridge/projects/infernet/ docs/discrete%20bayesian%20network.aspx\n42\nmodel.\nWe call our procedure interactive model search because the user specifies the\nsearch space through scripts like Figure 3.1. The user may initiate a larger search, perhaps over the space of all Bayesian networks as an automated search algorithm would, by searching over the presence and direction of every arrow. See [CG01b] for a survey of learning algorithms on several variants of Bayesian networks. In short, ModelWizard opens a design space in which the user insightfully scripts ad hoc searches through model subspaces, writing in automated selection or prioritization as needed to search spaces large enough to be useful without resorting to infeasible, exhaustive search.\nModel selection by data likelihood is prone to overfitting. In remaining exam-\nples we switch to the cross-validation inference scoring method: holding out a test set of data and measuring test set predictive accuracy according to a user-defined error function, such as the 0-1 loss function on predicting rain."}, {"heading": "3.2 Derived Model Family: Naive Bayes", "text": "We now illustrate data-transforming and derived operations. Suppose we want to construct a Naive Bayes model to classify gender given height, weight and footsize as features, for a dataset taken from [Wik14]. Naive Bayes is a machine learning family of classifiers where a class column determines the distribution behind feature columns, and the features are assumed independent given class.\nFigure 3.3 shows our beginning all-input state. A new difficulty is that our\ndata source has string type for gender, a type which Tabular cannot perform inference over. CreateTableUniques and Link in Code Listing 3.2 overcome this difficulty, the former by creating a new table of gender\u2019s unique values called T gender, and the\n43\nlet! tn = CreateTableUniques \"tmain\" [\"gender\"] do! Link tn [\"gender\",\"gender\"] \"tmain\"\"gender\" do! ModelDiscrete \"tmain\" \"gender\" do! ModelGaussian \"tmain\" \"height\" do! ModelGaussian \"tmain\" \"weight\" do! ModelGaussian \"tmain\" \"footsize\" do! Index \"tmain\" \"height\" \"gender\" [] do! Index \"tmain\" \"weight\" \"gender\" [] do! Index \"tmain\" \"footsize\" \"gender\" []\nCode Listing 3.2: Naive Bayes script expressed in primitives\nlatter by changing tmain\u2019s data such that gender is an integer link to the appropriate ID column of T gender. The annotation \u2018pk\u2019 in the gender column of T gender enforces that every row of T gender must have a unique value. Because T gender has two rows, Gender\u2019s new \u2018link(T gender)\u2019 type is synonymous with \u2018upto(2).\u2019\nFigure 3.4 shows the final Naive Bayes model after running the rest of Code\nListing 3.2. ModelGaussian is similar to ModelDiscrete in that it allows inference\n44\nlet NaiveBayes tn classname = OPM {\ndo! TypeNominal tn classname do! ModelDiscrete tn classname let! schema ,_ = GetState let table = getSchemaTableByName schema tn let classcol=getColumnByName table classname let rngColList= // collect feature columns\ntable.columns |> List.filter (fun col ->\nmatch (col ,classcol) with | a,b when a.name = b.name -> false | {name=n},_ when n=\"ID\" -> false | IndependentOf -> true | _ -> false)\n// Model & Index features by classcol for col in rngColList do\nif col.markup = Input then do! Model tn col.name do! Index tn col.name classname []}\nCode Listing 3.3: NaiveBayes derived operation\nof mean and precision for a Gaussian distribution generating a column.\nNaive Bayes is a common pattern in machine learning, and as [LBFPPB13]\nconcurs, probabilistic programming languages naturally express Naive Bayes. The derived operation in Code Listing 3.3 provides one such expression to facilitate its construction on future datasets. It offers abstraction in that we no longer need worry about its component operations or F# code and can treat it as a black box. Alternatively, we gain understanding into how Naive Bayes works by looking at its code. The derived operation simplifies Code Listing 3.2 to the one-liner do! NaiveBayes \"tmain\" \"gender\".\nWe anticipate users use derived operations in two ways. On a basic level, users\nmay record a list of operations on a dataset, initially taken ad-hoc, for replay on a similar dataset. After recognizing that a list of operations is an instance of a more\n45\ncommon modeling pattern, as in the case of Naive Bayes, a user can abstract and generalize the derived operation to apply more widely. These operations include both transformations on the dataset, like the conversion of string gender values to links in a new table, and construction of a Tabular model."}, {"heading": "3.3 Hybrid Model: Internet Plant Sales", "text": "We now turn to a real world example: modeling Internet plant sales. We use more advanced pre-processing, capturing functional dependencies, in combination with the previous section\u2019s Naive Bayes model.\nFigure 3.5 shows a few rows from a dataset, acquired from staff at the Royal\nBotanic Gardens in Kew UK who are interested in predicting Wild Propagate: whether a sale is of a wild plant or a propagate plant grown in a greenhouse. Predicting whether a plant is wild or propagate is a key step in identifying illegal plant sales. Perhaps more important than prediction, it is important to understand how strongly other columns impact Wild Propagate, including sale price, country of sale, plant conservation priority, and status on the CITES list of endangered plant species [CIT73].\nIn database literature [Dat90], a functional dependence (FD) between domain\ncolumns and a range column hold when domain column values uniquely determine range column value. FDs may hold for continuous domains (as in the relationship y = x2 where x functionally determines y), but in this thesis we restrict FDs to only use discrete domains. We sometimes refer to an FD as an \u201cexact\u201d dependence,\n46\nbecause the domain of an FD exact ly determines the value of its range. The Internet plant sales dataset has two FDs: that Genus and Species determine conservation priority and On CITES status.\nCode Listing 3.4 presents a script we use to create a hybrid model, shown\nin Figure 3.6, for the Internet plant sales data. We infer the types of all columns in the dataset via TypeInferTable, create a separate table Genus Species explicitly enforcing two discovered functional dependencies via ExactInfer (described below), add a standard NaiveBayes on the remainder of tmain, and complete the model by modeling the determined columns in Genus Species. Ongoing research efforts build on this model to investigate how machine learning may aid CITES treaty enforcement via plant sales inspections."}, {"heading": "3.3.1 ExactInfer and Exact: Functional Dependencies", "text": "ExactInfer implements an algorithm to discover the best functional dependency given a set of domain columns. \u201cBest\u201d in this case refers to the functional dependence with largest range. We could for example, only find in the Internet plant sales data\n47\ndo! TypeInferTable \"tmain\" let! efd_rngcols = ExactInfer \"tmain\" [\"Genus\";\"Species\"] do! NaiveBayes \"tmain\" \"Wild_Propagate\" for col in efd_rngcols do\ndo! Model \"T_Genus_Species\" col\nCode Listing 3.4: ModelWizard script to construct plant sales model\nthat Genus and Species functionally determine On CITES status, but it would lead to a missed opportunity to find that Genus and Species functionally determine both On CITES and conserve priority.\nWe implement the derived operation ExactInfer in three phases.\n1. Find all columns functionally determined by the given domain columns. Do not\nconsider columns that have dependencies.\n2. If there is more than one range column, create a new table with domain columns\nas primary keys by calling CreateTableUniques.\n3. For each range column, perform an Exact.\nExact is a primitive operation that performs the steps below to take advantage\nof a functional dependency. Translations of each step to the Internet plant sales Exact tmain Genus Species On CITES are given in brackets.\nGiven a main table, a range column in that main table, and a link-type domain\ncolumn linking to a domain table [main table tmain, range column On CITES, and domain column Genus Species in tmain linking to the domain table T Genus Species (created by ExactInfer)],\n1. Create a new column in the domain table with the same name, type and model as\nthe range column in the main table. [Create On CITES column in T Genus Species.]\n48\n2. Set the data of the new column to the values functionally determined by the\nprimary key values in the same row. We implement this by building a map from domain column values to the unique range column value determined by the domain. [Set the data of On CITES in T Genus Species to values determined by the FD in tmain from Genus Species to On CITES.]\n3. Delete the data of the range column in the main table wherever the domain table\nlink is not missing. There is no actual loss of data here because one may recover the data by joining the main table to the domain table. Instead, think of the data deletion as a form of data compression. [Delete the data of On CITES in tmain in rows where Genus Species is not missing.]\n4. Modify the model of the range column in the main table to a reference to the range\ncolumn in the domain table through the domain table link. [Change the model of On CITES in tmain to Genus Species.On CITES.]\nSuccessively applying ExactInfer on a table\u2019s smallest possible domain set\nforms a basis for table normalization to Boyce-Codd Normal Form (BCNF). Table normalization is the process of decomposing a table into smaller tables to eliminate redundancies. Normalization to BCNF requires that the only FDs present in a table are either trivial FDs where the range is a subset of the domain, or FDs where the domain is a superkey (a superset of a candidate key: a set of columns that uniquely determine the FD\u2019s range). We meet these requirements.\nOne could imagine extending the ExactInfer infrastructure to discover all the\nFDs within a table and not just the ones with a particular given domain, similar to automated table normalization work in [BNB08]. However, fully automated table normalization tends to discover spurious FDs, that is, FDs present in a dataset but not true in general. Nevertheless, it is possible to improve ExactInfer\u2019s FD-discovery\n49\nalgorithms by following recent work in [HQRA+13] and older work in its references."}, {"heading": "3.3.2 \u201cPre-processing\u201d during Modeling", "text": "FD-handling is typically a pre-processing step, part of table normalization. In fact, a user that knows about the FDs in a dataset will almost surely account for them before beginning the modeling process at the point where he architects dataset table layout. What if the user does not know about a dataset FD, perhaps when there are hundreds of columns? This is the ideal scenario for ModelWizard to help an analyst discover and take advantage of a dataset fact during the modeling process. We leave it to future studies to determine whether the additional flexibility from conducting pre-processing at the same time as modeling will help in practice.\nIt is worth considering how the model would work in the case that we do not\nuse a separate table T Genus Species to capture functional dependencies. In this case, we would have a single table, tmain, with On CITES and conserve priority indexed by Genus and Species. Indexing the range columns by Genus and Species in the main table instead of a separate table models a FD in a looser sense, because we no longer restrict the range columns to a unique value given domain column values.\nIn the limit of an infinite dataset, the single-table model is equivalent to multi-\ntable one because the CDiscrete distributions on On CITES and conserve priority collapse to point mass distributions for each unique (Genus, Species) pair. Fixing a (Genus, Species) pair, the collapse occurs after running Bayesian updating on infinite datapoints that are all the same. In the case of finite data, the single-table model incurs some predictive performance penalty. This penalty grows small quickly if prior distributions are not too strong and dataset size is at least moderately large.\nBoth the single-table and multi-table model perform better than not captur-\ning a FD at all, i.e., indexing On CITES and conserve priority by Wild Propagate.\n50\nThis is because using tightly dependent columns\u2014the range and domain columns\u2014 too grossly violates the Naive Bayes assumption that features are independent given Wild Propagate.\nA prime future improvement to Figure 3.6\u2019s model is capturing the ordinal\nnature of column conserve priority. A common way to do this is to create a table of real-valued thresholds for each value of conserve priority (H, H-M, M, ...) and model the chosen conserve priority as a real-valued variable discretized by those thresholds. An \u201cordinal column template\u201d may make a nice addition to ModelWizard\u2019s primitive operations, and it serves as another activity typically performed during preprocessing that aids users during modeling.\nWe imagine many other kinds of pre-processing activities from related work\nmay enhance interactive model construction. The Bellman system presents algorithms to discover possible foreign key dependencies, columns formed from a concatenation or format change from other columns, different ways two tables may be joined through intermediate tables, and more, by comparing summary structures computed for database tables [DJMS02]. BayesDB offers capabilities to do similar preprocessing steps in the presence of more general data error through concepts like \u201cstochastic primary key and foreign key,\u201d which can be taken as primary keys containing duplicates due to noise and foreign keys containing changed or non-matching values due to noise [Bax14]. Integrating probabilistic models of internal database structure in the style of BayesDB with models of column value distributions in the style of Tabular is an open avenue for future work.\n51"}, {"heading": "3.4 User-Movie-Rating Recommendation", "text": "For another real world application, we turn to movie recommendation as popularized by the Netflix challenge [BL07]. We show how to build a general clustering model using a ModelWizard script and evaluate its predictive performance. We specifically compare against Singh and Graepel\u2019s application of InfernoDB to movie recommendations, as detailed in their first example that runs on synthetic user-movie-ratings data [SG12, SG13].\nOur data is identical to Singh and Graepel\u2019s synthetic dataset, consisting of\na table of users with attributes gender and age, a table of movies with attributes category and year, and a table of ratings with a rating from 0 to 10 and links to a user and movie. The dataset is small, having two movie genres, 20 users, 30 movies and 25 ratings. Figure 3.7 illustrates a few rows."}, {"heading": "3.4.1 InfernoDB: Schema-Derived Clustering Models", "text": "Singh and Graepel introduce the concept of automatically generating machine learning models based on the schema of a database, including types of columns in tables and foreign key relationships between tables.\nA column\u2019s type determines its base distribution: Real-valued columns have a\nGaussian distribution and discrete (link) columns have a Discrete distribution. We replace the Bernoulli distribution used by Singh and Graepel with equivalent Discrete2\n2Recall that we use Discrete as a synonym for a Categorical distribution.\n52\ndistributions of dimension two.\nEach table has a number of clusters. Tables with no foreign links (\u201cleaf tables\u201d)\nhave a fixed number of clusters, chosen by hand since the number of clusters is a crucial hyperparameter. Tables with foreign links (\u201cbody tables\u201d) have a number of clusters equal to the product of linked tables\u2019 numbers of clusters. Thus, a table with foreign links has clusters equivalent to the cross product of the clusters of tables it links to.\nEach row has a distribution over the clusters in its table. The distribution\nover clusters determines the parameters of each column\u2019s base distribution. The resulting parameters are a weighted average over each cluster\u2019s parameters, with weights determined by a Discrete distribution over clusters. Values for each column are drawn from their base distribution with these resulting parameters. For example, in a table with an age column that has 4 clusters, the age for a row in that table is drawn from a Gaussian distribution with parameters determined by that row\u2019s distribution over the four clusters by taking a weighted average.\nEach cluster\u2019s parameters are drawn from an uninformative prior distribution.\nThese priors are Gaussian (\u00b5 = 0, \u03c3 = 108) for the mean of a Gaussian, Gamma (k = 1,\u03b8 = 10) for the precision of a Gaussian, and Dirichlet \u03b1 = 1 for the probability vector of a Discrete. We chose these hyperparameters to match those used by Singh and Graepel.\nPresented another way, Singh and Graepel generate a clustering model formed\ninductively on tables:3\n1. Table T base case: no foreign links: create a hidden cluster variable that determines\nparameters of the distribution behind each column of T . The distribution behind a column depends on its type. Fix the number of clusters heuristically. 3We assume table links are acyclic; there must exist some table with no outgoing foreign keys.\n53\n2. Table T recursive case: T has foreign links: create a hidden cluster variable de-\ntermined by the cross product of clusters of rows in tables that T links to. A particular row\u2019s distribution over clusters is determined by the distribution over clusters in each linked row in foreign tables.\nThis model uses soft clustering in that we allow a row to take a non-point-\nmass distribution over clusters. Calculations involving a row\u2019s cluster take weighted averages over this distribution. This is also true of foreign links in the case of missing values for user and movie; we infer distributions over possible users and movies. This process is called link prediction. Its use case is when we have rows in our ratings table with a rating and movie, for example, and we wish to calculate which users are most likely to have given the movie that rating. A challenge to link prediction is high dimension distributions when there are many users or movies."}, {"heading": "3.4.2 Inferno Operation with Cluster Setting", "text": "Our Inferno implementation in ModelWizard has one major difference from Singh and Graepel\u2019s specification: it returns a map4 from \u201cleaf\u201d tables to a function that returns a ValidOp that, when later called, modifies the number of clusters in the corresponding leaf table. The extra returned functions allow the user to change the number of clusters after running the Inferno operation. Leaf tables have four clusters by default.\nCode Listing 3.5 lists the Inferno operation. Here are a few notes of explana-\ntion, matching Code Listing 3.5\u2019s commented annotations:\n(A) GetNontrivialLinkedTableNames ttgt returns a list of table names that ttgt\nlinks to. We exclude \u201ctrivial\u201d links to tables for nominal variables with no\n4The map is in the form of an association list, with elements of the form (key, value).\n54\nlet rec Inferno ttgt:ValidOp <( ColumnName list*(int ->ValidOp <unit >))list >=\nOPM { let! links = GetNontrivialLinkedTableNames ttgt // (A) let! linkLists = OPM {\nmatch links with | [] -> // base case\nlet! clusterColName =\nNewColumn ttgt \"cluster\" (NewColType.UptoColumn 4)\nlet modFun k = TypeUpto k ttgt clusterColName // (B) return [[ clusterColName], modFun]\n| _ -> // recursive case\nlet linkLists = ref [] // (C) for (c,t) in links do\nlet! ll = Inferno t linkLists := List.map (fun (m,vop) -> c::m,vop) ll\n@ !linkLists // (D)\nreturn !linkLists}\nlet! rngColList= GetConcreteColumns ttgt // (E) for c in rngColList do\nif c.markup = Input then do! Model ttgt c.name for ll in linkLists do\nmatch ll with | llhead ::llbody ,_ ->\ndo! Index ttgt c.name llhead llbody\n| [],_ -> failwith \"impossible\"\nreturn linkLists}\nCode Listing 3.5: Inferno ModelWizard Operation\n55\nnon-primary-key columns.\n(B) modFun is a function to change the number of clusters set one line above.\n(C) We construct linkLists by mutable assignment due to an artifact of OpMonad.\nThis has the same effect as a fold with the additional ability to execute ValidOps in the body of the fold.\n(D) prepends the current table\u2019s linked ColumnNames to the list of ColumnNames in\nlinked tables, building a correct list of ColumnNames across links to leaf tables.\n(E) GetConcreteColumns ttgt returns the columns that will be clustered by ttgt\u2019s\nnew cluster column. These are all the columns in the table that are not link columns to a \u201cnontrivial\u201d table (see (A)) and are not the ID or cluster column. We Model these columns if they are input."}, {"heading": "3.4.3 Hyperparameter Sweeps over Number of Clusters", "text": "We use the resulting cluster-number-change functions from Inferno to find the optimal numbers of clusters by hyperparameter sweep using cross-validation. We fashion our algorithm as iterative coordinate descent as follows: Let userk and titlek be numbers of clusters for the user and title table, respectively.\n1. Initialize userk, titlek := 4 (from Inferno).\n2. Hold titlek fixed and calculate the best scoring userk between 1 and 6 by running\ncross-validation 6 times and comparing scores.\n3. Hold userk fixed and calculate the best scoring titlek between 1 and 6 by running\ncross-validation 6 times and comparing scores.\n4. If userk or titlek changed, goto step 2 else stop.\n56\nThe algorithm generalizes to any number of leaf tables. We may also imagine a variant that does not fix the minimum or maximum number of clusters but instead searches for local score minima.\nWe use 5-fold cross-validation on the rating column to determine the best\nscoring numbers of clusters. To do this, we select 20% of the rows in table tmain to hold out their ratings value. We train our model to score on the remaining data and predict the values for the unknown 20% of rows. We use root mean square error (RMSE) to calculate error on predicted ratings versus their held out truths. Repeat for each randomly allocated partition of the dataset into 5 divisions. The best scoring model is the one with lowest mean cross-validation error.\nFigure 3.8 shows results of our sweep on Figure 3.7\u2019s dataset, and Figure 3.9\nshows the resulting Tabular model (omitting trivial nominal tables of unique values). We graph cross-validation errors of other numbers of clusters as different from the calculated optimal number of clusters: 5 for users and 4 for movies. The userk line holds titlek constant at its calculated optimal number of clusters and vice versa.\nThe numbers of clusters are optimal for predicting ratings. They are not neces-\nsarily optimal for predicting age, gender, category or year because we do not include prediction error from these columns in our RMSE plots. To include additional column prediction error in our calculation, we would have to define weights representing how much we care about one column\u2019s error relative to others, similar to a utility function. Defining column weights becomes arbitrary, and so we arbitrarily chose weight one on rating and zero everywhere else."}, {"heading": "3.4.4 Performance on Missing Data", "text": "To evaluate how well our ModelWizard implementation captures the concept behind InfernoDB, we create graphs of the same form as Figure 3 in [SG13]. The graphs\n57\n58\nshow model performance on predicting different columns with different numbers of randomly selected held out values. Unlike the previous section\u2019s procedure to find the optimal number of clusters, we hold out values from all non-link columns: age and gender of a user, category and year of a movie, and rating. There are 125 data values to hold out.\nFigure 3.10 shows the performance of the model derived from Inferno using\nthe calculated optimal number of clusters (that is, optimal for predicting ratings). For each number of held out data points on the x-axis, we plot five independent rounds of predicting that many held out values. Each round may have different held out values, and for each we calculate RMSE. Viewing several independent rounds per x-axis value highlights RMSE variance depending on how entries fall into training and test sets. The original InfernoDB set similarly plots multiple rounds of predicting a given number of held out values, except that the number of rounds is not necessarily five; the classic InfernoDB test did not fix the number of rounds.\nThe ModelWizard version of InfernoDB has roughly equivalent performance\nas the original version at medium to high numbers of missing data cells. For low numbers of missing data cells, the ModelWizard version appears to perform worse. We are unsure why this is so. The cause could be structural, indicating that there is a major difference between the two versions of the InfernoDB algorithm such as using different numbers of clusters, or inconsequential, perhaps from miscopied data sources or a similar reason."}, {"heading": "3.4.5 Alternatives", "text": "Matrix factorization is a popular alternative to cluster models for recommendation systems. In matrix factorization, each user and movie has a hidden vector of d dimensions, where each dimension can be thought of as a trait. Compatibility between users\n59\n60\nand movies is determined by the inner product of their trait vectors, resulting in a real-valued number which is shifted to the average rating. Traits of the same sign (in one dimension of the vector) contribute toward a higher rating and opposite-signed traits contribute toward a lower rating. Other models in the literature worth consideration are in the category of collaborative filtering; see [BHK98] for an overview.\nAnother alternative is to consider rating as an ordinal variable instead of a\nreal-valued one. As in Section 3.3\u2019s Internet plant sales application, this involves ordering ratings and inferring their latent threshold values that delineate and effectively discretize ratings.\nAs an alternative to Figure 3.7\u2019s multi-table user-movie-ratings schema, the\nInferno model also works for denormalized data for which we discover relationships in the dataset. For example, suppose we receive data in the form of Figure 3.11. We may use ExactInfer operations as in Section 3.3 to deduce a table schema that captures exact functional dependencies and then run Inferno on the resulting schema. The result is identical, and it shows how we may explore the space of dataset schemas while we explore the space of machine learning models in ModelWizard.\n61\nChapter 4 Discussion"}, {"heading": "4.1 Addressing Big Data\u2019s Challenges", "text": "A challenge to model construction is modern big data, often characterized by four components [EL13]:\n\u2022 Volume: large dataset size\n\u2022 Velocity: high rate of data acquisition, decreasing data value with time\n\u2022 Veracity: data uncertainty\n\u2022 Variety: multiple data sources, types and dimensions\nVolume was not a consideration in ModelWizard\u2019s initial design, as hinted from\nsmall to moderate dataset sizes in the Applications chapter. Luckily, there are many ways we imagine extending ModelWizard to better handle large datasets. A good starting point is adding additional data representations such as sparse data storage.\nOperating on data stored in a remote database instead of in-memory data could\nconsiderably increase scalability to large data. The Excel workbook data model already has the ability to accept remote database connections, which ModelWizard can tap into as it already does for data stored in spreadsheets as outlined in Section 2.7. We may leverage Excel\u2019s remote database support by rewriting operations to build remote queries and updates, perhaps using SQL language, instead of transforming data in memory. Constraints on data map to database column constraints, and user-defined data transformation operations map to stored procedures executed by a SQL engine at the database. NoSQL and NewSQL databases [GHTC13] are\n62\nalso in scope, though each additional database supported will require a separate set of drivers that operations interface with to access data\u2014 unless recent projects such as SQL++ [OPV14], D4M [KCGJ15], or the larger BigDawg proposal [DEK+15] succeed in providing a common language interface to many types of databases.\nThe shift in thinking is to construct a function that will check and modify\ndata, throwing an exception for data that does not fit an operation rather than eagerly modify data. Viewing operations as database transactions additionally enables parallel execution of nonconflicting operations\u2014that is, operations that do not modify the same part of Schema or Data as another operation that reads or modifies that part.\nSampling, lazy evaluation and background evaluation are further promising\ntechniques that would help ModelWizard handle larger datasets. Instead of running data-transforming operations on a dataset right away, we could run operations on samples from a dataset to see a preview of how the operation will affect the entire dataset. The same principle holds for inference operations; one can gauge how well a model performs by running on a sample. After obtaining preliminary results, ModelWizard can start a background process to run the operation on larger and larger samples, up to the full dataset, asynchronously returning progressively more accurate estimations of an operation\u2019s effect. This allows us to retain interactivity using quick estimations on samples, without losing correctness and accuracy when the user does not mind waiting.\nModelWizard script reuse will help in the case of many datasets with similar\ncolumns. A user may create a ModelWizard script that operates on one dataset and share that script with colleagues that have similar datasets, who can then apply the script with little to no change. It is easy to imagine libraries of ModelWizard scripts developed and downloaded for data scientists within and across organizations.\n63\nThe first component of velocity, high rate of data acquisition, is outside Model-\nWizard\u2019s scope. ModelWizard is designed to operate on fixed datasets with unknown structure rather than streaming data. A compromise is using ModelWizard to build a well-performing machine learning model for a fixed segment of streaming data on a development server, and then extract the machine learning model and implement it on a production server for streaming analysis. C# code for Tabular models built from ModelWizard is available through the Tabular GUI. Thus to reemphasize: ModelWizard is a tool for model exploration and construction, not for top-performance model deployment.\nThe second component of velocity, decreasing data value with time, is directly\nmodelable. We can include the time a data value is recorded as itself another column in the dataset. For example, take the problem of object tracking with dataset columns position and time. If our goal is to infer the object\u2019s position at time t, position observations close to time t are valuable whereas position observations farther before or after time t yield decreasingly less information. We may account for time dependence in modeling by including time difference between observations in the variance of position difference between observations.\nVeracity, the phenomena of data uncertainty, is also directly modelable. In-\ncluding data accuracy and precision information as additional columns enables us to use those columns in probabilistic modeling. For example, we could imagine extending Section 1.2\u2019s apple freefall example by including the imperfect mechanism by which we record apple elevations. Such an addition will not change the mean, most likely value of a predicted elevation (assuming the same mechanism records all data points), but it will return higher, more realistic variances. It also allows us to more strongly weigh sample rows that we have more confidence in, such as rows from using a higher precision measuring device.\n64\nModelWizard handles data variety fairly well, especially heterogeneous data\ntypes. ModelWizard\u2019s machine learning models operate on nominal, string-valued data, numeric data, and in the future, ordinal data. Multiple data sources are no problem as ModelWizard represents each with a separate table, from which we may create cross-table links.\nHigh dimensional data is not a strong suit for ModelWizard. Imagine importing\nraw image processing data: a single megapixel image in truecolor (one row) will have a million columns, each with a 24-bit integer. Large numbers of columns are not ideal for manual analysis, though one could write ModelWizard operations to process them systematically. A better use case for interactive modeling with ModelWizard is after feature extraction and dimensionality reduction, when each column has an interpretable meaning to data analysts."}, {"heading": "4.2 Extending Model Safety to Model Inferability", "text": "ModelWizard\u2019s ValidState, ValidOp and OpMonad constructs guarantee a base level of model safety: every model constructed by ModelWizard has a valid Tabular counterpart and will pass Tabular\u2019s type checking. Is it possible to extend model safety to guarantee that models constructed by ModelWizard are inferable, that is, can be run by the back-end inference engine to produce posterior distributions on parameters and missing values? Such a strong guarantee would enable data scientists to freely conjure models using the suite of operations available (respecting types so as not to throw exceptions), without worrying whether the model they construct will run.\nThe short answer is with great difficulty. Providing inferability guarantees is\ninherently specific to choice of inference engine. A formal proof requires modeling the syntax and semantics of a probabilistic program to prove that a certain subset\n65\nof programs are inferable by a particular inference engine, and then prove that ModelWizard operations will only generate programs (after Tabular compilation) within that inferable subset. All probabilistic programs are theoretically inferable, but different inference engine implementations will support some probabilistic program families more than others.\nAs a heuristic example derived from the author\u2019s experience, Infer.NET\u2019s ex-\npectation propagation algorithm performs more reliably for models with real-valued columns. Infer.NET\u2019s variational message passing algorithm performs more reliably for models with discrete columns. Some models containing both real-valued and discrete columns are inferable by neither algorithm, a problem that surfaced during model exploration for Section 3.3\u2019s Internet Plant Sales application. A chief cause of non-inferability is unimplemented message passing factors due to open research problems in variational inference."}, {"heading": "4.3 Extending to Usable Interactivity", "text": "Many researchers believe usability is a key factor in the extent of success for probabilistic programming tools [Gor13]. To truly make incremental model construction accessible to data scientists, we imagine a GUI or IDE wrapping ModelWizard\u2019s state, sketched in Figure 4.1. Regular users will find model exploration easier using a GUI, and \u201cpower users\u201d who want to create custom operations may do so via script editing.\nThe GUI would show the currently constructed State, with the ability to zoom\nin on Tabular Schema and a preview or sample of Data. At the right, we envision a list of operations performed so far. The GUI may tag operations with a model score based on given scoring criteria, such as cross-validation on a particular column of interest. Model log evidence is a useful default.\n66\nThe menu at the top of Figure 4.1 contains buttons that trigger operations,\ngrouped by category such as \u201cTyping\u201d and \u201cBase Models.\u201d Right clicking on columns in the Schema or Data preview will list operations relevant to that column\u2019s type. At a more advanced stage, we imagine suggesting operations based on common modeling paradigms learned from users modeling similar datasets.\nA particularly powerful meta-feature is undo: the ability to roll back an op-\neration and quickly try another one. One can implement undo by extending the OpMonad computation expression to include State monad functionality by tracking past states (or diffs between states) within the monad. Even in the current ModelWizard implementation, users could save intermediate states via ordinary F# let bindings, as demonstrated by state s1 in Figure 3.1, after which performing undo is simply reusing saved state. Undo enables users can quickly try an operation, see whether it makes sense or is silly, and quickly proceed or rollback.\nThe idea for a ModelWizard GUI is inspired by Microsoft Power Query\u2019s data\ntransformation script editor [Web14] and Proof General\u2019s Coq theorem proving script editor [Asp00]. Both editors lower the barrier for new users, raise situational awareness, and increase productivity while writing scripts. We wish the same for Model-\n67\nWizard users tackling model exploration.\nIn addition to visualization and interactive editing, a growing theme in systems\ndesign is to integrate the \u201cwhole machine learning pipeline\u201d from data mining and enrichment to data cleaning and modeling into a common framework, as outlined in [MBGM14] to support the activities of data enthusiasts that do not have formal training in data science. We view ModelWizard as a forward step in creating such an integrated analytics framework, paritcularly in the combination of preprocessing with modeling. Much future work (or combination with existing tools) is needed to incorporate data enrichment and cleaning."}, {"heading": "4.4 Interactive Theorem Proving Analogy", "text": "Model construction bears many similarities to theorem proving. Both benefit from interactivity as a happy medium between two extremes.\nManually proving theorems on pencil and paper requires expert knowledge and\ntraining to navigate the space of all possible proofs and formalize one that proves the theorem. The approach works as with manual model construction but at high labor cost, sometimes taking years of study in the proof\u2019s domain.\nAutomatically proving theorems requires powerful algorithms. For example,\nthe automated SMT solver Z3 [DMB08] succeeds for simple theorems but will fail for complex enough theorems due to the size of the space of all proofs. Automated model construction similarly fails in an unrestricted model space.\nWe aspire to use the design of interactive theorem provers as inspiration for\ninteractive model construction. Imagine operations as model construction tactics, lower ones like ModelDiscrete akin to Coq\u2019s rewrite tactic [BC04] and higher ones like ExactInfer akin to Isabelle\u2019s sledgehammer tactic [PB10]. Like tactics, operations\n68\nmay fail by throwing an exception and leaving state unchanged."}, {"heading": "4.5 Conclusion", "text": "Probabilistic programs offer a universal space to create machine learning models for a dataset. ModelWizard presents a new take on constructing machine learning models, focusing on iterative construction one-to-one with model exploration, rather than traditional specification of a probabilistic program entirely at once.\nOur primary contribution is planting the seeds for an interactive framework,\napplicable to any language and inference engine, to view data transformation and machine learning models in terms of composable building blocks we call operations, glued together by program code (F# in our case). In doing so we simultaneously realize realms of pre-processing, manual analysis, automated analysis and modeling itself. We further realize these realms in naturally familiar program structure such as conditionals, iteration, and procedural abstraction, along with opportunity for classic programming language boons such as static analysis, syntactic safety and compilation optimization.\nWe proffer ModelWizard as a solution scheme to three scientific goals: predic-\ntion, retained from inference on constructed machine learning models; understanding, built from model decomposition into understandable operations; and generalization, found through application of operations to new problems and domains. We postulate future work building on our framework\u2019s principles will naturally integrate with and augment data scientists\u2019 modus operandi to discover our world\u2019s true structure.\n69"}, {"heading": "A OpMonad Translation", "text": "As a demonstration for how let!, do! and other monadic syntax desugars into vanilla F# code, we present a simplified version of the derived operation TypeInferTable on the target table tmain, followed by its translation into an F# quotation that evaluates to a ValidOp<unit> value. The quotation includes calls to computation expression functions, defined in Code Listing 2.1 and bolded in the code below. OPM {let! schema = GetSchema\nlet table = getSchemaTableByName schema \"tmain\" for col in table.columns do // infer each column \u2019s\ndo! TypeInferColumn \"tmain\" col.name} // type in tmain\nQuotations.Expr <ValidOp <unit >> = Call (Some (Value (FSI_0008+OpMonad)), Delay ,\n[Lambda (unitVar ,\nCall (Some (Value (FSI_0008+OpMonad)), Bind ,\n[PropertyGet (None , GetSchema , []),\nLambda (_arg1 ,\nLet (schema , _arg1 ,\nLet (table ,\nCall (None , getSchemaTableByName ,\n[schema , PropertyGet (None , \"tmain\", [])]) ,\nCall (Some (Value (FSI_0008+OpMonad)), For,\n[Coerce (PropertyGet (Some (table),\ncolumns , []), IEnumerable \u20181),\nLambda (_arg2 ,\nLet (col , _arg2 ,\nCall (Some (Value (FSI_0008+OpMonad)),\nBind ,[Call (None ,\nTypeInferColumn , [PropertyGet (None , \"tmain\", []),\nPropertyGet (Some (col), name , [])]),\nLambda (_arg3 ,\nCall(Some (Value (FSI_0008+OpMonad)),\nReturn , [Value (<null >)]))])))]))))]))])\n70"}, {"heading": "B ModelWizard API of Primitive and Derived Operations", "text": "P stands for a Primitive operation; others are derived from primitives using OpMonad. Excel-interfacing and \u201chelper\u201d operations are omitted.\nCore bridge from State to F#: ///P: Return current state (schema ,data); does not modify state val GetState: ValidOp <State > Column typing: ///P: Change col \u2019s type to real , ensuring data values are numeric val TypeReal: TableName -> col:ColumnName -> ValidOp <unit > ///P: Change col \u2019s type to upto(n), ensuring data are integer mod n val TypeUpto: n:int -> TableName -> col:ColumnName -> ValidOp <unit > ///P: Create all -missing -value column. Return its name , guaranteed fresh val NewColumn: TableName -> ColumnName -> NewColType -> ValidOp <ColumnName > Table management: ///P: Create a unique value table from clist; return new table name val CreateTableUniques:TableName ->ColumnName list ->ValidOp <TableName > ///P: Transform fkt \u2019s columns into ID references to pkt val Link: pkt:TableName -> pktofkMap :( ColumnName*ColumnName) list\n-> fkt:TableName -> fklinkcol:ColumnName -> ValidOp <unit > Derived typing: /// Create a new table of col \u2019s unique values and link col to it val TypeNominal: TableName -> col:ColumnName -> ValidOp <unit > /// Examine col \u2019s data to infer its type val TypeInfer: TableName -> col:ColumnName -> ValidOp <unit > /// TypeInfer each column in the table val TypeInferTable: TableName -> ValidOp <unit > Base machine learning models: ///P: Place a CGaussian distr. on col , independent of other columns val ModelGaussian: TableName -> col:ColumnName -> ValidOp <unit > ///P: Place a CDiscrete distr. on col , independent of other columns val ModelDiscrete: TableName -> col:ColumnName -> ValidOp <unit > /// Place an appropriate default model on a column based on its type val Model: TableName -> col:ColumnName -> ValidOp <unit >\n71\nModel coupling: ///P: Use different distr. params on crng for each discrete cdom value /// The list is for indexing across table links. val Index: TableName -> crng:ColumnName -> cdom:ColumnName\n-> tableLinkList:ColumnName list -> ValidOp <unit >\n///P: Create linear dependence: crng (modeled Input) on cdom (typed real) val LinReg: TableName -> crng:ColumnName -> cdom:ColumnName ->ValidOp <unit > ///P: Create quadr. dependence: crng (modeled Input) on cdom (typed real) val QuadReg:TableName -> crng:ColumnName -> cdom:ColumnName ->ValidOp <unit > Derived machine learning models: /// Construct Naive Bayes model taking \u2019free \u2019 columns of tn as features. val NaiveBayes: tn:TableName -> classname:ColumnName -> ValidOp <unit > /// Construct Inferno clustering model , recursing on linked tables. /// Return a list of column links and functions to modify # of clusters val Inferno:TableName ->ValidOp <( ColumnName list*(int ->ValidOp <unit >)) list > Exact functional dependencies: ///P: If valid functional dependency , moves crng to cdom \u2019s linked table val Exact: TableName -> cdom:ColumnName -> crng:ColumnName ->ValidOp <unit > /// Find , capture & return columns exactly determined by the column list val ExactInfer:TableName ->domCols:ColumnName list ->ValidOp <ColumnName list > Inference: /// Compile current state , run inference and return data log evidence val ScoreLogEvidence: ValidOp <float > /// Return average RMSE from k runs of cross -validation on a real column val CrossValidate: TableName -> ColumnName -> k:int -> ValidOp <float > /// Find best number of clusters using the modify -#-of -cluster functions /// & scoring function , e.g. ScoreLogEvidence (higher scores better) or /// CrossValidate (lower scores better ). Returns plottable score data. val SweepNumberClusters: clusterFun :(int ->ValidOp <unit >) -> scoreFun: ValidOp <float > ->isHighBest:bool -> ValidOp <int*float* (int*float) list > /// Run Cross Validation , holding out and scoring data in arbitrary /// columns. Data held out ranges from 0 to all. Return scores averaged /// numRepeats times , for each column , for each # of held out points. val MissingDataAnalysis: colsHoldOut :( TableName*ColumnName) list\n-> colsScore :( TableName*ColumnName) list -> numRepeats:int -> ValidOp <Map <TableColumn ,(int*float) list >\n72"}], "references": [{"title": "Proof general: A generic tool for proof development", "author": ["David Aspinall"], "venue": "In Tools and Algorithms for the Construction and Analysis of Systems,", "citeRegEx": "Aspinall.,? \\Q2000\\E", "shortCiteRegEx": "Aspinall.", "year": 2000}, {"title": "BayesDB: querying the probable implications of tabular data", "author": ["Jay Baxter"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Baxter.,? \\Q2014\\E", "shortCiteRegEx": "Baxter.", "year": 2014}, {"title": "Generative or discriminative: Getting the best of both worlds. Bayesian statistics 8: proceedings of the eighth Valencia International Meeting", "author": ["JM Bernardo", "MJ Bayarri", "JO Berger", "AP Dawid", "D Heckerman", "AFM Smith", "M West"], "venue": null, "citeRegEx": "Bernardo et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bernardo et al\\.", "year": 2006}, {"title": "Interactive theorem proving and program development: Coq\u2019Art: the calculus of inductive constructions", "author": ["Yves Bertot", "Pierre Cast\u00e9ran"], "venue": "springer,", "citeRegEx": "Bertot and Cast\u00e9ran.,? \\Q2004\\E", "shortCiteRegEx": "Bertot and Cast\u00e9ran.", "year": 2004}, {"title": "Empirical analysis of predictive algorithms for collaborative filtering", "author": ["John S Breese", "David Heckerman", "Carl Kadie"], "venue": "In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Breese et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Breese et al\\.", "year": 1998}, {"title": "Variational methods for the dirichlet process", "author": ["David M Blei", "Michael I Jordan"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "Blei and Jordan.,? \\Q2004\\E", "shortCiteRegEx": "Blei and Jordan.", "year": 2004}, {"title": "The netflix prize", "author": ["James Bennett", "Stan Lanning"], "venue": "In Proceedings of KDD cup and workshop,", "citeRegEx": "Bennett and Lanning.,? \\Q2007\\E", "shortCiteRegEx": "Bennett and Lanning.", "year": 2007}, {"title": "Automatic database normalization and primary key generation", "author": ["Amir Hassan Bahmani", "Mahmoud Naghibzadeh", "Behnam Bahmani"], "venue": "In Electrical and Computer Engineering,", "citeRegEx": "Bahmani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bahmani et al\\.", "year": 2008}, {"title": "Model selection and Akaike\u2019s information criterion (AIC): The general theory and its analytical extensions", "author": ["Hamparsum Bozdogan"], "venue": null, "citeRegEx": "Bozdogan.,? \\Q1987\\E", "shortCiteRegEx": "Bozdogan.", "year": 1987}, {"title": "Operations for learning with graphical models", "author": ["Wray L Buntine"], "venue": "arXiv preprint cs/9412102,", "citeRegEx": "Buntine.,? \\Q1994\\E", "shortCiteRegEx": "Buntine.", "year": 1994}, {"title": "Learning bayesian belief network classifiers: Algorithms and system", "author": ["Jie Cheng", "Russell Greiner"], "venue": "Advances in Artificial Intelligence,", "citeRegEx": "Cheng and Greiner.,? \\Q2001\\E", "shortCiteRegEx": "Cheng and Greiner.", "year": 2001}, {"title": "Learning bayesian belief network classifiers: Algorithms and system", "author": ["Jie Cheng", "Russell Greiner"], "venue": "In Advances in Artificial Intelligence,", "citeRegEx": "Cheng and Greiner.,? \\Q2001\\E", "shortCiteRegEx": "Cheng and Greiner.", "year": 2001}, {"title": "An introduction to database systems, volume 7. Addison-wesley", "author": ["Christopher John Date"], "venue": null, "citeRegEx": "Date.,? \\Q1990\\E", "shortCiteRegEx": "Date.", "year": 1990}, {"title": "The BigDawg architecture and reference implementation", "author": ["Jennie Duggan", "Aaron Elmore", "Tim Kraska", "Sam Madden", "Tim Mattson", "Michael Stonebraker"], "venue": "New England Database Summit,", "citeRegEx": "Duggan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duggan et al\\.", "year": 2015}, {"title": "Mining database structure; or, how to build a data quality browser", "author": ["Tamraparni Dasu", "Theodore Johnson", "S. Muthukrishnan", "Vladislav Shkapenyuk"], "venue": "In Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "Dasu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Dasu et al\\.", "year": 2002}, {"title": "An efficient smt solver. In Tools and Algorithms for the Construction and Analysis of Systems, pages 337\u2013340", "author": ["Leonardo De Moura", "Nikolaj Bj\u00f8rner. Z"], "venue": null, "citeRegEx": "Moura and Z3,? \\Q2008\\E", "shortCiteRegEx": "Moura and Z3", "year": 2008}, {"title": "Probabilistic programming concepts", "author": ["Luc De Raedt", "Angelika Kimmig"], "venue": "arXiv preprint arXiv:1312.4328,", "citeRegEx": "Raedt and Kimmig.,? \\Q2013\\E", "shortCiteRegEx": "Raedt and Kimmig.", "year": 2013}, {"title": "Big dataconceptual modeling to the rescue", "author": ["David W. Embley", "Stephen W. Liddle"], "venue": "Conceptual Modeling,", "citeRegEx": "Embley and Liddle.,? \\Q2013\\E", "shortCiteRegEx": "Embley and Liddle.", "year": 2013}, {"title": "Probabilistic programming for advancing machine learning (PPAML) program kick-off", "author": ["Kathleen Fisher"], "venue": "Online at http: //ppaml.galois.com/wiki/attachment/wiki/Presentations/ PPAMLKickoffOverviewSlides.pdf?format=raw,", "citeRegEx": "Fisher.,? \\Q2013\\E", "shortCiteRegEx": "Fisher.", "year": 2013}, {"title": "Comprehensible classification models: A position paper", "author": ["Alex A. Freitas"], "venue": "SIGKDD Explor. Newsl.,", "citeRegEx": "Freitas.,? \\Q2014\\E", "shortCiteRegEx": "Freitas.", "year": 2014}, {"title": "Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper)", "author": ["Andrew Gelman"], "venue": "Bayesian Anal., 1(3):515\u2013534,", "citeRegEx": "Gelman.,? \\Q2006\\E", "shortCiteRegEx": "Gelman.", "year": 2006}, {"title": "Tabular: a schema-driven probabilistic programming language", "author": ["Andrew D. Gordon", "Thore Graepel", "Nicolas Rolland", "Claudio V. Russo", "Johannes Borgstr\u00f6m", "John Guiver"], "venue": "In POPL,", "citeRegEx": "Gordon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gordon et al\\.", "year": 2014}, {"title": "Probabilistic programming", "author": ["Andrew D Gordon", "Thomas A Henzinger", "Aditya V Nori", "Sriram K Rajamani"], "venue": "In International Conference on Software Engineering (ICSE, FOSE track),", "citeRegEx": "Gordon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gordon et al\\.", "year": 2014}, {"title": "Data management in cloud environments: Nosql and newsql data stores", "author": ["Katarina Grolinger", "Wilson A Higashino", "Abhinav Tiwari", "Miriam AM Capretz"], "venue": "Journal of Cloud Computing: Advances, Systems and Applications,", "citeRegEx": "Grolinger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Grolinger et al\\.", "year": 2013}, {"title": "The principles and practice of probabilistic programming", "author": ["Noah D Goodman"], "venue": "In ACM SIGPLAN Notices,", "citeRegEx": "Goodman.,? \\Q2013\\E", "shortCiteRegEx": "Goodman.", "year": 2013}, {"title": "An agenda for probabilistic programming: Usable, portable, and ubiquitous. In ISAT/DARPA workshop on \u201cProbabilistic Programming: Democratizing Machine Learning,", "author": ["Andrew D. Gordon"], "venue": null, "citeRegEx": "Gordon.,? \\Q2013\\E", "shortCiteRegEx": "Gordon.", "year": 2013}, {"title": "Probabilistic programs as spreadsheet queries", "author": ["Andrew D. Gordon", "Claudio Russo", "Marcin Szymczak", "Johannes Borgstrm", "Nicolas Rolland", "Thore Graepel", "Daniel Tarlow"], "venue": "Programming Languages and Systems,", "citeRegEx": "Gordon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gordon et al\\.", "year": 2015}, {"title": "Basic bayesian methods", "author": ["Mark E. Glickman", "David A. van Dyk"], "venue": "Topics in Biostatistics,", "citeRegEx": "Glickman and Dyk.,? \\Q2007\\E", "shortCiteRegEx": "Glickman and Dyk.", "year": 2007}, {"title": "Bayesian model averaging: a tutorial", "author": ["Jennifer A Hoeting", "David Madigan", "Adrian E Raftery", "Chris T Volinsky"], "venue": "Statistical science,", "citeRegEx": "Hoeting et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hoeting et al\\.", "year": 1999}, {"title": "Scalable discovery of unique column combinations", "author": ["Arvid Heise", "Jorge-Arnulfo Quian\u00e9-Ruiz", "Ziawasch Abedjan", "Anja Jentzsch", "Felix Naumann"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "Heise et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heise et al\\.", "year": 2013}, {"title": "Model selection error rates in nonparametric and parametric model comparisons", "author": ["Yongsung Joo", "Martin T Wells", "George Casella"], "venue": "In Borrowing Strength: Theory Powering Applications\u2013A Festschrift", "citeRegEx": "Joo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Joo et al\\.", "year": 2010}, {"title": "Associative arrays: Unified mathematics for spreadsheets, databases, matrices, and graphs", "author": ["Jeremy Kepner", "Julian Chaidez", "Vijay Gadepally", "Hayden Jansen"], "venue": "In New England Database Summit,", "citeRegEx": "Kepner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kepner et al\\.", "year": 2015}, {"title": "Aleatory or epistemic? does it matter", "author": ["Armen Der Kiureghian", "Ove Ditlevsen"], "venue": "Structural Safety,", "citeRegEx": "Kiureghian and Ditlevsen.,? \\Q2009\\E", "shortCiteRegEx": "Kiureghian and Ditlevsen.", "year": 2009}, {"title": "The discovery of structural form", "author": ["Charles Kemp", "Joshua B. Tenenbaum"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Kemp and Tenenbaum.,? \\Q2008\\E", "shortCiteRegEx": "Kemp and Tenenbaum.", "year": 2008}, {"title": "Causal reasoning with causal models", "author": ["Kevin B. Korb", "Charles R. Twardy", "Toby Handfield", "Graham Oppy"], "venue": "Technical report,", "citeRegEx": "Korb et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Korb et al\\.", "year": 2005}, {"title": "A probabilistic programming approach to naive bayes text classification", "author": ["Danilo Lucena", "Gustavo Brito", "Andrei Formiga", "Joao Pessoa-PBBrazil"], "venue": "In 2nd Brazilian Conference on Intelligent Systems,", "citeRegEx": "Lucena et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lucena et al\\.", "year": 2013}, {"title": "Computing: A vision for data science", "author": ["Chris A. Mattmann"], "venue": null, "citeRegEx": "Mattmann.,? \\Q2013\\E", "shortCiteRegEx": "Mattmann.", "year": 2013}, {"title": "Support the data enthusiast: Challenges for next-generation data-analysis systems", "author": ["Kristi Morton", "Magdalena Balazinska", "Dan Grossman", "Jock Mackinlay"], "venue": "Proc. VLDB Endow.,", "citeRegEx": "Morton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Morton et al\\.", "year": 2014}, {"title": "An introduction to graphical models. A Brief Introduction to Graphical Models and", "author": ["Kevin Murphy"], "venue": "Bayesian Networks,", "citeRegEx": "Murphy.,? \\Q2001\\E", "shortCiteRegEx": "Murphy.", "year": 2001}, {"title": "An efficient mcmc sampler for probabilistic programs", "author": ["Aditya V Nori", "Chung-Kil Hur", "Sriram K Rajamani", "Selva Samuel"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Nori et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nori et al\\.", "year": 2014}, {"title": "The sql++ semi-structured data model and query language: A capabilities survey of sql-on-hadoop, nosql and newsql databases", "author": ["Kian Win Ong", "Yannis Papakonstantinou", "Romain Vernoux"], "venue": "arXiv preprint arXiv:1405.3631,", "citeRegEx": "Ong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ong et al\\.", "year": 2014}, {"title": "Three years of experience with sledgehammer, a practical link between automatic and interactive theorem provers", "author": ["Lawrence C Paulson", "Jasmin Christian Blanchette"], "venue": "In PAAR@ IJCAR,", "citeRegEx": "Paulson and Blanchette.,? \\Q2010\\E", "shortCiteRegEx": "Paulson and Blanchette.", "year": 2010}, {"title": "Syntax matters: Writing abstract computations in F", "author": ["Tomas Petricek", "Don Syme"], "venue": "Pre-proceedings of TFP (Trends in Functional Programming),", "citeRegEx": "Petricek and Syme.,? \\Q2012\\E", "shortCiteRegEx": "Petricek and Syme.", "year": 2012}, {"title": "Potter\u2019s wheel: An interactive data cleaning system", "author": ["Vijayshankar Raman", "Joseph M Hellerstein"], "venue": "In VLDB,", "citeRegEx": "Raman and Hellerstein.,? \\Q2001\\E", "shortCiteRegEx": "Raman and Hellerstein.", "year": 2001}, {"title": "Unifying logic and probability", "author": ["Stuart Russell"], "venue": "Recent developments,", "citeRegEx": "Russell.,? \\Q2014\\E", "shortCiteRegEx": "Russell.", "year": 2014}, {"title": "Data curation at scale: The data tamer system", "author": ["Michael Stonebraker", "Daniel Bruckner", "Ihab F Ilyas", "George Beskales", "Mitch Cherniack", "Stanley B Zdonik", "Alexander Pagan", "Shan Xu"], "venue": "In CIDR,", "citeRegEx": "Stonebraker et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Stonebraker et al\\.", "year": 2013}, {"title": "Compiling relational database schemata into probabilistic graphical models", "author": ["Sameer Singh", "Thore Graepel"], "venue": "CoRR, abs/1212.0967,", "citeRegEx": "Singh and Graepel.,? \\Q2012\\E", "shortCiteRegEx": "Singh and Graepel.", "year": 2012}, {"title": "Automated probabilistic modeling for relational data", "author": ["Sameer Singh", "Thore Graepel"], "venue": "In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,", "citeRegEx": "Singh and Graepel.,? \\Q2013\\E", "shortCiteRegEx": "Singh and Graepel.", "year": 2013}, {"title": "Linear model selection by cross-validation", "author": ["Jun Shao"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Shao.,? \\Q1993\\E", "shortCiteRegEx": "Shao.", "year": 1993}, {"title": "An asymptotic equivalence of choice of model by crossvalidation and Akaike\u2019s criterion", "author": ["Mervyn Stone"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Stone.,? \\Q1977\\E", "shortCiteRegEx": "Stone.", "year": 1977}, {"title": "Dirichlet process. In Encyclopedia of machine learning, pages 280\u2013287", "author": ["Yee Whye Teh"], "venue": null, "citeRegEx": "Teh.,? \\Q2010\\E", "shortCiteRegEx": "Teh.", "year": 2010}, {"title": "A Dictionary of Statistics", "author": ["G. Upton", "I. Cook"], "venue": "Oxford Paperback Reference. OUP Oxford,", "citeRegEx": "Upton and Cook.,? \\Q2008\\E", "shortCiteRegEx": "Upton and Cook.", "year": 2008}], "referenceMentions": [], "year": 2016, "abstractText": "iii", "creator": "LaTeX with hyperref package"}}}