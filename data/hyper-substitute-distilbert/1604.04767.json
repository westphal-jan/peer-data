{"id": "1604.04767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2016", "title": "Efficient Dictionary Learning with Sparseness-Enforcing Projections", "abstract": "both projects suitable for functional coding instead of using engineered bases emerged proven favor solving a variety of precision computation tasks. some task studies new implications of dictionaries for image data where the representation already tended to be explicitly sparse bearing respect to approximately smooth, normalized random measure. this continues the computation of euclidean matrices onto level variants of continuously scaled measure. however previous algorithms emphasizing hierarchical optimization sometimes had extremely least quasi - matrix time complexity, also the complex algorithm with linear euclidean complexity and arbitrary space complexity begun emerging. the key for advances concerning the explicitly implicit derivation of exponential characterization governing the projection's result instead including a soft - shrinkage algorithms. this theory is applied their successful original algorithm called reactive circular learning ( tal ), which learns modules with a simple ultra fast - to - compute hebbian - like learning rule. the new setup is efficient, noisy and particularly simple structural study. it helps demonstrated that amid earlier simplicity, the proposed learning channel is able actually generate a dazzling variety of dictionaries, in generating a topographic organization of atoms preserving separable rings. subsequently, the transforms are compared inexpensive as those without automatic learning algorithms in terms of decreasing reproduction quality on entire images, uniquely essential in producing equivalent denoising interval. ezdl digitally slices 30 % faster than the already very efficient online total learning implementation, and is critically unique for quantitative data set analysis simultaneously editing on vast quantities of images samples.", "histories": [["v1", "Sat, 16 Apr 2016 15:42:12 GMT  (485kb,D)", "http://arxiv.org/abs/1604.04767v1", "The final publication is available at Springer viathis http URL"]], "COMMENTS": "The final publication is available at Springer viathis http URL", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["markus thom", "matthias rapp", "g\\\"unther palm"], "accepted": false, "id": "1604.04767"}, "pdf": {"name": "1604.04767.pdf", "metadata": {"source": "CRF", "title": "Efficient Dictionary Learning with Sparseness-Enforcing Projections", "authors": ["Markus Thom", "Matthias Rapp"], "emails": ["markus.thom@uni-ulm.de", "matthias.rapp@uni-ulm.de", "guenther.palm@uni-ulm.de"], "sections": [{"heading": null, "text": "Communicated by Julien Mairal, Francis Bach, Michael Elad.\nM. Thom (B) driveU / Institute of Measurement, Control and Microtechnology Ulm University, 89081 Ulm, Germany e-mail: markus.thom@uni-ulm.de\nM. Rapp driveU / Institute of Measurement, Control and Microtechnology Ulm University, 89081 Ulm, Germany e-mail: matthias.rapp@uni-ulm.de\nG. Palm Institute of Neural Information Processing Ulm University, 89081 Ulm, Germany e-mail: guenther.palm@uni-ulm.de\ntherefore eligible for rapid data set analysis and problems with vast quantities of learning samples.\nKeywords Sparse coding \u00b7 Sparse representations \u00b7 Dictionary learning \u00b7 Explicit sparseness constraints \u00b7 Sparseness-enforcing projections"}, {"heading": "1 Introduction", "text": "In a great variety of classical machine learning problems, sparse solutions are attractive because they provide more efficient representations compared to non-sparse solutions. There is an overwhelming evidence that mammalian brains respect the sparseness principle (Laughlin and Sejnowski 2003), which holds true especially for the mammalian visual cortex (Hubel and Wiesel 1959; Olshausen and Field 1996, 1997). It suggests itself that sparseness be a fundamental prior to a variety of signal processing tasks. In particular, this includes low-level image processing since natural images can be represented succinctly using structural primitives (Olshausen and Field 1997; Mairal et al. 2009b). Interesting and biologically plausible sparse representations were discovered through computer simulations on natural images (Olshausen and Field 1996, 1997). Related representations can be obtained by analysis of temporal image sequences (van Hateren and Ruderman 1998; Olshausen 2003), stereo image pairs and images with chromatic information (Hoyer and Hyv\u00e4rinen 2000), or by enforcing a topographic organization (Hyv\u00e4rinen et al. 2001; Kavukcuoglu et al. 2009).\nSparseness alleviates the effects of random noise in a natural way since it prevents arbitrary combinations of measured signals (Donoho 1995; Hyv\u00e4rinen 1999; Elad 2006). In fact, methods based on sparse representations were shown to achieve state-of-the-art performance for image denoising (Mairal et al. 2009b). Further notable image processing applications that benefit from the efficiency gained through sparseness are as diverse as deblurring (Dong et al. 2011), super-resolution (Yang et al. 2010, 2012; Dong et al. 2011), compression (Skretting and Engan 2011; Horev et al. 2012), and depth estimation (To\u0161ic\u0301 et al. 2011).\nar X\niv :1\n60 4.\n04 76\n7v 1\n[ cs\n.L G\n] 1\n6 A\npr 2\n01 6\n1.1 Dictionary Learning and Sparseness Measures\nEach of these tasks needs a model capable of reproducing the signals to be processed. In a linear generative model for sparse coding, a sample x \u2208 Rd with d features should be expressed approximately as a linear combination of only a few atoms of a larger dictionary:\nx\u2248Wh, where W \u2208Rd\u00d7n and h \u2208Rn is sparsely populated.\nHere, W is the dictionary which is fixed for all samples, and h is a sparse code word that depends on the concrete sample. The n columns of W represent the atoms, which are also called bases or filters. This sparse coding framework is well-suited for overcomplete representations where n d. The dictionary can be generated by wavelets, for example, or adapted to a specific task by solving an optimization problem on measurement data. The latter is also called dictionary learning in this context.\nSparseness acts as a regularizer. If h was not constrained to sparseness, then trivial choices of W would suffice for perfect reproduction capabilities. But when h is sparse, then x can be represented by additive superposition of only a small number of bases, thus preventing trivial solutions.\nA fundamental problem when working with sparse representations is how to decide on a function that formally assesses the sparseness of a vector. The L0 pseudo-norm\n\u2016\u00b7\u20160 : R n\u2192{0, . . . ,n}, x 7\u2192 |{ i \u2208 {1, . . . ,n} | xi 6= 0}| ,\nsimply counts the nonzero entries of its argument. It is a poor choice since it is non-continuous, prone to random noise and fails to fulfill desirable properties of meaningful sparseness measures (Hurley and Rickard 2009).\nThroughout this paper, we will use the smooth, normalized sparseness measure \u03c3 proposed by Hoyer (2004): \u03c3 : Rn \\{0}\u2192 [0, 1] , x 7\u2192 \u221a\nn\u2212 \u2016x\u20161/\u2016x\u20162\u221a n\u22121 .\nHere, \u2016\u00b7\u20161 and \u2016\u00b7\u20162 denote the Manhattan norm and the Euclidean norm, respectively. The normalization has been designed such that \u03c3 attains values between zero and one. When x \u2208Rn satisfies \u03c3(x) = 1, then all entries of x but one vanish. Conversely, when \u03c3(x) = 0, then all the entries of x are equal. The function \u03c3 interpolates smoothly between these extremes, see Fig. 1. Moreover, it is scale-invariant so that the same sparseness degree is obtained when a vector is multiplied with a nonzero number. Hence if a quantity is given in other units, for example in millivolts instead of volts, no adjustments whatsoever have to be made.\nThe sparseness degree with respect to \u03c3 does not change much if a small amount is added to all entries of a vector, whereas the L0 pseudo-norm would indicate that the new vector is completely non-sparse. These properties render Hoyer\u2019s sparseness measure intuitive, especially for nonexperts. It has been employed successfully for dictionary learning (Hoyer 2004; Potluru et al. 2013), and its smoothness results in improved generalization capabilities in classification tasks compared to when the L0 pseudo-norm is used (Thom and Palm 2013).\n1.2 Explicit Sparseness Constraints and Projections\nA common approach to dictionary learning is the minimization of the reproduction error between the original samples from a learning set and their approximations provided by a linear generative model under sparseness constraints (Olshausen and Field 1996, 1997; Kreutz-Delgado et al. 2003; Mairal et al. 2009a). It is beneficial for practitioners and endusers to enforce explicit sparseness constraints by demanding that all the code words h in a generative model possess a target sparseness degree of \u03c3H \u2208 (0, 1). This leads to optimization problems of the form\nmin W,h \u2016x\u2212Wh\u201622 so that \u03c3(h) = \u03c3H .\nHere, the objective function is the reproduction error implemented as Euclidean distance. Implicit sparseness constraints, on the other hand, augment the reproduction error with an additive penalty term, yielding optimization problems such as\nmin W,h \u2016x\u2212Wh\u201622 +\u03bb \u2016h\u20161 .\nHere, \u03bb > 0 is a trade-off constant and the Manhattan norm is used to penalize non-sparse code words as convex relaxation of the L0 pseudo-norm (Donoho 2006).\nTrading off the reproduction error against an additive sparseness penalty is non-trivial since the actual resulting code word sparseness cannot easily be predicted. Explicit constraints guarantee that the adjusted sparseness degree is met, making tuning of intransparent scale factors such as \u03bb\nin the example above obsolete. This way, one can concentrate on the actual application of the theory rather than having to develop an intuition of the meaning of each and every parameter.\nThe mathematical tool to achieve explicit sparseness is a sparseness-enforcing projection operator. This is a vectorvalued function which maps any given point in Euclidean space to its nearest point that achieves a pre-set target sparseness degree. One use case of this theory is projected gradient methods (Bertsekas 1999), where a given objective function should be optimized subject to hard side conditions. Replacing the parameters with their best approximations lying in a certain set after each update step ensures that the constraints are satisfied during optimization progress.\n1.3 Contributions of this Paper and Related Work\nThis paper studies dictionary learning under explicit sparseness constraints with respect to Hoyer\u2019s sparseness measure \u03c3 . A major part of this work is devoted to the efficient algorithmic computation of the sparseness-enforcing projection operator, which is an integral part in efficient dictionary learning. Several algorithms were proposed in the past to solve the projection problem (Hoyer 2004; Theis et al. 2005; Potluru et al. 2013; Thom and Palm 2013). Only Thom and Palm (2013) provided a complete and mathematically satisfactory proof of correctness for their algorithm. Moreover, all known algorithms have at least quasi-linear time complexity in the dimensionality of the vector that should be projected.\nIn this paper, we first derive a characterization of the sparseness projection and demonstrate that its computation is equivalent to finding the root of a real-valued auxiliary function, which constitutes a much simpler problem. This result is used in the proposition of an algorithm for the projection operator that is asymptotically optimal in the sense of complexity theory, that is the time complexity is linear and the space complexity is constant in the problem dimensionality. We show through experiments that when run on a real computing machine the newly proposed algorithm is far superior in its computational demands to previously proposed techniques, even for small problem dimensionalities.\nExisting approaches to dictionary learning that feature explicit sparseness constraints can be categorized into ones that use Hoyer\u2019s \u03c3 and ones that employ the L0 pseudonorm. Hoyer (2004) and Potluru et al. (2013) considered matrix factorization frameworks subject to \u03c3 constraints, with space requirements linear in the number of learning samples which prevents processing large data sets. Thom and Palm (2013) designed sparse code words as the result of the sparseness-enforcing projection operator applied to the product of the dictionary with the samples. This requires\nthe computation of the projection\u2019s gradient during learning, which is feasible yet difficult to implement and has nonnegligible adverse effects on the execution time.\nThe following approaches consider explicit L0 pseudonorm constraints: Aharon et al. (2006), Skretting and Engan (2010) and Coates and Ng (2011) infer sparse code words in each iteration compatible to the data and dictionary by employing basis pursuit or matching pursuit algorithms, which has a negative impact on the processing time. Zelnik-Manor et al. (2012) consider block-sparse representations, here the signals are assumed to reside in the union of few subspaces. Duarte-Carvajalino and Sapiro (2009) propose to simultaneously learn the dictionary and the sensing matrix from example image data, which results in improved reconstruction results in compressed sensing scenarios.\nIn addition to the contributions on sparseness projection computation, this paper proposes the Easy Dictionary Learning (EZDL) algorithm. Our technique aims at dictionary learning under explicit sparseness constraints in terms of Hoyer\u2019s sparseness measure \u03c3 using a simple, fast-tocompute and biologically plausible Hebbian-like learning rule. For each presented learning sample, the sparsenessenforcing projection operator has to be carried out. The ability to perform projections efficiently makes the proposed learning algorithm particularly efficient: 30 % less training time is required in comparison to the optimized Online Dictionary Learning method of Mairal et al. (2009a).\nExtensions of Easy Dictionary Learning facilitate alternative representations such as topographic atom organization or atom sparseness, which includes for example separable filters. We furthermore demonstrate the competitiveness of the dictionaries learned with our algorithm with those computed with alternative sophisticated dictionary learning algorithms in terms of reproduction and denoising quality of natural images. Since other tasks, such as deblurring or super-resolution, build upon the same optimization problem in the application phase as reproduction and denoising, it can be expected that EZDL dictionaries will exhibit no performance degradations in those tasks either.\nThe remainder of this paper is structured as follows. Section 2 derives a linear time and constant space algorithm for the computation of sparseness-enforcing projections. In Sect. 3, the Easy Dictionary Learning algorithm for explicitly sparseness-constrained dictionary learning is proposed. Section 4 reports experimental results on the performance of the newly proposed sparseness projection algorithm and the Easy Dictionary Learning algorithm. Section 5 concludes the paper with a discussion, and the appendix contains technical details of the mathematical statements from Sect. 2."}, {"heading": "2 Efficient Sparseness-Enforcing Projections", "text": "This section proposes a linear time and constant space algorithm for computation of projections onto level sets of Hoyer\u2019s \u03c3 . Formally, if \u03c3\u2217 \u2208 (0, 1) denotes a target sparseness degree and x \u2208 Rn is an arbitrary point, the point from the level set S := { s \u2208 Rn | \u03c3(s) = \u03c3\u2217 } that minimizes the Euclidean distance to x is sought. The function that computes argmins\u2208S \u2016x\u2212 s\u20162 is also called sparseness-enforcing projection operator since the situation where \u03c3(x) < \u03c3\u2217 is of particular interest.\nDue to symmetries of \u03c3 , the above described optimization problem can be reduced to finding the projection of a vector x \u2208 Rn\u22650 with non-negative coordinates onto the set\nT := { s \u2208 Rn\u22650 \u2223\u2223 \u2016s\u20161 = \u03bb1 and \u2016s\u20162 = \u03bb2 }\u2286 S,\nwhere \u03bb1 and \u03bb2 are target norms that should be chosen such that \u03c3\u2217 = ( \u221a n\u2212 \u03bb1/\u03bb2)/( \u221a n\u22121) (Hoyer 2004; Thom and Palm 2013). With this choice, \u03c3 constantly attains the value of \u03c3\u2217 on the entire set T , hence T is a subset of S.\nThom and Palm (2013) proved that T is the intersection of a scaled canonical simplex and a hypercircle. They further demonstrated that projections onto T can be computed with a finite number of alternating projections onto these two geometric structures. The proof of correctness of this method could not rely on the classical alternating projection method (see for example Deutsch 2001) due to the lacking convexity of T , rendering the proof arguments quite complicated.\nThe remainder of this section proceeds as follows. First, a succinct characterization of the sparseness projection is given. It is then shown that computing sparseness projections is equivalent to finding the zero of a monotone realvalued function. We prove that this can be achieved with optimal asymptotic complexity by proposing a new algorithm that solves the projection problem.\n2.1 Representation of the Projection\nThe main theoretical result of this paper is a representation theorem that characterizes the projection onto T . This was gained through an analysis of the intermediate points that emerge from the alternating projections onto a simplex and a hypercircle. A closed-form expression can then be provided by showing that the intermediate points in the projection algorithm satisfy a loop-invariant, a certain property fulfilled after each step of alternating projections.\nSince a mathematical rigorous treatment of this result is very technical, it is deferred to the appendix. We assume here that the vector x to be projected is given so that its projection is unique. Since it is guaranteed that for all points except for a null set there is exactly one projection (Theis\net al. 2005, Theorem 2.6), we can exclude points with nonunique projections in our considerations, which is no restriction in practice. We may now state a characterization of the projection outcome:\nRepresentation Theorem Let x \u2208Rn\u22650 and let p \u2208 T denote the unique projection of x onto T . Then there is exactly one real number \u03b1\u2217 with\np = \u03bb2 \u2016q\u20162 \u00b7q where q := max(x\u2212\u03b1\u2217 \u00b7 e, 0) \u2208 Rn.\nHere, e\u2208 {1}n is the n-dimensional vector where all entries are unity. If the indices of the positive coordinates of p are known, then \u03b1\u2217 can be computed directly from x with an explicit expression.\nIn words, the projection p is the point q rescaled to obtain an L2 norm of \u03bb2. The vector q is computed from the input vector x by subtracting the scalar \u03b1\u2217 from all the entries, and afterwards setting negative values to zero. It is remarkable that the projection admits such a simple representation although the target set for the projection is non-convex and geometrically quite complicated.\nThe function that maps a scalar \u03be \u2208 R to max(\u03be \u2212 t, 0) for a constant offset t \u2208 R is called the soft-shrinkage function. If t = 0, it is also called the rectifier function. Because the central element of the projection representation is a soft-shrinkage operation applied entry-wise to the input vector, carrying out projections onto level sets of Hoyer\u2019s sparseness measure can be interpreted as denoising operation (Donoho 1995; Hyv\u00e4rinen 1999; Elad 2006).\n2.2 Auxiliary Function for the Sparseness Projection\nThe projection problem can hence be reduced to determining the soft-shrinkage offset \u03b1\u2217, which is a one-dimensional problem on the real line. Further, it is reasonable that \u03b1\u2217 must be smaller than the maximum entry of x since otherwise q would be the null vector, which is absurd. In the usual case where \u03c3(x)< \u03c3\u2217 we can further conclude that \u03b1\u2217 must be non-negative. Otherwise, the result of the projection using the representation theorem would be less sparse than the input, which is impossible. Therefore, the projection problem can be further reduced to finding a number from the bounded interval [0, xmax) with xmax := maxi\u2208{1,...,n} xi.\nNext, a method for efficiently deciding whether the correct offset has been found is required. Similar to the projection onto a canonical simplex (Liu and Ye 2009), we can design a real-valued function that vanishes exactly at the wanted offset. The properties of Hoyer\u2019s \u03c3 allow us to formulate this function in an intuitive way. We call\n\u03a8 : [0, xmax)\u2192 R, \u03b1 7\u2192 \u2016max(x\u2212\u03b1 \u00b7 e, 0)\u20161 \u2016max(x\u2212\u03b1 \u00b7 e, 0)\u20162 \u2212 \u03bb1 \u03bb2 ,\n-0.75\n-0.50\n-0.25\n0.00\n0.25\n0.50\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nA u\nx ili\na ry\nF u\nn c ti o\nn\nOffset of Soft-Shrinkage \u03b1\n\u03a8' \u03a8\nxj xk\u03b1*\nFig. 2 Plot of the auxiliary function \u03a8 and its derivative for a random vector x. The derivative \u03a8 \u2032 was scaled using a positive number for improved visibility. The steps in \u03a8 \u2032 are exactly the places where \u03b1 coincides with an entry of x. It is enough to find an \u03b1 with \u03a8(x j) \u2265 0 and \u03a8(xk)< 0 for the neighboring entries x j and xk in x, because then the exact solution \u03b1\u2217 can be computed with a closed-form expression.\nthe auxiliary function for the projection onto T . The rationale for the concrete definition of \u03a8 is as fol-\nlows. Due to the representation theorem we know that the projection p onto T is merely a scaled version of the point q := max(x\u2212\u03b1\u2217 \u00b7 e, 0). Moreover \u03c3(p) = \u03c3\u2217, and due to the scale-invariance of Hoyer\u2019s \u03c3 follows \u03c3(q) = \u03c3\u2217. The essence of \u03c3 is the ratio of the L1 norm to the L2 norm of its argument. Hence here the scale constants that make \u03c3 normalized to the interval [0, 1] are omitted and the target norms are used instead. We therefore have that \u2016q\u20161/\u2016q\u20162 = \u03bb1/\u03bb2, and thus \u03a8(\u03b1\u2217) = 0. As \u03b1\u2217 is unique, we can conclude that no other offset of the soft-shrinkage operation leads to a correct solution. In fact, if \u03b1 6= \u03b1\u2217 and when we write q\u0303 := max(x\u2212\u03b1 \u00b7 e, 0), then we have that \u03c3(q\u0303) 6= \u03c3\u2217 and therefore \u03a8(\u03b1) 6= 0.\nAn exemplary plot of \u03a8 is depicted in Fig. 2. Clearly, the auxiliary function is continuous, differentiable except for isolated points and strictly decreasing except for its final part where it is constant. Since the constant part is always negative and starts at the offset equal to the second-largest entry x2nd-max := max{xi | i \u2208 {1, . . . ,n} and xi 6= xmax } of x, the feasible interval can be reduced even more. The step discontinuities in \u03a8 \u2032 coincide exactly with the entries of the input vector x. These appealing analytical properties greatly simplify computation of the zero of \u03a8 , since standard rootfinding algorithms such as bisection or Newton\u2019s method (see for example Traub 1964) can be employed to numerically find \u03b1\u2217.\n2.3 Linear Time and Constant Space Projection Algorithm\nWe can improve on merely a numerical solution by exploiting the special structure of \u03a8 to yield an analytical solution to the projection problem. This is due to the closed-form\nexpression for \u03b1\u2217 which requires the indices of the coordinates in which the result of the projection is positive to be known. The other way around, when \u03b1\u2217 is known, these indices are exactly the ones of the coordinates in which x is greater than \u03b1\u2217. When an offset sufficiently close to \u03b1\u2217 can be determined numerically, the appropriate index set can be determined and thus the exact value of \u03b1\u2217.\nThe decision if a candidate offset \u03b1 is close enough to the true \u03b1\u2217 can be made quite efficiently, since \u03b1\u2217 is coupled via index sets to individual entries of the input vector x. Hence, it suffices to find the right-most left neighbor x j of \u03b1 in the entries of x, and analogously the left-most right neighbor xk (see Fig. 2 for an example). Whenever \u03a8(x j)\u2265 0 and \u03a8(xk)< 0, the zero \u03b1\u2217 of \u03a8 must be located between x j and xk for continuity reasons. But then all values in x greater than \u03b1\u2217 are exactly the values greater than or equal to xk, which can be determined by simply scanning through the entries of x.\nBased upon these considerations, a flowchart of our proposed method for computing sparseness-enforcing projections is depicted in Fig. 3. The algorithm performs bisection and continuously checks for sign changes in the auxiliary function \u03a8 . As soon as this is fulfilled, \u03b1\u2217 is computed and the result of the projection is determined in-place. A complete formal presentation and discussion of our proposed algorithm is given in the appendix.\nEach intermediate step of the algorithm (that is, determination of the second-largest entry of x, evaluation of the auxiliary function, projection result computation by appli-\ncation of soft-shrinkage and scaling) can be carried out in a time complexity linear in the problem dimensionality n and a space complexity completely independent of n.\nTherefore, to show the overall algorithm possesses the same asymptotic complexity, it has to be proved that the number of bisection iterations required for finding \u03b1\u2217 is independent of n. The length of the initial interval is upperbounded by x2nd-max as discussed earlier. Bisection is moreover guaranteed to stop at the latest once the interval length is smaller than the minimum pairwise distance between distinct entries of x,\n\u03b4 := min { xk\u2212 x j \u2223\u2223 x j,xk \u2208X and x j < xk }> 0\nwhere X := {xi | i \u2208 {1, . . . ,n}}. This is due to the fact that it is enough to find a sufficiently small range to deduce an analytical solution. The required number of bisection iterations is then less than dlog2(x2nd-max/\u03b4 )e, see Liu and Ye (2009). This number is bounded from above regardless of the dimensionality of the input vector since x2nd-max is upper-bounded and \u03b4 is lower-bounded due to finite machine precision (Goldberg 1991).\nHence our sparseness-enforcing projection algorithm is asymptotically optimal in the sense of complexity theory. There may still be hidden constants in the asymptotic notation that render the proposed algorithm less efficient than previously known algorithms for a small input dimensionality. Experiments in Sect. 4 demonstrate that this is not the case."}, {"heading": "3 Explicitly Sparseness-Constrained Dictionary Learning", "text": "This section proposes the Easy Dictionary Learning (EZDL) algorithm for dictionary learning under explicit sparseness constraints. First, we introduce an ordinary formulation of the learning algorithm. Sparse code word inference with the sparseness projection algorithm proposed in Sect. 2 renders EZDL efficient and particularly simple to implement. We further discuss extensions that allow concentrating on different aspects of the data set under investigation, such as topographic organization of the atoms and atom sparseness. Only little implementation effort is required for these extensions and their computational demands are low. A description of the comprehensive EZDL learning algorithm is accompanied with several strategies that improve the optimization performance.\n3.1 EZDL\u2014Easy Dictionary Learning\nConventional dictionary learning algorithms operate in an alternating fashion, where code words are inferred with a\ncostly optimization procedure before updating the dictionary. EZDL learns a dictionary by first yielding sparse code words through simple inference models and then tuning the dictionary with a simple update step. An inference model is a function I : Rn \u2192 Rn which accepts filter responses in the form of the product of the dictionary W \u2208 Rd\u00d7n with a learning sample x \u2208 Rd , u := W T x \u2208 Rn, and produces a representation h := I(u) with certain desirable properties. Here, the combination with the sparseness-enforcing projection operator is particularly interesting since it provides a natural method to fulfill explicit sparseness constraints.\nWe call the choice I(u) := \u03a0\u03c3H (u) the ordinary inference model, where \u03a0\u03c3H denotes the Euclidean projection onto the set of all vectors that achieve a sparseness degree of \u03c3H \u2208 (0, 1) with respect to Hoyer\u2019s sparseness measure \u03c3 . This computation can also be interpreted as the trivial first iteration of a projected Landweber procedure for sparse code word inference (Bredies and Lorenz 2008).\nThe dictionary W is adapted to new data by minimizing the deviation between a learning sample x and its approximation Wh through the linear generative model. The goodness of the approximation is here assessed with a differentiable similarity measure \u03c1 : Rd \u00d7Rd \u2192 R, so that the EZDL optimization problem becomes:\nmin W \u03c1(Wh, x).\nNote that h is not a variable of the optimization problem since it is defined as output of an inference model. For the same reason, h does not need to be constrained further as it inherently satisfies explicit sparseness constraints.\nAlthough a wide range of similarity measures is possible, we decided to use the (Pearson product-moment) correlation coefficient since it is invariant to affine-linear transformations (Rodgers and Nicewander 1988). In the context of visual data set analysis, this corresponds to invariance with respect to DC components and gain factors. Moreover, differentiability of this similarity measure facilitates gradientbased learning (Thom and Palm 2013).\nIf \u03c1 \u2032 : Rd \u00d7Rd \u2192 Rd is the transposed derivative of \u03c1 with respect to its first argument and g := \u03c1 \u2032(Wh, x) \u2208 Rd denotes its value in Wh, the gradient of EEZDL for the dictionary is given by(\n\u2202EEZDL \u2202W\n)T = ghT \u2208 Rd\u00d7n.\nTherefore, when W is tuned with gradient descent a simple and biologically plausible Hebbian-like learning rule (Hyv\u00e4rinen et al. 2009) results:\nW new :=W \u2212\u03b7 \u00b7ghT , where \u03b7 > 0 denotes the step size.\nThis update step can be implemented efficiently with simple rank-1 updates (Blackford et al. 2002).\nWe here assumed h = I(W T x) to be constant and neglected that it actually depends on W . Without this assumption, the gradient would have to be extended with an additive term which comprises the gradient of the inference model (Thom and Palm 2013, Proposition 39). This, in turn, requires the computation of the sparseness-enforcing projection operator\u2019s gradient. This gradient can be computed with simple vector operations as we show in the appendix. However, this constitutes a significant computational burden compared to the simple and fast-to-compute EZDL update step. Section 4 demonstrates through experiments that this simplification is still perfectly capable of learning dictionaries.\n3.2 Topographic Atom Organization and Atom Sparseness\nTopographic organization of the dictionary\u2019s atoms similar to Self-organizing Maps (Kohonen 1990) or Topographic Independent Component Analysis (Hyv\u00e4rinen et al. 2001) can be achieved in a straightforward way with EZDL using alternative inference models proposed in this section. For this, the dictionary atoms are interpreted to be arranged on a two-dimensional grid. A spatial pooling operator subject to circular boundary conditions can then be used to incorporate interactions between atoms located adjacent on the grid. This can, for example, be realized by averaging each entry of the filter responses in a 3\u00d7 3 neighborhood on the grid. This convolution operation can be expressed as linear operator applied to the vector u \u2208 Rn, represented by a sparsely populated matrix G \u2208 Rn\u00d7n. With G containing all atom interactions, we call I(u) := \u03a0\u03c3H (Gu) the topographic inference model.\nAn alternative realization of topographic organization is enforcing structure sparseness on the filter responses reshaped as matrix to account for the grid layout. Structure sparseness can be assessed by application of a sparseness measure to the vector with the singular values of a matrix. When the L0 pseudo-norm is used, then this is the rank of a matrix. The matrix rank can also be measured more robustly through the ratio of the Schatten 1-norm to the Schatten 2- norm, which is essentially Hoyer\u2019s sparseness measure \u03c3 applied to the singular values (Lopes 2013).\nClearly, any matrix M \u2208 Ra\u00d7b where a,b \u2208 N can be reshaped to a vector m := vec(M) \u2208 Rab with the vectorization operator that stacks all columns of M on top of another (Neudecker 1969). This linear operation can be inverted provided the shape of the original matrix is known, M = vec\u22121a\u00d7b(m). In the following, let \u03a0\u03ba\u2217 denote the projection onto the set of all matrices that possess a target rank \u03ba\u2217 \u2208 {1, . . . ,min{a,b}}. A classical result states that any matrix can be projected onto the set of low-rank matrices by computation of its Singular Value Decomposition, applying an L0 pseudo-norm projection to the vector of singular\nvalues thus retaining only the largest ones, and finally computing the reconstruction using the modified singular values (Eckart and Young 1936).\nBased on these considerations, let r and c with n = rc be natural numbers describing the topographic grid layout dimensionalities. The rank-\u03baH topographic inference model is then defined by the composition I(u) := ( vec\u25e6\u03a0\u03baH \u25e6vec \u22121 r\u00d7c \u25e6\u03a0\u03c3H ) (u).\nIn words, this inference model operates as follows. It first approximates the filter responses u with a sparsely populated vector that attains a sparseness of \u03c3H with respect to Hoyer\u2019s \u03c3 . These sparsified filter responses are then laid out on a grid and replaced with those best approximating filter responses that meet a lower rank of \u03baH \u2208 {1, . . . ,min{r,c}}. Finally, the grid layout is reshaped to a vector since this should be the output type of each inference model.\nIf \u03baH = 1, then there are two vectors y \u2208 Rr and z \u2208 Rc such that vec\u22121r\u00d7c (I(u)) = yz\nT , that is the filter responses can be expressed as a dyadic product when reshaped to a grid. We will demonstrate with experiments that this results in an interesting topographic atom organization similar to Independent Subspace Analysis (Hyv\u00e4rinen and Hoyer 2000).\nAnalogous to Non-negative Matrix Factorization with Sparseness Constraints (Hoyer 2004), the atoms can be enforced to be sparse as well. To achieve this, it is sufficient to apply the sparseness projection to each column of the dictionary after each learning epoch:\nW newei := \u03a0\u03c3W (Wei) for all i \u2208 {1, . . . ,n},\nwhere ei \u2208 Rn is the i-th canonical basis vector that selects the i-th column from W , and \u03c3W \u2208 (0, 1) is the target degree of atom sparseness.\nIn the situation where the learning samples resemble image patches, the atoms can also be restricted to fulfill structure sparseness using a low-rank approximation. Suppose the image patches possess ph\u00d7 pw pixels, then the following projection can be carried out after each learning epoch: W newei := ( vec\u25e6\u03a0\u03baW \u25e6vec \u22121 ph\u00d7pw ) (Wei) for all i\u2208 {1, . . . ,n}.\nHere, \u03baW \u2208 {1, . . . ,min{ ph, pw }} denotes the target rank of each atom when reshaped to ph\u00d7 pw pixels. When the dictionary is used to process images through convolution, small values of \u03baW are beneficial since computations can then be speeded up considerably. For example, if \u03baW = 1 each atom can be expressed as outer product of vectors from Rph and Rpw , respectively. The convolutional kernel is separable in this case, and two one-dimensional convolutions lead to the same result as one two-dimensional convolution, but require only a fraction of operations (Hoggar 2006).\n3.3 Learning Algorithm\nIn the previous sections, a simple Hebbian-like learning rule was derived which depends on abstract inference models. The core of the inference models is the sparseness-enforcing projection operator discussed in Sect. 2, which guarantees that the code words always attain a sparseness degree easily controllable by the user. On presentation of a learning sample x, an update step of the dictionary W hence consists of\n(a) determining the filter responses (u :=W T x), (b) feeding the filter responses through the inference model\ninvolving a sparseness projection (h := I(u)), (c) computation of the approximation to the original learn-\ning sample (Wh) using the linear generative model, (d) assessing the similarity between the input sample and its\napproximation (\u03c1(Wh, x)) and the similarity measure\u2019s gradient (g := \u03c1 \u2032(Wh, x)), and eventually\n(e) adapting the current dictionary using a rank-1 update (W new :=W \u2212\u03b7 \u00b7ghT ).\nAll these intermediate steps can be carried out efficiently. The sparseness projection can be computed using few resources, and neither its gradient nor the gradient of the entire inference model is required. After each learning epoch it is possible to impose additional constraints on the atoms with atom-wise projections. In doing so, several aspects of the data set structure can be made more explicit since the atoms are then forced to become more simple feature detectors.\nAn example of an individual learning epoch is given in Algorithm 1, where the topographic inference model and low-rank structure sparseness are used. In addition, the explicit expressions for the correlation coefficient and its gradient are given (Thom and Palm 2013, Proposition 40). The outcome of this parameterization applied to patches from natural images is depicted in Fig. 9b and will be discussed in Sect. 4.2.3.\nThe efficiency of the learning algorithm can be improved with simple modifications described in the following. To reduce the learning time compared to when a randomly initialized dictionary was used, the dictionary should be initialized by samples randomly chosen from the learning set (Mairal et al. 2009a; Skretting and Engan 2010; Coates and Ng 2011; Thom and Palm 2013). When large learning sets with millions of samples are available, stochastic gradient descent has been proven to result in significantly faster optimization progress compared to when the degrees of freedom are updated only once for each learning epoch (Wilson and Martinez 2003; Bottou and LeCun 2004). The step size schedule \u03b7(\u03bd) := \u03b70/\u03bd, where \u03b70 > 0 denotes the initial step size and \u03bd \u2208 N \\ {0} is the learning epoch counter, is beneficial when the true gradient is not available but rather an erroneous estimate (Bertsekas 1999). After each learning epoch, all the atoms of the dictionary are normalized to\nAlgorithm 1 One epoch of the Easy Dictionary Learning algorithm, illustrated for the case of a topographic inference model and low-rank structure sparseness.\nInput: The algorithm accepts the following parameters: \u2013 An existing dictionary W \u2208 Rd\u00d7n with n atoms. \u2013 Random access to a learning set with samples from Rd\nthrough a function \"pick_sample\". \u2013 Step size for update steps \u03b7 \u2208 R>0. \u2013 Number of samples to be presented Mepoch \u2208 N. \u2013 For the topographic inference model:\n\u2013 Target degree of dictionary sparseness \u03c3H \u2208 (0, 1). \u2013 Topography matrix G \u2208 Rn\u00d7n describing the\ninteraction between adjacent atoms. \u2013 To obtain low-rank structure sparseness:\n\u2013 Number of pixels ph\u00d7 pw so that d = ph pw. \u2013 Target rank \u03baW \u2208 {1, . . . ,min{ ph, pw }}.\nOutput: Updated dictionary W \u2208 Rd\u00d7n. // Present samples and adapt dictionary.\n1 repeat Mepoch times // Pick a random sample. 2 x := pick_sample() \u2208 Rd ; // (a) Compute filter responses. 3 u :=W T x \u2208 Rn; // (b) Evaluate topographic inference model. 4 h := \u03a0\u03c3H (Gu) \u2208 Rn; // (c) Compute approximation to x. 5 x\u0303 :=Wh \u2208 Rd ; // (d) Compute correlation coefficient\n\u03c1x\u0303, x := \u03c1(Wh, x) and its gradient g. Here, e \u2208 {1}d is the all-ones vector.\n6 \u03bb := \u2016x\u0303\u201622\u2212 1/d \u00b7 ( eT x\u0303 )2 \u2208 R;\n7 \u00b5 := \u2016x\u201622\u2212 1/d \u00b7 ( eT x )2 \u2208 R;\n8 \u03c1x\u0303, x := ( x\u0303T x\u2212 1/d \u00b7 ( eT x\u0303 )( eT x )) / \u221a\n\u03bb \u00b5 \u2208 R; 9 g := 1\u221a\n\u03bb \u00b5\n( x\u2212 eT x/d \u00b7 e ) \u2212 \u03c1x\u0303, x\u03bb ( x\u0303\u2212 eT x\u0303/d \u00b7 e ) \u2208 Rd ;\n// (e) Adapt dictionary with rank-1 update. 10 W :=W \u2212\u03b7 \u00b7ghT \u2208 Rd\u00d7n; 11 end\n// Atom-wise projection for low-rank filters. 12 for i := 1 to n do // Extract i-th atom and normalize. 13 w := Wei/\u2016Wei\u20162 \u2208 Rd ;\n// Ensure atom has rank \u03baW when reshaped to ph\u00d7 pw pixels.\n14 w := ( vec\u25e6\u03a0\u03baW \u25e6vec \u22121 ph\u00d7pw ) (w) \u2208 Rd ; // Store modified atom back in dictionary. 15 Wei := w; 16 end\nunit scale. Since the learning rule is Hebbian-like, this prevents the atoms from growing arbitrarily large or becoming arbitrarily small (Hyv\u00e4rinen et al. 2009). This normalization step is also common in a multitude of alternative dictionary learning algorithms (Kreutz-Delgado et al. 2003; Hoyer 2004; Mairal et al. 2009a; Skretting and Engan 2010).\nSince the dictionary is modified with a rank-1 update where one of the factors is the result of an inference model and hence sparse, only the atoms that induce significant fil-\nter responses are updated. This may result in atoms that are never updated when the target sparseness degree \u03c3H is large. This behavior can be alleviated by adding random noise to the inference model\u2019s output prior to updating the dictionary, which forces all the atoms to be updated. We used random numbers sampled from a zero-mean normal distribution, where the standard deviation was multiplicatively annealed after each learning epoch. As optimization proceeds, the atoms are well-distributed in sample space such that the lifetime sparseness is approximately equal to the population sparseness (Willmore and Tolhurst 2001), and randomness is not needed anymore."}, {"heading": "4 Experimental Results", "text": "This section reports experimental results on the techniques proposed in this paper. We first evaluate alternative solvers for the sparseness projection\u2019s auxiliary function and show that our algorithm for the sparseness-enforcing projection operator is significantly faster than previously known methods. We then turn to the application of the projection in the Easy Dictionary Learning algorithm. First, the morphology of dictionaries learned on natural image patches is analyzed and put in context with previous methods. We further show that the resulting dictionaries are well-suited for the reproduction of entire images. They achieve a reproduction quality equivalent to dictionaries trained with alternative, significantly slower algorithms. Eventually, we analyze the performance of the dictionaries when employed for the image denoising task and find that, analogous to the reproduction experiments, no performance degradation can be observed.\n4.1 Performance of Sparseness-Enforcing Projections\nOur proposed algorithm for sparseness projections was implemented as C++ program using the GNU Scientific Library (Galassi et al. 2009). We first analyzed whether solvers other than bisection for locating the zero of the auxiliary function would result in an improved performance. Since \u03a8 is differentiable except for isolated points and its derivatives can be computed quite efficiently, Newton\u2019s method and Halley\u2019s method are straightforward to apply. We further verified whether Newton\u2019s method applied to a slightly transformed variant of the auxiliary function,\n\u03a8\u0303 : [0, xmax)\u2192 R, \u03b1 7\u2192 \u2016max(x\u2212\u03b1 \u00b7 e, 0)\u201621 \u2016max(x\u2212\u03b1 \u00b7 e, 0)\u201622 \u2212 \u03bb 2 1 \u03bb 22 ,\nwhere the minuend and the subtrahend were squared, would behave more efficiently. The methods based on derivatives were additionally safeguarded with bisection to guarantee new positions are always located within well-defined intervals (Press et al. 2007). This impairs the theoretical property\nthat only a constant number of steps is required to find a solution, but in practice a significantly smaller number of steps needs to be made compared to plain bisection.\nTo evaluate which solver would provide maximum efficiency in practical scenarios, one thousand random vectors for each problem dimensionality n \u2208 {22, . . . ,230} were sampled and the corresponding sparseness-enforcing projections for \u03c3\u2217 := 0.90 were computed using all four solvers. We have used the very same random vectors as input for all solvers, and counted the number of times the auxiliary function needed to be evaluated until the solution was found. The results of this experiment are depicted in Fig. 4.\nThe number of function evaluations required by plain bisection grew about linearly in log(n). Because the expected minimum difference of two distinct entries from a random vector gets smaller when the dimensionality of the random vector is increased, the expected number of function evaluations bisection needs increases with problem dimensionality. In either case, the length of the interval that has to be found is always bounded from below by the machine precision such that the number of function evaluations with bisection is bounded from above regardless of n.\nThe solvers based on the derivative of \u03a8 or \u03a8\u0303 , respectively, always required less function evaluations than bisection. They exhibited a growth clearly sublinear in log(n). For n= 230\u2248 109, Newton\u2019s method required 11 function evaluations in the mean, Halley\u2019s method needed 9 iterations, and Newton\u2019s method applied to \u03a8\u0303 found the solution in only 4.5 iterations. Therefore, in practice always the latter solver should be employed.\nIn another experiment, the time competing algorithms require for computing sparseness projections on a real computing machine was measured for a comparison. For this, the algorithms proposed by Hoyer (2004), Potluru et al. (2013), and Thom and Palm (2013) were implemented using the\nGNU Scientific Library (Galassi et al. 2009) by means of C++ programs. An Intel Core i7-4960X processor was used and all algorithms were run in a single-threaded environment. Random vectors were sampled for each problem dimensionality n \u2208 {22, . . . ,230} and initially projected to attain a sparseness of 0.50 with respect to Hoyer\u2019s \u03c3 . This initial projection better reflects the situation in practice where not completely random vectors have to be processed. Next, all four algorithms were used to compute projections with a target sparseness degree of \u03c3\u2217 := 0.90 and their run time was measured. The original algorithm of Hoyer (2004) was the slowest, so by taking the ratio of the run times of the other algorithms to the run time of that slowest algorithm the relative speed-up was obtained.\nFigure 5 visualizes the results of this experiment. For all tested problem dimensionalities, the here proposed linear time algorithm dominated all previously described methods. While the speed-up of the algorithms of Potluru et al. (2013) and Thom and Palm (2013) relative to the original algorithm of Hoyer (2004) were already significant, especially for small to medium dimensionalities, they got relatively slow for very large vectors. This is not surprising, because both methods start with sorting the input vector and have to store that permutation to be undone in the end.\nThe algorithm proposed in this paper is based on rootfinding of a monotone function and requires no sorting. Only the left and right neighbors of a scalar in a vector have to be found. This can be achieved by scanning linearly through the input vector, which is particularly efficient when huge vectors have to be processed. For n = 230, the proposed algorithm was more than 15 times faster than the methods of Hoyer (2004), Potluru et al. (2013) and Thom and Palm (2013). Because of this appealing asymptotic behavior, there is now no further obstacle to applying smooth sparseness methods to large-scale problems.\n4.2 Data Set Analysis with EZDL\nWe used the Easy Dictionary Learning algorithm as a data analysis tool to visualize the structure of patches from natural images under different aspects, which facilitates qualitative statements on the algorithm performance. For our experiments, we used the McGill calibrated color image database (Olmos and Kingdom 2004), where the images had either 768\u00d7576 pixels or 576\u00d7768 pixels. The images were desaturated (SMPTE RP 177-1993, Annex B.4) and quantized to 8-bit precision. We extracted 10 000 patches with 16\u00d7 16 pixels from each of the 314 images of the \"Foliage\" collection. The patches were extracted at random positions. Patches with vanishing variance were omitted since they carry no information. In total, we obtained 3.1 million samples for learning.\nWe learned dictionaries on the raw pixel values of this learning set and on whitened image patches. The learning samples were normalized to zero mean and unit variance as the only pre-processing for the raw pixel experiments. The whitened data was obtained using the canonical preprocessing from Section 5.4 of Hyv\u00e4rinen et al. (2009) with 128 principal components. EZDL was applied using each proposed combination of inference model and atom constraints. We presented 30 000 randomly selected learning samples in each of one thousand epochs, which corresponds to about ten sweeps through the entire learning set. The initial step size was set to \u03b70 := 1.\nA noisy version of the final dictionary could already be observed after the first epoch, demonstrating the effectiveness of EZDL for quick analysis. Since the optimization procedure is probabilistic in the initialization and selection of training samples, we repeated the training five times for each parameter set. We observed only minor variations between the five dictionaries for each parameter set.\nIn the following, we present details of dictionaries optimized on raw pixel values and analyze their filter characteristics. Then, we consider dictionaries learned with whitened data and dictionaries with separable atoms trained on raw pixel values and discuss the underlying reasons for their special morphology."}, {"heading": "4.2.1 Raw Pixel Values", "text": "Using the ordinary inference model, we trained two-times overcomplete dictionaries with n := 256 atoms on the normalized pixel values, where the dictionary sparseness degree was varied between 0.700 and 0.999. This resulted in the familiar appearance of dictionaries with Gabor-like filters, which resemble the optimal stimulus of visual neurons (Olshausen and Field 1996, 1997). While for small values of \u03c3H the filters were mostly small and sharply bounded, high sparseness resulted in holistic and blurry filters.\nWe used the methods developed by Jones and Palmer (1987) and Ringach (2002) for a more detailed analysis. In doing so, a two-dimensional Gabor function as defined in Equation (1) of Ringach (2002), that is a Gaussian envelope multiplied with a cosine carrier wave, was fit to each dictionary atom using the algorithm of Nelder and Mead (1965). We verified that these Gabor fits accurately described the atoms, which confirmed the Gabor-like nature of the filters.\nThe differences in the dictionaries due to varying sparseness degrees became apparent through analysis of the parameters of the fitted Gabor functions. Figure 6 shows the mean values of three influential Gabor parameters in dependence on \u03c3H . All parameters change continuously and monotonically with increasing sparseness. The spatial frequency, a factor in the cosine wave of the Gabor function, constantly decreases for increasing dictionary sparseness. The width of the Gaussian envelope, here broken down into the standard deviation in both principal orientations x\u2032 and y\u2032, is monotonically increasing. The envelope width in y\u2032 direction increases earlier, but in the end the envelope width in x\u2032 direction is larger.\nIt can be concluded that more sparse code words result in filters with lower frequency but larger envelope. Since an increased sparseness reduces the model\u2019s effective number of degrees of freedom, it prevents the constrained dictionaries from adapting precisely to the learning data. Similar to Principal Component Analysis, low-frequency atoms here minimize the reproduction error best when only very few effective atoms are allowed (Hyv\u00e4rinen et al. 2009; Olshausen and Field 1996).\nHistograms of the spatial phase of the filters, an additive term in the cosine wave of the Gabor function, are depicted in Fig. 7. For \u03c3H = 0.75, there is a peak at \u03c0/2 which corresponds to odd-symmetric filters (Ringach 2002). The distribution is clearly bimodal for \u03c3H = 0.99, with peaks at 0 and\n\u03c0/2 corresponding to even-symmetric and odd-symmetric filters, respectively. While the case of small \u03c3H matches the result of ordinary sparse coding, the higher dictionary sparseness results in filters with the same characteristics as the optimal stimulus of macaque visual neurons (Ringach 2002).\nThis analysis proves that EZDL\u2019s minimalistic learning rule is capable of generating biologically plausible dictionaries, which constitute a particularly efficient image coding scheme. To obtain dictionaries with diverse characteristics, it is enough to adjust the target degree of dictionary sparseness on a normalized scale. A major component in the learning algorithm is the sparseness projection, enforcing local competition among the atoms (Rozell et al. 2008) due to its absolute order-preservation property (Thom and Palm 2013, Lemma 12)."}, {"heading": "4.2.2 Whitened Image Patches", "text": "Whitening as a pre-processing step helps to reduce sampling artifacts and decorrelates the input data (Hyv\u00e4rinen et al. 2009). It also changes the intuition of the similarity measure in EZDL\u2019s objective function, linear features rather than single pixels are considered where each feature captures a multitude of pixels in the raw patches. This results in differences in the filter structure, particularly in the emergence of more low-frequency filters.\nThe dictionary depicted in Fig. 8a was learned using the topographic inference model with average-pooling of 3\u00d7 3 neighborhoods. We set the dictionary sparseness degree to \u03c3H := 0.85 and the number of atoms to n := 256, arranged on a 16\u00d7 16 grid. The dictionary closely resembles those gained through Topographic Independent Component Analysis (Hyv\u00e4rinen et al. 2001, 2009) and Invariant Predictive Sparse Decomposition (Kavukcuoglu et al. 2009). It should be noted that here the representation is two times overcom-\nplete due to the whitening procedure. Overcomplete representations are not inherently possible with plain Independent Component Analysis (Bell and Sejnowski 1997; Hyv\u00e4rinen et al. 2009) which limits its expressiveness, a restriction that does not hold for EZDL.\nThe emergence of topographic organization can be explained by the special design of the topographic inference model. The pooling operator acts as spatial low-pass filter on the filter responses, so that each smoothed filter response carries information on the neighboring filter responses. Filter response locality is retained after the sparseness projection, hence adjacent atoms receive similar updates with the Hebbian-like learning rule. Hence, there are only small differences between filters within the same vicinity. The learning process is similar to that of Self-organizing Maps (Kohonen 1990), where only the atom with the maximum filter response and the atoms in its direct surrounding are updated. However, EZDL simultaneously updates multiple clusters since the sparse code words laid out on the grid are multimodal.\nFurther, it is notable that we achieved a topographic organization merely through a linear operator G by simple average-pooling. This stands in contrast to the discussion from Bauer and Memisevic (2013) where the necessity of a nonlinearity such as multiplicative interaction or pooling with respect to the Euclidean norm was assumed. Our result that a simple linear operator plugged into the ordinary inference model already produces smooth topographies proves that linear interactions between the atoms are sufficient despite their minimality.\nFigure 8b shows a dictionary obtained with the rank-1 topographic inference model, using a sparseness degree of \u03c3H := 0.85 and n := 256 filters on a 16\u00d716 grid. Here, the sparse code words reshaped to a matrix are required to have unit rank which results in a specially organized filter layout. For example, rows three and four almost exclusively contain low-frequency filters, and the filters in the sixth column are all oriented vertically. The grouping of similar atoms into the same rows and columns is related to Independent Subspace Analysis, which yields groups of Gabor-like filters where all filters in a group have approximately the same frequency and orientation (Hyv\u00e4rinen and Hoyer 2000).\nThe rank-1 topographic inference model guarantees that code words can be expressed as the dyadic product of two vectors, where the factors themselves are sparsely populated because the code words are sparse. This causes the code words to possess a sparse rectangular structure when reshaped to account for the grid layout, that is non-vanishing activity is always concentrated in a few rows and columns. The Hebbian-like learning rule therefore induces similar updates to atoms located in common rows or columns, which explains the obtained group layout."}, {"heading": "4.2.3 Rank-1 Filters on Raw Pixel Values", "text": "Enforcing the filters themselves to have rank one by setting \u03baW := 1 resulted in bases similar to Discrete Cosine Transform (Watson 1994). This was also observed recently by Hawe et al. (2013) who considered a tensor decomposition of the dictionary, and by Rigamonti et al. (2013) who minimized the Schatten 1-norm of the atoms. Note that EZDL merely replaces the atoms after each learning epoch by their best rank-1 approximation. The computational complexity of this operation is negligible compared to an individual learning epoch using tens of thousands of samples and hence does not slow down the actual optimization.\nIf no ordering between the filters was demanded, a multitude of checkerboard-like filters and vertical and horizontal contrast bars was obtained, see Fig. 9a for a dictionary with\nn := 256 atoms and a sparseness degree of \u03c3H := 0.85. Atoms with diagonal structure cannot be present at all in such constrained dictionaries because diagonality requires a filter rank greater than one. Although all filters were paraxial due to the rank-1 constraint, they still resembled contrast fields because of their grating-like appearance. This is due to the representation\u2019s sparseness, which is known to induce this appearance.\nA similar filter morphology was obtained with a topographic filter ordering using a 16\u00d716 grid, though the filters were blurrier, as shown in Fig. 9b. Here, checkerboard-like filters are located in clusters, and filters with vertical and horizontal orientation are spatially adjacent. This is as expected, because with a sparse topography there are only a few local blobs of active entries in each code word, causing similar atoms to be grouped together and dissimilar at-\noms to be distant from each other. In the space of rank-1 filters, there can be either vertical or horizontal structures, or checkerboards combining both principal orientations.\nThe lack of fine details can be explained by the restriction of topographic organization, which reduces the effective number of degrees of freedom. Analogously to the situation in Sect. 4.2.1 where the dictionary sparseness was increased, the reproduction error can be reduced by a large amount using few filters with low frequencies. For the same reason, there are few such checkerboard-like filters: Minimization of the reproduction error is first achieved with principal orientations before checkerboards can be used to encode details in the image patches.\nIn conclusion, these results show that the variety of the dictionaries produced by EZDL can be vast simply by adapting easily interpretable parameters. The algorithm covers and reproduces well-known phenomena from the literature, and allows a precise visualization of a data set\u2019s structure. A first impression of the final dictionaries can already be obtained after one learning epoch, which takes no more than one second on a modern computer.\n4.3 Experiments on the Reproduction of Entire Images\nIt was demonstrated in Sect. 4.2 that the Easy Dictionary Learning algorithm produces dictionaries that correspond to efficient image coding schemes. We now analyze the suitability of EZDL dictionaries trained on raw pixel values for the reproduction of entire images. This is a prerequisite for several image processing tasks such as image enhancement and compression, since here essentially the same optimization problem should be solved as during reproduction with a given dictionary.\nOur analysis allows quantitative statements as the original images and their reproduction through sparse coding can be numerically compared on the signal level. Further, the EZDL dictionaries are compared with the results of the Online Dictionary Learning algorithm by Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algorithm by Skretting and Engan (2010) in terms of reproduction quality and learning speed.\nThe evaluation methodology was as follows. First, dictionaries with different parameter sets were trained on 3.1 million 8\u00d7 8 pixels natural image patches extracted from the \"Foliage\" collection of the McGill calibrated color image database (Olmos and Kingdom 2004). As in Sect. 4.2, 10 000 patches were picked from random positions from each of the desaturated and quantized images. All patches were normalized to zero mean and unit variance before training. The dictionaries were designed for a four times overcomplete representation, they had n := 256 atoms for samples with 64 pixels. After training, dictionary quality was\nmeasured using the 98 images of the \"Flowers\" collection from the same image database. This evaluation set was disjoint from the learning set.\nEach single image was subsequently divided into nonoverlapping blocks with 8\u00d7 8 pixels. All blocks were normalized to zero mean and unit variance, and then the optimized dictionaries were used to infer a sparse code word for each block. The resulting code word was then fed through a linear generative model using the currently investigated dictionary, and the mean value and variance were restored to match that of the original block.\nSparse code word inference was achieved with a projected Landweber procedure (Bredies and Lorenz 2008), essentially the same as projected gradient descent starting with the null vector. Using the sparseness-enforcing projection operator after each iteration, the code words were tuned to attain an inference sparseness \u03c3I , which need not necessarily be equal to the sparseness degree used for dictionary learning. The correlation coefficient was used as the similarity measure for inference to benefit from invariance to shifting and scaling. Automatic step size control was carried out with the bold driver heuristic (Bishop 1995). We note that due to the representation theorem on the sparseness projection this process can also be understood as Iterative SoftThresholding (Bredies and Lorenz 2008).\nA reproduced image is yielded by applying this procedure to all distinct blocks of the original image using a single dictionary. The deviation between this output image and the original image was assessed with the SSIM index (Wang and Bovik 2009), which yielded qualitatively similar results as the peak signal-to-noise ratio. The SSIM index is, however, normalized to values between zero and one, and\nit respects the local structure of images since it examines 11\u00d711 pixels neighborhoods through convolution. Because it measures the visual similarity between images, it is more intuitive than measures based on the pixel-wise squared error (Wang and Bovik 2009). This evaluation method yielded one number from the interval [0, 1] for each image and dictionary. Each parameterization for dictionary learning was used to train five dictionaries to compensate probabilistic effects, and here the mean of the resulting 490 SSIMs is reported."}, {"heading": "4.3.1 EZDL Results", "text": "Figure 10 visualizes the results obtained for dictionaries produced by Easy Dictionary Learning using dictionary sparseness degrees \u03c3H \u2208 {0.75, 0.85, 0.95, 0.99}. One thousand learning epochs were carried out, presenting 30 000 samples in each epoch, and using an initial step size of \u03b70 := 1. During dictionary learning, only the first trivial iteration of a Landweber procedure is carried out for sparse code word inference by computing h := \u03a0\u03c3H (W T x) for the ordinary inference model.\nWe first analyzed the impact of carrying out more Landweber iterations during the image reproduction phase, and the effect of varying the inference sparseness degree \u03c3I . A huge performance increase can be obtained by using more than one iteration for inference (Fig. 10a). Almost the optimal performance is achieved after ten iterations, and after one hundred iterations the method has converged. For \u03c3I = 0.75, the performance of dictionaries with \u03c3H = 0.75 and \u03c3H = 0.85 is about equal yielding the maximum SSIM of one. This value indicates that there is no visual difference between the reproduced images and the original images.\nWhen the inference sparseness \u03c3I is increased to 0.85, a difference in the choice of the dictionary sparseness \u03c3H becomes noticeable. For \u03c3H = 0.85, performance already degrades, and the degradation is substantial if \u03c3H = 0.75. It is more intuitive when \u03c3H and \u03c3I are put in relation. Almost no information is lost in the reproduction by enforcing a lower inference sparseness than dictionary sparseness (\u03c3I < \u03c3H ). Performance is worse if \u03c3I > \u03c3H which is plausible because the dictionary was not adapted for a higher sparseness degree. In the natural case \u03c3I = \u03c3H the performance mainly depends on their concrete value, where higher sparseness results in worse reproduction capabilities.\nTo further investigate this behavior, we set the number of Landweber iterations to 100 and varied \u03c3I smoothly in the interval [0.50, 1.0) for the four different dictionary sparseness degrees. The results of this experiment are depicted in Fig. 10b. For \u03c3I \u2264 0.80 there is hardly any difference in the reproduction quality irrespective of the value of \u03c3H . The difference when training dictionaries with different sparseness degrees first becomes visible for large values of \u03c3I . Performance is here better using dictionaries where very sparse code words were demanded during learning. Hence, tasks that require high code word sparseness should use dictionaries trained with high values of the dictionary sparseness."}, {"heading": "4.3.2 Comparison with Alternative Dictionary Learning Algorithms", "text": "For a comparison, we conducted experiments using the Online Dictionary Learning (ODL) algorithm of Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algorithm (RLS-DLA) by Skretting and Engan (2010). For inference of sparse code words, ODL minimizes the reproduction error under implicit sparseness constraints. RLS-\nDLA uses an external vector selection algorithm for inference, hence explicit constraints such as a target L0 pseudonorm can be demanded easily. Both algorithms update the dictionary after each sample presentation.\nODL does not require any step sizes to be adjusted. The crucial parameter for dictionary sparseness here is a number \u03bb \u2208 R>0, which controls the trade-off between reproduction capabilities and code word sparseness. We trained five dictionaries for each \u03bb \u2208 {0.50, 1.00, 1.50, 2.00} using n := 256 atoms and presenting 30 000 learning samples in each of one thousand epochs. Then, the same evaluation methodology as before with \u03c3I \u2208 [0.50, 1.0) was used to assess the reproduction capabilities. The results are shown in Fig. 11a. The choice of \u03bb is not as influential compared to \u03c3H in EZDL. There are only small performance differences for \u03c3I < 0.925, and for \u03c3I \u2265 0.925 hardly any difference can be observed. Similar to the EZDL experiments, large values of \u03bb result in better performance for large \u03c3I and worse performance for small \u03c3I .\nRLS-DLA provides a forgetting factor parameter which behaves analogously to the step size in gradient descent. We used the default forgetting factor schedule which interpolates between 0.99 and 1 using a cubic function over one thousand learning epochs. For inference, we chose an optimized Orthogonal Matching Pursuit variant (GharaviAlkhansari and Huang 1998) where a parameter \u03b6 \u2208 N controls the number of non-vanishing coordinates in the code words. We trained five dictionaries with n := 256 atoms for each \u03b6 \u2208 {4, 8, 16, 32}. Thirty thousand randomly drawn learning samples were presented in each learning epoch. The resulting reproduction performance is shown in Fig. 11b. Again, for large values of \u03c3I the dictionaries with the most sparse code words during learning perform best, that is those trained with small values of \u03b6 .\nTo compare all three dictionary learning algorithms independent of the concrete dictionary sparseness, we took the mean SSIM value that belonged to the best performing dictionaries for each feasible value of \u03c3I . This can also be interpreted as using the convex hull of the results shown in Fig. 10b and Fig. 11. This yielded one curve for each dictionary learning algorithm, depicted in Fig. 12. There is only a minor difference between the curves over the entire range of \u03c3I . It can hence be concluded that the algorithms learn dictionaries which are equally well-suited for the reproduction of entire images. Although EZDL uses a very simple learning rule, this is sufficient enough to achieve a performance competitive with that of two state-of-the-art dictionary learning algorithms."}, {"heading": "4.3.3 Comparison of Learning Speed", "text": "We moreover compared the learning speed of the methods and investigated the influence of the initial step size on the EZDL dictionaries. In doing so, we carried out reproduction experiments on entire images using dictionaries provided by the learning algorithms after certain numbers of learning epochs. In each of one thousand overall epochs, 30 000 learning samples where input to all three algorithms. The dictionary sparseness parameters were set to \u03c3H := 0.99 for EZDL, to \u03bb := 2.00 for Online Dictionary Learning, and to \u03b6 := 4 for the Recursive Least Squares Dictionary Learning Algorithm.\nTiming measurements on an Intel Core i7-4960X processor have shown that one EZDL learning epoch takes approximately 30 % less time than a learning epoch of ODL. RLS-DLA was roughly 25 times slower than EZDL. Although the employed vector selection method was present as part of an optimized software library, only a fairly unoptimized implementation of the actual learning algorithm was\navailable. Therefore, a comparison of this algorithm with regard to execution speed would be inequitable.\nThe inference sparseness was set to \u03c3I := 0.99 and one hundred Landweber iterations were carried out for sparse code word inference. The SSIM index was eventually used to assess the reproduction performance. Figure 13 visualizes the results of the learning speed analysis, averaged over all images from the evaluation set and five dictionaries trained for each parameterization to compensate probabilistic effects. Online Dictionary Learning is free of step sizes, dictionary performance increased instantly and constantly with each learning epoch. Only small performance gains could be observed after 100 learning epochs.\nThe performance of dictionaries trained with RLS-DLA was initially better than that of the ODL dictionaries, but improved more slowly. After a hundred epochs, however, it was possible to observe a significant performance gain. Although tweaking the forgetting factor schedule may have led to better early reproduction capabilities, RLS-DLA achieved a performance equivalent to that of ODL after 1000 epochs.\nIf EZDL\u2019s initial step size was set to unity, performance first degraded, but started to improve after the fifth epoch. After 100 epochs, the performance was identical to ODL\u2019s, and all three methods obtained equal reproduction capabilities after 1000 epochs. Reduction of the initial step size caused the dictionaries to perform better after few sample presentations. The quality of Online Dictionary Learning was achieved after 20 epochs for \u03b70 = 1/4, and for \u03b70 = 1/16 the EZDL dictionaries were always better in the mean until the 100th epoch. There is hardly any quality difference after 1000 epochs, a very small initial step size resulted however in a slightly worse performance.\nThe choice of the initial step size for EZDL is hence not very influential. Although \u03b70 = 1 caused small overfitting during the very first epochs, the dictionaries quickly recov-\nered so that no significant difference in reproduction capabilities could be observed after 100 epochs. Since an EZDL epoch is 30 % faster than an Online Dictionary Learning epoch, our proposed method produces better results earlier on an absolute time scale.\n4.4 Image Denoising Experiments\nWe have demonstrated in Sect. 4.3 that dictionaries learned with Easy Dictionary Learning are as good as those obtained from the Online Dictionary Learning algorithm of Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algorithm by Skretting and Engan (2010) in terms of the reproduction quality of entire images. In a final experiment, we investigated the suitability of EZDL dictionaries for image denoising using the image enhancement procedure proposed by Mairal et al. (2009b).\nThis method carries out semi-local block matching and finds sparse code words by imposing a group sparseness penalty on the Euclidean reproduction error. In doing so, a pre-learned dictionary is used which explains the appearance of uncorrupted images and further helps to resolve ambiguities if block matching fails to provide large enough groups. A linear generative model is finally used to find estimators of denoised image patches from the sparse code words. This procedure is quite robust if the input data is noisy, since sparseness provides a strong prior which well regularizes this ill-posed inverse problem (Kreutz-Delgado et al. 2003; Foucart and Rauhut 2013).\nThe denoising approach of Mairal et al. (2009b) also provides the possibility of dictionary adaptation while denoising concrete input data. We did not use this option as it would hinder resilient statements on a dictionary\u2019s eligibility if it would be modified with another dictionary learning algorithm during denoising.\nThe methodology of the experiment was as follows. We used the four-times overcomplete dictionaries trained on the 8\u00d7 8 pixels image patches from the \"Foliage\" collection of the McGill calibrated color image database (Olmos and Kingdom 2004) as models for uncorrupted images. The dictionary sparseness parameters were \u03c3H := 0.99 for EZDL, \u03bb := 2.00 for ODL and \u03b6 := 4 for RLS-DLA. For evaluation, we used the 81 images of the \"Animals\" collection from the McGill database, and converted them to 8-bit grayscales as in the previous experiments. The images were synthetically corrupted with additive white Gaussian noise. Five noisy images were generated for each original image and each standard deviation from {2.5, 5.0, . . . , 47.5, 50.0}.\nThese images were then denoised using the candidate dictionaries, where the window size for block matching was set to 32 pixels. Then, the similarity between the reconstructions and the original images was measured with the\npeak signal-to-noise ratio. We also evaluated the SSIM index, which led to qualitatively similar results.\nThe results are depicted in Fig. 14. Denoising performance degrades if the noise strength is increased. There is hardly any difference between the dictionaries trained with the three algorithms. The RLS-DLA and EZDL dictionaries perform slightly better for small synthetic noise levels, but this improvement is visually imperceptible. This result is not surprising, since Sect. 4.3 demonstrated that all three learning algorithms produce dictionaries equally well-suited for the reproduction of entire images.\nThe denoising procedure of Mairal et al. (2009b) aims at reproduction capabilities as well, with the modification of employing noisy samples as input. Image enhancement and compression applications such as those proposed by Yang et al. (2010, 2012), Dong et al. (2011), Skretting and Engan (2011) and Horev et al. (2012) which also use problem formulations based on the reproduction error can hence be expected to benefit from more efficiently learned dictionaries as well."}, {"heading": "5 Conclusions", "text": "This paper proposed the EZDL algorithm which features explicit sparseness constraints with respect to Hoyer\u2019s smooth sparseness measure \u03c3 . Pre-defined sparseness degrees are ensured to always be attained using a sparseness-enforcing projection operator. Building upon a succinct representation of the projection, we proved that the projection problem can be formulated as a root-finding problem. We presented a linear time and constant space algorithm for the projection which is superior to previous approaches in terms of theoretical computational complexity and execution time on real computing machines.\nEZDL adapts dictionaries to measurement data through simple rank-1 updates. The sparseness projection serves as\nfoundation for sparse code word inference. Due to the projection efficiency and since no complicated gradients are required, our proposed learning algorithm is significantly faster than even the optimized ODL algorithm. Topographic atom organization and atom sparseness can be realized with very simple extensions, allowing for versatile sparse representations of data sets. Its simplicity and efficiency does not hinder EZDL from producing dictionaries competitive with those generated by ODL and RLS-DLA in terms of the reproduction and denoising performance on natural images. Alternative image processing methods based on sparse representations rely on dictionaries subject to the same criteria, and can thus be expected to benefit from EZDL\u2019s advantages as well.\nAcknowledgements The authors are grateful to Heiko Neumann, Florian Sch\u00fcle, and Michael Gabb for helpful discussions. We would like to thank Julien Mairal and Karl Skretting for making implementations of their algorithms available. Parts of this work were performed on the computational resource bwUniCluster funded by the Ministry of Science, Research and Arts and the Universities of the State of BadenW\u00fcrttemberg, Germany, within the framework program bwHPC. This work was supported by Daimler AG, Germany."}, {"heading": "Appendix: Technical Details and Proofs for Section 2", "text": "This appendix studies the algorithmic computation of Euclidean projections onto level sets of Hoyer\u2019s \u03c3 in greater detail, and in particular proves the correctness of the algorithm proposed in Sect. 2.\nFor a non-empty subset M\u2286Rn of the Euclidean space and a point x \u2208 Rn, we call\nprojM(x) := {y \u2208M | \u2016y\u2212 x\u20162 = infz\u2208M \u2016z\u2212 x\u20162 }\nthe set of Euclidean projections of x onto M (Deutsch 2001). Since we only consider situations in which projM(x) = {y} is a singleton, we may also write y = projM(x).\nWithout loss of generality, we can compute projT (x) for a vector x \u2208 Rn\u22650 within the non-negative orthant instead of projS(x) for an arbitrary point x \u2208 Rn to yield sparseness-enforcing projections, where T and S are as defined in Sect. 2. First, the actual scale is irrelevant as we can simply re-scale the result of the projection (Thom and Palm 2013, Remark 5). Second, the constraint that the projection lies in the non-negative orthant Rn\u22650 can easily be handled by flipping the signs of certain coordinates (Thom and Palm 2013, Lemma 11). Finally, all entries of x can be assumed non-negative with Corollary 19 from Thom and Palm (2013).\nWe note that T is non-convex because of the \u2016s\u20162 = \u03bb2 constraint. Moreover, T 6= /0 for all target sparseness degrees \u03c3\u2217 \u2208 (0, 1) which we show here by construction (see also Remark 18 in Thom and Palm (2013) for further details): Let \u03c8 := ( \u03bb1\u2212 \u221a n\u03bb 22\u2212\u03bb 2 1/ \u221a n\u22121 ) /n > 0 and \u03c9 := \u03bb1\u2212 (n\u22121)\u03c8 > 0, then the point q := \u2211n\u22121i=1 \u03c8ei +\u03c9en \u2208 Rn lies in T , where ei \u2208Rn denotes the i-th canonical basis vector. Since all coordinates of q are positive, T always contains points with an L0 pseudonorm of n. If we had used the L0 pseudo-norm to measure sparseness, then q would have the same sparseness degree as, for example, the vector with all entries equal to unity. If, however, \u03c3\u2217 is close to one, then there is only one large value \u03c9 in q and all the other entries equaling \u03c8 are very small but positive. This simple example demonstrates that in situations where the presence of noise cannot be eliminated, Hoyer\u2019s \u03c3 is a much more robust sparseness measure than the L0 pseudo-norm.\nRepresentation Theorem\nBefore proving a theorem on the characterization of the projection onto T , we first fix some notation. As above, let ei \u2208 Rn denote the i-th canonical basis vector and let furthermore e := \u2211ni=1 ei \u2208 Rn be the vector where all entries are one. We note that if a point x resides in the non-negative orthant, then \u2016x\u20161 = eT x. Subscripts to vectors denote the corresponding coordinate, except for e and ei. For example, we have that xi = eTi x. We abbreviate \u03be \u2208 R\u22650 with \u03be \u2265 0 when it is clear that \u03be is a real number. When I \u2286 {1, . . . ,n} is an index set with d := |I| elements, say i1 < \u00b7 \u00b7 \u00b7< id , then the unique matrix VI \u2208 {0,1}d\u00d7n with VIx = ( xi1 , . . . , xid\n)T \u2208Rd for all x \u2208Rn is called the slicing operator. A useful relation between the L0 pseudo-norm, the Manhattan norm and the Euclidean norm is \u2016x\u20162 \u2264 \u2016x\u20161 \u2264 \u2016x\u2016 1/2 0 \u2016x\u20162 \u2264 \u221a n\u2016x\u20162 for all points x \u2208 Rn. We are now in a position to formalize the representation theorem:\nTheorem 1 Let x \u2208 Rn\u22650 \\T and p := projT (x) be unique. Then there is exactly one number \u03b1\u2217 \u2208 R such that\np = \u03b2 \u2217 \u00b7max(x\u2212\u03b1\u2217 \u00b7 e, 0) ,\nwhere \u03b2 \u2217 := \u03bb2/\u2016max(x\u2212\u03b1\u2217\u00b7e, 0)\u20162 > 0 is a scaling constant. Moreover, if I := { i \u2208 {1, . . . ,n} | pi > 0}= {i1, . . . , id}, d := |I| and i1 < \u00b7 \u00b7 \u00b7< id , denotes the set of the d coordinates in which p does not vanish, then\n\u03b1\u2217 = 1 d\n\u2016VIx\u20161\u2212\u03bb1 \u221a d \u2016VIx\u201622\u2212\u2016VIx\u2016 2 1\nd\u03bb 22 \u2212\u03bb 21  . Proof It is possible to prove this claim either constructively or implicitly, where both variants differ in whether the set I of all positive coordinates in the projection can be computed from x or must be assumed to be known. We first present a constructive proof based on a geometric analysis conducted in Thom and Palm (2013), which contributes to deepening our insight into the involved computations. As an alternative, we also provide a rigorous proof using the method of Lagrange multipliers which greatly enhances the unproven analysis of Potluru et al. (2013, Section 3.1).\nWe first note that when there are \u03b1\u2217 \u2208 R and \u03b2 \u2217 > 0 so that we have p= \u03b2 \u2217 \u00b7max(x\u2212\u03b1\u2217 \u00b7 e, 0), then \u03b2 \u2217 is determined already through \u03b1\u2217 because it holds that \u03bb2 = \u2016p\u20162 = \u03b2 \u2217 \u00b7 \u2016max(x\u2212\u03b1\u2217 \u00b7 e, 0)\u20162. We now show that the claimed representation is unique, and then present the two different proofs for the existence of the representation.\nUniqueness: It is d \u2265 2, since d = 0 would violate that \u2016p\u20161 > 0 and d = 1 is impossible because \u03c3\u2217 6= 1. We first show that there are two distinct indices i, j \u2208 I with pi 6= p j . Assume this was not the case, then pi =: \u03b3 , say, for all i \u2208 I. Let j := argmini\u2208I xi be the index of the smallest coordinate of x which has its index in I. Let \u03c8 := ( \u03bb1 \u2212 \u221a d\u03bb 22\u2212\u03bb 2 1/ \u221a d\u22121 ) /d \u2208 R and \u03c9 := \u03bb1 \u2212 (d \u2212 1)\u03c8 \u2208 R be numbers and define s := \u2211i\u2208I\\{ j}\u03c8ei +\u03c9e j \u2208 Rn. Then s \u2208 T like in the argument where we have shown that T is non-empty. Because of \u2016p\u20161 = \u2016s\u20161 and \u2016p\u20162 = \u2016s\u20162, it follows that \u2016x\u2212 p\u2016 2 2 \u2212\u2016x\u2212 s\u2016 2 2 = 2xT (s\u2212 p) = 2\u2211i\u2208I\\{ j} xi(\u03c8\u2212\u03b3)+2x j(\u03c9\u2212\u03b3)\u2265 2x j((d\u22121)\u03c8 +\u03c9\u2212 d\u03b3) = 2x j (\u2016s\u20161\u2212\u2016p\u20161) = 0. Hence s is at least as good an approximation to x as p, violating the uniqueness of p. Therefore, it is impossible that the set { pi | i \u2208 I } is a singleton.\nNow let i, j \u2208 I with pi 6= p j and \u03b1\u22171 ,\u03b1\u22172 ,\u03b2 \u22171 ,\u03b2 \u22172 \u2208 R such that p = \u03b2 \u22171 \u00b7max(x\u2212\u03b1\u22171 \u00b7 e, 0) = \u03b2 \u22172 \u00b7max(x\u2212\u03b1\u22172 \u00b7 e, 0). Clearly \u03b2 \u22171 6= 0 and \u03b2 \u22172 6= 0 as d 6= 0. It is 0 6= pi\u2212 p j = \u03b2 \u22171 (xi\u2212\u03b1\u22171 )\u2212\u03b2 \u22171 (x j\u2212\u03b1\u22171 ) = \u03b2 \u22171 (xi \u2212 x j), thus xi 6= x j holds. Moreover, 0 = pi \u2212 p j + p j \u2212 pi = (\u03b2 \u22171 \u2212\u03b2 \u22172 )(xi\u2212x j), hence \u03b2 \u22171 = \u03b2 \u22172 . Finally, we have that 0 = pi\u2212 pi = \u03b2 \u22171 (xi\u2212\u03b1\u22171 )\u2212\u03b2 \u22172 (xi\u2212\u03b1\u22172 ) = \u03b2 \u22171 (\u03b1\u22172 \u2212\u03b1\u22171 ), which yields \u03b1\u22171 = \u03b1\u22172 , and hence the representation is unique.\nExistence (constructive): Let H := {a \u2208 Rn | eT a = \u03bb1 } be the hyperplane on which all points in the non-negative orthant have an L1\nnorm of \u03bb1 and let C := Rn\u22650 \u2229H be a scaled canonical simplex. Further, let L := {q \u2208 H | \u2016q\u20162 = \u03bb2 } be a circle on H, and for an arbitrary index set I \u2286 {1, . . . ,n} let LI := {a \u2208 L | ai = 0 for all i 6\u2208 I } be a subset of L where all coordinates not indexed by I vanish. With Theorem 2 and Appendix D from Thom and Palm (2013) there exists a finite sequence of index sets I1, . . . , Ih \u2286 {1, . . . ,n} with I j ) I j+1 for j \u2208 {1, . . . ,h\u22121} such that projT (x) is the result of the finite sequence\nr(0) := projH(x), s(0) := projL(r(0)), r(1) := projC(s(0)), s(1) := projLI1(r(1)), . . . r( j) := projC(s( j\u22121)), s( j) := projLI j(r( j)), . . .\nr(h) := projC(s(h\u22121)), s(h) := projLIh(r(h)) = p.\nAll intermediate projections yield unique results because p was restricted to be unique. The index sets contain the indices of the entries that survive the projections onto C, I j := { i \u2208 {1, . . . ,n} | ri( j) 6= 0} for j \u2208 {1, . . . ,h}. In other words, p can be computed from x by alternating projections, where the sets L and LI j are non-convex for all j \u2208 {1, . . . ,h}. The expressions for the individual projections are given in Lemma 13, Lemma 17, Proposition 24, and Lemma 30, respectively, in Thom and Palm (2013).\nLet I0 := {1, . . . ,n} for completeness, then we can define the following constants for j \u2208 {0, . . . ,h}. Let d j := \u2223\u2223I j\u2223\u2223 be the number of relevant coordinates in each iteration, and let\n\u03b2 j :=\n\u221a d j\u03bb 22 \u2212\u03bb 21\nd j\u2016VI j x\u201622\u2212\u2016VI j x\u201621 and \u03b1 j := 1 d j\n( \u2016VI j x\u20161\u2212 \u03bb1 \u03b2 j ) be real numbers. We have that d j\u03bb 22 \u2212\u03bb 21 \u2265 dh\u03bb 22 \u2212\u03bb 21 \u2265 0 by construction which implies \u03b2 j > 0 for all j \u2208 {0, . . . ,h}. We now claim that the following holds:\n(a) si( j) = \u03b2 j \u00b7 (xi\u2212\u03b1 j) for all i \u2208 I j for all j \u2208 {0, . . . ,h}. (b) \u03b10 \u2264 \u00b7\u00b7 \u00b7 \u2264 \u03b1h and \u03b20 \u2264 \u00b7\u00b7 \u00b7 \u2264 \u03b2h. (c) xi \u2264 \u03b1 j for all i 6\u2208 I j for all j \u2208 {0, . . . ,h}. (d) p = \u03b2h \u00b7max(x\u2212\u03b1h \u00b7 e, 0).\nWe start by showing (a) with induction. For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 22 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u201622. We see that \u2016r(0)\u2212m\u201622 = \u2016x\u2212 \u2016x\u20161/n \u00b7 e\u201622 = \u2016x\u201622\u2212 \u2016x\u2016 2 1/n and therefore \u03b4 = \u03b20, and thus s(0) = \u03b20 \u00b7(x\u2212 1/n \u00b7 (\u2016x\u20161\u2212 \u03bb1/\u03b20)e), so the claim holds for the base case.\nSuppose that (a) holds for j and we want to show it also holds for j+ 1. It is r( j+ 1) = projC(s( j)) by definition, and Proposition 31 in Thom and Palm (2013) implies r( j + 1) = max(s( j)\u2212 t\u0302 \u00b7 e, 0) where t\u0302 \u2208 R can be expressed explicitly as t\u0302 = 1/d j+1 \u00b7 ( \u2211i\u2208I j+1 si( j)\u2212 \u03bb1 ) , which is the mean value of the entries that survive the simplex projection up to an additive constant. We note that t\u0302 is here always nonnegative, see Lemma 28(a) in Thom and Palm (2013), which we will need to show (b). Since I j+1 ( I j we yield si( j) = \u03b2 j \u00b7 (xi \u2212\u03b1 j) for all i \u2208 I j+1 with the induction hypothesis, and therefore we have that t\u0302 = 1/d j+1 \u00b7 ( \u03b2 j\u2016VI j+1 x\u20161\u2212d j+1\u03b2 j\u03b1 j\u2212\u03bb1 ) . We find that ri( j+1)> 0 for i \u2208 I j+1 by definition, and we can omit the rectifier so that ri( j+1) = si( j)\u2212 t\u0302. Using the induction hypothesis and the expression for t\u0302 we have ri( j+1) = \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 + \u03bb1/d j+1. For projecting onto LI j+1 , the distance between r( j + 1) and mI j+1 = \u03bb1/d j+1 \u00b7\u2211i\u2208I j+1 ei is required for computation of \u03b4 2 = ( \u03bb 22 \u2212 \u03bb 2 1/d j+1 )/ \u2016r( j+1)\u2212mI j+1\u201622, so that Lemma 30 from Thom and Palm (2013) can be applied. We have that \u2016r( j + 1)\u2212mI j+1\u201622 = \u2211i\u2208I j+1 ( \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 ) 2 =\n\u03b2 2j \u00b7 ( \u2016VI j+1 x\u201622\u2212\u2016VI j+1 x\u2016 2 1/d j+1 ) , and further \u03b4 = \u03b2 j+1/\u03b2 j. Now let i\u2208 I j+1 be an index, then we have si( j+ 1) = \u03b4 ri( j+ 1)+ (1\u2212 \u03b4 ) \u00b7 \u03bb1/d j+1 = \u03b2 j+1 \u00b7 ( xi\u2212 1/d j+1 \u00b7 ( \u2016VI j+1 x\u20161\u2212 \u03bb1/\u03b2 j+1 )) using Lemma 30 from Thom and Palm (2013). Therefore (a) holds for all j \u2208 {0, . . . ,h}.\nLet us now turn to (b). From the last paragraph, we know that \u03b4 = \u03b2 j+1/\u03b2 j for all j \u2208 {0, . . . ,h\u22121} for the projections onto LI j+1 . On the other hand, we have that \u2016r( j+1)\u2212mI j+1\u201622 = \u2016r( j+1)\u201622\u2212\u03bb 2 1/d j+1 from the proof of Lemma 30(a) from Thom and Palm (2013), and \u2016r( j+ 1)\u201622 \u2264 \u03bb 22 holds from the proof of Lemma 28(f) in Thom and Palm (2013), so \u03b4 \u2265 1 which implies \u03b20\u2264 \u00b7\u00b7 \u00b7 \u2264 \u03b2h. As noted above, the separator for projecting onto C satisfies t\u0302 \u2265 0 for all j \u2208 {0, . . . ,h\u22121}. By rearranging this inequality and using \u03b2 j \u2264 \u03b2 j+1, we conclude that \u03b1 j \u2264 1/d j+1 \u00b7 ( \u2016VI j+1 x\u20161\u2212 \u03bb1/\u03b2 j ) \u2264 \u03b1 j+1, hence \u03b10 \u2264 \u00b7\u00b7 \u00b7 \u2264 \u03b1h.\nFor (c) we want to show that the coordinates in the original vector x which will vanish in some iteration when projecting onto C are already small. The base case j = 0 of an induction for j is trivial since the complement of I0 is empty. In the induction step, we note that the complement of I j+1 can be partitioned into ICj+1 = I C j \u222a ( I j \u2229 ICj+1\n) since I j+1 ( I j . For i \u2208 ICj we already know that xi \u2264 \u03b1 j \u2264 \u03b1 j+1 by the induction hypothesis and (b). We have shown in (a) that si( j) = \u03b2 j \u00b7 (xi\u2212\u03b1 j) for all i \u2208 I j , and if i \u2208 I j \\ I j+1 then si( j) \u2264 t\u0302 since 0 = ri( j + 1) = max(si( j)\u2212 t\u0302, 0). By substituting the explicit expression for t\u0302 and solving for xi we yield xi \u2264 1/d j+1 \u00b7 ( \u2016VI j+1 x\u20161\u2212\u03bb1/\u03b2 j ) \u2264 \u03b1 j+1, and hence the claim holds for all i 6\u2208 I j+1. If we can now show that (d) holds, then the claim of the theorem follows by setting \u03b1\u2217 := \u03b1h and \u03b2 \u2217 := \u03b2h. We note that by construction p = s(h) and si(h) \u2265 0 for all coordinates i \u2208 {1, . . . ,n}. When i \u2208 Ih, then si(h) = \u03b2h \u00b7(xi\u2212\u03b1h) with (a), which is positive by requirement, so when the rectifier is applied nothing changes. If i 6\u2208 Ih then xi\u2212\u03b1h \u2264 0 by (c), and indeed \u03b2h \u00b7max(xi\u2212\u03b1h, 0) = 0 = si(h). The expression therefore holds for all i \u2208 {1, . . . ,n}, which completes the constructive proof of existence.\nExistence (implicit): Existence of the projection is guaranteed by the Weierstra\u00df extreme value theorem since T is compact. Now let f : Rn \u2192 R, s 7\u2192 \u2016s\u2212 x\u201622, be the objective function, and let the constraints be represented by the functions h1 : Rn \u2192 R, s 7\u2192 eT s\u2212 \u03bb1, h2 : Rn\u2192 R, s 7\u2192 \u2016s\u201622\u2212\u03bb 22 , and gi : Rn\u2192 R, s 7\u2192 \u2212si, for all indices i \u2208 {1, . . . ,n}. All these functions are continuously differentiable. If p = projT (x), then p is a local minimum of f subject to h1(p) = 0, h2(p) = 0 and g1(p)\u2264 0, . . . , gn(p)\u2264 0.\nFor application of the method of Lagrange multipliers we first have to show that p is regular, which means that the gradients of h1, h2 and gi for i 6\u2208 I evaluated in p must be linearly independent (Bertsekas 1999, Section 3.3.1). Let J := IC = { j1, . . . , jn\u2212d}, say, then |J| \u2264 n\u22122 since d \u2265 2. Hence we have at most n vectors from Rn for which we have to show linear independence. Clearly h\u20321(s) = e, h \u2032 2(s) = 2s and g\u2032i(s) = \u2212ei for all i \u2208 {1, . . . ,n}. Now let u1,u2 \u2208 R and v \u2208 Rn\u2212d with u1e+2u2 p\u2212\u2211n\u2212d\u00b5=1 v\u00b5 e j\u00b5 = 0 \u2208 Rn. Then, let \u00b5 \u2208 {1, . . . ,n\u2212d}, then p j\u00b5 = 0 by definition of I and hence by pre-multiplication of the equation above with eTj\u00b5 we yield u1\u2212 v\u00b5 = 0 \u2208 R. Therefore u1 = v\u00b5 for all \u00b5 \u2208 {1, . . . ,n\u2212 d}. On the other hand, if i \u2208 I then pi > 0 and eTi e j\u00b5 = 0 for all \u00b5 \u2208 {1, . . . ,n\u2212 d}. Hence u1 + 2u2 pi = 0 \u2208 R for all i \u2208 I. In the first paragraph of the uniqueness proof we have shown there are two distinct indices i, j \u2208 I with pi 6= p j . Since u1 +2u2 pi = 0 = u1 +2u2 p j and thus 0 = 2u2(pi\u2212 p j) we can conclude that u2 = 0, which implies u1 = 0. Then v1 = \u00b7 \u00b7 \u00b7= vn\u2212d = 0 as well, which shows that p is regular.\nThe Lagrangian is L : Rn \u00d7R\u00d7R\u00d7Rn \u2192 R, (s, \u03b1, \u03b2 , \u03b3) 7\u2192 f (s)+\u03b1h1(s)+\u03b2h2(s)+\u2211ni=1 \u03b3igi(s), and its derivative with respect to its first argument s is given by L\u2032(s, \u03b1, \u03b2 , \u03b3) := \u2202/\u2202 s L(s, \u03b1, \u03b2 , \u03b3) = 2(s\u2212x)+\u03b1 \u00b7e+2\u03b2 \u00b7s\u2212\u03b3 \u2208Rn. Now, Proposition 3.3.1 from Bertsekas (1999) guarantees the existence of Lagrange multipliers \u03b1\u0303, \u03b2\u0303 \u2208 R and \u03b3\u0303 \u2208Rn with L\u2032(p, \u03b1\u0303, \u03b2\u0303 , \u03b3\u0303) = 0, \u03b3\u0303i \u2265 0 for all i \u2208 {1, . . . ,n} and \u03b3\u0303i = 0 for i \u2208 I. Assume \u03b2\u0303 = \u22121, then 2x = \u03b1\u0303 \u00b7 e\u2212 \u03b3\u0303 since the derivative of L must vanish. Hence xi = \u03b1\u0303/2 for all i \u2208 I, and therefore { pi | i \u2208 I } is a singleton with Remark 10 from Thom and Palm (2013) as p was assumed unique and T is permutation-invariant. We have seen earlier that this is absurd, so \u03b2\u0303 6=\u22121 must hold.\nWrite \u03b1\u2217 := \u03b1\u0303/2, \u03b2 \u2217 := 1/(\u03b2\u0303+1) and \u03b3\u2217 := \u03b3\u0303/2 for notational convenience. We then obtain p = \u03b2 \u2217(x\u2212 \u03b1\u2217 \u00b7 e + \u03b3\u2217) because L\u2032 vanishes. Then h1(p) = 0 implies that \u03bb1 = \u2211i\u2208I pi = \u2211i\u2208I \u03b2 \u2217(xi\u2212\u03b1\u2217) = \u03b2 \u2217(\u2016VIx\u20161 \u2212 d\u03b1\u2217), and with h2(p) = 0 follows that \u03bb 22 = \u2211i\u2208I p2i = (\u03b2 \u2217)2 \u00b7 (\u2016VIx\u201622\u22122\u03b1\u2217\u2016VIx\u20161 +d \u00b7 (\u03b1\u2217)2). By taking the ratio \u03bb 2 1/\u03bb 22 and after elementary algebraic transformations we arrive at the quadratic equation a \u00b7 (\u03b1\u2217)2 + b \u00b7\u03b1\u2217 + c = 0, where a := d \u00b7 ( d\u2212 \u03bb 21/\u03bb 22 ) , b :=\n2\u2016VIx\u20161 \u00b7 ( \u03bb 21/\u03bb 22 \u2212d ) and c := \u2016VIx\u201621\u2212\u2016VIx\u201622 \u00b7\u03bb 2 1/\u03bb 22 are reals. The dis-\ncriminant is \u2206 := b2\u22124ac= 4 \u00b7\u03bb 21/\u03bb 22 \u00b7 ( d\u2212 \u03bb 21/\u03bb 22 ) \u00b7 ( d\u2016VIx\u201622\u2212\u2016VIx\u201621 ) . Since VIx\u2208Rd we have that d\u2016VIx\u201622\u2212\u2016VIx\u201621\u2265 0. Moreover, the number d is not arbitrary. As p exists by the Weierstra\u00df extreme value theorem with \u2016p\u20160 = d, \u2016p\u20161 = \u03bb1 and \u2016p\u20162 = \u03bb2, we find that \u03bb1 \u2264 \u221a d\u03bb2 and hence \u2206 \u2265 0, so there must be a real solution to the equation above. Solving the equation shows that\n\u03b1\u2217 \u2208 { 1 d ( \u2016VIx\u20161\u00b1\u03bb1 \u221a d\u2016VIx\u201622\u2212\u2016VIx\u201621\nd\u03bb 22 \u2212\u03bb 21\n)} ,\nhence from h1(p) = 0 we obtain\n\u03b2 \u2217 \u2208 { \u2213 \u221a d\u03bb 22 \u2212\u03bb 21 /\u221a d\u2016VIx\u201622\u2212\u2016VIx\u201621 } .\nSuppose \u03b1\u2217 is the number that arises from the \"+\" before the square root, then \u03b2 \u2217 is the number with the \"\u2212\" sign, thus \u03b2 \u2217 < 0. We have seen earlier that there are two distinct indices i, j \u2208 I with pi 6= p j . We can assume pi > p j , then 0 < pi\u2212 p j = \u03b2 \u2217(xi\u2212 x j) which implies that xi < x j . This is not possible as it violates the order-preservation property of projections onto permutation-invariant sets (Lemma 9(a) from Thom and Palm 2013). Thus our choice of \u03b1\u2217 was not correct in the first place, and \u03b1\u2217 must be as stated in the claim.\nIt remains to be shown that p is the result of a soft-shrinkage function. If i \u2208 I, then 0 < pi = \u03b2 \u2217(xi\u2212\u03b1\u2217), and \u03b2 \u2217 > 0 shows xi > \u03b1\u2217 such that pi = \u03b2 \u2217 \u00b7max(xi\u2212\u03b1\u2217, 0). When i 6\u2208 I, we have 0 = pi = \u03b2 \u2217(xi \u2212 \u03b1\u2217 + \u03b3\u2217i ) where \u03b3\u2217i \u2265 0 and still \u03b2 \u2217 > 0, thus xi \u2264 \u03b1\u2217 and pi = \u03b2 \u2217 \u00b7max(xi\u2212\u03b1\u2217, 0) holds. Therefore, the representation holds for all entries. ut\nFinding the set I containing the indices of the positive coordinates of the projection result is the key for algorithmic computation of the projection. Based on the constructive proof this could, for example, be achieved by carrying out alternating projections whose run-time complexity is between quasi-linear and quadratic in the problem dimensionality n and whose space complexity is linear. An alternative is the method proposed by Potluru et al. (2013), where the input vector is sorted and then each possible candidate for I is checked. Due to the sorting, I must be of the form I = {1, . . . ,d}, where now only d is unknown (see also the proof of Theorem 3 from Thom and Palm 2013). Here, the run-time complexity is quasi-linear and the space complexity is linear in the problem dimensionality since also the sorting permutation has to be stored. When n gets large, algorithms with a smaller computational complexity are mandatory.\nProperties of the Auxiliary Function\nWe have already informally introduced the auxiliary function in Sect. 2.2. Here is a satisfactory definition:\nDefinition 2 Let x \u2208 Rn\u22650 \\T be a point such that projT (x) is unique and \u03c3(x)< \u03c3\u2217. Let xmax denote the maximum entry of x, then\n\u03a8 : [0, xmax)\u2192 R, \u03b1 7\u2192 \u2016max(x\u2212\u03b1 \u00b7 e, 0)\u20161 \u2016max(x\u2212\u03b1 \u00b7 e, 0)\u20162 \u2212 \u03bb1 \u03bb2 ,\nis called auxiliary function for the projection onto T .\nWe call \u03a8 well-defined if all requirements from the definition are met. Note that the situation where \u03c3(x)\u2265 \u03c3\u2217 is trivial, because in this sparseness-decreasing setup we have that all coordinates of the projection must be positive. Hence I = {1, . . . ,n} in Theorem 1, and \u03b1\u2217 can be computed with the there provided formula.\nWe need more notation to describe the properties of \u03a8 . Let x \u2208Rn be a point. We write X := {xi | i \u2208 {1, . . . ,n}} for the set that contains the entries of x. Let xmin := minX be short for the smallest entry of x, and xmax := maxX and x2nd-max := maxX \\{xmax } denote the two largest entries of x. Further, q : R\u2192 Rn, \u03b1 7\u2192 max(x\u2212\u03b1 \u00b7 e, 0), denotes the curve that evolves from application of the soft-shrinkage function to x. The Manhattan norm and Euclidean norm of points from q is given by `1 : R\u2192R, \u03b1 7\u2192 \u2016q(\u03b1)\u20161, and `2 : R\u2192R, \u03b1 7\u2192 \u2016q(\u03b1)\u20162, respectively. Therefore, \u03a8 = `1/`2\u2212 \u03bb1/\u03bb2 and \u03a8\u0303 from Sect. 4.1 can be written as `21/`22\u2212\u03bb 2 1/\u03bb 22 , such that its derivative can be expressed in terms of \u03a8 \u2032 using the chain rule. The next result provides statements on the auxiliary function\u2019s analytical nature and links its zero with the projection onto T :\nLemma 3 Let x \u2208Rn\u22650 \\T be given such that the auxiliary function \u03a8 is well-defined. Then the following holds:\n(a) \u03a8 is continuous on [0, xmax). (b) \u03a8 is differentiable on [0, xmax)\\X . (c) \u03a8 is strictly decreasing on [0, x2nd-max), and on [x2nd-max, xmax) it\nis constant. (d) There is exactly one \u03b1\u2217 \u2208 (0, x2nd-max) with \u03a8(\u03b1\u2217) = 0. It is then\nprojT (x) = (\u03bb2/`2(\u03b1\u2217)) \u00b7q(\u03b1\u2217).\nProof In addition to the original claim, we also give explicit expressions for the derivative of \u03a8 and higher derivatives thereof in part (c). These are necessary to show that \u03a8 is strictly decreasing and constant, respectively, on the claimed intervals and for the explicit implementation of Algorithm 2.\n(a) The function q is continuous because so is the soft-shrinkage function. Hence `1, `2 and \u03a8 are continuous as compositions of continuous functions.\n(b) The soft-shrinkage function is differentiable everywhere except at its offset. Therefore, \u03a8 is differentiable everywhere except for when its argument coincides with an entry of x, that is on [0, xmax)\\X .\n(c) We start with deducing the first and second derivative of \u03a8 . Let x j,xk \u2208X \u222a{0}, x j < xk, such that there is no element from X between them. We here allow x j = 0 and xk = xmin when 0 6\u2208X for completeness. Then the index set I := { i \u2208 {1, . . . ,n} | xi > \u03b1 } of nonvanishing coordinates in q is constant for \u03b1 \u2208 (x j, xk), and the derivative of \u03a8 can be computed using a closed-form expression. For this, let d := |I| denote the number of non-vanishing coordinates in q on that interval. With `1(\u03b1) = \u2211i\u2208I (xi\u2212\u03b1) = \u2211i\u2208I xi\u2212d\u03b1 we obtain `\u20321(\u03b1) = \u2212d. Analogously, it is \u2202/\u2202\u03b1 `2(\u03b1)2 = \u2202/\u2202\u03b1 \u2211i\u2208I (xi\u2212\u03b1)2 = \u22122`1(\u03b1), and the chain rule yields `\u20322(\u03b1) = \u2202 \u2202\u03b1 \u221a `2(\u03b1)2 = \u2212`1(\u03b1)/`2(\u03b1). Applica-\ntion of the quotient rule gives \u03a8 \u2032(\u03b1) = ( `1(\u03b1)2/`2(\u03b1)2\u2212d ) /`2(\u03b1). The second derivative is of similar form. We find \u2202/\u2202\u03b1 `1(\u03b1)2 =\u22122d`1(\u03b1), and hence \u2202/\u2202\u03b1 ( `1(\u03b1)2/`2(\u03b1)2 ) = 2(`1(\u03b1)/`2(\u03b1)2) \u00b7 ( `1(\u03b1)2/`2(\u03b1)2\u2212d ) . We have \u2202/\u2202\u03b1 (1/`2(\u03b1)) = `1(\u03b1)/`2(\u03b1)3 and with the product rule we see that \u03a8 \u2032\u2032(\u03b1) = 3(`1(\u03b1)/`2(\u03b1)3) \u00b7 ( `1(\u03b1)2/`2(\u03b1)2\u2212d ) = 3\u03a8 \u2032(\u03b1) \u00b7 `1(\u03b1)/`2(\u03b1)2.\nFirst let \u03b1 \u2208 (x2nd-max, xmax). It is then d = 1, that is q has exactly one non-vanishing coordinate. In this situation we find `1(\u03b1) = `2(\u03b1) and \u03a8 \u2032 \u2261 0 on (x2nd-max, xmax), thus \u03a8 is constant on (x2nd-max, xmax) as a consequence of the mean value theorem from real analysis. Because \u03a8 is continuous, it is constant even on [x2nd-max, xmax).\nNext let \u03b1 \u2208 [0, x2nd-max) \\X , and let x j , xk, I and d as in the first paragraph. We have d \u2265 2 since \u03b1 < x2nd-max. It is furthermore `1(\u03b1)\u2264 \u221a d`2(\u03b1) as d = \u2016q(\u03b1)\u20160. This inequality is in fact strict, because q(\u03b1) has at least two distinct nonzero entries. Hence \u03a8 \u2032 is negative on the interval (x j, xk), and the mean value theorem guarantees that \u03a8 is strictly decreasing on this interval. This property holds for the entire interval [0, x2nd-max) due to the continuity of \u03a8 .\n(d) The requirement \u03c3(x) < \u03c3\u2217 implies \u2016x\u20161/\u2016x\u20162 > \u03bb1/\u03bb2 and thus \u03a8(0)> 0. Let \u03b1 \u2208 (x2nd-max, xmax) be arbitrary, then `1(\u03b1) = `2(\u03b1) as in (c), and hence \u03a8(\u03b1)< 0 since \u03bb2 < \u03bb1 must hold. The existence of \u03b1\u2217 \u2208 [0, x2nd-max) with \u03a8(\u03b1\u2217) = 0 then follows from the intermediate value theorem and (c). Uniqueness of \u03b1\u2217 is guaranteed because \u03a8 is strictly monotone on the relevant interval.\nDefine p := projT (x), then with Theorem 1 there is an \u03b1\u0303 \u2208 R so that p = (\u03bb2/\u2016max(x\u2212\u03b1\u0303\u00b7e, 0)\u20162) \u00b7max(x\u2212 \u03b1\u0303 \u00b7 e, 0) = (\u03bb2/`2(\u03b1\u0303)) \u00b7 q(\u03b1\u0303). Since p \u2208 T we obtain \u03a8(\u03b1\u0303) = 0, and the uniqueness of the zero of \u03a8 implies that \u03b1\u2217 = \u03b1\u0303 . ut\nAs described in Sect. 2.3, the exact value of the zero \u03b1\u2217 of \u03a8 can be found by inspecting the neighboring entries in x of a candidate offset \u03b1 . Let x j,xk \u2208 X be these neighbors with x j \u2264 \u03b1 < xk such that there is no element from X between x j and xk. When \u03a8 changes its sign from x j to xk, we know that its root must be located within this interval. Further, we then know that all coordinates with a value greater than x j must survive the sparseness projection, which yields the set I from Theorem 1 and thus the explicit representation of the projection. The next result gathers these ideas and shows that it is easy to verify whether a change of sign in \u03a8 is on hand.\nLemma 4 Let x \u2208 Rn\u22650 \\ T be given such that \u03a8 is well-defined and let \u03b1 \u2208 [0, xmax) be arbitrary. If \u03b1 < xmin, let x j := 0 and xk := xmin. Otherwise, let x j := max{xi | xi \u2208X and xi \u2264 \u03b1 } be the left neighbor and let xk := min{xi | xi \u2208X and xi > \u03b1 } be the right neighbor of \u03b1 . Both exist as the sets where the maximum and the minimum is taken are nonempty. Define I := { i \u2208 {1, . . . ,n} | xi > \u03b1 } and d := |I|. Then:\n(a) When \u03a8(x j) \u2265 0 and \u03a8(xk) < 0 then there is exactly one number \u03b1\u2217 \u2208 [x j, xk) with \u03a8(\u03b1\u2217) = 0. (b) It is `1(\u03be ) = \u2016VIx\u20161\u2212 d\u03be and `22(\u03be ) = \u2016VIx\u2016 2 2\u2212 2\u03be \u2016VIx\u20161 + d\u03be 2\nfor \u03be \u2208 {x j, \u03b1, xk }. (c) If the inequalities \u03bb2`1(x j) \u2265 \u03bb1`2(x j) and \u03bb2`1(xk) < \u03bb1`2(xk)\nare satisfied and p := projT (x) denotes the projection of x onto T , then I = { i \u2208 {1, . . . ,n} | pi > 0} and hence p can be computed exactly with Theorem 1.\nProof The claim in (a) is obvious with Lemma 3. (b) We find that `1(\u03b1)=\u2211i\u2208I(xi\u2212\u03b1)= \u2016VIx\u20161\u2212d\u03b1 and `2(\u03b1)2 = \u2211i\u2208I(xi\u2212\u03b1)2 = \u2016VIx\u201622\u22122\u03b1\u2016VIx\u20161 +d\u03b12. We have K = I \\ K\u0303 with K := { i \u2208 {1, . . . ,n} | xi > xk } and K\u0303 := { i \u2208 {1, . . . ,n} | xi = xk }. One yields that `1(xk) = \u2211i\u2208K(xi \u2212 xk) = \u2211i\u2208I(xi \u2212 xk)\u2212\u2211i\u2208K\u0303(xi \u2212 xk) = \u2016VIx\u20161 \u2212 dxk. The claim for `2(xk)2 follows analogously.\nLikewise I = J \\ J\u0303 with J := { i \u2208 {1, . . . ,n} | xi > x j } and J\u0303 := { i \u2208 {1, . . . ,n} | xi = x j }, and hence follows `1(x j) = \u2211i\u2208J(xi\u2212 x j) = \u2211i\u2208I(xi\u2212 x j)+\u2211i\u2208J\u0303(xi\u2212 x j) = \u2016VIx\u20161\u2212dx j . The value of `2(x j)2 can be computed in the same manner.\n(c) The condition in the claim is equivalent to the case of\u03a8(x j)\u2265 0 and \u03a8(xk) < 0, hence with (a) there is a number \u03b1\u2217 \u2208 [x j, xk) with \u03a8(\u03b1\u2217) = 0. Note that \u03b1 6= \u03b1\u2217 in general. Write p := projT (x) and let J := { i \u2208 {1, . . . ,n} | pi > 0}. With Theorem 1 follows that i \u2208 J if and only if xi > \u03b1\u2217. But this is equivalent to xi > x j , which in turn is equivalent to xi > \u03b1 , therefore I = J must hold. Thus we already had the correct set of non-vanishing coordinates of the projection in the first place, and \u03b1\u2217 and \u03b2 \u2217 can be computed exactly using the formula from the claim of Theorem 1, which yields the projection p. ut\nProof of Correctness of Projection Algorithm\nIn Sect. 2.3, we informally described our proposed algorithm for carrying out sparseness-enforcing projections, and provided a simplified flowchart in Fig. 3. After the previous theoretical considerations, we now propose and prove the correctness of a formalized algorithm for\nAlgorithm 2 Linear time and constant space evaluation of the auxiliary function \u03a8 .\nInput: Point to be projected x \u2208 Rn\u22650, target norms \u03bb1,\u03bb2 \u2208 R, position \u03b1 \u2208 [0, xmax) where \u03a8 should be evaluated. Output: Values \u03a8(\u03b1),\u03a8 \u2032(\u03b1),\u03a8 \u2032\u2032(\u03b1), \u03a8\u0303(\u03b1), \u03a8\u0303 \u2032(\u03b1) \u2208 R, finished \u2208 { true, false} indicating whether the correct interval has been found, numbers `1, `22 \u2208 R and d \u2208 N needed to exactly compute \u03b1\u2217 with \u03a8(\u03b1\u2217) = 0.\n// Initialize. 1 `1 := 0; `22 := 0; d := 0; 2 x j := 0; \u2206x j :=\u2212\u03b1; xk := \u221e; \u2206xk := \u221e; // Scan through x. 3 for i := 1 to n do 4 t := xi\u2212\u03b1; 5 if t > 0 then 6 `1 := `1 + xi; `22 := ` 2 2 + x 2 i ; d := d +1; 7 if t < \u2206xk then xk := xi; \u2206xk := t; 8 else 9 if t > \u2206x j then x j := xi; \u2206x j := t;\n10 end 11 end\n// Compute \u03a8(\u03b1), \u03a8 \u2032(\u03b1) and \u03a8 \u2032\u2032(\u03b1). 12 `1(\u03b1) := `1\u2212d\u03b1; `2(\u03b1)2 := `22\u22122\u03b1`1 +d\u03b12; 13 \u03a8(\u03b1) := `1(\u03b1)/ \u221a `2(\u03b1)2\u2212 \u03bb1/\u03bb2;\n14 \u03a8 \u2032(\u03b1) := ( `1(\u03b1)2/`2(\u03b1)2\u2212d ) / \u221a `2(\u03b1)2; 15 \u03a8 \u2032\u2032(\u03b1) := 3\u03a8 \u2032(\u03b1) \u00b7 `1(\u03b1)/`2(\u03b1)2;\n// Compute \u03a8\u0303(\u03b1) and \u03a8\u0303 \u2032(\u03b1). 16 \u03a8\u0303(\u03b1) := `1(\u03b1)2/`2(\u03b1)2\u2212 \u03bb 21/\u03bb 22 ; 17 \u03a8\u0303 \u2032(\u03b1) := 2`1(\u03b1)/ \u221a `2(\u03b1)2 \u00b7\u03a8 \u2032(\u03b1);\n// Check for sign change from \u03a8(x j) to \u03a8(xk). 18 finished := \u03bb2(`1\u2212dx j)\u2265 \u03bb1 \u221a `22\u22122x j`1 +dx2j and\n\u03bb2 (`1\u2212dxk)< \u03bb1 \u221a `22\u22122xk`1 +dx2k ;\n19 return ( \u03a8(\u03b1),\u03a8 \u2032(\u03b1),\u03a8 \u2032\u2032(\u03b1),\u03a8\u0303(\u03b1),\u03a8\u0303 \u2032(\u03b1),finished, `1, `22,d ) ;\nthe projection problem. Here, the overall method is split into an algorithm that evaluates the auxiliary function \u03a8 and, based on its derivative, returns additional information required for finding its zero (Algorithm 2). The other part, Algorithm 3, implements the root-finding procedure and carries out the necessary computations to yield the result of the projection. It furthermore returns information that will be required for computation of the projection\u2019s gradient, which we will discuss below.\nTheorem 5 Let x \u2208 Rn\u22650 and p := projT (x) be unique. Then Algorithm 3 computes p in a number of operations linear in the problem dimensionality n and with only constant additional space.\nProof We start by analyzing Algorithm 2, which evaluates \u03a8 at any given position \u03b1 . The output includes the values of the auxiliary function, its first and second derivative, and the value of the transformed auxiliary function and its derivative. There is moreover a Boolean value which indicates whether the interval with the sign change of \u03a8 has been found, and three additional numbers required to compute the zero \u03b1\u2217 of \u03a8 as soon as the correct interval has been found.\nLet I := { i \u2208 {1, . . . ,n} | xi > \u03b1 } denote the indices of all entries in x larger than \u03b1 . In the blocks from Line 1 to Line 11, the algorithm scans through all the coordinates of x. It identifies the elements of I, and computes numbers `1, `22, d, x j and xk on the fly. After Line 11, we clearly have that `1 = \u2016VIx\u20161, `22 = \u2016VIx\u2016 2 2 and d = |I|. Additionally, x j\nAlgorithm 3 Linear time and constant space algorithm for projections onto T . The auxiliary function \u03a8 is evaluated by calls to \"auxiliary\", which are carried out by Algorithm 2. This algorithm operates in-place, the input vector is overwritten by the output vector upon completion.\nInput: Point to be projected x \u2208 Rn\u22650, target norms \u03bb1,\u03bb2 \u2208 R, solver \u2208 {Bisection, Newton, NewtonSqr, Halley}. Output: Projection projT (x) \u2208 T as first element replacing the input vector, numbers `1, `22 \u2208 R and d \u2208 N needed to compute the projection\u2019s gradient with Algorithm 4.\n// Skip root-finding if decreasing sparseness. 1 ( \u03a8(\u03b1),\u03a8 \u2032(\u03b1),\u03a8 \u2032\u2032(\u03b1),\u03a8\u0303(\u03b1),\u03a8\u0303 \u2032(\u03b1),finished, `1, `22,d ) :=\nauxiliary(x, \u03bb1, \u03bb2, 0); 2 if \u03a8(\u03b1)\u2264 0 then go to Line 19; // Sparseness should be increased. 3 lo := 0; up := x2nd-max; \u03b1 := lo+1/2 \u00b7 (up\u2212 lo); 4 ( \u03a8(\u03b1),\u03a8 \u2032(\u03b1),\u03a8 \u2032\u2032(\u03b1),\u03a8\u0303(\u03b1),\u03a8\u0303 \u2032(\u03b1),finished, `1, `22,d ) :=\nauxiliary(x, \u03bb1, \u03bb2, \u03b1); // Start root-finding procedure. 5 while not finished do // Update bisection interval. 6 if \u03a8(a)> 0 then lo := \u03b1 else up := \u03b1; // One iteration of root-finding. 7 if solver = Bisection then \u03b1 := lo+1/2 \u00b7 (up\u2212 lo); 8 else // Use solvers based on derivatives. 9 if solver = Newton then \u03b1 := \u03b1\u2212\u03a8(\u03b1)/\u03a8 \u2032(\u03b1);\n10 else if solver = NewtonSqr then \u03b1 := \u03b1\u2212\u03a8\u0303(\u03b1)/\u03a8\u0303 \u2032(\u03b1); 11 else if solver = Halley then 12 h := 1\u2212 (\u03a8(\u03b1)\u03a8 \u2032\u2032(\u03b1))/(2\u03a8 \u2032(\u03b1)2); 13 \u03b1 := \u03b1\u2212\u03a8(\u03b1)/(max(0.5, min(1.5, h)) \u00b7\u03a8 \u2032(\u03b1)); 14 end // Use bisection if \u03b1 out of bounds. 15 if \u03b1 < lo or \u03b1 > up then \u03b1 := lo+1/2 \u00b7 (up\u2212 lo); 16 end // Evaluate auxiliary function anew. 17 ( \u03a8(\u03b1),\u03a8 \u2032(\u03b1),\u03a8 \u2032\u2032(\u03b1),\u03a8\u0303(\u03b1),\u03a8\u0303 \u2032(\u03b1),finished, `1, `22,d ) := auxiliary(x, \u03bb1, \u03bb2, \u03b1); 18 end\n// Correct interval has been found, compute \u03b1\u2217.\n19 \u03b1\u2217 := 1 d\n( `1\u2212\u03bb1 \u221a d`22\u2212 `21 /\u221a d\u03bb 22 \u2212\u03bb 21 ) ;\n// Compute result of the projection in-place. 20 \u03c1 := 0; 21 for i := 1 to n do 22 t := xi\u2212\u03b1\u2217; if t > 0 then xi := t; \u03c1 := \u03c1 + t2 else xi := 0; 23 end 24 for i := 1 to n do xi := (\u03bb2/\u221a\u03c1) \u00b7 xi; 25 return ( x, `1, `22, d ) ;\nand xk are the left and right neighbors, respectively, of \u03b1 . Therefore, the requirements of Lemma 4 are satisfied.\nThe next two blocks spanning from Line 12 to Line 17 compute scalar numbers according to Lemma 4(b), the definition of \u03a8 , the first two derivatives thereof given explicitly in the proof of Lemma 3(c), the definition of \u03a8\u0303 and its derivative given by the chain rule. In Line 18, it is checked whether the conditions from Lemma 4(c) hold using the statements from Lemma 4(b). The result is stored in the Boolean variable \"finished\".\nFinally all computed numbers are passed back for further processing. Algorithm 2 clearly needs time linear in n and only constant additional space.\nAlgorithm 3 performs the actual projection in-place, and outputs values needed for the gradient of the projection. It uses Algorithm 2 as sub-program by calls to the function \"auxiliary\". The algorithm first checks whether \u03a8(0) \u2264 0, which is fulfilled when \u03c3(x) \u2265 \u03c3\u2217. In this case, all coordinates survive the projection, computation of \u03b1\u2217 is straightforward with Theorem 1 using I = {1, . . . ,n}.\nOtherwise, Lemma 3(d) states that \u03b1\u2217 \u2208 (0, x2nd-max). We can find \u03b1\u2217 numerically with standard root-finding algorithms since \u03a8 is continuous and strictly decreasing on the interval (0, x2nd-max). The concrete variant is chosen by the parameter \"solver\" of Algorithm 3, implementation details can be found in Traub (1964) and Press et al. (2007).\nHere, the root-finding loop starting at Line 5 is terminated once Algorithm 2 indicates that the correct interval for exact computation of the zero \u03b1\u2217 has been identified. It is therefore not necessary to carry out root-finding until numerical convergence, it is enough to only come sufficiently close to \u03b1\u2217. Line 19 computes \u03b1\u2217 based on the projection representation given in Theorem 1. This line is either reached directly from Line 2 if \u03c3(x) \u2265 \u03c3\u2217, or when the statements from Lemma 4(c) hold. The block starting at Line 20 computes max(x\u2212\u03b1\u2217 \u00b7 e, 0) and stores this point\u2019s squared Euclidean norm in the variable \u03c1 . Line 24 computes the number \u03b2 \u2217 from Theorem 1 and multiplies every entry of max(x\u2212\u03b1\u2217 \u00b7 e, 0) with it, such that x finally contains the projection onto T . It would also be possible to create a new vector for the projection result and leave the input vector untouched, at the expense of additional memory requirements which are linear in the problem dimensionality.\nWhen solver = Bisection, the loop in Line 5 is repeated a constant number of times regardless of n (see Sect. 2.3), and since Algorithm 2 terminates in time linear in n, Algorithm 3 needs time only linear in n. Further, the amount of additional memory needed is independent of n, as for Algorithm 2, such that the overall space requirements are constant. Therefore, Algorithm 3 is asymptotically optimal in the sense of complexity theory. ut\nGradient of the Projection\nThom and Palm (2013) have shown that the projection onto T can be grasped as a function almost everywhere which is differentiable almost everywhere. An explicit expression for the projection\u2019s gradient was derived, which depended on the number of alternating projections required for carrying out the projection. Based on the characterization we gained through Theorem 1, we can derive a much simpler expression for the gradient which is also more efficient to compute:\nTheorem 6 Let x \u2208 Rn\u22650 \\ T such that p := projT (x) is unique. Let \u03b1\u2217,\u03b2 \u2217 \u2208 R, I \u2286 {1, . . . ,n} and d := |I| be given as in Theorem 1. When xi 6= \u03b1\u2217 for all i \u2208 {1, . . . ,n}, then projT is differentiable in x with \u2202/\u2202x projT (x) =V T I GVI , where the matrix G \u2208 Rd\u00d7d is given by\nG := \u221a\nb a Ed \u2212 1\u221a ab ( \u03bb 22 e\u0303e\u0303 T +d p\u0303p\u0303T \u2212\u03bb1 ( e\u0303 p\u0303T + p\u0303e\u0303T )) ,\nwith a := d \u2016VIx\u201622\u2212\u2016VIx\u2016 2 1 \u2208 R\u22650 and b := d\u03bb 22 \u2212 \u03bb 21 \u2208 R\u22650. Here, Ed \u2208 Rd\u00d7d is the identity matrix, e\u0303 := VIe \u2208 {1}d is the point where all coordinates are unity, and p\u0303 :=VI p \u2208 Rd>0.\nProof When xi 6= \u03b1\u2217 for all i \u2208 {1, . . . ,n}, then I and d are invariant to local changes in x. Therefore, \u03b1\u2217, \u03b2 \u2217 and projT are differentiable as composition of differentiable functions. In the following, we derive the claimed expression of the gradient of projT in x.\nLet x\u0303 := VIx \u2208 Rd , then \u03b1\u2217 = 1/d \u00b7 ( \u2016x\u0303\u20161\u2212\u03bb1 \u221a a/b ) . Define q\u0303 := VI \u00b7max(x\u2212\u03b1\u2217 \u00b7 e, 0), then q\u0303 = x\u0303\u2212\u03b1\u2217 \u00b7 e\u0303 \u2208 Rd>0 because xi > \u03b1\u2217 for all i \u2208 I. Further p\u0303 = \u03bb2 \u00b7 q\u0303/\u2016q\u0303\u20162, and we have p = V TI VI p = V TI p\u0303 since pi = 0 for all i 6\u2208 I. Application of the chain rule yields\n\u2202 p \u2202x = \u2202V TI p\u0303 \u2202 p\u0303 \u00b7 \u2202 \u2202 q\u0303\n( \u03bb2 \u00b7\nq\u0303 \u2016q\u0303\u20162\n) \u00b7 \u2202 (x\u0303\u2212\u03b1\n\u2217 \u00b7 e\u0303) \u2202 x\u0303 \u00b7 \u2202VIx \u2202x ,\nAlgorithm 4 Product of the gradient of the projection onto T with an arbitrary vector.\nInput: A point y \u2208 Rn, target norms \u03bb1,\u03bb2 \u2208 R, and the results of Algorithm 3: Projection p = projT (x), numbers `1, ` 2 2 \u2208 R and d \u2208 N. Output: z := (\u2202/\u2202x projT (x)) \u00b7 y \u2208 Rn. // Scan and slice input vectors.\n1 j := 0; p\u0303 \u2208 {0}d ; y\u0303 \u2208 {0}d ; sumy\u0303 := 0; scpp\u0303, y\u0303 := 0; 2 for i := 1 to n where pi > 0 do 3 j := j+1; p\u0303 j := pi; y\u0303 j := yi; 4 sumy\u0303 := sumy\u0303 + y\u0303 j; scp p\u0303, y\u0303 := scpp\u0303, y\u0303 + p\u0303 j \u00b7 y\u0303 j; 5 end\n// Compute gradient product in sliced space. 6 a := d`22\u2212 `21; b := d\u03bb 22 \u2212\u03bb 21 ; 7 y\u0303 := \u221a b/a \u00b7 y\u0303;\n8 y\u0303 := y\u0303+ \u221a 1/ab \u00b7 ( \u03bb1 \u00b7 sumy\u0303\u2212d \u00b7 scpp\u0303, y\u0303 ) \u00b7 p\u0303;\n9 y\u0303 := y\u0303+ \u221a 1/ab \u00b7 ( \u03bb1 \u00b7 scpp\u0303, y\u0303\u2212\u03bb 22 \u00b7 sumy\u0303 ) \u00b7 e\u0303;\n// Un-slice to yield final result. 10 j := 0; z \u2208 {0}n; 11 for i := 1 to n where pi > 0 do j := j+1; zi := y\u0303 j; 12 return z;\nand with H := \u03bb2 \u00b7 (\u2202/\u2202 q\u0303 (q\u0303/\u2016q\u0303\u20162)) \u00b7 (\u2202/\u2202 x\u0303 (x\u0303\u2212\u03b1\u2217 \u00b7 e\u0303)) \u2208 Rd\u00d7d follows \u2202 p/\u2202x =V TI HVI , thus it only remains to show G = H.\nOne obtains \u2202/\u2202 q\u0303 (q\u0303/\u2016q\u0303\u20162) = ( Ed \u2212 q\u0303q\u0303T/\u2016q\u0303\u201622 ) /\u2016q\u0303\u20162. Since all the entries of q\u0303 and x\u0303 are positive, their L1 norms equal the dot product with e\u0303. We have \u2016q\u0303\u20161 = \u2016VIx\u20161\u2212 d\u03b1\u2217 = \u03bb1 \u221a a/b, and we obtain that\n\u2016q\u0303\u201622 = \u2016x\u0303\u201622\u2212\u03b1\u2217 \u00b7 ( \u2016x\u0303\u20161 +\u03bb1 \u221a a/b ) = \u2016x\u0303\u201622\u2212 1/d \u00b7 ( \u2016x\u0303\u201621\u2212\u03bb 21 \u00b7 a/b ) =\na/d \u00b7 ( 1+ \u03bb 21/b ) = \u03bb 22 \u00b7 a/b. Now we can compute the gradient of \u03b1 . Clearly b is independent of x\u0303. It is \u2202a/\u2202 x\u0303 = 2dx\u0303T \u2212 2\u2016x\u0303\u20161e\u0303T \u2208 R1\u00d7d . Since x\u0303 = q\u0303+\u03b1\u2217 \u00b7 e\u0303 it follows that dx\u0303\u2212\u2016x\u0303\u20161e\u0303 = dq\u0303\u2212\u03bb1 \u221a a/b \u00b7 e\u0303, and hence \u2202 \u221a a/\u2202 x\u0303 = \u221a 1/a \u00b7 ( dq\u0303\u2212\u03bb1 \u221a a/b \u00b7 e\u0303\n)T . Therefore, we conclude that \u2202\u03b1\u2217/\u2202 x\u0303 = ( \u03bb 22/b ) \u00b7 e\u0303T \u2212 (\u03bb1/\u221aab) \u00b7 q\u0303T \u2208 R1\u00d7d .\nBy substitution into H and multiplying out we see that H = \u221a\nb a ( Ed \u2212 b\u03bb 22 a q\u0303q\u0303T )( Ed \u2212 \u03bb 22 b e\u0303e\u0303 T + \u03bb1\u221a ab e\u0303q\u0303T )\n= \u221a\nb a ( Ed \u2212 \u03bb 22 b e\u0303e\u0303 T + \u03bb1\u221a ab e\u0303q\u0303T\n\u2212 b\u03bb 22 a q\u0303q\u0303T + \u03bb1\u221a ab q\u0303e\u0303T \u2212 \u03bb 2 1 \u03bb 22 a q\u0303q\u0303T ) ,\nwhere q\u0303q\u0303T e\u0303e\u0303T = q\u0303 ( q\u0303T e\u0303 ) e\u0303T = \u03bb1 \u221a a/b \u00b7 q\u0303e\u0303T and q\u0303q\u0303T e\u0303q\u0303T = \u03bb1 \u221a\na/b \u00b7 q\u0303q\u0303T have been used. The claim then follows with b/\u03bb 22 a+ \u03bb 2 1/\u03bb 22 a = d/a and\nq\u0303 = \u221a a/b \u00b7 p\u0303. ut\nThe gradient has a particular simple form, as it is essentially a scaled identity matrix with additive combination of scaled dyadic products of simple vectors. In the situation where not the entire gradient but merely its product with an arbitrary vector is required (as for example in conjunction with the backpropagation algorithm), simple vector operations are already enough to compute the product:\nCorollary 7 Algorithm 4 computes the product of the gradient of the sparseness projection with an arbitrary vector in time and space linear in the problem dimensionality n.\nProof Let y \u2208 Rn be an arbitrary vector in the situation of Theorem 6, and define y\u0303 :=VIy \u2208 Rd . Then one obtains\nGy\u0303 = \u221a\nb a y\u0303+ 1\u221a ab\n( \u03bb1 \u00b7 e\u0303T y\u0303\u2212d \u00b7 p\u0303T y\u0303 ) \u00b7 p\u0303\n+ 1\u221a ab ( \u03bb1 \u00b7 p\u0303T y\u0303\u2212\u03bb 22 \u00b7 e\u0303T y\u0303 ) \u00b7 e\u0303 \u2208 Rd .\nAlgorithm 4 starts by computing the sliced vectors p\u0303 and y\u0303, and computes \"sumy\u0303\" which equals e\u0303T y\u0303 and \"scp p\u0303, y\u0303\" which equals p\u0303\nT y\u0303 after Line 5. It then computes a and b using the numbers output by Algorithm 3. From Line 7 to Line 9, the product Gy\u0303 is computed in-place by scaling of y\u0303, adding a scaled version of p\u0303, and adding a scalar to each coordinate. Since (\u2202/\u2202x projT (x)) \u00b7 y = V TI Gy\u0303, it just remains to invert the slicing. The complexity of the algorithm is clearly linear, both in time and space. ut\nIt has not escaped our notice that Corollary 7 can also be used to determine the eigensystem of the projection\u2019s gradient, which may prove useful for further analysis of gradient-based learning methods involving the sparseness-enforcing projection operator."}], "references": [{"title": "K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation", "author": ["Michal Aharon", "Michael Elad", "Alfred Bruckstein"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Aharon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Aharon et al\\.", "year": 2006}, {"title": "Feature Grouping from Spatially Constrained Multiplicative Interaction", "author": ["Felix Bauer", "Roland Memisevic"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Bauer and Memisevic.,? \\Q2013\\E", "shortCiteRegEx": "Bauer and Memisevic.", "year": 2013}, {"title": "The \"Independent Components\" of Natural Scenes are Edge Filters", "author": ["Anthony J. Bell", "Terrence J. Sejnowski"], "venue": "Vision Research,", "citeRegEx": "Bell and Sejnowski.,? \\Q1997\\E", "shortCiteRegEx": "Bell and Sejnowski.", "year": 1997}, {"title": "Nonlinear Programming", "author": ["Dimitri P. Bertsekas"], "venue": "Athena Scientific, Belmont, 2nd edition,", "citeRegEx": "Bertsekas.,? \\Q1999\\E", "shortCiteRegEx": "Bertsekas.", "year": 1999}, {"title": "Neural Networks for Pattern Recognition", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q1995\\E", "shortCiteRegEx": "Bishop.", "year": 1995}, {"title": "Large Scale Online Learning", "author": ["L\u00e9on Bottou", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bottou and LeCun.,? \\Q2004\\E", "shortCiteRegEx": "Bottou and LeCun.", "year": 2004}, {"title": "Linear Convergence of Iterative Soft-Thresholding", "author": ["Kristian Bredies", "Dirk A. Lorenz"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "Bredies and Lorenz.,? \\Q2008\\E", "shortCiteRegEx": "Bredies and Lorenz.", "year": 2008}, {"title": "The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization", "author": ["Adam Coates", "Andrew Y. Ng"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Coates and Ng.,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng.", "year": 2011}, {"title": "Best Approximation in Inner Product Spaces", "author": ["Frank Deutsch"], "venue": null, "citeRegEx": "Deutsch.,? \\Q2001\\E", "shortCiteRegEx": "Deutsch.", "year": 2001}, {"title": "Image Deblurring and Super-Resolution by Adaptive Sparse Domain Selection and Adaptive Regularization", "author": ["Weisheng Dong", "Lei Zhang", "Guangming Shi", "Xiaolin Wu"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Dong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2011}, {"title": "De-Noising by Soft-Thresholding", "author": ["David L. Donoho"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Donoho.,? \\Q1995\\E", "shortCiteRegEx": "Donoho.", "year": 1995}, {"title": "For Most Large Underdetermined Systems of Linear Equations the Minimal `1-norm Solution Is Also the Sparsest Solution", "author": ["David L. Donoho"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Donoho.,? \\Q2006\\E", "shortCiteRegEx": "Donoho.", "year": 2006}, {"title": "Learning to Sense Sparse Signals: Simultaneous Sensing Matrix and Sparsifying Dictionary Optimization", "author": ["Julio Martin Duarte-Carvajalino", "Guillermo Sapiro"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Duarte.Carvajalino and Sapiro.,? \\Q2009\\E", "shortCiteRegEx": "Duarte.Carvajalino and Sapiro.", "year": 2009}, {"title": "The Approximation of One Matrix by", "author": ["Carl Eckart", "Gale Young"], "venue": "Another of Lower Rank. Psychometrika,", "citeRegEx": "Eckart and Young.,? \\Q1936\\E", "shortCiteRegEx": "Eckart and Young.", "year": 1936}, {"title": "Why Simple Shrinkage Is Still Relevant for Redundant Representations", "author": ["Michael Elad"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Elad.,? \\Q2006\\E", "shortCiteRegEx": "Elad.", "year": 2006}, {"title": "A Mathematical Introduction to Compressive Sensing", "author": ["Simon Foucart", "Holger Rauhut"], "venue": null, "citeRegEx": "Foucart and Rauhut.,? \\Q2013\\E", "shortCiteRegEx": "Foucart and Rauhut.", "year": 2013}, {"title": "A Fast Orthogonal Matching Pursuit Algorithm", "author": ["Mohammad Gharavi-Alkhansari", "Thomas S. Huang"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Gharavi.Alkhansari and Huang.,? \\Q1998\\E", "shortCiteRegEx": "Gharavi.Alkhansari and Huang.", "year": 1998}, {"title": "What Every Computer Scientist Should Know About Floating-Point Arithmetic", "author": ["David Goldberg"], "venue": "ACM Computing Surveys,", "citeRegEx": "Goldberg.,? \\Q1991\\E", "shortCiteRegEx": "Goldberg.", "year": 1991}, {"title": "Separable Dictionary Learning", "author": ["Simon Hawe", "Matthias Seibert", "Martin Kleinsteuber"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Hawe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hawe et al\\.", "year": 2013}, {"title": "Mathematics of Digital Images: Creation, Compression, Restoration, Recognition", "author": ["Stuart G. Hoggar"], "venue": null, "citeRegEx": "Hoggar.,? \\Q2006\\E", "shortCiteRegEx": "Hoggar.", "year": 2006}, {"title": "Adaptive Image Compression using Sparse Dictionaries", "author": ["Inbal Horev", "Ori Bryt", "Ron Rubinstein"], "venue": "In Proceedings of the International Conference on Systems, Signals and Image Processing,", "citeRegEx": "Horev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Horev et al\\.", "year": 2012}, {"title": "Non-negative Matrix Factorization with Sparseness Constraints", "author": ["Patrik O. Hoyer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoyer.,? \\Q2004\\E", "shortCiteRegEx": "Hoyer.", "year": 2004}, {"title": "Independent Component Analysis Applied to Feature Extraction from Colour and Stereo Images. Network: Computation", "author": ["Patrik O. Hoyer", "Aapo Hyv\u00e4rinen"], "venue": "Neural Systems,", "citeRegEx": "Hoyer and Hyv\u00e4rinen.,? \\Q2000\\E", "shortCiteRegEx": "Hoyer and Hyv\u00e4rinen.", "year": 2000}, {"title": "Receptive Fields of Single Neurones in the Cat\u2019s Striate Cortex", "author": ["David H. Hubel", "Torsten N. Wiesel"], "venue": "Journal of Physiology,", "citeRegEx": "Hubel and Wiesel.,? \\Q1959\\E", "shortCiteRegEx": "Hubel and Wiesel.", "year": 1959}, {"title": "Comparing Measures of Sparsity", "author": ["Niall Hurley", "Scott Rickard"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Hurley and Rickard.,? \\Q2009\\E", "shortCiteRegEx": "Hurley and Rickard.", "year": 2009}, {"title": "Sparse Code Shrinkage: Denoising of Nongaussian Data by Maximum Likelihood Estimation", "author": ["Aapo Hyv\u00e4rinen"], "venue": "Neural Computation,", "citeRegEx": "Hyv\u00e4rinen.,? \\Q1999\\E", "shortCiteRegEx": "Hyv\u00e4rinen.", "year": 1999}, {"title": "Emergence of Phase- and ShiftInvariant Features by Decomposition of Natural Images into Independent Feature Subspaces", "author": ["Aapo Hyv\u00e4rinen", "Patrik O. Hoyer"], "venue": "Neural Computation,", "citeRegEx": "Hyv\u00e4rinen and Hoyer.,? \\Q2000\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Hoyer.", "year": 2000}, {"title": "Topographic Independent Component Analysis", "author": ["Aapo Hyv\u00e4rinen", "Patrik O. Hoyer", "Mika Inki"], "venue": "Neural Computation,", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2001}, {"title": "Natural Image Statistics \u2013 A Probabilistic Approach to Early Computational Vision", "author": ["Aapo Hyv\u00e4rinen", "Jarmo Hurri", "Patrik O. Hoyer"], "venue": null, "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2009}, {"title": "An Evaluation of the TwoDimensional Gabor Filter Model of Simple Receptive Fields in Cat Striate Cortex", "author": ["Judson P. Jones", "Larry A. Palmer"], "venue": "Journal of Neurophysiology,", "citeRegEx": "Jones and Palmer.,? \\Q1987\\E", "shortCiteRegEx": "Jones and Palmer.", "year": 1987}, {"title": "Learning Invariant Features through Topographic Filter Maps", "author": ["Koray Kavukcuoglu", "Marc\u2019Aurelio Ranzato", "Rob Fergus", "Yann LeCun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2009}, {"title": "The Self-Organizing Map", "author": ["Teuvo Kohonen"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Kohonen.,? \\Q1990\\E", "shortCiteRegEx": "Kohonen.", "year": 1990}, {"title": "Dictionary Learning Algorithms for Sparse Representation", "author": ["Kenneth Kreutz-Delgado", "Joseph F. Murray", "Bhaskar D. Rao", "Kjersti Engan", "Te-Won Lee", "Terrence J. Sejnowski"], "venue": "Neural Computation,", "citeRegEx": "Kreutz.Delgado et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kreutz.Delgado et al\\.", "year": 2003}, {"title": "Efficient Euclidean Projections in Linear Time", "author": ["Jun Liu", "Jieping Ye"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Liu and Ye.,? \\Q2009\\E", "shortCiteRegEx": "Liu and Ye.", "year": 2009}, {"title": "Estimating Unknown Sparsity in Compressed Sensing", "author": ["Miles E. Lopes"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Lopes.,? \\Q2013\\E", "shortCiteRegEx": "Lopes.", "year": 2013}, {"title": "Online Dictionary Learning for Sparse Coding", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "Non-local Sparse Models for Image Restoration", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro", "Andrew Zisserman"], "venue": "In Proceedings of the International Conference on Computer Vision,", "citeRegEx": "Mairal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2009}, {"title": "A Simplex Method for Function Minimization", "author": ["John Ashworth Nelder", "Roger Mead"], "venue": "The Computer Journal,", "citeRegEx": "Nelder and Mead.,? \\Q1965\\E", "shortCiteRegEx": "Nelder and Mead.", "year": 1965}, {"title": "Some Theorems on Matrix Differentiation with Special Reference to Kronecker Matrix Products", "author": ["Heinz Neudecker"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Neudecker.,? \\Q1969\\E", "shortCiteRegEx": "Neudecker.", "year": 1969}, {"title": "A Biologically Inspired Algorithm for the Recovery of Shading and Reflectance", "author": ["Adriana Olmos", "Frederick A.A. Kingdom"], "venue": "Images. Perception,", "citeRegEx": "Olmos and Kingdom.,? \\Q2004\\E", "shortCiteRegEx": "Olmos and Kingdom.", "year": 2004}, {"title": "Learning Sparse, Overcomplete Representations of Time-Varying Natural Images", "author": ["Bruno A. Olshausen"], "venue": "In Proceedings of the International Conference on Image Processing,", "citeRegEx": "Olshausen.,? \\Q2003\\E", "shortCiteRegEx": "Olshausen.", "year": 2003}, {"title": "Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images", "author": ["Bruno A. Olshausen", "David J. Field"], "venue": null, "citeRegEx": "Olshausen and Field.,? \\Q1996\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1996}, {"title": "Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1", "author": ["Bruno A. Olshausen", "David J. Field"], "venue": "Vision Research,", "citeRegEx": "Olshausen and Field.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1997}, {"title": "Block Coordinate Descent for Sparse NMF", "author": ["Vamsi K. Potluru", "Sergey M. Plis", "Jonathan Le Roux", "Barak A. Pearlmutter", "Vince D. Calhoun", "Thomas P. Hayes"], "venue": "In Proceedings of the International Conference on Learning Representations", "citeRegEx": "Potluru et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Potluru et al\\.", "year": 2013}, {"title": "Numerical Recipes: The Art of Scientific Computing", "author": ["William H. Press", "Saul A. Teukolsky", "William T. Vetterling", "Brian P. Flannery"], "venue": null, "citeRegEx": "Press et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Press et al\\.", "year": 2007}, {"title": "Learning Separable Filters", "author": ["Roberto Rigamonti", "Amos Sironi", "Vincent Lepetit", "Pascal Fua"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Rigamonti et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rigamonti et al\\.", "year": 2013}, {"title": "Spatial Structure and Symmetry of Simple-Cell Receptive Fields in Macaque Primary Visual Cortex", "author": ["Dario L. Ringach"], "venue": "Journal of Neurophysiology,", "citeRegEx": "Ringach.,? \\Q2002\\E", "shortCiteRegEx": "Ringach.", "year": 2002}, {"title": "Thirteen Ways to Look at the Correlation Coefficient", "author": ["Joseph Lee Rodgers", "W. Alan Nicewander"], "venue": "The American Statistician,", "citeRegEx": "Rodgers and Nicewander.,? \\Q1988\\E", "shortCiteRegEx": "Rodgers and Nicewander.", "year": 1988}, {"title": "Sparse Coding via Thresholding and Local Competition in Neural Circuits", "author": ["Christopher J. Rozell", "Don H. Johnson", "Richard G. Baraniuk", "Bruno A. Olshausen"], "venue": "Neural Computation,", "citeRegEx": "Rozell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rozell et al\\.", "year": 2008}, {"title": "Recursive Least Squares Dictionary Learning Algorithm", "author": ["Karl Skretting", "Kjersti Engan"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Skretting and Engan.,? \\Q2010\\E", "shortCiteRegEx": "Skretting and Engan.", "year": 2010}, {"title": "Image Compression Using Learned Dictionaries by RLS-DLA and Compared with K-SVD", "author": ["Karl Skretting", "Kjersti Engan"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Skretting and Engan.,? \\Q2011\\E", "shortCiteRegEx": "Skretting and Engan.", "year": 2011}, {"title": "First Results on Uniqueness of Sparse Non-Negative Matrix Factorization", "author": ["Fabian J. Theis", "Kurt Stadlthanner", "Toshihisa Tanaka"], "venue": "In Proceedings of the European Signal Processing Conference,", "citeRegEx": "Theis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2005}, {"title": "Sparse Activity and Sparse Connectivity in Supervised Learning", "author": ["Markus Thom", "G\u00fcnther Palm"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Thom and Palm.,? \\Q2013\\E", "shortCiteRegEx": "Thom and Palm.", "year": 2013}, {"title": "Learning Sparse Representations of Depth", "author": ["Ivana To\u0161i\u0107", "Bruno A. Olshausen", "Benjamin J. Culpepper"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "citeRegEx": "To\u0161i\u0107 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "To\u0161i\u0107 et al\\.", "year": 2011}, {"title": "Iterative Methods for the Solution of Equations", "author": ["Joe Fred Traub"], "venue": "Prentice-Hall, Englewood Cliffs,", "citeRegEx": "Traub.,? \\Q1964\\E", "shortCiteRegEx": "Traub.", "year": 1964}, {"title": "Independent Component Analysis of Natural Image Sequences Yields Spatio-Temporal Filters Similar to Simple Cells in Primary Visual Cortex", "author": ["J. Hans van Hateren", "Daniel L. Ruderman"], "venue": "Proceedings of the Royal Society B,", "citeRegEx": "Hateren and Ruderman.,? \\Q1998\\E", "shortCiteRegEx": "Hateren and Ruderman.", "year": 1998}, {"title": "Mean Squared Error: Love It or Leave It? A New Look at Signal Fidelity Measures", "author": ["Zhou Wang", "Alan C. Bovik"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Wang and Bovik.,? \\Q2009\\E", "shortCiteRegEx": "Wang and Bovik.", "year": 2009}, {"title": "Image Compression Using the Discrete Cosine Transform", "author": ["Andrew B. Watson"], "venue": "The Mathematica Journal,", "citeRegEx": "Watson.,? \\Q1994\\E", "shortCiteRegEx": "Watson.", "year": 1994}, {"title": "Characterizing the Sparseness of Neural Codes. Network: Computation", "author": ["Ben Willmore", "David J. Tolhurst"], "venue": "Neural Systems,", "citeRegEx": "Willmore and Tolhurst.,? \\Q2001\\E", "shortCiteRegEx": "Willmore and Tolhurst.", "year": 2001}, {"title": "The General Inefficiency of Batch Training for Gradient Descent Learning", "author": ["D. Randall Wilson", "Tony R. Martinez"], "venue": "Neural Networks,", "citeRegEx": "Wilson and Martinez.,? \\Q2003\\E", "shortCiteRegEx": "Wilson and Martinez.", "year": 2003}, {"title": "Image Super-Resolution via Sparse Representation", "author": ["Jianchao Yang", "John Wright", "Thomas S. Huang", "Yi Ma"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Yang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "Coupled Dictionary Training for Image Super-Resolution", "author": ["Jianchao Yang", "Zhaowen Wang", "Zhe Lin", "Scott Cohen", "Thomas Huang"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "Dictionary Optimization for Block-Sparse Representations", "author": ["Lihi Zelnik-Manor", "Kevin Rosenblum", "Yonina C. Eldar"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Zelnik.Manor et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zelnik.Manor et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 27, "context": "zation (Hyv\u00e4rinen et al. 2001; Kavukcuoglu et al. 2009).", "startOffset": 7, "endOffset": 55}, {"referenceID": 30, "context": "zation (Hyv\u00e4rinen et al. 2001; Kavukcuoglu et al. 2009).", "startOffset": 7, "endOffset": 55}, {"referenceID": 9, "context": "plications that benefit from the efficiency gained through sparseness are as diverse as deblurring (Dong et al. 2011), super-resolution (Yang et al.", "startOffset": 99, "endOffset": 117}, {"referenceID": 9, "context": "2011), super-resolution (Yang et al. 2010, 2012; Dong et al. 2011),", "startOffset": 24, "endOffset": 66}, {"referenceID": 20, "context": "compression (Skretting and Engan 2011; Horev et al. 2012), and depth estimation (To\u0161i\u0107 et al.", "startOffset": 12, "endOffset": 57}, {"referenceID": 53, "context": "2012), and depth estimation (To\u0161i\u0107 et al. 2011).", "startOffset": 28, "endOffset": 47}, {"referenceID": 21, "context": "Throughout this paper, we will use the smooth, normalized sparseness measure \u03c3 proposed by Hoyer (2004):", "startOffset": 91, "endOffset": 104}, {"referenceID": 43, "context": "It has been employed successfully for dictionary learning (Hoyer 2004; Potluru et al. 2013), and its smoothness results in improved generalization capabilities in classification tasks compared to when the L0 pseudo-norm is used (Thom and Palm 2013).", "startOffset": 58, "endOffset": 91}, {"referenceID": 32, "context": "A common approach to dictionary learning is the minimization of the reproduction error between the original samples from a learning set and their approximations provided by a linear generative model under sparseness constraints (Olshausen and Field 1996, 1997; Kreutz-Delgado et al. 2003; Mairal et al. 2009a).", "startOffset": 228, "endOffset": 309}, {"referenceID": 51, "context": "Several algorithms were proposed in the past to solve the projection problem (Hoyer 2004; Theis et al. 2005; Potluru et al. 2013; Thom and Palm 2013).", "startOffset": 77, "endOffset": 149}, {"referenceID": 43, "context": "Several algorithms were proposed in the past to solve the projection problem (Hoyer 2004; Theis et al. 2005; Potluru et al. 2013; Thom and Palm 2013).", "startOffset": 77, "endOffset": 149}, {"referenceID": 21, "context": "This paper studies dictionary learning under explicit sparseness constraints with respect to Hoyer\u2019s sparseness measure \u03c3 . A major part of this work is devoted to the efficient algorithmic computation of the sparseness-enforcing projection operator, which is an integral part in efficient dictionary learning. Several algorithms were proposed in the past to solve the projection problem (Hoyer 2004; Theis et al. 2005; Potluru et al. 2013; Thom and Palm 2013). Only Thom and Palm (2013) provided a complete and mathematically satisfactory proof of correctness for their algorithm.", "startOffset": 93, "endOffset": 488}, {"referenceID": 21, "context": "Existing approaches to dictionary learning that feature explicit sparseness constraints can be categorized into ones that use Hoyer\u2019s \u03c3 and ones that employ the L0 pseudonorm. Hoyer (2004) and Potluru et al.", "startOffset": 126, "endOffset": 189}, {"referenceID": 21, "context": "Existing approaches to dictionary learning that feature explicit sparseness constraints can be categorized into ones that use Hoyer\u2019s \u03c3 and ones that employ the L0 pseudonorm. Hoyer (2004) and Potluru et al. (2013) considered matrix factorization frameworks subject to \u03c3 constraints, with", "startOffset": 126, "endOffset": 215}, {"referenceID": 52, "context": "Thom and Palm (2013) designed sparse code words as the result of", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "The following approaches consider explicit L0 pseudonorm constraints: Aharon et al. (2006), Skretting and Engan (2010) and Coates and Ng (2011) infer sparse code words in each iteration compatible to the data and dictionary by employing basis pursuit or matching pursuit algorithms, which has a negative impact on the processing time.", "startOffset": 70, "endOffset": 91}, {"referenceID": 0, "context": "The following approaches consider explicit L0 pseudonorm constraints: Aharon et al. (2006), Skretting and Engan (2010) and Coates and Ng (2011) infer sparse code words in each iteration compatible to the data and dictionary by employing basis pursuit or matching pursuit algorithms, which has a negative impact on the processing time.", "startOffset": 70, "endOffset": 119}, {"referenceID": 0, "context": "The following approaches consider explicit L0 pseudonorm constraints: Aharon et al. (2006), Skretting and Engan (2010) and Coates and Ng (2011) infer sparse code words in each iteration compatible to the data and dictionary by employing basis pursuit or matching pursuit algorithms, which has a negative impact on the processing time.", "startOffset": 70, "endOffset": 144}, {"referenceID": 0, "context": "The following approaches consider explicit L0 pseudonorm constraints: Aharon et al. (2006), Skretting and Engan (2010) and Coates and Ng (2011) infer sparse code words in each iteration compatible to the data and dictionary by employing basis pursuit or matching pursuit algorithms, which has a negative impact on the processing time. Zelnik-Manor et al. (2012) consider block-sparse representations, here the signals are assumed to reside in the union of few subspaces.", "startOffset": 70, "endOffset": 362}, {"referenceID": 0, "context": "The following approaches consider explicit L0 pseudonorm constraints: Aharon et al. (2006), Skretting and Engan (2010) and Coates and Ng (2011) infer sparse code words in each iteration compatible to the data and dictionary by employing basis pursuit or matching pursuit algorithms, which has a negative impact on the processing time. Zelnik-Manor et al. (2012) consider block-sparse representations, here the signals are assumed to reside in the union of few subspaces. Duarte-Carvajalino and Sapiro (2009) propose to simultaneously learn the dictionary and the sensing matrix from example image data, which results in improved reconstruction results in compressed sensing scenarios.", "startOffset": 70, "endOffset": 508}, {"referenceID": 21, "context": "Our technique aims at dictionary learning under explicit sparseness constraints in terms of Hoyer\u2019s sparseness measure \u03c3 using a simple, fast-tocompute and biologically plausible Hebbian-like learning rule. For each presented learning sample, the sparsenessenforcing projection operator has to be carried out. The ability to perform projections efficiently makes the proposed learning algorithm particularly efficient: 30 % less training time is required in comparison to the optimized Online Dictionary Learning method of Mairal et al. (2009a).", "startOffset": 92, "endOffset": 545}, {"referenceID": 32, "context": "The required number of bisection iterations is then less than dlog2(x2nd-max/\u03b4 )e, see Liu and Ye (2009). This number is bounded from above regardless of the dimensionality of the input vector since x2nd-max is upper-bounded and \u03b4 is lower-bounded due to finite machine precision (Goldberg 1991).", "startOffset": 87, "endOffset": 105}, {"referenceID": 28, "context": "and biologically plausible Hebbian-like learning rule (Hyv\u00e4rinen et al. 2009) results:", "startOffset": 54, "endOffset": 77}, {"referenceID": 27, "context": "Topographic organization of the dictionary\u2019s atoms similar to Self-organizing Maps (Kohonen 1990) or Topographic Independent Component Analysis (Hyv\u00e4rinen et al. 2001) can be achieved in a straightforward way with EZDL using alternative inference models proposed in this section.", "startOffset": 144, "endOffset": 167}, {"referenceID": 28, "context": "arbitrarily small (Hyv\u00e4rinen et al. 2009).", "startOffset": 18, "endOffset": 41}, {"referenceID": 32, "context": "This normalization step is also common in a multitude of alternative dictionary learning algorithms (Kreutz-Delgado et al. 2003; Hoyer 2004; Mairal et al. 2009a; Skretting and Engan 2010).", "startOffset": 100, "endOffset": 187}, {"referenceID": 44, "context": "new positions are always located within well-defined intervals (Press et al. 2007).", "startOffset": 63, "endOffset": 82}, {"referenceID": 21, "context": "algorithms proposed by Hoyer (2004), Potluru et al.", "startOffset": 23, "endOffset": 36}, {"referenceID": 21, "context": "algorithms proposed by Hoyer (2004), Potluru et al. (2013), and Thom and Palm (2013) were implemented using the", "startOffset": 23, "endOffset": 59}, {"referenceID": 21, "context": "algorithms proposed by Hoyer (2004), Potluru et al. (2013), and Thom and Palm (2013) were implemented using the", "startOffset": 23, "endOffset": 85}, {"referenceID": 21, "context": "5 Speed-ups relative to the original algorithm of Hoyer (2004) obtained using alternative algorithms.", "startOffset": 50, "endOffset": 63}, {"referenceID": 21, "context": "50 with respect to Hoyer\u2019s \u03c3 . This initial projection better reflects the situation in practice where not completely random vectors have to be processed. Next, all four algorithms were used to compute projections with a target sparseness degree of \u03c3\u2217 := 0.90 and their run time was measured. The original algorithm of Hoyer (2004) was the slowest, so by taking the ratio of the run times of the other algorithms to the run time of that slowest algorithm the relative speed-up was obtained.", "startOffset": 19, "endOffset": 332}, {"referenceID": 42, "context": "While the speed-up of the algorithms of Potluru et al. (2013) and Thom and Palm (2013) relative to the original algorithm of Hoyer (2004) were already significant, espe-", "startOffset": 40, "endOffset": 62}, {"referenceID": 42, "context": "While the speed-up of the algorithms of Potluru et al. (2013) and Thom and Palm (2013) relative to the original algorithm of Hoyer (2004) were already significant, espe-", "startOffset": 40, "endOffset": 87}, {"referenceID": 21, "context": "(2013) and Thom and Palm (2013) relative to the original algorithm of Hoyer (2004) were already significant, espe-", "startOffset": 70, "endOffset": 83}, {"referenceID": 21, "context": "gorithm was more than 15 times faster than the methods of Hoyer (2004), Potluru et al.", "startOffset": 58, "endOffset": 71}, {"referenceID": 21, "context": "gorithm was more than 15 times faster than the methods of Hoyer (2004), Potluru et al. (2013) and Thom and Palm (2013).", "startOffset": 58, "endOffset": 94}, {"referenceID": 21, "context": "gorithm was more than 15 times faster than the methods of Hoyer (2004), Potluru et al. (2013) and Thom and Palm (2013). Because of this appealing asymptotic behavior, there", "startOffset": 58, "endOffset": 119}, {"referenceID": 25, "context": "4 of Hyv\u00e4rinen et al. (2009) with 128 principal components.", "startOffset": 5, "endOffset": 29}, {"referenceID": 29, "context": "We used the methods developed by Jones and Palmer (1987) and Ringach (2002) for a more detailed analysis.", "startOffset": 33, "endOffset": 57}, {"referenceID": 29, "context": "We used the methods developed by Jones and Palmer (1987) and Ringach (2002) for a more detailed analysis.", "startOffset": 33, "endOffset": 76}, {"referenceID": 29, "context": "We used the methods developed by Jones and Palmer (1987) and Ringach (2002) for a more detailed analysis. In doing so, a two-dimensional Gabor function as defined in Equation (1) of Ringach (2002), that is a Gaussian envelope multiplied with a cosine carrier wave, was fit to each dictionary atom using the algorithm of Nelder and Mead (1965).", "startOffset": 33, "endOffset": 197}, {"referenceID": 29, "context": "We used the methods developed by Jones and Palmer (1987) and Ringach (2002) for a more detailed analysis. In doing so, a two-dimensional Gabor function as defined in Equation (1) of Ringach (2002), that is a Gaussian envelope multiplied with a cosine carrier wave, was fit to each dictionary atom using the algorithm of Nelder and Mead (1965). We verified that these Gabor fits accurately described the atoms, which confirmed the Gabor-like nature of the filters.", "startOffset": 33, "endOffset": 343}, {"referenceID": 28, "context": "Principal Component Analysis, low-frequency atoms here minimize the reproduction error best when only very few effective atoms are allowed (Hyv\u00e4rinen et al. 2009; Olshausen and Field 1996).", "startOffset": 139, "endOffset": 188}, {"referenceID": 48, "context": "A major component in the learning algorithm is the sparseness projection, enforcing local competition among the atoms (Rozell et al. 2008) due to its absolute order-preservation property (Thom and Palm 2013, Lemma 12).", "startOffset": 118, "endOffset": 138}, {"referenceID": 28, "context": "Whitening as a pre-processing step helps to reduce sampling artifacts and decorrelates the input data (Hyv\u00e4rinen et al. 2009).", "startOffset": 102, "endOffset": 125}, {"referenceID": 30, "context": "Sparse Decomposition (Kavukcuoglu et al. 2009).", "startOffset": 21, "endOffset": 46}, {"referenceID": 28, "context": "Overcomplete representations are not inherently possible with plain Independent Component Analysis (Bell and Sejnowski 1997; Hyv\u00e4rinen et al. 2009) which limits its expressiveness, a restriction that does not hold for EZDL.", "startOffset": 99, "endOffset": 147}, {"referenceID": 1, "context": "This stands in contrast to the discussion from Bauer and Memisevic (2013) where the necessity of a nonlinearity such as multiplicative interaction or pooling with respect to the Euclidean norm was assumed.", "startOffset": 47, "endOffset": 74}, {"referenceID": 18, "context": "This was also observed recently by Hawe et al. (2013) who considered a tensor decomposition", "startOffset": 35, "endOffset": 54}, {"referenceID": 45, "context": "of the dictionary, and by Rigamonti et al. (2013) who minimized the Schatten 1-norm of the atoms.", "startOffset": 26, "endOffset": 50}, {"referenceID": 35, "context": "Further, the EZDL dictionaries are compared with the results of the Online Dictionary Learning algorithm by Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algo-", "startOffset": 108, "endOffset": 130}, {"referenceID": 49, "context": "rithm by Skretting and Engan (2010) in terms of reproduction quality and learning speed.", "startOffset": 9, "endOffset": 36}, {"referenceID": 35, "context": "(a) Results of the Online Dictionary Learning (ODL) algorithm by Mairal et al. (2009a), where \u03bb trades off between sparseness and reproduction capabilities in a model with implicit sparseness constraints.", "startOffset": 65, "endOffset": 87}, {"referenceID": 49, "context": "(b) Results of the Recursive Least Squares Dictionary Learning Algorithm (RLS-DLA) by Skretting and Engan (2010). Here, \u03b6 denotes the target L0 pseudo-norm of the code words for inference.", "startOffset": 86, "endOffset": 113}, {"referenceID": 35, "context": "line Dictionary Learning (ODL) algorithm of Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algorithm (RLS-DLA) by Skretting and Engan (2010).", "startOffset": 44, "endOffset": 66}, {"referenceID": 35, "context": "line Dictionary Learning (ODL) algorithm of Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algorithm (RLS-DLA) by Skretting and Engan (2010).", "startOffset": 44, "endOffset": 168}, {"referenceID": 35, "context": "Inference Sparseness \u03c3 I ODL by Mairal et al. (2009a)", "startOffset": 32, "endOffset": 54}, {"referenceID": 49, "context": "RLS-DLA by Skretting and Engan (2010)", "startOffset": 11, "endOffset": 38}, {"referenceID": 35, "context": "3 that dictionaries learned with Easy Dictionary Learning are as good as those obtained from the Online Dictionary Learning algorithm of Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algorithm by Skretting and Engan (2010) in terms of the reproduction quality of entire images.", "startOffset": 137, "endOffset": 159}, {"referenceID": 35, "context": "3 that dictionaries learned with Easy Dictionary Learning are as good as those obtained from the Online Dictionary Learning algorithm of Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algorithm by Skretting and Engan (2010) in terms of the reproduction quality of entire images.", "startOffset": 137, "endOffset": 251}, {"referenceID": 35, "context": "3 that dictionaries learned with Easy Dictionary Learning are as good as those obtained from the Online Dictionary Learning algorithm of Mairal et al. (2009a) and the Recursive Least Squares Dictionary Learning Algorithm by Skretting and Engan (2010) in terms of the reproduction quality of entire images. In a final experiment, we investigated the suitability of EZDL dictionaries for image denoising using the image enhancement procedure proposed by Mairal et al. (2009b).", "startOffset": 137, "endOffset": 474}, {"referenceID": 32, "context": "This procedure is quite robust if the input data is noisy, since sparseness provides a strong prior which well regularizes this ill-posed inverse problem (Kreutz-Delgado et al. 2003; Foucart and Rauhut 2013).", "startOffset": 154, "endOffset": 207}, {"referenceID": 35, "context": "The denoising approach of Mairal et al. (2009b) also provides the possibility of dictionary adaptation while denoising concrete input data.", "startOffset": 26, "endOffset": 48}, {"referenceID": 35, "context": "Standard Deviation of Gaussian Noise ODL by Mairal et al. (2009a)", "startOffset": 44, "endOffset": 66}, {"referenceID": 49, "context": "RLS-DLA by Skretting and Engan (2010)", "startOffset": 11, "endOffset": 38}, {"referenceID": 35, "context": "14 Denoising performance in terms of the peak signal-to-noise ratio (PSNR) using the denoising method proposed by Mairal et al. (2009b). There is only a small performance difference between dictionaries trained with ODL, RLS-DLA and EZDL.", "startOffset": 114, "endOffset": 136}, {"referenceID": 33, "context": "The denoising procedure of Mairal et al. (2009b) aims at reproduction capabilities as well, with the modification of employing noisy samples as input.", "startOffset": 27, "endOffset": 49}, {"referenceID": 9, "context": "(2010, 2012), Dong et al. (2011), Skretting and Engan (2011) and Horev et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 9, "context": "(2010, 2012), Dong et al. (2011), Skretting and Engan (2011) and Horev et al.", "startOffset": 14, "endOffset": 61}, {"referenceID": 9, "context": "(2010, 2012), Dong et al. (2011), Skretting and Engan (2011) and Horev et al. (2012) which also use problem formulations based on the reproduction error can hence be", "startOffset": 14, "endOffset": 85}, {"referenceID": 8, "context": "the set of Euclidean projections of x onto M (Deutsch 2001). Since we only consider situations in which projM(x) = {y} is a singleton, we may also write y = projM(x). Without loss of generality, we can compute projT (x) for a vector x \u2208 R\u22650 within the non-negative orthant instead of projS(x) for an arbitrary point x \u2208 Rn to yield sparseness-enforcing projections, where T and S are as defined in Sect. 2. First, the actual scale is irrelevant as we can simply re-scale the result of the projection (Thom and Palm 2013, Remark 5). Second, the constraint that the projection lies in the non-negative orthant R\u22650 can easily be handled by flipping the signs of certain coordinates (Thom and Palm 2013, Lemma 11). Finally, all entries of x can be assumed non-negative with Corollary 19 from Thom and Palm (2013). We note that T is non-convex because of the \u2016s\u20162 = \u03bb2 constraint.", "startOffset": 46, "endOffset": 809}, {"referenceID": 8, "context": "the set of Euclidean projections of x onto M (Deutsch 2001). Since we only consider situations in which projM(x) = {y} is a singleton, we may also write y = projM(x). Without loss of generality, we can compute projT (x) for a vector x \u2208 R\u22650 within the non-negative orthant instead of projS(x) for an arbitrary point x \u2208 Rn to yield sparseness-enforcing projections, where T and S are as defined in Sect. 2. First, the actual scale is irrelevant as we can simply re-scale the result of the projection (Thom and Palm 2013, Remark 5). Second, the constraint that the projection lies in the non-negative orthant R\u22650 can easily be handled by flipping the signs of certain coordinates (Thom and Palm 2013, Lemma 11). Finally, all entries of x can be assumed non-negative with Corollary 19 from Thom and Palm (2013). We note that T is non-convex because of the \u2016s\u20162 = \u03bb2 constraint. Moreover, T 6= / 0 for all target sparseness degrees \u03c3\u2217 \u2208 (0, 1) which we show here by construction (see also Remark 18 in Thom and Palm (2013) for further details): Let \u03c8 := ( \u03bb1\u2212 \u221a n\u03bb 2 2\u2212\u03bb 2 1/ \u221a n\u22121 ) /n > 0 and", "startOffset": 46, "endOffset": 1020}, {"referenceID": 51, "context": "We first present a constructive proof based on a geometric analysis conducted in Thom and Palm (2013), which contributes to deepening our insight into the involved computations.", "startOffset": 81, "endOffset": 102}, {"referenceID": 52, "context": "With Theorem 2 and Appendix D from Thom and Palm (2013) there exists a finite sequence of index sets I1, .", "startOffset": 35, "endOffset": 56}, {"referenceID": 52, "context": "The expressions for the individual projections are given in Lemma 13, Lemma 17, Proposition 24, and Lemma 30, respectively, in Thom and Palm (2013). Let I0 := {1, .", "startOffset": 127, "endOffset": 148}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162.", "startOffset": 66, "endOffset": 87}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162.", "startOffset": 66, "endOffset": 133}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162. We see that \u2016r(0)\u2212m\u20162 = \u2016x\u2212 \u2016x\u20161/n \u00b7 e\u20162 = \u2016x\u20162\u2212 \u2016x\u2016 2 1/n and therefore \u03b4 = \u03b20, and thus s(0) = \u03b20 \u00b7(x\u2212 1/n \u00b7 (\u2016x\u20161\u2212 \u03bb1/\u03b20)e), so the claim holds for the base case. Suppose that (a) holds for j and we want to show it also holds for j+ 1. It is r( j+ 1) = projC(s( j)) by definition, and Proposition 31 in Thom and Palm (2013) implies r( j + 1) = max(s( j)\u2212 t\u0302 \u00b7 e, 0) where t\u0302 \u2208 R can be expressed explicitly as t\u0302 = 1/d j+1 \u00b7 ( \u2211i\u2208I j+1 si( j)\u2212 \u03bb1 ) , which is the mean value of the entries that survive the simplex projection up to an additive constant.", "startOffset": 66, "endOffset": 550}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162. We see that \u2016r(0)\u2212m\u20162 = \u2016x\u2212 \u2016x\u20161/n \u00b7 e\u20162 = \u2016x\u20162\u2212 \u2016x\u2016 2 1/n and therefore \u03b4 = \u03b20, and thus s(0) = \u03b20 \u00b7(x\u2212 1/n \u00b7 (\u2016x\u20161\u2212 \u03bb1/\u03b20)e), so the claim holds for the base case. Suppose that (a) holds for j and we want to show it also holds for j+ 1. It is r( j+ 1) = projC(s( j)) by definition, and Proposition 31 in Thom and Palm (2013) implies r( j + 1) = max(s( j)\u2212 t\u0302 \u00b7 e, 0) where t\u0302 \u2208 R can be expressed explicitly as t\u0302 = 1/d j+1 \u00b7 ( \u2211i\u2208I j+1 si( j)\u2212 \u03bb1 ) , which is the mean value of the entries that survive the simplex projection up to an additive constant. We note that t\u0302 is here always nonnegative, see Lemma 28(a) in Thom and Palm (2013), which we will need to show (b).", "startOffset": 66, "endOffset": 864}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162. We see that \u2016r(0)\u2212m\u20162 = \u2016x\u2212 \u2016x\u20161/n \u00b7 e\u20162 = \u2016x\u20162\u2212 \u2016x\u2016 2 1/n and therefore \u03b4 = \u03b20, and thus s(0) = \u03b20 \u00b7(x\u2212 1/n \u00b7 (\u2016x\u20161\u2212 \u03bb1/\u03b20)e), so the claim holds for the base case. Suppose that (a) holds for j and we want to show it also holds for j+ 1. It is r( j+ 1) = projC(s( j)) by definition, and Proposition 31 in Thom and Palm (2013) implies r( j + 1) = max(s( j)\u2212 t\u0302 \u00b7 e, 0) where t\u0302 \u2208 R can be expressed explicitly as t\u0302 = 1/d j+1 \u00b7 ( \u2211i\u2208I j+1 si( j)\u2212 \u03bb1 ) , which is the mean value of the entries that survive the simplex projection up to an additive constant. We note that t\u0302 is here always nonnegative, see Lemma 28(a) in Thom and Palm (2013), which we will need to show (b). Since I j+1 ( I j we yield si( j) = \u03b2 j \u00b7 (xi \u2212\u03b1 j) for all i \u2208 I j+1 with the induction hypothesis, and therefore we have that t\u0302 = 1/d j+1 \u00b7 ( \u03b2 j\u2016VI j+1 x\u20161\u2212d j+1\u03b2 j\u03b1 j\u2212\u03bb1 ) . We find that ri( j+1)> 0 for i \u2208 I j+1 by definition, and we can omit the rectifier so that ri( j+1) = si( j)\u2212 t\u0302. Using the induction hypothesis and the expression for t\u0302 we have ri( j+1) = \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 + \u03bb1/d j+1. For projecting onto LI j+1 , the distance between r( j + 1) and mI j+1 = \u03bb1/d j+1 \u00b7\u2211i\u2208I j+1 ei is required for computation of \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/d j+1 )/ \u2016r( j+1)\u2212mI j+1\u20162, so that Lemma 30 from Thom and Palm (2013) can be applied.", "startOffset": 66, "endOffset": 1529}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162. We see that \u2016r(0)\u2212m\u20162 = \u2016x\u2212 \u2016x\u20161/n \u00b7 e\u20162 = \u2016x\u20162\u2212 \u2016x\u2016 2 1/n and therefore \u03b4 = \u03b20, and thus s(0) = \u03b20 \u00b7(x\u2212 1/n \u00b7 (\u2016x\u20161\u2212 \u03bb1/\u03b20)e), so the claim holds for the base case. Suppose that (a) holds for j and we want to show it also holds for j+ 1. It is r( j+ 1) = projC(s( j)) by definition, and Proposition 31 in Thom and Palm (2013) implies r( j + 1) = max(s( j)\u2212 t\u0302 \u00b7 e, 0) where t\u0302 \u2208 R can be expressed explicitly as t\u0302 = 1/d j+1 \u00b7 ( \u2211i\u2208I j+1 si( j)\u2212 \u03bb1 ) , which is the mean value of the entries that survive the simplex projection up to an additive constant. We note that t\u0302 is here always nonnegative, see Lemma 28(a) in Thom and Palm (2013), which we will need to show (b). Since I j+1 ( I j we yield si( j) = \u03b2 j \u00b7 (xi \u2212\u03b1 j) for all i \u2208 I j+1 with the induction hypothesis, and therefore we have that t\u0302 = 1/d j+1 \u00b7 ( \u03b2 j\u2016VI j+1 x\u20161\u2212d j+1\u03b2 j\u03b1 j\u2212\u03bb1 ) . We find that ri( j+1)> 0 for i \u2208 I j+1 by definition, and we can omit the rectifier so that ri( j+1) = si( j)\u2212 t\u0302. Using the induction hypothesis and the expression for t\u0302 we have ri( j+1) = \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 + \u03bb1/d j+1. For projecting onto LI j+1 , the distance between r( j + 1) and mI j+1 = \u03bb1/d j+1 \u00b7\u2211i\u2208I j+1 ei is required for computation of \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/d j+1 )/ \u2016r( j+1)\u2212mI j+1\u20162, so that Lemma 30 from Thom and Palm (2013) can be applied. We have that \u2016r( j + 1)\u2212mI j+1\u20162 = \u2211i\u2208I j+1 ( \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 ) 2 = \u03b2 2 j \u00b7 ( \u2016VI j+1 x\u20162\u2212\u2016VI j+1 x\u2016 2 1/d j+1 ) , and further \u03b4 = \u03b2 j+1/\u03b2 j. Now let i\u2208 I j+1 be an index, then we have si( j+ 1) = \u03b4 ri( j+ 1)+ (1\u2212 \u03b4 ) \u00b7 \u03bb1/d j+1 = \u03b2 j+1 \u00b7 ( xi\u2212 1/d j+1 \u00b7 ( \u2016VI j+1 x\u20161\u2212 \u03bb1/\u03b2 j+1 )) using Lemma 30 from Thom and Palm (2013). Therefore (a) holds for all j \u2208 {0, .", "startOffset": 66, "endOffset": 1883}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162. We see that \u2016r(0)\u2212m\u20162 = \u2016x\u2212 \u2016x\u20161/n \u00b7 e\u20162 = \u2016x\u20162\u2212 \u2016x\u2016 2 1/n and therefore \u03b4 = \u03b20, and thus s(0) = \u03b20 \u00b7(x\u2212 1/n \u00b7 (\u2016x\u20161\u2212 \u03bb1/\u03b20)e), so the claim holds for the base case. Suppose that (a) holds for j and we want to show it also holds for j+ 1. It is r( j+ 1) = projC(s( j)) by definition, and Proposition 31 in Thom and Palm (2013) implies r( j + 1) = max(s( j)\u2212 t\u0302 \u00b7 e, 0) where t\u0302 \u2208 R can be expressed explicitly as t\u0302 = 1/d j+1 \u00b7 ( \u2211i\u2208I j+1 si( j)\u2212 \u03bb1 ) , which is the mean value of the entries that survive the simplex projection up to an additive constant. We note that t\u0302 is here always nonnegative, see Lemma 28(a) in Thom and Palm (2013), which we will need to show (b). Since I j+1 ( I j we yield si( j) = \u03b2 j \u00b7 (xi \u2212\u03b1 j) for all i \u2208 I j+1 with the induction hypothesis, and therefore we have that t\u0302 = 1/d j+1 \u00b7 ( \u03b2 j\u2016VI j+1 x\u20161\u2212d j+1\u03b2 j\u03b1 j\u2212\u03bb1 ) . We find that ri( j+1)> 0 for i \u2208 I j+1 by definition, and we can omit the rectifier so that ri( j+1) = si( j)\u2212 t\u0302. Using the induction hypothesis and the expression for t\u0302 we have ri( j+1) = \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 + \u03bb1/d j+1. For projecting onto LI j+1 , the distance between r( j + 1) and mI j+1 = \u03bb1/d j+1 \u00b7\u2211i\u2208I j+1 ei is required for computation of \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/d j+1 )/ \u2016r( j+1)\u2212mI j+1\u20162, so that Lemma 30 from Thom and Palm (2013) can be applied. We have that \u2016r( j + 1)\u2212mI j+1\u20162 = \u2211i\u2208I j+1 ( \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 ) 2 = \u03b2 2 j \u00b7 ( \u2016VI j+1 x\u20162\u2212\u2016VI j+1 x\u2016 2 1/d j+1 ) , and further \u03b4 = \u03b2 j+1/\u03b2 j. Now let i\u2208 I j+1 be an index, then we have si( j+ 1) = \u03b4 ri( j+ 1)+ (1\u2212 \u03b4 ) \u00b7 \u03bb1/d j+1 = \u03b2 j+1 \u00b7 ( xi\u2212 1/d j+1 \u00b7 ( \u2016VI j+1 x\u20161\u2212 \u03bb1/\u03b2 j+1 )) using Lemma 30 from Thom and Palm (2013). Therefore (a) holds for all j \u2208 {0, . . . ,h}. Let us now turn to (b). From the last paragraph, we know that \u03b4 = \u03b2 j+1/\u03b2 j for all j \u2208 {0, . . . ,h\u22121} for the projections onto LI j+1 . On the other hand, we have that \u2016r( j+1)\u2212mI j+1\u20162 = \u2016r( j+1)\u20162\u2212\u03bb 2 1/d j+1 from the proof of Lemma 30(a) from Thom and Palm (2013), and \u2016r( j+ 1)\u20162 \u2264 \u03bb 2 2 holds from the proof of Lemma 28(f) in Thom and Palm (2013), so \u03b4 \u2265 1 which implies \u03b20\u2264 \u00b7\u00b7 \u00b7 \u2264 \u03b2h.", "startOffset": 66, "endOffset": 2200}, {"referenceID": 52, "context": "For j = 0, we have r(0) = x+ 1/n \u00b7 (\u03bb1\u2212\u2016x\u20161)e using Lemma 13 from Thom and Palm (2013). With Lemma 17 stated in Thom and Palm (2013), we have s(0) = \u03b4 r(0)+(1\u2212\u03b4 )m with m = \u03bb1/n \u00b7e and \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/n )/ \u2016r(0)\u2212m\u20162. We see that \u2016r(0)\u2212m\u20162 = \u2016x\u2212 \u2016x\u20161/n \u00b7 e\u20162 = \u2016x\u20162\u2212 \u2016x\u2016 2 1/n and therefore \u03b4 = \u03b20, and thus s(0) = \u03b20 \u00b7(x\u2212 1/n \u00b7 (\u2016x\u20161\u2212 \u03bb1/\u03b20)e), so the claim holds for the base case. Suppose that (a) holds for j and we want to show it also holds for j+ 1. It is r( j+ 1) = projC(s( j)) by definition, and Proposition 31 in Thom and Palm (2013) implies r( j + 1) = max(s( j)\u2212 t\u0302 \u00b7 e, 0) where t\u0302 \u2208 R can be expressed explicitly as t\u0302 = 1/d j+1 \u00b7 ( \u2211i\u2208I j+1 si( j)\u2212 \u03bb1 ) , which is the mean value of the entries that survive the simplex projection up to an additive constant. We note that t\u0302 is here always nonnegative, see Lemma 28(a) in Thom and Palm (2013), which we will need to show (b). Since I j+1 ( I j we yield si( j) = \u03b2 j \u00b7 (xi \u2212\u03b1 j) for all i \u2208 I j+1 with the induction hypothesis, and therefore we have that t\u0302 = 1/d j+1 \u00b7 ( \u03b2 j\u2016VI j+1 x\u20161\u2212d j+1\u03b2 j\u03b1 j\u2212\u03bb1 ) . We find that ri( j+1)> 0 for i \u2208 I j+1 by definition, and we can omit the rectifier so that ri( j+1) = si( j)\u2212 t\u0302. Using the induction hypothesis and the expression for t\u0302 we have ri( j+1) = \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 + \u03bb1/d j+1. For projecting onto LI j+1 , the distance between r( j + 1) and mI j+1 = \u03bb1/d j+1 \u00b7\u2211i\u2208I j+1 ei is required for computation of \u03b4 2 = ( \u03bb 2 2 \u2212 \u03bb 2 1/d j+1 )/ \u2016r( j+1)\u2212mI j+1\u20162, so that Lemma 30 from Thom and Palm (2013) can be applied. We have that \u2016r( j + 1)\u2212mI j+1\u20162 = \u2211i\u2208I j+1 ( \u03b2 jxi\u2212 \u03b2 j/d j+1 \u00b7 \u2016VI j+1 x\u20161 ) 2 = \u03b2 2 j \u00b7 ( \u2016VI j+1 x\u20162\u2212\u2016VI j+1 x\u2016 2 1/d j+1 ) , and further \u03b4 = \u03b2 j+1/\u03b2 j. Now let i\u2208 I j+1 be an index, then we have si( j+ 1) = \u03b4 ri( j+ 1)+ (1\u2212 \u03b4 ) \u00b7 \u03bb1/d j+1 = \u03b2 j+1 \u00b7 ( xi\u2212 1/d j+1 \u00b7 ( \u2016VI j+1 x\u20161\u2212 \u03bb1/\u03b2 j+1 )) using Lemma 30 from Thom and Palm (2013). Therefore (a) holds for all j \u2208 {0, . . . ,h}. Let us now turn to (b). From the last paragraph, we know that \u03b4 = \u03b2 j+1/\u03b2 j for all j \u2208 {0, . . . ,h\u22121} for the projections onto LI j+1 . On the other hand, we have that \u2016r( j+1)\u2212mI j+1\u20162 = \u2016r( j+1)\u20162\u2212\u03bb 2 1/d j+1 from the proof of Lemma 30(a) from Thom and Palm (2013), and \u2016r( j+ 1)\u20162 \u2264 \u03bb 2 2 holds from the proof of Lemma 28(f) in Thom and Palm (2013), so \u03b4 \u2265 1 which implies \u03b20\u2264 \u00b7\u00b7 \u00b7 \u2264 \u03b2h.", "startOffset": 66, "endOffset": 2285}, {"referenceID": 3, "context": "1 from Bertsekas (1999) guarantees the existence of Lagrange multipliers \u03b1\u0303, \u03b2\u0303 \u2208 R and \u03b3\u0303 \u2208Rn with L\u2032(p, \u03b1\u0303, \u03b2\u0303 , \u03b3\u0303) = 0, \u03b3\u0303i \u2265 0 for all i \u2208 {1, .", "startOffset": 7, "endOffset": 24}, {"referenceID": 3, "context": "1 from Bertsekas (1999) guarantees the existence of Lagrange multipliers \u03b1\u0303, \u03b2\u0303 \u2208 R and \u03b3\u0303 \u2208Rn with L\u2032(p, \u03b1\u0303, \u03b2\u0303 , \u03b3\u0303) = 0, \u03b3\u0303i \u2265 0 for all i \u2208 {1, . . . ,n} and \u03b3\u0303i = 0 for i \u2208 I. Assume \u03b2\u0303 = \u22121, then 2x = \u03b1\u0303 \u00b7 e\u2212 \u03b3\u0303 since the derivative of L must vanish. Hence xi = \u03b1\u0303/2 for all i \u2208 I, and therefore { pi | i \u2208 I } is a singleton with Remark 10 from Thom and Palm (2013) as p was assumed unique and T is permutation-invariant.", "startOffset": 7, "endOffset": 373}, {"referenceID": 43, "context": "An alternative is the method proposed by Potluru et al. (2013), where the input vector is sorted and then each possible candidate for I is checked.", "startOffset": 41, "endOffset": 63}, {"referenceID": 53, "context": "The concrete variant is chosen by the parameter \"solver\" of Algorithm 3, implementation details can be found in Traub (1964) and Press et al.", "startOffset": 112, "endOffset": 125}, {"referenceID": 44, "context": "The concrete variant is chosen by the parameter \"solver\" of Algorithm 3, implementation details can be found in Traub (1964) and Press et al. (2007). Here, the root-finding loop starting at Line 5 is terminated once Algorithm 2 indicates that the correct interval for exact computation of the zero \u03b1\u2217 has been identified.", "startOffset": 129, "endOffset": 149}], "year": 2016, "abstractText": "Learning dictionaries suitable for sparse coding instead of using engineered bases has proven effective in a variety of image processing tasks. This paper studies the optimization of dictionaries on image data where the representation is enforced to be explicitly sparse with respect to a smooth, normalized sparseness measure. This involves the computation of Euclidean projections onto level sets of the sparseness measure. While previous algorithms for this optimization problem had at least quasi-linear time complexity, here the first algorithm with linear time complexity and constant space complexity is proposed. The key for this is the mathematically rigorous derivation of a characterization of the projection\u2019s result based on a soft-shrinkage function. This theory is applied in an original algorithm called Easy Dictionary Learning (EZDL), which learns dictionaries with a simple and fast-to-compute Hebbian-like learning rule. The new algorithm is efficient, expressive and particularly simple to implement. It is demonstrated that despite its simplicity, the proposed learning algorithm is able to generate a rich variety of dictionaries, in particular a topographic organization of atoms or separable atoms. Further, the dictionaries are as expressive as those of benchmark learning algorithms in terms of the reproduction quality on entire images, and result in an equivalent denoising performance. EZDL learns approximately 30 % faster than the already very efficient Online Dictionary Learning algorithm, and is Communicated by Julien Mairal, Francis Bach, Michael Elad. M. Thom (B) driveU / Institute of Measurement, Control and Microtechnology Ulm University, 89081 Ulm, Germany e-mail: markus.thom@uni-ulm.de M. Rapp driveU / Institute of Measurement, Control and Microtechnology Ulm University, 89081 Ulm, Germany e-mail: matthias.rapp@uni-ulm.de G. Palm Institute of Neural Information Processing Ulm University, 89081 Ulm, Germany e-mail: guenther.palm@uni-ulm.de therefore eligible for rapid data set analysis and problems with vast quantities of learning samples.", "creator": "LaTeX with hyperref package"}}}