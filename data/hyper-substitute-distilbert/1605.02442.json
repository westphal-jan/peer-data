{"id": "1605.02442", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2016", "title": "Machine Learning Techniques with Ontology for Subjective Answer Evaluation", "abstract": "computerized evaluation of procedural learning is tested using machine learning elements like integrated semantic analysis ( mesa ), generalized lsa, bilingual evaluation understudy and maximum entropy. context, complex natural map of domain evaluation, strategies enhance the performance and these techniques. use simple examples makes the evaluation method holistic or consideration of keywords, pronouns, the right relevant index and usage of concepts can be checked. in linguistic paper, the above mentioned techniques are valid both for cross support similarity and inference on all input data consisting many overlapping answers containing springer scientist. computational ontology simulation computer graphics is investigated and developed. the syntax design advanced implementation houses java programming language matching tools such ole matlab, ` \\'eg \\'e, etc. ten questions from competing manufacturers with sixty answers for each question are used. testing. certain results are questioned though it is concluded that the categories are more advanced with process called ontology.", "histories": [["v1", "Mon, 9 May 2016 07:14:52 GMT  (1013kb)", "http://arxiv.org/abs/1605.02442v1", "11 pages, 5 figures, journal,this http URL2016"]], "COMMENTS": "11 pages, 5 figures, journal,this http URL2016", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.IR", "authors": ["m syamala devi", "himani mittal"], "accepted": false, "id": "1605.02442"}, "pdf": {"name": "1605.02442.pdf", "metadata": {"source": "CRF", "title": "MACHINE LEARNING TECHNIQUES WITH ONTOLOGY FOR SUBJECTIVE ANSWER EVALUATION", "authors": ["M. Syamala Devi", "Himani Mittal"], "emails": [], "sections": [{"heading": null, "text": "DOI: 10.5121/ijnlc.2016.5201 1\nComputerized Evaluation of English Essays is performed using Machine learning techniques like Latent Semantic Analysis (LSA), Generalized LSA, Bilingual Evaluation Understudy and Maximum Entropy. Ontology, a concept map of domain knowledge, can enhance the performance of these techniques. Use of Ontology makes the evaluation process holistic as presence of keywords, synonyms, the right word combination and coverage of concepts can be checked. In this paper, the above mentioned techniques are implemented both with and without Ontology and tested on common input data consisting of technical answers of Computer Science. Domain Ontology of Computer Graphics is designed and developed. The software used for implementation includes Java Programming Language and tools such as MATLAB, Prot\u00e9g\u00e9, etc. Ten questions from Computer Graphics with sixty answers for each question are used for testing. The results are analyzed and it is concluded that the results are more accurate with use of Ontology.\nKEYWORDS\nLatent Semantic Analysis (LSA), Maximum Entropy, Domain Ontology, Generalized LSA, BiLingual\nEvaluation Understudy"}, {"heading": "1. INTRODUCTION", "text": "The manual system for evaluation of Subjective Answers for technical subjects involves a lot of time and effort of the evaluator. Performing evaluation through computers using intelligent techniques ensures uniformity in marking as the same inference mechanism is used for all the students. Subjective answers are evaluated on the basis of content and style of writing. For technical subjects, emphasis is more on content. If standard keywords are found in students\u2019 answer then answer is correct. However, we cannot mark the answers by just counting the number of keywords. A more wholesome approach is required, which can evaluate on the basis of not only keyword presence but the semantic relationship between words and concepts.\nSeveral Machine Learning (ML) techniques exist that try to capture the latent relationship between words. The techniques explored in this paper are - Latent Semantic Analysis (LSA), Generalized Latent Semantic Analysis (GLSA), Maximum Entropy (MaxEnt) and BiLingual Evaluation Understudy (BLEU). These techniques are applied to Subjective Evaluation in previous work of other authors. The existing techniques show an accuracy of results from 59 to 93 percent. However, the evaluation is purely on the basis of keyword presence. An evaluation process is required which measures the wholesome relationship between words, words and concepts and among concepts. Ontology is a concept map of domain knowledge. In this paper, we explore the effect of using Ontology with ML techniques. All the above mentioned techniques\nare implemented both with and without Ontology and tested on common input data. The data was collected by conducting class tests of graduate and post-graduate classes. Then, Human evaluation of these answers was done. Correlations are calculated between human assigned marks and scores of each technique. The use of Ontology with these techniques ensures proper semantic based evaluation as the answers are matched against an exhaustive Knowledge Base. This ensures categorization of answers on the basis of concept category to which the answers belong. Ontology makes the evaluation process holistic as presence of keywords, synonyms, the right word combination and coverage of concepts can be checked. In addition to Subjective Evaluation, Ontology is also used in document categorization, plagiarism detection, sentiment analysis, information retrieval and information extraction.\nThe paper is organized as follows: Section 2 contains the review of related work. Section 3 and 4 state the algorithm steps followed to apply machine learning techniques to technical answer evaluation with and without Ontology. Section 5 discusses the implementation details of different techniques. In Section 6, testing and results are given. Section 7 includes analysis of results. Section 8 contains the conclusions of this research work."}, {"heading": "2. REVIEW OF RELATED WORK", "text": "The research for evaluating subjective answers using computers is ongoing for more than a decade. Several Machine Learning techniques are applied to Subjective Answer Evaluation. Table-1 contains a survey of techniques and tools in Subjective evaluation, already used and future techniques that can be used. The techniques are classified in five categories- Clustering techniques, classification techniques, hybrid natural language processing techniques, soft computing techniques and Semantic techniques.\nLatent Semantic Analysis (LSA) technique, proposed by Deerwester [23], is used to establish\nsimilarity between two contents. Intelligent Essay Assessor (IEA) tool uses LSA [3-7] for subjective evaluation of English essays. IEA was applied to TOEFL exam (ETS) and accuracy of results varies between 59 to 87 percent. Diana Perez [19-20] developed a tool-Atenea, using hybrid of LSA and BiLingual Evaluation Understudy (BLEU). The two techniques are applied independently and the results are combined by a linear equation. However, the coefficient of BLEU and LSA score in the linear equation is left as open question. Author has shown multiple combinations and average success rate is 50 percent of times. Electronic Essay Rater (E-Rater) [17] uses Natural Language Processing techniques to evaluate sentence structure. It is successfully used for AWA test in GMAT and has 84 to 93 percent accuracy. The set of features used are specific to AWA and TOEFL test; its applicability to technical answers is not proven. Discrepant essays are scored like regular essays. Generalized Latent Semantic Analysis (GLSA) [11] extends LSA. Instead of word by document matrix in LSA, phrase (n-gram groups of two or more words) by document matrix is created. Its success rate is 89 percent. C-rater [14-16] uses Maximum Entropy technique (MaxEnt). It has 80 percent agreement with the score assigned by a human-grader for short answers.\nMost of the above techniques are information retrieval techniques of Natural Language Processing. However, the new research area in natural language processing is semantic mapping and information extraction. The Knowledge Base, Ontology is used for semantic mapping. Domain Ontology represents knowledge in the form of Subject-Object-Predicate triples. The application of Ontology to document classification is discussed in [1]. It uses bag-of-concepts approach instead of bag-of-words approach. It maps the keyword to its corresponding Ontology concept and calculates mapping score (m). This score (m) is used to make a concept document matrix and given as input to SVM classifier. The use of Ontology to create bag-of-concepts improved the classification of documents. Turney [2] discusses three approaches to document\nrepresentation in natural language processing- term document vector (tdv), word context vector (wcv) as used in [9] and pair pattern vector (ppv). The tdv based methods find document similarity on the basis of frequency of terms in document. The wcv uses word context into consideration and accordingly the frequency is calculated. In ppv relationship between words is found along with the context using a text similarity score calculation method and using reasoning methods. In this paper, the domain Ontology is used by constructing concept map and calculating distance between two concepts."}, {"heading": "3. APPLICATION OF MACHINE LEARNING TECHNIQUES TO SUBJECTIVE EVALUATION WITHOUT ONTOLOGY", "text": "The machine learning techniques as used in implementing this work are discussed below. The input to all the techniques is keywords in the form of Model Answer and all Student Answers. The output is a similarity measure in the range of [0, 1], where a value of 0 indicates no similarity and 1 indicates high similarity. The input is pre-processed before applying techniques. The steps of pre-processing are: tokenization (find all words in all student answers), stop word removal (removing common words like a, the, as, an etc.), synonym search (for each word left after stop word removal, find its synonyms) and stemming (reduce the words to their stem).\nLatent Semantic Analysis (LSA): The first step is to construct the Term-Document Frequency (tdf) matrix. To calculate tdf matrix, the complete diction of possible words is found by collecting all unique words in model answer and all student answers- called as masterTermVector. Then, we calculate tdf by counting the number of times each word in masterTermVector appears in each student\u2019s answer. Singular Value Decomposition is performed on the tdf matrix. It represents the individual words and all student answers as vectors. The model weight is found by adding the word vectors of all keywords in model answer. Vector Product of model weight and each student\u2019s answer vector is the similarity score.\nGeneralized Latent Semantic Analysis (GLSA): GLSA finds the diction by generating phrases (ngrams) from model answer and all student answers. The phrase length is 2, 3 and 4 neighboring words using sliding window for NGRAM1, NGRAM2 and NGRAM3 respectively. These phrases constitute the masterTermVector. The frequency of each phrase in each student\u2019s answer is calculated to generate tdf. The remaining steps are same as in LSA.\nBiLingual Evaluation Understudy (BLEU): The technique calculates frequency of each word in each student\u2019s answer (sf) and keywords in model answer (kf). Ratio between sf and total number of words in each student\u2019s answer is calculated for each word. If sf value is larger than kf, then kf is used in this word average. The ratio is summed up for all the keywords in each student\u2019s answer to generate BLEU value.\nMaximum Entropy (MAXENT): In MaxEnt, the input is training essays (multiple model answers) and all student answers. No pre-processing steps are performed. It uses a Perceptron to evaluate answers. The training data is used to study the word context \u2013 that is words that mostly follow and precede the word under consideration. The entropy is calculated for the current word to appear in a given context. This entropy is calculated for each word in model answer. The entropy for all word pairs in the model answers and corresponding target marks are given to Perceptron for training and then it is used for all student answers evaluation. Then it reads one student\u2019s answer at a time and finds if student\u2019s answer entails the standard answer concepts."}, {"heading": "4. APPLICATION OF MACHINE LEARNING TECHNIQUES TO SUBJECTIVE EVALUATION WITH ONTOLOGY", "text": ""}, {"heading": "4.1. DESIGN OF ONTOLOGY", "text": "The Domain Ontology of Computer Graphics is prepared using subject-predicate-object representation. The following components of Ontology are defined:\n1) Classes: Sets, collections, concepts and types of objects. For example, the main class is Computer_graphics. Then Computer_graphics has computer_graphics applications, Computer_graphics_systems, types_of_media, etc. Then further classification of each is done as depicted in Figure-1. 2) Individuals: The classes have instances or objects as shown in Firgure-2. For example, image_processing class has individuals like color_coding, improve_picture_quality, improve_quality_of_shading, machine_perception and rearrange_picture_parts. The instances then further have properties. 3) Attributes: The classes and individuals have properties as shown in Figure-3. Some of the attributes identified are: isDefinedBy, uses, technique, purpose, standard, created_using, etc. 4) Relations among Classes: Apart from subclass relation, Ontology uses disjoint and equivalence relations. 5) Process representation: There are many processes and algorithms in graphics for example, working_of_CRT, working_of_plasma_panel, etc. These are represented as event class individuals as shown in Figure-4. These have properties like actor, target, output_of_event, input_to_event, part_of_event, predecessor, successor etc."}, {"heading": "4.2. USAGE OF ONTOLOGY", "text": "When the students\u2019 answers for the question are to be evaluated, the details related to the concept are fetched from the Ontology. The level of detail will depend on the type of question as shown in Table-2. When short questions are to be answered then directly related information is fetched. When longer questions are to be answered then more details are fetched. After the information is extracted from Ontology, a Multi-Hash map is created by collecting all the words corresponding to same concept. This Multi-Hash map is then used in evaluation. Apart from fetching the concept information from Ontology, the relation or similarity score between concepts is also fetched. If the concepts have a path between each other, then the length of such path is calculated. The combining of Ontology with the techniques mentioned in Section 3 is done using the design depicted in Figure 5. After performing preprocessing of words, the model answer and students\u2019 answers are given as input.\nThe Ontology is extracted for the concept on which question is based. The sentences in model answer are then classified as belonging to these concepts. Then the new extended version of Ontology concept map is used for finding similarity between concept map and students\u2019 answers. The path length between various Ontology concepts is used to decide the relative importance of each concept in Ontology and weight to be associated with this concept."}, {"heading": "4.3. MODIFICATION TO EXISTING TECHNIQUES", "text": "The following steps are performed for each technique discussed in Section-3. The technique takes as input- multi-hash map of Ontology concepts, distance between concepts, model answer and students\u2019 answers. First the sentences in model answer are clustered using Ontology concepts and merged with the Ontology map using the machine learning technique in consideration. Then the updated multi-Hash map is used to find correlation between each concept and students\u2019 answer\nusing same machine learning technique. The total number of concepts having positive correlation with students\u2019 answers (q) is multiplied by distance between the main concept and current concept. Then this value is divided by total number of concepts in multi-Hash Map to generate final score.\nAlong with the techniques mentioned in the above section, one more technique is applied with Ontology, the word-weight technique. In word-weight technique, after the words are fetched from Ontology, then words in model answer are combined with Ontology concepts and then weight of each keyword is calculated as \u2013 total occurrence of each word in students\u2019 answer divided by total number of words in students\u2019 answer."}, {"heading": "5. IMPLEMENTATION", "text": "LSA technique is implemented in Java programming language and MatLab. MatLab is used for performing Singular Value Decomposition (SVD) for LSA. Open source libraries are used for invoking MatLab [24] from java code. Synonym Search is done using WordNet [25];[26]. GLSA is implemented by extending LSA package by incorporating n-grams. All the programming and extension tools used are same as in LSA. BLEU is implemented in java programming language. The Java based Maximum Entropy package is freely available at http://maxent.sourceforge.net. The features and working of this package are understood and it is used for evaluation. Wordweight technique is implemented in Java. The details of tools and techniques used for implementation is given in Table 3. Subject Specific Ontology was implemented for a subject of computer science - Computer Graphics. The format used for Ontology is RDF and tool used to construct Ontology is Prot\u00e9g\u00e9 5.0 beta. In order to incorporate Ontology in the techniques, information in Ontology is extracted using Sparql query 1.1 and Apache Jena Library 2.13.0."}, {"heading": "6. TESTING AND RESULTS", "text": "There is no standard database to test subjective answer evaluation. Therefore, the database was created over a period of time by conducting class tests. All the techniques (as implemented above) for answer evaluation have been tested using this common database. The database consists of ten different questions with about sixty answers for each question from the domain of Computer Graphics. The marks are generated for each student\u2019s answer using the techniques with and without Ontology. Table-4 contains the correlations between human assigned score and machine generated score using each technique."}, {"heading": "7. ANALYSIS OF RESULTS", "text": "In this section, the results of each technique are analyzed. It is clear from the Table-4 that BLEU and LSA give the consistent performance with and without Ontology. With Ontology, Maximum Entropy shows a lot of improvement. LSA and GLSA, both overrate if keywords or phrases are repeated many times. BLEU technique is a word average technique and cannot capture the relationship between words. MaxEnt cannot identify the discrepant essays. BLEU identifies discrepant essays. Comparing NGRAM-1,2,3 with LSA, as they are theoretic improvements over LSA, do not give better results than LSA. Actually, they are giving a minimum performance. The use of n-gram size 1,2,3 gives almost same results. So, increasing the size of n-gram does not give better performance. With use of Ontology, all the techniques OBLEU, OLSA, ONG<1, 2, 3> and OMAX perform as good as original techniques. The improvement is not in the overall correlation but the individual marks assigned to each answer. The techniques discussed and implemented in this paper show a high correlation (upto 90 percent) with Human Performance. This is because human evaluation is by and large influenced by answer length, keyword presence and context of keywords. Without Ontology, only two levels of human mind modeling can be covered. Use of Ontology, on the other hand, looks not just for keywords but the keywords appearing in right context and thus models human mind more accurately.\nIn Table-5, all the techniques are compared on the basis of a number of parameters as selected from the working and definition of these techniques. A technique should have properties of Semantic study, negative-positive role, syntactic importance and discrepant essay identification; and should not have bag-of-words property. OMAX has best performance in semantic study (builds concept model for training of neural network), negative-positive role (takes care of word order and identifies yes and no roles) and syntactic importance (parses the sentences and finds importance of each word)."}, {"heading": "8. CONCLUSION AND SCOPE FOR FUTURE WORK", "text": "The techniques discussed and implemented in this paper show a high correlation (upto 90 percent) with Human Performance. This is because human evaluation is by and large influenced by answer length. keyword presence and context of keywords. Use of Ontology, checks for not only keywords but for the keywords appearing in right context. This aspect is lacking in techniques used without Ontology. The use of Ontology checks for presence of keywords,\nsynonyms, right word context and coverage of all concepts. It is concluded that using ML techniques with Ontology gives satisfactory results due to holistic evaluation. This work can be enhanced by using extended Ontology including the concepts of all computer science subjects. It can be further improved to provide students feedback about the missing concepts in their answers.\nTable 5. Performance comparison of techniques used\nCriteria Absolute Yes YES Intermediate NO\nSemantic study\nOMAX (builds concept model) LSA,OLSA, GLSA, OGLSA, MAX\n(build model of answers on basis of keywords)\nBLEU, OBLEU and OWW (since they are word average techniques)\nNegativePositive role\nMAX, OMAX (take word order into consideration and negative role specifically attached to the word itself) GLSA and OGLSA (take word order in consideration)\nBLEU, OBLEU, OWW, LSA,OLSA (no importance to word order and negative role words are treated independent of original words)\nSyntactic Importance\nOMAX, MAX (parsing is done)\nGLSA, OGLSA\nBLEU, OBLEU, OWW, LSA, OLSA\nBag of words LSA, OLSA,\nBLEU, OBLEU, OWW\nMAX, OMAX, GLSA, OGLSA\nTime taking OGLSA, GLSA\n(takes 15 minutes to execute)\nLSA, OLSA BLEU, OBLEU,\nOWW, MAX, OMAX\nDiscrepant Essay Identification\nBLEU, OBLEU OWW, MAX,\nGLSA, OGLSA, LSA, OLSA"}, {"heading": "9. REFERENCES", "text": "[1] B. Rujiang and L. Junhua, \u201cImproving documents classification with semantic features,\u201d 2nd Int.\nSymp. Electron. Commer. Secur. ISECS 2009, vol. 1, pp. 640\u2013643, 2009.\n[2] P. D. Turney and P. Pantel, \u201cFrom frequency to meaning: Vector space models of semantics,\u201d J.\nArtif. Intell. Res., vol. 37, pp. 141\u2013188, 2010.\n[3] T. K. Landauer, \u201cAutomatic Essay Assessment,\u201d Assess. Educ. Princ. Policy Pract., vol. 10, no. 3, pp.\n295\u2013308, 2003.\n[4] T. K. Landauer, P. W. Foltz, and D. Laham, \u201cAn introduction to latent semantic analysis,\u201d Discourse\nProcess., vol. 25, no. 2\u20133, pp. 259\u2013284, 1998.\n[5] T. K. Landauer and P. W. Foltz, \u201cAn introduction to latent semantic analysis,\u201d Discourse Process.,\nno. April 2012, pp. 37\u201341, 2012.\n[6] P. W. Foltz, W. Kintsch, and T. K. Landauer, \u201cThe measurement of textual coherence with latent\nsemantic analysis,\u201d Discourse Process., vol. 25, no. 2\u20133, pp. 285\u2013307, 1998.\n[7] P. W. Foltz, \u201cLatent Semantic Analysis for Text-Based,\u201d Behav. Res. Methods, Instruments Comput.,\nvol. 28, no. 2, pp. 197\u2013202, 1996.\n[8] T. Kakkonen, N. Myller, E. Sutinen, and J. Timonen, \u201cComparison of dimension reduction methods\nfor automated essay grading,\u201d Educ. Technol. Soc., vol. 11, no. 3, pp. 275\u2013288, 2008.\n[9] D. M. Blei, A. Y. Ng, and M. I. Jordan, \u201cLatent Dirichlet Allocation,\u201d J. Mach. Learn. Res., vol. 3,\nno. 4\u20135, pp. 993\u20131022, 2012.\n[10] T. Hofmann, \u201cProbabilistic latent semantic indexing,\u201d Sigir, pp. 50\u201357, 1999. [11] M. Islam, \u201cAutomated Essay Scoring Using Generalized,\u201d in Proceesings of 13th International\nConference on Computer and Information Technology (ICCIT 2010), 2010.\n[12] L. Rudner and T. Liang, \u201cAutomated essay scoring using Bayes\u2019 theorem,\u201d J. Technol. Learn. \u2026,\nvol. 1, no. 2, 2002.\n[13] L. Bin, L. Jun, Y. Jian-Min, and Z. Qiao-Ming, \u201cAutomated essay scoring using the KNN algorithm,\u201d\nProc. - Int. Conf. Comput. Sci. Softw. Eng. CSSE 2008, vol. 1, pp. 735\u2013738, 2008.\n[14] C. Leacock and M. Chodorow, \u201cC-rater: Automated scoring of short-answer questions,\u201d Comput.\nHum., vol. 37, no. 4, pp. 389\u2013405, 2003.\n[15] J. Z. Sukkarieh, \u201cUsing a MaxEnt classifier for the automatic content scoring of free-text responses,\u201d\nAIP Conf. Proc., vol. 1305, pp. 41\u201348, 2010.\n[16] J. Sukkarieh and S. Stoyanchev, \u201cAutomating Model Building in c-rater,\u201d Proc. 2009 Work. \u2026, no.\nAugust, pp. 61\u201369, 2009.\n[17] J. Burstein, K. Kukich, S. Wolff, C. Lu, M. Chodorow, L. Braden-Harder, and M. D. Harris,\n\u201cAutomated scoring using a hybrid feature identification technique,\u201d Proc. 17th Int. Conf. Comput. Linguist. -, vol. 1, p. 206, 1998.\n[18] D. Callear, J. Jerrams-Smith, and V. Soh, \u201cBridging gaps in computerised assessment of texts,\u201d Proc.\n- IEEE Int. Conf. Adv. Learn. Technol. ICALT 2001, pp. 139\u2013140, 2001.\n[19] P. Diana, A. Gliozzo, C. Strapparava, E. Alfonseca, P. Rodr, and B. Magnini, \u201cAutomatic\nAssessment of Students \u2019 free-text Answers underpinned by the Combination of a B LEU -inspired algorithm and Latent Semantic Analysis,\u201d Mach. Transl., 2005.\n[20] F. Noorbehbahani and a. a. Kardan, \u201cThe automatic assessment of free text answers using a modified\nBLEU algorithm,\u201d Comput. Educ., vol. 56, no. 2, pp. 337\u2013345, 2011.\n[21] W. Wang and B. Yu, \u201cText categorization based on combination of modified back propagation neural\nnetwork and latent semantic analysis,\u201d Neural Comput. Appl., vol. 18, no. 8, pp. 875\u2013881, 2009.\n[22] C. A. Kumar, M. Radvansky, and J. Annapurna, \u201cAnalysis of a Vector Space Model , Latent\nSemantic Indexing and Formal Concept Analysis for Information Retrieval,\u201d vol. 12, no. 1, pp. 34\u2013 48, 2012.\n[23] Deerwester, S., Dumais, S.T., Furnas,G.W., Landauer, T.K., & Harshman, R. (1990), Indexing by\nLatent Semantic Analysis, Journal of the American Society for Information Science, 41, 391-407.\n[24] \u201cMatlab Control.\u201d [Online]. Available: https://code.google.com/p/matlabcontrol/. [Accessed: 31-Dec-\n2015].\n[25] G. A. Miller,(1995) \u201cWordNet: a lexical database for English,\u201d Commun. ACM, vol. 38, no. 11, pp.\n39\u201341.\n[26] M. A. Finlayson, (2014) \u201cJava Libraries for Accessing the Princeton Wordnet: Comparison and\nEvaluation.,\u201d .), Proc. 7th Int. Glob. WordNet Conf., pp. 78\u201385."}], "references": [{"title": "Improving documents classification with semantic features", "author": ["B. Rujiang", "L. Junhua"], "venue": "2nd Int. Symp. Electron. Commer. Secur. ISECS 2009, vol. 1, pp. 640\u2013643, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "J. Artif. Intell. Res., vol. 37, pp. 141\u2013188, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic Essay Assessment", "author": ["T.K. Landauer"], "venue": "Assess. Educ. Princ. Policy Pract., vol. 10, no. 3, pp. 295\u2013308, 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "An introduction to latent semantic analysis", "author": ["T.K. Landauer", "P.W. Foltz", "D. Laham"], "venue": "Discourse Process., vol. 25, no. 2\u20133, pp. 259\u2013284, 1998.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "An introduction to latent semantic analysis", "author": ["T.K. Landauer", "P.W. Foltz"], "venue": "Discourse Process., no. April 2012, pp. 37\u201341, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "The measurement of textual coherence with latent semantic analysis", "author": ["P.W. Foltz", "W. Kintsch", "T.K. Landauer"], "venue": "Discourse Process., vol. 25, no. 2\u20133, pp. 285\u2013307, 1998.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Latent Semantic Analysis for Text-Based", "author": ["P.W. Foltz"], "venue": "Behav. Res. Methods, Instruments Comput., vol. 28, no. 2, pp. 197\u2013202, 1996.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Comparison of dimension reduction methods for automated essay grading", "author": ["T. Kakkonen", "N. Myller", "E. Sutinen", "J. Timonen"], "venue": "Educ. Technol. Soc., vol. 11, no. 3, pp. 275\u2013288, 2008.  International Journal on Natural Language Computing (IJNLC) Vol. 5, No.2, April 2016 11", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., vol. 3, no. 4\u20135, pp. 993\u20131022, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "Sigir, pp. 50\u201357, 1999.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Automated Essay Scoring Using Generalized", "author": ["M. Islam"], "venue": "Proceesings of 13th International Conference on Computer and Information Technology (ICCIT 2010), 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Automated essay scoring using Bayes\u2019 theorem", "author": ["L. Rudner", "T. Liang"], "venue": "J. Technol. Learn. ..., vol. 1, no. 2, 2002.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Automated essay scoring using the KNN algorithm", "author": ["L. Bin", "L. Jun", "Y. Jian-Min", "Z. Qiao-Ming"], "venue": "Proc. - Int. Conf. Comput. Sci. Softw. Eng. CSSE 2008, vol. 1, pp. 735\u2013738, 2008.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "C-rater: Automated scoring of short-answer questions", "author": ["C. Leacock", "M. Chodorow"], "venue": "Comput. Hum., vol. 37, no. 4, pp. 389\u2013405, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Using a MaxEnt classifier for the automatic content scoring of free-text responses", "author": ["J.Z. Sukkarieh"], "venue": "AIP Conf. Proc., vol. 1305, pp. 41\u201348, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Automating Model Building in c-rater", "author": ["J. Sukkarieh", "S. Stoyanchev"], "venue": "Proc. 2009 Work. ..., no. August, pp. 61\u201369, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Automated scoring using a hybrid feature identification technique", "author": ["J. Burstein", "K. Kukich", "S. Wolff", "C. Lu", "M. Chodorow", "L. Braden-Harder", "M.D. Harris"], "venue": "Proc. 17th Int. Conf. Comput. Linguist. -, vol. 1, p. 206, 1998.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Bridging gaps in computerised assessment of texts", "author": ["D. Callear", "J. Jerrams-Smith", "V. Soh"], "venue": "Proc. - IEEE Int. Conf. Adv. Learn. Technol. ICALT 2001, pp. 139\u2013140, 2001.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Automatic Assessment of Students \u2019 free-text Answers underpinned by the Combination of a B LEU -inspired algorithm and Latent Semantic Analysis", "author": ["P. Diana", "A. Gliozzo", "C. Strapparava", "E. Alfonseca", "P. Rodr", "B. Magnini"], "venue": "Mach. Transl., 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "The automatic assessment of free text answers using a modified BLEU algorithm", "author": ["F. Noorbehbahani", "a. a. Kardan"], "venue": "Comput. Educ., vol. 56, no. 2, pp. 337\u2013345, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Text categorization based on combination of modified back propagation neural network and latent semantic analysis", "author": ["W. Wang", "B. Yu"], "venue": "Neural Comput. Appl., vol. 18, no. 8, pp. 875\u2013881, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Analysis of a Vector Space Model , Latent Semantic Indexing and Formal Concept Analysis for Information Retrieval", "author": ["C.A. Kumar", "M. Radvansky", "J. Annapurna"], "venue": "vol. 12, no. 1, pp. 34\u2013 48, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Indexing by Latent Semantic Analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1990}, {"title": "WordNet: a lexical database for English,", "author": ["A. G"], "venue": "Commun. ACM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1995}, {"title": "Java Libraries for Accessing the Princeton Wordnet: Comparison and Evaluation.,", "author": ["M.A. Finlayson"], "venue": "Proc. 7th Int. Glob. WordNet Conf.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "Latent Semantic Analysis (LSA) technique, proposed by Deerwester [23], is used to establish similarity between two contents.", "startOffset": 65, "endOffset": 69}, {"referenceID": 2, "context": "Intelligent Essay Assessor (IEA) tool uses LSA [3-7] for subjective evaluation of English essays.", "startOffset": 47, "endOffset": 52}, {"referenceID": 3, "context": "Intelligent Essay Assessor (IEA) tool uses LSA [3-7] for subjective evaluation of English essays.", "startOffset": 47, "endOffset": 52}, {"referenceID": 4, "context": "Intelligent Essay Assessor (IEA) tool uses LSA [3-7] for subjective evaluation of English essays.", "startOffset": 47, "endOffset": 52}, {"referenceID": 5, "context": "Intelligent Essay Assessor (IEA) tool uses LSA [3-7] for subjective evaluation of English essays.", "startOffset": 47, "endOffset": 52}, {"referenceID": 6, "context": "Intelligent Essay Assessor (IEA) tool uses LSA [3-7] for subjective evaluation of English essays.", "startOffset": 47, "endOffset": 52}, {"referenceID": 18, "context": "Diana Perez [19-20] developed a tool-Atenea, using hybrid of LSA and BiLingual Evaluation Understudy (BLEU).", "startOffset": 12, "endOffset": 19}, {"referenceID": 19, "context": "Diana Perez [19-20] developed a tool-Atenea, using hybrid of LSA and BiLingual Evaluation Understudy (BLEU).", "startOffset": 12, "endOffset": 19}, {"referenceID": 16, "context": "Electronic Essay Rater (E-Rater) [17] uses Natural Language Processing techniques to evaluate sentence structure.", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": "Generalized Latent Semantic Analysis (GLSA) [11] extends LSA.", "startOffset": 44, "endOffset": 48}, {"referenceID": 13, "context": "C-rater [14-16] uses Maximum Entropy technique (MaxEnt).", "startOffset": 8, "endOffset": 15}, {"referenceID": 14, "context": "C-rater [14-16] uses Maximum Entropy technique (MaxEnt).", "startOffset": 8, "endOffset": 15}, {"referenceID": 15, "context": "C-rater [14-16] uses Maximum Entropy technique (MaxEnt).", "startOffset": 8, "endOffset": 15}, {"referenceID": 0, "context": "The application of Ontology to document classification is discussed in [1].", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "Turney [2] discusses three approaches to document", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "3 representation in natural language processing- term document vector (tdv), word context vector (wcv) as used in [9] and pair pattern vector (ppv).", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "2003 Landauer Inteliigent Essay Assessor Latent Semantic Processing 59-88% [3],[4]\u2013[7]", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "2003 Landauer Inteliigent Essay Assessor Latent Semantic Processing 59-88% [3],[4]\u2013[7]", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "2003 Landauer Inteliigent Essay Assessor Latent Semantic Processing 59-88% [3],[4]\u2013[7]", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "2008 Kakkonen Automatic essay Assessor LSA, Probabilistic LSA, Latent Dirichlet Allocation LSA better than rest [8],[9], [10]", "startOffset": 112, "endOffset": 115}, {"referenceID": 8, "context": "2008 Kakkonen Automatic essay Assessor LSA, Probabilistic LSA, Latent Dirichlet Allocation LSA better than rest [8],[9], [10]", "startOffset": 116, "endOffset": 119}, {"referenceID": 9, "context": "2008 Kakkonen Automatic essay Assessor LSA, Probabilistic LSA, Latent Dirichlet Allocation LSA better than rest [8],[9], [10]", "startOffset": 121, "endOffset": 125}, {"referenceID": 10, "context": "2010 Islam Generalized Latent Semantic Analysis 86-96% [11]", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "2002 Rudner Betsy Bays Theorem 80% [12]", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "2008 Li bin K-Nearest Neighbor 76% [13]", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "2012 Sukkarieh C-rater Maximum Entropy 80% [14]\u2013[16]", "startOffset": 43, "endOffset": 47}, {"referenceID": 15, "context": "2012 Sukkarieh C-rater Maximum Entropy 80% [14]\u2013[16]", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "1998 Burstein E-rater Hybrid of features 84-94% [17]", "startOffset": 48, "endOffset": 52}, {"referenceID": 17, "context": "2001 Callear Automated Text Marker Conceptual Dependency None [18]", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "2005 Perez Atenea BiLingual Evaluation Understudy, LSA 50% [19], [20]", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": "2005 Perez Atenea BiLingual Evaluation Understudy, LSA 50% [19], [20]", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "[21]", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "20082012 Ontology Based Methods Applied to Information Retrieval Only [1][2][22]", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "20082012 Ontology Based Methods Applied to Information Retrieval Only [1][2][22]", "startOffset": 73, "endOffset": 76}, {"referenceID": 21, "context": "20082012 Ontology Based Methods Applied to Information Retrieval Only [1][2][22]", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "The output is a similarity measure in the range of [0, 1], where a value of 0 indicates no similarity and 1 indicates high similarity.", "startOffset": 51, "endOffset": 57}, {"referenceID": 23, "context": "Synonym Search is done using WordNet [25];[26].", "startOffset": 37, "endOffset": 41}, {"referenceID": 24, "context": "Synonym Search is done using WordNet [25];[26].", "startOffset": 42, "endOffset": 46}], "year": 2016, "abstractText": "Computerized Evaluation of English Essays is performed using Machine learning techniques like Latent Semantic Analysis (LSA), Generalized LSA, Bilingual Evaluation Understudy and Maximum Entropy. Ontology, a concept map of domain knowledge, can enhance the performance of these techniques. Use of Ontology makes the evaluation process holistic as presence of keywords, synonyms, the right word combination and coverage of concepts can be checked. In this paper, the above mentioned techniques are implemented both with and without Ontology and tested on common input data consisting of technical answers of Computer Science. Domain Ontology of Computer Graphics is designed and developed. The software used for implementation includes Java Programming Language and tools such as MATLAB, Prot\u00e9g\u00e9, etc. Ten questions from Computer Graphics with sixty answers for each question are used for testing. The results are analyzed and it is concluded that the results are more accurate with use of Ontology.", "creator": "PScript5.dll Version 5.2.2"}}}