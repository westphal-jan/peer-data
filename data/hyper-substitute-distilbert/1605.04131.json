{"id": "1605.04131", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2016", "title": "Barzilai-Borwein Step Size for Stochastic Gradient Descent", "abstract": "sub family the major issues defining nonlinear gradient learning ( nhl ) methods tells how to decide 1 appropriate step size while running the algorithm. utilizing modern traditional line search technique does not apply for homogeneous problem computation, the alternative practice running sgd is either to draw a diminishing rule pattern, alternatively to tune a selected task size guiding hand. usually, approximately two approaches typically keep time consuming as software. a google paper, we compare or explore weak barzilai - matrix ( dfb ) method to randomly design step sizes for mp and its competitor : continuous variance reduced gradient ( svrg ) coefficients, which leads to alternating alternatives : sgd - ya and sub - bar. we prove that svrg - bb converges linearly for specific convex objective code. as a problem - catch, who prove the partial convergence result of uniformly replacing each optimal proposed algorithm [ 10 ], first approximation event has been overlooked in the domain. numerical experiments on different data sets show that the performance of sgd - bb and svrg - bb is improved to and whereas otherwise darker than sgd and svrg with best - tuned step sizes, and differs approximately like some advanced sgd varieties.", "histories": [["v1", "Fri, 13 May 2016 11:08:50 GMT  (1103kb,D)", "https://arxiv.org/abs/1605.04131v1", null], ["v2", "Mon, 23 May 2016 02:51:08 GMT  (902kb,D)", "http://arxiv.org/abs/1605.04131v2", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["conghui tan", "shiqian ma", "yu-hong dai", "yuqiu qian"], "accepted": true, "id": "1605.04131"}, "pdf": {"name": "1605.04131.pdf", "metadata": {"source": "CRF", "title": "Barzilai-Borwein Step Size for Stochastic Gradient Descent", "authors": ["Conghui Tan", "Shiqian Ma", "Yu-Hong Dai", "Yuqiu Qian"], "emails": ["chtan@se.cuhk.edu.hk,", "sqma@se.cuhk.edu.hk.", "dyh@lsec.cc.ac.cn.", "qyq79@connect.hku.hk."], "sections": [{"heading": "1 Introduction", "text": "The following optimization problem, which minimizes the sum of cost functions over samples from a finite training set, appears frequently in machine learning:\nmin F (x) \u2261 1 n n\u2211 i=1 fi(x), (1.1)\nwhere n is the sample size, and each fi : Rd \u2192 R is the cost function corresponding to the i-th sample data. Throughout this paper, we assume that each fi is convex and differentiable, and the function F is strongly convex. Problem (1.1) is challenging when n is extremely large so that computing F (x) and \u2207F (x) for given x is prohibited. Stochastic gradient descent (SGD) method and its variants have been the main approaches for solving (1.1). In the t-th iteration of SGD, a random training sample it is chosen from {1, 2, . . . , n} and the iterate xt is updated by\nxt+1 = xt \u2212 \u03b7t\u2207fit(xt), (1.2)\nwhere \u2207fit(xt) denotes the gradient of the it-th component function at xt, and \u03b7t > 0 is the step size (a.k.a. learning rate). In (1.2), it is usually assumed that \u2207fit is an unbiased estimation to \u2207F , i.e.,\nE[\u2207fit(xt) | xt] = \u2207F (xt). (1.3) \u2020Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong. Email: chtan@se.cuhk.edu.hk, sqma@se.cuhk.edu.hk. \u2021Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing, China. Email: dyh@lsec.cc.ac.cn. \u00a7Department of Computer Science, The University of Hong Kong, Hong Kong. Email: qyq79@connect.hku.hk.\nar X\niv :1\n60 5.\n04 13\n1v 2\n[ m\nat h.\nO C\n] 2\n3 M\nay 2\n01 6\nHowever, it is known that the total number of gradient evaluations of SGD depends on the variance of the stochastic gradients and it is of sublinear convergence rate for the strongly convex and smooth problem (1.1), which is inferior to the full gradient method. As a result, many works along this line have been focusing on designing variants of SGD that can reduce the variance and improve the complexity. Some popular methods along this line are briefly summarized as follows. The stochastic average gradient (SAG) method proposed by Le Roux et al. [22] updates the iterates by\nxt+1 = xt \u2212 \u03b7t n n\u2211 i=1 yti , (1.4)\nwhere at each iteration a random training sample it is chosen and y t i is defined as\nyti = { \u2207fi(xt) if i = it, yt\u22121i , otherwise.\nIt is shown in [22] that SAG converges linearly for strongly convex problems. The SAGA method proposed by Defazio et al. [7] is an improved version of SAG, and it does not require the strong convexity assumption. It is noted that SAG and SAGA need to store the latest gradients for the n component functions fi. The SDCA method proposed by Shalev-Shwartz and Zhang [24] also requires to store all the component gradients. The stochastic variance reduced gradient (SVRG) method proposed by Johnson and Zhang [10] is now widely used in the machine learning community for solving (1.1), because it achieves the variance reduction effect for SGD, and it does not need to store the n component gradients.\nAs pointed out by Le Roux et al. [22], one important issue regarding to stochastic algorithms (SGD and its variants) that has not been fully addressed in the literature, is how to choose an appropriate step size \u03b7t while running the algorithm. In classical gradient descent method, the step size is usually obtained by employing line search techniques. However, line search is computationally prohibited in stochastic gradient methods because one only has sub-sampled information of function value and gradient. As a result, for SGD and its variants used in practice, people usually use a diminishing step size \u03b7t, or use a best-tuned fixed step size. Neither of these two approaches can be efficient.\nSome recent works that discuss the choice of step size in SGD are summarized as follows. AdaGrad [8] scales the gradient by the square root of the accumulated magnitudes of the gradients in the past iterations, but it still requires a fixed step size \u03b7. [22] suggests a line search technique on the component function fik(x) selected in each iteration, to estimate step size for SAG. [13] suggests performing line search for an estimated function, which is evaluated by a Gaussian process with samples fit(xt). [14] suggests to generate the step sizes by a given function with an unknown parameter, and to use the online SGD to update this unknown parameter.\nOur contributions in this paper are in several folds.\n(i). We propose to use the Barzilai-Borwein (BB) method to compute the step size for SGD and SVRG. The two new methods are named as SGD-BB and SVRG-BB, respectively. The per-iteration computational cost of SGD-BB and SVRG-BB is almost the same as SGD and SVRG, respectively.\n(ii). We prove the linear convergence of SVRG-BB for strongly convex functions. As a byproduct, we show the linear convergence of SVRG with Option I (SVRG-I) proposed in [10]. Note that in [10] only convergence of SVRG with Option II (SVRG-II) was given, and the proof for SVRG-I has been missing in the literature. However, SVRG-I is numerically a better choice than SVRG-II, as demonstrated in [10].\n(iii). We conduct numerical experiments for SGD-BB and SVRG-BB on solving logistic regression and SVM problems. The numerical results show that SGD-BB and SVRG-BB are comparable to and sometimes even better than SGD and SVRG with best-tuned step sizes. We also compare SGD-BB with some advanced SGD variants, and demonstrate that our method is superior.\nThe rest of this paper is organized as follows. In Section 2 we briefly introduce the BB method in the deterministic setting. In Section 3 we propose our SVRG-BB method, and prove its linear convergence for strongly convex functions. As a by-product, we also prove the linear convergence of SVRG-I. In Section 4 we propose our SGD-BB method. A smoothing technique is also implemented to improve the performance of SGD-BB. We conduct numerical experiments for SVRG-BB and SGD-BB in Section 5. Finally, we draw some conclusions in Section 6."}, {"heading": "2 The Barzilai-Borwein Step Size", "text": "The BB method, proposed by Barzilai and Borwein in [3], has been proved to be very successful in solving nonlinear optimization problems. The key idea behind the BB method is motivated by quasi-Newton methods. Suppose we want to solve the unconstrained minimization problem\nmin x f(x), (2.1)\nwhere f is differentiable. A typical iteration of quasi-Newton methods for solving (2.1) takes the following form:\nxt+1 = xt \u2212B\u22121t \u2207f(xt), (2.2)\nwhere Bt is an approximation of the Hessian matrix of f at the current iterate xt. Different choices of Bt give different quasi-Newton methods. The most important feature of Bt is that it must satisfy the so-called secant equation:\nBtst = yt, (2.3)\nwhere st = xt \u2212 xt\u22121 and yt = \u2207f(xt)\u2212\u2207f(xt\u22121) for t \u2265 1. It is noted that in (2.2) one needs to solve a linear system, which may be time consuming when Bt is large and dense. One way to alleviate this burden is to use the BB method, which replaces Bt by a scalar matrix\n1 \u03b7t I.\nHowever, one cannot choose a scalar \u03b7t such that the secant equation (2.3) holds with Bt = 1 \u03b7t I. Instead, one can find \u03b7t such that the residual of the secant equation is minimized, i.e.,\nmin \u03b7t \u2225\u2225\u2225\u2225 1\u03b7t st \u2212 yt \u2225\u2225\u2225\u22252 2 ,\nwhich leads to the following choice of \u03b7t:\n\u03b7t = \u2016st\u201622 s>t yt . (2.4)\nTherefore, a typical iteration of the BB method for solving (2.1) is\nxt+1 = xt \u2212 \u03b7t\u2207f(xt), (2.5)\nwhere \u03b7t is computed by (2.4).\nRemark 2.1. Another choice of \u03b7t is obtained by solving\nmin \u03b7t \u2016st \u2212 \u03b7tyt\u201622,\nwhich leads to\n\u03b7t = s>t yt \u2016yt\u201622 . (2.6)\nIn this paper, we will focus on the choice in (2.4), because the practical performance of (2.4) and (2.6) are similar.\nFor convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [19, 20, 9, 5, 6, 4] and references therein. Recently, BB method has been successfully applied for solving problems arising from emerging applications, such as compressed sensing [28], sparse reconstruction [27] and image processing [26]."}, {"heading": "3 Barzilai-Borwein Step Size for SVRG", "text": "We see from (2.5) and (2.4) that the BB method does not need any parameter and the step size is computed while running the algorithm. This has been the main motivation for us to work out a black-box stochastic gradient descent method that can compute the step size automatically without requiring any parameters. In this section, we propose to incorporate the BB step size to SVRG which leads to the SVRG-BB method.\nThe following assumption is made throughout this section.\nAssumption 3.1. We assume that (1.3) holds for any xt. We assume that the objective function F (x) is \u00b5-strongly convex, i.e.,\nF (y) \u2265 F (x) +\u2207F (x)>(y \u2212 x) + \u00b5 2 \u2016x\u2212 y\u201622, \u2200x, y \u2208 Rd.\nWe also assume that the gradient of each component function fi(x) is L-Lipschitz continuous, i.e.,\n\u2016\u2207fi(x)\u2212\u2207fi(y)\u20162 \u2264 L\u2016x\u2212 y\u20162, \u2200x, y \u2208 Rd.\nUnder this assumption, it is easy to see that \u2207F (x) is also L-Lipschitz continuous:\n\u2016\u2207F (x)\u2212\u2207F (y)\u20162 \u2264 L\u2016x\u2212 y\u20162, \u2200x, y \u2208 Rd."}, {"heading": "3.1 SVRG Method", "text": "The SVRG method proposed by Johnson and Zhang [10] for solving (1.1) is described as in Algorithm 1.\nAlgorithm 1 Stochastic Variance Reduced Gradient (SVRG) Method\nParameters: update frequency m, step size \u03b7, initial point x\u03030 for k = 0, 1, \u00b7 \u00b7 \u00b7 do gk = 1 n \u2211n i=1\u2207fi(x\u0303k)\nx0 = x\u0303k \u03b7k = \u03b7 for t = 0, \u00b7 \u00b7 \u00b7 ,m\u2212 1 do\nRandomly pick it \u2208 {1, . . . , n} xt+1 = xt \u2212 \u03b7k(\u2207fit(xt)\u2212\u2207fit(x\u0303k) + gk)\nend for Option I: x\u0303k+1 = xm Option II: x\u0303k+1 = xt for randomly chosen t \u2208 {1, . . . ,m}\nend for\nThere are two loops in SVRG (Algorithm 1). In the outer loop (each outer iteration is called an epoch), a full gradient gk is computed, which is used in the inner loop for generating stochastic gradients with lower variance. x\u0303 is then chosen, based on the outputs of inner loop, for the next outer loop. Note that two options for choosing x\u0303 are suggested in SVRG. Intuitively, Option I in SVRG (denoted as SVRG-I) is a better choice than Option II (denoted as SVRG-II), because the former used the latest information from the inner loop. This has been confirmed numerically in [10] where SVRG-I was applied to solve real applications. However, the convergence analysis is only available for SVRG-II (see, e.g., [10], [12] and [2]), and the convergence for SVRG-I has been missing in the literature. We now cite the convergence analysis of SVRG-II given in [10] as follows.\nTheorem 3.2 ([10]). Consider SVRG in Algorithm 1 with Optioin II. Let x\u2217 be the optimal solution to problem (1.1). Assume that m is sufficiently large so that\n\u03b1 := 1\n\u00b5\u03b7(1\u2212 2L\u03b7)m +\n2L\u03b7\n1\u2212 2L\u03b7 < 1, (3.1)\nthen we have linear convergence in expectation for SVRG:\nE [F (x\u0303k)\u2212 F (x\u2217)] \u2264 \u03b1k[F (x\u03030)\u2212 F (x\u2217)].\nThere has been a series of follow-up works on SVRG and its variants. Xiao and Zhang [29] developed a proximal SVRG method for minimizing the finite sum function plus a nonsmooth regularizer. [17] applied Nesterov\u2019s acceleration technique to SVRG to improve the convergence rate that depends on the condition number L/\u00b5. [2] proved if the full gradient computation gk was replaced by a growing-batch estimation, the linear convergence rate can be preserved. [1] and [21] showed that SVRG with minor modifications can converge to a stationary point for nonconvex optimization problems."}, {"heading": "3.2 SVRG-BB Method", "text": "It is noted that in SVRG, the step size \u03b7 needs to be provided by the user. According to (3.1), the choice of \u03b7 is dependent on L, which may be difficult to estimate in practice. In this section, we propose the SVRG-BB method that computes the step size using the BB method. Our SVRG-BB algorithm is described in Algorithm 2. Note that the only difference between SVRG and SVRG-BB is that in the latter we use BB method to compute the step size \u03b7k, instead of using a prefixed \u03b7 as in SVRG.\nRemark 3.3. A few remarks are in demand for the SVRG-BB algorithm.\n1. One may notice that \u03b7k is equal to the step size computed by the BB formula (2.4) divided by m. This is because in the inner loop for updating xt, m unbiased gradient estimators are added to x0 to get xm.\n2. If we always set \u03b7k = \u03b7 in SVRG-BB instead of using (3.2), then it reduces to SVRG-I.\n3. For the first outer loop of SVRG-BB, a step size \u03b70 needs to be specified, because we are not able to compute the BB step size for the first outer loop. However, we observed from our numerical experiments that the performance of SVRG-BB is not sensitive to the choice of \u03b70.\n4. The BB step size can also be naturally incorporated to other SVRG variants, such as SVRG with batching [2].\nAlgorithm 2 SVRG with BB step size (SVRG-BB)\nParameters: update frequency m, initial point x\u03030, initial step size \u03b70 (only used in the first epoch) for k = 0, 1, \u00b7 \u00b7 \u00b7 do gk = 1 n \u2211n i=1\u2207fi(x\u0303k)\nif k > 0 then\n\u03b7k = 1 m \u00b7 \u2016x\u0303k \u2212 x\u0303k\u22121\u201622/(x\u0303k \u2212 x\u0303k\u22121)>(gk \u2212 gk\u22121) (3.2)\nend if x0 = x\u0303k for t = 0, \u00b7 \u00b7 \u00b7 ,m\u2212 1 do\nRandomly pick it \u2208 {1, . . . , n} xt+1 = xt \u2212 \u03b7k(\u2207fit(xt)\u2212\u2207fit(x\u0303k) + gk)\nend for x\u0303k+1 = xm\nend for"}, {"heading": "3.3 Linear Convergence Analysis", "text": "In this section, we analyze the linear convergence of SVRG-BB (Algorithm 2) for solving (1.1) with strongly convex objective F (x), and as a by-product, our analysis also proves the linear convergence of SVRG-I.\nThe following lemma, which is from [16], is useful in our analysis.\nLemma 3.4 (co-coercivity). If f(x) : Rd \u2192 R is convex and its gradient is L-Lipschitz continuous, then\n\u2016\u2207f(x)\u2212\u2207f(y)\u201622 \u2264 L(x\u2212 y)>(\u2207f(x)\u2212\u2207f(y)), \u2200x, y \u2208 Rd.\nIn the following, we first prove the following lemma, which reveals the relationship between the distances of two consecutive iterates to the optimal point.\nLemma 3.5. Define\n\u03b1k := (1\u2212 2\u03b7k\u00b5(1\u2212 \u03b7kL))m + 4\u03b7kL\n2\n\u00b5(1\u2212 \u03b7kL) . (3.3)\nFor both SVRG-I and SVRG-BB, we have the following inequality for the k-th epoch:\nE \u2016x\u0303k+1 \u2212 x\u2217\u201622 < \u03b1k\u2016x\u0303k \u2212 x \u2217\u201622,\nwhere x\u2217 is the optimal solution to (1.1).\nProof. Let vtit = \u2207fit(xt) \u2212 \u2207fit(x\u0303k) + \u2207F (x\u0303k) for the k-th epoch of SVRG-I or SVRG-BB. Then,\nE\u2016vtit\u2016 2 2 =E \u2016(\u2207fit(xt)\u2212\u2207fit(x\u2217)) \u2212 (\u2207fit(x\u0303k)\u2212\u2207fit(x\u2217)) +\u2207F (x\u0303k)\u2016 2 2\n\u22642E \u2016\u2207fit(xt)\u2212\u2207fit(x\u2217)\u2016 2 2 + 4E \u2016\u2207fit(x\u0303k)\u2212\u2207fit(x \u2217)\u201622 + 4\u2016\u2207F (x\u0303k)\u2016 2 2 \u22642LE [ (xt \u2212 x\u2217)>(\u2207fi(xt)\u2212\u2207fi(x\u2217)) ] + 4L2\u2016x\u0303k \u2212 x\u2217\u201622 + 4L2\u2016x\u0303k \u2212 x\u2217\u201622 =2L(xt \u2212 x\u2217)>\u2207F (xt) + 8L2\u2016x\u0303k \u2212 x\u2217\u201622,\nwhere in the first inequality we used the inequality (a \u2212 b)2 \u2264 2a2 + 2b2 twice, in the second inequality we applied Lemma 3.4 to fit(x) and used the Lipschitz continuity of \u2207fit and \u2207F , and in the last equality we used the facts that E[\u2207fit(x)] = \u2207F (x) and \u2207F (x\u2217) = 0.\nIn the next, we bound the distance of xt+1 to x \u2217 conditioned on xt and x\u0303k.\nE\u2016xt+1 \u2212 x\u2217\u201622 =E\u2016xt \u2212 \u03b7kvtit \u2212 x\n\u2217\u201622 =\u2016xt \u2212 x\u2217\u201622 \u2212 2\u03b7kE[(xt \u2212 x\u2217)>vtit ] + \u03b7 2 kE\u2016vtit\u2016 2 2 =\u2016xt \u2212 x\u2217\u201622 \u2212 2\u03b7k(xt \u2212 x\u2217)>\u2207F (xt) + \u03b72kE\u2016vtit\u2016 2 2 \u2264\u2016xt \u2212 x\u2217\u201622 \u2212 2\u03b7k(xt \u2212 x\u2217)>\u2207F (xt) + 2\u03b72kL(xt \u2212 x\u2217)>\u2207F (xt) + 8\u03b72kL2\u2016x\u0303k \u2212 x\u2217\u201622 =\u2016xt \u2212 x\u2217\u201622 \u2212 2\u03b7k(1\u2212 \u03b7kL)(xt \u2212 x\u2217)>\u2207F (xt) + 8\u03b72kL2\u2016x\u0303k \u2212 x\u2217\u201622 \u2264\u2016xt \u2212 x\u2217\u201622 \u2212 2\u03b7k\u00b5(1\u2212 \u03b7L)\u2016xt \u2212 x\u2217\u20162 + 8\u03b72kL2\u2016x\u0303k \u2212 x\u2217\u201622 =[1\u2212 2\u03b7k\u00b5(1\u2212 \u03b7kL)]\u2016xt \u2212 x\u2217\u201622 + 8\u03b72kL2\u2016x\u0303k \u2212 x\u2217\u201622,\nwhere in the third equality we used the fact that E[vtit ] = \u2207F (xt), and in the second inequality we used the strong convexity of F (x).\nBy recursively applying the above inequality over t, and noting that x\u0303k = x0 and x\u0303k+1 = xm, we can obtain\nE\u2016x\u0303k+1 \u2212 x\u2217\u201622\n\u2264 [1\u2212 2\u03b7k\u00b5(1\u2212 \u03b7L)]m \u2016x\u0303k \u2212 x\u2217\u201622 + 8\u03b72kL2 m\u22121\u2211 j=0 [1\u2212 2\u03b7k\u00b5(1\u2212 \u03b7L)]j \u2016x\u0303k \u2212 x\u2217\u201622\n< [ (1\u2212 2\u03b7k\u00b5(1\u2212 \u03b7L))m +\n4\u03b7kL 2\n\u00b5(1\u2212 \u03b7kL)\n] \u2016x\u0303k \u2212 x\u2217\u201622\n=\u03b1k\u2016x\u0303k \u2212 x\u2217\u201622.\nThe linear convergence of SVRG-I follows immediately.\nCorollary 3.6. In SVRG-I, if m and \u03b7 are chosen such that\n\u03b1 := (1\u2212 2\u03b7\u00b5(1\u2212 \u03b7L))m + 4\u03b7L 2\n\u00b5(1\u2212 \u03b7L) < 1, (3.4)\nthen SVRG-I (Algorithm 1 with Option I) converges linearly in expectation:\nE \u2016x\u0303k \u2212 x\u2217\u201622 < \u03b1 k\u2016x\u03030 \u2212 x\u2217\u201622.\nRemark 3.7. We now give some remarks on this convergence result.\n1. To the best of our knowledge, this is the first time that the linear convergence of SVRG-I is established.\n2. The condition required in (3.3) is different from the condition required in (3.1) for SVRGII. As m \u2192 +\u221e, the first term in (3.1) converges to 0 sublinearly, while the first term in (3.3) converges to 0 linearly. On the other hand, the second term in (3.1) reveals that m depends on the condition number L/\u00b5 linearly, while the second term in (3.3) suggests that m depends on condition number L/\u00b5 quadratically. As a result, if the problem is ill-conditioned, then the convergence rate given in Corollary 3.6 might be slow.\n3. The convergence result given in Corollary 3.6 is for the iterates x\u0303k, while the one given in Theorem 3.2 is for the objective function values F (x\u0303k).\nThe following theorem establishes the linear convergence of SVRG-BB (Algorithm 2).\nTheorem 3.8. Denote \u03b8 = (1\u2212 e\u22122\u00b5/L)/2. It is easy to see that \u03b8 \u2208 (0, 1/2). In SVRG-BB, if m is chosen such that\nm > max\n{ 2\nlog(1\u2212 2\u03b8) + 2\u00b5/L ,\n4L2\n\u03b8\u00b52 + L \u00b5\n} , (3.5)\nthen SVRG-BB (Algorithm 2) converges linearly in expectation:\nE \u2016x\u0303k \u2212 x\u2217\u201622 < (1\u2212 \u03b8) k\u2016x\u03030 \u2212 x\u2217\u201622.\nProof. Using the strong convexity of function F (x), it is easy to obtain the following upper bound for the BB step size computed in Algorithm 2.\n\u03b7k = 1 m \u00b7 \u2016x\u0303k \u2212 x\u0303k\u22121\u2016 2 2 (x\u0303k \u2212 x\u0303k\u22121)>(gk \u2212 gk\u22121)\n\u2264 1 m \u00b7 \u2016x\u0303k \u2212 x\u0303k\u22121\u2016 2 2\n\u00b5\u2016x\u0303k \u2212 x\u0303k\u22121\u201622 =\n1\nm\u00b5 .\nSimilarly, by the L-Lipschitz continuity of \u2207F (x), it is easy to obtain that \u03b7k is uniformly lower bounded by 1/(mL). Therefore, \u03b1k in (3.3) can be bounded as:\n\u03b1k \u2264 [ 1\u2212 2\u00b5\nmL\n( 1\u2212 L\nm\u00b5\n)]m +\n4L2\nm\u00b52[1\u2212 L/(m\u00b5)] \u2264 exp { \u2212 2\u00b5 mL ( 1\u2212 L m\u00b5 ) \u00b7m } + 4L2 m\u00b52[1\u2212 L/(m\u00b5)]\n= exp { \u22122\u00b5 L + 2 m } +\n4L2\nm\u00b52 \u2212 L\u00b5 ,\nSubstituting (3.5) into the above inequality yields\n\u03b1k < exp { \u22122\u00b5 L + log(1\u2212 2\u03b8) + 2\u00b5 L } +\n4L2\n4L2/\u03b8 + L\u00b5\u2212 L\u00b5 = (1\u2212 2\u03b8) + \u03b8 = 1\u2212 \u03b8.\nThe desired result follows by applying Lemma 3.5."}, {"heading": "4 Barzilai-Borwein Step Size for SGD", "text": "In this section, we propose to incorporate the BB method to SGD (1.2). The BB method does not apply to SGD directly, because SGD never computes the full gradient \u2207F (x). In SGD, \u2207fit(xt) is an unbiased estimation for \u2207F (xt) when it is uniformly sampled (see [15, 30] for studies on importance sampling, which does not sample it uniformly). Therefore, one may suggest to use \u2207fit+1(xt+1) \u2212 \u2207fit(xt) to estimate \u2207F (xt+1) \u2212 \u2207F (xt) when computing the BB step size using formula (2.4). However, this approach does not work well because of the variance of the stochastic gradient estimates. The recent work by Sopy la and Drozda [25] suggested several variants of this idea to compute an estimated BB step size using the stochastic gradients. However, these ideas lack theoretical justifications and the numerical results in [25] show that these approaches are inferior to existing methods such as averaged SGD [18].\nThe SGD-BB algorithm we propose in this paper works in the following manner. We call every m iterations of SGD as one epoch. Following the idea of SVRG-BB, SGD-BB also uses the same step size computed by the BB formula in every epoch. Our SGD-BB algorithm is described as in Algorithm 3.\nRemark 4.1. We have a few remarks about SGD-BB (Algorithm 3).\nAlgorithm 3 SGD with BB step size (SGD-BB)\nParameters: update frequency m, initial step sizes \u03b70 and \u03b71 (only used in the first two epochs), weighting parameter \u03b2 \u2208 (0, 1), initial point x\u03030 for k = 0, 1, \u00b7 \u00b7 \u00b7 do\nif k > 0 then \u03b7k = 1 m \u00b7 \u2016x\u0303k \u2212 x\u0303k\u22121\u2016 2 2/|(x\u0303k \u2212 x\u0303k\u22121)>(g\u0302k \u2212 g\u0302k\u22121)| end if x0 = x\u0303k g\u0302k+1 = 0 for t = 0, \u00b7 \u00b7 \u00b7 ,m\u2212 1 do\nRandomly pick it \u2208 {1, . . . , n} xt+1 = xt \u2212 \u03b7k\u2207fit(xt) (\u2217) g\u0302k+1 = \u03b2\u2207fit(xt) + (1\u2212 \u03b2)g\u0302k+1\nend for x\u0303k+1 = xm\nend for\n1. SGD-BB takes the average of the stochastic gradients in one epoch as an estimation of the full gradient.\n2. Note that for computing \u03b7k in Algorithm 3, we actually take the absolute value for the BB formula (2.4). This is because that unlike SVRG-BB, g\u0302k in Algorithm 3 is the average of m stochastic gradients at different iterates, not an exact full gradient. As a result, the step size generated by (2.4) can be negative. This can be seen from the following argument. Suppose \u03b2 is chosen such that\ng\u0302k = 1\nm m\u22121\u2211 t=0 \u2207fit(xt), (4.1)\nwhere we use the same notation as in Algorithm 2 and xt (t = 0, 1, . . . ,m\u2212 1) denote the iterates in the (k \u2212 1)-st epoch. From (4.1), it is easy to see that\nx\u0303k \u2212 x\u0303k\u22121 = \u2212m\u03b7k\u22121g\u0302k.\nBy substituting this equality into the equation for computing \u03b7k in Algorithm 3, we have\n\u03b7k = 1 m \u00b7 \u2016x\u0303k \u2212 x\u0303k\u22121\u2016 2 |(x\u0303k \u2212 x\u0303k\u22121)>(g\u0302k \u2212 g\u0302k\u22121)|\n= 1 m \u00b7 \u2016 \u2212m\u03b7k\u22121g\u0302k\u2016 2 |(\u2212m\u03b7k\u22121g\u0302k)>(g\u0302k \u2212 g\u0302k\u22121)|\n= \u03b7k\u22121\u2223\u22231\u2212 g\u0302>k g\u0302k\u22121/\u2016g\u0302k\u201622\u2223\u2223 . (4.2)\nWithout taking the absolute value, the denominator of (4.2) is g\u0302>k g\u0302k\u22121/\u2016g\u0302k\u201622 \u2212 1, which can be negative in stochastic settings.\n3. Moreover, from (4.2) we have the following observations. If g\u0302>k g\u0302k\u22121 < 0, then \u03b7k is smaller than \u03b7k\u22121. This is reasonable because g\u0302 > k g\u0302k\u22121 < 0 indicates that the step size is too large\nand we need to shrink it. If g\u0302>k g\u0302k\u22121 > 0, then it indicates that we should be more aggressive to take larger step size. We found from our numerical experiments that when the iterates are close to optimum, the size of g\u0302k and g\u0302k\u22121 do not differentiate much. As a result, \u03b7k is usually increased from \u03b7k\u22121 by using (4.2). Hence, the way we compute \u03b7k in Algorithm 3 is in a sense to dynamically adjust the step size, by evaluating whether we are moving the iterates along the right direction. This kind of idea can be traced back to [11].\n4. Furthermore, in order to make sure the averaged stochastic gradients g\u0302k in (4.1) is close to \u2207F (x\u0303k), it is natural to emphasize more on the latest sample gradients. Therefore, in Algorithm 3 we update g\u0302k recursively using\ng\u0302k+1 = \u03b2\u2207fit(xt) + (1\u2212 \u03b2)g\u0302k+1,\nstarting from g\u0302k+1 = 0, where \u03b2 \u2208 (0, 1) is a weighting parameter.\nNote that SGD-BB requires the averaged gradients in two epochs to compute the BB step size, which can only be done starting from the third epoch. Therefore, we need to specify the step sizes \u03b70 and \u03b71 for the first two epochs. From our numerical experiments, we found that the performance of SGD-BB is not sensitive to choices of \u03b70 and \u03b71."}, {"heading": "4.1 Smoothing Technique for the Step Sizes", "text": "Due to the randomness of the stochastic gradients, the step size computed in SGD-BB may vibrate drastically sometimes and this may cause instability of the algorithm. Inspired by [14], we propose the following smoothing technique to stabilize the step size.\nIt is known that in order to guarantee the convergence of SGD, the step sizes are required to be diminishing. Similar as in [14], we assume the step sizes are in the form of C/\u03c6(k), where C > 0 is an unknown constant that needs to be estimated, \u03c6(k) is a pre-specified function that controls the decreasing rate of the step size, and a typical choice of function \u03c6 is \u03c6(k) = k + 1. In the k-th epoch of Algorithm 3, we have all the previous step sizes \u03b72, \u03b73, . . . , \u03b7k generated by the BB method, while the step sizes generated by the function C/\u03c6(k) are given by C/\u03c6(2), C/\u03c6(3), . . . , C/\u03c6(k). In order to ensure that these two sets of step sizes are close to each other, we solve the following optimization problem to determine the unknown parameter C:\nC\u0302k := argmin C k\u2211 j=2 [ log C \u03c6(j) \u2212 log \u03b7j ]2 . (4.3)\nHere we take the logarithms of the step sizes to ensure that the estimation is not dominated by those \u03b7j \u2019s with large magnitudes. It is easy to verify that the solution to (4.3) is given by\nC\u0302k = k\u220f j=2 [\u03b7j\u03c6(j)] 1/(k\u22121) .\nTherefore, the smoothed step size for the k-th epoch of Algorithm 3 is:\n\u03b7\u0303k = C\u0302k/\u03c6(k) = k\u220f j=2 [\u03b7j\u03c6(j)] 1/(k\u22121) /\u03c6(k). (4.4)\nThat is, we replace the \u03b7k in equation (\u2217) of Algorithm 3 by \u03b7\u0303k in (4.4). In practice, we do not need to store all the \u03b7j \u2019s and C\u0302k can be computed recursively by\nC\u0302k = C\u0302 (k\u22122)/(k\u22121) k\u22121 \u00b7 [\u03b7k\u03c6(k)] 1/(k\u22121) ."}, {"heading": "4.2 Incorporating BB Step Size to SGD Variants", "text": "The BB step size and the smoothing technique we used in SGD-BB (Algorithm 3) can also be used in other variants of SGD. In this section, we use SAG as an example to illustrate how to incorporate the BB step size. SAG with BB step size (denoted as SAG-BB) is described as in Algorithm 4. Because SAG does not need diminishing step sizes to ensure convergence, in the smoothing technique we just choose \u03c6(k) \u2261 1. In this case, the smoothed step size \u03b7\u0303k is equal to the geometric mean of all previous BB step sizes.\nAlgorithm 4 SAG with BB step size (SAG-BB)\nParameters: update frequency m, initial step sizes \u03b70 and \u03b71 (only used in the first two epochs), weighting parameter \u03b2 \u2208 (0, 1), initial point x\u03030 yi = 0 for i = 1, . . . , n for k = 0, 1, \u00b7 \u00b7 \u00b7 do\nif k > 0 then \u03b7k = 1 m \u00b7 \u2016x\u0303k \u2212 x\u0303k\u22121\u2016 2 2/|(x\u0303k \u2212 x\u0303k\u22121)>(g\u0302k \u2212 g\u0302k\u22121)|\n\u03b7\u0303k = (\u220fk j=2 \u03b7j ) 1 k\u22121\n. smoothing technique\nend if x0 = x\u0303k g\u0302k+1 = 0 for t = 0, \u00b7 \u00b7 \u00b7 ,m\u2212 1 do\nRandomly pick it \u2208 {1, . . . , n} yit = \u2207fit(xt) xt+1 = xt \u2212 \u03b7kn \u2211n i=1 yi . SAG update\ng\u0302k+1 = \u03b2\u2207fit(xt) + (1\u2212 \u03b2)g\u0302k+1 end for x\u0303k+1 = xm\nend for"}, {"heading": "5 Numerical Experiments", "text": "In this section, we conduct some numerical experiments to demonstrate the efficacy of our SVRG-BB (Algorithm 2) and SGD-BB (Algorithm 3) algorithms. In particular, we apply SVRG-BB and SGD-BB to solve two standard testing problems in machine learning: logistic regression with `2-norm regularization\n(LR) min x\nF (x) = 1\nn n\u2211 i=1 log [ 1 + exp(\u2212bia>i x) ] + \u03bb 2 \u2016x\u201622, (5.1)\nand the squared hinge loss SVM with `2-norm regularization\n(SVM) min x\nF (x) = 1\nn n\u2211 i=1 ( [1\u2212 bia>i x]+ )2 + \u03bb 2 \u2016x\u201622, (5.2)\nwhere ai \u2208 Rd and bi \u2208 {\u00b11} are the feature vector and class label of the i-th sample, respectively, and \u03bb > 0 is a weighting parameter.\nWe tested SVRG-BB and SGD-BB for (5.1) and (5.2) for three standard real data sets, which were downloaded from the LIBSVM website1. Detailed information of these three data sets are given in Table 1."}, {"heading": "5.1 Numerical Results of SVRG-BB", "text": "In this section, we compare SVRG-BB (Algorithm 2) with SVRG (Algorithm 1) for solving (5.1) and (5.2). We used the best-tuned step size for SVRG, and three different initial step sizes \u03b70 for SVRG-BB. For both SVRG-BB and SVRG, we set m = 2n as suggested in [10].\nThe comparison results of SVRG-BB and SVRG are shown in Figure 1. In all the six subfigures, the x-axis denotes the number of epochs k, i.e., the number of outer loops in Algorithm 2. In Figures 1(a), 1(b) and 1(c), the y-axis denotes the sub-optimality F (x\u0303k) \u2212 F (x\u2217), and in Figures 1(d), 1(e) and 1(f), the y-axis denotes the step size \u03b7k. Note that x\n\u2217 is obtained by running SVRG with the best-tuned step size until it converges, which is a common practice in the testing of stochastic gradient descent methods. In all the six sub-figures, the dashed lines correspond to SVRG with fixed step sizes given in the legends of the figures. Moreover, the dashed lines in black color always represent SVRG with best-tuned fixed step size, and the green dashed lines use a smaller fixed step size, and the red dashed lines use a larger fixed step size, compared with the best-tuned ones. The solid lines correspond to SVRG-BB with different initial step sizes \u03b70. The solid lines with blue, purple and yellow colors in Figures 1(a) and 1(d) correspond to \u03b70 = 10, 1, and 0.1, respectively; the solid lines with blue, purple and yellow colors in Figures 1(b) and 1(e) correspond to \u03b70 = 1, 0.1, and 0.01, respectively; the solid lines with blue, purple and yellow colors in Figures 1(c) and 1(f) correspond to \u03b70 = 0.1, 0.01, and 0.001, respectively.\nIt can be seen from Figures 1(a), 1(b) and 1(c) that, SVRG-BB can always achieve the same level of sub-optimality as SVRG with the best-tuned step size. Although SVRG-BB needs slightly more epochs compared with SVRG with the best-tuned step size, it clearly outperforms SVRG with the other two choices of step sizes. Moreover, from Figures 1(d), 1(e) and 1(f) we see that the step sizes computed by SVRG-BB converge to the best-tuned step sizes after about\n1www.csie.ntu.edu.tw/~cjlin/libsvmtools/.\n10 to 15 epochs. From Figure 1 we also see that SVRG-BB is not sensitive to the choice of \u03b70. Therefore, SVRG-BB has very promising potential in practice because it generates the best step sizes automatically while running the algorithm."}, {"heading": "5.2 Numerical Results of SGD-BB", "text": "In this section, we compare SGD-BB with smoothing technique (Algorithm 3) with SGD for solving (5.1) and (5.2). We set m = n, \u03b2 = 10/m and \u03b71 = \u03b70 in our experiments. We used \u03c6(k) = k + 1 when applying the smoothing technique. Since SGD requires diminishing step size to converge, we tested SGD with diminishing step size in the form \u03b7/(k+ 1) with different constants \u03b7. The comparison results are shown in Figure 2. Similar as Figure 1, the dashed line with black color represents SGD with the best-tuned \u03b7, and the green and red dashed lines correspond to the other two choices of \u03b7. The solid lines with blue, purple and yellow colors in Figures 2(a) and 2(d) correspond to \u03b70 = 10, 1, and 0.1, respectively; the solid lines with blue, purple and yellow colors in Figures 2(b) and 2(e) correspond to \u03b70 = 1, 0.1, and 0.01, respectively; the solid lines with blue, purple and yellow colors in Figures 2(c) and 2(f) correspond to \u03b70 = 0.1, 0.01, and 0.001, respectively.\nFrom Figures 2(a), 2(b) and 2(c) we can see that SGD-BB gives comparable or even better sub-optimality than SGD with best-tuned diminishing step size, and SGD-BB is significantly better than SGD with the other two choices of step size. From Figures 2(d), 2(e) and 2(f) we see that after only a few epochs, the step sizes generated by SGD-BB approximately coincide with the best-tuned diminishing step sizes. It can also be seen that after only a few epochs, the step sizes are stabilized by the smoothing technique and they approximately follow the same decreasing trend as the best-tuned diminishing step sizes."}, {"heading": "5.3 Comparison with Other Methods", "text": "In this section, we compare our SGD-BB (Algorithm 3) and SAG-BB (Algorithm 4) with three existing methods: AdaGrad [8], SAG with line search (denoted as SAG-L) [22], and a stochastic quasi-Newton method: oLBFGS [23]. For both SGD-BB and SAG-BB, we set m = n and \u03b2 = 10/m. Because these methods have very different per-iteration complexity, we compare their CPU time needed to achieve the same sub-optimality.\nFigures 3(a), 3(b) and 3(c) show the comparison results of SGD-BB and AdaGrad. From these figures we see that AdaGrad usually has a very quick start, but in many cases the convergence becomes slow in later iterations. Besides, AdaGrad is still somewhat sensitive to the initial step sizes. Especially, when a small initial step size is used, AdaGrad is not able to increase the step size to a suitable level. As a contrast, SGD-BB converges very fast in all three tested problems, and it is not sensitive to the initial step size \u03b70.\nFigures 3(d), 3(e) and 3(f) show the comparison results of SAG-BB and SAG-L. From these figures we see that the SAG-L is quite robust and is not sensitive to the choice of \u03b70. However,\nSAG-BB is much faster than SAG-L to reach the same sub-optimality on the tested problems. Figures 3(g), 3(h) and 3(i) show the comparison results of SGD-BB and oLBFGS. For oLBFGS we used a best-tuned step size. From these figures we see that oLBFGS is much slower than SGD-BB, which is mainly because oLBFGS needs more computational effort per iteration."}, {"heading": "6 Conclusion", "text": "In this paper we proposed to use the BB method to compute the step sizes for SGD and SVRG, which leads to two new stochastic gradient methods: SGD-BB and SVRG-BB. We proved the linear convergence of SVRG-BB for strongly convex function, and as a by-product, we proved the linear convergence of the original SVRG with option I for strongly convex function. We also proposed a smoothing technique to stabilize the step sizes generated in SGD-BB, and we showed how to incorporate the BB method to other SGD variants such as SAG. We conducted numerical experiments on real data sets to compare the performance of SVRG-BB and SGD-BB with existing methods. The numerical results showed that the performance of our SVRG-BB and SGD-BB is comparable to and sometimes even better than the original SVRG and SGD with best-tuned step sizes, and is superior to some advanced SGD variants."}], "references": [{"title": "Variance reduction for faster non-convex optimization", "author": ["Z. Allen-Zhu", "E. Hazan"], "venue": "arXiv preprint arXiv:1603.05643,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Stop wasting my gradients: Practical SVRG", "author": ["R. Babanezhad", "M.O. Ahmed", "A. Virani", "M. Schmidt", "K. Kone\u010dn\u1ef3", "S. Sallinen"], "venue": "Advances in Neural Information Processing Systems, pages 2242\u20132250,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Two-point step size gradient methods", "author": ["J. Barzilai", "J.M. Borwein"], "venue": "IMA Journal of Numerical Analysis, 8(1):141\u2013148,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1988}, {"title": "A new analysis on the Barzilai-Borwein gradient method", "author": ["Y.-H. Dai"], "venue": "Journal of Operations Research Society of China, 1(2):187\u2013198,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Projected Barzilai-Borwein methods for large-scale boxconstrained quadratic programming", "author": ["Y.-H. Dai", "R. Fletcher"], "venue": "Numerische Mathematik, 100(1):21\u201347,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "The cyclic Barzilai-Borwein method for unconstrained optimization", "author": ["Y.-H. Dai", "W.W. Hager", "K. Schittkowski", "H. Zhang"], "venue": "IMA Journal of Numerical Analysis, 26(3):604\u2013 627,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["A. Defazio", "F. Bach", "S. Lacoste-Julien"], "venue": "Advances in Neural Information Processing Systems, pages 1646\u20131654,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "On the Barzilai-Borwein method", "author": ["R. Fletcher"], "venue": "Optimization and control with applications, pages 235\u2013256. Springer,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "Advances in Neural Information Processing Systems, pages 315\u2013323,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Accelerated stochastic approximation", "author": ["H. Kesten"], "venue": "The Annals of Mathematical Statistics, 29(1):41\u201359,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1958}, {"title": "Semi-stochastic gradient descent methods", "author": ["J. Kone\u010dn\u1ef3", "P. Richt\u00e1rik"], "venue": "arXiv preprint arXiv:1312.1666,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic line searches for stochastic optimization", "author": ["M. Mahsereci", "P. Hennig"], "venue": "arXiv preprint arXiv:1502.02846,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Speed learning on the fly", "author": ["P.Y. Mass\u00e9", "Y. Ollivier"], "venue": "arXiv preprint arXiv:1511.02540,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm", "author": ["D. Needell", "N. Srebro", "R. Ward"], "venue": "NIPS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Introductory lectures on convex optimization, volume 87", "author": ["Y. Nesterov"], "venue": "Springer Science & Business Media,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Stochastic proximal gradient descent with acceleration techniques", "author": ["A. Nitanda"], "venue": "Advances in Neural Information Processing Systems, pages 1574\u20131582,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM J. Control and Optimization, 30:838\u2013855,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1992}, {"title": "On the Barzilai and Borwein choice of steplength for the gradient method", "author": ["M. Raydan"], "venue": "IMA Journal of Numerical Analysis, 13(3):321\u2013326,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1993}, {"title": "The Barzilai and Borwein gradient method for the large scale unconstrained minimization problem", "author": ["M. Raydan"], "venue": "SIAM Journal on Optimization, 7(1):26\u201333,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Stochastic variance reduction for nonconvex optimization", "author": ["S.J. Reddi", "A. Hefny", "S. Sra", "B. Poczos", "A. Smola"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["R.L. Roux", "M. Schmidt", "F. Bach"], "venue": "Advances in Neural Information Processing Systems, pages 2663\u20132671,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "A stochastic quasi-newton method for online convex optimization", "author": ["N.N. Schraudolph", "J. Yu", "S. G\u00fcnter"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 436\u2013443,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Jornal of Machine Learning Research, 14:567\u2013599,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic gradient descent with Barzilai-Borwein update step for svm", "author": ["K. Sopy  la", "P. Drozda"], "venue": "Information Sciences,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Projected Barzilai-Borwein methods for large scale nonnegative image restorations", "author": ["Y. Wang", "S. Ma"], "venue": "Inverse Problems in Science and Engineering, 15(6):559\u2013583,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "A fast algorithm for sparse reconstruction based on shrinkage, subspace optimization, and continuation", "author": ["Z. Wen", "W. Yin", "D. Goldfarb", "Y. Zhang"], "venue": "SIAM J. SCI. COMPUT, 32(4):1832\u20131857,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "R.D. Nowak", "M.A.T. Figueiredo"], "venue": "IEEE Transactions on Signal Processing, 57(7):2479\u20132493,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["L. Xiao", "T. Zhang"], "venue": "SIAM Journal on Optimization, 24(4):2057\u20132075,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic optimization with importance sampling for regularized loss minimization", "author": ["P. Zhao", "T. Zhang"], "venue": "ICML,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "As a by-product, we prove the linear convergence result of SVRG with Option I proposed in [10], whose convergence result is missing in the literature.", "startOffset": 90, "endOffset": 94}, {"referenceID": 21, "context": "[22] updates the iterates by", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "It is shown in [22] that SAG converges linearly for strongly convex problems.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "[7] is an improved version of SAG, and it does not require the strong convexity assumption.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "The SDCA method proposed by Shalev-Shwartz and Zhang [24] also requires to store all the component gradients.", "startOffset": 53, "endOffset": 57}, {"referenceID": 9, "context": "The stochastic variance reduced gradient (SVRG) method proposed by Johnson and Zhang [10] is now widely used in the machine learning community for solving (1.", "startOffset": 85, "endOffset": 89}, {"referenceID": 21, "context": "[22], one important issue regarding to stochastic algorithms (SGD and its variants) that has not been fully addressed in the literature, is how to choose an appropriate step size \u03b7t while running the algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "AdaGrad [8] scales the gradient by the square root of the accumulated magnitudes of the gradients in the past iterations, but it still requires a fixed step size \u03b7.", "startOffset": 8, "endOffset": 11}, {"referenceID": 21, "context": "[22] suggests a line search technique on the component function fik(x) selected in each iteration, to estimate step size for SAG.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] suggests performing line search for an estimated function, which is evaluated by a Gaussian process with samples fit(xt).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] suggests to generate the step sizes by a given function with an unknown parameter, and to use the online SGD to update this unknown parameter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "As a byproduct, we show the linear convergence of SVRG with Option I (SVRG-I) proposed in [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "Note that in [10] only convergence of SVRG with Option II (SVRG-II) was given, and the proof for SVRG-I has been missing in the literature.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "However, SVRG-I is numerically a better choice than SVRG-II, as demonstrated in [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "The BB method, proposed by Barzilai and Borwein in [3], has been proved to be very successful in solving nonlinear optimization problems.", "startOffset": 51, "endOffset": 54}, {"referenceID": 18, "context": "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [19, 20, 9, 5, 6, 4] and references therein.", "startOffset": 108, "endOffset": 128}, {"referenceID": 19, "context": "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [19, 20, 9, 5, 6, 4] and references therein.", "startOffset": 108, "endOffset": 128}, {"referenceID": 8, "context": "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [19, 20, 9, 5, 6, 4] and references therein.", "startOffset": 108, "endOffset": 128}, {"referenceID": 4, "context": "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [19, 20, 9, 5, 6, 4] and references therein.", "startOffset": 108, "endOffset": 128}, {"referenceID": 5, "context": "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [19, 20, 9, 5, 6, 4] and references therein.", "startOffset": 108, "endOffset": 128}, {"referenceID": 3, "context": "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [19, 20, 9, 5, 6, 4] and references therein.", "startOffset": 108, "endOffset": 128}, {"referenceID": 27, "context": "Recently, BB method has been successfully applied for solving problems arising from emerging applications, such as compressed sensing [28], sparse reconstruction [27] and image processing [26].", "startOffset": 134, "endOffset": 138}, {"referenceID": 26, "context": "Recently, BB method has been successfully applied for solving problems arising from emerging applications, such as compressed sensing [28], sparse reconstruction [27] and image processing [26].", "startOffset": 162, "endOffset": 166}, {"referenceID": 25, "context": "Recently, BB method has been successfully applied for solving problems arising from emerging applications, such as compressed sensing [28], sparse reconstruction [27] and image processing [26].", "startOffset": 188, "endOffset": 192}, {"referenceID": 9, "context": "1 SVRG Method The SVRG method proposed by Johnson and Zhang [10] for solving (1.", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "This has been confirmed numerically in [10] where SVRG-I was applied to solve real applications.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": ", [10], [12] and [2]), and the convergence for SVRG-I has been missing in the literature.", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": ", [10], [12] and [2]), and the convergence for SVRG-I has been missing in the literature.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": ", [10], [12] and [2]), and the convergence for SVRG-I has been missing in the literature.", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "We now cite the convergence analysis of SVRG-II given in [10] as follows.", "startOffset": 57, "endOffset": 61}, {"referenceID": 9, "context": "2 ([10]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "Xiao and Zhang [29] developed a proximal SVRG method for minimizing the finite sum function plus a nonsmooth regularizer.", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "[17] applied Nesterov\u2019s acceleration technique to SVRG to improve the convergence rate that depends on the condition number L/\u03bc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] proved if the full gradient computation gk was replaced by a growing-batch estimation, the linear convergence rate can be preserved.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] and [21] showed that SVRG with minor modifications can converge to a stationary point for nonconvex optimization problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[1] and [21] showed that SVRG with minor modifications can converge to a stationary point for nonconvex optimization problems.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "The BB step size can also be naturally incorporated to other SVRG variants, such as SVRG with batching [2].", "startOffset": 103, "endOffset": 106}, {"referenceID": 15, "context": "The following lemma, which is from [16], is useful in our analysis.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "In SGD, \u2207fit(xt) is an unbiased estimation for \u2207F (xt) when it is uniformly sampled (see [15, 30] for studies on importance sampling, which does not sample it uniformly).", "startOffset": 89, "endOffset": 97}, {"referenceID": 29, "context": "In SGD, \u2207fit(xt) is an unbiased estimation for \u2207F (xt) when it is uniformly sampled (see [15, 30] for studies on importance sampling, which does not sample it uniformly).", "startOffset": 89, "endOffset": 97}, {"referenceID": 24, "context": "The recent work by Sopy la and Drozda [25] suggested several variants of this idea to compute an estimated BB step size using the stochastic gradients.", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "However, these ideas lack theoretical justifications and the numerical results in [25] show that these approaches are inferior to existing methods such as averaged SGD [18].", "startOffset": 82, "endOffset": 86}, {"referenceID": 17, "context": "However, these ideas lack theoretical justifications and the numerical results in [25] show that these approaches are inferior to existing methods such as averaged SGD [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 10, "context": "This kind of idea can be traced back to [11].", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "Inspired by [14], we propose the following smoothing technique to stabilize the step size.", "startOffset": 12, "endOffset": 16}, {"referenceID": 13, "context": "Similar as in [14], we assume the step sizes are in the form of C/\u03c6(k), where C > 0 is an unknown constant that needs to be estimated, \u03c6(k) is a pre-specified function that controls the decreasing rate of the step size, and a typical choice of function \u03c6 is \u03c6(k) = k + 1.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "For both SVRG-BB and SVRG, we set m = 2n as suggested in [10].", "startOffset": 57, "endOffset": 61}, {"referenceID": 7, "context": "3 Comparison with Other Methods In this section, we compare our SGD-BB (Algorithm 3) and SAG-BB (Algorithm 4) with three existing methods: AdaGrad [8], SAG with line search (denoted as SAG-L) [22], and a stochastic quasi-Newton method: oLBFGS [23].", "startOffset": 147, "endOffset": 150}, {"referenceID": 21, "context": "3 Comparison with Other Methods In this section, we compare our SGD-BB (Algorithm 3) and SAG-BB (Algorithm 4) with three existing methods: AdaGrad [8], SAG with line search (denoted as SAG-L) [22], and a stochastic quasi-Newton method: oLBFGS [23].", "startOffset": 192, "endOffset": 196}, {"referenceID": 22, "context": "3 Comparison with Other Methods In this section, we compare our SGD-BB (Algorithm 3) and SAG-BB (Algorithm 4) with three existing methods: AdaGrad [8], SAG with line search (denoted as SAG-L) [22], and a stochastic quasi-Newton method: oLBFGS [23].", "startOffset": 243, "endOffset": 247}], "year": 2016, "abstractText": "One of the major issues in stochastic gradient descent (SGD) methods is how to choose an appropriate step size while running the algorithm. Since the traditional line search technique does not apply for stochastic optimization algorithms, the common practice in SGD is either to use a diminishing step size, or to tune a fixed step size by hand, which can be time consuming in practice. In this paper, we propose to use the Barzilai-Borwein (BB) method to automatically compute step sizes for SGD and its variant: stochastic variance reduced gradient (SVRG) method, which leads to two algorithms: SGD-BB and SVRGBB. We prove that SVRG-BB converges linearly for strongly convex objective functions. As a by-product, we prove the linear convergence result of SVRG with Option I proposed in [10], whose convergence result is missing in the literature. Numerical experiments on standard data sets show that the performance of SGD-BB and SVRG-BB is comparable to and sometimes even better than SGD and SVRG with best-tuned step sizes, and is superior to some advanced SGD variants.", "creator": "LaTeX with hyperref package"}}}