{"id": "1605.04227", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2016", "title": "Relation Schema Induction using Tensor Factorization with Side Information", "abstract": "evaluating physical documentation like documents from a specific domain ( e. w., medical web journals ), how do we automatically trigger structured schema of objects i. br., type x operator attribute indicate relations ( e. ae., undergo ( ds, diagnosis ) ) - a necessary determining step towards building a knowledge tensor ( msc ) or of the hypothetical set of documents? we refer to ourselves problem as reverse schema engineering ( pts ). while classical information computation ( eva ) techniques aim at extracting fundamental level analyses of the processes ( cm, er, chemotherapy ), ourselves treat'a induce the wholly unknown schema of relations systematically. tensors provide precisely natural representation for such triples, and factorization of general relation delivers several plausible solution for the rsi problem. to reduce requirements of our knowledge, statistical integration methods have not been used for the rsi optimization. while fill his gap currently propose coupled non - objective tensor approach ( cntf ), a tensor factorization treatment which is able essentially get additional raw information just a desired way : more complete relation schema induction. we view interesting findings on one large - world fields : demonstrate cntf's supremacy over proof - of - the - future baselines differing across terms by flow and structure.", "histories": [["v1", "Thu, 12 May 2016 19:44:04 GMT  (323kb,D)", "https://arxiv.org/abs/1605.04227v1", null], ["v2", "Tue, 17 May 2016 04:57:09 GMT  (323kb,D)", "http://arxiv.org/abs/1605.04227v2", null], ["v3", "Wed, 16 Nov 2016 04:53:42 GMT  (286kb,D)", "http://arxiv.org/abs/1605.04227v3", "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, November 2016. Austin, TX"]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.DB", "authors": ["madhav nimishakavi", "uday singh saini", "partha p talukdar"], "accepted": true, "id": "1605.04227"}, "pdf": {"name": "1605.04227.pdf", "metadata": {"source": "CRF", "title": "Relation Schema Induction using Tensor Factorization with Side Information", "authors": ["Madhav Nimishakavi", "Uday Singh Saini"], "emails": ["madhav@csa.iisc.ernet.in", "uday.s.saini@gmail.com", "ppt@cds.iisc.ac.in"], "sections": [{"heading": "1 Introduction", "text": "Over the last few years, several techniques to build Knowledge Graphs (KGs) from large unstructured text corpus have been proposed, examples include NELL (Mitchell et al., 2015) and Google Knowledge Vault (Dong et al., 2014). Such KGs consist of millions of entities (e.g., Oslo, Norway, etc.),\ntheir types (e.g., isA(Oslo, City), isA(Norway, Country)), and relationships among them (e.g., cityLocatedInCountry(Oslo, Norway)). These KG construction techniques are called ontology-guided as they require as input list of relations, their schemas (i.e., their type signatures, e.g., cityLocatedInCountry(City, Country)), and seed instances of each such relation. Listing of such relations and their schemas are usually prepared by human domain experts.\nThe reliance on domain expertise poses significant challenges when such ontology-guided KG construction techniques are applied to domains where domain experts are either not available or are too expensive to employ. Even when such a domain expert may be available for a limited time, she may be able to provide only a partial listing of relations and their schemas relevant to that particular domain. Moreover, this expert-mediated model is not scalable when new data in the domain becomes available, bringing with it potential new relations of interest. In order to overcome these challenges, we need automatic techniques which can discover relations and their schemas from unstructured text data itself, without requiring extensive human input. We refer to this problem as Relation Schema Induction (RSI).\nIn contrast to ontology-guided KG construction techniques mentioned above, Open Information Extraction (OpenIE) techniques (Etzioni et al., 2011) aim to extract surface-level triples from unstructured text. Such OpenIE triples may provide a suitable starting point for the RSI problem. In fact, KB-LDA,\nar X\niv :1\n60 5.\n04 22\n7v 3\n[ cs\n.I R\n] 1\n6 N\na topic modeling-based method for inducing an ontology from SVO (Subject-Verb-Object) triples was recently proposed in (Movshovitz-Attias and Cohen, 2015). We note that ontology induction (Velardi et al., 2013) is a more general problem than RSI, as we are primarily interested in identifying categories and relations from a domain corpus, and not necessarily any hierarchy over them. Nonetheless, KB-LDA maybe used for the RSI problem and we use it as a representative of the state-of-the-art of this area.\nInstead of a topic modeling approach, we take a tensor factorization-based approach for RSI in this paper. Tensors are a higher order generalization of matrices and they provide a natural way to represent OpenIE triples. Applying tensor factorization methods over OpenIE triples to identify relation schemas is a natural approach, but one that has not been explored so far. Also, a tensor factorizationbased approach presents a flexible and principled way to incorporate various types of side information. Moreover, as we shall see in Section 4, compared to state-of-the-art baselines such as KB-LDA, tensor factorization-based approach results in better and faster solution for the RSI problem. In this paper, we make the following contributions:\n\u2022 We present Schema Induction using Coupled Tensor Factorization (SICTF), a novel and principled tensor factorization method which jointly factorizes a tensor constructed out of OpenIE triples extracted from a domain corpus, along with various types of additional side information for relation schema induction.\n\u2022 We compare SICTF against state-of-the-art baseline on various real-world datasets from diverse domains. We observe that SICTF is not only significantly more accurate than such\nbaselines, but also much faster. For example, SICTF achieves 14x speedup over KB-LDA (Movshovitz-Attias and Cohen, 2015).\n\u2022 We have made the data and code available 1."}, {"heading": "2 Related Work", "text": "Schema Induction: Properties of SICTF and other related methods are summarized in Table 12. A method for inducing (binary) relations and the categories they connect was proposed by (Mohamed et al., 2011). However, in that work, categories and their instances were known a-priori. In contrast, in case of SICTF, both categories and relations are to be induced. A method for event schema induction, the task of learning high-level representations of complex events and their entity roles from unlabeled text, was proposed in (Chambers, 2013). This gives the schemas of slots per event, but our goal is to find schemas of relations. (Chen et al., 2013) and (Chen et al., 2015) deal with the problem of finding semantic slots for unsupervised spoken language understanding, but we are interested in finding schemas of relations relevant for a given domain. Methods for link prediction in the Universal Schema setting using matrix and a combination of matrix and tensor factorization are proposed in (Riedel et al., 2013) and (Singh et al., 2015), respectively. Instead of link prediction where relation schemas are assumed to be given, SICTF focuses on discovering such relation schemas. Moreover, in contrast to such\n1https://github.com/malllabiisc/sictf 2Please note that not all methods mentioned in the table are directly comparable with SICTF, the table only illustrates the differences. KB-LDA is the only method which is directly comparable.\nmethods which assume access to existing KGs, the setting in this paper is unsupervised.\nTensor Factorization: Due to their flexibility of representation and effectiveness, tensor factorization methods have seen increased application in Knowledge Graph (KG) related problems over the last few years. Methods for decomposing ontological KGs such as YAGO (Suchanek et al., 2007) were proposed in (Nickel et al., 2012; Chang et al., 2014b; Chang et al., 2014a). In these cases, relation schemas are known in advance, while we are interested in inducing such relation schemas from unstructured text. A PARAFAC (Harshman, 1970) based method for jointly factorizing a matrix and tensor for data fusion was proposed in (Acar et al., 2013). In such cases, the matrix is used to provide auxiliary information (Narita et al., 2012; Erdos and Miettinen, 2013). Similar PARAFAC-based ideas are explored in Rubik (Wang et al., 2015) to factorize structured electronic health records. In contrast to such structured data sources, SICTF aims at inducing relation schemas from unstructured text data. Propstore, a tensor-based model for distributional semantics, a problem different from RSI, was presented in (Goyal et al., 2013). Even though coupled factorization of tensor and matrices constructed out of unstructured text corpus provide a natural and\nplausible approach for the RSI problem, they have not yet been explored \u2013 we fill this gap in this paper.\nOntology Induction: Relation Schema Induction can be considered a sub problem of Ontology Induction (Velardi et al., 2013). Instead of building a full-fledged hierarchy over categories and relations as in ontology induction, we are particularly interested in finding relations and their schemas from unstructured text corpus. We consider KB-LDA3 (Movshovitz-Attias and Cohen, 2015), a topic-modeling based approach for ontology induction, as a representative of this area. Among all prior work, KB-LDA is most related to SICTF. While both KB-LDA and SICTF make use of noun phrase side information, SICTF is also able to exploit relational side information in a principled manner. In Section 4, through experiments on multiple realworld datasets, we observe that SICTF is not only more accurate than KB-LDA but also significantly faster with a speedup of 14x.\nA method for canonicalizing noun and relation phrases in OpenIE triples was recently proposed in (Gala\u0301rraga et al., 2014). The main focus of this approach is to cluster lexical variants of a single entity or relation. This is not directly relevant for RSI, as\n3In this paper, whenever we refer to KB-LDA, we only refer to the part of it that learns relations from unstructured data.\nwe are interested in grouping multiple entities of the same type into one cluster, and use that to induce relation schema."}, {"heading": "3 Our Approach: Schema Induction using Coupled Tensor Factorization (SICTF)", "text": ""}, {"heading": "3.1 Overview", "text": "SICTF poses the relation schema induction problem as a coupled factorization of a tensor along with matrices containing relevant side information. Overall architecture of the SICTF system is presented in Figure 1. First, a tensor X \u2208 Rn\u00d7n\u00d7m+ is constructed to store OpenIE triples and their scores extracted from the text corpus4. Here, n and m represent the number of NPs and relation phrases, respectively. Following (Movshovitz-Attias and Cohen, 2015), SICTF makes use of noun phrase (NP) side information in the form of (noun phrase, hypernym). Additionally, SICTF also exploits relationrelation similarity side information. These two side information are stored in matrices W \u2208 {0, 1}n\u00d7h and S \u2208 {0, 1}m\u00d7m, where h is the number of hypernyms extracted from the corpus. SICTF then performs collective non-negative factorization over X , W , and S to output matrix A \u2208 Rn\u00d7c+ and the core tensor R \u2208 Rc\u00d7c\u00d7m+ . Each row in A corresponds to an NP, while each column corresponds to an induced category (latent factor). For brevity, we shall refer to the induced category corresponding to the qth column of A as Aq. Each entry Apq in the output matrix provides a membership score for NP p in induced category Aq. Please note that each induced category is represented using the NPs participating in it, with the NPs ranked by their membership scores in the induced category. In Figure 1, A2 = [(John, 0.9), (Sam, 0.8), . . .] is an induced category.\nEach slice of the core tensor R is a matrix which corresponds to a specific relation, e.g., the matrix Rundergo highlighted in Figure 1 corresponds to the relation undergo. Each cell in this matrix corresponds to an induced schema connecting two induced categories (two columns of the A matrix), with the cell value representing model\u2019s score of the induced schema. For example, in Figure 1, undergo(A2, A4) is an induced relation schema with\n4R+ is the set of non-negative reals.\nscore 0.8 involving relation undergo and induced categories A2 and A4.\nIn Section 3.2, we present details of the side information used by SICTF, and then in Section 3.3 present details of the optimization problem solved by SICTF."}, {"heading": "3.2 Side Information", "text": "\u2022 Noun Phrase Side Information: Through this type of side information, we would like to capture type information of as many noun phrases (NPs) as possible. We apply Hearst patterns (Hearst, 1992), e.g., \u201d<Hypernym> such as <NP>\u201d, over the corpus to extract such (NP, Hypernym) pairs. Please note that neither hypernyms nor NPs are pre-specified, and they are all extracted from the data by the patterns. Examples of a few such pairs extracted from two different datasets are shown in Table 2. These extracted tuples are stored in a matrix Wn\u00d7h whose rows correspond to NPs and columns correspond to extracted hypernyms. We define,\nWij = { 1, if NPi belongs to Hypernymj 0, otherwise .\nPlease note that we don\u2019t expectW to be a fully specified matrix, i.e., we don\u2019t assume that we know all possible hypernyms for a given NP.\n\u2022 Relation Side Information: In addition to the side information involving NPs, we would also\nlike to take prior knowledge about textual relations into account during factorization. For example, if we know two relations to be similar to one another, then we also expect their induced schemas to be similar as well. Consider the following sentences \u201dMary purchased a stuffed animal toy.\u201c and \u201dJanet bought a toy car for her son.\u201d. From these we can say that both relations purchase and buy have the schema (Person, Item). Even if one of these relations is more abundant than the other in the corpus, we still want to learn similar schemata for both the relations. As mentioned before, S \u2208 Rm\u00d7m+ is the relation similarity matrix, where m is the number of textual relations. We define,\nSij = { 1, if Similarity(Reli, Relj) \u2265 \u03b3 0, otherwise\nwhere \u03b3 is a threshold5. For the experiments in this paper, we use cosine similarity over word2vec (Mikolov et al., 2013) vector representations of the relational phrases. Examples of a few similar relation pairs are shown in Table 3."}, {"heading": "3.3 SICTF Model Details", "text": "SICTF performs coupled non-negative factorization of the input triple tensor Xn\u00d7n\u00d7m along with the two side information matrices Wn\u00d7h and Sm\u00d7m by solving the following optimization problem.\nmin A,V,R m\u2211 k=1 f(Xk, A,Rk) + fnp(W,A, V ) + frel(S,R)\n(1) where,\nf(Xk, A,Rk) =\u2016 X:,:,k \u2212AR:,:,kAT \u20162F +\u03bbR \u2016 R:,:,k \u20162F fnp(W,A, V ) = \u03bbnp \u2016W \u2212AV \u20162F +\u03bbA \u2016 A \u20162F\n+ \u03bbV \u2016 V \u20162F\nfrel(S,R) = \u03bbrel m\u2211 i=1 m\u2211 j=1 Sij \u2016 R:,:,i \u2212R:,:,j \u20162F\nAi,j \u2265 0,Vj,r \u2265 0, Rp,q,k \u2265 0 (non negative) \u2200 1 \u2264 i \u2264 n, 1 \u2264 r \u2264 h, 1 \u2264 j, p, q \u2264 c, 1 \u2264 k \u2264 m\n5For the experiments in this paper, we set \u03b3 = 0.7, a relatively high value, to focus on highly similar relations and thereby justifying the binary S matrix.\nIn the objective above, the first term f(Xk, A,Rk) minimizes reconstruction error for the kth relation, with additional regularization on the R:,:,k matrix6. The second term, fnp(W,A, V ), factorizes the NP side information matrix Wn\u00d7h into two matrices An\u00d7c and Vc\u00d7h, where c is the number of induced categories. We also enforce A to be non-negative. Typically, we require c h to get a lower dimensional embedding of each NP (rows of A). Finally, the third term frel(S,R) enforces the requirement that two similar relations as given by the matrix S should have similar signatures (given by the corresponding R matrix). Additionally, we require V and R to be non-negative, as marked by the (nonnegative) constraints. In this objective, \u03bbR, \u03bbnp, \u03bbA, \u03bbV , and \u03bbrel are all hyper-parameters.\nWe derive non-negative multiplicative updates for A, Rk and V following the rules proposed in (Lee and Seung, 2000), which has the following general form:\n\u03b8i = \u03b8i  \u2202C(\u03b8)\u2212\u2202\u03b8i \u2202C(\u03b8)+\n\u2202\u03b8i \u03b1 HereC(\u03b8) represents the cost function of the non-\nnegative variables \u03b8 and \u2202C(\u03b8) \u2212\n\u2202\u03b8i and \u2202C(\u03b8)\n\u2212\n\u2202\u03b8i are the\nnegative and positive parts of the derivative of C(\u03b8) (M\u00f8rup et al., 2008). (Lee and Seung, 2000) proved that for \u03b1 = 1, the cost functionC(\u03b8) monotonically decreases with the multiplicative updates 7. C(\u03b8) for SICTF is given in equation (1). The above procedure will give the following updates:\nA \u2190 A \u2217\n\u2211 k (XkAR T k +X T k ARk) + \u03bbnpWV T\nA(B\u0303 + \u03bbAI + \u03bbnpV V T ) B\u0303 = \u2211 k (RkA TARTk +R T kA TARk)\nRk \u2190 Rk \u2217 ATXkA+ 2 \u03bbrel\nm\u2211 j=1 RjSkj\nATARkATA+ D\u0303\nD\u0303 = 2 \u03bbrel Rk m\u2211 j=1 Skj + \u03bbRRk\nV \u2190 V \u2217 \u03bbnpA TW\n\u03bbnpATAV + \u03bbV V\n6For brevity, we also refer to R:,:,k as Rk, and similarly X:,:,k as Xk\n7We also use \u03b1 = 1.\nIn the equations above, \u2217 is the Hadamard or element-wise product8. In all our experiments, we find the iterative updates above to converge in about 10-20 iterations."}, {"heading": "4 Experiments", "text": "In this section, we evaluate performance of different methods on the Relation Schema Induction (RSI) task. Specifically, we address the following questions.\n\u2022 Which method is most effective on the RSI task? (Section 4.3.1)\n\u2022 How important are the additional side information for RSI? (Section 4.3.2)\n\u2022 What is the importance of non-negativity in RSI with tensor factorization? (Section 4.3.3)"}, {"heading": "4.1 Experimental Setup", "text": "Datasets: We used two datasets for the experiments in this paper, they are summarized in Table 4. For MEDLINE dataset, we used Stanford CoreNLP (Manning et al., 2014) for coreference resolution and Open IE v4.09 for triple extraction. Triples with Noun Phrases that have Hypernym information were retained. We obtained the StackOverflow triples directly from the authors of (Movshovitz-Attias and Cohen, 2015), which were also prepared using a very similar process. In both datasets, we use corpus frequency of triples for constructing the tensor.\nSide Information: Seven Hearst patterns such as \u201d<hypernym> such as <NP>\u201d, \u201d<NP> or other <hypernym>\u201d etc., given in (Hearst, 1992) were used to extract NP side information from the MEDLINE documents. NP side information for the StackOverflow dataset was obtained from the authors of (Movshovitz-Attias and Cohen, 2015).\nAs described in Section 3, word2vec embeddings of the relation phrases were used to extract relationsimilarity based side-information. This was done for\n8(A \u2217B)i,j = Ai,j \u00d7Bi,j 9Open IE v4.0: http://knowitall.github.io/openie/\nboth datasets. Cosine similarity threshold of \u03b3 = 0.7 was used for the experiments in the paper.\nSamples of side information used in the experiments are shown in Table 2 and Table 3. A total of 2067 unique NP-hypernym pairs were extracted from MEDLINE data and 16,639 were from StackOverflow data. 25 unique pairs of relation phrases out of 1172 were found to be similar in MEDLINE data, whereas 280 unique pairs of relation phrases out of approximately 3200 were found similar in StackOverflow data.\nHyperparameters were tuned using grid search and the set which gives minimum reconstruction error for both X and W was chosen. We set \u03bbnp = \u03bbrel = 100 for StackOverflow, and \u03bbnp = 0.05 and \u03bbrel = 0.001 for Medline and we use c = 50 for our experiments. Please note that our setting is unsupervised, and hence there is no separate train, dev and test sets."}, {"heading": "4.2 Evaluation Protocol", "text": "In this section, we shall describe how the induced schemas are presented to human annotators and how final accuracies are calculated. In factorizations produced by SICTF and other ablated versions of SICTF, we first select a few top relations with best reconstruction score. The schemas induced for each selected relation k is represented by the matrix slice Rk of the core tensor obtained after factorization (see Section 3). From each such matrix, we identify the indices (i, j) with highest values. The indices i and j select columns of the matrix A. A few top ranking NPs from the columns Ai and Aj along with the relation k are presented to the human annotator, who then evaluates whether the tuple Relationk(Ai, Aj) constitutes a valid schema for relation k. Examples of a few relation schemas induced by SICTF are presented in Table 5. A human annotator would see the first and second columns of this table and then offer judgment as indicated in the third column of the table. All such judgments across all top-reconstructed relations are aggregated to get the final accuracy score. This evaluation protocol was also used in (Movshovitz-Attias and Cohen, 2015) to measure learned relation accuracy.\nAll evaluations were blind, i.e., the annotators were not aware of the method that generated the output they were evaluating. Moreover, the anno-\ntators are experts in software domain and has highschool level knowledge in medical domain. Though recall is a desirable statistic to measure, it is very challenging to calculate it in our setting due to the non-availability of relation schema annotated text on large scale."}, {"heading": "4.3 Results", "text": ""}, {"heading": "4.3.1 Effectiveness of SICTF", "text": "Experimental results comparing performance of various methods on the RSI task in the two datasets are presented in Figure 2(a). RSI accuracy is calculated based on the evaluation protocol described in Section 4.2. Performance number of KB-LDA for StackOveflow dataset is taken directly from the (Movshovitz-Attias and Cohen, 2015) paper, we used our implementation of KB-LDA for the MEDLINE dataset. Annotation accuracies from two annotators were averaged to get the final accuracy.\nFrom Figure 2(a), we observe that SICTF outperforms KB-LDA on the RSI task. Please note that the inter-annotator agreement for SICTF is 88% and 97% for MEDLINE and StackOverflow datasets respectively. This is the main result of the paper.\nIn addition to KB-LDA, we also compared SICTF with PARAFAC, a standard tensor factorization method. PARAFAC induced extremely poor and small number of relation schemas, and hence we didn\u2019t consider it any further.\nRuntime comparison: Runtimes of SICTF and KB-LDA over both datasets are compared in Figure 2(b). From this figure, we find that SICTF is able to achieve a 14x speedup on average over KBLDA10. In other words, SICTF is not only able to\n10Runtime of KB-LDA over the StackOverflow dataset was obtained from the authors of (Movshovitz-Attias and Cohen, 2015) through personal communication. Our own implementation also resulted in similar runtime over this dataset.\ninduce better relation schemas, but also do so at a significantly faster speed."}, {"heading": "4.3.2 Importance of Side Information", "text": "One of the central hypothesis of our approach is that coupled factorization through additional side information should result in better relation schema induction. In order to evaluate this thesis further, we compare performance of SICTF with its ablated versions: (1) SICTF (\u03bbrel = 0), which corresponds to the setting when no relation side information is used, (2) SICTF (\u03bbnp = 0), which corresponds to the setting when no noun phrases side information is used, and (3) SICTF (\u03bbrel = 0, \u03bbnp = 0), which corresponds to the setting when no side information of any kind is used. Hyperparameters are separately tuned for the variants of SICTF. Results are presented in the first four rows of Table 6. From this, we observe that additional coupling through the side information significantly helps improve SICTF performance. This further validates the central thesis of our paper."}, {"heading": "4.3.3 Importance of Non-Negativity on Relation Schema Induction", "text": "In the last row of Table 6, we also present an ablated version of SICTF when no side information no non-negativity constraints are used. Comparing the last two rows of this table, we observe that non-negativity constraints over the A matrix and core tensor R result in significant improvement in performance. We note that the last row in Table 6 is equivalent to RESCAL (Nickel et al., 2011) and the fourth row is equivalent to Non-Negative RESCAL (Krompa\u00df et al., 2013), two tensor factor-\nization techniques. We also note that none of these tensor factorization techniques have been previously used for the relation schema induction problem.\nThe reason for this improved performance may be explained by the fact that absence of non-negativity constraint results in an under constrained factorization problem where the model often overgenerates incorrect triples, and then compensates for this overgeneration by using negative latent factor weights. In contrast, imposition of non-negativity constraints restricts the model further forcing it to commit to specific semantics of the latent factors in A. This improved interpretability also results in better RSI accuracy as we have seen above. Similar benefits of non-negativity on interpretability have also been observed in matrix factorization (Murphy et al., 2012)."}, {"heading": "5 Conclusion", "text": "Relation Schema Induction (RSI) is an important first step towards building a Knowledge Graph (KG) out of text corpus from a given domain. While human domain experts have traditionally prepared listing of relations and their schemas, this expert-mediated model poses significant challenges in terms of scalability and coverage. In order to overcome these challenges, in this paper, we present SICTF, a novel non-negative coupled tensor matrix factorization method for relation schema induction. SICTF is flexible enough to incorporate various types of side information during factorization. Through extensive experiments on realworld datasets, we find that SICTF is not only more accurate but also significantly faster (about 11.8x speedup) compared to state-of-the-art baselines. As part of future work, we hope to analyze CNTF and\nits optimization further, assign labels to induced categories, and also apply the model to more domains. We hope to make all code and datasets used in the paper publicly available upon publication of the paper."}, {"heading": "Acknowledgement", "text": "Thanks to the members of MALL Lab, IISc who read our drafts and gave valuable feedback and we also thank the reviewers for their constructive reviews. This research has been supported in part by Bosch Engineering and Business Solutions and Google."}], "references": [{"title": "Understanding data fusion within the framework of coupled matrix and tensor factorizations", "author": ["Acar et al.2013] Evrim Acar", "Morten Arendt Rasmussen", "Francesco Savorani", "Tormod Ns", "Rasmus Bro"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "Acar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Acar et al\\.", "year": 2013}, {"title": "Event schema induction with a probabilistic entity-driven model", "author": ["Nathanael Chambers"], "venue": "In EMNLP,", "citeRegEx": "Chambers.,? \\Q2013\\E", "shortCiteRegEx": "Chambers.", "year": 2013}, {"title": "Typed tensor decomposition of knowledge bases for relation extraction", "author": ["Chang et al.2014a] Kai-Wei Chang", "Wen tau Yih", "Bishan Yang", "Christopher Meek"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Chang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2014}, {"title": "Typed tensor decomposition of knowledge bases for relation extraction", "author": ["Chang et al.2014b] Kai-Wei Chang", "Wen-tau Yih", "Bishan Yang", "Christopher Meek"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Chang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2014}, {"title": "Unsupervised induction and filling of semantic slots for spoken dialogue systems using frame-semantic parsing", "author": ["Chen et al.2013] Yun-Nung Chen", "William Y. Wang", "Alexander I. Rudnicky"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Matrix factorization with knowledge graph propagation for unsupervised spoken language understanding", "author": ["Chen et al.2015] Yun-Nung Chen", "William Yang Wang", "Anatole Gershman", "Alexander I. Rudnicky"], "venue": "ACL", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["Dong et al.2014] Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Discovering facts with boolean tensor tucker decomposition", "author": ["Erdos", "Miettinen2013] Dora Erdos", "Pauli Miettinen"], "venue": "In Proceedings of the 22Nd ACM International Conference on Information & Knowledge Management,", "citeRegEx": "Erdos et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Erdos et al\\.", "year": 2013}, {"title": "Open information extraction: The second generation", "author": ["Etzioni et al.2011] Oren Etzioni", "Anthony Fader", "Janara Christensen", "Stephen Soderland", "Mausam Mausam"], "venue": "In IJCAI,", "citeRegEx": "Etzioni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Etzioni et al\\.", "year": 2011}, {"title": "A structured distributional semantic model: Integrating structure with semantics", "author": ["Goyal et al.2013] Kartik Goyal", "Sujay Kumar", "Jauhar Huiying", "Li Mrinmaya", "Sachan Shashank", "Srivastava Eduard Hovy"], "venue": null, "citeRegEx": "Goyal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goyal et al\\.", "year": 2013}, {"title": "Foundations of the PARAFAC procedure: Models and conditions for an", "author": ["R.A. Harshman"], "venue": "explanatory\u201d multi-modal factor analysis. UCLA Working Papers in Phonetics,", "citeRegEx": "Harshman.,? \\Q1970\\E", "shortCiteRegEx": "Harshman.", "year": 1970}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["Marti A. Hearst"], "venue": "Proceedings of the 14th International Conference on Computational Linguistics,", "citeRegEx": "Hearst.,? \\Q1992\\E", "shortCiteRegEx": "Hearst.", "year": 1992}, {"title": "Non-negative tensor factorization with rescal. Tensor Methods for Machine Learning, ECML workshop", "author": ["Maximilian Nickel", "Xueyan Jiang", "Volker Tresp"], "venue": null, "citeRegEx": "Krompa\u00df et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krompa\u00df et al\\.", "year": 2013}, {"title": "Algorithms for non-negative matrix factorization", "author": ["Lee", "Seung2000] Daniel D. Lee", "H. Sebastian Seung"], "venue": "In In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2000}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Associa-", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Discovering relations between noun categories", "author": ["Estevam R. Hruschka", "Jr.", "Tom M. Mitchell"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Mohamed et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2011}, {"title": "Algorithms for sparse non-negative TUCKER", "author": ["M\u00f8rup et al.2008] M. M\u00f8rup", "L.K. Hansen", "S.M. Arnfred"], "venue": "Neural Computation,", "citeRegEx": "M\u00f8rup et al\\.,? \\Q2008\\E", "shortCiteRegEx": "M\u00f8rup et al\\.", "year": 2008}, {"title": "Kb-lda: Jointly learning a knowledge base of hierarchy, relations, and facts", "author": ["Movshovitz-Attias", "Cohen2015] Dana MovshovitzAttias", "William W. Cohen"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Movshovitz.Attias et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Movshovitz.Attias et al\\.", "year": 2015}, {"title": "Learning effective and interpretable semantic models using non-negative sparse embedding", "author": ["Murphy et al.2012] Brian Murphy", "Partha Pratim Talukdar", "Tom M Mitchell"], "venue": "In COLING,", "citeRegEx": "Murphy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Murphy et al\\.", "year": 2012}, {"title": "Tensor factorization using auxiliary information. Data Mining and Knowledge Discovery, 25(2):298\u2013324", "author": ["Kohei Hayashi", "Ryota Tomioka", "Hisashi Kashima"], "venue": null, "citeRegEx": "Narita et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Narita et al\\.", "year": 2012}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "Proceedings of the 28th International Conference on Machine Learn-", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Factorizing yago: Scalable machine learning for linked data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 21st International Conference on World Wide Web, WWW", "citeRegEx": "Nickel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": "In Human Language Technologies: Conference of the North American Chapter of the Association", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Towards Combined Matrix and Tensor Factorization for Universal Schema Relation Extraction", "author": ["Singh et al.2015] Sameer Singh", "Tim Rockt\u00e4schel", "Sebastian Riedel"], "venue": "In NAACL Workshop on Vector Space Modeling for NLP (VSM)", "citeRegEx": "Singh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2015}, {"title": "Yago: a core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of WWW", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Ontolearn reloaded: A graphbased algorithm for taxonomy induction", "author": ["Stefano Faralli", "Roberto Navigli"], "venue": null, "citeRegEx": "Velardi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Velardi et al\\.", "year": 2013}, {"title": "Rubik: Knowledge guided tensor factorization and completion for health data analytics", "author": ["Wang et al.2015] Yichen Wang", "Robert Chen", "Joydeep Ghosh", "Joshua C. Denny", "Abel N. Kho", "You Chen", "Bradley A. Malin", "Jimeng Sun"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": ", 2015) and Google Knowledge Vault (Dong et al., 2014).", "startOffset": 35, "endOffset": 54}, {"referenceID": 8, "context": "In contrast to ontology-guided KG construction techniques mentioned above, Open Information Extraction (OpenIE) techniques (Etzioni et al., 2011) aim to extract surface-level triples from unstructured text.", "startOffset": 123, "endOffset": 145}, {"referenceID": 24, "context": "Universal Schema (Singh et al., 2015) Link Prediction No No No No", "startOffset": 17, "endOffset": 37}, {"referenceID": 26, "context": "We note that ontology induction (Velardi et al., 2013) is a more general problem than RSI, as we are primarily interested in identifying categories and relations from a domain corpus, and not necessarily any hierarchy over them.", "startOffset": 32, "endOffset": 54}, {"referenceID": 16, "context": "A method for inducing (binary) relations and the categories they connect was proposed by (Mohamed et al., 2011).", "startOffset": 89, "endOffset": 111}, {"referenceID": 1, "context": "A method for event schema induction, the task of learning high-level representations of complex events and their entity roles from unlabeled text, was proposed in (Chambers, 2013).", "startOffset": 163, "endOffset": 179}, {"referenceID": 4, "context": "(Chen et al., 2013) and (Chen et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": ", 2013) and (Chen et al., 2015) deal with the problem of finding semantic slots for unsupervised spoken language understanding, but we are interested in finding schemas of relations relevant for a given domain.", "startOffset": 12, "endOffset": 31}, {"referenceID": 23, "context": "Methods for link prediction in the Universal Schema setting using matrix and a combination of matrix and tensor factorization are proposed in (Riedel et al., 2013) and (Singh et al.", "startOffset": 142, "endOffset": 163}, {"referenceID": 24, "context": ", 2013) and (Singh et al., 2015), respectively.", "startOffset": 12, "endOffset": 32}, {"referenceID": 25, "context": "Methods for decomposing ontological KGs such as YAGO (Suchanek et al., 2007) were proposed in (Nickel et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 22, "context": ", 2007) were proposed in (Nickel et al., 2012; Chang et al., 2014b; Chang et al., 2014a).", "startOffset": 25, "endOffset": 88}, {"referenceID": 10, "context": "A PARAFAC (Harshman, 1970) based method for jointly factorizing a matrix and tensor for data fusion was proposed in (Acar et al.", "startOffset": 10, "endOffset": 26}, {"referenceID": 0, "context": "A PARAFAC (Harshman, 1970) based method for jointly factorizing a matrix and tensor for data fusion was proposed in (Acar et al., 2013).", "startOffset": 116, "endOffset": 135}, {"referenceID": 20, "context": "In such cases, the matrix is used to provide auxiliary information (Narita et al., 2012; Erdos and Miettinen, 2013).", "startOffset": 67, "endOffset": 115}, {"referenceID": 27, "context": "Similar PARAFAC-based ideas are explored in Rubik (Wang et al., 2015) to factorize structured electronic health records.", "startOffset": 50, "endOffset": 69}, {"referenceID": 9, "context": "Propstore, a tensor-based model for distributional semantics, a problem different from RSI, was presented in (Goyal et al., 2013).", "startOffset": 109, "endOffset": 129}, {"referenceID": 26, "context": "Ontology Induction: Relation Schema Induction can be considered a sub problem of Ontology Induction (Velardi et al., 2013).", "startOffset": 100, "endOffset": 122}, {"referenceID": 11, "context": "We apply Hearst patterns (Hearst, 1992), e.", "startOffset": 25, "endOffset": 39}, {"referenceID": 15, "context": "For the experiments in this paper, we use cosine similarity over word2vec (Mikolov et al., 2013) vector representations of the relational phrases.", "startOffset": 74, "endOffset": 96}, {"referenceID": 17, "context": "HereC(\u03b8) represents the cost function of the nonnegative variables \u03b8 and \u2202C(\u03b8) \u2212 \u2202\u03b8i and \u2202C(\u03b8) \u2212 \u2202\u03b8i are the negative and positive parts of the derivative of C(\u03b8) (M\u00f8rup et al., 2008).", "startOffset": 163, "endOffset": 183}, {"referenceID": 14, "context": "For MEDLINE dataset, we used Stanford CoreNLP (Manning et al., 2014) for coreference resolution and Open IE v4.", "startOffset": 46, "endOffset": 68}, {"referenceID": 11, "context": ", given in (Hearst, 1992) were used to extract NP side information from the MEDLINE documents.", "startOffset": 11, "endOffset": 25}, {"referenceID": 21, "context": "We note that the last row in Table 6 is equivalent to RESCAL (Nickel et al., 2011) and the fourth row is equivalent to Non-Negative RESCAL (Krompa\u00df et al.", "startOffset": 61, "endOffset": 82}, {"referenceID": 12, "context": ", 2011) and the fourth row is equivalent to Non-Negative RESCAL (Krompa\u00df et al., 2013), two tensor factorization techniques.", "startOffset": 64, "endOffset": 86}, {"referenceID": 19, "context": "Similar benefits of non-negativity on interpretability have also been observed in matrix factorization (Murphy et al., 2012).", "startOffset": 103, "endOffset": 124}], "year": 2016, "abstractText": "Given a set of documents from a specific domain (e.g., medical research journals), how do we automatically build a Knowledge Graph (KG) for that domain? Automatic identification of relations and their schemas, i.e., type signature of arguments of relations (e.g., undergo(Patient, Surgery)), is an important first step towards this goal. We refer to this problem as Relation Schema Induction (RSI). In this paper, we propose Schema Induction using Coupled Tensor Factorization (SICTF), a novel tensor factorization method for relation schema induction. SICTF factorizes Open Information Extraction (OpenIE) triples extracted from a domain corpus along with additional side information in a principled way to induce relation schemas. To the best of our knowledge, this is the first application of tensor factorization for the RSI problem. Through extensive experiments on multiple real-world datasets, we find that SICTF is not only more accurate than state-of-the-art baselines, but also significantly faster (about 14x faster).", "creator": "LaTeX with hyperref package"}}}