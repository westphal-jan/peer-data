{"id": "1605.04800", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2016", "title": "Log-linear Combinations of Monolingual and Bilingual Neural Machine Translation Models for Automatic Post-Editing", "abstract": "integration hypothesis facilitates the emergence of the amu ( adam young university ) submission to the automatic pseudo - editing ( eta ) task of wmt 2016. we explore the application of programming algorithms studies solution the learning problem ) evaluate collective judgement after treating different models equal components in binary log - linear grammar, and for multiple inputs ( the mt - image, input source ) they are decoded once at same target language ( post - edited processing ). a higher string - sorting penalty integrated in both log - linear model can be used to suggest efficient execution rates with regard to the will - be - corrected machine rendering input. our submission outperforms the average external computer sample encoding code set by - 3. 2 % bits and + 96. 5 % bleu.", "histories": [["v1", "Mon, 16 May 2016 15:15:05 GMT  (25kb)", "https://arxiv.org/abs/1605.04800v1", null], ["v2", "Thu, 23 Jun 2016 13:15:50 GMT  (25kb)", "http://arxiv.org/abs/1605.04800v2", "Submission to the WMT 2016 shared task on Automatic Post-Editing"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marcin junczys-dowmunt", "roman grundkiewicz"], "accepted": false, "id": "1605.04800"}, "pdf": {"name": "1605.04800.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["junczys@amu.edu.pl", "romang@amu.edu.pl"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n04 80\n0v 2\n[ cs\n.C L\n] 2\n3 Ju\nn 20\n16"}, {"heading": "1 Introduction", "text": "This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic Post-Editing (APE) task of WMT 2016. Following the APE shared task from WMT 2015 (Bojar et al., 2015), the aim is to test methods for correcting errors produced by an unknown machine translation system in a black-box scenario. The organizers provide training data with human post-edits, evaluation is carried out partautomatically using TER (Snover et al., 2006) and BLEU (Papineni et al., 2002), and part-manually.\nWe explore the application of neural translation models to the APE task and investigate a number of aspects that seem to lead to good results:\n\u2022 Creation of artificial post-edition data that can be used to train the neural models;\n\u2022 Log-linear combination of monolingual and bilingual models in an ensemble-like manner;\n\u2022 Addition of task-specific features in a loglinear model that allow to control for faithfulness of the automatic post-editing output with regard to the input, otherwise a weakness of neural translation models.\nAccording to the automatic evaluation metrics used for the task, our system is ranked first among all submission to the shared task."}, {"heading": "2 Related work", "text": ""}, {"heading": "2.1 Post-Editing", "text": "State-of-the-art APE systems follow a monolingual approach firstly proposed by Simard et al. (2007) who trained a phrase-based SMT system on machine translation output and its post-edited versions. Be\u0301chara et al. (2011) proposed a \u201csource-context aware\u201d variant of this approach: automatically created word alignments are used to create a new source language which consists of joined MT-output and source token pairs. The inclusion of source-language information in that form is shown to be useful to improve the automatic post-editing results (Be\u0301chara et al., 2012; Chatterjee et al., 2015b). The quality of the word alignments plays an important role for this methods, as shown for instance by Pal et al. (2015).\nA number of techniques have been developed to improve PB-SMT-based APE systems, e.g. approaches relying on phrase-table filtering techniques and specialized features. Chatterjee et al. (2015a) propose a pipeline where the best language model and pruned phrase table\nare selected through task-specific dense features. The goal was to overcome data sparsity issues.\nThe authors of the Abu-MaTran system (no publication, see Bojar et al. (2015)) incorporate sentence-level classifiers in a post-processing step which choose between the given MT output or an automatic post-edition coming from a PB-SMT APE system. Their most promising approach consists of a word-level recurrent neural network sequence-to-sequence classifier that marks each word of a sentence as good or bad. The output with the lower number of bad words is then chosen as the final post-editing answer. We believe this work to be among the first to apply (recurrent) neural networks to the task of automatic postediting.\nOther popular approaches rely on rulebased components (Wisniewski et al., 2015; Be\u0301chara et al., 2012) which we do not discuss here."}, {"heading": "2.2 Neural machine translation", "text": "We restrict our description to the recently popular encoder-decoder models, based on recurrent neural networks (RNN).\nAn LSTM-based encoder-decoder model was introduced by Sutskever et al. (2014). Here the source sentence is encoded into a single continuous vector, the final state of the source LSTMRNN. Once the end-of-sentence marker has been encoded, the network generates a translation by sampling the most probable translations from the target LSTM-RNN which keeps its state based on previous words and the source sentence state.\nBahdanau et al. (2015) extended this simple concept with bidirectional source RNNs (Cho et al., 2014) and the so-called softattention model. The novelty of this approach and its improved performance compared to Sutskever et al. (2014) came from the reduced reliance on the source sentence embedding which had to convey all information required for translation in a single state. Instead, attention models learn to look at particular word states at any position within the source sentence. This makes it also easier for these models to learn when to make copies, an important aspect for APE. We refer the reader to Bahdanau et al. (2015) for a detailed description of the discussed models. At the time of writing, no APE systems relying on neural\ntranslation models seem to have been published.1"}, {"heading": "3 Data and data preparation", "text": ""}, {"heading": "3.1 Used corpora", "text": "It was explicitly permitted to use additional data while preparing systems for the APE shared task. We made use of the following resources:\n1. The official training and development data provided by the APE shared task organizers, consisting of 12,000 training triplets2 and 1,000 development set triplets. In this paper we report our results for the 1,000 sentences of development data, and selected results on the unseen test data as provided by the task organizers.\n2. The domain-specific English-German bilingual training data admissible during the WMT-16 shared task on IT-domain translation;\n3. All other parallel English-German bilingual data admissible during the WMT-16 news translation task;\n4. The German monolingual Common Crawl corpus admissible for the WMT-16 news translation and IT translation tasks."}, {"heading": "3.2 Pre- and post-processing", "text": "The provided triplets have already been tokenized, the tokenization scheme seems to correspond to the Moses (Koehn et al., 2007) tokenizer without escaped special characters, so we re-apply escaping. All other data is tokenized with the Moses tokenizer with standard settings per language. We truecase the data with the Moses truecaser.\nTo deal with the limited ability of neural translation models to handle out-of-vocabulary words we split tokens into subword units, following Sennrich et al. (2015b).\nSubword units were learned using a modified version of the byte pair encoding (BPE) compression algorithm (Gage, 1994).\n1An accepted ACL 2016 paper is scheduled to appear: Santanu Pal, Sudip Kumar Naskar, Mihaela Vela and Josef van Genabith. A Neural Network based Approach to Automated Post-Editing. Proceedings of the 54th Annual Meetings of the Association for Computational Linguistics, August 2016.\n2A triplet consists of the English source sentence, a German machine translation output, and the German manually post-edited correction of that output.\nSennrich et al. (2015b) modified the algorithm to work on character level instead of on bytes. The most frequent pairs of characters are iteratively replaced by a new character sequence created by merging the pairs of existent sequences. Frequent words are thus represented by single symbols and infrequent ones are divided into smaller units. The final size of the vocabulary is equal to the sum of merge operations and the number of initial characters. This method effectively reduces the number of unknown words to zero, as characters are always available as the smallest fall-back units. Sennrich et al. (2015b) showed that this method can deal with German compound nouns (relieving us from applying special methods to handle these) as well as transliterations for Russian-English.\nThis seems particularly useful in the case of APE, where we do not wish the neural models to \u201challucinate\u201d output when encountering unknown tokens. A faithful transliteration is more desirable. We chose vocabularies of 40,000 units per language. For German MT output and post-edited sentences we used the same set of subword units."}, {"heading": "4 Artificial post-editing data", "text": "The provided post-editing data is orders of magnitude too small to train our neural models, and even with the in-domain training data from the IT translation task, we quickly see overfitting effects for a first English-German translation system. Inspired by Sennrich et al. (2015a) \u2014 who use backtranslated monolingual data to enrich bilingual training corpora \u2014 we decided to create artificial training triplets."}, {"heading": "4.1 Bootstrapping monolingual data", "text": "We applied cross-entropy filtering (Moore and Lewis, 2010) to the German Common Crawl corpus performing the following steps:\n\u2022 We filtered the corpus for \u201cwell-formed\u201d lines which start with a capital Unicode letter character and end in an end-of-sentence punctuation mark. We require the line to contain at least 30 Unicode letters.\n\u2022 The corpus has been preprocessed as described above, including subword units, which may have a positive effect on crossentropy filtering as they allow to score unknown words.\n\u2022 Next, we built an in-domain trigram language model (Heafield et al., 2013) from the German post-editing training data and the German IT-task data, and a similarly sized outof-domain language model from the Common Crawl data.\n\u2022 We calculated cross-entropy scores for the first one billion lines of the corpus according to the two language models;\n\u2022 We sorted the corpus by increasing crossentropy and kept the first 10 million entries for round-trip translation and the top 100 million entries for language modeling."}, {"heading": "4.2 Round-trip translation", "text": "For the next step, two phrase-based translation models, English-German and German-English, were created using the admissible parallel training data from the IT task. Word-alignments were computed with fast-align (Dyer et al., 2013), the dynamic-suffix array (Germann, 2015) holds the translation model. The top 10% bootstrapped monolingual data was used for language modeling in case of the English-German model, for the German-English translation system the language model was built only from the target side of the parallel in-domain corpora.3\nThe top 1% of the bootstrapped data have first been translated from German to English and next backwards from English to German. The intermediate English translations were preserved. In order to translate these 10 million sentences quickly (twice), we applied small stack-sizes and cubepruning-pop-limits of around 100, completing the round-trip translation in about 24 hours.\nThis procedure left us with 10 million artificial post-editing triplets, where the source German data is treated as post-edited data, the German\u2192English translated data is the English source, the round-trip translation results are the new uncorrected MT-output."}, {"heading": "4.3 Filtering for TER", "text": "We hope that a round-trip translation process produces literal translations that may be more-orless similar to post-edited triplets, where the distance between MT-output and post-edited text is\n3These models were not meant to be state-of-the-art quality systems. Our main objective was to create them within a few hours.\ngenerally smaller than between MT-output and human-produced translations of the same source. Having that much data available, we could continue our filtering process by trying to mimic the TER-statistics of the provided APE training corpus. While TER scores do only take into account the two German language parts of the triplet, it seems reasonable that filtering for better GermanGerman pairs automatically results in a higher quality of the intermediate English part.\nTo achieve this, we represented each triplet in the APE training data as a vector of elementary TER statistics computed for the MT-output and the post-edited correction, such as the sentence length, the frequency of edit operations, and the sentence-level TER score. We do the same for the to-be-filtered artificial triplet corpus. The similarity measure is the inverse Euclidean distance over these vector representations.\nIn a first step, outliers which diverge from any maximum or minimum value of the reference vectors by more than 10% were removed. For example, we filtered triplets with post-edited sentences that were 10% longer than the longest post-edited sentence in the reference.\nIn the second step, for each triplet from the reference set we select n nearest neighbors. Candidates that have been chosen for one reference set triplet were excluded for the following triplets. If more than the 100 triplets had to be traversed to satisfy the exclusion criterion, less than n or even 0 candidates were selected. Two subsets have been created, one for n = 1 and one for n = 10. Table 1 sets the characteristics of the obtained corpora in relation to the provided training and development data. The smaller set (round-trip.n1) follows the TER statistics of the provided training and development data quite closely, but consists only of 5% of the artificial triplets. The larger set (round-trip.n10) consists of roughly 43% of the\ndata, but has weaker TER scores."}, {"heading": "5 Experiments", "text": "Following the post-editing-by-machine-translation paradigm, we explore the application of softattention neural translation models to post-editing. Analogous to the two dominating approaches described in Section 2.1, we investigate methods that are purely monolingual as well as a simple method to include source language information in a more natural way than it has been done for phrase-based machine translation.\nThe neural machine translation systems explored in this work are attentional encoderdecoder models (Bahdanau et al., 2015), which have been trained with Nematus4. We used minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. Models were trained with\n4https://github.com/rsennrich/nematus\nAdadelta (Zeiler, 2012), reshuffling the corpus between epochs. As mentioned before tokens were split into subword units, 40,000 per language. For decoding, we used AmuNMT5, our C++/CUDA decoder for NMT models trained with Nematus with a beam size of 12 and length normalization."}, {"heading": "5.1 MT-output to post-editing", "text": "We started training the monolingual MT-PE model with the MT and PE data from the larger artificial triplet corpus (round-trip.n10). The model has been trained for 4 days, saving a model every 10, 000 mini-batches. Quick convergence can be observed for the monolingual task and we switched to fine-tuning after the 300,000-th iteration with a mix of the provided training data and the smaller round-trip.n1 corpus. The original post-editing data was oversampled 20 times and concatenated with round-trip.n1.\nThis resulted in the performance jump shown in Figure 1 (mt\u2192pe, blue). Training were continued for another 100,000 iterations and stopped when overfitting effects became apparent. Training directly with the smaller training data without the initial training on round-trip.n10 lead to even earlier overfitting.\nEntry mt\u2192pe in Table 2 contains the results of the single-best model on the development set which outperforms the baseline significantly. Models for ensembling are selected among the periodically saved parameter dumps of one training run. An ensemble mt\u2192pe\u00d74 consisting of the four best models shows only modest improvements over the single model. The same development set has been used to select the best-performing models, results may therefore be slightly skewed."}, {"heading": "5.2 Source to post-editing", "text": "We proceed similarly for the English-German NMT training. When fine-tuning with the smaller corpus with oversampled post-editing data, we also add all in-domain parallel training data from the IT-task, roughly 200,000 sentences. Finetuning results in a much larger jump than in the monolingual case, but the overall performance of the NMT system is still weaker than the uncorrected MT-baseline.\nAs for the monolingual case, we evaluate the single-best model (src\u2192pe) and an ensemble\n5https://github.com/emjotde/amunmt\n(src\u2192pe\u00d74) of the four best models of a training run. The src\u2192pe\u00d74 system is not able to beat the MT baseline, but the ensemble is significantly better than the single model."}, {"heading": "5.3 Log-linear combinations and tuning", "text": "AmuNMT can be configured to accept different inputs to different members of a model ensemble as long as the target language vocabulary is the same. We can therefore build a decoder that takes both, German MT output and the English source sentence, as parallel input, and produces post-edited German as output. Since once the input sentence has been provided to a NMT model it essentially turns into a language model, this can be achieved without much effort. In theory an unlimited number of inputs can be combined in this way without the need of specialized multi-input training procedures (Zoph and Knight, 2016).6\nIn NMT ensembles, homogeneous models are typically weighted equally. Here we combine different models and equal weighting does not work. Instead, we treat each ensemble component as a feature in a traditional log-linear model and perform weighting as parameter tuning with BatchMira (Cherry and Foster, 2012). AmuNMT can produce Moses-compatible n-best lists and we devised an iterative optimization process similar to the one available in Moses. We tune the weights on the development set towards lower TER scores; two iterations seem to be enough. When ensembling one mt\u2192pe model and one src\u2192pe model, the assigned weights correspond roughly to 0.8\n6Which are still worth investigating for APE and likely to yield better results.\nand 0.2 respectively. The linear combination of all eight models (mt\u2192pe\u00d74 / src\u2192pe\u00d74) improves quality by 0.9 TER and 1.2 BLEU, however, weights were tuned on the same data."}, {"heading": "5.4 Enforcing faithfulness", "text": "We extend AmuNMT with a simple Post-Editing Penalty (PEP). To ensure that the system is fairly conservative \u2014 i.e. the correction process does not introduce too much new material \u2014 every word in the system\u2019s output that was not seen in its input incurs a penalty of -1.\nDuring decoding this is implemented efficiently as a matrix of dimensions batch size \u00d7 target vocabulary size where all columns that match source words are assigned 0 values, all other words \u22121. This feature can then be used as if it was another ensemble model and tuned with the same procedure as described above.\nPEP introduces a precision-like bias into the decoding process and is a simple means to enforce a certain faithfulness with regard to the input via string matching. This is not easily accomplished within the encoder-decoder framework which abstracts away from any string representations. A recall-like variant (penalize for missing input words in the output) cannot be realized at decode-time as it is not known which words have been omitted until the very end of the decoding process. This could only work as a final re-ranking criterion, which we did not explore in this paper. The bag-of-words approach grants the NMT model the greatest freedom with regard to reordering and fluency for which these models seem to be naturally well-suited.\nAs before, we tune the combination on the development set. The resulting system (mt\u2192pe\u00d74 / src\u2192pe\u00d74 / pep) can again improve post-editing quality. We see a total improvement of -3.7% TER and +6.0% BLEU over the given MT baseline on the development set. The log-linear combination of different features improves over the purely monolingual ensemble by -1.8% TER and +2.1% BLEU."}, {"heading": "6 Final results and conclusions", "text": "We submitted the output of the last system (mt\u2192pe\u00d74 / src\u2192pe\u00d74 / pep) as our final proposition for the APE shared task, and mt\u2192pe\u00d74 as a contrastive system. Table 3 contains the results on the unseen test set for our two systems (in bold)\nand the best system of any other submitting team as reported by the task organizers (for more details and manually judged results \u2014 which were not yet available at the time of writing \u2014 see the shared task overview paper). Results are sorted by TER from best to worse. For our best system, we see improvements of -3.2% TER and +5.5% BLEU over the unprocessed baseline 1 (uncorrected MT), and -1.5% TER and +1.5% BLEU over our contrastive system. The organizers also provide results for a standard phrase-based Moses set-up (baseline 2) that can hardly beat baseline 1 (-0.1% TER, +1.4% BLEU). Both our systems outperform the next-best submission by large margins. In the light of these last results, our system seems to be quite successful.\nWe could demonstrate the following:\n\u2022 Neural machine translation models can be successfully applied to APE;\n\u2022 Artificial APE triplets help against early overfitting and make it possible to overcome the problem of too little training data;\n\u2022 Log-linear combinations of neural machine translation models with different input languages can be used as a method of combining MT-output and source data for APE to positive effects;\n\u2022 Task specific features can be easily integrated into the log-linear models and can control the faithfulness of the APE results.\nFuture work should include the investigation of integrated multi-source approaches like\n(Zoph and Knight, 2016) and better schemes of dealing with overfitting. We also plan to apply our methods to the data of last year\u2019s APE task."}, {"heading": "7 Acknowledgements", "text": "This work is partially funded by the National Science Centre, Poland (Grant No. 2014/15/N/ST6/02330)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Statistical post-editing for a statistical MT system", "author": ["Yanjun Ma", "Josef van Genabith"], "venue": "In Proceedings of the 13th Machine Translation Summit,", "citeRegEx": "B\u00e9chara et al\\.,? \\Q2011\\E", "shortCiteRegEx": "B\u00e9chara et al\\.", "year": 2011}, {"title": "An evaluation of statistical post-editing systems applied to RBMT and SMT systems", "author": ["Rapha\u00ebl Rubino", "Yifan He", "Yanjun Ma", "Josef van Genabith"], "venue": "In Proceedings of COLING", "citeRegEx": "B\u00e9chara et al\\.,? \\Q2012\\E", "shortCiteRegEx": "B\u00e9chara et al\\.", "year": 2012}, {"title": "Findings of the 2015 Workshop on Statistical Machine Translation", "author": ["Turchi."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 1\u201346, Lisbon, Portugal. Association for Computational Linguistics.", "citeRegEx": "Turchi.,? 2015", "shortCiteRegEx": "Turchi.", "year": 2015}, {"title": "The FBK participation in the WMT15 automatic post-editing shared task", "author": ["Marco Turchi", "Matteo Negri"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Chatterjee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chatterjee et al\\.", "year": 2015}, {"title": "Exploring the planet of the APEs: a comparative study of state-of-the-art methods for MT automatic post-editing", "author": ["Marion Weller", "Matteo Negri", "Marco Turchi"], "venue": "In Proceedings of the 53rd Annual", "citeRegEx": "Chatterjee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chatterjee et al\\.", "year": 2015}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Cherry", "Foster2012] Colin Cherry", "George Foster"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Cherry et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cherry et al\\.", "year": 2012}, {"title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A simple, fast, and effective reparameterization of IBM model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "A new algorithm for data compression", "author": ["Philip Gage"], "venue": "The C Users Journal,", "citeRegEx": "Gage.,? \\Q1994\\E", "shortCiteRegEx": "Gage.", "year": 1994}, {"title": "Sampling phrase tables for the Moses statistical machine translation system", "author": ["Ulrich Germann"], "venue": "Prague Bulletin of Mathematical Linguistics,", "citeRegEx": "Germann.,? \\Q2015\\E", "shortCiteRegEx": "Germann.", "year": 2015}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn"], "venue": "In Proceedings of the 51st Annual Meeting of the Association", "citeRegEx": "Heafield et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Moses: Open source toolkit", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Intelligent selection of language model training data", "author": ["Moore", "Lewis2010] Robert C. Moore", "William Lewis"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Moore et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Moore et al\\.", "year": 2010}, {"title": "USAAR-SAPE: An English\u2013Spanish statistical automatic post-editing system", "author": ["Pal et al.2015] Santanu Pal", "Mihaela Vela", "Sudip Kumar Naskar", "Josef van Genabith"], "venue": null, "citeRegEx": "Pal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pal et al\\.", "year": 2015}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting on Association", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "arXiv preprint arXiv:1511.06709", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Statistical phrase-based post-editing", "author": ["Simard et al.2007] Michel Simard", "Cyril Goutte", "Pierre Isabelle"], "venue": "In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Simard et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2007}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul"], "venue": "In Proceedings of Association for Machine Translation in the Americas,", "citeRegEx": "Snover et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems 27: 28th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Why predicting post-edition is so hard? failure analysis of LIMSI submission to the APE shared task", "author": ["Nicolas P\u00e9cheux", "Fran\u00e7ois Yvon"], "venue": "In Proceedings of the Tenth Workshop on Statistical Ma-", "citeRegEx": "Wisniewski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wisniewski et al\\.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Multi-source neural translation", "author": ["Zoph", "Knight2016] Barret Zoph", "Kevin Knight"], "venue": "arXiv preprint arXiv:1601.00710", "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "The organizers provide training data with human post-edits, evaluation is carried out partautomatically using TER (Snover et al., 2006) and BLEU (Papineni et al.", "startOffset": 114, "endOffset": 135}, {"referenceID": 15, "context": ", 2006) and BLEU (Papineni et al., 2002), and part-manually.", "startOffset": 17, "endOffset": 40}, {"referenceID": 2, "context": "The inclusion of source-language information in that form is shown to be useful to improve the automatic post-editing results (B\u00e9chara et al., 2012; Chatterjee et al., 2015b).", "startOffset": 126, "endOffset": 174}, {"referenceID": 14, "context": "State-of-the-art APE systems follow a monolingual approach firstly proposed by Simard et al. (2007) who trained a phrase-based SMT system on machine translation output and its post-edited versions.", "startOffset": 79, "endOffset": 100}, {"referenceID": 1, "context": "B\u00e9chara et al. (2011) proposed a \u201csource-context aware\u201d variant of this approach: automatically created word alignments are used to create a new source language which consists of joined MT-output and source token pairs.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "important role for this methods, as shown for instance by Pal et al. (2015).", "startOffset": 58, "endOffset": 76}, {"referenceID": 4, "context": "Chatterjee et al. (2015a) propose a pipeline where", "startOffset": 0, "endOffset": 26}, {"referenceID": 21, "context": "Other popular approaches rely on rulebased components (Wisniewski et al., 2015; B\u00e9chara et al., 2012) which we do not discuss here.", "startOffset": 54, "endOffset": 101}, {"referenceID": 2, "context": "Other popular approaches rely on rulebased components (Wisniewski et al., 2015; B\u00e9chara et al., 2012) which we do not discuss here.", "startOffset": 54, "endOffset": 101}, {"referenceID": 20, "context": "An LSTM-based encoder-decoder model was introduced by Sutskever et al. (2014). Here the source sentence is encoded into a single continuous vector, the final state of the source LSTMRNN.", "startOffset": 54, "endOffset": 78}, {"referenceID": 7, "context": "(2015) extended this simple concept with bidirectional source RNNs (Cho et al., 2014) and the so-called soft-", "startOffset": 67, "endOffset": 85}, {"referenceID": 20, "context": "The novelty of this approach and its improved performance compared to Sutskever et al. (2014) came from the reduced reliance on the source sentence embedding which had to convey all information required for trans-", "startOffset": 70, "endOffset": 94}, {"referenceID": 0, "context": "the reader to Bahdanau et al. (2015) for a detailed description of the discussed models.", "startOffset": 14, "endOffset": 37}, {"referenceID": 12, "context": "The provided triplets have already been tokenized, the tokenization scheme seems to correspond to the Moses (Koehn et al., 2007) tokenizer without escaped special characters, so we re-apply escaping.", "startOffset": 108, "endOffset": 128}, {"referenceID": 16, "context": "To deal with the limited ability of neural translation models to handle out-of-vocabulary words we split tokens into subword units, following Sennrich et al. (2015b). Subword units were learned using a mod-", "startOffset": 142, "endOffset": 166}, {"referenceID": 9, "context": "ified version of the byte pair encoding (BPE) compression algorithm (Gage, 1994).", "startOffset": 68, "endOffset": 80}, {"referenceID": 16, "context": "Inspired by Sennrich et al. (2015a) \u2014 who use backtranslated monolingual data to enrich bilingual training corpora \u2014 we decided to create artificial training triplets.", "startOffset": 12, "endOffset": 36}, {"referenceID": 11, "context": "\u2022 Next, we built an in-domain trigram language model (Heafield et al., 2013) from the German post-editing training data and the German IT-task data, and a similarly sized outof-domain language model from the Common Crawl data.", "startOffset": 53, "endOffset": 76}, {"referenceID": 8, "context": "Word-alignments were computed with fast-align (Dyer et al., 2013), the dynamic-suffix array (Germann, 2015) holds the translation model.", "startOffset": 46, "endOffset": 65}, {"referenceID": 10, "context": ", 2013), the dynamic-suffix array (Germann, 2015) holds the translation model.", "startOffset": 34, "endOffset": 49}, {"referenceID": 0, "context": "decoder models (Bahdanau et al., 2015), which have been trained with Nematus4.", "startOffset": 15, "endOffset": 38}, {"referenceID": 22, "context": "Adadelta (Zeiler, 2012), reshuffling the corpus between epochs.", "startOffset": 9, "endOffset": 23}], "year": 2016, "abstractText": "This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic Post-Editing (APE) task of WMT 2016. We explore the application of neural translation models to the APE problem and achieve good results by treating different models as components in a log-linear model, allowing for multiple inputs (the MT-output and the source) that are decoded to the same target language (post-edited translations). A simple string-matching penalty integrated within the log-linear model is used to control for higher faithfulness with regard to the raw machine translation output. To overcome the problem of too little training data, we generate large amounts of artificial data. Our submission improves over the uncorrected baseline on the unseen test set by -3.2% TER and +5.5% BLEU and outperforms any other system submitted to the shared-task by a large margin.", "creator": "LaTeX with hyperref package"}}}