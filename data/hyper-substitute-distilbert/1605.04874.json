{"id": "1605.04874", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2016", "title": "Gearbox Fault Detection through PSO Exact Wavelet Analysis and SVM Classifier", "abstract": "temporal - variation methods analyzing filter - based deformation measuring data have mainly used doing particularly efficient procedures. below these methods, continuous mechanical transform ( cwt ) - one among the worst time - frequency method makes boon noted incorporating both stationary and shifting signals. some instances of cwt are problem parameter overlapping and distortion ofsignals. in this condition, a highly recently used redundant hardware exists however doing it may cause false signals to misinterpretation before the operator. while this approximation data modified method as exact wavelet analysis is used could minimize the effects being differential worm distortion in case of gearbox inversion. to define exact wavelet procedures, particle swarm optimization ( pso ) algorithm has been used for this purpose. this method have been implemented for the acceleration signals from target acceleration sensor input by cisco data - 1710 card from a temporal tracker setup in amirkabir university academic technology. gearbox measures been fabricated in both vertical and chipped tooth extraction conditions. kernelized support vector retrieval ( svm ) with radial basis functions has performed data extracted features from exact fourier analysis for diagnosis. numerical efficiency of curve classifier is be evaluated with adding other signals acquired from the differential analysis. this values show that in evaluation of advantages, plant parameter wavelet composition has better ability from feature extraction with price parameter more important factors. in addition, species specific wavelet demand better parameter comparing on genetic tractor ( ga ) fixed wavelet optimal condition achieving equal population method of factoring mutation and null results pso prediction. further classifier coding the cluster image in r shows very best results and its ability has been considered.", "histories": [["v1", "Thu, 12 May 2016 23:29:29 GMT  (2811kb)", "http://arxiv.org/abs/1605.04874v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["amir hosein zamanian", "abdolreza ohadi"], "accepted": false, "id": "1605.04874"}, "pdf": {"name": "1605.04874.pdf", "metadata": {"source": "CRF", "title": "Gearbox Fault Detection through PSO Exact Wavelet Analysis and SVM Classifier", "authors": ["Amir Hosein Zamanian", "Abdolreza Ohadi"], "emails": [], "sections": [{"heading": null, "text": "1\nmethod of signal processing categorized to time domain, frequency domain or time-frequency domain analysis. Although features extracted from time domain analysis (i.e. statistical features) can detect fault (same as [1-3]), but generally these methods need extensive data mining procedure for feature selection so different types of data mining algorithm and decision trees are applied with these features. On the other hand, there exist some problems for frequency methods (e.g. Fast Fourier Transform). Indeed, frequency methods eliminate the effect of time in the signal, so these methods only indicate the frequencies and its intensity, but they do not reveal how many frequencies exist and with what intensity these frequencies repeat. These methods are mainly applied to detect for harmonics in signals. For the sake of overcome these disadvantages, the timefrequency methods (Gabor transform, Hilbert-Haung transform, Wavelet transform, etc) take lots of attractions for researchers who would like to study signal in frequency content but they wouldn\u2019t like to eliminate time information. Generally, time-frequency methods provide energy distribution of signals; hence, these methods seem to be what researchers are looking for. Among these, continuous wavelet transform (CWT) is mainly considered as an effective tool for vibrationbased signal processing for fault detection. CWT provides a multi-resolution in time-frequency analysis for characterizing the transitory features of nonstationary signals [4]. Two problems, \u201cdistortion\u201d and \u201coverlapping\u201d, suffer CWT which are completely discussed by Tse et al. [4]. Tse et al. [4], to solve the problem of distortion and overlapping, firstly used the term of \u201cexact wavelet analysis\u201d. They provided an opportunity to find most appropriate scale and wavelet daughter shape in each time frame of inspected signal by a GA-based optimization process by defining an objective function which indicates similarity between wavelet coefficient and the inspected signal. So, for any time frame, GA is allowed to find the scale parameter and some parameters related to the shape of the wavelet presented by Brode [5]. The authors of this paper believe that in best condition, when the parameters is found by GA, the exact wavelet cannot find the most appropriate wavelet although it can find the best wavelet family presented by Brode [5]. So, here we use Morlet wavelet and reduce the optimization parameters to one parameter; scale variable. In this case, the optimization space reduces from 3 to 1 . This speeds up the optimization process and decreases the\n2 ISME2010, 11-13 May, 2010\nprobability of local optima. To speed up process even more, the GA has been replaced by PSO algorithm. Support vector machine (SVM) has been used in many applications of machine learning because of high accuracy and good generalization capability. SVM classifies better than artificial neural networks (ANN) because of the principle of risk minimization. In ANN, traditional empirical risk minimization (ERM) is used on training data set to minimize the error, whereas in SVM, structural risk minimization (SRM) is used to minimize an upper bound on the expected risk [3]. SVM has been successfully applied in automated detection of machines [1,2] based on learning patterns. Jack and Nandi [2] compared SVM by ANN for classification of fault in bearing by feature (statistical and spectral) selection based on genetic algorithm (GA). They showed that ANN tends to be faster to train and more robust than SVM in case of bearing vibration signals. In contrary, Samanta [1] made an investigation based on statistical features acquired form gears, the effectiveness of both ANN and SVM was compared. It has been shown that, for most of the cases considered, the classification accuracy of SVM is better than ANN when the GA-based feature selection has not been used. However, with GA-based feature selection the performance of both classifiers is comparable. In part two of the current work, a brief introduction to exact wavelet transform is presented, in part three PSO is introduced, part four is reserved for SVM classifier and in part five the algorithm is applied on an experimental setup test. Summary and conclusions take final part of this paper."}, {"heading": "Exact Wavelet Analysis", "text": "There exists two kind of exact wavelet analyses, the first method utilizes the concept of \u2018\u2018maximum matching mechanism\u2019\u2019 to determine the most appropriate coefficients to represent the inspected raw signal. In CWTs with a given signal, within the selected time frame, if a daughter wavelet, which is generated by a particular scale, has the largest value of wavelet coefficient, it often implies that the shape of that daughter wavelet can match the shape of the inspected signal better than other daughter wavelets generated by other scales [4]. The advantage of this method is its simplicity and higher computational speed, whereas its disadvantage refers to the fact that it cannot find appropriate daughter wavelet with the geometric shape exactly similar to the inspected signal within the selected time frame. In addition, selection of mother wavelet is not adaptive to the inspected signal [4]. The second method is aimed to provide a direct measure of the similarity in shapes between the daughter wavelet and the inspected signal. Instead of using the largest value of wavelet coefficient, the \u2018\u2018normalized dot product\u2019\u2019 of the daughter wavelet and the inspected signal is adopted for measuring their similarity in shape [4]. Continuous wavelet transform is defined as,\n,( , ) ( ) ( )      a bW a b x t t dt , (1) where\n   1 2 , ( )   a b t a t b a (2)\nis a window function called mother wavelet and a and b are real-valued parameters, b is the translation parameter indicating the position, and a is the scale parameter. In this paper, a modified version of second method has been selected by considering that most appropriate wavelet is not selected by exact wavelet. The Morlet wavelet (Fig. 1) is considered as most appropriate wavelet because of its similarity to response of impulse function. Therefore, in each time frame only scale is optimized and consequently, in each translation, the algorithm gives the most appropriate scale. The advantage of this method is that the optimization algorithm does not waste time for calculation of wavelet parameters, so optimization process goes faster.\nFor each selected time frame, the similarity of wavelet and portion of signal is calculated by normalized dot product between the exact wavelet coefficients and the portion of inspected signal, a fitness index can be obtained to evaluate the degree of matching. The index is calculated using a cosine function of two vectors [4].\n  1 2 2\n1 1\ncos , , 1 , n\ni ii n n i ii i\nc x n N\nc x \n \n     C X (3)\nwhere C and X , stand for the vectors of the wavelet coefficients and the portion of the inspected signal, respectively. The variables ic and ix represent the elements of the vectors and N is the number of signal samples. The calculated index from the fitness function provides a measure to evaluate the similarity of the two vectors not only in their magnitudes but also in their geometrical shapes. The higher indexes of fitness function indicates that more similarity exist between the derived wavelet and the portion of the inspected signal. The index of the cosine function approaches to 1 when a perfect match exist, whilst the zero value index shows a mismatch [4]."}, {"heading": "Particle Swarm Optimization", "text": "Particle swarm optimization (PSO) is a population based stochastic optimization technique developed by Kennedy and Eberhart [6] in 1995, inspired by social behavior of bird flocking or fish schooling. PSO started with population of random solutions and updating the solutions in next generation to find optimal solution [7]. The potential solutions in PSO are called particles.\n3 ISME2010, 11-13 May, 2010\nThese particles fly thorough hyper space of the problem by following best particles [7]. All particles have fitness values, which are evaluated by the fitness function, and have velocities, which direct the flying of the particles. The particles fly through the problem space by following the current optimum particles. PSO is initialized with a group of random particles and then searches for optima by updating generations [7]. In each iteration, particles are updated by following two \"best\" values. The first one is the best solution (fitness) which a particle is achieved so far (pb) and the second one is the best global solution obtained so far in all particles of the population (gb) [7].Velocity and position of particles are updated with the following equations:\n   1 1 1 2 2 ,t t t t t ti i i i ic r c r     v v pb p gb p (4) 1 1,t t ti i i   p p v\n(5) where iv , ip are the i-th particle velocity and particle position (solution), respectively. tipb and tgb are already\ndefined, 1r , 2r are random numbers between (0,1) and 1c , 2c are learning factors (usually 1 2 2 c c ) [7]. Velocities of particles on each dimension are constrained to a maximum velocity\nmaxV .\nSupport Vector Machine The foundations of support vector machines (SVM) have been developed by Vapnik in 1995 [8] and are gaining popularity due to many attractive features, and promising empirical performance. The formulation embodies the structural risk minimization (SRM) principle, which has been shown to be superior to traditional empirical risk minimization (ERM) principle, employed by conventional neural networks [9]. The concept of support vector machine is extensive, a brief introduction of SVM presented here, the readers are referred to [9, 10] for more details. Without loss of generality, the classification problem can be restricted to consideration of the two-class problem. SVM can be considered to create a line or hyper-plane between two set of data for classification [1]. Consider the problem of separating the set of training vectors belonging to two separate classes\n      1 1, ,..., , , , 1,1 ,   l l nD y y x yx x (6) with a hyper-plane, . 0. bw x (7) In the case of two-dimensional situation, the action of the SVM can be explained easily. In this situation, SVM try to find a line which separate two classes of data (feature sets) by a line (hyper plane). This line separates data into two parts so the data on the right hand belong to one class (Class A) and the data on the left hand belong to the other class (Class B). Many lines have the ability to separate data truly. However, SVM try to find that line which has the maximum Euclidean distance between the nearest data to this line either in Class A and B. The data, which has the minimum distance to this line, are called support vectors (SVs) that are shown\nin Fig. 2. Since training SVM with SVs is sufficed, the rest of data can be neglected.\nThe SVs are located in two parallel lines, which are parallel to the separating line. The margin equations for class A and B are as follows: . 1 (ClassA),b w x (8) and . 1 (ClassB).b  w x (9) Once the SVM has been trained, a decision function in Eq. (10) determines that each test sample belongs to which side of decision boundary (i.e. which class).    sgn . f x bw x , (10)\nThe SVM training is obtained by optimizing an objective function that is presented in Eq.(11)\n 2 1 1 1 . , 2\nl l\ni i i i i L y b        iw w x (11) where l is the number of training sets, and\ni is Lagrange multipliers\u2019 coefficients obtained by the following constraints.  . 1,i iy b w x (12)\nThe solution can be obtained as follows\n1 1 ,\nl l\ni i i i i i i y v     w x x (13) where\ni i iv y (14) Substituting of Eq. (13) to Eq.(10) leads to Eq.(15).\n    1 sgn . \n    \n  \nl\ni i i\nf x v bx x (15)\nThe set of vectors is said to be optimally separated by the hyper plane if it is separated without error and the distance between the closest vector to the hyper plane is maximal [9]. In the case of non-separable data with linear hyper plane, a hyper plane should be defined that allows linear separation in the higher dimension (corresponding to nonlinear separating hyper planes) [1]. To do this, the data should be mapped to some other spaces, using a mapping . : n   where  is a Hilbert space (as a generalization of Euclidean space [10]), so by defining  \u03a6 x , the data can be transformed to the new space, by defining a kernel function  .iK x x in Eq.(16). The former equations can be modified by changing the dot product of .ix x to  .iK x x .\n     . .i j i jK x x \u03a6 x \u03a6 x (16) The defined kernel function emphasize that  \u03a6 x is not necessary to be known explicitly. Therefore, Eq.(15) changes to Eq.(17).\n    1 sgn . \n    \n  \nl\ni i i\nf x v K bx x (17)\nThere exists different kernel functions, a common function called Radial Basis Function (RBF) which is used in this work, given by Eq.(18).\n   2 2, exp 2  i j i jK x x x x (18) The parameter  is width of RBF kernel, which is an important parameter in classification performance and can be determined by statistical computations or by iterative process to choose optimum value [1]. For the non-separable data, where overlap exists between the classes, the range of parameters iv should be bounded to reduce the effect of outliers on the boundary defined by SVs (i.e. iv C ). For separable cases, C is infinity while for nonseparable cases, it may be varied, depending on the number of allowable errors in the trained solution: high C permits few errors while low C allows a higher proportion of errors in the solution [1,10]."}, {"heading": "Experimental Results", "text": "To evaluate the efficiency of exact wavelet, the algorithm was implemented on an experimental vibration signal of gearbox. This process was done for two conditions, normal and chipped tooth gear (as shown in Fig. 3). In the chipped tooth case, 50% of a tooth profile from top to pitch circle (addendum) was eliminated with linear slop from top to pitch circle. The signals were acquired from a gearbox setup test designed in Amirkabir University of Technology (Tehran Polytechnic) for this proposes, as shown in Fig. 5. The vibrating signals were obtained from 2D accelerometer (ADXL210JQC) mounted on gearbox frame (Fig. 4). Sampling frequency was set as 10kHz. The acceleration frequency content is in the range of 0~5kHz. These obtained signals were fed to A/D converter (Advantech\u2122 PCI-1710, 12-bit, 100kS/s) and was recorded by real-time workshop of MATLAB software. The gearbox rotates with nominal speed of 3- phase electromotor (1420 RPM). The driver ( 1 15N ) and driven ( 2 110N ) gear provide speed ratio of 7.33:1 for gearbox. The disk brake system has been considered to provide appropriate load on the system. Although number of samples for each feature sets is completely arbitrary, however it is not appropriate to select small portion of signal. So each feature sets considered here contains 1250 signal samples (approximately equal to 3 round of driver gear). 80 samples for each normal and chipped tooth gear condition (totally 160 feature sets) have been created. The exact wavelet was implemented by PSO algorithm with parameters shown in Table 1. Range of scale is considered between 1 and 32. To create a feature set, distribution of scales in 1250 samples was counted and divided to 16 ranges of scales. Fig. 6 shows two feature\nsets, one of them corresponds to normal gear and another one belongs to chipped tooth gear. The difference of feature sets is obviously apparent. Distributions of number of data point belong to each scale level for normal and chipped tooth gear conditions create 16 feature. These feature sets have been used to train SVM classifier. The programming of these procedures was done by MATLAB.\n18th Annual International Conference on Mechanical Engineering-ISME2010 11-13 May, 2010, Sharif University of Technology, Tehran, Iran\n5\nSVM classifier was trained with 60 feature sets, and 100 feature sets were remained for test success measurement. The result of SVM classifier is presented in Table 2."}, {"heading": "100 94 100 99 100 100 100 100", "text": "It is clear in Table 2 that the feature sets is completely linear separable, so use of linear SVM bring 100% success in classification. Nonlinear SVM with RBF\nbring excellent classification when the  parameter increases, because SVM tends to have a linear manner."}, {"heading": "Comparison of PSO and GA exact", "text": "The authors applied the exact wavelet by GA, with similar parameter of PSO and the computational time was almost 40 times more in case of GA exact wavelet analysis. However, GA solutions had better optimization performance in finding most appropriate scale and PSO generally filled in local maxima. It should be noted that the PSO and GA solutions were close to each other and the difference in solutions does not affect the mentioned feature extraction. The solution for 20 sample of signal is plotted in Fig. 7. Because of stochastic nature of both GA and PSO methods, the solutions shown in Fig. 7 might change in different runs."}, {"heading": "Conclusions", "text": "In this paper, a modified version of exact wavelet analysis, which speeds up the exact wavelet process, is introduced. It has been shown that PSO exact wavelet can speed up the process almost 40 times comparing to GA. It is also shown than parameters of exact wavelet can be reduced by using of appropriate wavelet (i.e. Morlet wavelet) to prevent designing wavelet. That speed up more, without destroying feature sets. The SVM classifier has been used with 16 feature, although SVM with radial bases can classify with test\n6 ISME2010, 11-13 May, 2010\nsuccess of 100% by considering suitable sigma as shown in table 1. However, it has been shown that the feature sets are linear separable, so linear SVM in case of classification of normal and chipped tooth gear will suffice. The main problem that suffers exact wavelet even with this modified form is computational time that cannot be used in real time application."}, {"heading": "Acknowledgment", "text": "The authors should appreciate to Mr. Mohammad Sarikhani, educational instructor in Amirkabir University of Technology\u2019s Vibration and Dynamics of Machine Lab. for his valuables consultations and Mr. Mohammad Rostami Ghomi, MSc electronic student of Sharif University of Technology for designing some electronics equipment for this project."}], "references": [{"title": "Gear fault detection using artificial neural networks and support vector machines with genetic algorithms", "author": ["B. Samanta"], "venue": "Mechanical Systems and Signal Processing", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Fault detection using support vector machines and artificial neural networks, augmented by genetic algorithms", "author": ["L.B. Jack", "A.K. Nandi"], "venue": "Mechanical Systems and Signal Processing\u201d", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "A comparative study on classification of features by SVM and PSVM extracted using Morlet wavelet for fault diagnosis of spur bevel gear box", "author": ["N. Saravanan", "V.N. Kumar Siddabattuni", "K.I. Ramachandran"], "venue": "Expert Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Machine fault diagnosis through an effective exact wavelet analysis", "author": ["P.W. Tse", "Y. Wen-xian", "H.Y. Tam"], "venue": "Journal of Sound and Vibration,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "New wavelet class for fine structure identification", "author": ["B.L. Borde"], "venue": "SPIE ,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Particle Swarm Optimization", "author": ["J. Kennedy", "R. Eberhart"], "venue": "IEEE International Conference on Neural Networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "The Nature of Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}, {"title": "Support Vector Machines for Classification and Regression", "author": ["S.R. Gunn"], "venue": "Technical Report,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "A Tutorial on Support Vector Machines for Pattern Recognition", "author": ["C.J. Burges"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "There are several methods in fault detection, in one view the signals categorized to vibrating signals, acoustic emitted (AE) signals and electric current signals obtained from systems [1].", "startOffset": 185, "endOffset": 188}, {"referenceID": 0, "context": "statistical features) can detect fault (same as [1-3]), but generally these methods need extensive data mining procedure for feature selection so different types of data mining algorithm and decision trees are applied with these features.", "startOffset": 48, "endOffset": 53}, {"referenceID": 1, "context": "statistical features) can detect fault (same as [1-3]), but generally these methods need extensive data mining procedure for feature selection so different types of data mining algorithm and decision trees are applied with these features.", "startOffset": 48, "endOffset": 53}, {"referenceID": 2, "context": "statistical features) can detect fault (same as [1-3]), but generally these methods need extensive data mining procedure for feature selection so different types of data mining algorithm and decision trees are applied with these features.", "startOffset": 48, "endOffset": 53}, {"referenceID": 3, "context": "CWT provides a multi-resolution in time-frequency analysis for characterizing the transitory features of nonstationary signals [4].", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4], to solve the problem of distortion and overlapping, firstly used the term of \u201cexact wavelet analysis\u201d.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "So, for any time frame, GA is allowed to find the scale parameter and some parameters related to the shape of the wavelet presented by Brode [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "The authors of this paper believe that in best condition, when the parameters is found by GA, the exact wavelet cannot find the most appropriate wavelet although it can find the best wavelet family presented by Brode [5].", "startOffset": 217, "endOffset": 220}, {"referenceID": 2, "context": "In ANN, traditional empirical risk minimization (ERM) is used on training data set to minimize the error, whereas in SVM, structural risk minimization (SRM) is used to minimize an upper bound on the expected risk [3].", "startOffset": 213, "endOffset": 216}, {"referenceID": 0, "context": "SVM has been successfully applied in automated detection of machines [1,2] based on learning patterns.", "startOffset": 69, "endOffset": 74}, {"referenceID": 1, "context": "SVM has been successfully applied in automated detection of machines [1,2] based on learning patterns.", "startOffset": 69, "endOffset": 74}, {"referenceID": 1, "context": "Jack and Nandi [2] compared SVM by ANN for classification of fault in bearing by feature (statistical and spectral) selection based on genetic algorithm (GA).", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "In contrary, Samanta [1] made an investigation based on statistical features acquired form gears, the effectiveness of both ANN and SVM was compared.", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "In CWTs with a given signal, within the selected time frame, if a daughter wavelet, which is generated by a particular scale, has the largest value of wavelet coefficient, it often implies that the shape of that daughter wavelet can match the shape of the inspected signal better than other daughter wavelets generated by other scales [4].", "startOffset": 335, "endOffset": 338}, {"referenceID": 3, "context": "In addition, selection of mother wavelet is not adaptive to the inspected signal [4].", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "Instead of using the largest value of wavelet coefficient, the \u2018\u2018normalized dot product\u2019\u2019 of the daughter wavelet and the inspected signal is adopted for measuring their similarity in shape [4].", "startOffset": 190, "endOffset": 193}, {"referenceID": 3, "context": "The index is calculated using a cosine function of two vectors [4].", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "The index of the cosine function approaches to 1 when a perfect match exist, whilst the zero value index shows a mismatch [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "Particle Swarm Optimization Particle swarm optimization (PSO) is a population based stochastic optimization technique developed by Kennedy and Eberhart [6] in 1995, inspired by social behavior of bird flocking or fish schooling.", "startOffset": 152, "endOffset": 155}, {"referenceID": 6, "context": "Support Vector Machine The foundations of support vector machines (SVM) have been developed by Vapnik in 1995 [8] and are gaining popularity due to many attractive features, and promising empirical performance.", "startOffset": 110, "endOffset": 113}, {"referenceID": 7, "context": "The formulation embodies the structural risk minimization (SRM) principle, which has been shown to be superior to traditional empirical risk minimization (ERM) principle, employed by conventional neural networks [9].", "startOffset": 212, "endOffset": 215}, {"referenceID": 7, "context": "The concept of support vector machine is extensive, a brief introduction of SVM presented here, the readers are referred to [9, 10] for more details.", "startOffset": 124, "endOffset": 131}, {"referenceID": 8, "context": "The concept of support vector machine is extensive, a brief introduction of SVM presented here, the readers are referred to [9, 10] for more details.", "startOffset": 124, "endOffset": 131}, {"referenceID": 0, "context": "SVM can be considered to create a line or hyper-plane between two set of data for classification [1].", "startOffset": 97, "endOffset": 100}, {"referenceID": 8, "context": "[10]", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The set of vectors is said to be optimally separated by the hyper plane if it is separated without error and the distance between the closest vector to the hyper plane is maximal [9].", "startOffset": 179, "endOffset": 182}, {"referenceID": 0, "context": "In the case of non-separable data with linear hyper plane, a hyper plane should be defined that allows linear separation in the higher dimension (corresponding to nonlinear separating hyper planes) [1].", "startOffset": 198, "endOffset": 201}, {"referenceID": 8, "context": "where \uf048 is a Hilbert space (as a generalization of Euclidean space [10]), so by defining \uf028 \uf029 \u03a6 x , the data can", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "The parameter \uf073 is width of RBF kernel, which is an important parameter in classification performance and can be determined by statistical computations or by iterative process to choose optimum value [1].", "startOffset": 200, "endOffset": 203}, {"referenceID": 0, "context": "For separable cases, C is infinity while for nonseparable cases, it may be varied, depending on the number of allowable errors in the trained solution: high C permits few errors while low C allows a higher proportion of errors in the solution [1,10].", "startOffset": 243, "endOffset": 249}, {"referenceID": 8, "context": "For separable cases, C is infinity while for nonseparable cases, it may be varied, depending on the number of allowable errors in the trained solution: high C permits few errors while low C allows a higher proportion of errors in the solution [1,10].", "startOffset": 243, "endOffset": 249}], "year": 2011, "abstractText": "Time-frequency methods for vibration-based gearbox faults detection have been considered the most efficient method. Among these methods, continuous wavelet transform (CWT) as one of the best time-frequency method has been used for both stationary and transitory signals. Some deficiencies of CWT are problem of overlapping and distortion of signals. In this condition, a large amount of redundant information exists so that it may cause false alarm or misinterpretation of the operator. In this paper a modified method called Exact Wavelet Analysis is used to minimize the effects of overlapping and distortion in case of gearbox faults. To implement exact wavelet analysis, Particle Swarm Optimization (PSO) algorithm has been used for this purpose. This method have been implemented for the acceleration signals from 2D acceleration sensor acquired by AdvantechTM PCI-1710 card from a gearbox test setup in Amirkabir University of Technology. Gearbox has been considered in both healthy and chipped tooth gears conditions. Kernelized Support Vector Machine (SVM) with radial basis functions has used the extracted features from exact wavelet analysis for classification. The efficiency of this classifier is then evaluated with the other signals acquired from the setup test. The results show that in comparison of CWT, PSO Exact Wavelet Transform has better ability in feature extraction in price of more computational effort. In addition, PSO exact wavelet has better speed comparing to Genetic Algorithm (GA) exact wavelet in condition of equal population because of factoring mutation and crossover in PSO algorithm. SVM classifier with the extracted features in gearbox shows very good results and its ability has been proved.", "creator": "Microsoft Word - ISME2010-3820-Changed Email.doc"}}}