{"id": "1605.06049", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "A Multi-Batch L-BFGS Method for Machine Learning", "abstract": "careful consideration of how companies prepare sequential random gradient descent ( sgd ) application has received much attention of the thousands. in this role, we focus significantly on linear methods often skip a sizeable fraction of the training set passing each startup to facilitate parallelism, and consequently utilizes second - base branching. in order to begin the continuation process, we follow a partial - batch approach in which smaller batch scales down each iteration. iteration inherently gives the algorithm a stochastic flavor that can influence instability in l - bfgs, even popular batch method computer machine evaluation. these difficulties arise because l - cv employs gradient estimation continuously update polynomial initial approximations ; when marginal gradients are folded into different data points unrelated implementation can errors performed. this paper shows how to perform remarkably reasonably - fast updating in linear multi - validation setting, replicate the behavior of the algorithm such a distributed array platform, and studies its slight advantage for constructing the convex regression dependent inputs.", "histories": [["v1", "Thu, 19 May 2016 16:53:50 GMT  (1382kb,D)", "http://arxiv.org/abs/1605.06049v1", "32 pages, 22 figures"], ["v2", "Sun, 23 Oct 2016 22:48:01 GMT  (1390kb,D)", "http://arxiv.org/abs/1605.06049v2", "NIPS 2016. 31 pages, 22 figures"]], "COMMENTS": "32 pages, 22 figures", "reviews": [], "SUBJECTS": "math.OC cs.LG stat.ML", "authors": ["albert s berahas", "jorge nocedal", "martin tak\u00e1c"], "accepted": true, "id": "1605.06049"}, "pdf": {"name": "1605.06049.pdf", "metadata": {"source": "CRF", "title": "A Multi-Batch L-BFGS Method for Machine Learning", "authors": ["Albert S. Berahas"], "emails": ["albertberahas@u.northwestern.edu", "j-nocedal@northwestern.edu", "takac.mt@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "It is common in machine learning to encounter optimization problems involving very large datasets with millions of parameters. To deal with the computational demands imposed by such applications, high performance implementations of stochastic gradient and batch quasi-Newton methods have been developed [1, 8, 6]. In this paper we study a batch approach based on the L-BFGS method [18] that strives to reach the right balance between efficient learning and productive parallelism.\nIn supervised learning, one seeks to minimize empirical risk,\nF (w) := 1\nn n\u2211 i=1 f(w;xi, yi) def = 1 n n\u2211 i=1 fi(w),\nwhere (xi, yi)ni=1 denote the training examples and f(\u00b7;x, y) : Rd \u2192 R is the composition of a prediction function (parametrized by w) and a loss function. The training problem consists of finding an optimal choice of the parameters w \u2208 Rd with respect to F , i.e., to compute a solution of the problem\nmin w\u2208Rd\nF (w) = 1\nn n\u2211 i=1 fi(w). (1.1)\nar X\niv :1\n60 5.\n06 04\n9v 1\n[ m\nat h.\nO C\nAt present, the preferred optimization method is the stochastic gradient descent (SGD) method [21, 3], and its variants [11, 22, 9], which are implemented either in an asynchronous manner (e.g. when using a parameter server in a distributed setting) or following a synchronous mini-batch approach that exploits parallelism in the gradient evaluation [2, 20, 10]. A drawback of the asynchronous approach is that it cannot use large batches, as this would cause updates to become too dense, and compromise the stability and scalability of the method [13, 20]. As a result, the algorithm spends more time in communication as compared to computation. On the other hand, using a synchronous mini-batch approach one can achieve a near-linear decrease in the number of SGD iterations as the mini-batch size is increased, up to a certain point after which not much speed-up can be obtained [24].\nAn alternative to SGD is a batch method, such as L-BFGS, which is able to reach high training accuracy and allows one to perform more computation per node, so as to achieve a better balance with communication costs [25]. Batch methods are, however, not as efficient learning algorithms as SGD in a sequential setting [4]. To benefit from the strength of both methods some high performance systems employ SGD at the start and later switch to a batch method [1].\nMulti-Batch Method. In this paper, we follow a different approach consisting of a single method that selects a sizeable subset (batch) of the training data to compute a step, and changes this batch at each iteration to improve the learning abilities of the method. We call this a multi-batch approach to differentiate it from the mini-batch approach used in conjunction with SGD, which employs a very small subset of the training data. When using large batches it is natural to employ a quasiNewton method, as incorporating second-order information imposes little computational overhead and improves the stability and speed of the method. We focus here on the L-BFGS method, which employs gradient information to update an estimate of the Hessian and computes a step inO(d) flops, where d is the number of variables. The multi-batch approach can, however, cause difficulties to LBFGS because this method employs gradient differences to update Hessian approximations. When the gradients used in these differences are based on different data points, the updating procedure can be unstable. Similar difficulties arise in a parallel implementation of the standard L-BFGS method, if some of the computational nodes devoted to the evaluation of the function and gradient are unable to return results on time\u2014as this again amounts to using different data points to evaluate the function and gradient at the beginning and the end of the iteration. The goal of this paper is to show that stable quasi-Newton updating can be achieved in both settings without incurring extra computational cost, or special synchronization. The key is to perform quasi-Newton updating based on the overlap between consecutive batches. The only restriction is that this overlap should not be too small, something that can be achieved in most situations.\nContributions. We describe a novel implementation of the batch L-BFGS method that is robust in the absence of sample consistency; i.e. when different samples are used to evaluate the objective function and its gradient at consecutive iterations. Our numerical experiments validate the method proposed in this paper \u2014 which we call the multi-batch L-BFGS method \u2014 and show that it achieves a good balance between computation and communication costs. We also analyze the convergence properties of the new method on both convex and nonconvex problems using a fixed steplength strategy (which is popular in practice)."}, {"heading": "2 The Robust Quasi-Newton Method", "text": "In a pure batch approach, one applies a gradient based method, such as L-BFGS [18], to the deterministic optimization problem (1.1). When the number n of training examples is large, it is natural to parallelize the evaluation of F and\u2207F by assigning the computation of the component functions fi to different processors. If this is done on a distributed platform, it is possible for some of the computational nodes to be slower than the rest. In this case, the contribution of the slow (or unresponsive) computational nodes could be ignored given the stochastic nature of the true objective function, which is expected (as opposed to empirical) risk. This leads, however, to an inconsistency in the objective function and gradient at the beginning and at the end of the iteration, which can be detrimental to quasi-Newton methods. Thus, we seek to find a fault-tolerant variant of the batch L-BFGS method that is capable of dealing with slow or unresponsive computational nodes.\nA similar challenge arises in a multi-batch implementation of the L-BFGS method in which the entire training set T = {(xi, yi)ni=1 is not employed at every iteration, but rather, a subset of the\ndata is used to compute the gradient. Specifically, we consider a method in which the dataset is randomly divided into a number of batches\u2014say 10, 50, or 100\u2014and the minimization is performed with respect to a different batch at every iteration. Specifically, at the k-th iteration, the algorithm chooses Sk \u2282 {1, . . . , n}, computes\nFSk(wk) = 1 |Sk| \u2211 i\u2208Sk fi (wk) , g Sk k := \u2207FSk(wk) = 1 |Sk| \u2211 i\u2208Sk \u2207fi (wk) , (2.2)\nand takes a step along the direction\u2212HkgSkk , whereHk is an approximation to\u22072F (wk)\u22121. Allowing the sample Sk to change freely at every iteration gives this approach flexibility of implementation and is beneficial to the learning process, as we show in Section 4. (We refer to Sk as the sample of training points, even though Sk only indexes those points.)\nThe case of unresponsive computational nodes and the multi-batch method are similar, from the point of view of quasi-Newton updating. The main difference is that node failures create unpredictable changes to the samples Sk, whereas a multi-batch method has control over sample generation. In either case, the algorithm employs a stochastic approximation to the gradient and can no longer be considered deterministic. We must, however, distinguish our setting from that of the classical SGD method, which employs small mini-batches and noisy gradient approximations. Our algorithm operates with much larger batches so that distributing the function evaluation is beneficial and the computing time of gSkk is not overwhelmed by communication costs. This gives rise to gradients with relatively small variance and justifies the use of a second-order method such as L-BFGS.\nRobust Quasi-Newton Updating. The difficulties created by the use of a different sample Sk at each iteration can be circumvented if consecutive samples Sk and Sk+1 overlap, so that Ok = Sk \u2229 Sk+1 6= \u2205. One can then perform stable quasi-Newton updating by computing gradient differences based on this overlap, i.e., by defining\nyk+1 = g Ok k+1 \u2212 gOkk , sk = wk+1 \u2212 wk, (2.3)\nin the notation given in (2.2). The correction pair (yk, sk) can then be used in the BFGS update. When the overlap set Ok is not too small, yk is a useful approximation of the curvature of the objective function F along the most recent displacement, and will lead to a productive quasi-Newton step. This observation is based on an important property of Newton-like methods, namely that there is much more freedom in choosing a Hessian approximation than in computing the gradient [5, 14]. Thus, a smaller sampleOk can be employed for updating the inverse Hessian approximationHk than for computing the batch gradient gSkk in the search direction\u2212HkgSkk . In summary, by ensuring that unresponsive nodes do not constitute the vast majority of all working nodes in a fault-tolerant parallel implementation, or by exerting a small degree of control over the creation of the samples Sk in the multi-batch method, one can design a robust method that naturally builds upon the fundamental properties of BFGS updating.\nWe should mention in passing that a commonly used fix for ensuring stability of quasi-Newton updating in machine learning is to enforce gradient consistency [23], i.e. to use the same sample Sk to compute gradient evaluations at the beginning and the end of the iteration. Another popular remedy is to use the same batch Sk for multiple iterations [17], e.g., from 3 to 20 iterations, alleviating the gradient inconsistency problem at the price of slower convergence, then move to a new batch and discard the cached optimization history. In this paper, we assume that such sample consistency is not possible (in the fault-tolerant case) or desirable (in the multi-batch method), and wish to design a new variant of L-BFGS that imposes minimal restrictions in the sample changes."}, {"heading": "2.1 Specification of the Method", "text": "At the k-th iteration, the multi-batch BFGS algorithm chooses a set Sk \u2282 {1, . . . , n} and computes a new iterate by the formula\nwk+1 = wk \u2212 \u03b1kHkgSkk , (2.4) where \u03b1k is the steplength, gSkk is the batch gradient (2.2) and Hk is the inverse BFGS Hessian matrix approximation that is updated at every iteration by means of the formula\nHk+1 = V T k HkVk + \u03c1ksks T k , \u03c1k = 1 yTk sk , Vk = I \u2212 \u03c1kyksTk .\nTo compute the correction vectors (sk, yk), we determine the overlap setOk = Sk\u2229Sk+1 consisting of the samples that are common at the k-th and k + 1-st iterations. We define\nFOk(wk) = 1 |Ok| \u2211 i\u2208Ok fi (wk) , \u2207FOk(wk) = gOkk = 1 |Ok| \u2211 i\u2208Ok \u2207fi (wk) ,\nand compute the correction vectors as in (2.3). In this paper we assume that \u03b1k is constant.\nIn the limited memory version, the matrix Hk is defined at each iteration as the result of applying m BFGS updates to a multiple of the identity matrix, using a set of m correction pairs {si, yi} kept in storage. The memory parameter m is typically in the range 2 to 20. When computing the matrixvector product in (2.4) it is not necessary to form that matrix Hk since one can obtain this product via the two-loop recursion [18], using the m most recent correction pairs {si, yi}. After the step has been computed, the oldest pair (sj , yj) is discarded and the new curvature pair is stored.\nA pseudo-code of the proposed method is given below. The parameter r denotes the percentage of samples in the dataset used to define the gradient, i.e., |S| = r\u00b7n100 . The parameter o denotes the length of overlap between consecutive samples, and is defined as a percentage of the number of samples in a given batch S, i.e., |O| = o\u00b7|S|100 .\nAlgorithm 1 Multi-Batch L-BFGS Input: w0 (initial iterate), T = {(xi, yi), for i = 1, . . . , n} (training set), m (memory parameter), r (batch, % of n), o (overlap, % of batch), k \u2190 0 (iteration counter).\n1: Create initial batch S0 . As shown in Firgure 1 2: for k = 0, 1, 2, ... do 3: Calculate the search direction pk = \u2212HkgSkk . Using L-BFGS formula 4: Choose the step length \u03b1k > 0 5: Compute wk+1 = wk + \u03b1kpk 6: Create the next batch Sk+1 7: Compute the curvature pairs sk+1 = wk+1 \u2212 wk and yk+1 = gOkk+1 \u2212 gOkk 8: Replace the oldest pair (si, yi) by sk+1, yk+1 9: end for"}, {"heading": "2.2 Sample Generation", "text": "We now discuss how the sample Sk+1 is created at each iteration (Line 6 in Algorithm 1).\nDistributed Computing with Faults. Consider a distributed implementation in which slave nodes read the current iterate wk from the master node, compute a local gradient on a subset of the dataset, and send it back to the master node for aggregation in the calculation (2.2). Given a time (computational) budget, it is possible for some nodes to fail to return a result. The schematic in Figure 1a illustrates the gradient calculation across two iterations, k and k + 1, in the presence of faults. Here Bi, i = 1, ..., B denote the batches of data that each slave node i receives (where T = \u222aiBi), and \u2207\u0303f(w) is the gradient calculation using all nodes that responded within the preallocated time. Let Jk \u2282 {1, 2, ..., B} be the index of all nodes that returned a gradient at the k-th iteration, and Jk+1 \u2282 {1, 2, ..., B} be corresponding set at iteration k + 1. Thus, Sk = \u222aj\u2208JkBj and Sk+1 = \u222aj\u2208Jk+1Bj , and we define Ok = \u222aj\u2208Jk\u2229Jk+1Bj .The simplest implementation in this setting preallocates the data on each compute node, which requires minimal data communication, i.e., only one data transfer. In this case the samples Sk will be independent if node failures occur randomly. On the other hand, if the same set of nodes are faulty, then sample creation will be biased, which is harmful both in theory and practice. One way to ensure independent sampling is to shuffle and redistribute the data to all notes after a certain number of iterations.\nMulti-batch Sampling. We propose two strategies for the multi-batch setting.\nFigure 1b illustrates the sample creation process in the first strategy. The dataset is shuffled and batches are generated by collecting subsets of the training set, in order. Every set (except S0) is of the form Sk = {Ok\u22121, Nk, Ok}, where Ok\u22121 and Ok are the overlapping sets with batches Sk\u22121 and Sk+1 respectively, and Nk are the samples that are unique to batch Sk. After each pass through the dataset, the samples are reshuffled, and the procedure described above is repeated. In our\nimplementation samples are drawn without replacement, guaranteeing that after every pass (epoch) all samples are used. This strategy has the advantage that it requires no extra computation in the evaluation of gOkk and g Ok k+1, but the samples {Sk} are not independent.\nThe second sampling strategy is simpler and requires less control. At every iteration k, the batch Sk is created by randomly selecting |Sk| elements from {1, . . . n}. The overlapping setOk is formed by randomly selecting |Ok| elements from Sk (subsampling). This strategy is slightly more expensive since gOkk+1 requires extra computation, but if the overlap is small this cost is not significant."}, {"heading": "3 Convergence Analysis", "text": "In this section, we analyze the convergence properties of the multi-batch L-BFGS method when applied to the minimization of strongly convex and nonconvex objective functions, using a fixed step length strategy. We assume that the goal is to minimize the empirical risk F given in (1.1), but note that a similar analysis could be used to study the minimization of the expected risk R."}, {"heading": "3.1 Strongly Convex case", "text": "Due to the stochastic nature of the multi-batch approach, every iteration of Algorithm 1 employs a gradient that contains errors that do not converge to zero. Therefore, by using a fixed step length strategy one cannot establish convergence to the optimal solution w?, but only convergence to a neighborhood of w? [16]. Nevertheless, this result is of interest, as it reflects the common practice of using a fixed step length, and decreasing it only if the desired testing error has not been achieved. It also illustrates the tradeoffs that arise between the size of the batch and the step length.\nIn our analysis, we make the following (fairly standard) assumptions about the objective function and the algorithm."}, {"heading": "Assumptions A.", "text": "1. F (w) is twice continuously differentiable. 2. There exist positive constants \u03bb\u0302 and \u039b\u0302 such that, \u03bb\u0302I \u22072FO(w) \u039b\u0302I , for all w \u2208 Rd, and\nall sets O \u2282 {1, 2, . . . , n} of cardinality o100 r\u00b7n100 . 3. There is a constant \u03b3 such that, ES [ \u2016\u2207FS(w)\u2016 ]2 \u2264 \u03b32, for all w \u2208 Rd, and all batches S \u2282 {1, 2, . . . , n} of cardinality r\u00b7n100 . 4. The samples S are drawn independently, and \u2207FS(w) is an unbiased estimator of the gradient \u2207F (w) for all w \u2208 Rd, i.e., ES [\u2207FS(w)] = \u2207F (w).\nNote that Assumption A2 implies that the entire Hessian\u22072F (w) also satisfies\n\u03bbI \u22072F (w) \u039bI, \u2200w \u2208 Rd,\nfor some constants \u03bb,\u039b > 0. Assuming that every sub-sampled function FO(w) is strongly convex is not unreasonable as a regularization term is commonly added in practice when that is not the case.\nWe begin by showing that the inverse Hessian approximations Hk generated by the multi-batch L-BFGS method have eigenvalues that are uniformly bounded above and away from zero. Lemma 3.1. If Assumptions A.1-A.2 above hold, there exist constants 0 < \u00b51 \u2264 \u00b52 such that the Hessian approximations {Hk} generated by Algorithm 1 satisfy\n\u00b51I Hk \u00b52I, for k = 0, 1, 2, . . .\nUtilizing Lemma 3.1, we show that the multi-batch L-BFGS method with a constant step length converges to a neighborhood of the optimal solution. Theorem 3.2. Suppose that Assumptions A.1-A.4 hold and let F (w?) = F ? denote the optimal value of F . Let {wk} be the iterates generated by Algorithm 1 method with \u03b1k = \u03b1 \u2208 (0, 12\u00b51\u03bb ), and starting from w0. Then for all k \u2265 0,\nE[F (wk)\u2212 F ?] \u2264 (1\u2212 2\u03b1\u00b51\u03bb)k[F (w0)\u2212 F ?] + [1\u2212 (1\u2212 \u03b1\u00b51\u03bb)k] \u03b1\u00b522\u03b3 2\u039b\n4\u00b51\u03bb\nk\u2192\u221e\u2212\u2212\u2212\u2212\u2192 \u03b1\u00b5 2 2\u03b3 2\u039b\n4\u00b51\u03bb .\nThe bound provided by this theorem has two components: (i) a term decaying linearly to zero, and (ii) a term identifying the neighborhood of convergence. Note that a larger step length yields a more favorable constant in the linearly decaying term, at the cost of an increase in the size of the neighborhood of convergence. We will consider again these tradeoffs in Section 4, where we also note that larger batches increase the opportunities for parallelism and improve the limiting accuracy in the solution, but slow down the learning abilities of the algorithm.\nOne can establish convergence of the multi-batch L-BFGS method to the optimal solution w? by employing a sequence of step lengths {\u03b1k} that converge to zero according to the schedule proposed by Robbins and Monro [21]. However, that provides only a sublinear rate of convergence, which is of little interest in our context where large batches are employed and some type of linear convergence is expected. In this light, Theorem 3.2 is more relevant to practice."}, {"heading": "3.2 Nonconvex case", "text": "The BFGS method is known to fail on noconvex problems [15, 7]. Even for L-BFGS, which makes only a small number of updates at each iteration, one cannot guarantee that the Hessian approximations have eigenvalues that are uniformly bounded above and away from zero. To establish convergence of the BFGS method in the nonconvex case cautious updating procedures have been proposed [12]. Here we employ a strategy that is well suited to our particular algorithm: we skip the update, i.e., set Hk+1 = Hk, if the curvature condition\nyTk sk \u2265 \u2016sk\u20162 (3.5) is not satisfied, where > 0 is a predetermined constant. Using said mechanism guarantees that the eigenvalues of the Hessian matrix approximations generated by the multi-batch L-BFGS method are bounded above and away from zero (Lemma 3.3). The analysis presented this section is based on the following assumptions."}, {"heading": "Assumptions B.", "text": "1. F (w) is twice continuously differentiable. 2. The gradients of F are \u039b-Lipschitz continuous and the gradients of FO are \u039bO-Lipschitz contin-\nuous, for all w \u2208 Rd, and all sets O \u2282 {1, 2, . . . , n} of cardinality o100 r\u00b7n100 . 3. The function F (w) is bounded below by a scalar F\u0302 . 4. There exist constants \u03b3 \u2265 0 and \u03b7 > 0 such that, ES [ \u2016\u2207FS(w)\u2016\n]2 \u2264 \u03b32 + \u03b7\u2016\u2207F (w)\u20162, for all w \u2208 Rd, and all batches S \u2282 {1, 2, . . . , n} of cardinality r\u00b7n100 . 5. The samples S are drawn independently, and \u2207FS(w) is an unbiased estimator of the true gradient \u2207F (w) for all w \u2208 Rd, i.e., E[\u2207FS(w)] = \u2207F (w).\nThe next result shows that the inverse Hessian approximations Hk generated by the multi-batch LBFGS method, with the cautious update scheme, have eigenvalues that are uniformly bounded above and away from zero. Lemma 3.3. Suppose that Assumptions B 1-2 hold and let > 0 be given. Let {Hk} be the Hessian approximations generated by Algorithm 1, with the modification that Hk+1 = Hk whenever (3.5) is not satisfied. Then, then there exist constants 0 < \u00b51 \u2264 \u00b52 such that\n\u00b51I Hk \u00b52I, for k = 0, 1, 2, . . .\nWe can now establish the following result about the behavior of the gradient norm for the multi-batch L-BFGS method with a cautious update strategy. Theorem 3.4. Suppose that Assumptions B 1-5 above hold and let > 0 be given. Let {wk} be the iterates generated by Algorithm 1, with \u03b1k = \u03b1 \u2208 (0, \u00b51\u00b522\u03b7\u039b ), initial point w0, and with the modification that Hk+1 = Hk whenever (3.5) is not satisfied. Then,\nE\n[ 1\nL L\u2211 k=1\n\u2016\u2207F (wk)\u20162 ] \u2264 \u03b1\u00b5 2 2\u03b3 2\u039b\n2\u00b51 \u2212 \u03b1\u00b522\u03b7\u039b + 2[F (w0)\u2212 F ?] L(2\u03b1\u00b51 \u2212 \u03b12\u00b522\u03b7\u039b)\n(3.6)\nL\u2192\u221e\u2212\u2212\u2212\u2212\u2192 \u03b1\u00b5 2 2\u03b3 2\u039b\n2\u00b51 \u2212 \u03b1\u00b522\u03b7\u039b . (3.7)\nThis result bounds the average norm of the gradient of F over the first L iterations, and shows that the iterates spend increasingly more time in regions where the objective function has a small gradient."}, {"heading": "4 Numerical Results", "text": "In this Section, we present numerical results that evaluate the proposed robust L-BFGS scheme (Algorithm 1) on logistic regression problems. Figure 2 shows its performance on the webspam1 dataset, where we compare it against three methods: (i) multi-batch L-BFGS without enforcing sample consistency (L-BFGS); here gradient differences are computed using different samples, i.e., yk = g Sk+1 k+1 \u2212gSkk ; (ii) multi-batch gradient descent (Gradient Descent), which is obtained by setting Hk = I in Algorithm 1; and, (iii) serial SGD, where at every iteration one sample is used to compute the gradient. We run each method for 10 different random seeds. For Algorithm 1, we report results for different batch (r%) and overlap (o%) sizes. The proposed method is more stable than the standard L-BFGS method; this is especially noticeable when r is small. On the other hand, serial SGD achieves similar accuracy as the robust L-BFGS method, and at a similar rate (e.g., r = 1%), at the cost of n communications per epochs versus 100/(1\u2212o) communications per epoch. Figure 2 also indicates that the robust L-BFGS method is not too sensitive to the size of overlap o%. Similar behavior was observed on other datasets, in regimes where r \u00b7owas not too small. We note in passing that the L-BFGS step was computed using the a vector-free implementation proposed in [6].\nWe also explore the performance of robust L-BFGS in the presence of node failures (faults), and compare it to the multi-batch variant that does not enforce sample consistency (L-BFGS). Figure 3 illustrates the performance of the methods on webspam dataset, for various probabilities of node failures p \u2208 {0.1, 0.3, 0.5}, and suggests that the robust L-BFGS variant is more stable.\n1LIBSVM: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html. webspam: n = 350, 000, d = 16, 609, 143.\nLastly, we study the strong and weak scaling properties of the robust L-BFGS method on an artificial dataset (Figure 4). For various values of r andK, we measure the time needed to compute a gradient (Gradient), and the time needed to compute and communicate the gradient (Gradient+C), as well as, the time needed to compute the L-BFGS direction (L-BFGS), and its associated communication overhead (L-BFGS+C). The figure on the left shows strong scaling of the proposed algorithm on a problem with n = 107 samples and d = 104 (randomly chosen sparsity, 160 non-zero elements). The size of input data was 24GB, and we varied the number of MPI processes, K \u2208 {1, 2, . . . , 128}. The time it takes to compute the gradient decreases with K, however, for small values of r, the communication is much more expensive than the time needed to compute the gradient. For large K, the local data is about 180MB; if r = 10%, only 18MB of data need to be processed per iteration, amounting to minimal computation. The figure on the right shows weak scaling of the proposed algorithm. The size of the problem is the same as before, but the sparsity level was varied. Each sample has 10 \u00b7 K non-zero elements, thus for any K the size of local problem is roughly 1.5GB (for K = 128 size of data 192GB). We observe almost constant time for the gradient computation while the cost of computing the L-BFGS direction decreases with K; however, if communication is considered, the overall time needed to compute the L-BFGS direction increases slightly. For K = 128 (192 GB of data) and r = 10%, almost 20GB of data are processed per iteration in below 0.1 second, which implies that one epoch would take around 1 second."}, {"heading": "5 Conclusion", "text": "This paper describes a novel variant of the L-BFGS method that is robust and efficient in two settings. The first occurs in the presence of node failures in a distributed computing implementation; the second arises when one wishes to employ a different batch at each iteration in order to accelerate learning. The proposed method avoids the pitfalls of using inconsitent gradient differences by performing quasi-Newton updating based on the overlap between consecutive samples. Numerical results show that the method is efficient in practice, and a convergence analysis illustrates its theoretical properties."}, {"heading": "A Proofs and Technical Results", "text": ""}, {"heading": "A.1 Assumptions", "text": "We first restate the Assumptions that we use in the Convergence Analysis section (Section 3). Assumption A and B are used in the strongly convex and nonconvex cases, respectively."}, {"heading": "Assumptions A", "text": "1. F (w) is twice continuously differentiable.\n2. There exist positive constants \u03bb\u0302 and \u039b\u0302 such that\n\u03bb\u0302I \u22072FO(w) \u039b\u0302I, (A.8)\nfor all w \u2208 Rd, and all sets O \u2282 {1, 2, . . . , n} of cardinality o100 r\u00b7n100 . 3. There is a constant \u03b3 such that,\nES [ \u2016\u2207FS(w)\u2016 ]2 \u2264 \u03b32, (A.9) for all w \u2208 Rd, and all batches S \u2282 {1, 2, . . . , n} of cardinality r\u00b7n100 .\n4. The samples S are drawn independently, and\u2207FS(w) is an unbiased estimator of the true gradient\u2207F (w) for all w \u2208 Rd, i.e.,\nE [ \u2207FS(w) ] = \u2207F (w). (A.10)"}, {"heading": "Assumptions B", "text": "1. F (w) is twice continuously differentiable.\n2. The gradients of F are \u039b-Lipschitz continuous and the gradients of FO are \u039bO-Lipschitz continuous, for all w \u2208 Rd, and all sets O \u2282 {1, 2, . . . , n} of cardinality o100 r\u00b7n100 .\n3. The function F (w) is bounded below by a scalar F\u0302 .\n4. There exist constants \u03b3 \u2265 0 and \u03b7 > 0 such that,\nES [ \u2016\u2207FS(w)\u2016 ]2 \u2264 \u03b32 + \u03b7\u2016\u2207F (w)\u20162, (A.11) for all w \u2208 Rd, and all batches S \u2282 {1, 2, . . . , n} of cardinality r\u00b7n100 .\n5. The samples S are drawn independently, and\u2207FS(w) is an unbiased estimator of the true gradient\u2207F (w) for all w \u2208 Rd, i.e.,\nE [ \u2207FS(w) ] = \u2207F (w). (A.12)\nA.2 Lemma 3.1 (A.1)\nThe following Lemma shows that the eigenvalues of the matrices generated by the multi-batch LBFGS method are bounded above and away from zero (strongly convex case). Lemma A.1. If Assumptions A.1-A.2 above hold, there exist constants 0 < \u00b51 \u2264 \u00b52 such that the Hessian approximations {Hk} generated by the multi-batch L-BFGS method satisfy\n\u00b51I Hk \u00b52I, for k = 0, 1, 2, . . .\nProof. Instead of analyzing the inverse Hessian approximation Hk, we study the direct Hessian approximation Bk = H\u22121k . In this case, the limited memory quasi-Newton updating formula is given as follows\n1. Set B(0)k = yTk yk sTk yk I and m\u0303 = min{k,m}; where m is the memory (L-BFGS).\n2. For i = 0, ..., m\u0303\u2212 1 set j = k \u2212 m\u0303+ 1 + i and compute\nB (i+1) k = B (i) k \u2212\nB (i) k sjs T j B (i) k\nsTj B (i) k sj\n+ yjy\nT j\nyTj sj .\n3. Set Bk+1 = B (m\u0303) k .\nThe curvature pairs sk and yk are updated via the following formulae\nyk+1 = g Ok k+1 \u2212 gOkk , sk = wk+1 \u2212 wk. (A.13)\nA consequence of Assumption A2 is that the eigenvalues of any sub-sampled Hessian (|O| samples) are bounded above and away from zero. Utilizing this consequence, the convexity of component functions and the definitions (A.13), we have\nyTk sk \u2265 1\n\u039b\u0302 \u2016yk\u20162 \u21d2 \u2016yk\u20162 yTk sk \u2264 \u039b\u0302 (A.14)\nOn the other hand, strong convexity of the sub-sampled functions, the consequence of Assumption A2 and definitions (A.13), provide a lower bound,\nyTk sk \u2264 1\n\u03bb\u0302 \u2016yk\u20162 \u21d2 \u2016yk\u20162 yTk sk \u2265 \u03bb\u0302 (A.15)\nCombining the upper and lower bounds (A.14) and (A.15)\n\u03bb\u0302 \u2264 \u2016yk\u2016 2\nyTk sk \u2264 \u039b\u0302. (A.16)\nThe above proves that the eigenvalues of the matrices B(0)k = yTk yk sTk yk\nI at the start of the L-BFGS update cycles are bounded above and away from zero, for all k. We now use a Trace-Determinant argument to show that the eigenvalues of Bk are bounded above and away from zero.\nLet Tr(B) and det(B) denote the trace and determinant of matrix B, respectively, and set ji = k \u2212 m\u0303+ i. Then,\nTr(Bk+1) = Tr(B (0) k )\u2212 Tr m\u0303\u2211 i=1 (B(i)k sjisTjiB(i)k sTjiB (i) k sji ) + Tr m\u0303\u2211 i=1 yjiy T ji yTjisji\n\u2264 Tr(B(0)k ) + m\u0303\u2211 i=1 \u2016yji\u20162 yTjisji\n\u2264 Tr(B(0)k ) + m\u0303\u039b\u0303 \u2264 C1, (A.17)\nfor some positive constant C1, where the inequalities above are due to (A.16), and the fact that the eigenvalues of the initial L-BFGS matrix B(0)k are bounded above and away from zero.\nUsing a result due to Powell [19], the determinant of the matrix Bk+1 generated by the multi-batch L-BFGS method can be expressed as,\ndet(Bk+1) = det(B (0) k ) m\u0303\u220f i=1\nyTjisji\nsTjiB (i\u22121) k sji\n= det(B (0) k ) m\u0303\u220f i=1 yTjisji sTjisji\nsTjisji\nsTjiB (i\u22121) k sji\n\u2265 det(B(0)k ) ( \u03bb\u0303 C1 )m\u0303 \u2265 C2, (A.18)\nfor some positive constant C2, where the above inequalities are due to the fact that the largest eigenvalue of B(i)k is less than C1 and (A.16).\nThe trace (A.17) and determinant (A.18) inequalities derived above imply that largest eigenvalues of all matrices Bk are bounded above, uniformly, and that the smallest eigenvalues of all matrices Bk are bounded away from zero, uniformly.\nA.3 Theorem 3.2 (A.2)\nUtilizing the result from Lemma 3.1, we now prove a linear convergence result to a neighborhood of the optimal solution, for the case where Assumptions A hold. Theorem A.2. Suppose that Assumptions A.1-A.4 above hold. Let {wk} be the iterates generated by the multi-batch L-BFGS method with\n\u03b1k = \u03b1 \u2208 (0, 1\n2\u00b51\u03bb ),\nwhere w0 is the starting point, and F (w?) = F ? is the optimal solution. Then for all k \u2265 0,\nE[F (wk)\u2212 F ?] \u2264 (1\u2212 2\u03b1\u00b51\u03bb)k[F (w0)\u2212 F ?] + [1\u2212 (1\u2212 \u03b1\u00b51\u03bb)k]\u03b1\u00b5 2 2\u03b3 2\u039b 4\u00b51\u03bb k\u2192\u221e\u2212\u2212\u2212\u2212\u2192 \u03b1\u00b5 2 2\u03b3 2\u039b 4\u00b51\u03bb\nProof. We have that\nF (wk+1) = F (wk \u2212 \u03b1Hk\u2207FSk(wk))\n\u2264 F (wk) +\u2207F (wk)T (\u2212\u03b1Hk\u2207FSk(wk)) + \u039b\n2 \u2016\u03b1Hk\u2207FSk(wk)\u20162\n\u2264 F (wk)\u2212 \u03b1\u2207F (wk)THk\u2207FSk(wk) + \u03b12\u00b522\u039b\n2 \u2016\u2207FSk(wk)\u20162, (A.19)\nwhere the first inequality arises due to the fact that the largest eigenvalue of F is \u039b and the second inequality arises because of the bounds on the Hessian matrix approximation.\nTaking the expectation (over Sk) of equation (A.19)\nESk [F (wk+1)] \u2264 F (wk)\u2212 \u03b1\u2207F (wk)THk\u2207F (wk) + \u03b12\u00b522\u039b\n2 ESk\n[ \u2016\u2207FSk(wk)\u2016 ]2 \u2264 F (wk)\u2212 \u03b1\u00b51\u2016\u2207F (wk)\u20162 + \u03b12\u00b522\u03b3 2\u039b\n2 , (A.20)\nwhere in the first inequality we make use of the unbiased nature of sample gradients, and the second inequality arises because of the bounds on the Hessian matrix approximation and the boundedness assumption of the norm of the sample gradient.\nSince F is \u03bb-strongly convex, we can use the following relationship between the norm of the gradient squared, and the distance from the k-th iterate to the optimal solution.\n2\u03bb[F (wk)\u2212 F ?] \u2264 \u2016\u2207F (wk)\u20162. Using the above with (A.20),\nESk [F (wk+1)] \u2264 F (wk)\u2212 \u03b1\u00b51\u2016\u2207F (wk)\u20162 + \u03b12\u00b522\u03b3 2\u039b\n2\n\u2264 F (wk)\u2212 2\u03b1\u00b51\u03bb[F (wk)\u2212 F ?] + \u03b12\u00b522\u03b3 2\u039b\n2 . (A.21)\nLet\n\u03c6k = E[F (wk)\u2212 F ?], (A.22) where the expectation is over all batches S0, S1, ..., Sk\u22121 and all history starting with w0. Thus (A.21) can be expressed as,\n\u03c6k+1 \u2264 (1\u2212 2\u03b1\u00b51\u03bb)\u03c6k + \u03b12\u00b522\u03b3 2\u039b\n2 , (A.23)\nfrom which we deduce that in order to reduce the value with respect to the previous function value, the step length needs to be in the range\n\u03b1 \u2208 ( 0, 1\n2\u00b51\u03bb\n) .\nSubtracting \u03b1\u00b5 2 2\u03b3 2\u039b 4\u00b51\u03bb from either side of (A.23) yields\n\u03c6k+1 \u2212 \u03b1\u00b522\u03b3 2\u039b\n4\u00b51\u03bb \u2264 (1\u2212 2\u03b1\u00b51\u03bb)\u03c6k +\n\u03b12\u00b522\u03b3 2\u039b\n2 \u2212 \u03b1\u00b5\n2 2\u03b3 2\u039b\n4\u00b51\u03bb = (1\u2212 2\u03b1\u00b51\u03bb) [ \u03c6k \u2212 \u03b1\u00b522\u03b3 2\u039b\n4\u00b51\u03bb\n] . (A.24)\nRecursive application of (A.24) yields\n\u03c6k \u2212 \u03b1\u00b522\u03b3 2\u039b\n4\u00b51\u03bb \u2264 (1\u2212 2\u03b1\u00b51\u03bb)k\n[ \u03c60 \u2212 \u03b1\u00b522\u03b3 2\u039b\n4\u00b51\u03bb\n] ,\nand thus,\n\u03c6k \u2264 (1\u2212 2\u03b1\u00b51\u03bb)k\u03c60 + [ 1\u2212 (1\u2212 \u03b1\u00b51\u03bb)k ]\u03b1\u00b522\u03b32\u039b 4\u00b51\u03bb . (A.25)\nFinally using the definition of \u03c6k (A.22) with the above expression yields the desired result, E [ F (wk)\u2212 F ? ] \u2264 ( 1\u2212 2\u03b1\u00b51\u03bb )k[ F (w0)\u2212 F ? ] + [ 1\u2212 (1\u2212 \u03b1\u00b51\u03bb)k ]\u03b1\u00b522\u03b32\u039b 4\u00b51\u03bb .\nA.4 Lemma 3.3 (A.3)\nThe following Lemma shows that the eigenvalues of the matrices generated by the multi-batch LBFGS method are bounded above and away from zero (nonconvex case). Lemma A.3. If Assumptions B 1-2 above hold, and the L-BFGS Hessian approximation update is skipped if\nyTk sk < \u2016sk\u20162, (A.26) then there exist constants 0 < \u00b51 \u2264 \u00b52 such that the Hessian approximations {Hk} generated by the multi-batch L-BFGS method satisfy\n\u00b51I Hk \u00b52I, for k = 0, 1, 2, . . .\nProof. Similar to the proof of Lemma 3.1, we study the direct Hessian approximation Bk = H\u22121k .\nThe curvature pairs sk and yk are updated via the following formulae\nyk+1 = g Ok k+1 \u2212 gOkk , sk = wk+1 \u2212 wk. (A.27)\nThe skipping mechanism (A.26) provides both an upper and lower bound on the quantity \u2016yk\u2016 2\nyTk sk ,\nwhich in turn ensures that the initial L-BFGS Hessian approximation is bounded above and away from zero. The lower bound is attained by repeated application of Cauchy\u2019s inequality to condition (3.5).\n\u2016sk\u20162 \u2264 yTk sk \u2264 \u2016yk\u2016\u2016sk\u2016,\nand\n\u2016yk\u2016 \u2265 \u2016sk\u2016 \u2016yk\u20162 \u2265 \u2016sk\u2016\u2016yk\u2016\n\u2265 sTk yk.\nRe-arranging the above expression yields the desired lower bound,\n\u2264 \u2016yk\u2016 2\nsTk yk . (A.28)\nThe upper bound is attained by the Lipschitz continuity of sample gradients,\nyTk sk \u2265 \u2016sk\u20162\n\u2265 \u2016yk\u2016 2\n\u039b2Ok ,\nRe-arranging the above expression yields the desired lower bound,\n\u2016yk\u20162 sTk yk \u2264 \u039b 2 Ok . (A.29)\nCombining (A.29) and (A.28),\n\u2264 \u2016yk\u2016 2 yTk sk \u2264 \u039b 2 Ok .\nThe above proves that the eigenvalues of the matrices B(0)k = yTk yk sTk yk\nI at the start of the L-BFGS update cycles are bounded above and away from zero, for all k. The rest of the proof follows the same trace-determinant argument as in the proof of Lemma 3.1.\nA.5 Theorem 3.4 (A.4)\nUtilizing the result from Lemma 3.3, we now prove a linear convergence result to a neighborhood of the optimal solution, for the case where Assumptions B hold. Theorem A.4. Suppose that Assumptions B 1-5 above hold. Let {wk} be the iterates generated by the multi-batch L-BFGS method with\n\u03b1k = \u03b1 \u2208 (0, \u00b51 \u00b522\u03b7\u039b ),\nwhere w0 is the starting point, and F (w?) = F ? is the optimal solution. Also, suppose that if\nyTk sk < \u2016sk\u20162, for some > 0, the inverse L-BFGS Hessian approximation is skipped, Hk+1 = Hk. Then, for all k \u2265 0,\nE[ 1\nL L\u2211 k=1 \u2016\u2207F (wk)\u20162] \u2264 \u03b1\u00b522\u03b3 2\u039b 2\u00b51 \u2212 \u03b1\u00b522\u03b7\u039b + 2[F (w0)\u2212 F ?] L(2\u03b1\u00b51 \u2212 \u03b12\u00b522\u03b7\u039b)\nL\u2192\u221e\u2212\u2212\u2212\u2212\u2192 \u03b1\u00b5 2 2\u03b3 2\u039b\n2\u00b51 \u2212 \u03b1\u00b522\u03b7\u039b .\nProof. Starting with (A.20),\nESk [F (wk+1)] \u2264 F (wk)\u2212 \u03b1\u00b51\u2016\u2207F (wk)\u20162 + \u03b12\u00b522\u039b\n2 ESk\n[ \u2016\u2207FSk(wk)\u2016 ]2 \u2264 F (wk)\u2212 \u03b1\u00b51\u2016\u2207F (wk)\u20162 + \u03b12\u00b522\u039b\n2 (\u03b32 + \u03b7\u2016\u2207F (w)\u20162)\n= F (wk)\u2212 \u03b1 ( \u00b51 \u2212 \u03b1\u00b522\u03b7\u039b\n2\n) \u2016\u2207F (wk)\u20162 + \u03b12\u00b522\u03b3 2\u039b\n2 ,\nwhere the second inequality holds due to Assumption B4. Taking an expectation over all batches S0, S1, ..., Sk\u22121 and all history starting with w0 yields\n\u03c6k+1 \u2212 \u03c6k \u2264 \u2212\u03b1 ( \u00b51 \u2212 \u03b1\u00b522\u03b7\u039b\n2\n) E\u2016\u2207F (wk)\u20162 + \u03b12\u00b522\u03b3 2\u039b\n2 , (A.30)\nwhere \u03c6k = E[F (wk)]. Summing (A.30) over the first L iterations L\u2211 k=0 [\u03c6k+1 \u2212 \u03c6k] \u2264 \u2212\u03b1 ( \u00b51 \u2212 \u03b1\u00b522\u03b7\u039b 2 ) L\u2211 k=0 E\u2016\u2207F (wk)\u20162 + L\u2211 k=0 \u03b12\u00b522\u03b3 2\u039b 2\n= \u2212\u03b1 ( \u00b51 \u2212 \u03b1\u00b522\u03b7\u039b\n2 ) E [ L\u2211 k=0 \u2016\u2207F (wk)\u20162 ] + \u03b12\u00b522\u03b3 2\u039bL 2 . (A.31)\nThe left-hand-side of the above inequality is a telescoping sum\nL\u2211 k=0 [\u03c6k+1 \u2212 \u03c6k] = \u03c6L+1 \u2212 \u03c60\n= E[F (wL+1)]\u2212 F (w0) \u2265 F ? \u2212 F (w0).\nSubstituting the above expression into (A.31) and re-arranging terms\nE [ L\u2211 k=0 \u2016\u2207F (wk)\u20162 ] \u2264 \u03b1\u00b5 2 2\u03b3 2\u039bL 2\u00b51 \u2212 \u03b1\u00b522\u03b7\u039b + 2[F (w0)\u2212 F ?] 2\u03b1\u00b51 \u2212 \u03b12\u00b522\u03b7\u039b .\nDividing the above equation by L completes the proof."}, {"heading": "B Extended Numerical Experiments - Real Datasets", "text": "In this Section, we present further numerical results, on the datasets listed in Table 1, in both the multi-batch and fault-tolerant settings. Note, that some of the datasets are too small, and thus, there is no reason to run them on a distributed platform; however, we include them as they are part of the standard benchmarking datasets.\nNotation. Let n denote the number of training samples in a given dataset, d the dimension of the parameter vector w, and K the number of MPI processes used. The parameter r denotes the percentage of samples in the dataset used to define the gradient, i.e., |S| = r\u00b7n100 . The parameter o denotes the length of overlap between consecutive samples, and is defined as a percentage of the number of samples in a given batch S, i.e., |O| = o\u00b7|S|100 .\nWe focus on logistic regression classification; the objective function is given by\nmin w\u2208Rd\nF (w) = 1\nn n\u2211 i=1 log(1 + e\u2212y i(wT xi)) + \u03c3 2 \u2016w\u20162,\nwhere (xi, yi)ni=1 denote the training examples, and \u03c3 = 1 n is the regularization parameter."}, {"heading": "B.1 Multi-batch L-BFGS Implementation", "text": "For the experiments in this section, we ran four methods:\n\u2022 Robust L-BFGS (Algorithm 1), \u2022 (L-BFGS) multi-batch L-BFGS without enforcing sample consistency; here gradient differ-\nences are computed using different samples, i.e., yk = g Sk+1 k+1 \u2212 gSkk ,\n\u2022 (Gradient Descent) multi-batch gradient descent; which is obtained by setting Hk = I in Algorithm 1, \u2022 (SGD) serial SGD; where at every iteration one sample is used to compute the gradient.\nIn Figures 5-13 we show the evolution of \u2016\u2207F (w)\u2016 for different step lengths \u03b1, and for various batch (r%) and overlap (o%) sizes. for Robust L-BFGS, L-BFGS, Gradient Descent and SGD on Non-Robust L-BFGS on the datasets listed in Table 1. Each Figure (5-13) consists of 10 plots that illustrate the performance of the methods with the following parameters:\n\u2022 Top 3 plots: \u03b1 = 1, o = 20% and r = 1%, 5%, 10% \u2022 Middle 3 plots: \u03b1 = 0.1, o = 20% and r = 1%, 5%, 10% \u2022 Bottom 4 plots: \u03b1 = 1, r = 1% and o = 5%, 10%, 20%, 30%\nAs is expected for quasi-Newton methods, robust L-BFGS performs best with a step-size \u03b1 = 1, for the most part."}, {"heading": "B.2 Fault-tolerant L-BFGS Implementation", "text": "If we run a distributed algorithm, for example on a shared computer cluster, then we may experience delays. Such delays can be caused by other processes running on the same compute node, node failures and for other reasons. As a result, given a computational (time) budget, these delays may cause nodes to fail to return a value. To illustrate this behavior, and to motivate the robust faulttolerant L-BFGS method, we run a simple benchmark MPI code on two different environments:\n\u2022 Amazon EC2 \u2013 Amazon EC2 is a cloud system provided by Amazon. It is expected that if load balancing is done properly, the execution time will have small noise; however, the network and communication can still be an issue. (4 MPI processes)\n\u2022 Shared Cluster \u2013 In our shared cluster, multiple jobs run on each node, with some jobs being more demanding than others. Even though each node has 16 cores, the amount of resources each job can utilize changes over time. In terms of communication, we have a GigaBit network. (11 MPI processes, running on 11 nodes)\nWe run a simple code on the cluster/cloud, with MPI communication. We generate two matrices A,B \u2208 Rn\u00d7n,then synchronize all MPI processes and compute C = A \u00b7B using the GSL C-BLAS library. The time is measured and recorded as computational time. After the matrix product is computed, the result is sent to the master/root node using asynchronous communication, and the time required to do the asynchronous communication is recorded. The process is repeated 3000 times.\nThe results of the experiment described above are captured in Figure 14. As expected, on the Amazon EC2 cloud, the matrix-matrix multiplication takes roughly the same time for all replications and the noise in communication is relatively small. In this example the cost of communication is negligible when compared to computation. On our shared cluster, one cannot guarantee that all resources are exclusively used on a specific process, and thus, the computation and communication time is considerably more stochastic and unbalanced. For some cases the difference between the minimum and maximum computation (communication) time varies by an order of magnitude or more. Hence, on such a platform, a fault-tolerant algorithm that would use information from nodes that returned an update within a preallocated budget is a natural choice.\nIn Figures 15-19 we show a comparison of the proposed robust L-BFGS method and the multibatch L-BFGS method that does not enforce sample consistency (L-BFGS). In these experiments, p denotes the probability that single node (MPI process) will not return a gradient evaluated on local data within a given time budget. We illustrate the performance of the methods for \u03b1 = 0.1 and p \u2208 {0.1, 0.2, 0.3, 0.4, 0.5}. We observe that the robust implementation is not affected much by the failure probability p."}, {"heading": "C Scaling of Robust L-BFGS Implementation", "text": "In this Section, we study the strong and weak scaling properties of the robust L-BFGS method on artificial data. For various values of r and K, we measure the time needed to compute a gradient (Gradient), and the time needed to compute and communicate the gradient (Gradient+C), as well as, the time needed to compute the L-BFGS direction (L-BFGS), and its associated communication overhead (L-BFGS+C)."}, {"heading": "C.1 Strong Scaling", "text": "Figure 20 depicts the strong scaling properties of our proposed algorithm. We generated a dataset with n = 107 and d = 104, where each sample had 160 non-zero elements (dataset size 24GB). We ran our code for different values of r (different batch sizes Sk), with K = 1, 2, . . . , 128 number of MPI processes.\nOne can observe that the compute time for the gradient and the L-BFGS direction decreases as K is increased. However, when communication time is considered, the combined cost increases slightly as K is increased. Notice, that even when r = 10% (i.e., we process 10% of all samples in one iteration) the amount of local work is not sufficient to overcome the communication cost."}, {"heading": "C.2 Weak Scaling - Fixed Problem Dimension, Increasing Data Size", "text": "In order to illustrate the weak scaling properties of the algorithm, we generated a data-matrix X \u2208 R10\n7\u00d7104 , and ran it on a shared cluster with K = 1, 2, 4, 8, . . . , 128 MPI processes. For a given number of MPI processes (K), each sample had 10\u00b7K non-zero elements. Effectively, the dimension of the problem was fixed, but sparsity of the data was increased as more MPI processes were used. The size of input was 1.5 \u00b7K GBs (i.e. 1.5GB per MPI process); for K = 128 we generated 192GB of data.\nThe compute time for the gradient is almost constant, this is because the amount of work per MPI process (rank) is almost identical; see Figure 21. On the other hand, because we are using a VectorFree L-BFGS implementation [6] for computing the L-BFGS direction, the amount of time needed for each node to compute the L-BFGS direction is decreasing asK is increased. However, increasing K does lead to larger communication overhead, which can be seen in Figure 21.\nC.3 Increasing Problem Dimension, Fixed Data Size, Fixed K\nIn this experiment, we investigate the effect of a change in the dimension d of the problem, to the performance of the algorithm. We fixed the size of data (29GB) and the number of MPI processes\nK = 8. We generated data with n = 107 samples, where each sample had 200 non-zero elements. Figure 22 shows that increasing the dimension d has a mild effect on the computation time of the gradient, while the effect on the time needed to compute the L-BFGS direction is more apparent. However, if communication time is taken into consideration, the time required for the gradient computation and the L-BFGS direction computation increase as d is increased."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>The question of how to parallelize the stochastic gradient descent (SGD) method<lb>has received much attention in the literature. In this paper, we focus instead on<lb>batch methods that use a sizeable fraction of the training set at each iteration to<lb>facilitate parallelism, and that employ second-order information. In order to im-<lb>prove the learning process, we follow a multi-batch approach in which the batch<lb>changes at each iteration. This inherently gives the algorithm a stochastic flavor<lb>that can cause instability in L-BFGS, a popular batch method in machine learning.<lb>These difficulties arise because L-BFGS employs gradient differences to update<lb>the Hessian approximations; when these gradients are computed using different<lb>data points the process can be unstable. This paper shows how to perform stable<lb>quasi-Newton updating in the multi-batch setting, illustrates the behavior of the<lb>algorithm in a distributed computing platform, and studies its convergence proper-<lb>ties for both the convex and nonconvex cases.", "creator": "TeX"}}}