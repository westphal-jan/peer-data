{"id": "1605.06296", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "On the Robustness of Decision Tree Learning under Label Noise", "abstract": "in most practical problems of supervised learning, predominantly random data escapes from explicit label cancellation. eventually, it is excellent... check how likely is a learning algorithm of compute label noise. experimentally, decision trees could typically found to be more robust against label loops than fewer and faster validation. additional paper presents complete theoretical results to show if decision problem algorithms are robust to analyzing data noise under the assumption of large sample complexity. we also achieve some sample complexity results for this robustness. between extensive simulations we prove this robustness.", "histories": [["v1", "Fri, 20 May 2016 11:31:26 GMT  (32kb)", "https://arxiv.org/abs/1605.06296v1", null], ["v2", "Fri, 26 Aug 2016 08:58:06 GMT  (58kb)", "http://arxiv.org/abs/1605.06296v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aritra ghosh", "naresh manwani", "p s sastry"], "accepted": false, "id": "1605.06296"}, "pdf": {"name": "1605.06296.pdf", "metadata": {"source": "CRF", "title": "On the Robustness of Decision Tree Learning under Label Noise", "authors": ["Aritra Ghosh", "Naresh Manwani"], "emails": ["aritraghosh.iem@gmail.com", "nareshmanwani@gmail.com", "sastry@ee.iisc.ernet.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n06 29\n6v 2\n[ cs\n.L G\n] 2\nKeywords: Robust learning, Decision trees, Label noise"}, {"heading": "1. Introduction", "text": "Decision tree is among the most widely used machine learning approaches (Wu et al., 2007). Interpretability, applicability to all types of features, less demands on data pre-processing and scalability are some of the reasons for its popularity. In general, decision tree is learnt in a top down greedy fashion where, at each node, a split rule is learnt by minimizing some objective function.\nFor learning a decision tree classifier, we make use of labeled training data. When the class labels in the training data may be incorrect, it is referred to as label noise. Subjectivity and other errors in human labeling, measurement errors, insufficient feature space are some of the main reasons behind label noise. In many large data problems, labeled samples are often obtained through crowd sourcing and the unreliability of such labels is another reason for label noise. Learning from positive and unlabeled samples can also be cast as a problem of learning under label noise (du Plessis et al., 2014). Thus, learning classifiers in the presence of label noise is an important problem (Fre\u0301nay and Verleysen, 2014). It is generally accepted that among all the classification methods, decision tree is probably closest to \u2018off-the-shelf\u2019 method which has all the desirable properties including robustness to outliers (Hastie et al., 2005).\nWhile there are many results about generalization bounds for decision trees (Mansour and McAllester, 2000; Kearns and Mansour, 1998), not many theoretical results are known about the robustness of decision tree learning in presence of label noise. It is observed that label noise in\nc\u00a9 A. Ghosh, N. Manwani & P.S. Sastry.\nthe training data increases size of the learnt tree; detecting and removing noisy examples improves the learnt tree (Brodley and Friedl, 1999). Recently, Nettleton et al. (2010) empirically studied robustness of different classifiers under label noise. While decision tree learning is better than SVM or logistic regression in terms of robustness to label noise, it is also seen that naive Bayes is more robust than decision trees. In this paper, we present a theoretical study of such robustness properties of decision trees.\nRecently, many analytical results are reported on robust learning of classifiers, using the framework of risk minimization. The robustness or noise tolerance of risk minimization depends on the loss function used. Long and Servedio (2010) proved that any convex potential loss is not robust to uniform or symmetric label noise. Another result is that some of the standard convex losses are not robust to symmetric label noise while the 0-1 loss is (Manwani and Sastry, 2013). It is noted by du Plessis et al. (2014) that convex surrogates losses are not good for learning from positive and unlabeled data. A general sufficient condition on the loss function for risk minimization to be robust is derived in (Ghosh et al., 2015). The 0-1 loss, sigmoid loss and ramp loss are shown to satisfy this condition while convex losses such as hinge loss (used in SVM) and the logistic loss do not satisfy this condition. Interestingly, it is possible to have a convex loss (which is not a convex potential) that satisfies this sufficient condition and the corresponding risk minimization essentially amounts to a highly regularized SVM (van Rooyen et al., 2015). Robust risk minimization strategies under the so called class-conditional (or asymmetric) label noise are also proposed (Natarajan et al., 2013; Scott et al., 2013). Some sufficient conditions for robustness of risk minimization under 0-1 loss, ramp loss and sigmoid loss when the training data is corrupted with most general non-uniform label noise are also presented in (Ghosh et al., 2015). None of these results are applicable for decision trees because the popular decision tree learning algorithms cannot be cast as risk minimization.\nIn this paper, we analyze learning of decision trees under label noise. We consider some of the popular impurity function based methods for learning of decision trees. We show, in the large sample limit, that under symmetric or uniform label noise the split rule that optimizes the objective function under noisy data is the same as that under noise-free data. We explain how this results in the learning algorithm being robust to label noise, under the assumption that the number of samples at every node is large. We also derive some sample complexity bounds to indicate how large a sample we need at a node. We also explain how these results indicate robustness of random forest also. We present empirical results to show that trees learnt with noisy data give accuracies that are comparable with those learnt with noise-free data. We also show empirically that the random forests algorithm is robust to label noise. For comparison we also present results obtained with SVM algorithm."}, {"heading": "2. Label Noise and Decision Tree Robustness", "text": "In this paper, we only consider binary decision trees for binary classification. We use the same notion of noise tolerance as in (Manwani and Sastry, 2013; van Rooyen et al., 2015)."}, {"heading": "2.1. Label Noise", "text": "Let X \u2282 Rd be the feature space and let Y = {1,\u22121} be the class labels. Let S = {(x1, yx1), (x2, yx2), . . . , (xN , yxN )} \u2208 (X \u00d7Y)N be the ideal noise-free data drawn iid from\na fixed but unknown distribution D over X\u00d7Y. The learning algorithm does not have access to this data. The noisy training data given to the algorithm is S\u03b7 = {(xi, y\u0303xi), i = 1, \u00b7 \u00b7 \u00b7 , N}, where y\u0303xi = yxi with probability (1 \u2212 \u03b7xi) and y\u0303xi = \u2212yxi with probability \u03b7xi . As a notation, for any x, yx denotes its \u2018true\u2019 label while y\u0303x denotes the noisy label. Thus, \u03b7x = Pr[yx 6= y\u0303x | x]. We use D\u03b7 to denote the joint probability distribution of x and y\u0303x.\nWe say that the noise is uniform or symmetric if \u03b7x = \u03b7, \u2200x. Note that, under symmetric noise, a sample having wrong label is independent of the feature vector and the \u2018true\u2019 class of the sample. Noise is said to be class conditional or asymmetric if \u03b7x = \u03b7+, for all patterns of class +1 and \u03b7x = \u03b7\u2212, for all patterns of class \u22121. When noise rate \u03b7x is a general function of x, it is termed as non-uniform noise. Note that the value of \u03b7 is unknown to the learning algorithm."}, {"heading": "2.2. Criteria for Learning Split Rule at a Node of Decision Trees", "text": "Most decision tree learning algorithms grow the tree in top down fashion starting with all training data at the root node. At any node, the algorithm selects a split rule to optimize a criterion and uses that split rule to split the data into the left and right children of this node; then the same process is recursively applied to the children nodes till the node satisfies the criterion to become a leaf. Let F denote a set of split rules. Suppose, a split rule f \u2208 F at a node v, sends a fraction a of the samples at v to the left child vl and the remaining fraction (1\u2212 a) to the right child vr. Then many algorithms select a f \u2208 F to maximize a criterion C(f) = G(v)\u2212 (aG(vl) + (1\u2212 a)G(vr)) (1) where G(\u00b7) is a so called impurity measure. There are many such impurity measures. Of the samples at any node v, suppose a fraction p are of positive class and a fraction q = (1\u2212 p) are of negative class. Then the gini impurity is defined by GGini = 2pq (Breiman et al., 1984); entropy based impurity is defined as GEntropy = \u2212p log p\u2212 q log q (Quinlan, 1986); and misclassification impurity is defined as GMC = min{p, q}. Often the criterion C is called the gain. Hence, we also use gainGini(f) to refer to C(f) when G is GGini and similarly for other impurity measures.\nA split criterion different from impurity is twoing rule, first proposed by Breiman et al. (1984). Consider a split rule f at a node v. Let pl (pr), ql (qr) be the fraction of positive and negative class samples at the left (right) child vl (vr). (We have, apl + (1 \u2212 a)pr = p, aql + (1 \u2212 a)qr = q, p and q are the fractions for parent node v). Then twoing rule selects f \u2208 F which maximizes GTwoing(f) = a(1\u2212 a)[|pl \u2212 pr|+ |ql \u2212 qr|]2/4."}, {"heading": "2.3. Noise Tolerance of Decision Tree", "text": "By noise tolerance we desire the following. A decision tree learnt with noisy labels in training data should have the same test error (on noise-free test set) as that of the tree learnt using noise-free training data. One way of achieving such robustness is if the decision tree learning algorithm learns the same tree in presence of label noise as it would learn with noise free data.1 Since label noise is random, on any specific noise-corrupted training data, the tree learnt would also be random. Hence, we say the learning method is robust if, in\n1. For simplicity, we do not consider pruning of the tree.\nthe limit as training set size goes to infinity, the algorithm learns the same tree with noisy as well as noise-free training data. We then argue that this implies we learn the same tree (with a high probability) if given sufficient number of samples. We also provide sample complexity results for this. Below, we formalize this notion.\nDefinition 1 A split criterion C is said to be noise-tolerant if\nargmin f\u2208F C(f) = argmin f\u2208F\nC\u03b7(f)\nwhere C(f) is the value of the split criterion C for a split rule f \u2208 F on noise free data and C\u03b7(f) is the value of the criterion function for f on noisy data, in the limit as the data size goes to infinity.\nLet the decision tree learnt from training sample S be represented as LearnTree(S) and let the classification of any x by this tree be represented as LearnTree(S)(x).\nDefinition 2 A decision tree learning algorithm LearnTree is said to be noise-tolerant if the probability of misclassification, under the noise-free distribution, of the tree learnt with noisy samples is same as that learnt with noise-free samples. That is,\nPD(LearnTree(S)(x) 6= yx) = PD(LearnTree(S\u03b7)(x) 6= yx)\nNote that for the above to hold it is sufficient if LearnTree(S) is same as LearnTree(S\u03b7)."}, {"heading": "3. Theoretical Results", "text": "Robustness of decision tree learning requires the robustness of the split criterion at each non-leaf node and robustness of the labeling rule at each leaf node. We consider each of these in turn."}, {"heading": "3.1. Robustness Of Split Rules", "text": "As mentioned earlier, most decision tree algorithms select a split rule, f , by maximizing C(f) defined by (1). Hence we are interested in comparing, for any specific f , the value of C(f) with its value, in the large sample limit, when labels are flipped under symmetric label noise.\nLet the noise-free samples at a node v be {(xi, yi), i = 1, \u00b7 \u00b7 \u00b7 , n}. Under label noise, the samples at this node would become {(xi, y\u0303i), i = 1, \u00b7 \u00b7 \u00b7 , n}. Suppose in the noise-free case a split rule f sends nl of these n samples to the left child, vl, and nr = n\u2212 nl to right child, vr. Note that a split rule is a function of only the feature vector. (For example, in an oblique decision tree the split rule could be: send a x to left child if wTx+ w0 > 0). Since the split rule depends only on the feature vector x and not the labels, the points that go to vl and vr would be the same for the noisy samples also. Thus, nl and a = nl/n would be same in both cases. However, what changes with label noise are the class labels on examples and hence the number of examples of different classes at a node.\nLet n+ and n\u2212 = n \u2212 n+ be the number of samples of the two classes at node v in the noise-free case. Similarly, let n+l and n \u2212 l = nl \u2212 n+l be the number of samples of the two classes at vl and define n + r , n \u2212 r similarly. Let the corresponding quantities in the noisy\ncase be n\u0303+, n\u0303\u2212, n\u0303+l , n\u0303 \u2212 l etc. Define random variables, Zi, i = 1, \u00b7 \u00b7 \u00b7 , n by Zi = 1 if y\u0303i 6= yi and Zi = 0 otherwise. Thus, Zi are indicators of whether or not label on the i th example is corrupted. By definition of symmetric label noise, Zi are iid Bernoulli random variables with expectation \u03b7.\nLet p = n+/n, q = n\u2212/n = (1 \u2212 p) be the fractions of the two classes at v under noisefree samples. Let pl, ql and pr, qr be these fractions for vl and vr. Let the corresponding quantities for the noisy samples case be p\u0303, q\u0303, p\u0303l, q\u0303l etc. Let p\n\u03b7, q\u03b7 be the values of p\u0303, q\u0303 in the large sample limit and similarly define p\u03b7l , q \u03b7 l ,p \u03b7 r , q \u03b7 r .\nThe value of n\u0303+ is the number of i such that y\u0303i = +1. Similarly, the value of n + l would\nbe the number of i such that xi is in vl and y\u0303i = +1. Hence we have\np\u0303 = n\u0303+\nn =\n1\nn\n\n\n\u2211\ni:y\u0303i=+1\n1\n\n = 1\nn\n\n\n\u2211\ni:yi=+1\n(1\u2212 Zi) + \u2211\ni:yi=\u22121\nZi\n\n (2)\np\u0303l = n\u0303+l nl = 1 nl\n\n\n\u2211\ni:xi\u2208vl,y\u0303i=+1\n1\n\n = 1\nnl\n\n\n\u2211\ni:xi\u2208vl,yi=+1\n(1\u2212 Zi) + \u2211\ni:xi\u2208vl,yi=\u22121\nZi\n\n (3)\nAll the above expressions involve sums of independent random variables. Hence the values of the above quantities in the large sample limit can be calculated, by laws of large numbers, by essentially replacing each Zi by its expected value. Thus, from the above, we get\np\u03b7 = p(1\u2212 \u03b7) + q\u03b7 = p(1\u2212 2\u03b7) + \u03b7; p\u03b7l = pl(1\u2212 \u03b7) + ql\u03b7 = pl(1 \u2212 2\u03b7) + \u03b7 (4) We emphasize here that, under symmetric label noise, the corruption of label is independent of feature vector and true label and thus we have Pr[Zi = 1] = Pr[Zi = 1|yi] = Pr[Zi = 1|xi \u2208 B, yi] = \u03b7, for any subset B of the feature space. We have used this fact in deriving the eq.(4). Comparing the expressions for p\u03b7 and p\u03b7l , we see that, essentially, at any node (in the large sample limit) the fraction of examples whose labels are corrupted is the same. This is intuitively clear because under symmetric label noise the corruption of class label does not depend on the feature vector.\nTo find the large sample limit of criterion C(f) under label noise, we need values of the impurity function in the large sample limit which in turn needs p\u03b7, q\u03b7, p\u03b7l etc. which are as given above. For example, the Gini impurity is given by G(v) = 2pq for the noise free case. For the noisy sample, its value can be written as G\u0303(v) = 2p\u0303q\u0303. Its value in the large sample limit would be G\u03b7(v) = 2p\u03b7q\u03b7. Another way this can be seen is as follows. Using eq.(2) one can show that E\u03b7[p\u0303q\u0303] = p \u03b7q\u03b7 \u2212 \u03b7(1\u2212\u03b7)n which is p\u03b7q\u03b7 as n goes to infinity.\nUsing the above we can now prove the following theorem about robustness of split criteria.\nTheorem 3 Splitting criterion based on gini impurity, mis-classification rate and twoing rule are noise-tolerant (as per definition 1) to symmetric label noise given \u03b7 6= 0.5.\nProof\nAs in the above, let p and q be the fractions of the two classes at v. For any split f , let a be the fraction of points at the left child (vl). Recall from above that the fraction a is\nsame for noisy and noise-free data. \u2022 Gini Impurity For a node v, the gini impurity is Ggini(v) = 2pq. Under symmetric label noise, gini impurity (under large sample limit) becomes (using eq.(4)),\nG\u03b7 Gini (v) = 2p\u03b7q\u03b7 = 2[((1 \u2212 2\u03b7)p + \u03b7)((1 \u2212 2\u03b7)q + \u03b7)] = 2pq(1\u2212 2\u03b7)2 + (\u03b7 \u2212 \u03b72) = GGini(v)(1 \u2212 2\u03b7)2 + (\u03b7 \u2212 \u03b72)\nSimilar expressions hold for G\u03b7 Gini (vl) and G \u03b7 Gini (vr). The (large sample) value of criterion or impurity gain of f under label noise can be written as\ngain\u03b7Gini(f) = G \u03b7 Gini(v)\u2212 [a G \u03b7 Gini(vl) + (1\u2212 a)G \u03b7 Gini(vr)]\n= (1\u2212 2\u03b7)2[GGini(v)\u2212 a GGini(vl)\u2212 (1\u2212 a)Gini(vr)] = (1\u2212 2\u03b7)2gainGini(f)\nThus for any \u03b7 6= 0.5, if gainGini(f1) > gainGini(f2), then gain \u03b7 Gini (f1) > gain\u03b7 Gini (f2). Which means that a maximizer of impurity gain based on gini index under noise-free samples will be also a maximizer of gain under symmetric label noise, under large sample limit.\n\u2022Misclassification rate For node v, misclassification impurity is, GMC(v) = min{p, q}. Under symmetric label noise with \u03b7 < 0.5, in the large sample limit, value of impurity is, (using eq.(4)),\nG\u03b7MC(v) = min{p \u03b7, q\u03b7} = min{(1 \u2212 2\u03b7)p + \u03b7, (1 \u2212 2\u03b7)q + \u03b7}\n= (1\u2212 2\u03b7)GMC(v) + \u03b7\nIn presence of symmetric label noise, expected impurity gain for a split f can be written as\ngain\u03b7MC(f) = G \u03b7 MC(v)\u2212 [a G \u03b7 MC(vl) + (1\u2212 a)G \u03b7 MC(vr)]\n= (1\u2212 2\u03b7)[G\u03b7MC(v)\u2212 a G \u03b7 MC(vl)\u2212 (1\u2212 a)G \u03b7 MC(vr)] = (1\u2212 2\u03b7)gainMC(f)\nwhere (1\u22122\u03b7) > 0 because we are considering the case \u03b7 < 0.5. When \u03b7 > 0.5, one can similarly show that gain\u03b7\nMC (f) = (2\u03b7 \u2212 1)gainmc(f). This completes proof of noise-tolerance\nof impurity based on misclassification rate.\n\u2022 Twoing rule Using the same notation defined Sec 2.2 for twoing criterion, for a split f , objective can be rewritten as\nGTwoing(f) = a(1\u2212 a)\n4\n[ |pl \u2212 pr|+ |ql \u2212 qr| ]2 = a(1\u2212 a)[pl \u2212 pr]2\nWhen there is symmetric label noise, p\u03b7l = (1\u2212 2\u03b7)pl + \u03b7 and p \u03b7 r = (1\u2212 2\u03b7)pr + \u03b7.\nG\u03b7Twoing(f) = a(1\u2212 a)[p \u03b7 l \u2212 p\u03b7r ]2 = a(1\u2212 a)(1\u2212 2\u03b7)2[pl \u2212 pr]2\n= (1\u2212 2\u03b7)2GTwoing(f)\nThus, the maximizer of twoing rule does not change when there is symmetric label noise.\nThe above theorem shows that impurity gain (using gini or misclassification rate) based criteria are noise-tolerant for symmetric label noise as per Definition 1.\nRemark 4 Impurity based on entropy Another popular criterion is impurity gain based on entropy which is not considered in the above theorem. The impurity gain based on entropy is not noise-tolerant as per definition 1 as shown by the following counterexample.\nConsider a case where a node has n samples (n is large). Suppose, under split rule f1 we get nl = nr = 0.5n, n + l = 0.05n and n + r = 0.25n. Suppose there is another split rule f2 under which we get nl = 0.3n and nr = 0.7n with n + l = 0.003n and n + r = 0.297n. Then it can be easily shown that gainEntropy(f1) < gainEntropy(f2); but, under symmetric label noise with \u03b7 = 40%, gain\u03b7 Entropy (f2) < gain \u03b7 Entropy (f1).\nHowever, we would like to emphasize that the above example may be a non-generic one. In large number of simulations we have seen that the split rule that maximizes the criterion is same under noisy and noise-free cases. Thus, impurity gain based on entropy for learning decision trees is also fairly robust to label noise."}, {"heading": "3.2. Robustness of Labeling Rule at Leaf Nodes", "text": "We next consider the robustness of criterion to assign a class label to a leaf node. A popular approach is to take majority vote at the leaf node. We prove that, majority voting is robust to symmetric label noise in the sense that (in the large sample limit) the fraction of positive examples would be more under label noise if the fraction of positive examples is higher in noise-free case. We also show that it can be robust to non-uniform noise also under a restrictive condition.\nTheorem 5 Let \u03b7x < 0.5,\u2200x. (a). Then, majority voting at a leaf node is robust to symmetric label noise. (b). It is also robust to nonuniform label noise if all the points at the leaf node belong to one class in the noise free data.\nProof\nLet p and q = 1\u2212 p be the fraction of positive and negative samples at leaf node v. (a) Under symmetric label noise, the relevant fractions are p\u03b7 = (1 \u2212 \u03b7)p + \u03b7q and q\u03b7 = (1\u2212 \u03b7)q + \u03b7p. Thus, p\u03b7 \u2212 q\u03b7 = (1\u2212 2\u03b7)(p \u2212 q). Since \u03b7 < 0.5, (p\u03b7 \u2212 q\u03b7) will have the same sign as (p\u2212 q), proving robustness of the majority voting.\n(b) Let v contain all the points from the positive class. Thus, p = 1, q = 0. Let x1, \u00b7 \u00b7 \u00b7 ,xn be the samples at v. Under non-uniform noise (with \u03b7x < 0.5,\u2200x),\np\u03b7 = 1\nn\nn \u2211\ni=1\n(1\u2212 \u03b7xi) > 0.5\nn\nn \u2211\ni=1\n1 = 0.5 (5)\nThus, the majority vote will assign positive label to the leaf node v. This proves the second part of the theorem."}, {"heading": "3.3. Robustness of Decision Tree Learning Under Symmetric Label Noise : Large Sample Analysis", "text": "We have proved that some of the popular split criteria are noise-tolerant. What we have shown is that the split rule that maximizes the criterion under noise-free samples is same as that which maximizes the value of criterion under symmetric label noise (under large sample limit). This means, under large sample assumption, the same split rule would be learnt at any node irrespective of whether the labels come from noise-free data or noisy data. (Here we assume for simplicity that there is a unique split rule maximizing the criterion at each node. Otherwise we need some prefixed rule to break ties).2\nOur result for leaf node labeling implies that, under large sample assumption, with majority rule a leaf node would get the same label under noisy or noise-free data. To conclude that we learn the same tree, we need to examine the rule for deciding when a node becomes a leaf. If this is determined by the depth of the node or number of samples at the node then it is easy to see that the same tree would be learnt with noisy and noise-free data. In many algorithms one makes a node as leaf if no split rule gives positive value to the gain. This will also lead to learning of the same tree with noisy samples as with noise-free samples, because we showed that the gain under noisy case is a linear function of the gain under noise-free case.\nRemark 6 Robustness under general noise: In our analysis so far, we have only considered symmetric label noise. In the simplest case of asymmetric noise, namely, classconditional noise, noise rate is same for all feature vectors of a class though it may be different for different classes. In the risk minimization framework, class conditional noise can be taken care when the noise rates are known (or can be estimated) (Natarajan et al., 2013; Scott et al., 2013; Ghosh et al., 2015). We can extend the analysis presented in Sec.3.1 to relate expected fraction of examples of a class in the noisy and noise-free cases using the two noise rates. Thus, if the noise rates are assumed known (or can be reliably estimated) it should be possible to extend the analysis here to the case of class-conditional noise. In the general case when noise rates are not known (and cannot be reliably estimated), it appears difficult to establish robustness of impurity based split criteria."}, {"heading": "3.4. Sample Complexity under Noise", "text": "We established robustness of decision tree learning algorithms under large sample limit. Hence an interesting question is that of how large the sample size should be for our assertions about robustness to hold with a large probability. We provide some sample complexity bounds in this subsection. (Proofs of Lemmas 7 and 8 are given in Appendix).\nLemma 7 Let leaf node v have n samples. Under symmetric label noise with \u03b7 < 0.5, majority voting will not fail with probability at least 1 \u2212 \u03b4 when n \u2265 2\n\u03c12(1\u22122\u03b7)2 ln(1\u03b4 ), where\n\u03c1 is the difference between fraction of positive and negative samples in the noise-free case.\n2. Here we are assuming that the xi at the node are same in the noisy and noise-free cases. These are same at the root. If in the two cases we learn the same split at the root, then at both its children the samples would be same in the noise and noise-free cases and so on.\nThe sample size needed increases with increasing \u03b7, which is intuitive. It also increases with decreasing \u03c1. The value of \u03c1 tells us the \u2018margin of majority\u2019 in the noise-free case and hence when \u03c1 is small we should expect to need more examples in the noisy case.\nLemma 8 Let there be n samples at a non-leaf node v and given two splits f1 and f2, suppose gain (gini, misclassification, twoing rule) for f1 is higher than that for f2. Under symmetric label noise with \u03b7 6= 0.5, gain from f1 will be higher with probability 1 \u2212 \u03b4 when n \u2265 O( 1\u03c12(1\u22122\u03b7)2 ln( 1 \u03b4 )), where \u03c1 denotes the difference between gain of the two splits in the noise-free case.\nWhile these results, shed some lights on sample complexity, we emphasize that these bounds are loose and are obtained using concentration inequalities. Also we want to point out, large sample in leaf implies large sample in non-leaf nodes. In practice, sample size needed is not high. In experimental section, we provide results on how many training samples are needed for robust learning of decision trees on a synthetic dataset."}, {"heading": "3.5. Noise Robustness in Random Forest", "text": "A random forest (Breiman, 2001) is a collection of randomized tree classifiers. We represent the set of trees as gn = {gn(x, \u03c01), \u00b7 \u00b7 \u00b7 , gn(x, \u03c0m)}. Here \u03c01, \u00b7 \u00b7 \u00b7 , \u03c0m are iid random variables, conditioned on data, which are used for partitioning the nodes. Finally, majority vote is taken among the random tree classifiers for prediction. We denote this classifier as g\u0304n.\nIn a purely random forest classifier, partitioning does not depend on the class labels. At each step, a node is chosen randomly and a feature is selected randomly for the split. A split threshold is chosen uniformly randomly from the interval of the selected feature. This procedure is done k times. A greedily grown random forest classifier is a set of randomized tree classifiers. Each tree is grown greedily by improving impurity with some randomization. At each node, a random subset of features are chosen. Tree is grown by computing the best split among those random features only. Breiman\u2019s random forest classifier uses gini impurity gain (Breiman, 2001).\nRemark 9 A purely random forest classifier/ greedily grown random forest, g\u0304n, is robust to symmetric label noise with \u03b7 < 0.5 under large sample assumption.\nWe need to show each randomized tree is robust to label noise in both cases. In purely random forest, randomization is on the partitions and the partitions do not depend on class labels (which may be noisy). We proved robustness of majority vote at leaf nodes under symmetric label noise. Thus, for a purely random forest, g\u0304\u2217\u03b7 = g\u0304\u2217. That is, the classifier learnt with noisy labels would be same as that learnt with noise-free samples. Similarly for a greedily grown trees with gini impurity measure, we showed that each tree is robust because of both split rule robustness and majority voting robustness. Thus when large sample assumption holds, greedily grown random forest will also be robust to symmetric label noise.\nRemark 10 Sample complexity of Random forest: Empirically we observe that, often random forest has better robustness than a single decision tree in finite sample cases. For a classifier, generalization error can be written as,\nerrorgen = errorbias + errvariance + \u03c3 2 noise\nUnder symmetric label noise, errorbias is same for single decision tree as well as random forest. Thus generalization error is controlled by errorvariance. If pairwise correlation of each trees is \u03c1 and variance is \u03c32 for each tree, then random forest, consisting N trees, has variance, (Hastie et al., 2005)\nerrorvariance = \u03c1\u03c3 2 + 1\u2212 \u03c1 N \u03c32\nIntuitively, if a single decision tree is learnt with noisy samples, our results imply that its classification decision on a new point would be same as noise free case in an expected sense. If we have many independent decision trees, variance in the classification will decrease. If the decision trees are highly correlated, then the variance reduction might not be significant."}, {"heading": "4. Empirical Illustration", "text": "In this section, we illustrate our robustness results for learning of decision trees and random forest. We also present results with SVM. While, SVM has been proved to be non-robust even under symmetric label noise, its sensitivity towards noise widely varies (Long and Servedio, 2010; Nettleton et al., 2010; Manwani and Sastry, 2013; van Rooyen et al., 2015). We also provide results on sample complexity for robust learning of decision trees and random forest."}, {"heading": "4.1. Dataset Description", "text": "We used four 2D synthetic datasets. Details are given below. (Here n denotes total number of samples, p+, p\u2212 represent the class conditional densities, and U(A) denotes uniform distribution over set A).\n\u2022 Dataset 1: Checker board 2by2 Pattern: Data uniform over [0, 2]\u00d7 [0, 2] and one class region being ([0, 1] \u00d7 [0, 1]) \u222a ([1, 2] \u00d7 [1, 2]) and n = 30000\n\u2022 Dataset 2: Checker board 4by4 Pattern: Extension of the above to a 4\u00d7 4 grid.\n\u2022 Dataset 3: Imbalance Linear Data. p+ = U([0, 0.5]\u00d7[0, 1]) and p\u2212 = U([0.5, 1]\u00d7[0, 1]). Prior probabilities of classes are 0.9 & 0.1, and n = 40000.\n\u2022 Dataset 4: Imbalance and Asymmetric Linear Data. p+ = U([0, 0.5] \u00d7 [0, 1]) and p\u2212 = U([0.5, 0.7] \u00d7 [0.4, 0.6]). Prior probabilities are 0.8 & 0.2, and n = 40000.\nWe also present results for 6 UCI datasets (Lichman, 2013)."}, {"heading": "4.2. Experimental Setup", "text": "We used decision tree implementation in scikit learn library (Pedregosa et al., 2011). We present results only with gini impurity based decision tree classifier. (We observed that decision trees learnt using twoing rule and misclassification rate have similar performance). For random forest classifier (RF) we used scikit learn library. Number of trees in random forest was set to 100. For SVM we used libsvm package (Chang and Lin, 2011).\nIn subsection 4.3 we present results to illustrate sample complexity for robust learning where training set size and size of leaf nodes is varied as explained there.\nIn subsection 4.4, we compare accuracies of decision tree learning, random forest and SVM for which the following setup is used. Minimum leaf size is the only user-chosen parameter in random forest and decision trees. For synthetic datasets, minimum samples in leaf node was restricted to 250. For UCI datasets, it was restricted to 50. For SVM, we used linear kernel (l) for Synthetic Datasets 3, 4 and quadratic kernel (p) for Checker board 2by2 data. In all other datasets we used gaussian kernel (g). For SVM, we selected hyper-parameters using validation data. (Validation range for C is 0.01-500 and for \u03b3 in the Gaussian kernel it is 0.001-10). We used 20% data for testing and 20% for validation. Symmetric label noise was varied from 0% \u2212 40%. As synthetic datasets are separable, we also experimented with class conditional noise with the two noise rates for the two classes being 40% and 20%. In all experiments, noise was introduced only on training and validation data. Test set was noise free."}, {"heading": "4.3. Effect of sample size on robustness of learning", "text": "Here we discuss sensitivity of decision tree learning (under label noise) on sample size. We present experimental results on the test accuracy for different sample sizes using the 2by2 checker board data.\nTo study effect of sample size in leaf nodes, we choose a leaf sample size and learn decision tree and random forest with different noise levels. (The training set size is fixed at 20000). We do this for a number of choices for leaf sample size. The test accuracies in all these cases are shown in Figure 1(a). As can be seen from the figure, even when training data size is huge, we do not get robustness if leaf sample size is small. This is in accordance with our analysis (as in Lemma 7) because minimum sample size is needed for the majority rule to be correct with a large probability. A leaf sample size of 50 seems sufficient to take care of even 30% noise. As expected, random forest has better robustness.\nNext we experiment with varying the (noisy) training data size. The results are shown in Figure 1(b). It can be seen that with 400/4000 sample size decision tree learnt has good test accuracy (95%) at 20%/40% noise (the sample ratio is close to (1\u22122\u00d70.4) 2\n(1\u22122\u00d70.2)2 = 1/9 as\nprovided in lemma. 7). We need larger sample size for higher level of noise. This is also as expected from our analysis."}, {"heading": "4.4. Comparison of accuracies of learnt classifiers", "text": "The average test accuracy and standard deviation (over 10 runs) on different data sets under different levels of noise are shown in Table 1 for synthetic datasets and in Table 2 for UCI datasets. In table 2 we also indicate the dimension of feature vector (d), the number of positive and negative samples in the data (n+, n\u2212).\nFor synthetic datasets the sample sizes are large and hence we expect good robustness. As can be seen from Table 1, for noise-free data, decision tree, random forest and SVM have all similar accuracies. However, with 30% or 40% noise, the accuracies of SVM are much poorer than those of decision tree and random forest. For example on datasets 3 and 4, the accuracies of decision tree and random forest continue to be 99% even at 40% noise while those of SVM drop to about 90% and 80% respectively. This illustrates the robustness of\ndecision tree learning as indicated by our analysis. It can be seen that decision tree and random forest are robust to class conditional noise also, even without knowledge about noise rate (as indicated by last column in the table). Our current analysis does not prove this robustness; as remarked earlier, this is one possible extension of the theoretical analysis presented here.\nSimilar performance is seen on UCI data sets also as shown in Table 2. For breast cancer dataset, accuracy of decision tree also drops with noise while for random forest the drop is significantly less. This is also expected because the total sample size here is less. Although SVM has significantly higher accuracy than decision tree in 0% noise, at 40% noise its accuracy drops more than that of decision tree. In all other data sets also, decision tree and random forest are more robust than SVM as can be seen from the table.\nAs explained earlier, our analysis shows that decision tree learning is robust in large sample case. Thus, though decision tree learning may not be robust to label noise when training set size is small, the robustness improves with increasing training set size. This is demonstrated by our results on synthetic data sets. However, this is not true of a standard algorithm such as SVM. For example, Datasets 3 and 4 represent very simple two dimensional problems. Though we have 40000 samples here, SVM does not learn well under label noise. On the other hand, the accuracies of decision tree and random forest at 30% noise are as good as their accuracies at 0% noise and these accuracies are very high."}, {"heading": "5. Conclusion", "text": "In this paper, we investigated the robustness of decision tree learning under label noise. In many current applications one needs to take care of label noise in training data. Hence, it is very desirable to have learning algorithms that are not affected by label noise. Since most impurity based top-down decision tree algorithms learns split rules based on fractions of positive and negative samples at a node, one can expect that they should have some\nrobustness. We proved that decision tree algorithms based on gini or misclassification impurity and the twoing rule algorithm are all robust to symmetric label noise. We showed that, under large sample assumption, with a high probability, the same tree would be learnt with noise-free data as with noisy data. We also provided some sample complexity results for the robustness. Through extensive empirical investigations we illustrated the robust learning of decision tree and random forest. Decision tree approach is very popular in many practical applications. Hence, the robustness results presented in this paper are\ninteresting. All the results we proved are for symmetric noise. Extending these results to class conditional and non-uniform noise is an important direction for future research."}, {"heading": "Appendix A. Sample Complexity Bounds", "text": "Proof [of Lemma 7] Let n+ and n\u2212 denote the positive and negative samples at the node under noise-free case. (Note n = n+ + n\u2212). Without loss of generality assume that positive class is in majority and hence, by definition, \u03c1 = (n+ \u2212 n\u2212)/n. Let n\u0303+ and n\u0303\u2212 be the positive and negative samples under the noisy case.\nLet Xi, i = 1, \u00b7 \u00b7 \u00b7 , n+ be random variables with Pr[Xi = 1] = 1 \u2212 Pr[Xi = 0] = \u03b7. Let Xi, i = n\n+ + 1, \u00b7 \u00b7 \u00b7 , n be random variables with Pr[Xi = \u22121] = 1 \u2212 Pr[Xi = 0] = \u03b7. Let Sn = \u2211n i=1 Xi. Then, under symmetric label noise, we have n\u0303p \u2212 n\u0303n = (np \u2212 nn)\u2212 2Sn = \u03c1n\u2212 2Sn. Also, note that ESn = \u03b7n+ \u2212 \u03b7n\u2212 = \u03b7\u03c1n. Now we have\nPr[n\u0303+ \u2212 n\u0303\u2212 < 0] = Pr[\u03c1n \u2212 2Sn < 0] = Pr[2Sn \u2212 2ESn > \u03c1n(1\u2212 2\u03b7)]\n\u2264 exp ( \u2212\u03c1 2n(1\u2212 2\u03b7)2\n2\n)\nwhere the last line follows from hoeffding\u2019s inequality. If we want this probability to be less than \u03b4 then we would need n > 2\n\u03c12(1\u22122\u03b7)2 ln(1\u03b4 ). This completes the proof.\nProof [Of Lemma 8] Lets assume parent node v contains n samples whereas left child vl (right child vr) contains nl = na (nr = n \u2212 na) samples. Note under noise, for a split rule f at node v, for both parent as well as child, these numbers remain same as noise free case. For a\nparent node v, suppose, p (p\u0303) and q (q\u0303) are the positive and negative fraction under noisefree (noisy) data with n samples. Similarly pl, ql, p\u0303l, q\u0303l (pr, qr, p\u0303r, q\u0303r) is defined for left (and right child). Thus under symmetric label noise \u03b7, we can write for any node (note, E\u03b7(p\u0303) = p \u03b7), Pr[|p\u0303\u2212 p\u03b7| > \u01eb] \u2264 2e\u22122n\u01eb2 (6) We want to bound how finite sample estimates of different impurity gain differs from the large sample assumption (or the expectation). We use \u01eb1, \u01eb2 and \u01eb3 to denote the finite sample error (from the expectation) for positive fraction in parent, left and right child respectively (note this in turn bounds negative fraction also). We set \u01eb1 = \u01eb, \u01eb2 = \u01eb/ \u221a a and\n\u01eb3 = \u01eb/ \u221a 1\u2212 a. The probability can be upper bounded using hoeffding bound in eq. (6) as,\nPr [ ( |p\u0303\u2212p\u03b7| \u2265 \u01eb1 ) \u222a ( |p\u0303l\u2212p\u03b7l | \u2265 \u01eb2 ) \u222a ( |p\u0303r\u2212p\u03b7r | \u2265 \u01eb3 ) ] \u2264 2(e\u22122n\u01eb21+e\u22122nl\u01eb22+e\u22122nr\u01eb23) = 6e\u22122n\u01eb2 (7) Note that, this probability does not depend on any split and can be applied to any arbitrary split. Also note, for twoing rule, first term is not required in RHS and LHS. Given the complement of this event (lets call it as \u2018all fractions are \u01eb-accurate\u2019 event), we compute how finite sample impurity gain deviates from the large sample limit.\n\u2022 Gini Impurity: For a node v, after some simplification, using eq. 6,7, we can bound the finite sample noise estimate as (for gini G\u0303 = 2p\u0303q\u0303),\n|p\u0303q\u0303 \u2212 p\u03b7q\u03b7| \u2264 |\u01eb(p\u03b7 \u2212 q\u03b7)|\nThus we can bound finite noisy sample gain from gini impurity as,\n| \u02c6gain\u03b7Gini(f)\u2212 gain \u03b7 Gini (f)| \u2264 2|\u01eb1(p\u03b7 \u2212 q\u03b7)|+ 2a|\u01eb2(p\u03b7l \u2212 q \u03b7 l )|+ 2(1\u2212 a)|\u01eb3(p\u03b7r \u2212 q\u03b7r )|\n\u2264 2(1 \u2212 2\u03b7) [ |\u01eb1(p\u2212 q)|+ a|\u01eb2(pl \u2212 ql)|+ (1\u2212 a)|\u01eb3(pr \u2212 qr)| ]\n\u2264 2(1 \u2212 2\u03b7)[|\u01eb1(p\u2212 q)|+ |a\u01eb2|+ |(1\u2212 a)\u01eb3|] \u2264 6(1 \u2212 2\u03b7)\u01eb\nUnder noise free case, we assume the difference of gini gain between two splits is \u03c1. Under noise corrupted signal label, expected difference is \u03c1\u03b7 = (1\u2212 2\u03b7)2\u03c1.\nSetting \u01eb = \u03c1\u03b7/12(1 \u2212 2\u03b7) = \u03c1(1\u2212 2\u03b7)/12 for both the splits in eq. 7, we get the upper bound on probability of ordering change as, 12e\u2212n\u03c1 2(1\u22122\u03b7)2/72.\n\u2022Misclassification Impurity: For misclassification impurity, for a node v, we have\n|min(p\u0303, q\u0303)\u2212min(p\u03b7, q\u03b7)| \u2264 |\u01eb|\nThus we can bound finite noisy sample gain for misclassification impurity as,\n| \u02c6gain\u03b7MC(f)\u2212 gain \u03b7 MC (f)| \u2264 |\u01eb1|+ a|\u01eb2|+ (1\u2212 a)|\u01eb3|\n\u2264 |\u01eb|+ |\u01eb \u221a a|+ |\u01eb \u221a 1\u2212 a| \u2264 3\u01eb\nIf \u03c1 is the difference in gain in noise free case, under noise, difference in gain becomes, \u03c1(1\u2212 2\u03b7). Thus we can set \u01eb = \u03c1(1\u2212 2\u03b7)/6 in eq. 7 for both of the splits to get the probability bound.\n\u2022Twoing Rule: Similarly for twoing rule we bound the gain assuming \u2018all fractions are \u01eb-accurate\u2019 event. We get, after simplification,\n|G\u0302\u03b7Twoing(f)\u2212G \u03b7 Twoing(f)| \u2264 a(1\u2212 a)(|\u01eb2 \u2212 \u01eb3|)(|p \u03b7 l \u2212 p\u03b7r |)\n\u2264 (1\u2212 2\u03b7)(|\u01eb(1 \u2212 a) \u221a a|+ |\u01eba \u221a 1\u2212 a|)(|pl \u2212 pr|) \u2264 (1\u2212 2\u03b7) 2 (|\u01eb|+ |\u01eb|) \u2264 (1\u2212 2\u03b7)\u01eb\nNote \u221a a \u221a 1\u2212 a \u2264 1/2. Under noise, difference of gain becomes (1\u2212 2\u03b7)2\u03c1. Here we can set \u01eb = \u03c1(1\u2212 2\u03b7)/2 to bound the probability of ordering change.\nThus for all cases, required sample size in parent node is n \u2265 O( 1 \u03c12(1\u22122\u03b7)2 ln(1\u03b4 ))"}], "references": [{"title": "Classification and Regression Trees", "author": ["L. Breiman", "J. Friedman", "R. Olshen", "C. Stone"], "venue": null, "citeRegEx": "Breiman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Breiman et al\\.", "year": 1984}, {"title": "Identifying mislabeled training data", "author": ["Carla E. Brodley", "Mark A. Friedl"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Brodley and Friedl.,? \\Q1999\\E", "shortCiteRegEx": "Brodley and Friedl.", "year": 1999}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Analysis of learning from positive and unlabeled data", "author": ["Marthinus C du Plessis", "Gang Niu", "Masashi Sugiyama"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Plessis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Plessis et al\\.", "year": 2014}, {"title": "Classification in the presence of label noise: a survey", "author": ["Ben\u00f4\u0131t Fr\u00e9nay", "Michel Verleysen"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "Fr\u00e9nay and Verleysen.,? \\Q2014\\E", "shortCiteRegEx": "Fr\u00e9nay and Verleysen.", "year": 2014}, {"title": "Making risk minimization tolerant to label", "author": ["Aritra Ghosh", "Naresh Manwani", "PS Sastry"], "venue": "noise. Neurocomputing,", "citeRegEx": "Ghosh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2015}, {"title": "The elements of statistical learning: data mining, inference and prediction", "author": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman", "James Franklin"], "venue": "The Mathematical Intelligencer,", "citeRegEx": "Hastie et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2005}, {"title": "A fast, bottom-up decision tree pruning algorithm with near-optimal generalization", "author": ["Michael J Kearns", "Yishay Mansour"], "venue": "In ICML,", "citeRegEx": "Kearns and Mansour.,? \\Q1998\\E", "shortCiteRegEx": "Kearns and Mansour.", "year": 1998}, {"title": "Random classification noise defeats all convex potential boosters", "author": ["Philip M Long", "Rocco A Servedio"], "venue": "Machine Learning,", "citeRegEx": "Long and Servedio.,? \\Q2010\\E", "shortCiteRegEx": "Long and Servedio.", "year": 2010}, {"title": "Generalization bounds for decision trees", "author": ["Yishay Mansour", "David A McAllester"], "venue": "In COLT,", "citeRegEx": "Mansour and McAllester.,? \\Q2000\\E", "shortCiteRegEx": "Mansour and McAllester.", "year": 2000}, {"title": "Noise tolerance under risk minimization", "author": ["Naresh Manwani", "PS Sastry"], "venue": "Cybernetics, IEEE Transactions on,", "citeRegEx": "Manwani and Sastry.,? \\Q2013\\E", "shortCiteRegEx": "Manwani and Sastry.", "year": 2013}, {"title": "Learning with noisy labels", "author": ["Nagarajan Natarajan", "Inderjit S Dhillon", "Pradeep K Ravikumar", "Ambuj Tewari"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Natarajan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Natarajan et al\\.", "year": 2013}, {"title": "A study of the effect of different types of noise on the precision of supervised learning techniques", "author": ["David F Nettleton", "Albert Orriols-Puig", "Albert Fornells"], "venue": "Artificial intelligence review,", "citeRegEx": "Nettleton et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nettleton et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "Thus, learning classifiers in the presence of label noise is an important problem (Fr\u00e9nay and Verleysen, 2014).", "startOffset": 82, "endOffset": 110}, {"referenceID": 6, "context": "It is generally accepted that among all the classification methods, decision tree is probably closest to \u2018off-the-shelf\u2019 method which has all the desirable properties including robustness to outliers (Hastie et al., 2005).", "startOffset": 200, "endOffset": 221}, {"referenceID": 9, "context": "While there are many results about generalization bounds for decision trees (Mansour and McAllester, 2000; Kearns and Mansour, 1998), not many theoretical results are known about the robustness of decision tree learning in presence of label noise.", "startOffset": 76, "endOffset": 132}, {"referenceID": 7, "context": "While there are many results about generalization bounds for decision trees (Mansour and McAllester, 2000; Kearns and Mansour, 1998), not many theoretical results are known about the robustness of decision tree learning in presence of label noise.", "startOffset": 76, "endOffset": 132}, {"referenceID": 1, "context": "the training data increases size of the learnt tree; detecting and removing noisy examples improves the learnt tree (Brodley and Friedl, 1999).", "startOffset": 116, "endOffset": 142}, {"referenceID": 10, "context": "Another result is that some of the standard convex losses are not robust to symmetric label noise while the 0-1 loss is (Manwani and Sastry, 2013).", "startOffset": 120, "endOffset": 146}, {"referenceID": 5, "context": "A general sufficient condition on the loss function for risk minimization to be robust is derived in (Ghosh et al., 2015).", "startOffset": 101, "endOffset": 121}, {"referenceID": 11, "context": "Robust risk minimization strategies under the so called class-conditional (or asymmetric) label noise are also proposed (Natarajan et al., 2013; Scott et al., 2013).", "startOffset": 120, "endOffset": 164}, {"referenceID": 5, "context": "Some sufficient conditions for robustness of risk minimization under 0-1 loss, ramp loss and sigmoid loss when the training data is corrupted with most general non-uniform label noise are also presented in (Ghosh et al., 2015).", "startOffset": 206, "endOffset": 226}, {"referenceID": 1, "context": "the training data increases size of the learnt tree; detecting and removing noisy examples improves the learnt tree (Brodley and Friedl, 1999). Recently, Nettleton et al. (2010) empirically studied robustness of different classifiers under label noise.", "startOffset": 117, "endOffset": 178}, {"referenceID": 1, "context": "the training data increases size of the learnt tree; detecting and removing noisy examples improves the learnt tree (Brodley and Friedl, 1999). Recently, Nettleton et al. (2010) empirically studied robustness of different classifiers under label noise. While decision tree learning is better than SVM or logistic regression in terms of robustness to label noise, it is also seen that naive Bayes is more robust than decision trees. In this paper, we present a theoretical study of such robustness properties of decision trees. Recently, many analytical results are reported on robust learning of classifiers, using the framework of risk minimization. The robustness or noise tolerance of risk minimization depends on the loss function used. Long and Servedio (2010) proved that any convex potential loss is not robust to uniform or symmetric label noise.", "startOffset": 117, "endOffset": 766}, {"referenceID": 1, "context": "the training data increases size of the learnt tree; detecting and removing noisy examples improves the learnt tree (Brodley and Friedl, 1999). Recently, Nettleton et al. (2010) empirically studied robustness of different classifiers under label noise. While decision tree learning is better than SVM or logistic regression in terms of robustness to label noise, it is also seen that naive Bayes is more robust than decision trees. In this paper, we present a theoretical study of such robustness properties of decision trees. Recently, many analytical results are reported on robust learning of classifiers, using the framework of risk minimization. The robustness or noise tolerance of risk minimization depends on the loss function used. Long and Servedio (2010) proved that any convex potential loss is not robust to uniform or symmetric label noise. Another result is that some of the standard convex losses are not robust to symmetric label noise while the 0-1 loss is (Manwani and Sastry, 2013). It is noted by du Plessis et al. (2014) that convex surrogates losses are not good for learning from positive and unlabeled data.", "startOffset": 117, "endOffset": 1043}, {"referenceID": 10, "context": "We use the same notion of noise tolerance as in (Manwani and Sastry, 2013; van Rooyen et al., 2015).", "startOffset": 48, "endOffset": 99}, {"referenceID": 0, "context": "Then the gini impurity is defined by GGini = 2pq (Breiman et al., 1984); entropy based impurity is defined as GEntropy = \u2212p log p\u2212 q log q (Quinlan, 1986); and misclassification impurity is defined as GMC = min{p, q}.", "startOffset": 49, "endOffset": 71}, {"referenceID": 0, "context": "Then the gini impurity is defined by GGini = 2pq (Breiman et al., 1984); entropy based impurity is defined as GEntropy = \u2212p log p\u2212 q log q (Quinlan, 1986); and misclassification impurity is defined as GMC = min{p, q}. Often the criterion C is called the gain. Hence, we also use gainGini(f) to refer to C(f) when G is GGini and similarly for other impurity measures. A split criterion different from impurity is twoing rule, first proposed by Breiman et al. (1984). Consider a split rule f at a node v.", "startOffset": 50, "endOffset": 465}, {"referenceID": 11, "context": "In the risk minimization framework, class conditional noise can be taken care when the noise rates are known (or can be estimated) (Natarajan et al., 2013; Scott et al., 2013; Ghosh et al., 2015).", "startOffset": 131, "endOffset": 195}, {"referenceID": 5, "context": "In the risk minimization framework, class conditional noise can be taken care when the noise rates are known (or can be estimated) (Natarajan et al., 2013; Scott et al., 2013; Ghosh et al., 2015).", "startOffset": 131, "endOffset": 195}, {"referenceID": 6, "context": "If pairwise correlation of each trees is \u03c1 and variance is \u03c32 for each tree, then random forest, consisting N trees, has variance, (Hastie et al., 2005)", "startOffset": 131, "endOffset": 152}, {"referenceID": 8, "context": "While, SVM has been proved to be non-robust even under symmetric label noise, its sensitivity towards noise widely varies (Long and Servedio, 2010; Nettleton et al., 2010; Manwani and Sastry, 2013; van Rooyen et al., 2015).", "startOffset": 122, "endOffset": 222}, {"referenceID": 12, "context": "While, SVM has been proved to be non-robust even under symmetric label noise, its sensitivity towards noise widely varies (Long and Servedio, 2010; Nettleton et al., 2010; Manwani and Sastry, 2013; van Rooyen et al., 2015).", "startOffset": 122, "endOffset": 222}, {"referenceID": 10, "context": "While, SVM has been proved to be non-robust even under symmetric label noise, its sensitivity towards noise widely varies (Long and Servedio, 2010; Nettleton et al., 2010; Manwani and Sastry, 2013; van Rooyen et al., 2015).", "startOffset": 122, "endOffset": 222}, {"referenceID": 2, "context": "For SVM we used libsvm package (Chang and Lin, 2011).", "startOffset": 31, "endOffset": 52}], "year": 2016, "abstractText": "In most practical problems of classifier learning, the training data suffers from the label noise. Hence, it is important to understand how robust is a learning algorithm to such label noise. This paper presents some theoretical analysis to show that many popular decision tree algorithms are robust to symmetric label noise under large sample size. We also present some sample complexity results which provide some bounds on the sample size for the robustness to hold with a high probability. Through extensive simulations we illustrate this robustness.", "creator": "LaTeX with hyperref package"}}}