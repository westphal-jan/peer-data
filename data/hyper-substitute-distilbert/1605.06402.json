{"id": "1605.06402", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks", "abstract": "modeling mathematics modeling ( taps ) have achieved major advantages although recent implementation. their performance versus modeling neuroscience lacks matched nor gained some authors even surpassed human capabilities. deep emotion networks can display complex vector - discrete operators ; overcoming this advantage adds at the cost of intense computational and memory requirements. state - the - art networks inspire billions of arithmetic operations intensive interpretation of parameters. databases accelerate embedded devices such as atm, google applications and monitoring assistants with the astonishing power of deep learning, dedicated hardware systems use specifically affected the decrease both execution time consuming electricity consumption. scientific applications where software connection to the cloud leaves not necessary or grid logic is uncertain, testing needs only be done locally. many particle accelerators or deep neural networks have been proposed recently. a potentially basic challenge underlying accelerator ai is hardware - oriented computing of deep networks, too enables program - efficient inference. practitioners present merlin, a fast and easy framework for conceptual animation. ristretto promotes arbitrary computational arithmetic by a sophisticated virtual accelerator. accelerator framework reduces the custom - ordering of logic parameters and outputs of resource - starved layers, which reduces its chip price for larger units significantly. so, researchers can estimate the generic hardware multipliers sooner, lacking in shorter array - only algorithms. the tool fine - tunes trimmed quickly to achieve high classification productivity. why millions of deep neural engineers can be fast - consuming, ristretto uses highly optimized routines which conditional on the gpu. this triggers fast compression of rapidly given image. given a compression tolerance between 1 %, ristretto can successfully condense caffenet / update to 32 - bit. the code for ristretto exceeds available.", "histories": [["v1", "Fri, 20 May 2016 15:22:29 GMT  (1632kb,D)", "http://arxiv.org/abs/1605.06402v1", "Master's Thesis, University of California, Davis; 73 pages and 29 figures"]], "COMMENTS": "Master's Thesis, University of California, Davis; 73 pages and 29 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["philipp gysel"], "accepted": false, "id": "1605.06402"}, "pdf": {"name": "1605.06402.pdf", "metadata": {"source": "CRF", "title": "Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks", "authors": ["Philipp Matthias Gysel"], "emails": [], "sections": [{"heading": null, "text": "Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks\nBy\nPhilipp Matthias Gysel B.S. (Bern University of Applied Sciences, Switzerland) 2012\nThesis\nSubmitted in partial satisfaction of the requirements for the degree of\nMaster of Science\nin\nElectrical and Computer Engineering\nin the\nOffice of Graduate Studies\nof the\nUniversity of California\nDavis\nApproved:\nProfessor S. Ghiasi, Chair\nProfessor J. D. Owens\nProfessor V. Akella\nProfessor Y. J. Lee\nCommittee in Charge\n2016\n-i-\nar X\niv :1\n60 5.\n06 40\n2v 1\n[ cs\n.C V\n] 2\n0 M\nay 2\n01 6\nCopyright c\u00a9 2016 by\nPhilipp Matthias Gysel\nAll rights reserved.\nTo my family ...\n-ii-\nContents\nList of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix"}, {"heading": "1 Introduction 1", "text": ""}, {"heading": "2 Convolutional Neural Networks 3", "text": "2.1 Training and Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 Layer Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.4 Computational Complexity and Memory Requirements . . . . . . . . . . 11 2.5 ImageNet Competition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.6 Neural Networks With Limited Numerical Precision . . . . . . . . . . . . 14"}, {"heading": "3 Related Work 19", "text": "3.1 Network Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.2 Accelerators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21"}, {"heading": "4 Fixed Point Approximation 25", "text": "4.1 Baseline Convolutional Neural Networks . . . . . . . . . . . . . . . . . . 25 4.2 Fixed Point Format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 4.3 Dynamic Range of Parameters and Layer Outputs . . . . . . . . . . . . . 27 4.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29"}, {"heading": "5 Dynamic Fixed Point Approximation 32", "text": "5.1 Mixed Precision Fixed Point . . . . . . . . . . . . . . . . . . . . . . . . . 32 5.2 Dynamic Fixed Point . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 5.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n-iii-"}, {"heading": "6 Minifloat Approximation 38", "text": "6.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 6.2 IEEE-754 Single Precision Standard . . . . . . . . . . . . . . . . . . . . . 38 6.3 Minifloat Number Format . . . . . . . . . . . . . . . . . . . . . . . . . . 38 6.4 Data Path for Accelerator . . . . . . . . . . . . . . . . . . . . . . . . . . 39 6.5 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 6.6 Comparison to Previous Work . . . . . . . . . . . . . . . . . . . . . . . . 42"}, {"heading": "7 Turning Multiplications Into Bit Shifts 43", "text": "7.1 Multiplier-free Arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . 43 7.2 Maximal Number of Shifts . . . . . . . . . . . . . . . . . . . . . . . . . . 44 7.3 Data Path for Accelerator . . . . . . . . . . . . . . . . . . . . . . . . . . 45 7.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45"}, {"heading": "8 Comparison of Different Approximations 47", "text": "8.1 Fixed Point Approximation . . . . . . . . . . . . . . . . . . . . . . . . . 48 8.2 Dynamic Fixed Point Approximation . . . . . . . . . . . . . . . . . . . . 49 8.3 Minifloat Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 8.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49"}, {"heading": "9 Ristretto: An Approximation Framework for Deep CNNs 51", "text": "9.1 From Caffe to Ristretto . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 9.2 Quantization Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 9.3 Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 9.4 Fast Forward and Backward Propagation . . . . . . . . . . . . . . . . . . 53 9.5 Ristretto From a User Perspective . . . . . . . . . . . . . . . . . . . . . . 54 9.6 Release of Ristretto . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 9.7 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n-iv-\nList of Figures\n2.1 Network architecture of AlexNet . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Convolution between input feature maps and filters . . . . . . . . . . . . 6 2.3 Pseudo-code for convolutional layer . . . . . . . . . . . . . . . . . . . . . 7 2.4 Fully connected layer with activation . . . . . . . . . . . . . . . . . . . . 8 2.5 Pooling layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.6 Parameter size and arithmetic operations in CaffeNet and VGG-16 . . . 11 2.7 ImageNet networks: accuracy vs size . . . . . . . . . . . . . . . . . . . . 13 2.8 Inception architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.9 Data path with limited numerical precision . . . . . . . . . . . . . . . . . 15 2.10 Quantized layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.1 ASIC vs FPGA vs GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n4.1 Dynamic range of values in LeNet . . . . . . . . . . . . . . . . . . . . . . 27 4.2 Dynamic range of values in CaffeNet . . . . . . . . . . . . . . . . . . . . 28 4.3 Fixed point results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n5.1 Fixed point data path . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 5.2 Dynamic fixed point representation . . . . . . . . . . . . . . . . . . . . . 34 5.3 Static vs dynamic fixed point . . . . . . . . . . . . . . . . . . . . . . . . 35\n6.1 Minifloat number representation . . . . . . . . . . . . . . . . . . . . . . . 39 6.2 Minifloat data path . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 6.3 Minifloat results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n7.1 Representation for integer-power-of-two parameter . . . . . . . . . . . . . 44 7.2 Multiplier-free data path . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n8.1 Approximation of LeNet . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 8.2 Approximation of CIFAR-10 . . . . . . . . . . . . . . . . . . . . . . . . . 48\n-v-\n8.3 Approximation of CaffeNet . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n9.1 Quantization flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 9.2 Fine-tuning with full precision weights . . . . . . . . . . . . . . . . . . . 53 9.3 Network brewing with Caffe . . . . . . . . . . . . . . . . . . . . . . . . . 54 9.4 Network quantization with Ristretto . . . . . . . . . . . . . . . . . . . . 55\n-vi-\nList of Tables\n3.1 ASIC vs FPGA vs GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n4.1 Fixed point results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n5.1 Dynamic fixed point quantization . . . . . . . . . . . . . . . . . . . . . . 36 5.2 Dynamic fixed point results . . . . . . . . . . . . . . . . . . . . . . . . . 37\n6.1 Minifloat results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n7.1 Multiplier-free arithmetic results . . . . . . . . . . . . . . . . . . . . . . . 45\n-vii-\nAbstract\nRistretto: Hardware-Oriented Approximation of Convolutional Neural\nNetworks\nConvolutional neural networks (CNN) have achieved major breakthroughs in recent years. Their performance in computer vision have matched and in some areas even surpassed human capabilities. Deep neural networks can capture complex non-linear features; however this ability comes at the cost of high computational and memory requirements. State-ofart networks require billions of arithmetic operations and millions of parameters.\nTo enable embedded devices such as smart phones, Google glasses and monitoring cameras with the astonishing power of deep learning, dedicated hardware accelerators can be used to decrease both execution time and power consumption. In applications where fast connection to the cloud is not guaranteed or where privacy is important, computation needs to be done locally. Many hardware accelerators for deep neural networks have been proposed recently. A first important step of accelerator design is hardware-oriented approximation of deep networks, which enables energy-efficient inference.\nWe present Ristretto, a fast and automated framework for CNN approximation. Ristret-\nto simulates the hardware arithmetic of a custom hardware accelerator. The framework reduces the bit-width of network parameters and outputs of resource-intense layers, which reduces the chip area for multiplication units significantly. Alternatively, Ristretto can remove the need for multipliers altogether, resulting in an adder-only arithmetic. The tool fine-tunes trimmed networks to achieve high classification accuracy.\nSince training of deep neural networks can be time-consuming, Ristretto uses highly optimized routines which run on the GPU. This enables fast compression of any given network.\nGiven a maximum tolerance of 1%, Ristretto can successfully condense CaffeNet and\nSqueezeNet to 8-bit. The code for Ristretto is available.\n-viii-\nAcknowledgments\nFirst and foremost, I want to thank my major advisor Professor Soheil Ghiasi for his guidance, inspiration and encouragement that he gave me during my graduate studies. Thanks to him, I had the privilege to do research in a dynamic research group with excellent students. He provided my with all the ideas, equipment and mentorship I needed for writing this thesis.\nSecond I would like to thank the graduate students at UC Davis who contributed to my research. I was fortunate to work together with members of the LEPS Group, the Architecture Group as well as the VLSI Computation Lab. Most notably, Mohammad Motamedi, Terry O\u2019Neill, Dan Fong and Joh Pimentel helped me with advice, technical knowledge and paper editing. I consider myself extremely lucky that I had their support during my graduate studies, and I look forward to continuing our friendship in the years to come. I\u2019m humbled by the opportunity to do research with Mohammad Motamedi, a truly bright PhD student. Our early joint research projects motivated me to solve challenging problems and to strive for extraordinary research results.\nThird, I am grateful to all members of my thesis committee: Professor John Owens, Venkatesh Akella, and Yong J. Lee. Professor Owens spurred me on to conduct an in-depth analysis of related work; additionally he gave me valuable improvement suggestions for my thesis. Early in this research project, Professor Akella guided me in reading papers on hardware acceleration of neural networks. Professor Lee helped me significantly to improve the final version of this document.\nFinally I\u2019d like to thank my family for supporting my studies abroad, and especially my girlfriend Thirza. I am grateful to my family and friends for always motivating me to pursue my academic goals; without them I would not have come this far.\n-ix-\nChapter 1\nIntroduction\nOne of the major competitions in AI and computer vision is the ImageNet Large Scale Visual Recognition Competition (Russakovsky et al., 2015). This annually held competition has seen state-of-the-art image classification accuracies by deep networks such as AlexNet by Krizhevsky et al. (2012), VGG (Simonyan and Zisserman, 2015), GoogleNet (Szegedy et al., 2015) and ResNet (He et al., 2015). All winners since 2012 have used deep convolutional neural networks. These networks contain millions of parameters and require billions of arithmetic operations.\nTraining of large networks like AlexNet is a very time-consuming process and can take multiple days or even weeks. The training procedure of these networks is only possible thanks to recent advancements in graphics processing units. High-end GPUs enable fast deep learning, thanks to their large throughput and memory capacity. When training AlexNet with Berkeley\u2019s deep learning framework Caffe (Jia et al., 2014) and Nvidia\u2019s cuDNN (Chetlur et al., 2014), a Tesla K-40 GPU can process an image in just 4ms.\nWhile GPUs are an excellent accelerator for deep learning in the cloud, mobile systems are much more sensitive to energy consumption. In order to deploy deep learning algorithm in energy-constraint mobile systems, various approaches have been offered to reduce the computational and memory requirements of convolutional neural networks (CNNs). Various FPGA-based accelerators (Suda et al., 2016; Qiu et al., 2016) have proven that it is possible to use reconfigurable hardware for end-to-end inference of large CNNs like AlexNet and VGG. Moreover, we see an increasing amount of ASIC designs for deep\n1\nCNNs (Chen et al., 2016; Sim et al., 2016; Han et al., 2016a).\nBefore implementing hardware accelerators, a first crucial step consists of condensing the neural network in question. Various work has been conducted recently to reduce the computational and memory requirements of neural networks. However, no opensource project exists which would help a hardware developer to quickly and automatically determine the best way to reduce the complexity of a trained neural network. Moreover, too aggressive compression of neural networks leads to reduction in classification accuracy.\nIn this thesis we present Ristretto, a framework for automated neural network approximation. The framework is open source and we hope it will speed up the development process of energy efficient hardware accelerators. Our framework focuses on condensing neural networks without adding any computational complexity such as decompression or sparse matrix multiplication. Ristretto is a Caffe-based approximation framework and was developed for fast, automated, efficient and flexible CNN compression for later deployment in hardware accelerators. Ristretto aims at lowering the area requirements for processing elements, and lowering the memory footprint which in turn reduces or eliminates off-chip memory communication.\nThis thesis analyses the resource-requirements of convolutional neural networks. Based on these findings, different approximation strategies are proposed to reduce the resourceintense parts of CNNs. For all different approximation strategies, we present an in-depth analysis of the compression vs accuracy trade-off.\nParts of this thesis are based on previous publications with two other authors: Mohammad Motamedi from the University of California, Davis and Professor S. Ghiasi from the same university.\n\u2022 Hardware-oriented Approximation of Convolutional Neural Networks,\nPhilipp Gysel, Mohammad Motamedi, and Soheil Ghiasi, arXiv preprint arXiv: 1604.03168 (2016). Gysel et al. (2016)\n\u2022 PLACID: a Platform for Accelerator Creation for DCNNs, Mohammad\nMotamedi, Philipp Gysel and Soheil Ghiasi, under review. Motamedi et al. (2016)\n2\nChapter 2\nConvolutional Neural Networks\nOver time, different feature extraction algorithms have been used for image processing tasks. SIFT (Lowe, 2004) and HOG (histogram of oriented gradients by Dalal and Triggs (2005)) were state-of-art for feature extraction, but they relied on handcrafted features. Neural networks in contrast can automatically create both high-level and low-level features. For a long time, deep neural networks were hindered by their computational complexity. However, advances in both personal computers and general purpose computing have enable the training of larger networks with more parameters. In 2012, the first deep convolutional neural network with 8 parameter layers was proposed by Krizhevsky et al. (2012). State-of-the art deep CNNs use a series of convolutional layers which enables them to extract very high-level features from images. Convolutional neural networks have proven to overshadow conventional neural networks in complex vision tasks."}, {"heading": "2.1 Training and Inference", "text": "Convolutional neural networks have two computation phases. The forward propagation is used for classification, and the backward propagation for training. Like other algorithms in machine learning, CNNs use a supervised training procedure to find network parameters which yield good classification accuracy. Throughout this thesis, we use the terms parameters and weights interchangeably.\n3"}, {"heading": "2.1.1 Forward Propagation", "text": "Input to the forward propagation is the image to classify. The forward path consists of different layers which process the input data in a chained manner. Deep CNNs use many such layers, the last of which is used to predict the image class.\nA typical CNN architecture termed AlexNet is shown in Figure 2.1. As input to the network, we use an RGB image with dimensions 224\u00d7224. The first five layers in AlexNet are convolutional layers, and the last three are fully connected layers. Convolutional layers use 2d filters to extract features from the input. The first convolutional layer generates 2\u00d748 feature maps, each of which represents the presence or absence of a lowlevel feature in the input image. To reduce the spatial dimension of feature maps and to add translation-invariance, pooling layers are used which do sub-sampling. Moreover, a non-linear layer is added which enables the network to learn non-linear features.\nThe last convolutional layer creates 2\u00d7128 feature maps with spatial dimension 13\u00d713. This layer is followed by three dense layers, which produce the weighted sum of inputs. The last layer has 1000 output nodes, which are the predictions for the 1000 image classes.\nForward propagation depends on network parameters in two ways. First, the convolutional layers rely on feature filters. Second, the fully connected layers contain many parameters, each of which serves as weighted connection between a specific input and output node. These parameters are learned during the training procedure.\n4"}, {"heading": "2.1.2 Backward Propagation", "text": "Training of a deep CNN requires thousands or even millions of labeled images. The network is exposed to these images, and the network parameters are gradually updated to make up for prediction errors. The purpose of backward propagation is to find the error gradient with respect to each network parameter. In a later step this error gradient is used for a parameter update.\nTraining is done in batches of images. Several images are run through the network in forward path. Denoting x as the network input, w as the network parameters and f as the overall CNN function, the network output is given by z\u2032 = f(x,w). Since all images are labeled, the desired network output z is known. Given many pairs of images and ground-truth (x1, z1), ..., (xn, zn), we define a loss function l(z, z \u2032) which denotes the penalty for predicting z\u2032 instead of z.\nWe average the loss for a batch of images, and update the parameters according to\nthe formula below:\nwt+1 = wt \u2212 \u03b1 \u00b7 \u03b4l \u03b4w (wt) (2.1)\nThis formula requires us to calculate the network output error w.r.t. each parameter. The above parameter update is called stochastic gradient descent, which relies on a learning rate \u03b1. There actually exist many optimizations for this parameter update such as Nesterov momentum as explained by Bengio et al. (2013) or Adam rule (Kingma and Ba, 2015). All these learning rules require the error gradient w.r.t each network parameter. Moreover these optimization procedures all work in batch mode, i.e., a batch of images (e.g. 256 images) is run through the network and the parameter update is based on the average error gradient. The error surface of neural networks is non-convex. Nevertheless, batch-based learning rules can avoid local minima by using many different training examples.\nFor computation of the error gradient w.r.t the network parameters, we first compute the error gradient with respect to each layer output, starting with the second last layer and back propagation to the second layer. In a second step, the gradients w.r.t. the layer\n5\noutputs can be used to compute the gradients w.r.t. the network parameters, using the chain rule for derivatives."}, {"heading": "2.2 Layer Types", "text": "Deep convolutional neural networks process the input data layer by layer. Each layer has a specific purpose as explained in the following paragraphs.\nConvolutional layer: This layer type is used for feature extraction. Deep CNNs have many convolutional layers; AlexNet for example has five layers of this type. The feature extraction is done by a series of L convolutional layers. Each layer uses 2d kernel filters which extract features from input feature maps (IFM). The result of multiple feature extractions is summed up to form one output feature map (OFM). This process is shown in Figure 2.2, where two filter banks, each with three kernels, are used to generate 2 output feature maps. The number of input and output feature maps is denoted by N and M , and the size of one output feature map is R\u00d7C. One kernel has dimension K\u00d7K, and a layer has N\u00d7M of these kernels. The feature extraction consists of a series of multiplication-accumulation (MAC) operations, as shown in Figure 2.3. Each output pixel is the sum of the 2d convolutions between the input feature maps and the respective kernels. To generate the neighboring output pixels, the kernel stack is slided across the spacial dimension by stride S.\nThe time and space complexity of convolutional layers is given in Equations 2.2 and 2.3. Assuming a stride S of 1, the computational complexity is R \u00b7 C times larger than the number of parameters. It is for this reason that the computational bottleneck of deep CNNs comes from convolutional layers.\n6\nruntime = O(RCMNK2) (2.2)\nparameter size = O(MNK2) (2.3)\nFully connected layer: Fully connected layers serve the same purpose as convolutional layers, namely feature extraction. Fully connected layers, alternatively termed dense layers, build the weighted sums of their input. Thus all input nodes are connected to all output nodes, which requires a relatively large amount of network parameters. Fully connected layers are the basic building block of classic neural networks, which are normally a concatenation of dense layers. In convolutional neural networks, the first layers are normally convolutional layers, and only one or two layer at the very end are dense.\nThe mathematical function of a fully connected layer is a simple matrix-vector product. The layer output nodes z depend on the input vector x, the parameter-matrix W and the bias b (Equation 2.4). Denoting N = |x| as the number of input nodes and M = |z| as the number of outputs, the time and space complexity is given in Equations 2.5 and 2.6. For layers with many input nodes, the parameter size usually makes up for a large part of the overall network size.\nMost fully connected layers are followed by a non-linear activation function f . Figure\n2.4 depicts a fully connected layer followed by an activation layer.\nz = W \u00b7 x + b (2.4)\n7\nruntime = O(NM) (2.5)\nparameter size = O(NM) (2.6)\nRectified Linear Unit (ReLU): To enable neural networks to capture non-linear relations, several non-linear layers are inserted into the network architecture. Traditionally we used the Sigmoid or hyperbolic tangent function for this purpose. However, those classic non-linear functions have several disadvantages. First, their gradient becomes very small for large values, which means the error gradient during training vanishes. Moreover these two non-linear functions are relatively costly in terms of computing power.\nAs alternative, nearly all state-of-art CNNs use Rectified Linear Units (ReLU). This activation function was first proposed by Nair and Hinton (2010) for Restricted Boltzmann Machines. The work by Krizhevsky et al. (2012) was the first to apply this simplified activation to a deep neural networks. Deep networks trained with this activation function converge much faster.\nThe function of a ReLU layer maps negative values to zero: f(x) = max(0, x).\nNormalization layers: Local Response Normalization (LRN) layers serve the purpose of normalization across feature maps or across the spatial dimension. There are generally two type of LRN layers: LRN across feature maps and LRN within feature maps. The\n8\nfirst type of normalization serves the purpose of normalizing neurons in the same spatial position, but different feature maps. This creates competition between neurons generated by different kernels and improves the top-1 accuracy of AlexNet (Krizhevsky et al., 2012) by 1.4%. The exact mathematical formulation of LRN across channels can be found in the same paper.\nMany state-of-art CNNs use LRN layers to increase accuracy (Krizhevsky et al., 2012; Szegedy et al., 2015). One notable exception is the network by Simonyan and Zisserman (2015) which performs very well without any kind of normalization.\nMost recently, a new normalization strategy termed Batch Normalization (BN) was proposed (Ioffe and Szegedy, 2015). This strategy was adopted by the most recent winner of the ILSVRC competition (Russakovsky et al., 2015).\nWhile both normalization strategies help for faster convergence and better prediction accuracy, they also add computational overhead, especially batch normalization. Unfortunately these normalization layers require a very large dynamic range for intermediate values. In AlexNet for example, the intermediate values of LRN layers are 214 times larger than any intermediate value from another layer. For this reason this thesis assumes LRN and BN layers are to be implemented in floating point, and we concentrate on the approximation of other layer types. Notice that previous work by Suda et al. (2016) chose 32-bit floating point for FPGA-based LRN layers.\nPooling: Pooling layers are normally used between successive convolutional layers in CNNs. They can be considered as sub-sampling functions. The purpose of pooling layers\n9\nis to reduce the spatial dimension of feature maps and encode translation invariance. By reducing the feature map dimensions, we also reduce the parameter size and computational complexity, and thus lower chances for over-fitting.\nThe most commonly used pooling type in CNNs is MAX pooling, although other types such as average pooling and L2-norm pooling exist. MAX pooling does not required any arithmetic operation except comparisons. Since this layer type is cheap in terms of computation and parameter size, this thesis leaves this layer type as is and we perform no approximation.\nAn example of a MAX pooling operation is shown in Figure 2.5, where 2x2 kernels are used to extract the maximum value. The result is stored as output on the right side of the Figure. Pooling layers are associated with a kernel size and a stride. The stride indicates how many pixels are skipped between pooling operations. Notice that in the above example, the data size is reduced by 75%."}, {"heading": "2.3 Applications", "text": "Deep convolutional neural networks have pushed the limits of artificial intelligence in a wide range of applications. Recent winners (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; Szegedy et al., 2015; He et al., 2015) of ImageNet (Russakovsky et al., 2015) competition have continuously improved machine\u2019s abilities in image classification. The most recent winners of this competition even outperform human vision. Besides image classification, deep networks show state-of-art performance in object detection (Girshick, 2015) as well as speech recognition (Hinton et al., 2012). Other applications include playing games (Silver et al., 2016), as well as art (Gatys et al., 2015). Notice that artificial neural networks as well as recurrent neural networks could potentially be approximated in a similar way as we do in this thesis.\n10"}, {"heading": "2.4 Computational Complexity and Memory Require-", "text": "ments\nThe complexity of deep CNNs can be split into two parts. First, the convolutional layers contain more than 90% of the required arithmetic operations. The second resource-intense layer type are fully connected layers, which contain over 90% of the network parameters. An energy-efficient accelerator for CNNs needs to 1) offer a large enough computational throughput and 2) offer a memory-bandwidth that can keep the processing elements busy.\nCaffeNet is the Caffe-version of AlexNet by Krizhevsky et al. (2012). CaffeNet was developed for the ImageNet data set, which has 1000 image classes. Figure 2.6 shows the required arithmetic operations and parameters size of AlexNet by layer type. The major part of arithmetic operations comes from convolutional layers: this layer type requires a total of 2.15 G operations. The arithmetic operations in all other layers sum up to 117 M operations. The parameter size of CaffeNet is 250 MB, of which 235 MB comes from fully connected layers.\nThe same trend can be observed for the 16-layer version of VGG by Simonyan and Zisserman (2015): Extracting features in convolutional layers is computation-intense, while fully connected layers are memory-intense. Since most computation and memory require-\n11\nments come from fully connected and convolutional layers, this thesis concentrates on approximating these two layer types only."}, {"heading": "2.5 ImageNet Competition", "text": "The ILSVRC competition (Russakovsky et al., 2015) is a large scale image classification and detection challenge which has been held annually since 2010. More than fifty institutions have participated in the challenge, among them companies like Google, Microsoft, Qualcomm, as well as various universities. In its first year the competition consisted of an image classification challenge only. In the meantime, different new challenges were introduced such as object detection and localization in images, object detection from video as well as scene classification.\nFor training of the classifier, ILSVRC provides the ImageNet data set, which consisted originally of 1000 image categories and a total of 1.2 million images. In the meantime, this data base has been significantly expanded, and currently holds over 14 million labeled images.\nA common performance measure for deep CNNs is their classification accuracy on the ILSVRC 2012 data set. For this challenge, the classifier is trained on a training data set, and tested by the researcher on a validation data set. For the official score in the ILSVRC competition, a private test data set is used which is not available to the public. Most researchers give their performance numbers in top-1 and top-5 measure. The top-1 measure gives the percentage of images that were classified correctly on either the validation or test set. Since the ImageNet data set has very fine-grained image classes which are sometimes even hard for humans to distinguish, most researchers prefer to use the top-5 measure. Here, an image is considered as classified correctly if one of the top5 predictions is correct. In the remainder of this thesis, we will concentrate on top-1 accuracy, since this captures better how CNN approximation affects network accuracy."}, {"heading": "2.5.1 Network Size vs Accuracy", "text": "On one hand, recent network architectures indicate that deeper networks perform better. The winner of 2014\u2019s localization challenge (Simonyan and Zisserman, 2015) experimented\n12\nwith different depths for their network. Going from 11 parameter layers to 19 improved their top-1 classification for over 4%. Another experiment by the winner of 2015 (He et al., 2015) used very deep networks. Their network architecture improves by over 2% when expanding the net from 34 to 152 layers.\nOn the other hand, some research shows that even relatively small networks can achieve good classification performance. In the classification challenge of 2014, GoogLeNet (Szegedy et al., 2015) outperformed VGG (Simonyan and Zisserman, 2015) with a network capacity that was over 19X smaller. The GoogLeNet network is based on the inception idea described in section 2.5.2. A newer network architecture by Iandola et al. (2016) uses an adapted inception concept with smaller convolutional kernels. Their small, yet quite accurate network termed SqueezeNet was developed specifically for mobile devices. SqueezeNet has the accuracy of AlexNet (Krizhevsky et al., 2012), but contains 50X fewer parameters. SqueezeNet avoids fully connected layers, which drastically reduces the parameter size to below 5 MB.\nIn summary, there is a delicate trade-off between network size and accuracy (see Figure 2.7). GoogLeNet finds a good balance for reducing both classification error and network size. This network by Google outperforms AlexNet and VGG in both aspects. SqueezeNet is the smallest architecture, but its accuracy is not outstanding. ResNet has the best accuracy, but it is also the largest network both in terms of parameter layers and parameter size.\n13"}, {"heading": "2.5.2 Inception Idea", "text": "The inception idea is a concept proposed by Szegedy et al. (2015) which was used to build the GoogLeNet architecture. GoogLeNet was developed with the goal of achieving high classification accuracy on the ImageNet data set with a budget of 1.5 billion MAC operations for inference. The authors avoid sparse representations and chose to use readily available dense components. The GoogLeNet architecture contains several replicas of the inception module shown in Figure 2.8. An inception module contains 1\u00d71, 3\u00d73 and 5\u00d75 convolution kernels. In order to reduce the number of feature maps and thus the computational complexity, 1\u00d71 convolutions are added in front of the expensive 3\u00d73 and 5\u00d75 convolutions. The inception module also contains an alternative max pooling path."}, {"heading": "2.6 Neural Networks With Limited Numerical Pre-", "text": "cision\nThis section focuses on limited precision arithmetic for convolutional neural networks. Most deep learning frameworks (Jia et al., 2014; Theano Development Team, 2016; Abadi et al., 2015) use 32-bit or 64-bit floating point for CNN training and inference. However, it has be shown (Du et al., 2015) that CNNs have a relatively high error resilience; moreover CNNs can be trained in a discrete parameter space. In the following subsections, we\n14\ndescribe the process of quantizing a full precision network to limited precision numbers. Additionally we introduce different rounding schemes for the quantization step, and we describe different options for optimizing the classification accuracy of a limited precision network."}, {"heading": "2.6.1 Quantization", "text": "As discussed previously, the goal of this thesis is to provide a framework for approximating the forward path of any given CNN. For this purpose we compress the number format in convolutional and fully connected layers. These two layer types, which are the most resource-demanding part of a deep network, require the same arithmetic operations, namely a series of multiplication-and-accumulation (MAC). In this thesis we simulate the arithmetic of a hardware accelerator. The simulated data path is shown in Figure 2.9.\nThe difference between this simulated data path and the original full precision data path is the quantization step of weights, layer inputs, and layer outputs. Therefore the condensed networks will suffer from quantization errors, which can affect the network accuracy.\nIn this thesis we propose a framework which can approximate 32-bit floating point networks by condensed ones which use quantized values. In order to simulate a condensed\n15\nlayer, given a full precision reference network, the following three steps are required:\n\u2022 Quantization of the layer input and weights to reduced precision format (using m and n bits for number representation, respectively)\n\u2022 Perform the MAC operations using the quantized values\n\u2022 The final result is again quantized\nThese steps are summarized in Figure 2.10."}, {"heading": "2.6.1.1 Data path in hardware", "text": "In Figure 2.9, the layer input values and layer weights serve as input to the multiplication units. To leverage the inherent sources of parallelism, a hardware accelerator will use many of these units in parallel. Each multiplication unit gets one layer input value and one network weight per computation round. The different results are accumulated in an adder tree, and the final sum is the layer output. Notice that some of the layer input and weight values are actually reused for different multipliers, depending on the exact characteristics of the layer in question.\nTo reduce the number of bits required for number representation, our approximation framework quantizes both the layer inputs and weights. The resulting values can be represented using significantly fewer bits. As a result, the multiplication units require less area. In order to simplify simulation of hardware, our framework uses 32-bit floating point for accumulation. To achieve the same result in a hardware accelerator, the adder\n16\ntree should use 32-bit. Adders are much cheaper in terms of area and power, compared to multipliers. Thus it is acceptable to use more bits for number representation in the adder tree.\nAs a last step, the layer output is quantized to limited-precision format. This reduces the memory requirements, especially if results need to be written back to off-chip memory."}, {"heading": "2.6.2 Rounding Schemes", "text": "Different rounding schemes can be used for value quantization.\nRound nearest even: Round-nearest-even is an unbiased scheme which rounds to the nearest discrete value. Denoting as the quantization step size and bxc as the largest quantization value less or equal to x, Gupta et al. (2015) define round nearest as follows:\nround(x) = bxc, if bxc \u2264 x \u2264 x+ 2\nbxc+ , if bxc+ 2 < x \u2264 x+\n(2.7)\nAs round-nearest-even is deterministic, we chose this rounding scheme for inference, i.e., at test time all the parameters are rounded deterministically, and so are the layer outputs.\nRound stochastic: Another rounding scheme termed stochastic rounding was used by Gupta et al. (2015) for the weight updates of 16-bit neural networks. Gupta et al. (2015) define stochastic rounding as follows:\nround(x) = bxc, w.p. 1\u2212 x\u2212bxc\nbxc+ , w.p. x\u2212bxc\n(2.8)\nStochastic rounding adds randomness to the quantization procedure, which can have an averaging effect during training. We chose to use this rounding scheme when quantizing network parameters during fine-tuning, as explained later in the next subsection. Stochastic rounding has an expected rounding error of zero, i.e. E(round(x)) = 0."}, {"heading": "2.6.3 Optimization in Discrete Parameter Space", "text": "Training of neural networks can be seen as an optimization problem, where the goal is to find the optimal set of parameters which minimizes the classification error for a given set\n17\nof images. A practical solution to this problem is to use stochastic gradient descent, as explained in subsection 2.1.1. In the traditional setting of 64-bit floating point training, this optimization is a continuous problem with a smooth error surface. The error surface of neural networks depends on its input and its current parameters. For quantized networks, this error surface becomes discrete. This new optimization problem \u2013 where the goal is to find an optimal set of discrete valued parameters \u2013 is an NP-hard problem. One approach to find a good set of discrete parameters is to train in limited precision \u2018from scratch\u2019. In this approach, we would train the network with quantized parameters right from the start. All the weight updates would be discrete. We chose to use another approach: Our framework first trains a network in the continuous domain, then quantizes the parameters, and finally fine-tunes in discrete parameter space. This way we can fully leverage pre-trained networks which saves considerable amount of time.\nDuring this retraining procedure, the network learns how to classify images with limited precision parameters. Since the network weights can only have discrete values, the main challenge consists in the weight update. We adopt the idea of previous work by Courbariaux et al. (2015) which uses full precision shadow weights. Small weight updates \u2206w are applied to the full precision weights w, whereas the discrete weights w\u2032 are sampled from the full precision weights. The sampling during fine-tuning is done with stochastic rounding. For more details on the fine-tuning procedure of quantized networks, please refer to section 9.3.\n18\nChapter 3\nRelated Work\nIn the first section of this chapter, we review different network approximation techniques. In the second part, we describe related work in hardware accelerator design."}, {"heading": "3.1 Network Approximation", "text": ""}, {"heading": "3.1.1 Fixed Point Approximation", "text": "Various solutions have been offered to reduce the resource-requirement of CNNs. Traditionally neural networks are trained in 32-bit floating point. However fixed point arithmetic is less resource hungry than floating point arithmetic. Moreover, it has been shown that fixed point arithmetic is adequate for neural network computation. This observation has been leveraged recently to condense deep CNNs. Gupta et al. (2015) show that networks on datasets like CIFAR-10 (10 images classes) can be trained in 16-bit. Further trimming of the same network uses as low as 7-bit multipliers (Courbariaux et al., 2014). Another approach by Courbariaux et al. (2015) uses only binary weights, again on the same network. A similar proposal represents the weights of the same network with +1, 0 and -1 values (Sung et al., 2015). While these proposed fixed point schemes work well with small networks, only limited work has been done on large CNNs like AlexNet."}, {"heading": "3.1.2 Network Pruning and Shared Weights", "text": "Off-chip memory access makes up for a significant part of the total energy budget of any data-intense application. As deep CNNs have typically more than 10 MB of parameters,\n19\nan important step is to compress the size of the network parameters. The deep compression pipeline proposed by Han et al. (2016b) addresses this problem. The authors achieve a network parameter compression rate of up to 49X for deep CNNs using a three-step pipeline. In a first step, the \u2019unimportant\u2019 connections of a trained network are removed. The resulting sparse network is then retrained to regain its classification accuracy, and the pruning step is repeated. After some iterations of pruning and fine tuning, the remaining parameters are clustered together to form shared weights. These shared weights are again fine-tuned to find optimal centroids. In a last step, a lossless data compression scheme (Huffman Coding) is applied to the final weights."}, {"heading": "3.1.3 Binary Networks", "text": "Since memory access has a relatively high energy cost, it is desirable to reduce the network parameter size. This motivated BinaryConnect (Courbariaux et al., 2015), a work which represents weights in binary format, rather than in traditional 32-bit floating point. This approach reduces parameter size by factor 32X and removes the need of multiplications in the forward path. BinaryConnect achieves near-state-of-art performance on 10-class datasets (MNIST, CIFAR-10, SVHN).\nA later work by Lin et al. (2016) takes this idea a step further by turning multiplications in the backward propagation into bit shifts. Layer activations are approximated by integer power of 2 numbers, while error gradients are retained in full precision. This proposal significantly reduces the hardware requirements for accelerators.\nCombining the two previous ideas, \u2018Binarized Neural Network\u2019 (Courbariaux et al., 2016) uses binary weights and layer activations. These numbers are constraint to +1 and -1 for both forward and backward propagation. Convolutional neural networks mainly consist of multiply-accumulate operations. For a binarized network, these operations are replaced by binary XNOR and binary count. To improve training results, the proposed method uses a bit-shift-based batch normalization as well as a shift-based parameter update.\nFinally the work of Rastegari et al. (2016) applies the idea of binary networks to ImageNet data set. The three previously mentioned approaches work well with small\n20\nnetworks; however they show limited performance on large networks for the ImageNet data set. The work by Rastegari et al. (2016) proposes two network architectures. Both are based on AlexNet (Krizhevsky et al., 2012) and use different degrees of binarization. First, the proposed Binary-Weights-Network shows a speedup of 2X for CPU implementation and achieves an accuracy within 3% of AlexNet. Second, XNOR- Net has binary weights and layer outputs. XNOR-Net turns most MAC operations into binary XNOR and bit count, however at a relatively high accuracy drop (12.4%)."}, {"heading": "3.2 Accelerators", "text": "Different accelerator platforms have been used to accelerate CNN inference. In what follows we review proposed accelerators on GPUs, FPGAs and ASICs."}, {"heading": "3.2.1 GPU Acceleration", "text": "Given the high throughput and memory bandwidth of today\u2019s GPUs, different research has focused on accelerating GPU-based inference of neural networks. A proposal by Denton et al. (2014) uses clustered filters and low rank approximation. They achieve a speedup of 2X for convolutional layers of AlexNet, compared to a non-optimized GPU implementation. Another work by Mathieu et al. (2013) achieves better results by replacing convolution through FFT. Finally the neural network compression pipeline proposed by Han et al. (2016b) uses pruning and weight-sharing. When this compression is applied to dense layers of AlexNet, forward propagation is 4X faster and 3X more energy efficient. In their later paper (Han et al., 2016a), they show a Titan X based GPU implementation with a throughput of 3.23 TOPs.\nHigh-end GPUs require a lot of energy. As a case in point, Nvidia\u2019s Tesla K-40 has an average power consumption of 235 W when running DGEMM. This motivated researchers to consider accelerator platforms with smaller power budgets."}, {"heading": "3.2.2 FPGA-based Accelerators", "text": "Field programmable gate arrays (FPGA) can offer high throughput per power. FPGAbased accelerators have a shorter development time than ASICs, however they can\u2019t match the throughput of GPUs. Different FPGA-based accelerators for neural networks have\n21\nbeen proposed. An approach by Zhang et al. (2015) uses Vivado HLS to accelerate the convolutional layers of AlexNet. Their floating point implementation achieves a throughput of over 60 GFLOPs at a power budget of 20 W.\nA subsequent proposal by Suda et al. (2016) uses OpenCL to implement whole VGG (Simonyan and Zisserman, 2015) net on an Altera Stratix V board. Their throughputoptimized design achieves an overall throughput of 117.8 GOPs. Finally a recent Xilinxbased implementation (Qiu et al., 2016) achieves the start-of-art throughput of 137 GOPs. Their 16-bit fixed point implementation requires less than 10 W."}, {"heading": "3.2.3 Custom Accelerators (ASIC)", "text": "Custom architectures have the highest throughput and energy efficiency, however their design time is significant. DaDianNao by Chen et al. (2014) is a super-computer for machine learning at 28 nm technology. Their chip relies on large on-chip memory (which takes up nearly half of the area) and achieves significant speedups and power savings compared to the GPU. A later implementation termed Eyeriss (Chen et al., 2016) can run the convolutional layers of AlexNet in forward path at 34 frames per second (74.6 GOPs), using only 0.278 W. The chip is about 2X slower than a throughput optimized embedded GPU, but 13X more energy efficient. Eyeriss uses 16-bit fixed point. Finally EIE (Han et al., 2016a) is an ASIC which leverages the deep compression pipeline (Han et al., 2016b). EIE infers significantly compressed networks 13X faster than a GeForce GTX Titan X. Since the neural network is pruned and has shared weights, the whole network fits to on-chip memory which allows to infer images using just 0.6 W."}, {"heading": "3.2.4 Comparison Of Accelerator Platforms", "text": "In this section we compare different accelerator platforms in terms of throughput and throughput per power. We take the performance numbers from recently published papers, the source of our numbers can be found in Table 3.1.\n22\nThe ASIC design by Chen et al. (2016) is optimized for large networks and low power consumption. Their work concentrates on convolutional layers of AlexNet. Other works which concentrate on fully connected layers only show similar throughput (Han et al., 2016a). Predictably, the ASIC design shows the highest throughput per power (see Figure 3.1).\nFor GPU performance, we consider an implementation which uses cuBLAS for fully connected layers of AlexNet. Convolutional layers would yield lower throughput, since this layer type requires rearrangement of data before the matrix-matrix multiplication.\n23\nThe acceleration on the GPU achieves the highest throughput among all accelerators (see Figure 3.1). The high GPU throughput of 3.23 TOP/s comes at a relatively high power cost (250 W). Embedded GPUs require less power, but their throughput is proportionally lower. As a case in point, we consider the mobile GPU implementation of AlexNet by Chen et al. (2016). When comparing the two GPU implementations, the mobile GPU\u2019s throughput per power is only slightly better than that of the high-end GPU (15.2 GOP/s/W vs 12.9 GOP/s/W).\nThe FPGA implementation from Qiu et al. (2016) is an end-to-end implementation of the 16-layer version of VGG. The FPGA implementation uses 16-bit fixed point arithmetic to reduce memory and computation requirements. Moreover the authors use pruning in fully connected layers to reduce parameter size. Another work by Suda et al. (2016) achieves nearly the same throughput without weight pruning. The FPGA implementation is head-to-head with the embedded GPU implementation. The latter has 11% more throughput and 6% more throughput per power.\n24\nChapter 4\nFixed Point Approximation\nThis chapter covers the approximation of convolutional neural networks with fixed point numbers. While normal inference is done in 32-bit floating point, using bit-width reduced format for intermediate results can increase both throughput and energy efficiency of hardware accelerators."}, {"heading": "4.1 Baseline Convolutional Neural Networks", "text": "In the remainder of this document, we will discuss different approaches for approximating CNNs in a hardware friendly manner. In each section, we approximate the following CNNs:\n1. LeNet1 was proposed by LeCun et al. (1998). This network consists of two con-\nvolutional and two fully connected layers and can be used to classify handwritten digits (MNIST dataset).\n2. The CIFAR-10 data set (Krizhevsky, 2009) has 10 image classes such as airplanes,\nbird, and truck. The CIFAR-10 Full model2 was developed by Caffe for the CIFAR-10 data set. The network has three convolutional layers followed by one fully connected layer. Moreover, the model has two local response normalization (LRN) layers.\n1https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet_train_test.prototxt 2https://github.com/BVLC/caffe/blob/master/examples/cifar10/cifar10_full_train_test.prototxt\n25\n3. CaffeNet3 is the Caffe version of AlexNet (Krizhevsky et al., 2012) which is the\nwinner of the 2012 ILSVRC competition. This network can classify images into the 1000 ImageNet categories, which vary from animal and plant species to various human-made objects. CaffeNet has five convolutional layers, three fully connected layers and two LRN layers. CaffeNet has 60 million parameters and 650,000 neurons.\n4. GoogLeNet4 was proposed by Szegedy et al. (2015) and won the 2014 ILSVRC\ncompetition. This network is based on the inception idea, which uses convolutional and pooling layers with small kernel sizes. GoogLeNet has 12X fewer parameters than AlexNet but manages to improve the accuracy significantly.\n5. SqueezeNet5 by Iandola et al. (2016) was developed with the goal of a small\nnetwork with the accuracy of AlexNet (Krizhevsky et al., 2012). SqueezeNet relies on convolutional layers with 1x1 and 3x3 kernels. No fully connected layers or normalization layers are needed."}, {"heading": "4.2 Fixed Point Format", "text": "Standard inference of deep neural networks uses 32-bit floating point. We replace the parameter and layer outputs with the following fixed point number format: [IL.FL], where IL and FL denote the integer and fractional length of the numbers, respectively. The number of bits used to represent each value is therefor IL+FL. To quantize floating point numbers to fixed point, we use round-nearest. We use 2s-complement numbers, thus the largest positive value we can represent is:\nxmax = 2 IL\u22121 \u2212 2\u2212FL (4.1)\nNote that in the following experiments, all truncated numbers use a shared fixed point format, i.e., they share the same integer and fractional length. For a representation using dynamic adaption of integer and fractional part, please refer to chapter 5.\n3https://github.com/BVLC/caffe/blob/master/models/bvlc_reference_caffenet/train_val.prototxt 4https://github.com/BVLC/caffe/wiki/Model-Zoo 5https://github.com/DeepScale/SqueezeNet/blob/master/SqueezeNet_v1.0/train_val.prototxt\n26"}, {"heading": "4.3 Dynamic Range of Parameters and Layer Out-", "text": "puts\nIn this subsection we analyze the dynamic range of numbers in two neural networks. This analysis will help to understand the optimal choice for integer and fractional bits in fixed point representation."}, {"heading": "4.3.1 Dynamic Range in Small CNN", "text": "In a first step we do this analysis for LeNet. We performed the forward propagation of 100 images to compute intermediate values in the network. The value distribution is shown in Figure 4.1. Note that this histogram data is the result of truncating all values to integer power of two. We can see that on average, parameters are smaller than layer outputs. 99% of the trained network parameters are between 20 and 2\u221210. For fully connected layers however, 99% of the layer outputs are in the range 25...2\u22124.\nIn order to quantize both the layer outputs and network parameters to 8-bit fixed point, a part of the values needs to be saturated. We achieved the best quantization results with the Q.4.4 format. This indicates that large layer outputs are more important than small network parameters.\n27"}, {"heading": "4.3.2 Dynamic Range in Large CNN", "text": "This subsection contains the analysis for the relatively large CaffeNet network. We performed the forward propagation of 50 images on a trained CaffeNet network. The resulting dynamic range is shown in Figure 4.2. Similarly to a small network, parameters tend to be smaller than layer outputs. However, for this large network, the average difference between these two number categories is much larger. This is to be expected, since layer outputs are the result of a multiplication accumulation process, which yields a much larger result for big layers. As a case in point, we can compare the relatively large second parameter layer (447.9M MAC operations) with the relatively small last parameter layer (4.1M MAC operations). While the second layer\u2019s largest value is larger than 29, all values are below 26 in the last layer.\nSince the dynamic range of values is much larger than in LeNet, more bits are required for a fixed point representations. Our experiments show the best 16-bit fixed point results when using the Q9.7 format. Notice that a significant part of the network parameters gets saturated in this quantization, since there are not enough fractional bits. Only very few layer outputs (0.46% in convolutional layers) are too large to be represented, while a large part of the parameters (21.23%) is truncated to zero. Similarly to the analysis with LeNet, large layer outputs are more important than small parameters.\n28"}, {"heading": "4.4 Results", "text": "This subsection covers the results of quantizing trained 32-bit floating point networks to fixed point."}, {"heading": "4.4.1 Optimal Integer and Fractional Length", "text": "For a given network and target bit-width, the layer outputs and network parameters of convolutional and fully connected layers all share the same fixed point format. The bitwidth is the sum of integer and fractional length. The choice of fractional length is crucial and will decide which values need to be saturated. Our quantization procedure tries different partitionings of the bit-width into integer and fractional part. The best setting is retained and the resulting fixed point network is fine-tuned. Notice that different choices for integer and fractional length are conceivable. For example, only the layer output quantization could be considered to find a good partitioning, since the network parameters can be adapted in the fine-tuning step. However, our experiments on three different networks show that a joint optimization of layer outputs and parameters yields the best results after fine-tuning."}, {"heading": "4.4.2 Quantization to Fixed Point", "text": "We quantized three of our baseline networks to fixed point: LeNet, CIFAR-10 and CaffeNet. To calculate relative accuracy of a bit-width reduced network, we divide the fixed point accuracy by the 32-bit floating point accuracy. First we consider the relatively small LeNet network for handwritten digit recognition. The quantization from 32-bit floating point to 8-bit fixed point incurs a relative accuracy loss of 10.3% (see Figure 4.3). After fine-tuning, the absolute accuracy loss shrinks to 0.27% (Table 4.1), indicating LeNet works well in 8-bit fixed point.\n29\nThe second baseline network we consider is CIFAR-10. This network classifies images into classes such as truck, ship, dog, bird. As this is a more challenging task which requires a larger network, the layer outputs are larger too, and the network is more sensitive to quantization errors. When the network is quantized to 8-bit, the network output is random and the accuracy drops to 10%. Since our quantization framework is unable to achieve good results at 8-bit, we double the bit-width. The network works fine in 16-bit, with a relative accuracy loss below 1%. The best results were achieved using 8 integer bits, whereas LeNet only required 4 integer bits (see Table 4.1).\nFinally we quantize CaffeNet, a network for ImageNet classification. As before, 8-bit quantization yields poor results, which is the reason we choose 16-bit fixed point. The\n30\nrelative accuracy loss after quantization is 8.4% and the fine-tuned network achieves an accuracy within 4.5% of the baseline (compared in absolute values).\nIn order to increase the accuracy of the quantized networks, we introduce dynamic\nfixed point in the next section.\n31\nChapter 5\nDynamic Fixed Point Approximation\nIn this chapter we discuss quantization of a floating point CNN to a dynamic fixed point version. We extend the fixed point format to dynamic fixed point, and show how it can be used to further decrease parameter size while maintaining a high prediction accuracy."}, {"heading": "5.1 Mixed Precision Fixed Point", "text": "The data path of fully connected and convolutional layers consists of a series of MAC operations (multiplication and accumulation), as shown in Figure 5.1. The layer activations are multiplied with the network weights, and these multiplication results are accumulated to form the output.\n32\nAs shown by Lin et al. (2015); Qiu et al. (2016), it is a good approach to use mixed precision, i.e., different parts of a CNN use different bit-widths. In Figure 5.1, m and n refer to the number of bits used to represent layer outputs and layer weights, respectively. Multiplication results are accumulated using an adder tree which gets thicker towards the end. The adder outputs in the first level are m+n+ 1 bits wide, and the bit-width grows by 1 bit in each level. In the last level, the bit-width is m + n + lg2(x), where x is the number of multiplication operations per output value. In the last stage, the bias is added to form the layer output. For each network layer, we need to find a good balance between reducing the bit-widths (m and n) and maintaining a good classification accuracy."}, {"heading": "5.2 Dynamic Fixed Point", "text": "The different parts of a CNN have a significant dynamic range. In large layers, the outputs are the result of thousands of accumulations, thus the network parameters are much smaller than the layer outputs. Fixed point has only limited capability to cover a wide dynamic range. Dynamic fixed point can be a good solution to overcome this problem, as shown by Courbariaux et al. (2014). In dynamic fixed point, each number is represented as follows:\n(\u22121)s \u00b7 2\u2212FL B\u22122\u2211 i=0 2i \u00b7 xi (5.1)\nHere B denotes the bit-width, s the sign bit, FL is the fractional length, and x the mantissa bits. Since the intermediate values in a network have different ranges, it is desirable to group fixed point numbers into groups with constant FL. So the number of bits allocated to the fractional part is constant within that group, but different compared to other groups. Each network layer is split into two groups: one for the layer outputs, one for the layer weights. This allows to better cover the dynamic range of both layer outputs and weights, as weights are normally significantly smaller. On the hardware side, it is possible to realize dynamic fixed point arithmetic using bit shifters.\n33\nThe concept of dynamic fixed point is depicted in Figure 5.2, where two numbers are both represented in 8 bits, but belong to a different group (i.e., they have different fractional length)."}, {"heading": "5.2.1 Choice of Number Format", "text": "When we approximate a neural network with dynamic fixed point numbers, we need to choose a number format for each number group. Each layer has two such groups: the layer parameters and the layer outputs. Within each group, all numbers are represented using the same integer and fractional length.\nTo find the optimal set of number formats, we could perform an exhaustive search, however this is not efficient for large neural networks. Instead, we follow a specific rule that automatically determines the required number of integer bits. More specifically, we choose enough bits to avoid saturation. So for a given set of numbers S, the required integer length IL is given by Equation 5.2.\nIL = dlg2(max S x+ 1)e (5.2)\nThis relation defines the integer length of layer parameters. For layer outputs, we reduce the integer length by one, since our experiments show slightly better results this way."}, {"heading": "5.3 Results", "text": "In this section we present the results of approximating 32-bit floating point networks by condensed dynamic fixed point models. All classification accuracies were obtained\n34\nrunning the respective network on the whole validation dataset. We follow the general approximation procedure explained in section 2.6."}, {"heading": "5.3.1 Impact of Dynamic Fixed Point", "text": "We used our Ristretto framework to quantize CaffeNet (AlexNet) into fixed point, and compare traditional fixed point with dynamic fixed point. To allow a simpler comparison, all layer outputs and network parameters share the same bit-width. Results show a good performance of static fixed point for as low as 18-bit (Figure 5.3). However, when reducing the bit-width further, the accuracy starts to drop significantly, while dynamic fixed point has a stable accuracy. We can conclude that dynamic fixed point performs significantly better for such a large network. The reason is that dynamic fixed point allows us to adapt the number of bits allocated to integer and fractional part, according to the dynamic range of different parts of the network."}, {"heading": "5.3.2 Quantization of Individual Network Parts", "text": "In this section, we present the results for approximating different parts of a network. For each experiment, only one category is quantized to dynamic fixed point, and the rest remains in full precision. Table 5.1 shows the quantization impact for three different networks. For each network, we quantize the layer outputs, the convolutional kernels (CONV), and the parameters of fully connected layers (FC) independently. In all three\n35\nnets, the convolution kernels and layer activations can be trimmed to 8-bit with an absolute accuracy change of only 0.3%. Fully connected layers are more affected from trimming weights to 8-bit, the absolute change is maximally 0.9%. Interestingly, LeNet weights can be trimmed to as low as 2 bits, with absolute accuracy change below 0.4%."}, {"heading": "5.3.3 Fine-tuned Dynamic Fixed Point Networks", "text": "Here we report the accuracy of five networks that were condensed and fine-tuned with Ristretto. All networks use dynamic fixed point parameters as well as dynamic fixed point layer outputs for convolutional and fully connected layers. LeNet performs well in 2/4- bit, while CIFAR-10 and the three ImageNet CNNs can be trimmed to 8-bit (see Table 5.2). Surprisingly, these compressed networks still perform nearly as well as their floating point baseline. The relative accuracy drops of LeNet, CIFAR-10 and SqueezeNet are very small (<0.6%), whereas the approximation of the larger CaffeNet and GoogLeNet incurs a slightly higher cost (0.9% and 2.3% respectively).\n36\nThe SqueezeNet (Iandola et al., 2016) architecture was developed with the goal of a small CNN that performs well on the ImageNet data set. Ristretto can make the already small network even smaller, so that its parameter size is less than 2 MB. This condensed network is well- suited for deployment in smart mobile systems.\nAll five 32-bit floating point networks can be approximated well in 8-bit and 4-bit fixed point. For a hardware implementation, this reduces the size of multiplication units by about one order of magnitude. Moreover, the required memory bandwidth is reduced by 4\u20138X. Finally, it helps to hold 4\u20138X more parameters in on-chip buffers.\nSome previous work (Courbariaux et al., 2014) concentrated on training with fixed point arithmetic from the start and shows little performance decline for as short as 7- bit fixed point numbers on LeNet. Our approach is different in that we train with high numerical precision, then quantize to fixed point, and finally fine-tune the fixed point network. Our condensed model achieves superior accuracy with as low as 4-bit fixed point, on the same data set. While more sophisticated data compression schemes could be used to achieve higher network size reduction, our approach is very hardware friendly and imposes no additional overhead such as decompression.\n37\nChapter 6\nMinifloat Approximation"}, {"heading": "6.1 Motivation", "text": "Chapters 4 and 5 concentrated on fixed point approximation of deep CNNs. Since the training of neural networks is normally done in floating point, it is an intuitive approach to condense these models to smaller floating point numbers. This section analyses the network approximation through minifloat, i.e., floating point numbers with 16 bits or smaller."}, {"heading": "6.2 IEEE-754 Single Precision Standard", "text": "According to IEEE-754 standard, single precision numbers have 1 sign bit, 8 exponent bits and 23 mantissa bits. The mantissa\u2019s first bit (always \u20191\u2019) is added implicitly, and the stored exponent is biased by 127. Numbers with all zeros or ones in the exponent have a special meaning. An exponent with all zeros either represents the number 0 or a denormalized number, depending on the mantissa bits. For the case of all ones in the exponent, the number represented is either +/-INF or NaN."}, {"heading": "6.3 Minifloat Number Format", "text": "In order to condense networks and reduce their computational and memory requirements, we will represent floating point numbers with much fewer bits than the IEEE-754 standard. We follow the standard to a large degree when going to 12-bit, 8-bit, or 6-bit numbers, but our format differs in some details. Namely, the exponent bias is lowered according to\n38\nthe number of bits assigned to the exponent:\nbias = 2bits\u22121 \u2212 1 (6.1)\nHere bits denotes the number of bits assigned to the exponent. Another difference to the IEEE standard is that we don\u2019t support denormalized numbers, INF and NaN. INF is replaced by saturated numbers, and denormalized numbers are replace by 0. Finally, the number of bits assigned to the exponent and mantissa part don\u2019t follow a specific rule. To be more precise, our Ristretto framework automatically searches for the best balance between exponent and mantissa bits. As a case in point, a 16-bit minifloat number (Figure 6.1) could be represented with 1 sign bit, 5 exponent bits and 10 mantissa bits."}, {"heading": "6.3.1 Network-specific Choice of Number Format", "text": "Similar to dynamic fixed point, we need to choose a specific number format per bit-width, i.e., partition the available bits into exponent and mantissa. In order to approximate a neural network with minifloat numbers, we need to find a suitable number of exponent and mantissa bits. We use enough exponent bits to avoid saturation:\nbits = dlg2(lg2(max S x)\u2212 1) + 1e (6.2)\nS is the set of numbers which we approximate. This choice of exponent bits assures\nno saturation happens, under the assumption that we use infinitely many mantissa bits."}, {"heading": "6.4 Data Path for Accelerator", "text": "The data path of convolutional and fully connected layers is depicted in Figure 6.2. For simplicity, we only consider fixed precision arithmetic, i.e., all number categories shared the same minifloat format. Similar to the fixed point data path, network parameters and\n39\nlayer inputs are multiplied and accumulated. Input to each multiplier is a pair of numbers, each in minifloat format. The output of each multiplier is 3 bits wider than the input numbers. In a next step, the multiplication results are accumulated in full precision. In a last step the bias is added in minifloat format, and the final result is trimmed to minifloat.\nWhen implemented in a hardware accelerator, the data path\u2019s input and output are minifloat numbers. In case the neural network in question is too large to fit into on-chip memory, the layer outputs and parameters need to be stored in off-chip memory. Since both these number categories are represented in minifloat, we can achieve significant energy savings thanks to reduced data transfer."}, {"heading": "6.5 Results", "text": "In this section, we analyze the impact of lowering the bit-width of floating point numbers. We used our approximation framework to query different CNNs which use minifloat numbers in convolutional and fully connected layers. To find the accuracy of the condensed networks, we follow the quantization flow described in section 2.6.\nWe quantized three CNNs to 12, 8 and 6-bit minifloat. The quantization is done for layer outputs and parameters of fully connected and convolutional layers. For each network, we show the classification accuracy of both the 32-bit baseline, followed by minifloat versions (Figure 6.3). We calculate the normalized accuracy by dividing the\n40\nminifloat network\u2019s performance by the 32-bit floating point network accuracy.\nThe results indicate that LeNet has no classification loss when shrinking layer outputs and layer parameters to 8-bit. CIFAR-10 and CaffeNet can be used in 12-bit, again with no loss in accuracy.\nWe fine-tuned the 8-bit versions of the three networks. Table 6.1 shows the accuracy of the minifloat networks. CIFAR-10 has an absolute accuracy drop below 1%, and CaffeNet incurs an absolute drop of 4.6%. For LeNet, minifloat actually increases accuracy. Minifloat adds more regularization to LeNet and increases the accuracy by 0.05%, compared to the 32-bit network.\nCompared to dynamic fixed point results in section 5.3.3, minifloat requires more bits. For 8-bit CaffeNet, the absolute accuracy drop of dynamic fixed point is small (below 1%), whereas minifloat incurs a relatively large drop (4.38%)."}, {"heading": "6.6 Comparison to Previous Work", "text": "Previous work (Deng et al.) approximated network parameters of AlexNet (CaffeNet) with 8-bit minifloat. They analyze the impact of quantizing a varying percentage of the network parameters. Our results achieve significantly better accuracy, thanks to a careful choice of minifloat format and a fine-tuning step.\n42\nChapter 7\nTurning Multiplications Into Bit\nShifts"}, {"heading": "7.1 Multiplier-free Arithmetic", "text": "Hardware accelerators for convolutional neural networks need to be energy-efficient to allow for deployment in mobile devices. Fully connected layers and convolutional layers consist of additions and multiplications, of which the latter requires a much larger chip area. This motivated previous research to eliminate all multipliers by using integer power of two weights (Tang and Kwan, 1993; Mahoney and Elhanany, 2008). These weights can be considered as minifloat numbers with zero mantissa bits. By using such weights, all multiplications turn into bit shifts, which can save a significant amount of energy on a hardware accelerator.\nWe now detail the approximation of convolutional and fully connected layers with multiplier-free arithmetic. Although we assume strictly positive weights in this discussion, it is straight forward the expand the approximation procedure to both positive and negative weights. The computation of convolutional and fully connected layers consists of multiplication-and-accumulation operations. Equation 7.1 shows the necessary operations in a full precision network. The layer inputs xj are multiplied with layer parameters wj and the accumulation yields the result zi. To simplify this discussion, we assume the input data has been rearranged such that output zi = w T \u00b7 x. In order to switch to multiplier-free arithmetic, we first approximate parameters by the closest integer-power-\n43\nof-two number (Equation 7.2). Now output zi can be approximated by equation 7.3 which is multiplier-free. Notice that the last equation relies on the power-of-two exponents ej, not the original parameters.\nzi = \u2211 j xj \u00b7 wj (7.1)\nej = round(lg2(wj)) (7.2)\nzi \u2248 \u2211 j xj<<ej (7.3)"}, {"heading": "7.2 Maximal Number of Shifts", "text": "Nearly all network weights of a trained network are between +1 and \u22121, but most of them are close to zero. The quantization of these parameters to power-of-two has the highest impact (in terms of absolute value change) to the weights close to +1 and \u22121. These weights will only be able to take on the values 1, 1 2 , 1 4 and so on.\nWe encode the number of shifts in 4 bits (see Figure 7.1). This implies the parameter exponents can have 8 different values. We choose to represent the exponent values such that ei \u2208 [\u22128, ...,\u22121] and use this format for the subsequent experiments.\nThe motivation for this format is two-fold. First of all, using only 4 bits for parameters reduces the memory requirements tremendously. Second, the smallest possible value in this format is 2\u22128. Parameters smaller than that have only a minor effect on the network output. Moreover only few parameters are below this smallest value. Our analysis from section 4.3.2 shows only 10.97% of parameters in CaffeNet are lower than the smallest possible value. For LeNet, this percentage is even smaller (5.83%).\n44"}, {"heading": "7.3 Data Path for Accelerator", "text": "The adder-only arithmetic of convolutional and fully connected layers is shown Figure 7.2. The 4-bit parameters indicate how many bit-shifts are required for the layer inputs. To enable shifts by multiple bits in one clock cycle, barrel shifters should be used. Notice that this data path has no multipliers at all, which can potentially save significant chip area. To simplify this analysis, we only focus on the impact of removing multiplications. The layer inputs and outputs are kept in full precision format."}, {"heading": "7.4 Results", "text": "We used our Ristretto framework to simulate the effect of removing all multiplications from convolutional and fully connected layers. Our framework quantizes all network\n45\nparameters to the nearest integer-power-of-two number. Table 7.1 compares networks with power-of-two weights and networks with single precision weights.\nFor LeNet, the absolute classification accuracy drop for the quantized weights is 0.1%. CIFAR-10 and CaffeNet are more affected by the weight quantization (4.21% and 3.65% absolute accuracy drop). At first glance, the results for the larger two networks might be discouraging. However, it is surprising that the nets with weight quantization still have a decent classification accuracy. The power-of-two weights can be stored in just 4 bits (the exponents range from -1 to -8). This allows for tremendous energy savings: First the traffic to off-chip memory is reduced, as the weights are not 32-bit but 4-bit. Second, multipliers are replaced with simple bit shifters.\n46\nChapter 8\nComparison of Different\nApproximations\nIn this chapter, we compare the different approximation strategies for convolutional neural networks. For this purpose, we consider three networks: LeNet, CIFAR-10 and CaffeNet. We analyze how well the approximation schemes can lower the bit-width without hurting accuracy. In all experiments, the parameters and layer outputs of convolutional and fully connected layers are condensed to smaller bit-width. The approximation results without fine-tuning are shown in Figures 8.1, 8.2, 8.3. For all three neural networks, dynamic fixed point has the best performance, followed by minifloat approximation. All approximation schemes perform well at 16-bit, but as we lower the bit-width the accuracy drops.\n47"}, {"heading": "8.1 Fixed Point Approximation", "text": "Fixed point is the approximation scheme that requires the least energy and development time for a hardware accelerator. However, it is also the approximation with the poorest performance for small bit-widths. For CaffeNet for example, the dynamic range of values is significant, as shown in subsection 4.3.2. In 15-bit convolutional layers, 0.45% of layer outputs are too large and need to be represented in saturated format. When moving to 14- bit fixed point, 2.82% of the layer outputs are saturated. Since large layer outputs are very important for the network\u2019s accuracy, this leads to a significant accuracy drop (see\n48\nFigure 8.1)."}, {"heading": "8.2 Dynamic Fixed Point Approximation", "text": "Dynamic fixed point shows the best performance among the three approximation schemes. Dynamic fixed point combines the advantages of both fixed point and minifloat: On one hand this format allows to use all bits for the mantissa part which helps for a good accuracy. On the other hand dynamic fixed point can cover a large dynamic range, just like floating point, thanks to the exponent that is stored implicitly. LeNet can be approximated with just 5-bit numbers, achieving the same accuracy as the 32-bit floating point model. The same holds true for an 8-bit CIFAR-10 network. Finally the CaffeNet architecture can be approximated with 8-bit dynamic fixed point, at an absolute accuracy drop of 0.3%."}, {"heading": "8.3 Minifloat Approximation", "text": "Floating point numbers can cover a large dynamic range, thanks to their exponent. Minifloat performs significantly better than static fixed point. However minifloat approximation shows a sharp accuracy drop when going to very low bit-widths. This sharp drop is at the point where there are not enough bits for the exponent. For LeNet for example, the accuracy of 5-bit arithmetic is 97.96%. In this setting, we use 4 bits for the exponent, no mantissa bits and one sign bit. When we lower the bit-width further, the exponent is unable to cover the dynamic range of values, and the accuracy drops sharply to 10.09%. For the other two networks, we can see a similar effect. Both CIFAR-10 and CaffeNet need 5 exponent bits, according to Equation 6.2. Since we need one more bit for the sign, those two networks need at least 6-bit minifloat numbers in order to achieve good classification performance."}, {"heading": "8.4 Summary", "text": "Dynamic Fixed point is very well suited for approximation of neural networks. This approximation shows the best accuracy at low bit-widths. Although dynamic fixed point requires some more chip area than pure fixed point arithmetic, this approximation is very\n49\nwell suited for hardware acceleration of neural networks. The bit-width can be reduced to 4-bit or 8-bit for LeNet, CIFAR-10, and CaffeNet. This reduces the required memory bandwidth and footprint significantly, which is expected to yield significant energy savings for FPGA and ASIC designs.\n50\nChapter 9\nRistretto: An Approximation\nFramework for Deep CNNs"}, {"heading": "9.1 From Caffe to Ristretto", "text": "According to Wikipedia, Ristretto is \u2018a short shot of espresso coffee made with the normal amount of ground coffee but extracted with about half the amount of water\u2019. Similarly, our compressor removes the unnecessary parts of a CNN, while making sure the essence - the ability to predict classes from images - is preserved. With its strong community and fast training for deep CNNs, Caffe created by Jia et al. (2014) is an excellent framework to build on. Ristretto takes a trained model as input, and automatically brews a condensed network version. Input and output of Ristretto are a network description file (prototxt) and the network parameters. The condensed model in Caffe-format can then be used for a hardware accelerator."}, {"heading": "9.2 Quantization Flow", "text": "Ristretto can condense any 32-bit floating point network to either fixed point, minifloat or integer power of two parameters. Ristretto\u2019s quantization flow has five stages (Figure 9.1). In the first step, the dynamic range of the weights is analyzed to find a compressed number representation. For dynamic fixed point, Ristretto allocates enough bits to the integer part to avoids saturation of large values. Similarly, for minifloat approximation, the framework makes sure enough bits are allocated to the exponent. To quantize full\n51\nprecision numbers into a smaller number format, Ristretto uses round nearest even.\nThe second step runs several thousand images in forward path. The generated layer activations are analyzed to generate statistical parameters. Ristretto allocates enough bits to the new number format to avoid saturation of layer activations.\nNext Ristretto performs a binary search to find the optimal number of bits for convolutional weights, fully connected weights, and layer outputs. For dynamic fixed point, a certain network part is quantized, while the rest remains in floating point. Since there are three network parts that should use independent bit-widths, iteratively quantizing one network part allows us to find the optimal bit-width for each part. Once a good trade-off between small number representation and classification accuracy is found, the resulting network can be fine-tuned."}, {"heading": "9.3 Fine-tuning", "text": "In order to make up for the accuracy drop incurred by quantization, the quantized network is fine-tuned in Ristretto. During this retraining procedure, the network learns how to classify images with discrete-valued parameters w\u2032. During fine-tuning, we will calculate small weight updates \u2206w. Since these small weight updates may be below the quantization step size of the discrete parameters, we also keep a set of full precision weights w.\nRistretto uses the fine-tuning procedure shown in Figure 9.2. For each batch, the full precision weights are quantized to reduced-precision format. During forward propagation, these discrete weights are used to compute the layer outputs zl . Each layer l turns its input batch xl into output zl, according to its function fl : (xl, w \u2032) \u2192 zl. Assuming the last layer computes the loss, we denote f as the overall CNN function. The goal of back\n52\npropagation is to compute the error gradient \u03b4f \u03b4w with respect to each quantized parameter. For parameter updates we use the Adam rule by Kingma and Ba (2015). As an important observation, we do not quantize layer outputs during fine-tuning. We use floating point layer outputs instead, which enables Ristretto to analytically compute the error gradient with respect to each parameter. In contrast, scoring of the network is done with reduced precision layer outputs.\nTo achieve the best fine-tuning results, we used a learning rate that is an order of magnitude lower than the last full precision training iteration. Since the choice of hyper parameters for retraining is crucial (Bergstra and Bengio, 2012), Ristretto relies on minimal human intervention in this step."}, {"heading": "9.4 Fast Forward and Backward Propagation", "text": "Ristretto brews a condensed network with reduced precision weights and layer activations. For simulation of the forward propagation in hardware, Ristretto uses full floating point for accumulation. This follows the thought of Gupta et al. (2015) and is conform with our description of the forward data path in subsection 2.6.1.1. During fine-tuning, the full precision weights need to be quantized for each batch, but after that all computation can be done in floating point (Figure 9.2). Therefore Ristretto can fully leverage opti-\n53\nmized matrix- matrix multiplication routines for both forward and backward propagation. Thanks to its fast implementation on the GPU, a fixed point CaffeNet can be tested on the ILSVRC 2014 validation dataset (50k images) in less than 2 minutes (using one Tesla K-40 GPU)."}, {"heading": "9.5 Ristretto From a User Perspective", "text": "Ristretto is based on the highly optimized Caffe-framework and follows its principles. A Caffe user will appreciate the smooth and easy-to-understand integration of Ristretto.\nCaffe: Development of an image processing algorithm in Caffe starts with a network description file (see Figure 9.3). This file is written by the user and contains the hyper parameters of the neural network architecture. The Caffe framework uses the Google Protocol Buffer format to encode the network information. Networks are represented as directed acyclic graphs. The vertices are layers which do computation based on the input data and layer parameters. Data flows from one layer to another in so called \u2018blobs\u2019.\nAs second item, the Caffe user needs a labeled training data set. Finally Caffe requires a solver file, which contains the hyper parameters for training, such as initial learning rate and training duration.\nOnce the network description file, the data set and the solver are prepared, the Caffe tool can be used for training. The result of training is a file containing the trained network parameters. This parameter file \u2013 together with the network description file \u2013 can be deployed for classification of arbitrary images.\n54\nRistretto: Our approximation framework can be seen as a feature extension to Caffe. In fact, for the network compression pipeline described in Figure 9.4, some steps require the traditional Caffe tool (which was left as-is). Ristretto starts where Caffe ends: a trained network serves as input to the quantization pipeline. The user has several options to choose from for quantization, for instance he can set the error margin and an approximation strategy. The Ristretto tool quantizes the trained 32-bit floating point network to the smallest possible bit-width representation. Ristretto produces the network description of the condensed model, which follows the format of Caffe. The condensed model contains limited-precision layers, which are a Ristretto-specific feature. Moreover each layer has quantization parameters, such as the number of integer and fractional bits in the case of fixed point approximation.\nAt this point, the quantized model description could be used to score the network on a data set. However, in order to increase accuracy, the user is advised to fine-tune the new model. The user writes a new solver file which will be used for fine-tuning. The Caffe tool will fine-tune the condensed model to achieve the highest possible accuracy."}, {"heading": "9.6 Release of Ristretto", "text": "Ristretto is released as open source project and has the following strengths:\n\u2022 Automation: Ristretto performs automatic trimming of any given CNN.\n\u2022 Flexibility: Various trimming schemes are supported.\n55\n\u2022 Accuracy: Ristretto fine-tunes trimmed networks.\n\u2022 Speed: Ristretto runs on the GPU and leverages optimized CUDA routines.\nRistretto has a homepage1 and the source code is available2."}, {"heading": "9.7 Future Work", "text": "Ristretto follows the modular source code architecture of Caffe. New features such as new limited precision layer types can be added to Ristretto easily. In this section we discuss different possible future steps."}, {"heading": "9.7.1 Network Pruning", "text": "The most energy-costly operation for CNN accelerators is off-chip memory access. Since large networks don\u2019t fit into on-chip memory, it is imperative to compress the network. Most network weights come from fully connected layers. It has been shown that a significant part of the connections in fully connected layers can be removed. Previous work (Han et al., 2016b) achieves high network compression rates with no loss in classification accuracy."}, {"heading": "9.7.2 Binary Networks", "text": "The first published work to represent ImageNet networks with binary weights was by Rastegari et al. (2016). Their results show that very deep networks can be approximated with binary weights, although at an accuracy drop of around 3% for CaffeNet and 6% for GoogLeNet. Substituting 32-bit floating point parameters with just one bit of information necessarily reduces the network\u2019s accuracy to extract features from images. The challenge with binary parameters is to achieve high prediction accuracy without increasing the parameter size or adding additional computational overhead."}, {"heading": "9.7.3 C-Code Generation", "text": "Many academic projects use high-level synthesis tools for FPGA and ASIC based accelerators. The standard development tool-chain starts with a C-implementation of the\n1http://ristretto.lepsucd.com/ 2https://github.com/pmgysel/caffe\n56\nalgorithm, which then undergoes many unit tests to verify correct functionality. In a next step the C-code is manually converted to System-C which serves as input to high level synthesis (HLS). Highly optimized HLS tools can produce very efficient Verilog code within a fraction of the time which would be needed for manual Verilog coding. We plan to add a feature to Ristretto which allows for automatic generation of the C-code of a condensed network. This feature will produce the necessary code files as well as a dump of the extracted low-precision parameters.\nWe hope that Ristretto will enable researchers to speed up their development time for CNN accelerators. Hopefully Ristretto will gain traction in the community. As it is an open-source project, the community can help adding new features to the framework, which will make the tool even more powerful.\n57\nReferences\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S.,\nDavis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mane\u0301, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Vie\u0301gas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X. TensorFlow: LargeScale Machine Learning on Heterogeneous Systems, 2015. URL http://tensorflow. org/.\nBengio, Y., Boulanger-Lewandowski, N., and Pascanu, R. Advances in Optimizing Re-\ncurrent Networks. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8624\u20138628. IEEE, 2013.\nBergstra, J. and Bengio, Y. Random Search for Hyper-Parameter Optimization. The\nJournal of Machine Learning Research, 13(1):281\u2013305, 2012.\nChen, Y.-H., Krishna, T., Emer, J., and Sze, V. Eyeriss: An Energy-Efficient Recon-\nfigurable Accelerator for Deep Convolutional Neural Networks. In IEEE International Solid-State Circuits Conference, ISSCC 2016, Digest of Technical Papers, pages 262\u2013 263, 2016.\nChen, Y., Luo, T., Liu, S., Zhang, S., He, L., Wang, J., Li, L., Chen, T., Xu, Z., Sun,\nN., and Temam, O. DaDianNao: A Machine-Learning Supercomputer. In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture, pages 609\u2013622. IEEE Computer Society, 2014.\nChetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B., and\nShelhamer, E. cuDNN: Efficient Primitives for Deep Learning. arXiv preprint arXiv:1410.0759, 2014.\n58\nCourbariaux, M., David, J.-P., and Bengio, Y. Training Deep Neural Networks with Low\nPrecision Multiplications. arXiv preprint arXiv:1412.7024, 2014.\nCourbariaux, M., Bengio, Y., and David, J.-P. BinaryConnect: Training Deep Neural\nNetworks with binary weights during propagations. In Advances in Neural Information Processing Systems, pages 3105\u20133113, 2015.\nCourbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y. Binarized Neural\nNetworks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1. arXiv preprint arXiv:1602.02830, 2016.\nDalal, N. and Triggs, B. Histograms of oriented gradients for human detection. In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition, 2005, volume 1, pages 886\u2013893. IEEE, 2005.\nDeng, Z., Xu, C., Cai, Q., and Faraboschi, P. Reduced-Precision Memory Value\nApproximation for Deep Learning. http://www.labs.hpe.com/techreports/2015/ HPL-2015-100.html. Accessed: 2016-05-09.\nDenton, E. L., Zaremba, W., Bruna, J., LeCun, Y., and Fergus, R. Exploiting Linear\nStructure Within Convolutional Networks for Efficient Evaluation. In Advances in Neural Information Processing Systems, pages 1269\u20131277, 2014.\nDu, Z., Lingamneni, A., Chen, Y., Palem, K. V., Temam, O., and Wu, C. Leveraging the\nError Resilience of Neural Networks for Designing Highly Energy Efficient Accelerators. Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on, 34 (8):1223\u20131235, 2015.\nGatys, L. A., Ecker, A. S., and Bethge, M. A Neural Algorithm of Artistic Style. arXiv\npreprint arXiv:1508.06576, 2015.\nGirshick, R. Fast R-CNN. In Proceedings of the IEEE International Conference on\nComputer Vision, pages 1440\u20131448, 2015.\n59\nGupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. Deep Learning with\nLimited Numerical Precision. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1737\u20131746, 2015.\nGysel, P., Motamedi, M., and Ghiasi, S. Hardware-oriented Approximation of Convolu-\ntional Neural Networks. arXiv preprint arXiv:1604.03168, 2016.\nHan, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M. A., and Dally, W. J.\nEIE: Efficient Inference Engine on Compressed Deep Neural Network. arXiv preprint arXiv:1602.01528, 2016a.\nHan, S., Mao, H., and Dally, W. J. Deep Compression: Compressing Deep Neural Net-\nworks with Pruning, Trained Quantization and Huffman Coding. In International Conference on Learning Representations, 2016b.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep Residual Learning for Image Recognition.\narXiv preprint arXiv:1512.03385, 2015.\nHinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A.-R., Jaitly, N., Senior, A.,\nVanhoucke, V., Nguyen, P., Sainath, T. N., et al. Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups. Signal Processing Magazine, IEEE, 29(6):82\u201397, 2012.\nIandola, F. N., Moskewicz, M. W., Ashraf, K., Han, S., Dally, W. J., and Keutzer, K.\nSqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size. arXiv:1602.07360, 2016.\nIoffe, S. and Szegedy, C. Batch Normalization: Accelerating Deep Network Training by\nReducing Internal Covariate Shift. In Proceedings of The 32nd International Conference on Machine Learning, pages 448\u2013456, 2015.\nJia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama,\nS., and Darrell, T. Caffe: Convolutional Architecture for Fast Feature Embedding.\n60\nIn Proceedings of the ACM International Conference on Multimedia, pages 675\u2013678. ACM, 2014.\nKarpathy, A. Stanford CS231n course: Convolutional Neural Networks for Visual Recog-\nnition. http://cs231n.github.io/neural-networks-1/. Accessed: 2016-05-09.\nKingma, D. and Ba, J. Adam: A Method for Stochastic Optimization. In International\nConference on Learning Representations, 2015.\nKrizhevsky, A. Learning Multiple Layers of Features from Tiny Images. Master\u2019s thesis,\nUniversity of Toronto, 2009.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet Classification with Deep\nConvolutional Neural Networks. In Advances in Neural Information Processing Systems, pages 1097\u20131105, 2012.\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-Based Learning Applied to\nDocument Recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\nLin, D., Talathi, S., and Annapureddy, S. Fixed Point Quantization of Deep Convolutional\nNetworks. arXiv preprint arXiv:1511.06393, 2015.\nLin, Z., Courbariaux, M., Memisevic, R., and Bengio, Y. Neural Networks with Few\nMultiplications. In International Conference on Learning Representations, 2016.\nLowe, D. G. Distinctive Image Features from Scale-Invariant Keypoints. International\nJournal of Computer Vision, 60(2):91\u2013110, 2004.\nMahoney, V. and Elhanany, I. A backpropagation neural network design using adder-only\narithmetic. In 51st Midwest Symposium on Circuits and Systems, 2008, pages 894\u2013897. IEEE, 2008.\nMathieu, M., Henaff, M., and LeCun, Y. Fast Training of Convolutional Networks through\nFFTs. arXiv preprint arXiv:1312.5851, 2013.\n61\nMotamedi, M., Gysel, P., and Ghiasi, S. PLACID: A Platform for Accelerator Creation\nfor DCNNs. Under review, 2016.\nNair, V. and Hinton, G. E. Rectified Linear Units Improve Restricted Boltzmann Ma-\nchines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814, 2010.\nQiu, J., Wang, J., Yao, S., Guo, K., Li, B., Zhou, E., Yu, J., Tang, T., Xu, N., Song,\nS., Wang, Y., and Yang, H. Going Deeper with Embedded FPGA Platform for Convolutional Neural Network. In Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, pages 26\u201335, 2016.\nRastegari, M., Ordonez, V., Redmon, J., and Farhadi, A. XNOR-Net: ImageNet Classifi-\ncation Using Binary Convolutional Neural Networks. arXiv preprint arXiv:1603.05279, 2016.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy,\nA., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):211\u2013 252, 2015.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G.,\nSchrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.\nSim, J., Park, J.-S., Kim, M., Bae, D., Choi, Y., and Kim, L.-S. A 1.42 TOPS/W deep\nconvolutional neural network recognition processor for intelligent IoE systems. In 2016 IEEE International Solid-State Circuits Conference (ISSCC), pages 264\u2013265. IEEE, 2016.\nSimonyan, K. and Zisserman, A. Very Deep Convolutional Networks for Large-Scale\nImage Recognition. In International Conference on Learning Representations, 2015.\n62\nSuda, N., Chandra, V., Dasika, G., Mohanty, A., Ma, Y., Vrudhula, S., Seo, J.-s., and\nCao, Y. Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks. In Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, pages 16\u201325. ACM, 2016.\nSung, W., Shin, S., and Hwang, K. Resiliency of Deep Neural Networks under Quantiza-\ntion. arXiv preprint arXiv:1511.06488, 2015.\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke,\nV., and Rabinovich, A. Going Deeper with Convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139, 2015.\nTang, C. Z. and Kwan, H. K. Multilayer Feedforward Neural Networks with Single\nPowers-of-Two Weights. Signal Processing, IEEE Transactions on, 41(8):2724\u20132727, 1993.\nTheano Development Team. Theano: A Python framework for fast computation of\nmathematical expressions. arXiv e-prints, abs/1605.02688, May 2016. URL http: //arxiv.org/abs/1605.02688.\nZhang, C., Li, P., Sun, G., Guan, Y., Xiao, B., and Cong, J. Optimizing FPGA-based\nAccelerator Design for Deep Convolutional Neural Networks. In Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, pages 161\u2013170. ACM, 2015.\n63"}], "references": [{"title": "Advances in Optimizing Recurrent Networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Random Search for Hyper-Parameter Optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bergstra and Bengio,? \\Q2012\\E", "shortCiteRegEx": "Bergstra and Bengio", "year": 2012}, {"title": "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks", "author": ["Chen", "Y.-H", "T. Krishna", "J. Emer", "V. Sze"], "venue": "In IEEE International Solid-State Circuits Conference,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "DaDianNao: A Machine-Learning Supercomputer", "author": ["Y. Chen", "T. Luo", "S. Liu", "S. Zhang", "L. He", "J. Wang", "L. Li", "T. Chen", "Z. Xu", "N. Sun", "O. Temam"], "venue": "In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "cuDNN: Efficient Primitives for Deep Learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": "arXiv preprint arXiv:1410.0759,", "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Training Deep Neural Networks with Low Precision Multiplications", "author": ["M. Courbariaux", "David", "J.-P", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "David", "J.-P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1", "author": ["M. Courbariaux", "I. Hubara", "D. Soudry", "R. El-Yaniv", "Y. Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Dalal and Triggs,? \\Q2005\\E", "shortCiteRegEx": "Dalal and Triggs", "year": 2005}, {"title": "Reduced-Precision Memory Value Approximation for Deep Learning. http://www.labs.hpe.com/techreports/2015/ HPL-2015-100.html", "author": ["Z. Deng", "C. Xu", "Q. Cai", "P. Faraboschi"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2016}, {"title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Leveraging the Error Resilience of Neural Networks for Designing Highly Energy Efficient Accelerators", "author": ["Z. Du", "A. Lingamneni", "Y. Chen", "K.V. Palem", "O. Temam", "C. Wu"], "venue": "Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on,", "citeRegEx": "Du et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Du et al\\.", "year": 2015}, {"title": "A Neural Algorithm of Artistic Style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Girshick,? \\Q2015\\E", "shortCiteRegEx": "Girshick", "year": 2015}, {"title": "Deep Learning with Limited Numerical Precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Hardware-oriented Approximation of Convolutional Neural Networks", "author": ["P. Gysel", "M. Motamedi", "S. Ghiasi"], "venue": "arXiv preprint arXiv:1604.03168,", "citeRegEx": "Gysel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gysel et al\\.", "year": 2016}, {"title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "Mohamed", "A.-R", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "Sainath", "T. N"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size", "author": ["F.N. Iandola", "M.W. Moskewicz", "K. Ashraf", "S. Han", "W.J. Dally", "K. Keutzer"], "venue": null, "citeRegEx": "Iandola et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Iandola et al\\.", "year": 2016}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Stanford CS231n course: Convolutional Neural Networks for Visual Recognition. http://cs231n.github.io/neural-networks-1", "author": ["A. Karpathy"], "venue": null, "citeRegEx": "Karpathy,? \\Q2016\\E", "shortCiteRegEx": "Karpathy", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, University of Toronto,", "citeRegEx": "Krizhevsky,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-Based Learning Applied to Document Recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Fixed Point Quantization of Deep Convolutional Networks", "author": ["D. Lin", "S. Talathi", "S. Annapureddy"], "venue": "arXiv preprint arXiv:1511.06393,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Neural Networks with Few Multiplications", "author": ["Z. Lin", "M. Courbariaux", "R. Memisevic", "Y. Bengio"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Distinctive Image Features from Scale-Invariant Keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Lowe,? \\Q2004\\E", "shortCiteRegEx": "Lowe", "year": 2004}, {"title": "A backpropagation neural network design using adder-only arithmetic", "author": ["V. Mahoney", "I. Elhanany"], "venue": "In 51st Midwest Symposium on Circuits and Systems,", "citeRegEx": "Mahoney and Elhanany,? \\Q2008\\E", "shortCiteRegEx": "Mahoney and Elhanany", "year": 2008}, {"title": "Fast Training of Convolutional Networks through FFTs", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.5851,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "PLACID: A Platform for Accelerator Creation for DCNNs", "author": ["M. Motamedi", "P. Gysel", "S. Ghiasi"], "venue": "Under review,", "citeRegEx": "Motamedi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Motamedi et al\\.", "year": 2016}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Going Deeper with Embedded FPGA Platform for Convolutional Neural Network", "author": ["J. Qiu", "J. Wang", "S. Yao", "K. Guo", "B. Li", "E. Zhou", "J. Yu", "T. Tang", "N. Xu", "S. Song", "Y. Wang", "H. Yang"], "venue": "In Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,", "citeRegEx": "Qiu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2016}, {"title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "A 1.42 TOPS/W deep convolutional neural network recognition processor for intelligent IoE systems", "author": ["J. Sim", "Park", "J.-S", "M. Kim", "D. Bae", "Y. Choi", "Kim", "L.-S"], "venue": "IEEE International Solid-State Circuits Conference (ISSCC),", "citeRegEx": "Sim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sim et al\\.", "year": 2016}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Simonyan and Zisserman,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks", "author": ["N. Suda", "V. Chandra", "G. Dasika", "A. Mohanty", "Y. Ma", "S. Vrudhula", "Seo", "J.-s", "Y. Cao"], "venue": "In Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,", "citeRegEx": "Suda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Suda et al\\.", "year": 2016}, {"title": "Resiliency of Deep Neural Networks under Quantization", "author": ["W. Sung", "S. Shin", "K. Hwang"], "venue": "arXiv preprint arXiv:1511.06488,", "citeRegEx": "Sung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sung et al\\.", "year": 2015}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Multilayer Feedforward Neural Networks with Single Powers-of-Two Weights", "author": ["C.Z. Tang", "H.K. Kwan"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Tang and Kwan,? \\Q1993\\E", "shortCiteRegEx": "Tang and Kwan", "year": 1993}, {"title": "Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks", "author": ["C. Zhang", "P. Li", "G. Sun", "Y. Guan", "B. Xiao", "J. Cong"], "venue": "In Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 37, "context": "(2012), VGG (Simonyan and Zisserman, 2015), GoogleNet (Szegedy et al.", "startOffset": 12, "endOffset": 42}, {"referenceID": 40, "context": "(2012), VGG (Simonyan and Zisserman, 2015), GoogleNet (Szegedy et al., 2015) and ResNet (He et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 17, "context": ", 2015) and ResNet (He et al., 2015).", "startOffset": 19, "endOffset": 36}, {"referenceID": 4, "context": ", 2014) and Nvidia\u2019s cuDNN (Chetlur et al., 2014), a Tesla K-40 GPU can process an image in just 4ms.", "startOffset": 27, "endOffset": 49}, {"referenceID": 38, "context": "Various FPGA-based accelerators (Suda et al., 2016; Qiu et al., 2016) have proven that it is possible to use reconfigurable hardware for end-to-end inference of large CNNs like AlexNet and VGG.", "startOffset": 32, "endOffset": 69}, {"referenceID": 33, "context": "Various FPGA-based accelerators (Suda et al., 2016; Qiu et al., 2016) have proven that it is possible to use reconfigurable hardware for end-to-end inference of large CNNs like AlexNet and VGG.", "startOffset": 32, "endOffset": 69}, {"referenceID": 21, "context": "This annually held competition has seen state-of-the-art image classification accuracies by deep networks such as AlexNet by Krizhevsky et al. (2012), VGG (Simonyan and Zisserman, 2015), GoogleNet (Szegedy et al.", "startOffset": 125, "endOffset": 150}, {"referenceID": 2, "context": "CNNs (Chen et al., 2016; Sim et al., 2016; Han et al., 2016a).", "startOffset": 5, "endOffset": 61}, {"referenceID": 36, "context": "CNNs (Chen et al., 2016; Sim et al., 2016; Han et al., 2016a).", "startOffset": 5, "endOffset": 61}, {"referenceID": 15, "context": "Gysel et al. (2016)", "startOffset": 0, "endOffset": 20}, {"referenceID": 31, "context": "Motamedi et al. (2016)", "startOffset": 0, "endOffset": 23}, {"referenceID": 28, "context": "SIFT (Lowe, 2004) and HOG (histogram of oriented gradients by Dalal and Triggs (2005)) were state-of-art for feature extraction, but they relied on handcrafted features.", "startOffset": 5, "endOffset": 17}, {"referenceID": 8, "context": "SIFT (Lowe, 2004) and HOG (histogram of oriented gradients by Dalal and Triggs (2005)) were state-of-art for feature extraction, but they relied on handcrafted features.", "startOffset": 62, "endOffset": 86}, {"referenceID": 8, "context": "SIFT (Lowe, 2004) and HOG (histogram of oriented gradients by Dalal and Triggs (2005)) were state-of-art for feature extraction, but they relied on handcrafted features. Neural networks in contrast can automatically create both high-level and low-level features. For a long time, deep neural networks were hindered by their computational complexity. However, advances in both personal computers and general purpose computing have enable the training of larger networks with more parameters. In 2012, the first deep convolutional neural network with 8 parameter layers was proposed by Krizhevsky et al. (2012). State-of-the art deep CNNs use a series of convolutional layers which enables them to extract very high-level features from images.", "startOffset": 62, "endOffset": 609}, {"referenceID": 23, "context": "Network architecture of AlexNet by Krizhevsky et al. (2012).", "startOffset": 35, "endOffset": 60}, {"referenceID": 22, "context": "(2013) or Adam rule (Kingma and Ba, 2015).", "startOffset": 20, "endOffset": 41}, {"referenceID": 0, "context": "There actually exist many optimizations for this parameter update such as Nesterov momentum as explained by Bengio et al. (2013) or Adam rule (Kingma and Ba, 2015).", "startOffset": 108, "endOffset": 129}, {"referenceID": 31, "context": "Image credit: Motamedi et al. (2016).", "startOffset": 14, "endOffset": 37}, {"referenceID": 31, "context": "Image credit: Motamedi et al. (2016).", "startOffset": 14, "endOffset": 37}, {"referenceID": 30, "context": "This activation function was first proposed by Nair and Hinton (2010) for Restricted Boltzmann Machines.", "startOffset": 47, "endOffset": 70}, {"referenceID": 23, "context": "The work by Krizhevsky et al. (2012) was the first to apply this simplified activation to a deep neural networks.", "startOffset": 12, "endOffset": 37}, {"referenceID": 24, "context": "This creates competition between neurons generated by different kernels and improves the top-1 accuracy of AlexNet (Krizhevsky et al., 2012) by 1.", "startOffset": 115, "endOffset": 140}, {"referenceID": 24, "context": "Many state-of-art CNNs use LRN layers to increase accuracy (Krizhevsky et al., 2012; Szegedy et al., 2015).", "startOffset": 59, "endOffset": 106}, {"referenceID": 40, "context": "Many state-of-art CNNs use LRN layers to increase accuracy (Krizhevsky et al., 2012; Szegedy et al., 2015).", "startOffset": 59, "endOffset": 106}, {"referenceID": 20, "context": "Most recently, a new normalization strategy termed Batch Normalization (BN) was proposed (Ioffe and Szegedy, 2015).", "startOffset": 89, "endOffset": 114}, {"referenceID": 22, "context": "This creates competition between neurons generated by different kernels and improves the top-1 accuracy of AlexNet (Krizhevsky et al., 2012) by 1.4%. The exact mathematical formulation of LRN across channels can be found in the same paper. Many state-of-art CNNs use LRN layers to increase accuracy (Krizhevsky et al., 2012; Szegedy et al., 2015). One notable exception is the network by Simonyan and Zisserman (2015) which performs very well without any kind of normalization.", "startOffset": 116, "endOffset": 418}, {"referenceID": 20, "context": "Most recently, a new normalization strategy termed Batch Normalization (BN) was proposed (Ioffe and Szegedy, 2015). This strategy was adopted by the most recent winner of the ILSVRC competition (Russakovsky et al., 2015). While both normalization strategies help for faster convergence and better prediction accuracy, they also add computational overhead, especially batch normalization. Unfortunately these normalization layers require a very large dynamic range for intermediate values. In AlexNet for example, the intermediate values of LRN layers are 2 times larger than any intermediate value from another layer. For this reason this thesis assumes LRN and BN layers are to be implemented in floating point, and we concentrate on the approximation of other layer types. Notice that previous work by Suda et al. (2016) chose 32-bit floating point for FPGA-based LRN layers.", "startOffset": 90, "endOffset": 823}, {"referenceID": 24, "context": "Recent winners (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; Szegedy et al., 2015; He et al., 2015) of ImageNet (Russakovsky et al.", "startOffset": 15, "endOffset": 109}, {"referenceID": 37, "context": "Recent winners (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; Szegedy et al., 2015; He et al., 2015) of ImageNet (Russakovsky et al.", "startOffset": 15, "endOffset": 109}, {"referenceID": 40, "context": "Recent winners (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; Szegedy et al., 2015; He et al., 2015) of ImageNet (Russakovsky et al.", "startOffset": 15, "endOffset": 109}, {"referenceID": 17, "context": "Recent winners (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; Szegedy et al., 2015; He et al., 2015) of ImageNet (Russakovsky et al.", "startOffset": 15, "endOffset": 109}, {"referenceID": 13, "context": "Besides image classification, deep networks show state-of-art performance in object detection (Girshick, 2015) as well as speech recognition (Hinton et al.", "startOffset": 94, "endOffset": 110}, {"referenceID": 18, "context": "Besides image classification, deep networks show state-of-art performance in object detection (Girshick, 2015) as well as speech recognition (Hinton et al., 2012).", "startOffset": 141, "endOffset": 162}, {"referenceID": 35, "context": "Other applications include playing games (Silver et al., 2016), as well as art (Gatys et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 12, "context": ", 2016), as well as art (Gatys et al., 2015).", "startOffset": 24, "endOffset": 44}, {"referenceID": 23, "context": "CaffeNet is the Caffe-version of AlexNet by Krizhevsky et al. (2012). CaffeNet was developed for the ImageNet data set, which has 1000 image classes.", "startOffset": 44, "endOffset": 69}, {"referenceID": 23, "context": "CaffeNet is the Caffe-version of AlexNet by Krizhevsky et al. (2012). CaffeNet was developed for the ImageNet data set, which has 1000 image classes. Figure 2.6 shows the required arithmetic operations and parameters size of AlexNet by layer type. The major part of arithmetic operations comes from convolutional layers: this layer type requires a total of 2.15 G operations. The arithmetic operations in all other layers sum up to 117 M operations. The parameter size of CaffeNet is 250 MB, of which 235 MB comes from fully connected layers. The same trend can be observed for the 16-layer version of VGG by Simonyan and Zisserman (2015): Extracting features in convolutional layers is computation-intense, while fully connected layers are memory-intense.", "startOffset": 44, "endOffset": 639}, {"referenceID": 37, "context": "The winner of 2014\u2019s localization challenge (Simonyan and Zisserman, 2015) experimented", "startOffset": 44, "endOffset": 74}, {"referenceID": 17, "context": "Another experiment by the winner of 2015 (He et al., 2015) used very deep networks.", "startOffset": 41, "endOffset": 58}, {"referenceID": 40, "context": "In the classification challenge of 2014, GoogLeNet (Szegedy et al., 2015) outperformed VGG (Simonyan and Zisserman, 2015) with a network capacity that was over 19X smaller.", "startOffset": 51, "endOffset": 73}, {"referenceID": 37, "context": ", 2015) outperformed VGG (Simonyan and Zisserman, 2015) with a network capacity that was over 19X smaller.", "startOffset": 25, "endOffset": 55}, {"referenceID": 24, "context": "SqueezeNet has the accuracy of AlexNet (Krizhevsky et al., 2012), but contains 50X fewer parameters.", "startOffset": 39, "endOffset": 64}, {"referenceID": 17, "context": "Another experiment by the winner of 2015 (He et al., 2015) used very deep networks. Their network architecture improves by over 2% when expanding the net from 34 to 152 layers. On the other hand, some research shows that even relatively small networks can achieve good classification performance. In the classification challenge of 2014, GoogLeNet (Szegedy et al., 2015) outperformed VGG (Simonyan and Zisserman, 2015) with a network capacity that was over 19X smaller. The GoogLeNet network is based on the inception idea described in section 2.5.2. A newer network architecture by Iandola et al. (2016) uses an adapted inception concept with smaller convolutional kernels.", "startOffset": 42, "endOffset": 605}, {"referenceID": 40, "context": "2 Inception Idea The inception idea is a concept proposed by Szegedy et al. (2015) which was used to build the GoogLeNet architecture.", "startOffset": 61, "endOffset": 83}, {"referenceID": 40, "context": "Image credit: Szegedy et al. (2015).", "startOffset": 14, "endOffset": 36}, {"referenceID": 11, "context": "However, it has be shown (Du et al., 2015) that CNNs have a relatively high error resilience; moreover CNNs can be trained in a discrete parameter space.", "startOffset": 25, "endOffset": 42}, {"referenceID": 14, "context": "Denoting as the quantization step size and bxc as the largest quantization value less or equal to x, Gupta et al. (2015) define round nearest as follows:", "startOffset": 101, "endOffset": 121}, {"referenceID": 14, "context": "Round stochastic: Another rounding scheme termed stochastic rounding was used by Gupta et al. (2015) for the weight updates of 16-bit neural networks.", "startOffset": 81, "endOffset": 101}, {"referenceID": 14, "context": "Round stochastic: Another rounding scheme termed stochastic rounding was used by Gupta et al. (2015) for the weight updates of 16-bit neural networks. Gupta et al. (2015) define stochastic rounding as follows:", "startOffset": 81, "endOffset": 171}, {"referenceID": 5, "context": "We adopt the idea of previous work by Courbariaux et al. (2015) which uses full precision shadow weights.", "startOffset": 38, "endOffset": 64}, {"referenceID": 5, "context": "Further trimming of the same network uses as low as 7-bit multipliers (Courbariaux et al., 2014).", "startOffset": 70, "endOffset": 96}, {"referenceID": 39, "context": "A similar proposal represents the weights of the same network with +1, 0 and -1 values (Sung et al., 2015).", "startOffset": 87, "endOffset": 106}, {"referenceID": 11, "context": "Gupta et al. (2015) show that networks on datasets like CIFAR-10 (10 images classes) can be trained in 16-bit.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Further trimming of the same network uses as low as 7-bit multipliers (Courbariaux et al., 2014). Another approach by Courbariaux et al. (2015) uses only binary weights, again on the same network.", "startOffset": 71, "endOffset": 144}, {"referenceID": 16, "context": "The deep compression pipeline proposed by Han et al. (2016b) addresses this problem.", "startOffset": 42, "endOffset": 61}, {"referenceID": 6, "context": "This motivated BinaryConnect (Courbariaux et al., 2015), a work which represents weights in binary format, rather than in traditional 32-bit floating point.", "startOffset": 29, "endOffset": 55}, {"referenceID": 7, "context": "Combining the two previous ideas, \u2018Binarized Neural Network\u2019 (Courbariaux et al., 2016) uses binary weights and layer activations.", "startOffset": 61, "endOffset": 87}, {"referenceID": 5, "context": "This motivated BinaryConnect (Courbariaux et al., 2015), a work which represents weights in binary format, rather than in traditional 32-bit floating point. This approach reduces parameter size by factor 32X and removes the need of multiplications in the forward path. BinaryConnect achieves near-state-of-art performance on 10-class datasets (MNIST, CIFAR-10, SVHN). A later work by Lin et al. (2016) takes this idea a step further by turning multiplications in the backward propagation into bit shifts.", "startOffset": 30, "endOffset": 402}, {"referenceID": 5, "context": "This motivated BinaryConnect (Courbariaux et al., 2015), a work which represents weights in binary format, rather than in traditional 32-bit floating point. This approach reduces parameter size by factor 32X and removes the need of multiplications in the forward path. BinaryConnect achieves near-state-of-art performance on 10-class datasets (MNIST, CIFAR-10, SVHN). A later work by Lin et al. (2016) takes this idea a step further by turning multiplications in the backward propagation into bit shifts. Layer activations are approximated by integer power of 2 numbers, while error gradients are retained in full precision. This proposal significantly reduces the hardware requirements for accelerators. Combining the two previous ideas, \u2018Binarized Neural Network\u2019 (Courbariaux et al., 2016) uses binary weights and layer activations. These numbers are constraint to +1 and -1 for both forward and backward propagation. Convolutional neural networks mainly consist of multiply-accumulate operations. For a binarized network, these operations are replaced by binary XNOR and binary count. To improve training results, the proposed method uses a bit-shift-based batch normalization as well as a shift-based parameter update. Finally the work of Rastegari et al. (2016) applies the idea of binary networks to ImageNet data set.", "startOffset": 30, "endOffset": 1268}, {"referenceID": 24, "context": "Both are based on AlexNet (Krizhevsky et al., 2012) and use different degrees of binarization.", "startOffset": 26, "endOffset": 51}, {"referenceID": 32, "context": "The work by Rastegari et al. (2016) proposes two network architectures.", "startOffset": 12, "endOffset": 36}, {"referenceID": 10, "context": "A proposal by Denton et al. (2014) uses clustered filters and low rank approximation.", "startOffset": 14, "endOffset": 35}, {"referenceID": 10, "context": "A proposal by Denton et al. (2014) uses clustered filters and low rank approximation. They achieve a speedup of 2X for convolutional layers of AlexNet, compared to a non-optimized GPU implementation. Another work by Mathieu et al. (2013) achieves better results by replacing convolution through FFT.", "startOffset": 14, "endOffset": 238}, {"referenceID": 10, "context": "A proposal by Denton et al. (2014) uses clustered filters and low rank approximation. They achieve a speedup of 2X for convolutional layers of AlexNet, compared to a non-optimized GPU implementation. Another work by Mathieu et al. (2013) achieves better results by replacing convolution through FFT. Finally the neural network compression pipeline proposed by Han et al. (2016b) uses pruning and weight-sharing.", "startOffset": 14, "endOffset": 379}, {"referenceID": 37, "context": "(2016) uses OpenCL to implement whole VGG (Simonyan and Zisserman, 2015) net on an Altera Stratix V board.", "startOffset": 42, "endOffset": 72}, {"referenceID": 33, "context": "Finally a recent Xilinxbased implementation (Qiu et al., 2016) achieves the start-of-art throughput of 137 GOPs.", "startOffset": 44, "endOffset": 62}, {"referenceID": 39, "context": "An approach by Zhang et al. (2015) uses Vivado HLS to accelerate the convolutional layers of AlexNet.", "startOffset": 15, "endOffset": 35}, {"referenceID": 36, "context": "A subsequent proposal by Suda et al. (2016) uses OpenCL to implement whole VGG (Simonyan and Zisserman, 2015) net on an Altera Stratix V board.", "startOffset": 25, "endOffset": 44}, {"referenceID": 2, "context": "A later implementation termed Eyeriss (Chen et al., 2016) can run the convolutional layers of AlexNet in forward path at 34 frames per second (74.", "startOffset": 38, "endOffset": 57}, {"referenceID": 2, "context": "DaDianNao by Chen et al. (2014) is a super-computer for machine learning at 28 nm technology.", "startOffset": 13, "endOffset": 32}, {"referenceID": 2, "context": "6 GOP/s 278 mW 268 GOP/s/W Chen et al. (2016) Xilinx Zynq ZC706 137 GOP/s 9.", "startOffset": 27, "endOffset": 46}, {"referenceID": 2, "context": "6 GOP/s 278 mW 268 GOP/s/W Chen et al. (2016) Xilinx Zynq ZC706 137 GOP/s 9.63 W 14.2 GOP/s/W Qiu et al. (2016) NVIDIA TK1 155 GOP/s 10.", "startOffset": 27, "endOffset": 112}, {"referenceID": 2, "context": "6 GOP/s 278 mW 268 GOP/s/W Chen et al. (2016) Xilinx Zynq ZC706 137 GOP/s 9.63 W 14.2 GOP/s/W Qiu et al. (2016) NVIDIA TK1 155 GOP/s 10.2 W 15.2 GOP/s/W Chen et al. (2016) Titan X 3.", "startOffset": 27, "endOffset": 172}, {"referenceID": 2, "context": "6 GOP/s 278 mW 268 GOP/s/W Chen et al. (2016) Xilinx Zynq ZC706 137 GOP/s 9.63 W 14.2 GOP/s/W Qiu et al. (2016) NVIDIA TK1 155 GOP/s 10.2 W 15.2 GOP/s/W Chen et al. (2016) Titan X 3.23 TOP/s 250 W 12.9 GOP/s/W Han et al. (2016a)", "startOffset": 27, "endOffset": 229}, {"referenceID": 2, "context": "The ASIC design by Chen et al. (2016) is optimized for large networks and low power consumption.", "startOffset": 19, "endOffset": 38}, {"referenceID": 2, "context": "As a case in point, we consider the mobile GPU implementation of AlexNet by Chen et al. (2016). When comparing the two GPU implementations, the mobile GPU\u2019s throughput per power is only slightly better than that of the high-end GPU (15.", "startOffset": 76, "endOffset": 95}, {"referenceID": 2, "context": "As a case in point, we consider the mobile GPU implementation of AlexNet by Chen et al. (2016). When comparing the two GPU implementations, the mobile GPU\u2019s throughput per power is only slightly better than that of the high-end GPU (15.2 GOP/s/W vs 12.9 GOP/s/W). The FPGA implementation from Qiu et al. (2016) is an end-to-end implementation of the 16-layer version of VGG.", "startOffset": 76, "endOffset": 311}, {"referenceID": 2, "context": "As a case in point, we consider the mobile GPU implementation of AlexNet by Chen et al. (2016). When comparing the two GPU implementations, the mobile GPU\u2019s throughput per power is only slightly better than that of the high-end GPU (15.2 GOP/s/W vs 12.9 GOP/s/W). The FPGA implementation from Qiu et al. (2016) is an end-to-end implementation of the 16-layer version of VGG. The FPGA implementation uses 16-bit fixed point arithmetic to reduce memory and computation requirements. Moreover the authors use pruning in fully connected layers to reduce parameter size. Another work by Suda et al. (2016) achieves nearly the same throughput without weight pruning.", "startOffset": 76, "endOffset": 601}, {"referenceID": 25, "context": "LeNet was proposed by LeCun et al. (1998). This network consists of two convolutional and two fully connected layers and can be used to classify handwritten digits (MNIST dataset).", "startOffset": 22, "endOffset": 42}, {"referenceID": 23, "context": "The CIFAR-10 data set (Krizhevsky, 2009) has 10 image classes such as airplanes, bird, and truck.", "startOffset": 22, "endOffset": 40}, {"referenceID": 24, "context": "CaffeNet is the Caffe version of AlexNet (Krizhevsky et al., 2012) which is the winner of the 2012 ILSVRC competition.", "startOffset": 41, "endOffset": 66}, {"referenceID": 40, "context": "GoogLeNet was proposed by Szegedy et al. (2015) and won the 2014 ILSVRC competition.", "startOffset": 26, "endOffset": 48}, {"referenceID": 24, "context": "(2016) was developed with the goal of a small network with the accuracy of AlexNet (Krizhevsky et al., 2012).", "startOffset": 83, "endOffset": 108}, {"referenceID": 19, "context": "SqueezeNet by Iandola et al. (2016) was developed with the goal of a small network with the accuracy of AlexNet (Krizhevsky et al.", "startOffset": 14, "endOffset": 36}, {"referenceID": 26, "context": "As shown by Lin et al. (2015); Qiu et al.", "startOffset": 12, "endOffset": 30}, {"referenceID": 26, "context": "As shown by Lin et al. (2015); Qiu et al. (2016), it is a good approach to use mixed precision, i.", "startOffset": 12, "endOffset": 49}, {"referenceID": 5, "context": "Dynamic fixed point can be a good solution to overcome this problem, as shown by Courbariaux et al. (2014). In dynamic fixed point, each number is represented as follows:", "startOffset": 81, "endOffset": 107}, {"referenceID": 19, "context": "The SqueezeNet (Iandola et al., 2016) architecture was developed with the goal of a small CNN that performs well on the ImageNet data set.", "startOffset": 15, "endOffset": 37}, {"referenceID": 5, "context": "Some previous work (Courbariaux et al., 2014) concentrated on training with fixed point arithmetic from the start and shows little performance decline for as short as 7bit fixed point numbers on LeNet.", "startOffset": 19, "endOffset": 45}, {"referenceID": 41, "context": "This motivated previous research to eliminate all multipliers by using integer power of two weights (Tang and Kwan, 1993; Mahoney and Elhanany, 2008).", "startOffset": 100, "endOffset": 149}, {"referenceID": 29, "context": "This motivated previous research to eliminate all multipliers by using integer power of two weights (Tang and Kwan, 1993; Mahoney and Elhanany, 2008).", "startOffset": 100, "endOffset": 149}, {"referenceID": 22, "context": "For parameter updates we use the Adam rule by Kingma and Ba (2015). As an important observation, we do not quantize layer outputs during fine-tuning.", "startOffset": 46, "endOffset": 67}, {"referenceID": 1, "context": "Since the choice of hyper parameters for retraining is crucial (Bergstra and Bengio, 2012), Ristretto relies on minimal human intervention in this step.", "startOffset": 63, "endOffset": 90}, {"referenceID": 14, "context": "This follows the thought of Gupta et al. (2015) and is conform with our description of the forward data path in subsection 2.", "startOffset": 28, "endOffset": 48}, {"referenceID": 34, "context": "2 Binary Networks The first published work to represent ImageNet networks with binary weights was by Rastegari et al. (2016). Their results show that very deep networks can be approximated with binary weights, although at an accuracy drop of around 3% for CaffeNet and 6% for GoogLeNet.", "startOffset": 101, "endOffset": 125}], "year": 2016, "abstractText": "Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks Convolutional neural networks (CNN) have achieved major breakthroughs in recent years. Their performance in computer vision have matched and in some areas even surpassed human capabilities. Deep neural networks can capture complex non-linear features; however this ability comes at the cost of high computational and memory requirements. State-ofart networks require billions of arithmetic operations and millions of parameters. To enable embedded devices such as smart phones, Google glasses and monitoring cameras with the astonishing power of deep learning, dedicated hardware accelerators can be used to decrease both execution time and power consumption. In applications where fast connection to the cloud is not guaranteed or where privacy is important, computation needs to be done locally. Many hardware accelerators for deep neural networks have been proposed recently. A first important step of accelerator design is hardware-oriented approximation of deep networks, which enables energy-efficient inference. We present Ristretto, a fast and automated framework for CNN approximation. Ristretto simulates the hardware arithmetic of a custom hardware accelerator. The framework reduces the bit-width of network parameters and outputs of resource-intense layers, which reduces the chip area for multiplication units significantly. Alternatively, Ristretto can remove the need for multipliers altogether, resulting in an adder-only arithmetic. The tool fine-tunes trimmed networks to achieve high classification accuracy. Since training of deep neural networks can be time-consuming, Ristretto uses highly optimized routines which run on the GPU. This enables fast compression of any given network. Given a maximum tolerance of 1%, Ristretto can successfully condense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available.", "creator": "LaTeX with hyperref package"}}}