{"id": "1605.06636", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2016", "title": "Deep Transfer Learning with Joint Adaptation Networks", "abstract": "substitution strategies rely after massive amounts versus labeled data cannot support powerful models. for a target task short of labeled variables, strong learning enables model evaluation from a trusted source layer. this task uses deep collaboration theories during a more general scenario that the joint correlation of items among labels may change substantially related mechanisms. starting on the theory of differential nash embedding of programs, a novel utility predict theory is proposed to directly compare joint correlation involving domains, eliminating the need of marginal - conditional variability. statistical prediction offers supported within deep convolutional networks, where collective dataset shifts frequently linger in multiple task - dynamic performance patterns despite the classifier assumption. a set of alternative adaptation scenarios are made uniquely match the joint distributions of these layers between domains by minimizing residual joint distribution discrepancy, which will reduce trained efficiently using back - propagation. experiments show theoretically the new approach yields conflict detection systems continuum technique on standard domain adaptation datasets.", "histories": [["v1", "Sat, 21 May 2016 12:56:14 GMT  (112kb,D)", "http://arxiv.org/abs/1605.06636v1", null], ["v2", "Thu, 17 Aug 2017 07:35:59 GMT  (428kb,D)", "http://arxiv.org/abs/1605.06636v2", "34th International Conference on Machine Learning"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["mingsheng long", "han zhu", "jianmin wang 0001", "michael i jordan"], "accepted": true, "id": "1605.06636"}, "pdf": {"name": "1605.06636.pdf", "metadata": {"source": "CRF", "title": "Deep Transfer Learning with Joint Adaptation Networks", "authors": ["Mingsheng Long", "Jianmin Wang", "Michael I. Jordan"], "emails": ["mingsheng@tsinghua.edu.cn,", "jimwang@tsinghua.edu.cn,", "jordan@berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "Deep networks have significantly improved the state of the arts for diverse machine learning problems and applications. Unfortunately, the impressive performance gains come only when massive amounts of labeled data are available for supervised training. Since manual labeling of sufficient training data for diverse application domains on-the-fly is often prohibitive, for a target task short of labeled data, there is strong motivation to build effective learners that can leverage rich labeled data in a different source domain. However, this learning paradigm suffers from the shift in data distributions across different domains, which poses a major obstacle in adapting predictive models for the target task [1].\nLearning a discriminative model in the presence of the shift between training and test distributions is known as transfer learning or domain adaptation [1]. Previous shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating the instance importance without using target labels [1, 2, 3, 4, 5, 6]. Recent deep transfer learning methods leverage deep networks to learn more transferable feature representations by embedding domain adaptation in the pipeline of deep learning, which can simultaneously disentangle the explanatory factors of variations behind data and match the marginal distributions across domains [7, 8, 9, 10].\nTransfer learning becomes more challenging when the domains may change by the joint distributions P (X,Y) of features and labels, which is a natural scenario in practical applications. Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11]. As target labels are unavailable, the adaptation is executed by minimizing the discrepancy between marginal distributions instead of joint distributions. How to address a seamless adaptation of joint distributions remains an open problem.\nIn this paper, we address deep transfer learning under the general scenario that the joint distributions of features and labels may change substantially across domains. Instead of separate adaptations of\nPreliminary work. Copyright by the author(s).\nar X\niv :1\n60 5.\n06 63\n6v 1\n[ cs\n.L G\n] 2\n1 M\nay 2\nmarginal and conditional distributions which often require strong independence and/or smoothness assumptions on the factorized distributions [4], we propose a novel joint distribution discrepancy that can directly compare joint distributions by embedding them into reproducing kernel Hilbert spaces, which eliminates the need of marginal-conditional factorization for separate adaptation. Transfer learning is enabled in deep convolutional networks, where the dataset shifts may linger in multiple task-specific feature layers and the classifier layer, because deep features eventually transition from general to specific along the network and the transferability of features and classifiers decreases when the domain discrepancy increases [12]. We craft a family of joint adaptation networks to match the joint distributions of all the task-specific layers of deep networks across domains by minimizing the joint distribution discrepancy, which can be trained efficiently using back-propagation. Empirical evidence shows that the approach yields state of the art results on standard domain adaptation datasets."}, {"heading": "2 Related Work", "text": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17]. The main technical difficulty of transfer learning is how to reduce the shifts in data distributions across domains. Most existing methods learn a shallow representation model by which domain discrepancy is minimized, which however cannot suppress domain-specific exploratory factors of variations. Deep networks learn abstract representations that disentangle the explanatory factors of variations in data [18] and extract transferable factors underlying different populations [19, 20], which can only reduce, but not remove, the cross-domain discrepancy [12]. The state of the art work on deep domain adaptation bridges deep learning and transfer learning [7, 8, 9, 10] by integrating domain-adaptation modules into deep network architectures to boost transfer performance.\nTransfer learning from different domains in practical applications is more challenging as the domains may change by the joint distributions P (X,Y) of features and labels. Previous deep transfer learning methods [7, 8, 9, 10] mainly correct the shifts in the marginal distributions. Note that [8, 10] partially align the source and target classes based on pseudo labels, but both rely on a rather strong assumption that the adaptations of marginal distributions P (X) and target distributions P (Y) can be independent. Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11], which is a remarkable step towards full-transfer of joint distributions. Since the target labels are unavailable in unsupervised domain adaptation, adaptation is performed by minimizing the discrepancy between the marginal distributions instead of the joint distributions, while the conditional shift is taken into account through P (X) = \u2211 Y P (X|Y)P (Y). This paper addresses the challenge of joint distribution adaptation by minimizing a novel joint distribution discrepancy, which is naturally implemented in deep networks."}, {"heading": "3 Joint Adaptation Networks", "text": "In domain adaptation problems, we are given a source domain Ds = {(xsi ,ysi )} ns i=1 of ns labeled examples and a target domain Dt = {xti} nt i=1 of nt unlabeled examples. The source domain and target domain are sampled from joint distributions P (X,Y) and Q(X,Y) respectively, and P 6= Q. The goal of this paper is to craft a deep neural network y = f (x) that formally reduces the shifts in joint distributions across domains and enables learning transferable features and classifiers, such that the target risk Rt (f) = Pr(x,y)\u223cQ [f (x) 6= y] can be minimized via the source domain supervision. Transfer learning is a challenging machine learning paradigm due to the scarcity of labeled data in the target domain, and the classifier trained on the source domain cannot generalize to the target domain due to the substantial shifts in joint distributions P (X,Y) 6= Q(X,Y). The distribution shifts may stem from the marginal distributions P (X) 6= Q(X) (a.k.a. covariate shift [2]), the conditional distributions P (Y|X) 6= Q(Y|X) (a.k.a. conditional shift [4]), or both (a.k.a. dataset shift [21]). In general, the presence of conditional shift leads to an ill-posed problem, and the additional assumption that the conditional distribution changes only under location-scale transformations on X is commonly imposed to make the problem tractable [4, 5, 6]. However, this assumption is still very strong for practical domain adaptation problems and it is nontrivial to justify when and how this assumption can hold. Also, how to seamlessly address covariate shift and conditional shift remains an open problem."}, {"heading": "3.1 Joint Distribution Discrepancy", "text": "Many existing methods address the transfer learning problem mainly by bounding the target error with the source error plus a discrepancy metric between the marginal distributions P (X) and Q(X) of the source and target domains [22]. Two classes of statistics have been explored for two-sample testing, which accepts or rejects the null hypothesis P (X) = Q(X) based on the two samples respectively generated from P (X) and Q(X): Energy Distance (ED) and Maximum Mean Discrepancy (MMD) [23]. As these statistics are defined for the marginal distributions, they are only applied to covariate shift adaptation problems [2, 3, 8], which include state of the art deep adaptation methods [8, 9, 10].\nIn this paper, we propose to directly measure the discrepancy between joint distributions P (X,Y) andQ(X,Y) (denoted by P andQ for short) based on the theory of kernel embedding of distributions [11], in which a probability distribution is represented as an element of a reproducing kernel Hilbert space (RKHS). Let X and Y be random variables with codomains \u2126X and \u2126Y and joint distribution P (X,Y). Given kernels k\u03c6 on \u2126X\u00d7\u2126X and k\u03c8 on \u2126Y\u00d7\u2126Y, the Moore-Aronszajn Theorem asserts the existence of corresponding RKHSsH\u03c6 andH\u03c8 in which the element \u03c6(x) and \u03c8(y) satisfy the reproducing property \u3008f, \u03c6 (x)\u3009H\u03c6 = f (x), \u3008f, \u03c8 (y)\u3009H\u03c8 = f (y) for all f \u2208 H, x \u2208 \u2126X, y \u2208 \u2126Y, where \u03c6(x) : x 7\u2192 H\u03c6 and \u03c8(y) : y 7\u2192 H\u03c8 are the feature mappings, \u3008\u03c6 (x) , \u03c6 (x\u2032)\u3009 = k\u03c6 (x,x\u2032) and \u3008\u03c8 (y) , \u03c8 (y\u2032)\u3009 = k\u03c8 (y,y\u2032). Assume the tensor product of two Hilbert spacesH , H\u03c6\u2297H\u03c8 is also a Hilbert space that satisfies \u3008\u03c6 (x)\u2297 \u03c8 (y) , \u03c6 (x\u2032)\u2297 \u03c8 (y\u2032)\u3009H = k\u03c6 (x,x\u2032)\u2297 k\u03c8 (y,y\u2032). The kernel embedding C\u03c6,\u03c8 (P ) of joint distribution P (X,Y) in the Hilbert spaceH\u03c6\u2297H\u03c8 is defined as\nC\u03c6,\u03c8 (P ) , EP [\u03c6 (X)\u2297 \u03c8 (Y)] = \u222b\n\u2126X\u00d7\u2126Y \u03c6 (x)\u2297 \u03c8 (y) dP (x,y), (1)\nwhere EP [\u00b7] denotes taking expectation over random variable that follows distribution P . Given n pairs of training examples {(xi,yi)}ni=1 drawn i.i.d. from P , the empirical estimate of C\u03c6,\u03c8 (P ) is\nC\u0302\u03c6,\u03c8 (P ) = 1\nn n\u2211 i=1 \u03c6 (xi)\u2297 \u03c8 (yi). (2)\nWe require characteristic kernels k\u03c6 and k\u03c8 such that the kernel embedding C\u03c6,\u03c8 (P ) is injective, and that the embedding of distributions into infinite-dimensional feature spaces can preserve all of the statistical features of arbitrary distributions, which removes the necessity of density estimation of P .\nWe extend the Maximum Mean Discrepancy (MMD) [24] to measure the discrepancy between joint distributions. MMD is a distance-measure between marginal distributions P (Xs) and Q(Xt), which is defined as the squared distance between the kernel embeddings of marginal distributions in RKHS, MMD (PXs , QXt) ,\n\u2225\u2225EPXs [\u03c6 (Xs)]\u2212 EQXt [\u03c6 (Xt)]\u2225\u22252H\u03c6 . We propose a novel Joint Distribution Discrepancy (JDD), which measures the discrepancy between joint distributions P (Xs,Ys) and Q(Xt,Yt) as the squared distance between the kernel embeddings of the joint distributions in RKHS,\nD (P,Q) , \u2016C\u03c6,\u03c8 (P )\u2212 C\u03c6,\u03c8 (Q)\u20162H = \u2225\u2225EP [\u03c6 (Xs)\u2297 \u03c8 (Ys)]\u2212 EQ [\u03c6 (Xt)\u2297 \u03c8 (Yt)]\u2225\u22252H , (3)\nwhereH , H\u03c6\u2297H\u03c8 is the tensor product Hilbert space that satisfies the reproducing property. Based on the theory of kernel two-sample testing [24], we have P = Q if and only if D(P,Q) = 0. Given source domain Ds = {(xsi ,ysi )} ns i=1 of ns labeled examples and target domain Dt = {(xti,yti)} nt i=1 of nt unlabeled points drawn i.i.d. from P and Q respectively, the empirical estimate of D(P,Q) is\nD\u0302 (P,Q) = \u2225\u2225\u2225C\u0302\u03c6,\u03c8 (P )\u2212 C\u0302\u03c6,\u03c8 (Q)\u2225\u2225\u22252\nH\n= \u2225\u2225\u2225\u2225 1ns \u2211nsi=1 \u03c6 (xsi )\u2297 \u03c8 (ysi )\u2212 1nt \u2211ntj=1 \u03c6 (xtj)\u2297 \u03c8 (ytj) \u2225\u2225\u2225\u22252 H\n= 1\nn2s ns\u2211 i=1 ns\u2211 j=1 k\u03c6 ( xsi ,x s j ) k\u03c8 ( ysi ,y s j ) + 1 n2t nt\u2211 i=1 nt\u2211 j=1 k\u03c6 ( xti,x t j ) k\u03c8 ( yti ,y t j ) \u2212 2 nsnt ns\u2211 i=1 nt\u2211 j=1 k\u03c6 ( xsi ,x t j ) k\u03c8 ( ysi ,y t j ) . (4)\nNote that for unsupervised domain adaptation the target labels {yti} are unavailable and we have to infer the target labels by a transductive learning paradigm, of which the details will be forthcoming.\nIn the presence of deep neural networks, the training data may be represented by the features generated by multiple hidden layers as {X`}`\u2208L. Since deep features eventually transition from general to specific along the network [12, 8], transferability of features at different layers may be task-dependent and it is safer to model the joint distribution discrepancy based on multiple task-specific layers. We extend JDD to measure the discrepancy in task-specific features {X`}`\u2208L as the squared distance between the kernel embeddings of joint distributions P ( {Xs`}`\u2208L,Ys ) and Q ( {Xt`}`\u2208L,Yt ) as\nDL (P,Q) , \u2225\u2225\u2225EP [\u2297 `\u2208L \u03c6 ( Xs` ) \u2297 \u03c8 (Ys) ] \u2212 EQ [\u2297 `\u2208L \u03c6 ( Xt` ) \u2297 \u03c8 ( Yt )]\u2225\u2225\u22252 H , (5)\nwhereH , \u2297\n`\u2208LH\u03c6\u2297H\u03c8 is the tensor product of Hilbert spaces that satisfies reproducing property \u3008 \u2297\n`\u2208L \u03c6(x `)\u2297 \u03c8(y), \u2297 `\u2208L \u03c6(x \u2032`)\u2297 \u03c8(y\u2032)\u3009H = \u220f `\u2208L k\u03c6(x `,x\u2032 ` )k\u03c8(y,y\n\u2032). For source domain Ds = {({xs`i }`\u2208L,ysi )} ns i=1 of ns labeled examples and target domain Dt = {({xt`i }`\u2208L,yti)} nt i=1 of nt unlabeled examples drawn i.i.d. from P and Q respectively, the empirical estimate of DL(P,Q) is\nD\u0302L (P,Q) = \u2225\u2225\u2225\u2225 1ns \u2211nsi=1\u2297`\u2208L \u03c6 (xs`i )\u2297 \u03c8 (ysi )\u2212 1nt \u2211ntj=1\u2297`\u2208L \u03c6 (xt`j )\u2297 \u03c8 (ytj) \u2225\u2225\u2225\u22252 H\n= 1\nn2s ns\u2211 i=1 ns\u2211 j=1 \u220f `\u2208L k\u03c6 ( xs`i ,x s` j ) k\u03c8 ( ysi ,y s j ) + 1 n2t nt\u2211 i=1 nt\u2211 j=1 \u220f `\u2208L k\u03c6 ( xt`i ,x t` j ) k\u03c8 ( yti ,y t j ) \u2212 2 nsnt ns\u2211 i=1 nt\u2211 j=1 \u220f `\u2208L k\u03c6 ( xs`i ,x t` j ) k\u03c8 ( ysi ,y t j ) .\n(6) This multilayer JDD is designed to seamlessly match the deep architectures for network adaptation. The building modules for deep networks to perform joint distribution adaptation are shown in Figure 1."}, {"heading": "3.2 Joint Adaptation Networks", "text": "Deep networks [18] can learn distributed, compositional, and abstract representations for natural data such as image and text. This paper addresses joint distribution adaptation within deep networks for learning transferable features and classifiers. We extend deep convolutional networks (CNNs), i.e. AlexNet [25] and GoogLeNet [26], to novel joint adaptation networks (JANs) as shown in Figure 1. Denote by f(x) the CNN classifier, and the empirical error of CNN on the source domain data Ds is\nmin f\n1\nns ns\u2211 i=1 J (f (xsi ) ,y s i ), (7)\nwhere J(\u00b7, \u00b7) is the cross-entropy loss function. The deep features learned by CNNs can disentangle the exploratory factors of variations in data distributions and promote knowledge adaptation [20, 18].\nHowever, the latest literature findings also reveal that the deep features can reduce, but not remove, the cross-domain distribution discrepancy [12, 8]. The deep features in standard CNNs must eventually\ntransition from general to specific along the network, and the transferability of features and classifiers decreases when the cross-domain discrepancy increases [12]. In other words, the shifts in the joint distributions linger even after multilayer feature abstractions. In this paper, we fine-tune CNNs on labeled source examples and require the joint distributions of the source and target domains to become similar under multiple task-specific feature layers ` \u2208 L and the classifier layer. This is implemented by jointly minimizing a penalty that substitutes multilayer activations {({x`i}`\u2208L,yi)} to JDD (6) as\nmin f\n1\nns ns\u2211 i=1 J (f (xsi ) ,y s i ) + \u03bbD\u0302L (Ds,Dt) , (8)\nwhere \u03bb > 0 is a tradeoff parameter for the JDD penalty, Ds = {({xs`i }`\u2208L,ysi )} ns i=1 and Dt = {({xt`i }`\u2208L,yti)} nt i=1 are the source and target domains represented by features of multiple layers L. For the JAN model based on AlexNet we set L = {fc6, fc7, fc8} while for the JAN model based on GoogLeNet we set L = {in9, fc3}, as these layers are tailored to task-specific structures, which are not safely transferable and should be jointly adapted by minimizing both the empirical error and JDD.\nIt is worth noting that for unsupervised domain adaptation the target labels {yti} are unavailable. In order to compute JDD (6), we adopt a transductive learning paradigm which is a natural option for semi-supervised learning: we substitute the ground-truth labels y\u2217 in (6) with the predictions made by the CNN classifier f(x\u2217), where \u2217 denotes a data point from either the source or target domain. Note that, f(x\u2217) is not a single number indicating the assigned category of x\u2217 (which is rather inaccurate); it is the distribution over the label space, indicating the probability of assigning x\u2217 to each category. Hence, the JDD penalty (6) simultaneously regularizes both the features and the classifier such that the shifts in joint distributions across domains can be formally corrected in RKHSs, which is made end-to-end learnable by deep networks. The iterative nature of the back-propagation algorithm nicely fits the transductive learning paradigm, such that JDD (6) can be computed in each feed-forward pass.\nA limitation of JDD (6) is quadratic complexity, which is inefficient for scalable deep transfer learning. Motivated by the unbiased estimate of MMD [24], we devise a similar linear-time estimate of JDD as\nD\u0302L = 2 ns ns/2\u2211 i=1 {\u220f `\u2208L k\u03c6 ( xs`2i\u22121,x s` 2i ) k\u03c8 ( ys2i\u22121,y s 2i ) + \u220f `\u2208L k\u03c6 ( xt`2i\u22121,x t` 2i ) k\u03c8 ( yt2i\u22121,y t 2i ) \u2212 \u220f `\u2208L k\u03c6 ( xs`2i\u22121,x t` 2i ) k\u03c8 ( ys2i\u22121,y t 2i ) \u2212 \u220f `\u2208L k\u03c6 ( xs`2i,x t` 2i\u22121 ) k\u03c8 ( ys2i,y t 2i\u22121 )} . (9)\nThis linear-time estimate nicely fits into the mini-batch stochastic gradient descent (SGD) algorithm implemented in [8], based on which the training of the JAN models can scale linearly to large samples."}, {"heading": "4 Experiments", "text": "We evaluate the joint adaptation networks against state of the art transfer learning and deep learning methods on standard domain adaptation benchmarks. The codes and datasets will be available online."}, {"heading": "4.1 Setup", "text": "Office-31 [13] is a standard benchmark for domain adaptation in computer vision, comprising 4,652 images and 31 categories collected from three distinct domains: Amazon (A), which contains images downloaded from amazon.com, Webcam (W) and DSLR (D), which contain images taken by web camera and digital SLR camera with different settings, respectively. We evaluate all methods across three transfer tasks A\u2192W, D\u2192W and W\u2192 D, which are widely adopted by prior deep transfer learning methods [27, 7, 9], and another three transfer tasks A\u2192 D, D\u2192 A and W\u2192 A as in [8, 10]. ImageCLEF-DA1 is a benchmark dataset for the ImageCLEF 2014 domain adaptation challenge, which is organized by selecting the 12 common categories shared by the following four public datasets, each is considered as a domain: Caltech-256 (C), ImageNet ILSVRC 2012 (I), Pascal VOC 2012 (P), and Bing (B). We permute all domain combinations and build 12 transfer tasks: C\u2192 I, C \u2192 P, C\u2192 B, I\u2192 C, I\u2192 P, I\u2192 B, P\u2192 C, P\u2192 I, P\u2192 B, B\u2192 C, B\u2192 I, and B\u2192 P. It is worth\n1http://imageclef.org/2014/adaptation\nnoting that, some domains (e.g. B) contain low-quality images that are more difficult to categorize than the other domains (e.g. C). This makes this dataset a good compliment to the Office-31 dataset.\nWe compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9]. TCA is a conventional transfer learning method based on MMD-regularized Kernel PCA. GFK is a manifold learning method that interpolates across an infinite number of intermediate subspaces to bridge domains. DDC is the first method that maximizes domain invariance by adding to AlexNet an adaptation layer using linear-kernel MMD [24]. DAN learns more transferable features by embedding deep features of multiple task-specific layers to reproducing kernel Hilbert spaces (RKHSs) and matching different distributions optimally using multi-kernel MMD. RevGrad improves domain adaptation by making the source and target domains indistinguishable for a discriminative domain classifier via an adversarial training paradigm.\nWe examine the influence of deep representations for domain adaptation by employing the breakthrough AlexNet [25] and the state of the art GoogLeNet [26] for learning deep representations. For AlexNet, we follow DeCAF [27] and use the activations of the fc7 layer as image representation. For GoogLeNet, we use the activations of the last inception layer in9 as image representation. To go deeper with the efficacy of joint adaptation based on minimizing the Joint Distribution Discrepancy (JDD), we evaluate two variants of joint adaptation networks (JANs): JAN-xy and JAN-xxy, by using the two-layer adaptation module and the three-layer adaptation module in Figure 1, respectively.\nWe follow standard evaluation protocols for unsupervised domain adaptation [8, 9]. For both Office-31 and ImageCLEF-DA datasets, we use all labeled source examples and all unlabeled target examples. We compare the average classification accuracy of each method on five random experiments, and report the standard error of the classification accuracies by different experiments of the same transfer task. We perform model selection by tuning the hyper-parameters using cross-validation on labeled source data. For MMD-based methods (TCA, DDC, and DAN) and JAN, we use the Gaussian kernel with bandwidth set to the median pairwise squared distances on training data, i.e. median trick [24].\nWe implement all deep methods based on the Caffe framework, and fine-tune from Caffe-trained models of AlexNet [25] and GoogLeNet [26], both are pre-trained on the ImageNet dataset. Due to limited training points in our datasets, (1) for AlexNet, we fix convolutional layers conv1\u2013conv3 copied from pre-trained model, fine-tune conv4\u2013conv5 and fully connected layers fc6\u2013fc7, and train classifier layer fc8, all via back propagation; (2) for GoogLeNet, we fix inception (convolutionpooling) layers inc1\u2013inc3 copied from pre-trained model, fine-tune inc4\u2013inc9 and train classifier layer fc3, all via back propagation. Since the classifier is trained from scratch, we set its learning rate to be 10 times that of the lower layers. We use mini-batch stochastic gradient descent (SGD) with momentum of 0.9 and the learning rate annealing strategy implemented in RevGrad [9]: the learning rate is not selected through a grid search due to high computational cost\u2014it is adjusted during SGD using the following formula: \u03b7p = \u03b70(1+\u03b1p)\u03b2 , where p is the training progress linearly changing from 0 to 1, \u03b70 = 0.01, \u03b1 = 10 and \u03b2 = 0.75, which is optimized to promote convergence and low error on the source domain. We set \u03bb = 0 within 100 iterations and then set it to the cross-validated value, which allows the JDD penalty to be less sensitive to noisy signals at early stages of training process."}, {"heading": "4.2 Results", "text": "The classification accuracy results on the Office-31 dataset for unsupervised domain adaptation based on AlexNet and GoogLeNet are shown in Table 1 and Table 2, respectively. Note that for fair comparison, the results of the state of the art methods, DAN [8] and RevGrad [9], are directly reported from their original papers. The proposed JAN models, JAN-xy and JAN-xxy, outperform all comparison methods on most transfer tasks. In particular, JANs promote the classification accuracy substantially on hard transfer tasks, e.g. A\u2192W and A\u2192 D, where the source and target domains are substantially different, and produce comparable classification accuracy on easy transfer tasks, D \u2192W and W\u2192 D, where the source and target domains are similar [13]. The encouraging results highlight the vital importance of joint distribution adaptation in deep neural networks, and suggest that JANs are able to learn more transferable features and classifiers for effective domain adaptation.\nFrom the results, we can make several interesting observations. (1) Standard deep-learning methods (AlexNet and GoogLeNet) perform comparably with traditional shallow transfer-learning methods\n(TCA and GFK) that use deep-network features (AlexNet-fc7 and GoogLeNet-in9) as input. The only difference between these two sets of methods is that deep networks can further take the advantage of supervised fine-tuning on the source-labeled data, while TCA and GFK can individually benefit from their domain adaptation procedures. These results confirm the current practice that deep networks learn abstract feature representations, which can only reduce, but not remove, the domain discrepancy [12]. (2) Deep-transfer learning methods that reduce the domain discrepancy by domain-adaptive deep networks (DDC, DAN and RevGrad) substantially outperform both standard deep-learning methods and traditional shallow transfer-learning methods. This validates that incorporating domain-adaptation modules into deep networks can improve domain adaptation performance. (3) The proposed joint adaptation network (JAN) models outperform previous methods by large margins and set new state of the art results on these benchmark datasets. Different from all previous deep-transfer learning methods that only adapt the marginal distributions across domains based on feature layers (one layer for RevGrad and multilayer for DAN), JAN initiates a principled methodology for adapting the joint distributions across domains based on both feature layers and classifier layer in an end-to-end deep learning framework, which can fully correct the dataset shifts in joint distributions across domains.\nWe go deeper with the efficacy of JAN by showing results of its variants, JAN-xy and JAN-xxy, which use the two-layer adaptation module and the three-layer adaptation module in Figure 1, respectively. The three-layer adaptation model JAN-xxy achieves significantly better results than the two-layer adaptation model JAN-xy. This highlights the importance of deeper adaptation of joint distributions. Since deep features eventually transition from general to specific along the network, there may be multiple layers where the features are not safely transferable [12, 8], hence it is safer to adapt the joint distributions based on multi-layer features instead of single-layer features. It is noteworthy that this paper initiates a principled way to model multi-layer features in a kernel embedding framework [11].\nThe four domains in ImageCLEF-DA are more balanced than that of Office-31, but some domains (e.g. B) comprise low-quality images that are much more difficult to categorize. With more difficult transfer tasks, we are expecting to testify whether transfer learning works on harder problems. Since the transfer tasks are more difficult, we report the results with GoogLeNet only, which are shown in Table 3. The JAN model outperforms all comparison methods on most transfer tasks, including the harder ones, e.g. C\u2192B and I\u2192B. This evidence on diverse datasets validates that the JAN model can learn transferable features robust to joint distribution shifts for domain adaptation on hard problems."}, {"heading": "4.3 Discussion", "text": "Feature Visualization: We visualize in Figures 2(a)\u20132(d) the activations of task A\u2192W learned by DAN and JAN respectively using the t-SNE embeddings [27]. Compared with the activations given by DAN in Figure 2(a)\u20132(b), the activations given by JAN in Figures 2(c)\u20132(d) show that the target categories are discriminated much more clearly by the source classifier of JAN, which suggests that the joint adaptation of deep features and labels is a powerful approach to effective domain adaptation.\nDistribution Discrepancy: The theory of domain adaptation [22, 28] suggests the A-distance as a measure of cross-domain discrepancy, which, together with the source risk, will bound the target risk. The proxy A-distance is defined as dA = 2 (1\u2212 2 ), where is the generalization error of a domain classifier (e.g. kernel SVM) trained on the binary problem of classifying source and target. Figure 3(a) shows dA on tasks A\u2192W and W\u2192 D with features of CNN, DAN, and JAN, respectively. We observe that dA using JAN features is much smaller than dA using CNN and DAN features, which suggests that JAN features are more transferable. As domains W and D are very similar, dA of task W\u2192 D is much smaller than dA of task A\u2192W, which explains the better performance of W\u2192 D. A limitation ofA-distance is that it cannot measure the cross-domain discrepancy of joint distributions, which is addressed by the proposed joint distribution discrepancy (JDD) measure (4). We compute JDD (4) across domains using CNN, DAN and JAN activations respectively, based on the features in fc7 and labels in fc8 under the AlexNet architecture. The results in Figure 3(b) show that JDD using JAN features is much smaller than JDD using CNN and DAN features, which evidence that JANs can successfully reduce the shifts in joint distributions to learn more transferable features and classifiers.\nParameter Sensitivity: We investigate the effects of the JDD penalty parameter \u03bb. Figure 3(c) shows the transfer performance of JAN based on AlexNet and GoogLeNet respectively, by varying \u03bb \u2208 {0.1, 0.4, 0.7, 1, 4, 7, 10} on task A\u2192 D. The accuracy of JAN first increases and then decreases as \u03bb varies and shows a bell-shaped curve. This confirms the motivation of learning deep features and adapting joint distributions, since a good trade-off between them can enhance network transferability."}, {"heading": "5 Conclusion", "text": "This paper presented a novel approach to deep transfer learning, which enables direct adaptation of joint distributions across domains. Different from previous methods, we eliminate the requirement for separate adaptations of marginal and conditional distributions, which are often subject to rather strong independence assumptions. The discrepancy between joint distributions of multiple features and labels can be computed by embedding the joint distributions in a tensor-product Hilbert space, which can be naturally implemented through most deep networks. We will constitute as future work the further evaluations on larger-scale datasets and the semi-supervised domain adaptation scenarios."}], "references": [{"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "TKDE, 22(10):1345\u20131359,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Direct importance estimation with model selection and its application to covariate shift adaptation", "author": ["M. Sugiyama", "S. Nakajima", "H. Kashima", "P.V. Buenau", "M. Kawanabe"], "venue": "NIPS,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "TNNLS, 22(2):199\u2013210,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Domain adaptation under target and conditional shift", "author": ["K. Zhang", "B. Sch\u00f6lkopf", "K. Muandet", "Z. Wang"], "venue": "ICML,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Flexible transfer learning under support and model shift", "author": ["X. Wang", "J. Schneider"], "venue": "NIPS,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation with conditional transferable components", "author": ["M. Gong", "K. Zhang", "T. Liu", "D. Tao", "C. Glymour", "B. Sch\u00f6lkopf"], "venue": "ICML,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["E. Tzeng", "J. Hoffman", "N. Zhang", "K. Saenko", "T. Darrell"], "venue": "CoRR, abs/1412.3474,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning transferable features with deep adaptation networks", "author": ["M. Long", "Y. Cao", "J. Wang", "M.I. Jordan"], "venue": "ICML,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Y. Ganin", "V. Lempitsky"], "venue": "ICML,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Simultaneous deep transfer across domains and tasks", "author": ["E. Tzeng", "J. Hoffman", "N. Zhang", "K. Saenko", "T. Darrell"], "venue": "ICCV,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Hilbert space embeddings of conditional distributions with applications to dynamical systems", "author": ["L. Song", "J. Huang", "A. Smola", "K. Fukumizu"], "venue": "ICML,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "ECCV,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "ICCV,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "CVPR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "LSDA: Large scale detection through adaptation", "author": ["J. Hoffman", "S. Guadarrama", "E. Tzeng", "R. Hu", "J. Donahue", "R. Girshick", "T. Darrell", "K. Saenko"], "venue": "NIPS,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR, 12:2493\u20132537,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "TPAMI, 35(8):1798\u20131828,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "CVPR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Dataset shift in machine learning", "author": ["J. Quionero-Candela", "M. Sugiyama", "A. Schwaighofer", "N.D. Lawrence"], "venue": "The MIT Press,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "MLJ, 79(1-2):151\u2013175,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Equivalence of distance-based and rkhs-based statistics in hypothesis testing", "author": ["D. Sejdinovic", "B. Sriperumbudur", "A. Gretton", "K. Fukumizu"], "venue": "The Annals of Statistics, 41(5):2263\u20132291,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "A kernel two-sample test", "author": ["A. Gretton", "K. Borgwardt", "M. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "JMLR, 13:723\u2013773,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "ICML,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Y. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "COLT,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "However, this learning paradigm suffers from the shift in data distributions across different domains, which poses a major obstacle in adapting predictive models for the target task [1].", "startOffset": 182, "endOffset": 185}, {"referenceID": 0, "context": "Learning a discriminative model in the presence of the shift between training and test distributions is known as transfer learning or domain adaptation [1].", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "Previous shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating the instance importance without using target labels [1, 2, 3, 4, 5, 6].", "startOffset": 192, "endOffset": 210}, {"referenceID": 1, "context": "Previous shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating the instance importance without using target labels [1, 2, 3, 4, 5, 6].", "startOffset": 192, "endOffset": 210}, {"referenceID": 2, "context": "Previous shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating the instance importance without using target labels [1, 2, 3, 4, 5, 6].", "startOffset": 192, "endOffset": 210}, {"referenceID": 3, "context": "Previous shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating the instance importance without using target labels [1, 2, 3, 4, 5, 6].", "startOffset": 192, "endOffset": 210}, {"referenceID": 4, "context": "Previous shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating the instance importance without using target labels [1, 2, 3, 4, 5, 6].", "startOffset": 192, "endOffset": 210}, {"referenceID": 5, "context": "Previous shallow transfer learning methods bridge the source and target domains by learning invariant feature representations or estimating the instance importance without using target labels [1, 2, 3, 4, 5, 6].", "startOffset": 192, "endOffset": 210}, {"referenceID": 6, "context": "Recent deep transfer learning methods leverage deep networks to learn more transferable feature representations by embedding domain adaptation in the pipeline of deep learning, which can simultaneously disentangle the explanatory factors of variations behind data and match the marginal distributions across domains [7, 8, 9, 10].", "startOffset": 316, "endOffset": 329}, {"referenceID": 7, "context": "Recent deep transfer learning methods leverage deep networks to learn more transferable feature representations by embedding domain adaptation in the pipeline of deep learning, which can simultaneously disentangle the explanatory factors of variations behind data and match the marginal distributions across domains [7, 8, 9, 10].", "startOffset": 316, "endOffset": 329}, {"referenceID": 8, "context": "Recent deep transfer learning methods leverage deep networks to learn more transferable feature representations by embedding domain adaptation in the pipeline of deep learning, which can simultaneously disentangle the explanatory factors of variations behind data and match the marginal distributions across domains [7, 8, 9, 10].", "startOffset": 316, "endOffset": 329}, {"referenceID": 9, "context": "Recent deep transfer learning methods leverage deep networks to learn more transferable feature representations by embedding domain adaptation in the pipeline of deep learning, which can simultaneously disentangle the explanatory factors of variations behind data and match the marginal distributions across domains [7, 8, 9, 10].", "startOffset": 316, "endOffset": 329}, {"referenceID": 3, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11].", "startOffset": 21, "endOffset": 30}, {"referenceID": 4, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11].", "startOffset": 21, "endOffset": 30}, {"referenceID": 5, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11].", "startOffset": 21, "endOffset": 30}, {"referenceID": 10, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11].", "startOffset": 177, "endOffset": 181}, {"referenceID": 3, "context": "marginal and conditional distributions which often require strong independence and/or smoothness assumptions on the factorized distributions [4], we propose a novel joint distribution discrepancy that can directly compare joint distributions by embedding them into reproducing kernel Hilbert spaces, which eliminates the need of marginal-conditional factorization for separate adaptation.", "startOffset": 141, "endOffset": 144}, {"referenceID": 11, "context": "Transfer learning is enabled in deep convolutional networks, where the dataset shifts may linger in multiple task-specific feature layers and the classifier layer, because deep features eventually transition from general to specific along the network and the transferability of features and classifiers decreases when the domain discrepancy increases [12].", "startOffset": 351, "endOffset": 355}, {"referenceID": 0, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 137, "endOffset": 152}, {"referenceID": 1, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 137, "endOffset": 152}, {"referenceID": 2, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 137, "endOffset": 152}, {"referenceID": 3, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 137, "endOffset": 152}, {"referenceID": 4, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 137, "endOffset": 152}, {"referenceID": 12, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 262, "endOffset": 278}, {"referenceID": 13, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 262, "endOffset": 278}, {"referenceID": 14, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 262, "endOffset": 278}, {"referenceID": 15, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 262, "endOffset": 278}, {"referenceID": 16, "context": "Transfer learning aims to build learning machines that are generalizable across different domains of different probability distributions [1, 2, 3, 4, 5], which enables learning on novel domains without labeled data and finds wide applications in computer vision [13, 14, 15, 16] and natural language processing [17].", "startOffset": 311, "endOffset": 315}, {"referenceID": 17, "context": "Deep networks learn abstract representations that disentangle the explanatory factors of variations in data [18] and extract transferable factors underlying different populations [19, 20], which can only reduce, but not remove, the cross-domain discrepancy [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 18, "context": "Deep networks learn abstract representations that disentangle the explanatory factors of variations in data [18] and extract transferable factors underlying different populations [19, 20], which can only reduce, but not remove, the cross-domain discrepancy [12].", "startOffset": 179, "endOffset": 187}, {"referenceID": 19, "context": "Deep networks learn abstract representations that disentangle the explanatory factors of variations in data [18] and extract transferable factors underlying different populations [19, 20], which can only reduce, but not remove, the cross-domain discrepancy [12].", "startOffset": 179, "endOffset": 187}, {"referenceID": 11, "context": "Deep networks learn abstract representations that disentangle the explanatory factors of variations in data [18] and extract transferable factors underlying different populations [19, 20], which can only reduce, but not remove, the cross-domain discrepancy [12].", "startOffset": 257, "endOffset": 261}, {"referenceID": 6, "context": "The state of the art work on deep domain adaptation bridges deep learning and transfer learning [7, 8, 9, 10] by integrating domain-adaptation modules into deep network architectures to boost transfer performance.", "startOffset": 96, "endOffset": 109}, {"referenceID": 7, "context": "The state of the art work on deep domain adaptation bridges deep learning and transfer learning [7, 8, 9, 10] by integrating domain-adaptation modules into deep network architectures to boost transfer performance.", "startOffset": 96, "endOffset": 109}, {"referenceID": 8, "context": "The state of the art work on deep domain adaptation bridges deep learning and transfer learning [7, 8, 9, 10] by integrating domain-adaptation modules into deep network architectures to boost transfer performance.", "startOffset": 96, "endOffset": 109}, {"referenceID": 9, "context": "The state of the art work on deep domain adaptation bridges deep learning and transfer learning [7, 8, 9, 10] by integrating domain-adaptation modules into deep network architectures to boost transfer performance.", "startOffset": 96, "endOffset": 109}, {"referenceID": 6, "context": "Previous deep transfer learning methods [7, 8, 9, 10] mainly correct the shifts in the marginal distributions.", "startOffset": 40, "endOffset": 53}, {"referenceID": 7, "context": "Previous deep transfer learning methods [7, 8, 9, 10] mainly correct the shifts in the marginal distributions.", "startOffset": 40, "endOffset": 53}, {"referenceID": 8, "context": "Previous deep transfer learning methods [7, 8, 9, 10] mainly correct the shifts in the marginal distributions.", "startOffset": 40, "endOffset": 53}, {"referenceID": 9, "context": "Previous deep transfer learning methods [7, 8, 9, 10] mainly correct the shifts in the marginal distributions.", "startOffset": 40, "endOffset": 53}, {"referenceID": 7, "context": "Note that [8, 10] partially align the source and target classes based on pseudo labels, but both rely on a rather strong assumption that the adaptations of marginal distributions P (X) and target distributions P (Y) can be independent.", "startOffset": 10, "endOffset": 17}, {"referenceID": 9, "context": "Note that [8, 10] partially align the source and target classes based on pseudo labels, but both rely on a rather strong assumption that the adaptations of marginal distributions P (X) and target distributions P (Y) can be independent.", "startOffset": 10, "endOffset": 17}, {"referenceID": 3, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11], which is a remarkable step towards full-transfer of joint distributions.", "startOffset": 21, "endOffset": 30}, {"referenceID": 4, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11], which is a remarkable step towards full-transfer of joint distributions.", "startOffset": 21, "endOffset": 30}, {"referenceID": 5, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11], which is a remarkable step towards full-transfer of joint distributions.", "startOffset": 21, "endOffset": 30}, {"referenceID": 10, "context": "Another line of work [4, 5, 6] gives principled approaches to correcting both target and conditional shifts based on the theory of kernel embedding of conditional distributions [11], which is a remarkable step towards full-transfer of joint distributions.", "startOffset": 177, "endOffset": 181}, {"referenceID": 1, "context": "covariate shift [2]), the conditional distributions P (Y|X) 6= Q(Y|X) (a.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "conditional shift [4]), or both (a.", "startOffset": 18, "endOffset": 21}, {"referenceID": 20, "context": "dataset shift [21]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 3, "context": "In general, the presence of conditional shift leads to an ill-posed problem, and the additional assumption that the conditional distribution changes only under location-scale transformations on X is commonly imposed to make the problem tractable [4, 5, 6].", "startOffset": 246, "endOffset": 255}, {"referenceID": 4, "context": "In general, the presence of conditional shift leads to an ill-posed problem, and the additional assumption that the conditional distribution changes only under location-scale transformations on X is commonly imposed to make the problem tractable [4, 5, 6].", "startOffset": 246, "endOffset": 255}, {"referenceID": 5, "context": "In general, the presence of conditional shift leads to an ill-posed problem, and the additional assumption that the conditional distribution changes only under location-scale transformations on X is commonly imposed to make the problem tractable [4, 5, 6].", "startOffset": 246, "endOffset": 255}, {"referenceID": 21, "context": "Many existing methods address the transfer learning problem mainly by bounding the target error with the source error plus a discrepancy metric between the marginal distributions P (X) and Q(X) of the source and target domains [22].", "startOffset": 227, "endOffset": 231}, {"referenceID": 22, "context": "Two classes of statistics have been explored for two-sample testing, which accepts or rejects the null hypothesis P (X) = Q(X) based on the two samples respectively generated from P (X) and Q(X): Energy Distance (ED) and Maximum Mean Discrepancy (MMD) [23].", "startOffset": 252, "endOffset": 256}, {"referenceID": 1, "context": "As these statistics are defined for the marginal distributions, they are only applied to covariate shift adaptation problems [2, 3, 8], which include state of the art deep adaptation methods [8, 9, 10].", "startOffset": 125, "endOffset": 134}, {"referenceID": 2, "context": "As these statistics are defined for the marginal distributions, they are only applied to covariate shift adaptation problems [2, 3, 8], which include state of the art deep adaptation methods [8, 9, 10].", "startOffset": 125, "endOffset": 134}, {"referenceID": 7, "context": "As these statistics are defined for the marginal distributions, they are only applied to covariate shift adaptation problems [2, 3, 8], which include state of the art deep adaptation methods [8, 9, 10].", "startOffset": 125, "endOffset": 134}, {"referenceID": 7, "context": "As these statistics are defined for the marginal distributions, they are only applied to covariate shift adaptation problems [2, 3, 8], which include state of the art deep adaptation methods [8, 9, 10].", "startOffset": 191, "endOffset": 201}, {"referenceID": 8, "context": "As these statistics are defined for the marginal distributions, they are only applied to covariate shift adaptation problems [2, 3, 8], which include state of the art deep adaptation methods [8, 9, 10].", "startOffset": 191, "endOffset": 201}, {"referenceID": 9, "context": "As these statistics are defined for the marginal distributions, they are only applied to covariate shift adaptation problems [2, 3, 8], which include state of the art deep adaptation methods [8, 9, 10].", "startOffset": 191, "endOffset": 201}, {"referenceID": 10, "context": "In this paper, we propose to directly measure the discrepancy between joint distributions P (X,Y) andQ(X,Y) (denoted by P andQ for short) based on the theory of kernel embedding of distributions [11], in which a probability distribution is represented as an element of a reproducing kernel Hilbert space (RKHS).", "startOffset": 195, "endOffset": 199}, {"referenceID": 23, "context": "We extend the Maximum Mean Discrepancy (MMD) [24] to measure the discrepancy between joint distributions.", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "Based on the theory of kernel two-sample testing [24], we have P = Q if and only if D(P,Q) = 0.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "Since deep features eventually transition from general to specific along the network [12, 8], transferability of features at different layers may be task-dependent and it is safer to model the joint distribution discrepancy based on multiple task-specific layers.", "startOffset": 85, "endOffset": 92}, {"referenceID": 7, "context": "Since deep features eventually transition from general to specific along the network [12, 8], transferability of features at different layers may be task-dependent and it is safer to model the joint distribution discrepancy based on multiple task-specific layers.", "startOffset": 85, "endOffset": 92}, {"referenceID": 17, "context": "Deep networks [18] can learn distributed, compositional, and abstract representations for natural data such as image and text.", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "AlexNet [25] and GoogLeNet [26], to novel joint adaptation networks (JANs) as shown in Figure 1.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "AlexNet [25] and GoogLeNet [26], to novel joint adaptation networks (JANs) as shown in Figure 1.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "The deep features learned by CNNs can disentangle the exploratory factors of variations in data distributions and promote knowledge adaptation [20, 18].", "startOffset": 143, "endOffset": 151}, {"referenceID": 17, "context": "The deep features learned by CNNs can disentangle the exploratory factors of variations in data distributions and promote knowledge adaptation [20, 18].", "startOffset": 143, "endOffset": 151}, {"referenceID": 11, "context": "However, the latest literature findings also reveal that the deep features can reduce, but not remove, the cross-domain distribution discrepancy [12, 8].", "startOffset": 145, "endOffset": 152}, {"referenceID": 7, "context": "However, the latest literature findings also reveal that the deep features can reduce, but not remove, the cross-domain distribution discrepancy [12, 8].", "startOffset": 145, "endOffset": 152}, {"referenceID": 11, "context": "transition from general to specific along the network, and the transferability of features and classifiers decreases when the cross-domain discrepancy increases [12].", "startOffset": 161, "endOffset": 165}, {"referenceID": 23, "context": "Motivated by the unbiased estimate of MMD [24], we devise a similar linear-time estimate of JDD as", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "This linear-time estimate nicely fits into the mini-batch stochastic gradient descent (SGD) algorithm implemented in [8], based on which the training of the JAN models can scale linearly to large samples.", "startOffset": 117, "endOffset": 120}, {"referenceID": 12, "context": "Office-31 [13] is a standard benchmark for domain adaptation in computer vision, comprising 4,652 images and 31 categories collected from three distinct domains: Amazon (A), which contains images downloaded from amazon.", "startOffset": 10, "endOffset": 14}, {"referenceID": 26, "context": "We evaluate all methods across three transfer tasks A\u2192W, D\u2192W and W\u2192 D, which are widely adopted by prior deep transfer learning methods [27, 7, 9], and another three transfer tasks A\u2192 D, D\u2192 A and W\u2192 A as in [8, 10].", "startOffset": 136, "endOffset": 146}, {"referenceID": 6, "context": "We evaluate all methods across three transfer tasks A\u2192W, D\u2192W and W\u2192 D, which are widely adopted by prior deep transfer learning methods [27, 7, 9], and another three transfer tasks A\u2192 D, D\u2192 A and W\u2192 A as in [8, 10].", "startOffset": 136, "endOffset": 146}, {"referenceID": 8, "context": "We evaluate all methods across three transfer tasks A\u2192W, D\u2192W and W\u2192 D, which are widely adopted by prior deep transfer learning methods [27, 7, 9], and another three transfer tasks A\u2192 D, D\u2192 A and W\u2192 A as in [8, 10].", "startOffset": 136, "endOffset": 146}, {"referenceID": 7, "context": "We evaluate all methods across three transfer tasks A\u2192W, D\u2192W and W\u2192 D, which are widely adopted by prior deep transfer learning methods [27, 7, 9], and another three transfer tasks A\u2192 D, D\u2192 A and W\u2192 A as in [8, 10].", "startOffset": 207, "endOffset": 214}, {"referenceID": 9, "context": "We evaluate all methods across three transfer tasks A\u2192W, D\u2192W and W\u2192 D, which are widely adopted by prior deep transfer learning methods [27, 7, 9], and another three transfer tasks A\u2192 D, D\u2192 A and W\u2192 A as in [8, 10].", "startOffset": 207, "endOffset": 214}, {"referenceID": 2, "context": "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9].", "startOffset": 138, "endOffset": 141}, {"referenceID": 14, "context": "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9].", "startOffset": 170, "endOffset": 174}, {"referenceID": 24, "context": "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9].", "startOffset": 214, "endOffset": 218}, {"referenceID": 25, "context": "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9].", "startOffset": 233, "endOffset": 237}, {"referenceID": 6, "context": "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9].", "startOffset": 268, "endOffset": 271}, {"referenceID": 7, "context": "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9].", "startOffset": 303, "endOffset": 306}, {"referenceID": 8, "context": "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [3], Geodesic Flow Kernel (GFK) [15], Convolutional Neural Network (AlexNet [25] and GoogLeNet [26]), Deep Domain Confusion (DDC) [7], Deep Adaptation Network (DAN) [8], and Reverse Gradient (RevGrad) [9].", "startOffset": 339, "endOffset": 342}, {"referenceID": 23, "context": "DDC is the first method that maximizes domain invariance by adding to AlexNet an adaptation layer using linear-kernel MMD [24].", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "We examine the influence of deep representations for domain adaptation by employing the breakthrough AlexNet [25] and the state of the art GoogLeNet [26] for learning deep representations.", "startOffset": 109, "endOffset": 113}, {"referenceID": 25, "context": "We examine the influence of deep representations for domain adaptation by employing the breakthrough AlexNet [25] and the state of the art GoogLeNet [26] for learning deep representations.", "startOffset": 149, "endOffset": 153}, {"referenceID": 26, "context": "For AlexNet, we follow DeCAF [27] and use the activations of the fc7 layer as image representation.", "startOffset": 29, "endOffset": 33}, {"referenceID": 7, "context": "We follow standard evaluation protocols for unsupervised domain adaptation [8, 9].", "startOffset": 75, "endOffset": 81}, {"referenceID": 8, "context": "We follow standard evaluation protocols for unsupervised domain adaptation [8, 9].", "startOffset": 75, "endOffset": 81}, {"referenceID": 23, "context": "median trick [24].", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "We implement all deep methods based on the Caffe framework, and fine-tune from Caffe-trained models of AlexNet [25] and GoogLeNet [26], both are pre-trained on the ImageNet dataset.", "startOffset": 111, "endOffset": 115}, {"referenceID": 25, "context": "We implement all deep methods based on the Caffe framework, and fine-tune from Caffe-trained models of AlexNet [25] and GoogLeNet [26], both are pre-trained on the ImageNet dataset.", "startOffset": 130, "endOffset": 134}, {"referenceID": 8, "context": "9 and the learning rate annealing strategy implemented in RevGrad [9]: the learning rate is not selected through a grid search due to high computational cost\u2014it is adjusted during SGD using the following formula: \u03b7p = \u03b70 (1+\u03b1p) , where p is the training progress linearly changing from 0 to 1, \u03b70 = 0.", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "Note that for fair comparison, the results of the state of the art methods, DAN [8] and RevGrad [9], are directly reported from their original papers.", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "Note that for fair comparison, the results of the state of the art methods, DAN [8] and RevGrad [9], are directly reported from their original papers.", "startOffset": 96, "endOffset": 99}, {"referenceID": 12, "context": "A\u2192W and A\u2192 D, where the source and target domains are substantially different, and produce comparable classification accuracy on easy transfer tasks, D \u2192W and W\u2192 D, where the source and target domains are similar [13].", "startOffset": 213, "endOffset": 217}, {"referenceID": 2, "context": "Method A\u2192W D\u2192W W\u2192 D A\u2192 D D\u2192 A W\u2192 A Avg TCA [3] 61.", "startOffset": 43, "endOffset": 46}, {"referenceID": 14, "context": "8 GFK [15] 60.", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": "7 AlexNet [25] 61.", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "1 DDC [7] 61.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "6 RevGrad [9] 73.", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "3 DAN [8] 68.", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "Method A\u2192W D\u2192W W\u2192 D A\u2192 D D\u2192 A W\u2192 A Avg TCA [3] 68.", "startOffset": 43, "endOffset": 46}, {"referenceID": 14, "context": "4 GFK [15] 71.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "7 GoogLeNet [26] 71.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "5 DDC [7] 72.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "1 DAN [8] 76.", "startOffset": 6, "endOffset": 9}, {"referenceID": 11, "context": "These results confirm the current practice that deep networks learn abstract feature representations, which can only reduce, but not remove, the domain discrepancy [12].", "startOffset": 164, "endOffset": 168}, {"referenceID": 11, "context": "Since deep features eventually transition from general to specific along the network, there may be multiple layers where the features are not safely transferable [12, 8], hence it is safer to adapt the joint distributions based on multi-layer features instead of single-layer features.", "startOffset": 162, "endOffset": 169}, {"referenceID": 7, "context": "Since deep features eventually transition from general to specific along the network, there may be multiple layers where the features are not safely transferable [12, 8], hence it is safer to adapt the joint distributions based on multi-layer features instead of single-layer features.", "startOffset": 162, "endOffset": 169}, {"referenceID": 10, "context": "It is noteworthy that this paper initiates a principled way to model multi-layer features in a kernel embedding framework [11].", "startOffset": 122, "endOffset": 126}, {"referenceID": 2, "context": "Method C\u2192I C\u2192P C\u2192B I\u2192C I\u2192P I\u2192B P\u2192C P\u2192I P\u2192B B\u2192C B\u2192I B\u2192P Avg TCA [3] 83.", "startOffset": 63, "endOffset": 66}, {"referenceID": 14, "context": "3 GFK [15] 84.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "0 GoogLeNet [26] 85.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "4 DDC [7] 86.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "1 DAN [8] 90.", "startOffset": 6, "endOffset": 9}, {"referenceID": 26, "context": "Feature Visualization: We visualize in Figures 2(a)\u20132(d) the activations of task A\u2192W learned by DAN and JAN respectively using the t-SNE embeddings [27].", "startOffset": 148, "endOffset": 152}, {"referenceID": 21, "context": "Distribution Discrepancy: The theory of domain adaptation [22, 28] suggests the A-distance as a measure of cross-domain discrepancy, which, together with the source risk, will bound the target risk.", "startOffset": 58, "endOffset": 66}, {"referenceID": 27, "context": "Distribution Discrepancy: The theory of domain adaptation [22, 28] suggests the A-distance as a measure of cross-domain discrepancy, which, together with the source risk, will bound the target risk.", "startOffset": 58, "endOffset": 66}], "year": 2016, "abstractText": "Deep networks rely on massive amounts of labeled data to learn powerful models. For a target task short of labeled data, transfer learning enables model adaptation from a different source domain. This paper addresses deep transfer learning under a more general scenario that the joint distributions of features and labels may change substantially across domains. Based on the theory of Hilbert space embedding of distributions, a novel joint distribution discrepancy is proposed to directly compare joint distributions across domains, eliminating the need of marginal-conditional factorization. Transfer learning is enabled in deep convolutional networks, where the dataset shifts may linger in multiple task-specific feature layers and the classifier layer. A set of joint adaptation networks are crafted to match the joint distributions of these layers across domains by minimizing the joint distribution discrepancy, which can be trained efficiently using back-propagation. Experiments show that the new approach yields state of the art results on standard domain adaptation datasets.", "creator": "LaTeX with hyperref package"}}}