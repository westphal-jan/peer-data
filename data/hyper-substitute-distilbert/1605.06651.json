{"id": "1605.06651", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2016", "title": "Gambler's Ruin Bandit Problem", "abstract": "regarding this proposal, we propose a new sequential decision making idea called { \\ em hardware's ruin bandit problem } ( grbp ). in each round module the grbp processor user removes a gambler'h ruin problem with two possible actions : a { \\ em scheduling program } that moves the agent downstream during the state locally creating the current state ; perform a { \\ em successor action } that move potential learner somewhere into one than a two intermediate states ( goal and dead - head state ). the whichever round ends to their concurrent state grows reached. see first specify grbp as option optimization option, and prove that parallel continuity policy exist characterized by a simple threshold rule. convergence problem is solved for determining time budget. briefly, we discuss the case when the query ignores probabilities already unknown and take logarithmic independently specific regret bounds. will also get instability condition whose certain the learner only records integer deviation. numerous conditions including optimal quality performance system can be chosen as bypass protocol, as which successful continuation action leading to the conservative condition and path restoration action proportional to the surgery.", "histories": [["v1", "Sat, 21 May 2016 14:43:44 GMT  (273kb,D)", "https://arxiv.org/abs/1605.06651v1", null], ["v2", "Mon, 11 Jul 2016 13:54:20 GMT  (416kb,D)", "http://arxiv.org/abs/1605.06651v2", null], ["v3", "Thu, 29 Sep 2016 03:51:57 GMT  (504kb,D)", "http://arxiv.org/abs/1605.06651v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nima akbarzadeh", "cem tekin"], "accepted": false, "id": "1605.06651"}, "pdf": {"name": "1605.06651.pdf", "metadata": {"source": "CRF", "title": "Gambler\u2019s Ruin Bandit Problem", "authors": ["Nima Akbarzadeh", "Cem Tekin"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION\nMulti-armed bandits (MAB) are used to model a plethora of applications that require sequential decision making under uncertainty ranging from clinical trials [1] to web advertising [2]. In the conventional MAB [3], [4] the learner chooses an action from a finite set of actions at each round, and receives a random reward. The goal of the learner is to maximize its long-term expected reward by choosing actions that yield high rewards. This is a non-trivial task, since the reward distributions are not known beforehand. Numerous orderoptimal index-based learning rules have been developed for the conventional MAB [4]\u2013[6]. These rules act myopically by choosing the action with the maximum index in each round.\nSituations that require multiple actions to be taken in each round cannot be modeled using conventional MAB. As an example, consider medical treatment administration. At the beginning of each round a patient arrives to the intensive care unit (ICU) with a random initial health state. The goal state is defined as discharge and dead-end state is defined as death. Actions correspond to treatment options that move the patient randomly over the state space. The objective is to maximize the expected number of patients that are discharged by learning the optimal treatment policy using the observations gathered from the previous patients. In the example given above, each round corresponds to a goaloriented Markov Decision Process (MDP) with dead-ends\nCem Tekin is supported by TUBITAK 2232 Fellowship (116C043).\n[7]. The learner knows the state space, goal and dead-end states, but does not know the state transition probabilities a priori. At each round, the learner chooses a sequence of actions and only observes the state transitions that result from the chosen actions. In the literature, this kind of feedback information is called bandit feedback [8].\nMotivated by the application described above, we propose a new MAB problem in which multiple arms are selected in each round until a terminal state is reached. Due to its resemblance to the Gambler\u2019s Ruin Problem [9]\u2013[11], we call this new MAB problem the Gambler\u2019s Ruin Bandit Problem (GRBP). In GRBP, the system proceeds in a sequence of rounds \u03c1 \u2208 {1, 2, . . .}. Each round is modeled as an MDP (as in Fig. 1 ) with unknown state transition probabilities and terminal (absorbing) states. The set of terminal states includes a goal state G and a dead-end state D, and the non-terminal states are ordered between the goal and dead-end states. In each non-terminal state, there are two possible actions: a continuation action (action C) that moves the learner randomly over the state space around the current state; and a terminal action (action F ) that moves the learner directly into a terminal state. Starting from a random, non-terminal initial state, the learner chooses a sequence of actions and observes the resulting state transitions until a terminal state is reached. The learner incurs a unit reward if the goal state is reached. Otherwise, it incurs no reward. The goal of the learner is to maximize its cumulative expected reward over the rounds.\nIf the state transition probabilities were known beforehand, an omnipotent oracle with unlimited computational power could calculate the optimal policy that maximizes the probability of hitting the goal state from any initial state, and then select its actions according to the optimal policy. We define the regret of the learner by round \u03c1 as the difference in the expected number of times the goal state is reached by the omnipotent oracle and the learner by round \u03c1.\nFirst, we show that the optimal policy for GRBP can be computed in a straightforward manner: there exists a threshold state above which it is always optimal to take action C and on or below which it is always optimal to take action F . Then, we propose an online learning algorithm for the learner, and bound its regret for two different regions that the actual state transition probabilities can lie in. The regret is bounded (finite) in one region, while it is logarithmic in the number of rounds in the other region. These bounds are problem-specific, in the sense that they are functions of the state transition probabilities. Finally, we illustrate the\nar X\niv :1\n60 5.\n06 65\n1v 3\n[ cs\n.L G\n] 2\n9 Se\np 20\n16\nbehavior of the regret as a function of the state transition probabilities through numerical experiments.\nThe contributions of this paper can be summarized as follows: \u2022 We define a new MAB problem, called GRBP, in which\nthe learner takes a sequence of actions in each round with the objective of reaching to the goal state. \u2022 We show that using conventional MAB algorithms such as UCB1 [4] in GRBP by enumerating all deterministic Markov policies is very inefficient and results in high regret. \u2022 We prove that the optimal policy for GRBP has a threshold form and the value of the threshold can be calculated in a computationally efficient way. \u2022 We derive bounds on the regret of the learner with respect to an omnipotent oracle that acts optimally. Unlike conventional MAB where the regret growth is at least logarithmic in the number of rounds [3], in GRBP regret can be either logarithmic or bounded, based on the values of the state transition probabilities. We explicitly characterize the region of state transition probabilities in which the regret is bounded.\nRemainder of the paper is organized as follows. Related work is given in Section II. GRBP is defined in Section III. Form of the optimal policy for the GRBP is given in Section IV. The learning algorithm for GRBP is given in Section V together with its regret analysis. Numerical results are shown in Section VI. Conclusion is given in Section VII."}, {"heading": "II. RELATED WORK", "text": ""}, {"heading": "A. Gambler\u2019s Ruin Problem", "text": "If action F is removed from the GRBP, it becomes the Gambler\u2019s Ruin Problem. In the model of Hunter et al. [10] of the Gambler\u2019s Ruin Problem, in addition to the standard outcome of moving one state to the left or right, two extra outcomes are also considered. One outcome changes the state immediately to G, while the other outcome changes the state immediately to D. These outcomes are referred to as Windfall and Catastrophe outcomes, respectively. The ruin and winning probabilities and the duration of the game are calculated based on these additional outcomes. In another model [11], modifications such as the chance of absorption in states other than G and D and staying in the same state are\nconsidered. The ruin and winning probabilities are calculated according to the proposed state transition model. Unlike GRBP which is an MDP, the Gambler\u2019s Ruin Problem is a Markov chain. Moreover, the ruin and winning probabilities in the models above can be calculated exactly since the transition probabilities are assumed to be known."}, {"heading": "B. MDPs", "text": "GRBP is closely related to goal oriented MDPs and stochastic shortest path problems [12]. For these problems, in each state (or time epoch), an action has to be taken with the aim of reaching to the goal state (G) with minimum cost. For this task, the optimal policy have to be determined beforehand using the set of known transition probabilities. Recently, progress has been made in obtaining solutions for MDPs that have dead-end (D) states in addition to goal (G) states [7], [13]. These solutions require value iteration and heuristic search methods to be performed using the knowledge of transition probabilities. To the best of our knowledge, a reinforcement learning algorithm that works without knowing the transition probabilities a priori and that achieves logarithmic regret bounds, has not been developed yet for these problems.\nReinforcement learning in MDPs is considered by numerous researchers [14], [15]. In these works, it is assumed that the underlying MDP is unknown but ergodic, i.e., it is possible to reach from any state to all other states with a positive probability under any policy. These works adopt the principle of optimism under uncertainty to choose an action that maximizes the expected reward among a set of MDP models that are consistent with the estimated transition probabilities. Unlike these works, in GRBP (i) the MDP is not ergodic, and (ii) the reward is obtained only in the terminal state and not after each chosen action."}, {"heading": "C. Multi-armed Bandits", "text": "Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]\u2013 [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem). The performance of a learning algorithm for a MAB problem is computed using the notion of regret. For the stochastic MAB problem [3], the regret is defined as the difference between the total (expected) reward of the learning algorithm and an oracle which acts optimally based on complete knowledge of the problem parameters. It is shown that the regret grows logarithmically in the number of rounds for this problem.\nGRBP can be viewed as a MAB problem in which each arm corresponds to a policy. Since the set of possible deterministic policies for the GRBP is exponential in the number of states, it is infeasible to use algorithms developed for MAB problems to directly learn the optimal policy by experimenting with different policies over different rounds.\nIn addition, GRBP model does not fit into the combinatorial models proposed in prior works [18]. Due to these differences, existing MAB solutions cannot solve GRBP in an efficient way. Therefore, a new learning methodology that exploits the structure of the GRBP is needed."}, {"heading": "III. PROBLEM FORMULATION", "text": ""}, {"heading": "A. Definition of the GRBP", "text": "In the GRBP, the system is composed of a finite set of states S := {D, 1, . . . , G}, where integer D = 0 denotes the dead-end state and G denotes the goal state. The set of initial (starting) states is denoted by S\u0303 := {1, . . . , G\u22121}. The system operates in rounds (\u03c1 = 1, 2, . . .). The initial state of each round is drawn from a probability distribution q(s), s \u2208 S\u0303 over the set of initial states S\u0303, such that 1 \u2212 q(1) > 0. The current round ends and the next round starts when the learner hits state D or G. Because of this, D and G are called terminal states. All other states are called non-terminal states. Each round is divided into multiple time slots in which the learner takes an action in each time slot from the action set A := {C,F} with the aim of reaching to state G. Here, C denotes the continuation action and F is the terminal action. According to Fig. 1, action C moves the learner one state to the right or to the left of the current state. Action F moves the learner directly to one of the terminal states. Possible outcomes of each action in a non-terminal state s is shown in Fig. 1. Let s\u03c1t denote the state at the beginning of the tth time slot of round \u03c1 and a\u03c1t denote the action taken at the tth time slot of round \u03c1. The state transition probabilities for action C are given by\nPr(s\u03c1t+1 = s+ 1|s\u03c1t = s, a\u03c1t = C) = pC , t \u2265 1, s \u2208 S\u0303 Pr(s\u03c1t+1 = s\u2212 1|s\u03c1t = s, a\u03c1t = C) = pD, t \u2265 1, s \u2208 S\u0303\nwhere pC + pD = 1. The state transition probabilities for action F are given by\nPr(s\u03c1t+1 = G|s\u03c1t = s, a\u03c1t = F ) = pF , t \u2265 1, s \u2208 S\u0303 Pr(s\u03c1t+1 = D|s\u03c1t = s, a\u03c1t = F ) = 1\u2212 pF , t \u2265 1, s \u2208 S\u0303\nwhere 0 < pF < 1. If the state transition probabilities are known, each round can be modeled as a MDP and an optimal policy can be found by dynamic programming [12], [19].\nB. Value Functions, Rewards and the Optimal Policy\nLet \u03c0 = (\u03c01, \u03c02, . . .), where \u03c0t : S\u0303 \u2192 A, t \u2265 1 represent a deterministic Markov policy. \u03c0 is a stationary policy if \u03c0t = \u03c0t\u2032 for all t and t\u2032. For this case we will simply use \u03c0 : S\u0303 \u2192 A to denote a stationary deterministic Markov policy. Since the time horizon is infinite within a round and the state transition probabilities are time-invariant, it is sufficient to search for the optimal policy within the set of stationary deterministic Markov policies, which is denoted by \u03a0. Let V \u03c0(s) denote the probability of reaching to G by using policy \u03c0 given that the system is in state s. Let Q\u03c0(s, a) denote the\nprobability of reaching to G by taking action a in state s, and then continuing according to policy \u03c0. We have\nQ\u03c0(s, C) = pCV \u03c0(s+ 1) + pDV \u03c0(s\u2212 1), Q\u03c0(s, F ) = pF\nfor s \u2208 S\u0303. Hence, V \u03c0(s), s \u2208 S\u0303 can be computed by solving the following set of equations:\nV \u03c0(G) = 1, V \u03c0(D) = 0, V \u03c0(s) = Q\u03c0(s, \u03c0(s)), \u2200s \u2208 S\u0303 where \u03c0(s) denotes the action selected by \u03c0 in state s. The value of policy \u03c0 is defined as\nV \u03c0 := \u2211 s\u2208S\u0303 q(s)V \u03c0(s).\nThe optimal policy is denoted by\n\u03c0\u2217 := arg max \u03c0\u2208\u03a0 V \u03c0\nand the value of the optimal policy is denoted by\nV \u2217 := max \u03c0\u2208\u03a0 V \u03c0.\nThe optimal policy is characterized by Bellman optimality equations for all s \u2208 S\u0303 V \u2217(s) = max{pFV \u2217(G), pCV \u2217(s+ 1) + pDV \u2217(s\u2212 1)},\n= max{pF , pCV \u2217(s+ 1) + pDV \u2217(s\u2212 1)}. (1) As it is sufficient to search for the optimal policy within stationary deterministic Markov policies and since there are only two actions that can be taken in each s \u2208 S\u0303, the number of all such policies is 2G\u22121. In Section IV, we will prove that the optimal policy for GRBP has a simple threshold form, which reduces the number of policies to learn from 2G\u22121 to 2."}, {"heading": "C. Online Learning in the GRBP", "text": "As we described in the previous subsection, when the state transition probabilities are known, optimal solution and its probability of reaching to the goal can be found by solving Bellman optimality equations. When the learner does not know pC and pF , the optimal policy cannot be computed a priori, and hence needs to be learned. We define the learning loss of the learner, who is not aware of the optimal policy a priori, with respect to an oracle, who knows the optimal policy from the initial round, as the regret given by\nReg(T ) := TV \u2217 \u2212 T\u2211 \u03c1=1 V \u03c0\u0302\u03c1\nwhere \u03c0\u0302\u03c1 denotes the policy that is used by the learner in round \u03c1. Let N\u03c0(T ) denote the number of times policy \u03c0 is used by the learner by round T . For any policy \u03c0, let \u2206\u03c0 := V\n\u2217\u2212V \u03c0 denote the suboptimality gap of that policy. The regret can be rewritten as\nReg(T ) = \u2211 \u03c0\u2208\u03a0 N\u03c0(T )\u2206\u03c0. (2)\nIn this paper, we will design learning algorithms that minimize the growth rate of the expected regret, i.e., E[Reg(T )]. A straightforward way to do this will be to employ UCB1 algorithm [4] or its variants [6] by taking each policy as an arm. The result below state a logarithmic bound on the expected regret when UCB1 is used.\nTheorem 1. When UCB1 in [4] is used to select the policy to follow at the beginning of each round (with set of arms \u03a0), we have\nE[Reg(T )] = 8 \u2211\n\u03c0:V \u03c0<V \u2217\nlog T\n\u2206\u03c0 +\n( 1 + \u03c02\n3 )\u2211 \u03c0\u2208\u03a0 \u2206\u03c0.\nProof: See [4]. As shown in Theorem 1, the expected regret of UCB1 depends linearly on the number of suboptimal policies. For GRBP, the number of policies can be very large. For instance, we have 2G\u22121 different stationary deterministic Markov policies for the defined problem. These imply that using UCB1 to learn the optimal policy is highly inefficient for the GRBP. The learning algorithm we propose in Section V exploits a result on the form of the optimal policy that will be derived in Section IV to learn the optimal policy in a fast manner. This learning algorithm calculates an estimated optimal policy using the estimated transition probabilities, and hence learns much faster than applying UCB1 naively. Moreover, it can even achieve bounded regret (instead of logarithmic regret) under some special cases."}, {"heading": "IV. FORM OF THE OPTIMAL POLICY", "text": "In this section, we prove that the optimal policy for GRBP has a threshold form. The value of the threshold depends only on the state transition probabilities and the number of states. First, we give the definition of a stationary threshold policy.\nDefinition 1. \u03c0 is a stationary threshold policy if there exists \u03c4 \u2208 {0, 1, . . . , G\u2212 1} such that \u03c0(s) = C for all s > \u03c4 and \u03c0(s) = F for all s \u2264 \u03c4 . We use \u03c0tr\u03c4 to denote the stationary threshold policy with threshold \u03c4 . The set of stationary threshold policies is given by \u03a0tr := {\u03c0tr\u03c4 }\u03c4={0,1,...,G\u22121}.\nThe next lemma constrains the set of policies that the optimal policy lies in.\nLemma 1. In the GRBP it is always optimal to select action C at s \u2208 S\u0303 \u2212 {1}.\nProof: By (1), for s \u2208 S\u0303 \u2212 {1} we have V \u2217(s) = max{pF , pCV \u2217(s+ 1) + pDV \u2217(s\u2212 1)}.\nIf V \u2217(s) = pF , this implies that\npCV \u2217(s+ 1) + pDV \u2217(s\u2212 1) \u2264 pF \u21d2\nV \u2217(s\u2212 1) \u2264 p F \u2212 pCV \u2217(s+ 1)\npD . (3)\nBy definition,\npF \u2264 V \u2217(s),\u2200s \u2208 S\u0303. (4)\nTherefore,\npF \u2212 pCV \u2217(s+ 1) pD \u2264 p F \u2212 pCpF pD = pF\nwhich in combination with (3) implies that V \u2217(s\u2212 1) \u2264 pF . According to (4) we find that V \u2217(s \u2212 1) = pF . Then, we conclude that\nV \u2217(s) = pF \u21d2 V \u2217(s\u2212 1) = pF ,\u2200s \u2208 S\u0303 \u2212 {1}. This also implies that\nV \u2217(s+ 1) \u2264 p F \u2212 pDV \u2217(s\u2212 1)\npC = pF .\nConsequently, if V \u2217(s) = pF for some s \u2208 S\u0303 \u2212 {1}, then V \u2217(s) = pF ,\u2200s \u2208 S\u0303 \u2212 {1}. (5)\nBy (5), if V \u2217(s) = pF for some s \u2208 S\u0303 \u2212 {1}, then this implies that V \u2217(G\u2212 1) = pF . Since V \u2217(G) = 1, we have V \u2217(G\u2212 1) = max{pF , pC + pDpF } = pF\n\u21d2 pF \u2265 pC + pDpF\n\u21d2 pF (1\u2212 pD) \u2265 pC \u21d2 pF \u2265 1\u21d2 pF = 1. This shows that unless pF = 1, it is suboptimal to select action F in states S\u0303 \u2212{1} and since pF = 1 is a trivial case, we disregard that. Hence, it is always optimal to select action C at s \u2208 S\u0303 \u2212 {1}.\nThe result of Lemma 1 holds independently from the set of transition probabilities and the number of states. Lemma 1 leaves out only two candidates for the optimal policy. The first candidate is the policy which selects action C at any state s \u2208 S\u0303. The second candidate selects action C in all states except state 1. Hence, the optimal policy is always in set {\u03c0tr0 , \u03c0tr1 }. This reduces the set of policies to consider from 2G\u22121 to 2. Let r := pD/pC denote the failure ratio of action C. The next lemma gives the value functions for \u03c0tr1 and \u03c0tr0 .\nLemma 2. In the GRBP we have\n(i) V \u03c0 tr 1 (s) =  pF + (1\u2212 pF ) 1\u2212 r s\u22121 1\u2212 rG\u22121 , when r 6= 1\npF + (1\u2212 pF ) s\u2212 1 G\u2212 1 , when r = 1\n(ii) V \u03c0 tr 0 (s) =  1\u2212 rs 1\u2212 rG , when r 6= 1 s\nG , when r = 1\nfor s \u2208 S\u0303. Proof: (i):\nFor \u03c0tr1 we have: V \u03c0 tr 1 (G) = 1 V \u03c0 tr 1 (G\u2212 1) = pCV \u03c0tr1 (G) + pDV \u03c0tr1 (G\u2212 2) . . . V \u03c0 tr 1 (2) = pCV \u03c0 tr 1 (3) + pDV \u03c0 tr 1 (1)\nV \u03c0 tr 1 (1) = pF\n\u21d2  (pC + pD)V \u03c0 tr 1 (G\u2212 1) = pC + pDV \u03c0tr1 (G\u2212 2) . . .\n(pC + pD)V \u03c0 tr 1 (2) = pCV \u03c0 tr 1 (3) + pDpF\n\u21d2  pC(V \u03c0 tr 1 (G\u2212 1)\u2212 1) = pD(V \u03c0 tr 1 (G\u2212 2)\u2212 V \u03c0tr1 (G\u2212 1)) . . . pC(V \u03c0 tr 1 (s+ 1)\u2212 V \u03c0tr1 (s+ 2)) = pD(V \u03c0 tr 1 (s)\u2212 V \u03c0tr1 (s+ 1)) . . .\npC(V \u03c0 tr 1 (2)\u2212 V \u03c0tr1 (3)) = pD(pF \u2212 V \u03c0tr1 (2))\n\u21d2  V \u03c0 tr 1 (G\u2212 1)\u2212 1 = rG\u22122(pF \u2212 V \u03c0tr1 (2)) . . . V \u03c0 tr 1 (s)\u2212 V \u03c0tr1 (s+ 1) = rs\u22121(pF \u2212 V \u03c0tr1 (2)) . . .\nV \u03c0 tr 1 (2)\u2212 V \u03c0tr1 (3) = r(pF \u2212 V \u03c0tr1 (2))\n\u21d2\n(6)\nSummation of all the terms results in\n1\u2212 V \u03c0tr1 (2) = (V \u03c0tr1 (2)\u2212 pF )( G\u22122\u2211 i=1 ri)\u21d2 (7)\nV \u03c0 tr 1 (2)( G\u22122\u2211 i=0 ri) = 1 + pF ( G\u22122\u2211 i=1 ri)\u21d2\nV \u03c0 tr 1 (2)( G\u22122\u2211 i=0 ri) = 1\u2212 pF + pF ( G\u22122\u2211 i=0 ri)\u21d2\nV \u03c0 tr 1 (2) = pF + 1\u2212 pF ( \u2211G\u22122 i=0 r i) \u21d2 V \u03c0 tr 1 (2) = pF + (1\u2212 pF ) 1\u2212 r\n1\u2212 rG\u22121 .\nThen, for sth state, we have to sum up to (s\u2212 1)th equation in (6):\nV \u03c0 tr 1 (s)\u2212 V \u03c0tr1 (2) = (V \u03c0tr1 (2)\u2212 pF )( s\u22122\u2211 i=1 ri)\u21d2\nV \u03c0 tr 1 (s) = pF + (V \u03c0 tr 1 (2)\u2212 pF )( s\u22122\u2211 i=0 ri)\u21d2 (8) V \u03c0 tr 1 (s) = pF + (1\u2212 pF ) 1\u2212 r s\u22121\n1\u2212 rG\u22121 . (9)\nFor the fair case, r has to be set to 1 in (7) and (8). Then,\nV \u03c0 tr 1 (2) = pF + (1\u2212 pF ) 1\nG\u2212 1 and\nV \u03c0 tr 1 (s) = pF + (1\u2212 pF ) s\u2212 1\nG\u2212 1 .\nCase (ii): Since action F is never selected by \u03c0tr0 , for this case, standard analysis of the gambler\u2019s ruin problem applies.\nThus, the probability of hitting G from state s is\n(1\u2212 rs)/(1\u2212 rG) (10) for r 6= 1 and s/G for r = 1 [20].\nThe form of the optimal policy is given in the following theorem.\nTheorem 2. In the GRBP, the optimal policy is \u03c0tr\u03c4\u2217 , where\n\u03c4\u2217 =  sign(pF \u2212 1\u2212 r 1\u2212 rG ), when r 6= 1\nsign(pF \u2212 1 G ), when r = 1\nwhere sign(x) = 1 if x is nonnegative and 0 otherwise.\nProof: Since we have found in Lemma 1 that it is always optimal to select action C when the state is in {2, . . . , G\u22121}, to find the optimal policy, it is sufficient to compare the value functions of the two policies for s = 1. When r 6= 1, this gives \u03c0\u2217 = \u03c0tr1 if\n1\u2212 r 1\u2212 rG < p F\nand \u03c0\u2217 = \u03c0tr0 otherwise. 1 Similarly, if r = 1 and 1/G < pF , then \u03c0\u2217 = \u03c0tr1 . Otherwise, \u03c0 \u2217 = \u03c0tr0 . Using these, the value of the optimal threshold is given as\n\u03c4\u2217 =  sign(pF \u2212 1\u2212 r 1\u2212 rG ) if r 6= 1\nsign(pF \u2212 1 G ) if r = 1\nwhich completes the proof. When r 6= 1, the term (1 \u2212 r)/(1 \u2212 rG) represents probability of hitting G starting from state 1 by always selecting action C. This probability is equal to 1/G when r = 1. Because of this, it is optimal to take the terminal action in some cases for which pC > pF . Although the continuation action can move the system state in the direction of the goal state for some time, the long term chance of hitting the goal state by taking the continuation action can be lower than the chance of hitting the goal state by immediately taking the terminal action at state 1.\nEquation of the boundary for which the optimal policy changes from \u03c0tr0 to \u03c0 tr 1 is\npF = B(r) := (1\u2212 r)/(1\u2212 rG) (11) when r 6= 1. This decision boundary is illustrated in Fig. 2 for different values of G. We call the region of transition probabilities for which \u03c0tr0 is optimal as the exploration region, and the region for which \u03c0tr1 is optimal as the noexploration region. In exploration region, the optimal policy does not take action F in any round. Therefore, any learning algorithm that needs to learn how well action F performs, needs to explore action F . As the value of G increases, area of the exploration region decreases due to the fact that probability of hitting the goal state by only taking action C decreases.\n1When (1\u2212 r)/(1\u2212 rG) = pF both \u03c0tr1 and \u03c0tr0 are optimal. For this case, we favor \u03c0tr1 because it always ends the current round."}, {"heading": "V. AN ONLINE LEARNING ALGORITHM AND ITS REGRET ANALYSIS", "text": "In this section, we propose a learning algorithm that minimizes the regret when the state transition probabilities are unknown. The proposed algorithm forms estimates of state transition probabilities based on the history of state transitions, and then, uses these estimates together with the form of the optimal policy obtained in Section IV to calculate an estimated optimal policy at each round."}, {"heading": "A. Greedy Exploitation with Threshold Based Exploration", "text": "The learning algorithm for the GRBP is called Greedy Exploitation with Threshold Based Exploration (GETBE) and its pseudocode is given in Algorithm 1. Unlike conventional MAB algorithms [3], [4], [6] which require all arms to be sampled at least logarithmically many times, GETBE does not need to sample all policies (arms) logarithmically many times to find the optimal policy with a sufficiently high probability. GETBE achieves this by utilizing the form of the optimal policy derived in the previous section. Although GETBE does not require all policies to be explored, it requires exploration of action F when the estimated optimal policy never selects action F . This forced exploration is done to guarantee that GETBE does not get stuck in the suboptimal policy.\nGETBE keeps counters NGF (\u03c1), NF (\u03c1), N u C(\u03c1) and NC(\u03c1): (i) NGF (\u03c1) is the number of times action F is selected and terminal state G is entered upon selection of action F by the beginning of round \u03c1, (ii) NF (\u03c1) is the number of times action F is selected by the beginning of round \u03c1, (iii) NuC(\u03c1) is the number of times transition from some state s to s+ 1 happened (i.e., the state moved up) after selecting action C by the beginning of round \u03c1, (iv) NC(\u03c1) is the number of times action C is selected by the beginning of round \u03c1. Let TF (\u03c1) and TC(\u03c1) represent the number of times action F and action C is selected in round \u03c1, respectively. Since, action F is a terminal action, it can be selected at most once in each round. However, action C can be selected multiple times in the same round. Let TGF (\u03c1) and T u C(\u03c1) represent the number of times state G is reached after the selection of action F and\nthe number of times the state moved up after the selection of action C in round \u03c1, respectively.\nAt the beginning of round \u03c1, GETBE forms the transition probability estimates p\u0302F\u03c1 := N G F (\u03c1)/NF (\u03c1) and p\u0302 C \u03c1 := NuC(\u03c1)/NC(\u03c1) that correspond to actions F and C, respectively. Then, it computes the estimated optimal policy \u03c0\u0302\u03c1 by using the form of the optimal policy given in Theorem 2 for the GRBP. If \u03c0\u0302\u03c1 = \u03c0tr1 , then GETBE operates in greedy exploitation mode by acting according to \u03c0tr1 for the entire round. Else if \u03c0\u0302\u03c1 = \u03c0tr0 , then GETBE operates in triggered exploration mode and selects action F in the first time slot of that round if NF (\u03c1) < D(\u03c1), where D(\u03c1) is a non-decreasing control function that is an input of GETBE. This control function helps GETBE to avoid getting stuck in the suboptimal policy by forcing the selection of action F , although it is suboptimal according to \u03c0\u0302\u03c1. When NF (\u03c1) \u2265 D(\u03c1), GETBE employs \u03c0\u0302\u03c1 for the entire round.\nAt the end of round \u03c1 the values of counters are updated as follows:\nNF (\u03c1+ 1) = NF (\u03c1) + TF (\u03c1) NGF (\u03c1+ 1) = N G F (\u03c1) + T G F (\u03c1)\nNC(\u03c1+ 1) = NC(\u03c1) + TC(\u03c1) NuC(\u03c1+ 1) = N u C(\u03c1) + T u C(\u03c1). (12)\nThese values are used to estimate the transition probabilities that will be used at the beginning of round \u03c1+ 1, for which the above procedure repeats. In the analysis of GETBE, we will show that when NF (\u03c1) \u2265 D(\u03c1), the probability that GETBE selects the suboptimal policy is very small, which implies that the regret incurred is very small."}, {"heading": "B. Regret Analysis", "text": "In this section, we bound the (expected) regret of GETBE. We show that GETBE achieves bounded regret when the unknown transition probabilities lie in no-exploration region and logarithmic (in number of rounds) regret when the unknown transition probabilities lie in exploration region. Based on Theorem 2, GETBE only needs to learn the optimal policy from the set of policies {\u03c0tr0 , \u03c0tr1 }. Using this fact and taking the expectation of (2), the expected regret of GETBE can be written as\nE[Reg(T )] = \u2211\n\u03c0\u2208{\u03c0tr0 ,\u03c0tr1 }\nE[N\u03c0(T )]\u2206\u03c0. (13)\nLet \u2206(s) := |V \u03c0tr1 (s)\u2212V \u03c0tr0 (s)|, s \u2208 S\u0303 be the suboptimality gap when the initial state is s. For any \u03c0 \u2208 {\u03c0tr0 , \u03c0tr1 }, we have \u2206\u03c0 \u2264 \u2206max, where \u2206max := maxs\u2208S\u0303 \u2206(s). The next lemma gives closed-form expressions for \u2206(s) and \u2206max.\nLemma 3. We have\n\u2206(s) =  G\u2212s G\u22121 |pF \u2212 1 G | if r = 1\nrG\u22121\u2212rs\u22121 rG\u22121\u22121 |pF \u2212 1\u2212 r 1\u2212 rG | if r 6= 1\n2I(\u00b7) denotes the indicator function which is 1 if the expression inside evaluates true and 0 otherwise.\nAlgorithm 1 GETBE Algorithm 1: Input : G,D(\u03c1) 2: Initialize: Take action C and then action F once to form initial\nestimates: NGF (1), NF (1) = 1, N u C(1), NC(1) = 1 (Round(s) to form the initial estimates (at most 2 rounds) are ignored in the regret analysis). \u03c1 = 1\n3: while \u03c1 \u2265 1 do 4: Get initial state s\u03c11 \u2208 S\u0303, t = 1 5: p\u0302F\u03c1 = NGF (\u03c1)\nNF (\u03c1) , p\u0302C\u03c1 =\nNuC(\u03c1) NC(\u03c1) , r\u0302\u03c1 = 1\u2212 p\u0302C\u03c1 p\u0302C\u03c1\n6: if p\u0302u\u03c1 = 0.5 then 7: \u03c4\u0302\u03c1 = sign(p\u0302F\u03c1 \u2212 1/G) 8: else 9: \u03c4\u0302\u03c1 = sign(p\u0302F\u03c1 \u2212\n1\u2212 r\u0302\u03c1 1\u2212 (r\u0302\u03c1)G )\n10: end if 11: Set \u03c0\u0302\u03c1 = \u03c0tr\u03c4\u0302\u03c1 12: while s\u03c1t 6= G or D do 13: if (\u03c0\u0302\u03c1 = \u03c0tr0 && NF (\u03c1) < D(\u03c1)) || (s\u03c1t \u2264 \u03c4\u0302\u03c1) then 14: Select action F , observe state s\u03c1t+1 15: TF (\u03c1) = TF (\u03c1) + 1, TGF (\u03c1) = I(s \u03c1 t+1 = G) 2 16: else 17: Select action C, observe state s\u03c1t+1 18: TC(\u03c1) = TC(\u03c1) + 1 19: TuC(\u03c1) = T u C(\u03c1) + I(s \u03c1 t+1 = s \u03c1 t + 1) 20: t = t+ 1 21: end if 22: end while 23: Update the counters according to (12) 24: \u03c1 = \u03c1+ 1 25: end while\nand\n\u2206max =  |pF \u2212 1 G | if r = 1\n|pF \u2212 1\u2212 r 1\u2212 rG | if r 6= 1\nProof: According to Lemma 2 we have Case (i) r = 1:\n\u2206(s) = |V \u03c0tr1 (s)\u2212 V \u03c0tr0 (s)|= |pF + (1\u2212 pF ) s\u2212 1 G\u2212 1 \u2212 s G |\n= |pF (G\u2212 s G\u2212 1) + s\u2212 1 G\u2212 1 \u2212 s G |= G\u2212 s G\u2212 1 |p F \u2212 1 G |.\nThe above equation is maximized when s = 1. Therefore, when r = 1,\n\u2206max = max s\u2208S\u0303 \u2206(s) = |pF \u2212 1 G |.\nCase (ii) r 6= 1: \u2206(s) = |V \u03c0tr1 (s)\u2212 V \u03c0tr0 (s)|\n= |pF + (1\u2212 pF ) r s\u22121 \u2212 1 rG\u22121 \u2212 1 \u2212 rs \u2212 1 rG \u2212 1 | = |pF (r G\u22121 \u2212 rs\u22121 rG\u22121 \u2212 1 ) + rs\u22121 \u2212 1 rG\u22121 \u2212 1 \u2212 rs \u2212 1 rG \u2212 1 | = |pF (r G\u22121 \u2212 rs\u22121 rG\u22121 \u2212 1 ) + rs \u2212 rs\u22121 + rG\u22121 \u2212 rG (1\u2212 rG\u22121)(1\u2212 rG) | = ( rG\u22121 \u2212 rs\u22121 rG\u22121 \u2212 1 )|p F \u2212 1\u2212 r 1\u2212 rG |.\nAgain, the above equation is maximized when s = 1. Therefore, when r 6= 1,\n\u2206max = max s\u2208S\u0303 \u2206(s) = |pF \u2212 1\u2212 r 1\u2212 rG |.\nNext, we bound E[N\u03c0(T )] for the suboptimal policy in a series of lemmas. From (11), it is clear that the boundary is a function of r. Let r = 1\u2212xx . Then, the boundary becomes a function of x by which we have\nB(x) = (1\u2212 1\u2212 x x )/(1\u2212 (1\u2212 x x )G).\nLet \u03b4 be the minimum Euclidean distance of pair (pC , pF ) from the boundary (x,B(x)) given in Fig. 2. The value of \u03b4 specifies the hardness of GRBP. When \u03b4 is small, it is harder to distinguish the optimal policy from the suboptimal policy. If the pair of estimated transition probabilities (p\u0302C\u03c1 , p\u0302 F \u03c1 ) in round \u03c1 lies within a ball around (pC , pF ) with radius less than \u03b4, then GETBE will select the optimal policy in that round. The probability that GETBE selects the optimal policy is lower bounded by the probability that the estimated transition probabilities lie in a ball centered at (pC , pF ) with radius \u03b4.\nThe following lemma provides a lower bound on the expected number of times each action is selected by GETBE. This result will be used when bounding the regret of GETBE.\nLemma 4. (i) Let pF,1 be the probability of taking action F in round \u03c1 when \u03c0\u0302\u03c1 = \u03c0tr1 and pC,1 be the probability of taking action C at least once in round \u03c1 when \u03c0\u0302\u03c1 = \u03c0tr1 . Then,\npC,1 = 1\u2212 q(1)\npF,1 =\n{\u2211G\u22121 s=1\nG\u2212s G\u22121q(s) if r = 1\u2211G\u22121\ns=1 rs\u22121\u2212rG\u22121 1\u2212rG\u22121 q(s) if r 6= 1 .\n(ii) Let D(\u03c1) := \u03b3 log \u03c1 where \u03b3 > 1/p2F,1, and\nfa(\u03c1) := { 0.5pC,1\u03c1, for a = C 0.5(pF,1\u03b3 \u2212\u221a\u03b3) log \u03c1, for a = F\nLet \u03c1\u2032C be the first round in which 0.5pC,1\u03c1\u2212 pC,1dD(\u03c1)e\u2212\u221a \u03c1\u2212 dD(\u03c1)e log \u03c1 becomes positive and \u03c1\u2032F be the first\nround in which both 0.5(pF,1\u03b3 \u2212 \u221a\u03b3) log \u03c1 \u2212 \u221a\nlog \u03c1 and \u03c1 \u2212 2 \u2212 dD(\u03c1)e becomes positive. Then for a \u2208 {F,C} we have\nPr (Na(\u03c1) < fa(\u03c1)) \u2264 1\n\u03c12 , for \u03c1 \u2265 \u03c1\u2032a.\nProof: The following expressions will be used in the proof: \u2022 N0(\u03c1) : Number of rounds by \u03c1 for which \u03c0\u0302\u03c1 = \u03c0tr0 . \u2022 N1(\u03c1) : Number of rounds by \u03c1 for which \u03c0\u0302\u03c1 = \u03c0tr1 . \u2022 NF,1(\u03c1) : Number of rounds by \u03c1 for which action F\nis taken when \u03c0\u0302\u03c1 = \u03c0tr1 . \u2022 NC,1(\u03c1) : Number of rounds by \u03c1 for which action C\nis taken when \u03c0\u0302\u03c1 = \u03c0tr1 .\n\u2022 na(\u03c1) : Indicator function of the event that action a is selected for at least once in round \u03c1.\n(i) When \u03c0\u0302\u03c1 = \u03c0tr1 , action C is not taken only if the initial state is 1. Hence,\npC,1 = 1\u2212 Pr(s\u03c11 = 1) = 1\u2212 q(1).\nLet H1 denote the event that state 1 is reached before state G when \u03c0\u0302\u03c1 = \u03c0tr1 . We have\npF,1 = G\u22121\u2211 s=1 Pr(H1|s\u03c11 = s)q(s).\nWhen r = 1, pF,1 is equivalent to the ruin probability (probability of hitting the terminal state 1) of a fair gambler\u2019s ruin problem over G\u22121 states, where states 1 and G are the terminal states. For this problem, the probability of hitting G from state s is s\u22121G\u22121 . Hence, probability of hitting state 1 from state s is\nPr(H1|s\u03c11 = s) = 1\u2212 s\u2212 1 G\u2212 1 = G\u2212 s G\u2212 1 .\nWhen r 6= 1, the problem is equivalent to an unfair gambler\u2019s ruin problem with G\u22121 states in which probability of hitting G from state s is 1\u2212r s\u22121\n1\u2212rG\u22121 . Then, the probability of hitting state 1 from state s becomes\nPr(H1|s\u03c11 = s) = 1\u2212 1\u2212 rs\u22121 1\u2212 rG\u22121 = rs\u22121 \u2212 rG\u22121 1\u2212 rG\u22121 .\n(ii) Since action C might be selected for more than once in a round, we have Na(\u03c1) \u2265 na(1)+na(2)+ \u00b7 \u00b7 \u00b7+na(\u03c1). This holds because in the initialization of GETBE, each action is selected once. Basically, we derive the lower bounds for Na(\u03c1 + 1), but these lower bounds also hold for Na(\u03c1) because of the way GETBE is initialized. For a set of rounds \u03c1 \u2208 T \u2282 {1, . . . , T}, na(\u03c1)s are in general not identically distributed. But if \u03c0\u0302\u03c1 is same for all rounds \u03c1 \u2208 T , then na(\u03c1)s are identically distributed.\nFirst, assume that N1(\u03c1) = k, 0 \u2264 k \u2264 \u03c1. Then, the probability that action C is selected at least once in each of these k rounds is pC,1. Let ji denote the index of the round in which the estimated optimal policy is \u03c0tr1 for the ith time. The sequence of Bernoulli random variables nC(ji), i = 1, . . . , k are independent and identically distributed. Hence, the Hoeffding bound given in Appendix A can be used to upper-bound the deviation probability of sum of these random variables from the expected sum. Since the estimated optimal policy will be \u03c0tr0 for the remaining \u03c1\u2212k rounds, the number of times action F is selected in all of these rounds will be at most dD(\u03c1)e. Therefore, the probability of taking action C is zero for at most dD(\u03c1)e rounds. Let \u03c1D := \u03c1\u2212dD(\u03c1)e and N \u2032C(\u03c1) denote the sum of \u03c1 random variables that are drawn from an independent identically distributed Bernoulli random process with parameter pC,1. Then,\nNC(\u03c1) \u2265 \u03c1\u2212 k \u2212 dD(\u03c1)e+ k\u2211 i=1 nC(ji)\n\u2265 \u03c1\u2212dD(\u03c1)e\u2211\ni=1\nnC(ji) = N \u2032 C(\u03c1\u2212 dD(\u03c1)e)\n= N \u2032C(\u03c1D). (14)\nAccording to the Hoeffding bound in Appendix A, we have for z > 0\nPr (N \u2032C(\u03c1D)\u2212 E(N \u2032C(\u03c1D)) \u2264 \u2212z) \u2264 e\u22122z 2/\u03c1D .\nWhen z = \u221a \u03c1D log \u03c1 the above bound becomes\nPr(N \u2032C(\u03c1D) \u2264 E(N \u2032C(\u03c1D))\u2212 \u221a \u03c1D log \u03c1) \u2264 1\n\u03c12\n\u21d2Pr(N \u2032C(\u03c1D) \u2264 pC,1(\u03c1\u2212 dD(\u03c1)e)\u2212\u221a (\u03c1\u2212 dD(\u03c1)e) log \u03c1) \u2264 1\n\u03c12 .\nThen, by using (14) we obtain\nPr(NC(\u03c1) \u2264 pC,1(\u03c1\u2212 dD(\u03c1)e)\u2212\u221a (\u03c1\u2212 dD(\u03c1)e) log \u03c1) \u2264 1\n\u03c12 .\nSince \u03c1\u2032C is the first round in which 0.5pC,1\u03c1\u2212pC,1dD(\u03c1)e\u2212\u221a \u03c1D log \u03c1 becomes positive, on or after \u03c1\u2032C , we have\npC,1\u03c1D \u2212 \u221a \u03c1D log \u03c1 > 0.5pC,1\u03c1. Therefore, we replace\npC,1\u03c1D \u2212 \u221a \u03c1D log \u03c1 with 0.5pC,1\u03c1 and then\nPr(NC(\u03c1) \u2264 0.5pC,1\u03c1) \u2264 1\n\u03c12 , for \u03c1 \u2265 \u03c1\u2032C\nwhich is equivalent to\nPr(NC(\u03c1) \u2264 fC(\u03c1)) \u2264 1\n\u03c12 , for \u03c1 \u2265 \u03c1\u2032C . (15)\nAgain, assume that N1(\u03c1) = k. Then, the probability of selecting action F is pF,1 in each of these k rounds. Let R denote the set of the remaining \u03c1\u2212k rounds. For a round \u03c1r \u2208 R, action F is selected only if NF (\u03c1r) \u2264 D(\u03c1r). Among the rounds in R, the number of rounds in which action F is selected is bounded below by min{\u03c1\u2212k, dD(\u03c1\u2212k)e}. Then, nF (ji), i = 1, 2, . . . is a sequence of i.i.d. Bernoulli random variables with parameter pF,1. From the argument above, we obtain\nNF (\u03c1) \u2265 min{\u03c1\u2212 k, dD(\u03c1\u2212 k)e}+ k\u2211 i=1 nF (ji)\n\u2265 k+min{\u03c1\u2212k,dD(\u03c1\u2212k)e}\u2211\ni=1\nnF (ji)\nWhen min{\u03c1\u2212 k, dD(\u03c1\u2212 k)e} = \u03c1\u2212 k, we have\nNF (\u03c1) \u2265 \u03c1\u2211 i=1 nF (ji) \u2265 dD(\u03c1)e\u2211 i=1 nF (ji), for \u03c1 \u2265 \u03c1\u2032F .\nWhen min{\u03c1\u2212 k, dD(\u03c1\u2212 k)e} = dD(\u03c1\u2212 k)e, we have\nNF (\u03c1) \u2265 k+dD(\u03c1\u2212k)e\u2211\ni=1\nnF (ji).\nNext, we will show that D(\u03c1\u2212 k) + k \u2265 D(\u03c1) when \u03c1 is sufficiently large. First, min{\u03c1\u2212k, dD(\u03c1\u2212k)e} = dD(\u03c1\u2212k)e implies that\n\u03c1 \u2265 dD(\u03c1\u2212 k)e+ k \u2265 D(\u03c1\u2212 k) + k. (16) Also, D(\u03c1\u2212 k) + k \u2265 D(\u03c1) should imply that\nD(\u03c1)\u2212D(\u03c1\u2212 k) \u2264 k \u21d2 \u03b3 log(\u03c1/(\u03c1\u2212 k)) \u2264 k \u21d2\n\u03c1/(\u03c1\u2212 k) \u2264 ek/\u03b3 \u21d2 \u03c1 \u2265 ke k/\u03b3\nek/\u03b3 \u2212 1 (17)\nUsing the results in (16) and (17), we conclude that D(\u03c1\u2212 k) + k \u2265 D(\u03c1) holds when\nD(\u03c1\u2212 k) + k \u2265 ke k/\u03b3\nek/\u03b3 \u2212 1 . (18)\nBy setting D(\u03c1) = \u03b3 log \u03c1 and manipulating (18) we get\n\u03b3 log(\u03c1\u2212 k) + k \u2265 ke k/\u03b3\nek/\u03b3 \u2212 1 \u21d2\n\u03b3 log(\u03c1\u2212 k) \u2265 k( e k/\u03b3\nek/\u03b3 \u2212 1 \u2212 1)\u21d2\nlog(\u03c1\u2212 k) \u2265 k/\u03b3 ek/\u03b3 \u2212 1 \u21d2 \u03c1\u2212 k \u2265 e k/\u03b3 ek/\u03b3\u22121 \u21d2\n\u03c1 \u2265 k + e k/\u03b3 ek/\u03b3\u22121 . (19)\nFirst, we evaluate the term h(k) := e k/\u03b3\nek/\u03b3\u22121 . We will show that h(k) \u2208 [1, e] for all k \u2208 R+. By applying L\u2019Hopital\u2019s rule we get\nlim k\u21920\nk/\u03b3 ek/\u03b3 \u2212 1 = limk\u21920 1/\u03b3 (1/\u03b3)ek/\u03b3 = 1\nand\nlim k\u2192\u221e\nk/\u03b3\nek/\u03b3 \u2212 1 = 0\nThese two conditions and using the fact that exponential function is continuous we conclude that\nlim k\u21920\ne k/\u03b3 ek/\u03b3\u22121 = e limk\u21920 k/\u03b3 ek/\u03b3\u22121 = e\nand\nlim k\u2192\u221e\ne k/\u03b3 ek/\u03b3\u22121 = e limk\u2192\u221e k/\u03b3 ek/\u03b3\u22121 = 1.\nNext, we will show that e k/\u03b3\nek/\u03b3\u22121 is decreasing in k. Since this is a monotonically increasing function of k/\u03b3\nek/\u03b3\u22121 , it is sufficient to show that k/\u03b3\nek/\u03b3\u22121 is decreasing in k. We have\nd\ndk\nk/\u03b3 ek/\u03b3 \u2212 1 = 1 \u03b3 (e k/\u03b3 \u2212 1)\u2212 k\u03b3 (ek/\u03b3/\u03b3) (ek/\u03b3 \u2212 1)2\nThe denominator is always positive for k > 0. Therefore, we only consider the numerator and write it as\n1 \u03b3 (ek/\u03b3 \u2212 1)\u2212 k \u03b3 (ek/\u03b3/\u03b3) = (\u03b3 \u2212 k)ek/\u03b3 \u2212 \u03b3 \u03b32 .\nAs the denominator is positive, we only need to show that (\u03b3\u2212k)ek/\u03b3\u2212\u03b3 is always negative. The derivative of the above expression is \u2212(k/\u03b3)ek/\u03b3 , which is negative for k > 0. We also have (\u03b3\u2212k)ek/\u03b3\u2212\u03b3 = 0 at k = 0. These two conditions imply that (\u03b3 \u2212 k)ek/\u03b3 \u2212 \u03b3 is always negative for k > 0, by which we conclude that e k/\u03b3\nek/\u03b3\u22121 is decreasing in k. Hence, we have\n1 \u2264 e k/\u03b3 ek/\u03b3\u22121 \u2264 e\nThis implies that k + e k/\u03b3\nek/\u03b3\u22121 \u2264 k + e. Hence (19) holds when \u03c1 \u2265 k+ e. This implies that when k \u2264 \u03c1\u2212 e, we have\nk+dD(\u03c1\u2212k)e\u2211 i=1 nF (ji) \u2265 dD(\u03c1)e\u2211 i=1 nF (ji).\nThe only cases that are left out are k = \u03c1, k = \u03c1 \u2212 1 and k = \u03c1 \u2212 2. But we know from the definition of \u03c1\u2032F that for \u03c1 \u2265 \u03c1\u2032F , \u03c1 \u2212 2 \u2212 dD(\u03c1)e is positive. Hence for these cases we also have\nk+dD(\u03c1\u2212k)e\u2211 i=1 nF (ji) \u2265 \u03c1\u22122\u2211 i=1 nF (ji) \u2265 dD(\u03c1)e\u2211 i=1 nF (ji).\nLet N \u2032F (\u03c1) denote the sum over nF (ji) for \u03c1 rounds. From all of the cases we derived above, we obtain\nNF (\u03c1) \u2265 dD(\u03c1)e\u2211 i=1 nF (ji) = N \u2032 F (dD(\u03c1)e) for \u03c1 \u2265 \u03c1\u2032F (20)\nNow, by using Hoeffding bound we have\nPr (N \u2032F (dD(\u03c1)e)\u2212 E(N \u2032F (dD(\u03c1)e)) \u2264 \u2212z) \u2264 e\u22122z 2/dD(\u03c1)e\nand if z = \u221a dD(\u03c1)e log \u03c1 then,\nPr(N \u2032F (dD(\u03c1)e) < E(N \u2032F (dD(\u03c1)e))\u2212 \u221a dD(\u03c1)e log \u03c1) \u2264 1\n\u03c12 Pr(N \u2032F (dD(\u03c1)e) < pF,1dD(\u03c1)e \u2212 \u221a dD(\u03c1)e log \u03c1) \u2264 1\n\u03c12 .\nBy using (20), we get Pr(NF (\u03c1) < pF,1dD(\u03c1)e \u2212 \u221a dD(\u03c1)e log \u03c1) \u2264 1\n\u03c12 . (21)\nThen, by using D(\u03c1) = \u03b3 log \u03c1, \u03b3 > 1/p2F,1, we have pF,1dD(\u03c1)e \u2212 \u221a dD(\u03c1)e log \u03c1\n= pF,1d\u03b3 log \u03c1e \u2212 \u221a d\u03b3 log \u03c1e log \u03c1\n\u2265 pF,1\u03b3 log \u03c1\u2212 \u221a (\u03b3 log \u03c1+ 1) log \u03c1\n= pF,1\u03b3 log \u03c1\u2212 \u221a \u03b3 log2 \u03c1+ log \u03c1\n\u2265 pF,1\u03b3 log \u03c1\u2212 \u221a \u03b3 log2 \u03c1\u2212 \u221a log \u03c1 (22) = (pF,1\u03b3 \u2212 \u221a \u03b3) log \u03c1\u2212 \u221a log \u03c1 (23)\nwhere (22) occurs due to the subadditivity3 of the square root. Next, we will show that (23) becomes positive when \u03c1\n3For a, b > 0 we have \u221a a+ \u221a b > \u221a a+ b since ( \u221a a+ \u221a b)2 > a+ b.\nis large enough. To do this, we first show that the first term in (23) is always positive. This is proven by observing that\n\u03b3 > 1/p2F,1 \u21d2 pF,1 \u221a \u03b3 \u2212 1 > 0\u21d2 pF,1\u03b3 \u2212 \u221a \u03b3 > 0. (24)\nSince log \u03c1 increases at a higher rate than \u221a\nlog \u03c1, it can be shown that 0.5(pF,1\u03b3 \u2212 \u221a\u03b3) log \u03c1 \u2212 \u221a log \u03c1 will always increase after some round. Since lim\u03c1\u2192\u221e 0.5(pF,1\u03b3 \u2212\u221a \u03b3) log \u03c1\u2212\u221alog \u03c1 =\u221e, this term is expected to be positive after some round. From the statement of the lemma, it is known that \u03c1\u2032F is greater than or equal to this round. Therefore, for \u03c1 \u2265 \u03c1\u2032F , (pF,1\u03b3 \u2212 \u221a \u03b3) log \u03c1 \u2212 \u221alog \u03c1 >\n0.5(pF,1\u03b3 \u2212\u221a\u03b3) log \u03c1. Using this and (23), we obtain\npF,1dD(\u03c1)e \u2212 \u221a dD(\u03c1)e log \u03c1\n\u2265 (pF,1\u03b3 \u2212 \u221a \u03b3) log \u03c1\u2212 \u221a log \u03c1 \u2265 0.5(pF,1\u03b3 \u2212 \u221a \u03b3) log \u03c1.\nThen, we use this result and (21) to get\nPr(NF (\u03c1) \u2264 0.5(pF,1\u03b3 \u2212 \u221a \u03b3) log \u03c1) \u2264 1\n\u03c12 , for \u03c1 \u2265 \u03c1\u2032F\nwhich is equivalent to\nPr(NF (\u03c1) \u2264 fF (\u03c1)) \u2264 1\n\u03c12 , for \u03c1 \u2265 \u03c1\u2032F . (25)\nThe (expected) regret given in (13) can be decomposed into two parts: (i) regret in rounds in which the suboptimal policy is selected, (ii) regret in rounds in which the optimal policy is selected and GETBE explores. Let IR(T ) denote the number of rounds by round T in which the suboptimal policy is selected. The first part of the regret is upper bounded by E(IR(T )), since the reward in a round can be either 0 or 1. Similarly, the second part of the regret is upper bounded by the number of explorations when the optimal policy is \u03c0tr0 . When the optimal policy is \u03c0 tr 1 , exploration will only be performed when the suboptimal policy is selected. Hence, there is no additional regret due to explorations, since all the regret is accounted for in the first part of the regret.\nLet A\u03c1 denote the event that the suboptimal policy is selected in round \u03c1. Let\nC\u03c1 := {|pC \u2212 p\u0302C\u03c1 |\u2265 \u03b4/ \u221a 2} \u222a {|pF \u2212 p\u0302F\u03c1 |\u2265 \u03b4/ \u221a 2}.\nIt can be shown that on event Cc\u03c1 the Euclidian distance between (pC , pF ) and (p\u0302C\u03c1 , p\u0302 F \u03c1 ) is less than \u03b4. This implies that on event Cc\u03c1, the optimal policy is selected. Therefore, C\u03c1 contains the event that the optimal policy is not selected. Using the linearity of expectation and the union bound, we obtain\nE[IR(T )] = E[ T\u2211 \u03c1=1 I(A\u03c1)]\n\u2264 T\u2211 \u03c1=1 \u2211 a\u2208{F,C} Pr ( |pa \u2212 p\u0302a\u03c1|\u2265 \u03b4/ \u221a 2 ) . (26)\nLet Iexp\u03c1 be the indicator function of the event that GETBE explores. By the above discussion we have\nE[Reg(T )|\u03c0\u2217 = \u03c0tr1 ] \u2264 E[IR(T )] (27) E[Reg(T )|\u03c0\u2217 = \u03c0tr0 ] \u2264 \u2206maxE[IR(T )]\n+ E[ T\u2211 \u03c1=1 Iexp\u03c1 ]. (28)\nNext, we bound the expected regret of GETBE for the GRBP using (27) and (28).\nTheorem 3. Let x1 := ( 1 + \u221a (24pF,1/\u03b42) + 1 ) /2pF,1. Assume that the control function is\nD(\u03c1) = \u03b3 log \u03c1 where \u03b3 > max{(x1)2, 1\n(pF,1)2 }.\nLet \u03c1\u2032\u2032 be the first round in which \u03b42 \u2265 3 log \u03c1fa(\u03c1) for both actions, \u03c1\u2032 := max{\u03c1\u2032C , \u03c1\u2032F , \u03c1\u2032\u2032}, and\nw := 4\u03c1\u2032 + 2\u03c02\n3 (1 +\n1\n1\u2212 e\u2212\u03b42 )\nThen, the regret of GETBE is bounded by\nE[Reg(T )|\u03c0\u2217 = \u03c0tr1 ] \u2264 w\nand E[Reg(T )|\u03c0\u2217 = \u03c0tr0 ] \u2264 dD(T )e+ w\u2206max.\nProof: First, we bound E[IR(T )]. For this, we replace the order of summations in (26) and we have\nE[IR(T )] \u2264 \u2211\na\u2208{F,C} T\u2211 \u03c1=1 Pr ( |pa \u2212 p\u0302a\u03c1|\u2265 \u03b4/ \u221a 2 ) . (29)\nLet N\u2217F (\u03c1) := N G F (\u03c1) and N \u2217 C(\u03c1) := N u C(\u03c1). By using the law of total probability and Hoeffding inequality, we obtain for a \u2208 {F,C} T\u2211 \u03c1=1 Pr ( |pa \u2212 p\u0302a\u03c1|\u2265 \u03b4\u221a 2 )\n= T\u2211 \u03c1=1 \u221e\u2211 n=1 Pr ( |pa \u2212 N \u2217 a (\u03c1) Na(\u03c1) |\u2265 \u03b4\u221a 2 |Na(\u03c1) = n ) Pr (Na(\u03c1) = n)\n= T\u2211 \u03c1=1 \u221e\u2211 n=1 Pr ( |npa \u2212N\u2217a (\u03c1)|\u2265 n \u03b4\u221a 2 ) Pr (Na(\u03c1) = n)\n\u2264 T\u2211 \u03c1=1 \u221e\u2211 n=1 2e\u22122 (n\u03b4/ \u221a 2)2 n Pr (Na(\u03c1) = n)\n= T\u2211 \u03c1=1 \u221e\u2211 n=1 2e\u2212n\u03b4 2 Pr (Na(\u03c1) = n). (30)\nFor each action, we use the result of Lemma 4 and divide the summation in (30) into two summations. Note that the\nbounds on Na(\u03c1) given in Lemma 4 hold when \u03c1 \u2265 \u03c1\u2032 \u2265 max{\u03c1\u2032C , \u03c1\u2032F }. Therefore, we have\nT\u2211 \u03c1=1 \u221e\u2211 n=1 2e\u2212n\u03b4 2 Pr (Na(\u03c1) = n) = \u03c1\u2032\u22121\u2211 \u03c1=1 \u221e\u2211 n=1 2e\u2212n\u03b4 2 Pr (Na(\u03c1) = n)+\nT\u2211 \u03c1=\u03c1\u2032 \u221e\u2211 n=1 2e\u2212n\u03b4 2 Pr (Na(\u03c1) = n)\n= K \u2032 + T\u2211 \u03c1=\u03c1\u2032 \u221e\u2211 n=1 2e\u2212n\u03b4 2 Pr (Na(\u03c1) = n) (31)\nwhere K \u2032 = \u03c1\u2032\u22121\u2211 \u03c1=1 \u221e\u2211 n=1 2e\u2212n\u03b4 2 Pr (Na(\u03c1) = n) which is finite since \u03c1\u2032 is finite. Since \u221e\u2211 n=1 Pr (Na(\u03c1) = n) = 1\nand as e\u2212n\u03b4 2 \u2264 1, then we have \u221e\u2211 n=1 e\u2212n\u03b4 2 Pr (Na(\u03c1) = n) \u2264 1.\nTherefore, an upper bound on K \u2032 can be given as\nK \u2032 = \u03c1\u2032\u22121\u2211 \u03c1=1 \u221e\u2211 n=1 2e\u2212n\u03b4 2 Pr (Na(\u03c1) = n) \u2264 \u03c1\u2032\u22121\u2211 \u03c1=1 2 < 2\u03c1\u2032.\n(32)\nWe have T\u2211\n\u03c1=\u03c1\u2032 \u221e\u2211 n=1 2e\u2212n\u03b4 2 Pr (Na(\u03c1) = n) = T\u2211 \u03c1=\u03c1\u2032 fa(\u03c1)\u2211 n=1 2e\u2212n\u03b4 2 Pr (Na(\u03c1) = n)+\nT\u2211 \u03c1=\u03c1\u2032 \u221e\u2211 n=fa(\u03c1)+1 2e\u2212n\u03b4 2 Pr (Na(\u03c1) = n). (33)\nFor the first summation in (33), we use (15) and (25) for each action as an upper bound since it is the case when n \u2264 fa(\u03c1). Therefore,\nT\u2211 \u03c1=\u03c1\u2032 fa(\u03c1)\u2211 n=1 2e\u2212n\u03b4 2 Pr (Na(\u03c1) = n) \u2264 T\u2211 \u03c1=1 fa(\u03c1)\u2211 n=1 2e\u2212n\u03b4 2 \u03c12\n\u2264 \u221e\u2211 \u03c1=1 \u221e\u2211 n=1 2e\u2212n\u03b4 2 \u03c12 =\n\u03c02\n3(1\u2212 e\u2212\u03b42) (34)\nFor the second summation in (33), we first show that \u03b42 \u2265 3 log \u03c1 fa(\u03c1)\nfor each action when \u03c1 \u2265 \u03c1\u2032. For a = F , we have \u03b42 \u2265 3 log \u03c1fF (\u03c1) = 6 pF,1\u03b3\u2212 \u221a \u03b3 since \u03b3 \u2265\n(x1) 2. The proof is as follows. Note that the term pF,1\u03b3\u2212\u221a\u03b3\nis positive because of (24). In order to have \u03b42 \u2265 6pF,1\u03b3\u2212\u221a\u03b3 , we must have pF,1\u03b3\u2212\u221a\u03b3\u22126/\u03b42 \u2265 0. This can be re-written as a second order polynomial function, which is given by\ng(x) = ax2 + bx+ c \u2265 0\nwhere a = pF,1, b = \u22121,c = \u22126/\u03b42 and x = \u221a\u03b3. Since \u03b3 is positive, we will find positive values of x for which g(x) is non-negative. Also, g(x) is a convex function since its second derivative is 2a, which is positive. Hence, g(x) is non-negative for positive x\u2019s which are greater than the largest root. The roots of g(x) are given as\nx1 = 1 +\n\u221a 1 +\n24pF,1 \u03b42\n2pF,1 , x2 =\n1\u2212 \u221a\n1 + 24pF,1 \u03b42\n2pF,1 .\nIt is clear that only x1 is positive. Thus, g(x) is non-negative for x = \u221a \u03b3 \u2265 x1. Therefore, \u03b3 has to be greater than (x1)2 so that \u03b42 \u2265 6pF,1\u03b3\u2212\u221a\u03b3 . For a = C we have 3 log \u03c1fC(\u03c1) = 3 log \u03c1 0.5pC,1\u03c1\n. This quantity decreases as \u03c1 increases and converges to zero in the limit \u03c1 goes to infinity. Hence, this quantity becomes smaller than \u03b42 after some round. Hence, for \u03c1 \u2265 \u03c1\u2032, we have \u03b42 \u2265 3 log \u03c1fa(\u03c1) for both actions. Thus,\nT\u2211 \u03c1=\u03c1\u2032 \u221e\u2211 n=fa(\u03c1)+1 2e\u2212n\u03b4 2 Pr (Na(\u03c1) = n)\n\u2264 T\u2211\n\u03c1=\u03c1\u2032\n2e\u2212fa(\u03c1)\u03b4 2 \u221e\u2211 n=fa(\u03c1)+1 Pr (Na(\u03c1) = n)\n\u2264 T\u2211\n\u03c1=\u03c1\u2032\n2e\u2212fa(\u03c1)\u03b4 2 \u2264 T\u2211 \u03c1=\u03c1\u2032 2e\u22123 log \u03c1\n\u2264 \u221e\u2211 \u03c1=1 2 \u03c13 \u2264 \u03c0 2 3 . (35)\nFinally, we combine the results of (35), (34) and (32) together with the result of (31) and sum the final result over the two actions to get a bound for the expression in (29). This results in\nE[IR(T )] \u2264 2(2\u03c1\u2032 + \u03c0 2 3(1\u2212 e\u2212\u03b42) + \u03c02 3 )\n= 4\u03c1\u2032 + 2\u03c02\n3 (1 +\n1\n1\u2212 e\u2212\u03b42 ) = w. (36)\nAssume the optimal policy is \u03c0tr1 . Then, the expected number of rounds in which the suboptimal policy is selected is finite and bounded by w (independent of T ) in (36). In this case, the exploration is done only when the suboptimal policy is selected and there will be no extra regret term due to exploration. Therefore,\nE[Reg(T )|\u03c0\u2217 = \u03c0tr1 ] \u2264 4\u03c1\u2032 + 2\u03c02\n3 (1 +\n1\n1\u2212 e\u2212\u03b42 ) = w.\nAssume the optimal policy is \u03c0tr0 . Similar to the previous case, the expected number of rounds in which the suboptimal policy is selected is at most w. Since the suboptimal policy for this case is \u03c0tr1 , it will always be played if it is selected (no exploration). Hence, the regret in these rounds is at most \u2206max. However, the learner will explore action F when the optimal policy is selected. This results in additional regret. Since, the number of explorations of GETBE by round T is bounded by dD(T )e, the regret that will result from explorations is also bounded by dD(T )e. Therefore,\nE[Reg(T )|\u03c0\u2217 = \u03c0tr0 ] \u2264 dD(T )e+ w\u2206max.\nTheorem 3 bounds the expected regret of GETBE. When \u03c0\u2217 = \u03c0tr1 , Reg(T ) = O(1) since both actions will be selected with positive probability by the optimal policy at each round. When \u03c0\u2217 = \u03c0tr0 , Reg(T ) = O(D(T )) since GETBE forces to explore action F logarithmically many times to avoid getting stuck in the suboptimal policy."}, {"heading": "VI. NUMERICAL RESULTS", "text": "We create a synthetic medical treatment selection problem based on [21]. Each state is assumed to be a stage of gastric cancer (G = 4, D = 0). The goal state is defined as at least three years of survival. Action C is assumed to be chemotherapy and action F is assumed to be surgery. For action C, pC is determined by using the average survival rates for young and old groups at different stages of cancer given in [21]. For each stage, the survival rate at three years is taken to be the probability of hitting G by taking action C continuously. With this information, we set pC = 0.45. Also, the five-year survival rate of surgery given in [22] (29%) is used to set pF = 0.3.\nThe regrets shown in Fig. 3 and 4 correspond to different variants of GETBE, named as GETBE-SM, GETBE-PS and GETBE-UCB. Each variant updates the state transition probabilities in a different way. GETBE-SM uses the control function together with sample mean estimates of the state transition probabilities. Unlike GETBE-SM, GETBE-UCB and GETBE-PS do not use the control function. GETBEPS uses posterior sampling from the Beta distribution [17] to sample and update pF and pC . GETBE-UCB adds an inflation term that is equal to \u221a 2 log(NF (\u03c1)+NC(\u03c1))\nNa(\u03c1) to the\nsample mean estimates of the state transition probabilities that correspond to action a. PS-PolSelection and UCBPolSelection algorithms treat each policy as a super-arm, and use PS and UCB methods to select the best policy among the two threshold policies. Instead of updating the state transition probabilities, they directly update the rewards of the policies.\nInitial state distribution is taken to be the uniform distribution. Initial estimates of the transition probabilities are formed by setting NF (1) = 1, NGF (1) \u223c Unif[0, 1], NC(1) = 1, NuC(1) \u223c Unif[0, 1]. The time horizon is taken to be 5000 rounds, and the control function is set to be D(\u03c1) = 15 log \u03c1. Reported results are averaged over 200 iterations.\nIn Fig. 3 the regrets of GETBE and other algorithms are shown for pF and pC values given above. For this case, the the optimal policy is \u03c0tr1 and all variants of GETBE achieve finite regret, as expected. However, the regrets of UCBPolSelection and PS-PolSelection increase logarithmically, since they sample each policy logarithmically many times.\nNext, we set pC = 0.65 and pF = 0.3, in order to show how the algorithms perform when the optimal policy is \u03c0tr0 .\nThe result for this case is given in Fig. 4. As expected, the regret grows logarithmically over the rounds for all variants of GETBE, PS-PolSelection and UCB-PolSelection. GETBEPS achieves the lowest regret for this case.\nFig. 5 illustrates the regret of GETBE-SM as a function of pF and pC for T = 1000. As the state transition probabilities shift from the no-exploration region to the exploration region the regret increases as expected."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we introduced the Gambler\u2019s Ruin Bandit Problem. We characterized the form of the optimal policy for this problem, and then developed a learning algorithm called GETBE that operates on the GRBP to learn the optimal policy when the transition probabilities are unknown. We proved that the regret of this algorithm is either bounded (finite) or logarithmic in the number of rounds based on the region that the true transition probabilities lie in. In addition to the regret bounds, we illustrated the performance of our algorithm via numerical experiments."}], "references": [{"title": "Multi-armed bandit models for the optimal design of clinical trials: Benefits and challenges", "author": ["S.S. .Villar", "J. Bowden", "J. Wason"], "venue": "Statistical Science, vol. 30, no. 2, pp. 199\u2013215, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "RELEAF: An algorithm for learning and exploiting relevance", "author": ["C. Tekin", "M. van der Schaar"], "venue": "IEEE J. Sel. Topics Signal Process., vol. 9, no. 4, pp. 716\u2013727, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Robbins", "Herbert"], "venue": "Advances in Applied Mathematics, vol. 6, no. 1, pp. 4\u201322, 1985.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1985}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "Cesa-bianchi", "N. o", "P. Fischer"], "venue": "Machine Learning, vol. 47, pp. 235\u2013256, 2002.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "The KL-UCB algorithm for bounded stochastic bandits and beyond", "author": ["Garivier", "Aurelien", "Cappe", "Olivier"], "venue": "COLT, 2011, pp. 359\u2013376.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem", "author": ["P. Auer", "R. Ortner"], "venue": "Periodica Mathematica Hungarica, vol. 61, no. 1-2, pp. 55\u201365, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A theory of goal-oriented mdps with dead ends", "author": ["A. Kolobov", "Mausam", "D. Weld"], "venue": "UAI, 2012, pp. 438\u2013447.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Regret analysis of stochastic and non-stochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning, vol. 5, no. 1, pp. 1\u2013122, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "On the classical ruin problems", "author": ["L. Takacs"], "venue": "J. Amer. Statisistical Association, vol. 64, pp. 889\u2013906, 1969.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1969}, {"title": "Gambler\u2019s ruin with catastrophe and windfalls", "author": ["B. Hunter", "A.C. Krinik", "C. Nguyen", "J.M. Switkes", "H.F. von Bremen"], "venue": "Statistical Theory and Practice, vol. 2, no. 2, pp. 199\u2013219, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Maximum and minimum of modified gamblers ruin problem, arxiv:1301.2702", "author": ["T. van Uem"], "venue": "2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic programming and optimal control", "author": ["D. Bertsekas"], "venue": "Athena Scientific.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 0}, {"title": "Stochastic safest and shortest path problems", "author": ["F. Teichteil-Konigsbuch"], "venue": "AAAI, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimistic linear programming gives logarithmic regret for irreducible MDPs", "author": ["A. Tewari", "P. Bartlett"], "venue": "Advances in Neural Information Processing Systems, vol. 20, pp. 1505\u20131512, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["P. Auer", "T. Jaksch", "R. Ortner"], "venue": "Advances in Neural Information Processing Systems, 2009, pp. 89\u201396.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "A dynamic allocation index for the sequential design of experiments", "author": ["J.C. Gittins", "Jones", "D.M."], "venue": "Progress in Statistics Gani, J. (ed.), pp. 241\u2013266, 1974.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1974}, {"title": "Analysis of Thompson sampling for the multi-armed bandit problem", "author": ["S. Agrawal", "N. Goyal"], "venue": "The Journal of Machine Learning Research, vol. 23, no. 39, pp. 285\u2013294, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Combinatorial bandits", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Journal of Computer and System Sciences, vol. 78, no. 5, pp. 1404\u20131422, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamic programming and modern control theory", "author": ["R. Bellman", "R.E. Kalaba"], "venue": "Citeseer,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1965}, {"title": "On the gambler\u2019s ruin problem for a finite markov chain", "author": ["M.A. El-Shehawey"], "venue": "Statistics and Probability Letters, vol. 79, pp. 1590\u2013 1595, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Characteristics and prognosis of gastric cancer in young patients", "author": ["T. Isobe", "K. Hashimoto", "J. Kizaki", "M. Miyagi", "K. Aoyagi", "K. Koufuji", "K. Shirouzu"], "venue": "International Journal of Oncology, vol. 30, no. 1, pp. 43\u201349, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Multi-armed bandits (MAB) are used to model a plethora of applications that require sequential decision making under uncertainty ranging from clinical trials [1] to web advertising [2].", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "INTRODUCTION Multi-armed bandits (MAB) are used to model a plethora of applications that require sequential decision making under uncertainty ranging from clinical trials [1] to web advertising [2].", "startOffset": 194, "endOffset": 197}, {"referenceID": 2, "context": "In the conventional MAB [3], [4] the learner chooses an action from a finite set of actions at each round, and receives a random reward.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "In the conventional MAB [3], [4] the learner chooses an action from a finite set of actions at each round, and receives a random reward.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "Numerous orderoptimal index-based learning rules have been developed for the conventional MAB [4]\u2013[6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "Numerous orderoptimal index-based learning rules have been developed for the conventional MAB [4]\u2013[6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "In the literature, this kind of feedback information is called bandit feedback [8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "Due to its resemblance to the Gambler\u2019s Ruin Problem [9]\u2013[11], we call this new MAB problem the Gambler\u2019s Ruin Bandit Problem (GRBP).", "startOffset": 53, "endOffset": 56}, {"referenceID": 10, "context": "Due to its resemblance to the Gambler\u2019s Ruin Problem [9]\u2013[11], we call this new MAB problem the Gambler\u2019s Ruin Bandit Problem (GRBP).", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "\u2022 We show that using conventional MAB algorithms such as UCB1 [4] in GRBP by enumerating all deterministic Markov policies is very inefficient and results in high regret.", "startOffset": 62, "endOffset": 65}, {"referenceID": 2, "context": "Unlike conventional MAB where the regret growth is at least logarithmic in the number of rounds [3], in GRBP regret can be either logarithmic or bounded, based on the values of the state transition probabilities.", "startOffset": 96, "endOffset": 99}, {"referenceID": 9, "context": "[10] of the Gambler\u2019s Ruin Problem, in addition to the standard outcome of moving one state to the left or right, two extra outcomes are also considered.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "In another model [11], modifications such as the chance of absorption in states other than G and D and staying in the same state are considered.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "MDPs GRBP is closely related to goal oriented MDPs and stochastic shortest path problems [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "Recently, progress has been made in obtaining solutions for MDPs that have dead-end (D) states in addition to goal (G) states [7], [13].", "startOffset": 126, "endOffset": 129}, {"referenceID": 12, "context": "Recently, progress has been made in obtaining solutions for MDPs that have dead-end (D) states in addition to goal (G) states [7], [13].", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "Reinforcement learning in MDPs is considered by numerous researchers [14], [15].", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "Reinforcement learning in MDPs is considered by numerous researchers [14], [15].", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]\u2013 [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).", "startOffset": 164, "endOffset": 168}, {"referenceID": 3, "context": "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]\u2013 [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).", "startOffset": 241, "endOffset": 244}, {"referenceID": 5, "context": "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]\u2013 [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).", "startOffset": 246, "endOffset": 249}, {"referenceID": 3, "context": "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]\u2013 [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).", "startOffset": 288, "endOffset": 291}, {"referenceID": 16, "context": "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]\u2013 [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).", "startOffset": 314, "endOffset": 318}, {"referenceID": 7, "context": "Multi-armed Bandits Over the last decade many variations of the MAB problem is studied and many different learning algorithms are proposed, including Gittins index [16], upper confidence bound policies (UCB-1, UCB-2, Normalized UCB, KL-UCB) [4]\u2013 [6], greedy policies ( -greedy algorithm) [4] and Thompson sampling [17] (see [8] for a comprehensive analysis of the MAB problem).", "startOffset": 324, "endOffset": 327}, {"referenceID": 2, "context": "For the stochastic MAB problem [3], the regret is defined as the difference between the total (expected) reward of the learning algorithm and an oracle which acts optimally based on complete knowledge of the problem parameters.", "startOffset": 31, "endOffset": 34}, {"referenceID": 17, "context": "In addition, GRBP model does not fit into the combinatorial models proposed in prior works [18].", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "If the state transition probabilities are known, each round can be modeled as a MDP and an optimal policy can be found by dynamic programming [12], [19].", "startOffset": 142, "endOffset": 146}, {"referenceID": 18, "context": "If the state transition probabilities are known, each round can be modeled as a MDP and an optimal policy can be found by dynamic programming [12], [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 3, "context": "A straightforward way to do this will be to employ UCB1 algorithm [4] or its variants [6] by taking each policy as an arm.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "A straightforward way to do this will be to employ UCB1 algorithm [4] or its variants [6] by taking each policy as an arm.", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "When UCB1 in [4] is used to select the policy to follow at the beginning of each round (with set of arms \u03a0), we have", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "Proof: See [4].", "startOffset": 11, "endOffset": 14}, {"referenceID": 19, "context": "Thus, the probability of hitting G from state s is (1\u2212 r)/(1\u2212 r) (10) for r 6= 1 and s/G for r = 1 [20].", "startOffset": 99, "endOffset": 103}, {"referenceID": 2, "context": "Unlike conventional MAB algorithms [3], [4], [6] which require all arms to be sampled at least logarithmically many times, GETBE does not need to sample all policies (arms) logarithmically many times to find the optimal policy with a sufficiently high probability.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "Unlike conventional MAB algorithms [3], [4], [6] which require all arms to be sampled at least logarithmically many times, GETBE does not need to sample all policies (arms) logarithmically many times to find the optimal policy with a sufficiently high probability.", "startOffset": 40, "endOffset": 43}, {"referenceID": 5, "context": "Unlike conventional MAB algorithms [3], [4], [6] which require all arms to be sampled at least logarithmically many times, GETBE does not need to sample all policies (arms) logarithmically many times to find the optimal policy with a sufficiently high probability.", "startOffset": 45, "endOffset": 48}, {"referenceID": 20, "context": "NUMERICAL RESULTS We create a synthetic medical treatment selection problem based on [21].", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "For action C, p is determined by using the average survival rates for young and old groups at different stages of cancer given in [21].", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "GETBEPS uses posterior sampling from the Beta distribution [17] to sample and update p and p .", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "Initial estimates of the transition probabilities are formed by setting NF (1) = 1, N F (1) \u223c Unif[0, 1], NC(1) = 1, N C(1) \u223c Unif[0, 1].", "startOffset": 98, "endOffset": 104}, {"referenceID": 0, "context": "Initial estimates of the transition probabilities are formed by setting NF (1) = 1, N F (1) \u223c Unif[0, 1], NC(1) = 1, N C(1) \u223c Unif[0, 1].", "startOffset": 130, "endOffset": 136}], "year": 2016, "abstractText": "In this paper, we propose a new multi-armed bandit problem called the Gambler\u2019s Ruin Bandit Problem (GRBP). In the GRBP, the learner proceeds in a sequence of rounds, where each round is a Markov Decision Process (MDP) with two actions (arms): a continuation action that moves the learner randomly over the state space around the current state; and a terminal action that moves the learner directly into one of the two terminal states (goal and dead-end state). The current round ends when a terminal state is reached, and the learner incurs a positive reward only when the goal state is reached. The objective of the learner is to maximize its long-term reward (expected number of times the goal state is reached), without having any prior knowledge on the state transition probabilities. We first prove a result on the form of the optimal policy for the GRBP. Then, we define the regret of the learner with respect to an omnipotent oracle, which acts optimally in each round, and prove that it increases logarithmically over rounds. We also identify a condition under which the learner\u2019s regret is bounded. A potential application of the GRBP is optimal medical treatment assignment, in which the continuation action corresponds to a conservative treatment and the terminal action corresponds to a risky treatment such as surgery. I. INTRODUCTION Multi-armed bandits (MAB) are used to model a plethora of applications that require sequential decision making under uncertainty ranging from clinical trials [1] to web advertising [2]. In the conventional MAB [3], [4] the learner chooses an action from a finite set of actions at each round, and receives a random reward. The goal of the learner is to maximize its long-term expected reward by choosing actions that yield high rewards. This is a non-trivial task, since the reward distributions are not known beforehand. Numerous orderoptimal index-based learning rules have been developed for the conventional MAB [4]\u2013[6]. These rules act myopically by choosing the action with the maximum index in each round. Situations that require multiple actions to be taken in each round cannot be modeled using conventional MAB. As an example, consider medical treatment administration. At the beginning of each round a patient arrives to the intensive care unit (ICU) with a random initial health state. The goal state is defined as discharge and dead-end state is defined as death. Actions correspond to treatment options that move the patient randomly over the state space. The objective is to maximize the expected number of patients that are discharged by learning the optimal treatment policy using the observations gathered from the previous patients. In the example given above, each round corresponds to a goaloriented Markov Decision Process (MDP) with dead-ends Cem Tekin is supported by TUBITAK 2232 Fellowship (116C043). [7]. The learner knows the state space, goal and dead-end states, but does not know the state transition probabilities a priori. At each round, the learner chooses a sequence of actions and only observes the state transitions that result from the chosen actions. In the literature, this kind of feedback information is called bandit feedback [8]. Motivated by the application described above, we propose a new MAB problem in which multiple arms are selected in each round until a terminal state is reached. Due to its resemblance to the Gambler\u2019s Ruin Problem [9]\u2013[11], we call this new MAB problem the Gambler\u2019s Ruin Bandit Problem (GRBP). In GRBP, the system proceeds in a sequence of rounds \u03c1 \u2208 {1, 2, . . .}. Each round is modeled as an MDP (as in Fig. 1 ) with unknown state transition probabilities and terminal (absorbing) states. The set of terminal states includes a goal state G and a dead-end state D, and the non-terminal states are ordered between the goal and dead-end states. In each non-terminal state, there are two possible actions: a continuation action (action C) that moves the learner randomly over the state space around the current state; and a terminal action (action F ) that moves the learner directly into a terminal state. Starting from a random, non-terminal initial state, the learner chooses a sequence of actions and observes the resulting state transitions until a terminal state is reached. The learner incurs a unit reward if the goal state is reached. Otherwise, it incurs no reward. The goal of the learner is to maximize its cumulative expected reward over the rounds. If the state transition probabilities were known beforehand, an omnipotent oracle with unlimited computational power could calculate the optimal policy that maximizes the probability of hitting the goal state from any initial state, and then select its actions according to the optimal policy. We define the regret of the learner by round \u03c1 as the difference in the expected number of times the goal state is reached by the omnipotent oracle and the learner by round \u03c1. First, we show that the optimal policy for GRBP can be computed in a straightforward manner: there exists a threshold state above which it is always optimal to take action C and on or below which it is always optimal to take action F . Then, we propose an online learning algorithm for the learner, and bound its regret for two different regions that the actual state transition probabilities can lie in. The regret is bounded (finite) in one region, while it is logarithmic in the number of rounds in the other region. These bounds are problem-specific, in the sense that they are functions of the state transition probabilities. Finally, we illustrate the ar X iv :1 60 5. 06 65 1v 3 [ cs .L G ] 2 9 Se p 20 16 D\t\r   D+1\t\r   s-\u00ad\u20101\t\r   s G-\u00ad\u20101\t\r   G ... ... s+1", "creator": "LaTeX with hyperref package"}}}