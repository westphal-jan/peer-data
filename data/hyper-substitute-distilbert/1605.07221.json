{"id": "1605.07221", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Global Optimality of Local Search for Low Rank Matrix Recovery", "abstract": "we show proved there contain no large additional assumptions in obtaining non - maximum canonical parametrization of low - rank distribution from such incoherent linear measurements. with noisy factors also show all local minima are very convex to a optimal consistency. however with computed local bound within saddle points, estimation yields a polynomial time global stability guarantee at gamma gradient descent { \\ em from random drift }.", "histories": [["v1", "Mon, 23 May 2016 22:05:42 GMT  (1060kb,D)", "https://arxiv.org/abs/1605.07221v1", "21 pages, 3 figures"], ["v2", "Fri, 27 May 2016 00:54:17 GMT  (1061kb,D)", "http://arxiv.org/abs/1605.07221v2", "21 pages, 3 figures"]], "COMMENTS": "21 pages, 3 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["srinadh bhojanapalli", "behnam neyshabur", "nati srebro"], "accepted": true, "id": "1605.07221"}, "pdf": {"name": "1605.07221.pdf", "metadata": {"source": "CRF", "title": "Global Optimality of Local Search for Low Rank Matrix Recovery", "authors": ["Srinadh Bhojanapalli", "Behnam Neyshabur", "Nathan Srebro"], "emails": ["srinadh@ttic.edu", "bneyshabur@ttic.edu", "nati@ttic.edu"], "sections": [{"heading": "1 Introduction", "text": "Low rank matrix recovery problem is heavily studied and has numerous applications in collaborative filtering, quantum state tomography, clustering, community detection, metric learning and multi-task learning [21, 12, 9, 27].\nWe consider the \u201cmatrix sensing\u201d problem of recovering a low-rank (or approximately low rank) p.s.d. matrix1 X\u2217 \u2208 Rn\u00d7n, given a linear measurement operator A : Rn\u00d7n \u2192 Rm and noisy measurements y = A(X\u2217) + w, where w is an i.i.d. noise vector. An estimator for X\u2217 is given by the rank-constrained, non-convex problem\nminimize X:rank(X)\u2264r\n\u2016A(X)\u2212 y\u20162. (1)\nThis matrix sensing problem has received considerable attention recently [30, 29, 26]. This and other rank-constrained problems are common in machine learning and related fields, and have been used for applications discussed above. A typical theoretical approach to low-rank problems, including (1) is to relax the low-rank constraint to a convex constraint, such as the trace-norm of X . Indeed, for matrix sensing, Recht et al. [20] showed that if the measurements are noiseless and the measurement operator A satisfies a restricted isometry property, then a low-rank X\u2217 can be recovered as the unique solution to a convex relaxation of (1). Subsequent work established similar guarantees also for the noisy and approximate case [14, 6].\nHowever, convex relaxations to the rank are not the common approach employed in practice. In this and other low-rank problems, the method of choice is typically unconstrained local optimization (via e.g. gradient descent, SGD or alternating minimization) on the factorized parametrization\nminimize U\u2208Rn\u00d7r\nf(U) = \u2016A(UU>)\u2212 y\u20162, (2)\nwhere the rank constraint is enforced by limiting the dimensionality of U . Problem (2) is a non-convex optimization problem that could have many bad local minima (as we show in Section 5), as well as saddle points. Nevertheless, local optimization seems to work very well in practice. Working on (2) is much cheaper computationally and allows scaling to large-sized problems\u2014the number of optimization variables is only O(nr) rather than O(n2), and the updates are \u2217srinadh@ttic.edu \u2020bneyshabur@ttic.edu \u2021nati@ttic.edu 1We study the case where X\u2217 is PSD. We believe the techniques developed here can be used to extend results to the general case.\nar X\niv :1\n60 5.\n07 22\n1v 2\n[ st\nat .M\nL ]\n2 7\nM ay\nusually very cheap, especially compared to typical methods for solving the SDP resulting from the convex relaxation. There is therefore a significant disconnect between the theoretically studied and analyzed methods (based on convex relaxations) and the methods actually used in practice.\nRecent attempts at bridging this gap showed that, some form of global \u201cinitialization\u201d, typically relying on singular value decomposition, yields a solution that is already close enough to X\u2217; that local optimization from this initializer gets to the global optima (or to a good enough solution). Jain et al. [15], Keshavan [17] proved convergence for alternating minimization algorithm provided the starting point is close to the optimum, while Zheng and Lafferty [30], Zhao et al. [29], Tu et al. [26], Chen and Wainwright [8], Bhojanapalli et al. [2] considered gradient descent methods on the factor space and proved local convergence. But all these studies rely on global initialization followed by local convergence, and do not tackle the question of the existence of spurious local minima or deal with optimization starting from random initialization. There is therefore still a disconnect between this theory and the empirical practice of starting from random initialization and relying only on the local search to find the global optimum.\nIn this paper we show that, under a suitable incoherence condition on the measurement operator A (defined in Section 2), with noiseless measurements and with rank(X\u2217) \u2264 r, the problem (2) has no spurious local minima (i.e. all local minima are global and satisfy X\u2217 = UU>). Furthermore, under the same conditions, all saddle points have a direction with significant negative curvature, and so using a recent result of Ge et al. [10] we can establish that stochastic gradient descent from random initialization converges to X\u2217 in polynomial number of iterations. We extend the results also to the noisy and approximately-low-rank settings, where we can guarantee that every local minima is close to a global minimum. The incoherence condition we require is weaker than conditions used to establish recovery through local search, and so our results also ensures recovery in polynomial time under milder conditions than what was previously known. In particular, with i.i.d. Gaussian measurements, we ensure no spurious local minima and recovery through local search with the optimal number O(nr) of measurements.\nRelated Work Our work is heavily inspired by Bandeira et al. [1], who recently showed similar behavior for the problem of community detection\u2014this corresponds to a specific rank-1 problem with a linear objective, elliptope constraints and a binary solution. Here we take their ideas, extend them and apply them to matrix sensing with general rank-r matrices. In the past several months, similar type of results were also obtained for other non-convex problems (where the source of non-convexity is not a rank constraint), specifically complete dictionary learning [24] and phase recovery [25]. A related recent result of a somewhat different nature pertains to rank unconstrained linear optimization on the elliptope, showing that local minima of the rank-constrained problem approximate well the global optimum of the rank unconstrained convex problem, even though they might not be the global minima (in fact, the approximation guarantee for the actual global optimum is better) [18].\nAnother non-convex low-rank problem long known to not possess spurious local minima is the PCA problem, which can also be phrased as matrix approximation with full observations, namely minrank(X)\u2264r \u2016A\u2212X\u2016F (e.g. [23]). Indeed, local search methods such as the power-method are routinely used for this problem. Recently local optimization methods for the PCA problem working more directly on the optimized formulation have also been studied, including SGD [22] and Grassmannian optimization [28]. These results are somewhat orthogonal to ours, as they study a setting in which it is well known there are never any spurious local minima, and the challenge is obtaining satisfying convergence rates.\nThe seminal work of Burer and Monteiro [3] proposed low-rank factorized optimization for SDPs, and showed that for extremely high rank r > \u221a m (number of constraints), an Augmented Lagrangian method converges asymptotically to the optimum. It was also shown that (under mild conditions) any rank deficient local minima is a global minima [4, 16], providing a post-hoc verifiable sufficient condition for global optimality. However, this does not establish any a-priori condition, based on problem structure, implying the lack of spurious local minima.\nWhile preparing this manuscript, we also became aware of parallel work [11] studying the same question for the related but different problem of matrix completion. For this problem they obtain a similar guarantee, though with suboptimal dependence on the incoherence parameters and so suboptimal sample complexity, and requiring adding a specific non-standard regularizer to the objective\u2014this is not needed for our matrix sensing results.\nWe believe our work, together with the parallel work of [11], are the first to establish the lack of spurious local minima and the global convergence of local search from random initialization for a non-trivial rank-constrained problem (beyond PCA with full observations) with rank r > 1. Notation. For matrices X,Y \u2208 Rn\u00d7n, their inner product is \u3008X,Y \u3009 = trace ( X>Y ) . We use \u2016X\u2016F , \u2016X\u20162 and\n\u2016X\u2016\u2217 for the Frobenius, spectral and nuclear norms of a matrix respectively. Given a matrix X , we use \u03c3i (X) to denote singular values of X in decreasing order. Xr = arg minrank(Y )\u2264r \u2016X \u2212 Y \u2016F denotes the rank-r approximation of X , as obtained via its truncated singular value decomposition. We use plain capitals R and Q to denote orthonormal matrices."}, {"heading": "2 Formulation and Assumptions", "text": "We write the linear measurement operator A : Rn\u00d7n \u2192 Rm as A(X)i = \u3008Ai,X\u3009 where Ai \u2208 Rn\u00d7n, yielding yi = \u3008Ai,X\u2217\u3009+ wi, i = 1, \u00b7 \u00b7 \u00b7 ,m. We assume wi \u223c N (0, \u03c32w) is i.i.d Gaussian noise. We are generally interested in the high dimensional regime where the number of measurements m is usually much smaller than the dimension n2.\nEven if we know that rank(X\u2217) \u2264 r, having many measurements might not be sufficient for recovery if they are not \u201cspread out\u201d enough. E.g., if all measurements only involve the first n/2 rows and columns, we would never have any information on the bottom-right block. A sufficient condition for identifiability of a low-rank X\u2217 from linear measurements by Recht et al. [20] is based on restricted isometry property defined below.\nDefinition 2.1 (Restricted Isometry Property). Measurement operator A : Rn\u00d7n \u2192 Rm (with rows Ai, i = 1, \u00b7 \u00b7 \u00b7 ,m) satisfies (r, \u03b4r) RIP if for any n\u00d7 n matrix X with rank \u2264 r,\n(1\u2212 \u03b4r)\u2016X\u20162F \u2264 1\nm m\u2211 i=1 \u3008Ai,X\u30092 \u2264 (1 + \u03b4r)\u2016X\u20162F . (3)\nIn particular, X\u2217 of rank r is identifiable if \u03b42r < 1 [see 20, Theorem 3.2]. One situation in which RIP is obtained is for random measurement operators. For example, matrices with i.i.d. N (0, 1) entries satisfy (r, \u03b4r)-RIP when m = O(nr\u03b42 ) [see 6, Theorem 2.3]. This implies identifiability based on i.i.d. Gaussian measurement with m = O(nr) measurements (coincidentally, the number of degrees of freedom in X\u2217, optimal up to a constant factor)."}, {"heading": "3 Main Results", "text": "We are now ready to present our main result about local minima for the matrix sensing problem (2). We first present the results for noisy sensing of exact low rank matrices, and then generalize the results also to approximately low rank matrices.\nNow we will present our result characterizing local minima of f(U), for low-rank X\u2217. Recall that measurements are y = A(X\u2217) + w, where entries of w are i.i.d. Gaussian - wi \u223c N (0, \u03c32w).\nTheorem 3.1. Consider the optimization problem (2) where y = A(X\u2217) + w, w is i.i.d. N (0, \u03c32w), A satisfies (2r, \u03b42r)-RIP with \u03b42r < 110 , and rank(X\n\u2217) \u2264 r. Then, with probability \u2265 1 \u2212 10n2 (over the noise), for any local minimum U of f(U):\n\u2016UU> \u2212X\u2217\u2016F \u2264 20 \u221a log(n)\nm \u03c3w.\nIn particular, in the noiseless case (\u03c3w = 0) we have UU> = X\u2217 and so f(U) = 0 and every local minima is global. In the noiseless case, we can also relax the RIP requirement to \u03b42r < 1/5 (see Theorem 4.1 in Section 4). In the noisy case we cannot expect to ensure we always get to an exact global minima, since the noise might cause tiny fluctuations very close to the global minima possibly creating multiple very close local minima. But we show that all local minima are indeed very close to some factorization U\u2217U\u2217> = X\u2217 of the true signal, and hence to a global optimum, and this \u201cradius\u201d of local minima decreases as we have more observations.\nThe proof of the Theorem for the noiseless case is presented in Section 4. The proof for the general setting follows along the same lines and can be found in the Appendix.\nSo far we have discussed how all local minima are global, or at least very close to a global minimum. Using a recent result by Ge et al. [10] on the convergence of SGD for non-convex functions, we can further obtain a polynomial bound on the number of SGD iterations required to reach the global minima. The main condition that needs to be established\nin order to ensure this, is that all saddle points of (2) satisfy the \u201cstrict saddle point condition\u201d, i.e. have a direction with significant negative curvature:\nTheorem 3.2 (Strict saddle). Consider the optimization problem (2) in the noiseless case, where y = A(X\u2217), A satisfies (2r, \u03b42r)-RIP with \u03b42r < 110 , and rank(X\n\u2217) \u2264 r. Let U be a first order critical point of f(U) with UU> 6= X\u2217. Then the smallest eigenvalue of the Hessian satisfies\n\u03bbmin\n[ 1\nm \u22072(f(U))\n] \u2264 \u22124\n5 \u03c3r(X\n\u2217).\nNow consider the stochastic gradient descent updates,\nU+ = Projb\n( U \u2212 \u03b7 ( m\u2211 i=1 ( \u2329 Ai,UU >\u232a\u2212 yi)AiU + \u03c8)) , (4) where \u03c8 is uniformly distributed on the unit sphere and Projb is a projection onto \u2016U\u2016F \u2264 b. Using Theorem 3.2 and the result of Ge et al. [10] we can establish:\nTheorem 3.3 (Convergence from random initialization). Consider the optimization problem (2) under the same noiseless conditions as in Theorem 3.2. Using b \u2265 \u2016U\u2217\u2016F , for some global optimum U\u2217 of f(U), for any , c > 0, after T = poly ( 1\n\u03c3r(X\u2217) , \u03c31(X\n\u2217), b, 1 , log(1/c) )\niterations of (4) with an appropriate stepsize \u03b7, starting from a random point uniformly distributed on \u2016U\u2016F = b, with probability at least 1\u2212 c, we reach an iterate UT satisfying\n\u2016UT \u2212U\u2217\u2016F \u2264 .\nThe above result guarantees convergence of noisy gradient descent to a global optimum. Alternatively, second order methods such as cubic regularization (Nesterov and Polyak [19]) and trust region (Cartis et al. [7]) that have guarantees based on the strict saddle point property can also be used here.\nRIP Requirement: Our results require (2r, 1/10)-RIP for the noisy case and (2r, 1/5)-RIP for the noiseless case. Requiring (2r, \u03b42r)-RIP with \u03b42r < 1 is sufficient to ensure uniqueness of the global optimum of (1), and thus recovery in the noiseless setting [20], but all known efficient recovery methods require stricter conditions. The best guarantees we are aware of require (5r, 1/10)-RIP [20] or (4r, 0.414)-RIP [6] using a convex relaxation. Our requirement is not directly comparable to the latter, as we require RIP on a smaller set, but with a lower (stricter) \u03b4. Alternatively, (6r, 1/10)-RIP is required for global initialization followed by non-convex optimization [26]\u2014our requirement is strictly better. In terms of requirements on (2r, \u03b42r)-RIP for non-convex methods, the best we are aware of is requiring \u03b42r < \u2126(1/r) [15, 29, 30]\u2013this is a much stronger condition than ours, and it yields a suboptimal required number of spherical Gaussian measurements of \u2126(nr3). So, compared to prior work our requirement is very mild\u2014it ensures efficient recovery even in a regime not previously covered by any guarantee on efficient recovery, and requires the optimal number of spherical Gaussian measurements (up to a constant factor) of O(nr).\nExtension to Approximate Low Rank We can also obtain similar results that deteriorate gracefully if X\u2217 is not exactly low rank, but is close to being low-rank (see proof in the Appendix):\nTheorem 3.4. Consider the optimization problem (2) where y = A(X\u2217) and A satisfies (2r, \u03b42r)-RIP with \u03b42r < 1100 , Then, for any local minima U of f(U):\n\u2016UU> \u2212X\u2217\u2016F \u2264 4(\u2016X\u2217 \u2212X\u2217r \u2016F + \u03b42r\u2016X\u2217 \u2212X\u2217r \u2016\u2217),\nwhere X\u2217r is the best rank r approximation of X \u2217.\nThis theorem guarantees that any local optimum of f(U) is close to X\u2217 upto an error depending on \u2016X\u2217 \u2212X\u2217r \u2016. For the low-rank noiseless case we have X\u2217 = X\u2217r and the right hand side vanishes. When X\n\u2217 is not exactly low rank, the best recovery error we can hope for is \u2016X\u2217 \u2212X\u2217r \u2016F , since UU> is at most rank k. On the right hand side of Theorem 3.4, we have also a nuclear norm term, which might be higher, but it also gets scaled down by \u03b42r, and so by the number of measurements."}, {"heading": "4 Proof for the Noiseless Case", "text": "In this section we present the proof characterizing the local minima of problem (2). For ease of exposition we first present the results for the noiseless case (w = 0). Proof for the general case can be found in the Appendix.\nTheorem 4.1. Consider the optimization problem (2) where y = A(X\u2217), A satisfies (2r, \u03b42r)-RIP with \u03b42r < 15 , and rank(X\u2217) \u2264 r. Then, for any local minimum U of f(U):\nUU> = X\u2217.\nFor the proof of this theorem we first discuss the implications of the first and second order optimality conditions and then show how to combine them to yield the result.\nOur proof techniques are different from existing results characterizing local minima of dictionary learning [24], phase retrieval [25] and community detection [1, 18]. For most of these problems Hessian is PSD only for points close to optimum. However, Hessian of f(U) can be PSD even for points far from to optima. Hence we need new directions to use the second order conditions.\nInvariance of f(U) over r\u00d7r orthonormal matrices introduces additional challenges in comparing a given stationary point to a global optimum. We have to find the best orthonormal matrix R to align a given stationary point U to a global optimum U\u2217, where U\u2217U\u2217> = X\u2217, to combine results from the first and second order conditions, without degrading the isometry constants.\nConsider a local optimum U that satisfies first and second order optimality conditions of problem (2). In particular U satisfies\u2207f(U) = 0 and z>\u22072f(U)z \u2265 0 for any z \u2208 Rn\u00b7r. Now we will see how these two conditions constrain the error UU> \u2212U\u2217U\u2217>.\nFirst we present the following consequence of the RIP assumption [see 5, Lemma 2.1].\nLemma 4.1. Given two n\u00d7 n rank-r matrices X and Y , and a (2r, \u03b4)-RIP measurement operator A, the following holds: \u2223\u2223\u2223\u2223\u2223 1m m\u2211 i=1 \u3008Ai,X\u3009 \u3008Ai,Y \u3009 \u2212 \u3008X,Y \u3009 \u2223\u2223\u2223\u2223\u2223 \u2264 \u03b4\u2016X\u2016F \u2016Y \u2016F . (5)"}, {"heading": "4.1 First order optimality", "text": "First we will consider the first order condition,\u2207f(U) = 0. For any stationary point U this implies\u2211\ni\n\u2329 Ai,UU > \u2212U\u2217U\u2217> \u232a AiU = 0. (6)\nNow using the isometry property of Ai gives us the following result.\nLemma 4.2. [First order condition] For any first order stationary point U of f(U), andA satisfying the (2r, \u03b4)-RIP (3), the following holds: \u2016(UU> \u2212U\u2217U\u2217>)QQ>\u2016F \u2264 \u03b4 \u2225\u2225\u2225UU> \u2212U\u2217U\u2217>\u2225\u2225\u2225\nF ,\nwhere Q is an orthonormal matrix that spans the column space of U .\nThis lemma states that any stationary point of f(U) is close to a global optimum U\u2217 in the subspace spanned by columns of U . Notice that the error along the orthogonal direction \u2016X\u2217Q\u22a5Q>\u22a5\u2016F can still be large making the distance between X and X\u2217 arbitrarily far.\nProof of Lemma 4.2. Let U = QR, for some orthonormal Q. Consider any matrix of the form ZQR\u22121>. The first order optimality condition then implies,\nm\u2211 i=1 \u2329 Ai,UU > \u2212U\u2217U\u2217> \u232a \u2329 Ai,UR \u22121Q>Z> \u232a = 0\nThe above equation together with Restricted Isometry Property (equation (5)) gives us the following inequality:\u2223\u2223\u2223\u2329UU> \u2212U\u2217U\u2217>, QQ>Z>\u232a\u2223\u2223\u2223 \u2264 \u03b4 \u2225\u2225\u2225UU> \u2212U\u2217U\u2217>\u2225\u2225\u2225 F \u2225\u2225QQ>Z>\u2225\u2225 F .\nNote that for any matrix A, \u2329 A, QQ>Z \u232a = \u2329 QQ>A,Z \u232a . Furthermore, for any matrix A, sup{Z:\u2016Z\u2016F\u22641} \u3008A,Z\u3009 = \u2016A\u2016F . Hence the above inequality implies the lemma statement."}, {"heading": "4.2 Second order optimality", "text": "We now consider the second order condition to show that the error along Q\u22a5Q>\u22a5 is indeed bounded well. Let\u22072f(U) be the hessian of the objective function. Note that this is an n \u00b7 r \u00d7 n \u00b7 r matrix. Fortunately for our result we need to only evaluate the Hessian along the direction vec(U \u2212U\u2217R) for some orthonormal matrix R. Here vec(.) denotes writing a matrix in vector form.\nLemma 4.3. [Hessian computation] Let U be a first order critical point of f(U). Then for any r \u00d7 r orthonormal matrix R and \u2206j = \u2206eje>j ( \u2206 = U \u2212U\u2217R),\nr\u2211 j=1 vec (\u2206j) > [\u22072f(U)]vec (\u2206j) = m\u2211 i=1 ( r\u2211 j=1 4 \u2329 Ai,U\u2206 > j \u232a2 \u2212 2\u2329Ai,UU> \u2212U\u2217U\u2217>\u232a2), Proof of Lemma 4.3. For any matrix Z, taking directional second derivative of the function f(U) with respect to Z we get:\nvec (Z)> [ \u22072f(U) ] vec (Z) = vec (Z)> lim\nt\u21920\n[ \u2207f (U + t(Z))\u2212\u2207f(U)\nt ] = 2\nm\u2211 i=1 [ 2 \u2329 Ai,UZ >\u232a2 + \u2329Ai,UU> \u2212U\u2217U\u2217>\u232a \u2329Ai,ZZ>\u232a ]\nSetting Z = \u2206j = (U \u2212U\u2217R)eje>j and using the first order optimality condition on U , we get,\nvec ( (U \u2212U\u2217R)eje>j )> [\u22072f(U)]vec ((U \u2212U\u2217R)eje>j ) =\nm\u2211 i=1 4 \u2329 Ai,U\u2206 > j \u232a2 + 2 \u2329 Ai,UU > \u2212U\u2217U\u2217> \u232a \u2329 Ai,\u2206j\u2206 > j \u232a (i) =\nm\u2211 i=1 4 \u2329 Ai,Ueje > j \u2206 > j \u232a2 + 2 \u2329 Ai,UU > \u2212U\u2217U\u2217> \u232a \u2329 Ai,U \u2217eje > j (U \u2217eje > j ) >\u232a\n(ii) = m\u2211 i=1 4 \u2329 Ai,Ueje > j \u2206 > j \u232a2 \u2212 2\u2329Ai,UU> \u2212U\u2217U\u2217>\u232a\u2329Ai,Ueje>j U> \u2212U\u2217eje>j U\u2217>\u232a . (i) and (ii) follow from the first order optimality condition (6),\nm\u2211 i=1 \u2329 Ai,UU >\u232aUeje>j = m\u2211 i=1 \u2329 Ai,U \u2217U\u2217> \u232a Ueje > j ,\nfor j = 1 \u00b7 \u00b7 \u00b7 r. Finally taking sum over j from 1 to r gives the result.\nHence from second order optimality of U we get,\nCorollary 4.1. [Second order optimality] Let U be a local minimum of f(U) . For any r \u00d7 r orthonormal matrix R, r\u2211 j=1 m\u2211 i=1 4 \u2329 Ai,U\u2206 > j \u232a2 \u2265 1 2 m\u2211 i=1 \u2329 Ai,UU > \u2212U\u2217U\u2217> \u232a2 , (7)\nFurther for A satisfying (2r, \u03b4) -RIP (equation (3)) we have, r\u2211 j=1 \u2016Ueje>j (U \u2212U\u2217R)>\u20162F \u2265 1\u2212 \u03b4 2(1 + \u03b4) \u2016UU> \u2212U\u2217U\u2217>\u20162F . (8)\nThe proof of this result follows simply by applying Lemma 4.3. The above Lemma gives a bound on the distance in the factor (U) space \u2016U(U \u2212U\u2217R)>\u20162F . To be able to compare the second order condition to the first order condition we need a relation between \u2016U(U \u2212U\u2217R)>\u20162F and \u2016X \u2212X\u2217\u20162F . Towards this we show the following result.\nLemma 4.4. Let U and U\u2217 be two n\u00d7 r matrices, and Q is an orthonormal matrix that spans the column space of U . Then there exists an r\u00d7 r orthonormal matrix R such that for any first order stationary point U of f(U), the following holds:\nr\u2211 j=1 \u2016Ueje>j (U \u2212U\u2217R)>\u20162F \u2264 1 8 \u2016UU> \u2212U\u2217U\u2217>\u20162F + 34 8 \u2016(UU> \u2212U\u2217U\u2217>)QQ>\u20162F .\nThis Lemma bounds the distance in the factor space (\u2016(U \u2212 U\u2217R)U>\u20162F ) with \u2016UU> \u2212 U\u2217U\u2217 >\u20162F and \u2016(UU> \u2212 U\u2217U\u2217>)QQ>\u20162F . Combining this with the result from second order optimality (Corollary 4.1) shows \u2016UU>\u2212U\u2217U\u2217>\u20162F is bounded by a constant factor of \u2016(UU>\u2212U\u2217U\u2217\n>)QQ>\u20162F . This implies \u2016X\u2217Q\u22a5Q\u22a5\u2016F is bounded, opposite to what the first order condition implied (Lemma 4.2). The proof of the above lemma is in Section E. Hence from the above optimality conditions we get the proof of Theorem 4.1.\nProof of Theorem 4.1. Assuming UU> 6= U\u2217U\u2217>, from Lemmas 4.2, 4.4 and Corollary 4.1 we get,( 1\u2212 \u03b4\n2(1 + \u03b4) \u2212 1 8\n) \u2016UU> \u2212U\u2217U\u2217>\u20162F \u2264 34 8 \u03b42 \u2225\u2225\u2225(UU> \u2212U\u2217U\u2217>)\u2225\u2225\u22252 F .\nIf \u03b4 \u2264 15 the above inequality holds only if UU > = U\u2217U\u2217>."}, {"heading": "5 Necessity of RIP", "text": "We showed that there are no spurious local minima only under a restricted isometry assumption. A natural question is whether this is necessary, or whether perhaps the problem (2) never has any spurious local minima, perhaps similarly to the non-convex PCA problem minU\n\u2225\u2225A\u2212UU>\u2225\u2225. A good indication that this is not the case is that (2) is NP-hard, even in the noiseless case when y = A(X\u2217) for rank(X\u2217) \u2264 k [20] (if we don\u2019t require RIP, we can have each Ai be non-zero on a single entry in which case (2) becomes a matrix completion problem, for which hardness has been shown even under fairly favorable conditions [13]). That is, we are unlikely to have a poly-time algorithm that succeeds for any linear measurement operator. Although this doesn\u2019t formally preclude the possibility that there are no spurious local minima, but it just takes a very long time to find a local minima, this scenario seems somewhat unlikely.\nTo resolve the question, we present an explicit example of a measurement operatorA and y = A(X\u2217) (i.e. f(X\u2217) = 0), with rank(X\u2217) = r, for which (1), and so also (2), have a non-global local minima.\nExample 1: Let f(X) = (X11 +X22 \u2212 1)2 + (X11 \u2212 1)2 and consider (1) with r = 1 (i.e. a rank-1 constraint). For X\u2217 = [ 1 0 0 0 ] we have f(X\u2217) = 0 and rank(X\u2217) = 1. But X = [ 0 0 0 1 ] is a rank 1 local minimum with f(X) = 1.\nWe can be extended the construction to any rank r by simply adding \u2211r+2 i=3 (Xii \u2212 1)2 to the objective, and padding both the global and local minimum with a diagonal beneath the leading 2\u00d7 2 block. In Example 1, we had a rank-r problem, with a rank-r exact solution, and a rank-r local minima. Another question we can ask is what happens if we allow a larger rank than the rank of the optimal solution. That is, if we have f(X\u2217) = 0 with low rank(X\u2217), even rank(X\u2217) = 1, but consider (1) or (2) with a high r. Could we still have non-global local minima? The answer is yes...\nExample 2: Let f(X) = (X11 +X22 +X33 \u2212 1)2 + (X11 \u2212 1)2 + (X22 \u2212X33)2 and consider the problem (1)\nwith a rank r = 2 constraint. We can verify that X\u2217 = 1 0 00 0 0 0 0 0  is a rank=1 global minimum with f(X\u2217) = 0, but X =\n0 0 00 1/2 0 0 0 1/2  is a local minimum with f(X) = 1. Also for an arbitrary large rank constraint r > 1 (taking r to be odd for simplicity), extend the objective to f(X) = (X11 \u2212 1)2 + \u2211(r\u22121)/2 i=1 [ (X11 +X2i,2i +X(2i+1),(2i+1) \u2212 1)2\n+(X2i,2i \u2212X(2i+1),(2i+1))2 ] . We still have a rank-1 global minimum X\u2217 with a single non-zero entry X\u221711 = 1, while X = (I \u2212X\u2217)/2 is a local minimum with f(X) = 1."}, {"heading": "6 Conclusion", "text": "We established that under conditions similar to those required for convex relaxation recovery guarantees, the non-convex formulation of matrix sensing (2) does not exhibit any spurious local minima (or, in the noisy and approximate settings, at least not outside some small radius around a global minima), and we can obtain theoretical guarantees on the success of optimizing it using SGD from random initialization. This matches the methods frequently used in practice, and can explain their success. This guarantee is very different in nature from other recent work on non-convex optimization for low-rank problems, which relied heavily on initialization to get close to the global optimum, and on local search just for the final local convergence to the global optimum. We believe this is the first result, together with the parallel work of Ge et al. [11], on the global convergence of local search for common rank-constrained problems that are worst-case hard.\nOur result suggests that SVD initialization is not necessary for global convergence, and random initialization would succeed under similar conditions (in fact, our conditions are even weaker than in previous work that used SVD initialization). To investigate empirically whether SVD initialization is indeed helpful for ensuring global convergence, in Figure 1 we compare recovery probability of random rank-k matrices for random and SVD initialization\u2014there is no significant difference between the two.\nBeyond the implications for matrix sensing, we are hoping these type of results could be a first step and serve as a model for understanding local search in deep networks. Matrix factorization, such as in (2), is a depth-two neural network with linear transfer\u2014an extremely simple network, but already non-convex and arguably the most complicated\nnetwork we have a good theoretical understanding of. Deep networks are also hard to optimize in the worst case, but local search seems to do very well in practice. Our ultimate goal is to use the study of matrix recovery as a guide in understating the conditions that enable efficient training of deep networks."}, {"heading": "Acknowledgements", "text": "Authors would like to thank Afonso Bandeira for discussions, Jason Lee and Tengyu Ma for sharing and discussing their work. This research was supported in part by an NSF RI-AF award and by Intel ICRI-CI."}, {"heading": "A Numerical Simulations", "text": "In this section we present simulation results for performance of gradient descent over f(U). We consider measurements yi = \u3008Ai,X\u2217\u3009, where Ai are i.i.d Gaussian with each entry distributed as N (0, 1/m). X\u2217 is a 100 \u00d7 100 rank r random p.s.d matrix with \u2016X\u2217\u2016F = 1. r is varied from 1 to 20 in the experiments.\nWe consider both standard gradient descent and noisy gradient descent (4) with step size 1\u2016U\u20162 . We add noise of magnitude 1e\u2212 4 for the noisy gradient updates. Each method is run until convergence (max of 200 iterations). Let the output of gradient descent be U\u0302 . A run of this experiment is considered success if the final error \u2016U\u0302U\u0302> \u2212X\u2217\u2016F \u2264 1e\u2212 2. Each experiment is repeated for 20 times and average probability of success is computed.\nWe repeat the above procedure starting from both random initialization and SVD initialization. For SVD initialization, the initial point is set to be the rank r approximation of \u2211m i=1 yiAi as suggested by Jain et al. [15]. In figure 2 we have the plots for the cases discussed above. All of them have phase transition around number of samples m = 2 \u00b7 n \u00b7 r. This is in agreement with the results in Section 3. f(U) has no local minima oncem \u2265 2 \u00b7n \u00b7r and random initialization has same performance as SVD initialization.\nIn figure 3, the left two plots show error \u2016U\u0302U\u0302>\u2212X\u2217\u2016F/\u2016X\u2217\u2016F behaves with varying rank and number of samples for random and SVD initializations. The rightmost plot shows the phase transition for rank 10 case for all the methods. Again we notice no significant difference between these methods."}, {"heading": "B Proof for the Noisy Case", "text": "In this section we present the proof characterizing the local minima of problem (2). Recall y = A(X\u2217) + w, where X\u2217 is a rank-r matrix and w is i.i.d. N (0, \u03c32w).\nWe consider local optimum that satisfies first and second order optimality conditions of problem (2). In particular U satisfies\u2207f(U) = 0 and z>\u22072f(U)z \u2265 0 for any z \u2208 Rn\u00b7r. Now we will see how these two conditions constrain the error UU> \u2212U\u2217U\u2217>.\nB.1 First order optimality First we will consider the first order condition,\u2207f(U) = 0. For any stationary point U this implies\u2211\ni\n\u2329 Ai,UU > \u2212U\u2217U\u2217> \u232a AiU = m\u2211 i=1 wiAiU . (9)\nNow using the isometry property of Ai gives us the following result.\nLemma B.1. [First order condition] For any first order stationary point U of f(U), andA satisfying the (2r, \u03b4)-RIP (3), the following holds:\n\u2016(UU> \u2212U\u2217U\u2217>)QQ>\u2016F \u2264 \u03b4 \u2225\u2225\u2225UU> \u2212U\u2217U\u2217>\u2225\u2225\u2225\nF + 2\n\u221a (1 + \u03b4) log(n)\nm \u03c3w,\nw.p. \u2265 1\u2212 1n2 , where Q is an orthonormal matrix that spans the column space of U .\nThis lemma states that any stationary point of f(U) is close to a global optimum U\u2217 in the subspace spanned by columns of U . Notice that the error along the orthogonal direction \u2016X\u2217Q\u22a5Q>\u22a5\u2016F can still be large making the distance between X and X\u2217 arbitrarily big.\nProof of Lemma B.1. Let U = QR, for some orthonormal Q. Consider any matrix of the form ZQR\u22121>. The first order optimality condition then implies,\nm\u2211 i=1 \u2329 Ai,UU > \u2212U\u2217U\u2217> \u232a \u2329 Ai,UR \u22121Q>Z> \u232a = m\u2211 i=1 wiAiUR \u22121Q>Z>.\nThe above equation together with Restricted Isometry Property(equation (5)) gives us the following inequality:\u2223\u2223\u2223\u2329UU> \u2212U\u2217U\u2217>, QQ>Z>\u232a\u2223\u2223\u2223 \u2264 \u03b4 \u2225\u2225\u2225UU> \u2212U\u2217U\u2217>\u2225\u2225\u2225 F \u2225\u2225QQ>Z>\u2225\u2225 F + 2 \u221a (1 + \u03b4) log(n) m \u03c3w\u2016Z>\u2016F ,\nby Cauchy Schwarz inequality and Lemma E.2. Note that for any matrixA, \u2329 A, QQ>Z \u232a = \u2329 AQQ>,Z \u232a . Furthermore, for any matrix A, sup{Z:\u2016Z\u2016F\u22641} \u3008A,Z\u3009 = \u2016A\u2016F . Hence the above inequality implies the lemma statement.\nB.2 Second order optimality We will now consider the second order condition to show that the error along Q\u22a5Q>\u22a5 is indeed bounded well. Let \u22072f(U) be the hessian of the objective function. Note that this is an n \u00b7 r \u00d7 n \u00b7 r matrix. Fortunately for our result we need to only evaluate the Hessian along the direction vec(U \u2212U\u2217R) for some orthonormal matrix R.\nLemma B.2. [Hessian computation] Let U be a first order critical point of f(U). Then for any r \u00d7 r orthonormal matrix R and \u2206 = U \u2212U\u2217R,\nr\u2211 j=1 vec (\u2206j) > [\u22072f(U)]vec (\u2206j)\n= m\u2211 i=1  r\u2211 j=1 4 \u2329 Ai,U\u2206 > j \u232a2 \u2212 2\u2329Ai,UU> \u2212U\u2217U\u2217>\u232a2 \u2212 2wi \u3008Ai,X \u2212X\u2217\u3009  ,\nProof of Lemma B.2. For any matrix Z, taking directional second derivative of the function f(U) with respect to Z we get:\nvec (Z)> [ \u22072f(U) ] vec (Z) = vec (Z)> lim\nt\u21920\n[ \u2207f (U + t(Z))\u2212\u2207f(U)\nt ] = 2\nm\u2211 i=1 [ 2 \u2329 Ai,UZ >\u232a2 + (\u2329Ai,UU> \u2212U\u2217U\u2217>\u232a\u2212 wi) \u2329Ai,ZZ>\u232a ] Setting Z = \u2206j = (U \u2212U\u2217R)eje>j and using the first order optimality condition on U , we get,\nvec ( (U \u2212U\u2217R)eje>j )> [\u22072f(U)]vec ((U \u2212U\u2217R)eje>j ) =\nm\u2211 i=1 4 \u2329 Ai,U\u2206 > j \u232a2 + 2( \u2329 Ai,UU > \u2212U\u2217U\u2217> \u232a \u2212 wi) \u2329 Ai,\u2206j\u2206 > j \u232a =\nm\u2211 i=1 4 \u2329 Ai,Ueje > j \u2206 > j \u232a2 + 2( \u2329 Ai,UU > \u2212U\u2217U\u2217> \u232a \u2212 wi) \u2329 Ai,U \u2217eje > j (U \u2217eje > j ) >\u232a\n= m\u2211 i=1 4 \u2329 Ai,Ueje > j \u2206 > j \u232a2 \u2212 2\u2329Ai,UU> \u2212U\u2217U\u2217>\u232a\u2329Ai,Ueje>j U> \u2212U\u2217eje>j U\u2217>\u232a \u2212 2wi \u2329 Ai,Ueje > j U > \u2212U\u2217eje>j U\u2217 > \u232a . (10)\nwhere the last equality is again by the first order optimality condition (9).\nHence from second order optimality of U we get,\nCorollary B.1. [Second order optimality] Let U be a local minimum of f(U) . For any r \u00d7 r orthonormal matrix R, w.p. \u2265 1\u2212 1n2 ,\n1\n2 m\u2211 i=1 \u2329 Ai,UU > \u2212U\u2217U\u2217> \u232a2 \u2264 m\u2211 i=1 r\u2211 j=1 \u2329 Ai,U\u2206 > j \u232a2 + \u221a log(n)\u03c3w\u2016A(X \u2212X\u2217)\u20162\n\u2264 m\u2211 i=1 r\u2211 j=1 \u2329 Ai,U\u2206 > j \u232a2 + 5 log(n)\u03c32w + 1 20 m\u2211 i=1 \u3008Ai,X \u2212X\u2217\u30092\nFurther for A satisfying (2r, \u03b4) -RIP (equation (3)) we have,\n1\u2212 \u03b4 2(1 + \u03b4) \u2016UU> \u2212U\u2217U\u2217>\u20162F \u2264 r\u2211 j=1 \u2016U\u2206>j \u20162F + 1 20 \u2016X \u2212X\u2217\u20162F + 5 log(n) m(1 + \u03b4) \u03c32w. (11)\nHence from the above optimality conditions we get the proof of Theorem 4.1.\nProof of Theorem 3.1. Assuming UU> 6= U\u2217U\u2217>, from Lemma 4.4 and Corollary B.1 we get, with probability \u2265 1\u2212 2n2 ,(\n1\u2212 \u03b4 2(1 + \u03b4)\n) \u2016UU> \u2212U\u2217U\u2217>\u20162F\n\u2264 1 8 \u2016X \u2212X\u2217\u20162F + 34 8 \u2016X \u2212X\u2217QQ>\u20162F + 1 20 \u2016X \u2212X\u2217\u20162F + 5 log(n) m(1 + \u03b4) \u03c32w\n(i) \u2264 ( 1\n8 +\n1\n20\n) \u2016X \u2212X\u2217\u20162F + 34\n8\n( 2\u03b42\u2016X \u2212X\u2217\u20162F + 8 (1 + \u03b4) log(n)\nm \u03c32w\n) + 5 log(n)\nm(1 + \u03b4) \u03c32w.\n(i) follows from Lemma B.1. The above inequality implies,( 1\u2212 \u03b4\n2(1 + \u03b4) \u2212 1 8 \u2212 1 20 \u2212 34 4 \u03b42 ) \u2016UU> \u2212U\u2217U\u2217>\u20162F \u2264 34 (1 + \u03b4) log(n) m \u03c32w + 5 log(n) m(1 + \u03b4) \u03c32w.\nIf \u03b4 \u2264 110 , the above inequality reduces to \u2016UU > \u2212U\u2217U\u2217>\u2016F \u2264 c \u221a log(n) m \u03c3w, for some constant c \u2264 17, w.p\n\u2265 1\u2212 2n2 ."}, {"heading": "C Proof for the High Rank Case", "text": "In this section we will present the proof for the inexact case, where rank(X\u2217) \u2265 r. Recall that measurements are y = A(X\u2217).\nLet SVD of X\u2217 be Q\u2217\u03a3\u2217Q\u2217>. With slight abuse of notation we use X\u2217jr+1:(j+1)r to denote the jth rank r block Q\u2217jr+1:(j+1)r\u03a3 \u2217 jr+1:(j+1)rQ \u2217 jr+1:(j+1)r\n>, where Q\u2217jr+1:(j+1)r denotes the restriction of Q to columns jr + 1 to (j + 1)r.\nC.1 First order optimality First we will consider the first order condition,\u2207f(U) = 0. For any stationary point U this implies\u2211\ni\n\u2329 Ai,UU > \u2212U\u2217U\u2217> \u232a AiU = 0. (12)\nNow using the isometry property of Ai gives us the following result.\nLemma C.1. [First order condition] For any first order stationary point U of f(U), and {Ai} satisfying the (2r, \u03b4)RIP (3), the following holds:\n\u2016X \u2212QQ>X\u2217r \u2016F \u2264 \u03b4 \u2016X \u2212X\u2217r \u2016F + \u2016(X \u2217 \u2212X\u2217r )QQ>\u2016F + \u03b4\u2016X\u2217 \u2212X\u2217r \u2016\u2217.\nwhere Q is an orthonormal matrix that spans the column space of U .\nThis lemma states that any stationary point of f(U) is close to a global optimum U\u2217 in the subspace spanned by columns of U . Notice that the error along the orthogonal direction \u2016X\u2217Q\u22a5Q>\u22a5\u2016F can still be large making the distance between X and X\u2217 arbitrarily big.\nProof of Lemma C.1. Let U = QR, for some orthonormal Q. Consider any matrix of the form ZQR\u2212>. The first order optimality condition then implies,\nm\u2211 i=1 \u3008Ai,X \u2212X\u2217r \u3009 \u2329 Ai,UR \u22121Q>Z> \u232a = m\u2211 i=1 \u3008Ai,X\u2217 \u2212X\u2217r \u3009 \u2329 Ai,UR \u22121Q>Z> \u232a .\nNote that X \u2212 X\u2217r is atmost rank-2r. Hence, the above equation together with Restricted Isometry Property(equation (5)) gives us the following inequality:\u2223\u2223\u2329X \u2212X\u2217r , QQ>Z>\u232a\u2223\u2223\u2212\u03b4 \u2016X \u2212X\u2217r \u2016F \u2225\u2225QQ>Z>\u2225\u2225F\n\u2264 1 m m\u2211 i=1\n\u2329 Ai,\n\u2211 j X\u2217jr+1:(j+1)r\n\u232a\u2329 Ai, QQ >Z> \u232a\n\u2264 \u2211 j \u2329 X\u2217jr+1:(j+1)r, QQ >Z> \u232a + \u03b4\u2016X\u2217jr+1:(j+1)r\u2016F\n\u2264 \u2016(X\u2217 \u2212X\u2217r )QQ>\u2016F + \u03b4\u2016X\u2217 \u2212X\u2217r \u2016\u2217. The last inequality follows from \u2211 j \u2016X\u2217jr+1:(j+1)r\u2016F \u2264 \u2016X\n\u2217 \u2212X\u2217r \u2016\u2217. The above inequalities are true for any Z. Further note that for any matrix A, \u2329 A, QQ>Z \u232a = \u2329 AQQ>,Z \u232a . Furthermore, for any matrix A, sup{Z:\u2016Z\u2016F\u22641}\n\u3008A,Z\u3009 = \u2016A\u2016F . Hence the above inequality implies the Lemma.\nC.2 Second order optimality We will now consider the second order condition to show that the error along Q\u22a5Q>\u22a5 is indeed bounded well. Let \u22072f(U) be the hessian of the objective function. Note that this is an n \u00b7 r \u00d7 n \u00b7 r matrix. Fortunately for our result we need to only evaluate the Hessian along the direction vec(U \u2212U\u2217R) for some orthonormal matrix R. Lemma C.2. [Hessian computation] Let U be a first order critical point of f(U). Then for any n\u00d7 r matrix Z,\nvec (Z)> [ \u22072f(U) ] vec (Z) = m\u2211 i=1 4 \u2329 Ai,UZ >\u232a2 + 2\u2329Ai,UU> \u2212U\u2217U\u2217>\u232a \u2329Ai,ZZ>\u232a , Further let U be a local minimum of f(U) and A satisfying (2r, \u03b4) -RIP (equation (3)). Then,\n(1\u2212 3\u03b4)\u2016X \u2212X\u2217r \u20162F \u2264 4(1 + \u03b4) r\u2211 j=1 \u2016U\u2206>j \u20162F + \u2016X\u2217 \u2212X\u2217r \u20162F + \u03b4\u2016X\u2217 \u2212X\u2217r \u20162\u2217.\nProof of Lemma C.2. For any matrix Z, taking directional second derivative of the function f(U) with respect to Z we get:\nvec (Z)> [ \u22072f(U) ] vec (Z) = vec (Z)> lim\nt\u21920\n[ \u2207f (U + t(Z))\u2212\u2207f(U)\nt ] = 2\nm\u2211 i=1 [ 2 \u2329 Ai,UZ >\u232a2 + \u2329Ai,UU> \u2212U\u2217U\u2217>\u232a \u2329Ai,ZZ>\u232a ]. Setting Z = \u2206j = (U \u2212U\u2217R)eje>j we get,\nr\u2211 j=1 vec ( (U \u2212U\u2217R)eje>j )> [\u22072f(U)]vec ((U \u2212U\u2217R)eje>j ) =\nm\u2211 i=1 ( r\u2211 j=1 4 \u2329 Ai,Ueje > j (U \u2212U\u2217rR)> \u232a2 + 2 r\u2211 j=1 \u2329 Ai,UU > \u2212U\u2217U\u2217> \u232a \u2329 Ai, (U \u2212U\u2217rR)eje>j (U \u2212U\u2217rR)> \u232a )\n(i) = m\u2211 i=1 ( r\u2211 j=1 4 \u2329 Ai,U\u2206 > j \u232a2 + 2 \u2329 Ai,UU > \u2212U\u2217U\u2217> \u232a \u2329 Ai,U \u2217 rR(U \u2217 rR) > \u2212X \u232a ).\n(i) is by the first order optimality condition (12). Hence from second order optimality of U we get,\nm\u2211 i=1 4 r\u2211 j=1 \u2329 Ai,U\u2206 > j \u232a2 \u2265 m\u2211 i=1 2 \u3008Ai,X \u2212X\u2217\u3009 \u3008Ai,X \u2212X\u2217r \u3009 . (13)\n1\nm m\u2211 i=1 \u3008Ai,X \u2212X\u2217\u3009 \u3008Ai,X \u2212X\u2217r \u3009 = 1 m m\u2211 i=1 \u3008Ai,X \u2212X\u2217r \u3009 2 + \u3008Ai,X\u2217r \u2212X\u2217\u3009 \u3008Ai,X \u2212X\u2217r \u3009\n(i) \u2265 (1\u2212 \u03b4)\u2016X \u2212X\u2217r \u20162F \u2212 1\nm m\u2211 i=1 \u2211 j=1 \u2329 Ai,X \u2217 jr+1:(j+1)r \u232a \u3008Ai,X \u2212X\u2217r \u3009 (ii)\n\u2265 (1\u2212 \u03b4)\u2016X \u2212X\u2217r \u20162F \u2212 \u2211 j=1 \u2329 X \u2212X\u2217r ,X\u2217jr+1:(j+1)r \u232a \u2212 \u03b4 \u2211 j=1 \u2016X \u2212X\u2217r \u2016F \u2016X\u2217jr+1:(j+1)r\u2016F\n= (1\u2212 \u03b4)\u2016X \u2212X\u2217r \u20162F \u2212 \u3008X \u2212X\u2217r ,X\u2217 \u2212X\u2217r \u3009 \u2212 \u03b4 \u2211 j=1 \u2016X \u2212X\u2217r \u2016F \u2016X\u2217jr+1:(j+1)r\u2016F\n\u2265 (1\u2212 \u03b4)\u2016X \u2212X\u2217r \u20162F \u2212 1 2 \u2016X \u2212X\u2217r \u20162F \u2212 1 2 \u2016X\u2217 \u2212X\u2217r \u20162F \u2212 \u03b4 \u2211 j=1 \u2016X \u2212X\u2217r \u2016F \u2016X\u2217jr+1:(j+1)r\u2016F\n(iii)\n\u2265 (1\u2212 \u03b4)\u2016X \u2212X\u2217r \u20162F \u2212 1 2 \u2016X \u2212X\u2217r \u20162F \u2212 1 2 \u2016X\u2217 \u2212X\u2217r \u20162F \u2212 \u03b4 1 2\n( \u2016X \u2212X\u2217r \u20162F + \u2016X\u2217 \u2212X\u2217r \u20162\u2217 ) =\n1\u2212 3\u03b4 2 \u2016X \u2212X\u2217r \u20162F \u2212 1 2 \u2016X\u2217 \u2212X\u2217r \u20162F \u2212 \u03b4 2 \u2016X\u2217 \u2212X\u2217r \u20162\u2217. (14)\n(i) is from using RIP and splitting X\u2217\u2212X\u2217r into rank-r components X\u2217\u2212X\u2217r = \u2211n/r\u22121 j=1 X \u2217 jr+1:(j+1)r. (ii) follows\nfrom using RIP (5). (iii) follows from \u2211 j \u2016X\u2217jr+1:(j+1)r\u2016F \u2264 \u2016X\n\u2217 \u2212X\u2217r \u2016\u2217. The Lemma now follows by combining equations (13), (14) and using RIP (3).\nHence from the above optimality conditions we get the proof of Theorem 3.4.\nProof of Theorem 3.4. Assuming UU> 6= U\u2217rU\u2217r >, from Lemma 4.4 we know,\nr\u2211 j=1 \u2016U\u2206>j \u20162F \u2264 1 8 \u2016UU> \u2212U\u2217rU\u2217r >\u20162F + 34 8 \u2016(UU> \u2212U\u2217rU\u2217r >)QQ>\u20162F , (15)\nfor some orthonormal R. Hence combining equations (15),with Lemma C.2 we get,\n1\u2212 3\u03b4 2 \u2016X \u2212X\u2217r \u20162F \u2264 1 2 \u2016X\u2217 \u2212X\u2217r \u20162F + \u03b4 2 \u2016X\u2217 \u2212X\u2217r \u20162\u2217\n+ 2(1 + \u03b4)\n( 1\n8 \u2016X \u2212X\u2217r \u20162F +\n34\n8 \u2016(X \u2212X\u2217r )QQ>\u20162F\n) .\nThis implies,\n1\u2212 7\u03b4 4 \u2016X \u2212X\u2217r \u20162F \u2264 1 2 \u2016X\u2217 \u2212X\u2217r \u20162F + \u03b4 2 \u2016X\u2217 \u2212X\u2217r \u20162\u2217 + (1 + \u03b4) 17 2 \u2016(X \u2212X\u2217r )QQ>\u20162F . (16)\nFinally from Lemma C.1 we know,\n\u2016X \u2212X\u2217rQQ>\u20162F \u2264 ( \u03b4 \u2016X \u2212X\u2217r \u2016F + \u2016(X \u2217 \u2212X\u2217r )QQ>\u2016F + \u03b4\u2016X\u2217 \u2212X\u2217r \u2016\u2217 )2\n\u2264 11 10 \u2016(X\u2217 \u2212X\u2217r )QQ>\u20162F + 22\u03b42 \u2016X \u2212X\u2217r \u2016 2 F + 22\u03b4 2\u2016X\u2217 \u2212X\u2217r \u20162\u2217. (17)\nThe last inequality follows from just using 2ab \u2264 a2 + b2. Combining equations (16) and (17) gives,(\n1\u2212 7\u03b4 4 \u2212 17 \u2217 22(1 + \u03b4)\u03b4 2 2\n) \u2016X \u2212X\u2217r \u20162F \u2264 1\n2 \u2016X\u2217 \u2212X\u2217r \u20162F +\n( \u03b4\n2 +\n17 \u2217 22\u03b42\n2\n) \u2016X\u2217 \u2212X\u2217r \u20162\u2217\n+ (1 + \u03b4) 17 \u2217 11\n20 \u2016(X\u2217 \u2212X\u2217r )QQ>\u20162F\nSubstituting \u03b4 = 1100 gives,\n\u2016X \u2212X\u2217r \u20162F \u2264 5 2 \u2016X\u2217 \u2212X\u2217r \u20162F + 12\u03b4\u2016X\u2217 \u2212X\u2217r \u20162\u2217 + 10\u2016(X\u2217 \u2212X\u2217r )QQ>\u20162F .\n\u2264 13\u2016X\u2217 \u2212X\u2217r \u20162F + 12\u03b4\u2016X\u2217 \u2212X\u2217r \u20162\u2217."}, {"heading": "D Proofs for Section 3", "text": "In this section we present the proofs for the strict saddle theorem (Theorem 3.2) and the convergence guarantees (Theorem 3.3). The proofs use the Lemmas developed in Section 4 and the supporting Lemmas from Section E.\nProof of Theorem 3.2. From Lemma 4.3 we know that\nr\u2211 j=1 vec (\u2206j) > [ 1 m \u22072f(U) ] vec (\u2206j)\n= 1\nm m\u2211 i=1 ( r\u2211 j=1 4 \u2329 Ai,U\u2206 > j \u232a2 \u2212 2\u2329Ai,UU> \u2212U\u2217U\u2217>\u232a2 \u2264 4(1 + \u03b4)\nr\u2211 j=1 \u2016U\u2206>j \u20162F \u2212 2(1\u2212 \u03b4)\u2016UU> \u2212U\u2217U\u2217 >\u20162F , (18)\nwhere the last inequality follows from the RIP (3). Now applying Lemma 4.4 in equation (18) we get,\nr\u2211 j=1 vec (\u2206j) > [ 1 m \u22072f(U) ] vec (\u2206j)\n\u2264 (1 + \u03b4) ( 1\n2 \u2016UU> \u2212U\u2217U\u2217>\u20162F + 17\u2016(UU> \u2212U\u2217U\u2217\n>)QQ>\u20162F ) \u2212 2(1\u2212 \u03b4)\u2016UU> \u2212U\u2217U\u2217>\u20162F\n= 17(1 + \u03b4)\u2016(UU> \u2212U\u2217U\u2217>)QQ>\u20162F \u2212 (3\u2212 5\u03b4)\n2 \u2016UU> \u2212U\u2217U\u2217>\u20162F\n(i) \u2264 [ 17(1 + \u03b4)\u03b42 \u2212 (3\u2212 5\u03b4)\n2\n] \u2225\u2225\u2225UU> \u2212U\u2217U\u2217>\u2225\u2225\u22252 F\n(ii) \u2264 \u22121 \u00b7 \u2225\u2225\u2225UU> \u2212U\u2217U\u2217>\u2225\u2225\u22252\nF . (19)\n(i) follows from Lemma 4.2. (ii) follows from \u03b4 \u2264 1/10. Now notice that from lemma E.1\n\u2016X \u2212X\u2217\u20162F \u2265 2( \u221a\n2\u2212 1)\u2016(U \u2212U\u2217R)(U\u2217R)>\u20162F \u2265 2( \u221a\n2\u2212 1)\u03c3r(X\u2217)\u2016U \u2212U\u2217R\u20162F . (20)\nFinally notice that \u2206j = \u2206eje>j have orthogonal columns. Hence,\n\u03bbmin\n[ 1\nm \u22072(f(U ,V )) ] \u2264 1 \u2016U \u2212U\u2217R\u20162F r\u2211 j=1 vec (\u2206j) > [ 1 m \u22072f(U) ] vec (\u2206j)\n(i) \u2264 \u22121 \u2016U \u2212U\u2217R\u20162F \u2225\u2225\u2225UU> \u2212U\u2217U\u2217>\u2225\u2225\u22252 F\n(ii) \u2264 \u22122( \u221a\n2\u2212 1)\u03c3r(X\u2217)\u2016U \u2212U\u2217R\u20162F \u2016U \u2212U\u2217R\u20162F\n\u2264 \u22124 5 \u03c3r(X \u2217).\n(i) follows from equation (19). (ii) follows from equation (20).\nProof of Theorem 3.3. To prove this theorem we use Theorem 6 of Ge et al. [10]. We need to show that f(U) satisfies, 1) strict saddle property, 2) local strong convexity, 3) f is bounded, smooth and has Lipschitz Hessian.\nThe boundedness assumption easily follows from assuming we are optimizing over a bounded domain b such that, \u2016U\u2217\u2016F \u2264 b. Note that we can have any reasonable upper bound on the optimum and we can easily estimate this from\u2211 i y 2 i which is \u2265 (1\u2212 \u03b4)\u2016X\u2217\u20162F for the noiseless case.\nFinally all the calculations below are for scaled version of f(x) by 1m . Note that this does not change the number of iterations as both smoothness and strong convexity parameters are scaled by the same constant.\nSmoothness constant \u03b2: Recall that smoothness of f is bounded by maximum eigenvalue of Hessian over the domain. Hence, \u03b2 = maxZ:\u2016Z\u2016F\u22641 Z\n>\u22072f(U)Z. We have computed this projection of Hessian in Lemma B.2. Hence,\n\u03b2 = 2 max Z:\u2016Z\u20162F\u22641 m\u2211 i=1 [ 2 \u2329 Ai,UZ >\u232a2 + \u2329Ai,UU> \u2212U\u2217U\u2217>\u232a \u2329Ai,ZZ>\u232a ] (i)\n\u2264 max Z:\u2016Z\u20162F\u22641\n2 ( 2(1 + \u03b4)\u2016U\u20162F \u2016Z\u20162F + (1 + \u03b4)\u2016X \u2212X\u2217\u2016F \u2016ZZ>\u2016F ) \u2264 4(1 + \u03b4)b2 + (1 + \u03b4)2b \u2264 5b2 + 3b.\n(i) follows from the RIP. \u03c1- Lipschitz Hessian: Now we will compute the Lipschitz constant of Hessian of f(U). We will first bound the spectral norm of difference of Hessian at two points U , V in terms of \u2016U \u2212 V \u2016F along orthogonal direction Zi and combine them to get bound on \u03c1.. Given two n\u00d7 r matrices U ,V ,\u2329\n\u22072f(U)\u2212\u22072f(V ),ZZ> \u232a\n\u2264 2 max Z:\u2016Z\u20162F\u22641 m\u2211 i=1 [ 2 \u2329 Ai,UZ >\u232a2 + \u2329Ai,UU> \u2212U\u2217U\u2217>\u232a \u2329Ai,ZZ>\u232a ]\n\u2212 m\u2211 i=1 [ 2 \u2329 Ai,V Z >\u232a2 + \u2329Ai,V V > \u2212U\u2217U\u2217>\u232a \u2329Ai,ZZ>\u232a ] \u2264 4(1 + \u03b4)(\u2016UZ>\u20162F \u2212 \u2016V Z>\u20162F ) + 2(1 + \u03b4)\u2016UU>V V >\u2016F \u2016ZZ>\u2016F \u2264 4(1 + \u03b4)\u2016Z\u20162F (\u2016U \u2212 V \u20162F + 2\u2016U\u2016F \u2016U \u2212 V \u2016F ) + 2(1 + \u03b4)\u2016UU>V V >\u2016F \u2264 \u2016Z\u20162F \u2016U \u2212 V \u2016F (8(1 + \u03b4)b+ 4(1 + \u03b4)b) = \u2016Z\u20162F \u2016U \u2212 V \u2016F (12(1 + \u03b4)b) . (21)\nHence, using the variational characterization of the Frobenius norm, the Hessian Lipschitz constant is bounded by max {Zi} \u2211 i \u2329 \u22072f(U)\u2212\u22072f(V ),ZiZ>i \u232a , where Zi are orthogonal with \u2211 i \u2016Zi\u20162F \u2264 1. Hence from equation (21) we get \u03c1 = O(b).\nStrict saddle property: So far we have shown regularity properties of f(U). Now we will discuss the strict saddle property. Theorem 3.2 shows that \u03bbmin [ \u22072(f(U)) ] \u2264 \u221225 \u03c3r(X\n\u2217). To use results of [10] we need to show this property over an neighborhood of any saddle point U . For this first recall by smoothness, \u2016\u2207f(U)\u2212\u2207f(V )\u2016F \u2264 \u03b2\u2016U\u2212V \u2016F . Therefore \u2207f(V ) \u2264 , when \u2016U \u2212 V \u2016F \u2264 \u03b2 . Further we know the Hessian spectral norm is \u03c1 Lipschitz from equation (21). Hence, for any direction Z,\nZ> ( \u22072(f(V ))\u2212\u22072(f(U)) ) Z> \u2264 \u03c1\u2016U \u2212 V \u2016F \u2264 \u03c1\n\u03b2 .\nIn particular choosing Z to be the projection direction, U \u2212U\u2217 implies from Theorem 3.2,\nZ> ( \u22072(f(V )) ) Z> \u2264 \u22122\n5 \u03c3r(X\n\u2217) + \u03c1\n\u03b2 .\nHence for all V in the bowl of radius around U , where \u2264 \u03b25\u03c1\u03c3r(X \u2217),\n\u03bbmin [ \u22072(f(V )) ] \u2264 \u22121\n5 \u03c3r(X\n\u2217). (22)\nLocal strong convexity: Finally we need to show that the function is \u03b1 strongly convex in a neighborhood \u03b8 around the optimum U\u2217R, for any orthonormal R. This easily follows from existing local convergence results for this problem. For example, Lemma 6.1 of Bhojanapalli et al. [2] states that, for \u2016U \u2212U\u2217R\u2016F \u2264 \u03c3r(X \u2217) 200\u03c31(X\u2217) \u03c3r(U \u2217R),\n\u3008\u2207f(U),U \u2212U\u2217R\u3009 \u2265 2 3 \u03b7\u2016\u2207f(U)\u20162F + 27 200 \u03c3r(U \u2217R)2\u2016U \u2212U\u2217R\u20162F . (23)\nfor \u03b4 = 110 and some step size \u03b7 \u221d 1 \u2016X\u2217\u20162 . Hence f(U) is locally strong convex with \u03b1 = 27 200\u03c3r(U \u2217R)2 in the neighborhood of radius \u03b8 = \u03c3r(X \u2217)\n200\u03c31(X\u2217) \u03c3r(U \u2217R) around the optimum. Substituting these parameters in the Theorem 6 of Ge et al. [10] gives the result."}, {"heading": "E Supporting Lemmas", "text": "In this section we present the supporting results used in the proofs above.\nLemma (4.4). Let U and U\u2217 be two n\u00d7 r matrices, and Q is an orthonormal matrix that spans the column space of U . Then there exists an r \u00d7 r orthonormal matrix R such that for any first order stationary point U of f(U), the following holds:\nr\u2211 j=1 \u2016Ueje>j (U \u2212U\u2217R)>\u20162F \u2264 1 8 \u2016UU> \u2212U\u2217U\u2217>\u20162F + 34 8 \u2016(UU> \u2212U\u2217U\u2217>)QQ>\u20162F .\nProof of Lemma 4.4. To prove this we will expand terms on the both sides in terms of U and \u2206 = U \u2212 U\u2217R and then compare. First notice the following properties of R that minimizes \u2016U\u2217R \u2212 U\u2016F . Let LSP> be the SVD of U\u2217>U . Then, R = LP>. Hence, R>U\u2217>U = PSP> = U>U\u2217R is a PSD matrix. This implies, U>\u2206 = U>U \u2212U>U\u2217R = U>U \u2212R>U\u2217>U = \u2206>U .\nLet columns of U be orthogonal, else we can multiply U by an orthonormal matrix and UR will satisfy this. Since UR is also local minimum, and UU> = URR>U>, results for UR will also hold for U . Let Q be the orthonormal matrix that spans the column space of U and Q\u22a5Q>\u22a5 = I \u2212QQ>. Similarly let Qj span Ueje>j . Note that Qj are orthonormal since columns of U are orthogonal. Hence,\n\u2016(U \u2212U\u2217R)eje>j U>\u20162F = \u2016Ueje>j U> \u2212QjQ>j U\u2217Reje>j U> \u2212Qj\u22a5Q>j\u22a5U\u2217Reje>j U>\u20162F = \u2016Ueje>j U> \u2212QjQ>j U\u2217Reje>j U>\u20162F + \u2016Qj\u22a5Q>j\u22a5U\u2217Reje>j U>\u20162F\n\u2264 \u2016Ueje>j U> \u2212QjQ>j U\u2217Reje>j (QjQ>j U\u2217R)>\u20162F\n2( \u221a 2\u2212 1) + \u2016Qj\u22a5Q>j\u22a5U\u2217Reje>j U>\u20162F .\n(24)\nThe last inequality follows from Lemma E.1 and the fact that e>j U >U\u2217Rej \u2265 0,\u2200j as U>U\u2217R is PSD. Now we will bound the second term in the above equation. The main idea here is to split this term into error between the subspaces of X,X\u2217 and then error between their singular values, since both of them are bounded by distance \u2016X \u2212X\u2217QQ>\u2016F . Let Q\u2217 be an orthonormal matrix that spans the column space of X\u2217. Also let X = Q\u03a32UQ >.\n\u2016Qj\u22a5Q>j\u22a5U\u2217Reje>j U>\u20162F = trace(e>j R>U\u2217 >Qj\u22a5Q > j\u22a5U \u2217Reje > j U >Uej) = trace ( e>j R >U\u2217>Qj\u22a5Q > j\u22a5U \u2217Rej [ e>j U >Uej \u2212 e>j R>U\u2217 >QjQ > j QjQ > j U \u2217Rej + e > j R >U\u2217>QjQ > j U \u2217Rej ]) (i)\n\u2264 1 8 (e>j R >U\u2217>Qj\u22a5Q > j\u22a5U \u2217Rej) 2\ufe38 \ufe37\ufe37 \ufe38\nterm1\n+2 (e>j U >Uej \u2212 e>j R>U\u2217 >QjQ > j U \u2217Rej) 2\ufe38 \ufe37\ufe37 \ufe38\nterm2\n+ (Qj\u22a5Q > j\u22a5U \u2217Reje > j (QjQ > j U \u2217R)>)2\ufe38 \ufe37\ufe37 \ufe38 term3 . (25)\nwhere (i) follows from Cauchy-Schwarz inequality.\nWe will use the following inequality through the rest of the proof. So we state it first for any matrix T . r\u2211 j=1 (e>j T >T ej) 2 \u2264 r\u2211 j=1 r\u2211 k=1 (e>j T >T ek) 2\n= r\u2211 j=1 e>j T >\n[ r\u2211\nk=1\nT eke > k T > ] T ej = r\u2211 j=1 e>j T >TT>T ej\n= \u2016T>T \u20162F = \u2016TT>\u20162F . (26)\nNow we will bound each of the terms in equation . Term 1: Let, T = Qj\u22a5Q>j\u22a5U \u2217R. Then applying inequaltiy from equation (26) we get,\nr\u2211 j=1 (e>j R >U\u2217>Qj\u22a5Q > j\u22a5U \u2217Rej) 2 = r\u2211 j=1 (e>j T >T ej) 2\n\u2264 \u2016T>T \u20162F = \u2016R>U\u2217 >Q\u22a5Q > \u22a5U \u2217R\u20162F . (27)\nFurther,\n\u2016R>U\u2217>Q\u22a5Q>\u22a5U\u2217R\u20162F = trace(U\u2217 >Q\u22a5Q > \u22a5U \u2217U\u2217>Q\u22a5Q > \u22a5U \u2217)\n= trace(Q\u22a5Q > \u22a5X \u2217Q\u22a5Q > \u22a5X \u2217) \u2264 \u2016Q\u22a5Q>\u22a5X\u2217\u20162F \u2264 \u2016X \u2212X\u2217\u20162F . (28)\nTerm 2:\n(e>j U >Uej \u2212 e>j R>U\u2217 >QjQ > j U \u2217Rej) 2\n= (e>j U >Uej) 2 + (e>j R >U\u2217>QjQ > j U \u2217Rej) 2 \u2212 2e>j U>Ueje>j R>U\u2217 >QjQ > j U \u2217Rej = \u2016Ueje>j U>\u20162F + \u2016QjQ>j U\u2217Reje>j R>U\u2217 >QjQ > j \u20162F \u2212 2 trace(e>j U>Ueje>j R>U\u2217 >QjQ > j U \u2217Rej)\n(i) = \u2016Ueje>j U>\u20162F + \u2016QjQ>j U\u2217Reje>j R>U\u2217 >QjQ > j \u20162F \u2212 2 trace(e>j R>U\u2217 >Ueje > j U >U\u2217Rej) = \u2016Ueje>j U> \u2212QjQ>j U\u2217Reje>j R>U\u2217 >QjQ > j \u20162F . (29)\n(i) follows from e>j U >Uej = \u2016Uj\u20162F and \u2016Uj\u20162FQjQ>j = Ueje>j U>. Now from orthogonality of Qj we have,\nr\u2211 j=1 \u2016Ueje>j U> \u2212QjQ>j U\u2217Reje>j R>U\u2217 >QjQ > j \u20162F \u2264 \u2016UU> \u2212QQ>U\u2217U\u2217 >QQ>\u20162F . (30)\nTerm 3: Finally we bound the last term in equation (25) similar to the first term, which gives,\nr\u2211 j=1 (Qj\u22a5Q > j\u22a5U \u2217Reje > j (QjQ > j U \u2217R)>)2 \u2264 \u2016UU> \u2212U\u2217U\u2217>QQ>\u20162F .\nSubstituting the above equations (27), (28), (29) and (30) in (24) and (25) gives the result.\nThe following lemma relates the error \u2016(U \u2212 Y )U>\u2016F with \u2016UU> \u2212 Y Y >\u2016F under some conditions on U and Y . This is a generalization of Lemma 5.4 in [26] and the proof follows similarly.\nLemma E.1. Let U and Y be two n\u00d7 r matrices. Further let U>Y = Y >U be a PSD matrix. Then,\n\u2016(U \u2212 Y )U>\u20162F \u2264 1\n2( \u221a 2\u2212 1) \u2016UU> \u2212 Y Y >\u20162F .\nProof. To prove this we will expand terms on the both sides in terms of U and \u2206 = U \u2212 Y and then compare.\n\u2016(UU> \u2212 Y Y >\u20162F = \u2016(U\u2206> + \u2206U> \u2212\u2206\u2206>)\u20162F = trace ( \u2206U>U\u2206> + U\u2206>\u2206U> + \u2206\u2206>\u2206\u2206> + 2\u2206U>\u2206U> \u2212 2\u2206\u2206>\u2206U> \u2212 2\u2206\u2206>U\u2206> ) (i) = trace ( 2U>U\u2206>\u2206 + (\u2206>\u2206)2 + 2(U>\u2206)2 \u2212 4\u2206>\u2206U>\u2206\n) (ii) = trace ( 2U>U\u2206>\u2206 + (\u2206>\u2206\u2212 \u221a 2U>\u2206)2 \u2212 2(2\u2212 \u221a 2)\u2206>\u2206U>\u2206\n) (iii)\n\u2265 2 trace ([ U>U \u2212 (2\u2212 \u221a 2)U>\u2206 ] \u2206>\u2206 )\n= 2 trace ([ ( \u221a 2\u2212 1)U>U + (2\u2212 \u221a 2)U>Y ] \u2206>\u2206 )\n(iv) \u2265 2 trace ( ( \u221a 2\u2212 1)U>U\u2206>\u2206 ) .\n(i) follows from the following properties of trace: trace(AB) = trace(BA) and trace(A) = trace(A>). (ii) follows from completing the squares. (iii) follows from trace(A2) \u2265 0. (iv) follows from the hypothesis of the lemma (U>Y is PSD) and trace(AB) \u2265 0 for PSD matrices A and B.\nFinally notice that \u2016(U \u2212 Y )U>\u20162F = trace(U>U\u2206>\u2206). This completes the proof.\nWe recall the standard Gaussian random variable concentration here.\nLemma E.2. Let wi \u2248 N (0, \u03c3w), then\nm\u2211 i=1 wixi \u2264 2 \u221a log(n)\u03c3w\u2016x\u2016,\nwith probability \u2265 1\u2212 1n2 .\nProof. Recall E [etwi ] = e\u03c32wt2/2. Then by Markov\u2019s inequality, P ( \u2211m i=1 wixi \u2265 c\u2016x\u2016) \u2264 e\u03c3 2 w\u2016x\u2016 2t2/2 etc\u2016x\u2016 \u2264 e\u2212c\n2/2\u03c32w , by setting t = c\u03c32w\u2016x\u2016 . Choosing c = 2 \u221a log(n)\u03c3w completes the proof."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>We show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix<lb>recovery from incoherent linear measurements. With noisy measurements we show all local minima are very close to a<lb>global optimum. Together with a curvature bound at saddle points, this yields a polynomial time global convergence<lb>guarantee for stochastic gradient descent from random initialization.", "creator": "LaTeX with hyperref package"}}}