{"id": "1605.07262", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Measuring Neural Net Robustness with Constraints", "abstract": "despite having high accuracy, cumulative nets comprise usually shown to be susceptible to clinical examples, where a small perturbation to an input can cause it to is mislabeled. we propose specifications for applying the variability as such flow filter : provide a specific algorithm accurately approximating these metrics based around an accumulation of robustness outside a linear process. guidelines show all our metrics will be adaptive to measuring the robustness of deep probability flows with methods than the mnist with cifar - md datasets. objective findings covers more informative judgments of robustness metrics subsequent to experiments based on existing algorithms. furthermore, i note how existing advantages to improving robustness \" overfit \" to meaningful examples, using a specific algorithm. next, surveys show that external analysis can use used to additionally construct neural gate measurement characteristics according to the metrics that mechanisms share, but also relative to previously undisclosed improvements.", "histories": [["v1", "Tue, 24 May 2016 02:18:21 GMT  (719kb,D)", "http://arxiv.org/abs/1605.07262v1", null], ["v2", "Fri, 16 Jun 2017 11:58:51 GMT  (817kb,D)", "http://arxiv.org/abs/1605.07262v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["osbert bastani", "yani ioannou", "leonidas lampropoulos", "dimitrios vytiniotis", "aditya v nori", "antonio criminisi"], "accepted": true, "id": "1605.07262"}, "pdf": {"name": "1605.07262.pdf", "metadata": {"source": "CRF", "title": "Measuring Neural Net Robustness with Constraints", "authors": ["Osbert Bastani", "Yani Ioannou", "Leonidas Lampropoulos"], "emails": ["obastani@cs.stanford.edu", "yai20@cam.ac.uk", "llamp@seas.upenn.edu", "dimitris@microsoft.com", "adityan@microsoft.com", "antcrim@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Recent work [20] shows that it is often possible to construct an input mislabeled by a neural net by perturbing a correctly labeled input by a tiny amount in a carefully chosen direction. Lack of robustness can be problematic in a variety of settings, such as changing camera lens or lighting conditions, successive frames in a video, or adversarial attacks in security-critical applications [17].\nA number of approaches have since been proposed to improve robustness [6, 5, 1, 7, 19]. However, work in this direction has been handicapped by the lack of objective measures of robustness. A typical approach to improving the robustness of a neural net f is to use an algorithm A to find adversarial examples, augment the training set with these examples, and train a new neural net f \u2032 [5]. Then, robustness is evaluated by using the same algorithm A to find adversarial examples for f \u2032\u2014if A discovers fewer adversarial examples for f \u2032 than for f , then f \u2032 is concluded to be more robust than f . However, f \u2032 may have overfit to adversarial examples generated by A\u2014in particular, there is no reason why a different algorithm A\u2032 may find fewer adversarial examples for f \u2032 than for f . Having an objective robustness measure is vital not only to reliably compare different algorithms, but also to understand robustness of production neural nets\u2014e.g., when deploying a login system based on face recognition, a security team may need to evaluate the risk of an attack using adversarial examples.\nIn this paper, we study the problem of measuring robustness. We propose to use two statistics of the robustness \u03c1(f,x\u2217) of f at point x\u2217 (i.e., the L\u221e distance from x\u2217 to the nearest adversarial example) [20]. The first one measures the frequency with which adversarial examples occur; the other measures the severity of such adversarial examples. Both statistics depend on a parameter , which intuitively specifies the threshold below which adversarial examples should not exist (i.e., points x with L\u221e distance to x\u2217 less than should be assigned the same label as x\u2217).\nar X\niv :1\n60 5.\n07 26\n2v 1\n[ cs\n.L G\n] 2\n4 M\nThe key challenge is efficiently computing \u03c1(f,x\u2217). We give an exact formulation of this problem as an intractable optimization problem. To recover tractability, we approximate this optimization problem by constraining the search to a convex region Z(x\u2217) around x\u2217. Furthermore, we devise an iterative approach to solving the resulting linear program that produces an order of magnitude speed-up. Common neural nets (specifically, those using rectified linear units as activation functions) are in fact piecewise linear functions [15]; we choose Z(x\u2217) to be the region around x\u2217 on which f is linear. Since the linear nature of neural nets is often the cause of adversarial examples [5], our choice of Z(x\u2217) focuses the search where adversarial examples are most likely to exist. We evaluate our approach on a deep convolutional neural network f for MNIST. We estimate \u03c1(f,x\u2217) using both our algorithm ALP and (as a baseline) the algorithm AL-BFGS introduced by [20]. We show that ALP produces a substantially more accurate estimate of \u03c1(f,x\u2217) than AL-BFGS. We then use data augmentation with each algorithm to improve the robustness of f , resulting in fine-tuned neural nets fLP and fL-BFGS. According to AL-BFGS, fL-BFGS is more robust than f , but not according to ALP. In other words, fL-BFGS overfits to adversarial examples computed using AL-BFGS. In contrast, fLP is more robust according to both AL-BFGS and ALP. Furthermore, to demonstrate scalability, we apply our approach to evaluate the robustness of the 23-layer network-in-network (NiN) neural net [13] for CIFAR-10, and reveal a surprising lack of robustness. We fine-tune NiN and show that robustness improves, albeit only by a small amount. In summary, our contributions are:\n\u2022 We formalize the notion of pointwise robustness studied in previous work [5, 20, 6] and propose two statistics for measuring robustness based on this notion (\u00a72).\n\u2022 We show how computing pointwise robustness can be encoded as a constraint system (\u00a73). We approximate this constraint system with a tractable linear program and devise an optimization for solving this linear program an order of magnitude faster (\u00a74).\n\u2022 We demonstrate experimentally that our algorithm produces substantially more accurate measures of robustness compared to algorithms based on previous work, and show evidence that neural nets fine-tuned to improve robustness (\u00a75) can overfit to adversarial examples identified by a specific algorithm (\u00a76)."}, {"heading": "1.1 Related work", "text": "The susceptibility of neural nets to adversarial examples was discovered by [20]. Given a test point x\u2217 with predicted label `\u2217, an adversarial example is an input x\u2217 + r with predicted label ` 6= `\u2217 where the adversarial perturbation r is small (in L\u221e norm). Then, [20] devises an approximate algorithm for finding the smallest possible adversarial perturbation r. Their approach is to minimize the combined objective loss(f(x\u2217 + r), `) + c\u2016r\u2016\u221e, which is an instance of box-constrained convex optimization that can be solved using L-BFGS-B. The constant c is optimized using line search.\nOur formalization of the robustness \u03c1(f,x\u2217) of f at x\u2217 corresponds to the notion in [20] of finding the minimal \u2016r\u2016\u221e. We propose an exact algorithm for computing \u03c1(f,x\u2217) as well as a tractable approximation. The algorithm in [20] can also be used to approximate \u03c1(f,x\u2217); we show experimentally that our algorithm is substantially more accurate than [20].\nThere has been a range of subsequent work studying robustness; [16] devises an algorithm for finding purely synthetic adversarial examples (i.e., no initial image x\u2217), [21] searches for adversarial examples using random perturbations, showing that adversarial examples in fact exist in large regions of the pixel space, [18] shows that even intermediate layers of neural nets are not robust to adversarial noise, and [3] seeks to explain why neural nets may generalize well despite poor robustness properties.\nStarting with [5], a major focus has been on devising faster algorithms for finding adversarial examples. Their idea is that adversarial examples can then be computed on-the-fly and used as training examples, analogous to data augmentation approaches typically used to train neural nets [10]. To find adversarial examples quickly, [5] chooses the adversarial perturbation r to be in the direction of the signed gradient of loss(f(x\u2217 + r), `) with fixed magnitude. Intuitively, given only the gradient of the loss function, this choice of r is most likely to produce an adversarial example with \u2016r\u2016\u221e \u2264 . In this direction, [7] extends this idea to norms beyond the L\u221e norm, [6] takes the approach of [20] but fixes c, and [19] formalizes [5] as robust optimization.\nA key shortcoming of these lines of work is that robustness is typically measured using the same algorithm used to find adversarial examples, in which case the resulting neural net may have overfit to adversarial examples generating using that algorithm. For example, [5] shows improved accuracy\nto adversarial examples generated using their own signed gradient method, but do not consider whether robustness increases for adversarial examples generated using more precise approaches such as [20]. Similarly, [7] compares accuracy to adversarial examples generated using both itself and [5] (but not [20]), and [19] only considers accuracy on adversarial examples generated using their own approach on the baseline network. The aim of our paper is to provide metrics for evaluating robustness, and to demonstrate the importance of using such impartial measures to compare robustness.\nAdditionally, there has been work on designing neural network architectures [6] and learning procedures [1] that improve robustness to adversarial perturbations, though they do not obtain state-of-theart accuracy on the unperturbed test sets. There has also been work using smoothness regularization related to [5] to train neural nets, focusing on improving accuracy rather than robustness [14].\nRobustness has also been studied in more general contexts; [22] studies the connection between robustness and generalization, [2] establishes theoretical lower bounds on the robustness of linear and quadratic classifiers, and [4] seeks to improve robustness by promoting resiliance to deleting features during training. More broadly, robustness has been identified as a desirable property of classifiers beyond prediction accuracy. Traditional metrics such as (out-of-sample) accuracy, precision, and recall help users assess prediction accuracy of trained models; our work aims to develop analogous metrics for assessing robustness."}, {"heading": "2 Robustness Metrics", "text": "Consider a classifier f : X \u2192 L, where X \u2286 Rn is the input space and L = {1, ..., L} are the labels. We assume that training and test points x \u2208 X have distribution D. We first formalize the notion of robustness at a point, and then describe two statistics to measure robustness. Our two statistics depend on a parameter , which captures the idea that we only care about robustness below a certain threshold\u2014we disregard adversarial examples x whose L\u221e distance to x\u2217 is greater than . We use = 20 in our experiments on MNIST and CIFAR-10 (on the pixel scale 0-255).\nPointwise robustness. Intuitively, f is robust at x\u2217 \u2208 X if a \u201csmall\u201d perturbation to x\u2217 does not affect the assigned label. We are interested in perturbations sufficiently small that they do not affect human classification; an established condition is \u2016x\u2212 x\u2217\u2016\u221e \u2264 for some parameter . Formally, we say f is (x\u2217, )-robust if for every x such that \u2016x\u2212x\u2217\u2016\u221e \u2264 , f(x) = f(x\u2217). Finally, the pointwise robustness \u03c1(f,x\u2217) of f at x\u2217 is the minimum for which f fails to be (x\u2217, )-robust:\n\u03c1(f,x\u2217) def = inf{ \u2265 0 | f is not (x\u2217, )-robust}. (1)\nThis definition formalizes the notion of robustness in [5, 6, 20].\nAdversarial frequency. Given a parameter , the adversarial frequency\n\u03c6(f, ) def = Prx\u2217\u223cD[\u03c1(f,x\u2217) \u2264 ]\nmeasures how often f fails to be (x\u2217, )-robust. In other words, if f has high adversarial frequency, then it fails to be (x\u2217, )-robust for many inputs x\u2217.\nAdversarial severity. Given a parameter , the adversarial severity\n\u00b5(f, ) def = Ex\u2217\u223cD[\u03c1(f,x\u2217) | \u03c1(f,x\u2217) \u2264 ]\nmeasures the severity with which f fails to be robust at x\u2217 conditioned on f not being (x\u2217, )-robust. We condition on pointwise robustness since once f is (x\u2217, )-robust at x\u2217, then the degree to which f is robust at x\u2217 does not matter. Smaller \u00b5(f, ) corresponds to worse adversarial severity, since f is more susceptible to adversarial examples if the distances to the nearest adversarial example are small.\nThe frequency and severity capture different robustness behaviors. A neural net may have high adversarial frequency but low adversarial severity, indicating that most adversarial examples are about distance away from the original point x\u2217. Conversely, a neural net may have low adversarial frequency but high adversarial severity, indicating that it is typically robust, but occasionally severely fails to be robust. Frequency is typically the more important metric, since a neural net with low adversarial frequency is robust most of the time. Indeed, adversarial frequency corresponds to the\naccuracy on adversarial examples used to measure robustness in [5, 19]. Severity can be used to differentiate between neural nets with similar adversarial frequency.\nGiven a set of samples X \u2286 X drawn i.i.d. from D, we can estimate \u03c6(f, ) and \u00b5(f, ) as follows:\n\u03c6\u0302(f, ,X) def = |{x\u2217 \u2208 X | \u03c1(f,x\u2217) \u2264 }|\n|X|\n\u00b5\u0302(f, ,X) def = \u2211 x\u2217\u2208X \u03c1(f,x\u2217)I[\u03c1(f,x\u2217) \u2264 ] |{x\u2217 \u2208 X | \u03c1(f,x\u2217) \u2264 }| .\nIn practice, X = Xtest is taken to be the test set."}, {"heading": "3 Computing Pointwise Robustness", "text": ""}, {"heading": "3.1 Overview", "text": "Consider the training points in Figure 1 (a) colored based on the ground truth label. To classify this data, we train a two-layer neural net f(x) = argmax`{(W2g(W1x))`}, where the ReLU function g is applied pointwise. Figure 1 (a) includes contours of the per-point loss function of this neural net.\nExhaustively searching the input space to determine the distance \u03c1(f,x\u2217) to the nearest adversarial example for input x\u2217 (labeled `\u2217) is intractable. Recall that neural nets with rectified-linear (ReLU) units as activations are piecewise linear [15]. Since adversarial examples exists because of this linearity in the neural net [5], we restrict our search to the region Z(x\u2217) around x\u2217 on which the neural net is linear. This region around x\u2217 is defined by the activation of the ReLU function: for each i, if (W1x\u2217)i \u2265 0 (resp., (W1x\u2217) \u2264 0), we constrain to the half-space {x | (W1x)i \u2265 0} (resp., {x | (W1x)i \u2264 0}). The intersection of these half-spaces is convex, so it admits efficient search. Figure 1 (b) shows one such convex region 1.\nAdditionally, x is labeled ` exactly when f(x)` \u2265 f(x)`\u2032 for each `\u2032 6= `. These constraints are linear since f is linear on Z(x\u2217). Therefore, we can find the distance to the nearest input with label ` 6= `\u2217 by minimizing \u2016x \u2212 x\u2217\u2016\u221e on Z(x\u2217). Finally, we can perform this search for each label ` 6= `\u2217, though for efficiency we take ` to be the label assigned the second-highest score by f . Figure 1 (b) shows the adversarial example found by our algorithm in our running example. In Figure 1 note that the direction of the nearest adversarial example is not necessary aligned with the signed gradient of the loss function, as observed by others [7]."}, {"heading": "3.2 Formulation as Optimization", "text": "We compute \u03c1(f, ) by expressing (1) as constraints C, which consist of 1Our neural net has 8 hidden units, but for this x\u2217, 6 of the half-spaces entirely contain the convex region.\n\u2022 Linear relations; specifically, inequalities C \u2261 (wTx+ b \u2265 0) and equalities C \u2261 (wTx+ b = 0), where x \u2208 Rm (for some m) are variables and w \u2208 Rm, b \u2208 R are constants.\n\u2022 Conjunctions C \u2261 C1 \u2227 C2, where C1 and C2 are themselves constraints. Both constraints must be satisfied for the conjunction to be satisfied.\n\u2022 Disjunctions C \u2261 C1\u2228C2, where C1 and C2 are themselves constraints. One of the constraints must be satisfied for the disjunction to be satisfied.\nThe feasible set F(C) of C is the set of x \u2208 Rm that satisfy C; C is satisfiable if F(C) is nonempty. In the next section, we show that the condition f(x) = ` can be expressed as constraints Cf (x, `); i.e., f(x) = ` if and only if Cf (x, `) is satisfiable. Then, \u03c1(f, ) can be computed as follows:\n\u03c1(f,x\u2217) = min 6\u0300=`\u2217 \u03c1(f,x\u2217, `) (2)\n\u03c1(f,x\u2217, `) def = inf{ \u2265 0 | Cf (x, `) \u2227 \u2016x\u2212 x\u2217\u2016\u221e \u2264 satisfiable}. (3)\nThe optimization problem is typically intractable; we discuss a tractable approximation in \u00a74."}, {"heading": "3.3 Encoding a Neural Network", "text": "We show how to encode the constraint f(x) = ` as constraints Cf (x, `) when when f is a neural net. We assume f has form f(x) = argmax`\u2208L {[ f (k)(f (k\u22121)(...(f (1)(x))...)) ] ` } , where the ith layer of the network is a function f (i) : Rni\u22121 \u2192 Rni , with n0 = n and nk = |L|. We describe the encoding of fully-connected and ReLU layers; convolutional layers are encoded similarly to fully-connected layers, and max-pooling layers are encoded similarly to ReLU layers. We introduce the variables x(0), . . . ,x(k) into our constraints, with the interpretation that x(i) represents the output vector of layer i of the network; i.e., x(i) = f (i)(x(i\u22121)). The constraint Cin(x) \u2261 (x(0) = x) encodes the input layer. Then, for each layer f (i), we encode the computation of x(i) given x(i\u22121) as a constraint Ci.\nFully-connected layer. In this case, x(i) = f (i)(x(i\u22121)) = W (i)x(i\u22121) + b(i), which we encode using the constraints Ci \u2261 \u2227ni j=1 { x (i) j =W (i) j x (i\u22121) + b (i) j } , where W (i)j is the j-th row of W (i).\nReLU layer. In this case, x(i)j = max {x (i\u22121) j , 0} (for each 1 \u2264 j \u2264 ni), which we encode using the constraints Ci \u2261 \u2227ni j=1 Cij , where Cij = (x (i\u22121) j <0 \u2227 x (i) j =0) \u2228 (x (i\u22121) j \u2265 0 \u2227 x (i) j =x (i\u22121) j ).\nFinally, the constraints Cout(`) \u2261 \u2227\n`\u2032 6=` { x (k) ` \u2265 x (k) `\u2032 } ensure that the output label is `. Together,\nthe constraints Cf (x, `) \u2261 Cin(x) \u2227 (\u2227k i=1 Ci ) \u2227 Cout(`) encodes the computation of f :\nTheorem 1 For any x \u2208 X and ` \u2208 L, we have f(x) = ` if and only if Cf (x, `) is satisfiable."}, {"heading": "4 Approximate Computation of Pointwise Robustness", "text": "Convex restriction. The challenge to solving (3) is the non-convexity of the feasible set of Cf (x, `). To recover tractability, we approximate (3) by constraining the feasible set to x \u2208 Z(x\u2217), where Z(x\u2217) \u2286 X is carefully chosen so that the constraints C\u0302f (x, `) \u2261 Cf (x, `) \u2227 (x \u2208 Z(x\u2217)) have convex feasible set. We call C\u0302f (x, `) the convex restriction of Cf (x, `). In some sense, convex restriction is the opposite of convex relaxation. Then, we can approximately compute robustness:\n\u03c1\u0302(f,x\u2217, `) def = inf{ \u2265 0 | C\u0302f (x, `) \u2227 \u2016x\u2212 x\u2217\u2016\u221e \u2264 satisfiable}. (4)\nThe objective is optimized over x \u2208 Z(x\u2217), which approximates the optimum over x \u2208 X .\nChoice of Z(x\u2217). We construct Z(x\u2217) as the feasible set of constraints D(x\u2217); i.e., Z(x\u2217) = F(D(x\u2217)). We now describe how to construct D(x\u2217).\nNote that F(wTx+ b = 0) and F(wTx+ b \u2265 0) are convex sets. Furthermore, if F(C1) and F(C2) are convex, then so is their conjunction F(C1 \u2227 C2). However, their disjunction F(C1 \u2228 C2) may not be convex; for example, F((x \u2265 0) \u2228 (y \u2265 0)). The potential non-convexity of disjunctions makes (3) difficult to optimize.\nWe can eliminate disjunction operations by choosing one of the two disjuncts to hold. For example, note that for C1 \u2261 C2 \u2228 C3, we have both F(C2) \u2286 F(C1) and F(C3) \u2286 F(C1). In other words, if we replace C1 with either C2 or C3, the feasible set of the resulting constraints can only become smaller. Taking D(x\u2217) \u2261 C2 (resp., D(x\u2217) \u2261 C3) effectively replaces C1 with C2 (resp., C3). To restrict (3), for every disjunction C1 \u2261 C2 \u2228 C3, we systematically choose either C2 or C3 to replace the C1 choice. In particular, we choose C2 if x\u2217 satisfies C2 (i.e., x\u2217 \u2208 F(C2)) and choose C3 otherwise. In our constraints, disjunctions are always mutually exclusive so x\u2217 never simultaneously satisfies both C2 and C3. Then, we take D(x\u2217) to be the conjunction of all our choices. The resulting constraints C\u0302f (x, `) contains only conjunctions of linear relations, so its feasible set is convex. In fact, it can be expressed as a linear program (LP) and can be solved using any standard LP solver.\nFor example, consider a rectified linear layer (as before, max pooling layers are similar). The original constraint added for unit j of rectified linear layer f (i) is(\nx (i\u22121) j \u2264 0 \u2227 x (i) j = 0 ) \u2228 ( x (i\u22121) j \u2265 0 \u2227 x (i) j = x (i\u22121) j ) To restrict this constraint, we evaluate the neural network on the seed input x\u2217 and look at the input to f (i), which equals x(i\u22121)\u2217 = f (i\u22121)(...(f (1)(x\u2217))...). Then, for each 1 \u2264 j \u2264 ni:\nD(x\u2217)\u2190 D(x\u2217) \u2227\n{ x (i\u22121) j \u2264 0 \u2227 x (i) j = x (i\u22121) j if (x (i\u22121) \u2217 )j \u2264 0\nx (i\u22121) j \u2265 0 \u2227 x (i) j = 0 if (x (i\u22121) \u2217 )j > 0.\nIterative constraint solving. We implement an optimization for solving LPs by lazily adding constraints as necessary. Given all constraints C, we start off solving the LP with the subset of equality constraints C\u0302 \u2286 C, which yields a (possibly infeasible) solution z. If z is feasible, then z is also an optimal solution to the original LP; otherwise, we add to C\u0302 the constraints in C that are not satisfied by z and repeat the process. This process always yields the correct solution, since in the worst case C\u0302 becomes equal to C. In practice, this optimization is an order of magnitude faster than directly solving the LP with constraints C.\nSingle target label. For simplicity, rather than minimize over \u03c1(f,x\u2217, `) for each ` 6= `\u2217, we fix ` to be the second most probable label f\u0303(x\u2217); i.e.,\n\u03c1\u0302(f,x\u2217) def = inf{ \u2265 0 | C\u0302f (x, f\u0303(x\u2217)) \u2227 \u2016x\u2212 x\u2217\u2016\u221e \u2264 satisfiable}. (5)\nApproximate robustness statistics. We can use \u03c1\u0302 in our statistics \u03c6\u0302 and \u00b5\u0302 defined in \u00a72. Because \u03c1\u0302 is an overapproximation (i.e., \u03c1\u0302(f,x\u2217) \u2265 \u03c1(f,x\u2217)), the estimates \u03c6\u0302 and \u00b5\u0302 may be biased (in particular, \u03c6\u0302(f, ) \u2264 \u03c6(f, )). In \u00a76, we show empirically that our algorithm produces substantially less biased estimates than those based on existing algorithms for finding adversarial examples."}, {"heading": "5 Improving Neural Net Robustness", "text": "Finding adversarial examples. We can use our algorithm for estimating \u03c1\u0302(f,x\u2217) to compute adversarial examples. Given x\u2217, the value of x computed by the optimization procedure used to solve (5) is an adversarial example for x\u2217 with \u2016x\u2212 x\u2217\u2016\u221e = \u03c1\u0302(f,x\u2217).\nFinetuning. We use fine-tuning to reduce a neural net\u2019s susceptability to adversarial examples. First, we use an algorithm A to compute adversarial examples for each x\u2217 \u2208 Xtrain and add them to the training set. Then, we continue training the network on a the augmented training set at a reduced training rate. We can repeat this process multiple rounds (denoted T ); at each round, we only consider x\u2217 in the original training set (rather than the augmented training set).\nRounding errors. MNIST images are represented as integers, so we must round the perturbation to obtain an image, which oftentimes results in non-adversarial examples. When fine-tuning, we add a constraint x(k)` \u2265 x (k) `\u2032 + \u03b1 for all `\n\u2032 6= `, which eliminates this problem by ensuring that the neural net has high confidence on its adversarial examples. In our experiments, we fix \u03b1 = 3.0.\nSimilarly, we modified the L-BFGS-B baseline so that during the line search over c, we only count x\u2217+r as adversarial if x (k) ` \u2265 x (k) `\u2032 +\u03b1 for all `\n\u2032 6= `. We choose \u03b1 = 0.15, since larger \u03b1 causes the baseline to find significantly fewer adversarial examples, and small \u03b1 results in smaller improvement in robustness. With this choice, rounding errors occur on 8.3% of the adversarial examples we find on the MNIST training set."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Adversarial Images for CIFAR-10 and MNIST", "text": "We find adversarial examples for the neural net LeNet [12] (modified to use ReLUs instead of sigmoids) trained to classify MNIST [11], and for the network-in-network (NiN) neural net [13] trained to classify CIFAR-10 [9]. Both neural nets are trained using Caffe [8]. For MNIST, Figure 2 (b) shows an adversarial example (labeled 1) we find for the image in Figure 2 (a) labeled 3, and Figure 2 (c) shows the corresponding adversarial perturbation scaled so the difference is visible (it has L\u221e norm 17). For CIFAR-10, Figure 2 (e) shows an adversarial example labeled \u201ctruck\u201d for the image in Figure 2 (d) labeled \u201cautomobile\u201d, and Figure 2 (f) shows the corresponding scaled adversarial perturbation (which has L\u221e norm 3)."}, {"heading": "6.2 Comparison to Other Algorithms on MNIST", "text": "We compare our algorithm for estimating \u03c1 to the baseline L-BFGS-B algorithm proposed by [20]. We use the tool provided by [21] to compute this baseline. For both algorithms, we use adversarial target label ` = f\u0303(x\u2217). We use LeNet in our comparisons, since we find that it is substantially more robust than the neural nets considered in most previous work (including [20]). We also use versions\nof LeNet fine-tuned using both our algorithm and the baseline with T = 1, 2. To focus on the most severe adversarial examples, we use a stricter threshold for robustness of = 20 pixels.\nWe performed a similar comparison to the signed gradient algorithm proposed by [5] (with the signed gradient multiplied by = 20 pixels). For LeNet, this algorithm found only one adversarial example on the MNIST test set (out of 10,000) and four adversarial examples on the MNIST training set (out of 60,000), so we omit results 2.\nResults. In Figure 3, we plot the number of test points x\u2217 for which \u03c1\u0302(f,x\u2217) \u2264 , as a function of , where \u03c1\u0302(f,x\u2217) is estimated using (a) the baseline and (b) our algorithm. These plots compare the robustness of each neural network as a function of . In Table 1, we show results evaluating the robustness of each neural net, including the adversarial frequency and the adversarial severity. The running time of our algorithm and the baseline algorithm are very similar; in both cases, computing \u03c1\u0302(f,x\u2217) for a single input x\u2217 takes about 1.5 seconds. For comparison, without our iterative constraint solving optimization, our algorithm took more than two minutes to run.\nDiscussion. For every neural net, our algorithm produces substantially higher estimates of the adversarial frequency. In other words, our algorithm estimates \u03c1\u0302(f,x\u2217) with substantially better accuracy compared to the baseline.\nAccording to the baseline metrics shown in Figure 3 (a), the baseline neural net (red) is similarly robust to our neural net (blue), and both are more robust than the original LeNet (black). Our neural net is actually more robust than the baseline neural net for smaller values of , whereas the baseline neural net eventually becomes slightly more robust (i.e., where the red line dips below the blue line). This behavior is captured by our robustness statistics\u2014the baseline neural net has lower adversarial frequency (so it has fewer adversarial examples with \u03c1\u0302(f,x\u2217) \u2264 ) but also has worse adversarial severity (since its adversarial examples are on average closer to the original points x\u2217).\nHowever, according to our metrics shown in Figure 3 (b), our neural net is substantially more robust than the baseline neural net. Again, this is reflected by our statistics\u2014our neural net has substantially lower adversarial frequency compared to the baseline neural net, while maintaining similar adversarial severity. Taken together, our results suggest that the baseline neural net is overfitting to the adversarial examples found by the baseline algorithm. In particular, the baseline neural net does not learn the adversarial examples found by our algorithm. On the other hand, our neural net learns both the adversarial examples found by our algorithm and those found by the baseline algorithm."}, {"heading": "6.3 Scaling to CIFAR-10", "text": "We also implemented our approach for the for the CIFAR-10 network-in-network (NiN) neural net [13], which obtains 91.31% test set accuracy. Computing \u03c1\u0302(f,x\u2217) for a single input on NiN takes about 10-15 seconds on an 8-core CPU. Unlike LeNet, NiN suffers severely from adversarial examples\u2014we measure a 61.5% adversarial frequency and an adversarial severity of 2.82 pixels. Our neural net (NiN fine-tuned using our algorithm and T = 1) has test set accuracy 90.35%, which is similar to the test set accuracy of the original NiN. As can be seen in Figure 3 (c), our neural net improves slightly in terms of robustness, especially for smaller . As before, these improvements are reflected in our metrics\u2014the adversarial frequency of our neural net drops slightly to 59.6%, and the adversarial severity improves to 3.88. Nevertheless, unlike LeNet, our fine-tuned version of NiN remains very prone to adversarial examples. In this case, we believe that new techniques are required to significantly improve robustness."}, {"heading": "7 Conclusion", "text": "We have shown how to formulate, efficiently estimate, and improve the robustness of neural nets using an encoding of the robustness property as a constraint system. Future work includes devising better approaches to improving robustness on large neural nets such as NiN and studying properties beyond robustness.\n2Futhermore, the signed gradient algorithm cannot be used to estimate adversarial severity since all the adversarial examples it finds have L\u221e norm ."}], "references": [{"title": "Visual causal feature learning", "author": ["K. Chalupka", "P. Perona", "F. Eberhardt"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Analysis of classifers\u2019 robustness to adversarial perturbations", "author": ["A. Fawzi", "O. Fawzi", "P. Frossard"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Ensemble robustness of deep learning algorithms", "author": ["Jiashi Feng", "Tom Zahavy", "Bingyi Kang", "Huan Xu", "Shie Mannor"], "venue": "arXiv preprint arXiv:1602.02389,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["Amir Globerson", "Sam Roweis"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S. Gu", "L. Rigazio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Learning with a strong adversary", "author": ["Ruitong Huang", "Bing Xu", "Dale Schuurmans", "Csaba Szepesv\u00e1ri"], "venue": "CoRR, abs/1511.03034,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Learning multiple layers of features from tiny", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Intelligent Signal Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Distributional smoothing with virtual adversarial training", "author": ["Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Ken Nakae", "Shin Ishii"], "venue": "stat, 1050:25,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F. Mont\u00fafar", "Razvan Pascanu", "KyungHyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["Nicolas Papernot", "Patrick McDaniel", "Ian Goodfellow", "Somesh Jha", "Z Berkay Celik", "Ananthram Swami"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Adversarial manipulation of deep representations", "author": ["Sara Sabour", "Yanshuai Cao", "Fartash Faghri", "David J Fleet"], "venue": "arXiv preprint arXiv:1511.05122,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Understanding adversarial training: Increasing local stability of neural nets through robust optimization", "author": ["Uri Shaham", "Yutaro Yamada", "Sahand Negahban"], "venue": "arXiv preprint arXiv:1511.05432,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Exploring the space of adversarial images", "author": ["Pedro Tabacof", "Eduardo Valle"], "venue": "CoRR, abs/1510.05328,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Robustness and generalization", "author": ["Huan Xu", "Shie Mannor"], "venue": "Machine learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "Recent work [20] shows that it is often possible to construct an input mislabeled by a neural net by perturbing a correctly labeled input by a tiny amount in a carefully chosen direction.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "Lack of robustness can be problematic in a variety of settings, such as changing camera lens or lighting conditions, successive frames in a video, or adversarial attacks in security-critical applications [17].", "startOffset": 204, "endOffset": 208}, {"referenceID": 5, "context": "A number of approaches have since been proposed to improve robustness [6, 5, 1, 7, 19].", "startOffset": 70, "endOffset": 86}, {"referenceID": 4, "context": "A number of approaches have since been proposed to improve robustness [6, 5, 1, 7, 19].", "startOffset": 70, "endOffset": 86}, {"referenceID": 0, "context": "A number of approaches have since been proposed to improve robustness [6, 5, 1, 7, 19].", "startOffset": 70, "endOffset": 86}, {"referenceID": 6, "context": "A number of approaches have since been proposed to improve robustness [6, 5, 1, 7, 19].", "startOffset": 70, "endOffset": 86}, {"referenceID": 17, "context": "A number of approaches have since been proposed to improve robustness [6, 5, 1, 7, 19].", "startOffset": 70, "endOffset": 86}, {"referenceID": 4, "context": "A typical approach to improving the robustness of a neural net f is to use an algorithm A to find adversarial examples, augment the training set with these examples, and train a new neural net f \u2032 [5].", "startOffset": 197, "endOffset": 200}, {"referenceID": 18, "context": ", the L\u221e distance from x\u2217 to the nearest adversarial example) [20].", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "Common neural nets (specifically, those using rectified linear units as activation functions) are in fact piecewise linear functions [15]; we choose Z(x\u2217) to be the region around x\u2217 on which f is linear.", "startOffset": 133, "endOffset": 137}, {"referenceID": 4, "context": "Since the linear nature of neural nets is often the cause of adversarial examples [5], our choice of Z(x\u2217) focuses the search where adversarial examples are most likely to exist.", "startOffset": 82, "endOffset": 85}, {"referenceID": 18, "context": "We estimate \u03c1(f,x\u2217) using both our algorithm ALP and (as a baseline) the algorithm AL-BFGS introduced by [20].", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "\u2022 We formalize the notion of pointwise robustness studied in previous work [5, 20, 6] and propose two statistics for measuring robustness based on this notion (\u00a72).", "startOffset": 75, "endOffset": 85}, {"referenceID": 18, "context": "\u2022 We formalize the notion of pointwise robustness studied in previous work [5, 20, 6] and propose two statistics for measuring robustness based on this notion (\u00a72).", "startOffset": 75, "endOffset": 85}, {"referenceID": 5, "context": "\u2022 We formalize the notion of pointwise robustness studied in previous work [5, 20, 6] and propose two statistics for measuring robustness based on this notion (\u00a72).", "startOffset": 75, "endOffset": 85}, {"referenceID": 18, "context": "The susceptibility of neural nets to adversarial examples was discovered by [20].", "startOffset": 76, "endOffset": 80}, {"referenceID": 18, "context": "Then, [20] devises an approximate algorithm for finding the smallest possible adversarial perturbation r.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "Our formalization of the robustness \u03c1(f,x\u2217) of f at x\u2217 corresponds to the notion in [20] of finding the minimal \u2016r\u2016\u221e.", "startOffset": 84, "endOffset": 88}, {"referenceID": 18, "context": "The algorithm in [20] can also be used to approximate \u03c1(f,x\u2217); we show experimentally that our algorithm is substantially more accurate than [20].", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "The algorithm in [20] can also be used to approximate \u03c1(f,x\u2217); we show experimentally that our algorithm is substantially more accurate than [20].", "startOffset": 141, "endOffset": 145}, {"referenceID": 14, "context": "There has been a range of subsequent work studying robustness; [16] devises an algorithm for finding purely synthetic adversarial examples (i.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": ", no initial image x\u2217), [21] searches for adversarial examples using random perturbations, showing that adversarial examples in fact exist in large regions of the pixel space, [18] shows that even intermediate layers of neural nets are not robust to adversarial noise, and [3] seeks to explain why neural nets may generalize well despite poor robustness properties.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": ", no initial image x\u2217), [21] searches for adversarial examples using random perturbations, showing that adversarial examples in fact exist in large regions of the pixel space, [18] shows that even intermediate layers of neural nets are not robust to adversarial noise, and [3] seeks to explain why neural nets may generalize well despite poor robustness properties.", "startOffset": 176, "endOffset": 180}, {"referenceID": 2, "context": ", no initial image x\u2217), [21] searches for adversarial examples using random perturbations, showing that adversarial examples in fact exist in large regions of the pixel space, [18] shows that even intermediate layers of neural nets are not robust to adversarial noise, and [3] seeks to explain why neural nets may generalize well despite poor robustness properties.", "startOffset": 273, "endOffset": 276}, {"referenceID": 4, "context": "Starting with [5], a major focus has been on devising faster algorithms for finding adversarial examples.", "startOffset": 14, "endOffset": 17}, {"referenceID": 9, "context": "Their idea is that adversarial examples can then be computed on-the-fly and used as training examples, analogous to data augmentation approaches typically used to train neural nets [10].", "startOffset": 181, "endOffset": 185}, {"referenceID": 4, "context": "To find adversarial examples quickly, [5] chooses the adversarial perturbation r to be in the direction of the signed gradient of loss(f(x\u2217 + r), `) with fixed magnitude.", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "In this direction, [7] extends this idea to norms beyond the L\u221e norm, [6] takes the approach of [20] but fixes c, and [19] formalizes [5] as robust optimization.", "startOffset": 19, "endOffset": 22}, {"referenceID": 5, "context": "In this direction, [7] extends this idea to norms beyond the L\u221e norm, [6] takes the approach of [20] but fixes c, and [19] formalizes [5] as robust optimization.", "startOffset": 70, "endOffset": 73}, {"referenceID": 18, "context": "In this direction, [7] extends this idea to norms beyond the L\u221e norm, [6] takes the approach of [20] but fixes c, and [19] formalizes [5] as robust optimization.", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "In this direction, [7] extends this idea to norms beyond the L\u221e norm, [6] takes the approach of [20] but fixes c, and [19] formalizes [5] as robust optimization.", "startOffset": 118, "endOffset": 122}, {"referenceID": 4, "context": "In this direction, [7] extends this idea to norms beyond the L\u221e norm, [6] takes the approach of [20] but fixes c, and [19] formalizes [5] as robust optimization.", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "For example, [5] shows improved accuracy", "startOffset": 13, "endOffset": 16}, {"referenceID": 18, "context": "to adversarial examples generated using their own signed gradient method, but do not consider whether robustness increases for adversarial examples generated using more precise approaches such as [20].", "startOffset": 196, "endOffset": 200}, {"referenceID": 6, "context": "Similarly, [7] compares accuracy to adversarial examples generated using both itself and [5] (but not [20]), and [19] only considers accuracy on adversarial examples generated using their own approach on the baseline network.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "Similarly, [7] compares accuracy to adversarial examples generated using both itself and [5] (but not [20]), and [19] only considers accuracy on adversarial examples generated using their own approach on the baseline network.", "startOffset": 89, "endOffset": 92}, {"referenceID": 18, "context": "Similarly, [7] compares accuracy to adversarial examples generated using both itself and [5] (but not [20]), and [19] only considers accuracy on adversarial examples generated using their own approach on the baseline network.", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "Similarly, [7] compares accuracy to adversarial examples generated using both itself and [5] (but not [20]), and [19] only considers accuracy on adversarial examples generated using their own approach on the baseline network.", "startOffset": 113, "endOffset": 117}, {"referenceID": 5, "context": "Additionally, there has been work on designing neural network architectures [6] and learning procedures [1] that improve robustness to adversarial perturbations, though they do not obtain state-of-theart accuracy on the unperturbed test sets.", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "Additionally, there has been work on designing neural network architectures [6] and learning procedures [1] that improve robustness to adversarial perturbations, though they do not obtain state-of-theart accuracy on the unperturbed test sets.", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "There has also been work using smoothness regularization related to [5] to train neural nets, focusing on improving accuracy rather than robustness [14].", "startOffset": 68, "endOffset": 71}, {"referenceID": 12, "context": "There has also been work using smoothness regularization related to [5] to train neural nets, focusing on improving accuracy rather than robustness [14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 20, "context": "Robustness has also been studied in more general contexts; [22] studies the connection between robustness and generalization, [2] establishes theoretical lower bounds on the robustness of linear and quadratic classifiers, and [4] seeks to improve robustness by promoting resiliance to deleting features during training.", "startOffset": 59, "endOffset": 63}, {"referenceID": 1, "context": "Robustness has also been studied in more general contexts; [22] studies the connection between robustness and generalization, [2] establishes theoretical lower bounds on the robustness of linear and quadratic classifiers, and [4] seeks to improve robustness by promoting resiliance to deleting features during training.", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "Robustness has also been studied in more general contexts; [22] studies the connection between robustness and generalization, [2] establishes theoretical lower bounds on the robustness of linear and quadratic classifiers, and [4] seeks to improve robustness by promoting resiliance to deleting features during training.", "startOffset": 226, "endOffset": 229}, {"referenceID": 4, "context": "This definition formalizes the notion of robustness in [5, 6, 20].", "startOffset": 55, "endOffset": 65}, {"referenceID": 5, "context": "This definition formalizes the notion of robustness in [5, 6, 20].", "startOffset": 55, "endOffset": 65}, {"referenceID": 18, "context": "This definition formalizes the notion of robustness in [5, 6, 20].", "startOffset": 55, "endOffset": 65}, {"referenceID": 4, "context": "accuracy on adversarial examples used to measure robustness in [5, 19].", "startOffset": 63, "endOffset": 70}, {"referenceID": 17, "context": "accuracy on adversarial examples used to measure robustness in [5, 19].", "startOffset": 63, "endOffset": 70}, {"referenceID": 13, "context": "Recall that neural nets with rectified-linear (ReLU) units as activations are piecewise linear [15].", "startOffset": 95, "endOffset": 99}, {"referenceID": 4, "context": "Since adversarial examples exists because of this linearity in the neural net [5], we restrict our search to the region Z(x\u2217) around x\u2217 on which the neural net is linear.", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "In Figure 1 note that the direction of the nearest adversarial example is not necessary aligned with the signed gradient of the loss function, as observed by others [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 18, "context": "Our method discovers more adversarial examples than the baseline [20] for each neural net, hence producing better estimates.", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "We find adversarial examples for the neural net LeNet [12] (modified to use ReLUs instead of sigmoids) trained to classify MNIST [11], and for the network-in-network (NiN) neural net [13] trained to classify CIFAR-10 [9].", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "We find adversarial examples for the neural net LeNet [12] (modified to use ReLUs instead of sigmoids) trained to classify MNIST [11], and for the network-in-network (NiN) neural net [13] trained to classify CIFAR-10 [9].", "startOffset": 129, "endOffset": 133}, {"referenceID": 8, "context": "We find adversarial examples for the neural net LeNet [12] (modified to use ReLUs instead of sigmoids) trained to classify MNIST [11], and for the network-in-network (NiN) neural net [13] trained to classify CIFAR-10 [9].", "startOffset": 217, "endOffset": 220}, {"referenceID": 7, "context": "Both neural nets are trained using Caffe [8].", "startOffset": 41, "endOffset": 44}, {"referenceID": 18, "context": "We compare our algorithm for estimating \u03c1 to the baseline L-BFGS-B algorithm proposed by [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "We use the tool provided by [21] to compute this baseline.", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": "We use LeNet in our comparisons, since we find that it is substantially more robust than the neural nets considered in most previous work (including [20]).", "startOffset": 149, "endOffset": 153}, {"referenceID": 4, "context": "We performed a similar comparison to the signed gradient algorithm proposed by [5] (with the signed gradient multiplied by = 20 pixels).", "startOffset": 79, "endOffset": 82}], "year": 2016, "abstractText": "Despite having high accuracy, neural nets have been shown to be susceptible to adversarial examples, where a small perturbation to an input can cause it to become mislabeled. We propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program. We show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. Our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms. Furthermore, we show how existing approaches to improving robustness \u201coverfit\u201d to adversarial examples generated using a specific algorithm. Finally, we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose, but also according to previously proposed metrics.", "creator": "LaTeX with hyperref package"}}}