{"id": "1605.07272", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Matrix Completion has No Spurious Local Minimum", "abstract": "matrix completion includes no basic machine learning problem that creates many meanings, consisting in bounded filtering using integer classes. traditional non - functional optimization algorithms are useful and generalized in cognition. despite recent progress even considering a non - convex varieties converge past a good initial point, it remains paradox whose cluster or clustered methods suffices in practice. opponents prove that it commonly used proto - convex objective solution whose matrix completion needs no spurious regional minima - - all local minima and also mean global. therefore, many rapid optimization algorithms such as ( stochastic ) gradient descent can provably distribute itself efficiently with \\ textit { \u2026 } initialization algorithm polynomial time.", "histories": [["v1", "Tue, 24 May 2016 02:53:27 GMT  (440kb,D)", "http://arxiv.org/abs/1605.07272v1", null], ["v2", "Fri, 16 Sep 2016 19:58:48 GMT  (451kb,D)", "http://arxiv.org/abs/1605.07272v2", "to appear in NIPS 2016 (with oral). added a section that explains the intuition of the proof"], ["v3", "Sun, 29 Jan 2017 18:45:48 GMT  (457kb,D)", "http://arxiv.org/abs/1605.07272v3", "NIPS'16 best student paper. added references and fixed typos"]], "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["rong ge 0001", "jason d lee", "tengyu ma"], "accepted": true, "id": "1605.07272"}, "pdf": {"name": "1605.07272.pdf", "metadata": {"source": "CRF", "title": "Matrix Completion has No Spurious Local Minimum", "authors": ["Rong Ge", "Jason D. Lee", "Tengyu Ma"], "emails": ["rongge@cs.duke.edu.", "jasondlee88@gmail.com.", "tengyu@cs.princeton.edu."], "sections": [{"heading": "1 Introduction", "text": "Matrix completion is the problem of recovering a low rank matrix from partially observed entries. It has been widely used in collaborative filtering and recommender systems [Kor09, RS05], dimension reduction [CLMW11] and multiclass learning [AFSU07]. There has been extensive work on designing efficient algorithms for matrix completion with guarantees. One earlier line of results (see [Rec11, CT10, CR09] and the references therein) rely on convex relaxations. These algorithms achieve strong statistical guarantees, but are quite computationally expensive in practice.\nMore recently, there has been growing interest in analyzing non-convex algorithms for matrix completion [KMO10, JNS13, Har14, HW14, SL15, ZWL15, CW15]. Let M \u2208 d\u00d7d be the target matrix with rank r d that we aim to recover, and let \u2126 = {(i, j) : Mi, j is observed} be the set of observed entries. These methods are instantiations of optimization algorithms applied to the objective1,\nf (X) = 1 2 \u2211 (i, j)\u2208\u2126 [ Mi, j \u2212 (XX>)i, j ]2 , (1.1)\nThese algorithms are much faster than the convex relaxation algorithms, which is crucial for their empirical success in large-scale collaborative filtering applications [Kor09].\nAll of the theoretical analysis for the nonconvex procedures require careful initialization schemes: the initial point should already be close to optimum. In fact, Sun and Luo [SL15] showed that after this initialization the problem is effectively strongly-convex, hence many different optimization procedures can be analyzed by standard techniques from convex optimization.\nHowever, in practice people commonly use a random initialization, which still leads to robust and fast convergence. How can these practical algorithms find the optimal solution in spite of the non-convexity? In this work we investigate this question and show that the matrix completion objective has no spurious local minimum. More precisely, we show that any local minimum X of objective function f (\u00b7) is also a global minimum with f (X) = 0, and recovers the correct low rank matrix M. \u2217Duke University, rongge@cs.duke.edu. \u2020UC Berkeley, jasondlee88@gmail.com. \u2021Princeton University, tengyu@cs.princeton.edu. Supported in part by Simons Award in Theoretical Computer Science and IBM PhD Fellowship. 1In this paper we focus on the symmetric case when true M has symmetric decomposition M = ZZT . Some of previous papers work on the asymmetric case when M = ZWT , which is relatively harder than the symmetric case.\nar X\niv :1\n60 5.\n07 27\n2v 1\n[ cs\n.L G\n] 2\n4 M\nay 2\n01 6\nOur characterization of the structure in the objective function implies that (stochastic) gradient descent from arbitrary starting point converge to a global minimum. This is because gradient descent converges to a local minimum [GHJY15, LSJR16], and for our objective function a local minimum is also a global one."}, {"heading": "1.1 Main results", "text": "Assume the target matrix M is symmetric and each entry of M is observed with probability p independently 2. We assume M = ZZ> for some matrix Z \u2208 d.\nThere are two known issues with matrix completion. First, the choice of Z is not unique since for any orthonormal matrix R \u2208 r\u00d7r, we have M = (ZR)(ZR)>. Our goal is to find one of these equivalent solutions.\nAnother issue is that matrix completion is impossible when M is \u201caligned\u201d with standard basis. For example, when M is the identity matrix in its first r \u00d7 r block, we will very likely be observing only 0 entries. To address this issue, we make the following standard assumption:\nAssumption 1. For any row Zi of Z, we have\n\u2016Zi\u2016 6 \u00b5/ \u221a d \u00b7 \u2016Z\u2016F .\nMoreover, Z has a bounded condition number \u03c3max(Z)/\u03c3min(Z) = \u03ba.\nThroughout this paper we think of \u00b5 and \u03ba as small constants, and the sample complexity depends polynomially on these two parameters. Also note that this assumption is independent of the choice of Z: all Z such that ZZT = M have the same row norms and Frobenius norm.\nThis assumption is similar to the \u201cincoherence\u201d assumption [CR09]. Our assumption is the same to the ones used in analyzing non-convex algorithms [KMO10, SL15].\nWe enforce X to also satisfy this assumption by a regularizer\nf (X) = 1 2 \u2211 (i, j)\u2208\u2126 [ Mi, j \u2212 (XX>)i, j ]2 + R(X), (1.2)\nwhere R(X) is a function that penalizes X when one of its rows is too large. See Section 3 for the precise definition. Our main result shows that in this setting, the regularized objective function has no spurious local minimum:\nTheorem 1.1. [Informal] All local minimum of the regularized objective (1.1) satisfy XXT = ZZT = M when p > poly(\u03ba, r, \u00b5, log d)/d.\nCombined with the results in [GHJY15, LSJR16] (see more discussions in Section 1.2), we have,\nTheorem 1.2 (Informal). With high probability, stochastic gradient descent on the regularized objective (1.1) will converge to a solution X such that XXT = ZZT = M in polynomial time from any starting point. Gradient descent will converge to such a point with probability 1 from a random starting point.\nOur results are also robust to noise. Even if each entry is corrupted with Gaussian noise of standard deviation \u00b52\u2016Z\u20162F/d (comparable to the entry itself!), we can still guarantee that all the local minima satisfy \u2016XXT \u2212 ZZT \u2016F 6 \u03b5 when p is large enough, see more discussions in Appendix B.\nOur main technique is to show that every point that satisfies the first and second order necessary conditions for optimality must be a desired solution. To achieve this we use new ideas to analyze the effect of the regularizer and show how it is useful in modifying the first and second order conditions to exclude any spurious local minimum.\n2The entries (i, j) and ( j, i) are the same. With probability p we observe both entries and otherwise we observe neither."}, {"heading": "1.2 Related Work", "text": "Matrix Completion. The earlier theoretical works on matrix completion analyzed the nuclear norm heuristic [Rec11, CT10, CR09]. This line of work has the cleanest and strongest theoretical guarantees; [CT10, Rec11] showed that if |\u2126| & dr\u00b52 log2 d the nuclear norm convex relaxation recovers the exact underlying low rank matrix. The solution can be computed via the solving a convex program in polynomial time. However the primary disadvantage of nuclear norm methods is their computational and memory requirements. The fastest known algorithms have running time O(d3) and require O(d2) memory, which are both prohibitive for moderate to large values of d. These concerns led to the development of the low-rank factorization paradigm of [BM03]; Burer and Monteiro proposed factorizing the optimization variable M\u0302 = XXT , and optimizing over X \u2208 d\u00d7r , instead of M\u0302 \u2208 d\u00d7d . This approach only requires O(dr) memory, and a single gradient iteration takes time O(r|\u2126|), so has much lower memory requirement and computational complexity than the nuclear norm relaxation. On the other hand, the factorization causes the optimization problem to be non-convex in X, which leads to theoretical difficulties in analyzing algorithms. Under incoherence and sufficient sample size assumptions, [KMO10] showed that well-initialized gradient descent recovers M. Similary, [HW14, Har14, JNS13] showed that well-initialized alternating least squares or block coordinate descent converges to M, and [CW15] showed that well-initialized gradient descent converges to M. [SL15, ZWL15] provided a more unified analysis by showing that with careful initialization many algorithms, including gradient descent and alternating least squres, succeed. [SL15] accomplished this by showing an analog of strong convexity in the neighborhood of the solution M.\nNon-convex Optimization. Recently, a line of work analyzes non-convex optimization by separating the problem into two aspects: the geometric aspect which shows the function has no spurious local minimum and the algorithmic aspect which designs efficient algorithms can converge to local minimum that satisfy first and (relaxed versions) of second order necessary conditions.\nOur result is the first that explains the geometry of the matrix completion objective. Similar geometric results are only known for a few problems: phase retrieval/synchronization, tensor decomposition, dictionary learning [GHJY15, SQW15, BBV16]. The matrix completion objective requires different tools due to the sampling of the observed entries, as well as carefully managing the regularizer to restrict the geometry. Parallel to our work Bhojanapalli et al.[BNS16] showed similar results for matrix sensing, which is closely related to matrix completion.\nOn the algorithmic side, it is known that second order algorithms like cubic regularization [NP06] and trust-region [SQW15] algorithms converge to local minima that approximately satisfy first and second order conditions. Gradient descent is also known to converge to local minima [LSJR16] from a random starting point. Stochastic gradient descent can converge to a local minimum in polynomial time from any starting point [Pem90, GHJY15]. All of these results can be applied to our setting, implying various heuristics people use in practice are guaranteed to solve matrix completion."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Notations", "text": "For \u2126 \u2282 [d] \u00d7 [d], let P\u2126 be the operator that maps a matrix A to P\u2126(A), where P\u2126(A) has the same values as A on \u2126, and 0 outside of \u2126.\nWe will use the following matrix norms: \u2016 \u00b7 \u2016F the frobenius norm, \u2016 \u00b7 \u2016 spectral norm, |A|\u221e elementwise infinity norm, and |A|p\u2192q = max\u2016x\u2016p=1 \u2016A\u2016q. We use the shorthand \u2016A\u2016\u2126 = \u2016P\u2126A\u2016F . The trace inner product of two matrices is \u3008A, B\u3009 = tr(A>B), and \u03c3min(X), \u03c3max(X) are the smallest and largest singular values of X. We also use Xi to denote the i-th row of a matrix X."}, {"heading": "2.2 Necessary conditions for Optimality", "text": "Given an objective function f (x) : n \u2192 , we use \u2207 f (x) to denote the gradient of the function, and \u22072 f (x) to denote the Hessian of the function (\u22072 f (x) is an n\u00d7n matrix where [\u22072 f (x)]i, j = \u2202 2\n\u2202xi\u2202x j f (x)). It is well known that local minima\nof the function f (x) must satisfy some necessary conditions:\nDefinition 2.1. A point x satisfies the first order necessary condition for optimality (later abbreviated as first order optimality condition) if \u2207 f (x) = 0. A point x satisfies the second order necessary condition for optimality (later abbreviated as second order optimality condition)if \u22072 f (x) 0.\nThese conditions are necessary for a local minimum because otherwise it is easy to find a direction where the function value decreases. We will also consider a relaxed second order necessary condition, where we only require the smallest eigenvalue of the Hessian \u22072 f (x) to be not very negative:\nDefinition 2.2. For \u03c4 > 0, a point x satisfies the \u03c4-relaxed second order optimality condition, if \u22072 f (x) \u2212\u03c4 \u00b7 I.\nThis relaxation to the second order condition makes the conditions more robust, and allows for efficient algorithms.\nTheorem 2.3. [NP06, SQW15, GHJY15] If every point x that satisfies first order and \u03c4-relaxed second order necessary condition is a global minimum, then many optimization algorithms (cubic regularization, trust-region, stochastic gradient descent) can find the global minimum up to \u03b5 error in function value in time poly(1/\u03b5, 1/\u03c4, d)."}, {"heading": "3 Warm-up: Rank-1 matrix completion", "text": "In this section we analyze the geometry of the objective function for rank r = 1 case with a simple and clean proof. This case illustrates our main ideas. The rank r analysis follows from the same approach and is shown in the next section.\nIn this case, assume M = zz>, where \u2016z\u2016 = 1, and \u2016z\u2016\u221e 6 \u00b5\u221ad . The objective function simplifies to,\nf (x) = 1 2 \u2016P\u2126(M \u2212 xx>)\u20162F + \u03bbR(x) . (3.1)\nHere we use the the regularization R(x)\nR(x) = d\u2211\ni=1\nh(xi), and h(t) = (|t| \u2212 \u03b1)4 t>\u03b1 .\nThe parameters \u03bb and \u03b1 will be chosen later as in Theorem 3.2. We will choose \u03b1 > 10\u00b5/ \u221a\nd so that R(x) = 0 for incoherent x, and thus it only penalizes coherent x. Moreover, we note that R(x) has Lipschitz second order derivative.\nWe first state the optimality conditions, whose proof is deferred to Appendix A.\nProposition 3.1. The first order optimality condition of objective (3.1) is,\n2P\u2126(M \u2212 xx>)x = \u03bb\u2207R(x) , (3.2)\nand the second order optimality condition requires:\n\u2200v \u2208 d, \u2016P\u2126(vx> + xv>)\u20162F + \u03bbv>\u22072R(x)v > 2v>P\u2126(M \u2212 xx>)v . (3.3)\nMoreover, The \u03c4-relaxed second order optimality condition requires\n\u2200v \u2208 d, \u2016P\u2126(vx> + xv>)\u20162F + \u03bbv>\u22072R(x)v > 2v>P\u2126(M \u2212 xx>)v \u2212 \u03c4\u2016v\u20162 . (3.4)\nWe give the precise version of Theorem 1.1 for the rank-1 case.\nTheorem 3.2. For p > c\u00b5 6 log1.5 d d where c is a large enough absolute constant, set \u03b1 = 10\u00b5 \u221a\n1/d and \u03bb > \u00b52 p/\u03b12.Then, with high probability over the randomness of \u2126, the only points in d that satisfy first and (or \u03c4-relaxed with \u03c4 < \u22120.1p) second order optimality conditions are z and \u2212z.\nIf we observe every entry of M, Theorem 3.2 follows from simple linear algebra: the first order optimality condition would require Mx = \u2016x\u20162x, and therefore x has to be an eigenvector of M. The second order optimality condition will make sure x is the eigenvector with largest eigenvalue. When we only observe a small subset of entries, we will use concentration bounds to prove more robust versions of these arguments. In the rest of this section, we will first prove that when x is constrained to be incoherent (and hence the regularizer is 0 and concentration is straightforward) and satisfies the optimality conditions, then x has to be z or \u2212z. Then we go on to explain how the regularizer helps us to change the geometry of those points that are far away from z so that we can rule out them from being local minimum.\nFor simplicity, we will focus on the part that shows a local minimum x must be close enough to z.\nLemma 3.3. In the setting of Theorem 3.2, suppose x satisfies the first-order and second-order optimality condition (3.2) and (3.3). Then when p is defined as in Theorem 3.2,\u2225\u2225\u2225xx> \u2212 zz>\u2225\u2225\u22252F 6 O(\u03b5) . where \u03b5 = \u00b53(pd)\u22121/2.\nThis turns out to be the main challenge. Once we proved x is close, we can apply the result of Sun and Luo [SL15] (see Lemma C.1), and obtain Theorem 3.2."}, {"heading": "3.1 Handling incoherent x", "text": "To demonstrate the key idea, in this section we restrict our attention to the subset of d which contains incoherent x with `2 norm bounded by 1, that is, we consider,\nB = { x : \u2016x\u2016\u221e 6 \u00b5 \u221a\nd , \u2016x\u2016 6 1\n} . (3.5)\nNote that the desired solution z is in B, and the regularization R(x) vanishes inside B. The general rational is to assume x satisfies the first and second order optimality condition, and then use these\nconditions to deduce a sequence of properties that x must satisfy\nLemma 3.4. Under the setting of Theorem 3.2 , with high probability over the choice of \u2126, for any x \u2208 B that satisfies second-order optimality condition (3.3) we have,\n\u2016x\u20162 > 1/4.\nThe same is true if x \u2208 B only satisfies \u03c4-relaxed second order optimality condition for \u03c4 6 0.1p.\nProof. We plug in v = z in the second-order optimality condition (3.3), and obtain that\u2225\u2225\u2225P\u2126(zx> + xz>)\u2225\u2225\u22252F > 2z>P\u2126(M \u2212 xx>)z . (3.6) Intuitively, when restricted to \u2126, the squared Frobenius on the LHS and the quadratic form on the RHS should both be approximately a p fraction of the unrestricted case. In fact, both LHS and RHS can be written as the sum of terms of the form \u3008P\u2126(uvT ), P\u2126(stT )\u3009, because\u2225\u2225\u2225P\u2126(zx> + xz>)\u2225\u2225\u22252F = 2\u3008P\u2126(zxT ), P\u2126(zxT )\u3009 + 2\u3008P\u2126(zxT ), P\u2126(xzT )\u3009\n2z>P\u2126(M \u2212 xx>)z = 2\u3008P\u2126(zzT ), P\u2126(zzT )\u3009 \u2212 2\u3008P\u2126(xxT ), P\u2126(zzT )\u3009.\nTherefore we can use concentration inequalities (Theorem D.1), and simplify the equation\nLHS of (3.6) = p \u2225\u2225\u2225zx> + xz>\u2225\u2225\u22252F \u00b1 O( \u221apd\u2016x\u20162\u221e\u2016z\u20162\u221e\u2016x\u20162\u2016z\u20162)\n= 2p\u2016x\u20162\u2016z\u20162 + 2p\u3008x, z\u30092 \u00b1 O(p\u03b5) , (Since x, z \u2208 B)\nwhere \u03b5 = O(\u00b52 \u221a\nlog d pd ). Similarly, by Theorem D.1 again, we have\nRHS of (3.6) = 2 ( \u3008P\u2126(zz>), P\u2126(zz>)\u3009 \u2212 \u3008P\u2126(xx>), P\u2126(zz>)\u3009 ) (Since M = zz>)\n= 2p\u2016z\u20164 \u2212 2p\u3008x, z\u30092 \u00b1 O(p\u03b5) (by Theorem D.1 and x, z \u2208 B)\n(Note that even we use the \u03c4-relaxed second order optimality condition, the RHS only becomes 1.99p\u2016z\u20164 \u2212 2p\u3008x, z\u30092 \u00b1 O(p\u03b5) which does not effect the later proofs.)\nTherefore plugging in estimates above back into equation (3.6), we have that\n2p\u2016x\u20162\u2016z\u20162 + 2p\u3008x, z\u30092 \u00b1 O(p\u03b5) > 2\u2016z\u20164 \u2212 2\u3008x, z\u30092 \u00b1 O(p\u03b5) ,\nwhich implies that 6p\u2016x\u20162\u2016z\u20162 > 2p\u2016x\u20162\u2016z\u20162 + 4p\u3008x, z\u30092 > 2p\u2016z\u20164 \u2212 O(p\u03b5). Using \u2016z\u20162 = 1, and \u03b5 being sufficiently small, we complete the proof.\nNext we use first order optimality condition to pin down another property of x \u2013 it has to be close to z after scaling. Note that this doesn\u2019t mean directly that x has to be close to z since x = 0 also satisfies first order optimality condition (and therefore the conclusion (3.7) below).\nLemma 3.5. With high probability over the randomness of \u2126, for any x \u2208 B that satisfies first-order optimality condition (3.2), we have that x also satisfies \u2225\u2225\u2225\u3008z, x\u3009z \u2212 \u2016x\u20162x\u2225\u2225\u2225 6 O(\u03b5) . (3.7) where \u03b5 = O\u0303(\u00b53(pd)\u22121/2).\nProof. Note that since x \u2208 B, we have R(x) = 0. Therefore first-order optimality condition says that\nP\u2126(M \u2212 xx>)x = P\u2126(zz>)x \u2212 P\u2126(xx>)x = 0 . (3.8)\nAgain, intuitively we hope P\u2126(zzT ) \u2248 pzzT and P\u2126(xxT )x \u2248 p\u2016x\u20162x. These are made precise by the concentration inequalities Lemma D.4 and Theorem D.2 respectively.\nBy Theorem D.2, we have that with high probability over the choice of \u2126, for every x \u2208 B,\n\u2016P\u2126(xx>)x \u2212 pxx>x\u2016F 6 p\u03b5\u2016x\u20163 6 p\u03b5 (3.9)\nwhere \u03b5 = O\u0303(\u00b53(pd)\u22121/2). Similarly, by Lemma D.4, we have that for with high probability over the choice of \u2126,\u2225\u2225\u2225P\u2126(zz>) \u2212 pzz>\u2225\u2225\u2225 6 \u03b5p . for \u03b5 = O\u0303(\u00b52(pd)\u22121/2). Therefore for every x,\u2225\u2225\u2225P\u2126(zz>)x \u2212 pzz>x\u2225\u2225\u2225 6 \u03b5p\u2016x\u2016 6 \u03b5p . (3.10) Plugging in estimates (3.10) and (3.9) into equation (3.8), we complete the proof.\nFinally we combine the two optimality conditions and show (3.7) implies xxT must be close to zzT . Lemma 3.6. Suppose vector x satisfies that \u2016x\u20162 > 1/4, and that \u2225\u2225\u2225\u3008z, x\u3009z \u2212 \u2016x\u20162x\u2225\u2225\u2225 6 \u03b4 . Then for \u03b4 \u2208 (0, 0.1),\u2225\u2225\u2225xx> \u2212 zz>\u2225\u2225\u22252F 6 O(\u03b4) .\nProof. We write z = ux + v where u \u2208 and v is a vector orthogonal to x. Now we know \u3008z, x\u3009z = u2\u2016x\u20162x + u\u2016x\u20162v, therefore \u03b4 > \u2225\u2225\u2225\u3008z, x\u3009z \u2212 \u2016x\u20162x\u2225\u2225\u2225 = \u2016x\u20162 \u221au2\u2016v\u20162 + (1 \u2212 u2)2.\nIn particular, we know |1\u2212u2| 6 4\u03b4 and u\u2016v\u2016 6 4\u03b4. This means |u| \u2208 1\u00b1 3\u03b4 and \u2016v\u2016 6 8\u03b4. Now we expand xxT \u2212 zzT :\nxxT \u2212 zzT = (1 \u2212 u2)xxT + uxvT + uvxT + vvT\nIt is clear that all the terms have norm bounded by O(\u03b4), therefore \u2225\u2225\u2225xx> \u2212 zz>\u2225\u2225\u22252F 6 O(\u03b4)."}, {"heading": "3.2 Extension to general x", "text": "We have shown when x is incoherent and satisfies first and second order optimality conditions, then it must be close to z or \u2212z. Now we need to consider more general cases when x may have some very large coordinates. Here the main intuition is that the first order optimality condition with a proper regularizer is enough to guarantee that x cannot have a entry that is too much bigger than \u00b5/ \u221a d.\nLemma 3.7. With high probability over the choice of \u2126, for any x that satisfies first-order order optimality condition (3.2), we have \u2016x\u2016\u221e 6 4 max { \u03b1, \u00b5 \u221a p/\u03bb } . (3.11)\nHere we recall that \u03b1 was chosen to be 10\u00b5/ \u221a d and \u03bb is chosen to be large so that the \u03b1 dominates the second term \u00b5 \u221a p/\u03bb in the setting of Theorem 3.2.\nProof of Lemma 3.7. Suppose i? = max j |x j|. Without loss of generality, suppose xi? > 0. Suppose i?-th row of \u2126 consists of entries with index [i]\u00d7S i? . If |xi? | 6 2\u03b1, we are done. Therefore in the rest of the proof we assume |xi? | > 2\u03b1. Note that when p > c(log d)/d for sufficiently large constant c, with high probability over the choice of \u2126, we have |S i? | 6 2pd. In the rest of argument we are working with such an \u2126 with |S i? | 6 2pd.\nWe will compare the i?-th coordinate of LHS and RHS of first-order optimality condition (3.2). For preparation, we have\n\u2223\u2223\u2223(P\u2126(M)x)i? \u2223\u2223\u2223 = \u2223\u2223\u2223\u2223(P\u2126(zz>)x)i? \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2211 j\u2208S i? zi?z jx j \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 6 |xi? |\n\u2211 j\u2208S i? |zi?z j| 6 |xi? | \u00b7 \u00b52/d \u00b7 |S i? | 6 2|xi? |p\u00b52 (3.12)\nwhere the last step we used the fact that |S i? | 6 2pd. Moreover, we have that (P\u2126(xx>)x)i? = \u2211 j\u2208S i? xi? x2j > 0 ,\nand that\n(\u03bb\u2207R(x))i? = 4\u03bb(|xi? | \u2212 \u03b1)3 sign(xi? ) > \u03bb\n2 |xi? |3 (Since xi? > 2\u03b1)\nNow plugging in the bounds above into the i?-th coordinate of equation (3.2), we obtain\n4|xi? |p\u00b52 > 2(P\u2126(M \u2212 xx>)x)i? > (\u03bb\u2207R(x))i? > \u03bb\n2 |xi? |3 ,\nwhich implies that |xi? | 6 4 \u221a p\u00b52/\u03bb.\nSetting \u03bb > \u00b52 p/\u03b12 and \u03b1 = 10\u00b5 \u221a\n1/d, Lemma 3.7 ensures that any x that satisfies first-order optimality condition is the following ball, B\u2032 = { x \u2208 d : \u2016x\u2016\u221e 6 4\u03b1 } .\nThen we would like to continue to use arguments similar to Lemma 3.4 and 3.5. However, things have become more complicated as now we need to consider the contribution of the regularizer.\nLemma 3.8 (Extension of Lemma 3.4). In the setting of Theorem 3.2, with high probability over the choice of \u2126, suppose x \u2208 B\u2032 satisfies second-order optimality condition (3.3) or \u03c4-relaxed condition for \u03c4 6 0.1p, we have \u2016x\u20162 > 1/8.\nThe guarantees and proofs are very similar to Lemma 3.4. The main intuition is that we can restrict our attentions to coordinates whose regularizer is equal to 0. See Section A for details.\nWe will now deal with first order optimality condition. We first write out the basic extension of Lemma 3.5, which follows from the same proof except we now include the regularizer term.\nLemma 3.9 (Basic extension of Lemma 3.5). With high probability over the randomness of \u2126, for any x \u2208 B\u2032 that satisfies first-order optimality condition (3.2), we have that x also satisfies\u2225\u2225\u2225\u3008z, x\u3009z \u2212 \u2016x\u20162x \u2212 \u03b3 \u00b7 \u2207R(x)\u2225\u2225\u2225 6 O(\u03b5) . (3.13) where \u03b5 = O\u0303(\u00b56(pd)\u22121/2) and \u03b3 = \u03bb/(2p) > 0.\nNext we will show that we can remove the regularizer term, the main observation here is nonzero entries \u2207R(x) all have the same sign as the corresponding entries in x. See Section A for details.\nLemma 3.10. Suppose x \u2208 B\u2032 satisfies that \u2016x\u20162 > 1/8, under the same assumption as Lemma 3.9. we have,\u2225\u2225\u2225\u3008x, z\u3009z \u2212 \u2016x\u20162x\u2225\u2225\u2225 6 O(\u03b5) (3.14) Finally we combine Lemma 3.7, Lemma 3.8 and Lemma 3.10 and prove Lemma 3.6. The argument are also summarized in Figure 1, where we partition d into regions where our lemmas apply."}, {"heading": "4 Rank-r case", "text": "In this section we show how to extend the results in Section 3 to recover matrices of rank r.\nRecall that in this case we assume the original matrix M = ZZT , where Z \u2208 d\u00d7r. We also assume Assumption 1. The objective function is very similar to the rank 1 case\nf (X) = 1 2 \u2225\u2225\u2225P\u2126(M \u2212 XX>)\u2225\u2225\u22252F + \u03bbR(X) , (4.1) where R(X) = \u2211d i=1 r(\u2016Xi\u2016) . and recall that r(t) = (|t| \u2212 \u03b1)4 t>\u03b1. Here \u03b1 and \u03bb are again parameters that we will determined later. Without loss of generality, we assume that \u2016Z\u20162F = r in this section. This implies that \u03c3max(Z) > 1 > \u03c3min(Z). Now we shall state the first and second order optimality conditions:\nProposition 4.1. If X is a local optimum of objective function (4.1), its first order optimality condition is,\n2P\u2126(M)X = 2P\u2126(XX>)X + \u03bb\u2207R(X) , (4.2)\nand the second order optimality condition is equivalent to\n\u2200V \u2208 d\u00d7r, \u2016P\u2126(VX> + XV>)\u20162F + \u03bb\u3008V>,\u22072R(X)V\u3009 > 2\u3008P\u2126(M \u2212 XX>),VV>\u3009 . (4.3)\nNote that the regularizer now is more complicated than the one dimensional case, but luckily we still have the following nice property.\nProposition 4.2. We have that \u2207R(X) = \u0393X where \u0393 \u2208 d\u00d7d is a diagonal matrix with \u0393ii = 4(\u2016Xi\u2016\u2212\u03b1) 4\n\u2016Xi\u2016 \u2016Xi\u2016>\u03b1. As a direct consequence, \u3008(\u2207R(X))i, Xi\u3009 > 0 for every i \u2208 [d].\nNow we are ready to state the precise version of Theorem 1.1:\nTheorem 4.3. Suppose p > C max{\u00b56\u03ba16r4, \u00b54\u03ba4r6}d\u22121 log1.5 d where C is a large enough constant. Let \u03b1 = 4\u00b5\u03bar/ \u221a d, \u03bb > \u00b52rp/\u03b12. Then with high probability over the randomness of \u2126, any local minimum X of f (\u00b7) satisfies that f (X) = 0, and in particular, ZZ> = XX>.\nThe proof of this Theorem follows from a similar path as Theorem 3.2. We first notice that because of the regularizer, any matrix X that satisfies first order optimality condition must be somewhat incoherent (this is analogues to Lemma 3.7):\nLemma 4.4. Suppose |S i| 6 2pd. Then for any X satisfies 1st order optimality (4.2), we have\n\u2016X\u20162\u2192\u221e = max i \u2016Xi\u2016 6 4 max\n{ \u03b1, \u00b5 \u221a rp/\u03bb } (4.4)\nProof. Assume i? = argmaxi \u2016Xi\u2016. Suppose the ith row of \u2126 consists of entries with index [i] \u00d7 S i. If \u2016Xi?\u2016 6 2\u03b1, then we are done. Therefore in the rest of the proof we assume \u2016Xi?\u2016 > 2\u03b1.\nWe will compare the i-th row of LHS and RHS of (4.2). For preparation, we have (P\u2126(M)x)i? = ( P\u2126(ZZ>)X ) i? = ( P\u2126(ZZ>) ) i? X (4.5)\nThen we have that \u2225\u2225\u2225\u2225(P\u2126(ZZ>))i?\u2225\u2225\u2225\u22251 = \u2211 j\u2208S i? |\u3008Zi? ,Z j\u3009|\n6 \u2211 j\u2208S i? \u2016Zi?\u2016\u2016Z j\u2016 6 \u2211 j\u2208S i? \u00b52r/d|S 1| (by incoherence of Z) 6 2\u00b52rp . (by |S i? | 6 2pd)\nTherefore we can bound the `2 norm of LHS of 1st order optimality condition (4.2) by\u2225\u2225\u2225\u2225(P\u2126(ZZ>)X)i?\u2225\u2225\u2225\u2225 6 \u2225\u2225\u2225\u2225(P\u2126(ZZ>))i?\u2225\u2225\u2225\u22251 \u2225\u2225\u2225X>\u2225\u2225\u22251\u21922 6 2\u00b52rp \u2016X\u20162\u2192\u221e (by \u2016X\u20162\u2192\u221e =\n\u2225\u2225\u2225X>\u2225\u2225\u22251\u21922) = 2\u00b52rp \u2016Xi?\u2016 (4.6)\nNext we lowerbound the norm of the RHS of equation (4.2). We have that (P\u2126(XX>)X)i? = \u2211 j\u2208S i? \u3008Xi? , X j\u3009X j = Xi \u2211 j\u2208Xi? X>j X j ,\nwhich implies that\n\u3008(P\u2126(XX>)X)i? , Xi?\u3009 = Xi? \u2211\nj\u2208Xi? X>j X j\n X>i? > 0 . (4.7)\nUsing Proposition 4.2 we obtain that \u3008(P\u2126(XX>)X)i? , (\u2207R(X))i?\u3009 = \u0393iiXi? \u2211\nj\u2208Xi? X>j X j  X>i? > 0 . (4.8) It follows that \u2225\u2225\u2225(P\u2126(XX>)X)i? + (\u03bb\u2207R(X))i?\u2225\u2225\u2225 > \u2016(\u03bb\u2207R(X))i?\u2016 (by equation (4.8))\n= 4\u03bb(\u2016Xi?\u2016 \u2212 \u03b1)3 \u2016Xi?\u2016 \u00b7 \u2016Xi?\u2016 (by Proposition 4.2) > \u03bb\n2 \u2016Xi?\u20163 (by the assumptino \u2016Xi?\u2016 > 2\u03b1)\nTherefore plugging in equation above and equation (4.6) into 1st order optimality condition (4.2). We obtain that \u2016Xi?\u2016 6 \u221a 8\u00b52rp/\u03bb which completes the proof.\nNext, we prove a property implied by first order optimality condition, which is similar to Lemma 3.9.\nLemma 4.5. In the setting of Theorem 4.3, with high probability over the choice of \u2126, for any X that satisfies 1st order optimality condition (4.2), we have\n\u2016X\u20162F 6 2r\u03c3max(Z)2 . (4.9)\nMoreover, we have \u03c3max(X) 6 2\u03c3max(Z)r1/6 . (4.10) and \u2225\u2225\u2225ZZT X \u2212 XXT X \u2212 \u03b3\u2207R(X)\u2225\u2225\u2225F 6 O(\u03b4) (4.11) where \u03b4 = O(\u00b53\u03ba3r2 log0.75(d)\u03c3max(Z)\u22123(dp)\u22121/2) and \u03b3 = \u03bb/(2p) > 0.\nProof. If \u2016X\u2016F 6 \u221a r\u03c3max(Z)2 we are done. When \u2016X\u2016F > \u221a\nr\u03c3max(Z)2, by Lemma 4.4, we have that max \u2016Xi\u2016 6 4\u03b1 = 4\u00b5\u03bar/ \u221a d, and therefore max \u2016Xi\u2016 6 \u03bd\u2016X\u2016F with \u03bd = O(\u00b5\u03ba\n\u221a r/\u03c3max(Z)). Then by Theorem D.2, we have that\u2225\u2225\u2225P\u2126(ZZ>)X \u2212 pZZ>X\u2225\u2225\u2225F 6 p\u03b4 , (4.12)\nand \u2225\u2225\u2225P\u2126(XX>)X \u2212 XX>X\u2225\u2225\u2225F 6 p\u03b4 , (4.13) where \u03b4 = O(\u00b53\u03ba3r2 log0.75(d)\u03c3max(Z)\u22123(dp)\u22121/2). These two imply equation (4.11). Moreover, we have\np \u2225\u2225\u2225ZZ>X\u2225\u2225\u2225F = \u2225\u2225\u2225P\u2126(ZZ>)X\u2225\u2225\u2225F \u00b1 p\u03b4 = \u2225\u2225\u2225P\u2126(XX>)X + \u03bbR(X)\u2225\u2225\u2225F \u00b1 p\u03b4 (by equation (4.2))\n> \u2225\u2225\u2225P\u2126(XX>)X\u2225\u2225\u2225F \u00b1 p\u03b4 (by equation (4.8)) > p\n\u2225\u2225\u2225XX>X\u2225\u2225\u2225F \u00b1 2p\u03b4 (4.14) Suppose X has singular value \u03c31 > . . . > \u03c3r. Then we have\n\u2225\u2225\u2225ZZ>X\u2225\u2225\u22252F 6 \u2016ZZ>\u20162\u2016X\u20162F 6 \u03c3max(Z)4\u2016X\u20162F = \u03c3max(Z)4(\u03c321 + \u00b7 \u00b7 \u00b7 + \u03c32r ). On the other hand,\n\u2225\u2225\u2225XX>X\u2225\u2225\u22252F = \u03c361 + \u00b7 \u00b7 \u00b7 + \u03c36r . Therefore, equation (4.14) implies that (1 + O(\u03b4))\u03c3max(Z)4\nr\u2211 i=1 \u03c32i > r\u2211 i=1 \u03c36i (4.15)\nThen we have (by Proposition E.1) we complete the proof.\nNow we look at the second order optimality condition, this condition implies the smallest singular value of X is large (similar to Lemma 3.8). Note that this lemma is also true even if x only satisfies relaxed second order optimality condition with \u03c4 = 0.01p\u03c3min(Z).\nLemma 4.6. In the setting of Theorem 4.3. With high probability over the choice of \u2126, suppose X satisfies equation (4.9), (4.4) the 2nd order optimality condition (4.3). Then,\n\u03c3min(X) > 1 4 \u03c3min(Z) (4.16)\nProof. Let J = {i : \u2016Xi\u2016 6 \u03b1}. Let v \u2208 r such that \u2016Xv\u2016 = \u03c3min(X). . Let ZJ be the matrix that has the same i-th row as Z for every i \u2208 J and 0 elsewhere. Since ZJ has column rank at most r, by variational characterization of singular values, we have that for there exists unit vector zJ \u2208 col-span(ZJ) such that \u2016X>zJ\u2016 6 \u03c3min(X).\nWe claim that \u03c3min(ZJ) > 12\u03c3min(Z). Let L = [d] \u2212 J. Since for any i \u2208 L it holds that \u2016Xi\u2016 > \u03b1, we have |L|\u03b12 6 \u2016X\u20162F 6 2r\u03c3max(Z)2 (by equation (4.9)), and it follows that |L| 6 2r\u03c3max(Z)2/\u03b12. Therefore,\n\u03c3min(ZJ) > \u03c3min(Z) \u2212 \u03c3max(ZL) > \u03c3min(Z) \u2212 \u2016ZL\u2016F > \u03c3min(Z) \u2212 \u221a |L|r\u00b52/d > \u03c3min(Z) \u2212 \u221a 2r2\u03c3max(Z)2\u00b52/(\u03b12d)\n> 1 2 \u03c3min(Z) . (by \u03b1 > r\u03ba\u00b5\u221a d )\nSince zJ \u2208 col-span(ZJ) is a unit vector, we have that zJ can be written as zJ = ZJ\u03b2 where \u2016\u03b2\u2016 6 1\u03c3min(ZJ ) 6 O(1/\u03c3min(Z)). Therefore this in turn implies that \u2016zJ\u2016\u221e 6 \u2016ZJ\u20162\u2192\u221e\u2016\u03b2\u2016 6 O(\u00b5 \u221a r/d/\u03c3min(Z)) 6 O(\u00b5\u03ba \u221a r/d).\nWe will plug in V = zJvT in the 2nd order optimality condition (4.3). Note that since zJ \u2208 col-span(ZJ), it is supported on subset J, and therefore \u22072R(X)V = 0. Therefore the term about regularization in (4.3) will vanish. For simplicity, let y = X>zJ , w = Xv We obtain that taking V = zJv> in equation (4.3) will result in\u2225\u2225\u2225P\u2126(wz>J + zJw>)\u2225\u2225\u22252F > 2\u3008P\u2126(ZZ> \u2212 XX>), zJz>J \u3009 (4.17) Note that we have that \u2016w\u2016\u221e 6 \u2016X\u20162\u2192\u221e\u2016v\u2016 6 \u00b5 \u221a r/d. Recalling that \u2016zJ\u2016\u221e 6 O(\u00b5\u03ba \u221a r/d), by Theorem D.1, we have\nthat p \u2225\u2225\u2225wz>J + zJw>\u2225\u2225\u22252F > 2p\u3008ZZ> \u2212 XX>, zJz>J \u3009 \u2212 \u03b4p (4.18)\nwhere \u03b4 = O(\u00b52\u03bar2(pd)\u22121/2). Then simple algebraic manipulation gives that\n\u3008w, zJ\u30092 + \u2016w\u20162\u2016zJ\u20162 + \u2016X>zJ\u20162 > \u2016Z>zJ\u20162 \u2212 \u03b4/2 (4.19)\nNote that \u3008w, zJ\u3009 = \u3008v, X>zJ\u3009 = \u3008y, v\u3009. Recall that \u2016zJ\u2016 = 1 and z \u2208 col-span(ZJ), and therefore \u2016Z>zJ\u2016 = \u2016Z>J zJ\u2016 > \u03c32min(ZJ). Moreover, recall that \u2016y\u2016 = \u2016X>zJ\u2016 6 \u03c3min(X). Using these with equation (4.19) we obtain that\n\u3008w, zJ\u30092 + \u2016w\u20162\u2016zJ\u20162 + \u2016X>zJ\u20162 6 \u3008y, v\u30092 + \u2016w\u20162 + \u2016y\u20162\n6 2\u2016y\u20162 + \u03c32min(X) (by Cauchy-Schwarz and \u2016w\u2016 = \u03c3min(X).) 6 3\u03c32min(X) (by \u2016y\u2016 6 \u03c3min(X).)\nTherefore together with equation (4.19) and \u2016Z>zJ\u2016 > \u03c32min(ZJ) we obtain that\n\u03c3min(X) > (1/2 \u2212\u2126(\u03b4))\u03c3min(ZJ) (4.20)\nTherefore combining equation (4.20) and the lower bound on \u03c3min(ZJ) we complete the proof.\nSimilar as before, we show it is possible to remove the regularizer term here, again the intuition is that the regularizer is always in the same direction as X.\nLemma 4.7. Suppose X satisfies equation (4.4) and (4.16) and (4.10), then for any \u03b3 > 0,\u2225\u2225\u2225ZZT X \u2212 XXT X\u2225\u2225\u22252F 6 \u2225\u2225\u2225ZZT X \u2212 XXT X \u2212 \u03b3\u2207R(X)\u2225\u2225\u22252F (4.21) Proof. Let L = {i : \u2016Xi\u20162 > \u03b1}. For i < L, we have that (\u2207R(X))i = 0. Therefore it suffices to prove that for every i \u2208 L,\u2225\u2225\u2225ZiZ>X \u2212 XiX>X\u2225\u2225\u22252 6 \u2225\u2225\u2225ZiZ>X \u2212 XiX>X \u2212 (\u03b3\u2207R(X))i\u2225\u2225\u22252 It suffices to prov that \u3008(\u2207R(X))i, XiX>X \u2212 ZiZ>Z\u3009 > 0 (4.22) By proposition 4.2, we have \u2207R(X))i = \u0393Xi for \u0393 > 0. Then\n\u3008(\u2207R(X))i, XiX>X\u3009 = \u0393\u3008Xi, XiX>X\u3009 > \u0393 \u2016Xi\u20162 \u03c3min(XT X)\n> 1 4 \u0393 \u2016Xi\u20162 \u03c3min(Z) (by equation 4.16)\nOn the other hand, we have\n\u3008(\u2207R(X))i,ZiZ>Z\u3009 = \u0393\u3008Xi,ZiZ>X\u3009 6 \u0393\u2016Xi\u2016\u2016Zi\u2016\u03c3max(ZT X) 6 \u0393\u2016Xi\u2016\u2016Zi\u2016\u03c3max(Z)\u03c3max(X) 6 \u0393\u2016Xi\u2016\u2016Zi\u2016\u03c3max(Z)2r1/6 (by equation (4.10))\n6 1\n10 \u0393\u2016Xi\u20162\u03c3max(Z)2r\u22121/3 (by \u2016Xi\u2016 > \u03b1 > 10\n\u221a r\u2016Zi\u2016)\nTherefore combining two equations above we obtain equation (4.22) which completes the proof.\nFinally we show the form in Equation (4.21) implies ZZT is close to XXT (this is similar to Lemma 3.6).\nLemma 4.8. Suppose X and Z satisfies that \u03c3min(X) > 1/4 \u00b7 \u03c3min(Z) and that\u2225\u2225\u2225ZZT X \u2212 XXT X\u2225\u2225\u22252F 6 \u03b42 (4.23) where \u03b4 6 \u03c33min(Z)/C for a large enough constant C, then\n\u2016XX> \u2212 ZZ>\u20162F 6 O(\u03b4\u03ba2/\u03c3min(Z)). (4.24)\nProof. The proof is similar to the one-dimensional case, we will separate Z into the directions that are in column span of X and its orthogonal subspace. We will then show the projection of Z in the column span is close to X, and the projection on the orthogonal subspace must be small.\nLet Z = U + V where U = Projspan(X)Z is the projection of Z to the column span of X, and V is the projection to the orthogonal subspace. Then since VT X = 0 we know\nZZT X = (U + V)(U + V)T X = UUT X + VUT X.\nHere columns of the first term UUT X are in the column span of X, and the columns second term VUT X are in the orthogonal subspace. Therefore,\n\u2016ZZT X \u2212 XXT X\u20162F = \u2016UUT X \u2212 XXT X\u20162F + \u2016VUT X\u20162F 6 \u03b42.\nIn particular, both terms should be bounded by \u03b42. Therefore \u2016UUT \u2212 XXT \u20162F 6 \u03b42/\u03c32min(X) 6 16\u03b42/\u03c32min(Z). Also, we know \u03c3min(UUT X) > \u03c3min(XXT X) \u2212 \u03b4 > \u03c3min(Z)3/128 if \u03b4 6 \u03c3min(Z)3/128. Therefore \u03c3min(UT X) is at least \u03c3min(Z)3/\u2016Z\u2016128. Now \u2016V\u20162F 6 \u03b42/\u03c3min(UT X)2 6 O(\u03b42\u2016Z\u20162/\u03c3min(Z)6). Finally, we can bound \u2016UVT \u2016F by \u2016U\u2016\u2016V\u2016F 6 \u2016Z\u2016\u2016V\u2016F (last inequality is because U is a projection of Z), which at least \u2126(\u2016V\u20162F) when \u03b4 6 \u03c3min(Z)3/128, therefore\n\u2016ZZT \u2212 XXT \u2016F 6 \u2016UUT \u2212 XXT \u2016F + 2\u2016UVT \u2016F + \u2016VVT \u2016F 6 O(\u03b4\u2016Z\u20162/\u03c3min(Z)3).\nLast thing we need to prove the main theorem is a result from Sun and Luo[SL15], which shows whenever XXT is close to ZZT , the function is essentially strongly convex, and the only points that have 0 gradient are points where XXT = ZZT , this is explained in Lemma C.1. Now we are ready to prove Theorem 4.3:\nProof of Theorem 4.3. Suppose X satisfies 1st and 2nd order optimality condition. Then by Lemma 4.5 and Lemma 4.4, we have that X satisfies equation (4.4), (4.9), (4.10) and (4.11). Then by Lemma 4.6, we obtain that \u03c3min(X) > 1/6 \u00b7 \u03c3min(Z). Now by Lemma 4.7 and equation (4.11), we have that \u2225\u2225\u2225ZZT X \u2212 XXT X\u2225\u2225\u2225F 6 \u03b4 for \u03b4 6 c\u03c3min(Z)3/\u03ba2 for sufficiently small constant c. Then by Lemma 4.8 we obtain that \u2016ZZ> \u2212 XX>\u2016F 6 c\u03c3min(Z)2 for sufficiently small constant c. By Lemma C.1, in this region the only points that satisfy the first order optimality condition must satisfy XXT = ZZT .\nHandling Noise. To handle noise, notice that we can only hope to get an approximate solution in presence of noise, and to get that our Lemmas only depend on concentration bounds which still apply in the noisy setting. See Section B for details."}, {"heading": "5 Conclusions", "text": "Although the matrix completion objective is non-convex, we showed the objective function has very nice properties that ensures the local minima are also global. This property gives guarantees for many basic optimization algorithms. An important open problem is the robustness of this property under different model assumptions: Can we extend the result to handle asymmetric matrix completion? Is it possible to add weights to different entries (similar to the settings studied in [LLR16])? Can we replace the objective function with a different distance measure rather than Frobenius norm (which is related to works on 1-bit matrix sensing [DPvdBW14])? We hope this framework of analyzing the geometry of objective function can be applied to other problems."}, {"heading": "A Omitted Proofs in Section 3", "text": "We first prove the equivalent form of the first and second order optimality conditions:\nLemma A.1 (Proposition 3.1 restated). The first order optimality condition of objective (3.1) is,\n2P\u2126(M \u2212 xx>)x = \u03bb\u2207R(x) , (A.1)\nand the second order optimality condition requires:\n\u2200v \u2208 d, \u2016P\u2126(vx> + xv>)\u20162F + \u03bbv>\u22072R(x)v > 2v>P\u2126(M \u2212 xx>)v . (A.2)\nMoreover, The \u03c4-relaxed second order optimality condition requires\n\u2200v \u2208 d, \u2016P\u2126(vx> + xv>)\u20162F + \u03bbv>\u22072R(x)v > 2v>P\u2126(M \u2212 xx>)v \u2212 \u03c4\u2016v\u20162 . (A.3)\nProof. We take the Taylor\u2019s expansion around point x. Let \u03b4 be an infinitesimal vector, we have\nf (x + \u03b4) = 1 2 \u2016P\u2126(M \u2212 (x + \u03b4)(x + \u03b4)>)\u20162F + \u03bbR(x + \u03b4) + o(\u2016\u03b4\u20162)\n= 1 2 \u2016P\u2126(M \u2212 xx> \u2212 (x\u03b4> + \u03b4x>) \u2212 \u03b4\u03b4>)\u20162F + \u03bb\n( R(x) + \u3008\u2207R(x), \u03b4\u3009 + 1\n2 \u03b4T\u22072R(x)\u03b4\n) + o(\u2016\u03b4\u20162)\n= 1 2 \u2016M \u2212 xx>\u20162\u2126 + \u03bbR(x)\n\u2212 \u3008P\u2126(M \u2212 xx>), x\u03b4> + \u03b4x>\u3009 + \u3008\u2207R(x), \u03b4\u3009 + o(\u2016\u03b4\u20162)\n\u2212 \u3008P\u2126(M \u2212 xx>), \u03b4\u03b4>\u3009 + 1 2 \u2016x\u03b4> + \u03b4x>\u20162F + 1 2 \u03bb\u03b4>\u22072R(x)\u03b4 + o(\u2016\u03b4\u20162)."}, {"heading": "By symmetry \u3008P\u2126(M\u2212xx>), x\u03b4>\u3009 = \u3008P\u2126(M\u2212xx>), \u03b4x>\u3009 = \u3008P\u2126(M\u2212xx>)x, \u03b4\u3009, so the first order optimality condition", "text": "is \u2200\u03b4, \u3008\u22122P\u2126(M \u2212 xx>)x + \u03bb\u2207R(x), \u03b4\u3009 = 0, which is equivalent to that 2P\u2126(M \u2212 xx>)x = \u03bb\u2207R(x).\nThe second order optimality condition says \u2212\u3008P\u2126(M \u2212 xx>), \u03b4\u03b4>\u3009 + 12 \u2016x\u03b4> + \u03b4x>\u20162F + 1 2\u03bb\u03b4 >\u22072R(x)\u03b4 > 0 for every \u03b4, which is exactly equivalent to Equation (3.3).\nNext we show the full proof for the second order optimality condition:\nLemma A.2 (Lemma 3.8 restated). In the setting of Theorem 3.2, with high probability over the choice of \u2126, suppose x \u2208 B\u2032 satisfies second-order optimality condition (3.3) or \u03c4-relaxed condition for \u03c4 6 0.1p, we have \u2016x\u20162 > 1/8.\nProof. If \u2016x\u2016 > 1, then we are done. Therefore in the rest of the proof we assume \u2016x\u2016 6 1. The proof is very similar to Lemma 3.4. We plug in v = zJ instead into equation (3.3), where J = {i : |xi| 6 \u03b1}. Note that \u2207R(zJ) vanishes. We plug in v = zJ in the equation (3.3) and obtain that x satisfies that\u2225\u2225\u2225P\u2126(zJ x> + xz>J )\u2225\u2225\u22252F > 2z>J P\u2126(M \u2212 xx>)zJ . (A.4)\nNote that we assume \u2016x\u2016\u221e 6 2\u03b1, and in the beginning of the proof we assume wlog \u2016x\u2016 6 1. Moreover, we have \u2016zJ\u2016 6 \u00b5\u221ad an, \u2016zJ\u2016 6 1. Similarly to the derivation in the proof of Lemma 3.4, we apply Theorem D.1 (twice) and obtain that with high probability over the choice of \u2126, for every x, for \u03b5 = O\u0303(\u00b52(pd)\u22121/2),\nLHS of (A.4) = p \u2225\u2225\u2225zJ x> + xz>J \u2225\u2225\u22252F \u00b1 O(p\u03b5) = 2p\u2016x\u20162\u2016zJ\u20162 + 2p\u3008x, zJ\u30092 \u00b1 O(p\u03b5) . RHS of (A.4) = 2 ( \u3008P\u2126(zz>), P\u2126(zJz>J )\u3009 \u2212 \u3008P\u2126(xx>), P\u2126(zJz>J )\u3009 ) (Since M = zz>)\n= 2\u2016zJ\u20164 \u2212 2\u3008x, zJ\u30092 \u00b1 O(p\u03b5) . (by Theorem D.1)\n(Again notice that using \u03c4-relaxed second order optimality condition does not effect the RHS by too much, so it does not change later steps.) Therefore plugging the estimates above back into equation (A.4), we have that\np\u2016x\u20162\u2016zJ\u20162 + 2p\u3008x, zJ\u30092 > p\u2016zJ\u20164 \u00b1 O(p\u03b5) ,\nUsing Cauchy-Schwarz, we have \u2016x\u20162\u2016zJ\u20162 > \u3008x, zJ\u30092, and therefore we obtain that \u2016zJ\u20162\u2016x\u20162 > 13\u2016zJ\u20164 \u2212 O(\u03b5). Finally, we claim that \u2016zJ\u20162 > 1/2, which completes the proof since \u2016x\u20162 > 13\u2016zJ\u20162 \u2212 O(\u03b5) > 1/8. Claim A.3. Suppose \u03b1 > 4\u00b5\u221a d\nand x satisfies \u2016x\u2016\u221e 6 4\u03b1 and \u2016x\u2016 6 2. Let J = {i : |xi| 6 \u03b1}. Then we have that \u2016zJ\u2016 > 1/2.\nThe claim can be simply proved as follows: Since \u2016x\u20162 6 2 we have that |Jc| 6 2/\u03b12 and therefore \u2016zJc\u20162 6 2\u00b52/(d\u03b12). This further implies that \u2016zJ\u20162 = \u2016z\u20162 \u2212 \u2016zL\u20162 > (1 \u2212 2\u00b52/(d\u03b12)) > 12 because \u03b1 > 2\u00b5\u221a d .\nLemma A.4 (Lemma 3.10 restated). Suppose x \u2208 B\u2032 satisfies that \u2016x\u20162 > 1/8, under the same assumption as Lemma 3.9. we have, \u2225\u2225\u2225\u3008x, z\u3009z \u2212 \u2016x\u20162x\u2225\u2225\u2225 6 O(\u03b5) (A.5) Proof. We consider the subset of coordinates J = {i : |xi| 6 \u03b1} where the regularization g(x) doesn\u2019t have any effect. Let L = {i : |xi| > \u03b1} be the complement of J. By Claim A.3, we have that \u2016zL\u2016 > 1/2.\nFor coordinates that are in J, by equation (3.13), we have that\u2225\u2225\u2225\u3008x, z\u3009zJ \u2212 \u2016x\u20162xJ\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u3008x, z\u3009zJ \u2212 ((\u2016x\u20162xJ + \u03b3\u2207R(x)))J\u2225\u2225\u2225\u2225 6 O(\u03b5) . (A.6) Note by triangle inequality, (A.6) also implies \u3008x, z\u3009 > \u2016x\u20163 \u2212 O(\u03b5) > 1/20. Therefore we can divide both side by\n\u3008x, z\u3009, and let \u03b2 = \u2016x\u20162/\u3008x, z\u3009.\n\u2016zJ \u2212 \u03b2xJ\u2016 6 O(\u03b5) . (A.7) We now claim \u03b2 is large:\nClaim A.5. \u03b2 > 1 \u2212 O(\u03b5)\nProof. We first claim \u2016x\u2016 6 1 + O(\u03b5). This is because \u3008\u2207R(x), x\u3009 > 0, so \u2016\u2016x\u20162x + \u03b3\u2207R(x)\u2016 > \u2016\u2016x\u20162x\u2016 = \u2016x\u20163, and \u2016\u3008x, z\u3009z\u2016 6 \u2016x\u2016. When \u2016x\u2016 > 1 + C\u03b5 for large C we have \u2225\u2225\u2225\u3008z, x\u3009z \u2212 \u2016x\u20162x \u2212 \u03b3 \u00b7 \u2207R(x)\u2225\u2225\u2225 > C\u2032\u03b5 for large C\u2032 and that contradicts with (3.13).\nThis implies the norm of xJ is bounded by (1 + O(\u03b5))zJ because for any coordinate i in L, we know |xi| > \u03b1 > |zi|. Now we use (A.7) and triangle inequality:\n\u2016\u03b2xJ\u2016 > \u2016zJ\u2016 \u2212 O(\u03b5) (A.8) > (1 \u2212 O(\u03b5))\u2016zJ\u2016 (using \u2016zJ\u2016 > 1/2.) > (1 \u2212 O(\u03b5))\u2016xJ\u2016\nUsing the Claim, we can now consider the case when i \u2208 L. Without loss of generality let\u2019s assume xi > \u03b1. Since we have shown that \u03b2xi > (1 \u2212 O(\u03b5))\u03b1, we have that \u03b2xi > (1 \u2212 O(\u03b5))\u03b1 > |zi|. Moreover we have \u03bb\u2207R(x)i > 0 when xi > \u03b1. This implies |(\u03b2xi + \u03b3\u2207R(x)i) \u2212 zi| > |\u03b2xi \u2212 zi|. Therefore,\n\u2016zL \u2212 \u03b2xL\u2016 6 \u2225\u2225\u2225zL \u2212 (\u03b2x + \u03b3\u2207R(x))L\u2225\u2225\u2225\nCombining this with (A.6), we have the result."}, {"heading": "B Handling Noise", "text": "Suppose instead of observing the matrix ZZT , we actually observe a noisy version M = ZZT + N, where N is a Gaussian matrix with independent N(0, \u03c32) entries. In this case we should not hope to exactly recover ZZT (as two close Z\u2019s may generate the same observation). In this Section we show even with fairly large noise our arguments can still hold.\nTheorem B.1. Let \u00b5\u0302 = max{\u00b5, \u221a 4\u03c3d \u221a\nlog d r }. Suppose p > Cm\u0302u 6\u03ba12r4d\u22121\u03b5\u22122 log1.5 d where C is a large enough\nconstant. Let \u03b1 = 2\u00b5\u0302\u03bar/ \u221a\nd, \u03bb > \u00b5\u03022rp/\u03b12. Then with high probability over the randomness of \u2126, any local minimum X of f (\u00b7) satisfies\n\u2016XXT \u2212 ZZT \u2016F 6 \u03b5. In fact, a noise level \u03c3 \u221a log d 6 \u00b52r/d (when the noise is almost as large as the maximum possible entry) does not\nchange the conclusions of Lemmas in this Section.\nProof. There are only three places in the proof where the noise will make a difference. These are: 1. The infinity norm bound of M, used in Lemma 4.4. 2. The LHS of first order optimality condition (Equation (4.2)). 3. The RHS of second order optimality condition (Equation (4.3)).\nWhat we require in these three steps are: 1. |M|\u221e should be smaller than \u00b52r/d. 2. \u3008P\u2126(N),W\u3009 should be smaller than |\u3008P\u2126(N), P\u2126(W)\u3009| 6 O(\u03c3|Z|\u221edr log d + \u221a pd2r\u03c32|W |\u221e\u2016W\u2016F log d). 3. \u2016P\u2126(N)\u2016 6 \u03b5p\u2016ZZT \u2016F . When we define the\n\u00b5\u0302 = max{\u00b5, \u221a 4\u03c3d \u221a\nlog d r }, all of these are satisfied (by Lemma D.5 and D.6).\nNow we can follow the proof and see \u03b4 6 c\u03b5\u03c3min(Z)/\u03ba2 for small enough constant c, and By Lemma 4.8 we know \u2016XXT \u2212 ZZT \u2016F 6 \u03b5."}, {"heading": "C Finding the Exact Factorization", "text": "In Section 4, we showed that any point that satisfies the first and second order necessary condition must satisfy \u2016XXT \u2212 ZZT \u2016F 6 c for a small enough constant c. In this section we will show that in fact XXT must be exactly equal to ZZT . The proof technique here is mostly based on the work of Sun and Luo[SL15]. However we have to modify their proof because we use slightly different regularizers, and we work in the symmetric case. The main Lemma in [SL15] can be rephrased as follows in our setting:\nLemma C.1 (Analog to Lemma 3.1 in [SL15]). Assuming p is at least C\u00b54r6\u03ba4d\u22121 log d for large enough d, for any X such that \u2016XXT \u2212 ZZT \u2016F 6 \u03b5 6 \u03c3min(Z)2/100, \u2016X\u20162\u2192\u221e is bounded as in Lemma 4.4, there must be a matrix U such that UUT = ZZT and\n\u3008\u2207 f (X), X \u2212 U\u3009 > p 4 \u2016M \u2212 XXT \u20162F .\nAs a corollary, the points in this set that satisfy first order optimality condition must have XXT = ZZT .\nIn order to prove this main lemma, we separate f (X) = g(X) + R(X) where g(X) = 12 \u2016P\u2126(M \u2212 XXT \u20162F , and R(X) is the regularizer. We will first show that the regularizer always has a positive correlation with X \u2212 U. Claim C.2. For any U such that UUT = ZZT ,\n\u3008\u2207R(X), X \u2212 U\u3009 > 0.\nProof. Since the regularizer is applied to individual rows, we will focus on a row Xi. The key observation here is that for each row Xi, \u2207R(Xi) is 0 when \u2016Xi\u2016 6 2\u00b5/ \u221a d 6 \u03b1. When \u2016Xi\u2016 is larger \u2207R(Xi) is always in the same direction as\nXi. Therefore we know either \u2207R(Xi) = 0, or \u2207R(Xi) = \u03bbXi for some \u03bb > 0 and \u2016Xi\u2016 > 2\u00b5/ \u221a\nd > 2\u2016Zi\u2016 = 2\u2016Ui\u2016. The innerproduct \u3008\u2207R(Xi), Xi \u2212 Ui\u3009 is 0 in the former case, and is at least \u03bb(\u2016Xi\u20162 \u2212 \u2016Xi\u2016\u2016Ui\u2016) in the latter case.\nNote that this Claim is much more complicated in [SL15] (Claim 4.1) in order to deal with the asymmetric case. Next we will prove the gradient of g(X) has a large correlation with X \u2212 U\nClaim C.3. There exists a matrix U where UUT = ZZT , such that\n\u3008\u2207R(X), X \u2212 U\u3009 > p 4 \u2016M \u2212 XXT \u20162F .\nThe proof of this Claim follows from the same strategy as in [SL15]. We will first show that there exists a matrix U such that \u2016U \u2212 X\u2016F is small, and then use concentration bounds to show the lowerbound on innerproduct. The concentration bounds are exactly the same as [SL15] (see the proof of Proposition 4.3), the only thing we need to do here is an equivalent version of Proposition 4.1 and 4.2:\nClaim C.4. Suppose \u2016XXT \u2212 M\u2016F = \u03b5 6 \u03c3min(Z)2/100, there exists a matrix U such that UUT = M and \u2016X \u2212 U\u2016F 6 5\u03b5 \u221a r/\u03c3min(Z)2.\nNote that in this claim we do not need to prove anything about the row norm/Frobenius norm of U, because whenever UUT = M, U must have the same row norm and Frobenius norm as the original low rank component Z. That is why the proof here is again much simpler than [SL15].\nProof. Without loss of generality we assume M is a diagonal matrix with first r diagonal terms being \u03c31(Z)2, \u03c32(Z)2, ..., \u03c3r(Z)2 (this can be done by a change of basis). We use M\u2032 to denote the first r \u00d7 r submatrix of M.\nNow we write X in a block form XT = (VT WT ), where V is an r \u00d7 r matrix and W is an (d \u2212 r) \u00d7 r matrix. Clearly, we want to truncate the W part (because the corresponding part in M is empty), and we need to \u201cfix\u201d the V part so that the first r \u00d7 r submatrix in XXT is exactly equal to M.\nWe will construct the first r\u00d7 r block of U as P = VQ = V(VT M\u22121V)\u22121/2 (where Q := (VT M\u22121V)\u22121/2), all the other entries of U are 0. The difference in U and X is equal to \u2016U \u2212 X\u2016F 6 \u2016P \u2212 V\u2016F + \u2016W\u2016F .\nSince \u2016XXT \u2212 M\u2016F = \u03b5, we know \u2016M\u2032 \u2212 VVT \u20162F + 2\u2016VWT \u20162F 6 \u03b52. In particular both terms are smaller than \u03b52. First, we bound \u2016W\u2016F . Note that since \u2016M\u2032 \u2212 VVT \u2016F 6 \u03b5 6 \u03c3min(Z)2/100, we know \u03c3min(V)2 > 0.99\u03c3min(Z)2. Therefore \u03c3min(V) > 0.9\u03c3min(Z). Now we know \u2016W\u2016F 6 \u2016VWT \u2016F/\u03c3min(V) 6 2\u03b5/\u03c3min(Z). Next we bound \u2016P \u2212 V\u20162F . Since \u2016M\u2032 \u2212 VVT \u2016F 6 \u03b5 le\u03c3min(Z)2/100, we know (1 \u2212 2\u03b5/\u03c3min(Z)2)VVT M\u2032 (1 + 2\u03b52/\u03c3min(Z)2)VVT . This implies \u2016V\u2016F 6 1.1\u2016Z\u2016F , and (1 \u2212 2\u03b5/\u03c3min(Z)2)I VT M\u22121V (1 + 2\u03b5/\u03c3min(Z)2)I. Therefore the matrix Q is also very close to identity, in particular, \u2016Q\u2212 I\u2016 6 2\u03b5/\u03c3min(Z)2. Now we know \u2016P\u2212V\u2016F = \u2016V\u2016F\u2016Q\u2212 I\u2016 6 3\u03b5\u2016Z\u2016F/\u03c3min(Z)2. Using the fact that \u2016Z\u2016F = 1 we know \u2016U \u2212 X\u2016F 6 \u2016P \u2212 V\u2016F + \u2016W\u2016F 6 5\u03b5 \u221a r/\u03c3min(Z)2."}, {"heading": "D Concentration inequality", "text": "In this section we prove the concentration inequalities used in the main part. We first show that the inner-product of two low rank matrices is preserved after restricting to the observed entries. This is mostly used in arguments about the second order necessary conditions.\nTheorem D.1. With high probability over the choice of \u2126, for any two rank-r matrices W,Z \u2208 d\u00d7d, we have\n|\u3008P\u2126(W), P\u2126(Z)\u3009 \u2212 p\u3008W,Z\u3009| 6 O(|W |\u221e|Z|\u221edr log d + \u221a pdr|W |\u221e|Z|\u221e \u2016W\u2016F \u2016Z\u2016F log d)\nProof. Since both LHS and RHS are bilinaer in both W and Z, without loss of generality we assume the Frobenius norms of W and Z are all equal to 1. Note that in this case we should expect |W |\u221e > 1/d.\nLet \u03b4i, j be the indicator variable for \u2126, we know \u3008P\u2126(W,Z\u3009 = \u2211 i, j \u03b4i, jWi, jZi, j,\nand in expectation it is equal to p\u3008W,Z\u3009. Let Q = \u2211i, j(\u03b4i, j \u2212 p)Wi, jZi, j. We can then view Q as a sum of independent entries (note that \u03b4i, j = \u03b4 j,i, but we can simply merge the two terms and the variance is at most a factor 2 larger). The expectation [Q] = 0. Each entry in the sum is bounded by |W |\u221e|Z|\u221e, and the variance is bounded by\n[Q] 6 p \u2211 i, j (Wi, jZi, j)2\n6 p max i, j |Wi, j|2 \u2211 i, j Z2i, j\n6 p|W |2\u221e.\nSimilarly, we also know [Q] 6 p|Z|2\u221e and hence [Q] 6 p|W |\u221e|Z|\u221e. Now we can apply Bernstein\u2019s inequality, with probability at most \u03b7,\n|Q \u2212 [Q]| > |W |\u221e|Z|\u221e log 1/\u03b7 + \u221a\np|W |\u221e|Z|\u221e log(1/\u03b7). By Proposition E.2, there is a set \u0393 of size dO(dr) such that for any rank r matrix X, there is a matrix X\u0302 \u2208 \u0393 such that \u2016X \u2212 X\u0302\u2016F 6 1/d3. When W and Z come from this set, we can set \u03b7 = d\u2212Cdr for a large enough constant C. By union bound, with high probability\n|Q \u2212 [Q]| 6 O(|W |\u221e|Z|\u221edr log d + \u221a pdr|W |\u221e|Z|\u221e log d).\nWhen W and Z are not from this set \u0393, let W\u0302 and Z\u0302 be the closest matrix in \u0393, then we know |\u3008P\u2126(W), P\u2126(Z)\u3009 \u2212 p\u3008W,Z\u3009 \u2212 (\u3008P\u2126(W\u0302), P\u2126(Z\u0302)\u3009 \u2212 p\u3008W\u0302, Z\u0302\u3009)| 6 O(1/d3) |W |\u221e|Z|\u221edr log d. Therefore we still have\n|\u3008P\u2126(W), P\u2126(Z)\u3009 \u2212 p\u3008W,Z\u3009| 6 O(|W |\u221e|Z|\u221edr log d + \u221a pdr|W |\u221e|Z|\u221e \u2016W\u2016F \u2016Z\u2016F log d).\nNext Theorem shows P\u2126(XXT )X is roughly equal to pXXT X, this is one of the major terms in the gradient.\nTheorem D.2. When p > C\u03bd 6r log1.5 d\nd\u03b52 for a large enough constant C, With high probability over the randomness of \u2126, for any matrix X \u2208 d\u00d7r such that \u2016Xi\u2016 6 \u03bd \u221a 1 d \u2016X\u2016F , we have\n\u2016P\u2126(XX>)X \u2212 pXXT X\u2016F 6 p\u03b5\u2016X\u20163F (D.1)\nProof. Without loss of generality we assume \u2016X\u2016F = 1. Let \u03b4i, j be the indicator variable for \u2126, we first prove the result when \u03b4i, j are independent, then we will use standard techniques to show the same argument works for \u03b4i, j = \u03b4 j,i.\nNote that [P\u2126(XX>)X]i = \u2211 j \u03b4i, j\u3008Xi, X j\u3009X j,\nwhose expectation is equal to [pXXT X]i = p \u2211 j \u3008Xi, X j\u3009X j.\nWe know \u2016Xi\u2016 6 \u03bd \u221a 1 d , therefore each term is bounded by \u03bd 3(1/d)3/2. Let Zi be a random variable that is equal to \u2016P\u2126(XX>)X]i \u2212 [pXXT X]i\u20162, then it is easy to see [Zi] 6 pd\u03bd6(r/d)3 = p\u03bd6/d2. and the variance [Zi] = [Z2i ] \u2212 [Zi]2 6 pd\u03bd12(1/d)6 + 2 [Zi]2 6 3 [Zi]2 (as long as p > 1/d). Our goal now is to prove \u2211d i=1 Zi 6 p\n2\u03b52 for all X.\nLet Zi be a truncated version of Zi. That is, Zi = Zi when Zi 6 [2pd\u03bd3(1/d)3/2]2, and Zi = [2pd\u03bd3(1/d)3/2]2 otherwise. It\u2019s not hard to see Zi has smaller mean and variance compared to Zi. Also, by vector\u2019s Bernstein\u2019s inequality, we know\n[ \u221a Zi 6 t] 6 d exp \u2212 t2p\u03bd6 d2 + t \u03bd3 d3/2  . Notice that this is only relevant when t 6 O(p\u03bd3d\u22121/2) (because otherwise the probability is 0) and in that regime\nthe variance term always dominates. Therefore Zi is the square of a subgaussian random variable. By the Bernstein\u2019s inequality, we know the moments of \u221a Zi \u2212 [ \u221a\nZi] are dominated by a Gaussian distribution with variance O( [Zi \u221a log d).\nNow we can use the concentration bound for quadratics of the subgaussian random variables[HKZ12]: we know that with probability exp(\u2212t),\nd\u2211 i=1 Zi 6 O( [Z2i ] \u221a log d(d + 2 \u221a dt + 2t)).\nthis means with probability exp(\u2212Cdrlogd) with some large constant C, we know \u2211di=1 Zi 6 O(p\u03bd6r log1.5 d/d). The probability is low enough for us to union bound over all X in a standard \u03b5-net such that every other X is within distance (\u03b5/d)6. Therefore we know with high probability for all X in the \u03b5-net we have \u2211d i=1 Zi 6 O(p\u03bd 6r log1.5 d/d), which is smaller than p2\u03b52 when p > C\u03bd 6r log1.5 d\nd\u03b52 for a large enough constant C. For any X\u0302 that is not in the \u03b5-net, let X be the closest point of X in the net, then \u2016X \u2212 X\u0302\u2016F 6 1/d6, therefore the bound of X\u0302 clearly follows from the bound of X. Now to convert sum of Zi to sum of Zi, notice that with high probability there are at most 2pd entries in \u2126 for every\nrow. When that happens Zi is always bounded by [2pd\u03bd3(1/d)3/2]2 so Zi = Zi. Let event 1 be \u2211d i=1 Zi 6 p 2\u03b52 for all X, and let event 2 be that there are at most 2pd entries per row, we know with high probability both event happens, and in that case \u2211d i=1 Zi 6 p 2\u03b52 for all X.\nHandling \u03b4i, j = \u03b4 j,i. First notice that the diagonal entries \u03b4i,i\u2019s cannot change the Frobenius norm by more than O(\u03bd3(1/d)3/2 \u00b7 \u221a d) 6 p\u03b5 so we can ignore the diagonal terms. Now for independent terms \u03b4i, j, let \u03b3 j,i = \u03b4i, j, then by union bound both \u03b4i, j and \u03b3i, j satisfy the equation, and by triangle\u2019s inequality (\u03b4i, j + \u03b3i, j)/2 also satisfies the inequality. Let \u03c4i, j be the true indicator of \u2126 (hence \u03c4i, j = \u03c4 j,i), and \u03c4\u2032i, j be an independent copy, we know (\u03c4i, j + \u03c4 \u2032 i, j)/2 has the same distribution as (\u03b4i, j +\u03b3i, j)/2 (for off-diagonal entries), therefore with high probability the equation is true for (\u03c4i, j +\u03c4\u2032i, j)/2. The Theorem then follows from the standard Claim below for decoupling (note that sup\u2016X\u2016F=1 \u2016P\u2126(XX\nT )X \u2212 pXXT X\u2016F is a norm for the indicator variables of \u2126): Claim D.3 (see e.g. [? ]). Let X,Y be two iid random variables, then\n[\u2016X\u2016 > t] 6 3 [\u2016X + Y\u2016 > 2t 3 ].\nProof. Let X,Y,Z be iid random variables then,\n[X > t] = [\u2016(X + Y) + (X + Z) \u2212 (Y + Z)\u2016 > 2t] 6 [\u2016X + Y\u2016 > 2t/3] + [\u2016X + Z\u2016 > 2t/3] + [\u2016Y + Z\u2016 > 2t/3]\n6 3 [\u2016X + Y\u2016 > 2t 3 ].\nFinally we argue that random sampling of a matrix gives a nice spectral approximation. This is a standard Lemma that is used in arguing about the P\u2126(M)X term in the gradient (P\u2126(M \u2212 XXT )X).\nLemma D.4. Suppose W \u2208 d\u00d7d satisfies that |W |\u221e 6 \u03bdd \u2016W\u2016F , then with high probability (1 \u2212 d\u221210) over the choice of \u2126, \u2016P\u2126(W) \u2212 pW\u2016 6 \u03b5p\u2016W\u2016F . where \u03b5 = O(\u03bd \u221a log d/(pd)).\nProof. Without loss of generality we assume \u2016W\u2016F = 1. The proof follows simply from application of Bernstein inequality. We view P\u2126(W) as P\u2126(W) = \u2211\ni, j\u2208[d]2 si jWi j\u03b4i j\nwhere \u03b4i j \u2208 d\u00d7d is the indicator matrix for entry (i, j), and si j \u2208 {0, 1} are independent Bernoulli variable with probability p of being 1. Then we have that [P\u2126(W)] = pW and \u2016si jWi j\u03b4i j\u2016 6 \u03bdd \u2016W\u2016F . Moreover, we compute the variance by \u2211\ni, j\u2208[d]2 [si jW2i j\u03b4 > i j\u03b4i j] = \u2211 i, j\u2208[d]2 [si jW2i j\u03b4 j j]\n= \u2211 j\u2208[d] p \u2211 i\u2208d W2i j  \u03b4 j j (D.2) Therefore \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i, j\u2208[d]2 [si jW2i j\u03b4 > i j\u03b4i j] \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 6 p\u03bd 2 d\nSimilarly we can control \u2225\u2225\u2225\u2225\u2211i, j\u2208[d]2 [si jW2i j\u03b4i j\u03b4>i j]\u2225\u2225\u2225\u2225 by p\u03bd2/d (again notice that although \u03b4i, j = \u03b4 j,i the bounds here are correct up to constant factors). Then it follows from non-commutative Bernstein inequality [Imb10] that\n\u2126\n[\u2016P\u2126(W) \u2212 p(W)\u2016 > \u03b5p] 6 d exp(\u22122\u03b52 pd/\u03bd2) .\nConcentration Lemmas for Noise Matrix N. Next we will state the concentration lemmas that are necessary when observed matrix is perturbed by Gaussian noise. The proof of these Lemmas are really exactly the same (in fact even simpler) than the corresponding Theorem that we have just proven. The first Lemma is used in the same settings as Theorem D.1.\nLemma D.5. Let N be a random matrix with independent Gaussian entries N(0, \u03c32). With high probability over the support \u2126 and the Gaussian N, for any low rank matrix W, we have\n|\u3008P\u2126(N), P\u2126(W)\u3009| 6 O(\u03c3|Z|\u221edr log d + \u221a pd2r\u03c32|W |\u221e\u2016W\u2016F log d\nProof. The proof is exactly the same as Theorem D.1 as |\u3008P\u2126(N), P\u2126(W)\u3009| is a sum of independent entries that follows from the same Bernstein\u2019s inequality.\nNext we show that random sampling entries of a Gaussian matrix gives a matrix with low spectral norm.\nLemma D.6. Let N be a random Gaussian matrix with independent Gaussian entries N(0, \u03c32), with high probability over the choice of \u2126 and N, we have \u2016P\u2126(N)\u2016 6 \u03b5p\u03c3d, where \u03b5 = O( \u221a log d/pd).\nProof. Again the proof follows from the same argument as Lemma D.4."}, {"heading": "E Auxiliary Lemmas", "text": "Proposition E.1. Let a1, . . . , ar > 0, C > 0. Then C4(a21 + \u00b7 \u00b7 \u00b7 + a2r ) > a61 + \u00b7 \u00b7 \u00b7 + a6r implies that a21 + \u00b7 \u00b7 \u00b7 + a2r 6 C2r and that max ai 6 Cr1/6.\nProof. By Cauchy-Schwarz inequality, we have, r\u2211 i=1 a2i   r\u2211 i=1 a6i  >  r\u2211 i=1 a4i 2 > 1r  r\u2211 i=1 a2i 2  2\nUsing the assumption and equation above we have that a21 + \u00b7 \u00b7 \u00b7 + a2r 6 C2r. This implies with the condition that a61 + \u00b7 \u00b7 \u00b7 + a6r 6 C6r, which implis that max ai 6 Cr1/6.\nProposition E.2. For any \u03b6 \u2208 (0, 1), there is a set \u0393 of rank r d \u00d7 d matrices, such that for any rank r d \u00d7 d matrix X with Frobenius norm at most 1, there is a matrix X\u0302 \u2208 \u0393 with \u2016X \u2212 X\u0302\u2016F 6 \u03b6. The size of \u0393 is bounded by (d/\u03b6)O(dr).\nProof. Standard construction of \u03b5-net shows that there is a set P \u2282 d of size (d/\u03b5)O(d) such that for any \u2016u\u2016 6 1, there is a u\u0302 \u2208 P such that \u2016u \u2212 u\u0302\u2016 6 \u03b5. Such construction can also be applied to matrices and Frobenius norm as that is the same as vectors and `2 norm.\nHere we let \u03b5 = 0.1\u03b6, and construct three sets P1, P2, P3 where P1 is an \u03b5-net for d \u00d7 r matrices with Frobenius norm at most \u221a r, P2 is an \u03b5-net for r \u00d7 r diagonal matrices whose Frobenius norm is bounded by 1, and P3 is an \u03b5-net for r \u00d7 d matrices with Frobenius norm at most \u221a\nr. Now we define \u0393 = {U\u0302D\u0302V\u0302 |U\u0302 \u2208 P1, D\u0302 \u2208 P2, V\u0302 \u2208 P3}. Clearly the size of \u0393 is as promised. For any rank r d \u00d7 d matrix X, suppose its Singular Value Decomposition is UDV , we can find U\u0302 \u2208 P1, D\u0302 \u2208 P2 and V\u0302 \u2208 P3 that are \u03b5 close to U,D,V respectively. Therefore U\u0302D\u0302V\u0302 \u2208 \u0393 and it is easy to check\n\u2016UDV \u2212 U\u0302D\u0302V\u0302\u2016F 6 8\u03b5 6 \u03b6."}], "references": [{"title": "Uncovering shared structures in multiclass classification", "author": ["Yonatan Amit", "Michael Fink", "Nathan Srebro", "Shimon Ullman"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Amit et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amit et al\\.", "year": 2007}, {"title": "On the low-rank approach for semidefinite programs arising in synchronization and community detection", "author": ["Afonso S Bandeira", "Nicolas Boumal", "Vladislav Voroninski"], "venue": "arXiv preprint arXiv:1602.04426,", "citeRegEx": "Bandeira et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bandeira et al\\.", "year": 2016}, {"title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization", "author": ["Samuel Burer", "Renato DC Monteiro"], "venue": "Mathematical Programming,", "citeRegEx": "Burer and Monteiro.,? \\Q2003\\E", "shortCiteRegEx": "Burer and Monteiro.", "year": 2003}, {"title": "Do we need good initialization for low rank matrix recovery", "author": ["Srinadh Bhojanapalli", "Behnam Neyshabur", "Nathan Srebro"], "venue": "Personal Communication,", "citeRegEx": "Bhojanapalli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bhojanapalli et al\\.", "year": 2016}, {"title": "Robust principal component analysis", "author": ["Emmanuel J Cand\u00e8s", "Xiaodong Li", "Yi Ma", "John Wright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["Emmanuel J Cand\u00e8s", "Terence Tao"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2010\\E", "shortCiteRegEx": "Cand\u00e8s and Tao.", "year": 2010}, {"title": "Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees", "author": ["Yudong Chen", "Martin J Wainwright"], "venue": "arXiv preprint arXiv:1509.03025,", "citeRegEx": "Chen and Wainwright.,? \\Q2015\\E", "shortCiteRegEx": "Chen and Wainwright.", "year": 2015}, {"title": "1-bit matrix completion", "author": ["Mark A Davenport", "Yaniv Plan", "Ewout van den Berg", "Mary Wootters"], "venue": "Information and Inference,", "citeRegEx": "Davenport et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Davenport et al\\.", "year": 2014}, {"title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": null, "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Understanding alternating minimization for matrix completion", "author": ["Moritz Hardt"], "venue": "In FOCS 2014. IEEE,", "citeRegEx": "Hardt.,? \\Q2014\\E", "shortCiteRegEx": "Hardt.", "year": 2014}, {"title": "A tail inequality for quadratic forms of subgaussian random vectors", "author": ["Daniel Hsu", "Sham M Kakade", "Tong Zhang"], "venue": "Electron. Commun. Probab,", "citeRegEx": "Hsu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2012}, {"title": "Fast matrix completion without the condition number", "author": ["Moritz Hardt", "Mary Wootters"], "venue": "COLT", "citeRegEx": "Hardt and Wootters.,? \\Q2014\\E", "shortCiteRegEx": "Hardt and Wootters.", "year": 2014}, {"title": "Sums of random Hermitian matrices and an inequality by Rudelson", "author": ["R. Imbuzeiro Oliveira"], "venue": "ArXiv e-prints,", "citeRegEx": "Oliveira.,? \\Q2010\\E", "shortCiteRegEx": "Oliveira.", "year": 2010}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "Matrix completion from a few entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "The bellkor solution to the netflix grand prize", "author": ["Yehuda Koren"], "venue": "Netflix prize documentation,", "citeRegEx": "Koren.,? \\Q2009\\E", "shortCiteRegEx": "Koren.", "year": 2009}, {"title": "Recovery guarantee of weighted low-rank approximation via alternating minimization", "author": ["Yuanzhi Li", "Yingyu Liang", "Andrej Risteski"], "venue": "arXiv preprint arXiv:1602.02262,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Gradient descent converges to minimizers", "author": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "University of California, Berkeley,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Cubic regularization of Newton method and its global performance", "author": ["Yurii Nesterov", "Boris T Polyak"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov and Polyak.,? \\Q2006\\E", "shortCiteRegEx": "Nesterov and Polyak.", "year": 2006}, {"title": "Nonconvergence to unstable points in urn models and stochastic approximations", "author": ["Robin Pemantle"], "venue": "The Annals of Probability,", "citeRegEx": "Pemantle.,? \\Q1990\\E", "shortCiteRegEx": "Pemantle.", "year": 1990}, {"title": "A simpler approach to matrix completion", "author": ["Benjamin Recht"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Recht.,? \\Q2011\\E", "shortCiteRegEx": "Recht.", "year": 2011}, {"title": "Fast maximum margin matrix factorization for collaborative prediction", "author": ["Jasson DM Rennie", "Nathan Srebro"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Rennie and Srebro.,? \\Q2005\\E", "shortCiteRegEx": "Rennie and Srebro.", "year": 2005}, {"title": "Guaranteed matrix completion via nonconvex factorization", "author": ["Ruoyu Sun", "Zhi-Quan Luo"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "Sun and Luo.,? \\Q2015\\E", "shortCiteRegEx": "Sun and Luo.", "year": 2015}, {"title": "When are nonconvex problems not scary", "author": ["Ju Sun", "Qing Qu", "John Wright"], "venue": "arXiv preprint arXiv:1510.06096,", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "A nonconvex optimization framework for low rank matrix estimation", "author": ["Tuo Zhao", "Zhaoran Wang", "Han Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization suffices in practice. We prove that the commonly used non-convex objective function for matrix completion has no spurious local minima \u2013 all local minima must also be global. Therefore many popular optimization algorithms such as (stochastic) gradient descent can provably solve matrix completion with arbitrary initialization in polynomial time.", "creator": "LaTeX with hyperref package"}}}