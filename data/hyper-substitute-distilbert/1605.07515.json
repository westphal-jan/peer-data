{"id": "1605.07515", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2016", "title": "Neural Semantic Role Labeling with Dependency Path Embeddings", "abstract": "that paper introduces a paradigm scenario describing statistical role formation that makes use of knowledge sequence modeling feasible. our approach is formulated beneath scientific observation that generic syntactic complexes and mathematical phenomena, disguised as nested subordinations and nominal networks, are not marked well by existing models. mathematical model treats such instances as sub - sequences producing lexicalized mathematical modules and possesses suitable embedding representations. scholars gradually learn these neural embeddings dramatically improve results over comparable state - including - the - art semantic role labelers, and evaluate conceptual projections obtained by our computers.", "histories": [["v1", "Tue, 24 May 2016 15:54:48 GMT  (38kb)", "https://arxiv.org/abs/1605.07515v1", "Accepted at ACL 2016"], ["v2", "Mon, 18 Jul 2016 09:08:51 GMT  (34kb)", "http://arxiv.org/abs/1605.07515v2", "Camera-ready ACL paper"]], "COMMENTS": "Accepted at ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["michael roth", "mirella lapata"], "accepted": true, "id": "1605.07515"}, "pdf": {"name": "1605.07515.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mroth@inf.ed.ac.uk", "mlap@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n07 51\n5v 2\n[ cs\n.C L\n] 1\n8 Ju\nl 2 01\n6"}, {"heading": "1 Introduction", "text": "The goal of semantic role labeling (SRL) is to identify and label the arguments of semantic predicates in a sentence according to a set of predefined relations (e.g., \u201cwho\u201d did \u201cwhat\u201d to \u201cwhom\u201d). Semantic roles provide a layer of abstraction beyond syntactic dependency relations, such as subject and object, in that the provided labels are insensitive to syntactic alternations and can also be applied to nominal predicates. Previous work has shown that semantic roles are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015).\nThe task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In\nSystem Analysis\nmate-tools *He had [troubleA0] raising [fundsA1]. mateplus *He had [troubleA0] raising [fundsA1]. TensorSRL *He had trouble raising [fundsA1]. easySRL *He had trouble raising [fundsA1].\ntheir work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008).\nMost semantic role labeling approaches to date rely heavily on lexical and syntactic indicator features. Through the availability of large annotated resources, such as PropBank (Palmer et al., 2005), statistical models based on such features achieve high accuracy. However, results often fall short when the input to be labeled involves instances of linguistic phenomena that are relevant for the labeling decision but appear infrequently at training time. Examples include control and raising verbs, nested conjunctions or other recursive structures, as well as rare nominal predicates. The difficulty lies in that simple lexical and syntactic indicator features are not able to model interactions trig-\ngered by such phenomena. For instance, consider the sentence He had trouble raising funds and the analyses provided by four publicly available tools in Table 1 (mate-tools, Bjo\u0308rkelund et al. (2010); mateplus, Roth and Woodsend (2014); TensorSRL, Lei et al. (2015); and easySRL, Lewis et al. (2015)). Despite all systems claiming state-of-the-art or competitive performance, none of them is able to correctly identify He as the agent argument of the predicate raise. Given the complex dependency path relation between the predicate and its argument, none of the systems actually identifies He as an argument at all.\nIn this paper, we develop a new neural network model that can be applied to the task of semantic role labeling. The goal of this model is to better handle control predicates and other phenomena that can be observed from the dependency structure of a sentence. In particular, we aim to model the semantic relationships between a predicate and its arguments by analyzing the dependency path between the predicate word and each argument head word. We consider lexicalized paths, which we decompose into sequences of individual items, namely the words and dependency relations on a path. We then apply long-short term memory networks (Hochreiter and Schmidhuber, 1997) to find a recurrent composition function that can reconstruct an appropriate representation of the full path from its individual parts (Section 2). To ensure that representations are indicative of semantic relationships, we use semantic roles as target labels in a supervised setting (Section 3).\nBy modeling dependency paths as sequences of words and dependencies, we implicitly address the data sparsity problem. This is the case because we use single words and individual dependency relations as the basic units of our model. In contrast, previous SRL work only considered full syntactic paths. Experiments on the CoNLL-2009 benchmark dataset show that our model is able to outperform the state-of-the-art in English (Section 4), and that it improves SRL performance in other languages, including Chinese, German and Spanish (Section 5)."}, {"heading": "2 Dependency Path Embeddings", "text": "In the context of neural networks, the term embedding refers to the output of a function f within the network, which transforms an arbitrary input into a real-valued vector output. Word embeddings, for\ninstance, are typically computed by forwarding a one-hot word vector representation from the input layer of a neural network to its first hidden layer, usually by means of matrix multiplication and an optional non-linear function whose parameters are learned during neural network training.\nHere, we seek to compute real-valued vector representations for dependency paths between a pair of words \u3008wi,w j\u3009. We define a dependency path to be the sequence of nodes (representing words) and edges (representing relations between words) to be traversed on a dependency parse tree to get from node wi to node w j. In the example in Figure 1, the dependency path from raising to he is raising NMOD\u2212\u2212\u2212\u2192 trouble OBJ\u2212\u2212\u2192had SBJ\u2190\u2212he.\nAnalogously to how word embeddings are computed, the simplest way to embed paths would be to represent each sequence as a one-hot vector. However, this is suboptimal for two reasons: Firstly, we expect only a subset of dependency paths to be attested frequently in our data and therefore many paths will be too sparse to learn reliable embeddings for them. Secondly, we hypothesize that dependency paths which share the same words, word categories or dependency relations should impact SRL decisions in similar ways. Thus, the words and relations on the path should drive representation learning, rather than the full path on its own. The following sections describe how we address representation learning by means of modeling dependency paths as sequences of items in a recurrent neural network."}, {"heading": "2.1 Recurrent Neural Networks", "text": "The recurrent model we use in this work is a variant of the long-short term memory (LSTM) network. It takes a sequence of items X = x1, ...,xn as input, recurrently processes each item xt \u2208 X at a time, and finally returns one embedding state en\nfor the complete input sequence. For each time step t, the LSTM model updates an internal memory state mt that depends on the current input as well as the previous memory state mt\u22121. In order to capture long-term dependencies, a so-called gating mechanism controls the extent to which each component of a memory cell state will be modified. In this work, we employ input gates i, output gates o and (optional) forget gates f. We formalize the state of the network at each time step t as follows:\nit = \u03c3([Wmimt\u22121]+Wxixt +bi) (1)\nft = \u03c3([Wmfmt\u22121]+Wxfxt +bf) (2) mt = it \u2299 (Wxmxt)+ ft \u2299mt\u22121 +bm (3)\not = \u03c3([Wmomt ]+Wxoxt +bo) (4) et = ot \u2299\u03c3(mt) (5)\nIn each equation, W describes a matrix of weights to project information between two layers, b is a layer-specific vector of bias terms, and \u03c3 is the logistic function. Superscripts indicate the corresponding layers or gates. Some models described in Section 3 do not make use of forget gates or memory-to-gate connections. In case no forget gate is used, we set ft = 1. If no memoryto-gate connections are used, the terms in square brackets in (1), (2), and (4) are replaced by zeros."}, {"heading": "2.2 Embedding Dependency Paths", "text": "We define the embedding of a dependency path to be the final memory output state of a recurrent LSTM layer that takes a path as input, with each input step representing a binary indicator for\na part-of-speech tag, a word form, or a dependency relation. In the context of semantic role labeling, we define each path as a sequence from a predicate to its potential argument.1 Specifically, we define the first item x1 to correspond to the part-of-speech tag of the predicate word wi, followed by its actual word form, and the relation to the next word wi+1. The embedding of a dependency path corresponds to the state en returned by the LSTM layer after the input of the last item, xn, which corresponds to the word form of the argument head word w j. An example is shown in Figure 2.\nThe main idea of this model and representation is that word forms, word categories and dependency relations can all influence role labeling decisions. The word category and word form of the predicate first determine which roles are plausible and what kinds of path configurations are to be expected. The relations and words seen on the path can then manipulate these expectations. In Figure 2, for instance, the verb raising complements the phrase had trouble, which makes it likely that the subject he is also the logical subject of raising.\nBy using word forms, categories and dependency relations as input items, we ensure that specific words (e.g., those which are part of complex predicates) as well as various relation types (e.g., subject and object) can appropriately influence the representation of a path. While learning corresponding interactions, the network is also able to determine which phrases and dependency relations might not influence a role assignment decision (e.g., coordinations)."}, {"heading": "2.3 Joint Embedding and Feature Learning", "text": "Our SRL model consists of four components depicted in Figure 3: (1) an LSTM component takes lexicalized dependency paths as input, (2) an additional input layer takes binary features as input, (3) a hidden layer combines dependency path embeddings and binary features using rectified linear units, and (4) a softmax classification layer produces output based on the hidden layer state as input. We therefore learn path embeddings jointly with feature detectors based on traditional, binary indicator features.\nGiven a dependency path X , with steps xk \u2208 {x1, ...,xn}, and a set of binary features B as input, we use the LSTM formalization from equa-\n1We experimented with different sequential orders and found this to lead to the best validation set results.\ntions (1\u20135) to compute the embedding en at time step n and formalize the state of the hidden layer h and softmax output sc for each class category c as follows:\nh = max(0,WBhB+Wehen +bh) (6)\nsc = Wesc en +W hs c h+b s c\n\u03a3i(Wesi en +Whsi h+bsi ) (7)"}, {"heading": "3 System Architecture", "text": "The overall architecture of our SRL system closely follows that of previous work (Toutanova et al., 2008; Bjo\u0308rkelund et al., 2009) and is depicted in Figure 4. We use a pipeline that consists of the following steps: predicate identification and disambiguation, argument identification, argument classification, and re-ranking. The neural-network components introduced in Section 2 are used in the last three steps. The following sub-sections describe all components in more detail."}, {"heading": "3.1 Predicate Identification and Disambiguation", "text": "Given a syntactically analyzed sentence, the first two steps in an end-to-end SRL system are to identify and disambiguate the semantic predicates in the sentence. Here, we focus on verbal and nominal predicates but note that other syntactic categories have also been construed as predicates in the NLP literature\n(e.g., prepositions; Srikumar and Roth (2013)). For both identification and disambiguation steps, we apply the same logistic regression classifiers used in the SRL components of mate-tools (Bjo\u0308rkelund et al., 2010). The classifiers for both tasks make use of a range of lexico-syntactic indicator features, including predicate word form, its predicted part-of-speech tag as well as dependency relations to all syntactic children."}, {"heading": "3.2 Argument Identification and Classification", "text": "Given a sentence and a set of sense-disambiguated predicates in it, the next two steps of our SRL system are to identify all arguments of each predicate and to assign suitable role labels to them. For both steps, we train several LSTM-based neural network models as described in Section 2. In particular, we train separate networks for nominal and verbal predicates and for identification and classification. Following the findings of earlier work (Xue and Palmer, 2004), we assume that different feature sets are relevant for the respective tasks and hence different embedding representations should be learned. As binary input features,\nwe use the following sets from the SRL literature (Bjo\u0308rkelund et al., 2010).\nLexico-syntactic features Word form and word category of the predicate and candidate argument; dependency relations from predicate and argument to their respective syntactic heads; full dependency path sequence from predicate to argument.\nLocal context features Word forms and word categories of the candidate argument\u2019s and predicate\u2019s syntactic siblings and children words.\nOther features Relative position of the candidate argument with respect to the predicate (left, self, right); sequence of part-of-speech tags of all words between the predicate and the argument."}, {"heading": "3.3 Reranker", "text": "As all argument identification (and classification) decisions are independent of one another, we apply as the last step of our pipeline a global reranker. Given a predicate p, the reranker takes as input the n best sets of identified arguments as well as their n best label assignments and predicts the best overall argument structure. We implement the reranker as a logistic regression classifier, with hidden and embedding layer states of identified arguments as features, offset by the argument label, and a binary label as output (1: best predicted structure, 0: any other structure). At test time, we select the structure with the highest overall score, which we compute as the geometric mean of the global regression and all argument-specific scores."}, {"heading": "4 Experiments", "text": "In this section, we demonstrate the usefulness of dependency path embeddings for semantic role labeling. Our hypotheses are that (1) modeling dependency paths as sequences will lead to better representations for the SRL task, thus increasing labeling precision overall, and that (2) embeddings will address the problem of data sparsity, leading to higher recall. To test both hypotheses, we experiment on the in-domain and out-of-domain test sets provided in the CoNLL-2009 shared task (Hajic\u030c et al., 2009) and compare results of our system, henceforth PathLSTM, with systems that do not involve path embeddings. We compute precision, recall and F1-score using the official CoNLL-2009 scorer.2 The code is available at\n2Some recently proposed SRL models are only evaluated on the CoNLL 2005 and 2012 data sets, which lack nomi-\nhttps://github.com/microth/PathLSTM.\nModel selection We train argument identification and classification models using the XLBP toolkit for neural networks (Monner and Reggia, 2012). The hyperparameters for each step were selected based on the CoNLL 2009 development set. For direct comparison with previous work, we use the same preprocessing models and predicate-specific SRL components as provided with mate-tools (Bohnet, 2010; Bjo\u0308rkelund et al., 2010). The types and ranges of hyperparameters considered are as follows: learning rate \u03b1 \u2208 [0.00006,0.3], dropout rate d \u2208 [0.0,0.5], and hidden layer sizes |e| \u2208 [0,100], |h| \u2208 [0,500]. In addition, we experimented with different gating mechanisms (with/without forget gate) and memory access settings (with/without connections between all gates and the memory layer, cf. Section 2). The best parameters were chosen using the Spearmint hyperparameter optimization toolkit (Snoek et al., 2012), applied for approx. 200 iterations, and are summarized in Table 2.\nResults The results of our in- and out-of-domain experiments are summarized in Tables 3 and 5, respectively. We present results for different system configurations: \u2018local\u2019 systems make classification decisions independently, whereas \u2018global\u2019 systems include a reranker or other global inference mechanisms; \u2018single\u2019 refers to one model and \u2018ensemble\u2019 refers to combinations of multiple models.\nIn the in-domain setting, our PathLSTM model achieves 87.7% (single) and 87.9% (ensemble) F1score, outperforming previously published best results by 0.4 and 0.2 percentage points, respectively. At a F1-score of 86.7%, our local model (using no reranker) reaches the same performance as state-of-the-art local models. Note that differences in results between systems might originate from the application of different preprocessing techniques as each system comes with its own syntactic components. For direct comparison, we evaluate against mate-tools, which use the same preprocessing techniques as PathLSTM. In comparison, we see improvements of +0.8\u20131.0 percentage points absolute in F1-score.\nIn the out-of-domain setting, our system achieves new state-of-the-art results of 76.1%\nnal predicates or dependency annotations. We do not list any results from those models here.\n3Results are taken from Lei et al. (2015).\n(single) and 76.5% (ensemble) F1-score, outperforming the previous best system by Roth and Woodsend (2014) by 0.2 and 0.6 absolute points, respectively. In comparison to mate-tools, we observe absolute improvements in F1-score of +0.4\u20130.8%.\nDiscussion To determine the sources of individual improvements, we test PathLSTM models without specific feature types and directly compare PathLSTM and mate-tools, both of which use the same preprocessing methods. Table 4 presents in-domain test results for our system when specific feature types are omitted. The overall low results indicate that a combination of dependency path embeddings and binary features is required to\nidentify and label arguments with high precision.\nFigure 5 shows the effect of dependency path embeddings at mitigating sparsity: if the path between a predicate and its argument has not been observed at training time or only infrequently, conventional methods will often fail to assign a role. This is represented by the recall curve of mate-tools, which converges to zero for arguments with unseen paths. The higher recall curve for PathLSTM demonstrates that path embeddings can alleviate this problem to some extent. For unseen paths, we observe that PathLSTM improves over mate-tools by an order of magnitude, from 0.9% to 9.6%. The highest absolute gain, from 12.8% to 24.2% recall, can be observed for dependency paths that occurred between 1 and 10 times during training.\nFigure 7 plots role labeling performance for sentences with varying number of words. There are two categories of sentences in which the improvements of PathLSTM are most noticeable: Firstly, it better handles short sentences that con-\ntain expletives and/or nominal predicates (+0.8% absolute in F1-score). This is probably due to the fact that our learned dependency path representations are lexicalized, making it possible to model argument structures of different nominals and distinguishing between expletive occurrences of \u2018it\u2019 and other subjects. Secondly, it improves performance on longer sentences (up to +1.0% absolute in F1-score). This is mainly due to the handling of dependency paths that involve complex structures, such as coordinations, control verbs and nominal predicates.\nWe collect instances of different syntactic phenomena from the development set and plot the\nlearned dependency path representations in the embedding space (see Figure 6). We obtain a projection onto two dimensions using t-SNE (Van der Maaten and Hinton, 2008). Interestingly, we can see that different syntactic configurations are clustered together in different parts of the space and that most instances of the PropBank roles A0 and A1 are separated. Example phrases in the figure highlight predicate-argument pairs that are correctly labeled by PathLSTM but not by mate-tools. Path embeddings are essential for handling these cases as indicator features do not generalize well enough.\nFinally, Table 6 shows results for nominal and verbal predicates as well as for different (gold)\nrole labels. In comparison to mate-tools, we can see that PathLSTM improves precision for all argument types of nominal predicates. For verbal predicates, improvements can be observed in terms of recall of proto-agent (A0) and protopatient (A1) roles, with slight gains in precision for the A2 role. Overall, PathLSTM does slightly worse with respect to modifier roles, which it labels with higher precision but at the cost of recall."}, {"heading": "5 Path Embeddings in other Languages", "text": "In this section, we report results from additional experiments on Chinese, German and Spanish data. The underlying question is to which extent the improvements of our SRL system for English also generalize to other languages. To answer this question, we train and test separate SRL models for each language, using the system architecture and hyperparameters discussed in Sections 3 and 4, respectively.\nWe train our models on data from the CoNLL-2009 shared task, relying on the same features as one of the participating systems (Bjo\u0308rkelund et al., 2009), and evaluate with the official scorer. For direct comparison, we rely on the (automatic) syntactic preprocessing information provided with the CoNLL test data and compare our results with the best two systems for each language that make use of the same preprocessing information.\nThe results, summarized in Table 7, indicate that PathLSTM performs better than the system by Bjo\u0308rkelund et al. (2009) in all cases. For German and Chinese, PathLSTM achieves the best overall F1-scores of 80.1% and 79.4%, respectively."}, {"heading": "6 Related Work", "text": "Neural Networks for SRL Collobert et al. (2011) pioneered neural networks for the task of semantic role labeling. They developed a feed-forward network that uses a convolution function over windows of words to assign SRL labels. Apart from constituency boundaries, their system does not make use of any syntactic information. Foland and Martin (2015) extended their model and showcased significant improvements when including binary indicator features for dependency paths. Similar features were used by FitzGerald et al. (2015), who include role labeling predictions by neural networks as factors in a global model.\nThese approaches all make use of binary features derived from syntactic parses either to indicate constituency boundaries or to represent full dependency paths. An extreme alternative has been recently proposed in Zhou and Xu (2015), who model SRL decisions with a multi-layered LSTM network that takes word sequences as input but no syntactic parse information at all.\nOur approach falls in between the two extremes: we rely on syntactic parse information but rather than solely making using of sparse binary features, we explicitly model dependency paths in a neural network architecture.\nOther SRL approaches Within the SRL literature, recent alternatives to neural network architectures include sigmoid belief networks\n(Henderson et al., 2013) as well as low-rank tensor models (Lei et al., 2015). Whereas Lei et al. only make use of dependency paths as binary indicator features, Henderson et al. propose a joint model for syntactic and semantic parsing that learns and applies incremental dependency path representations to perform SRL decisions. The latter form of representation is closest to ours, however, we do not build syntactic parses incrementally. Instead, we take syntactically preprocessed text as input and focus on the SRL task only.\nApart from more powerful models, most recent progress in SRL can be attributed to novel features. For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences. Zapirain et al. (2013) propose different selection preference models in order to deal with the sparseness of lexical features. Roth and Woodsend (2014) address the same problem with word embeddings and compositions thereof. Roth and Lapata (2015) recently introduced features that model the influence of discourse on role labeling decisions.\nRather than coming up with completely new features, in this work we proposed to revisit some well-known features and represent them in a novel way that generalizes better. Our proposed model is inspired both by the necessity to overcome the problems of sparse lexico-syntactic features and by the recent success of SRL models based on neural networks.\nDependency-based embeddings The idea of embedding dependency structures has previously been applied to tasks such as relation classification and sentiment analysis. Xu et al. (2015) and Liu et al. (2015) use neural networks to embed dependency paths between entity pairs. To identify the relation that holds between two entities, their approaches make use of pooling layers that detect parts of a path that indicate a specific relation. In contrast, our work aims at modeling an individual path as a complete sequence, in which every item is of relevance. Tai et al. (2015) and Ma et al. (2015) learn embeddings of dependency structures representing full sentences, in a sentiment classification task. In our model, embeddings are learned jointly with other features, and as a result problems that may result from erroneous parse trees are mitigated."}, {"heading": "7 Conclusions", "text": "We introduced a neural network architecture for semantic role labeling that jointly learns embeddings for dependency paths and feature combinations. Our experimental results indicate that our model substantially increases classification performance, leading to new state-of-the-art results. In a qualitive analysis, we found that our model is able to cover instances of various linguistic phenomena that are missed by other methods.\nBeyond SRL, we expect dependency path embeddings to be useful in related tasks and downstream applications. For instance, our representations may be of direct benefit for semantic and discourse parsing tasks. The jointly learned feature space also makes our model a good starting point for cross-lingual transfer methods that rely on feature representation projection to induce new models (Kozhevnikov and Titov, 2014).\nAcknowledgements We thank the three anonymous ACL referees whose feedback helped to substantially improve the present paper. The support of the Deutsche Forschungsgemeinschaft (Research Fellowship RO 4848/1-1; Roth) and the European Research Council (award number 681760; Lapata) is gratefully acknowledged."}], "references": [{"title": "Shallow semantic trees for smt", "author": ["Wilker Aziz", "Miguel Rios", "Lucia Specia."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 316\u2013322, Edinburgh, Scotland.", "citeRegEx": "Aziz et al\\.,? 2011", "shortCiteRegEx": "Aziz et al\\.", "year": 2011}, {"title": "Multilingual semantic role labeling", "author": ["Anders Bj\u00f6rkelund", "Love Hafdell", "Pierre Nugues."], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, pages 43\u201348, Boulder, Colorado.", "citeRegEx": "Bj\u00f6rkelund et al\\.,? 2009", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2009}, {"title": "A high-performance syntactic and semantic dependency parser", "author": ["Anders Bj\u00f6rkelund", "Bernd Bohnet", "Love Hafdell", "Pierre Nugues."], "venue": "Coling 2010: Demonstration Volume, pages 33\u201336, Beijing, China.", "citeRegEx": "Bj\u00f6rkelund et al\\.,? 2010", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2010}, {"title": "Top accuracy and fast dependency parsing is not a contradiction", "author": ["Bernd Bohnet."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, pages 89\u201397, Beijing, China.", "citeRegEx": "Bohnet.,? 2010", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "Multilingual dependency-based syntactic and semantic parsing", "author": ["Wanxiang Che", "Zhenghua Li", "Yongqiang Li", "Yuhang Guo", "Bing Qin", "Ting Liu."], "venue": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared", "citeRegEx": "Che et al\\.,? 2009", "shortCiteRegEx": "Che et al\\.", "year": 2009}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Semi-supervised semantic role labeling using the Latent Words Language Model", "author": ["Koen Deschacht", "Marie-Francine Moens."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 21\u201329, Singapore.", "citeRegEx": "Deschacht and Moens.,? 2009", "shortCiteRegEx": "Deschacht and Moens.", "year": 2009}, {"title": "Semantic role labeling with neural network factors", "author": ["Nicholas FitzGerald", "Oscar T\u00e4ckstr\u00f6m", "Kuzman Ganchev", "Dipanjan Das."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 960\u2013970,", "citeRegEx": "FitzGerald et al\\.,? 2015", "shortCiteRegEx": "FitzGerald et al\\.", "year": 2015}, {"title": "Dependency-based semantic role labeling using convolutional neural networks", "author": ["William Foland", "James Martin."], "venue": "Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics, pages 279\u2013288, Denver,", "citeRegEx": "Foland and Martin.,? 2015", "shortCiteRegEx": "Foland and Martin.", "year": 2015}, {"title": "Automatic labeling of semantic roles", "author": ["Daniel Gildea", "Daniel Jurafsky."], "venue": "Computational Linguistics, 28(3):245\u2013288.", "citeRegEx": "Gildea and Jurafsky.,? 2002", "shortCiteRegEx": "Gildea and Jurafsky.", "year": 2002}, {"title": "Multilingual joint parsing of syntactic and semantic dependencies with a latent variable model", "author": ["James Henderson", "Paola Merlo", "Ivan Titov", "Gabriele Musillo."], "venue": "Computational Linguistics, 39(4):949\u2013998.", "citeRegEx": "Henderson et al\\.,? 2013", "shortCiteRegEx": "Henderson et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Open-domain semantic role labeling by modeling word spans", "author": ["Fei Huang", "Alexander Yates."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 968\u2013 978, Uppsala, Sweden.", "citeRegEx": "Huang and Yates.,? 2010", "shortCiteRegEx": "Huang and Yates.", "year": 2010}, {"title": "The effect of syntactic representation on semantic role labeling", "author": ["Richard Johansson", "Pierre Nugues."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics, pages 393\u2013400, Manchester, United Kingdom.", "citeRegEx": "Johansson and Nugues.,? 2008", "shortCiteRegEx": "Johansson and Nugues.", "year": 2008}, {"title": "A framework for multi-document abstractive summarization based on semantic role labelling", "author": ["Atif Khan", "Naomie Salim", "Yogan Jaya Kumar."], "venue": "Applied Soft Computing, 30:737\u2013747.", "citeRegEx": "Khan et al\\.,? 2015", "shortCiteRegEx": "Khan et al\\.", "year": 2015}, {"title": "The importance of syntactic parsing and inference in semantic role labeling", "author": ["Vasin Punyakanok", "Dan Roth", "Wen-tau Yih."], "venue": "Computational Linguistics, 34(2):257\u2013287.", "citeRegEx": "Punyakanok et al\\.,? 2008", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2008}, {"title": "Contextaware frame-semantic role labeling", "author": ["Michael Roth", "Mirella Lapata."], "venue": "Transactions of the Association for Computational Linguistics, 3:449\u2013460.", "citeRegEx": "Roth and Lapata.,? 2015", "shortCiteRegEx": "Roth and Lapata.", "year": 2015}, {"title": "Composition of word representations improves semantic role labelling", "author": ["Michael Roth", "Kristian Woodsend."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 407\u2013413, Doha, Qatar.", "citeRegEx": "Roth and Woodsend.,? 2014", "shortCiteRegEx": "Roth and Woodsend.", "year": 2014}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P. Adams."], "venue": "Advances in Neural Information Processing Systems, pages 2951\u20132959, Lake Tahoe, Nevada.", "citeRegEx": "Snoek et al\\.,? 2012", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Modeling semantic relations expressed by prepositions", "author": ["Vivek Srikumar", "Dan Roth."], "venue": "Transactions of the Association for Computational Linguistics, 1:231\u2013242.", "citeRegEx": "Srikumar and Roth.,? 2013", "shortCiteRegEx": "Srikumar and Roth.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "A global joint model for semantic role labeling", "author": ["Kristina Toutanova", "Aria Haghighi", "Christopher Manning."], "venue": "Computational Linguistics, 34(2):161\u2013191.", "citeRegEx": "Toutanova et al\\.,? 2008", "shortCiteRegEx": "Toutanova et al\\.", "year": 2008}, {"title": "Visualizing data using t-SNE", "author": ["Laurens Van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9:2579\u20132605.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Modeling the translation of predicate-argument structure for smt", "author": ["Deyi Xiong", "Min Zhang", "Haizhou Li."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 902\u2013911, Jeju Island, Korea.", "citeRegEx": "Xiong et al\\.,? 2012", "shortCiteRegEx": "Xiong et al\\.", "year": 2012}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Calibrating features for semantic role labeling", "author": ["Nianwen Xue", "Martha Palmer."], "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 88\u201394, Barcelona, Spain.", "citeRegEx": "Xue and Palmer.,? 2004", "shortCiteRegEx": "Xue and Palmer.", "year": 2004}, {"title": "Selectional preferences for semantic role classification", "author": ["Be\u00f1at Zapirain", "Eneko Agirre", "Llu\u0131\u0301s M\u00e0rquez", "Mihai Surdeanu"], "venue": "Computational Linguistics,", "citeRegEx": "Zapirain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zapirain et al\\.", "year": 2013}, {"title": "Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies", "author": ["Hai Zhao", "Wenliang Chen", "Jun\u2019ichi Kazama", "Kiyotaka Uchimoto", "Kentaro Torisawa"], "venue": null, "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}, {"title": "End-to-end learning of semantic role labeling using recurrent neural networks", "author": ["Jie Zhou", "Wei Xu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat-", "citeRegEx": "Zhou and Xu.,? 2015", "shortCiteRegEx": "Zhou and Xu.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Previous work has shown that semantic roles are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al.", "startOffset": 177, "endOffset": 216}, {"referenceID": 23, "context": "Previous work has shown that semantic roles are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al.", "startOffset": 177, "endOffset": 216}, {"referenceID": 14, "context": ", 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015).", "startOffset": 76, "endOffset": 95}, {"referenceID": 0, "context": "Previous work has shown that semantic roles are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015). The task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In System Analysis", "startOffset": 178, "endOffset": 434}, {"referenceID": 15, "context": "Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008).", "startOffset": 64, "endOffset": 111}, {"referenceID": 13, "context": ", 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008).", "startOffset": 121, "endOffset": 149}, {"referenceID": 1, "context": "For instance, consider the sentence He had trouble raising funds and the analyses provided by four publicly available tools in Table 1 (mate-tools, Bj\u00f6rkelund et al. (2010); mateplus, Roth and Woodsend (2014); TensorSRL, Lei et al.", "startOffset": 148, "endOffset": 173}, {"referenceID": 1, "context": "For instance, consider the sentence He had trouble raising funds and the analyses provided by four publicly available tools in Table 1 (mate-tools, Bj\u00f6rkelund et al. (2010); mateplus, Roth and Woodsend (2014); TensorSRL, Lei et al.", "startOffset": 148, "endOffset": 209}, {"referenceID": 1, "context": "For instance, consider the sentence He had trouble raising funds and the analyses provided by four publicly available tools in Table 1 (mate-tools, Bj\u00f6rkelund et al. (2010); mateplus, Roth and Woodsend (2014); TensorSRL, Lei et al. (2015); and easySRL, Lewis et al.", "startOffset": 148, "endOffset": 239}, {"referenceID": 1, "context": "For instance, consider the sentence He had trouble raising funds and the analyses provided by four publicly available tools in Table 1 (mate-tools, Bj\u00f6rkelund et al. (2010); mateplus, Roth and Woodsend (2014); TensorSRL, Lei et al. (2015); and easySRL, Lewis et al. (2015)).", "startOffset": 148, "endOffset": 273}, {"referenceID": 11, "context": "We then apply long-short term memory networks (Hochreiter and Schmidhuber, 1997) to find a recurrent composition function that can reconstruct an appropriate representation of the full path from its individual parts (Section 2).", "startOffset": 46, "endOffset": 80}, {"referenceID": 21, "context": "The overall architecture of our SRL system closely follows that of previous work (Toutanova et al., 2008; Bj\u00f6rkelund et al., 2009) and is depicted in Figure 4.", "startOffset": 81, "endOffset": 130}, {"referenceID": 1, "context": "The overall architecture of our SRL system closely follows that of previous work (Toutanova et al., 2008; Bj\u00f6rkelund et al., 2009) and is depicted in Figure 4.", "startOffset": 81, "endOffset": 130}, {"referenceID": 2, "context": "For both identification and disambiguation steps, we apply the same logistic regression classifiers used in the SRL components of mate-tools (Bj\u00f6rkelund et al., 2010).", "startOffset": 141, "endOffset": 166}, {"referenceID": 17, "context": ", prepositions; Srikumar and Roth (2013)).", "startOffset": 16, "endOffset": 41}, {"referenceID": 25, "context": "Following the findings of earlier work (Xue and Palmer, 2004), we assume that different feature sets are relevant for the respective tasks and hence different embedding representations should be learned.", "startOffset": 39, "endOffset": 61}, {"referenceID": 2, "context": "we use the following sets from the SRL literature (Bj\u00f6rkelund et al., 2010).", "startOffset": 50, "endOffset": 75}, {"referenceID": 3, "context": "For direct comparison with previous work, we use the same preprocessing models and predicate-specific SRL components as provided with mate-tools (Bohnet, 2010; Bj\u00f6rkelund et al., 2010).", "startOffset": 145, "endOffset": 184}, {"referenceID": 2, "context": "For direct comparison with previous work, we use the same preprocessing models and predicate-specific SRL components as provided with mate-tools (Bohnet, 2010; Bj\u00f6rkelund et al., 2010).", "startOffset": 145, "endOffset": 184}, {"referenceID": 18, "context": "The best parameters were chosen using the Spearmint hyperparameter optimization toolkit (Snoek et al., 2012), applied for approx.", "startOffset": 88, "endOffset": 108}, {"referenceID": 17, "context": "5% (ensemble) F1-score, outperforming the previous best system by Roth and Woodsend (2014) by 0.", "startOffset": 66, "endOffset": 91}, {"referenceID": 1, "context": "We train our models on data from the CoNLL-2009 shared task, relying on the same features as one of the participating systems (Bj\u00f6rkelund et al., 2009), and evaluate with the official scorer.", "startOffset": 126, "endOffset": 151}, {"referenceID": 1, "context": "The results, summarized in Table 7, indicate that PathLSTM performs better than the system by Bj\u00f6rkelund et al. (2009) in all cases.", "startOffset": 94, "endOffset": 119}, {"referenceID": 1, "context": "4 Bj\u00f6rkelund et al. (2009) 82.", "startOffset": 2, "endOffset": 27}, {"referenceID": 1, "context": "4 Bj\u00f6rkelund et al. (2009) 82.4 75.1 78.6 Zhao et al. (2009) 80.", "startOffset": 2, "endOffset": 61}, {"referenceID": 1, "context": "1 Bj\u00f6rkelund et al. (2009) 81.", "startOffset": 2, "endOffset": 27}, {"referenceID": 1, "context": "1 Bj\u00f6rkelund et al. (2009) 81.2 78.3 79.7 Che et al. (2009) 82.", "startOffset": 2, "endOffset": 60}, {"referenceID": 1, "context": "2 Bj\u00f6rkelund et al. (2009) 78.", "startOffset": 2, "endOffset": 27}, {"referenceID": 5, "context": "Neural Networks for SRL Collobert et al. (2011) pioneered neural networks for the task of semantic role labeling.", "startOffset": 24, "endOffset": 48}, {"referenceID": 5, "context": "Neural Networks for SRL Collobert et al. (2011) pioneered neural networks for the task of semantic role labeling. They developed a feed-forward network that uses a convolution function over windows of words to assign SRL labels. Apart from constituency boundaries, their system does not make use of any syntactic information. Foland and Martin (2015) extended their model and showcased significant improvements when including binary indicator features for dependency paths.", "startOffset": 24, "endOffset": 351}, {"referenceID": 5, "context": "Neural Networks for SRL Collobert et al. (2011) pioneered neural networks for the task of semantic role labeling. They developed a feed-forward network that uses a convolution function over windows of words to assign SRL labels. Apart from constituency boundaries, their system does not make use of any syntactic information. Foland and Martin (2015) extended their model and showcased significant improvements when including binary indicator features for dependency paths. Similar features were used by FitzGerald et al. (2015), who include role labeling predictions by neural networks as factors in a global model.", "startOffset": 24, "endOffset": 529}, {"referenceID": 28, "context": "An extreme alternative has been recently proposed in Zhou and Xu (2015), who model SRL decisions with a multi-layered LSTM network that takes word sequences as input but no syntactic parse information at all.", "startOffset": 53, "endOffset": 72}, {"referenceID": 10, "context": "(Henderson et al., 2013) as well as low-rank tensor models (Lei et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 6, "context": "For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences.", "startOffset": 14, "endOffset": 41}, {"referenceID": 6, "context": "For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences.", "startOffset": 14, "endOffset": 68}, {"referenceID": 6, "context": "For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences. Zapirain et al. (2013) propose different selection preference models in order to deal with the sparseness of lexical features.", "startOffset": 14, "endOffset": 204}, {"referenceID": 6, "context": "For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences. Zapirain et al. (2013) propose different selection preference models in order to deal with the sparseness of lexical features. Roth and Woodsend (2014) address the same problem with word embeddings and compositions thereof.", "startOffset": 14, "endOffset": 333}, {"referenceID": 6, "context": "For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences. Zapirain et al. (2013) propose different selection preference models in order to deal with the sparseness of lexical features. Roth and Woodsend (2014) address the same problem with word embeddings and compositions thereof. Roth and Lapata (2015) recently introduced features that model the influence of discourse on role labeling decisions.", "startOffset": 14, "endOffset": 428}, {"referenceID": 23, "context": "Xu et al. (2015) and Liu et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 23, "context": "Xu et al. (2015) and Liu et al. (2015) use neural networks to embed dependency paths between entity pairs.", "startOffset": 0, "endOffset": 39}, {"referenceID": 20, "context": "Tai et al. (2015) and Ma et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 20, "context": "Tai et al. (2015) and Ma et al. (2015) learn embeddings of dependency structures representing full sentences, in a sentiment classification task.", "startOffset": 0, "endOffset": 39}], "year": 2016, "abstractText": "This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the observation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as subsequences of lexicalized dependency paths and learns suitable embedding representations. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers, and showcase qualitative improvements obtained by our method.", "creator": "LaTeX with hyperref package"}}}