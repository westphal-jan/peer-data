{"id": "1605.07722", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Yum-me: A Personalized Nutrient-based Meal Recommender System", "abstract": "increasingly ubiquitous computing communities notably addressed health enhanced wellness behaviors such as healthy eats. healthy meal recommendations give sufficient potential to aided individuals prevent or manage conditions particularly as freezing and obesity. conversely, using people's cooking outcomes and selecting healthy recommendations especially appeal to their palate reduce harmful. research approaches either only support high - level preferences also require a prolonged learning transition. examples incorporate yum - s, a diet healthy person recommender software designed so meet contestants'lifestyle goals, fewer restrictions, nutrient fine - time food components. marrying ideas from cognitive cognitive learning and healthy eating tools, r - me enables a simple sounding more competitive preference profiling procedure addressing an image - assisted intelligent learning perspective, collectively projects the learned trends concerning the domain ; living food quality versus establish ones that will appeal to the electorate. we present the discussion and implementation of a - me, and further investigate the initial critical component of it : standardization, utilizing state - of - metal - art food image analysis model. physicians reviewed industry's superior processes incorporating careful benchmarking, and enhance its applicability using a wide array of dietary applications. cochrane specifically recommended feasibility prediction effectiveness of pg - me through a 60 - person controlled study, in which yum - me improves the recommendation processing rate by 88. 63 % that contemporary traditional food preference survey.", "histories": [["v1", "Wed, 25 May 2016 04:13:49 GMT  (2833kb,D)", "http://arxiv.org/abs/1605.07722v1", null], ["v2", "Wed, 21 Dec 2016 14:48:18 GMT  (4275kb,D)", "http://arxiv.org/abs/1605.07722v2", null], ["v3", "Sun, 30 Apr 2017 17:43:02 GMT  (4274kb,D)", "http://arxiv.org/abs/1605.07722v3", null]], "reviews": [], "SUBJECTS": "cs.HC cs.AI cs.CV cs.IR", "authors": ["longqi yang", "cheng-kang hsieh", "hongjian yang", "nicola dell", "serge belongie", "curtis cole", "deborah estrin"], "accepted": false, "id": "1605.07722"}, "pdf": {"name": "1605.07722.pdf", "metadata": {"source": "META", "title": "Yum-me: Personalized Healthy Meal Recommender System", "authors": ["Longqi Yang", "Cheng-Kang Hsieh", "Hongjian Yang"], "emails": ["ylongqi@cs.cornell.edu", "changun@cs.ucla.edu", "hy457@cornell.edu", "nixdell@cornell.edu", "sjb344@cornell.edu", "destrin@cs.cornell.edu"], "sections": [{"heading": "ACM Classification Keywords", "text": "H.5.m. Information Interfaces and Presentation (e.g. HCI): Miscellaneous"}, {"heading": "Author Keywords", "text": "Healthy meal recommendation; personalization; visual interface; food preferences"}, {"heading": "INTRODUCTION", "text": "Healthy eating plays a critical role in our daily well-being and is indispensable in preventing and managing conditions such as diabetes, high blood pressure, cancer, mental illnesses, asthma, etc[36, 20, 9]. In particular, for children and young people, the adoption of healthy dietary habits has been shown to be beneficial to early cognitive development [38]. In the Ubicomp community, various applications designed to promote healthy behaviors, including diet, have been proposed and studied [28, 14, 27, 15].\nAmong those applications, the studies and products that target healthy meal recommendations have attracted much attention in academia [46] and industry [3] alike. Fundamentally, the goal of these systems is to suggest food alternatives that cater to individuals\u2019 health goals and help users develop healthy eating behavior by following the recommendations [1].\nAkin to most recommender systems, learning users\u2019 preferences is a necessary step in recommending healthy meals that users are more likely to find desirable [1]. However, the current food preference elicitation approaches, including 1) on-boarding surveys and 2) food journaling, still suffer from many limitations. Specifically, a typical on-boarding survey asks a number of multi-choice questions about general food preferences. For example, PlateJoy [3], a daily meal planner app, elicits preferences for healthy goals and dietary restrictions with the following questions:\n(1) How do you prefer to eat? No restrictions, dairy free, gluten free, kid friendly, pescatarian, paleo, vegetarian...\n(2) Are there any ingredients you prefer to avoid? avocado, eggplant, eggs, seafood, shellfish, lamb, peanuts, tofu....\nWhile the answers to these questions can certainly be used to create a rough dietary plan and avoid clearly unacceptable choices, they do not generate meal recommendations that cater to each person\u2019s food preferences, and thus they suffer from lower recommendation acceptance rates, which is shown in our user study.\nFood journaling is another popular approach for food preference elicitation. For example, Nutrino [2], a personal meal recommender, asks users to log their daily food consumptions and learn users\u2019 fine-grained food preferences. With the proliferation of ubiquitous computing devices and their\nar X\niv :1\n60 5.\n07 72\n2v 1\n[ cs\n.H C\n] 2\n5 M\nay 2\n01 6\nincreasing image-capture capability, food journaling is indeed a promising approach for fine-grained food preference learning. However, as a typical downside of systems relying on user-generated data, food journaling suffers from the coldstart problem, where recommendations cannot be made or are subject to low accuracy when the user has not yet generated a sufficient amount of data. For example, a previous study showed that an active food-journaling user makes about 3.5 entries per day [16]. It would take a non-trivial amount of time for the system to acquire sufficient amount of data to make recommendations, and the collected samples may subject to sampling biases as well [16]. Moreover, the habit of photo food journaling is difficult to adopt and adhere to[16].\nIn this work, we develop Yum-me, a meal recommender that learns users\u2019 preferences based on general food images and then projects them into a healthy-food domain. We build upon a previously developed visual online learning framework, PlateClick[47], which learned users\u2019 fined-grained food preferences with simple pairwise comparisons between food images. PlateClick did not address the challenge of using food preferences learned in the context of \u201cgeneral food\u201d and applying it to recommendations of healthy meals, where users\u2019 health goals and specific dietary restrictions need to be taken into account. Specifically, Yum-me learns users\u2019 fine-grained food preferences through PlateClick\u2019s adaptive visual interface and then attempts to generate meal recommendations that cater to the user\u2019s health goals, food restrictions, as well as personal appetite for food. We present the design and implementation of Yum-me that can be used by people who have food restrictions, such as vegetarian, vegan, kosher, or halal, and have health goals for adjustment of calorie, fat, and protein, intake.\nFor such a visual-based learning framework to work, one of the most critical components is a robust food image analysis model. Toward that end, as an additional contribution of this work we present a novel, unified food image analysis model called FoodDist. Based on deep convolutional networks and multi-task learning [31], FoodDist is the best-of-its-kind Euclidean distance embedding for food images, in which similar food items have smaller distances while dissimilar food items have larger distances. FoodDist allows Yum-me to learn users\u2019 fine-grained food preferences accurately via similarity assess-\nments on food images. Besides preference learning, FoodDist can be applied to other food-image-related tasks in ubiquitous computing, such as food image detection, classification, retrieval, and clustering. We benchmark FoodDist with the Food-101 dataset [10], the largest dataset for food images. The results suggest the superior performance of FoodDist over prior approaches (including the one proposed in the PlateClick paper) [47, 33, 10]. FoodDist is currently powering the backend of Yum-me and will be made available on Github.\nWe evaluate the desirability of Yum-me recommendations through a 60-person user study, where each user rates the healthy meal recommendations made by Yum-me relative to those made using a traditional survey-based approach. The study results show that, compared to the traditional survey based recommender, our system significantly improves the acceptance rate of the recommended healthy meals by 42.63%.\nNote that Yum-me is not intended to be a replacement for general food surveys or food journaling. Instead, we argue that Yum-me is complementary to the existing systems, and is particularly beneficial in the cold-start phase. As an overview, we summarize our contributions to the Ubicomp community as follow:\n\u2022 We present Yum-me, a healthy-meal recommender system that leverages users\u2019 fine-grained food preferences to generate personalized recommendations that are superior in the context of their health goals, while being closer to their preferences than using traditional techniques.\n\u2022 We develop FoodDist, a state-of-the-art unified food image analysis model that significantly outperforms all the existing models and can be applied to other ubiquitous dietary applications.\n\u2022 We evaluate the system in a 60-person user study and demonstrate its capability in improving the desirability of recommended healthy options.\nThis paper presents an example of eliciting fine-grained preferences for recommender systems. The applicability of this personalization paradigm is not limited to food, but may also\nbe applicable to general consumer products, content, entertainment, and IoT."}, {"heading": "RELATED WORK", "text": "Healthy meals recommendation system Traditional food and recipe recommendation systems learn users\u2019 dietary preferences from users\u2019 online activities, including ratings [21, 22], past recipe choices [41, 23], browsing history [44, 46, 2], etc. For example, Svensson et al. [41] built a social navigation system that recommends recipes based on the recipe choices made by different users. Van Pinxteren et al. [46] proposed to learn a recipe similarity measure from crowd card-sorting and make recommendations based on the self-reported meals. Beyond using users\u2019 online activities, food logging and journaling learn users\u2019 real food consumption history and require users\u2019 active involvement, such as taking food images [16] or writing down ingredients and metainformation [46].\nThe above systems, while able to learn users\u2019 detailed food preference, share a common limitation, that is they need to wait until a user generates enough data before their recommendations can be effective for this user (i.e., the cold-start problem). Therefore, most commercial applications, for example, Zipongo [6] and Shopwell [4] adopt onboarding surveys to more quickly elicit users\u2019 general food preferences. For instance, Zipongo\u2019s questionnaires [6] ask users about their nutrient intake, lifestyle, habits, and food preferences, and then make day-to-day and week-to-week healthy meals recommendations; ShopWell\u2019s survey [4] are designed to avoid certain food allergens, e.g., gluten, fish, corn, or poultry, and find meals that match to particular lifestyles, e.g., healthy pregnancy or athletic training.\nYum-me fills a vacuum that the prior approaches were not able to achieve, namely a rapid elicitation of users\u2019 fine-grained food preferences for immediate healthy meal recommendations. Based on PlateClick\u2019s online learning framework [47], Yum-me infers users\u2019 preferences for each single food item among a large food dataset, and projects these preferences for general food items into the domain that meets each individual user\u2019s health goals. We see Yum-me as a complement to the existing food preference elicitation approaches that further filters the food items selected by a traditional onboarding survey based on users\u2019 fine-grained taste for food, and allows a system to serve tailored recommendations upon the first use of the system.\nUbiquitous food image analysis The tasks of analyzing food images are very important in many ubiquitous dietary applications that actively or passively collect food images from mobile [16] and wearable [7, 42, 34] devices. The estimation of food intake and its nutritional information is helpful to our health [35] as it provides detailed records of our dietary history. Previous work mainly conducted the analysis by leveraging the crowd [35, 43] and computer vision algorithms [10, 33].\nNoronha et al. [35] crowdsourced nutritional analysis of food images by leveraging the wisdom of untrained crowds. The\nstudy demonstrated the possibility of estimating a meal\u2019s calories, fat, carbohydrates, and protein by aggregating the opinions from a large number of people; Turner-McGrievy et al. [43] elicit the crowd to rank the healthiness of several food items and validate the results against the ground truth provided by trained observers. Although this approach has been justified to be accurate, it inherently requires human resources that restrict it from scaling to large number of users and providing real time feedback.\nTo overcome the limitations of crowds and automate the analysis process, in the computer vision and Ubicomp communities, there are numerous papers discussing algorithms for food image analysis, including classification [10, 33, 29, 8], retrieval [30], and nutrient estimation [33, 40, 13, 24]. Most of the previous work [10] leveraged hand-crafted image features. However, traditional approaches were only demonstrated in special contexts, such as a specific restaurant [8] or particular type of cuisine [29] and the performances of the models might degrade when they are applied to food images in the wild.\nIn this paper, we designed FoodDist using deep convolutional neural network based multitask learning [12], which has been shown to be successful in improving model generalization power and performance in several applications [49, 17]. The main challenge of multitask learning is to design appropriate network structures and sharing mechanisms across tasks. With our proposed network structure, we show that FoodDist achieves superior performance when applied to the largest available real-world food image dataset [10], and when compared to prior approaches."}, {"heading": "PERSONALIZED HEALTHY MEAL RECOMMENDATIONS", "text": "Our personalized healthy-meal recommendation system, Yumme, operates over a given inventory of food items and suggests the items that will appeal to the users\u2019 pallate and meet their health goals and dietery restrictions. A high-level overview of Yum-me\u2019s recommendation process is shown in Fig. 1 and described as follows:\n\u2022 First, users answer a simple survey to specify their dietary restrictions and health goals. This information is used by Yum-me to filter food items and create an initial set of recommendation candidates.\n\u2022 Users then use PlateClick\u2019s visual interface [47] to express their fine-grained food preferences through simple comparisons of food items. The learned preferences are used to further re-rank the recommendations and present to the users.\nWe describe five major components that enable the above recommendation process: 1) large-scale food datasets for different dietary restrictions, 2) the user survey, 3) food suitability ranking, 4) PlateClick visual interface, and 5) fine-grained preference re-ranking.\nLarge scale food database To account for the dietary restrictions in many cultures and religions, or people\u2019s personal choices, we prepare a separate food dataset for each of the following dietary restrictions:"}, {"heading": "No restrictions, Vegetarian, Vegan, Kosher, Halal 1", "text": "For each diet type, we pulled over 10,000 main dish recipes along with their images and metadata (ingredients, nutrients, tastes, etc.) from the Yummly API [5]. The total number of recipes is around 50,000. In order to customize food recommendations for people with specific dietary restrictions, e.g., vegetarian and vegan, we filter recipes by setting the allowedDiet[] parameter in the search API. For kosher or halal, we explicitly rule out certain ingredients by setting excludedIngredient[] parameter. The lists of excluded ingredients are shown as below:\n\u2022 Kosher: pork, rabbit, horse meat, bear, shellfish, shark, eel, octopus, octopuses, moreton bay bugs, frog.\n\u2022 Halal: pork, blood sausage, blood, blood pudding, alcohol, grain alcohol, pure grain alcohol, ethyl alcohol.\nOne challenge in using a public food image API is that many recipes returned by the API contain non-food images and incomplete nutritional information. Therefore, we further filter the items with the following criteria: the recipe should have 1) nutritional information of calories, protein and fat, and 2) at least one food image. In order to automate this process, we build a binary classifier based on a deep convolutional neural network to filter out non-food images. As suggested by [33], we treat the whole training set of Food-101 dataset [10] as one generic food category and sampled the same number of images (75,750) from the ImageNet dataset [18] as our nonfood category. We took the pretrained VGG CNN model [39] and replaced the final 1000 dimensional softmax with a single logistic node. For the validation, we use the Food-101 testing dataset along with the same number of images sampled from ImageNet (25,250). We trained the binary classifier using the Caffe framework [26] and it reached 98.7% validation accuracy. We applied the criteria to all the datasets and the final statistics are shown in Table 1.\nFig. 2 shows the visualizations of the collected datasets2. For each of the recipe images, we embed it into an 1000- dimensional feature space using FoodDist (described later in FoodDist section) and then project all the images onto a 2-D plane using t-Distributed Stochastic Neighbor Embedding(tSNE) [45]. For visibility, we further divide the 2-D plane into several blocks; from each of which, we sample a representative food image residing in that block to present in the figure. Fig.2 demonstrates the large diversity and coverage of the collected datasets. Also, the embedding results clearly demonstrate the effectiveness of FoodDist in grouping similar food items together while pushing dissimilar items away. This is important to the performance of Yum-me, as discussed in User Study section."}, {"heading": "User survey", "text": "The user survey is designed to elicit user\u2019s high-level dietary restrictions and health goals. Users can specify their dietary\n1Our system is not restricted to these five dietary restrictions and we will extend the system functionalities to other categories in the future. 2We show the embeddings of only two representative databases (no restrictions and vegetarian) due to space limitations.\nrestrictions among the five categories mentioned-above and indicate their health goals in terms of the desired amount of calories, protein and fat. We choose these nutrients for their high relevance to many common health goals, such as weight control [19], sports performance [11], etc. We provide three options for each of these nutrients, including reduce, maintain, and increase. The user\u2019s diet type is used to select the appropriate food dataset, and the food items in the dataset are further ranked by their suitability to users\u2019 health goals based on the nutritional facts."}, {"heading": "Food suitability ranking", "text": "To measure the suitability of food items given users\u2019 health goals, we rank the recipes in terms of different nutrients in both ascending and descending order, such that each recipe is associated with six ranking values, i.e., rcalories,a, rcalories,d , rprotein,a, rprotein,d , rfat,a and rfat,d , where a and d stand for ascending and descending respectively. The final suitability value for each recipe given the health goal is calculated as follows:\nu = \u2211 n\u2208U \u03b1n,arn,a + \u2211 n\u2208U \u03b1n,drn,d (1)\nwhere U = {calories, protein, fat}. The indicator coefficient \u03b1n,a = 1 \u21d0\u21d2 nutrient n is rated as reduce and \u03b1n,d = 1 \u21d0\u21d2 nutrient n is rated as increase. Otherwise \u03b1n,a = 0 and \u03b1n,d = 0. Finally, given a user\u2019s responses to the survey, we rank the suitability of all the recipes in the corresponding database and select top-M items (around top 10%) as the candidate pool of healthy meals for this user. In our initial prototype, we set M = 500.\nPlateClick: fine-grained preference elicitation Based on the food suitability ranking, a candidate pool of healthy meals is created. However, not all the meals in this candidate pool will suit the user\u2019s palate. Therefore, PlateClick is used to further identify recipes that cater to the user\u2019s taste through eliciting their fine-grained food preferences. PlateClick is a visual online learning framework proposed in [47]. It learns users\u2019 fine-grained food preferences by presenting users with food images and ask them to choose ones that look delicious. In the following, we provide a brief introduction to PlateClick\u2019s online learning algorithm; for details, see [47].\nFormally, the food preference learning task can be defined as: given a large target set of food items S, we represent user\u2019s preferences as a distribution over all the possible food items, i.e. p = [p1, ..., p|S|],\u2211i pi = 1, where each element pi denotes\nthe user\u2019s favorable scale for item i. Since the number of items, |S|, is usually quite large and intractable to elicit each pi directly from the user 3, the approach PlateClick takes is to adaptively choose a much smaller subset V to present to the user, and propagate the users\u2019 preferences for those items to the rest items based on their visual similarity. Specifically, as Fig.1 shows, PlateClick\u2019s adaptive preference elicitation process can be divided into two phases:\nPhase I: In each of the first 2 iterations, we present ten food images and ask users to tap all the items that look delicious to them.\nPhase II: In each of the subsequent 13 iterations, we present a pair of food images and ask users to either compare the food pair and tap on the one that looks delicious to them or tap on \u201cYuck\u201d if neither of the items appeal to their taste.\nThe prior study showed that |V| \u2264 46 |S| is sufficient to learn users\u2019 preferences if imagesV are chosen properly. PlateClick adaptively chooses image sets for each iteration based on an exploration-exploitation strategy such that the images can cover a diverse range of food items and efficiently explore users\u2019 preferences. After each iteration t, we update the preference distribution pt\u22121 to pt using label propagation based on user\u2019s selections, where pt denotes the updated preference distribution at iteration t. The probability that the preference for food i will be propagated to food j is determined by the similarity between them, denoted by wi j, defined as follows:\nwi j =\n{ e\u2212 1 2\u03b12 \u2016 f (xi)\u2212 f (x j)\u20162 : \u2016 f (xi)\u2212 f (x j)\u2016 \u2264 \u03b4\n0 : \u2016 f (xi)\u2212 f (x j)\u2016> \u03b4 (2)\nwhere \u03b12 = 1|S|2 \u2211i, j\u2208S\u2016 f (xi)\u2212 f (x j)\u2016 2. f (xi) and f (x j) are feature vectors of food images xi and x j extracted by a feature extractor, denoted by f , and \u2016 f (xi)\u2212 f (x j)\u2016 represents the euclidean distance between item i and item j in the embedding.\n3The target set is often the whole food database that different applications use. For example, the size of Yummly database can be up to 1-million [5].\nWe use FoodDist as the feature extractor such that if the food items are similar to each other (i.e. \u2016 f (xi)\u2212 f (x j)\u2016 is close to 0), then wi j is approximately 1. Otherwise, with the increasing of the pairwise distance, the value of wi j smoothly goes down and finally reaches 0 when \u2016 f (xi)\u2212 f (x j)\u2016 is larger than a certain threshold \u03b4 . More details about FoodDist appear in FoodDist section."}, {"heading": "Preference re-ranking", "text": "Through PlateClick, we learn users\u2019 general food preferences. In order to project this information into the domain of healthy meal recommendations, we further re-rank the M candidate healthy food items with the users\u2019 preferences (i.e. pi for i \u2208M), and pick the top N items to present to the user. Note that, the general food preferences learned by PlateClick can be used not only for healthy meal recommendations but also for other dietary applications through a similar projection scheme. Also, in order to prevent repeating items between preferences learning and recommendation, in the recommendation phase, we remove those food images that are already used in PlateClick.\nFundamentally, for Yum-me to work, a robust feature extractor f for food is essential because both the accuracy of the inferred food preferences p = [p1, ..., p|S|] and the effectiveness of PlateClick\u2019s online learning algorithm all depend on a feature extractor f that can extract a robust feature vector f (xi) for every image i such that the food images\u2019 euclidean distances reflect their similarity. In the next section, we present such a model: FoodDist and discuss its broader applicability to food image analysis tasks in dietary applications."}, {"heading": "FOODDIST", "text": "Formally, the goal of FoodDist is to learn a feature extractor (embedding) f such that given an image x, f (x) will project it to an N dimensional feature vector and the euclidean distances between vectors reflect the similarities between food images, as Fig.3 shows. Formally speaking, if image x1 is more similar to image x2 than image x3, \u2016 f (x1)\u2212 f (x2)\u2016 < \u2016 f (x1)\u2212 f (x3)\u2016. We build FoodDist based on recent advances in deep Convolutional Neural Networks (CNN), which provides a powerful\nframework for automatic feature learning. Traditional feature representations for images are mostly hand-crafted, and were used with feature descriptors, such as SIFT (Scale Invariant Feature Transform)[32] etc., which are invariant to object scale and illumination and improve the generalizability of the trained model. However, when image characteristics are diverse the one-size-fits-all feature extractor performs poorly. In contrast, deep learning adapts the features to particular image characteristics and extracts features that are most discriminative in the given task [37].\nAs we present below, a feature extractor for food images can be learned through classification and metric learning, or through multitask learning, which concurrently performs the former two tasks. We demonstrate that the proposed multitask learning approach enjoys the benefits of both classification and metric learning, and achieves the best performance."}, {"heading": "Learning with classification", "text": "One common way to learn a feature extractor for labeled data is to train a neural network that performs classification (i.e. mapping input to labels), and takes the output of a hidden layer as the feature representations; specifically, using a feedforward deep convolutional neural network with n-layers (as the upper half of the Fig. 4 shows):\nF(x) = gn (gn\u22121 (. . .gi(. . .g1(x) . . .))) (3)\nwhere gi(.) represents the computation of i-th layer (e.g. convolution, pooling, fully-connected, etc.), and F(x) is the output class label. The difference of the output class label to the ground truth (i.e. the error) is back-propagated throughout the whole network from layer n to the layer 1. We can take the output of the layer n\u22121 as the feature representation of x, which is equivalent to having a feature extractor f as:\nf (x) = gn\u22121 (. . .gi(. . .g1(x) . . .)) (4)\nUsually, the last few layers will be fully-connected layers, and the last layer gn(.) is roughly equivalent to a linear classifier that is built on the features f (x) [25]. Therefore, f (x) is discriminative in separating instances under different categorical labels, and the euclidean distances between normalized feature vectors can reflect the similarities between images."}, {"heading": "Metric Learning", "text": "Different from the classification approach, where the feature extractor is a by-product, metric learning proposes to learn the distance embedding directly from the paired inputs of similar and dissimilar examples. Prior work [47] used a Siamese network to learn a feature extractor for food images. The structure of a siamese network resembles that in Fig. 4 but without Class label, Fully connected, 101 and Softmax Loss layers. The inputs of the siamese network are a pair of food images x1,x2. Each image goes through an sharing weight CNN and the output of each network is regarded as the feature representation, i.e. f (x1) and f (x2), respectively. Our goal is for f (x1) and f (x2) to have a small distance value (close to 0) if x1 and x2 are similar food items; otherwise, they should have a larger distance value. The value of contrastive loss is then back-propagated to optimize the siamese network:\nL (x1,x2, l) = 1 2 lD2 + 1 2 (1\u2212 l)max(0,m\u2212D)2 (5)\nwhere similarity label l \u2208 {0,1} indicates whether the input pair of food items x1, x2 are similar or not (l = 1 for similar, l = 0 for dissimilar), m > 0 is the margin for dissimilar items and D is the Euclidean distance between f (x1) and f (x2) in embedding space. Minimizing the contrastive loss will pull similar pairs together and push dissimilar farther away (larger than a margin m) and it exactly matches the goal.\nThe major advantage of metric learning is that the network will be directly optimized for our final goal, i.e. a robust distance measure between images. However, as shown in the model benchmarks, using the pairwise information alone does not improve the embedding performance as the process of sampling pairs loses the label information, which is arguably more discriminative than (dis)similar pairs.\nMultitask Learning: concurrently optimize both tasks Both methods above have their pros and cons. Learning with classification leverages the label information, but the network is not directly optimized to our goal. As a result, although the feature vectors are learned to be separable in the linear space, the intra- and inter- categorical distances might still be unbalanced. On the other hand, metric learning is explicitly optimized for our final objective by pushing the distances between dissimilar food items larger than a margin m. Nevertheless, sampling the similar or dissimilar pairs loses much label information. For example, given a pair of items with different labels, we only consider the dissimilarity between the two categories they belong to, but overlook the fact that each\nitem is also different from the rest n\u22122 categories, where n is total number of categories.\nIn order to leverage the benefits of both tasks, we propose a multitask learning design [25] for FoodDist. The idea of multitask learning is to share part of the model across tasks so as to improve the generalization ability of the learned model [25]. In our case, as Fig.4 shows, we share the parameters between the classification network and siamese network, and optimize them simultaneously. We use the base structure of the siamese network and share the upper CNN with a classification network where the output of the CNN is fed into a cascade of fully connected layer and softmax loss layer. The final loss of the whole network is the weighted sum of the softmax loss Lsoftmax and contrastive loss Lcontrastive:\nL = \u03c9Lsoftmax +(1\u2212\u03c9)Lcontrastive (6)\nOur benchmark results suggest that the feature extractor built with multitask learning achieves the best of both worlds: i.e. it achieves the best performance for both classification and euclidean distance based retrieval tasks. In the following, we present the benchmark methodology and results.\nModel details and benchmark setup We train all the models using Food-101 training dataset, which contains 75,750 food images from 101 food categories (750 instances for each category) [10]. To the best of our knowledge, Food-101 is the largest and most challenging publicly available dataset for food images. We implement models using Caffe[26] and experiment two CNN architectures in our framework: AlexNet[31], which won the first place at ILSVRC2012 challenge, and VGG[39], which is the state-of-the-art CNN model. The inputs to the networks are image crops of sizes 224\u00d7 224 (VGG)\nor 227\u00d7 227 (AlexNet). They are randomly sampled from an pixelwise-mean-subtracted image or its horizontal flip. In our benchmark, we train four different feature extractors: AlexNet+Learning with classification (AlexNet+CL), AlextNet+Multitask learning (AlexNet+MT), VGG+Learning with classification (VGG+CL) and VGG+Multitask learning (VGG+ML, FoodDist). For the multitask learning framework, we sample the similar and dissimilar image pairs with 1:10 ratio from the Food-101 dataset based on the categorical labels to be consistent with the previous work[47]. The models are fine-tuned based on the networks pre-trained with the ImageNet data. We use Stochastic Gradient Decent with a mini-batch size of 64, and each network is trained for 10\u00d7104 iterations. The initial learning rate is set to 0.001 and we use a weight decay of 0.0005 and momentum of 0.9.\nWe compare the performance of four feature extractors, including FoodDist, with the state-of-the-art food image analysis models using Food-101 testing dataset, which contains 25,250 food images from 101 food categories (250 instances for each category). The performance for classification and retrieval tasks are evaluated as follow:\n\u2022 Classification: We test the performance of using learned image features for classification. For the classification deep neural network in each of the models above, we adopt the standard 10-crop testing. i.e. the network makes a prediction by extracting ten patches (the four corner patches and the center patch in the original images and their horizontal reflections), and averaging the predictions at the softmax layer. The metrics used in this paper are Top-1 accuracy and Top-5 accuracy.\n\u2022 Retrieval: We use a retrieval task to evaluate the quality of the euclidean distances between extracted features. Ideally, the distances should be smaller for similar image pairs and\nlarger for dissimilar pairs. Therefore, as suggested by previous work [47, 48], We check the nearest k-neighbors of each test image, for k = 1,2, ...,N, where N = 25250 is the size of the testing dataset, and calculate the Precision and Recall values for each k. We use mean Average Precision (mAP) as the evaluation metric to compare the performance. For every method, the Precision/Recall values are averaged over all the images in the testing set."}, {"heading": "Performance", "text": "The classification and retrieval performance of all models are summarized in Table 2 and Table 3 respectively. FoodDist performs the best among four models and is significantly better than the state-of-the-art approaches in both tasks. For the classification task, the classifier built on FoodDist features achieves 83.09% Top-1 accuracy, which significantly outperforms the original RFDC [10] model and the proprietary GoogLeNet model [33]; For the retrieval task, FoodDist doubles the mAP value reported by previous work [47] that only used the AlexNet and siamese network architecture. The benchmark results demonstrate that FoodDist features possess high generalization ability and the euclidean distances between feature vectors reflect the similarities between food images with great fidelity. In addition, as we can observe from both tables, the multitask learning based approach always performs better than learning with classification for both tasks no matter which CNN is used. This further justifies the proposed multitask learning approach and its advantage of incorporating both label and pairwise distance information that makes the learned features more generalizable and meaningful in the euclidean distance embedding.\nFoodDist for a wide range of food image analysis tasks Essentially, FoodDist provides a unified model to extract features from food images so that they are discriminative in the\nclassification and clustering tasks, and its pairwise euclidean distances are meaningful in reflecting similarities. The model is rather efficient (< 0.5s/f on 8-core commodity processors) and can be ported to mobile devices with the publicly-available caffe-android-lib framework4. In addition to enabling Yumme, we plan to release FoodDist model and API to the community that can be used to fuel other nutritional applications. For the sake of space, we only briefly discuss two sample use cases below:\n\u2022 Food/Meal recognition: Given a set of labels, e.g. food categories, cuisines, and restaurants etc., the task of food and meal recognition could be approached by first extracting food image features from FoodDist and then train a linear classifier, e.g. Logistic regression, SVM etc., to classify the food images that are beyond the categories given in the Food-101 dataset.\n\u2022 Nutritional fact estimation: With the emergence of largescale food item or recipe databases, such as Yummly, the problem of nutritional facts estimation might be converted to a simple nearest-neighbor retrieval task: i.e. given a query image, we find its closest neighbor in the FoodDist based on their euclidean distance, and use that neighbor\u2019s nutritional information to estimate the nutrition facts of the query image [33]."}, {"heading": "USER STUDY", "text": "We conducted a user study to validate the efficacy of Yumme recommendations. We recruited 60 participants through the university mailing list, Facebook, and Twitter. The goal of the user study was to compare Yum-me recommendations with a widely-used baseline approach, i.e. a traditional food preference survey. We used a within-subjects study design in which each participant expressed their opinions regarding the meals recommended by both of the recommenders, and the effectiveness of the systems were compared on a per-user basis."}, {"heading": "Study Design", "text": "We created a traditional recommendation system by randomly picking N out of M meals in the candidate pool to recommend to the users. The values of N and M are controlled such that N = 10,M = 500 for both Yum-me and the traditional 4https://github.com/sh1r0/caffe-android-lib\nbaseline. The user study consists of three phases, as Fig.5 shows: (1) Each participant was asked to indicate their diet type and health goals through our basic user survey. (2) Each participant was then asked to use PlateClick. (3) 20 meal recommendations were arranged in a random order and presented to the participant at the same time, where 10 of them are made by Yum-me, and the other 10 are generated by the baseline. The participant was asked to express their opinion by dragging each of the 20 meals into either the Yummy or the No way bucket. To overcome the fact that humans would tend to balance the buckets if their previous choices were shown, the food item disappeared after the user dragged it into a bucket. In this way, users were not reminded of how many meals they had put into each bucket.\nThe user study systems were implemented as web services and participants accessed the study from desktop or mobile browsers. We chose a web service for its wide accessibility to the population, but we could easily fit Yum-me into other ubiquitous devices, as mentioned earlier."}, {"heading": "Participants", "text": "The most common dietary choice among our 60 participants was No restrictions (48), followed by Vegetarian (9), Halal (2) and Kosher (1). No participants chose Vegan. Participant preferences in terms of nutrients are summarized in Table.4. For Calories and Fat, the top two goals were Reduce and Maintain. For Protein, participants tended to choose either Increase or Maintain. For health goals, the top four participant choices were Maintain calories-Maintain protein-Maintain fat (20), Reduce calories-Maintain protein-Reduce fat (10), Reduce calories-Maintain protein-Maintain fat (10) and Reduce calories-Increase protein-Reduce fat (5). The statistics match well with the common health goals among the general population, i.e. people who plan to control weight and improve sports performance tend to reduce the intake calories and fat, and increase the amount of protein."}, {"heading": "Quantitative analysis", "text": "We calculated the participant acceptance rate of meal recommendations as: # Meals in Yummy bucket# Recommended meals .\nThe cumulative distribution of the acceptance rate is shown in Fig.6 and the average values per approach are presented in Table.5. The results clearly demonstrate that Yum-me significantly improves the quality of the presented food items. The per-user difference between the two approaches was normally distributed5, and a paired Student\u2019s t-test indicated a significant difference between the two methods (p < 0.0001).6\nTo further quantify the improvement provided by the Yumme system, we calculated the difference between the acceptance rates of the two systems, i.e. difference = Yum-me acceptance rate\u2212baseline acceptance rate. The distribution and average values of the differences are presented in Fig.7 and Table.5 respectively. It is noteworthy that Yum-me 5A Shapiro Wilk W test was not significant (p= 0.12), which justifies that the difference is normally distributed. 6We also performed a non-parametric Wilcoxon signed-rank test and found a comparable result.\noutperformed the baseline by 42.63% in terms of the number of preferred recommendations, which demonstrates its utility over the traditional meal recommendation approach."}, {"heading": "Qualitative analysis", "text": "To qualitatively understand the personalization mechanism of Yum-me, we randomly pick 3 participants with no dietary restrictions and with the health goal of reducing calories. For each such user, we select top-20 general food items the user likes most (inferred by PlateClick). These food items played important roles in selecting the healthy meals to recommend to the user. To visualize this relationship, among these top-20 items, we further select two food items that are most similar to the healthy items Yum-me recommended to the users and present three such examples in Fig.8. Intuitively, our system is able to recommend healthy food items that are visually similar to the food items a user like, but the recommended items are of lower calories due to the use of healthier ingredients or different cooking styles. These examples showcase how Yumme can project users\u2019 general food preferences to the domain of the healthy options, and find the ones that can most appeal to users."}, {"heading": "Error analysis", "text": "Through a closer examination of the cases where our system performed relatively poorly, we observed a negative corre-\nlation between the entropy of the learned preference distribution p 7 and the improvement of Yum-me over the baseline (r = \u22120.32, p = 0.026). This correlation suggests that when user\u2019s preference distributions are more concentrated, the recommended meals tend to perform better. This is not too suprising because the entropy of the preference distribution roughly reflects the degree of confidence the system has in the users\u2019 preferences, where the confidence is higher if the entropy is lower and vice versa. In Fig.9, we show the evolution of the entropy value as the users are making more comparisons through PlateClick. The results demonstrate that the system becomes more confident about user\u2019s preferences as users provide more feedback."}, {"heading": "CONCLUSION AND FUTURE WORK", "text": "In this paper, we propose Yum-me, a novel healthy meal recommender that makes health meal recommendations catering to users\u2019 fine-grained food preferences. We further present FoodDist, a best-of-its-kind unified food image analysis model. The user study and benchmarking results demonstrate the effectiveness of Yum-me and superior performance of FoodDist model. Looking forward, we envision that the idea of using visual similarity for preference elicitation can power a wider range of applications in the Ubicomp community. (1) User-centric dietary profile: the fine-grained food preference learned by Yum-me can be seen as a general dietary profile of each user and be projected to other domains to enable more dietary applications, such as suggesting proper meal plans for diabetes patients. Moreover, a personal dietary API can be built on top of this profile to enable the sharing and improvementments across multiple dietary applications. (2) Food image analysis API for deeper content understanding: With the release of the FoodDist model and API, many dietary applications, in particular the ones that capture a large number of food images, might benefit from a deeper understanding of their image contents. For instance, food journaling applications could benefit from the automatic analysis of food images to summarize the day-to-day food intake or trigger timely reminders and suggestions when needed. (3) Finegrained preference elicitation leveraging visual interfaces. The idea of eliciting users\u2019 fine-grained preference via visual interfaces is also applicable to other domains in Ubicomp. The key insight here is that visual contents capture many subtle 7Entropy of preference distribution: H(p) =\u2212\u2211i pi log pi\nvariations among objects that text or categorical data cannot capture; and the learned representations can be used as an effective medium to enable fine-grained preferences learning. For instance, the IoT, wearable, and mobile systems for entertainments, consumer products, and general content deliveries can leverage such an adaptive visual interface to design an onboarding process that learns users\u2019 preferences in a much shorter time and potentially more pleasantly than the traditional approaches."}], "references": [{"title": "Feasibility testing of an automated image-capture method to aid dietary recall. European journal of clinical nutrition", "author": ["Lenore Arab", "Deborah Estrin", "Donnie H Kim", "Jeff Burke", "Jeff Goldman"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Menu-match: restaurant-specific food logging from images", "author": ["Oscar Beijbom", "Neel Joshi", "Dan Morris", "Scott Saponas", "Siddharth Khullar"], "venue": "In Applications of Computer Vision (WACV),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Nutrition and depression: implications for improving mental health among childbearing-aged women", "author": ["Lisa M Bodnar", "Katherine L Wisner"], "venue": "Biological psychiatry 58,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Food-101\u2013mining discriminative components with random forests", "author": ["Lukas Bossard", "Matthieu Guillaumin", "Luc Van Gool"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Nutrition and sports performance", "author": ["JR Brotherhood"], "venue": "Sports Medicine 1,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1984}, {"title": "Multitask learning", "author": ["Rich Caruana"], "venue": "Machine learning 28,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Volume estimation using food specific shape templates in mobile image-based dietary assessment", "author": ["Junghoon Chae", "Insoo Woo", "SungYe Kim", "Ross Maciejewski", "Fengqing Zhu", "Edward J Delp", "Carol J Boushey", "David S Ebert"], "venue": "In IS&T/SPIE Electronic Imaging. International Society for Optics and Photonics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Lunch Line: using public displays and mobile devices to encourage healthy eating in an  organization", "author": ["Kerry Shih-Ping Chang", "Catalina M Danis", "Robert G Farrell"], "venue": "In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Rethinking the mobile food journal: Exploring opportunities for lightweight photo-based capture", "author": ["Felicia Cordeiro", "Elizabeth Bales", "Erin Cherry", "James Fogarty"], "venue": "In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Instance-aware Semantic Segmentation via Multi-task Network Cascades", "author": ["Jifeng Dai", "Kaiming He", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.04412", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Effect of diet and controlled exercise on weight loss in obese children", "author": ["Leonard H Epstein", "Rena R Wing", "Barbara C Penner", "Mary Jeanne Kress"], "venue": "The Journal of pediatrics 107,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1985}, {"title": "Content-boosted matrix factorization for recommender systems: experiments with recipe recommendation", "author": ["Peter Forbes", "Mu Zhu"], "venue": "In Proceedings of the fifth ACM conference on Recommender systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Intelligent food planning: personalized recipe recommendation", "author": ["Jill Freyne", "Shlomo Berkovsky"], "venue": "In Proceedings of the 15th international conference on Intelligent user interfaces", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "A personalized recipe advice system to promote healthful choices", "author": ["Gijs Geleijnse", "Peggy Nachtigall", "Pim van Kaam", "Luci\u00ebnne Wijgergangs"], "venue": "In Proceedings of the 16th international conference on Intelligent user interfaces", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Food image analysis: Segmentation, identification and weight estimation", "author": ["Ye He", "Chang Xu", "Neha Khanna", "Carol J Boushey", "Edward J Delp"], "venue": "In Multimedia and Expo (ICME),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "http://goodfeli.github.io/dlbook/ Book in preparation for", "author": ["Yoshua Bengio Ian Goodfellow", "Aaron Courville"], "venue": "Deep Learning", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proceedings of the ACM International Conference on Multimedia", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Sensing fork and persuasive game for improving eating behavior", "author": ["Azusa Kadomura", "Cheng-Yuan Li", "Yen-Chang Chen", "Hao-Hua Chu", "Koji Tsukada", "Itiro Siio"], "venue": "In Proceedings of the 2013 ACM conference on Pervasive and ubiquitous computing adjunct publication", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Persuasive technology to improve eating behavior using a sensor-embedded fork", "author": ["Azusa Kadomura", "Cheng-Yuan Li", "Koji Tsukada", "Hao-Hua Chu", "Itiro Siio"], "venue": "In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Food image recognition with deep convolutional features", "author": ["Yoshiyuki Kawano", "Keiji Yanai"], "venue": "In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "FoodLog: capture, analysis and retrieval of personal food images via web", "author": ["Keigo Kitamura", "Toshihiko Yamasaki", "Kiyoharu Aizawa"], "venue": "In Proceedings of the ACM multimedia 2009 workshop on Multimedia for cooking and eating activities", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["David G Lowe"], "venue": "International journal of computer vision 60,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}, {"title": "Im2Calories: towards an automated mobile vision food diary", "author": ["Austin Meyers", "Nick Johnston", "Vivek Rathod", "Anoop Korattikara", "Alex Gorban", "Nathan Silberman", "Sergio Guadarrama", "George Papandreou", "Jonathan Huang", "Kevin P Murphy"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Understanding food consumption lifecycles using wearable cameras", "author": ["Kher Hui Ng", "Victoria Shipp", "Richard Mortier", "Steve Benford", "Martin Flintham", "Tom Rodden"], "venue": "Personal and Ubiquitous Computing 19,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Platemate: crowdsourcing nutritional analysis from food photographs", "author": ["Jon Noronha", "Eric Hysen", "Haoqi Zhang", "Krzysztof Z Gajos"], "venue": "In UIST", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Diabetes and Healthy Eating A Systematic Review of the Literature", "author": ["Rachel Clare Povey", "David Clark-Carter"], "venue": "The Diabetes Educator 33,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["Ali Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Young people and healthy eating: a systematic review of research on barriers and facilitators", "author": ["Jonathan Shepherd", "Angela Harden", "Rebecca Rees", "Ginny Brunton", "Jo Garcia", "Sandy Oliver", "Ann Oakley"], "venue": "Health Education Research 21,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR abs/1409.1556", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Estimating nutritional value from food images based on semantic segmentation", "author": ["Kyoko Sudo", "Kazuhiko Murasaki", "Jun Shimamura", "Yukinobu Taniguchi"], "venue": "In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Designing and evaluating kalas: A social navigation system for food recipes", "author": ["Martin Svensson", "Kristina H\u00f6\u00f6k", "Rickard C\u00f6ster"], "venue": "ACM Transactions on Computer-Human Interaction (TOCHI) 12,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2005}, {"title": "Feasibility of identifying eating moments from first-person images leveraging human computation", "author": ["Edison Thomaz", "Aman Parnami", "Irfan Essa", "Gregory D Abowd"], "venue": "In Proceedings of the 4th International SenseCam & Pervasive Imaging Conference", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "The use of crowdsourcing for dietary self-monitoring: crowdsourced ratings of food pictures are comparable to ratings by trained observers", "author": ["Gabrielle M Turner-McGrievy", "Elina E Helander", "Kirsikka Kaipainen", "Jose Maria Perez-Macias", "Ilkka Korhonen"], "venue": "Journal of the American Medical Informatics Association 22,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Recipe recommendation method by considering the user\u00e2\u0102\u0179s preference and ingredient quantity of target recipe", "author": ["Mayumi Ueda", "Syungo Asanuma", "Yusuke Miyawaki", "Shinsuke Nakajima"], "venue": "In Proceedings of the International MultiConference of Engineers and Computer Scientists,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Visualizing data using t-SNE", "author": ["Laurens Van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2008}, {"title": "Deriving a recipe similarity measure for recommending healthful meals", "author": ["Youri van Pinxteren", "Gijs Geleijnse", "Paul Kamsteeg"], "venue": "In Proceedings of the 16th international conference on Intelligent user interfaces", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2011}, {"title": "2015a. PlateClick: Bootstrapping Food Preferences Through an Adaptive Visual Interface", "author": ["Longqi Yang", "Yin Cui", "Fan Zhang", "John P Pollak", "Serge Belongie", "Deborah Estrin"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Beyond Classification: Latent User Interests  Profiling from Visual Contents Analysis", "author": ["Longqi Yang", "Cheng-Kang Hsieh", "Deborah Estrin"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Facial landmark detection by deep multi-task learning", "author": ["Zhanpeng Zhang", "Ping Luo", "Chen Change Loy", "Xiaoou Tang"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}], "referenceMentions": [{"referenceID": 27, "context": "Healthy eating plays a critical role in our daily well-being and is indispensable in preventing and managing conditions such as diabetes, high blood pressure, cancer, mental illnesses, asthma, etc[36, 20, 9].", "startOffset": 196, "endOffset": 207}, {"referenceID": 2, "context": "Healthy eating plays a critical role in our daily well-being and is indispensable in preventing and managing conditions such as diabetes, high blood pressure, cancer, mental illnesses, asthma, etc[36, 20, 9].", "startOffset": 196, "endOffset": 207}, {"referenceID": 29, "context": "In particular, for children and young people, the adoption of healthy dietary habits has been shown to be beneficial to early cognitive development [38].", "startOffset": 148, "endOffset": 152}, {"referenceID": 19, "context": "In the Ubicomp community, various applications designed to promote healthy behaviors, including diet, have been proposed and studied [28, 14, 27, 15].", "startOffset": 133, "endOffset": 149}, {"referenceID": 7, "context": "In the Ubicomp community, various applications designed to promote healthy behaviors, including diet, have been proposed and studied [28, 14, 27, 15].", "startOffset": 133, "endOffset": 149}, {"referenceID": 18, "context": "In the Ubicomp community, various applications designed to promote healthy behaviors, including diet, have been proposed and studied [28, 14, 27, 15].", "startOffset": 133, "endOffset": 149}, {"referenceID": 37, "context": "Among those applications, the studies and products that target healthy meal recommendations have attracted much attention in academia [46] and industry [3] alike.", "startOffset": 134, "endOffset": 138}, {"referenceID": 8, "context": "5 entries per day [16].", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "It would take a non-trivial amount of time for the system to acquire sufficient amount of data to make recommendations, and the collected samples may subject to sampling biases as well [16].", "startOffset": 185, "endOffset": 189}, {"referenceID": 8, "context": "Moreover, the habit of photo food journaling is difficult to adopt and adhere to[16].", "startOffset": 80, "endOffset": 84}, {"referenceID": 38, "context": "We build upon a previously developed visual online learning framework, PlateClick[47], which learned users\u2019 fined-grained food preferences with simple pairwise comparisons between food images.", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "Based on deep convolutional networks and multi-task learning [31], FoodDist is the best-of-its-kind Euclidean distance embedding for food images, in which similar food items have smaller distances while dissimilar food items have larger distances.", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "We benchmark FoodDist with the Food-101 dataset [10], the largest dataset for food images.", "startOffset": 48, "endOffset": 52}, {"referenceID": 38, "context": "The results suggest the superior performance of FoodDist over prior approaches (including the one proposed in the PlateClick paper) [47, 33, 10].", "startOffset": 132, "endOffset": 144}, {"referenceID": 24, "context": "The results suggest the superior performance of FoodDist over prior approaches (including the one proposed in the PlateClick paper) [47, 33, 10].", "startOffset": 132, "endOffset": 144}, {"referenceID": 3, "context": "The results suggest the superior performance of FoodDist over prior approaches (including the one proposed in the PlateClick paper) [47, 33, 10].", "startOffset": 132, "endOffset": 144}, {"referenceID": 12, "context": "Traditional food and recipe recommendation systems learn users\u2019 dietary preferences from users\u2019 online activities, including ratings [21, 22], past recipe choices [41, 23], browsing history [44, 46, 2], etc.", "startOffset": 133, "endOffset": 141}, {"referenceID": 13, "context": "Traditional food and recipe recommendation systems learn users\u2019 dietary preferences from users\u2019 online activities, including ratings [21, 22], past recipe choices [41, 23], browsing history [44, 46, 2], etc.", "startOffset": 133, "endOffset": 141}, {"referenceID": 32, "context": "Traditional food and recipe recommendation systems learn users\u2019 dietary preferences from users\u2019 online activities, including ratings [21, 22], past recipe choices [41, 23], browsing history [44, 46, 2], etc.", "startOffset": 163, "endOffset": 171}, {"referenceID": 14, "context": "Traditional food and recipe recommendation systems learn users\u2019 dietary preferences from users\u2019 online activities, including ratings [21, 22], past recipe choices [41, 23], browsing history [44, 46, 2], etc.", "startOffset": 163, "endOffset": 171}, {"referenceID": 35, "context": "Traditional food and recipe recommendation systems learn users\u2019 dietary preferences from users\u2019 online activities, including ratings [21, 22], past recipe choices [41, 23], browsing history [44, 46, 2], etc.", "startOffset": 190, "endOffset": 201}, {"referenceID": 37, "context": "Traditional food and recipe recommendation systems learn users\u2019 dietary preferences from users\u2019 online activities, including ratings [21, 22], past recipe choices [41, 23], browsing history [44, 46, 2], etc.", "startOffset": 190, "endOffset": 201}, {"referenceID": 32, "context": "[41] built a social navigation system that recommends recipes based on the recipe choices made by different users.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[46] proposed to learn a recipe similarity measure from crowd card-sorting and make recommendations based on the self-reported meals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Beyond using users\u2019 online activities, food logging and journaling learn users\u2019 real food consumption history and require users\u2019 active involvement, such as taking food images [16] or writing down ingredients and metainformation [46].", "startOffset": 176, "endOffset": 180}, {"referenceID": 37, "context": "Beyond using users\u2019 online activities, food logging and journaling learn users\u2019 real food consumption history and require users\u2019 active involvement, such as taking food images [16] or writing down ingredients and metainformation [46].", "startOffset": 229, "endOffset": 233}, {"referenceID": 38, "context": "Based on PlateClick\u2019s online learning framework [47], Yum-me infers users\u2019 preferences for each single food item among a large food dataset, and projects these preferences for general food items into the domain that meets each individual user\u2019s health goals.", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "The tasks of analyzing food images are very important in many ubiquitous dietary applications that actively or passively collect food images from mobile [16] and wearable [7, 42, 34] devices.", "startOffset": 153, "endOffset": 157}, {"referenceID": 0, "context": "The tasks of analyzing food images are very important in many ubiquitous dietary applications that actively or passively collect food images from mobile [16] and wearable [7, 42, 34] devices.", "startOffset": 171, "endOffset": 182}, {"referenceID": 33, "context": "The tasks of analyzing food images are very important in many ubiquitous dietary applications that actively or passively collect food images from mobile [16] and wearable [7, 42, 34] devices.", "startOffset": 171, "endOffset": 182}, {"referenceID": 25, "context": "The tasks of analyzing food images are very important in many ubiquitous dietary applications that actively or passively collect food images from mobile [16] and wearable [7, 42, 34] devices.", "startOffset": 171, "endOffset": 182}, {"referenceID": 26, "context": "The estimation of food intake and its nutritional information is helpful to our health [35] as it provides detailed records of our dietary history.", "startOffset": 87, "endOffset": 91}, {"referenceID": 26, "context": "Previous work mainly conducted the analysis by leveraging the crowd [35, 43] and computer vision algorithms [10, 33].", "startOffset": 68, "endOffset": 76}, {"referenceID": 34, "context": "Previous work mainly conducted the analysis by leveraging the crowd [35, 43] and computer vision algorithms [10, 33].", "startOffset": 68, "endOffset": 76}, {"referenceID": 3, "context": "Previous work mainly conducted the analysis by leveraging the crowd [35, 43] and computer vision algorithms [10, 33].", "startOffset": 108, "endOffset": 116}, {"referenceID": 24, "context": "Previous work mainly conducted the analysis by leveraging the crowd [35, 43] and computer vision algorithms [10, 33].", "startOffset": 108, "endOffset": 116}, {"referenceID": 26, "context": "[35] crowdsourced nutritional analysis of food images by leveraging the wisdom of untrained crowds.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[43] elicit the crowd to rank the healthiness of several food items and validate the results against the ground truth provided by trained observers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "To overcome the limitations of crowds and automate the analysis process, in the computer vision and Ubicomp communities, there are numerous papers discussing algorithms for food image analysis, including classification [10, 33, 29, 8], retrieval [30], and nutrient estimation [33, 40, 13, 24].", "startOffset": 219, "endOffset": 234}, {"referenceID": 24, "context": "To overcome the limitations of crowds and automate the analysis process, in the computer vision and Ubicomp communities, there are numerous papers discussing algorithms for food image analysis, including classification [10, 33, 29, 8], retrieval [30], and nutrient estimation [33, 40, 13, 24].", "startOffset": 219, "endOffset": 234}, {"referenceID": 20, "context": "To overcome the limitations of crowds and automate the analysis process, in the computer vision and Ubicomp communities, there are numerous papers discussing algorithms for food image analysis, including classification [10, 33, 29, 8], retrieval [30], and nutrient estimation [33, 40, 13, 24].", "startOffset": 219, "endOffset": 234}, {"referenceID": 1, "context": "To overcome the limitations of crowds and automate the analysis process, in the computer vision and Ubicomp communities, there are numerous papers discussing algorithms for food image analysis, including classification [10, 33, 29, 8], retrieval [30], and nutrient estimation [33, 40, 13, 24].", "startOffset": 219, "endOffset": 234}, {"referenceID": 21, "context": "To overcome the limitations of crowds and automate the analysis process, in the computer vision and Ubicomp communities, there are numerous papers discussing algorithms for food image analysis, including classification [10, 33, 29, 8], retrieval [30], and nutrient estimation [33, 40, 13, 24].", "startOffset": 246, "endOffset": 250}, {"referenceID": 24, "context": "To overcome the limitations of crowds and automate the analysis process, in the computer vision and Ubicomp communities, there are numerous papers discussing algorithms for food image analysis, including classification [10, 33, 29, 8], retrieval [30], and nutrient estimation [33, 40, 13, 24].", "startOffset": 276, "endOffset": 292}, {"referenceID": 31, "context": "To overcome the limitations of crowds and automate the analysis process, in the computer vision and Ubicomp communities, there are numerous papers discussing algorithms for food image analysis, including classification [10, 33, 29, 8], retrieval [30], and nutrient estimation [33, 40, 13, 24].", "startOffset": 276, "endOffset": 292}, {"referenceID": 6, "context": "To overcome the limitations of crowds and automate the analysis process, in the computer vision and Ubicomp communities, there are numerous papers discussing algorithms for food image analysis, including classification [10, 33, 29, 8], retrieval [30], and nutrient estimation [33, 40, 13, 24].", "startOffset": 276, "endOffset": 292}, {"referenceID": 15, "context": "To overcome the limitations of crowds and automate the analysis process, in the computer vision and Ubicomp communities, there are numerous papers discussing algorithms for food image analysis, including classification [10, 33, 29, 8], retrieval [30], and nutrient estimation [33, 40, 13, 24].", "startOffset": 276, "endOffset": 292}, {"referenceID": 3, "context": "Most of the previous work [10] leveraged hand-crafted image features.", "startOffset": 26, "endOffset": 30}, {"referenceID": 1, "context": "However, traditional approaches were only demonstrated in special contexts, such as a specific restaurant [8] or particular type of cuisine [29] and the performances of the models might degrade when they are applied to food images in the wild.", "startOffset": 106, "endOffset": 109}, {"referenceID": 20, "context": "However, traditional approaches were only demonstrated in special contexts, such as a specific restaurant [8] or particular type of cuisine [29] and the performances of the models might degrade when they are applied to food images in the wild.", "startOffset": 140, "endOffset": 144}, {"referenceID": 5, "context": "In this paper, we designed FoodDist using deep convolutional neural network based multitask learning [12], which has been shown to be successful in improving model generalization power and performance in several applications [49, 17].", "startOffset": 101, "endOffset": 105}, {"referenceID": 40, "context": "In this paper, we designed FoodDist using deep convolutional neural network based multitask learning [12], which has been shown to be successful in improving model generalization power and performance in several applications [49, 17].", "startOffset": 225, "endOffset": 233}, {"referenceID": 9, "context": "In this paper, we designed FoodDist using deep convolutional neural network based multitask learning [12], which has been shown to be successful in improving model generalization power and performance in several applications [49, 17].", "startOffset": 225, "endOffset": 233}, {"referenceID": 3, "context": "With our proposed network structure, we show that FoodDist achieves superior performance when applied to the largest available real-world food image dataset [10], and when compared to prior approaches.", "startOffset": 157, "endOffset": 161}, {"referenceID": 38, "context": "\u2022 Users then use PlateClick\u2019s visual interface [47] to express their fine-grained food preferences through simple comparisons of food items.", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "As suggested by [33], we treat the whole training set of Food-101 dataset [10] as one generic food category and sampled the same number of images (75,750) from the ImageNet dataset [18] as our nonfood category.", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "As suggested by [33], we treat the whole training set of Food-101 dataset [10] as one generic food category and sampled the same number of images (75,750) from the ImageNet dataset [18] as our nonfood category.", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": "As suggested by [33], we treat the whole training set of Food-101 dataset [10] as one generic food category and sampled the same number of images (75,750) from the ImageNet dataset [18] as our nonfood category.", "startOffset": 181, "endOffset": 185}, {"referenceID": 30, "context": "We took the pretrained VGG CNN model [39] and replaced the final 1000 dimensional softmax with a single logistic node.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "We trained the binary classifier using the Caffe framework [26] and it reached 98.", "startOffset": 59, "endOffset": 63}, {"referenceID": 36, "context": "For each of the recipe images, we embed it into an 1000dimensional feature space using FoodDist (described later in FoodDist section) and then project all the images onto a 2-D plane using t-Distributed Stochastic Neighbor Embedding(tSNE) [45].", "startOffset": 239, "endOffset": 243}, {"referenceID": 11, "context": "We choose these nutrients for their high relevance to many common health goals, such as weight control [19], sports performance [11], etc.", "startOffset": 103, "endOffset": 107}, {"referenceID": 4, "context": "We choose these nutrients for their high relevance to many common health goals, such as weight control [19], sports performance [11], etc.", "startOffset": 128, "endOffset": 132}, {"referenceID": 38, "context": "PlateClick is a visual online learning framework proposed in [47].", "startOffset": 61, "endOffset": 65}, {"referenceID": 38, "context": "In the following, we provide a brief introduction to PlateClick\u2019s online learning algorithm; for details, see [47].", "startOffset": 110, "endOffset": 114}, {"referenceID": 23, "context": "Traditional feature representations for images are mostly hand-crafted, and were used with feature descriptors, such as SIFT (Scale Invariant Feature Transform)[32] etc.", "startOffset": 160, "endOffset": 164}, {"referenceID": 28, "context": "In contrast, deep learning adapts the features to particular image characteristics and extracts features that are most discriminative in the given task [37].", "startOffset": 152, "endOffset": 156}, {"referenceID": 16, "context": ") is roughly equivalent to a linear classifier that is built on the features f (x) [25].", "startOffset": 83, "endOffset": 87}, {"referenceID": 38, "context": "Prior work [47] used a Siamese network to learn a feature extractor for food images.", "startOffset": 11, "endOffset": 15}, {"referenceID": 16, "context": "In order to leverage the benefits of both tasks, we propose a multitask learning design [25] for FoodDist.", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "The idea of multitask learning is to share part of the model across tasks so as to improve the generalization ability of the learned model [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": "We train all the models using Food-101 training dataset, which contains 75,750 food images from 101 food categories (750 instances for each category) [10].", "startOffset": 150, "endOffset": 154}, {"referenceID": 17, "context": "We implement models using Caffe[26] and experiment two CNN architectures in our framework: AlexNet[31], which won the first place at ILSVRC2012 challenge, and VGG[39], which is the state-of-the-art CNN model.", "startOffset": 31, "endOffset": 35}, {"referenceID": 22, "context": "We implement models using Caffe[26] and experiment two CNN architectures in our framework: AlexNet[31], which won the first place at ILSVRC2012 challenge, and VGG[39], which is the state-of-the-art CNN model.", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "We implement models using Caffe[26] and experiment two CNN architectures in our framework: AlexNet[31], which won the first place at ILSVRC2012 challenge, and VGG[39], which is the state-of-the-art CNN model.", "startOffset": 162, "endOffset": 166}, {"referenceID": 38, "context": "For the multitask learning framework, we sample the similar and dissimilar image pairs with 1:10 ratio from the Food-101 dataset based on the categorical labels to be consistent with the previous work[47].", "startOffset": 200, "endOffset": 204}, {"referenceID": 38, "context": "Therefore, as suggested by previous work [47, 48], We check the nearest k-neighbors of each test image, for k = 1,2, .", "startOffset": 41, "endOffset": 49}, {"referenceID": 39, "context": "Therefore, as suggested by previous work [47, 48], We check the nearest k-neighbors of each test image, for k = 1,2, .", "startOffset": 41, "endOffset": 49}, {"referenceID": 3, "context": "09% Top-1 accuracy, which significantly outperforms the original RFDC [10] model and the proprietary GoogLeNet model [33]; For the retrieval task, FoodDist doubles the mAP value reported by previous work [47] that only used the AlexNet and siamese network architecture.", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "09% Top-1 accuracy, which significantly outperforms the original RFDC [10] model and the proprietary GoogLeNet model [33]; For the retrieval task, FoodDist doubles the mAP value reported by previous work [47] that only used the AlexNet and siamese network architecture.", "startOffset": 117, "endOffset": 121}, {"referenceID": 38, "context": "09% Top-1 accuracy, which significantly outperforms the original RFDC [10] model and the proprietary GoogLeNet model [33]; For the retrieval task, FoodDist doubles the mAP value reported by previous work [47] that only used the AlexNet and siamese network architecture.", "startOffset": 204, "endOffset": 208}, {"referenceID": 3, "context": "RFDC\u2217[10] 50.", "startOffset": 5, "endOffset": 9}, {"referenceID": 24, "context": "76% \u2212\u2212 GoogleLeNet\u2217[33] 79% \u2212\u2212", "startOffset": 19, "endOffset": 23}, {"referenceID": 38, "context": "Food-CNN\u2217[47] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "given a query image, we find its closest neighbor in the FoodDist based on their euclidean distance, and use that neighbor\u2019s nutritional information to estimate the nutrition facts of the query image [33].", "startOffset": 200, "endOffset": 204}], "year": 2017, "abstractText": "Many ubiquitous computing projects have addressed health and wellness behaviors such as healthy eating. Healthy meal recommendations have the potential to help individuals prevent or manage conditions such as diabetes and obesity. However, learning people\u2019s food preferences and making healthy recommendations that appeal to their palate is challenging. Existing approaches either only learn high-level preferences or require a prolonged learning period. We propose Yum-me, a personalized healthy-meal recommender system designed to meet individuals\u2019 health goals, dietary restrictions, and finegrained food preferences. Marrying ideas from user preference learning and healthy eating promotion, Yum-me enables a simple and accurate food preference profiling procedure via an image-based online learning framework, and projects the learned profile into the domain of healthy food options to find ones that will appeal to the user. We present the design and implementation of Yum-me, and further discuss the most critical component of it: FoodDist, a state-of-the-art food image analysis model. We demonstrate FoodDist\u2019s superior performance through careful benchmarking, and discuss its applicability across a wide array of dietary applications. We validate the feasibility and effectiveness of Yum-me through a 60-person user study, in which Yum-me improves the recommendation acceptance rate by 42.63% over the traditional food preference survey. ACM Classification", "creator": "LaTeX with hyperref package"}}}