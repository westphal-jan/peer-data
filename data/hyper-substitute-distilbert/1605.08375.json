{"id": "1605.08375", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Generalization Properties and Implicit Regularization for Multiple Passes SGM", "abstract": "we study both generalization properties inside stochastic correction methods for operators with convex loss rates and linearly sampled polynomials. one show that, only the absence limits sensitivity with constraints, the validity or approximation properties of the algorithm can be controlled by losses either in step - voltage or the weight of passes over the bandwidth. in optimization version, these artifacts can be seen to control a measure of rounding bias. numerical results record the exact findings.", "histories": [["v1", "Thu, 26 May 2016 17:37:51 GMT  (54kb,D)", "http://arxiv.org/abs/1605.08375v1", "26 pages, 4 figures. To appear in ICML 2016"]], "COMMENTS": "26 pages, 4 figures. To appear in ICML 2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["junhong lin", "raffaello camoriano", "lorenzo rosasco"], "accepted": true, "id": "1605.08375"}, "pdf": {"name": "1605.08375.pdf", "metadata": {"source": "CRF", "title": "Generalization Properties and Implicit Regularization for Multiple Passes SGM", "authors": ["Junhong Lin", "Raffaello Camoriano", "Lorenzo Rosasco"], "emails": ["jhlin5@hotmail.com", "raffaello.camoriano@iit.it", "lrosasco@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "The stochastic gradient method (SGM), often called stochastic gradient descent, has become an algorithm of choice in machine learning, because of its simplicity and small computational cost especially when dealing with big data sets [5].\nDespite its widespread use, the generalization properties of the variants of SGM used in practice are relatively little understood. Most previous works consider generalization properties of SGM with only one pass over the data, see e.g. [14] or [15] and references therein, while in practice multiple passes are usually considered. The effect of multiple passes has been studied extensively for the optimization of an empirical objective [6], but the role for generalization is less clear. In practice, early-stopping of the number of iterations, for example monitoring a hold-out set error, is a strategy often used to regularize. Moreover, the step-size is typically tuned to obtain the best results. The study in this paper is a step towards grounding theoretically these commonly used heuristics.\nOur starting points are a few recent works considering the generalization properties of different variants of SGM. One first series of results focus on least squares, either with one [21, 20, 10], or multiple (deterministic) passes over the data [16]. In the former case it is shown that, in general, if only one pass over the data is considered, then the step-size needs to be tuned to ensure optimal results. In [16] it is shown that a universal step-size choice can be taken, if multiple passes are considered. In this case, it is the stopping time that needs to be tuned.\nIn this paper, we are interested in general, possibly non smooth, convex loss functions. The analysis for least squares heavily exploits properties of the loss and does not generalize to this broader setting. Here, our starting points are the results in [12, 11, 15] considering convex loss functions. In [12], early\nar X\niv :1\n60 5.\n08 37\n5v 1\n[ cs\n.L G\n] 2\n6 M\nstopping of a (kernelized) batch subgradient method is analyzed, whereas in [11] the stability properties of SGM for smooth loss functions are considered in a general stochastic optimization setting and certain convergence results are derived. In [15], a more complex variant of SGM is analyzed and shown to achieve optimal rates.\nSince we are interested in analyzing regularization and generalization properties of SGM, in this paper we consider a general non-parametric setting. In this latter setting, the effects of regularization are typically more evident since it can directly affect the convergence rates. In this context, the difficulty of a problem is characterized by an assumption on the approximation error. Under this condition, the need for regularization becomes clear. Indeed, in the absence of other constraints, the good performance of the algorithm relies on a bias-variance trade-off that can be controlled by suitably choosing the step-size and/or the number of passes. These latter parameters can be seen to act as regularization parameters. Here, we refer to the regularization as \u2018implicit\u2019, in the sense that it is achieved neither by penalization nor by adding explicit constraints. The two main variants of the algorithm are the same as in least squares: one pass over the data with tuned step-size, or, fixed step-size choice and number of passes appropriately tuned. While in principle optimal parameter tuning requires explicitly solving a bias-variance trade-off, in practice adaptive choices can be implemented by cross-validation. In this case, both algorithm variants achieve optimal results, but different computations are entailed. In the first case, multiple single pass SGM need to be considered with different step-sizes, whereas in the second case, early stopping is used. Experimental results, complementing the theoretical analysis, are given and provide further insights on the properties of the algorithms.\nThe rest of the paper is organized as follows. In Section 2, we describe the supervised learning setting and the algorithm, and in Section 3, we state and discuss our main results. The proofs are postponed to the supplementary material. In Section 4, we present some numerical experiments on real datasets.\nNotation. For notational simplicity, [m] denotes {1, 2, \u00b7 \u00b7 \u00b7 ,m} for any m \u2208 N. The notation ak . bk means that there exists a universal constant C > 0 such that ak \u2264 Cbk for all k \u2208 N. Denote by dae the smallest integer greater than a for any given a \u2208 R."}, {"heading": "2 Learning with SGM", "text": "In this section, we introduce the supervised learning problem and the SGM algorithm.\nLearning Setting. Let X be a probability space and Y be a subset of R. Let \u03c1 be a probability measure on Z = X \u00d7 Y. Given a measurable loss function V : R\u00d7R\u2192 R+, the associated expected risk E = EV is defined as\nE(f) = \u222b Z V (y, f(x))d\u03c1.\nThe distribution \u03c1 is assumed to be fixed, but unknown, and the goal is to find a function minimizing the expected risk given a sample z = {zi = (xi, yi)}mi=1 of size m \u2208 N independently drawn according to \u03c1. Many classical examples of learning algorithms are based on empirical risk minimization, that is replacing the expected risk with the empirical risk Ez = EVz defined as\nEz(f) = 1\nm m\u2211 j=1 V (yj , f(xj)).\nIn this paper, we consider spaces of functions which are linearly parameterized. Consider a possibly non-linear data representation/feature map \u03a6 : X \u2192 F , mapping the data space in Rp, p \u2264 \u221e, or more generally in a (real separable) Hilbert space with inner product \u3008\u00b7, \u00b7\u3009 and norm \u2016 \u00b7 \u2016. Then, for w \u2208 F we consider functions of the form\nfw(x) = \u3008w,\u03a6(x)\u3009, \u2200x \u2208 X. (1)\nExamples of the above setting include the case where we consider infinite dictionaries, \u03c6j : X \u2192 R, j = 1, . . . , so that \u03a6(x) = (\u03c6j(x)) \u221e j=1, for all x \u2208 X, F = `2 and (1) corresponds to fw = \u2211p j=1 w j\u03c6j . Also, this setting includes, and indeed is equivalent to considering, functions defined by a positive definite kernel K : X \u00d7 X \u2192 R, in which case \u03a6(x) = K(x, \u00b7), for all x \u2208 X, F = HK the reproducing kernel Hilbert space associated with K, and (1) corresponds to the reproducing property\nfw(x) = \u3008w,K(x, \u00b7)\u3009,\u2200x \u2208 X. (2)\nIn the following, we assume the feature map to be measurable and define expected and empirical risks over functions of the form (1). For notational simplicity, we write E(fw) as E(w), and Ez(fw) as Ez(w).\nStochastic Gradient Method. For any fixed y \u2208 Y , assume the univariate function V (y, \u00b7) on R to be convex, hence its left-hand derivative V \u2032\u2212(y, a) exists at every a \u2208 R and is non-decreasing.\nAlgorithm 1. Given a sample z, the stochastic gradient method (SGM) is defined by w1 = 0 and\nwt+1 = wt \u2212 \u03b7tV \u2032\u2212(yjt , \u3008wt,\u03a6(xjt)\u3009)\u03a6(xjt), t = 1, . . . , T, (3)\nfor a non-increasing sequence of step-sizes {\u03b7t > 0}t\u2208N and a stopping rule T \u2208 N. Here, j1, j2, \u00b7 \u00b7 \u00b7 , jT are independent and identically distributed (i.i.d.) random variables1 from the uniform distribution on [m]. The (weighted) averaged iterates are defined by\nwt = t\u2211 k=1 \u03b7kwk/at, at = t\u2211 k=1 \u03b7k, t = 1, . . . , T.\nNote that T may be greater than m, indicating that we can use the sample more than once. We shall\nwrite J(t) to mean {j1, j2, \u00b7 \u00b7 \u00b7 , jt}, which will be also abbreviated as J when there is no confusion. The main purpose of the paper is to estimate the expected excess risk of the last iterate\nEz,J [E(wT )\u2212 inf w\u2208F E(w)],\nor similarly the expected excess risk of the averaged iterate wT , and study how different parameter settings in (1) affect the estimates. Here, the expectation Ez,J stands for taking the expectation with respect to J (given any z) first, and then the expectation with respect to z."}, {"heading": "3 Implicit Regularization for SGM", "text": "In this section, we present and discuss our main results. We begin in Subsection 3.1 with a universal convergence result and then provide finite sample bounds for smooth loss functions in Subsection 3.2, and for non-smooth functions in Subsection 3.3. As corollaries of these results we derive different implicit regularization strategies for SGM.\n1More precisely, j1, j2, \u00b7 \u00b7 \u00b7 , jT are conditionally independent given any z."}, {"heading": "3.1 Convergence", "text": "We begin presenting a convergence result, involving conditions on both the step-sizes and the number of iterations. We need some basic assumptions.\nAssumption 1. There holds\n\u03ba = sup x\u2208X\n\u221a \u3008\u03a6(x),\u03a6(x)\u3009 <\u221e. (4)\nFurthermore, the loss function is convex with respect to its second entry, and |V |0 := supy\u2208Y V (y, 0) <\u221e. Moreover, its left-hand derivative V \u2032\u2212(y, \u00b7) is bounded:\u2223\u2223V \u2032\u2212(y, a)\u2223\u2223 \u2264 a0, \u2200a \u2208 R, y \u2208 Y. (5)\nThe above conditions are common in statistical learning theory [19, 9]. For example, they are satisfied for the hinge loss V (y, a) = |1\u2212 ya|+ = max{0, 1\u2212 ya} or the logistic loss V (y, a) = log(1 + e\u2212ya) for all a \u2208 R, if X is compact and \u03a6(x) is continuous.\nThe bounded derivative condition (5) is implied by the requirement on the loss function to be Lipschitz\nin its second entry, when Y is a bounded domain. Given these assumptions, the following result holds.\nTheorem 1. If Assumption 1 holds, then\nlim m\u2192\u221e E[E(wt\u2217(m))]\u2212 inf w\u2208F E(w) = 0,\nprovided the sequence {\u03b7k}k and the stopping rule t\u2217(\u00b7) : N\u2192 N satisfy (A) limm\u2192\u221e \u2211t\u2217(m) k=1 \u03b7k m = 0, (B) and limm\u2192\u221e 1+ \u2211t\u2217(m) k=1 \u03b7 2 k\u2211t\u2217(m)\nk=1 \u03b7k = 0.\nAs seen from the proof in the appendix, Conditions (A) and (B) arise from the analysis of suitable sample, computational, and approximation errors. Condition (B) is similar to the one required by stochastic gradient methods [3, 7, 6]. The difference is that here the limit is taken with respect to the number of points, but the number of passes on the data can be bigger than one.\nTheorem 1 shows that in order to achieve consistency, the step-sizes and the running iterations need to be appropriately chosen. For instance, given m sample points for SGM with one pass2, i.e., t\u2217(m) = m, possible choices for the step-sizes are {\u03b7k = m\u2212\u03b1 : k \u2208 [m]} and {\u03b7k = k\u2212\u03b1 : k \u2208 [m]} for some \u03b1 \u2208 (0, 1). One can also fix the step-sizes a priori, and then run the algorithm with a suitable stopping rule t\u2217(m).\nThese different parameter choices lead to different implicit regularization strategies as we discuss next."}, {"heading": "3.2 Finite Sample Bounds for Smooth Loss Functions", "text": "In this subsection, we give explicit finite sample bounds for smooth loss functions, considering a suitable assumption on the approximation error.\nAssumption 2. The approximation error associated to the triplet (\u03c1, V,\u03a6) is defined by\nD(\u03bb) = inf w\u2208F\n{ E(w) + \u03bb\n2 \u2016w\u20162 } \u2212 inf w\u2208F E(w), \u2200\u03bb \u2265 0. (6)\nWe assume that for some \u03b2 \u2208 (0, 1] and c\u03b2 > 0, the approximation error satisfies\nD(\u03bb) \u2264 c\u03b2\u03bb\u03b2 , \u2200 \u03bb > 0. (7) 2We slightly abuse the term \u2018one pass\u2019, to mean m iterations.\nIntuitively, Condition (7) quantifies how hard it is to achieve the infimum of the expected risk. In particular, it is satisfied with \u03b2 = 1 when3 \u2203w\u2217 \u2208 F such that infw\u2208F E(w) = E(w\u2217). More formally, the condition is related to classical terminologies in approximation theory, such as K-functionals and interpolation spaces [19, 9]. The following remark is important for later discussions.\nRemark 1 (SGM and Implicit Regularization). Assumption 2 is standard in statistical learning theory when analyzing Tikhonov regularization [9, 19]. Besides, it has been shown that Tikhonov regularization can achieve best performance by choosing an appropriate penalty parameter which depends on the unknown parameter \u03b2 [9, 19]. In other words, in Tikhonov regularization, the penalty parameter plays a role of regularization. In this view, our coming results show that SGM can implicitly implement a form of Tikhonov regularization by controlling the step-size and/or the number of passes.\nA further assumption relates to the smoothness of the loss, and is satisfied for example by the logistic\nloss.\nAssumption 3. For all y \u2208 Y , V (y, \u00b7) is differentiable and V \u2032(y, \u00b7) is Lipschitz continuous with a constant L > 0, i.e.\n|V \u2032(y, b)\u2212 V \u2032(y, a)| \u2264 L|b\u2212 a|, \u2200a, b \u2208 R.\nThe following result characterizes the excess risk of both the last and the average iterate for any fixed\nstep-size and stopping time.\nTheorem 2. If Assumptions 1, 2 and 3 hold and \u03b7t \u2264 2/(\u03ba2L) for all t \u2208 N, then for all t \u2208 N,\nE[E(wt)\u2212 inf w\u2208F E(w)] . \u2211t k=1 \u03b7k m + \u2211t k=1 \u03b7 2 k\u2211t\nk=1 \u03b7k +\n( 1\u2211t\nk=1 \u03b7k\n)\u03b2 ,\nand\nE[E(wt)\u2212 inf w\u2208F E(w)] . \u2211t k=1 \u03b7k m t\u22121\u2211 k=1 \u03b7k \u03b7t(t\u2212 k) + ( t\u22121\u2211 k=1 \u03b72k \u03b7t(t\u2212 k) + \u03b7t ) + (\u2211t k=1 \u03b7k )1\u2212\u03b2 \u03b7tt .\nThe proof of the above result follows more or less directly from combining ideas and results in [12, 11] and is postponed to the appendix. The constants in the bounds are omitted, but given explicitly in the proof. While the error bound for the weighted average looks more concise than the one for the last iterate, interestingly, both error bounds lead to similar generalization properties.\nThe error bounds are composed of three terms related to sample error, computational error, and approximation error. Balancing these three error terms to achieve the minimum total error bound leads to optimal choices for the step-sizes {\u03b7k} and total number of iterations t\u2217. In other words, both the step-sizes {\u03b7k} and the number of iterations t\u2217 can play the role of a regularization parameter. Using the above theorem, general results for step-size \u03b7k = \u03b7t\n\u2212\u03b8 with some \u03b8 \u2208 [0, 1), \u03b7 = \u03b7(m) > 0 can be found in Proposition 3 from the appendix. Here, as corollaries we provide four different parameter choices to obtain the best bounds, corresponding to four different regularization strategies.\nThe first two corollaries correspond to fixing the step-sizes a priori and using the number of iterations as a regularization parameter. In the first result, the step-size is constant and depends on the number of sample points.\n3The existence of at least one minimizer in F is met for example when F is compact, or finite dimensional. In general, \u03b2 does not necessarily have to be 1, since the hypothesis space may be chosen as a general infinite dimensional space, for example in non-parametric regression.\nCorollary 1. If Assumptions 1, 2 and 3 hold and \u03b7t = \u03b71/ \u221a m for all t \u2208 N for some positive constant \u03b71 \u2264 2/(\u03ba2L), then for all t \u2208 N, and gt = wt (or wt),\nE[E(gt)\u2212 inf w\u2208F E(w)] . t log t\u221a m3 + log t\u221a m +\n(\u221a m\nt\n)\u03b2 . (8)\nIn particular, if we choose t\u2217 = dm \u03b2+3 2(\u03b2+1) e,\nE[E(gt\u2217)\u2212 inf w\u2208F E(w)] . m\u2212 \u03b2 \u03b2+1 logm. (9)\nIn the second result the step-sizes decay with the iterations.\nCorollary 2. If Assumptions 1, 2 and 3 hold and \u03b7t = \u03b71/ \u221a t for all t \u2208 N with some positive constant \u03b71 \u2264 2/(\u03ba2L), then for all t \u2208 N, and gt = wt (or wt),\nE[E(gt)\u2212 inf w\u2208F E(w)] .\n\u221a t log t\nm + log t\u221a t + 1 t\u03b2/2 . (10)\nParticularly, when t\u2217 = dm 2 \u03b2+1 e, we have (9).\nIn both the above corollaries the step-sizes are fixed a priori, and the number of iterations becomes the regularization parameter controlling the total error. Ignoring the logarithmic factor, the dominating terms in the bounds (8), (10) are the sample and approximation errors, corresponding to the first and third terms of RHS. Stopping too late may lead to a large sample error, while stopping too early may lead to a large approximation error. The ideal stopping time arises from a form of bias-variance trade-off and requires in general more than one pass over the data. Indeed, if we reformulate the results in terms of number of passes, we have that dm 1\u2212\u03b2 2(1+\u03b2) e passes are needed for the constant step-size {\u03b7t = \u03b71/ \u221a m}t, while dm 1\u2212\u03b2 1+\u03b2 e passes are needed for the decaying step-size {\u03b7t = \u03b71/ \u221a t}t. These observations suggest in particular that while both step-size choices achieve the same bounds, the constant step-size can have a computational advantage since it requires less iterations.\nNote that one pass over the data suffices only in the limit case when \u03b2 = 1, while in general it will be suboptimal, at least if the step-size is fixed. In fact, Theorem 2 suggests that optimal results could be recovered if the step-size is suitably tuned. The next corollaries show that this is indeed the case. The first result corresponds to a suitably tuned constant step-size.\nCorollary 3. If Assumptions 1, 2 and 3 hold and \u03b7t = \u03b71m \u2212 \u03b2\u03b2+1 for all t \u2208 N for some positive constant \u03b71 \u2264 2/(\u03ba2L), then for all t \u2208 N, and gt = wt (or wt),\nE[E(gt)\u2212 inf w\u2208F E(w)] . m\u2212 \u03b2+2 \u03b2+1 t log t+m\u2212 \u03b2 \u03b2+1 log t+m\n\u03b22\n\u03b2+1 t\u2212\u03b2 .\nIn particular, we have (9) for t\u2217 = m.\nThe second result corresponds to tuning the decay rate for a decaying step-size.\nCorollary 4. If Assumptions 1, 2 and 3 hold and \u03b7t = \u03b71t \u2212 \u03b2\u03b2+1 for all t \u2208 N for some positive constant \u03b71 \u2264 2/(\u03ba2L), then for all t \u2208 N, and gt = wt (or wt),\nE[E(gt)\u2212 inf w\u2208F E(w)] . m\u22121t 1 \u03b2+1 log t+ t\u2212 \u03b2 \u03b2+1 log t+ t\u2212 \u03b2 \u03b2+1 .\nIn particular, we have (9) for t\u2217 = m.\nThe above two results confirm that good performances can be attained with only one pass over the data, provided the step-sizes are suitably chosen, that is using the step-size as a regularization parameter.\nRemark 2. If we further assume that \u03b2 = 1, as often done in the literature, the convergence rates from Corollaries 1-4 are of order O(m\u22121/2), which are the same as those in, e.g., [18].\nFinally, the following remark relates the above results to data-driven parameter tuning used in prac-\ntice.\nRemark 3 (Bias-Variance and Cross-Validation). The above results show how the number of iterations/passes controls a bias-variance trade-off, and in this sense acts as a regularization parameter. In practice, the approximation properties of the algorithm are unknown and the question arises of how the parameter can be chosen. As it turns out, cross-validation can be used to achieve adaptively the best rates, in the sense that the rate in (9) is achieved by cross-validation or more precisely by hold-out cross-validation. These results follow by an argument similar to that in Chapter 6 from [19] and are omitted."}, {"heading": "3.3 Finite Sample Bounds for Non-smooth Loss Functions", "text": "Theorem 2 holds for smooth loss functions and it is natural to ask if a similar result holds for non-smooth losses such as the hinge loss. Indeed, analogous results hold, albeit current bounds are not as sharp.\nTheorem 3. If Assumptions 1 and 2 hold, then \u2200t \u2208 N,\nE[E(wt)\u2212 inf w\u2208F E(w)] . \u221a\u2211t k=1 \u03b7k m + \u2211t k=1 \u03b7 2 k\u2211t\nk=1 \u03b7k +\n( 1\u2211t\nk=1 \u03b7k\n)\u03b2 ,\nand\nE[E(wt)\u2212 inf w\u2208F E(w)] . \u221a\u2211t k=1 \u03b7k m t\u22121\u2211 k=1 \u03b7k \u03b7t(t\u2212 k) + t\u22121\u2211 k=1 \u03b72k \u03b7t(t\u2212 k) + \u03b7t + (\u2211t k=1 \u03b7k )1\u2212\u03b2 \u03b7tt .\nThe proof of the above theorem is based on ideas from [12], where tools from Rademacher complexity\n[2, 13] are employed. We postpone the proof in the appendix.\nUsing the above result with concrete step-sizes as those for smooth loss functions, we have the following\nexplicit error bounds and corresponding stopping rules. Corollary 5. Under Assumptions 1 and 2, let \u03b7t = 1/ \u221a m for all t \u2208 N. Then for all t \u2208 N, and gt = wt (or wt),\nE[E(gt)\u2212 inf w\u2208F E(w)] .\n\u221a t log t\nm3/4 + log t\u221a m +\n(\u221a m\nt\n)\u03b2 .\nIn particular, if we choose t\u2217 = dm 2\u03b2+3 4\u03b2+2 e,\nE[E(gt\u2217)\u2212 inf w\u2208F E(w)] . m\u2212 \u03b2 2\u03b2+1 logm. (11)\nCorollary 6. Under Assumptions 1 and 2, let \u03b7t = 1/ \u221a t for all t \u2208 N. Then for all t \u2208 N, and gt = wt (or wt),\nE[E(gt)\u2212 inf w\u2208F E(w)] . t 1/4 log t\u221a m + log t\u221a t + 1 t\u03b2/2 .\nIn particular, if we choose t\u2217 = dm 2 2\u03b2+1 e, there holds (11).\nFrom the above two corollaries, we see that the algorithm with constant step-size 1/ \u221a m can stop earlier than the one with decaying step-size 1/ \u221a t when \u03b2 \u2264 1/2, while they have the same convergence rate, since m 2\u03b2+3 4\u03b2+2 /m 2 2\u03b2+1 = m 2\u03b2\u22121 4\u03b2+1 . Note that the bound in (11) is slightly worse than that in (9), see Section 3.4 for more discussion.\nSimilar to the smooth case, we also have the following results for SGM with one pass where regular-\nization is realized by step-size.\nCorollary 7. Under Assumptions 1 and 2, let \u03b7t = m \u2212 2\u03b22\u03b2+1 for all t \u2208 N. Then for all t \u2208 N, and gt = wt (or wt),\nE[E(gt)\u2212 inf w\u2208F E(w)] . m\u2212 4\u03b2+1 4\u03b2+2\n\u221a t log t+m\u2212 2\u03b2 2\u03b2+1 log t+m 2\u03b22 2\u03b2+1 t\u2212\u03b2 ."}, {"heading": "In particular, (11) holds for t\u2217 = m.", "text": "Corollary 8. Under Assumptions 1 and 2, let \u03b7t = t \u2212 2\u03b22\u03b2+1 for all t \u2208 N. Then for all t \u2208 N, and gt = wt (or wt),\nE[E(gt)\u2212 inf w\u2208F E(w)] . m\u2212 12 t 1 4\u03b2+2 log t+ t\u2212 min(2\u03b2,1) 2\u03b2+1 log t+ t\u2212 \u03b2 2\u03b2+1 ."}, {"heading": "In particular, (11) holds for t\u2217 = m.", "text": ""}, {"heading": "3.4 Discussion and Proof Sketch", "text": "As mentioned in the introduction, the literature on theoretical properties of the iteration in Algorithm 1 is vast, both in learning theory and in optimization. A first line of works focuses on a single pass and convergence of the expected risk. Approaches in this sense include classical results in optimization (see [14] and references therein), but also approaches based on so-called \u201conline to batch\u201d conversion (see [15] and references therein). The latter are based on analyzing a sequential prediction setting and then on considering the averaged iterate to turn regret bounds in expected risk bounds. A second line of works focuses on multiple passes, but measures the quality of the corresponding iteration in terms of the minimization of the empirical risk. In this view, Algorithm 1 is seen as an instance of incremental methods for the minimization of objective functions that are sums of a finite, but possibly large, number of terms [4]. These latter works, while interesting in their own right, do not yield any direct information on the generalization properties of considering multiple passes.\nHere, we follow the approach in [5] advocating the combination of statistical and computational errors. The general proof strategy is to consider several intermediate steps to relate the expected risk of the empirical iteration to the minimal expected risk. The argument we sketch below is a simplified and less sharp version with respect to the one used in the actual proof, but it is easier to illustrate and still carries some important aspects which are useful for comparison with related results.\nConsider an intermediate element w\u0303 \u2208 F and decompose the excess risk as\nEE(wt)\u2212 inf w\u2208F E = E(E(wt)\u2212 Ez(wt)) + E(Ez(wt)\u2212 Ez(w\u0303)) + EEz(w\u0303)\u2212 inf w\u2208F E .\nThe first term on the right-hand side is the generalization error of the iterate. The second term can be seen as a computational error. To discuss the last term, it is useful to consider a few different choices for w\u0303. Assuming the empirical and expected risks to have minimizers w\u2217z and w \u2217, a possibility is to set\nw\u0303 = w\u2217z, this can be seen to be the choice made in [11]. In this case, it is immediate to see that the last term is negligible since,\nEEz(w\u0303) = E min w\u2208F Ez(w) \u2264 min w\u2208F EEz(w) = min w\u2208F E(w),\nand hence,\nEEz(w\u0303)\u2212 min w\u2208F E \u2264 0.\nOn the other hand, in this case the computational error depends on the norm \u2016w\u2217z\u2016 which is in general hard to estimate. A more convenient choice is to set w\u0303 = w\u2217. A reasoning similar to the one above shows that the last term is still negligible and the computational error can still be controlled depending on \u2016w\u2217\u2016. In a non-parametric setting, the existence of a minimizer is not ensured and corresponds to a limit case where there is small approximation error. Our approach is then to consider an almost minimizer of the expected risk with a prescribed accuracy. Following [12], we do this introducing Assumption (6) and choosing w\u0303 as the unique minimizer of E + \u03bb\u2016 \u00b7 \u20162, \u03bb > 0. Then the last term in the error decomposition can be upper bounded by the approximation error.\nFor the generalization error, the stability results from [11] provide sharp estimates for smooth loss functions and in the \u2018capacity independent\u2019 limit, that is under no assumptions on the covering numbers of the considered function space. For this setting, the obtained bound is optimal in the sense that it matches the best available bound for Tikhonov regularization [19, 9]. For the non-smooth case a standard argument based on Rademacher complexity can be used, and easily extended to be capacity dependent. However, the corresponding bound is not sharp and improvements are likely to hinge on deriving better norm estimates for the iterates. The question does not seem to be straightforward and is deferred to a future work.\nThe computational error for the averaged iterates can be controlled using classic arguments [6], whereas for the last iterate the arguments in [12, 18] are needed. Finally, Theorems 2, 3 result from estimating and balancing the various error terms with respect to the choice of the step-size and number of passes.\nWe conclude this section with some perspective on the results in the paper. We note that since the primary goal of this study was to analyze the implicit regularization effect of step-size and number of passes, we have considered a very simple iteration. However, it would be very interesting to consider more sophisticated, \u2018accelerated\u2019 iterations [17], and assess the potential advantages in terms of computational and generalization aspects. Similarly, we chose to keep the analysis in the paper relatively simple, but several improvements can be considered for example deriving high probability bounds and sharper error bounds under further assumptions. Some of these improvements are relatively straightforward, see e.g. [12], but others will require non-trivial extensions of results developed for Tikhonov regularization in the last few years. Finally, here we only referred to a simple cross-validation approach to parameter tuning, but it would clearly be very interesting to find ways to tune parameters online. A remarkable result in this direction is derived in [15], where it is shown that, in the capacity independent setting, adaptive online parameter tuning is indeed possible."}, {"heading": "4 Numerical Simulations", "text": "We carry out some numerical simulations to illustrate our results4. The experiments are executed 10 times each, on the benchmark datasets5 reported in Table 1, in which the Gaussian kernel bandwidths \u03c3 used by SGM and SIGM6 for each learning problem are also shown. Here, the loss function is the hinge loss7. The experimental platform is a server with 12 \u00d7 Intelr Xeonr E5-2620 v2 (2.10GHz) CPUs and 132 GB of RAM. Some of the experimental results, as specified in the following, have been obtained by running the experiments on subsets of the data samples chosen uniformly at random. In order to apply hold-out cross-validation, the training set is split in two parts: one for empirical risk minimization and the other for validation error computation (80% - 20%, respectively). All the samples are randomly shuffled at each repetition."}, {"heading": "4.1 Regularization in SGM and SIGM", "text": "In this subsection, we illustrate four concrete examples showing different regularization effects of the step-size in SGM and the number of passes in SIGM. In all these four examples, we consider the Adult dataset with sample size n = 1000.\nIn the first experiment, the SIGM step-size is fixed as \u03b7 = 1/ \u221a n. The test error computed with respect to the hinge loss at each pass is reported in Figure 1(a). Note that the minimum test error is reached for a number of passes smaller than 20, after which it significantly increases, a so-called overfitting regime. This result clearly illustrates the regularization effect of the number of passes. In the second experiment, we consider SIGM with decaying step-size (\u03b7 = 1/4 and \u03b8 = 1/2). As shown in Figure 1(b), overfitting is not observed in the first 100 passes. In this case, the convergence to the optimal solution appears slower\n4Code: lcsl.github.io/MultiplePassesSGM 5Datasets: archive.ics.uci.edu/ml and www.csie.ntu.edu.tw/~cjlin/libsvmtools/ datasets/ 6In what follows, we name one pass SGM and multiple passes SGM as SGM and SIGM, respectively. 7Experiments with the logistic loss have also been carried out, showing similar empirical results to those considering the\nhinge loss. The details are not included in this text due to space limit.\nthan that in the fixed step-size case.\nIn the last two experiments, we consider SGM and show that the step-size plays the role of a regularization parameter. For the fixed step-size case, i.e., \u03b8 = 0, we perform SGM with different \u03b7 \u2208 (0, 1] (logarithmically scaled). We plot the errors in Figure 2(a), showing that a large step-size (\u03b7 = 1) leads to overfitting, while a smaller one (e.g., \u03b7 = 10\u22123) is associated to oversmoothing. For the decaying step-size case, we fix \u03b71 = 1/4, and run SGM with different \u03b8 \u2208 [0, 1]. The errors are plotted in Figure 2(b), from which we see that the exponent \u03b8 has a regularization effect. In fact, a more \u2018aggressive\u2019 choice (e.g., \u03b8 = 0, corresponding to a fixed step-size) leads to overfitting, while for a larger \u03b8 (e.g., \u03b8 = 1) we observe oversmoothing."}, {"heading": "4.2 Accuracy and Computational Time Comparison", "text": "In this subsection, we compare SGM with cross-validation and SIGM with benchmark algorithm LIBSVM [8], both in terms of accuracy and computational time. For SGM, with 30 parameter guesses, we use cross-validation to tune the step-size (either setting \u03b8 = 0 while tuning \u03b7, or setting \u03b7 = 1/4 while tuning \u03b8). For SIGM, we use two kinds of step-size suggested by Section 3: \u03b7 = 1/ \u221a m and \u03b8 = 0, or \u03b7 = 1/4 and \u03b8 = 1/2, using early stopping via cross-validation. The test errors with respect to the hinge loss, the test relative misclassification errors and the computational times are collected in Table 2.\nWe first start comparing accuracies. The results in Table 2 indicate that SGM with constant and decaying step-sizes and SIGM with fixed step-size reach comparable test errors, which are in line with the LIBSVM baseline. Observe that SIGM with decaying step-size attains consistently higher test errors, a phenomenon already illustrated in Section 4.1 in theory.\nWe now compare the computational times for cross-validation. We see from Table 2 that the training times of SIGM and SGM, either with constant or decaying step-sizes, are roughly the same. We also observe that SGM and SIGM are faster than LIBSVM on relatively large datasets (Adult with n = 32562, and Ijcnn1 with n = 49990). Moreover, for small datasets (BreastCancer with n = 400, Adult with n = 1000, and Ijcnn1 with n = 1000), SGM and SIGM are comparable with or slightly slower than LIBSVM."}, {"heading": "Acknowledgments", "text": "This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. L. R. acknowledges the financial support of the Italian Ministry of Education, University and Research FIRB project RBFR12M3AC. The authors would like to thank\nDr. Francesco Orabona for the fruitful discussions on this research topic, and Dr. Silvia Villa and the referees for their valuable comments."}, {"heading": "A Basic Lemmas", "text": "The following basic lemma is useful to our proofs, which will be used several times. Its proof follows from the convexity of V (y, \u00b7) and the fact that V \u2032\u2212(y, a) is bounded.\nLemma 1. Under Assumption 1, for any k \u2208 N and w \u2208 F , we have\n\u2016wk+1 \u2212 w\u20162 \u2264 \u2016wk \u2212 w\u20162 + (a0\u03ba)2\u03b72k + 2\u03b7k [V (yjk , \u3008w,\u03a6(xjk)\u3009)\u2212 V (yjk , \u3008wk,\u03a6(xjk)\u3009)] . (12)\nProof. Since wk+1 is given by (3), by expanding the inner product, we have\n\u2016wk+1\u2212w\u20162 = \u2016wk\u2212w\u20162+\u03b72k\u2016V \u2032\u2212(yjk , \u3008wk,\u03a6(xjk)\u3009)\u03a6(xjk)\u20162+2\u03b7kV \u2032\u2212(yjk , \u3008wk,\u03a6(xjk)\u3009) \u3008w \u2212 wk,\u03a6(xjk)\u3009 .\nThe bounded assumption (4) implies that \u2016\u03a6(xjk)\u2016 \u2264 \u03ba and by (5), |V \u2032\u2212(yjk , \u3008wk,\u03a6(xjk)\u3009)| \u2264 a0. We thus have\n\u2016wk+1 \u2212 w\u20162 \u2264 \u2016wk \u2212 w\u20162 + (a0\u03ba)2\u03b72k + 2\u03b7kV \u2032\u2212(yjk , \u3008wk,\u03a6(xjk)\u3009)[\u3008w,\u03a6(xjk)\u3009 \u2212 \u3008wk,\u03a6(xjk)\u3009].\nUsing the convexity of V (yjk , \u00b7) which tells us that\nV \u2032\u2212(yjk , a)(b\u2212 a) \u2264 V (yjk , b)\u2212 V (yjk , a), \u2200a, b \u2208 R,\nwe reach the desired bound. The proof is complete.\nTaking the expectation of (12) with respect to the random variable jk, and noting that wk is inde-\npendent from jk given z, one can get the following result.\nLemma 2. Under Assumption 1, for any fixed k \u2208 N, given any z, assume that w \u2208 F is independent of the random variable jk. Then we have\nEjk [\u2016wk+1 \u2212 w\u20162] \u2264 \u2016wk \u2212 w\u20162 + (a0\u03ba)2\u03b72k + 2\u03b7k (Ez(w)\u2212 Ez(wk)) . (13)"}, {"heading": "B Sample Errors", "text": "Note that our goal is to bound the excess generalization error E[E(wT )\u2212 infw\u2208F E(w)], whereas the lefthand side of (13) is related to an empirical error. The difference between the generalization and empirical errors is a so-called sample error. To estimate this sample error, we introduce the following lemma, which gives a uniformly upper bound for sample errors over a ball BR = {w \u2208 F : \u2016w\u2016 \u2264 R}. Its proof is based on a standard symmetrization technique and Rademacher complexity, e.g. [1, 13]. For completeness, we provide a proof here.\nLemma 3. Assume (4) and (5). For any R > 0, we have\u2223\u2223\u2223\u2223Ez [ sup w\u2208BR (E(w)\u2212 Ez(w)) ]\u2223\u2223\u2223\u2223 \u2264 2a0\u03baR\u221am .\nProof. Let z\u2032 = {z\u2032i = (x\u2032i, y\u2032i)}mi=1 be another training sample from \u03c1, and assume that it is independent from z. We have\nEz [\nsup w\u2208BR\n(E(w)\u2212 Ez(w)) ] = Ez [\nsup w\u2208BR\nEz\u2032 [Ez\u2032(w)\u2212 Ez(w)] ] \u2264 Ez,z\u2032 [ sup w\u2208BR (Ez\u2032(w)\u2212 Ez(w)) ] .\nLet \u03c31, \u03c32, . . . , \u03c3m be independent random variables drawn from the Rademacher distribution, i.e. Pr(\u03c3i = +1) = Pr(\u03c3i = \u22121) = 1/2 for i = 1, 2, . . . ,m. Using a standard symmetrization technique, for example in [13], we get\nEz [\nsup w\u2208BR\n(E(w)\u2212 Ez(w)) ] \u2264 Ez,z\u2032,\u03c3 [ sup w\u2208BR { 1 m m\u2211 i=1 \u03c3i(V (y \u2032 i, \u3008w,\u03a6(x\u2032i)\u3009)\u2212 V (yi, \u3008w,\u03a6(xi)\u3009)) }]\n\u2264 2Ez,\u03c3 [ sup w\u2208BR { 1 m m\u2211 i=1 \u03c3iV (yi, \u3008w,\u03a6(xi)\u3009) }] .\nWith (5), by applying Talagrand\u2019s contraction lemma, see e.g. [1], we derive Ez [\nsup w\u2208BR\n(E(w)\u2212 Ez(w)) ] \u2264 2a0Ez,\u03c3 [ sup w\u2208BR 1 m m\u2211 i=1 \u03c3i\u3008w,\u03a6(xi)\u3009 ] = 2a0Ez,\u03c3 [ sup w\u2208BR \u2329 w, 1 m m\u2211 i=1 \u03c3i\u03a6(xi) \u232a] .\nUsing Cauchy-Schwartz inequality, we reach Ez [\nsup w\u2208BR\n(E(w)\u2212 Ez(w)) ] \u2264 2a0Ez,\u03c3 [ sup w\u2208BR \u2016w\u2016 \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 \u03c3i\u03a6(xi) \u2225\u2225\u2225\u2225\u2225 ] \u2264 2a0REz,\u03c3 [\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 \u03c3i\u03a6(xi) \u2225\u2225\u2225\u2225\u2225 ] .\nBy Jensen\u2019s inequality, we get\nEz [\nsup w\u2208BR\n(E(w)\u2212 Ez(w)) ] \u2264 2a0R Ez,\u03c3 \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 \u03c3i\u03a6(xi) \u2225\u2225\u2225\u2225\u2225 2 1/2 = 2a0R[ 1 m2 Ez,\u03c3 m\u2211 i=1 \u2016\u03a6(xi)\u20162 ]1/2 .\nThe desired result thus follows by introducing (4) to the above. Note that the above procedure also applies if we replace E(w)\u2212 Ez(w) with Ez(w)\u2212 E(w). The proof is complete.\nThe following lemma gives upper bounds on the iterated sequence.\nLemma 4. Under Assumption 1. Then for any t \u2208 N, we have\n\u2016wt+1\u2016 \u2264 \u221a\u221a\u221a\u221a(a0\u03ba)2 t\u2211 k=1 \u03b72k + 2|V |0 t\u2211 k=1 \u03b7k.\nProof. Using Lemma 1 with w = 0, we have\n\u2016wk+1\u20162 \u2264 \u2016wk\u20162 + (a0\u03ba)2\u03b72k + 2\u03b7k [V (yjk , 0)\u2212 V (yjk , \u3008wk,\u03a6(xjk)\u3009)] .\nNoting that V (y, a) \u2265 0 and V (yjk , 0) \u2264 |V |0, we thus get\n\u2016wk+1\u20162 \u2264 \u2016wk\u20162 + (a0\u03ba)2\u03b72k + 2\u03b7k|V |0.\nApplying this inequality iteratively for k = 1, \u00b7 \u00b7 \u00b7 , t, and introducing with w1 = 0, one can get that\n\u2016wt+1\u20162 \u2264 (a0\u03ba)2 t\u2211\nk=1\n\u03b72k + 2|V |0 t\u2211\nk=1\n\u03b7k,\nwhich leads to the desired result by taking square root on both sides.\nAccording to the above two lemmas, we can bound the sample errors as follows.\nLemma 5. Assume (4) and (5). Then, for any k \u2208 N,\n|Ez,J [Ez(wk)\u2212 E(wk)]| \u2264 2a0\u03baRk\u221a\nm ,\nwhere\nRk = \u221a\u221a\u221a\u221a(a0\u03ba)2 t\u2211 k=1 \u03b72k + 2|V |0 t\u2211 k=1 \u03b7k. (14)\nWhen the loss function is smooth, by Theorems 2.2 and 3.9 from [11], we can control the sample\nerrors as follows.\nLemma 6. Under Assumptions 1 and 3, let \u03b7t \u2264 2/(\u03ba2L) for all k \u2208 [T ],\n|Ez,J [Ez(wk)\u2212 E(wk)]| \u2264 2(a0\u03ba)\n2 \u2211k i=1 \u03b7i m .\nProof. Note that by (4), Assumption 3 and (2), for all (x, y) \u2208 z, w, w\u2032 \u2208 F ,\n\u2016V \u2032(y, \u3008w,\u03a6(x)\u3009)\u03a6(x)\u2212 V \u2032(y, \u3008w\u2032,\u03a6(x)\u3009)\u03a6(x)\u2016 \u2264 \u03ba|V \u2032(y, \u3008w,\u03a6(x)\u3009)\u2212 V \u2032(y, \u3008w\u2032,\u03a6(x)\u3009)|\n\u2264\u03baL|\u3008w,\u03a6(x)\u3009 \u2212 \u3008w\u2032,\u03a6(x)\u3009| = \u03baL|\u3008w \u2212 w\u2032,\u03a6(x)\u3009| \u2264 \u03baL\u2016w \u2212 w\u2032\u2016\u2016\u03a6(x)\u2016 \u2264\u03ba2L\u2016w \u2212 w\u2032\u2016,\nand\n\u2016V \u2032(y, \u3008w,\u03a6(x)\u3009)\u03a6(x)\u2016 \u2264 \u03baa0.\nThat is, for every (x, y) \u2208 z, V (y, \u3008\u00b7,\u03a6(x)\u3009) is (\u03ba2L)-smooth and (\u03baa0)-Lipschitz. Now the results follow directly by using Theorems 2.2 and 3.8 from [11]."}, {"heading": "C Excess Errors for Weighted Averages", "text": "Lemma 7. Under Assumption 1, assume that there exists a non-decreasing sequence {bk > 0}k such that\n|Ez,J [Ez(wk)\u2212 E(wk)]| \u2264 bk, \u2200k \u2208 [T ]. (15)\nThen for any t \u2208 [T ] and any fixed w \u2208 F ,\nt\u2211 k=1 2\u03b7kEz,J [E(wk)] \u2264 bt t\u2211 k=1 2\u03b7k + (a0\u03ba) 2 t\u2211 k=1 \u03b72k + t\u2211 k=1 2\u03b7kE(w) + \u2016w\u20162. (16)\nProof. By Lemma 2, we have (13). Rewriting \u2212Ez(wk) as\n\u2212Ez(wk) + E(wk)\u2212 E(wk),\ntaking the expectation with respect to J(T ) and z on both sides, noting that w is independent of J and z, and applying Condition (15), we derive\nEz,J [\u2016wk+1 \u2212 w\u20162] \u2264 Ez,J [\u2016wk \u2212 w\u20162] + (a0\u03ba)2\u03b72k + 2\u03b7k (E(w)\u2212 Ez,J [E(wk)]) + 2\u03b7kbk,\nwhich is equivalent to\n2\u03b7kEz,J [E(wk)] \u2264 2\u03b7kE(w) + Ez,J [\u2016wk \u2212 w\u20162 \u2212 \u2016wk+1 \u2212 w\u20162] + (a0\u03ba)2\u03b72k + 2\u03b7kbk.\nSumming up over k = 1, \u00b7 \u00b7 \u00b7 , t, and introducing with w1 = 0,\nt\u2211 k=1 2\u03b7kEz,J [E(wk)] \u2264 t\u2211 k=1 2\u03b7kE(w) + \u2016w\u20162 + (a0\u03ba)2 t\u2211 k=1 \u03b72k + t\u2211 k=1 2\u03b7kbk.\nThe proof can be finished by noting that bk is non-decreasing.\nNow, we are in a position to prove Theorem 1.\nProof of Theorem 1. According to Lemma 5, Condition (15) is satisfied for\nbt = 2a0\u03ba\n\u221a\u2211t k=1(a0\u03ba\u03b7k) 2 + 2|V |0 \u2211t k=1 \u03b7k\u221a\nm .\nBy Lemma 7, we thus have (16). Dividing both sides by \u2211t k=1 2\u03b7k, and using the convexity of V (y, \u00b7) which implies \u2211t k=1 \u03b7kE(wk)\u2211t\nk=1 \u03b7k \u2265 E( \u2211t k=1 \u03b7kwk\u2211t k=1 \u03b7k ) = E(wt), (17)\nwe get that\nEz,J [E(wt)] \u2264 bt + (a0\u03ba)\n2\n2\n\u2211t k=1 \u03b7\n2 k\u2211t\nk=1 \u03b7k + E(w) + \u2016w\u2016 2 2 \u2211t k=1 \u03b7k .\nFor any fixed > 0, we know that there exists a w \u2208 F , such that E(w ) \u2264 infw\u2208F E(w) + . Letting t = t\u2217(m), and w = w , we have\nEz,J [ E(wt\u2217(m),w) ] \u2264 bt\u2217(m) + (a0\u03ba) 2\n2\n\u2211t\u2217(m) k=1 \u03b7\n2 k\u2211t\u2217(m) k=1 \u03b7k + inf w\u2208F E(w) + + \u2016w \u2016 2 2 \u2211t\u2217(m) k=1 \u03b7k .\nLetting m\u2192\u221e , and using Conditions (A) and (B) which imply\nlim m\u2192\u221e 1\u2211t\u2217(m) k=1 \u03b7k = 0, lim m\u2192\u221e\n\u2211t\u2217(m) k=1 \u03b7\n2 k\u2211t\u2217(m)\nk=1 \u03b7k = 0, and lim m\u2192\u221e\n\u2211t\u2217(m) k=1 \u03b7 2 k\nm = lim m\u2192\u221e\n\u2211t\u2217(m) k=1 \u03b7\n2 k\u2211t\u2217(m)\nk=1 \u03b7k\n\u2211t\u2217(m) k=1 \u03b7k m = 0,\nwe reach\nlim m\u2192\u221e\nEz,J [ E(wt\u2217(m),w) ] \u2264 inf w\u2208F E(w) + .\nSince > 0 is arbitrary, the desired result thus follows. The proof is complete.\nLemma 8. Under the assumptions of Lemma 7, let Assumption 2 hold. Then for any t \u2208 [T ],\nt\u2211 k=1 2\u03b7kEz,J [ E(wk)\u2212 inf w\u2208F E(w) ] \u2264 bt t\u2211 k=1 2\u03b7k + (a0\u03ba) 2 t\u2211 k=1 \u03b72k + 2c\u03b2 ( t\u2211 k=1 \u03b7k )1\u2212\u03b2 . (18)\nProof. By Lemma 7, we have (16). Subtracting \u2211t k=1 2\u03b7k infw\u2208F E(w) from both sides,\nt\u2211 k=1 2\u03b7kEz,J [ E(wk)\u2212 inf w\u2208F E(w) ] \u2264 bt t\u2211 k=1 2\u03b7k + (a0\u03ba) 2 t\u2211 k=1 \u03b72k + t\u2211 k=1 2\u03b7k [ E(w)\u2212 inf w\u2208F E(w) ] + \u2016w\u20162.\nTaking the infimum over w \u2208 F , recalling that D(\u03bb) is defined by (6), we have t\u2211\nk=1\n2\u03b7kEz,J [ E(wk)\u2212 inf\nw\u2208F E(w)\n] \u2264 bt t\u2211 k=1 2\u03b7k + (a0\u03ba) 2 t\u2211 k=1 \u03b72k + t\u2211 k=1 2\u03b7kD ( 1\u2211t k=1 \u03b7k ) .\nUsing Assumption 2 to the above, we get the desired result. The proof is complete.\nCollecting some of the above analysis, we get the following result.\nProposition 1. Under the assumptions of Lemma 8, we have\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 bt +\n(a0\u03ba) 2\n2\n\u2211t k=1 \u03b7\n2 k\u2211t\nk=1 \u03b7k + c\u03b2\n( 1\u2211t\nk=1 \u03b7k\n)\u03b2 . (19)\nProof. By Lemma 8, we have (18). Dividing both sides by \u2211t k=1 2\u03b7k, and using (17), we get the desired bound."}, {"heading": "D From Weighted Averages to the Last Iterate", "text": "A basic tool for studying the convergence for iterates is the following decomposition, as often done in [18] for classical online learning or subgradient descent algorithms [12]. It enables us to study the weighted excess generalization error 2\u03b7tEz,J [E(wt) \u2212 infw\u2208F E(w)] in terms of \u201cweighted averages\u201d and moving weighted averages. In what follows, we will write Ez,J as E for short.\nLemma 9. We have 2\u03b7tE { E(wt)\u2212 inf\nw\u2208F E(w) } \u2264 1 t t\u2211 k=1 2\u03b7kE { E(wk)\u2212 inf w\u2208F E(w) } + t\u22121\u2211 k=1\n1\nk(k + 1) t\u2211 i=t\u2212k+1 2\u03b7iE {E(wi)\u2212 E(wt\u2212k)} . (20)\nProof. Let {ut}t be a real-valued sequence. For k = 1, \u00b7 \u00b7 \u00b7 , t\u2212 1,\n1 k t\u2211 i=t\u2212k+1 ui \u2212 1 k + 1 t\u2211 i=t\u2212k ui = 1 k(k + 1)\n{ (k + 1)\nt\u2211 i=t\u2212k+1 ui \u2212 k t\u2211 i=t\u2212k ui\n} =\n1\nk(k + 1) t\u2211 i=t\u2212k+1 (ui \u2212 ut\u2212k).\nSumming over k = 1, \u00b7 \u00b7 \u00b7 , t\u2212 1, and rearranging terms, we get\nut = 1\nt t\u2211 i=1 ui + t\u22121\u2211 k=1\n1\nk(k + 1) t\u2211 i=t\u2212k+1 (ui \u2212 ut\u2212k).\nChoosing ut = 2\u03b7tE {E(wt)\u2212 infw\u2208F E(w)} in the above, we get\n2\u03b7tE { E(wt)\u2212 inf\nw\u2208F E(w)\n} = 1\nt t\u2211 i=1 2\u03b7iE { E(wi)\u2212 inf w\u2208F E(w) }\n+ t\u22121\u2211 k=1\n1\nk(k + 1) t\u2211 i=t\u2212k+1 ( 2\u03b7iE { E(wi)\u2212 inf w\u2208F E(w) } \u2212 2\u03b7t\u2212kE { E(wt\u2212k)\u2212 inf w\u2208F E(w) }) ,\nwhich can be rewritten as 2\u03b7tE { E(wt)\u2212 inf\nw\u2208F E(w) } = 1\nt t\u2211 k=1 2\u03b7kE { E(wk)\u2212 inf w\u2208F E(w) } + t\u22121\u2211 k=1\n1\nk(k + 1) t\u2211 i=t\u2212k+1 2\u03b7iE {E(wi)\u2212 E(wt\u2212k)}\n+ t\u22121\u2211 k=1 1 k + 1\n[ 1\nk t\u2211 i=t\u2212k+1 2\u03b7i \u2212 2\u03b7t\u2212k\n] E { E(wt\u2212k)\u2212 inf\nw\u2208F E(w)\n} .\nSince, E(wt\u2212k) \u2212 infw\u2208F E(w) \u2265 0 and that {\u03b7t}t\u2208N is a non-increasing sequence, we know that the last term of the above inequality is at most zero. Therefore, we get the desired result. The proof is complete.\nThe first term of the right-hand side of (20) is the weighted excess generalization error, and it can be estimated easily by (18), while the second term (sum of moving averages) can be estimated by the following lemma.\nLemma 10. Under the assumptions of Lemma 7, we have\nt\u22121\u2211 k=1\n1\nk(k + 1) t\u2211 i=t\u2212k+1 2\u03b7iE {E(wi)\u2212 E(wt\u2212k)}\n\u2264 t\u22121\u2211 i=1 (a0\u03ba\u03b7i) 2 + 4bt\u03b7i t\u2212 i \u2212 1 t t\u2211 k=1 (a0\u03ba\u03b7k) 2 + 4bt\u03b7k) + (a0\u03ba\u03b7t) 2 + 4bt\u03b7t.\n(21)\nProof. Given any sample z, note that wt\u2212k is depending only on j1, j2, \u00b7 \u00b7 \u00b7 , jt\u2212k\u22121, and thus is independent from ji+1 for any t \u2265 i \u2265 t\u2212 k. Following from Lemma 2, for any i \u2265 t\u2212 k,\nEji+1 [\u2016wi+1 \u2212 wt\u2212k\u20162] \u2264 \u2016wi \u2212 wt\u2212k\u20162 + (a0\u03ba)2\u03b72i + 2\u03b7i (Ez(wt\u2212k)\u2212 Ez(wi)) .\nTaking the expectation on both sides, and bounding E[Ez(wt\u2212k)\u2212 Ez(wi)] as\n= E[Ez(wt\u2212k)\u2212 E(wt\u2212k) + E(wi)\u2212 Ez(wi) + E(wt\u2212k)\u2212 E(wi)] \u2264 2bt + E[E(wt\u2212k)\u2212 E(wi)]\nby Condition (15), and rearranging terms, we get\n2\u03b7iE [E(wi)\u2212 E(wt\u2212k)] \u2264 E[\u2016wi \u2212 wt\u2212k\u20162 \u2212 \u2016wi+1 \u2212 wt\u2212k\u20162] + (a0\u03ba)2\u03b72i + 4\u03b7ibt.\nSumming up over i = t\u2212 k, \u00b7 \u00b7 \u00b7 , t, we get\nt\u2211 i=t\u2212k 2\u03b7iE [E(wi)\u2212 E(wt\u2212k)] \u2264 (a0\u03ba)2 t\u2211 i=t\u2212k \u03b72i + 4bt t\u2211 i=t\u2212k \u03b7i.\nThe left-hand side is exactly \u2211t i=t\u2212k+1 2\u03b7iE [E(wi)\u2212 E(wt\u2212k)] . Thus, dividing both sides by k(k + 1), and then summing up over k = 1, \u00b7 \u00b7 \u00b7 , t\u2212 1,\nt\u22121\u2211 k=1\n1\nk(k + 1) t\u2211 i=t\u2212k+1 2\u03b7iE {E(wi)\u2212 E(wt\u2212k)} \u2264 t\u22121\u2211 k=1\n1\nk(k + 1) t\u2211 i=t\u2212k ((a0\u03ba\u03b7i) 2 + 4bt\u03b7i).\nExchanging the order in the sum, and setting \u03bei = (a0\u03ba\u03b7i) 2 + 4bt\u03b7i for all i \u2208 [t], we obtain\nt\u22121\u2211 k=1\n1\nk(k + 1) t\u2211 i=t\u2212k+1 2\u03b7iE {E(wi)\u2212 E(wt\u2212k)} \u2264 t\u22121\u2211 i=1 t\u22121\u2211 k=t\u2212i\n1\nk(k + 1) \u03bei + t\u22121\u2211 k=1\n1\nk(k + 1) \u03bet\n= t\u22121\u2211 i=1 ( 1 t\u2212 i \u2212 1 t ) \u03bei + ( 1\u2212 1 t ) \u03bet\n= t\u22121\u2211 i=1 1 t\u2212 i \u03bei + \u03bet \u2212 1 t t\u2211 k=1 \u03bek.\nFrom the above analysis, we can conclude the proof.\nProposition 2. Under the assumptions of Lemma 8, we have\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 bt\n( 1 +\nt\u22121\u2211 k=1 2\u03b7k \u03b7t(t\u2212 k)\n) +\nt\u22121\u2211 k=1 (a0\u03ba\u03b7k) 2 2\u03b7t(t\u2212 k) + (a0\u03ba) 2\u03b7t 2 + c\u03b2 \u03b7tt\n( t\u2211\nk=1\n\u03b7k\n)1\u2212\u03b2 (22)\nProof. Plugging (18) and (21) into (20), by a direct calculation, we get\n2\u03b7tEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 2c\u03b2 t\n( t\u2211\nk=1\n\u03b7k\n)1\u2212\u03b2 +\nt\u22121\u2211 k=1 (a0\u03ba\u03b7k) 2 + 4bt\u03b7k t\u2212 k \u2212 2bt t t\u2211 k=1 \u03b7k + (a0\u03ba\u03b7t) 2 + 4bt\u03b7t.\nSince {\u03b7t}t is non-increasing, 2btt \u2211t k=1 \u03b7k \u2265 2bt\u03b7t. Thus,\n2\u03b7tEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 2c\u03b2 t\n( t\u2211\nk=1\n\u03b7k )1\u2212\u03b2 + 2\u03b7tbt +\nt\u22121\u2211 k=1 (a0\u03ba\u03b7k) 2 + 4bt\u03b7k t\u2212 k + (a0\u03ba\u03b7t) 2.\nDividing both sides with 2\u03b7t, and rearranging terms, one can conclude the proof.\nNow, we are ready to prove Theorems 2 and 3.\nProof of Theorem 2. By Lemma 6, the condition (15) is satisfied with bk = 2(a0\u03ba) 2 \u2211k i=1 \u03b7i/m. It thus follows from Propositions 1 and 2 that\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 2(a0\u03ba)2 \u2211t k=1 \u03b7k m + (a0\u03ba) 2 2 \u2211t k=1 \u03b7 2 k\u2211t\nk=1 \u03b7k + c\u03b2\n( 1\u2211t\nk=1 \u03b7k\n)\u03b2 , (23)\nand\nEz,J [E(wt)\u2212 inf w\u2208F E(w)]\n\u22642(a0\u03ba)2 \u2211t k=1 \u03b7k m ( 1 + t\u22121\u2211 k=1 2\u03b7k \u03b7t(t\u2212 k) ) + (a0\u03ba) 2 2 t\u22121\u2211 k=1 \u03b72k \u03b7t(t\u2212 k) + (a0\u03ba) 2 2 \u03b7t + c\u03b2 (\u2211t k=1 \u03b7k )1\u2212\u03b2 \u03b7tt . (24)\nBy noting that 1 \u2264 \u03b7t\u22121/\u03b7t \u2264 \u2211t\u22121 k=1 \u03b7k/(\u03b7t(t\u2212 k)),\nEz,J [E(wt)\u2212 inf w\u2208F E(w)]\n\u22646(a0\u03ba)2 \u2211t k=1 \u03b7t m t\u22121\u2211 k=1 \u03b7k \u03b7t(t\u2212 k) + (a0\u03ba) 2 2 t\u22121\u2211 k=1 \u03b72k \u03b7t(t\u2212 k) + (a0\u03ba) 2 2 \u03b7t + c\u03b2\n(\u2211t k=1 \u03b7k )1\u2212\u03b2 \u03b7tt .\n(25)\nThe proof is complete.\nProof of Theorems 3. By Propositions 1 and 2, we have (19) and (22). Also, by Lemma 5, we have bt \u2264 2a0\u03baRt\u221am . Then\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 2a0\u03ba Rt\u221a m + (a0\u03ba)\n2\n2\n\u2211t k=1 \u03b7\n2 k\u2211t\nk=1 \u03b7k + c\u03b2\n( 1\u2211t\nk=1 \u03b7k\n)\u03b2 , (26)\nand\nEz,J [E(wt)\u2212 inf w\u2208F E(w)]\n\u22642a0\u03ba Rt\u221a m\n( 1 +\nt\u22121\u2211 k=1 2\u03b7k \u03b7t(t\u2212 k)\n) + (a0\u03ba) 2\n2 t\u22121\u2211 k=1 \u03b72k \u03b7t(t\u2212 k) + (a0\u03ba) 2 2 \u03b7t + c\u03b2\n(\u2211t k=1 \u03b7k )1\u2212\u03b2 \u03b7tt .\n(27)\nNote that 1 \u2264 \u03b7t\u22121/\u03b7t since \u03b7t is non-increasing. Thus,\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 6a0\u03ba Rt\u221a m t\u22121\u2211 k=1 \u03b7k \u03b7t(t\u2212 k) + (a0\u03ba) 2 2 t\u22121\u2211 k=1 \u03b72k \u03b7t(t\u2212 k) + (a0\u03ba) 2 2 \u03b7t + c\u03b2\n(\u2211t k=1 \u03b7k )1\u2212\u03b2 \u03b7tt .\n(28)\nRecall that Rt is given by (14) and that \u03b7k is non-increasing, we thus have\nRt \u2264 \u221a (a0\u03ba)2\u03b71 + 2|V |0 \u221a\u221a\u221a\u221a t\u2211 k=1 \u03b7k. (29)\nIntroducing the above into (26) and (28),\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 2a0\u03ba\n\u221a (a0\u03ba)2\u03b71 + 2|V |0 \u221a\u2211t k=1 \u03b7k m + (a0\u03ba) 2 2 \u2211t k=1 \u03b7 2 k\u2211t\nk=1 \u03b7k + c\u03b2\n( 1\u2211t\nk=1 \u03b7k\n)\u03b2 ,\n(30)\nand\nEz,J [E(wt)\u2212 inf w\u2208F E(w)]\n\u22646a0\u03ba \u221a (a0\u03ba)2\u03b71 + 2|V |0 \u221a\u2211t k=1 \u03b7k m + (a0\u03ba) 2 2 t\u22121\u2211 k=1 \u03b72k \u03b7t(t\u2212 k) + (a0\u03ba) 2 2 \u03b7t + c\u03b2 (\u2211t k=1 \u03b7k )1\u2212\u03b2 \u03b7tt ."}, {"heading": "E Explicit Convergence Rates", "text": "In this section, we prove Corollaries 1-8. We first introduce the following basic estimates.\nLemma 11. Let \u03b8 \u2208 R+, and t \u2208 N, t \u2265 3. Then\nt\u2211 k=1 k\u2212\u03b8 \u2264  t 1\u2212\u03b8/(1\u2212 \u03b8), when \u03b8 < 1, log t+ 1, when \u03b8 = 1, \u03b8/(\u03b8 \u2212 1), when \u03b8 > 1,\nand\nt\u2211 k=1 k\u2212\u03b8 \u2265\n{ 1\u22124\u03b8\u22121 1\u2212\u03b8 t 1\u2212\u03b8 when \u03b8 < 1,\nln t when \u03b8 = 1.\nProof. By using\nt\u2211 k=1 k\u2212\u03b8 = 1 + t\u2211 k=2 \u222b k k\u22121 duk\u2212\u03b8 \u2264 1 + t\u2211 k=2 \u222b k k\u22121 u\u2212\u03b8du = 1 + \u222b t 1 u\u2212\u03b8du,\nwhich leads to the first part of the result. Similarly,\nt\u2211 k=1 k\u2212\u03b8 = t\u2211 k=1 k\u2212\u03b8 \u2265 t\u2211 k=1 \u222b k+1 k u\u2212\u03b8du = \u222b t+1 1 u\u2212\u03b8du,\nwhich leads to the second part of the result. The proof is complete.\nLemma 12. Let q \u2208 R+ and t \u2208 N, t \u2265 3. Then\nt\u22121\u2211 k=1 1 t\u2212 k k\u2212q \u2264  2 q[2 + (1\u2212 q)\u22121]t\u2212q log t, when q < 1, 8t\u22121 log t, when q = 1, (2q + 2q)/(q \u2212 1)t\u22121, when q > 1,\nProof. We split the sum into two parts\nt\u22121\u2211 k=1 1 t\u2212 k k\u2212q = \u2211 t/2\u2264k\u2264t\u22121 1 t\u2212 k k\u2212q + \u2211 1\u2264k<t/2 1 t\u2212 k k\u2212q\n\u2264 2qt\u2212q \u2211\nt/2\u2264k\u2264t\u22121\n1\nt\u2212 k + 2t\u22121 \u2211 1\u2264k<t/2 k\u2212q\n= 2qt\u2212q \u2211\n1\u2264k\u2264t/2\nk\u22121 + 2t\u22121 \u2211\n1\u2264k<t/2\nk\u2212q.\nApplying Lemma 11, we get\nt\u22121\u2211 k=1 1 t\u2212 k k\u2212q \u2264 2qt\u2212q(log(t/2) + 1) +  2 qt\u2212q/(1\u2212 q), when q < 1, 4t\u22121 log t, when q = 1, 2qt\u22121/(q \u2212 1), when q > 1,\nwhich leads to the desired result by using t\u2212q+1 log t \u2264 1/(e(q \u2212 1)) \u2264 1/(2(q \u2212 1)) when q > 1.\nThe bounds in the above two lemmas involve constant factor 1/(1\u2212 \u03b8) or 1/(1\u2212 q), which tend to be infinity as \u03b8 \u2192 1 or q \u2192 1. To avoid these, we introduce the following complement results.\nLemma 13. Let \u03b8 \u2208 R+, and t \u2208 N, with t \u2265 3. Then\nt\u2211 k=1 k\u2212\u03b8 \u2264 tmax(1\u2212\u03b8,0)2 log t.\nProof. Note that\nt\u2211 k=1 k\u2212\u03b8 = t\u2211 k=1 k\u22121k1\u2212\u03b8 \u2264 tmax(1\u2212\u03b8,0) t\u2211 k=1 k\u22121.\nThe proof can be finished by applying Lemma 11.\nLemma 14. Let q \u2208 R+ and t \u2208 N, t \u2265 3. Then\nt\u22121\u2211 k=1 1 t\u2212 k k\u2212q \u2264 4t\u2212min(q,1) log t.\nProof. Note that\nt\u22121\u2211 k=1 1 t\u2212 k k\u2212q = t\u22121\u2211 k=1 k1\u2212q (t\u2212 k)k \u2264 tmax(1\u2212q,0) t\u22121\u2211 k=1\n1\n(t\u2212 k)k ,\nand that by Lemma 11,\nt\u22121\u2211 k=1\n1\n(t\u2212 k)k =\n1 t t\u22121\u2211 k=1 ( 1 t\u2212 k + 1 k ) = 2 t t\u22121\u2211 k=1 1 k \u2264 4 t log t.\nWith the above estimates and Theorems 2, 3, we can get the following two propositions.\nProposition 3. Under Assumptions 1, 2 and 3, let \u03b7t = \u03b7t \u2212\u03b8 for some positive constant \u03b7 \u2264 1\u03ba2L with \u03b8 \u2208 [0, 1) for all t \u2208 N. Then for all t \u2208 N,\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 2(a0\u03ba)\n2 1\u2212 \u03b8 \u03b7t1\u2212\u03b8 m + (a0\u03ba) 2(1\u2212 \u03b8) 1\u2212 4\u03b8\u22121 \u03b7t\u2212min(\u03b8,1\u2212\u03b8) log t+ c\u03b2\n( 1\u2212 \u03b8\n1\u2212 4\u03b8\u22121\n)\u03b2 ( 1\n\u03b7t1\u2212\u03b8\n)\u03b2 ,\nand\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 18(a0\u03ba)\n2 1\u2212 \u03b8 \u03b7t1\u2212\u03b8 log t m + 3(a0\u03ba) 2\u03b7t\u2212min(\u03b8,1\u2212\u03b8) log t+ c\u03b2 1\u2212 \u03b8\n( 1\n\u03b7t1\u2212\u03b8\n)\u03b2 .\nProof. Following the proof of Theorem 2, we have (23) and (24). We first consider the case wt. With \u03b7t = \u03b7t \u2212\u03b8, (23) reads as\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 2(a0\u03ba)2\n\u03b7 \u2211t k=1 k \u2212\u03b8\nm +\n(a0\u03ba) 2\n2\n\u03b7 \u2211t k=1 k\n\u22122\u03b8\u2211t k=1 k \u2212\u03b8 + c\u03b2\n( 1\n\u03b7 \u2211t k=1 k \u2212\u03b8\n)\u03b2 .\nLemma 11 tells us that\n1\u2212 4\u03b8\u22121\n1\u2212 \u03b8 t1\u2212\u03b8 \u2264 t\u2211 k=1 k\u2212\u03b8 \u2264 t 1\u2212\u03b8 1\u2212 \u03b8 .\nThus,\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 2(a0\u03ba)\n2 1\u2212 \u03b8 \u03b7t1\u2212\u03b8 m + (a0\u03ba) 2(1\u2212 \u03b8) 2(1\u2212 4\u03b8\u22121) \u03b7 \u2211t k=1 k \u22122\u03b8 t1\u2212\u03b8 + c\u03b2 ( 1\u2212 \u03b8 1\u2212 4\u03b8\u22121 )\u03b2 ( 1 \u03b7t1\u2212\u03b8 )\u03b2 .\nUsing Lemma 13 to the above, we can get the first part of the desired results. Now consider the case wt. With \u03b7t = \u03b7t \u2212\u03b8, (24) is exactly\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 2(a0\u03ba)2\n\u03b7 \u2211t k=1 k \u2212\u03b8\nm\n( 1 + 2t\u03b8\nt\u22121\u2211 k=1 k\u2212\u03b8 t\u2212 k\n)\n+ (a0\u03ba)\n2\n2 \u03b7t\u2212\u03b8\n( t2\u03b8\nt\u22121\u2211 k=1 k\u22122\u03b8 t\u2212 k + 1\n) + c\u03b2 (\u2211t k=1 k \u2212\u03b8 )1\u2212\u03b2\n\u03b7\u03b2t1\u2212\u03b8 .\nApplying Lemma 14 to bound \u2211t\u22121 k=1 k\u2212\u03b8 t\u2212k and \u2211t\u22121 k=1 k\u22122\u03b8 t\u2212k , by a simple calculation, we derive\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 2(a0\u03ba)2\n\u03b7 \u2211t k=1 k \u2212\u03b8\nm \u00b7 (9 log t) + 3(a0\u03ba)2\u03b7t\u2212min(\u03b8,1\u2212\u03b8) log t+ c\u03b2\n(\u2211t k=1 k \u2212\u03b8 )1\u2212\u03b2\n\u03b7\u03b2t1\u2212\u03b8 .\nUsing Lemma 11 to upper bound \u2211t k=1 k \u2212\u03b8, one can get the second part of the desired results.\nProposition 4. Under Assumptions 1 and 2, let \u03b7t = \u03b7t \u2212\u03b8 for all t \u2208 N, with 0 < \u03b7 \u2264 1 and \u03b8 \u2208 [0, 1). Then for all t \u2208 N,\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 2a0\u03ba\n\u221a (a0\u03ba)2 + 2|V |0\n1\u2212 \u03b8\n\u221a \u03b7t1\u2212\u03b8\nm +\n(a0\u03ba) 2(1\u2212 \u03b8)\n1\u2212 4\u03b8\u22121 \u03b7t\u2212min(\u03b8,1\u2212\u03b8) log t\n+c\u03b2\n( 1\u2212 \u03b8\n1\u2212 4\u03b8\u22121\n)\u03b2 ( 1\n\u03b7t1\u2212\u03b8\n)\u03b2 ,\nand\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 18a0\u03ba \u221a\n(a0\u03ba)2 + 2|V |0 1\u2212 \u03b8\n\u221a \u03b7t1\u2212\u03b8\nm log t+ 3(a0\u03ba)\n2\u03b7t\u2212min(\u03b8,1\u2212\u03b8) log t+ c\u03b2\n1\u2212 \u03b8\n( 1\n\u03b7t1\u2212\u03b8\n)\u03b2 .\nProof. Following the proof of Theorem 3, we have (26) and (27), where Rt satisfies (29). Comparing (26), (27) with (23), (24), we find that the differences are the terms related sample errors, i.e., the term\n2(a0\u03ba 2) \u2211t k=1 \u03b7k/m in (23), (24), while 2a0\u03baRt/ \u221a m in (26), (27). Thus, following from the proof of Proposition 3, we get\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 2a0\u03baRt\u221a m +\n(a0\u03ba) 2(1\u2212 \u03b8)\n1\u2212 4\u03b8\u22121 \u03b7t\u2212min(\u03b8,1\u2212\u03b8) log t+ c\u03b2\n( 1\u2212 \u03b8\n1\u2212 4\u03b8\u22121\n)\u03b2 ( 1\n\u03b7t1\u2212\u03b8\n)\u03b2 ,\nand\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 2a0\u03baRt\u221a m \u00b7 9 log t+ 3(a0\u03ba)2\u03b7t\u2212min(\u03b8,1\u2212\u03b8) log t+ c\u03b2 1\u2212 \u03b8\n( 1\n\u03b7t1\u2212\u03b8\n)\u03b2 .\nRecall that Rt satisfies (29), with \u03b7t = \u03b7t \u2212\u03b8, where \u03b7 \u2264 1, by Lemma 11, we know that Rt \u2264 \u221a\n(a0\u03ba)2 + 2|V |0 1\u2212 \u03b8\n\u221a \u03b7t1\u2212\u03b8.\nFrom the above analysis, one can conclude the proof.\nWe are ready to prove Corollaries 1-8.\nProof of Corollary 1. Applying Proposition 3 with \u03b8 = 0, \u03b7 = \u03b71/ \u221a m, we derive\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 2\u03b71(a0\u03ba)2 t\u221a m3 + 2(a0\u03ba) 2\u03b71 log t\u221a m + 2c\u03b2\n\u03b7\u03b21\n(\u221a m\nt\n)\u03b2 ,\nand\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 18\u03b71(a0\u03ba)2 t log t\u221a m3 + 3\u03b71(a0\u03ba) 2 log t\u221a m + c\u03b2\n\u03b7\u03b21\n(\u221a m\nt\n)\u03b2 .\nThe proof is complete.\nProof of Corollary 2. Applying Proposition 3 with \u03b7 = \u03b71, \u03b8 = 1/2, we get\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 4(a0\u03ba)2\u03b71\n\u221a t m + (a0\u03ba) 2\u03b71 log t\u221a t + c\u03b2\u03b7 \u2212\u03b2 1 1 t\u03b2/2 ,\nand\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 36(a0\u03ba)2\u03b71\n\u221a t log t\nm + 3(a0\u03ba) 2\u03b71 log t\u221a t + 2c\u03b2\u03b7 \u2212\u03b2 1 1 t\u03b2/2 .\nProof of Corollary 3. Applying Proposition 3 with \u03b8 = 0 and \u03b7 = \u03b71m \u2212 \u03b2\u03b2+1 , we get\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 2\u03b71(a0\u03ba)2m\u2212 \u03b2+2 \u03b2+1 t+ 2\u03b71(a0\u03ba) 2m\u2212 \u03b2 \u03b2+1 log t+\n2c\u03b2 \u03b7\u03b21 m \u03b22 \u03b2+1 t\u2212\u03b2 ,\nand\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 18\u03b71(a0\u03ba)2m\u2212 \u03b2+2 \u03b2+1 t log t+ 3\u03b71(a0\u03ba) 2m\u2212 \u03b2 \u03b2+1 log t+\nc\u03b2 \u03b7\u03b21 m \u03b22 \u03b2+1 t\u2212\u03b2 .\nThe proof is complete.\nProof of Corollary 4. Applying Proposition 3 with \u03b7 = \u03b71 and \u03b8 = \u03b2 \u03b2+1 ,\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 4(a0\u03ba)2\u03b71\nt 1 \u03b2+1\nm + 2\u03b71(a0\u03ba)\n2t\u2212 \u03b2 \u03b2+1 + 2c\u03b2\u03b7 \u2212\u03b2 1 t \u2212 \u03b2\u03b2+1 ,\nand\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 36(a0\u03ba)2\u03b71\nt 1 1+\u03b2 log t\nm + 3\u03b71(a0\u03ba)\n2t\u2212 \u03b2 \u03b2+1 log t+ 2c\u03b2\u03b7 \u2212\u03b2 1 t \u2212 \u03b2\u03b2+1 .\nFor the above two inequalities, we used that \u03b2 \u2208 (0, 1], \u03b8 = \u03b2\u03b2+1 \u2264 1/2 and 4 \u03b8\u22121 \u2264 1/2. Proof of Corollary 5. Applying Proposition 4 with \u03b7 = 1/ \u221a m and \u03b8 = 0,\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 2a0\u03ba\n\u221a (a0\u03ba)2 + 2|V |0 \u221a t\nm3/4 + 2(a0\u03ba) 2 log t\u221a m + 2c\u03b2\n(\u221a m\nt\n)\u03b2 ,\nand\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 18a0\u03ba\n\u221a (a0\u03ba)2 + 2|V |0 \u221a t log t\nm3/4 + 3(a0\u03ba) 2 log t\u221a m + c\u03b2\n(\u221a m\nt\n)\u03b2 .\nThe proof is complete.\nProof of Corollary 6. Applying Proposition 4 with \u03b7 = 1 and \u03b8 = 1/2, we get\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 2\n\u221a 2a0\u03ba \u221a (a0\u03ba)2 + 2|V |0\nt1/4\u221a m + (a0\u03ba) 2 log t\u221a t + c\u03b2 t\u03b2/2 .\nand\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 18\n\u221a 2a0\u03ba \u221a (a0\u03ba)2 + 2|V |0\nt1/4 log t\u221a m + 3(a0\u03ba) 2 log t\u221a t + 2c\u03b2 1 t\u03b2/2 .\nProof of Corollary 7. Using Proposition 4 with \u03b7 = m\u2212 2\u03b2 2\u03b2+1 and \u03b8 = 0, we get\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 2a0\u03ba\n\u221a (a0\u03ba)2 + 2|V |0m\u2212 4\u03b2+1 4\u03b2+2 \u221a t+ 2(a0\u03ba) 2m\u2212 2\u03b2 2\u03b2+1 log t+ 2c\u03b2m 2\u03b22 2\u03b2+1 t\u2212\u03b2 ,\nand\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 18a0\u03ba\n\u221a (a0\u03ba)2\u03b7 + 2|V |0m\u2212 4\u03b2+1 4\u03b2+2 \u221a t log t+ 3(a0\u03ba) 2m\u2212 2\u03b2 2\u03b2+1 log t+ c\u03b2m 2\u03b22 2\u03b2+1 t\u2212\u03b2 .\nThe proof is complete.\nProof of Corollary 8. Let \u03b8 = 2\u03b22\u03b2+1 . Obviously, \u03b8 \u2208 [0, 2 3 ] since \u03b2 \u2208 (0, 1]. Thus, 1 1\u2212\u03b8 = 2\u03b2 + 1 \u2264 3,\n1\u2212\u03b8 1\u22124\u03b8\u22121 \u2264 1 1\u22124\u22121/3 \u2264 2. Following from Proposition 4,\nEz,J [E(wt)]\u2212 inf w\u2208F E(w) \u2264 2\n\u221a 3a0\u03ba \u221a (a0\u03ba)2 + 2|V |0 t 1 4\u03b2+2\n\u221a m + 2(a0\u03ba) 2t\u2212 min(2\u03b2,1) 2\u03b2+1 log t+ 2c\u03b2t \u2212 \u03b22\u03b2+1 ,\nand\nEz,J [E(wt)\u2212 inf w\u2208F E(w)] \u2264 18\n\u221a 3a0\u03ba \u221a (a0\u03ba)2 + 2|V |0 t 1 4\u03b2+2\n\u221a m log t+ 3(a0\u03ba) 2t\u2212 min(2\u03b2,1) 2\u03b2+1 log t+ 3c\u03b2t \u2212 \u03b22\u03b2+1 ."}], "references": [{"title": "Local rademacher complexities", "author": ["Peter L Bartlett", "Olivier Bousquet", "Shahar Mendelson"], "venue": "Annals of Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartlett", "Shahar Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Nonlinear Programming", "author": ["Dimitri P Bertsekas"], "venue": "Athena scientific,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Incremental gradient, subgradient, and proximal methods for convex optimization: A survey", "author": ["Dimitri P Bertsekas"], "venue": "Optimization for Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "The tradeoffs of large scale learning", "author": ["Olivier Bousquet", "L\u00e9on Bottou"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Stochastic subgradient methods", "author": ["Stephen Boyd", "Almir Mutapcic"], "venue": "Notes for EE364b,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Subgradient methods", "author": ["Stephen Boyd", "Lin Xiao", "Almir Mutapcic"], "venue": "Lecture notes of EE392o,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Learning Theory: an Approximation", "author": ["Felipe Cucker", "Ding-Xuan Zhou"], "venue": "Theory Viewpoint,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Non-parametric stochastic approximation with large step sizes", "author": ["Aymeric Dieuleveut", "Francis Bach"], "venue": "arXiv preprint arXiv:1408.0361,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Iterative regularization for learning with convex loss functions", "author": ["Junhong Lin", "Lorenzo Rosasco", "Ding-Xuan Zhou"], "venue": "The Journal of Machine Learning Research, To appear,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Generalization error bounds for Bayesian mixture algorithms", "author": ["Ron Meir", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["Arkadi Nemirovski", "Anatoli Juditsky", "Guanghui Lan", "Alexander Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Simultaneous model selection and optimization through parameter-free stochastic learning", "author": ["Francesco Orabona"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Learning with incremental iterative regularization", "author": ["Lorenzo Rosasco", "Silvia Villa"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes", "author": ["Ohad Shamir", "Tong Zhang"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Support Vector Machines", "author": ["Ingo Steinwart", "Andreas Christmann"], "venue": "Springer Science Business Media,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Online learning as stochastic approximation of regularization paths: Optimality and almost-sure convergence", "author": ["Pierre Tarres", "Yuan Yao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "The stochastic gradient method (SGM), often called stochastic gradient descent, has become an algorithm of choice in machine learning, because of its simplicity and small computational cost especially when dealing with big data sets [5].", "startOffset": 233, "endOffset": 236}, {"referenceID": 13, "context": "[14] or [15] and references therein, while in practice multiple passes are usually considered.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[14] or [15] and references therein, while in practice multiple passes are usually considered.", "startOffset": 8, "endOffset": 12}, {"referenceID": 5, "context": "The effect of multiple passes has been studied extensively for the optimization of an empirical objective [6], but the role for generalization is less clear.", "startOffset": 106, "endOffset": 109}, {"referenceID": 19, "context": "One first series of results focus on least squares, either with one [21, 20, 10], or multiple (deterministic) passes over the data [16].", "startOffset": 68, "endOffset": 80}, {"referenceID": 9, "context": "One first series of results focus on least squares, either with one [21, 20, 10], or multiple (deterministic) passes over the data [16].", "startOffset": 68, "endOffset": 80}, {"referenceID": 15, "context": "One first series of results focus on least squares, either with one [21, 20, 10], or multiple (deterministic) passes over the data [16].", "startOffset": 131, "endOffset": 135}, {"referenceID": 15, "context": "In [16] it is shown that a universal step-size choice can be taken, if multiple passes are considered.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "Here, our starting points are the results in [12, 11, 15] considering convex loss functions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 10, "context": "Here, our starting points are the results in [12, 11, 15] considering convex loss functions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 14, "context": "Here, our starting points are the results in [12, 11, 15] considering convex loss functions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 11, "context": "In [12], early", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "stopping of a (kernelized) batch subgradient method is analyzed, whereas in [11] the stability properties of SGM for smooth loss functions are considered in a general stochastic optimization setting and certain convergence results are derived.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "In [15], a more complex variant of SGM is analyzed and shown to achieve optimal rates.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "(5) The above conditions are common in statistical learning theory [19, 9].", "startOffset": 67, "endOffset": 74}, {"referenceID": 8, "context": "(5) The above conditions are common in statistical learning theory [19, 9].", "startOffset": 67, "endOffset": 74}, {"referenceID": 2, "context": "Condition (B) is similar to the one required by stochastic gradient methods [3, 7, 6].", "startOffset": 76, "endOffset": 85}, {"referenceID": 6, "context": "Condition (B) is similar to the one required by stochastic gradient methods [3, 7, 6].", "startOffset": 76, "endOffset": 85}, {"referenceID": 5, "context": "Condition (B) is similar to the one required by stochastic gradient methods [3, 7, 6].", "startOffset": 76, "endOffset": 85}, {"referenceID": 18, "context": "More formally, the condition is related to classical terminologies in approximation theory, such as K-functionals and interpolation spaces [19, 9].", "startOffset": 139, "endOffset": 146}, {"referenceID": 8, "context": "More formally, the condition is related to classical terminologies in approximation theory, such as K-functionals and interpolation spaces [19, 9].", "startOffset": 139, "endOffset": 146}, {"referenceID": 8, "context": "Assumption 2 is standard in statistical learning theory when analyzing Tikhonov regularization [9, 19].", "startOffset": 95, "endOffset": 102}, {"referenceID": 18, "context": "Assumption 2 is standard in statistical learning theory when analyzing Tikhonov regularization [9, 19].", "startOffset": 95, "endOffset": 102}, {"referenceID": 8, "context": "Besides, it has been shown that Tikhonov regularization can achieve best performance by choosing an appropriate penalty parameter which depends on the unknown parameter \u03b2 [9, 19].", "startOffset": 171, "endOffset": 178}, {"referenceID": 18, "context": "Besides, it has been shown that Tikhonov regularization can achieve best performance by choosing an appropriate penalty parameter which depends on the unknown parameter \u03b2 [9, 19].", "startOffset": 171, "endOffset": 178}, {"referenceID": 11, "context": "The proof of the above result follows more or less directly from combining ideas and results in [12, 11] and is postponed to the appendix.", "startOffset": 96, "endOffset": 104}, {"referenceID": 10, "context": "The proof of the above result follows more or less directly from combining ideas and results in [12, 11] and is postponed to the appendix.", "startOffset": 96, "endOffset": 104}, {"referenceID": 17, "context": ", [18].", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "These results follow by an argument similar to that in Chapter 6 from [19] and are omitted.", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "The proof of the above theorem is based on ideas from [12], where tools from Rademacher complexity [2, 13] are employed.", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "The proof of the above theorem is based on ideas from [12], where tools from Rademacher complexity [2, 13] are employed.", "startOffset": 99, "endOffset": 106}, {"referenceID": 12, "context": "The proof of the above theorem is based on ideas from [12], where tools from Rademacher complexity [2, 13] are employed.", "startOffset": 99, "endOffset": 106}, {"referenceID": 13, "context": "Approaches in this sense include classical results in optimization (see [14] and references therein), but also approaches based on so-called \u201conline to batch\u201d conversion (see [15] and references therein).", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "Approaches in this sense include classical results in optimization (see [14] and references therein), but also approaches based on so-called \u201conline to batch\u201d conversion (see [15] and references therein).", "startOffset": 175, "endOffset": 179}, {"referenceID": 3, "context": "In this view, Algorithm 1 is seen as an instance of incremental methods for the minimization of objective functions that are sums of a finite, but possibly large, number of terms [4].", "startOffset": 179, "endOffset": 182}, {"referenceID": 4, "context": "Here, we follow the approach in [5] advocating the combination of statistical and computational errors.", "startOffset": 32, "endOffset": 35}, {"referenceID": 10, "context": "w\u0303 = w\u2217 z, this can be seen to be the choice made in [11].", "startOffset": 53, "endOffset": 57}, {"referenceID": 11, "context": "Following [12], we do this introducing Assumption (6) and choosing w\u0303 as the unique minimizer of E + \u03bb\u2016 \u00b7 \u2016, \u03bb > 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "For the generalization error, the stability results from [11] provide sharp estimates for smooth loss functions and in the \u2018capacity independent\u2019 limit, that is under no assumptions on the covering numbers of the considered function space.", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "For this setting, the obtained bound is optimal in the sense that it matches the best available bound for Tikhonov regularization [19, 9].", "startOffset": 130, "endOffset": 137}, {"referenceID": 8, "context": "For this setting, the obtained bound is optimal in the sense that it matches the best available bound for Tikhonov regularization [19, 9].", "startOffset": 130, "endOffset": 137}, {"referenceID": 5, "context": "The computational error for the averaged iterates can be controlled using classic arguments [6], whereas for the last iterate the arguments in [12, 18] are needed.", "startOffset": 92, "endOffset": 95}, {"referenceID": 11, "context": "The computational error for the averaged iterates can be controlled using classic arguments [6], whereas for the last iterate the arguments in [12, 18] are needed.", "startOffset": 143, "endOffset": 151}, {"referenceID": 17, "context": "The computational error for the averaged iterates can be controlled using classic arguments [6], whereas for the last iterate the arguments in [12, 18] are needed.", "startOffset": 143, "endOffset": 151}, {"referenceID": 16, "context": "However, it would be very interesting to consider more sophisticated, \u2018accelerated\u2019 iterations [17], and assess the potential advantages in terms of computational and generalization aspects.", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "[12], but others will require non-trivial extensions of results developed for Tikhonov regularization in the last few years.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "A remarkable result in this direction is derived in [15], where it is shown that, in the capacity independent setting, adaptive online parameter tuning is indeed possible.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "For the decaying step-size case, we fix \u03b71 = 1/4, and run SGM with different \u03b8 \u2208 [0, 1].", "startOffset": 81, "endOffset": 87}, {"referenceID": 7, "context": "2 Accuracy and Computational Time Comparison In this subsection, we compare SGM with cross-validation and SIGM with benchmark algorithm LIBSVM [8], both in terms of accuracy and computational time.", "startOffset": 143, "endOffset": 146}, {"referenceID": 0, "context": "[1] Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Peter L Bartlett and Shahar Mendelson.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Dimitri P Bertsekas.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Dimitri P Bertsekas.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Olivier Bousquet and L\u00e9on Bottou.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Stephen Boyd and Almir Mutapcic.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Stephen Boyd, Lin Xiao, and Almir Mutapcic.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Chih-Chung Chang and Chih-Jen Lin.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Felipe Cucker and Ding-Xuan Zhou.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Aymeric Dieuleveut and Francis Bach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Moritz Hardt, Benjamin Recht, and Yoram Singer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Junhong Lin, Lorenzo Rosasco, and Ding-Xuan Zhou.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Ron Meir and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Francesco Orabona.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Lorenzo Rosasco and Silvia Villa.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Mark Schmidt, Nicolas Le Roux, and Francis Bach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Ohad Shamir and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Ingo Steinwart and Andreas Christmann.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Pierre Tarres and Yuan Yao.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1, 13].", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "[1, 13].", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "Using a standard symmetrization technique, for example in [13], we get", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "[1], we derive", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "9 from [11], we can control the sample errors as follows.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "8 from [11].", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "A basic tool for studying the convergence for iterates is the following decomposition, as often done in [18] for classical online learning or subgradient descent algorithms [12].", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "A basic tool for studying the convergence for iterates is the following decomposition, as often done in [18] for classical online learning or subgradient descent algorithms [12].", "startOffset": 173, "endOffset": 177}, {"referenceID": 1, "context": "Obviously, \u03b8 \u2208 [0, 2 3 ] since \u03b2 \u2208 (0, 1].", "startOffset": 15, "endOffset": 24}, {"referenceID": 2, "context": "Obviously, \u03b8 \u2208 [0, 2 3 ] since \u03b2 \u2208 (0, 1].", "startOffset": 15, "endOffset": 24}], "year": 2016, "abstractText": "We study the generalization properties of stochastic gradient methods for learning with convex loss functions and linearly parameterized functions. We show that, in the absence of penalizations or constraints, the stability and approximation properties of the algorithm can be controlled by tuning either the step-size or the number of passes over the data. In this view, these parameters can be seen to control a form of implicit regularization. Numerical results complement the theoretical findings.", "creator": "LaTeX with hyperref package"}}}