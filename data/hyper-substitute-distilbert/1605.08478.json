{"id": "1605.08478", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Model-Free Imitation Learning with Policy Optimization", "abstract": "adaptive lesson learning, either agent learns multiple behaviors behave without an environment with an unknown means function repeatedly producing expert instruction. existing agile learning algorithms continually aid solving a sequence variable planning or expense estimation difficulties. these algorithms are therefore not directly applicable upon large, high - dimensional environments, in their performance can likely degrade or faulty planning problems are not solved to compensate. on the apprenticeship learning formalism, we advocate alternative model - related practices for both any parameterized retail policy that performs any price mildly well as an introductory policy on an unknown cost function, focusing on sample trajectories from the classroom. evolutionary approach, drawn on policy gradients, scales to large continuous processes with guaranteed bonuses to sufficient estimates.", "histories": [["v1", "Thu, 26 May 2016 23:43:32 GMT  (459kb,D)", "http://arxiv.org/abs/1605.08478v1", "In Proceedings of the 33rd International Conference on Machine Learning, 2016"]], "COMMENTS": "In Proceedings of the 33rd International Conference on Machine Learning, 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["jonathan ho", "jayesh k gupta", "stefano ermon"], "accepted": true, "id": "1605.08478"}, "pdf": {"name": "1605.08478.pdf", "metadata": {"source": "META", "title": "Model-Free Imitation Learning with Policy Optimization", "authors": ["Jonathan Ho", "Jayesh K. Gupta", "Stefano Ermon"], "emails": ["HOJ@CS.STANFORD.EDU", "JKG@CS.STANFORD.EDU", "ERMON@CS.STANFORD.EDU"], "sections": [{"heading": "1. Introduction", "text": "To use reinforcement learning, the learner needs access to a cost or reward signal to identify desirable outcomes. The dependence between the cost function and the corresponding optimal policy, however, is generally complex, as it involves planning. In practice, eliciting a cost function that achieves desired behavior can be difficult (Bagnell, 2015). An alternative and often more practical approach, called imitation learning, is to encode preferences and differentiate between desirable and undesirable outcomes using demonstrations provided by an expert (Pomerleau, 1991; Russell, 1998).\nThe simplest approach to imitation learning is behavioral cloning, in which the goal is to learn the relationship between states and optimal actions as a supervised learning\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nproblem (Pomerleau, 1991). While conceptually simple and theoretically sound (Syed & Schapire, 2010), small inaccuracies of the learned model compound over time, and can lead to situations that are quite different from the ones encountered during training. This is often referred to as the problem of cascading errors, and is related to covariate shift (Ross & Bagnell, 2010; Bagnell, 2015).\nInverse reinforcement learning (IRL) methods (Russell, 1998; Ng & Russell, 2000; Ratliff et al., 2006; Ziebart et al., 2008), which are some of the most successful approaches to imitation learning, assume that the behavior the learner desires to imitate is generated by an expert behaving optimally with respect to an unknown cost function. IRL algorithms train models over entire trajectories of behavior instead of individual actions, and hence do not suffer from cascading error problems. Furthermore, because the assumption of expert optimality acts as a prior on the space of policies, IRL algorithms can allow the learner to generalize expert behavior to unseen states much more effectively than if the learner had tried to produce a policy or value function instead (Ng & Russell, 2000; Bagnell, 2015).\nUnfortunately, the assumption of expert optimality leads to expensive design choices in IRL algorithms. At each iteration, to determine whether a certain cost function c fits an expert policy \u03c0E , the IRL algorithm must compare the return of \u03c0E with the return of all other possible policies. Most IRL algorithms do this by running a reinforcement learning algorithm on c (Neu & Szepesva\u0301ri, 2009). Because reinforcement learning must be run at each iteration, IRL algorithms can be extremely expensive to run in large domains.\nWe forgo learning a cost function. We propose a method that directly learns a policy from expert trajectories, exploiting for learning signal a class of cost functions, which distinguish the expert policy from all others. We first develop a simple, unified view of a certain class of imitation learning algorithms called apprenticeship learning algorithms (Abbeel & Ng, 2004; Syed et al., 2008). This view naturally leads to the development of a gradient-based optimization formulation over parameterized policies for ap-\nar X\niv :1\n60 5.\n08 47\n8v 1\n[ cs\n.L G\n] 2\n6 M\nay 2\n01 6\nprenticeship learning. We then provide two model-free realizations of these optimization algorithms: one is based on a standard policy gradient algorithm, and the other is based on a recently developed policy gradient algorithm that incorporates trust region constraints to stabilize optimization. We demonstrate the effectiveness of our approach on control problems with very high-dimensional observations (over 600 continuous features), for which we train neural network control policies from scratch."}, {"heading": "2. Preliminaries", "text": "We begin by defining basic notions from reinforcement learning. We are given an environment consisting of a state space S, an action space A, a dynamics model p(s\u2032|s, a), and an initial state distribution p0(s0). Agents act according to stationary stochastic policies \u03c0(a|s), which specify action choice probabilities for each state. We will work with finite S and A, but our methods will extend to continuous spaces.\nWith respect to a cost function c : S \u00d7 A \u2192 R, a discount factor \u03b3 \u2208 [0, 1), and a policy \u03c0, the stateaction value function Qc\u03c0(s, a), the state value function V c\u03c0 (s), and the advantage function A c \u03c0(s, a) are defined\nas Qc\u03c0(st, at) = Ep0,p,\u03c0 [\u2211\u221e t\u2032=t \u03b3 t\u2032\u2212tc(st\u2032 , at\u2032) ] , V c\u03c0 (s) = Ea\u223c\u03c0(\u00b7|s) [Qc\u03c0(s, a)], and Ac\u03c0(s, a) = Qc\u03c0(s, a) \u2212 V c\u03c0 (s). The expected cost of \u03c0 is \u03b7c(\u03c0) = Es0\u223cp0 [V c\u03c0 (s0)]. For clarity, when c is a parameterized function, written as cw for a parameter vector w, we will replace c by w in the names of these quantities\u2014for example, \u03b7w, Qw\u03c0 , etc. We define the \u03b3-discounted state visitation distribution of a policy \u03c0 by \u03c1\u03c0(s) = \u2211\u221e t=0 \u03b3\ntPp0,\u03c0 [st = s], where Pp0,\u03c0 [st = s] is the probability of landing in state s at time t, when following \u03c0 starting from s0 \u223c p0. When convenient, we will overload notation for state-action visitation distributions: \u03c1\u03c0(s, a) = \u03c0(a|s)\u03c1\u03c0(s), allowing us to write expected cost as \u03b7c(\u03c0) = \u2211 s,a \u03c1\u03c0(s, a) c(s, a) = E\u03c1\u03c0 [c(s, a)]."}, {"heading": "3. Apprenticeship learning", "text": "To address the imitation learning problem, we adopt the apprenticeship learning formalism, in which the learner must find a policy that performs at least as well as the expert \u03c0E on an unknown true cost function ctrue : S \u00d7A \u2192 R (Abbeel & Ng, 2004; Syed et al., 2008). Formally, the learner\u2019s goal is to find a policy \u03c0 such that \u03b7ctrue(\u03c0) \u2264 \u03b7ctrue(\u03c0E), given a dataset of trajectory samples from \u03c0E .\nTo do this, apprenticeship learning algorithms carry with them the assumption that the true cost function belongs to a class of cost functions C. Accordingly, they seek a policy\n\u03c0 that performs as well as \u03c0E for all c \u2208 C\u2014that is, they seek to satisfy the constraints\n\u03b7c(\u03c0) \u2264 \u03b7c(\u03c0E) for all c \u2208 C (1)\nBecause ctrue \u2208 C by assumption, satisfying this family of constraints ensures successful apprenticeship learning. We can reformulate this constraint satisfaction problem as an optimization problem by defining the objective\n\u03b4C(\u03c0, \u03c0E) = sup c\u2208C\n\u03b7c(\u03c0)\u2212 \u03b7c(\u03c0E) (2)\nIntuitively, the cost functions in C distinguish the expert from all other policies, assigning high expected cost to nonexpert policies and low expected cost to the expert policy. If \u03b4c(\u03c0, \u03c0E) > 0, then there exists some cost in C such that \u03c0 performs worse than \u03c0E\u2014in this case, \u03c0 is a poor solution to the apprenticeship learning problem. On the other hand, if \u03b4c(\u03c0, \u03c0E) \u2264 0, then \u03c0 performs at least as well as \u03c0E for all costs in C, and therefore satisfies the apprenticeship learning constraints (1).\nHaving defined the objective, the job of an apprenticeship learning algorithm is to solve the optimization problem\nminimize \u03c0 \u03b4C(\u03c0, \u03c0E). (3)\nSo far, we have described a general framework for defining apprenticeship learning algorithms. To instantiate this framework, two ingredients must be provided: a cost function class C, and an optimization algorithm to solve (3). Our goal in this paper is to address the optimization ingredient, so we will use linearly parameterized cost functions in our experiments, although the development of our method is agnostic to the particulars of the cost function class. In Section 4, we will develop a method for approximately solving (3) over a class of parameterized stochastic policies (for example, neural network policies), assuming generic access to a method for solving the maximization (2) over costs for fixed policies \u03c0. Our method will perform gradient-based stochastic optimization on policy parameters\u2014we refer to this strategy as policy optimization."}, {"heading": "3.1. Examples from prior work", "text": "Before delving into our method, we first review two prototypical examples of apprenticeship learning algorithms. We show how they fall into the framework detailed in this section (namely, how they choose the cost function class C), and we briefly describe their solution techniques for solving (3), which differ vastly from our new policy optimization method.\nFeature expectation matching Abbeel & Ng (2004) define C by first fixing a set of basis cost functions c1, . . . , ck,\nwhere cj : S\u00d7A \u2192 R, and then defining the cost class as a certain set of linear combinations of these basis functions:\nClinear = { cw , \u2211k i=1 wici \u2223\u2223\u2223 \u2016w\u20162 \u2264 1} (4) The structure of Clinear allows the expected costs with respect to cw to be written as an inner product of w with a certain feature expectation vector of \u03c0, defined as \u03c6(\u03c0) , E\u03c1\u03c0 [ \u2211\u221e t=0 \u03b3\nt\u03c6(st, at)], where \u03c6(s, a) = [c1(s, a) \u00b7 \u00b7 \u00b7 ck(s, a)]T . Because any cost in Clinear can be written as cw(s, a) = w \u00b7 \u03c6(s, a), linearity of expectation yields \u03b7w(\u03c0) = E [ \u2211\u221e t=0 \u03b3 tw \u00b7 \u03c6(st, at)] = w \u00b7 \u03c6(\u03c0).\nBased on this observation, Abbeel & Ng (2004) propose to match feature expectations; that is, to find a policy \u03c0 such that \u03c6(\u03c0) \u2248 \u03c6(\u03c0E), thereby guaranteeing that \u03b7w(\u03c0) = w \u00b7\u03c6(\u03c0) \u2248 w \u00b7\u03c6(\u03c0E) = \u03b7w(\u03c0E) for all cost functions cw \u2208 Clinear. We can understand feature expectation matching as minimization of \u03b4Clinear , because\n\u03b4Clinear(\u03c0, \u03c0E) = sup \u2016w\u20162\u22641 E\u03c1\u03c0 [w \u00b7 \u03c6(s, a)]\u2212 E\u03c1\u03c0E [w \u00b7 \u03c6(s, a)]\n= sup \u2016w\u20162\u22641\nw \u00b7 (\u03c6(\u03c0)\u2212 \u03c6(\u03c0E))\n= \u2016\u03c6(\u03c0)\u2212 \u03c6(\u03c0E)\u20162. (5)\nTo solve this problem, Abbeel & Ng (2004) propose to incrementally generate a set of policies by inverse reinforcement learning (Ng & Russell, 2000). At each iteration, their algorithm finds a cost function that assigns low expected cost to the expert and high expected cost to previously found policies. Then, it adds to the set of policies the optimal policy for this cost, computed via reinforcement learning. These steps are repeated until there is no cost function that makes the expert perform much better than the previously found policies. The final policy, which minimizes (5), is produced by stochastically mixing the generated policies using weights calculated by a quadratic program. This algorithm is quite expensive to run for large MDPs, because it requires running reinforcement learning at each iteration.\nGame-theoretic approaches Syed & Schapire (2007); Syed et al. (2008) proposed two apprenticeship learning algorithms, Multiplicative Weights Apprenticeship Learning (MWAL) and Linear Programming Apprenticeship Learning (LPAL), that also use basis cost functions, but with the weights restricted to give a convex combination:\nCconvex = { cw , \u2211k i=1 wici \u2223\u2223\u2223 wi \u2265 0, \u2211i wi = 1} (6) MWAL uses a multiplicative weights update method to solve the resulting optimization problem, and like Abbeel and Ng\u2019s method, requires running reinforcement learning in its inner loop. Syed et al. (2008) address this computational complexity with their LPAL method. They notice\nthat restricting the weights on the basis functions to lie on the simplex allows the maximization over costs to be performed instead over a finite set: the problem (3) with Cconvex can be written as min\u03c0 maxi\u2208{1,...,k} \u03b7ci(\u03c0) \u2212 \u03b7ci(\u03c0E). Syed et al. are therefore able to formulate a single linear program on state-action visitation frequencies that simultaneously encodes (3) and Bellman flow constraints on the frequencies to ensure that they can be generated by some policy in the environment.\nInspired by LPAL, we will formulate an unconstrained optimization approach to apprenticeship learning. We propose to optimize (3) directly over parameterized stochastic policies instead of state-action visitation distributions, allowing us to scale to large spaces without keeping variables for each state and action. We keep our formulation general enough to allow for any cost function class C, but because the focus of this paper is the optimization over policies, we will use linearly parameterized cost classes Clinear and Cconvex in our experiments. (Note that despite assuming linearity, this setting is already more general than LPAL, as the maximization over Clinear cannot be written as a maximization over a finite set.)"}, {"heading": "4. Policy optimization for apprenticeship learning", "text": "Having reviewed existing algorithms for solving various settings of apprenticeship learning, we now delve into policy optimization strategies that directly operate on (3) as a stochastic optimization problem. To allow us to scale to large, continuous environments, we first fix a class of smoothly parameterized stochastic policies \u03a0 = {\u03c0\u03b8 | \u03b8 \u2208 \u0398}, where \u0398 is a set of valid parameter vectors. With this class of policies, our goal is to solve the optimization problem (3) over policy parameters:\nminimize \u03b8 \u03b4C(\u03c0\u03b8, \u03c0E)\nwhere \u03b4C(\u03c0\u03b8, \u03c0E) = supc\u2208C \u03b7 c(\u03c0\u03b8)\u2212\u03b7c(\u03c0E). We propose find a local minimum to this problem using gradient-based stochastic optimization. To this end, let us first examine the gradient of \u03b4C with respect to \u03b8. Letting c\u2217 denote the cost function that achieves the supremum in \u03b4C ,1 we have\n\u2207\u03b8\u03b4C(\u03c0\u03b8, \u03c0E) = \u2207\u03b8\u03b7c \u2217 (\u03c0\u03b8) (7)\nThis formula dictates the basic structure a gradient-based algorithm must take to minimize \u03b4C\u2014it must first compute c\u2217 for a fixed \u03b8, then it must use this c\u2217 to improve \u03b8 for the next iteration. (Our algorithm in Section 4.2 will actually have to identify a cost function defined by a more complicated criterion, but as we will see in Section 4.3,\n1In this paper, we only work with classes C for which this supremum is achieved.\nAlgorithm 1 IM-REINFORCE Input: Expert trajectories \u03c4E , initial policy parameters. \u03b80 for i = 0, 1, 2, . . . do\nRoll out trajectories \u03c4 \u223c \u03c0\u03b8i Compute c\u0302 achieving the supremum in (10),\nwith expectations taken over \u03c4 and \u03c4E Estimate the gradient\u2207\u03b8\u03b7c\u0302(\u03c0\u03b8)|\u03b8=\u03b8i (8) with \u03c4 Use the gradient to take a step from \u03b8i to \u03b8i+1\nend for\nthis will not pose significant difficulty.) Because the cost c\u2217 effectively defines a reinforcement learning problem at the current policy \u03c0\u03b8, we can interpret gradient-based optimization of \u03b4C as a procedure that alternates between (1) fitting a local reinforcement learning problem to generate learning signal for imitation, and (2) improving the policy with respect to this local problem. We will discuss how to find c\u2217 in Section 4.3; for now, we will only discuss strategies for policy improvement."}, {"heading": "4.1. Policy gradient", "text": "The most straightforward method of optimizing (3) is stochastic gradient descent with an estimate of the gradient (7):\n\u2207\u03b8\u03b7c \u2217 (\u03c0\u03b8) = E\u03c1\u03c0\u03b8 [ \u2207\u03b8 log \u03c0\u03b8(a|s)Qc \u2217 \u03c0\u03b8 (s, a) ] (8)\nThis is the classic policy gradient formula for the cost function c\u2217 (Sutton et al., 1999). To estimate this from samples, we propose the following algorithm, called IM-REINFORCE, which parallels the development of REINFORCE (Williams, 1992) for reinforcement learning. As input, IM-REINFORCE is given expert trajectories (that is, rollouts of the expert policy). At each iteration, for the current parameter vector \u03b80, trajectories are sampled using \u03c00 , \u03c0\u03b80 . Then, the cost c\u0302 attaining the supremum of an empirical estimate of \u03b4C is calculated to satisfy:\n\u03b4\u0302C(\u03c00, \u03c0E) = sup c\u2208C E\u0302\u03c1\u03c00 [c(s, a)]\u2212 E\u0302\u03c1\u03c0E [c(s, a)] (9)\n= E\u0302\u03c1\u03c00 [c\u0302(s, a)]\u2212 E\u0302\u03c1\u03c0E [c\u0302(s, a)] (10)\nHere, E\u0302 denotes empirical expectation using rollout samples. We describe how to compute c\u0302 in detail in Section 4.3; how to do so depends on C.\nWith c\u0302, IM-REINFORCE then estimates the gradient \u2207\u03b8\u03b7c\u0302 using the formula (8), where the state-action value Qc\u0302\u03c00(s, a) is estimated using discounted future sums of c\u0302 costs along rollouts for \u03c00. Finally, to complete the iteration, IM-REINFORCE takes a step in the resulting gradient direction, producing new policy parameters \u03b8 ready for the next iteration. These steps are summarized in Algorithm 1."}, {"heading": "4.2. Monotonic policy improvements", "text": "While IM-REINFORCE is straightforward to implement, the gradient estimator (8) exhibits extremely high variance, making the algorithm very slow to converge, or even diverge for reasonably large step sizes. This variance issue is not unique to our apprenticeship learning formulation, and is a hallmark difficulty of policy gradient algorithms for reinforcement learning (Peters & Schaal, 2008).\nThe reinforcement learning literature contains a vast number of techniques for calculating high-quality policy parameter steps based on Monte Carlo estimates of the gradient. We make no attempt to fully review these techniques here. Instead, we will directly draw inspiration from a recently developed algorithm called trust region policy optimization (TRPO), a model-free policy search algorithm capable of quickly training large neural network stochastic policies for complex tasks (Schulman et al., 2015).\nTRPO for reinforcement learning In this section, we will review TRPO for reinforcement learning, and in the next, we will develop an analogous algorithm for our apprenticeship learning setting. For now, we will drop the cost function superscript c, because the cost is fixed in the reinforcement learning setting.\nSuppose we have a current policy \u03c00 that we wish to improve. We can write the performance of a new policy \u03c0 in terms of the performance of \u03c00 (Kakade & Langford, 2002):\n\u03b7(\u03c0) = \u03b7(\u03c00) + E\u03c1\u03c0Ea\u223c\u03c0(\u00b7|s)[A\u03c00(s, a)] (11)\nVanilla policy gradient methods, such as the one described in the previous section, improve \u03b7(\u03c0) by taking a step on a local approximation at \u03c00:\nL(\u03c0) , \u03b7(\u03c00) + A\u03c00(\u03c0) (12)\nwhere A\u03c00(\u03c0) = Es\u223c\u03c1\u03c00Ea\u223c\u03c0(\u00b7|s) [A\u03c00(s, a)]. If the policies are parameterized by \u03b8 (that is \u03c00 = \u03c0\u03b80 and \u03c0 = \u03c0\u03b8), then L matches \u03b7 to first order at \u03b80, and therefore taking a small gradient step on L guarantees improvement of \u03b7. However, there is little guidance on how large this step can be, and in cases when the gradient can only be estimated, the required step size might be extremely small to compensate for noise. Schulman et al. (2015) address this by showing that minimizing a certain surrogate loss function can guarantee policy improvement with a large step size. Define the following penalized variant of L:\nM(\u03c0) , L(\u03c0) + 2 \u03b3\n(1\u2212 \u03b3)2 max s DKL(\u03c00(\u00b7|s) \u2016 \u03c0(\u00b7|s))\n(13)\nwhere = maxs,a |A\u03c00(s, a)|. Schulman et al. prove that M upper bounds \u03b7:\n\u03b7(\u03c0) \u2264M(\u03c0) (14)\nBecause KL divergence is zero when its arguments are equal, this inequality shows thatM majorizes 2 \u03b7 at \u03c00. Using M as a majorizer for \u03b7 in a majorization-minimization algorithm leads to an algorithm guaranteeing monotonic policy improvement at each iteration.\nUnfortunately, as M is currently defined, computing the maximum-KL divergence term over the whole state space is intractable. Schulman et al. propose to relax this to an average over state space, which can be approximated by samples:\nDKL(\u03c00 \u2016 \u03c0) , Es\u223c\u03c1\u03c00 [DKL(\u03c00(\u00b7|s) \u2016 \u03c0(\u00b7|s))] (15)\nThey find that this average-KL formulation works well empirically, and that algorithm\u2019s stability could be improved by further reformulating the cost as a trust region constraint. This leads to the TRPO step computation\nminimize \u03b8\nL(\u03c0\u03b8) s.t. DKL(\u03c00 \u2016 \u03c0\u03b8) \u2264 \u2206 (16)\nwhere all constants in Equation (13) are folded into a predefined trust region size \u2206 > 0. To solve this step computation problem, the objective L and the KL divergence constraint must be approximated using samples and then minimized with gradient-based constrained optimization. The sample approximation can be done using a similar strategy to the one described in Section 4.1. Further discussion on sampling methodologies and effective optimization algorithms for solving this constrained problem can be found in Schulman et al. (2015).\nTRPO for apprenticeship learning Now, we describe how to adapt TRPO to apprenticeship learning (3). Reintroducing the c superscripts, we wish to compute an improvement step from \u03b80 to \u03b8 for the optimization problem\nminimize \u03b8 sup c\u2208C\n\u03b7c(\u03c0\u03b8)\u2212 \u03b7c(\u03c0E) (17)\nWe wish to derive a majorizer for this objective, analogous to the majorizer (13) for a fixed cost function. Observe that if {f\u03b1} and {g\u03b1} are families of functions such that g\u03b1 majorizes f\u03b1 at x0 for all \u03b1, then sup\u03b1 g\u03b1 majorizes sup\u03b1 f\u03b1 at x0. We can therefore derive a TRPO-style algorithm for apprenticeship learning as follows. First, to remove the dependence of in (13) on any particular cost function, we assume that all cost functions in C are bounded by Cmax,3 and\n2M is said to majorize \u03b7 at x0 if M \u2265 \u03b7 with equality at x0. 3In practice, this is easy to satisfy for Clinear and Cconvex by en-\nsuring that the cost basis functions are bounded.\nthen we let \u2032 , 2Cmax1\u2212\u03b3 \u2265 supc maxs,a |A c \u03c00(s, a)|. Now, we can define M c(\u03c0) analogously to (13):\nM c(\u03c0) , Lc(\u03c0) + 2 \u2032\u03b3\n(1\u2212 \u03b3)2 max s DKL(\u03c00(\u00b7|s) \u2016 \u03c0(\u00b7|s))\nBy the definition of \u2032 and (14), we have that for all c \u2208 C,\n\u03b7c(\u03c0)\u2212 \u03b7c(\u03c0E) \u2264M c(\u03c0)\u2212 \u03b7c(\u03c0E), (18)\nand consequently, we obtain an upper bound for the apprenticeship objective:\n\u03b4C(\u03c0, \u03c0E) = sup c\u2208C\n\u03b7c(\u03c0)\u2212 \u03b7c(\u03c0E)\n\u2264 sup c\u2208C M c(\u03c0)\u2212 \u03b7c(\u03c0E) ,MC(\u03c0, \u03c0E). (19)\nSince the inequalities (18) become equalities at \u03c0 = \u03c00, inequality (19) does too, and thus MC(\u03c0, \u03c0E) majorizes the apprenticeship learning objective \u03b4C(\u03c0, \u03c0E) at \u03c0 = \u03c00. Importantly, the KL divergence cost in MC(\u03c0, \u03c0E) does not depend on c:\nMC(\u03c0, \u03c0E) = ( sup c\u2208C Lc(\u03c0)\u2212 \u03b7c(\u03c0E) )\n(20)\n+ 2 \u2032\u03b3\n(1\u2212 \u03b3)2 max s DKL(\u03c00(\u00b7|s) \u2016 \u03c0(\u00b7|s))\nHence, we can apply the same empirically justified transformation that led to TRPO: replacing the maximum-KL cost by an average-KL constraint. We therefore propose to compute steps for our apprenticeship learning setting by solving the following trust region subproblem:\nminimize \u03b8 sup c\u2208C\nLc(\u03c0\u03b8)\u2212 \u03b7c(\u03c0E)\nsubject to DKL(\u03c00 \u2016 \u03c0\u03b8) \u2264 \u2206 (21)\nwhere again, all constants are folded into \u2206. To solve this trust region subproblem in the finite-sample regime, we approximate the KL constraint using samples from \u03c00, just as TRPO does for its trust region problem (16). The objective of (21), however, warrants more attention, because of the interplay between the maximization over c and minimization over \u03b8. Let f(\u03b8) be the objective of (21). We wish to derive a finite-sample approximation to f suitable as an objective for the subproblem, so for computational reasons, we would like to avoid trajectory sampling within optimization for this subproblem.\nTo do so, we introduce importance sampling with \u03c00 as the proposal distribution for the advantage term of f , thereby avoiding the need to sample from \u03c0\u03b8 as \u03b8 varies:\nf(\u03b8) = sup c\u2208C\n\u03b7c(\u03c00)\u2212 \u03b7c(\u03c0E) + Ac\u03c00(\u03c0\u03b8)\n= sup c\u2208C E\u03c1\u03c00 [c(s, a)]\u2212 E\u03c1\u03c0E [c(s, a)] +\nE\u03c1\u03c00 [ \u03c0\u03b8(a|s) \u03c00(a|s) (Qc\u03c00(s, a)\u2212 V c \u03c00(s)) ] (22)\nAlgorithm 2 IM-TRPO Input: Expert trajectories \u03c4E , initial policy params. \u03b80, trust region size \u2206 for i = 0, 1, 2, . . . do\nRoll out trajectories \u03c4 \u223c \u03c0\u03b8i Find \u03c0\u03b8i+1 minimizing Equation (23)\nsubject to DKL(\u03c0\u03b8i \u2016 \u03c0\u03b8i+1) \u2264 \u2206, with expectations taken over \u03c4 and \u03c4E (15)\nend for\nAt first glance, it seems that the last term of equation (22) requires multiple rollouts for the Q and V parts separately. However, this is not the case, because the identity\nEa\u223c\u03c00(\u00b7|s) [ \u03c0(a|s) \u03c00(a|s) V c\u03c00(s) ] = V c\u03c00(s) = Ea\u223c\u03c00(\u00b7|s)[Q c \u03c00(s, a)]\nlets us write\nf(\u03b8) = sup c\u2208C E\u03c1\u03c00 [c(s, a)]\u2212 E\u03c1\u03c0E [c(s, a)] +\nE\u03c1\u03c00 [( \u03c0\u03b8(a|s) \u03c00(a|s) \u2212 1 ) Qc\u03c00(s, a) ] (23) Replacing expectations with empirical ones gives the final form of the objective that we use to define the finitesample trust region subproblem. Solving these trust region subproblems yields our final algorithm, which we call IM-TRPO (Algorithm 2). The computational power needed to minimize this trust region cost is not much greater than that of the TRPO subproblem (16), assuming that the supremum over C is easily computable. We will show next in Section 4.3 that solving IM-TRPO subproblems indeed poses no significant difficulty over the computation (10) necessary for IM-REINFORCE."}, {"heading": "4.3. Finding cost functions", "text": "Until now, we deferred discussion of finding cost functions that achieve the supremum in the apprenticeship learning objective (2). We address this issue here for the feature expectation matching setting as described in Section 3.1, with cost functions Clinear parameterized linearly by `2-bounded weight vectors (4). The case for Cconvex can be derived similarly and is omitted for space reasons.\nAs mentioned in Section 4.1, the apprenticeship learning algorithm only has access to sample trajectories from \u03c0 and \u03c0E , so we will consider finding the cost that achieves the supremum of the empirical apprenticeship learning objective \u03b4\u0302C (10). Using c\u0302 to denote the optimal cost for this empirical objective, we have\n\u03b4\u0302Clinear(\u03c0, \u03c0E) = sup \u2016w\u20162\u22641\nw \u00b7 (\u03c6\u0302(\u03c0)\u2212 \u03c6\u0302(\u03c0E)), (24)\nwhere \u03c6\u0302 is the empirical feature expectation vector. The supremum in this equation is attained by a vector with a\nclosed-form expression: w\u0302 , (\u03c6\u0302(\u03c0) \u2212 \u03c6\u0302(\u03c0E))/\u2016\u03c6\u0302(\u03c0) \u2212 \u03c6\u0302(\u03c0E)\u2016, which can be inserted directly into (8) for IMREINFORCE. However, this w\u0302 does not suffice for IMTRPO, which, for the objective of the trust region subproblem (23), requires a maximizer w\u0302 that must be recomputed for every optimization step for the subproblem. This recomputation is not difficult or expensive, as we will now demonstrate. For linear costs, the empirical trust region subproblem objective is given by:\nf\u0302(\u03b8) = sup \u2016w\u20162\u22641 E\u0302\u03c1\u03c00 [w \u00b7 \u03c6(s, a)]\u2212 E\u0302\u03c1\u03c0E [w \u00b7 \u03c6(s, a)] +\nE\u0302\u03c1\u03c00 [( \u03c0\u03b8(a|s) \u03c00(a|s) \u2212 1 ) Qw\u03c00(s, a) ] (25)\nNow let \u03c6(\u03c00|s0, a0) , E\u03c00 [ \u2211\u221e t=0 \u03b3\nt\u03c6(st, at) | s0, a0] and \u03c8(\u03c0\u03b8) , E\u03c1\u03c00 [( \u03c0\u03b8(a|s) \u03c00(a|s) \u2212 1 ) \u03c6(\u03c00|s, a) ] , both of which are readily estimated from the very same rollout trajectories from \u03c00 used to estimate expected costs. With these, we get Qw\u03c00(s, a) = w \u00b7 \u03c6(\u03c00|s, a), which lets us write (25) as:\nf\u0302(\u03b8) = sup \u2016w\u20162\u22641\nw \u00b7 ( \u03c6\u0302(\u03c00)\u2212 \u03c6\u0302(\u03c0E) + \u03c8\u0302(\u03c0\u03b8) ) (26)\nThis reveals that the supremum is achieved by\nw\u0302 , \u03c6\u0302(\u03c00)\u2212 \u03c6\u0302(\u03c0E) + \u03c8\u0302(\u03c0\u03b8) \u2016\u03c6\u0302(\u03c00)\u2212 \u03c6\u0302(\u03c0E) + \u03c8\u0302(\u03c0\u03b8)\u2016 , (27)\nwhich is straightforward to compute. Note that this vector depends on \u03b8; that is, it changes as the trust region subproblem (21) is optimized, and must be recomputed with each step of the algorithm for solving the trust region subproblem. However, by construction, all empirical expectations are taken with respect to \u03c00, which does not change as \u03b8 changes, and hence no simulations in the environment are required for these recomputations."}, {"heading": "5. Experiments", "text": "We evaluated our approach in a variety of scenarios: finite gridworlds of varying sizes, the continuous planar navigation task of Levine and Koltun (2012), a family of continuous environments of varying numbers of observation features (Karpathy, 2015), and a variation of Levine & Koltun\u2019s highway driving simulation, in which the agent receives high-dimensional egocentric observation features.\nIn all of the continuous environments, we used policies constructed according to Schulman et al. (2015): the policies have Gaussian action distributions, with mean given by a multi-layer perceptron taking observations as input, and standard deviations given by an extra set of parameters. Details on the environments and training methodology are in the supplement.\nComparing against globally optimal methods As mentioned in Section 3.1, LPAL (Syed et al., 2008) finds a global optimum for the apprenticeship problem (3) with Cconvex in finite state and action spaces. In contrast, our approach scales to high-dimensional spaces but is only guaranteed to find a local optimum of (3), as described in Section 4. To evaluate the quality of our local optima, we tested IM-REINFORCE, using Cconvex to learn tabular Boltzmann policies with value iteration for exact gradient evaluation, against LPAL and a behavioral cloning baseline.\nWe evaluated the learned policies for the three algorithms on 64 \u00d7 64 gridworlds on varying amounts of expert data. In each trial, we randomly generated costs in the world, and we generated expert data by sampling behavior from an optimal policy computed with value iteration. To evaluate an algorithm, we computed the ratio of learned policy performance to the expert\u2019s performance. We also ran a timing test, in which we evaluated the computation time for each algorithm on varying gridworld sizes, with fixed dataset sizes, for 10 trials each.\nThe results are displayed in Figure 1. We found that despite our local optimality guarantee, IM-REINFORCE learned policies achieving at least 98% the performance of policies learned by LPAL, with similar sample complexity. IMREINFORCE\u2019s training times also scaled favorably compared to LPAL. For a large gridworld with 65536 states, LPAL took on average 10 minutes to train with large variance across instantiations of the expert, whereas our algorithm consistently took around 4 minutes.\nComparing against continuous IRL Next, we evaluated our algorithms in a small, continuous environment: the objectworld environment of Levine and Koltun (2012), in which the agent moves in a plane to seek out Gaussianshaped costs, given only expert data generated either by globally or locally optimal expert policies. We compared the trajectories produced by IM-TRPO with Clinear to those produced by trajectory optimization on a cost learned by Levine and Koltun\u2019s CIOC algorithm, a model-based IRL method designed for continuous settings with full knowledge of dynamics derivatives. The basis functions we used for Clinear were the same as those used by CIOC to define\nlearned cost functions. The results are in Figure 2.\nWe found that even though our method is model-free and does not use dynamics derivatives, it consistently learned policies achieving zero excess cost (the difference in expected true cost compared to the expert, measured by averaging over 100 rollouts), matching the performance of optimal trajectories for cost functions learned by CIOC.\nVarying dimension To evaluate our algorithms\u2019 performance with varying environment dimension, we used a family of environments inspired by Karpathy (2015). In these environments, the agent moves in a plane populated by colored moving targets to be either captured or avoided, depending on color. The action space is two-dimensional, allowing the agent to apply forces to move itself in any direction. The agent has a number of sensors Nsensors facing outward with uniform angular spacing. Each sensor detects the presence of a target with 5 continuous features indicating the nearest target\u2019s distance, color, and relative velocity to the agent. These sensor features, along with an indicators of whether the agent is currently capturing a target, lead to observation features of dimension 5 \u00b7 Nsensors + 2, which are fed to the policy. Varying Nsensors yields a family of environments with differing observation dimension.\nFor Nsensors set to 5, 10, and 20 (yielding 27, 52, and 102 observation features, respectively), we first generated expert data by executing policies learned by reinforcement learning on a true cost, which was a linear combination of basis functions indicating control effort and intersection with targets. Then, we ran both IM-REINFORCE and IM-TRPO using Clinear on the same basis functions, and we measured the excess cost of each learned policy.\nWe found that IM-TRPO achieved nearly perfect imitation in this setting, and the performance was not significantly affected by the dimensionality of the space. IMREINFORCE\u2019s learning also progressed, but was far outpaced by IM-TRPO (see Figure 3). We also verified that IM-TRPO\u2019s overhead of computing c\u0302 (27) for each step of solving its trust region subproblem was negligible, verify-\ning our claim in Section 4.3. On our system, iterations for plain TRPO for reinforcement learning and IM-TRPO both took 8-9 seconds each for this environment, with no statistically significant difference.\nHighway driving Finally, we ran IM-TRPO on a variation of the highway driving task of Levine & Koltun (2012). In this task, the learner must imitate driving behaviors (aggressive, tailgating, and evasive) in a continuous driving simulation. The observations in the original driving task were the actual states of the whole environment, including positions and velocities of all cars at all points on the road. To introduce more realism, we modified the environment by providing policies only egocentric observation features: readings from 30 equally spaced rangefinders that detect the two road edges, readings from 60 equally spaced rangefinders that detect cars, and speed and angular velocity of the agent\u2019s car. These observations effectively form a depth image of nearby cars and lane markings within the agent\u2019s field of view; see Figure 4. We aggregated these readings over a window of 5 timesteps, yielding a 610- dimensional partial observations.\nWe ran IM-TRPO with Clinear, using basis functions representing quadratic features derived from those of Levine & Koltun. Despite the fact that we provided only highdimensional partial observations, our model-free approach learned policies achieving behavior comparable to the trajectories generated by CIOC, which was provided full state features and a full environment model. The policies learned by our algorithm generated behavior that both qualitatively\nand quantitatively resembled the demonstrated behavior, as shown in Table 1."}, {"heading": "6. Discussion and future work", "text": "We showed that carefully blending state-of-the-art policy gradient algorithms for reinforcement learning with local cost function fitting lets us successfully train neural network policies for imitation in high-dimensional, continuous environments. Our method is able to identify a locally optimal solution, even in settings where optimal planning is out of reach. This is a significant advantage over competing algorithms that require repeatedly solving planning problems in an inner loop. In fact, when the inner planning problem is only approximately solved, competing algorithms do not even provide local optimality guarantees (Ermon et al., 2015).\nOur approach does not use expert interaction or reinforcement signal, fitting in a family of such approaches that includes apprenticeship learning and inverse reinforcement learning. When either of these additional resources is provided, alternative approaches (Kim et al., 2013; Daume\u0301 III et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011) may be more sample efficient, and investigating ways to combine these resources with our framework is an interesting research direction.\nWe focused on the policy optimization component of apprenticeship learning, rather than the design of appropriate cost function classes. We believe this is an important area for future work. Nonlinear cost function classes have been successful in IRL (Ratliff et al., 2009; Levine et al., 2011) as well as in other machine learning problems reminiscent of ours, in particular that of training generative image models. In the language of generative adversarial networks (Goodfellow et al., 2014), the policy parameterizes a generative model of state-action pairs, and the cost function serves as an adversary. Apprenticeship learning with large cost function classes capable of distinguishing between arbitrary state-action visitation distributions would, enticingly, open up the possibility of exact imitation."}, {"heading": "Acknowledgements", "text": "We thank John Schulman for valuable conversations about TRPO. This work was supported by a grant from the SAILToyota Center for AI Research and by a National Science Foundation Graduate Research Fellowship (grant no. DGE-114747)."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "An invitation to imitation", "author": ["Bagnell", "J Andrew"], "venue": "Technical report,", "citeRegEx": "Bagnell and Andrew.,? \\Q2015\\E", "shortCiteRegEx": "Bagnell and Andrew.", "year": 2015}, {"title": "Search-based structured prediction", "author": ["Daum\u00e9 III", "Hal", "Langford", "John", "Marcu", "Daniel"], "venue": "Machine learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In Proceedings of the 19th International Conference on Machine Learning,", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "URL http://cs.stanford.edu/people/ karpathy/reinforcejs/waterworld.html", "author": ["Karpathy", "Andrej"], "venue": "Reinforcejs: Waterworld demo,", "citeRegEx": "Karpathy and Andrej.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Andrej.", "year": 2015}, {"title": "Learning from limited demonstrations", "author": ["Kim", "Beomjoon", "Farahmand", "Amir-massoud", "Pineau", "Joelle", "Precup", "Doina"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2013}, {"title": "Continuous inverse optimal control with locally optimal examples", "author": ["Levine", "Sergey", "Koltun", "Vladlen"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Levine et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2012}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["Levine", "Sergey", "Popovic", "Zoran", "Koltun", "Vladlen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levine et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2011}, {"title": "Training parsers by inverse reinforcement learning", "author": ["Neu", "Gergely", "Szepesv\u00e1ri", "Csaba"], "venue": "Mach. Learn.,", "citeRegEx": "Neu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Neu et al\\.", "year": 2009}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Ng", "Andrew Y", "Russell", "Stuart J"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "Ng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2000}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["Peters", "Jan", "Schaal", "Stefan"], "venue": "Neural networks,", "citeRegEx": "Peters et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2008}, {"title": "Efficient training of artificial neural networks for autonomous navigation", "author": ["Pomerleau", "Dean A"], "venue": "Neural Computation,", "citeRegEx": "Pomerleau and A.,? \\Q1991\\E", "shortCiteRegEx": "Pomerleau and A.", "year": 1991}, {"title": "Maximum margin planning", "author": ["Ratliff", "Nathan D", "Bagnell", "J Andrew", "Zinkevich", "Martin A"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Ratliff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2006}, {"title": "Learning to search: Functional gradient techniques for imitation learning", "author": ["Ratliff", "Nathan D", "Silver", "David", "Bagnell", "J Andrew"], "venue": "Autonomous Robots,", "citeRegEx": "Ratliff et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2009}, {"title": "Efficient reductions for imitation learning", "author": ["Ross", "St\u00e9phane", "Bagnell", "Drew"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "St\u00e9phane", "Gordon", "Geoffrey J", "Bagnell", "Drew"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Learning agents for uncertain environments", "author": ["Russell", "Stuart"], "venue": "In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pp", "citeRegEx": "Russell and Stuart.,? \\Q1998\\E", "shortCiteRegEx": "Russell and Stuart.", "year": 1998}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael", "Moritz", "Philipp"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "A game-theoretic approach to apprenticeship learning", "author": ["Syed", "Umar", "Schapire", "Robert E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Syed et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2007}, {"title": "A reduction from apprenticeship learning to classification", "author": ["Syed", "Umar", "Schapire", "Robert E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Syed et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2010}, {"title": "Apprenticeship learning using linear programming", "author": ["Syed", "Umar", "Bowling", "Michael", "Schapire", "Robert E"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Syed et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2008}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Ziebart", "Brian D", "Maas", "Andrew L", "Bagnell", "J Andrew", "Dey", "Anind K"], "venue": "In AAAI,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 13, "context": "Inverse reinforcement learning (IRL) methods (Russell, 1998; Ng & Russell, 2000; Ratliff et al., 2006; Ziebart et al., 2008), which are some of the most successful approaches to imitation learning, assume that the behavior the learner desires to imitate is generated by an expert behaving optimally with respect to an unknown cost function.", "startOffset": 45, "endOffset": 124}, {"referenceID": 24, "context": "Inverse reinforcement learning (IRL) methods (Russell, 1998; Ng & Russell, 2000; Ratliff et al., 2006; Ziebart et al., 2008), which are some of the most successful approaches to imitation learning, assume that the behavior the learner desires to imitate is generated by an expert behaving optimally with respect to an unknown cost function.", "startOffset": 45, "endOffset": 124}, {"referenceID": 22, "context": "We first develop a simple, unified view of a certain class of imitation learning algorithms called apprenticeship learning algorithms (Abbeel & Ng, 2004; Syed et al., 2008).", "startOffset": 134, "endOffset": 172}, {"referenceID": 22, "context": "To address the imitation learning problem, we adopt the apprenticeship learning formalism, in which the learner must find a policy that performs at least as well as the expert \u03c0E on an unknown true cost function ctrue : S \u00d7A \u2192 R (Abbeel & Ng, 2004; Syed et al., 2008).", "startOffset": 229, "endOffset": 267}, {"referenceID": 20, "context": "Game-theoretic approaches Syed & Schapire (2007); Syed et al. (2008) proposed two apprenticeship learning algorithms, Multiplicative Weights Apprenticeship Learning (MWAL) and Linear Programming Apprenticeship Learning (LPAL), that also use basis cost functions, but with the weights restricted to give a convex combination:", "startOffset": 50, "endOffset": 69}, {"referenceID": 20, "context": "Syed et al. (2008) address this computational complexity with their LPAL method.", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "This is the classic policy gradient formula for the cost function c\u2217 (Sutton et al., 1999).", "startOffset": 69, "endOffset": 90}, {"referenceID": 18, "context": "Instead, we will directly draw inspiration from a recently developed algorithm called trust region policy optimization (TRPO), a model-free policy search algorithm capable of quickly training large neural network stochastic policies for complex tasks (Schulman et al., 2015).", "startOffset": 251, "endOffset": 274}, {"referenceID": 18, "context": "Schulman et al. (2015) address this by showing that minimizing a certain surrogate loss function can guarantee policy improvement with a large step size.", "startOffset": 0, "endOffset": 23}, {"referenceID": 18, "context": "Further discussion on sampling methodologies and effective optimization algorithms for solving this constrained problem can be found in Schulman et al. (2015).", "startOffset": 136, "endOffset": 159}, {"referenceID": 18, "context": "In all of the continuous environments, we used policies constructed according to Schulman et al. (2015): the policies have Gaussian action distributions, with mean given by a multi-layer perceptron taking observations as input, and standard deviations given by an extra set of parameters.", "startOffset": 81, "endOffset": 104}, {"referenceID": 22, "context": "1, LPAL (Syed et al., 2008) finds a global optimum for the apprenticeship problem (3) with Cconvex in finite state and action spaces.", "startOffset": 8, "endOffset": 27}, {"referenceID": 6, "context": "When either of these additional resources is provided, alternative approaches (Kim et al., 2013; Daum\u00e9 III et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011) may be more sample efficient, and investigating ways to combine these resources with our framework is an interesting research direction.", "startOffset": 78, "endOffset": 161}, {"referenceID": 16, "context": "When either of these additional resources is provided, alternative approaches (Kim et al., 2013; Daum\u00e9 III et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011) may be more sample efficient, and investigating ways to combine these resources with our framework is an interesting research direction.", "startOffset": 78, "endOffset": 161}, {"referenceID": 14, "context": "Nonlinear cost function classes have been successful in IRL (Ratliff et al., 2009; Levine et al., 2011) as well as in other machine learning problems reminiscent of ours, in particular that of training generative image models.", "startOffset": 60, "endOffset": 103}, {"referenceID": 8, "context": "Nonlinear cost function classes have been successful in IRL (Ratliff et al., 2009; Levine et al., 2011) as well as in other machine learning problems reminiscent of ours, in particular that of training generative image models.", "startOffset": 60, "endOffset": 103}, {"referenceID": 3, "context": "In the language of generative adversarial networks (Goodfellow et al., 2014), the policy parameterizes a generative model of state-action pairs, and the cost function serves as an adversary.", "startOffset": 51, "endOffset": 76}], "year": 2016, "abstractText": "In imitation learning, an agent learns how to behave in an environment with an unknown cost function by mimicking expert demonstrations. Existing imitation learning algorithms typically involve solving a sequence of planning or reinforcement learning problems. Such algorithms are therefore not directly applicable to large, high-dimensional environments, and their performance can significantly degrade if the planning problems are not solved to optimality. Under the apprenticeship learning formalism, we develop alternative model-free algorithms for finding a parameterized stochastic policy that performs at least as well as an expert policy on an unknown cost function, based on sample trajectories from the expert. Our approach, based on policy gradients, scales to large continuous environments with guaranteed convergence to local minima.", "creator": "LaTeX with hyperref package"}}}