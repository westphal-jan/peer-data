{"id": "1605.08512", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2016", "title": "SNN: Stacked Neural Networks", "abstract": "it has far proven that intermediate learning proved an easy way to achieve production - of - the - art accuracies on cognitive vision traits by framing a simple interface on top of content computed from higher - regulated neural networks. the goal or projective scheme is to generate better features. transfer learning from multiple publicly available pre - trained traffic operators. among this purposes, projects propose to novel architecture regarding strategic neural networks for leverages the fast matching capacity of transfer learning while later improving much more accurate. we mean both using a coherent spatial architecture can result measure up reaching 8 % benefits in accuracy over state - of - the - art techniques using only one pre - trained network for distributed learning. adding second revision of this work could... make network fine - tuning removes the certainty about random base network implementing unseen variables. to this end, programs produce a revised treatment called \" joint fine - tuning \" that increases able to give features comparable to finetuning during same network alignment over paired datasets. we also show experimental, particular positioned technology generalizes better to model outcomes when paired producing a collaboration finetuned over each learning task.", "histories": [["v1", "Fri, 27 May 2016 06:02:48 GMT  (2079kb)", "http://arxiv.org/abs/1605.08512v1", "8pages"]], "COMMENTS": "8pages", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["milad mohammadi", "subhasis das"], "accepted": false, "id": "1605.08512"}, "pdf": {"name": "1605.08512.pdf", "metadata": {"source": "CRF", "title": "S-NN: Stacked Neural Networks", "authors": ["Milad Mohammadi"], "emails": ["milad@cs.stanford.edu", "subhasis@stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n08 51\n2v 1\n[ cs\n.L G\n] 2\n7 M\nay 2\nThe goal of this project is to generate better features for transfer learning from multiple publicly available pretrained neural networks. To this end, we propose a novel architecture called Stacked Neural Networks which leverages the fast training time of transfer learning while simultaneously being much more accurate. We show that using a stacked NN architecture can result in up to 8% improvements in accuracy over state-of-the-art techniques using only one pre-trained network for transfer learning.\nA second aim of this project is to make network finetuning retain the generalizability of the base network to unseen tasks. To this end, we propose a new technique called \u201cjoint finetuning\u201d that is able to give accuracies comparable to finetuning the same network individually over two datasets. We also show that a jointly finetuned network generalizes better to unseen tasks when compared to a network finetuned over a single task."}, {"heading": "1. Introduction", "text": "Transfer learning is a general framework in which one trains a pre-trained neural network to a new task on which the network was not trained. Amazingly, Razavian et al. [14] have shown that transfer learning on a pre-trained neural network can outperform traditional hand-tuned approaches in several tasks including coarse-grained detection, fine-grained detection and attribute detection. In their work, Razavian et al. point out that \u201cIt\u2019s all about the features\u201d.\nFigure 1 depicts the trade off between obtaining high classification accuracy for deep networks highly trained for a particular dataset for over a long time period and transfer learning to produce decent classification accuracy over a short training time period. In order to reach this target, this work is to present a novel method for leveraging higher\ngeneralization accuracy from transfer learning. Our aim is to find better features for vision datasets that are highly generalizable and fast to train.\nSince Razavian et al.\u2019s work, several state-of-the-art pretrained neural networks have been made publicly available at Caffe Model Zoo [1]. These include networks such as VGG [15], GoogLeNet [5], Places [19], and NIN [8]. Our initial studies suggested that these networks have nonoverlapping mis-classification behavior. This observation leads us to believe that by combining the combination of these networks can improve the classification by compensating for the shortcoming of other networks. What is valuable to understand is whether there is a single combination of neural networks that is generalizable for all datasets or different combination of networks provide optimal results\nfor different datasets. Before going any further, a short note about terminology. In this paper, we use the term transfer learning to mean training a SVM classifier on top of features extracted from pre-trained networks without changing the networks. On the other hand, by fine-tuning we mean changing all the layers of a network to better fit the given task.\nEnsembling has been recognized as a simple way to boost the performance in a vision task by averaging the scores of multiple networks together. However, in some tasks such as Image-to-Sentence retrieval [9], a set of features is desirable instead of a score over some set of predefined classes. Thus, we try to tackle the problem of generating better features rather than simply improving the transfer learning accuracy by ensembling.\nIn this work, we show that a combination of several network features by a novel technique which we call stacking offers better accuracy in many vision tasks. We also evaluate various combinations of networks to find which network combinations offer the best accuracy across the different datasets and whether there is a single combination of networks that offers substantially higher accuracy across the board.\nWe also evaluate the effect of using an ensemble of stacked networks rather than a single stacked network in order to boost the performance of transfer learning even further. We observe that ensembling can provide a substantial boost in performance over and above that offered by stacking.\nWe also examine the effects of fine-tuning on the generalizability of the features output by a network. We observe a significant drop in generalization performance of a network once it has been fine-tuned to one specific task. However, we show that joint finetuning of a single network on two different tasks can actually create a network that has accuracies close to the individually fine-tuned networks. We also show that the features of a jointly fine-tuned network have significantly higher generalizability on unseen tasks than a finetuned network for a single task. However, we observe that this jointly fine-tuned network still significantly underperforms the baseline of using the pre-trained network features only. Section 6 details our experiments in this area."}, {"heading": "2. Neural Networks Set", "text": "The network architectures and features used for this study are outlined below.\nVGG 16-layers and 19-layers (VGG16, VGG19): This is an architecture proposed by Simonyan et al. [15] which uses a very deep network (16 and 19 layers respectively) with smaller convolution filters of 3\u00d73 size to obtain stateof-the-art accuracies on the ImageNet 2014 Challenge. We use the fc7 layer of the VGG network as the features layer.\nGoogLeNet: This is an architecture used by Szegedy et\nal. [16], which uses several \u201cInception\u201d modules to create a deeper network with 22 layers while having much fewer parameters than other networks such as VGG and AlexNet. We use pool5/5x5 s1 layer of GoogLeNet as the features layer.\nPlaces: This is a network created by Zhou et al. [19]. It has the same architecture as AlexNet [7] but trained on the Places dataset instead of ImageNet to enable better performance in scene-centric tasks. We use the fc7 layer of Places as the features layer.\nNetwork In Network (NIN): This is a network architecture used by Lin et al. [8] which uses neural networks as the layer transfer function instead of a convolution followed by a non-linearity. We use the pool4 layer as the features."}, {"heading": "3. Datasets", "text": "Below, we describe the attributes of each of the datasets used for our study evaluation.\nCaltech-UCSD Birds 200-2011 [17]: This is a dataset of 200 different species of birds. The dataset consists of 11,788 images.\nCaltech256 [4]: This is a dataset of 256 object categories containing 30,607 images. The dataset is collected from Google images.\nFood-101 [2]: This is a dataset of 101 distinct food categories with 1,000 foods per category.\nLISA Traffic Sign Dataset [10]: This dataset contains 7,855 annotations on 6,610 video frames captured on US roads. Each image is labeled with the traffic signs visible on the images as well as the location of the sign. It covers 47 of the US traffic signs.\nMIT scene [13]: This is an indoor scene dataset with 15,620 images with 67 categories each of which containing at least 100 images.\nOxford flowers [12]: This is a collection of 102 groups of flowers each with 40 to to 256 flowers."}, {"heading": "4. Methodology", "text": "In this section, we formally define Stacked Neural Networks and discuss the studies we conducted to construct a novel deep learning framework for improving the state-ofthe-art prediction accuracy of deep neural networks (NN)."}, {"heading": "4.1. Feature Stacking", "text": "Stacked Neural Networks (S-NN) is defined as a combination of publicly available neural network architectures whose features are extracted at an intermediate layer of the network, and then concatenated together to form a larger feature set. Figure 2 illustrates this idea in detail. The concatenated feature vector is used to train a classifier layer which consists of an optional dropout layer, an affine layer and an SVM loss function. The impact of the dropout\nlayer will be discussed in detail in Section 5. While Figure 2 shows all five convolutional neural networks (CNN) as members of the S-NN, any combination of these CNN\u2019s is also considered a S-NN. For instance, {GoogLeNet, VGG16} and {NIN, Places, VGG19} are examples of a 2- network and 3-network S-NN\u2019s. We will evaluate the effect of different network combinations in Section 5 showing that S-NN\u2019s deliver higher classification accuracy than singlenetwork structures."}, {"heading": "4.2. Ensemble of S-NN\u2019s", "text": "It is shown in the literature [3] that an ensemble of multiple independently trained networks can improve the prediction accuracy by reducing the classification error rate. Each combination of S-NN\u2019s produces a new feature vector and a new set of scores. To further improve the classification accuracy, we studied the effect the ensemble mean of scores on the final network. Figure 3 shows a number of S-NN\u2019s whose scores are combined into an ensemble score. While any arbitrary group of S-NN\u2019s may be used to generate an ensemble score, we choose to compute this score by stacking a S-NN with all its network combination subsets. For example, given a S-NN containing three networks {NIN, VGG19, GoogLeNet}, we take the ensemble score of all its network subsets: {NIN}, {VGG19}, {GoogLeNet}, {NIN, VGG19}, {NIN, GoogLeNet}, {VGG19, GoogLeNet}, {NIN, VGG19}, {NIN, VGG19, GoogLeNet}. This method of forming en-\nsembles allows us to compare the performance of each SNN combination against other S-NN\u2019s as discussed in detail in Section 5, Figure 5."}, {"heading": "5. Results Discussion", "text": "In this section we analyze the impact of S-NN and ensemble of S-NN\u2019s. In doing so, we answer two key questions in this section:\n1. What is the impact of multiple networks on performance? In other words, does more networks necessarily mean more performance?\n2. What is the most generalizable S-NN architecture if we were to pick a combination of these five NN\u2019s?\nFigure 4 is aimed to answer question (1). It shows our best experimental results on all combinations of 1, 2, 3, 5 S-NN\u2019s for all datasets. Each S-NN combination was evaluated using 10\u2019s different hyperparameter sweeps (i.e. learning rate, regularization factor, number of epochs). The list of hyperparamters used in this work is included in Table 1. We evaluated the validation accuracy for single-network, 2- network, 3-network, and 5-network stack combinations. All 2-network experiments do not use the dropout layer while all 3-network experiments use the dropout layer (see Figure 2); we discovered dropout becomes an important training element once the number of networks is larger than two. For the case of five networks, however, we experimented the network performance with and without the dropout layer. Figure 4 points out for most networks, the case of 5 S-NN\nis favorable. It also indicates that the case of 2 S-NN is strongly competitive with 5 S-NN.\nFigure 5 is aimed to answer question (2). It shows all possible network combinations and represents their accuracy degradation compared to the best S-NN combination, which is 5-networks, for all datasets. This Figure indicates for a single-network case, GoogLeNet is the most generalizable CNN as it has the least mean and standard deviation in accuracy degradation. For the 2-network case, VGG16+GoogLeNet make the best network. This is a sensible choice because (1) the two network are among the best publicly available networks, and (2) they are constructed based on two different architectural assumptions, making them relatively uncorrelated from the misclassification behavior standpoint. For the 3-network case, VGG16+GoogLeNet+Places is the best S-NN. For the 4- network case, VGG16+VGG19+GoogLeNet+Places form the best classification choice indicating the poor generalization ability of the NIN CNN.\nNotice the case with 5 networks has relatively negligible accuracy superiority relative to the 2-network case, making two strong CNN\u2019s like GoogLeNet and VGG16 quite sufficient.\nFigure 6 focuses on extracting the best results obtained on each dataset under different experimental settings. It compares the single-network case with S-NN without dropout and S-NN with dropout training. It further shows the best accuracies for the single-network ensembles as well as the Stacked Ensemble model shown earlier. The singlenetwork ensemble refers to taking the mean of scores of individual networks in order to construct the final score. Interestingly enough, these results are more superior than the previous S-NN approach without score ensembles. To further improve this performance, the Stack Ensemble mode also includes the scores of S-NN architectures evaluated in Figure 4 in order to generate even more accurate classifications.\nThe black lines in Figure 6 show the state-of-the-art accuracies in each given dataset when data augmentation is applied and the dashed black lines show the state-of-theart accuracies without data augmentation [11, 2, 17, 4, 13]. Authors have not been able to identify any publications explicitly showing the state-of-the-art classification results for the LISA dataset. While we have not performed data augmentation as part of this study, we believe in doing so, we can surpass the state-of-the-art performance results even on datasets our results is below the solid line at the moment.\nFigure 7 shows the confusion matrices for the standalone classification performance of the GoogLeNet, Places, and VGG16 CNN\u2019s when running the MIT Scene dataset. To avoid clutter, this Figure illustrates 11 classes. Here, we draw a number of interesting insights from these results when comparing similar confusion cells in different matrices:\n\u2022 The BH misclassification happens 104 times in VGG16 and 96 times in GoogLeNet while only 32 times in Places. The combined S-NN only has 39 misclassifications. This shows the error correction power networks features in compensating for each other\u2019s weakness. This effect can also be observed in DI where the misclassification for the S-NN is zero while GoogLeNet misclassifies 8 images in this cell.\n\u2022 The IE misclassification happens 8 times in GoogLeNet and 0 times in Places and VGG16. Despite the presence of VGG16 and Places network, the S-NN confuses 5 images to this cell. This shows despite the presence of other networks to completely eliminate this type of image misclassification in S-NN, the score from GoogLeNet dominates the overall outcome more often. In Section 7.2, a potential solution to this problem is discussed.\n\u2022 The FB misclassification never happens to single-net cases. However, the stack of features shows 1 misclassification in this cell. While a negligible misclassification case, this shows the stack of NN features can have some adversarial effect in the final outcome. In Section 7.2, a potential solution to this problem is discussed."}, {"heading": "6. Joint Training of Multiple Tasks", "text": "As part of this work, we also looked at how the generalization of network features suffers as a result of finetuning a network on a given task. To perform these experiments, we considered three tasks from different domains: Food-101 (task A), MIT Scene (task B), and Caltech256 (task C). In\nall of these experiments, we used the VGG16 network. We describe our experiments and observations below.\nRecall from Section 1 that by fine-tuning we mean actually changing the network parameters, while by transfer learning we mean only using an SVM layer on top of a pretrained network."}, {"heading": "6.1. Generalization Loss by Fine-Tuning", "text": "In our first experiment, we investigated the generalization loss by network finetuning and ways to avoid this problem.\nTo show loss of generalizability by fine-tuning, we first fine-tuned VGG16 on task A to create the network VGG16A, and independently on task B to create a network VGG16B. Subsequently, we evaluated the transfer learning accuracy of task B on VGG16A. We compared this with the accuracy of VGG16B on task B. The results are summarized in Table 2.\nThis comparison shows that indeed fine-tuning VGG16 on task A reduced the transfer-learning accuracy on task B drastically (43.0% compared to 72.2%). Thus, an interesting question is whether it is possible to fine-tune a single network to give accuracies similar to the individually finetuned networks on both the tasks A and B?\nOur experiments show that the answer to this question is\n\u201cYes\u201d. Below we describe the methodology for achieving this result.\nThe network architecture is shown in Figure 8. We trained a single network with the concatenation of multiple dataset inputs in each minibatch. Each minibatch consisted of 4 images, 2 of which were from task A and 2 from task B. The output features from the network are then fed into two independent linear classifier layers, one for each of the tasks. The final loss is taken to be the sum of softmax losses from the two classifier layers. Let us denote this jointly trained network by VGG16AB.\nTable 2 shows the resulting accuracy from this approach. It can be seen that the accuracy of VGG16AB on task A (68.0%) is close to the accuracy of VGG16A (69.6%) while the accuracy on task B (70.6%) is close to VGG16B (72.2%). Thus, it can be inferred that it is indeed possible to gain the best of both VGG16A and VGG16B in a single network."}, {"heading": "6.2. Generalization of Jointly-Tuned networks", "text": "In the previous section, we saw that jointly tuning a network on two tasks can give accuracies comparable to individual finetuning. We also saw that individual finetuning destroys a lot of the generalization capabilities of the original network. These two facts bring up the next question: what is the generalization capability of a jointly tuned network?\nTo answer this question, we took a third task C (Caltech256 in our experiments), and evaluated the transfer learning accuracy of task C over VGG16, VGG16A and VGG16AB. The results are summarized in Table 3.\nIt can be seen that, indeed, the generalization capabilities of a jointly finetuned network are higher. Although the jointly finetuned network still does not reach the transfer learning capability of the baseline network alone, it does succeed in mitigating a lot of the degradation coming from individual finetuning. We suspect that the reason behind this phenomenon is that the jointly-finetuned network is forced to finetune towards more \u201cgeneral\u201d properties of the tasks rather than being extremely specific to one particular task.\nFrom these two experiments, we can conclude that joint finetuning is a promising direction to look into for finetuning networks without losing their generalization capabilities."}, {"heading": "7. Future Direction", "text": ""}, {"heading": "7.1. Parallel Training of Networks", "text": "As shown in [18], training neural networks at deeper layers can improve the classification accuracy. While the S-NN\u2019s, explained in the previous sections, are targeted toward agile training and high generalization accuracy, it is reasonable to hypothesize if they were trained together, their collective classification error would drop. So, instead of using the data for when each of these networks were pretrained independently, we allow the backward propagation training method to broadcast the classifier gradients to all networks. Since all networks iterate through the same set of gradients in parallel, they all influence each other\u2019s weight and bias values. Due to the limited computation capability available to us, we have been unable to generate results for this step. We will continue working on this scheme via once we have access to more powerful GPU machines."}, {"heading": "7.2. Weighted S-NN", "text": "Our analysis of the confusion matrices on the available datasets made us realize while the combination of network features in S-NN helps reduce errors, it is also the case that some networks introduce excessive confusion to the error rate of S-NN. To reduce such adversarial impacts of S-NN, we plan to combine features by applying weights to each network feature. The weight coefficients will be dependent\non the prediction accuracy of each network in classifying a given dataset. For instance, if GoogLetNet CNN shows weaker classification performance relative to the Places CNN, the relative contribution of GoogLeNet features will be reduced. To do so, the feature vector of each network is multiplied by a scalar value in [0, 1]. This value is computed by dividing the classification accuracy of each NN by the accuracy of the network with the best result. For instance, assume a S-NN consisting of {GoogLeNet, VGG16}. If GoogLeNet and VGG16 have individual classification accuracy of 0.3 and 0.6 respectively, the GoogLeNet and VGG features are multiplied by 0.5 and 1 respectively before stacking their features."}, {"heading": "7.3. Data Augmentation", "text": "Data augmentation has improved classification accuracy via diversifying NN features. Literatures [11, 2, 17] show substantial improvement in the MIT Scene, CUB 200, and Oxford Flowers datasets when their networks are trained using data augmentation. While we have been short in time to try this technique, we believe it will substantially boost our prediction accuracy on most datasets if not all."}, {"heading": "7.4. Parallel Wimpy Networks", "text": "Inspired by the notion of ensembles presented by Hinton et al. [6], this study proves combining multiple powerful networks leads to more substantial performance gains. It also proves training a single network on multiple datasets can deliver better generalization accuracy. The next milestone we would like to tackle is to evaluate the possibility of building numerous small, fast-to-train networks trained on multiple datasets and stacked as S-NN\u2019s. We call them a stack of wimpy neural networks. Such a technique is interesting to us from two fronts. First is to find if multiple wimpy S-NN\u2019s can do as well as (or better than) a powerful network like VGG19. Second is to find if this architecture can help reduce computation overhead demand of recent deep neural networks, such as VGG19 by enabling a much more parallelizable network architecture with the ability to be conveniently offloaded onto multiple computation units (i.e. CPU\u2019s or GPU\u2019s)."}, {"heading": "8. Conclusion", "text": "In this work, we presented Stacked Neural Networks, a novel technique in extracting higher generalization accuracy from the state-of-the-art neural networks in the public domain. We evaluated various NN stack combinations and discovered that while a five-CNN stack delivers the best accuracy, the stack of two CNN\u2019s can deliver similar accuracy gains while consuming much less computation power. We also presented the classification accuracy improvements of generating the ensemble of S-NN\u2019s. The combination of\nthese techniques enabled us to boost the classification accuracy beyond the state-of-the-art results presented in previous literature.\nFurthermore, we evaluated the effect of training multiple datasets on one network. Interestingly enough, we concluded that it is possible to jointly finetune a single network over multiple datasets and still obtain accuracies that are almost similar to individual finetuning of the networks. We also show that these jointly finetuned networks have better generalization capabilities than individually finetuned variants.\nS-NN proves the presence of fruitful impact in fostering collaborative neural network classification to improve generalization accuracy in transfer learning."}], "references": [{"title": "Food-101 \u2013 mining discriminative components with random forests", "author": ["L. Bossard", "M. Guillaumin", "L. Van Gool"], "venue": "European Conference on Computer Vision,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Caltech-256 object category dataset", "author": ["G. Griffin", "A. Holub", "P. Perona"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Dark knowledge", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "Lecture,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc.,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Network In Network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv:1312.4400 [cs], Dec.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep Captioning with Multimodal Recurrent Neural Networks (m- RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. Yuille"], "venue": "arXiv:1412.6632 [cs], Dec.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Visionbased traffic sign detection and analysis for intelligent driver assistance systems: Perspectives and survey", "author": ["A. Mogelmose", "M.M. Trivedi", "T.B. Moeslund"], "venue": "Intelligent Transportation Systems, IEEE Transactions on, 13(4):1484\u2013 1497,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "A visual vocabulary for flower classification", "author": ["M.-E. Nilsback", "A. Zisserman"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, volume 2, pages 1447\u20131454,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Automated flower classification over a large number of classes", "author": ["M.-E. Nilsback", "A. Zisserman"], "venue": "Computer Vision, Graphics & Image Processing, 2008. ICVGIP\u201908. Sixth Indian Conference on, pages 722\u2013729. IEEE,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Recognizing indoor scenes", "author": ["A. Quattoni", "A. Torralba"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Computer Vision and Pattern Recognition Work- 7  shops (CVPRW), 2014 IEEE Conference on, pages 512\u2013519. IEEE,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv:1409.1556 [cs], Sept.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv:1409.4842 [cs], Sept.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Learning Deep Features for Scene Recognition using Places Database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "NIPS,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "[14] have shown that transfer learning on a pre-trained neural network can outperform traditional hand-tuned approaches in several tasks including coarse-grained detection, fine-grained detection and attribute detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "These include networks such as VGG [15], GoogLeNet [5], Places [19], and NIN [8].", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "These include networks such as VGG [15], GoogLeNet [5], Places [19], and NIN [8].", "startOffset": 63, "endOffset": 67}, {"referenceID": 4, "context": "These include networks such as VGG [15], GoogLeNet [5], Places [19], and NIN [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "However, in some tasks such as Image-to-Sentence retrieval [9], a set of features is desirable instead of a score over some set of predefined classes.", "startOffset": 59, "endOffset": 62}, {"referenceID": 11, "context": "[15] which uses a very deep network (16 and 19 layers respectively) with smaller convolution filters of 3\u00d73 size to obtain stateof-the-art accuracies on the ImageNet 2014 Challenge.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[16], which uses several \u201cInception\u201d modules to create a deeper network with 22 layers while having much fewer parameters than other networks such as VGG and AlexNet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "It has the same architecture as AlexNet [7] but trained on the Places dataset instead of ImageNet to enable better performance in scene-centric tasks.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "[8] which uses neural networks as the layer transfer function instead of a convolution followed by a non-linearity.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Caltech256 [4]: This is a dataset of 256 object categories containing 30,607 images.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "Food-101 [2]: This is a dataset of 101 distinct food categories with 1,000 foods per category.", "startOffset": 9, "endOffset": 12}, {"referenceID": 6, "context": "LISA Traffic Sign Dataset [10]: This dataset contains 7,855 annotations on 6,610 video frames captured on US roads.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "MIT scene [13]: This is an indoor scene dataset with 15,620 images with 67 categories each of which containing at least 100 images.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "Oxford flowers [12]: This is a collection of 102 groups of flowers each with 40 to to 256 flowers.", "startOffset": 15, "endOffset": 19}, {"referenceID": 7, "context": "The black lines in Figure 6 show the state-of-the-art accuracies in each given dataset when data augmentation is applied and the dashed black lines show the state-of-theart accuracies without data augmentation [11, 2, 17, 4, 13].", "startOffset": 210, "endOffset": 228}, {"referenceID": 0, "context": "The black lines in Figure 6 show the state-of-the-art accuracies in each given dataset when data augmentation is applied and the dashed black lines show the state-of-theart accuracies without data augmentation [11, 2, 17, 4, 13].", "startOffset": 210, "endOffset": 228}, {"referenceID": 1, "context": "The black lines in Figure 6 show the state-of-the-art accuracies in each given dataset when data augmentation is applied and the dashed black lines show the state-of-theart accuracies without data augmentation [11, 2, 17, 4, 13].", "startOffset": 210, "endOffset": 228}, {"referenceID": 9, "context": "The black lines in Figure 6 show the state-of-the-art accuracies in each given dataset when data augmentation is applied and the dashed black lines show the state-of-theart accuracies without data augmentation [11, 2, 17, 4, 13].", "startOffset": 210, "endOffset": 228}, {"referenceID": 13, "context": "As shown in [18], training neural networks at deeper layers can improve the classification accuracy.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "Literatures [11, 2, 17] show substantial improvement in the MIT Scene, CUB 200, and Oxford Flowers datasets when their networks are trained using data augmentation.", "startOffset": 12, "endOffset": 23}, {"referenceID": 0, "context": "Literatures [11, 2, 17] show substantial improvement in the MIT Scene, CUB 200, and Oxford Flowers datasets when their networks are trained using data augmentation.", "startOffset": 12, "endOffset": 23}, {"referenceID": 2, "context": "[6], this study proves combining multiple powerful networks leads to more substantial performance gains.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "It has been proven that transfer learning provides an easy way to achieve state-of-the-art accuracies on several vision tasks by training a simple classifier on top of features obtained from pre-trained neural networks. The goal of this project is to generate better features for transfer learning from multiple publicly available pretrained neural networks. To this end, we propose a novel architecture called Stacked Neural Networks which leverages the fast training time of transfer learning while simultaneously being much more accurate. We show that using a stacked NN architecture can result in up to 8% improvements in accuracy over state-of-the-art techniques using only one pre-trained network for transfer learning. A second aim of this project is to make network finetuning retain the generalizability of the base network to unseen tasks. To this end, we propose a new technique called \u201cjoint finetuning\u201d that is able to give accuracies comparable to finetuning the same network individually over two datasets. We also show that a jointly finetuned network generalizes better to unseen tasks when compared to a network finetuned over a single task.", "creator": "LaTeX with hyperref package"}}}