{"id": "1605.08636", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2016", "title": "PAC-Bayesian Theory Meets Bayesian Inference", "abstract": "tables exhibit a strong link between frequentist pac - bayesian bounds and optimal bayesian empirical likelihood. that mean, taking objective negative quasi - term growth model, experiments show it the minimization and post - bayesian generalization bounds maximizes the bayesian marginal likelihood. evaluation provides an explicit cause could manipulate bayesian code's search criteria, under sufficient assumption that objective conclusions are guided by a we. i. m. distribution. third, upon calculating negative log - likelihood is an unbounded optimization function, we motivate mathematicians propose objective utility - bayesian theorem tailored for the sub - gamma loss paradigm, and authors show that simple approach is secure beside promising quantitative linear regression tasks.", "histories": [["v1", "Fri, 27 May 2016 13:41:33 GMT  (525kb,D)", "http://arxiv.org/abs/1605.08636v1", "under review"], ["v2", "Tue, 1 Nov 2016 14:49:05 GMT  (530kb,D)", "http://arxiv.org/abs/1605.08636v2", "To appear at NIPS 2016"], ["v3", "Sat, 3 Dec 2016 22:48:15 GMT  (529kb,D)", "http://arxiv.org/abs/1605.08636v3", "To appear at NIPS 2016 (revised version)"], ["v4", "Mon, 13 Feb 2017 17:14:52 GMT  (530kb,D)", "http://arxiv.org/abs/1605.08636v4", "Published at NIPS 2015 (this http URL)"]], "COMMENTS": "under review", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["pascal germain", "francis r bach", "alexandre lacoste", "simon lacoste-julien"], "accepted": true, "id": "1605.08636"}, "pdf": {"name": "1605.08636.pdf", "metadata": {"source": "CRF", "title": "PAC-Bayesian Theory Meets Bayesian Inference", "authors": ["Pascal Germain", "Francis Bach", "Alexandre Lacoste", "Simon Lacoste-Julien"], "emails": ["firstname.lastname@inria.fr", "allac@google.com"], "sections": [{"heading": "1 Introduction", "text": "Since its early beginning [Shawe-Taylor and Williamson, 1997], the PAC-Bayesian theory claims to provide \u201cPAC guarantees to Bayesian algorithms\u201d [McAllester, 1999]. However, despite the amount of work dedicated to this statistical learning theory\u2014many authors improved the initial results1 and/or generalized them for various machine learning setups2\u2014it is mostly used as a frequentist method. That is, under the assumptions that the learning samples are i.i.d.-generated by a data-distribution, this theory expresses probably approximately correct (PAC) bounds on the generalization risk. In other words, with probability 1\u2212\u03b4, the generalization risk is at most \u03b5 away from the training risk. The Bayesian side of PAC-Bayes comes mostly from the fact that these bounds are expressed on the averaging/aggregation/ensemble of multiple predictors (weighted by a posterior distribution) and incorporate prior knowledge. Although it is still sometimes referred as a theory that bridges the Bayesian and frequentist approach [e.g., Guyon et al., 2010], it has been merely used to explicitly justify Bayesian methods until now.3\nIn this work, we provide (up to our knowledge) the first direct connection between Bayesian inference techniques [summarized by Ghahramani, 2015] and PAC-Bayesian theory in a general setup. Our study is based on a simple but insightful connection between the Bayesian marginal likelihood and PAC-Bayesian bounds, that we obtain by considering the negative log-likelihood loss function (Section 3). By doing so, we provide an alternative explanation for the Bayesian Occam\u2019s razor criteria [Jeffreys and Berger, 1992, MacKay, 1992] in the context of model selection, explained as the complexity-accuracy trade-off appearing in most PAC-Bayesian results. In Section 4, we extend PAC-Bayes theorems to regression problems with unbounded loss, adapted to the negative log likelihood loss function. Finally, we study the Bayesian model selection from a PAC-Bayesian perspective (Section 5), and illustrate our finding on classical Bayesian regression tasks (Section 6).\n1Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc. 2Langford and Shawe-Taylor [2002], Seldin and Tishby [2010], Seldin et al. [2011, 2012], B\u00e9gin et al. [2014], Pentina and Lampert [2014], etc. 3Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Gr\u00fcnwald and Langford [2007], Lacoste-Julien et al. [2011] for other studies drawing links between frequentist statistics and Bayesian inference.\nar X\niv :1\n60 5.\n08 63\n6v 1\n[ st\nat .M\nL ]\n2 7\nM ay\n2 01"}, {"heading": "2 PAC-Bayesian Theory", "text": "We denote the learning sample (X,Y )={(xi, yi)}ni=1\u2208(X\u00d7Y)n, that contains n input-output pairs. The main assumption of frequentist learning theories\u2014including PAC-Bayes\u2014is that (X,Y ) is randomly sampled from a data generating distribution that we denoteD. Thus, we denote (X,Y )\u223cDn the i.i.d. observation of n elements. From a frequentist perspective, we consider in this work loss functions ` : F\u00d7X\u00d7Y \u2192 R, where F is a (discrete or continuous) set of predictors f : X \u2192 Y , and we write the empirical risk on the sample (X,Y ) and the generalization error on distribution D as\nL\u0302 `X,Y (f) = 1\nn n\u2211 i=1 `(f, xi, yi) ; L `D(f) = E (x,y)\u223cD `(f, x, y) .\nThe PAC-Bayesian theory [McAllester, 1999, 2003] studies an averaging of the above losses according to a posterior distribution \u03c1\u0302 overF . That is, it provides probably approximately correct generalization bounds on the (unknown) quantity Ef\u223c\u03c1\u0302 L `D(f) = Ef\u223c\u03c1\u0302 E(x,y)\u223cD `(f, x, y) , given the empirical estimate Ef\u223c\u03c1\u0302 L\u0302 `X,Y (f) and some other parameters. Among these, most PAC-Bayesian theorems rely on the Kullback-Leibler divergence KL(\u03c1\u0302\u2016\u03c0) = Ef\u223c\u03c1\u0302 ln[\u03c1\u0302(f)/\u03c0(f)] between a prior distribution \u03c0 over F\u2014specified before seeing the learning sample X,Y\u2014and the posterior \u03c1\u0302\u2014typically obtained by feeding a learning process with X,Y .\nTwo appealing aspects of PAC-Bayesian theorems are that they provide data-driven generalization bounds that are computed on the training sample (i.e., they do not rely on a testing sample) and that are uniformly valid for all \u03c1\u0302 over F . This explains why many works study them as model selection criteria or as an inspiration for learning algorithm conception. Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al. [2013], Noy and Crammer [2014]. Theorem 1 (Catoni, 2007). Given a distribution D over X \u00d7 Y , a hypothesis set F , a loss function `\u2032 : F \u00d7 X \u00d7 Y \u2192 [0, 1], a prior distribution \u03c0 over F , a \u03b4 \u2208 (0, 1], and a real number \u03b2 > 0, with probability at least 1\u2212 \u03b4 over the choice of (X,Y ) \u223c Dn, we have\n\u2200\u03c1\u0302 on F : E f\u223c\u03c1\u0302 L ` \u2032 D (f) \u2264 1 1\u2212 e\u2212\u03b2\n[ 1\u2212 e\u2212\u03b2 Ef\u223c\u03c1\u0302 L\u0302 `\u2032 X,Y (f)\u2212 1 n ( KL(\u03c1\u0302\u2016\u03c0)+ ln 1\u03b4 )] . (1)\nTheorem 1 is limited to loss functions mapping to the range [0, 1]. Through a straightforward rescaling we can extend it to any bounded loss, i.e., ` : F \u00d7X \u00d7Y \u2192 [a, b], where [a, b] \u2282 R. This is done by using \u03b2 := b\u2212 a and with the rescaled loss function `\u2032(f, x, y) := (`(f, x, y)\u2212a)/(b\u2212a) \u2208 [0, 1] . After few arithmetic manipulations, we can rewrite Equation (1) as\n\u2200\u03c1\u0302 on F : E f\u223c\u03c1\u0302 L `D(f) \u2264 a+ b\u2212a1\u2212ea\u2212b\n[ 1\u2212 exp ( \u2212E f\u223c\u03c1\u0302 L\u0302 `X,Y (f)+a\u2212 1n ( KL(\u03c1\u0302\u2016\u03c0)+ ln 1\u03b4 ))] . (2)\nFrom an algorithm design perspective, Equation (2) suggests optimizing a trade-off between the empirical expected loss and the Kullback-Leibler divergence. Indeed, for fixed \u03c0, X , Y , n, and \u03b4, minimizing Equation (2) is equivalent to find the distribution \u03c1\u0302 that minimizes\nn E f\u223c\u03c1\u0302 L\u0302 `X,Y (f) + KL(\u03c1\u0302\u2016\u03c0) . (3)\nAs mentioned by Zhang [2006], Catoni [2007], Germain et al. [2009], Lever et al. [2013], Alquier et al. [2015], the optimal Gibbs posterior \u03c1\u0302\u2217 is given by\n\u03c1\u0302\u2217(f) = 1ZX,Y \u03c0(f) e \u2212n L\u0302 `X,Y (f) , (4)\nwhere ZX,Y is a normalization term. Notice that the constant \u03b2 is now absorbed in the loss function as the rescaling factor setting the trade-off between the expected empirical loss and KL(\u03c1\u0302\u2016\u03c0)."}, {"heading": "3 Bridging Bayes and PAC-Bayes", "text": "In this section, we show that by choosing the negative-log-likelihood loss function, minimizing the PAC-Bayes bound is equivalent to maximizing the Bayesian marginal likelihood. To obtain this\nresult, we first consider the Bayesian approach that starts by defining a prior p(\u03b8) over the set of possible model parameters \u0398. This induces a set of probabilistic estimators f\u03b8 \u2208 F , mapping x to a probability distribution over Y . Then, we can estimate the likelihood of observing y given x and \u03b8, i.e., p(y|x, \u03b8) \u2261 f\u03b8(y|x).4 Using Bayes\u2019 rule, we obtain the posterior p(\u03b8|X,Y ):\np(\u03b8|X,Y ) = p(\u03b8) p(Y |X, \u03b8) p(Y |X) \u221d p(\u03b8) p(Y |X, \u03b8) , (5)\nwhere p(Y |X, \u03b8) = \u220fn i=1 p(yi|xi, \u03b8) and p(Y |X) = E\u03b8\u223cp(\u03b8) p(Y |X, \u03b8).\nTo bridge the Bayesian approach with the PAC-Bayesian framework, we consider the negative log-likelihood loss function [see Banerjee, 2006], denoted `nll and defined by\n`nll(f\u03b8, x, y) \u2261 \u2212 ln p(y|x, \u03b8) . (6)\nThen, we can relate the empirical loss L\u0302 `X,Y of a predictor to its likelihood:\nL\u0302 `nllX,Y (\u03b8) = 1\nn n\u2211 i=1 `nll(\u03b8, xi, yi) = \u2212 1 n n\u2211 i=1 ln p(yi|xi, \u03b8) = \u2212 1 n ln p(Y |X, \u03b8) ,\nor, the other way around, p(Y |X, \u03b8) = e\u2212nL\u0302 `nll X,Y (\u03b8). (7)\nUnfortunately, existing PAC-Bayesian theorems work with bounded loss functions or in very specific context [as Zhang, 2006, Dalalyan and Tsybakov, 2008], and `nll spans the whole real axis in its general form. To this end, in Section 4, we explore PAC-Bayes bounds for unbounded losses. Meanwhile, we consider priors with bounded likelihood. This can be done by assigning a prior of zero to any \u03b8 yielding \u2212 ln p(y|x, \u03b8) /\u2208 [a, b]. Now, using Equation (7) in the optimal posterior (Equation 4) simplifies to:\n\u03c1\u0302\u2217(\u03b8) = \u03c0(\u03b8) e\u2212n L\u0302\n`nll X,Y (\u03b8)\nZX,Y = p(\u03b8) p(Y |X, \u03b8) p(Y |X) = p(\u03b8|X,Y ) , (8)\nwhere the normalization constant ZX,Y corresponds to the Bayesian marginal likelihood: ZX,Y \u2261 p(Y |X) = \u222b\n\u0398\n\u03c0(\u03b8) e\u2212n L\u0302 `nll X,Y (\u03b8)d\u03b8 . (9)\nThis shows that the optimal PAC-Bayes posterior given by the generalization bound of Theorem 1 coincides with the Bayesian posterior, when one chooses `nll as loss function and \u03b2 := b\u2212a (as in Equation 2). Moreover, using the posterior of Equation (8) inside Equation (3), we obtain\nn E \u03b8\u223c\u03c1\u0302\u2217\nL\u0302 `nllX,Y (\u03b8) + KL(\u03c1\u0302 \u2217\u2016\u03c0) (10)\n= n \u222b \u0398 \u03c0(\u03b8) e \u2212n L\u0302 `nll X,Y (\u03b8) ZX,Y L\u0302 `nllX,Y (\u03b8) d\u03b8 + \u222b \u0398 \u03c0(\u03b8) e \u2212n L\u0302 `nll X,Y (\u03b8) ZX,Y ln [ \u03c0(\u03b8) e \u2212n L\u0302 `nll X,Y (\u03b8) \u03c0(\u03b8)ZX,Y ] d\u03b8\n= \u222b \u0398 \u03c0(\u03b8) e \u2212n L\u0302 `nll X,Y (\u03b8) ZX,Y [ ln 1ZX,Y ] d\u03b8 = ZX,Y ZX,Y ln 1ZX,Y = \u2212 lnZX,Y .\nIn other words, minimizing the PAC-Bayes bound is equivalent to maximizing the marginal likelihood. Thus, from the PAC-Bayesian standpoint, the latter encodes a trade-off between the averaged negative log-likelihood loss function and the prior-posterior Kullback-Leibler divergence.5 Although it appears in essence a very different problem, we note that the relation derived in Equation (10) is similar to the one used by variational Bayesian methods, which approximate a hardly computable Bayesian posterior by the \u201cclosest\u201d distribution belonging to a parametrized family.\nWe conclude this section by proposing a compact form of Theorems 1 by expressing it in terms of the marginal likelihood, as a direct consequence of Equation (10).\n4To stay aligned with the PAC-Bayesian setup, we only consider the discriminative case in this paper. One can extend to the generative setup by considering the likelihood of the form p(y, x|\u03b8) instead.\n5To our knowledge, this is the first time this has been reported in a general setup. The thesis of Seeger [2003, Section 3.2] foreseeing this by noticing that \u201cthe log marginal likelihood incorporates a similar trade-off as the PAC-Bayesian theorem\u201d, but using another variant of the PAC-Bayes bound and in the context of classification.\nCorollary 2. Given a data distribution D, a parameter set \u0398, a prior distribution \u03c0 over \u0398, a \u03b4 \u2208 (0, 1], if `nll lies in [a, b], we have, with probability at least 1\u2212 \u03b4 over the choice of (X,Y ) \u223c Dn,\nE \u03b8\u223c\u03c1\u0302\u2217\nL`nllD (\u03b8) \u2264 a+ b\u2212a 1\u2212ea\u2212b\n[ 1\u2212 ea n \u221a ZX,Y \u03b4 ] , (11)\nwhere \u03c1\u0302\u2217 is the Gibbs optimal posterior (Eq. 8) and ZX,Y is the marginal likelihood (Eq. 9).\nIn Section 5, we exploit the link between PAC-Bayesian bounds and Bayesian marginal likelihood to expose similarities between both frameworks in the context of model selection. Beforehand, next Section 4 extends the PAC-Bayesian generalization guarantees to unbounded loss function. This is mandatory to make our study fully valid, as the negative log-likelihood loss function is in general unbounded (as well as other common regression losses)."}, {"heading": "4 PAC-Bayesian Bounds for Regression", "text": "This section aims to extend the PAC-Bayesian results of Section 3 to real valued unbounded loss. These result are used in forthcoming sections to study `nll, but they are valid for broader classes of loss functions. Importantly, our new results are focused on regression problems, as opposed to the usual PAC-Bayesian classification framework.\nThe new bounds are obtained through a recent theorem of Alquier et al. [2015], stated below (we provide a proof in Appendix A.1 for completeness). Theorem 3 (Alquier et al. [2015]). Given a distribution D over X \u00d7 Y , a hypothesis set F , a loss function ` : F \u00d7 X \u00d7 Y \u2192 R, a prior distribution \u03c0 over F , a \u03b4 \u2208 (0, 1], and a real number \u03bb > 0, with probability at least 1\u2212\u03b4 over the choice of (X,Y ) \u223c Dn, we have\n\u2200\u03c1\u0302 on F : E f\u223c\u03c1\u0302 L `D(f) \u2264 E f\u223c\u03c1\u0302 L\u0302 `X,Y (f) +\n1\n\u03bb\n[ KL(\u03c1\u0302\u2016\u03c0) + ln 1\n\u03b4 + \u03a8`,\u03c0,D(\u03bb, n)\n] , (12)\nwhere \u03a8`,\u03c0,D(\u03bb, n) = ln E f\u223c\u03c0 E X\u2032,Y \u2032\u223cDn\nexp [ \u03bb ( L `D(f)\u2212 L\u0302 `X\u2032,Y \u2032(f) )] . (13)\nAlquier et al. [2015] used Theorem 3 to design a learning algorithm for {0, 1}-valued classification losses. Indeed, a bounded loss function ` : F \u00d7 X \u00d7 Y \u2192 [a, b] can be used along with Theorem 3 by applying the Hoeffding\u2019s lemma to Equation (13), that gives \u03a8`,\u03c0,D(\u03bb, n) \u2264 \u03bb2(b\u2212a)2/(2n). More specifically, with \u03bb := n, we obtain the following bound\n\u2200\u03c1\u0302 on F : E f\u223c\u03c1\u0302 L `D(f) \u2264 E f\u223c\u03c1\u0302 L\u0302 `X,Y (f) + 1n\n[ KL(\u03c1\u0302\u2016\u03c0) + ln 1\u03b4 ] + 12 (b\u2212 a) 2. (14)\nNote that the latter bound leads to the same trade-off as Theorem 1 (expressed by Equation 3). However, the choice \u03bb := n has the inconvenience that the bound value is at least 12 (b\u2212 a)\n2, even at the limit n\u2192\u221e. Note that another choice that makes the bound converge is \u03bb := \u221a n:\n\u2200\u03c1\u0302 on F : E f\u223c\u03c1\u0302 L `D(f) \u2264 E f\u223c\u03c1\u0302 L\u0302 `X,Y (f) + 1\u221an\n[ KL(\u03c1\u0302\u2016\u03c0) + ln 1\u03b4 + 1 2 (b\u2212 a) 2 ] . (15)\nA similar result to Equation (15) leads to long-life learning algorithms in Pentina and Lampert [2014].\nSub-Gaussian losses. In a regression context, it may be restrictive to consider strictly bounded loss functions. Therefore, we extend Theorem 3 to sub-Gaussian loss functions. We say that an unbounded loss function ` is sub-Gaussian with a variance factor s2 under a prior \u03c0 and a data-distribution D if it can be described by a sub-Gaussian random variable V , i.e., its moment generating function is upper bounded by the one of a normal distribution of variance s2 [see Boucheron et al., 2013, Section 2.3]:\n\u03c8 V\n(\u03bb) = ln E exp [ \u03bb(V \u2212EV ) ] \u2264 \u03bb 2s2\n2 , \u2200\u03bb \u2208 R , (16)\nThe above sub-Gaussian assumption corresponds to the Hoeffding assumption of Alquier et al. [2015], and allows to obtain the following result. Corollary 4. Given D, F , `, \u03c0 and \u03b4 defined in Theorem 3 statement, if the loss is sub-Gaussian with variance factor s2, we have, with probability at least 1\u2212\u03b4 over the choice of (X,Y ) \u223c Dn,\n\u2200\u03c1\u0302 on F : E f\u223c\u03c1\u0302 L `D(f) \u2264 E f\u223c\u03c1\u0302 L\u0302 `X,Y (f) + 1n\n[ KL(\u03c1\u0302\u2016\u03c0) + ln 1\u03b4 ] + 12 s 2.\nProof. For i = 1 . . . n, we denote `i a i.i.d. realization of the random variable `(f, x, y)\u2212L\u0302 `X\u2032,Y \u2032(f). \u03a8`,\u03c0,D(\u03bb, n) = ln E exp [ \u03bb n \u2211n i=1 `i ] = ln \u220fn i=1 E exp [ \u03bb n`i ] = \u2211n i=1 \u03c8`i( \u03bb n ) \u2264 n \u03bb2s2 2n2 = \u03bb2s2 2n ,\nwhere the inequality comes from the sub-Gaussian loss assumption (Equation 16). The result is then obtained from Theorem 3, with \u03bb := n.\nSub-Gamma losses. We say that an unbounded loss function ` is sub-Gamma with a variance factor s2 and scale parameter c, under a prior \u03c0 and a data-distribution D, if it can be described by a re-centered sub-Gamma random variable V\u2212EV [see Boucheron et al., 2013, Section 2.4], that is\n\u03c8 V\n(\u03bb) \u2264 s 2 c2 (\u2212 ln(1\u2212\u03bbc)\u2212 \u03bbc) \u2264 \u03bb2s2 2(1\u2212c\u03bb) , \u2200\u03bb \u2208 (0, 1 c ) .\nUnder this sub-Gamma assumption, we obtain the following new result, which is necessary to study the linear regression in next sections. Corollary 5. Given D, F , `, \u03c0 and \u03b4 defined in Theorem 3 statement, if the loss is sub-Gamma with variance factor s2 and scale c < 1, we have, with probability at least 1\u2212\u03b4 over (X,Y ) \u223c Dn,\n\u2200\u03c1\u0302 on F : E f\u223c\u03c1\u0302 L `D(f) \u2264 E f\u223c\u03c1\u0302 L\u0302 `X,Y (f) + 1n\n[ KL(\u03c1\u0302\u2016\u03c0) + ln 1\u03b4 ] + 12(1\u2212c) s 2 . (17)\nAs a special case, with ` := `nll and \u03c1\u0302 := \u03c1\u0302\u2217 (Eq. 8), we have E \u03b8\u223c\u03c1\u0302\u2217 L`nllD (\u03b8) \u2264 s2 2(1\u2212c) \u2212 1 n ln (ZX,Y \u03b4) .\nProof. Following the same path as in Corollary 4 proof (with \u03bb := n), we have \u03a8`,\u03c0,D(n, n) = ln E exp [ \u2211n i=1 `i] = ln \u220fn i=1 E exp [`i] = \u2211n i=1 \u03c8`i(1) \u2264 n s2 2(1\u2212c) = n s2 2(1\u2212c) ,\nwhere the inequality comes from the sub-Gamma loss assumption, with 1 \u2208 (0, 1c ).\nSquared loss. The parameters s2 and c of Corollary 5 rely on the chosen loss function and prior, and the assumptions concerning the data distribution. As an example, consider a regression problem where X \u00d7Y = Rd \u00d7R, a family of linear predictors fw(x) = w \u00b7 x, with w \u2208 Rd, and a Gaussian prior \u03c0 \u223c N (0, \u03c32\u03c0). Let assume that the input examples lie inside a ball of radius \u03b3 and the label of x is given by y = w\u2217 \u00b7 x + , where \u223c N (0, \u03c32 ) is a Gaussian noise. Under the squared loss function `sqr(fw,x, y) = (w \u00b7 x \u2212 y)2, we show in Appendix A.3 that Corollary 5 is valid with s2 \u2265 2\u2016w\u2217\u20162\u03b32 and c \u2264 2(\u03c32 + \u03c32\u03c0\u03b32)2. The latter term tells us that the bound degrades when the noise increases, as expected. Empirical values of the bound under the squared loss are computed in Section 6.\nRegression versus classification. The classical PAC-Bayesian theorems are stated in a classification context and bound the generalization error/loss of the stochastic Gibbs predictor G\u03c1\u0302. In order to predict the label of an example x \u2208 X , the Gibbs predictor first draws a hypothesis h \u2208 F according to \u03c1\u0302, and then returns h(x). Maurer [2004] shows that we can generalize PAC-Bayesian bounds on the generalization risk of the Gibbs classifier to any loss function with output between zero and one. Provided that y \u2208 {\u22121, 1} and h(x) \u2208 [\u22121, 1], a common choice is to use the linear loss function `\u203201(h, x, y) = 1 2 \u2212 1 2y h(x). The Gibbs generalization loss is then given by RD(G\u03c1\u0302) = E(x,y)\u223cD Eh\u223c\u03c1\u0302 ` \u2032 01(h, x, y) . Many PAC-Bayesian works use RD(G\u03c1\u0302) as a surrogate loss to study the zero-one classification loss of the majority vote classifier RD(B\u03c1\u0302):\nRD(B\u03c1\u0302) = Pr (x,y)\u223cD ( y E h\u223c\u03c1\u0302 h(x) < 0 ) = E (x,y)\u223cD I [ y E h\u223c\u03c1\u0302 h(x) < 0 ] , (18)\nwhere I[\u00b7] being the indicator function. Given a distribution \u03c1\u0302, an upper bound on the Gibbs risk is converted on an upper bound on the majority vote risk byRD(B\u03c1\u0302) \u2264 2RD(G\u03c1\u0302) [Langford and ShaweTaylor, 2002]. In some situations, this factor of two may be reached, i.e., RD(B\u03c1\u0302) ' 2RD(G\u03c1\u0302). In other situations, we may have RD(G\u03c1\u0302) = 0 even if RD(G\u03c1\u0302) = 12\u2212 (see Germain et al. [2015] for an extensive study). Indeed, these bounds obtained via the Gibbs risk are exposed to be loose and/or unrepresentative of the majority vote generalization error.6\n6It is noteworthy that the best PAC-Bayesian empirical bound values are so far obtained by considering a majority vote of linear classifiers, where prior and posterior are Gaussian [Langford and Shawe-Taylor, 2002, Ambroladze et al., 2006, Germain et al., 2009], similarly to the Bayesian linear regression analyzed in Section 6.\nIn the current work, we study regression losses instead of classification ones. That is, the provided results express upper bounds on Ef\u223c\u03c1\u0302 L `D(f) for any (bounded, sub-Gaussian, or sub-Gamma) losses. Of course, one may want to bound the regression loss of the averaged regressor F\u03c1\u0302(x) = Ef\u223c\u03c1\u0302 f(x). In this case, if the loss function ` is convex (as the squared loss), Jensen\u2019s inequality gives L `D(F\u03c1\u0302) \u2264 Ef\u223c\u03c1\u0302 L `D(f) . Note that a strict inequality replaces the factor two mentioned above for the classification case, due to the non-convex indicator function of Equation (18).\nNow that we state generalization bounds for real-valued loss functions, we can continue our study linking PAC-Bayesian results to Bayesian inference. In next section, we focus on model selection."}, {"heading": "5 Analysis of Model Selection", "text": "Let consider L distinct models {Mi}Li=1, each one defined by a set of parameters \u0398i. The PACBayesian theorems naturally suggest selecting the model that is best adapted for the given task by evaluating the bound for each model {Mi}Li=1 and selecting the one with the lowest bound [McAllester, 2003, Ambroladze et al., 2006, Zhang, 2006]. This is closely linked with the Bayesian model selection procedure, as we showed in Section 3 that minimizing the PAC-Bayes bound amounts to maximizing the marginal likelihood. Indeed, given a collection of L optimal Gibbs posteriors\u2014one for each mode\u2014given by Equation (8),\np(\u03b8|X,Y,Mi) \u2261 \u03c1\u0302\u2217i (\u03b8) = 1ZX,Y,i\u03c0i(\u03b8) e n L\u0302 `nllX,Y (\u03b8), for \u03b8 \u2208 \u0398i , (19)\nthe Bayesian Occam\u2019s razor criteria [Jeffreys and Berger, 1992, MacKay, 1992] chooses the one with the higher model evidence\np(Y |X,Mi) \u2261 ZX,Y,i = \u222b\n\u0398i\n\u03c0i(\u03b8) e \u2212nL\u0302 `X,Y (\u03b8) d\u03b8 . (20)\nCorollary 6 below formally links the PAC-Bayesian and the Bayesian model selection. To obtain this result, we simply use the bound of Corollary 5 L times, together with `nll and Equation (10). From the union bound (a.k.a. Bonferroni inequality), it is mandatory to compute each bound with a confidence parameter of \u03b4/L, to ensure that the final conclusion is valid with probability at least 1\u2212\u03b4. Corollary 6. Given a data distribution D, a family of model parameters {\u0398i}Li=1 and associated priors {\u03c0i}Li=1\u2014where \u03c0i is defined over \u0398i\u2014 , a \u03b4 \u2208 (0, 1], if the loss is sub-Gamma with parameters s2 and c < 1, then, with probability at least 1\u2212 \u03b4 over (X,Y ) \u223c Dn,\n\u2200i \u2208 {1, . . . , L} : E \u03b8\u223c\u03c1\u0302\u2217i L`nllD (\u03b8) \u2264 1 2(1\u2212c) s 2 \u2212 1n ln\n( ZX,Y,i \u03b4 L ) .\nwhere \u03c1\u0302\u2217i is the Gibbs optimal posterior (Eq. 19) and ZX,Y,i is the marginal likelihood (Eq. 20).\nHence, under the uniform prior over the L models, choosing the one with the best model evidence is equivalent to choosing the one with the lowest PAC-Bayesian bound.\nHierarchical Bayes. To perform proper inference of hyperparameters, we have to rely on the Hierarchical Bayes approach. This is done by considering an hyperprior p(\u03b7) over the set of hyperparameters H. Then, the prior p(\u03b8|\u03b7) can be conditioned on a choice of hyperparameter \u03b7. The Bayesian rule of Equation (5) becomes p(\u03b8, \u03b7|X,Y ) = p(\u03b7) p(\u03b8|\u03b7) p(Y |X,\u03b8)p(Y |X) .\nUnder the negative log-likelihood loss function, we can rewrite the results of Corollary 2 as a generalization bounds on E\u03b7\u223c\u03c1\u03020 E\u03b8\u223c\u03c1\u0302\u2217\u03b7 L `nll D (\u03b8), where \u03c1\u03020(\u03b7) \u221d \u03c00(\u03b7)ZX,Y,\u03b7 is the hyperposterior on H and \u03c00 the hyperprior. Indeed, Equation (11) becomes\nE \u03b8\u223c\u03c1\u0302\u2217 L`nllD (\u03b8) = E \u03b7\u223c\u03c1\u0302\u22170 E \u03b8\u223c\u03c1\u0302\u2217\u03b7 L`nllD (\u03b8) \u2264 1 2(1\u2212c) s 2 \u2212 1n ln\n( E\n\u03b7\u223c\u03c00 ZX,Y,\u03b7 \u03b4\n) . (21)\nTo relate to the bound obtained in Corollary 6, we consider the case of a discrete hyperparameter set, H = {\u03b7i}Li=1, with a uniform prior. Then, Equation (21) becomes\nE \u03b8\u223c\u03c1\u0302\u2217 L`nllD (\u03b8) = E \u03b7\u223c\u03c1\u0302\u22170 E \u03b8\u223c\u03c1\u0302\u2217\u03b7 L`nllD (\u03b8) \u2264 1 2(1\u2212c) s 2 \u2212 1n ln\n(\u2211L i=1 ZX,Y,\u03b7i \u03b4 L ) .\nThis bound is now function of \u2211L i=1 ZX,Y,\u03b7i instead maxi ZX,Y,\u03b7i as in Corollary 6. This yields a tighter bound, corroborating the Bayesian wisdom that model averaging performs best.\nWhen selecting a single hyperparameter \u03b7\u2217, the hierarchical representation is equivalent to choosing a deterministic hyperposterior, satisfying \u03c1\u03020(\u03b7\u2217) = 1 and 0 for every other values. We then have\nKL(\u03c1\u0302||\u03c0) = KL(\u03c1\u03020||\u03c00) + E \u03b7\u223c\u03c1\u03020 KL(\u03c1\u0302\u03b7||\u03c0\u03b7) = ln(L) + KL(\u03c1\u0302\u03b7\u2217 ||\u03c0\u03b7\u2217) .\nWith the optimal posterior for the selected \u03b7\u2217, we have\nn E \u03b8\u223c\u03c1\u0302 L\u0302 `nllX,Y (\u03b8) + KL(\u03c1\u0302||\u03c0) = n E \u03b8\u223c\u03c1\u0302\u2217\u03b7 L\u0302 `nllX,Y (\u03b8) + KL(\u03c1\u0302 \u2217 \u03b7\u2217 ||\u03c0\u03b7\u2217) + ln(L) = \u2212 ln(ZX,Y,\u03b7\u2217) + ln(L) = \u2212 ln ( ZX,Y,\u03b7\u2217\nL\n) .\nInserting this result into Equation (17), we fall back on the bound obtained in Corollary 6. Hence, by comparing the values of the bounds one can get an estimate on the consequence of performing model selection instead of model averaging."}, {"heading": "6 Linear Regression", "text": "In this section, we perform Bayesian linear regression using the parameterization of Bishop [2006]. The output space is Y := R and, for an arbitrary input spaceX , we use a mapping function\u03c6 :X\u2192Rd.\nThe model. Given (x, y) \u2208 X \u00d7 Y and model parameters \u03b8 := \u3008w, \u03c3\u3009 \u2208 Rd \u00d7 R+, we consider the likelihood p(y|x, \u3008w, \u03c3\u3009) = N (y|w \u00b7\u03c6(x), \u03c32). Thus, the negative log-likelihood loss is\n`nll(\u3008w, \u03c3 \u3009, x, y) = \u2212 ln p(y|x, \u3008w, \u03c3 \u3009) = 12 ln(2\u03c0\u03c3 2) + 12\u03c32 (y \u2212w \u00b7\u03c6(x)) 2 (22) For a fixed \u03c3, minimizing Equation (22) is equivalent to minimizing the square-loss function `sqr(w, x, y) = (y \u2212 w \u00b7 \u03c6(x))2. We also consider an isotropic Gaussian prior of mean 0 and variance \u03c32\u03c0: p(w|\u03c3\u03c0) = N (w|0, \u03c32\u03c0). For the sake of simplicity, we consider a fixed noise parameter \u03c32 and a fixed prior variance \u03c32\u03c0. The Gibbs optimal posterior (see Equation 8) is then given by\n\u03c1\u0302\u2217(w) \u2261 p(w|\u03c3, \u03c3\u03c0) = p(w|\u03c3,\u03c3\u03c0) p(Y |X,w,\u03c3,\u03c3\u03c0)p(Y |X,\u03c3,\u03c3\u03c0) = N (w | w\u0302, A \u22121) , (23)\nwhere A := 1\u03c32 \u03a6 T \u03a6 + 1\u03c32\u03c0 I ; w\u0302 := 1\u03c32A \u22121\u03a6Ty ; \u03a6 is a n\u00d7d matrix such that the ith line is \u03c6(xi) ; y := [y1, . . . yn] is the labels-vector ; and the negative log marginal likelihood is\n\u2212 ln ( ZD(\u03c3, \u03c3\u03c0) ) = 12\u03c32 \u2016y \u2212\u03a6w\u0302\u2016 2 + n2 ln(2\u03c0\u03c3 2) + 12\u03c32\u03c0 \u2016w\u0302\u20162 + 12 log |A|+ d ln\u03c3\u03c0\n= n L\u0302 `nllX,Y (w\u0302) + 1 2\u03c32 tr(\u03a6 T\u03a6A\u22121)\ufe38 \ufe37\ufe37 \ufe38\nnEw\u223c\u03c1\u0302\u2217 L\u0302 `nll X,Y (w)\n+ 12\u03c32\u03c0 tr(A\u22121)\u2212 d2 + 1 2\u03c32\u03c0 \u2016w\u0302\u20162 + 12 log |A|+ d ln\u03c3\u03c0\ufe38 \ufe37\ufe37 \ufe38\nKL ( N (w\u0302,A\u22121) \u2016N (0,\u03c32\u03c0I) ) . Last equality comes from 12\u03c32 tr(\u03a6 T\u03a6A\u22121)+ 1\u03c32\u03c0 tr(A\u22121)= tr( 12\u03c32 \u03a6 T\u03a6A\u22121+ 1\u03c32\u03c0 A\u22121)= tr(A\u22121A) =d (see Appendix A.4 for complete calculations). This exhibits how the Bayesian regression optimization problem can be express by the minimization of a PAC-Bayesian bound, expressed by a trade-off between Ew\u223c\u03c1\u0302\u2217 L\u0302 `nllX,Y (w) and KL ( N (w\u0302, A\u22121) \u2016N (0, \u03c32\u03c0 I) ) .\nModel selection experiment. To produce Figures 1a and 1b, we reimplemented the toy experiment of Bishop [2006, Subsection 3.5.1]. That is, we generated a learning sample of 15 data points according to y = sin(x) + , where x is uniformly sampled in the interval [0, 2\u03c0] and \u223c N (0, 14 ) is a Gaussian noise. We then learn seven different polynomial models with the regression given by Equation (23). More precisely, for a polynomial model of degree d, we map input x \u2208 R to a vector \u03c6(x) = [1, x1, x2, . . . , xd] \u2208 Rd+1, and we fix parameters \u03c32\u03c0 = 10.005 and \u03c3\n2 = 12 . Figure 1a illustrates the seven learned models. Figure 1b shows the marginal likelihood computed for each polynomial model, and is designed to reproduce Bishop [2006, Figure 3.14], where it is explained that the marginal likelihood correctly indicates that the polynomial model of degree d = 3 is \u201cthe simplest model which gives a good explanation for the observed data\u201d. We show that this claim is well quantified by the trade-off intrinsic to our PAC-Bayesian approach: the complexity KL term keeps increasing with the parameter d \u2208 {1, 2, . . . , 7}, while the empirical risk drastically decreases from d = 2 to d = 3, and only slightly afterward. Moreover, we show the generalization risk (computed on a test sample of size 1000) tends to increase with complex models (for d \u2265 4).\nEmpirical comparison of bound values. Figure 1c compares the values of the PAC-Bayesian bounds presented in this paper on a synthetic dataset where the inputs points are randomly generated to a Gaussian x \u223c N (0, I) in R20, and the outputs are given by y = w\u2217 \u00b7 x + , with \u2016w\u2217\u2016=1 and \u223c N (0, 13 ). We perform Bayesian linear regression in the input space, i.e., \u03c6(x)=x, fixing \u03c32\u03c0= 1 100 and \u03c3\n2=2. That is, we compute the optimal posterior of Equation (23) for training samples of sizes from 10 to 106. For each learned model, we compute empirical negative loss likelihood of Equation (22), and the three PAC-Bayes bounds, with confidence parameter of \u03b4= 120 . We estimate the bounds parameters a, b, s, c from observed samples.\nFor small and medium sized training samples (n . 104), the bound of Corollary 5, that we have developed for (unbounded) sub-Gamma losses, gives as far the tighter guarantees than the two other results for [a, b]-bounded losses. However, our new bound always maintains a gap of s2/2(1\u2212c) between its value and the expected loss. The result of Corollary 2 [adapted from Catoni, 2007] from bounded losses suffer from a similar gap, while having higher values that our sub-Gaussian result. Finally, the result of Theorem 3 [Alquier et al., 2015], combined with \u03bb = 1/ \u221a n (Eq 15), converges to the expected loss, but it provides good guarantees only for large training sample (n & 105). Note that the latter bound is not directly minimized by our \u201coptimal posterior\u201d, as opposed to the one with \u03bb = 1/n (Eq 14), for which we observe values in [19.2, 19.8] (not displayed on Figure 1c)."}, {"heading": "7 Conclusion", "text": "The first contribution of this paper is to bridge the concepts underlying the Bayesian and the PACBayesian approaches. This was done by showing that, under proper parametrization, the minimization of the PAC-Bayesian bound minimizes the marginal likelihood. This study, that relies on the realvalued negative log-likelihood loss function, motivates the second contribution of this paper, which is to prove PAC-Bayesian generalization bounds for regression with unbounded sub-Gamma loss functions, that provides generalization guarantees for the squared loss in regression tasks.\nIn this work, we studied model selection techniques. On a broader perspective, we would like to suggest both Bayesian and PAC-Bayesian frameworks may have more to learn from each other than what has been done lately. As future work, we plan to study other Bayesian techniques through the light of PAC-Bayesian tools, such as variational Bayes and empirical Bayes methods."}, {"heading": "A Supplementary material", "text": "A.1 Proof of Theorem 3 Proof. From Donsker-Varadhan\u2019s change of measure, with \u03c6(f) := \u03bb ( L `D(f)\u2212 L\u0302 `X,Y (f) ) , we have \u2200 \u03c1\u0302 on F :\n\u03bb (\nE f\u223c\u03c1\u0302 L `D(f)\u2212 E f\u223c\u03c1\u0302 L\u0302 `X,Y (f)\n) = E f\u223c\u03c1\u0302 \u03bb ( L `D(f)\u2212 L\u0302 `X,Y (f) ) \u2264 KL(\u03c1\u0302\u2016\u03c0) + ln ( E f\u223c\u03c0 e\u03bb ( L `D(f)\u2212L\u0302 ` X,Y (f) )) .\nNow, we apply apply Markov\u2019s inequality on the random variable Ef\u223c\u03c0 e \u03bb ( L `D(f)\u2212L\u0302 ` X,Y (f) ) :\nPr X,Y\u223cDn\n( \u03b6\u03c0(X,Y ) \u2264 1\n\u03b4 E X\u2032,Y \u2032\u223cDn \u03b6\u03c0(X\n\u2032, Y \u2032) ) \u2265 1\u2212 \u03b4 .\nThis implies that with probability at least 1\u2212\u03b4 over the choice of X,Y \u223c Dn, we have \u2200 \u03c1\u0302 on F :\nE f\u223c\u03c1\u0302 L `D(f) \u2264 E f\u223c\u03c1\u0302 L\u0302 `X,Y (f) +\n1\n\u03bb KL(\u03c1\u0302\u2016\u03c0) + ln EX\u2032,Y \u2032\u223cDn \u03a8`,\u03c0,D(\u03bb, n) \u03b4  .\nA.2 Proof of Equation (14) and (15)\nProof. For i = 1 . . . n, we denote `i a realization of the random variable `(f, x, y) \u2212 L\u0302 `X\u2032,Y \u2032(f). Each `i is i.i.d., zero-mean, and bounded by a\u2212 b and b\u2212 a, as `(f, x, y) \u2208 [a, b]. Thus,\nE f\u223c\u03c0 E X\u2032,Y \u2032\u223cDn\nexp [ \u03bb ( L `D(f)\u2212 L\u0302 `X\u2032,Y \u2032(f) )] = E exp\n[ \u03bb\nn n\u2211 i=1 `i\n]\n= n\u220f i=1 E exp [ \u03bb n `i ]\n\u2264 n\u220f i=1 exp [ \u03bb2(a\u2212 b\u2212 (b\u2212 a))2 8n2 ]\n= n\u220f i=1 exp [ \u03bb2(b\u2212 a)2 2n2 ] = exp [ \u03bb2(b\u2212 a)2\n2n\n] ,\nwhere the Inequality comes from Hoeffding\u2019s lemma.\nWith \u03bb := n, Equation (12) becomes Equation (14) :\nE f\u223c\u03c1\u0302 L `D(f) \u2264 E f\u223c\u03c1\u0302 L\u0302 `X,Y (f) +\n1\nn\n[ KL(\u03c1\u0302\u2016\u03c0) + ln 1\n\u03b4 + n2(b\u2212 a)2 2n ] = E\nf\u223c\u03c1\u0302 L\u0302 `X,Y (f) +\n1\nn\n[ KL(\u03c1\u0302\u2016\u03c0) + ln 1\n\u03b4 +\n] + 1\n2 (b\u2212 a)2 .\nSimilarly, with \u03bb := \u221a n, Equation (12) becomes Equation (15) .\nA.3 Study of the Squared Loss\nAssume that:\n\u2022 w \u223c N (0, \u03c32\u03c0 I) , (w \u2208 Rd)\n\u2022 \u223c N (0, \u03c32 ) \u2022 y = w\u2217 \u00b7 x + \u2022 \u2200x \u2208 X , \u03b3 \u2265 sup \u2016x\u2016\nWe have y|x \u223c N (x \u00b7w\u2217, \u03c32 ). Thus, z|x = (y \u2212w \u00b7 x)|x \u223c N (x \u00b7w\u2217, \u03c32 + \u03c32\u03c0\u2016x\u20162)\ne\u03c8 = E x E y|x E w exp(\u03bb[(y \u2212w \u00b7 x)2 \u2212E x E y|x E w (y \u2212w \u00b7 x)2])\n= E x E z|x exp(\u03bb[z2 \u2212E x E z|x z2])\n\u2264 E x E z|x exp(\u03bb[z2])\n( ) = E x 1\u221a 1\u2212 2\u03bb(\u03c32 + \u03c32\u03c0\u2016x\u20162)2 exp\n( \u03bb(w\u2217 \u00b7 x)2\n1\u2212 2\u03bb(\u03c32 + \u03c32\u03c0\u2016x\u20162)2 ) \u2264 E\nx 1\u221a 1\u2212 2\u03bb(\u03c32 + \u03c32\u03c0\u03b32)2 exp\n( \u03bb(w\u2217 \u00b7 x)2\n1\u2212 2\u03bb(\u03c32 + \u03c32\u03c0\u03b32)2 ) \u2264 1\u221a\n1\u2212 2\u03bb(\u03c32 + \u03c32\u03c0\u03b32)2 exp\n( \u03bb\u2016w\u2217\u20162\u03b32\n1\u2212 2\u03bb(\u03c32 + \u03c32\u03c0\u03b32)2\n) .\n\u03c8 \u2264 \u03bb\u2016w \u2217\u20162\u03b32\n1\u2212 2\u03bb(\u03c32 + \u03c32\u03c0\u03b32)2 \u2212 1 2 ln ( 1\u2212 2\u03bb(\u03c32 + \u03c32\u03c0\u03b32)2 ) \u2264 \u03bb\u2016w \u2217\u20162\u03b32\n1\u2212 2\u03bb(\u03c32 + \u03c32\u03c0\u03b32)2\n= 2\u03bb\u2016w\u2217\u20162\u03b32\n2(1\u2212 2\u03bb(\u03c32 + \u03c32\u03c0\u03b32)2)\n\u2264 2\u03bb 2s2\n2(1\u2212 c\u03bb) ,\nwith s2 \u2265 2\u2016w \u2217\u20162\u03b32 \u03bb and c \u2264 2(\u03c3 2 + \u03c3 2 \u03c0\u03b3 2)2. Note that the Equality ( ) is only valid for \u03bb < 1c .\nA.4 Linear Regression\nWe defined A := 1\u03c32 \u03a6 T \u03a6 + 1\u03c32\u03c0 I ; w\u0302 := 1\u03c32A \u22121\u03a6Ty ; \u03a6 as a n\u00d7d matrix such that the ith line is \u03c6(xi) ; y := [y1, . . . yn] as the labels-vector ; and we decompose of the marginal likelihood into the PAC-Bayesian trade-off:\n\u2212 ln ( ZD(\u03c3, \u03c3\u03c0) ) = 12\u03c32 \u2016y \u2212\u03a6w\u0302\u2016 2 + n2 ln(2\u03c0\u03c3 2) + 12\u03c32\u03c0 \u2016w\u0302\u20162 + 12 log |A|+ d ln\u03c3\u03c0\n= n L\u0302 `nllX,Y (w\u0302) + 1 2\u03c32 tr(\u03a6 T\u03a6A\u22121)\ufe38 \ufe37\ufe37 \ufe38\nnEw\u223c\u03c1\u0302\u2217 L\u0302 `nll X,Y (w)\n+ 12\u03c32\u03c0 tr(A\u22121)\u2212 d2 + 1 2\u03c32\u03c0 \u2016w\u0302\u20162 + 12 log |A|+ d ln\u03c3\u03c0\ufe38 \ufe37\ufe37 \ufe38\nKL ( N (w\u0302,A\u22121) \u2016N (0,\u03c32\u03c0I) ) , which is based on the following three equalities:\nn E w\u223c\u03c1\u0302 L\u0302 `nllX,Y (w) = E w\u223c\u03c1\u0302 \u2211 i \u2212 ln p(yi|xi,w)\n= E w\u223c\u03c1\u0302\n( n\n2 ln(2\u03c0\u03c32) +\n1\n2\u03c32 \u2211 i\n(yi \u2212w \u00b7\u03c6(xi))2 )\n= n\n2 ln(2\u03c0\u03c32) +\n1\n2\u03c32 E w\u223c\u03c1\u0302 \u2016y \u2212\u03a6w\u20162\n= n\n2 ln(2\u03c0\u03c32) +\n1\n2\u03c32 E w\u223c\u03c1\u0302\n( \u2016y\u20162 \u2212 2y\u03a6w + wT\u03a6T\u03a6w ) = n\n2 ln(2\u03c0\u03c32) +\n1\n2\u03c32\n( \u2016y\u20162 \u2212 2y\u03a6w\u0302 + E\nw\u223c\u03c1\u0302 wT\u03a6T\u03a6w ) = n\n2 ln(2\u03c0\u03c32) +\n1 2\u03c32 ( \u2016y\u20162 \u2212 2y\u03a6w\u0302 + tr ( \u03a6T\u03a6A\u22121 ) + w\u0302T\u03a6T\u03a6w\u0302 ) = n\n2 ln(2\u03c0\u03c32) +\n1 2\u03c32 \u2016y \u2212\u03a6w\u0302\u20162 + 1 2\u03c32 tr ( \u03a6T\u03a6A\u22121 ) = n L\u0302 `nllX,Y (w\u0302) + 1 2\u03c32 tr ( \u03a6T\u03a6A\u22121 )\nKL ( N (w\u0302, A\u22121) \u2016N (0, \u03c32\u03c0I) ) = 1\n2\n( tr ( (\u03c32\u03c0I) \u22121A\u22121 ) + 1\n\u03c32\u03c0 \u2016w\u0302\u20162 \u2212 d+ log |\u03c3 2 \u03c0I| |A| ) = 1\n2\n( 1\n\u03c32\u03c0 tr ( A\u22121 ) + 1 \u03c32\u03c0 \u2016w\u0302\u20162 \u2212 d+ log |A\u22121|+ d ln\u03c32\u03c0\n)\n1 2\u03c32 tr\n( \u03a6T\u03a6A\u22121 ) + 1\u03c32\u03c0 tr ( A\u22121 ) = tr ( 1 2\u03c32 \u03a6 T\u03a6A\u22121 + 1\u03c32\u03c0 A\u22121 )\n= tr ( A\u22121( 12\u03c32 \u03a6 T\u03a6 + 1\u03c32\u03c0 I) )\n= tr ( A\u22121A ) = d ."}], "references": [{"title": "On the properties of variational approximations of Gibbs posteriors", "author": ["Pierre Alquier", "James Ridgway", "Nicolas Chopin"], "venue": "ArXiv e-prints,", "citeRegEx": "Alquier et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alquier et al\\.", "year": 2015}, {"title": "Tighter PAC-Bayes bounds", "author": ["A. Ambroladze", "E. Parrado-Hern\u00e1ndez", "J. Shawe-Taylor"], "venue": "In NIPS,", "citeRegEx": "Ambroladze et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ambroladze et al\\.", "year": 2006}, {"title": "On Bayesian bounds", "author": ["Arindam Banerjee"], "venue": "In ICML, pages", "citeRegEx": "Banerjee.,? \\Q2006\\E", "shortCiteRegEx": "Banerjee.", "year": 2006}, {"title": "PAC-Bayesian theory for transductive learning", "author": ["Luc B\u00e9gin", "Pascal Germain", "Fran\u00e7ois Laviolette", "Jean-Francis Roy"], "venue": "In AISTATS,", "citeRegEx": "B\u00e9gin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "B\u00e9gin et al\\.", "year": 2014}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "A general framework for updating belief distributions", "author": ["P.G. Bissiri", "C.C. Holmes", "S.G. Walker"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Bissiri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bissiri et al\\.", "year": 2016}, {"title": "Concentration inequalities : a nonasymptotic theory of independence", "author": ["St\u00e9phane Boucheron", "G\u00e1bor Lugosi", "Pascal Massart"], "venue": "Oxford university press,", "citeRegEx": "Boucheron et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boucheron et al\\.", "year": 2013}, {"title": "PAC-Bayesian supervised classification: the thermodynamics of statistical learning, volume 56", "author": ["Olivier Catoni"], "venue": "Inst. of Mathematical Statistic,", "citeRegEx": "Catoni.,? \\Q2007\\E", "shortCiteRegEx": "Catoni.", "year": 2007}, {"title": "Aggregation by exponential weighting, sharp PAC-Bayesian bounds and sparsity", "author": ["Arnak S. Dalalyan", "Alexandre B. Tsybakov"], "venue": "Machine Learning,", "citeRegEx": "Dalalyan and Tsybakov.,? \\Q2008\\E", "shortCiteRegEx": "Dalalyan and Tsybakov.", "year": 2008}, {"title": "PAC-Bayesian learning of linear classifiers", "author": ["Pascal Germain", "Alexandre Lacasse", "Fran\u00e7ois Laviolette", "Mario Marchand"], "venue": "In ICML,", "citeRegEx": "Germain et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2009}, {"title": "Risk bounds for the majority vote: From a PAC-Bayesian analysis to a learning", "author": ["Pascal Germain", "Alexandre Lacasse", "Francois Laviolette", "Mario Marchand", "Jean-Francis Roy"], "venue": null, "citeRegEx": "Germain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2015}, {"title": "Probabilistic machine learning and artificial intelligence", "author": ["Zoubin Ghahramani"], "venue": "Nature, 521:452\u2013459,", "citeRegEx": "Ghahramani.,? \\Q2015\\E", "shortCiteRegEx": "Ghahramani.", "year": 2015}, {"title": "Suboptimal behavior of Bayes and MDL in classification under misspecification", "author": ["Peter Gr\u00fcnwald", "John Langford"], "venue": "Machine Learning,", "citeRegEx": "Gr\u00fcnwald and Langford.,? \\Q2007\\E", "shortCiteRegEx": "Gr\u00fcnwald and Langford.", "year": 2007}, {"title": "Learning efficient random maximum a-posteriori predictors with non-decomposable loss functions", "author": ["Tamir Hazan", "Subhransu Maji", "Joseph Keshet", "Tommi S. Jaakkola"], "venue": "In NIPS,", "citeRegEx": "Hazan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2013}, {"title": "Ockham\u2019s razor and Bayesian analysis", "author": ["William H. Jeffreys", "James O. Berger"], "venue": "American Scientist,", "citeRegEx": "Jeffreys and Berger.,? \\Q1992\\E", "shortCiteRegEx": "Jeffreys and Berger.", "year": 1992}, {"title": "Agnostic Bayes", "author": ["Alexandre Lacoste"], "venue": "PhD thesis, Universite\u0301 Laval,", "citeRegEx": "Lacoste.,? \\Q2015\\E", "shortCiteRegEx": "Lacoste.", "year": 2015}, {"title": "Approximate inference for the loss-calibrated Bayesian", "author": ["Simon Lacoste-Julien", "Ferenc Huszar", "Zoubin Ghahramani"], "venue": "In AISTATS,", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2011}, {"title": "PAC-Bayes & margins", "author": ["John Langford", "John Shawe-Taylor"], "venue": "In NIPS, pages 423\u2013430,", "citeRegEx": "Langford and Shawe.Taylor.,? \\Q2002\\E", "shortCiteRegEx": "Langford and Shawe.Taylor.", "year": 2002}, {"title": "Tighter PAC-Bayes bounds through distributiondependent priors", "author": ["Guy Lever", "Fran\u00e7ois Laviolette", "John Shawe-Taylor"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Lever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lever et al\\.", "year": 2013}, {"title": "A note on the PAC-Bayesian theorem", "author": ["Andreas Maurer"], "venue": "CoRR, cs.LG/0411099,", "citeRegEx": "Maurer.,? \\Q2004\\E", "shortCiteRegEx": "Maurer.", "year": 2004}, {"title": "Some PAC-Bayesian theorems", "author": ["David McAllester"], "venue": "Machine Learning,", "citeRegEx": "McAllester.,? \\Q1999\\E", "shortCiteRegEx": "McAllester.", "year": 1999}, {"title": "PAC-Bayesian stochastic model selection", "author": ["David McAllester"], "venue": "Machine Learning,", "citeRegEx": "McAllester.,? \\Q2003\\E", "shortCiteRegEx": "McAllester.", "year": 2003}, {"title": "Generalization bounds and consistency for latent structural probit and ramp loss", "author": ["David A. McAllester", "Joseph Keshet"], "venue": "In NIPS,", "citeRegEx": "McAllester and Keshet.,? \\Q2011\\E", "shortCiteRegEx": "McAllester and Keshet.", "year": 2011}, {"title": "Generalization error bounds for Bayesian mixture algorithms", "author": ["Ron Meir", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Meir and Zhang.,? \\Q2003\\E", "shortCiteRegEx": "Meir and Zhang.", "year": 2003}, {"title": "On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes", "author": ["Andrew Y. Ng", "Michael I. Jordan"], "venue": "In NIPS,", "citeRegEx": "Ng and Jordan.,? \\Q2001\\E", "shortCiteRegEx": "Ng and Jordan.", "year": 2001}, {"title": "Robust forward algorithms via PAC-Bayes and Laplace distributions", "author": ["Asf Noy", "Koby Crammer"], "venue": "In AISTATS,", "citeRegEx": "Noy and Crammer.,? \\Q2014\\E", "shortCiteRegEx": "Noy and Crammer.", "year": 2014}, {"title": "A PAC-Bayesian bound for lifelong learning", "author": ["Anastasia Pentina", "Christoph H. Lampert"], "venue": "In ICML,", "citeRegEx": "Pentina and Lampert.,? \\Q2014\\E", "shortCiteRegEx": "Pentina and Lampert.", "year": 2014}, {"title": "PAC-Bayesian generalization bounds for gaussian processes", "author": ["Matthias Seeger"], "venue": "JMLR, 3:233\u2013269,", "citeRegEx": "Seeger.,? \\Q2002\\E", "shortCiteRegEx": "Seeger.", "year": 2002}, {"title": "Bayesian Gaussian Process Models: PAC-Bayesian Generalisation Error Bounds and Sparse Approximations", "author": ["Matthias Seeger"], "venue": "PhD thesis, University of Edinburgh,", "citeRegEx": "Seeger.,? \\Q2003\\E", "shortCiteRegEx": "Seeger.", "year": 2003}, {"title": "PAC-Bayesian analysis of co-clustering and beyond", "author": ["Yevgeny Seldin", "Naftali Tishby"], "venue": null, "citeRegEx": "Seldin and Tishby.,? \\Q2010\\E", "shortCiteRegEx": "Seldin and Tishby.", "year": 2010}, {"title": "PAC-Bayesian analysis of contextual bandits", "author": ["Yevgeny Seldin", "Peter Auer", "Fran\u00e7ois Laviolette", "John Shawe-Taylor", "Ronald Ortner"], "venue": "In NIPS,", "citeRegEx": "Seldin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Seldin et al\\.", "year": 2011}, {"title": "PAC-Bayesian inequalities for martingales", "author": ["Yevgeny Seldin", "Fran\u00e7ois Laviolette", "Nicol\u00f2 Cesa-Bianchi", "John Shawe-Taylor", "Peter Auer"], "venue": "In UAI,", "citeRegEx": "Seldin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Seldin et al\\.", "year": 2012}, {"title": "A PAC analysis of a Bayesian estimator", "author": ["John Shawe-Taylor", "Robert C. Williamson"], "venue": "In COLT,", "citeRegEx": "Shawe.Taylor and Williamson.,? \\Q1997\\E", "shortCiteRegEx": "Shawe.Taylor and Williamson.", "year": 1997}, {"title": "Information-theoretic upper and lower bounds for statistical estimation", "author": ["Tong Zhang"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Zhang.,? \\Q2006\\E", "shortCiteRegEx": "Zhang.", "year": 2006}], "referenceMentions": [{"referenceID": 32, "context": "Since its early beginning [Shawe-Taylor and Williamson, 1997], the PAC-Bayesian theory claims to provide \u201cPAC guarantees to Bayesian algorithms\u201d [McAllester, 1999].", "startOffset": 26, "endOffset": 61}, {"referenceID": 20, "context": "Since its early beginning [Shawe-Taylor and Williamson, 1997], the PAC-Bayesian theory claims to provide \u201cPAC guarantees to Bayesian algorithms\u201d [McAllester, 1999].", "startOffset": 145, "endOffset": 163}, {"referenceID": 11, "context": "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al.", "startOffset": 15, "endOffset": 33}, {"referenceID": 4, "context": "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al.", "startOffset": 34, "endOffset": 48}, {"referenceID": 4, "context": "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc.", "startOffset": 34, "endOffset": 69}, {"referenceID": 4, "context": "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc.", "startOffset": 34, "endOffset": 99}, {"referenceID": 4, "context": "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc. Langford and Shawe-Taylor [2002], Seldin and Tishby [2010], Seldin et al.", "startOffset": 34, "endOffset": 138}, {"referenceID": 4, "context": "Seeger [2003], McAllester [2003], Catoni [2007], Lever et al. [2013], Tolstikhin and Seldin [2013], etc. Langford and Shawe-Taylor [2002], Seldin and Tishby [2010], Seldin et al.", "startOffset": 34, "endOffset": 164}, {"referenceID": 2, "context": "[2011, 2012], B\u00e9gin et al. [2014], Pentina and Lampert [2014], etc.", "startOffset": 14, "endOffset": 34}, {"referenceID": 2, "context": "[2011, 2012], B\u00e9gin et al. [2014], Pentina and Lampert [2014], etc.", "startOffset": 14, "endOffset": 62}, {"referenceID": 2, "context": "[2011, 2012], B\u00e9gin et al. [2014], Pentina and Lampert [2014], etc. Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al.", "startOffset": 14, "endOffset": 124}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al.", "startOffset": 57, "endOffset": 73}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al.", "startOffset": 57, "endOffset": 87}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al.", "startOffset": 57, "endOffset": 103}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Gr\u00fcnwald and Langford [2007], Lacoste-Julien et al.", "startOffset": 57, "endOffset": 126}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Gr\u00fcnwald and Langford [2007], Lacoste-Julien et al.", "startOffset": 57, "endOffset": 157}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Gr\u00fcnwald and Langford [2007], Lacoste-Julien et al.", "startOffset": 57, "endOffset": 180}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Gr\u00fcnwald and Langford [2007], Lacoste-Julien et al.", "startOffset": 57, "endOffset": 210}, {"referenceID": 2, "context": "Some indirect connections can be found in Seeger [2002], Banerjee [2006], Zhang [2006], Lacoste [2015], Bissiri et al. [2016]. See also Ng and Jordan [2001], Meir and Zhang [2003], Gr\u00fcnwald and Langford [2007], Lacoste-Julien et al. [2011] for other studies drawing links between frequentist statistics and Bayesian inference.", "startOffset": 57, "endOffset": 240}, {"referenceID": 7, "context": "Theorem 1 (Catoni, 2007).", "startOffset": 10, "endOffset": 24}, {"referenceID": 7, "context": "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al.", "startOffset": 18, "endOffset": 32}, {"referenceID": 7, "context": "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al.", "startOffset": 18, "endOffset": 111}, {"referenceID": 7, "context": "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al.", "startOffset": 18, "endOffset": 141}, {"referenceID": 7, "context": "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al. [2013], Noy and Crammer [2014].", "startOffset": 18, "endOffset": 162}, {"referenceID": 7, "context": "Theorem 1, due to Catoni [2007], has been used to derive or study learning algorithms in Germain et al. [2009], McAllester and Keshet [2011], Hazan et al. [2013], Noy and Crammer [2014]. Theorem 1 (Catoni, 2007).", "startOffset": 18, "endOffset": 186}, {"referenceID": 28, "context": "As mentioned by Zhang [2006], Catoni [2007], Germain et al.", "startOffset": 16, "endOffset": 29}, {"referenceID": 6, "context": "As mentioned by Zhang [2006], Catoni [2007], Germain et al.", "startOffset": 30, "endOffset": 44}, {"referenceID": 6, "context": "As mentioned by Zhang [2006], Catoni [2007], Germain et al. [2009], Lever et al.", "startOffset": 30, "endOffset": 67}, {"referenceID": 6, "context": "As mentioned by Zhang [2006], Catoni [2007], Germain et al. [2009], Lever et al. [2013], Alquier et al.", "startOffset": 30, "endOffset": 88}, {"referenceID": 0, "context": "[2013], Alquier et al. [2015], the optimal Gibbs posterior \u03c1\u0302\u2217 is given by \u03c1\u0302\u2217(f) = 1 ZX,Y \u03c0(f) e \u2212n L\u0302 ` X,Y (f) , (4)", "startOffset": 8, "endOffset": 30}, {"referenceID": 0, "context": "The new bounds are obtained through a recent theorem of Alquier et al. [2015], stated below (we provide a proof in Appendix A.", "startOffset": 56, "endOffset": 78}, {"referenceID": 0, "context": "The new bounds are obtained through a recent theorem of Alquier et al. [2015], stated below (we provide a proof in Appendix A.1 for completeness). Theorem 3 (Alquier et al. [2015]).", "startOffset": 56, "endOffset": 180}, {"referenceID": 26, "context": "A similar result to Equation (15) leads to long-life learning algorithms in Pentina and Lampert [2014].", "startOffset": 76, "endOffset": 103}, {"referenceID": 0, "context": "The above sub-Gaussian assumption corresponds to the Hoeffding assumption of Alquier et al. [2015], and allows to obtain the following result.", "startOffset": 77, "endOffset": 99}, {"referenceID": 19, "context": "Maurer [2004] shows that we can generalize PAC-Bayesian bounds on the generalization risk of the Gibbs classifier to any loss function with output between zero and one.", "startOffset": 0, "endOffset": 14}, {"referenceID": 8, "context": "In other situations, we may have RD(G\u03c1\u0302) = 0 even if RD(G\u03c1\u0302) = 12\u2212 (see Germain et al. [2015] for an extensive study).", "startOffset": 72, "endOffset": 94}, {"referenceID": 4, "context": "In this section, we perform Bayesian linear regression using the parameterization of Bishop [2006]. The output space is Y := R and, for an arbitrary input spaceX , we use a mapping function\u03c6 :X\u2192R.", "startOffset": 85, "endOffset": 99}, {"referenceID": 0, "context": "Finally, the result of Theorem 3 [Alquier et al., 2015], combined with \u03bb = 1/ \u221a n (Eq 15), converges to the expected loss, but it provides good guarantees only for large training sample (n & 10).", "startOffset": 33, "endOffset": 55}], "year": 2017, "abstractText": "We exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam\u2019s razor criteria, under the assumption that the data is generated by a i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-Gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.", "creator": "LaTeX with hyperref package"}}}