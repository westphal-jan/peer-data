{"id": "1605.08754", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Faster Eigenvector Computation via Shift-and-Invert Preconditioning", "abstract": "practitioners work using simpler algorithms with better complexities for locating the simplest eigenvector and a function $ \\ h $ - -,. e. computing a constant valued $ x $ such sum $ x ^ t \\ sigma x \\ ge ( r - \\ | ) \\ ch _ 1 ( \\ sigma ) $ :", "histories": [["v1", "Thu, 26 May 2016 03:53:00 GMT  (44kb,D)", "http://arxiv.org/abs/1605.08754v1", "Appearing in ICML 2016. Combination of work inarXiv:1509.05647andarXiv:1510.08896"]], "COMMENTS": "Appearing in ICML 2016. Combination of work inarXiv:1509.05647andarXiv:1510.08896", "reviews": [], "SUBJECTS": "cs.DS cs.LG math.NA math.OC", "authors": ["dan garber", "elad hazan", "chi jin", "sham m kakade", "cameron musco", "praneeth netrapalli", "aaron sidford"], "accepted": true, "id": "1605.08754"}, "pdf": {"name": "1605.08754.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Dan Garber", "Elad Hazan"], "emails": ["dgarber@ttic.edu", "ehazan@cs.princeton.edu", "chijin@eecs.berkeley.edu", "sham@cs.washington.edu", "cnmusco@mit.edu", "praneeth@microsoft.com", "asid@microsoft.com"], "sections": [{"heading": null, "text": "\u2022 Offline Eigenvector Estimation: Given an explicit A \u2208 Rn\u00d7d with \u03a3 = A>A, we show how to compute an approximate top eigenvector in time O\u0303 ([ nnz(A) + d sr(A)gap2 ] \u00b7 log 1/ ) and O\u0303 ([ nnz(A)3/4(d sr(A))1/4\u221a\ngap\n] \u00b7 log 1/ ) . Here nnz(A) is the number of nonzeros in A,\nsr(A) def = \u2016A\u20162F \u2016A\u201622\nis the stable rank, gap is the relative eigengap, and O\u0303(\u00b7) hides log factors in d and gap. By separating the gap dependence from the nnz(A) term, our first runtime improves upon the classical power and Lanczos methods. It also improves prior work using fast subspace embeddings [AC09, CW13] and stochastic optimization [Sha15c], giving significantly better dependencies on sr(A) and . Our second running time improves these\nfurther when nnz(A) \u2264 d sr(A)gap2 . \u2022 Online Eigenvector Estimation: Given a distribution D with covariance matrix \u03a3 and\na vector x0 which is an O(gap) approximate top eigenvector for \u03a3, we show how to refine to an approximation using O (\nv(D) gap\u00b7\n) samples from D. Here v(D) is a natural notion of\nvariance. Combining our algorithm with previous work to initialize x0, we obtain improved sample complexity and runtime results under a variety of assumptions on D.\nWe achieve our results using a general framework that we believe is of independent interest. We give a robust analysis of the classic method of shift-and-invert preconditioning to reduce eigenvector computation to approximately solving a sequence of linear systems. We then apply fast stochastic variance reduced gradient (SVRG) based system solvers to achieve our claims. We believe our results suggest the general effectiveness of shift-and-invert based approaches and imply that further computational gains may be reaped in practice.\n\u2217This paper combines work first appearing in [GH15] and [JKM+15]\nar X\niv :1\n60 5.\n08 75\n4v 1\n[ cs\n.D S]"}, {"heading": "1 Introduction", "text": "Given A \u2208 Rn\u00d7d, computing the top eigenvector of A>A is a fundamental problem in numerical linear algebra, applicable to principal component analysis [Jol02], spectral clustering and learning [NJW02, VW04], pagerank computation, and many other graph computations [PBMW99, Kor03, Spi07]. For instance, a degree-k principal component analysis is nothing more than performing k leading eigenvector computations. Given the ever-growing size of modern datasets, it is thus a key challenge to come up with more efficient algorithms for this basic computational primitive.\nIn this work we provide improved algorithms for computing the top eigenvector, both in the offline case, when A is given explicitly and in the online or statistical case where we access samples from a distribution D over Rd and wish to estimate the top eigenvector of the covariance matrix Ea\u223cD [ aa> ] . In the offline case, our algorithms are the fastest to date in a wide and meaningful regime of parameters. Notably, while the running time of most popular methods for eigenvector computations is a product of the size of the dataset (i.e. number of non-zeros in A) and certain spectral characteristics of A, which both can be quite large in practice, we present running times that actually split the dependency between these two quantities, and as a result may yield significant speedups. In the online case, our results yield improved sample complexity bounds and allow for very efficient streaming implementations with memory and processing-time requirements that are proportional to the size of a single sample.\nOn a high-level, our algorithms are based on a robust analysis of the classic idea of shiftand-invert preconditioning [Saa92], which allows us to efficiently reduce eigenvector computation to approximately solving a short sequence of well-conditioned linear systems in \u03bbI \u2212 A>A for some shift parameter \u03bb \u2248 \u03bb1(A). We then apply state-of-the-art stochastic gradient methods to approximately solve these linear systems."}, {"heading": "1.1 Our Approach", "text": "The well known power method for computing the top eigenvector of A>A starts with an initial vector x and repeatedly multiplies by A>A, eventually causing x to converge to the top eigenvector. For a random start vector, the power method converges in O(log(d/ )/gap) iterations, where gap = (\u03bb1\u2212 \u03bb2)/\u03bb1, \u03bbi denotes the ith largest eigenvalue of A>A, and we assume a high-accuracy regime where < gap. The dependence on this gap ensures that the largest eigenvalue is significantly amplified in comparison to the remaining values.\nIf the eigenvalue gap is small, one approach is replace A>A with a preconditioned matrix \u2013 i.e. a matrix with the same top eigenvector but a much larger gap. Specifically, let B = \u03bbI \u2212 A>A for some shift parameter \u03bb. If \u03bb > \u03bb1, we can see that the smallest eigenvector of B (the largest eigenvector of B\u22121) is equal to the largest eigenvector of A>A. Additionally, if \u03bb is close to \u03bb1, there will be a constant gap between the largest and second largest values of B\u22121. For example, if \u03bb = (1 + gap)\u03bb1, then we will have \u03bb1 ( B\u22121 ) = 1\u03bb\u2212\u03bb1 = 1 gap\u00b7\u03bb1 and \u03bb2 ( B\u22121 ) = 1\u03bb\u2212\u03bb2 = 1 2\u00b7gap\u00b7\u03bb1 .\nThis constant factor gap ensures that the power method applied to B\u22121 converges to the top eigenvector of A>A in just O(log(d/ )) iterations. Of course, there is a catch \u2013 each iteration of this shifted-and-inverted power method must solve a linear system in B, whose condition number is proportional 1gap . For small gap, solving this system via iterative methods is more expensive.\nFortunately, linear system solvers are incredibly well studied and there are many efficient iterative algorithms we can adapt to apply B\u22121 approximately. In particular, we show how to accelerate the iterations of the shifted-and-inverted power method using variants of Stochastic Variance Re-\nduced Gradient (SVRG) [JZ13]. Due to the condition number of B, we will not entirely avoid a 1 gap dependence, however, we can separate this dependence from the input size nnz(A). Typically, stochastic gradient methods are used to optimize convex functions that are given as the sum of many convex components. To solve a linear system (M>M)x = b we minimize the convex function f(x) = 12x >(M>M)x\u2212b>x with components \u03c8i(x) = 12x > (mim>i )x\u2212 1nb>x where mi is the i th row of M. Such an approach can be used to solve systems in A>A, however solving systems in B = \u03bbI \u2212A>A requires more care. We require an analysis of SVRG that guarantees convergence even when some of our components are non-convex. We give a simple analysis for this setting, generalizing recent work in the area [SS15, CR15].\nGiven fast approximate solvers for B, the second main piece of our algorithmic framework is a new error bound for the shifted-and-inverted power method, showing that it is robust to approximate linear system solvers, such as SVRG. We give a general analysis, showing exactly what accuracy each system must be solved to, allowing for faster implementations using linear solvers with\nweaker guarantees. Our proofs center around the potential function: G(x) def = \u2225\u2225\u2225Pv\u22a51 x\u2225\u2225\u2225B / \u2016Pv1x\u2016B, where Pv1 and Pv\u22a51 are the projections onto the top eigenvector and its complement respectively. This function resembles tangent based potential functions used in previous work [HP14] except that we use the B norm rather than the `2 norm. For the exact power method, this is irrelevant \u2013 progress is identical in both norms (see Lemma 38 of the Appendix). However, \u2016\u00b7\u2016B is a natural norm for measuring the progress of linear system solvers for B, so our potential function makes it possible to extend analysis to the case when B\u22121x is computed up to error \u03be with bounded \u2016\u03be\u2016B."}, {"heading": "1.2 Our Results", "text": "Our algorithmic framework described above offers several advantageous. We obtain improved running times for computing the top eigenvector in the offline model. In Theorem 16 we give an algorithm running in time O ([\nnnz(A) + d sr A gap2\n] \u00b7 [ log 1 + log\n2 d gap ]) , where sr(A) = \u2016A\u20162F / \u2016A\u2016 2 2 \u2264\nrank(A) is the stable rank and nnz(A) is the number of non-zero entries. Up to log factors, our runtime is in many settings proportional to the input size nnz(A), and so is very efficient for large matrices. In the case when nnz(A) \u2264 d sr(A) gap2 we apply the results of [FGKS15b, LMH15] to provide\nan accelerated runtime of O ([\nnnz(A)3/4(d sr(A))1/4\u221a gap ] \u00b7 [ log dgap log 1 + log 3 d gap ]) , shown in Theorem\n17. Finally, in the case when > gap, our results easily extend to give gap-free bounds (Theorems 35 and 36), identical to those shown above but with gap replaced by . Note that our offline results hold for any A and require no initial knowledge of the top eigenvector. In Section 6 we discuss how to estimate the parameters \u03bb1, gap, with modest additional runtime cost.\nOur algorithms return an approximate top eigenvector x with x>A>Ax \u2265 (1 \u2212 )\u03bb1. By choosing error \u00b7gap, we can ensure that x is actually close to v1 \u2013 i.e. that |x>v1| \u2265 1\u2212 . Further, we obtain the same asymptotic runtime since O ( log 1 \u00b7gap + log 2 d gap ) = O ( log 1 + log 2 d gap ) . We compare our runtimes with previous work in Table 1. In the online case, in Theorem 25, we show how to improve an O(gap) approximation to the\ntop eigenvector to an approximation with constant probability using O (\nv(D) gap\u00b7\n) samples where\nv(D) is a natural variance measure. Our algorithm is based on the streaming SVRG algorithm of [FGKS15a]. It requires just O(d) amortized time per sample, uses just O(d) space, and is easily parallelized. We can apply our result in a variety of regimes, using existing algorithms to obtain\nthe initial O(gap) approximation and our algorithm to improve. As shown in Table 2, this gives improved runtimes and sample complexities over existing work. Notably, we give better asymptotic sample complexity than known matrix concentration results for general distributions, and give the first streaming algorithm that is asymptotically optimal in the popular Gaussian spike model.\nOverall, our robust shifted-and-inverted power method analysis gives new understanding of this classical technique. It gives a means of obtaining provably accurate results when each iteration is implemented using fast linear system solvers with weak accuracy guarantees. In practice, this reduction between approximate linear system solving and eigenvector computation shows that optimized regression libraries can be leveraged for faster eigenvector computation in many cases. Furthermore, in theory we believe that the reduction suggests computational limits inherent in eigenvector computation as seen by the often easier-to-analyze problem of linear system solving. Indeed, in Section 7, we provide evidence that in certain regimes our statistical results are optimal."}, {"heading": "1.3 Previous Work", "text": "Offline Eigenvector Computation\nDue to its universal applicability, eigenvector computation in the offline case is extremely well studied. Classical methods, such as the QR algorithm, take roughly O(nd2) time to compute a full eigendecomposition. This can be accelerated to O(nd\u03c9\u22121), where \u03c9 < 2.373 is the matrix multiplication constant [Wil12, LG14], however this is still prohibitively expensive for large matrices. Hence, faster iterative methods are often employed, especially when only the top eigenvector (or a few of the top eigenvectors) is desired.\nAs discussed, the popular power method requires O (\nlog(d/ ) gap\n) iterations to converge to an\napproximate top eigenvector. Using Chebyshev iteration, or more commonly, the Lanczos method, this bound can be improved to O (\nlog(d/ )\u221a gap\n) [Saa92], giving total runtime of O ( nnz(A) \u00b7 log(d/ )\u221agap ) .\nWhen > gap, the gap terms in these runtimes can be replaced by . While we focus on the high-precision regime when < gap, we also give gap-free bounds in Section 8.\nUnfortunately, if nnz(A) is very large and gap is small, the above runtimes can still be quite expensive, and there is a natural desire to separate the 1\u221agap dependence from the nnz(A) term. One approach is to use random subspace embedding matrices [AC09, CW13] or fast row sampling algorithms [CLM+15], which can be applied in O(nnz(A)) time and yield a matrix A\u0303 which is a good spectral approximation to the original. The number of rows in A\u0303 depends only on the stable rank of A and the error of the embedding \u2013 hence it can be significantly smaller than n. Applying such a subspace embedding and then computing the top eigenvector of A\u0303>A\u0303 requires runtime O (nnz(A) + poly(sr(A), , gap)), achieving the goal of reducing runtime dependence on the input size nnz(A). Unfortunately, the dependence on is significantly suboptimal \u2013 such an approach cannot be used to obtain a linearly convergent algorithm. Further, the technique does not extend to the online setting, unless we are willing to store a full subspace embedding of our sampled rows.\nAnother approach, which we follow more closely, is to apply stochastic optimization techniques, which iteratively update an estimate to the top eigenvector, considering a random row of A with each update step. Such algorithms naturally extend to the online setting and have led to improved dependence on the input size for a variety of problems [Bot10]. Using variance-reduced stochastic\ngradient techniques, [Sha15c] achieves runtime O (( nnz(A) + dr 2n2\ngap2\u03bb21\n) \u00b7 log(1/ ) log log(1/ ) ) for\napproximately computing the top eigenvector of a matrix with constant probability. Here r is an\nupper bound on the squared row norms of A. In the best case, when row norms are uniform, this runtime can be simplified to O (( nnz(A) + d sr(A) 2\ngap2\n) \u00b7 log(1/ ) log log(1/ ) ) .\nThe result in [Sha15c] makes an important contribution in separating input size and gap dependencies using stochastic optimization techniques. Unfortunately, the algorithm requires an approximation to the eigenvalue gap and a starting vector that has a constant dot product with the top eigenvector. In [Sha15b] the analysis is extended to a random initialization, however loses polynomial factors in d. Furthermore, the dependencies on the stable rank and are suboptimal \u2013 we improve them to sr(A) and log(1/ ) respectively, obtaining true linear convergence.\nOnline Eigenvector Computation\nWhile in the offline case the primary concern is computation time, in the online, or statistical setting, research also focuses on minimizing the number of samples that are drawn from D in order to achieve a given accuracy. Especially sought after are results that achieve asymptotically optimal accuracy as the sample size grows large.\nWhile the result we give in Theorem 25 works for any distribution parameterized by a variance bound, in this section, in order to more easily compare to previous work, we normalize \u03bb1 = 1 and assume we have the row norm bound \u2016a\u201622 \u2264 O(d) which then gives us the variance bound\u2225\u2225Ea\u223cD [(aa>)2]\u2225\u22252 = O(d). Additionally, we compare runtimes for computing some x such that |x>v1| \u2265 1\u2212 , as this is the most popular guarantee studied in the literature. Theorem 25 is easily extended to this setting as obtaining x with xTAA>x \u2265 (1\u2212 \u00b7 gap)\u03bb1 ensures |x>v1| \u2265 1\u2212 . Our algorithm requires O ( d\ngap2\n) samples to find such a vector under the assumptions given above.\nThe simplest algorithm in this setting is to take n samples from D and compute the leading eigenvector of the empirical estimate E\u0302[aa>] = 1n \u2211n i=1 aia > i . By a matrix Bernstein bound, such as\ninequality of Theorem 6.6.1 of [Tro15], O ( d log d gap2 ) samples is enough to insure \u2225\u2225\u2225E\u0302[aa>]\u2212 E[aa>]\u2225\u2225\u2225 2 \u2264 \u221a \u00b7 gap. By Lemma 37 in the Appendix, this gives that, if x is set to the top eigenvector of E\u0302[aa>] it will satisfy |x>v1| \u2265 1\u2212 . x can be approximated with any offline eigenvector algorithm. A large body of work focuses on improving this simple algorithm, under a variety of assumptions on D. A common focus is on obtaining streaming algorithms, in which the storage space is just O(d) - proportional to the size of a single sample. In Table 2 we give a sampling of results in this\narea. All listed results rely on distributional assumptions at least as strong as those given above. Note that, in each setting, we can use the cited algorithm to first compute an O(gap) approximate eigenvector, and then refine this approximation to an approximation using O (\nd gap2 ) samples by applying our streaming SVRG based algorithm. This allows us to obtain improved runtimes and sample complexities. To save space, we do not include our improved runtime bounds in Table 2, however they are easy to derive by adding the runtime required by the given algorithm\nto achieve O(gap) accuracy, to O ( d2\ngap2\n) \u2013 the runtime required by our streaming algorithm.\nThe bounds given for the simple matrix Bernstein based algorithm described above, Krasulina/Oja\u2019s Algorithm [BDF13], and SGD [Sha15a] require no additional assumptions, aside from those given at the beginning of this section. The streaming results cited for [MCJ13] and [HP14] assume a is generated from a Gaussian spike model, where ai = \u221a \u03bb1\u03b3iv1+Zi and \u03b3i \u223c N (0, 1), Zi \u223c N (0, Id). We note that under this model, the matrix Bernstein results improve by a log d factor and so match our results in achieving asymptotically optimal convergence rate. The results of [MCJ13] and [HP14] sacrifice this optimality in order to operate under the streaming model. Our work gives the best of both works \u2013 a streaming algorithm giving asymptotically optimal results.\nThe streaming Alecton algorithm [SRO15] assumes E \u2225\u2225aa>Waa>\u2225\u2225 \u2264 O(1)tr(W) for any sym-\nmetric W that commutes with Eaa>. This is strictly stronger than our assumption that\u2225\u2225Ea\u223cD [(aa>)2]\u2225\u22252 = O(d). Algorithm\nSample Size\nRuntime Streaming? Our Sample Complexity\nMatrix Bernstein plus Lanczos (explicitly forming sampled matrix) O ( d log d gap2 ) O ( d3 log d gap2 ) \u00d7 O ( d log d gap3 + d gap2 ) Matrix Bernstein plus\nLanczos (iteratively applying sampled matrix)\nO ( d log d gap2 ) O ( d2 log d\u00b7log(d/ ) gap2.5 ) \u00d7 O ( d log d gap3 + d gap2 ) Memory-efficient PCA\n[MCJ13, HP14] O ( d log(d/ ) gap3 ) O ( d2 log(d/ ) gap3 ) \u221a O ( d log(d/gap) gap4 + d gap2 ) Alecton [SRO15] O(d log(d/ )gap2 ) O( d2 log(d/ ) gap2 ) \u221a O(d log(d/gap)gap3 + d gap2 )\nKrasulina / Oja\u2019s Algorithm [BDF13]\nO( d c1 gap2 c2 ) O( dc1+1 gap2 c2 ) \u221a\nO( d c1\ngap2+c2 + dgap2 )\nSGD [Sha15a] O(d 3 log(d/ ) 2 ) O( d4 log(d/ ) 2 ) \u221a\nO ( d3 log(d/gap)\ngap2 + d gap2 )\nTable 2: Summary of existing work on Online Eigenvector Estimation and improvements given by our results. Runtimes are for computing a unit vector x such that |x>v1| \u2265 1\u2212 . For each of these results we can obtain improved running times and sample complexities by running the algorithm to first compute an O(gap) approximate eigenvector, and then running our algorithm to obtain an\napproximation using an additional O\n(\nd gap2\n)\nsamples, O(d) space, and O(d) work per sample."}, {"heading": "1.4 Paper Organization", "text": "Section 2 Review problem definitions and parameters for our runtime and sample bounds.\nSection 3 Describe the shifted-and-inverted power method and show how it can be implemented using approximate system solvers.\nSection 4 Show how to apply SVRG to solve systems in our shifted matrix, giving our main runtime results for offline eigenvector computation.\nSection 5 Show how to use an online variant of SVRG to run the shifted-and-inverted power method, giving our main sampling complexity and runtime results in the statistical setting.\nSection 6 Show how to efficiently estimate the shift parameters required by our algorithms.\nSection 7 Give a lower bound in the statistical setting, showing that our results are asymptotically optimal for a wide parameter range.\nSection 8 Give gap-free runtime bounds, which apply when > gap."}, {"heading": "2 Preliminaries", "text": "We bold all matrix variables. We use [n] def = {1, ..., n}. For a symmetric positive semidefinite (PSD) matrix M we let \u2016x\u2016M def = \u221a x>Mx and \u03bb1(M), ..., \u03bbd(M) denote its eigenvalues in decreasing order. We use M N to denote the condition that x>Mx \u2264 x>Nx for all x."}, {"heading": "2.1 The Offline Problem", "text": "We are given a matrix A \u2208 Rn\u00d7d with rows a(1), ..., a(n) and wish to compute an approximation to the top eigenvector of \u03a3 def = A>A. Specifically, for error parameter we want a unit vector x such that x>\u03a3x \u2265 (1\u2212 )\u03bb1(\u03a3)."}, {"heading": "2.2 The Statistical Problem", "text": "We have access to an oracle returning independent samples from a distribution D on Rd and wish to compute the top eigenvector of \u03a3 def = Ea\u223cD [ aa> ] . Again, for error parameter we want to return a unit vector x such that x>\u03a3x \u2265 (1\u2212 )\u03bb1(\u03a3)."}, {"heading": "2.3 Problem Parameters", "text": "We parameterize the running times and sample complexities of our algorithms in terms of several natural properties of A, D, and \u03a3. Let \u03bb1, ..., \u03bbd denote the eigenvalues of \u03a3 in decreasing order and v1, ..., vd denote their corresponding eigenvectors. We define the eigenvalue gap by gap\ndef = \u03bb1\u2212\u03bb2\u03bb1 .\nWe use the following additional parameters for the offline and statistical problems respectively: \u2022 Offline Problem: Let sr(A) def= \u2211\ni \u03bbi \u03bb1 = \u2016A\u20162F \u2016A\u201622 denote the stable rank of A. Note that we\nalways have sr(A) \u2264 rank(A). Let nnz(A) denote the number of non-zero entries in A.\n\u2022 Online Problem: Let v(D) def= \u2225\u2225\u2225Ea\u223cD[(aa>)2]\u2225\u2225\u2225 2\n\u2016Ea\u223cD(aa>)\u201622 =\n\u2225\u2225\u2225Ea\u223cD[(aa>)2]\u2225\u2225\u2225 2\n\u03bb21 denote a natural upper\nbound on the variance of D in various settings. Note that v(D) \u2265 1."}, {"heading": "3 Algorithmic Framework", "text": "Here we develop our robust shift-and-invert framework. In Section 3.1 we provide a basic overview of the framework and in Section 3.2 we introduce the potential function we use to measure progress of our algorithms. In Section 3.3 we show how to analyze the framework given access to an exact linear system solver and in Section 3.4 we strengthen this analysis to work with an inexact linear system solver. Finally, in Section 3.5 we discuss initializing the framework."}, {"heading": "3.1 Shifted-and-Inverted Power Method Basics", "text": "We let B\u03bb def = \u03bbI\u2212\u03a3 denote the shifted matrix that we will use in our implementation of the shiftedand-inverted power method. As discussed, in order for B\u22121\u03bb to have a large eigenvalue gap, \u03bb should be set to (1 + c \u00b7 gap)\u03bb1 for some constant c \u2265 0. Throughout this section we assume that we have a crude estimate of \u03bb1 and gap and fix \u03bb to be a value satisfying ( 1 + gap150 ) \u03bb1 \u2264 \u03bb \u2264 ( 1 + gap100 ) \u03bb1. (See Section 6 for how we can compute such a \u03bb). For the remainder of this section we work with such a fixed value of \u03bb and therefore for convenience denote B\u03bb as B.\nNote that \u03bbi ( B\u22121 ) = 1\u03bbi(B) = 1 \u03bb\u2212\u03bbi and so \u03bb1(B\u22121) \u03bb2(B\u22121) = \u03bb\u2212\u03bb2\u03bb\u2212\u03bb1 \u2265 gap gap/100 = 100. This large gap will ensure that, assuming the ability to apply B\u22121, the power method will converge very quickly. In the remainder of this section we develop our error analysis for the shifted-and-inverted power method which demonstrates that approximate application of B\u22121 in each iteration in fact suffices."}, {"heading": "3.2 Potential Function", "text": "Our analysis of the power method focuses on the objective of maximizing the Rayleigh quotient, x>\u03a3x for a unit vector x. Note that as the following lemma shows, this has a direct correspondence to the error in maximizing |v>1 x|:\nLemma 1 (Bounding Eigenvector Error by Rayleigh Quotient). For a unit vector x let = \u03bb1 \u2212 x>\u03a3x. If \u2264 \u03bb1 \u00b7 gap then \u2223\u2223\u2223v>1 x\u2223\u2223\u2223 \u2265\u221a1\u2212 \u03bb1 \u00b7 gap . Proof. Among all unit vectors x such that = \u03bb1 \u2212 x>\u03a3x, a minimizer of\n\u2223\u2223v>1 x\u2223\u2223 has the form x = ( \u221a 1\u2212 \u03b42)v1 + \u03b4v2 for some \u03b4. We have\n= \u03bb1 \u2212 x>\u03a3x = \u03bb1 \u2212 \u03bb1(1\u2212 \u03b42)\u2212 \u03bb2\u03b42 = (\u03bb1 \u2212 \u03bb2)\u03b42.\nTherefore by direct computation,\u2223\u2223\u2223v>1 x\u2223\u2223\u2223 = \u221a1\u2212 \u03b42 = \u221a1\u2212 \u03bb1 \u2212 \u03bb2 = \u221a 1\u2212 \u03bb1 \u00b7 gap .\nIn order to track the progress of our algorithm we use a more complex potential function than just the Rayleigh quotient error, \u03bb1 \u2212 x>\u03a3x. Our potential function G is defined for x 6= 0 by\nG(x) def = \u2225\u2225\u2225Pv\u22a51 x\u2225\u2225\u2225B \u2016Pv1x\u2016B\nwhere Pv1 and Pv\u22a51 are the projections onto v1 and the subspace orthogonal to v1 respectively. Equivalently, we have that:\nG(x) =\n\u221a \u2016x\u20162B \u2212 ( v>1 B 1/2x )2\u2223\u2223v>1 B1/2x\u2223\u2223 = \u221a\u2211 i\u22652 \u03b12i \u03bbi(B\u22121)\u221a \u03b121\n\u03bb1(B\u22121)\n. (1)\nwhere \u03b1i = v > i x.\nWhen the Rayleigh quotient error = \u03bb1 \u2212 x>\u03a3x of x is small, we can show a strong relation between and G(x). We prove this in two parts. We first give a technical lemma, Lemma 2, that we will use several times for bounding the numerator of G. We then prove the connection in Lemma 3.\nLemma 2. For a unit vector x and = \u03bb1 \u2212 x>\u03a3x if \u2264 \u03bb1 \u00b7 gap then\n\u2264 x>Bx\u2212 (v>1 Bx)(v>1 x) \u2264 (\n1 + \u03bb\u2212 \u03bb1 \u03bb1 \u00b7 gap\n) .\nProof. Since B = \u03bbI\u2212\u03a3 and since v1 is an eigenvector of \u03a3 with eigenvalue \u03bb1 we have\nx>Bx\u2212 (v>1 Bx)(v>1 x) = \u03bb \u2016x\u2016 2 2 \u2212 x >\u03a3x\u2212 (\u03bbv>1 x\u2212 v>1 \u03a3x)(v>1 x) = \u03bb\u2212 \u03bb1 + \u2212 (\u03bbv>1 x\u2212 \u03bb1v>1 x)(v>1 x)\n= (\u03bb\u2212 \u03bb1) ( 1\u2212 (v>1 x)2 ) + .\nNow by Lemma 1 we know that |v>1 x| \u2265 \u221a\n1\u2212 \u03bb1\u00b7gap , giving us the upper bound. Furthermore, since trivially \u2223\u2223v>1 x\u2223\u2223 \u2264 1 and \u03bb\u2212 \u03bb1 > 0, we have the lower bound. Lemma 3 (Potential Function to Rayleigh Quotient Error Conversion). For a unit vector x and = \u03bb1 \u2212 x>\u03a3x if \u2264 12\u03bb1 \u00b7 gap, we have:\n\u03bb\u2212 \u03bb1 \u2264 G(x)2 \u2264\n( 1 +\n\u03bb\u2212 \u03bb1 \u03bb1 \u00b7 gap\n)( 1 +\n2\n\u03bb1 \u00b7 gap\n)\n\u03bb\u2212 \u03bb1 .\nProof. Since v1 is an eigenvector of B, we can write G(x) 2 = x>Bx\u2212(v>1 Bx)(v>1 x) (v>1 Bx)(v > 1 x) . Lemmas 1 and 2 then give us:\n\u03bb\u2212 \u03bb1 \u2264 G(x)2 \u2264\n( 1 +\n\u03bb\u2212 \u03bb1 \u03bb1 \u00b7 gap\n)\n(\u03bb\u2212 \u03bb1) ( 1\u2212 \u03bb1\u00b7gap ) .\nSince \u2264 12\u03bb1 \u00b7 gap, we have 1\n1\u2212 \u03bb1\u00b7gap\n\u2264 1 + 2 \u03bb1\u00b7gap . This proves the lemma."}, {"heading": "3.3 Power Iteration", "text": "Here we show that the shifted-and-inverted power iteration in fact makes progress with respect to our objective function given an exact linear system solver for B. Formally, we show that applying B\u22121 to a vector x decreases the potential function G(x) geometrically.\nTheorem 4. Let x be a unit vector with \u3008x, v1\u3009 6= 0 and let x\u0303 = B\u22121x, i.e. the power method update of B\u22121 on x. Then, under our assumption on \u03bb, we have:\nG(x\u0303) \u2264 \u03bb2 ( B\u22121 ) \u03bb1 (B\u22121) G(x) \u2264 1 100 G(x).\nNote that x\u0303 may no longer be a unit vector. However, G(x\u0303, v1) = G(cx\u0303, v1) for any scaling parameter c, so the theorem also holds for x\u0303 scaled to have unit norm.\nProof. Writing x in the eigenbasis, we have x = \u2211 i \u03b1ivi and x\u0303 = \u2211 i \u03b1i\u03bbi ( B\u22121 ) vi. Since \u3008x, v1\u3009 6= 0, \u03b11 6= 0 and by the equivalent formulation of G(x) given in (1):\nG(x\u0303) =\n\u221a\u2211 i\u22652 \u03b1 2 i\u03bbi(B\n\u22121)\u221a \u03b121\u03bb1(B \u22121) \u2264 \u03bb2 ( B\u22121 ) \u03bb1 (B\u22121) \u00b7\n\u221a\u2211 i\u22652\n\u03b12i \u03bbi(B\u22121)\u221a \u03b121\n\u03bb1(B\u22121)\n= \u03bb2 ( B\u22121 ) \u03bb1 (B\u22121) \u00b7G(x) .\nRecalling that \u03bb1(B\u22121) \u03bb2(B\u22121) = \u03bb\u2212\u03bb2\u03bb\u2212\u03bb1 \u2265 gap gap/100 = 100 yields the result.\nThe challenge in using the above theorem, and any traditional analysis of the shifted-andinverted power method, is that we don\u2019t actually have access to B\u22121. In the next section we show that the shifted-and-inverted power method is robust \u2013 we still make progress on our objective function even if we only approximate B\u22121x using a fast linear system solver."}, {"heading": "3.4 Approximate Power Iteration", "text": "We are now ready to prove our main result. We show that each iteration of the shifted-and-inverted power method makes constant factor expected progress on our potential function assuming we:\n1. Start with a sufficiently good x and an approximation of \u03bb1\n2. Can apply B\u22121 approximately using a system solver such that the function error (i.e. distance to B\u22121x in the B norm) is sufficiently small in expectation.\n3. Can estimate Rayleigh quotients over \u03a3 well enough to only accept updates that do not hurt progress on the objective function too much.\nThis third assumption is necessary since the second assumption is quite weak. An expected progress bound on the linear system solver allows, for example, the solver to occasionally return a solution that is entirely orthogonal to v1, causing us to make unbounded backwards progress on our potential function. The third assumption allows us to reject possibly harmful updates and ensure that we still make progress in expectation. In the offline setting, we can access A and are able to compute Rayleigh quotients exactly in time nnz(A) time. However, we only assume the ability to estimate quotients since in the online setting we only have access to \u03a3 through samples from D.\nOur general theorem for the approximate power iteration, Theorem 5, assumes that we can solve linear systems to some absolute accuracy in expectation. This is not completely standard. Typically, system solver analysis assumes an initial approximation to B\u22121x and then shows a relative progress bound \u2013 that the quality of the initial approximation is improved geometrically in each iteration of the algorithm. In Corollary 6 we show how to find a coarse initial approximation\nto B\u22121x, in fact just approximating B\u22121 with 1 x>Bx\nx. Using this approximation, we show that Theorem 5 actually implies that traditional system solver relative progress bounds suffice.\nNote that in both claims we measure error of the linear system solver using \u2016\u00b7\u2016B. This is a natural norm in which geometric convergence is shown for many linear system solvers and directly corresponds to the function error of minimizing f(w) = 12w >Bw \u2212 w>x to compute B\u22121x.\nTheorem 5 (Approximate Shifted-and-Inverted Power Iteration \u2013 Warm Start). Let x = \u2211\ni \u03b1ivi be a unit vector such that G(x) \u2264 1\u221a\n10 . Suppose we know some shift parameter \u03bb with ( 1 + gap150 ) \u03bb1 <\n\u03bb \u2264 ( 1 + gap100 ) \u03bb1 and an estimate \u03bb\u03021 of \u03bb1 such that 10 11 (\u03bb\u2212 \u03bb1) \u2264 \u03bb\u2212 \u03bb\u03021 \u2264 \u03bb\u2212 \u03bb1. Furthermore, suppose we have a subroutine solve(\u00b7) such that on any input x\nE [\u2225\u2225solve (x)\u2212B\u22121x\u2225\u2225\nB\n] \u2264 c1\n1000\n\u221a \u03bb1(B\u22121),\nfor some c1 < 1, and a subroutine q\u0302uot (\u00b7) that on any input x 6= 0\u2223\u2223\u2223q\u0302uot (x)\u2212 quot(x)\u2223\u2223\u2223 \u2264 1 30 (\u03bb\u2212 \u03bb1) for all nonzero x \u2208 Rd.\nwhere quot(x) def = x >\u03a3x x>x\n. Then the following update procedure:\nSet x\u0302 = solve (x) ,\nSet x\u0303 =  x\u0302 if { q\u0302uot (x\u0302) \u2265 \u03bb\u03021 \u2212 ( \u03bb\u2212 \u03bb\u03021 ) /6 and \u2016x\u0302\u20162 \u2265 2 3 1\n\u03bb\u2212\u03bb\u03021 x otherwise,\nsatisfies the following:\n\u2022 G(x\u0303) \u2264 1\u221a 10 and\n\u2022 E [G(x\u0303)] \u2264 325G(x) + c1 500 .\nThat is, not only do we decrease our potential function by a constant factor in expectation, but we are guaranteed that the potential function will never increase beyond 1/ \u221a 10.\nProof. The first claim follows directly from our choice of x\u0303 from x and x\u0302. If x\u0303 = x, it holds trivially by our assumption that G(x) \u2264 1\u221a\n10 . Otherwise, x\u0303 = x\u0302 and we know that \u03bb1 \u2212 quot (x\u0302) \u2264 \u03bb\u03021 \u2212 quot (x\u0302) \u2264 \u03bb\u03021 \u2212 q\u0302uot (x\u0302) + \u2223\u2223\u2223q\u0302uot (x\u0302)\u2212 quot (x\u0302)\u2223\u2223\u2223\n\u2264 \u03bb\u2212 \u03bb\u03021 6 + \u03bb\u2212 \u03bb1 30 \u2264 \u03bb\u2212 \u03bb1 5 \u2264 \u03bb1 \u00b7 gap 500 .\nThe claim then follows from Lemma 3 as G(x\u0302)2 \u2264 (\n1 + \u03bb\u2212 \u03bb1 \u03bb1 \u00b7 gap\n)( 1 +\n2 (\u03bb1 \u2212 quot (x\u0302)) \u03bb1 \u00b7 gap\n) \u03bb1 \u2212 quot (x\u0302)\n\u03bb\u2212 \u03bb1\n\u2264 101 100 \u00b7 251 250 \u00b7\n( \u03bb1\u00b7gap\n500 ) ( \u03bb1\u00b7gap\n150\n) \u2264 1\u221a 10 .\nAll that remains is to show the second claim, that E [G(x\u0303)] \u2264 325G(x) + 4c1 1000 . Let F denote the event that we accept our iteration and set x = x\u0302 = solve (x). That is:\nF def= { q\u0302uot (x\u0302) \u2265 \u03bb\u03021 \u2212\n\u03bb\u2212 \u03bb\u03021 6 } \u222a { \u2016x\u0302\u20162 \u2265 2 3\n1\n\u03bb\u2212 \u03bb\u03021\n} .\nUsing our bounds on \u03bb\u03021 and q\u0302uot (\u00b7), we know that q\u0302uot (x) \u2264 quot(x) + (\u03bb\u2212 \u03bb1)/30 and \u03bb\u2212 \u03bb\u03021 \u2264 \u03bb\u2212 \u03bb1. Therefore, since \u22121/6\u2212 1/30 \u2265 \u22121/2 we have\nF \u2286 {quot (x\u0302) \u2265 \u03bb1 \u2212 (\u03bb\u2212 \u03bb1) /2} \u222a { \u2016x\u0302\u20162 \u2265 2\n3\n1\n\u03bb\u2212 \u03bb1\n} ,\nWe will complete the proof in two steps. First we let \u03be def = x\u0302 \u2212B\u22121x and show that assuming F is true then G(x\u0302) and \u2016\u03be\u2016B are linearly related, i.e. expected error bounds on \u2016\u03be\u2016B correspond to expected error bounds on G(x\u0302). Second, we bound the probability that F does not occur and bound error incurred in this case. Combining these yields the result.\nTo show the linear relationship in the case where F is true, first note Lemma 1 shows that in this case \u2223\u2223\u2223v>1 x\u0302\u2016x\u0302\u20162 \u2223\u2223\u2223 \u2265\u221a1\u2212 \u03bb1\u2212quot(x\u0302)\u03bb1\u00b7gap \u2265 34 . Consequently, \u2016Pv1 x\u0302\u2016B = \u2223\u2223\u2223v>1 x\u0302\u2223\u2223\u2223\u221a\u03bb\u2212 \u03bb1 = \u2223\u2223\u2223\u2223v>1 x\u0302\u2016x\u0302\u20162 \u2223\u2223\u2223\u2223 \u00b7 \u2016x\u0302\u2016\u221a\u03bb\u2212 \u03bb1 \u2265 34 \u00b7 23 1\u221a\u03bb\u2212 \u03bb1 = \u221a \u03bb1(B\u22121) 2 . However, \u2225\u2225\u2225Pv\u22a51 x\u0302\u2225\u2225\u2225B \u2264 \u2225\u2225\u2225Pv\u22a51 B\u22121x\u2225\u2225\u2225B + \u2225\u2225\u2225Pv\u22a51 \u03be\u2225\u2225\u2225B \u2264 \u2225\u2225\u2225Pv\u22a51 B\u22121x\u2225\u2225\u2225B + \u2016\u03be\u2016B and by Theorem 4 and the definition of G we have\u2225\u2225\u2225Pv\u22a51 B\u22121x\u2225\u2225\u2225B = \u2225\u2225Pv1B\u22121x\u2225\u2225B \u00b7G(B\u22121x) \u2264 (|\u3008x, v1\u3009|\u221a\u03bb1(B\u22121)) \u00b7 G(x)100 . Taking expectations, using that |\u3008x, v1\u3009| \u2264 1, and combining these three inequalities yields\nE [G(x\u0302)|F ] = E  \u2225\u2225\u2225Pv\u22a51 B\u22121x\u2225\u2225\u2225B \u2016Pv1B\u22121x\u2016B \u2223\u2223\u2223\u2223\u2223\u2223F  \u2264 G(x) 50 + 2 E [\u2016\u03be\u2016B|F ]\u221a \u03bb1(B\u22121)\n(2)\nSo, conditioning on making an update and changing x (i.e. F occurring), we see that our potential function changes exactly as in the exact case (Theorem 4) with additional additive error due to our inexact linear system solve.\nNext we upper bound P [F ] and use it to compute E [\u2016\u03be\u2016B|F ]. We will show that G def= { \u2016\u03be\u2016B \u2264 1 100 \u00b7 \u221a \u03bb1 (B\u22121) } \u2286 F\nwhich then implies by Markov inequality that P [F ] \u2265 P [ \u2016\u03be\u2016B \u2264 1 100 \u00b7 \u221a \u03bb1 (B\u22121) ] \u2265 1\u2212 E [\u2016\u03be\u2016B] 1 100 \u00b7 \u221a \u03bb1 (B\u22121) \u2265 9 10 , (3)\nwhere we used the fact that E[\u2016\u03be\u2016B] \u2264 c1 1000 \u221a \u03bb1(B\u22121) for some c1 < 1.\nLet us now show that G \u2286 F . Suppose G is occurs. We can bound \u2016x\u0302\u20162 as follows: \u2016x\u0302\u20162 \u2265 \u2225\u2225B\u22121x\u2225\u2225 2 \u2212 \u2016\u03be\u20162 \u2265 \u2225\u2225B\u22121x\u2225\u2225\u2212\u221a\u03bb1 (B\u22121) \u2016\u03be\u2016B \u2265 |\u03b11|\u03bb1 ( B\u22121 ) \u2212 1\n100 \u00b7 \u03bb1\n( B\u22121 ) = 1\n\u03bb\u2212 \u03bb1\n( |\u03b11| \u2212 1\n100\n) \u2265 3\n4\n1\n\u03bb\u2212 \u03bb1 , (4)\nwhere we use Lemmas 2 and 3 to conclude that |\u03b11| \u2265 \u221a\n1\u2212 110 . We now turn to showing the Rayleigh quotient condition required by F . In order to do this, we first bound x\u0302>Bx\u0302\u2212 ( v>1 Bx\u0302 ) ( v>1 x\u0302 ) and then use Lemma 2. We have:\u221a\nx\u0302>Bx\u0302\u2212 ( v>1 Bx\u0302 ) ( v>1 x\u0302 ) = \u2225\u2225\u2225Pv\u22a51 x\u0302\u2225\u2225\u2225B \u2264 \u2225\u2225\u2225Pv\u22a51 B\u22121x\u2225\u2225\u2225B + \u2225\u2225\u2225Pv\u22a51 \u03be\u2225\u2225\u2225B\n\u2264 \u221a\u2211\ni\u22652 \u03b12i\u03bbi (B\n\u22121) + 1 100 \u00b7 \u221a \u03bb1 (B\u22121)\n\u2264 \u221a \u03bb2 (B\u22121) + 1 100 \u00b7 \u221a \u03bb1 (B\u22121) \u2264 1 9 \u221a \u03bb\u2212 \u03bb1,\nwhere we used the fact that \u03bb2 ( B\u22121 ) \u2264 1100\u03bb1 ( B\u22121 ) since \u03bb \u2264 \u03bb1 + gap100 in the last step. Now, using Lemma 2 and the bound on \u2016x\u0302\u20162, we conclude that\n\u03bb\u03021 \u2212 q\u0302uot (x\u0302) \u2264 \u03bb1 \u2212 quot (x\u0302) + \u2223\u2223\u2223quot (x\u0302)\u2212 q\u0302uot (x\u0302)\u2223\u2223\u2223+ \u03bb\u03021 \u2212 \u03bb1\n\u2264 x\u0302>Bx\u0302\u2212\n( v>1 Bx\u0302 ) ( v>1 x\u0302 ) \u2016x\u0302\u201622 + \u03bb\u2212 \u03bb1 30 + \u03bb\u2212 \u03bb1 11\n\u2264 1 81 (\u03bb\u2212 \u03bb1) \u00b7 16 9 (\u03bb\u2212 \u03bb1)2 + \u03bb\u2212 \u03bb1 8\n\u2264 (\u03bb\u2212 \u03bb1) /6 \u2264 ( \u03bb\u2212 \u03bb\u03021 ) /4. (5)\nCombining (4) and (5) shows that G \u2286 F there by proving (3). Using this and the fact that \u2016\u00b7\u2016B \u2265 0 we can upper bound E [\u2016\u03be\u2016B|F ] as follows:\nE [\u2016\u03be\u2016B|F ] \u2264 1\nP [F ] \u00b7 E [\u2016\u03be\u2016B] \u2264 c1 900 \u00b7 \u221a \u03bb1(B\u22121)\nPlugging this into (2), we obtain:\nE [G(x\u0302)|F ] \u2264 1 50 G(x) + 2E [\u2016\u03be\u2016B|F ]\u221a \u03bb1(B\u22121) \u2264 1 50 \u00b7G(x) + 2c1 900 .\nWe can now finally bound E [G(x\u0303)] as follows:\nE [G(x\u0303)] = P [F ] \u00b7 E [G(x\u0302)|F ] + (1\u2212 P [F ])G(x)\n\u2264 9 10\n( 1\n50 \u00b7G(x) + 2c1 900\n) + 1\n10 G(x) =\n3\n25 G(x) + 2c1 1000 .\nThis proves the theorem.\nCorollary 6 (Relative Error Linear System Solvers). For any unit vector x, we have:\u2225\u2225\u2225\u2225 1x>Bxx\u2212B\u22121x \u2225\u2225\u2225\u2225\nB\n\u2264 \u03b11 \u221a \u03bb1(B\u22121) \u00b7G(x) = \u03bb1 ( B\u22121 )\u221a\u221a\u221a\u221a\u2211 i\u22652 \u03b12i \u03bbi (B\u22121) , (6)\nwhere x = \u2211\ni \u03b1ivi is the decomposition of x along vi. Therefore, instantiating Theorem 5 with c1 = \u03b11G(x) gives E[G(x\u0303)] \u2264 425G(x) as long as:\nE [\u2225\u2225solve (x)\u2212B\u22121x\u2225\u2225\nB\n] \u2264 1\n1000 \u2225\u2225\u2225\u2225 1\u03bb\u2212 x>\u03a3xx\u2212B\u22121x \u2225\u2225\u2225\u2225\nB\n.\nProof. Since B is PSD we see that if we let f(w) = 12w >Bw \u2212 w>x, then the minimizer is B\u22121x. Furthermore note that 1 x>Bx\n= arg min\u03b2 f(\u03b2x) and therefore\u2225\u2225\u2225\u2225 1x>Bxx\u2212B\u22121x \u2225\u2225\u2225\u22252\nB\n= x>B\u22121x\u2212 1 x>Bx = 2 [ f ( x x>Bx ) \u2212 f(B\u22121x) ] =2 [ min \u03b2 f(\u03b2x)\u2212 f(B\u22121x) ] \u2264 2 [ f(\u03bb1 ( B\u22121 ) x)\u2212 f(B\u22121x)\n] =\u03bb1 ( B\u22121 )2 x>Bx\u2212 2\u03bb1 ( B\u22121 ) x>x+ x>B\u22121x\n= d\u2211 i=1 \u2223\u2223\u2223v>i B 12x\u2223\u2223\u22232 (\u03bb1 (B\u22121)\u2212 \u03bbi (B\u22121))2 \u2264 \u03bb1 (B\u22121)2\u2211 i\u22652 \u2223\u2223\u2223v>i B 12x\u2223\u2223\u22232 =\u03bb1 ( B\u22121\n)2\u2211 i\u22652 \u03b12i \u03bbi (B\u22121) ,\nwhich proves (6). Consequently\nc1 1000\n\u221a \u03bb1(B\u22121) = 1\n1000 \u03b11G(x)\n\u221a \u03bb1(B\u22121) \u2265 1\n1000 \u2225\u2225\u2225\u2225 1x>Bxx\u2212B\u22121x \u2225\u2225\u2225\u2225\nB\nwhich with Theorem 5 then completes the proof."}, {"heading": "3.5 Initialization", "text": "Theorem 5 and Corollary 6 show that, given a good enough approximation to v1, we can rapidly refine this approximation by applying the shifted-and-inverted power method. In this section, we cover initialization. That is, how to obtain a good enough approximation to apply these results.\nWe first give a simple bound on the quality of a randomly chosen start vector x0.\nLemma 7 (Random Initialization Quality). Suppose x \u223c N (0, I), and we initialize x0 as x\u2016x\u20162 , then with probability greater than 1\u2212O ( 1 d10 ) , we have:\nG(x0) \u2264 \u221a \u03ba(B\u22121)d10.5 \u2264 15 1\u221a\ngap \u00b7 d10.5\nwhere \u03ba(B\u22121) = \u03bb1(B \u22121)/\u03bbd(B \u22121).\nProof.\nG(x0) =G(x) = \u2225\u2225\u2225Pv\u22a51 x\u2225\u2225\u2225B \u2016Pv1x\u2016B = \u221a \u2016x\u20162B \u2212 ( v>1 B 1/2x )2\u2223\u2223v>1 B1/2x\u2223\u2223 = \u221a\u2211 i\u22652 (v>i x) 2 \u03bbi(B\u22121)\u221a (v>1 x) 2\n\u03bb1(B\u22121)\n,\n\u2264 \u221a \u03ba(B\u22121) \u00b7\n\u221a\u2211 i\u22652(v > i x)\n2\u2223\u2223v>1 x\u2223\u2223 Since {v>i x}i are independent standard normal Gaussian variables. By standard concentration arguments, with probability greater than 1\u2212 e\u2212\u2126(d), we have \u221a\u2211 i\u22652(v > i x) 2 = O( \u221a d). Meanwhile,\nv>1 x is just a one-dimensional standard Gaussian. It is easy to show P (\u2223\u2223v>1 x\u2223\u2223 \u2264 1d10 ) = O ( 1d10 ), which finishes the proof.\nWe now show that we can rapidly decrease our initial error to obtain the required G(x) \u2264 1\u221a 10\nbound for Theorem 5.\nTheorem 8 (Approximate Shifted-and-Inverted Power Method \u2013 Burn-In). Suppose we initialize x0 as in Lemma 7 and suppose we have access to a subroutine solve (\u00b7) such that\nE [\u2225\u2225solve (x)\u2212B\u22121x\u2225\u2225\nB\n] \u2264 1 3000\u03ba(B\u22121)d21 \u00b7 \u2225\u2225\u2225\u2225 1\u03bb\u2212 x>\u03a3xx\u2212B\u22121x \u2225\u2225\u2225\u2225 B\nwhere \u03ba(B\u22121) = \u03bb1(B \u22121)/\u03bbd(B \u22121). Then the following procedure,\nxt = solve (xt\u22121) / \u2016solve (xt\u22121)\u20162 after T = O ( log d+ log \u03ba(B\u22121)) ) iterations satisfies:\nG(xT ) \u2264 1\u221a 10 ,\nwith probability greater than 1\u2212O( 1 d10 ).\nProof. As before, we first bound the numerator and denominator of G(x\u0302) more carefully as follows: Numerator: \u2225\u2225\u2225Pv\u22a51 x\u0302\u2225\u2225\u2225B \u2264 \u2225\u2225\u2225Pv\u22a51 B\u22121x\u2225\u2225\u2225B + \u2225\u2225\u2225Pv\u22a51 \u03be\u2225\u2225\u2225B \u2264 \u2225\u2225\u2225Pv\u22a51 B\u22121x\u2225\u2225\u2225B + \u2016\u03be\u2016B\n= \u221a\u2211 i\u22652 ( vTi B \u22121/2x )2 + \u2016\u03be\u2016B = \u221a\u2211 i\u22652 \u03b1 2 i\u03bbi (B\n\u22121) + \u2016\u03be\u2016B , Denominator: \u2016Pv1 x\u0302\u2016B \u2265 \u2225\u2225Pv1B\u22121x\u2225\u2225B \u2212 \u2016Pv1\u03be\u2016B \u2265 \u2225\u2225Pv1B\u22121x\u2225\u2225B \u2212 \u2016\u03be\u2016B = \u2223\u2223vTi B\u22121/2x\u2223\u2223\u2212 \u2016\u03be\u2016B = \u03b11\u221a\u03bb1 (B\u22121)\u2212 \u2016\u03be\u2016B\nWe now use the above estimates to bound G(x\u0302).\nG(x\u0302) \u2264\n\u221a\u2211 i\u22652 \u03b1 2 i\u03bbi (B\n\u22121) + \u2016\u03be\u2016B \u03b11 \u221a \u03bb1 (B\u22121)\u2212 \u2016\u03be\u2016B\n\u2264 \u03bb2 ( B\u22121 )\u221a\u2211 i\u22652 \u03b12i \u03bbi(B\u22121) + \u2016\u03be\u2016B\n\u03bb1 (B\u22121) \u221a\n\u03b121 \u03bb1(B\u22121) \u2212 \u2016\u03be\u2016B\n= G(x) \u03bb2 ( B\u22121 ) + \u2016\u03be\u2016B / \u221a\u2211 i\u22652 \u03b12i \u03bbi(B\u22121)\n\u03bb1 (B\u22121)\u2212 \u2016\u03be\u2016B / \u221a \u03b121 \u03bb1(B\u22121)\nBy Lemma 7, we know with at least probability 1\u2212O( 1 d10 ), we have G(x0) \u2264 \u221a \u03ba(B\u22121)d10.5.\nConditioned on high probability result of G(x0), we now use induction to prove G(xt) \u2264 G(x0). It trivially holds for t = 0. Suppose we now have G(x) \u2264 G(x0), then by the condition in Theorem 8 and Markov inequality, we know with probability greater than 1\u2212 1\n100 \u221a \u03ba(B\u22121)d10.5 we have:\n\u2016\u03be\u2016B \u2264 1 30 \u221a \u03ba(B\u22121)d10.5 \u00b7 \u2225\u2225\u2225\u2225 1\u03bb\u2212 x>\u03a3xx\u2212B\u22121x \u2225\u2225\u2225\u2225 B\n\u2264 1 30 \u00b7 \u2225\u2225\u2225\u2225 1\u03bb\u2212 x>\u03a3xx\u2212B\u22121x \u2225\u2225\u2225\u2225 B min { 1, 1 G(x0) } \u2264 1 30 \u00b7 \u2225\u2225\u2225\u2225 1\u03bb\u2212 x>\u03a3xx\u2212B\u22121x \u2225\u2225\u2225\u2225 B min { 1, 1 G(x) }\n\u2264 \u03bb1 ( B\u22121 ) \u2212 \u03bb2 ( B\u22121 ) 4 min  \u221a\u221a\u221a\u221a\u2211\ni\u22652\n\u03b12i \u03bbi (B\u22121) ,\n\u221a \u03b121\n\u03bb1 (B\u22121)  The last inequality uses Corollary 6 with the fact that \u03bb2 ( B\u22121 ) \u2264 1100\u03bb1 ( B\u22121 ) . Therefore, we have: We will have:\nG(x\u0302) \u2264 \u03bb1 ( B\u22121 ) + 3\u03bb2 ( B\u22121 ) 3\u03bb1 (B\u22121) + \u03bb2 (B\u22121) \u00d7G(x) \u2264 1 2 G(x)\nThis finishes the proof of induction. Finally, by union bound, we know with probability greater than 1 \u2212 O( 1\nd10 ) in T = O(log d +\nlog \u03ba(B\u22121)) steps, we have:\nG(xT ) \u2264 1\n2T G(x0) \u2264 1\u221a 10"}, {"heading": "4 Offline Eigenvector Computation", "text": "In this section we show how to instantiate the framework of Section 3 in order to compute an approximate top eigenvector in the offline setting. As discussed, in the offline setting we can trivially compute the Rayleigh quotient of a vector in nnz(A) time as we have explicit access to A>A. Consequently the bulk of our work in this section is to show how we can solve linear systems in B efficiently in expectation, allowing us to apply Corollary 6 of Theorem 5.\nIn Section 4.1 we first show how Stochastic Variance Reduced Gradient (SVRG) [JZ13] can be adapted to solve linear systems of the form Bx = b. If we wanted, for example, to solve a linear system in a positive definite matrix like A>A, we would optimize the objective function f(x) = 12x\n>A>Ax \u2212 b>x. This function can be written as the sum of n convex components, \u03c8i(x) = 1 2x > (aia>i )x\u2212 1nb>x. In each iteration of traditional gradient descent, one computes the full gradient of f(xi) and takes a step in that direction. In stochastic gradient methods, at each iteration, a single component is sampled, and the step direction is based only on the gradient of the sampled component. Hence, we avoid a full gradient computation at each iteration, leading to runtime gains.\nUnfortunately, while we have access to the rows of A and so can solve systems in A>A, it is less clear how to solve systems in B = \u03bbI\u2212A>A. To do this, we will split our function into components of the form \u03c8i(x) = 1 2x > (wiI\u2212 aia>i )x\u2212 1nb>x for some set of weights wi with \u2211i\u2208[n]wi = \u03bb.\nImportantly, (wiI \u2212 aia>i ) may not be positive semidefinite. That is, we are minimizing a sum of functions which is convex, but consists of non-convex components. While recent results for minimizing such functions could be applied directly [SS15, CR15] here we show how to obtain stronger results by using a more general form of SVRG and analyzing the specific properties of our function (i.e. the variance).\nOur analysis shows that we can make constant factor progress in solving linear systems in B in time O (\nnnz(A) + d sr(A) gap2\n) . If d sr(A)\ngap2 \u2264 nnz(A) this gives a runtime proportional to the input size \u2013\nthe best we could hope for. If not, we show in Section 4.2 that it is possible to accelerate our system solver, achieving runtime O\u0303 (\nnnz(A)3/4(d sr(A))1/4\u221a gap\n) . This result uses the work of [FGKS15b, LMH15]\non accelerated approximate proximal point algorithms. With our solvers in place, in Section 4.3 we pull our results together, showing how to use these solvers in the framework of Section 3 to give faster running times for offline eigenvector computation."}, {"heading": "4.1 SVRG Based Solver", "text": "Here we provide a sampling based algorithm for solving linear systems in B. In particular we provide an algorithm for solving the more general problem where we are given a strongly convex function that is a sum of possibly non-convex functions that obey smoothness properties. We provide a general result on bounding the progress of an algorithm that solves such a problem by non-uniform sampling in Theorem 9 and then in the remainder of this section we show how to bound the requisite quantities for solving linear systems in B.\nTheorem 9 (SVRG for Sums of Non-Convex Functions). Consider a set of functions, {\u03c81, \u03c82, ...\u03c8n}, each mapping Rd \u2192 R. Let f(x) = \u2211 i \u03c8i(x) and let x\nopt def= arg minx\u2208Rd f(x). Suppose we have a probability distribution p on [n], and that starting from some initial point x0 \u2208 Rd in each iteration k we pick ik \u2208 [n] independently with probability pik and let\nxk+1 := xk \u2212 \u03b7\npi (5\u03c8i(xk)\u22125\u03c8i(x0)) + \u03b75 f(x0)\nfor some \u03b7. If f is \u00b5-strongly convex and if for all x \u2208 Rd we have\u2211 i\u2208[n] 1 pi \u2225\u22255\u03c8i(x)\u22125\u03c8i(xopt)\u2225\u222522 \u2264 2S [f(x)\u2212 f(xopt)] , (7) where S is a variance parameter, then for all m \u2265 1 we have\nE  1 m \u2211 k\u2208[m] f(xk)\u2212 f(xopt)  \u2264 1 1\u2212 2\u03b7S\u0304 [ 1 \u00b5\u03b7m + 2\u03b7S ] \u00b7 [ f(x0)\u2212 f(xopt)\n] Consequently, if we pick \u03b7 to be a sufficiently small multiple of 1/S\u0304 then when m = O(S/\u00b5) we can decrease the error by a constant multiplicative factor in expectation.\nProof. We first note that Eik [xk+1 \u2212 xk] = \u03b75 f(xk). This is, in each iteration, in expectation, we\nmake a step in the direction of the gradient. Using this fact we have: Eik \u2225\u2225xk+1 \u2212 xopt\u2225\u222522 = Eik \u2225\u2225(xk+1 \u2212 xk) + (xk \u2212 xopt)\u2225\u222522\n= \u2225\u2225xk \u2212 xopt\u2225\u222522 \u2212 2Eik(xk+1 \u2212 xk)>(xk \u2212 xopt) + Eik \u2016xk+1 \u2212 xk\u201622\n= \u2225\u2225xk \u2212 xopt\u2225\u222522 \u2212 2\u03b75 f(xk)> (xk \u2212 xopt)\n+ \u2211 i\u2208[n] \u03b72pi \u2225\u2225\u2225\u2225 1pi (5\u03c8i(xk)\u22125\u03c8i(x0)) +5f(x0) \u2225\u2225\u2225\u22252 2\nWe now apply the fact that \u2016x+ y\u201622 \u2264 2 \u2016x\u2016 2 2 + 2 \u2016y\u2016 2 2 to give:\u2211\ni\u2208[n]\npi \u2225\u2225\u2225\u2225 1pi (5\u03c8i(xk)\u22125\u03c8i(x0)) +5f(x0) \u2225\u2225\u2225\u22252\n2\n\u2264 \u2211 i\u2208[n] 2pi \u2225\u2225\u2225\u2225 1pi (5\u03c8i(xk)\u22125\u03c8i(xopt)) \u2225\u2225\u2225\u22252 2 + \u2211 i\u2208[n] 2pi \u2225\u2225\u2225\u2225 1pi (5\u03c8i(x0)\u22125\u03c8i(xopt))\u22125f(x0) \u2225\u2225\u2225\u22252 2 .\nThen, using that 5f(xopt) = 0 by optimality, that E \u2016x\u2212 Ex\u201622 \u2264 E \u2016x\u2016 2 2, and (7) we have:\u2211\ni\u2208[n]\npi \u2225\u2225\u2225\u2225 1pi (5\u03c8i(xk)\u22125\u03c8i(x0)) +5f(x0) \u2225\u2225\u2225\u22252\n2\n\u2264 \u2211 i\u2208[n] 2 pi \u2225\u22255\u03c8i(xk)\u22125\u03c8i(xopt)\u2225\u222522 + \u2211 i\u2208[n] 2pi \u2225\u2225\u2225\u2225 1pi (5\u03c8i(x0)\u22125\u03c8i(xopt))\u2212 (5f(x0)\u22125f(xopt)) \u2225\u2225\u2225\u22252 2\n\u2264 \u2211 i\u2208[n] 2 pi \u2225\u22255\u03c8i(xk)\u22125\u03c8i(xopt)\u2225\u222522 + \u2211 i\u2208[n] 2pi \u2225\u2225\u2225\u2225 1pi 5 \u03c8i(x0)\u22125\u03c8i(xopt)) \u2225\u2225\u2225\u22252 2\n\u2264 4S [ f(xk)\u2212 f(xopt) + f(x0)\u2212 f(xopt) ] Since f(xopt)\u2212 f(xk) \u2265 5f(xk)>(xopt \u2212 xk) by the convexity of f , these inequalities imply\nEik \u2225\u2225xk+1 \u2212 xopt\u2225\u222522 \u2264 \u2225\u2225xk \u2212 xopt\u2225\u222522 \u2212 2\u03b7 [f(xk)\u2212 f(xopt)]+ 4\u03b72S [f(xk)\u2212 f(xopt) + f(x0)\u2212 f(xopt)]\n= \u2225\u2225xk \u2212 xopt\u2225\u222522 \u2212 2\u03b7(1\u2212 2\u03b7S) (f(xk)\u2212 f(xopt))+ 4\u03b72S\u0304 (f(x0)\u2212 f(xopt))\nRearranging, we have: 2\u03b7(1\u2212 2\u03b7S) ( f(xk)\u2212 f(xopt) ) \u2264 \u2225\u2225xk \u2212 xopt\u2225\u222522 \u2212 Eik \u2225\u2225xk+1 \u2212 xopt\u2225\u222522 + 4\u03b72S\u0304 (f(x0)\u2212 f(xopt)) .\nAnd summing over all iterations and taking expectations we have:\nE 2\u03b7(1\u2212 2\u03b7S\u0304) \u2211 k\u2208[m] f(xk)\u2212 f(xopt)  \u2264 \u2225\u2225x0 \u2212 xopt\u2225\u222522 + 4m\u03b72S\u0304 [f(x0)\u2212 f(xopt)] . Finally, we use that by strong convexity,\n\u2225\u2225x0 \u2212 xopt\u2225\u222522 \u2264 2\u00b5 (f(x0)\u2212 f(xopt)) to obtain: E\n2\u03b7(1\u2212 2\u03b7S\u0304) \u2211 k\u2208[m] f(xk)\u2212 f(xopt)  \u2264 2 \u00b5 [ f(x0)\u2212 f(xopt) ] + 4m\u03b72S\u0304 [ f(x0)\u2212 f(xopt) ]\nand thus\nE  1 m \u2211 k\u2208[m] f(xk)\u2212 f(xopt)  \u2264 1 1\u2212 2\u03b7S\u0304 [ 1 \u00b5\u03b7m + 2\u03b7S\u0304 ] \u00b7 [ f(x0)\u2212 f(xopt) ]\nTheorem 9 immediately yields a solver for Bx = b. Finding the minimum norm solution to this system is equivalent to minimizing f(x) = 12x\n>Bx\u2212 b>x. If we take the common approach of applying a smoothness bound for each \u03c8i along with a strong convexity bound on f(x) we obtain:\nLemma 10 (Simple Variance Bound for SVRG). Let\n\u03c8i(x) def =\n1 2 x> ( \u03bb \u2016ai\u201622 \u2016A\u20162F I\u2212 aia>i ) x\u2212 1 n b>x\nso we have \u2211\ni\u2208[n] \u03c8i(x) = f(x) = 1 2x >Bx\u2212 b>x. Setting pi = \u2016ai\u201622 \u2016A\u20162F for all i, we have\n\u2211 i\u2208[n] 1 pi \u2225\u22255\u03c8i(x)\u22125\u03c8i(xopt)\u2225\u222522 = O ( \u2016A\u20164F \u03bb\u2212 \u03bb1 [ f(x)\u2212 f(xopt) ])\nProof. We first compute, for all i \u2208 [n]\n5\u03c8i(x) = ( \u03bb \u2016ai\u201622 \u2016A\u20162F I\u2212 aia>i ) x\u2212 1 n b. (8)\nWe have that each \u03c8i is \u03bb\u2016ai\u201622 \u2016A\u20162F + \u2016ai\u20162 smooth with respect to \u2016\u00b7\u20162. Specifically,\n\u2225\u22255\u03c8i(x)\u22125\u03c8i(xopt)\u2225\u22252 = \u2225\u2225\u2225\u2225\u2225 ( \u03bb \u2016ai\u201622 \u2016A\u20162F I\u2212 aia>i ) (x\u2212 xopt) \u2225\u2225\u2225\u2225\u2225 2\n\u2264 ( \u03bb \u2016ai\u201622 \u2016A\u20162F + \u2016ai\u20162 )\u2225\u2225x\u2212 xopt\u2225\u2225 2 .\nAdditionally, f(x) is \u03bbd(B) = \u03bb\u2212\u03bb1 strongly convex so we have \u2225\u2225x\u2212 xopt\u2225\u22252\n2 \u2264 2\u03bb\u2212\u03bb1\n[ f(x)\u2212 f(xopt) ] and putting all this together we have\n\u2211 i\u2208[n] 1 pi \u2225\u22255\u03c8i(x)\u22125\u03c8i(xopt)\u2225\u222522 \u2264\u2211 i\u2208[n] \u2016A\u20162F \u2016ai\u201622 \u00b7 \u2016ai\u201642\n( \u03bb\n\u2016A\u20162F + 1 )2 \u00b7 2 \u03bb\u2212 \u03bb1 [ f(x)\u2212 f(xopt) ] = O ( \u2016A\u20164F \u03bb\u2212 \u03bb1 [ f(x)\u2212 f(xopt) ])\nwhere the last step uses that \u03bb \u2264 2\u03bb1 \u2264 2 \u2016A\u20162F so \u03bb \u2016A\u20162F \u2264 2.\nAssuming that \u03bb = (1 + c \u00b7 gap)\u03bb1 for some constant c, the above bound means that we can make constant progress on our linear system by setting m = O(S/\u00b5) = O ( \u2016A\u20164F\n(\u03bb\u2212\u03bb1)2\n) = O ( sr(A)2\ngap2\n) .\nThis dependence on stable rank matches the dependence given in [Sha15c] (see discussion in Section 1.3), however we can show that it is suboptimal. We show to improve the bound to O (\nsr(A) gap2\n) by\nusing a better variance analysis. Instead of bounding each \u2225\u22255\u03c8i(x)\u22125\u03c8i(xopt)\u2225\u222522 term using the smoothness of \u03c8i, we more carefully bound the sum of these terms.\nLemma 11. (Improved Variance Bound for SVRG) For i \u2208 [n] let\n\u03c8i(x) def =\n1 2 x> ( \u03bb \u2016ai\u201622 \u2016A\u20162F I\u2212 aia>i ) x\u2212 1 n b>x\nso we have \u2211\ni\u2208[n] \u03c8i(x) = f(x) = 1 2x >Bx\u2212 b>x. Setting pi = \u2016ai\u201622 \u2016A\u20162F for all i, we have for all x\n\u2211 i\u2208[n] 1 pi \u2225\u22255\u03c8i(x)\u22125\u03c8i(xopt)\u2225\u222522 \u2264 4\u03bb1 \u2016A\u20162F\u03bb\u2212 \u03bb1 \u00b7 [f(x)\u2212 f(xopt)] . Proof. Using the gradient computation in (8) we have\n\u2211 i\u2208[n] 1 pi \u2225\u22255\u03c8i(x)\u22125\u03c8i(xopt)\u2225\u222522 = \u2211 i\u2208[n] \u2016A\u20162F \u2016ai\u201622 \u2225\u2225\u2225\u2225\u2225 ( \u03bb \u2016ai\u201622 \u2016A\u20162F I\u2212 aia>i ) (x\u2212 xopt) \u2225\u2225\u2225\u2225\u2225 2 2\n= \u2211 i\u2208[n] \u03bb2 \u2016ai\u201622 \u2016A\u20162F \u2225\u2225x\u2212 xopt\u2225\u22252 2 \u2212 2 \u2211 i\u2208[n] \u03bb \u2225\u2225x\u2212 xopt\u2225\u22252 aia>i\n+ \u2211 i\u2208[n] \u2016A\u20162F \u2016ai\u20162 \u2225\u2225x\u2212 xopt\u2225\u22252\u2016ai\u201622aia>i = \u03bb2 \u2225\u2225x\u2212 xopt\u2225\u22252 2 \u2212 2\u03bb \u2225\u2225x\u2212 xopt\u2225\u22252 \u03a3 + \u2016A\u20162F \u2225\u2225x\u2212 xopt\u2225\u22252 \u03a3 .\n\u2264 \u03bb \u2225\u2225x\u2212 xopt\u2225\u22252\nB + \u2016A\u20162F \u2225\u2225x\u2212 xopt\u2225\u22252 \u03a3 . (9)\nNow since\n\u03a3 \u03bb1I \u03bb1\n\u03bb\u2212 \u03bb1 B\nwe have \u2211 i\u2208[n] 1 pi \u2225\u22255\u03c8i(x)\u22125\u03c8i(xopt)\u2225\u222522 \u2264 ( \u03bb(\u03bb\u2212 \u03bb1) + \u2016A\u20162F \u00b7 \u03bb1 \u03bb\u2212 \u03bb1 )\u2225\u2225x\u2212 xopt\u2225\u22252 B\n\u2264 ( 2 \u2016A\u20162F \u03bb1 \u03bb\u2212 \u03bb1 )\u2225\u2225x\u2212 xopt\u2225\u22252 B\nwhere in the last inequality we just coarsely bound \u03bb(\u03bb\u2212\u03bb1) \u2264 \u03bb1 \u2016A\u20162F . Now since B is full rank, Bxopt = b, we can compute:\u2225\u2225x\u2212 xopt\u2225\u22252\nB = x>Bx\u2212 2b>x+ b>xopt = 2[f(x)\u2212 f(xopt)]. (10)\nThe result follows.\nPlugging the bound in Lemma 11 into Theorem 9 we have:\nTheorem 12. (Offline SVRG-Based Solver) Let S = 2\u03bb1\u2016\u03a3\u20162F \u03bb\u2212\u03bb1 , \u00b5 = \u03bb\u2212\u03bb1. The iterative procedure described in Theorem 9 with f(x) = 12x >Bx \u2212 b>x, \u03c8i(x) = 12x > ( \u03bb\u2016ai\u201622 \u2016\u03a3\u20162F I\u2212 aia>i ) x \u2212 b>x, pi = \u2016ai\u201622 \u2016\u03a3\u20162F , \u03b7 = 1/(8S) and m chosen uniformly at random from [64S/\u00b5] returns a vector xm such that\nE \u2225\u2225xm \u2212 xopt\u2225\u22252B \u2264 12 \u2225\u2225x0 \u2212 xopt\u2225\u22252B .\nFurther, assuming ( 1 + gap150 ) \u03bb1 < \u03bb \u2264 ( 1 + gap100 ) \u03bb1, this procedure runs in time O ( nnz(A) + d\u00b7sr(A) gap2 ) .\nProof. Lemma 11 tells us that\u2211 i\u2208[n] 1 pi \u2225\u22255\u03c8i(x)\u22125\u03c8i(xopt)\u2225\u222522 \u2264 2S [f(x)\u2212 f(xopt)] . Further f(x) = 12x\n>Bx \u2212 b>x is \u03bbd(B)-strongly convex and \u03bbd(B) = \u03bb \u2212 \u03bb1 = \u00b5. Plugging this into Theorem 9 and using (10) which shows \u2225\u2225x\u2212 xopt\u2225\u22252 B\n= 2[f(x)\u2212 f(xopt)] we have, for m chosen uniformly from [64S/\u00b5]:\nE  1 64S/\u00b5 \u2211 k\u2208[64S/\u00b5] f(xk)\u2212 f(xopt)  \u2264 4/3 \u00b7 [1/8 + 1/8] \u00b7 [f(x0)\u2212 f(xopt)] E [ f(xm)\u2212 f(xopt) ] \u2264 1\n2\n[ f(x0)\u2212 f(xopt) ] E \u2225\u2225xm \u2212 xopt\u2225\u22252B \u2264 12 \u2225\u2225x0 \u2212 xopt\u2225\u22252B .\nThe procedure requires O (nnz(A)) time to initially compute 5f(x0), along with each pi and the step size \u03b7 which depend on \u2016A\u20162F and the row norms of A. Each iteration then just requires O(d) time to compute 5\u03c8i(\u00b7) and perform the necessary vector operations. Since there are at most [64S/\u00b5] = O ( \u03bb1\u2016A\u20162F (\u03bb\u2212\u03bb1)2 ) iterations, our total runtime is\nO ( nnz(A) + d \u00b7\n\u03bb1 \u2016A\u20162F (\u03bb\u2212 \u03bb1)2\n) = O ( nnz(A) +\nd \u00b7 sr(A) gap2\n) .\nNote that if our matrix is uniformly sparse - i.e. all rows have sparsity at most ds, then the runtime is actually at most O (\nnnz(A) + ds\u00b7sr(A) gap2\n) ."}, {"heading": "4.2 Accelerated Solver", "text": "Theorem 12 gives a linear solver for B that makes progress in expectation and which we can plug into Theorems 5 and 8. However, we first show that the runtime in Theorem 12 can be accelerated in some cases. We apply a result of [FGKS15b], which shows that, given a solver for a regularized version of a convex function f(x), we can produce a fast solver for f(x) itself. Specifically:\nLemma 13 (Theorem 1.1 of [FGKS15b]). Let f(x) be a \u00b5-strongly convex function and let xopt def = arg minx\u2208Rd f(x). For any \u03b3 > 0 and any x0 \u2208 Rd, let f\u03b3,x0(x) def = f(x) + \u03b32 \u2016x\u2212 x0\u2016 2 2. Let x opt \u03b3,x0 def = arg minx\u2208Rd f\u03b3,x0(x). Suppose that, for all x0 \u2208 Rd, c > 0, \u03b3 > 0, we can compute a point xc such that\nEf\u03b3,x0(xc)\u2212 f\u03b3,x0(xopt\u03b3,x0) \u2264 1\nc\n[ f\u03b3,x0 \u2212 f\u03b3,x0(xopt\u03b3,x0) ] in time Tc. Then given any x0, c > 0, \u03b3 > 2\u00b5, we can compute x1 such that\nEf(x1)\u2212 f(xopt) \u2264 1\nc\n[ f(x0)\u2212 f(xopt) ] in time O ( T\n4 (\n2\u03b3+\u00b5 \u00b5 )3/2\u221ad\u03b3/\u00b5e log c) . We first give a new variance bound on solving systems in B when a regularizer is used. The\nproof of this bound is very close to the proof given for the unregularized problem in Lemma 11.\nLemma 14. For i \u2208 [n] let\n\u03c8i(x) def =\n1 2 x> ( \u03bb \u2016ai\u201622 \u2016A\u20162F I\u2212 aia>i ) x\u2212 1 n b>x+ \u03b3 \u2016ai\u201622 2 \u2016A\u20162F \u2016x\u2212 x0\u201622\nso we have \u2211\ni\u2208[n] \u03c8i(x) = f\u03b3,x0(x) = 1 2x >Bx\u2212 b>x+ \u03b32 \u2016x\u2212 x0\u2016 2 2. Setting pi = \u2016ai\u201622 \u2016A\u20162F for all i, we\nhave for all x\u2211 i\u2208[n] 1 pi \u2225\u22255\u03c8i(x)\u22125\u03c8i(xopt\u03b3,x0)\u2225\u222522 \u2264 ( \u03b32 + 12\u03bb1 \u2016A\u20162F \u03bb\u2212 \u03bb1 + \u03b3 )[ f\u03b3,x0(x)\u2212 f\u03b3,x0(xopt\u03b3,x0) ] Proof. We have for all i \u2208 [n]\n5\u03c8i(x) = ( \u03bb \u2016ai\u201622 \u2016A\u20162F I\u2212 aia>i ) x\u2212 1 n b+ \u03b3 \u2016ai\u201622 2 \u2016A\u20162F (x\u2212 2x0) (11)\nPlugging this in we have:\u2211 i\u2208[n] 1 pi \u2225\u22255\u03c8i(x)\u22125\u03c8i(xopt\u03b3,x0)\u2225\u222522 = \u2211 i\u2208[n] \u2016A\u20162F \u2016ai\u201622 \u2225\u2225\u2225\u2225\u2225 ( \u03bb \u2016ai\u201622 \u2016A\u20162F I\u2212 aia>i ) (x\u2212 xopt\u03b3,x0) + \u03b3 \u2016ai\u201622 2 \u2016A\u20162F (x\u2212 xopt\u03b3,x0) \u2225\u2225\u2225\u2225\u2225 2 2\nFor simplicity we now just use the fact that \u2016x+ y\u201622 \u2264 2 \u2016x\u2016 2 2 + 2 \u2016y\u2016 2 2 and apply our bound from equation (9) to obtain:\u2211 i\u2208[n] 1 pi\n\u2225\u22255\u03c8i(x)\u22125\u03c8i(xopt\u03b3,x0)\u2225\u222522 \u2264 2\u03bb2 \u2225\u2225x\u2212 xopt\u03b3,x0\u2225\u222522 \u2212 4\u03bb\u2225\u2225x\u2212 xopt\u03b3,x0\u2225\u22252\u03a3 + 2 \u2016\u03a3\u20162F \u2225\u2225x\u2212 xopt\u03b3,x0\u2225\u22252\u03a3 + 2\n\u2211 i\u2208[n] \u2016ai\u201622 \u2016A\u20162F \u03b32 4 \u2225\u2225x\u2212 xopt\u03b3,x0\u2225\u222522 \u2264 ( 2\u03bb2 + \u03b32/2 + 2\u03bb1 \u2016A\u20162F \u2212 4\u03bb1\u03bb )\u2225\u2225x\u2212 xopt\u03b3,x0\u2225\u222522\n\u2264 ( \u03b32/2 + 6\u03bb1 \u2016A\u20162F )\u2225\u2225x\u2212 xopt\u03b3,x0\u2225\u222522\nNow, f\u03b3,x0(\u00b7) is \u03bb\u2212 \u03bb1 + \u03b3 strongly convex, so\u2225\u2225x\u2212 xopt\u03b3,x0\u2225\u222522 \u2264 2\u03bb\u2212 \u03bb1 + \u03b3 [f\u03b3,x0(x)\u2212 f\u03b3,x0(xopt\u03b3,x0)] . So overall we have:\n\u2211 i\u2208[n] 1 pi \u2225\u22255\u03c8i(x)\u22125\u03c8i(xopt\u03b3,x0)\u2225\u222522 \u2264 ( \u03b32 + 12\u03bb1 \u2016A\u20162F \u03bb\u2212 \u03bb1 + \u03b3 )[ f\u03b3,x0(x)\u2212 f\u03b3,x0(xopt\u03b3,x0) ]\nWe can now use this variance bound to obtain an accelerated solver for B. We assume nnz(A) \u2264 d sr(A) gap2\n, as otherwise, the unaccelerated solver in Theorem 12 runs in O(nnz(A)) time and cannot be accelerated further.\nTheorem 15 (Accelerated SVRG-Based Solver). Assuming ( 1 + gap150 ) \u03bb1 < \u03bb \u2264 ( 1 + gap100 ) \u03bb1 and nnz(A) \u2264 d sr(A) gap2\n, applying the iterative procedure described in Theorem 9 along with the acceleration given by Lemma 13 gives a solver that returns x with\nE \u2225\u2225x\u2212 xopt\u2225\u22252\nB \u2264 1\n2 \u2225\u2225x0 \u2212 xopt\u2225\u22252B . in time O ( nnz(A)3/4(d sr(A))1/4\u221a gap \u00b7 log ( d gap )) .\nProof. Following Theorem 12, the variance bound of Lemma 14 means that we can make constant progress in minimizing f\u03b3,x0(x) in O (nnz(A) + dm) time where m = O ( \u03b32+12\u03bb1\u2016\u03a3\u20162F\n(\u03bb\u2212\u03bb1+\u03b3)2\n) . So,\nfor \u03b3 \u2265 2(\u03bb \u2212 \u03bb1) we can make 4 (\n2\u03b3+(\u03bb\u2212\u03bb1) \u03bb\u2212\u03bb1\n)3/2 progress, as required by Lemma 13 in time\nO ( (nnz(A) + dm) \u00b7 log (\n\u03b3 \u03bb\u2212\u03bb1\n)) time. Hence by Lemma 13 we can make constant factor expected\nprogress in minimizing f(x) in time:\nO (( nnz(A) + d\n\u03b32 + 12\u03bb1 \u2016A\u20162F (\u03bb\u2212 \u03bb1 + \u03b3)2\n) log ( \u03b3\n\u03bb\u2212 \u03bb1\n)\u221a \u03b3\n\u03bb\u2212 \u03bb1\n)\nBy our assumption, we have nnz(A) \u2264 d sr(A) gap2 = d\u03bb1\u2016A\u20162F (\u03bb\u2212\u03bb1)2 . So, if we let \u03b3 = \u0398 (\u221a d\u03bb1\u2016A\u20162F nnz(A) ) then\nusing a sufficiently large constant, we have \u03b3 \u2265 2(\u03bb\u2212 \u03bb1). We have \u03b3\u03bb\u2212\u03bb1 = \u0398 (\u221a d\u03bb1\u2016A\u20162F nnz(A)\u03bb21gap 2 ) =\n\u0398 (\u221a\nd sr(A) nnz(A)gap2\n) and can make constant expected progress in minimizing f(x) in time:\nO\n( nnz(A)3/4(d sr(A))1/4\n\u221a gap\n\u00b7 log ( d\ngap\n)) ."}, {"heading": "4.3 Shifted-and-Inverted Power Method", "text": "Finally, we are able to combine the solvers from Sections 4.1 and 4.2 with the framework of Section 3 to obtain faster algorithms for top eigenvector computation.\nTheorem 16 (Shifted-and-Inverted Power Method With SVRG). Let B = \u03bbI\u2212A>A for ( 1 + gap150 ) \u03bb1 \u2264\n\u03bb \u2264 ( 1 + gap100 ) \u03bb1 and let x0 \u223c N (0, I) be a random initial vector. Running the inverted power method on B initialized with x0, using the SVRG solver from Theorem 12 to approximately apply B\u22121 at each step, returns x such that with probability 1\u2212O ( 1 d10 ) , x>\u03a3x \u2265 (1\u2212 )\u03bb1 in total time\nO (( nnz(A) + d sr(A)\ngap2\n) \u00b7 ( log2 ( d\ngap\n) + log ( 1 ))) .\nNote that by instantiating the above theorem with \u2032 = \u00b7 gap, and applying Lemma 1 we can find a unit vector x such that |v>1 x| \u2265 1 \u2212 in the same asymptotic running time (an extra log(1/gap) term is absorbed into the log2(d/gap) term).\nProof. By Theorem 8, if we start with x0 \u223c N (0, I) we can run O ( log ( d gap )) iterations of the\ninverted power method, to obtain x1 with G(x1) \u2264 1\u221a10 with probability 1 \u2212 O ( 1 d10 ) . Each iteration requires applying an linear solver that decreases initial error in expectation by a factor of 1\npoly(d,1/gap) . Such a solver is given by applying the solver in Theorem 12 O ( log ( d gap )) times, decreasing error by a constant factor in expectation each time. So overall in order to find x1 with\nG(x1) \u2264 1\u221a10 , we require time O (( nnz(A) + d sr(A) gap2 ) \u00b7 log2 ( d gap )) .\nAfter this initial \u2018burn-in\u2019 period we can apply Corollary 6 of Theorem 5, which shows that running a single iteration of the inverted power method will decrease G(x) by a constant factor in expectation. In such an iteration, we only need to use a solver that decreases initial error by a constant factor in expectation. So we can perform each inverted power iteration in this stage in\ntime O (\nnnz(A) + d sr(A) gap2\n) .\nWith O ( log ( d )) iterations, we can obtain x with E [ G(x)2 ] = O ( d10 ) So by Markov\u2019s inequality, we have G(x)2 = O( ), giving us xT\u03a3x \u2265 (1\u2212O( ))\u03bb1 by Lemma 3. Union bounding over both stages gives us failure probability O ( 1 d10 ) , and adding the runtimes from the two stages gives us\nthe final result. Note that the second stage requires O ( log ( d )) = O(log d+ log(1/ )) iterations to\nachieve the high probability bound. However, the O(log d) term is smaller than the O ( log2 (\nd gap )) term, so is absorbed into the asymptotic notation.\nWe can apply an identical analysis using the accelerated solver from Theorem 15, obtaining the following runtime which beats Theorem 16 whenever nnz(A) \u2264 d sr(A)\ngap2 :\nTheorem 17 (Shifted-and-Inverted Power Method Using Accelerated SVRG). Let B = \u03bbI\u2212A>A for ( 1 + gap150 ) \u03bb1 \u2264 \u03bb \u2264 ( 1 + gap100 ) \u03bb1 and let x0 \u223c N (0, I) be a random initial vector. Assume that nnz(A) \u2264 d sr(A) gap2 . Running the inverted power method on B initialized with x0, using the accelerated SVRG solver from Theorem 15 to approximately apply B\u22121 at each step, returns x such that with probability 1\u2212O ( 1 d10 ) , |v>1 x| \u2265 1\u2212 in total time\nO\n(( nnz(A)3/4(d sr(A))1/4\n\u221a gap\n) \u00b7 ( log3 ( d\ngap\n) + log ( d\ngap\n) log ( 1 ))) ."}, {"heading": "5 Online Eigenvector Computation", "text": "Here we show how to apply the shifted-and-inverted power method framework of Section 3 to the online setting. This setting is more difficult than the offline case. As there is no canonical matrix A, and we only have access to the distribution D through samples, in order to apply Theorem 5 we must show how to both estimate the Rayleigh quotient (Section 5.1) as well as solve the requisite linear systems in expectation (Section 5.2).\nAfter laying this ground work, our main result is given in Section 5.3. Ultimately, the results in this section allow us to achieve more efficient algorithms for computing the top eigenvector in the statistical setting as well as improve upon the previous best known sample complexity for top eigenvector computation. As we show in Section 7 the bounds we provide in this section are in fact tight for general distributions."}, {"heading": "5.1 Estimating the Rayleigh Quotient", "text": "Here we show how to estimate the Rayleigh quotient of a vector with respect to \u03a3. Our analysis is standard \u2013 we first approximate the Rayleigh quotient by its empirical value on a batch of k samples and prove using Chebyshev\u2019s inequality that the error on this sample is small with constant probability. We then repeat this procedure O(log(1/p)) times and output the median. By a Chernoff bound this yields a good estimate with probability 1\u2212 p. The formal statement of this result and its proof comprise the remainder of this subsection.\nTheorem 18 (Online Rayleigh Quotient Estimation). Given \u2208 (0, 1], p \u2208 [0, 1], and unit vector x set k = d4 v(D) \u22122e and m = O(log(1/p)). For all i \u2208 [k] and j \u2208 [m] let a(j)i be drawn independently from D and set Ri,j = x>a(j)i (a (j) i ) >x and Rj = 1 k \u2211 i\u2208[k]Ri,j. If we let z be median\nvalue of the Rj then with probability 1\u2212 p we have \u2223\u2223z \u2212 x>\u03a3x\u2223\u2223 \u2264 \u03bb1.\nProof.\nVara\u223cD(x >aa>x) = Ea\u223cD(x>aa>x)2 \u2212 (Ea\u223cDx>aa>x)2\n\u2264 Ea\u223cD \u2016a\u201622 x >aa>x\u2212 (x>\u03a3x)2 \u2264 \u2225\u2225\u2225Ea\u223cD \u2016a\u201622 aa>\u2225\u2225\u2225\n2 = v(D)\u03bb21\nConsequently, Var(Ri,j) \u2264 v(D)\u03bb21, and since each of the a (j) i were drawn independently this implies that we have that Var(Rj) \u2264 v(D)\u03bb21/k. Therefore, by Chebyshev\u2019s inequality\nP [ |Rj \u2212 E[Rj ]| \u2265 2 \u221a v(D)\u03bb21 k ] \u2264 1 4 .\nSince E[Rj ] = x>\u03a3x and since we defined k appropriately this implies that\nP [\u2223\u2223\u2223Rj \u2212 x>\u03a3x\u2223\u2223\u2223 \u2265 \u03bb1] \u2264 1\n4 . (12)\nThe median z satisfies |z \u2212 x>\u03a3x| \u2264 as more than half of the Rj satisfy |Rj \u2212 x>\u03a3x| \u2264 . This happens with probability 1\u2212 p by Chernoff bound, our choice of m and (12)."}, {"heading": "5.2 Solving the Linear system", "text": "Here we show how to solve linear systems in B = \u03bbI \u2212\u03a3 in the streaming setting. We follow the general strategy of the offline algorithms in Section 4, replacing traditional SVRG with the streaming SVRG algorithm of [FGKS15a]. Similarly to the offline case we minimize f(x) = 12x\n>Bx\u2212 b>x and define for all a \u2208 supp(D),\n\u03c8a(x) def =\n1 2 x>(\u03bbI\u2212 aa>)x\u2212 b>x. (13)\ninsuring that f(x) = Ea\u223cD\u03c8a(x).. The performance of streaming SVRG [FGKS15a] is governed by three regularity parameters. As in the offline case, we use the fact that f(\u00b7) is \u00b5-strongly convexity for \u00b5 = \u03bb\u2212\u03bb1 and we require a smoothness parameter, denoted S, that satisfies:\n\u2200x \u2208 Rd : Ea\u223cD \u2225\u22255\u03c8a(x)\u22125\u03c8a(xopt)\u2225\u222522 \u2264 2S [f(x)\u2212 f(xopt)] . (14)\nFurthermore, we require an upper bound the variance, denoted \u03c32, that satisfies:\nEa\u223cD 1\n2 \u2225\u22255\u03c8a(xopt)\u2225\u22252(52f(xopt))\u22121 \u2264 \u03c32 . (15) With the following two lemmas we bound these parameters.\nLemma 19 (Streaming Smoothness). The smoothness parameter S def = \u03bb+ v(D)\u03bb21 \u03bb\u2212\u03bb1 satisfies (14).\nProof. Our proof is similar to the one for Lemma 10.\nEa\u223cD \u2225\u22255\u03c8a(x)\u22125\u03c8a(xopt)\u2225\u222522 = Ea\u223cD \u2225\u2225\u2225(\u03bbI\u2212 aa>)(x\u2212 xopt)\u2225\u2225\u222522\n= \u03bb2 \u2225\u2225x\u2212 xopt\u2225\u22252\n2 \u2212 2\u03bbEa\u223cD \u2225\u2225x\u2212 xopt\u2225\u22252 aa> + Ea\u223cD \u2225\u2225\u2225aa>(x\u2212 xopt)\u2225\u2225\u22252\n2 \u2264 \u03bb2 \u2225\u2225x\u2212 xopt\u2225\u22252\n2 \u2212 2\u03bb \u2225\u2225x\u2212 xopt\u2225\u22252 \u03a3 + \u2225\u2225\u2225Ea\u223cD \u2016a\u201622 aa>\u2225\u2225\u2225 2 \u00b7 \u2225\u2225x\u2212 xopt\u2225\u22252 2\n\u2264 \u03bb \u2225\u2225x\u2212 xopt\u2225\u22252\nB + v(D)\u03bb21 \u2225\u2225x\u2212 xopt\u2225\u22252 2 .\nSince f is \u03bb \u2212 \u03bb1-strongly convex, \u2225\u2225x\u2212 xopt\u2225\u22252\n2 \u2264 2\u03bb\u2212\u03bb1 [f(x) \u2212 f(x opt)]. Furthermore, since direct calculation reveals, 2[f(x)\u2212 f(xopt)] = \u2225\u2225x\u2212 xopt\u2225\u22252\nB , the result follows.\nLemma 20 (Streaming Variance). The variance parameter \u03c32 def = v(D)\u03bb21 \u03bb\u2212\u03bb1 \u2225\u2225xopt\u2225\u22252 2 satisfies (15).\nProof. We have\nEa\u223cD 1\n2 \u2225\u22255\u03c8a(xopt)\u2225\u22252(52f(xopt))\u22121 = Ea\u223cD 12 \u2225\u2225\u2225(\u03bbI\u2212 aa>)xopt \u2212 b\u2225\u2225\u22252B\u22121 = Ea\u223cD 1\n2 \u2225\u2225\u2225(\u03bbI\u2212 aa>)xopt \u2212Bxopt\u2225\u2225\u22252 B\u22121\n= Ea\u223cD 1\n2 \u2225\u2225\u2225(\u03a3\u2212 aa>)xopt\u2225\u2225\u22252 B\u22121 .\nApplying E \u2016a\u2212 Ea\u201622 = E \u2016a\u2016 2 2 \u2212 \u2016Ea\u2016 2 2 gives:\nEa\u223cD \u2225\u2225\u2225(\u03a3\u2212 aa>)xopt\u2225\u2225\u22252\nB\u22121 = Ea\u223cD\n\u2225\u2225xopt\u2225\u22252 aa>B\u22121aa> \u2212 \u2225\u2225xopt\u2225\u22252 \u03a3B\u22121\u03a3 \u2264 Ea\u223cD \u2225\u2225xopt\u2225\u22252 aa>B\u22121aa> .\nFurthermore, since B\u22121 1\u03bb\u2212\u03bb1 I we have\nEa\u223cDaa>B\u22121aa> 1 \u03bb\u2212 \u03bb1 Ea\u223cD(aa>)2\n(\u2225\u2225Ea\u223cD(aa>)2\u2225\u22252 \u03bb\u2212 \u03bb1 ) I = ( v(D)\u03bb21 \u03bb\u2212 \u03bb1 ) I .\nCombining these three equations yields the result.\nWith the regularity parameters bounded we can apply the streaming SVRG algorithm of [FGKS15a] to solve systems in B. We encapsulate the core iterative step of Algorithm 1 of [FGKS15a] as follows:\nDefinition 21 (Streaming SVRG Step). Given x0 \u2208 Rd and \u03b7, k,m > 0 we define a streaming SVRG step, x = ssvrg iter(x0, \u03b7, k,m) as follows. First we take k samples a1, ..., ak from D and set g = 1k \u2211 i\u2208[k] \u03c8ai where \u03c8ai is as defined in (13). Then for m\u0303 chosen uniformly at random from {1, ...,m} we draw m\u0303 additional samples a\u03031, ..., a\u0303m\u0303 from D. For t = 0, ..., m\u0303\u2212 1 we let\nxt+1 := xt \u2212 \u03b7\nL (5\u03c8a\u0303t(xt)\u22125\u03c8a\u0303t(x0) +5g(x0))\nand return xm\u0303 as the output.\nThe accuracy of the above iterative step is proven in Theorem 4.1 of [FGKS15a], which we include, using our notation below:\nTheorem 22 (Theorem 4.1 of [FGKS15a] 1). Let f(x) = Ea\u223cD\u03c8a(x) and let \u00b5, S, \u03c32 be the strong convexity, smoothness, and variance bounds for f(x). Then for any distribution over x0 we have that x := ssvrg iter(x0, \u03b7, k,m) has E[f(x)\u2212 f(xopt)] upper bounded by\n1\n1\u2212 4\u03b7 ( S \u00b5m\u03b7 + 4\u03b7 )[ Ef(x0)\u2212 f(xopt) ] + 1 + 2\u03b7 k \u221aS \u00b5 \u00b7 [Ef(x0)\u2212 f(xopt)] + \u03c3 2 . Using Theorem 22 we can immediately obtain the following guarantee for solve system in B:\nCorollary 23 (Streaming SVRG Solver - With Initial Point). Let \u00b5 = \u03bb \u2212 \u03bb1, S = \u03bb + v(D)\u03bb21 \u03bb\u2212\u03bb1 , and \u03c32 = v(D)\u03bb21 \u03bb\u2212\u03bb1 \u2225\u2225xopt\u2225\u22252 2 . Let c2, c3 \u2208 (0, 1) be any constants and set \u03b7 = c28 , m = [ S \u00b5c22 ] , and\nk = max {[\nS \u00b5c2\n] , [\nv(D)\u03bb21 (\u03bb\u2212\u03bb1)2c3\n]} . If to solve Bx = b for unit vector b with initial point x0, we use the\niterative procedure described in Definition 21 to compute x = ssvrg iter(x0, \u03b7, k,m) then: E \u2225\u2225x\u2212 xopt\u2225\u22252\nB \u2264 22c2 \u00b7 \u2225\u2225x0 \u2212 xopt\u2225\u22252B + 10c3\u03bb1(B\u22121). Further, the procedure requires O ( v(D) gap2 [ 1 c22 + 1c3 ]) samples from D.\n1Note that Theorem 4.1 in [FGKS15a] has an additional parameter of \u03b1, which bounds the Hessian of f(xopt) in comparison to the Hessian everywhere else. In our setting this parameter is 1 as 52f(y) = 52f(z) for all y and z.\nProof. Using the inequality (x+ y)2 \u2264 2x2 + 2y2 we have that\u221aS \u00b5 \u00b7 E[f(x0)\u2212 f(xopt)] + \u03c3 2 \u2264 2S \u00b5 \u00b7 E[f(x0)\u2212 f(xopt)] + 2\u03c32 Additionally, since b is a unit vector, we know that \u2225\u2225xopt\u2225\u22252 2 = \u2225\u2225B\u22121b\u2225\u22252 2 \u2264 1 (\u03bb\u2212\u03bb1)2 . Using equation\n(10), i.e. that \u2225\u2225x\u2212 xopt\u2225\u22252\nB = 2[f(x)\u2212 f(xopt)] for all x, we have by Theorem 22:\nE \u2225\u2225x\u2212 xopt\u2225\u22252\nB \u2264 1\n1\u2212 c2/2\n[( 8c2 +\nc2 2 + 4 + c2 2 \u00b7 c2 ) \u00b7 \u2225\u2225x0 \u2212 xopt\u2225\u22252B + 4 + c24k \u00b7 v(D)\u03bb21(\u03bb\u2212 \u03bb1)3 ] \u2264 22c2 \u00b7\n\u2225\u2225x0 \u2212 xopt\u2225\u22252B + 10c3\u03bb\u2212 \u03bb1 = 22c2 \u00b7 \u2225\u2225x0 \u2212 xopt\u2225\u22252B + 10c3\u03bb1(B\u22121). Since 1/(\u03bb\u2212 \u03bb1) = \u03bb1(B\u22121) we see that E \u2225\u2225x\u2212 xopt\u2225\u22252 B\nis as desired. All that remains is to bound the number of samples we used.\nNow the number of samples used to compute x is clearly at most m+ k Now\nm = S\n\u00b5c22 = O\n( \u03bb\nc22(\u03bb\u2212 \u03bb1) + v(D)\u03bb21 c22(\u03bb\u2212 \u03bb1)2\n) = O ( 1\nc22gap + v(D) c22gap 2\n) .\n. However since gap < 1 and v(D) \u2265 1 this simplifies to m = O (\nv c22gap 2\n) . Next to bound k\nwe can ignore the [ S \u00b5c2 ] term since this was already included in our bound of m and just bound\nv(D)\u03bb21 c3(\u03bb\u2212\u03bb1)2 = O\n( v(D)\ngap2c3\n) yielding our desired sample complexity.\nWhereas in the offline case, we could ensure that our initial error \u2225\u2225x0 \u2212 xopt\u2225\u22252B is small by simply scaling by the Rayleigh quotient (Corollary 6) in the online case estimating the Rayleigh quotient to sufficient accuracy would require too many samples. Instead, here simply show how to simply apply Corollary 23 iteratively to solve the desired linear systems to absolute accuracy without an initial point. Ultimately, due to the different error dependences in the online case this guarantee suffices and the lack of an initial point is not a bottleneck.\nCorollary 24 (Streaming SVRG Solver). There is a streaming algorithm that iteratively applies the solver of Corollary 23 to solve Bx = b for unit vector b and returns a vector x that satisfies\nE \u2225\u2225x\u2212 xopt\u2225\u22252\nB \u2264 10c\u03bb1(B\u22121) using O\n( v(D)\ngap2\u00b7c\n) samples from D.\nProof. Let x0 = 0. Then \u2225\u2225x0 \u2212 xopt\u2225\u22252B = \u2225\u2225B\u22121b\u2225\u22252B \u2264 \u03bb1(B\u22121) since b is a unit vector. If we apply Corollary 23 with c2 = 1 44 and c3 = 1 20 , then we will obtain x1 with E\n\u2225\u2225x1 \u2212 xopt\u2225\u22252B \u2264 12\u03bb1(B\u22121). If we then double c3 and apply the solver again we obtain x2 with E\n\u2225\u2225x1 \u2212 xopt\u2225\u22252B \u2264 14\u03bb1(B\u22121). Iterating in this way, after log(1/c) iterations we will have the desired guarantee: E \u2225\u2225x\u2212 xopt\u2225\u22252 B \u2264 10c\u03bb1(B \u22121). Our total sample cost in each iteration is, by Corollary 23, O ( v(D) gap2 [ 1 442 + 1c3 ]) . Since we double c3 each time, the cost corresponding to the 1 c3\nterms is dominated by the last iteration when we have c3 = O(c). So our overall sample cost is just:\nO ( v(D) gap2 [ 1 c + log(1/c) ]) = O ( v(D) gap2 \u00b7 c ) ."}, {"heading": "5.3 Online Shifted-and-Inverted Power Method", "text": "We now apply the results in Section 5.1 and Section 5.2 to the shifted-and-inverted power method framework of Section 3 to give our main result in the online setting, an algorithm that quickly refines a coarse approximation to v1 into a finer approximation.\nTheorem 25 (Online Shifted-and-Inverted Power Method \u2013 Warm Start). Let B = \u03bbI \u2212 A>A for ( 1 + gap150 ) \u03bb1 \u2264 \u03bb \u2264 ( 1 + gap100 ) \u03bb1 and let x0 be some vector with G(x0) \u2264 1\u221a10 . Running the shifted-and-inverted power method on B initialized with x0, using the streaming SVRG solver of Corollary 24 to approximately apply B\u22121 at each step, returns x such that x>\u03a3x \u2265 (1\u2212 )\u03bb1 with constant probability for any target < gap. The algorithm uses O( v(D)gap\u00b7 ) samples and amortized O(d) time per sample.\nWe note that by instantiating Theorem 25, with \u2032 = \u00b7 gap, and applying Lemma 1 we can find x such that |v>1 x| \u2265 1\u2212 with constant probability in time O\n( v(D)\ngap2\u00b7\n) .\nProof. By Lemma 3 it suffices to have G2(x) = O( gap) or equivalently G(x) = O( \u221a /gap). In\norder to succed with constant probability it suffices to have E [G(x)] = O( \u221a /gap) with constant probability. Since we start with G(x0) \u2264 1\u221a10 , we can achieve this using log(gap/ ) iterations of the approximate shifted-and-inverted power method of Theorem 5. In each iteration i we choose\nthe error parameter for Theorem 5 to be c1(i) = 1\u221a 10 \u00b7 ( 1 5 )i . Consequently,\nE [G(xi)] \u2264 3\n25 G(xi\u22121) +\n4\n1000 1\u221a 10 \u00b7 ( 1 5 )i and by induction E [G(xi)] \u2264 15i 1\u221a 10 . We halt when (15) i = O( \u221a /gap) and hence c1(i) =\nO( \u221a /gap).\nIn order to apply Theorem 5 we need a subroutine q\u0302uot (x) that lets us approximate quot(x) to within an additive error 130(\u03bb \u2212 \u03bb1) = O(gap \u00b7 \u03bb1). Theorem 18 gives us such a routine, requiring O (\nv(D) log log(gap/ ) gap2 ) = O( v(D)gap\u00b7 ) samples to succeed with probability 1 \u2212 O ( 1 log(gap/ ) ) (since <\ngap). Union bounding, the estimation succeeds in all rounds with constant probability.\nBy Corollary 24 with c = \u0398(c1(i) 2) the cost for solving each linear system solve is O\n( v(D)\ngap2c1(i)2\n) .\nSince c1(i) multiplies by a constant factor with each iteration the cost over all O(log(gap/d ) iterations is just a truncated geometric series and is proportional to cost in the last iteration, when c = \u0398 (\ngap\n) . So the total cost for solving the linear systems is O ( v(D) gap\u00b7 ) . Adding this to the\nnumber of samples for the Rayleigh quotient estimation yields the result."}, {"heading": "6 Parameter Estimation for Offline Eigenvector Computation", "text": "In Section 4, in order to invoke Theorems 5 and 8 we assumed knowledge of some \u03bb with (1 + c1 \u00b7 gap)\u03bb1 \u2264 \u03bb \u2264 (1 + c2 \u00b7 gap)\u03bb1 for some small constant c1 and c2. Here we show how to estimate this parameter using Algorithm 1, incurring a modest additional runtime cost.\nIn this section, for simplicity we initially assume that we have oracle access to compute B\u22121\u03bb x for any given x, and any \u03bb > \u03bb1. We will then show how to achieve the same results when we\nAlgorithm 1 Estimating the eigenvalue and the eigengap Input: A \u2208 Rn\u00d7d, \u03b1 1: w = [w1, w2]\u2190 N (0, 1)d\u00d72 2: t\u2190 O (\u03b1 log d) 3: [ \u03bb\u0303\n(0) 1 , \u03bb\u0303 (0) 2\n] \u2190 eigEstimate (( ATA )t w )\n4: \u03bb (0) \u2190 (1 + 12)\u03bb\u0303 (0) 1 5: i\u2190 0 6: while \u03bb (i) \u2212 \u03bb\u0303(i)1 < 110 ( \u03bb (i) \u2212 \u03bb\u0303(i)2 ) do 7: i\u2190 i+ 1 8: w = [w1, w2]\u2190 N (0, 1)d\u00d72\n9:\n[ \u03bb\u0302\n(i) 1 , \u03bb\u0302 (i) 2\n] \u2190 eigEstimate (( \u03bb (i\u22121) I\u2212ATA )\u2212t w ) 10: [ \u03bb\u0303\n(i) 1 , \u03bb\u0303 (i) 2\n] \u2190 [ \u03bb\n(i\u22121) \u2212 1 \u03bb\u0302 (i) 1 , \u03bb (i\u22121) \u2212 1 \u03bb\u0302 (i) 2 ] 11: \u03bb (i) \u2190 12 ( \u03bb\u0303 (i) 1 + \u03bb (i\u22121) ) 12: end while Output: \u03bb\ncan only compute B\u22121\u03bb x approximately. We use a result of [MM15] that gives gap free bounds for computing eigenvalues using the power method. The following is a specialization of Theorem 1 from [MM15]:\nTheorem 26. For any > 0, any matrix M \u2208 Rd\u00d7d with eigenvalues \u03bb1, ..., \u03bbd, and k \u2264 d, let W \u2208 Rd\u00d7k be a matrix with entries drawn independently from N (0, 1). Let eigEstimate(Y) be a function returning for each i, \u03bb\u0303i = v\u0303 > i Mv\u0303i where v\u0303i is the i\nth largest left singular vector of Y. Then setting [\u03bb\u03031, ..., \u03bb\u0303k] = eigEstimate ( MtW ) , for some fixed constant c and t = c\u03b1 log d for any \u03b1 > 1, with probability 1\u2212 1 d10 , we have for all i:\n|\u03bb\u0303i \u2212 \u03bbi| \u2264 1\n\u03b1 \u03bbk+1\nThroughout the proof, we assume \u03b1 is picked to be some large constant - e.g. \u03b1 > 100. Theorem 26 implies:\nLemma 27. Conditioning on the event that Theorem 26 holds for all iterates i, then the iterates of Algorithm 1 satisfy:\n0 \u2264 \u03bb1 \u2212 \u03bb\u0303(0)1 \u2264 1\n\u03b1 \u03bb1 and\n1\n2\n( 1\u2212 3\n\u03b1\n) \u03bb1 \u2264 \u03bb (0) \u2212 \u03bb1 \u2264 1\n2 \u03bb1, and,\n0 \u2264 \u03bb1 \u2212 \u03bb\u0303(i)1 \u2264 1\n\u03b1\u2212 1\n( \u03bb (i\u22121) \u2212 \u03bb1 ) and 1\n2\n( 1\u2212 1\n\u03b1\u2212 1\n)( \u03bb (i\u22121) \u2212 \u03bb1 ) \u2264 \u03bb(i) \u2212 \u03bb1 \u2264 1\n2\n( \u03bb (i\u22121) \u2212 \u03bb1 ) .\nProof. The proof can be decomposed into two parts:\nPart I (Lines 3-4): Theorem 26 tells us that \u03bb\u0303 (0) 1 \u2265 ( 1\u2212 1\u03b1 ) \u03bb1. This means that we have\n0 \u2264 \u03bb1 \u2212 \u03bb\u0303(0)1 \u2264 1\n\u03b1 \u03bb1 and\n1\n2\n( 1\u2212 3\n\u03b1\n) \u03bb1 \u2264 \u03bb (0) \u2212 \u03bb1 \u2264 1\n2 \u03bb1.\nPart II (Lines 5-6): Consider now iteration i. We now apply Theorem 26 to the matrix( \u03bb (i\u22121) I\u2212ATA )\u22121 . The top eigenvalue of this matrix is ( \u03bb (i\u22121) \u2212 \u03bb1 )\u22121 . This means that we\nhave ( 1\u2212 1\u03b1 ) ( \u03bb (i\u22121) \u2212 \u03bb1 )\u22121 \u2264 \u03bb\u0302(i)1 \u2264 ( \u03bb (i\u22121) \u2212 \u03bb1 )\u22121 , and hence we have,\n0 \u2264 \u03bb1 \u2212 \u03bb\u0303(i)1 \u2264 1\n\u03b1\u2212 1\n( \u03bb (i\u22121) \u2212 \u03bb1 ) and 1\n2\n( 1\u2212 1\n\u03b1\u2212 1\n)( \u03bb (i\u22121) \u2212 \u03bb1 ) \u2264 \u03bb(i) \u2212 \u03bb1 \u2264 1\n2\n( \u03bb (i\u22121) \u2212 \u03bb1 ) .\nThis proves the lemma.\nLemma 28. Recall we denote \u03bb2 def = \u03bb2 ( ATA ) and gap\ndef = \u03bb1\u2212\u03bb2\u03bb1 . Then conditioning on the event that Theorem 26 holds for all iterates i, the iterates of Algorithm 1 satisfy \u2223\u2223\u2223\u03bb2 \u2212 \u03bb\u0303(i)2 \u2223\u2223\u2223 \u2264\n1 \u03b1\u22121\n( \u03bb (i\u22121) \u2212 \u03bb2 ) , and \u03bb (i) \u2212 \u03bb\u0303(i)2 \u2265 gap\u03bb1 4 .\nProof. Since ( \u03bb (i\u22121) \u2212 \u03bb2 )\u22121 is the second eigenvalue of the matrix ( \u03bb (i\u22121) I\u2212ATA )\u22121 , Theorem 26 tells us that ( 1\u2212 1\n\u03b1\n)( \u03bb (i\u22121) \u2212 \u03bb2 )\u22121 \u2264 \u03bb\u0302(i)2 \u2264 ( 1 + 1\n\u03b1\n)( \u03bb (i\u22121) \u2212 \u03bb2 )\u22121 .\nThis immediately yields the first claim. For the second claim, we notice that\n\u03bb (i) \u2212 \u03bb\u0303(i)2 = \u03bb (i) \u2212 \u03bb2 + \u03bb2 \u2212 \u03bb\u0303(i)2 (\u03b61)\n\u2265 \u03bb(i) \u2212 \u03bb2 \u2212 1\n\u03b1\u2212 1\n( \u03bb (i\u22121) \u2212 \u03bb2 )\n= \u03bb (i) \u2212 \u03bb1 \u2212\n1\n\u03b1\u2212 1\n( \u03bb (i\u22121) \u2212 \u03bb1 ) + ( 1\u2212 1\n\u03b1\u2212 1\n) (\u03bb1 \u2212 \u03bb2)\n(\u03b62)\n\u2265 1 2\n( 1\u2212 3\n\u03b1\u2212 1\n)( \u03bb (i\u22121) \u2212 \u03bb1 ) + ( 1\u2212 1\n\u03b1\u2212 1\n) (\u03bb1 \u2212 \u03bb2) \u2265\ngap\u03bb1 4 ,\nwhere (\u03b61) follows from the first claim of this lemma, and (\u03b62) follows from Lemma 27.\nWe now state and prove the main result in this section:\nTheorem 29. Suppose \u03b1 > 100, and after T iterations, Algorithm 1 exits. Then with probability 1\u2212 \u2308 log 10 gap \u2309 +1\nd10 , we have T \u2264 \u2308 log 10gap \u2309 + 1, and:(\n1 + gap\n120\n) \u03bb1 \u2264 \u03bb (T ) \u2264 ( 1 + gap\n8\n) \u03bb1\nProof. By union bound, we know with probability 1 \u2212 \u2308 log 10 gap \u2309 +1\nd10 , Theorem 26 will hold for all iterates where i \u2264 \u2308 log 10gap \u2309 + 1.\nLet i = \u2308 log 10gap \u2309 , suppose the algorithm has not exited yet after i iterations, then since \u03bb (i)\u2212\u03bb1\ndecays geometrically, we have \u03bb (i)\u2212\u03bb1 \u2264 gap\u03bb110 . Therefore, Lemmas 27 and 28 imply that \u03bb (i+1)\u2212 \u03bb\u0303 (i+1) 1 \u2264 ( 1 2 + 1 \u03b1\u22121 )( \u03bb (i) \u2212 \u03bb1 ) \u2264 gap\u03bb115 , and\n\u03bb (i+1) \u2212 \u03bb\u0303(i+1)2 \u2265 \u03bb (i+1) \u2212 \u03bb2 \u2212 \u2223\u2223\u2223\u2223\u03bb2 \u2212 \u03bb\u0303(i+1)2 \u2223\u2223\u2223\u2223 \u2265 \u03bb1 \u2212 \u03bb2 \u2212 1\u03b1\u2212 1 ( \u03bb (i) \u2212 \u03bb2 )\n= gap\u03bb1 \u2212 1\n\u03b1\u2212 1\n( \u03bb (i) \u2212 \u03bb1 + \u03bb1 \u2212 \u03bb2 ) \u2265 3\n4 gap\u03bb1\nThis means that the exit condition on Line 6 must be triggered in i+ 1 iteration, proving the first part of the lemma.\nFor upper bound, by Lemmas 27, 28 and exit condition we know:\n\u03bb (T ) \u2212 \u03bb1 \u2264 \u03bb (T ) \u2212 \u03bb\u0303(T )1 \u2264 1\n10 (\u03bb\n(T ) \u2212 \u03bb\u0303(T )2 ) \u2264 1\n10\n( \u03bb (T ) \u2212 \u03bb2 + \u2223\u2223\u2223\u03bb2 \u2212 \u03bb\u0303(T )2 \u2223\u2223\u2223)\n\u2264 1 10\n( \u03bb (T ) \u2212 \u03bb2 + 1\n\u03b1\u2212 1 (\u03bb\n(T\u22121) \u2212 \u03bb2) )\n= 1\n10\n( \u03b1\n\u03b1\u2212 1 gap\u03bb1 + (\u03bb\n(T ) \u2212 \u03bb1) + 1\n\u03b1\u2212 1\n( \u03bb (T\u22121) \u2212 \u03bb1 ))\n\u2264 1 10\n( \u03b1\n\u03b1\u2212 1 gap\u03bb1 +\n\u03b1\n\u03b1\u2212 2\n( \u03bb (T ) \u2212 \u03bb1 ))\nSince \u03b1 > 100, this directly implies \u03bb (T ) \u2212 \u03bb1 \u2264 gap8 \u03bb1.\nFor lower bound, since as long as the Algorithm 1 does not exists, by Lemmas 28, we have\n\u03bb (T\u22121) \u2212 \u03bb\u0303(T\u22121)1 \u2265 110\n( \u03bb (T\u22121) \u2212 \u03bb\u0303(T\u22121)2 ) \u2265 gap\u03bb140 , and thus:\n\u03bb (T\u22121) \u2212 \u03bb1 = \u03bb (T\u22121) \u2212 \u03bb\u0303(T\u22121)1 \u2212 (\u03bb1 \u2212 \u03bb\u0303 (T\u22121) 1 ) \u2265 gap\u03bb1 40 \u2212 1 \u03b1\u2212 1\n( \u03bb (T\u22121) \u2212 \u03bb1 )\n\u2265 gap\u03bb1 40 \u2212 2 \u03b1\u2212 2\n( \u03bb (T ) \u2212 \u03bb1 ) \u2265 gap\u03bb1\n50\nBy Lemma 27, we know \u03bb (T ) \u2212 \u03bb1 \u2265 12(1\u2212 1 \u03b1\u22121(\u03bb (T\u22121) \u2212 \u03bb1)) > gap120\u03bb1\nNote that, although we proved the upper bound and lower bound in Theorem 29 with specific constants coefficient 18 and 1 120 , this analysis can easily be extended to any smaller constants by modifying the constant in the exit condition, and choosing \u03b1 larger. Also in the failure probability\n1\u2212\n\u2308 log 10gap \u2309 + 1\nd10 ,\nthe term d10 can be replaced by any poly(d) by adjusting the constant in setting t \u2190 O(\u03b1 log d) in Algorithm 1. Assuming log 1gap < poly(d), thus gives that Theorem 29 returns a correct result with high probability.\nFinally, we can also bound the runtime of algorithm 1, when we use SVRG based approximate linear system solvers for B\u03bb.\nTheorem 30. With probability 1\u2212O( 1 d10 log 1gap), Algorithm 1 runs in time\nO ([ nnz(A) + d sr(A)\ngap2\n] \u00b7 log3 ( d\ngap )) .\nProof. By Theorem 29, we know only O(log 1/gap) iterations of the algorithm are needed. In\neach iteration, the runtime is dominated by running eigEstimate (( \u03bb (i\u22121) I\u2212ATA )\u2212t w ) , which\nis dominated by computing ( \u03bb (i\u22121) I\u2212ATA )\u2212t w. Since t = O(log d), it\u2019s easy to verify that:\nto make Theorem 26 hold, we only need to approximate ( \u03bb (i\u22121) I\u2212ATA )\u22121 w up to accuracy\npoly(gap/d). By Theorem 12, we know this approximation can be calculated in time\nO ([ nnz(A) + d sr(A)\u03bb21\n(\u03bb (i\u22121) \u2212 \u03bb1)2\n] \u00b7 log ( d\ngap\n))\n. Combining Theorem 29 with Lemma 27, we know \u03bb (i\u22121)\u2212\u03bb1 \u2265 \u03bb (T )\u2212\u03bb1 \u2265 gap120 , thus approximately solving ( \u03bb (i\u22121) I\u2212ATA )\u22121 w can be done in time O\u0303 ( nnz(A) + d sr(A)\ngap2\n) . Finally, since the runtime\nof Algorithm 1 is dominated by repeating this subroutine t \u00d7 T = O(log d \u00b7 log(1/gap)) times, we finish the proof.\nNote that we can accelerate the runtime of Algorithm 1 to O\u0303 (\nnnz(A)3/4(d sr(A))1/4\u221a gap\n) , by simply\nreplacing the base solver for ( \u03bb (i\u22121) I\u2212ATA )\u22121 w with the accelerated solver in Theorem 15."}, {"heading": "7 Lower Bounds", "text": "Here we show that our online eigenvector estimation algorithm (Theorem 25) is asymptotically optimal - as sample size grows large it achieves optimal accuracy as a function of sample size. We rely on the following lower bound for eigenvector estimation in the Gaussian spike model:\nLemma 31 (Lower bound for Gaussian Spike Model [BJNP13]). Suppose data is generated as\nai = \u221a \u03bb\u03b9iv ? + Zi (16)\nwhere \u03b9i \u223c N (0, 1), and Zi \u223c N (0, Id). Let v\u0302 be some estimator of the top eigenvector v?. Then, there is some universal constant c0, so that for n sufficiently large, we have:\ninf v\u0302 max v?\u2208Sd\u22121\nE \u2016v\u0302 \u2212 v?\u20162 \u2265 c0 (1 + \u03bb)d\n\u03bb2n\nTheorem 32. Consider the problem of estimating the top eigenvector v1 of Ea\u223cDaa>, where we observe n i.i.d samples from unknown distribution D. If gap < 0.9, then there exists some universal constant c, such that for any estimator v\u0302 of top eigenvector, there always exists some hard distribution D so that for n sufficiently large:\nE \u2016v\u0302 \u2212 v1\u201622 \u2265 c v(D) gap2n\nProof. Suppose the claim of theorem is not true, then there exist some estimator v\u0302 so that\nE \u2016v\u0302 \u2212 v1\u201622 < c \u2032 v(D) gap2n\nholds for all distribution D, and for any fixed constant c\u2032 when n is sufficiently large. Let distribution D be the Gaussian Spike Model specified by Eq.(16), then by calculation, it\u2019s not hard to verify that:\nv(D) =\n\u2225\u2225\u2225Ea\u223cD [(aa>)2]\u2225\u2225\u2225 2\n\u2016Ea\u223cD(aa>)\u201622 = d+ 2 + 3\u03bb 1 + \u03bb\nSince we know gap = \u03bb1+\u03bb < 0.9, this implies \u03bb < 9, which gives v(D) < d+29 1+\u03bb < 30d 1+\u03bb . Therefore, we have that:\nE \u2016v\u0302 \u2212 v?\u201622 < c \u2032 v(D) gap2n < 30c\u2032 (1 + \u03bb)d \u03bb2n\nholds for all v? \u2208 Sd\u22121. Choose c\u2032 = c030 in Lemma 31 we have a contradiction.\n\u2016v\u0302 \u2212 v1\u201622 = 2\u22122v\u0302>v1, so this bound implies that- to obtain |v\u0302>v1| \u2265 1\u2212 , we need v(D) gap2n = O( ) so n = \u0398 (\nv(D) gap2\n) . This exactly matches the sample complexity given by Theorem 25."}, {"heading": "8 Gap-Free Bounds", "text": "In this section we demonstrate that our techniques can easily be extended to obtain gap-free runtime bounds, for the regime when \u2265 gap. In many ways these bounds are actually much easier to achieve than the gap dependent bounds since they require less careful error analysis.\nLet be our error parameter and m be the number of eigenvalues of \u03a3 that are \u2265 (1\u2212 /2)\u03bb1. Choose \u03bb = \u03bb1 + /100. We have \u03bb1(B\n\u22121) = 100 \u03bb1 . For i > m we have \u03bbi(B \u22121) < 2 \u03bb1 . \u03ba(B \u22121) \u2264 100 . Let Vb have columns equal to all bottom eigenvectors with eigenvalues \u03bbi < (1\u2212 /2)\u03bb1. Let Vt\nhave columns equal to the m remaining top eigenvectors. We define a simple modified potential:\nG\u0304(x) def = \u2016PVbx\u2016B \u2016Pv1x\u2016B =\n\u221a\u2211 i>m\n\u03b12i \u03bbi(B\u22121)\u221a \u03b121\n\u03bb1(B\u22121)\nWe have the following Lemma connecting this potential function to eigenvalue error:\nLemma 33. For unit x, if G\u0304(x) \u2264 c \u221a for sufficiently small constant c then \u03bb1 \u2212 x>\u03a3x \u2264 \u03bb1.\nProof.\nG\u0304(x) \u2265 \u2016PVbx\u20162 \u2016Pv1x\u20162 \u2265 \u2016PVbx\u20162 \u2016PVtx\u20162\nSo if G\u0304(x) \u2264 c \u221a then \u2016PVtx\u2016 2 2 c 2 \u2265 \u2016PVbx\u2016 2 2 and since \u2016PVtx\u2016 2 2 + \u2016PVbx\u2016 2 2 = 1, this gives \u2016PVtx\u2016 2 2 \u2265 1 1+c2 . So we have xT\u03a3x \u2265 PVtxT\u03a3xPVt \u2265 (1\u2212 /2)\u03bb1 1+c2 \u2265 1\u2212 for small enough c, giving the lemma.\nWe now follow the proof of Lemma 8, which is actually much simpler in the gap-free case.\nTheorem 34 (Approximate Shifted-and-Inverted Power Method \u2013 Gap-Free). Suppose we randomly initialize x0 as in Lemma 7 and suppose we have access to a subroutine solve (\u00b7) such that\nE [\u2225\u2225solve (x)\u2212B\u22121x\u2225\u2225\nB\n] \u2264\n3\n3000d21\n\u221a \u03bbd(B\u22121)\nThen the following procedure,\nxt = solve (xt\u22121) / \u2016solve (xt\u22121)\u2016\nafter T = O (log d/ ) iterations satisfies:\nG\u0304(xT ) \u2264 c \u221a ,\nwith probability greater than 1\u2212O( 1 d10 ). Proof. By Lemma 7, we know with at least probability 1 \u2212 O( 1 d10\n), we have G\u0304(x0) \u2264 G(x0) \u2264\u221a \u03ba(B\u22121)d10.5 = 100d 10.5\n. We want to show by induction that at iteration i we have G\u0304(xi) \u2264 1 2i \u00b7 100d10.5 , which will give us the lemma if we set T = log2\n( 100d10.5\nc 1.5\n) = O(log(d/ )).\nLet x\u0302 = solve (x) and \u03be = x\u0302\u2212B\u22121x. Following Lemma 8 we have: \u2016PVb (x\u0302)\u2016B \u2264 \u2225\u2225PVb (B\u22121x)\u2225\u2225B + \u2016PVb (\u03be)\u2016B \u2264 \u2225\u2225PVb (B\u22121x)\u2225\u2225B + \u2016\u03be\u2016B\n= \u221a\u2211 i>m \u03b12i\u03bbi(B \u22121) + \u2016\u03be\u2016B\n\u2264 \u03bbm+1(B\u22121) \u221a\u221a\u221a\u221a\u2211 i>m \u03b12i \u03bbi(B\u22121) + 3 3000d21 \u221a \u03bbm+1(B\u22121)  \u2264 2\u03bbm+1(B\u22121) max  \u221a\u221a\u221a\u221a\u2211\ni>m\n\u03b12i \u03bbi(B\u22121) , 3 3000d21 \u221a \u03bbm+1(B\u22121)  and\n\u2016Pv1 (x\u0302)\u2016B \u2265 \u2225\u2225Pv1 (B\u22121x)\u2225\u2225B \u2212 \u2016Pv1 (\u03be)\u2016B \u2265 \u2225\u2225Pv1 (B\u22121x)\u2225\u2225B \u2212 \u2016\u03be\u2016B\n= \u221a \u03b121\u03bb1(B \u22121)\u2212 \u2016\u03be\u2016B\n\u2265 \u03bb1(B\u22121) \u221a\u221a\u221a\u221a\u03b121 \u2212 6(3000d21)2 \u03bb1(B\u22121) .\nInitially, we have with high probability, by the argument in Lemma 7, \u03b11 \u2265 1d10 so we have\n\u2016Pv1 (x\u0302)\u2016B \u2265 \u03bb1(B\u22121) 2\n\u221a \u03b121\n\u03bb1(B\u22121) . This also holds by induction in each iteration.\nLet \u03b1\u03021 = |v>1 x\u0302|/ \u2016x\u0302\u20162. \u2016Pv1 (x\u0302)\u2016 2 B =\n\u03b1\u030221\u2016x\u0302\u2016 2 2\n\u03bb1(B\u22121) so we have\n\u03b1\u030221 \u2265 \u03bb1(B\n\u22121)2\n\u2016x\u0302\u201622\n( \u03b121 \u2212\n6\n(3000d21)2 ) and since \u2016x\u0302\u201622 \u2264 2 (\u2225\u2225B\u22121x\u2225\u22252 2 + 2 \u2016\u03be\u201622 ) \u2264 \u03bb1(B\u22121)2 + 2 6 (3000d21)2 \u2264 \u03bb1(B\u22121)2 ( 2 + 2\n6\n(3000d21)2\n) we\nhave:\n\u03b1\u030221 \u2265 1\n2.1\n( \u03b121 \u2212\n6\n(3000d21)2\n) \u2265 1\n3 \u03b121.\nSo over all log2\n( 100d10.5\nc 1.5 ) iterations, we always have \u03b1\u030221 \u2265 1d10 \u00b7 ( c 1.5 100d10.5 )log2 3 and so\n6\n(3000d21)2 <<\n1/2\u03b121. Combining the above bounds:\nG\u0304(x\u0302) \u2264 2\u03bbm+1\n( B\u22121 ) \u03bb1 (B\u22121) /2 \u00b7 max\n{\u221a\u2211 i>m \u03b12i \u03bbi(B\u22121) , 3 3000d21 \u221a \u03bbm+1(B\u22121) } \u221a\n\u03b121 \u03bb1(B\u22121)\n\u2264 4 50\nmax { G\u0304(x), O( \u221a ) } .\nThis is enough to give the Theorem.\nFinally, we combine Theorem 34 with the SVRG based solvers of Theorem 12 and 15 to obtain:\nTheorem 35 (Gap-Free Shifted-and-Inverted Power Method With SVRG). Let B = \u03bbI \u2212A>A for \u03bb = ( 1 + 100 ) and let x0 \u223c N (0, I) be a random initial vector. Running the inverted power method on B initialized with x0, using the SVRG solver from Theorem 12 to approximately apply B\u22121 at each step, returns x such that with probability 1\u2212O ( 1 d10 ) , x>\u03a3x \u2265 (1\u2212 )\u03bb1 in time\nO (( nnz(A) + d sr(A)\n2\n) \u00b7 log2 ( d )) .\nTheorem 36 (Accelerated Gap-Free Shifted-and-Inverted Power Method With SVRG). Let B = \u03bbI\u2212A>A for \u03bb = ( 1 + 100 ) and let x0 \u223c N (0, I) be a random initial vector. Running the inverted power method on B initialized with x0, using the SVRG solver from Theorem 15 to approximately apply B\u22121 at each step, returns x such that with probability 1\u2212O ( 1 d10 ) , x>\u03a3x \u2265 (1\u2212 )\u03bb1 in total time\nO ( nnz(A)3/4(d sr(A))1/4\u221a \u00b7 log3 ( d )) ."}, {"heading": "9 Acknowledgements", "text": "Sham Kakade acknowledges funding from the Washington Research Foundation for innovation in Data-intensive Discovery."}, {"heading": "A Appendix", "text": "Lemma 37 (Eigenvector Estimation via Spectral Norm Matrix Approximation). Let A>A have top eigenvector 1, top eigenvector v1 and eigenvalue gap gap. Let B\n>B be some matrix with\u2225\u2225A>A\u2212B>B\u2225\u2225 2 \u2264 O( \u221a \u00b7 gap). Let x be the top eigenvector of B>B. Then:\n|x>v1| \u2265 1\u2212 .\nProof. We can any unit vector y as y = c1v1 + c2v2 where v2 is the component of x orthogonal to v1 and c 2 1 + c 2 2 = 1. We know that\nv>1 B >Bv1 = v > 1 A >Av1 \u2212 vT1 (A>A\u2212B>B)v1 1\u2212 \u221a gap \u2264 v>1 B>Bv1 \u2264 1 + \u221a gap\nSimilarly we can compute:\nv>2 B >Bv2 = v > 2 A >Av2 \u2212 vT2 (A>A\u2212B>B)v2 1\u2212 gap\u2212 \u221a gap \u2264 v>2 B>Bv2 \u2264 1\u2212 gap + \u221a gap.\nand\n|v>1 B>Bv2| = |v>1 A>Av2 \u2212 vT1 (A>A\u2212B>B)v2| \u2264 \u221a gap.\nWe have x>BB>x = c21(v > 1 B >Bv1) + c 2 2(v > 2 B >Bv2) + 2c1c2 \u00b7 v>2 B>Bv1. We want to bound c1 \u2265 1\u2212 so c21 \u2265 1\u2212O( ). Since x is the top eigenvector of BB> we have:\nx>BB>x \u2265 v>1 BB>v1 c22(v > 2 B >Bv2) + 2c2v > 2 B >Bv1 \u2265 (1\u2212 c21)v>1 BB>v1\n2 \u221a 1\u2212 c21 \u221a gap \u2265 (1\u2212 c21) ( v>1 BB >v1 \u2212 v>2 BB>v2 )\n1\u221a 1\u2212 c21 \u2265 (1\u2212 2 \u221a )gap 2 \u221a gap\n1 1\u2212 c21 \u2265 1\u2212 5\n\u221a\n4\nThis means we need have 1\u2212 c21 \u2264 O( ) meaning c21 \u2265 1\u2212O( ) as desired.\nLemma 38 (Inverted Power Method progress in `2 and B norms). Let x be a unit vector with \u3008x, v1\u3009 6= 0 and let x\u0303 = B\u22121w, i.e. the power method update of B\u22121 on x. Then, we have both:\u2225\u2225\u2225Pv\u22a51 x\u0303\u2225\u2225\u2225B\n\u2016Pv1 x\u0303\u2016B \u2264 \u03bb2(B\n\u22121)\n\u03bb1(B\u22121) \u00b7 \u2225\u2225\u2225Pv\u22a51 x\u2225\u2225\u2225B \u2016Pv1x\u2016B\n(17)\nand \u2225\u2225\u2225Pv\u22a51 x\u0303\u2225\u2225\u22252 \u2016Pv1 x\u0303\u20162 \u2264 \u03bb2(B \u22121) \u03bb1(B\u22121) \u00b7 \u2225\u2225\u2225Pv\u22a51 x\u2225\u2225\u22252 \u2016Pv1x\u20162\n(18)\nProof. (17) was already shown in Lemma 4. We show (18) similarly. Writing x in the eigenbasis of B\u22121, we have x = \u2211 i \u03b1ivi and x\u0303 = \u2211 i \u03b1i\u03bbi ( B\u22121 ) vi. Since \u3008x, v1\u3009 6= 0, \u03b11 6= 0 and we have:\u2225\u2225\u2225Pv\u22a51 x\u0303\u2225\u2225\u22252 \u2016Pv1 x\u0303\u20162 = \u221a\u2211 i\u22652 \u03b1 2 i\u03bb 2 i (B \u22121)\u221a \u03b121\u03bb 2 1(B \u22121) \u2264 \u03bb2 ( B\u22121 ) \u03bb1 (B\u22121) \u00b7 \u221a\u2211 i\u22652 \u03b1 2 i\u221a \u03b121 = \u03bb2 ( B\u22121 ) \u03bb1 (B\u22121) \u00b7 \u2225\u2225\u2225Pv\u22a51 x\u2225\u2225\u22252 \u2016Pv1x\u20162 ."}], "references": [{"title": "The fast Johnson-Lindenstrauss transform and approximate nearest neighbors", "author": ["Nir Ailon", "Bernard Chazelle"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Ailon and Chazelle.,? \\Q2009\\E", "shortCiteRegEx": "Ailon and Chazelle.", "year": 2009}, {"title": "Minimax bounds for sparse PCA with noisy high-dimensional data", "author": ["Aharon Birnbaum", "Iain M Johnstone", "Boaz Nadler", "Debashis Paul"], "venue": "Annals of Statistics,", "citeRegEx": "Birnbaum et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Birnbaum et al\\.", "year": 2013}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L\u00e9on Bottou"], "venue": "In Proceedings of COMPSTAT,", "citeRegEx": "Bottou.,? \\Q2010\\E", "shortCiteRegEx": "Bottou.", "year": 2010}, {"title": "Uniform sampling for matrix approximation", "author": ["Michael B Cohen", "Yin Tat Lee", "Cameron Musco", "Christopher Musco", "Richard Peng", "Aaron Sidford"], "venue": "In Proceedings of the 6th Conference on Innovations in Theoretical Computer Science (ITCS),", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "Primal method for ERM with flexible minibatching schemes and non-convex losses", "author": ["Dominik Csiba", "Peter Richt\u00e1rik"], "venue": null, "citeRegEx": "Csiba and Richt\u00e1rik.,? \\Q2015\\E", "shortCiteRegEx": "Csiba and Richt\u00e1rik.", "year": 2015}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L Clarkson", "David P Woodruff"], "venue": "In Proceedings of the 45th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Clarkson and Woodruff.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson and Woodruff.", "year": 2013}, {"title": "Competing with the empirical risk minimizer in a single pass", "author": ["Roy Frostig", "Rong Ge", "Sham M Kakade", "Aaron Sidford"], "venue": "In Proceedings of the 28th Annual Conference on Computational Learning Theory (COLT),", "citeRegEx": "Frostig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Frostig et al\\.", "year": 2015}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["Roy Frostig", "Rong Ge", "Sham M Kakade", "Aaron Sidford"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Frostig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Frostig et al\\.", "year": 2015}, {"title": "Fast and simple PCA via convex optimization", "author": ["Dan Garber", "Elad Hazan"], "venue": null, "citeRegEx": "Garber and Hazan.,? \\Q2015\\E", "shortCiteRegEx": "Garber and Hazan.", "year": 2015}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "author": ["Chi Jin", "Sham M Kakade", "Cameron Musco", "Praneeth Netrapalli", "Aaron Sidford"], "venue": null, "citeRegEx": "Jin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2015}, {"title": "Principal component analysis", "author": ["Ian Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "Jolliffe.,? \\Q2002\\E", "shortCiteRegEx": "Jolliffe.", "year": 2002}, {"title": "On spectral graph drawing", "author": ["Yehuda Koren"], "venue": "In Computing and Combinatorics,", "citeRegEx": "Koren.,? \\Q2003\\E", "shortCiteRegEx": "Koren.", "year": 2003}, {"title": "Powers of tensors and fast matrix multiplication", "author": ["Fran\u00e7ois Le Gall"], "venue": "In Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation,", "citeRegEx": "Gall.,? \\Q2014\\E", "shortCiteRegEx": "Gall.", "year": 2014}, {"title": "A universal catalyst for first-order optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Randomized block krylov methods for stronger and faster approximate singular value decomposition", "author": ["Cameron Musco", "Christopher Musco"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Musco and Musco.,? \\Q2015\\E", "shortCiteRegEx": "Musco and Musco.", "year": 2015}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["Andrew Y Ng", "Michael I Jordan", "Yair Weiss"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2002}, {"title": "The PageRank citation ranking: bringing order to the Web", "author": ["Lawrence Page", "Sergey Brin", "Rajeev Motwani", "Terry Winograd"], "venue": null, "citeRegEx": "Page et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Page et al\\.", "year": 1999}, {"title": "Numerical methods for large eigenvalue problems", "author": ["Yousef Saad"], "venue": null, "citeRegEx": "Saad.,? \\Q1992\\E", "shortCiteRegEx": "Saad.", "year": 1992}, {"title": "Convergence of stochastic gradient descent for PCA", "author": ["Ohad Shamir"], "venue": null, "citeRegEx": "Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Shamir.", "year": 2015}, {"title": "Fast stochastic algorithms for SVD and PCA: Convergence properties and convexity", "author": ["Ohad Shamir"], "venue": null, "citeRegEx": "Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Shamir.", "year": 2015}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["Ohad Shamir"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Shamir.", "year": 2015}, {"title": "Spectral graph theory and its applications", "author": ["Daniel A Spielman"], "venue": "In null,", "citeRegEx": "Spielman.,? \\Q2007\\E", "shortCiteRegEx": "Spielman.", "year": 2007}, {"title": "Global convergence of stochastic gradient descent for some non-convex matrix problems", "author": ["Christopher D Sa", "Christopher Re", "Kunle Olukotun"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Sa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sa et al\\.", "year": 2015}, {"title": "An introduction to matrix concentration inequalities", "author": ["Joel A Tropp"], "venue": null, "citeRegEx": "Tropp.,? \\Q2015\\E", "shortCiteRegEx": "Tropp.", "year": 2015}, {"title": "A spectral algorithm for learning mixture models", "author": ["Santosh Vempala", "Grant Wang"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Vempala and Wang.,? \\Q2004\\E", "shortCiteRegEx": "Vempala and Wang.", "year": 2004}, {"title": "Multiplying matrices faster than CoppersmithWinograd", "author": ["Virginia Vassilevska Williams"], "venue": "In Proceedings of the 44th Annual ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "Williams.,? \\Q2012\\E", "shortCiteRegEx": "Williams.", "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "We give faster algorithms and improved sample complexities for estimating the top eigenvector of a matrix \u03a3 \u2013 i.e. computing a unit vector x such that x>\u03a3x \u2265 (1\u2212 )\u03bb1(\u03a3): \u2022 Offline Eigenvector Estimation: Given an explicit A \u2208 Rn\u00d7d with \u03a3 = A>A, we show how to compute an approximate top eigenvector in time \u00d5 ([ nnz(A) + d sr(A) gap2 ] \u00b7 log 1/ ) and \u00d5 ([ nnz(A)(d sr(A)) \u221a gap ] \u00b7 log 1/ ) . Here nnz(A) is the number of nonzeros in A, sr(A) def = \u2016A\u20162F \u2016A\u201622 is the stable rank, gap is the relative eigengap, and \u00d5(\u00b7) hides log factors in d and gap. By separating the gap dependence from the nnz(A) term, our first runtime improves upon the classical power and Lanczos methods. It also improves prior work using fast subspace embeddings [AC09, CW13] and stochastic optimization [Sha15c], giving significantly better dependencies on sr(A) and . Our second running time improves these further when nnz(A) \u2264 d sr(A) gap2 . \u2022 Online Eigenvector Estimation: Given a distribution D with covariance matrix \u03a3 and a vector x0 which is an O(gap) approximate top eigenvector for \u03a3, we show how to refine to an approximation using O ( v(D) gap\u00b7 ) samples from D. Here v(D) is a natural notion of variance. Combining our algorithm with previous work to initialize x0, we obtain improved sample complexity and runtime results under a variety of assumptions on D. We achieve our results using a general framework that we believe is of independent interest. We give a robust analysis of the classic method of shift-and-invert preconditioning to reduce eigenvector computation to approximately solving a sequence of linear systems. We then apply fast stochastic variance reduced gradient (SVRG) based system solvers to achieve our claims. We believe our results suggest the general effectiveness of shift-and-invert based approaches and imply that further computational gains may be reaped in practice. \u2217This paper combines work first appearing in [GH15] and [JKM15] ar X iv :1 60 5. 08 75 4v 1 [ cs .D S] 2 6 M ay 2 01 6", "creator": "LaTeX with hyperref package"}}}