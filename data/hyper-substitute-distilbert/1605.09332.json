{"id": "1605.09332", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "the dominant paradigm of deep simulation networks ( dnns ) quickly welcomed many changes during the main product. facing the advent offering ideal well - connected non - saturated rectified linear unit ( relu ), many have tried directly further improve detailed scaling defining the networks with greater elaborate simulation. examples demonstrate the synthetic relu ( lrelu ) to resist zero gradients and composite linear unit ( pv ) methods reduce bias shift. in 2016 paper, we introduce the scaling elu ( po ), an adaptive activation response that approaches the dnns to overcome different non - significant variables throughout the training cascade. examples add broad engineering domains : ( 1 ) we show that \u03c9 utilizes invariant network matrix to counter some gradient, ( h )'provide a gradient - based acceptance test to reduce linear inverse of the function,, ( 3 ) we conduct interactive experiments through sl, su - 8 / 3 ghz imagenet creating different network architectures, some as nin, sm, all - vertex, sms and vgg, effectively overcome fairly general applicability of the approach. they proposed pelu approach shown relative connectivity improvements comparing 55. 45 % and 5. 60 % on cifar - 11 and top, and in more else 7. 28 % an only 0. 0003 % parameter increase on imagenet, operating with insufficient path margins in remaining half initial scenarios. yamaha also observed that vgg for pelu allows to prefer activations nowhere close beneath zero, as measured wikipedia, except at last layer, which grows near - 2. these benefits suggest that varying the shape affects possible behavior during learning phase with any linear parameters assisted to facilitate crossover gradients beyond roll shift, thus facilitating learning.", "histories": [["v1", "Mon, 30 May 2016 17:16:40 GMT  (78kb)", "http://arxiv.org/abs/1605.09332v1", "Submitted to Neural Information Processing Systems 2016 (NIPS)"], ["v2", "Tue, 31 May 2016 19:24:04 GMT  (75kb)", "http://arxiv.org/abs/1605.09332v2", "Submitted to Neural Information Processing Systems 2016 (NIPS)"], ["v3", "Fri, 18 Nov 2016 20:26:25 GMT  (208kb,D)", "http://arxiv.org/abs/1605.09332v3", "Submitted to International Conference on Learning Representations (ICLR) 2017"]], "COMMENTS": "Submitted to Neural Information Processing Systems 2016 (NIPS)", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["ludovic trottier", "philippe gigu\\`ere", "brahim chaib-draa"], "accepted": false, "id": "1605.09332"}, "pdf": {"name": "1605.09332.pdf", "metadata": {"source": "CRF", "title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "authors": ["Ludovic Trottier", "Philippe Gigu\u00e8re", "Brahim Chaib-draa"], "emails": ["ludovic.trottier.1@ulaval.ca", "philippe.giguere@ift.ulaval.ca", "brahim.chaib-draa@ift.ulaval.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n09 33\n2v 1\n[ cs\n.L G\n] 3\n0 M\nay 2\n01 6"}, {"heading": "Parametric Exponential Linear Unit for", "text": ""}, {"heading": "Deep Convolutional Neural Networks", "text": ""}, {"heading": "Ludovic Trottier Philippe Gigu\u00e8re Brahim Chaib-draa", "text": ""}, {"heading": "Department of Computer Science and Software Engineering", "text": ""}, {"heading": "Laval University, Qu\u00e9bec, Canada", "text": "ludovic.trottier.1@ulaval.ca\nphilippe.giguere@ift.ulaval.ca\nbrahim.chaib-draa@ift.ulaval.ca\nMay 31, 2016\nThe activation function of Deep Neural Networks (DNNs) has undergone many changes during the last decades. Since the advent of the well-known non-saturated Rectified Linear Unit (ReLU), many have tried to further improve the performance of the networks with more elaborate functions. Examples are the Leaky ReLU (LReLU) to remove zero gradients and Exponential Linear Unit (ELU) to reduce bias shift. In this paper, we introduce the Parametric ELU (PELU), an adaptive activation function that allows the DNNs to adopt different non-linear behaviors throughout the training phase. We contribute in three ways: (1) we show that PELU increases the network flexibility to counter vanishing gradient, (2) we provide a gradient-based optimization framework to learn the parameters of the function, and (3) we conduct several experiments on MNIST, CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN, ResNet and Vgg, to demonstrate the general applicability of the approach. Our proposed PELU has shown relative error improvements of 4.45% and 5.68% on CIFAR10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet, along with faster convergence rate in almost all test scenarios. We also observed that Vgg using PELU tended to prefer activations saturating close to zero, as in ReLU, except at last layer, which saturated near -2. These results suggest that varying the shape of the activations during training along with the other parameters helps to control vanishing gradients and bias shift, thus facilitating learning."}, {"heading": "1. Introduction", "text": "Over the past few years, deep convolutional neural networks (CNNs) have been growing in popularity in the vision community (Kavukcuoglu et al., 2010; Farabet et al., 2013; Vinyals et al., 2015). Recently popularized by Krizhevsky et al. (2012) after winning the 2012 ImageNet competition (Russakovsky et al., 2015), their exceptional ability to capture high level abstractions from observations have led many to consider deeper networks. Since then, several attempts have been proposed to further increase the number of layers, such as the 19-layer Vgg (Simonyan and Zisserman, 2014), the 22- layer GoogleNet (Szegedy et al., 2015), or the most recent 200-layer ResNet (He et al., 2015a). However, building very deep neural networks (DNNs) gives rise to the vanishing and exploding gradients, which impede the learning process (Hochreiter, 1998). Vanishing and exploding gradients are a consequence of applying chain rule of derivation over many function compositions. During back-propagation, multiplying the gradient by a small value over and over may lead to an exponential magnitude decrease (vanishing). On the other hand, using activation functions allowing large output values can lead to larger and larger gradients (exploding). Since the weight update is proportional to the gradient, a small magnitude significantly affects the convergence rate when training very deep neural networks. This problem has been partly solved by using non-saturated activations in conjunction with batch normalization (BN) (Ioffe and Szegedy, 2015). Among the recently proposed non-saturated activations, Rectified Linear Unit (ReLU) (Nair and Hinton, 2010) has been adopted by many, due to its ease of use and exceptional performance (Glorot et al., 2011; Krizhevsky et al., 2012). Even though ReLU has interesting properties, such as sparsity and non-contracting first-order derivative, its discontinuity at the origin and zero gradient for negative arguments can hurt back-propagation. Moreover, its nonnegativity induces bias shift causing oscillations and impeded learning (Clevert et al., 2015).\nMore recently, several activation functions have been proposed to overcome these aforementioned limitations. Leaky ReLU (LReLU) (Maas et al., 2013) adds a positive slope to the negative part of ReLU to avoid zero gradient. Furthermore, He et al. (2015b) proposed learning LReLU slope parameter during training with Parametric ReLU (PReLU), while Xu et al. (2015) used stochastic regularization by repetitively sampling the slope from a uniform distribution with Randomized ReLU (RReLU). Although the reported experimental results showed performance improvements over ReLU, these activations yet suffer from the problem of increased variance, where neurons can have arbitrary large positive and negative outputs and still keep a proper weighted sum. The Exponential Linear Unit (ELU) (Clevert et al., 2015), defined as identity for positive arguments and exp(x) \u2212 1 for negative ones, deals with both the increased variance and bias shift problems. The non-zero negative part allows the DNNs to push each neuron\u2019s mean output toward zero, which acts as a unit natural gradient correction to ensure efficient learning (Clevert et al., 2015). By using a saturated negative part, the DNNs can no longer have arbitrary large negative outputs, which reduces variance. Even though ELU has interesting theoretical properties, such as non-contractive first order\nderivative for positive arguments, the DNNs can still easily get stuck in the saturated part during training where the gradient is almost zero. In this paper, we present Parametric ELU (PELU), an adaptive activation function based on a parameterization of ELU. Inspired by the work of He et al. (2015b), we define parameters controlling different aspects of the function and propose learning them with gradient descent during training. By adjusting the activation shape along with all the other parameters, the DNNs can adopt different non-linear behaviors in the course of the training phase. For instance, they can adjust the activation flatness (or peakiness) to deal with vanishing (or exploding) gradients, or change the saturation point on the negative side to control bias shift. Also, due to spreading each neuron\u2019s output value, a peak activation can help early during training to disentangle redundant neurons capturing similar concepts. By gradually flattening the activation, the DNNs could then progressively learn specialized neurons representing discriminative features. Our proposed PELU activation is related to two other approaches. The first one is the Adaptive Piecewise Linear (APL), which is a weighted sum of S parametrized Hinge functions (Agostinelli et al., 2014). As for PReLU, APL parameters are learned during training with gradient descent. Even though APL can in theory approximate any smooth functions, the amount of discontinuity grows linearly with S, which impairs back-propagation and can hurt training. Unlike APL, PELU is continuous at all points and has no hyper-parameters (like S) to be tuned. Another work by Goodfellow et al. (2013) is Maxout, which outputs the maximum over K affine functions for each input neuron. In addition to linearly growing discontinuities with K, Maxout also has the problem of multiplying by K the amount of weights to be learned in each layer. Unlike Maxout, PELU adds only 2L parameters, where L is the number of layers, which helps control over-fitting. Even though He et al. (2015b) inspired our work on PELU, our approach is different than PReLU. Unlike PReLU, our parameterization acts on both the positive and negative parts of the function, and is constrained to preserve continuity (therefore differentiability), which is beneficial for back-propagation. In addition to being hyper-parameter free, PELU has other interesting characteristics. It has the same computational complexity as ELU, and since parameters are defined layer-wise instead of per-neurons, the number of added parameters is small, which helps control over-fitting. Our experiments on various datasets (MNIST, CIFAR-10/100 and ImageNet) and several network architectures (Overfeat, Network in Network, Vgg, All-CNN and ResNet) show substantial performance improvements in both convergence speed and error rate over ReLU and ELU. The rest of the paper is organized as follows. In Section 2, we present our proposed PELU and show the experimentations in Section 3. We discuss the results in Section 4, and conclude in Section 5."}, {"heading": "2. Parametric Exponential Linear Unit (PELU)", "text": ""}, {"heading": "2.1. Definition", "text": "The Rectified Linear Unit (ReLU) (Nair and Hinton, 2010) activation, defined asmax{h, 0}, is non-negative for all pre-activations h, which gives a unit mean activation greater than zero. A recent analysis by Clevert et al. (2015) showed that units with non-zero mean activation acts as a bias for the next layer (known as the bias shift problem), which causes oscillations and impeded learning. Reducing bias shift corrects the gradient toward the unit gradient, which accelerates learning. To this effect, Clevert et al. (2015) proposed the Exponential Linear Unit (ELU), defined as identity for positive arguments and exp(h)\u2212 1 for negative ones. Since ELU outputs negative values for negative arguments, the network can push the mean activation toward zero, which reduces bias shift. Furthermore, as ELU saturates as the input gets negatively larger, the neurons cannot have arbitrary large positive and negative outputs yet still keep a proper weighted sum, thus reduces variance. By parameterizing ELU, we can give the network more control over the bias shift, and vanishing/exploding gradients. Let us start with the following parameterization:\nf(h) =\n{\nch if h \u2265 0 a(exp(h/b)\u2212 1) if h < 0 , a, b, c > 0 , (1)\nfor which the original ELU can be recovered when a = b = c = 1. Each parameter in (1) controls different aspects of the activation. Indeed, c changes the slope of the linear function in the positive quadrant (the larger c, the steeper the slope), b affects the scale of the exponential decay (the larger b, the smaller the decay), while a acts on the saturation point in the negative quadrant (the larger a, the lower the saturation point). Constraining the parameters in a positive way ensures that the function (1) stays monotonic, so that reducing the weight magnitude during training always lowers the neuron contribution. Using this parameterization, the network can control vanishing gradients by, say, increasing the slope with c or the exponential decay with b, and push the mean activation towards zero by lowering the saturation point with a. Since we want the network to adopt different non-linear behaviors during the course of the training phase, we now look into gradient descent and define update rules for each parameter, so that the network can adjust its behavior as it seems fit. However, a standard gradient update on parameters a, b, c would create a discontinuity at zero and impair back-propagation. Instead of relying on a projection operator to restore continuity after each update, we constrain our parameterization by forcing f to stay continuous at zero:\nlim h\u21920+ f(h) = lim h\u21920\u2212\nf(h) \u2261 \u2202ch\n\u2202h\n\u2223 \u2223 \u2223 \u2223\nh=0\n= \u2202a(exp(h/b) \u2212 1)\n\u2202h\n\u2223 \u2223 \u2223 \u2223\nh=0\n\u2261 c = a\nb . (2)\nIn (2), the continuous-at-zero constraint is expressed by equaling the derivative on both sides of zero, and solving for c. Incorporating (2) to f gives the proposed Parametric\nELU (PELU) activation:\nPELU: f(h) =\n{\na b h if h \u2265 0 a(exp(h/b)\u2212 1) if h < 0 , a, b > 0 (3)\nWith this parameterization, in addition to changing the saturation point and exponential decay respectively, both a, b adjust the slope of the linear function in the positive part to ensure continuity."}, {"heading": "2.2. Analysis", "text": "To understand the effect of the proposed parameterization, we now investigate the vanishing gradient for the following simple network, containing one neuron in each of its L layers:\nx = h0, hl = wlhl\u22121, zl = f(hl), E = \u2113(zL, y) (1 \u2264 l \u2264 L) (4)\nwhere we have omitted, without loss of generality, the biases for simplicity. In (4), \u2113 is the loss function between the network prediction zL and label y, which takes value E at x. In this case, it can be shown using the chain rule of derivation that the derivative of E with respect to any weight k is:\n\u2202E\n\u2202wk = hk\u22121f \u2032(hk)\n\n\nL \u220f\nj=k+1\nf \u2032(hj)wj\n\n\n\u2202E \u2202zL ,\nwhere f \u2032(hk) is a shortcut for \u2202zk \u2202hk . Vanishing gradient happens when the product term inside the bracket has a very small magnitude, which makes \u2202E \u2202wk \u2248 0. Since the updates are proportional to the gradients, the weights at lower layers converge more slowly than those at higher layers, due to the exponential decrease as k gets smaller (the product has L\u2212 k terms). One way the network can fight vanishing gradients is with f \u2032(hj)wj \u2261 f \u2032(wjhj\u22121)wj \u2265 1, so that the magnitude of the product does not tend to zero. Therefore, a natural way to investigate vanishing gradient is by analyzing the interaction between weight w and activation h, after dropping layer index j. Specifically, our goal is to find the range of values h for which f \u2032(wh)w \u2265 1. This will indicate how precise the activations h must be to manage vanishing gradients.\nTheorem 1. If w \u2265 b a and h < 0, then w\u2217 = exp(1) b a maximizes the interval length of h for which f \u2032(wh)w \u2265 1, which length takes value l\u2217 = a exp(\u22121).\nProof. With our proposed PELU, we have:\nf \u2032(wh)w =\n{\nw a b\nif h \u2265 0\nw a b exp(wh/b) if h < 0\n, a, b > 0 (5)\nAssuming w \u2265 b a and h < 0, and using the fact that w a b exp(wh/b) is monotonically increasing, the interval length l(w) of values h for which w a b exp(wh/b) \u2265 1 is given by\nthe magnitude of the zero of w a b exp(wh/b) \u2212 1. Solving the derivative equals zero for h gives l(w) = | log( b a 1 w )|( b w ). Using the fact that w \u2265 b a , it can be shown that l(w) is pseudo-concave, so it has a unique optimum. Maximizing l(w) with respect to w is thus the solution of solving the derivative equals zero, which gives l\u2217 = a exp(\u22121), at w\u2217 = exp(1) b\na . (See supplementary material for visual arguments.)\nThis result shows that in the optimal scenario where w = exp(1) b a , the length of negative values h for which f \u2032(wh)w \u2265 1 is no more than a exp(\u22121). Without our proposed parameterization (a, b = 1), dealing with vanishing gradient is mostly possible with positive arguments, which makes the negative ones (useful for bias shift) hurtful for back-propagation. With the proposed parameterization, a can be adjusted to increase the length a exp(\u22121) and allow more negative activations h to counter vanishing gradients. The ratio b\na can also be modified to ensure w \u2265 b a so that f \u2032(wh)w \u2265 1 for\nh > 0. Based on this analysis, the proposed parameterization gives more flexibility to the network, and the experiments in Section 3 have shown that the networks do indeed take advantage of it early during training."}, {"heading": "2.3. Optimization", "text": "PELU is trained simultaneously with all the network parameters using the standard back-propagation algorithm. We can derive the updates for both a and b with the chain rule of derivation on the objective E. The gradient of a and b for one layer is:\n\u2202E \u2202a = \u2211\ni\n\u2202E\n\u2202f(xi)\n\u2202f(xi)\n\u2202a ,\n\u2202E\n\u2202b =\n\u2211\ni\n\u2202E\n\u2202f(xi)\n\u2202f(xi)\n\u2202b , (6)\nwhere i sums over all elements of the tensor on which f is applied. The terms \u2202E \u2202f(xi) are the gradients propagated from the above layers, while \u2202f(x) \u2202a and \u2202f(x) \u2202b\nare the gradients of f with respect to a, b:\n\u2202f(x)\n\u2202a =\n{\nx b\nif x \u2265 0\nexp(x/b)\u2212 1 if x < 0 ,\n\u2202f(x)\n\u2202b =\n{\n\u2212ax b2\nif x \u2265 0\n\u2212 a b2 exp(x/b) if x < 0\n. (7)\nFor ImageNet experiments, we adopted stochastic gradient descent with momentum. To preserve the parameter positivity after the updates, we force them to always be greater than 0.1. We also constrained the saturation point to be close to -1 (as in ELU) by forcing a to be smaller than 2, i.e. by forbidding a saturation point lower than -2. In these conditions, the update rules are the following:\n\u2206a\u2190 \u00b5\u2206a\u2212 \u03b1 \u2202E\n\u2202a a\u2190 min{max {a+\u2206a, 0.1} , 2}\n, \u2206b\u2190 \u00b5\u2206b\u2212 \u03b1\n\u2202E\n\u2202b b\u2190 max {b+\u2206b, 0.1}\n(8)\nIn (8), \u00b5 is the momentum and \u03b1 is the learning rate. When specified by the training regimes, we also use a \u21132 weight decay regularization on both the weight matrices W\nand PELU parameters. This is different than PReLU, which did not use weight decay to avoid a shape bias towards ReLU. In our case, weight decay is necessary for a and b, otherwise the network could circumvent it for the W s by adjusting a or b, a behavior that would be hurtful for training."}, {"heading": "3. Experimentations", "text": "In this section, we present our experimentations in both unsupervised and supervised learning on the MNIST, CIFAR-10/100 and ImageNet tasks. Our goal is to compare the performance of different CNN architectures using either ReLU, ELU or PELU. Since Clevert et al. (2015) obtained higher performance improvements with ELU alone than with a combination of BN followed by ReLU, we also removed batch normalization before PELU."}, {"heading": "3.1. MNIST", "text": "Unsupervised learning is the task of learning a model to extract a feature representation from unlabeled observations without any supervision, which is generally useful for problems such as deep learning data fusion (Srivastava and Salakhutdinov, 2012). We first evaluated our proposed PELU activation on the task of learning a deep auto-encoder (refered to as DAA-Net) from MNIST images, without the labels. To this effect, we followed Desjardins et al. (2015) and defined an encoder composed of four fully connected layers of sizes 1000, 500, 250, 30, with a symmetrical decoder (the weights are not tied). We also added BN (for ReLU) and dropout (Srivastava et al., 2014) with probability 0.2 before and after the activation respectively, except for the last layer which we kept linear. We trained DAA-Net with RMSProp (Tijmen Tieleman, 2012) at a learning rate of 0.001, smoothing constant of 0.9 and a batch size of 128. Figure 1 presents the progression of the train and test mean square error averaged over five tries of DAA-Net on MNIST dataset. These results show that PELU outperforms ELU and ReLU for both convergence speed and reconstruction error. Note that the large difference between the train and test MSE (\u2248 15 vs \u2248 10\u22124) is solely due to using dropout before the last linear layer with MSE criterion during training, and is not a manifestation of ill-behaved optimization."}, {"heading": "3.2. CIFAR-10/100", "text": "In a recent study, Srivastava et al. (2015) noticed that adding more layers to suitably pretrained networks led to higher training error, which was surprisingly not a consequence of over-fitting. The performance degradation observed as depth increases was caused by the network\u2019s inability to disable the newly added layers, which would have been possible by learning identity mappings. This lack of capability is non-intuitive since efficient networks can be artificially deepened by stacking identity mappings, without any loss of performance. The existence of such a constructed solution has led He et al. (2015a) to consider skipping connections to help disable unnecessary layers, which they refer to as residual blocks. Originally trained with a combination of BN + ReLU, He et al. (2015a)\u2019s ResNet has recently been extended to ELU activation (Shah et al., 2016), with great success. Following Shah et al. (2016), we trained a ResNet with PELU activations, using their learning framework, on the CIFAR-10 and CIFAR-100 datasets (Krizhevsky et al., 2012) (60,000 32x32 colored images, 10 and 100 classes respectively). We used stochastic gradient descent with a weight decay of 0.001, momentum of 0.9 and mini batch-size of 128. The learning rate started at 0.1 and was divided by 10 after 81 epochs, and by 10 again after 122 epochs. We performed data augmentation as proposed by Lee et al. (2014): four pixels were added on each side of the image, and a random 32 x 32 crop was extracted, which was randomly flipped horizontally. Only color-normalized 32 x 32 images were used during the test phase. Our experiments were conducted with the 110-layer version of ResNet.\nWe also evaluated our proposed PELU on a smaller convolutional network (referred to as SmallNet) containing three convolutional layers followed by two fully connected layers. The convolutional layers were respectively composed of 32, 64 and 128 3x3 filters with 1x1 stride and 1x1 zero padding, each followed by BN, activation function, 2x2 max pooling with a stride of 2x2 and dropout with probability 0.2. The fully connected layers were defined as 2048\u2192 512, followed by BN, activation function, dropout with probability 0.5, and a final linear layer 512\u2192 10 for CIFAR-10 and 512\u2192 100 for CIFAR-100. SmallNet was trained using a cross entropy criterion with RMSProp (Tijmen Tieleman, 2012) at a learning rate of 0.001, smoothing constant of 0.9 and batch size of 512. We performed global pixel-wise mean subtraction, and used horizontal flip as data augmentation. Table 1 presents the test error results (in %) of SmallNet and ResNet110 on both tasks, with ELU, ReLU and PELU. For SmallNet, PELU reduced the error of ELU from 14.81% to 13.54% on CIFAR-10, and from 39.76% to 38.93% on CIFAR-100, which corresponds to a relative improvement of 8.58% and 2.09% respectively. As for ResNet110, PELU reduced the error of ELU from 5.62% to 5.37% on CIFAR-10, and from 26.55% to 25.04% on CIFAR-100, which corresponds to a relative improvement of 4.45% and 5.68% respectively. Importantly, PELU only adds 112 additional parameters to ResNet110, a negligible increase of 0.006% over the total number of parameters. Another preliminary experiment with ResNet32 has showed that PELU reduced the error of ELU from 7.30% to 6.70%, suggesting that our proposed approach is beneficial even with few layers. It is worth noting for ResNet110 that weight decay played an important role in ob-\ntaining these performances. Preliminary experiments conducted with a weight decay of 0.0001 showed no significant improvements of PELU over ELU. We observed larger differences between the train and test set error percentages with smaller weight decay, which indicated possible over-fitting. Although these experiments suggest using larger weight decays with PELU, a thorough investigation would help to better understand the effect of this regularization on PELU parameters. This is certainly an interesting avenue for future work."}, {"heading": "3.3. ImageNet", "text": "We finally tested the proposed PELU on ImageNet 2012 task (ILSVRC2012) using four different network architectures: ResNet18 (Shah et al., 2016; He et al., 2015a), Network in Network (NiN) (Lin et al., 2013), All-CNN (Springenberg et al., 2014) and Overfeat (Sermanet et al., 2013). As with the other experiments, we removed BN before ELU and PELU activations. However, due to NiN\u2019s relatively complex architecture, we used BN after each max pooling layer (every three layers) to help control vanishing gradient. Each network was trained with a momentum-based stochastic gradient descent (\u00b5 = 0.9) under the standard training regimes shown in Table 2.\nFigure 2 presents the error percentages of all four networks on ImageNet 2012 validation dataset. We see from these figures that PELU has the fastest convergence rate, which is indicated by both the large plateaus between the learning rate epoch changes and the low error percentages. Even though Overfeat with PELU was only sightly better than with ELU, all networks with PELU did outperform those with ReLU and ELU. Interestingly, PELU improved the error percentage of NiN from 40.40% to 36.06% with only 24 additional parameters, which corresponds to a relative improvement of 7.29% for\nonly 0.0003% parameter increase. Such a large improvement indicates that PELU acts on the networks in a different manner than the weights and biases, as stacking a new layer with 24 parameters would not have improved as much. These results thus indicate that the networks benefit from PELU, especially early during training where we see the most performance improvements. It is worth noting that training Overfeat with ELU was difficult. Indeed, even though we changed the batch size to 64, 128 or 256, it did not converge and stayed at an error rate close to 99%. This lack of convergence was caused by the presence of NaNs (not a number), indicating either overflows or division by 0. This was peculiar as we did not experience this problem with the other networks. On the other hand, we could see convergence when using BN before ELU or with small learning rate, but the network\nperformed either poorly or converged slowly. Since Overfeat with PELU did not have this NaN problem, we concluded that a too negative ELU saturation was causing the problem. By using a saturation at \u22120.5 instead of \u22121, we managed to train the network without difficulty. Even though the NaN problem proved relatively easy to solve, this demonstrates the usefulness of adaptive activations like PELU that can deal automatically with these problems."}, {"heading": "4. PELU Training Progression", "text": "In Section 2, we argued that, by adopting different non-linear behaviors throughout the training phase, our proposed PELU activation would give the networks more control over vanishing/exploding gradients and bias shift problems. In this section, we add to the experimental results of Section 3 supporting our claim with a visual evaluation of the non-linear behaviors adopted during training. To this effect, we trained a Vgg network (Simonyan and Zisserman, 2014), following Zagoruyko (2015) experimental framework, by replacing the ReLU activations with the proposed PELU. We trained the network using stochastic gradient descent with a learning rate of 1 (which is divided by 2 every 25 epochs), learning rate decay of 1e-7, momentum of 0.9, weight decay of 5e-4 and batch size of 128. We performed global pixel-wise mean subtraction, and used horizontal flip as data augmentation. Zagoruyko (2015) obtained 92.45% and 69.74% accuracy on CIFAR-10 and CIFAR100 respectively with ReLU activations, while we had 93.05% and 70.71% with our PELU. To shed light on the proposed parameterization dynamic variation over epoch, we visualized the 14 PELU activations throughout the training phase and analyzed them (the figures are available in the supplementary material). These results show that the network adopted different non-linear behaviors, both across the layers and across the epochs. The learned activation had a different slope for positive arguments, and interestingly, all activations eventually saturated at zero, except for layer 14 which saturated around -2. Even though Clevert et al. (2015) recommended using a negative saturation to reduce bias shift, the sparsity induced by saturating at zero seemed more important (in this case) in layers closed to the raw observations than in the last layer. In other experiments with Vgg, we tried adding BN at different locations in the network (after max pooling, before PELU), but we got the same convergence for the saturation point. This experiment also showed that each activation converged non-monotonically to its final shape. From layer 2 to 9, the network quickly increased the slopes (the value a/b) of the linear parts, and progressively decreased them, as well as quickly lowering the saturation point of layer 14. Since peak activations scatter the inputs more than flat ones, we believe this helped early during training to disentangle redundant neurons having similar values. Indeed, spreading neurons at the lower layer may help too quickly identify relevant structures within the data, as cluttered neurons all activate similarly. These results suggest that adopting different non-linear behaviors over the training epoch accelerates learning and improves performance."}, {"heading": "5. Conclusion", "text": "In this paper, we proposed the Parametric Exponential Linear Unit (PELU), an adaptive activation function that allows the networks to adopt different non-linear behaviors during the course of the training phase. Based on a parameterization of the previously proposed ELU activation, PELU enables the networks to change the saturation point for negative arguments and the peakiness of the linear part for positive ones. In this manner, the networks can have more control over bias shift and vanishing gradients. We conducted several experiments in unsupervised learning on the MNIST dataset, and supervised learning on CIFAR10/100 and ImageNet datasets. The results have shown that our proposed activation improves either the convergence rate or the error percentage for several network architectures, such as All-CNN, Vgg, ResNet, NiN and Overfeat. We observed as much as 7.28% relative improvement with only 0.0003% parameter increase, without any hyper-parameter tuning and a computational complexity of the same order as the original ELU. A visual investigation of the dynamic variation over the epochs of Vgg activations has shown that the network adopted different non-linear behaviors, both across the layers and across the epochs. Interesting avenues for future work include applying PELU to other network architectures, such as recurrent neural networks, and to other tasks, such as object detection."}, {"heading": "A. Supplementary Material", "text": ""}, {"heading": "A.1. Figures for Proof", "text": "In this section, we present Figure 3 containing the graph of the functions used during the analysis on vanishing gradient in Section 2.2 of the paper. The top part of Figure 3 contains the graph of f \u2032(wh)w, while the bottom part shows the graph of l(w). Both graphs suppose that w \u2265 b/a, as specified by the theorem. As seen in the top graph of Figure 3, the length of values h < 0 for which f \u2032(wh)w \u2265 1 is shown as the red segment l(w), which value is given by the absolute value of the zero of f \u2032(wh)w \u2212 1. From the bottom part of Figure 3, we can see that l(w), for w \u2265 b/a, is pseudo-concave, and thus has a unique optimum. Maximizing l(w) with respect to w can be done by solving the derivative equals zero. The solution w\u2217, taking value l(w\u2217) = l\u2217, is also plotted on the graph."}, {"heading": "A.2. PELU Training Progression", "text": "In this section, we provide the figures of Vgg PELU activations throughout the training phase on CIFAR-10, as discussed in Section 4 of the paper. Figure 4 presents the learned activations after training, while Figure 5 presents the progression of PELU parameters, for each layer.\nLearned Activations for Vgg on CIFAR\u221210 Task\nActivation Parameter Progression for Vgg on CIFAR\u221210 Task"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "The activation function of Deep Neural Networks (DNNs) has undergone many changes during the last decades. Since the advent of the well-known non-saturated Rectified Linear Unit (ReLU), many have tried to further improve the performance of the networks with more elaborate functions. Examples are the Leaky ReLU (LReLU) to remove zero gradients and Exponential Linear Unit (ELU) to reduce bias shift. In this paper, we introduce the Parametric ELU (PELU), an adaptive activation function that allows the DNNs to adopt different non-linear behaviors throughout the training phase. We contribute in three ways: (1) we show that PELU increases the network flexibility to counter vanishing gradient, (2) we provide a gradient-based optimization framework to learn the parameters of the function, and (3) we conduct several experiments on MNIST, CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN, ResNet and Vgg, to demonstrate the general applicability of the approach. Our proposed PELU has shown relative error improvements of 4.45% and 5.68% on CIFAR10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet, along with faster convergence rate in almost all test scenarios. We also observed that Vgg using PELU tended to prefer activations saturating close to zero, as in ReLU, except at last layer, which saturated near -2. These results suggest that varying the shape of the activations during training along with the other parameters helps to control vanishing gradients and bias shift, thus facilitating learning.", "creator": "LaTeX with hyperref package"}}}