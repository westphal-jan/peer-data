{"id": "1605.09696", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Generalized Multi-view Embedding for Visual Recognition and Cross-modal Retrieval", "abstract": "following this paper, the procedure connecting multi - view deviation from different visual lenses and modalities is considered. we propose a unified problem from subspace learning methods for ray rayleigh quotient, recognition is extensible for different criteria, supervised learning, and non - linear sampling. specific variations including multiple correlation reduction, partial least bit grammar and linear discriminant analysis are studied avoiding specific intrinsic and intrinsic reasoning approaching the main case. dynamic - linear extensions based on kernels splitting ( hence ) neural networks remained integrated, yield better performance underlying the functional ones. moreover, a novel optimization of multi - view linear approximation algebra remains proposed titled taking my smallest difference biased input. we demonstrate the effectiveness of 3 proposed multi - focus embedding procedures supporting collision artifact recognition and cross - modal image retrieval, potentially produces superior response utilizing particular applications compared to related methods.", "histories": [["v1", "Tue, 31 May 2016 16:11:16 GMT  (8493kb,D)", "https://arxiv.org/abs/1605.09696v1", "Submitted to IEEE Transactions on Image Processing"], ["v2", "Thu, 22 Sep 2016 15:58:20 GMT  (8738kb,D)", "http://arxiv.org/abs/1605.09696v2", null], ["v3", "Thu, 31 Aug 2017 09:17:50 GMT  (7015kb,D)", "http://arxiv.org/abs/1605.09696v3", null]], "COMMENTS": "Submitted to IEEE Transactions on Image Processing", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["guanqun cao", "alexandros iosifidis", "ke chen", "moncef gabbouj"], "accepted": false, "id": "1605.09696"}, "pdf": {"name": "1605.09696.pdf", "metadata": {"source": "CRF", "title": "Generalized Multi-view Embedding for Visual Recognition and Cross-modal Retrieval", "authors": ["Guanqun Cao", "Alexandros Iosifidis", "Ke Chen"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION\nPeople see the world differently, and objects are described from various point of views and modalities. Identifying an object can not only benefit from visual cues including color, texture and shape, but textual annotations from different observations and languages. Thanks to data enrichment from sensor technologies, the accuracy in image retrieval and recognition has been significantly improved by taking advantage of multiview and cross-domain learning [1], [2]. Since matching the data samples across various feature spaces directly is infeasible, subspace learning approaches, which learn a common feature space from multi-view spaces, becomes an effective approach in solving the problem.\nNumerous methods have been proposed in subspace learning. They can be grouped into three major categories based on the characteristics of machine learning: two-view learning and multi-view learning; unsupervised learning and supervised learning; and linear learning and non-linear learning. While traditional techniques in multivariate analysis take two inputs [3], multi-view methods have been proposed to find an optimal representation from more than two views [4], [5]. Compared to learning the feature transformation in an unsupervised manner, discriminative methods, such as Linear Discriminant Analysis (LDA) have been extended to multi-view cases. Additionally, the transformation can also be kernel-based or learned by (deep) neural nets to exploit their non-linear properties.\nTwo-view learning and multi-view learning: One of the most popular methods in multivariate statistics is Canonical Correlation Analysis (CCA) [6]. It seeks to maximize the correlation between two sets of variables. Alternatively, its\nmulti-view counterpart aims to obtain a common space from V > 2 views [4], [5], [7]. This is achieved either by scaling the cross-covariance matrices to incorporate the covariances from more than two views, or by finding the best rank-1 approximation of the data covariance tensor. A similar approach to find the common subspace is Partial Least Square Regressions [8]. It maximizes the cross-covariance from two views by regressing the data samples to the common space. Besides transformation and regression, Multi-view Fisher Discriminant Analysis (MFDA) [9] learns the transformation minimizing the difference between data samples of predicted labels. The Dropout regularization was introduced for the multi-view linear discirminant analysis in [10].\nUnsupervised learning and supervised learning: In contrast to unsupervised transformations, including CCA and PLS, LDA [11], [12] exploits the class labels effectively by maximizing the between-class scatter while minimizing the withinclass scatter simultaneously. CCA has been successfully combined with LDA to find a discriminative subspace in [13], [14], [15]. Coupled Spectral Regression (CSR) [16] projects two different inputs to the low-dimensional embedding of labels by PLS regressions. Consistent with the original LDA, a Multiview Discriminant Analysis (MvDA) [17] finds a discriminant representation over V views. The between-class scatter is maximized regardless of the difference between inter-view and intra-view covariances, while the within-class scatter is minimized in the mean time. Generalized Multi-view Analysis (GMA) [18] was proposed to maximize the intra-view discriminant information. Recently, a semi-supervised alternative [19] was also proposed for multi-view learning, which adopts a non-negative matrix factorization method for view mapping and a robust sparse regression model for clustering the labeled samples. Moreover, a multi-view information bottleneck method [20] was proposed to retain its discrimination and robustness for multi-view learning.\nLinear and non-linear learning: Many problems are not linearly separable and thereby kernel-based methods and learning representation by (deep) neural nets are introduced. By mapping the features to the high dimensional feature space using the kernel trick [21], kernel CCA [22] adopts a predefined kernel and limits its application on small datasets. Many linear multi-view methods subsequently made their kernel extension [23], [15], [24]. Kernel approximation [5] was adopted later to work on big data. Deep CCA [25] was proposed using neural nets to learn adaptive non-linear representations from two views, and uses the weights in the last layers to find the maximum correlation. A similar idea has been exploited on LDA [26]. PCANet [27] was introduced to\nar X\niv :1\n60 5.\n09 69\n6v 3\n[ cs\n.C V\n] 3\n1 A\nug 2\n01 7\nadopt a cascade of linear transformation, followed by binary hashing and block histograms.\nWe make several contributions in this paper: First, we propose a unified multi-view subspace learning method for CCA, PLS and LDA techniques using the graph embedding framework [11]. We design both intrinsic and penalty graphs to characterize the intra-view and inter-view information, respectively. The intra-view and inter-view covariance matrices are scaled up to incorporate more than two views for numerous techniques by exploiting their specific intrinsic and penalty graphs. In our proposed Multi-view Modular Discriminant Analysis (MvMDA), the two graphs also charaterize the within-class compactness and between-class separability. Based on the aforementioned characteristics of subspace learning algorithms, we propose a generalized objective function for multi-view subspace learning using Rayleigh quotient. This unified multi-view embedding approach can be solved as a generalized eigenvalue problem.\nSecond, we introduce a Multi-view Modular Discriminant Analysis (MvMDA) method by exploiting the distances between centers representing classes of different views. This is of particular interest since the resulting scatter encodes cross-view information, which empirically is shown to provide superior results. Third, we also extend the unified framework to the non-linear cases with kernels and (deep) neural networks. Kernel-based multi-view learning method is derived with an implicit kernel mapping. For larger datasets, we use the explicit kernel mapping [28] to approximate the kernel matrices. We also derive the formulation of stochastic gradient descent (SGD) for optimizing the objective function in the neural nets.\nLast but not least, we demonstrate the effectiveness of the proposed embedding methods on visual object recognition and cross-modal image retrieval. Specifically, zero-shot recognition is evaluated by discovering novel object categories based on the underlying intermediate representation [29], [30], [31]. Its performance is heavily dependent on the representation in the latent space shared by visual and semantic cues. We integrate observations from attributes as a middle-level semantic property for the joint learning. Superior recognition results are achieved by exploiting the latent feature space with nonlinear solutions learned from the multi-view representations. We also employ the proposed multi-view subspace learning methods for cross-modal image retrieval [1], [32], [33], [34]. This type of methods differs from the co-training methods for image classification [35] and web image reranking [36], [37]. In the experiments, we show promising retrieval results performed by embedding more modalities into the common feature space, and find that even conventional content-based image retrieval can be improved.\nThe rest of the paper is organized as follows. Section II reviews the related work. In Section III, we show the unified formulation to generalize the subspace learning methods. It is followed by the extension to multi-view techniques and derivation in kernels and neural nets. Then, in Section IV, we present the comparative results in zero-shot object recognition and cross-modal image retrieval on three popular multimedia datasets. Finally, Section V concludes the paper."}, {"heading": "II. RELATED WORK", "text": "In this section, we first define the common notations used throughout the paper. Then, we will briefly review the related methods for multi-view subspace learning. Moreover, recent work on non-linear methods concerning kernels and (deep) neural networks are discussed."}, {"heading": "A. Notations", "text": "We define the data matrix X = [x1,x2, . . . ,xN ], xi \u2208 RD, where N is the number of samples and D is the feature dimension. We also define Xv \u2208 RDv\u00d7N , v = 1, . . . , V for the feature vectors of the vth view, and discard the index in the single-view case for notation simplicity. Note that the dimensionality of the various feature spaces Dv may vary across the views. The covariance matrix is a statistics commonly used in CCA and PLS. We denote X\u0304v = Xv \u2212 1N Xv e e > as the centered data matrix. The cross-view covariance matrix between view i and j is then expressed as \u03a3ij = 1N X\u0304iX\u0304 > j =\n1 N Xi ( I \u2212 1N e e > ) X>j , where e \u2208 RN is a vector of ones and I \u2208 RN\u00d7N is the identity matrix. For the supervised learning problems, the class label of the sample xi is noted as ci \u2208 {1, 2, . . . , C}, where C is the number of classes. We define the class vector ec \u2208 RN with ec(i) = 1, if ci = c, and ec(i) = 0, otherwise. Wv \u2208 RDv\u00d7d, v = 1, . . . , V is the projection matrix for each view, d is the number of dimensions in the latent space. The feature dimension Dv in the\noriginal space of each view is usually high, which makes the distribution of the samples sparse, leading to several problems including the small sample size problem [39]. Therefore we want to project the samples to the latent space.\nThe generic projection function is defined to project X \u2208 RD\u00d7N to Y \u2208 Rd\u00d7N . We define the linear projection by Y = W>X. In kernel methods, we map the data to a Hilbert space F . Let us define \u03c6(\u00b7) as the non-linear function mapping xi \u2208 RD to F , and \u03a6 = [\u03c6(x1), . . . ,\u03c6(xN )] as the data matrix in F . In multi-view cases, \u03a6 = [\u03a6>1 , . . . ,\u03a6>V ]>. Since the dimensionality of F is arbitrary, the kernel trick [40] is exploited in order to implicitly map the data to F . The Gram matrix is given by\nKv = \u03ba(Xv,Xv) = \u03a6 > v \u00b7\u03a6v, (1)\nwhere \u03ba(\u00b7, \u00b7) is the so-called kernel function. The centered Gram matrix is K\u0304v = Kv \u2212 1N 1 Kv \u2212 1 N Kv 1\n> + 1N2 1Kv 1, where 1 \u2208 RN\u00d7N is an all-ones matrix. In order to find the optimal projection, we can express Wv of each view as a linear combination of the training samples in the kernel space based on the Representer Theorem [21], [41]. This can be expressed by using a new weight matrix Av as\nWv = \u03a6vAv. (2)\nIn the case where a neural network with M layers is considered, \u03b2j contains the weight parameters in the jth layer, j = 1, . . . ,M . The weights B = [\u03b21, . . . ,\u03b2M ] are learned by applying stochastic gradient descent (SGD), and h( \u00b7 ; B) is a non-linear mapping function which maps Xv to the representation of the last hidden layer Hv , i.e.\nHv = h(Xv; Bv), (3)\nwhere Bv is the weight matrix trained by applying backpropagation in the vth network."}, {"heading": "B. Canonical Correlation Analysis (CCA)", "text": "Canonical Correlation Analysis (CCA) [6], [42] is a conventional statistical technique which finds the maximum correlation between two sets of data samples X1 \u2208 RD1\u00d7N and X2 \u2208 RD2\u00d7N using the linear combination Y1 = W>1 X1 and Y2 = W > 2 X2. W1 and W2 are determined by optimizing:\nJ = arg max W1,W2 corr(W>1 X1,W > 2 X2) (4)\n= arg max W1,W2 W>1 \u03a312W2\u221a W>1 \u03a311W1 \u00b7 \u221a W>2 \u03a322W2 , (5)\nwhere\n\u03a3 =\n[ \u03a311 \u03a312\n\u03a321 \u03a322\n] = 1\nN\n[ X\u03041X\u0304 > 1 X\u03041X\u0304 > 2\nX\u03042X\u0304 > 1 X\u03042X\u0304 > 2\n] (6)"}, {"heading": "C. Kernel CCA", "text": "Kernel CCA finds the maximum correlation between two views after mapping them to the kernel space [22]. This is expressed by\nJ = arg max W1,W2 corr(W>1 \u03a61,W > 2 \u03a62) (7)\nWe use the kernel trick [40] and the Representer Theorem in (2), and derive the objective function for the kernel CCA as\nJ = arg max A1,A2 A>1 K1K2 A2\u221a A>1 K1K1 A1 \u00b7 \u221a A>2 K2K2 A2 . (8)"}, {"heading": "D. Deep CCA", "text": "Deep CCA maximizes the correlation between a pair of views by learning non-linear representations from the input data through multiple stacked layers of neurons [25], [43]. A linear CCA layer is added on top of both networks, and the inputs to the CCA layer depend on the network outputs H1 and H2. Similar to the non-linear case in (8), a modified objective function min\nW1,W2 \u2212 1N Tr\n( W1 >H1 H > 2 W2 ) is\noptimized, where W1,W2 are the projection matrices in the CCA layer, and the correlated outputs are Y1 = W>1 H1 and Y2 = W > 2 H2. A modified SGD method is developed with respect to the inputs H1 and H2 to the linear layer, which are also the outputs from the two networks. The objective function is expressed as Tr ( W>1 H1 H > 2 W2 ) = Tr(T>T) 1 2 , which describes the correlation as the sum of the top d singular vectors of T = \u03a3\u22121/211 \u03a312\u03a3 \u22121/2 22 whose definition can be found in [3]."}, {"heading": "E. Partial Least Squares (PLS) regression", "text": "Partial Least Squares (PLS) regression [8] is another dimensionality reduction technique derived from the linear combination of the input vectors X1 together with the target information which is considered as the second view X2. PLS maximizes the between-view covariance by solving\nJ = arg max W1,W2 [Tr(W>1 X1X > 2 W2)], (9)\nsubject to W>1 W1 = I,W > 2 W2 = I. (10)\nThe non-linear extensions of PLS are obtained in the similar manner as the ones in CCA."}, {"heading": "F. Generalized Multi-view Analysis (GMA)", "text": "GMA [18] is a generalized framework incorporating numerous dimensionality reduction methods. It maximizes the intra-view discriminant information, but ignores the inter-view information.\nJ = argmax W\nTr  V\u2211\ni V\u2211 i<j 2\u03bbijW > i XiX > j Wj + V\u2211 i=1 \u00b5iW > i PiWi  , subject to\nV\u2211 i W>i QiWi = I. (11)\nHere both P and Q are the intra-view covariance matrices. P is a square matrix and Q is a square symmetric definite matrix. We adopt Generalized Multiview Marginal Fisher Analysis (GMMFA) in this framework. The method is also kernelizable using the Representer Theorem and kernel trick."}, {"heading": "G. Linear Discriminant Analysis (LDA)", "text": "Linear Discriminant Analysis (LDA) [11], [44] finds the projection by maximizing the ratio of the between-class scatter to the within-class scatter. Let us define by \u00b5c the mean vector of the c\u2019th class, formed by Nc samples, and \u00b5 the global mean. Then, LDA optimizes the following criterion:\nJ = arg max W\nTr(W>P W) Tr(W>Q W) , (12)\nwhere P = C\u2211\nc=1\nNc(\u00b5c \u2212 \u00b5)(\u00b5c \u2212 \u00b5)> = X ( C\u2211\nc=1\n1\nNc ecec\n> \u2212 1\nN e e>\n) X>,\n(13)\nQ = N\u2211 i=1 (xi \u2212 \u00b5c)(xi \u2212 \u00b5c)> = X ( I\u2212 C\u2211 c=1 1 Nc ecec > ) X>. (14)\nNon-linear extensions with kernels include KDA [45] and KRDA [46]."}, {"heading": "H. Multi-view Discriminant Analysis (MvDA)", "text": "MvDA [17] is the multi-view verison of LDA which maximizes the ratio of the determinant of the between-class scatter matrix to that of the within-class scatter matrix. Its objective function is\nJ = arg max W\nTr(SMB ) Tr(SMW ) , (15)\nwhere the between-class scatter matrix is\nSMB = V\u2211 i=1 V\u2211 j=1 W>i Xi ( C\u2211 c=1 1 Nc ecec > \u2212 1 N e e> ) X>j Wj , (16)\nand the within-class scatter matrix is\nSMW = V\u2211 i=1 V\u2211 j=1 W>i Xi ( I\u2212 C\u2211 c=1 1 Nc ecec > ) X>j Wj . (17)\nW contains the eigenvectors of the matrix S = SMW \u22121\nSMB corresponding to the leading d eigenvalues \u03bbi."}, {"heading": "III. GENERALIZED MULTI-VIEW EMBEDDING", "text": "Here we propose a generalized expression of objective function for multi-view subspace learning. The generalized optimization problem is given by:\nJ = arg max W\nTr(W>PW) Tr(W>QW) (18)\nwhere P and Q are the matrices describing the inter-view and intra-view covariances, respectively. The above equation has the form of the Rayleigh quotient. Therefore, all subspace learning methods that maximize the criterion can be reduced to a generalized eigenvalue problem:\nPW = \u03c1 QW, (19)\nand the solution is given in (20) below:\nW =  W1 ...\nWV\n and \u03c1 = d\u2211\ni=1\n\u03bbi (20)\nare the generalized eigenvector and the sum of the top d generalized eigenvalues \u03bbi, respectively. W contains the projection matrices of all views, and \u03c1 is the value of Rayleigh quotient. We address the Rayleigh quotient as the uniform objective function, reaching out to all subspace learning methods in the paper. The non-linear multi-view embeddings can be achieved by kernel mappings, or (deep) neural networks optimized by SGD. Suppose we have a linear projection Y = W>X, Svij is a similarity weight matrix which encodes the intraview properties to be minimized, and S\u2032vij is a penalty weight expressing the inter-view properties to be maximized. Then based on [11], [47], we can express the objective function as follows\nJ = arg max W>W=I\nV\u2211 v=0 N\u2211 i=0 N\u2211 j=0 S\u2032vij\u2016W>v Xvi \u2212W>v Xvj\u20162\nV\u2211 v=0 N\u2211 i=0 N\u2211 j=0 Svij\u2016W>v Xvi \u2212W>v Xvj\u20162 (21)\n= arg max W>W=I\nTr(W>XL\u2032X>W) Tr(W>XLX>W) . (22)\nIn the kernel case, we also have\nJ = arg max A>KA=I\nTr(A>K L\u2032KA) Tr(A>K L KA) . (23)\nIn the above, we define the diagonal matrix of each view pair as Duv whose i-th element is [Duv]ii = \u2211 j [Suv]ij , and the total graph Laplacian matrix as L = D \u2212 S. Similarly, we have D\u2032,S\u2032,L\u2032 in the penalty graph.\nFor the non-linear mapping by neural networks, we deploy a linear embedding layer on top of the networks. This scheme is illustrated in Fig. 2. Since we have more than two input views, we train multiple neural networks whose outputs are connected to the linear layer and the objective is the same as in the linear case. By backpropagating the error of the weight matrix, we optimize the Rayleigh quotient criterion with respect to the non-linear feature representation from each view in the last hidden layer of the networks. The projection is found in the same way as in the linear case, and we will address the SGD formulation for the specific algortihms in the next section.\nFig. 3 illustrates the proposed framework graphically. We\ncan extract different types of low-level features from images, texts, and intermediate representations. The multi-modal feature vectors are passed through linear or non-linear projections to the latent space. The projected features characterize the properties of the intra-view compactness and inter-view separability based on the proposed criterion. We show the scaled inter-view and intra-view matrices for each multi-view algorithm in the next section. Then, the projection matrices are presented with respect to their own intrinsic and penalty graph matrices and the optimization methods."}, {"heading": "A. Scaling up the inter-view and intra-view covariance matrices", "text": "The idea behind multi-view CCA (MvCCA) is to maximize the correlation between all pairs of views. Its objective can be rephrased as maximizing the inter-view covariance while minimizing the intra-view covariance in the latent space. Therefore, we consider inter-view covariance matrices between different view representations in P and the covariance matrices of each view in Q. Multi-view PLS (MvPLS) maximizes the inter-view covariance directly. Since we also embed the target information for the subspace learning, the proposed MvPLS differs from MvCCA only in the intra-view minimization. Taking the class discrimination into consideration, the novel multi-view modular discriminant analysis (MvMDA) extends to separate the data of different classes between views while making the intra-class data compact. We illustrate the structure of P and Q for each method in Table I."}, {"heading": "B. Linear subspace learning", "text": "When the subspace projection is linear, we can obtain the latent feature vectors from each view as\nYv = W > v Xv, (24)\nand the projection matrix is derived directly by solving the generalized eigenvalue problem in (19). As shown in Table I,\nmulti-view CCA has the total covariance matrix \u03a3 = P + Q, and we derive its projection matrix by fulfilling the criterion below\nJ = argmax Wv,v=1,...,V\nTr\n( V\u2211\ni=1 V\u2211 j 6=i j=1 W>i Xi LX > j Wj\n)\nTr ( V\u2211 i=1 W>i Xi LX > i Wi ) , (25) where the Laplacian matrix L = I\u2212 1N e e\n>. Multi-view PLS has the same Laplacian matrix as the one in Multi-view CCA. We only optimize the Rayleigh quotient by maximizing the cross-covariance matrices between different views as\nJ = arg max W>W=I Tr ( V\u2211 i=1 V\u2211 j 6=i j=1 W>i Xi L X > j Wj ) , (26)\nwhose solution is the projection matrix.\nWe propose two ways to determine the projection matrix in multi-view LDA. The first appoach is the multi-view extension of the standard LDA, and its between-class scatter SB maximizes the distance between the class means from all views:\nSB = V\u2211 i=1 V\u2211 j=1 C\u2211 p=1 C\u2211 q=1 p 6=q (mip \u2212mjq)(mip \u2212mjq)>\n= V\u2211 i=1 V\u2211 j=1 W>i XiLBX > j Wj , (27)\nwhere the between-class Laplacian matrix is\nLB =  2 C\u2211 p=1 C\u2211 q=1 p 6=q ( V N2p ep e > p \u2212 1 NpNq ep e > q ) if i = j, \u22122 C\u2211\np=1 C\u2211 q=1 p6=q 1 NpNq ep e > q if i 6= j. (28)\nmip denotes the mean from the ith view of the pth class in the latent space, and ep is the N -dimensional class vector, with Np as the number of samples in the pth class. The class q is different from the class p.\nAlternatively, we propose the between-class scatter matrix which maximizes the distance between different class centers across different views. Since it considers the samples from the class of the specific view origin, we call it Multi-view Modular Discriminant Analysis (MvMDA), and its forumulation is\nS\u2032B = V\u2211 i=1 V\u2211 j=1 C\u2211 p=1 C\u2211 q=1 p 6=q (mip \u2212miq)(mjp \u2212mjq)>\n= V\u2211 i=1 V\u2211 j=1 W>i XiL \u2032 BX > j Wj , (29)\nand the Laplacian matrix is\nL\u2032B = 2 C\u2211 p=1 C\u2211 q=1 ( 1 N2p ep e > p \u2212 1 NpNq ep e > q ). (30)\nThe difference between the two approaches is that SB has 1N2c (V \u2212 1) V\u2211 i=1 C\u2211 c=1 W>i Xiece > c X > i Wi, while S \u2032 B has the\nterm 1N2c V\u2211 i=1 V\u2211 j=1 j 6=i C\u2211 c=1 W>i Xiece > c X > j Wj which suggests that the first proposal only considers the maximum of the intraview distances, while the second proposal can maximize the distance between different views. We also validate experimentally that the second proposal achieves better results. Detailed derivation of the two approaches of (27) and (29) are included in the supplementary material.\nWe extend the same formulation of within-class Laplacian\nmatrix in the latent space as the single-view LDA, i.e.\nSW = V\u2211 i=1 W>i Xi ( I\u2212 C\u2211 c=1 1 Nc ecec > ) X>i Wi\n= V\u2211 i=1 W>i Qii Wi, (31)\nwhere Qii = XiLWX>i , and LW = I\u2212 C\u2211\nc=1\n1 Nc ecec >. From\n(27) and (31), it is shown that the between-class and withinclass scatters are equivalent to the projected inter-view and intra-view covariance, respectively. The projection matrix of the multi-view LDA is found by optimizing the following objective function\nJ = argmax Wv,v=1,...,V\nTr ( V\u2211 i=1 V\u2211 j=1 W>i XiL \u2217 BX > j Wj ) Tr ( V\u2211 i=1 W>i XiLW X > i Wi\n) , (32) where L\u2217B is denoted as the Laplacian matrix of either LB or L\u2032B ."}, {"heading": "C. Kernel-based non-linear subspace learning", "text": "Exploiting the kernel trick in (1) and the Representer theorem in (2) and (24) can be expressed as follows\nYv = A > v \u03a6 > v \u03a6v = A > v Kv. (33)\nThe criterion of kernel multi-view CCA is then,\nJ = argmax Kv,v=1,...,V\nTr ( V\u2211 i=1 V\u2211 j 6=i j=1 A>i Ki LKjAj )\nTr ( V\u2211 i=1 A>i Ki LKiAi ) . (34)\nIt can be easily shown that the solution for Av is the same as (19).\nKernel multi-view PLS maximizes the covariance between pairs of feature vectors in the kernel space and therefore the objective function is\nJ = argmax Kv,v=1,...,V Tr\n( V\u2211\ni=1 V\u2211 j 6=i j=1 A>i Ki LKjAj\n) . (35)\nThe criterion for kernel multi-view discriminant analysis is\nJ = argmax Kv,v=1,...,V\nTr ( V\u2211 i=1 V\u2211 j=1 A>i KiL \u2217 BKjAj ) Tr ( V\u2211 i=1 A>i KiLWKiAi ) (36)"}, {"heading": "D. Non-linear subspace learning using (deep) neural networks", "text": "Exploiting the non-linear mapping using neural networks by (3), (24) can expressed as\nYv = W > v h(Xv; Bv) = W > v Hv. (37)\nSince the network outputs are combined by a linear layer as shown in Fig. 2, the parameters Bv of each network are\njointly trained to reach the optimal criterion value. After the transformation by neural networks, the projection becomes the same as the multi-view linear subspace learning with respect to Hv . Therefore, we need an additional optimization solved by SGD. We experimented with SGD without variance constraints, and found that we could obtain much better results with the projections constrained to have the unit variance, i.e. in Deep Multi-view CCA (DMvCCA), we have\nV\u2211 i=1 W>i Hi L H > i Wi = I. (38)\nWithout intra-view minimization, the optimization of Deep Multi-view PLS (DMvPLS) is constrained to have unit vari-\nance V\u2211 i=1 W>i Wi = I, while in Deep Multi-view Modular Discriminant Analysis (DMvMDA), we project the withinclass scatter into unit, i.e.\nV\u2211 i=1 W>i HiLWH > i Wi = I (39)\nWith the variance constraint, the expressions of the gradients in DMvCCA and DMvPLS are the same as\n\u2202J \u2202Hi = \u2202 \u2202Hi Tr\n( V\u2211\ni=1 V\u2211 j 6=i j=1 W>i Hi LH > j Wj\n)\n= V\u2211 i=1 V\u2211 j 6=i j=1 Wi W > j Hj L, (40)\nand the gradient of DMvMDA is computed as\n\u2202J \u2202Hi = \u2202 \u2202Hi Tr\n( V\u2211\ni=1 V\u2211 j=1 W>i Hi L \u2217 B H > j Wj\n)\n= V\u2211 i=1 V\u2211 j=1 Wi W > j Hj L \u2217 B , (41)\nDetailed derivation of (40) and (41) can be found in the supplementary material."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we evaluate the multi-view methods on two important multimedia applications: zero-shot recognition on the Animal with Attribute (AwA) dataset, and crossmodal image retrieval on the Wikipedia and Microsoft-COCO datasets."}, {"heading": "A. Experimental Setup", "text": "We conduct the experiments on three popular multimedia datasets. One common property in these datasets is that multimodal feature representations can be generated. The Animal with Attribute (AwA) dataset consists of 50 animal classes with 30, 475 images in total, and 85 class-level attributes. We follow the same setup as in [31] by splitting 40 classes (24, 295 images) to train the categorical model while the rest 10 classes with 6, 180 images for testing. Sample images from the test set are shown in Fig. 1. Each animal class contains more than one positive attribute, and the attributes are shared across classes\nwhich enables zero-shot recognition. The detailed class labels and attributes are provided in [31].\nWikipedia is a cross-modal dataset collected from the \u201cWikipedia featured articles\u201d [1]. The dataset is organized in 10 categories and consists of 2, 866 documents. Each document is a short paragraph with a median text length of 200 words, and is associated with a single image. We follow the train/test split in [1] who use 2, 173 training and 693 test pairs of images and documents.\nThe third dataset we use is the Microsoft COCO 2014 Dataset [48] (abbreviated as COCO in latter paragraphs). We collect the images belonging to at least one fine-grained category, which amounts to 82, 081 training images, and 40, 137 validation images. More than 5 human-annotated different captions are associated to each image. We follow the same definition in [48] to use 12 super classes as the class labels, and 91 fine-grained categories as the attributes. The class names and attributes are presented in Table II. The classes that the images belong to are highly semantic, and the same image can have multiple class labels. Meanwhile, similar images may belong to several different classes.\nWe use the following feature representations in the experiments:\n\u2022 Image feature by CNN models: We employ the off-theshelf CNN models as stated in [49] and [33] on all image datasets \u2014 Visual features are extracted by adopting two powerful pre-trained models. We rescale the size of the input images to 224\u00d7224, and generate the features from the outputs of the fc8 layer in a VGGNet with 16 weight layers [50] (denoted as VGG-16 in latter sections), and the loss3/classifier layer from a GoogleNet [51]. Both models produce 1000-dimension feature vectors. \u2022 Class label encoding: Since each image corresponds to one class label on the AwA and Wikipedia dataset, we can describe the image category using the textual feature mapped from the image feature. Specifically, we firstly train a 100-dimension skip-gram model [52] on\nthe entire English Wikipedia articles composed of 2.9 billion words. Then we can extract a separate set of word vectors from class labels of our datasets. In order to correlate the labels with the image contents, we train a ridge regressor with 10-fold cross-validation to map the VGG-16 image features to each dimension of the word vectors respectively. The regressor outputs are used as the class label features. \u2022 Attribute encoding: We also adopt another important modality from visual attributes on the AwA and COCO datasets. On the AwA dataset, we use the 50\u00d7 85 classattribute matrix in [53], [54] which specifies attribute probabilities of each class, while on the COCO dataset, we develop a 91-bin feature vector as attributes for each image of which 1\u2019s denote the image has the fine-grained tag and 0\u2019s otherwise. Then, we train a ridge regressor between the VGG-16 image feature and formulated attribute probabilities. The predicted probabilities associated with each image are used as the attribute feature. \u2022 Sentence encoding: A vital feature of cross-modal retreival system is that we make use of textual features directly. We can find a paragraph of text describing each image on the Wikipedia dataset, while on the COCO dataset, a similar paragraph can be developed by concantenating all captions from the annotators which are associated to each image. We generated the sentence vectors from the paragraphs by the pre-trained skip-thoughts model [55]. The model was trained over the MovieBook and BookCorpus dataset [56]. On the Wikipedia, we employ the combined-skip vector of 4800 dimensions, while due to the large size of COCO dataset, we only use the uniskip vector of 2400 dimensions.\nThe Experiment protocol and performance metrics are described below:\n\u2022 Zero-shot recognition on the AwA dataset: We follow a similar experiment pipeline as in [57], and the comparative results show the performance of the proposed multiview embedding methods. We project the multi-view representations to the latent space. Zero-shot recognition is achieved by semi-supervised label propagation on a transductive hypergraph in the latent space. Specifically, the cross-domain knowledge learned from the common semantic space is tranferred to the target space of 10 test animal classes via attributes. The prediction of target classes is undertaken on a hypergraph to better integrate different views. We replace the multi-view linear CCA for joint embedding in [57] with the generalized embedding methods. Since the same hypergraph is used, the recognition results indicate the different performance by the multi-view methods in this paper. For the evaluation metric, we use the average classification accuracy which is also employed in [31], [57]. \u2022 Cross-modal retrieval on the Wikipedia and COCO datasets: We perform two tasks in cross-modal retrieval, i.e. text query for image retrieval and image query for text retrieval. Moreover, a conventional content-based image retrieval system is evaluated in Section IV-C4. We\nfirst extract the test features in their own domains. A latent space is joinly learned from the image features, intermediate feature and sentence feature in the training set. Test features are then projected to the latent space by the trained model. The semantic matching from [1] is performed by training a logistic regressor over the embedded features from all of the ground truth samples which maps the projected features of both queries and to-be-retrieved images/texts towards the class labels. The feature vectors generated from the ground truth class labels are essentially the class vectors, whose dimensionality is the number of classes. We use the class probabilities from the regressor outputs for matching between modalities. We present the results using 11-point interpolated precision-recall (PR) curves. The Mean Average Precision (MAP) score, which is the average precision at the ranks where recall changes, can be computed based on the Precision Recall curves. The Average Precision (AP) measures the relevance between a query and retrieved items [58], and the MAP score calculates the mean AP by querying all items in the test set."}, {"heading": "B. Parameter Settings", "text": "The dimensionality d in the latent space is a pre-defined parameter. We will evaluate the effects of different d values in the following section. In the experiment, we use d = 50 for linear projections on all datasets. On the Wikipedia and AwA dataset, we choose d = 150 for kernel mappings, and d = 200 for the COCO dataset. For computational efficiency on the AwA and COCO dataset, an approximated RBF kernel mapping is adopted for the non-linear mappings. We set \u03c3 in the RBF kernel as the average distance between samples from different views/modalities, which is the natural scaling factor for each dataset. In all of the experiments, the original training set is further partitioned into a 80% training split and a 20% validation split.\nThe topology of neural networks has more variabilities, and we chose the optimal one according to the held-out validation set. We refer to [59], [60] for a detailed discussion on topologies. On the AwA dataset, we took 3 hidden layer, each with 1, 024 neurons with the relu activation before the 50-dimensional linear embedding layer. We only adopted the linear and kernel-based embeddings on the Wikipedia dataset in view of its small size. On the COCO dataset, we chose a single hidden layer with 1500 relu neurons, and the dimensionality of the final linear layer is also 1500. We experimented both with the whole batch and multiple mini-batches for SGD, and adopted a batch size of 200 which achieves a superior performance. The number of epoches is set to 50 empirically."}, {"heading": "C. Experimental Results", "text": "The abbreviations of the numerous methods are shown in Table III.\nTABLE III: List of Abbreviations\nLMvCCA / KMvCCA / KapMvCCA / DMvCCA Linear / Kernel / Approximate Kernel / Deep Multi-view Canonical Correlation Analysis LMvPLS / KMvPLS / KapMvPLS / DMvPLS Linear / Kernel / Approximate Kernel / Deep Multi-view Partial Least Square Regression SLMvDA/ SKMvDA Standard Linear / Kernel Multi-view Discriminant Analysis using (28) LMvMDA / KMvMDA / KapMvMDA / DMvMDA Linear / Kernel / Approximate Kernel / Deep Multi-view Modular Discriminant Analysis using (30) MULDA / KMUDA [15] Multi-view Uncorrelated Linear / Kernel Discrimiant Analysis MvDA [17] Multi-view Discrimiant Analysis GMA [18] Generalized Mult-view Analysis DCCA2 [25] Deep Canonical Correlation Analysis\n(a) 2-view LMvCCA (b) 3-view LMvCCA (c) 4-view LMvCCA\n(d) 4-view MvDA [17] (e) 4-view GMA [18] (f) 4-view DMvMDA\nFig. 4: The first row shows the 2-D visualization of embeddings by LMvCCA with an increasing number of views on the AwA dataset. The second row presents the embedding maps by different methods all with 4 views on the same dataset. The samples from different classes are denoted in different colors.\n1) Results on zero-shot recogntion: We visualize the embedded space in Fig. 4. We use the VGG-16 feature and class label encoding for two views, and augment attribute and GoogleNet encodings as the additional views. In the first row, it is shown with the increasing number of views in MvCCA, the latent feature vector progresses from being distributed incoherently to showing more distinct groups. In the second row, we compare different methods with 4 views. It is clearly shown we obtain a set of more compact and separable features by the proposed DMvMDA.\nRecognition accuracy of different methods is compared quantitatively in Table IV. The first group contains the linear projection results, the second uses the kernel methods, the third are the results by deep neural nets, and the last category includes several comparative results in the literature. The linear methods perform favorably in general while the leading recognition rates can be found in the non-linear methods using neural nets with 4 views. The kernel approximation does not provide superior results compared to linear methods due to the information loss in sampling [28]. Above all, the 4-view DMvMDA is reported to be the best method for zero-shot recognition. The results are also organized by the number of views in columns, and it is shown for all methods that we consistently obtain a better accuracy with more views. Specifically, the proposed LMvPLS achieves the highest accuracy with two input views. while the novel LMvMDA has a more discriminant representation in the latent space leading to a better recognition when more views are presented.\n2) Cross-modal retrieval results on the Wikipedia Dataset: Due to the limited number of samples, we use PCA before performing the subspace learning. We use the VGG-16 and sentence features for two views, and augment attribute and GoogleNet encodings as the additional modalities. It is shown that a better MAP score is obtained when enriching the latent feature with more modalities as shown in Table V. We also observe that the supervised methods perform better than the unsupervised counterparts, and non-linear projections by kernel methods are superior. KMvMDA achieves the best retrieval results with supervision and non-linearity.\nWe present more detailed results in the form of PR curves in Fig. 5. For image queries, KMvMDA consistently outperforms the other methods across all views, which can be explained by its utilization of class labels and kernel-based representations. For text queries, the supervised and non-linear methods also outperform their linear counterparts. KMvCCA and KMvMDA are the leading methods in this category, which shows the strength of cross-modal retrieval by making use of view difference.\n3) Cross-modal retrieval results on the COCO Dataset: The COCO dataset is much larger than the Wikipedia dataset, and we pay more attention to the non-linear methods especially the ones using neural networks. Many images have more than one class labels, and therefore we focus on the unsupervised learning algorithms. Similar to the experiments above, the MAP scores in Table VI show that a gain of retrieval accuracy can be obtained by embedding additional modalities into the latent space. DCCA2 [25] achieves a superior performance with 2 views thanks to its non-linear projection which makes the latent feature more discriminant for retrieval. However, its\nformulation limits the algorithm to 2 views, and DMvCCA and DMvPLS based on the proposed framework can improve the state-of-the-art method by increasing the number of modalities. From the PR curves in Fig. 6, we compare the methods using the proposed objective function with DCCA2 which contains two views. For image queries, KapMvCCA obtains the best retrieval result with 2 views, but it is further improved by the methods using neural networks benefitted by attributes and GoogleNet features. For text queries, it also suggests more modalities and neural network-based representations contribute to the retrieval performance. The cross-modal retrieval by the 4-view DMvCCA achieves the overall highest precision score on this dataset.\n4) Content-based Image Retrieval (CBIR) Performance on the COCO dataset: We also show the effectiveness of multiview embedding method on the conventional CBIR task in Fig. 7. We randomly pick two image-to-text pairs as queries, to perform image-to-image retrieval using both the VGG-16 visual feature and the projected visual feature by the 4-view DCCA. We also perform text-to-image retrieval by querying the corresponding captions of the query image used in CBIR in the last column. We observe the CBIR performance can be further improved by incorporating the semantic information. In Table VII, we present the quantitative results of CBIR by the projected visual features. \u201cRAW\u201d in the Table shows the retrieval results by visual features directly, while the rest are the multi-view embedding results. It is shown that more modalities and non-linear projections yield a discriminant\nlatent visual feature, which improves the retrieval performance."}, {"heading": "D. Parameter sensitivity analysis of dimension d in linear and kernel cases", "text": "The number of dimension of the feature vectors in the latent space is determined by the top d eigenvectors in the projection matrix, and it is pre-defined in the former experiments. Therefore in this section, we investigate the effect by the variation of d shown in Fig. 8 and 9, ranging from {10, 20, 50, 100, 150, 200}. The performance on the Wikipedia dataset is reported with both text queries on images and image queries on texts. The results on different number of views are also recorded. In general, we obtain a better retrieval performance when d is between 50 and 150. It can be explained by the fact that the most informative eigenvectors are included within the range. Therefore, d = 50 was chosen for\nthe multi-view linear embeddings in the experiments. Except LMvPLS and KMvPLS, we find the majority of the methods are robust to the dimensionality changes in the subspace."}, {"heading": "V. CONCLUSION", "text": "In this paper, we proposed a generalized multi-view embedding method using the graph embedding framework. We\nshowed multi-view CCA, PLS and LDA can be characterized by their specific intrinsic and penalty graph matrices within the same framework. A novel discriminant analysis method named MvMDA was introduced by exploiting the distances between class centers of different views. Meanwhile, we also studied non-linear embeddings, and found implicit and explicit kernel mappings for multi-view learning. A unified scheme for learning by neural networks was developed which combined the learned representations with a linear embedding layer. We thereby formulated the expression of stochastic gradient descent for optimizing the proposed objective function.\nWe validated the formulation by conducting experiments in zero-shot visual object recognition and cross-modal image retrieval. It was shown that supervised and non-linear subspace learning outperformed the unsupervised and linear methods when large amount of images and texts were available. Moreover, the recognition or retrieval performance were consistently improved by embedding more views/modalities into the latent feature space. We also performed the traditional CBIR experiments where the multi-view embeddings can contribute to the performance gain.\nInteresting future research directions include learning from the raw data to achieve an end-to-end solution for multi-view learning. We should further reduce the computational cost for kernel methods to cope with large scale of images. In addition, learning from incomplete and unlabeled multi-view data should be studied for video analysis."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this paper, the problem of multi-view embed-<lb>ding from different visual cues and modalities is considered.<lb>We propose a unified solution for subspace learning methods<lb>using the Rayleigh quotient, which is extensible for multiple<lb>views, supervised learning, and non-linear embeddings. Numer-<lb>ous methods including Canonical Correlation Analysis, Partial<lb>Least Square regression and Linear Discriminant Analysis are<lb>studied using specific intrinsic and penalty graphs within the<lb>same framework. Non-linear extensions based on kernels and<lb>(deep) neural networks are derived, achieving better performance<lb>than the linear ones. Moreover, a novel Multi-view Modular<lb>Discriminant Analysis (MvMDA) is proposed by taking the view<lb>difference into consideration. We demonstrate the effectiveness<lb>of the proposed multi-view embedding methods on visual object<lb>recognition and cross-modal image retrieval, and obtain superior<lb>results in both applications compared to related methods.", "creator": "LaTeX with hyperref package"}}}