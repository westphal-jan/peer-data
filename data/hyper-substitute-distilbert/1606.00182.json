{"id": "1606.00182", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2016", "title": "On the Troll-Trust Model for Edge Sign Prediction in Social Networks", "abstract": "eliminating the problem theoretical edge packet dependence, we efficiently constructed simple reflection graph ( anonymous reciprocal online social network ), and define task complexity to predict the implicit labels against the edges ( er. mo., the constructive or hostile nature of the social database ). many analytical heuristics modeling task complexity actually relied on discrete blocking - trust relations, therefore only each node employing computation of outgoing and incoming positive bounds. investigators predict that these heuristics act simultaneously understood, and rigorously analyzed, as approximators to the bayes hardness classifier for a simple probabilistic version of the reciprocal labels. simply then show whatever specific maximum likelihood signal providing this model approximately corresponds to the predictions of a neural propagation algorithm run while a polynomial version of corresponding inner fairy graph. extensive experiments on a number of real - based datasets initially demonstrated this hierarchy is competitive employing state - - - the - art classifiers to terms of both algorithm flexibility and scalability. finally, we show interesting troll - trust features we also be used to execute mathematical learning algorithms which have unique reasoning about when edges are adversarially labeled.", "histories": [["v1", "Wed, 1 Jun 2016 09:16:46 GMT  (145kb,D)", "https://arxiv.org/abs/1606.00182v1", "16 pages, 1 figure, submitted to nips"], ["v2", "Thu, 2 Jun 2016 13:39:36 GMT  (145kb,D)", "http://arxiv.org/abs/1606.00182v2", "remove notice line at the bottom"], ["v3", "Fri, 17 Jun 2016 16:47:46 GMT  (144kb,D)", "http://arxiv.org/abs/1606.00182v3", "v3 minor typos"], ["v4", "Fri, 14 Oct 2016 09:39:59 GMT  (102kb,D)", "http://arxiv.org/abs/1606.00182v4", "v4: incorporate NIPS reviews and improve presentation"], ["v5", "Tue, 28 Feb 2017 21:33:41 GMT  (111kb,D)", "http://arxiv.org/abs/1606.00182v5", "v5: accepted to AISTATS 2017"]], "COMMENTS": "16 pages, 1 figure, submitted to nips", "reviews": [], "SUBJECTS": "cs.LG cs.SI", "authors": ["g\\'eraud le falher", "nicol\\`o cesa-bianchi", "claudio gentile", "fabio vitale"], "accepted": false, "id": "1606.00182"}, "pdf": {"name": "1606.00182.pdf", "metadata": {"source": "CRF", "title": "On the Troll-Trust Model for Edge Sign Prediction in Social Networks", "authors": ["G\u00e9raud Le Falher", "Nicol\u00f2 Cesa-Bianchi", "Claudio Gentile", "Fabio Vitale"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Connections in social networks are mostly driven by the homophily assumption: linked individuals tend to be similar, sharing personality traits, attitudes, or interests. However, homophily alone is clearly not sufficient to explain the variety of social links. In fact, sociologists have long studied networks, hereafter called signed social networks, where also negative relationships \u2014like dissimilarity, disapproval or distrust\u2014 are explicitly displayed. The presence of negative relationships is\nProceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS) 2017, Fort Lauderdale, Florida, USA. JMLR: W&CP volume 54. Copyright 2017 by the author(s).\nalso a feature of many technology-mediated social networks. Known examples are Ebay, where users trust or distrust agents in the network based on their personal interactions, Slashdot, where each user can tag another user as friend or foe, and Epinion, where users can rate positively or negatively not only products, but also other users. Even in social networks where connections solely represent friendships, negative links can still emerge from the analysis of online debates among users.\nWhen the social network is signed, specific challenges arise in both network analysis and learning. On the one hand, novel methods are required to tackle standard tasks (e.g., user clustering, link prediction, targeted advertising/recommendation, analysis of the spreading of diseases in epidemiological models). On the other hand, new problems such as edge sign prediction, which we consider here, naturally emerge. Edge sign prediction is the problem of classifying the positive or negative nature of the links based on the network topology. Prior knowledge of the network topology is often a realistic assumption, for in several situations the discovery of the link sign can be more costly than acquiring the topological information of the network. For instance, when two users of an online social network communicate on a public web page, we immediately detect a link. Yet, the classification of the link sign as positive or negative may require complex techniques.\nFrom the modeling and algorithmic viewpoints, because of the huge amount of available networked data, a major concern in developing learning methods for edge sign prediction is algorithmic scalability. Many successful, yet simple heuristics for edge sign prediction are based on the troll-trust features, i.e., on the fraction of outgoing negative links (trollness) and incoming positive links (trustworthiness) at each node. We study such heuristics by defining a probabilistic generative model for the signs on the directed links of a given network, and show that these heuristics can be understood and analyzed as approximators to the Bayes optimal classifier for our generative model. We also gather empirical evidence supporting our probabilistic\nar X\niv :1\n60 6.\n00 18\n2v 5\n[ cs\n.L G\n] 2\n8 Fe\nb 20\n17\nmodel by observing that a logistic model trained on trollness and trustworthiness features alone is able to learn weights that, on all datasets considered in our experiments, consistently satisfy the properties predicted by our model.\nWe then introduce suitable graph transformations defining reductions from edge sign prediction to node sign prediction problems. This opens up the possibility of using the arsenal of known algorithmic techniques developed for node classification. In particular, we show that a Label Propagation algorithm, combined with our reduction, approximates the maximum likelihood estimator of our probabilistic generative model. Experiments on real-world data show the competitiveness of our approach in terms of both prediction performance (especially in the regime when training data are scarce) and scalability.\nFinally, we point out that the notions of trollness and trustworthiness naturally define a measure of complexity, or learning bias, for the signed network that can also be used to design online (i.e., sequential) learning algorithms for the edge sign prediction problem. The learning bias encourages settings where the nodes in the network have polarized features (e.g., trollness/trustworthiness are either very high or very low). Our online analysis holds under adversarial conditions, namely, without any stochastic assumption on the assignment of signs to the network links."}, {"heading": "1.1 Related work", "text": "Interest in signed networks can be traced back to the psychological theory of structural balance [4, 12] with its weak version [10]. The advent of online signed social networks has enabled a more thorough and quantitative understanding of that phenomenon. Among the several approaches related to our work, some extend the spectral properties of a graph to the signed case in order to find good embeddings for classification [18, 33]. However, the use of the adjacency matrix usually requires a quadratic running time in the number of nodes, which makes those methods hardly scalable to large graphs. Another approach is based on mining ego networks with SVM. Although this method seems to deliver good results [23], the running time makes it often impractical for large real-world datasets. An alternative approach, based on local features only and proposed in [19], relies on the so-called status theory for directed graphs [11]. Some works in active learning, using a more sophisticated bias based on the correlation clustering (CC) index [6, 5], provide strong theoretical guarantees. However, the bias used there is rather strong, since it assumes the existence of a 2-clustering of the nodes with a small CC index.\nWhereas our focus will be on binary prediction, re-\nsearchers have also considered a weighted version of the problem, where edges measure the amount of trust or distrust between two users (e.g., [11, 28, 25]). Other works have also considered versions of the problem where side information related to the network is available to the learning system. For instance, [24] uses the product purchased on Epinion in conjunction with a neural network, [8] identifies trolls by analysing the textual content of their post, and [32] uses SVM to perform transfer learning from one network to another. While many of these approaches have interesting performances, they often require extra information which is not always available (or reliable) and, in addition, may face severe scaling issues. The recent survey [29] contains pointers to many papers on edge sign prediction for signed networks, especially in the Data Mining area. Additional references, more closely related to our work, will be mentioned at the end of Section 4.1."}, {"heading": "2 Notation and Preliminaries", "text": "In what follows, we let G = (V,E) be a directed graph, whose edges (i, j) \u2208 E carry a binary label yi,j \u2208 {\u22121,+1}. The edge labeling will sometimes be collectively denoted by the |V | \u00d7 |V | matrix Y = [Yi,j ], where Yi,j = yi,j if (i, j) \u2208 E, and Yi,j = 0, otherwise. The corresponding edge-labeled graph will be denoted by G(Y ) = (V,E(Y )). We use Ein(i) and Eout(i) to denote, respectively, the set of edges incoming to and outgoing from node i \u2208 V , with din(i) =\n\u2223\u2223Ein(i)\u2223\u2223 and dout(i) =\n\u2223\u2223Eout(i)\u2223\u2223 being the in-degree and the out-degree of i. Moreover, d+in(i) is the number of edges (k, i) \u2208 Ein(i) such that yk,i = +1. We define d\u2212in(i), d + out(i), and d \u2212 out(i) similarly, so that, for instance, d\u2212out(i)/dout(i) is the fraction of outgoing edges from node i whose label in G(Y ) is \u22121. We call tr(i) = d\u2212out(i)/dout(i) the trollness of node i, and un(i) = d\u2212in(i)/din(i) the untrustworthiness of node i. Finally, we also use the notation Nin(i) and Nout(i) to represent, respectively, the in-neighborhood and the out-neighborhood of node i \u2208 V .\nGiven the directed graph G = (V,E), we define two edge-to-node reductions transforming the original graph G into other graphs. As we see later, these reductions are useful in turning the edge sign prediction problem into a node sign prediction problem (often called node classification problem), for which many algorithms are indeed available \u2014see, e.g., [3, 34, 13, 14, 7]. Although any node classification method could in principle be used, the reductions we describe next are essentially aimed at preparing the ground for quadratic energyminimization approaches computed through a Label Propagation algorithm (e.g., [34, 2]).\nThe first reduction, called G\u2192 G\u2032, builds an undirected graph G\u2032 = (V \u2032, E\u2032) as follows. Each node i \u2208 V has\ntwo copies in V \u2032, call them iin and iout. Each directed edge (i, j) in E is associated with one node, call it ei,j , in V \u2032, along with the two undirected edges (iout, ei,j) and (ei,j , jin). Hence |V \u2032| = 2|V | + |E| and |E\u2032| = 2|E|. Moreover, if G = G(Y ) is edge labeled, then this labeling transfers to the subset of nodes ei,j \u2208 V \u2032, so that G\u2032 is a graph G\u2032(Y ) = (V \u2032(Y ), E\u2032) with partiallylabeled nodes. The second reduction, called G\u2192 G\u2032\u2032, builds an undirected and weighted graph G\u2032\u2032 = (V \u2032\u2032, E\u2032\u2032). Specifically, we have V \u2032\u2032 \u2261 V \u2032 and E\u2032\u2032 \u2283 E\u2032, where the set E\u2032\u2032 also includes edges (iout, jin) for all i and j such that (i, j) \u2208 E. The edges in E\u2032 have weight 2, whereas the edges in E\u2032\u2032 \\E\u2032 have weight \u22121. Finally, as in the G\u2192 G\u2032 reduction, if G = G(Y ) is edge labeled, then this labeling transfers to the subset of nodes ei,j \u2208 V \u2032\u2032. Graph G\u2032, which will not be used in this paper, is an intermediate structure between G and G\u2032\u2032 and provides a conceptual link to the standard cutsize measure in node sign classification. Figure 1 illustrates the two reductions.\nThese reductions are meaningful only if they are able to approximately preserve label regularity when moving from edges to nodes. That is, if the edge sign prediction problem is easy for a given G(Y ) = (V,E(Y )), then the corresponding node sign prediction problems on G\u2032(Y ) = (V \u2032(Y ), E\u2032) and G\u2032\u2032(Y ) = (V \u2032\u2032(Y ), E) are also easy, and vice versa. While we could make this argument more quantitative, here we simply observe that if each node in G tends to be either troll or trustworthy, then few labels from the incoming and outgoing edges of each such node are sufficient to predict the labels on the remaining edges in G, and this translates to a small cutsize1 of G\u2032(Y ) over the nodes corresponding to the edges in G (the colored squares in Figure 1 (b)). Again, we would like to point out that these reductions serve two purposes: First, they allow us to use the many algorithms designed for the better studied problem of node sign prediction. Second, the reduction G\u2192 G\u2032\u2032 with the specific choice of edge weights is designed to make the Label Propagation solution approximate the maximum likelihood estimator associated with our generative model (see Section 4.1).Note also that efficient Label Propagation implementations exist that can leverage the sparsity of G\u2032\u2032.\nWe consider two learning settings associated with the problem of edge sign prediction: a batch setting and an online setting. In the batch setting, we assume that a training set of edges E0 has been drawn uniformly at random without replacement from E, we observe the labels in E0, and we are interested in predicting the sign of the remaining edges E \\ E0 by making as\n1 Recall that the cutsize of an undirected node-labeled graph G\u2032(Y ) is the number of edges in G\u2032 connecting nodes having mismatching labels.\nfew prediction mistakes as possible. The specific batch setting we study here assumes that labels are produced by a generative model which we describe in the next section, and our label regularity measure is a quadratic function (denoted by \u03a82G\u2032\u2032(Y ) \u2014see Section 6 for a definition), related to this model. \u03a82G\u2032\u2032(Y ) is small just when all nodes in G tend to be either troll or trustworthy.\nOn the other hand, the online setting we consider is the standard mistake bound model of online learning [20] where all edge labels are assumed to be generated by an adversary and sequentially presented to the learner according to an arbitrary permutation. For an online learning algorithm A, we are interested in measuring the total number of mistakes MA(Y ) the algorithm makes over G(Y ) when the worst possible presentation order of the edge labels in Y is selected by the adversary. Also in the online setting our label regularity measure, denoted here by \u03a8G(Y ), is small when nodes in G tend to be either troll or trustworthy. Formally, for fixed G and Y , let \u03a8in(j, Y ) = min { d\u2212in(j), d + in(j) } and\n\u03a8out(i, Y ) = min { d\u2212out(i), d + out(i) } . Let also \u03a8in(Y ) =\u2211\nj\u2208V \u03a8in(j, Y ) and \u03a8out(Y ) = \u2211 i\u2208V \u03a8out(i, Y ).\nThen we define \u03a8G(Y ) = min { \u03a8in(Y ),\u03a8out(Y ) }\n. The two measures \u03a82G\u2032\u2032(Y ) and \u03a8G(Y ) are conceptually related. Indeed, their value on real data is quite similar(see Table 2 in Section 6)."}, {"heading": "3 Generative Model for Edge Labels", "text": "We now define the stochastic generative model for edge labels we use in the batch learning setting. Given the graph G = (V,E), let the label yi,j \u2208 {\u22121,+1} of directed edge (i, j) \u2208 E be generated as follows. Each node i \u2208 V is endowed with two latent parameters pi, qi \u2208 [0, 1], which we assume to be generated, for each node i, by an independent draw from a fixed but unknown joint prior distribution \u00b5(p, q) over [0, 1]2. Each label yi,j \u2208 {\u22121,+1} is then generated by an independent draw from the mixture of pi and qj ,\nP ( yi,j = 1 ) = pi+qj 2 . The basic intuition is that the nature yi,j of a relationship i\u2192 j is stochastically determined by a mixture between how much node i tends to like other people (pi) and how much node j tends to be liked by other people (qj). In a certain sense, 1\u2212tr(i) is the empirical counterpart to pi, and 1\u2212 un(j) is the empirical counterpart to qj .\n2 Notice that the Bayes optimal prediction for yi,j is y \u2217(i, j) = sgn ( \u03b7(i, j)\u2212 12 ) ,\nwhere \u03b7(i, j) = P ( yi,j = 1 ) . Moreover, the probability of drawing at random a +1-labeled edge from Eout(i) 2 One might view our model as reminiscent of standard models for link generation in social network analysis, like the classical p1 model from [15]. Yet, the similarity falls short, for all these models aim at representing the likelihood of the network topology, rather than the probability of edge signs, once the topology is given.\nand the probability of drawing at random a +1-labeled edge from Ein(j) are respectively equal to\n1 2\n( pi+ 1\ndout(i) \u2211 j\u2208Nout(i) qj\n) and 1\n2\n( qj+ 1\ndin(j) \u2211 i\u2208Nin(j) pi\n) .\n(1)"}, {"heading": "4 Algorithms in the Batch Setting", "text": "Given G(Y ) = (V,E(Y )), we have at our disposal a training set E0 of labeled edges from E(Y ), our goal being that of building a predictive model for the labels of the remaining edges.\nOur first algorithm is an approximation to the Bayes optimal predictor y\u2217(i, j). Let us denote by t\u0302r(i) and u\u0302n(i) the trollness and the untrustworthiness of node i when both are computed on the subgraph induced by the training edges. We now design and analyze an edge classifier of the form\nsgn (( 1\u2212 t\u0302r(i) ) + ( 1\u2212 u\u0302n(j) ) \u2212 12 \u2212 \u03c4 ) , (2)\nwhere \u03c4 \u2265 0 is the only parameter to be trained. Despite its simplicity, this classifier works reasonably well in practice, as demonstrated by our experiments (see Section 6). Moreover, unlike previous edge sign prediction methods for directed graphs, our classifier comes with a rigorous theoretical motivation, since it approximates the Bayes optimal classifier y\u2217(i, j) with respect to the generative model defined in Section 3. It is important to point out that when we use 1\u2212 t\u0302r(i) and 1\u2212 u\u0302n(j) to estimate pi and qj , an additive bias shows up due to (1). This motivates the need of a threshold parameter \u03c4 to cancel this bias. Yet, the presence of a prior distribution \u00b5(p, q) ensures that this bias is the same for all edges (i, j) \u2208 E.\nOur algorithm works under the assumption that for given parameters Q (a positive integer) and \u03b1 \u2208 (0, 1) there exists a set3 EL \u2286 E of size 2Q\u03b1 where each vertex\n3 EL is needed to find an estimate \u03c4\u0302 of \u03c4 in (2) \u2014see\ni \u2208 V appearing as an endpoint of some edge in EL occurs at most once as origin \u2014i.e., (i, j)\u2014 and at most once as destination \u2014i.e., (j, i). Moreover, we assume E0 has been drawn from E at random without replacement, with m = |E0| = \u03b1 |E|. The algorithm performs the following steps:\n1. For each j \u2208 V , let u\u0302n(j) = d\u0302\u2212in(j)/d\u0302in(j), i.e., the fraction of negative edges found in Ein(j) \u2229 E0. 2. For each i \u2208 V , let t\u0302r(i) = d\u0302\u2212out(i)/d\u0302out(i), i.e., the fraction of negative edges found in Eout(i) \u2229 E0. 3. Let \u03c4\u0302 be the fraction of positive edges in EL \u2229 E0. 4. Any remaining edge (i, j) \u2208 E \\E0 is predicted as\ny\u0302(i, j) = sgn (( 1\u2212 t\u0302r(i) ) + ( 1\u2212 u\u0302n(j) ) \u2212 12 \u2212 \u03c4\u0302 ) .\nThe next result4 shows that if the graph is not too sparse, then the above algorithm can approximate the Bayes optimal predictor on nodes whose in-degree and out-degree is not too small.\nTheorem 1. Let G(Y ) = (V,E(Y )) be a directed graph with labels on the edges generated according to the model in Section 3. If the algorithm is run with parameter Q = \u2126(ln |V |), and \u03b1 \u2208 (0, 1) such that the above assumptions are satisfied, then y\u0302(i, j) = y\u2217(i, j) holds with high probability simultaneously for all test edges (i, j) \u2208 E such that dout(i), din(j) = \u2126(ln |V |), and \u03b7(i, j) = P(yi,j = 1) is bounded away from 12 .\nThe approach leading to Theorem 1 lets us derive the blc(tr, un) algorithm assessed in our experiments of Section 6, but it needs the graph to be sufficiently dense and the bias \u03c4 to be the same for all edges. In order to address these limitations, we now introduce a second method based on label propagation.\nStep 3 of the algorithm. Any undirected matching of G of size O(log |V |) can be used. In practice, however, we never computed EL, and estimated \u03c4 on the entire training set E0.\n4 All proofs are in the supplementary material."}, {"heading": "4.1 Approximation to Maximum Likelihood via Label Propagation", "text": "For simplicity, assume the joint prior distribution \u00b5(p, q) is uniform over [0, 1]2 with independent marginals, and suppose that we draw at random without replacement the training set E0 =( (i1, j1), yi1,j1), ((i2, j2), yi2,j2), . . . , ((im, jm), yim,jm ) , with m = |E0|. Then a reasonable approach to approximate y\u2217(i, j) would be to resort to a maximum likelihood estimator of the parameters {pi, qi}|V |i=1 based on E0. As showed in the supplementary material, the gradient of the log-likelihood function w.r.t. {pi, qi}|V |i=1 satisfies\n\u2202 log P ( E0 \u2223\u2223\u2223 {pi, qi}|V |i=1) \u2202p`\n(3)\n= m\u2211 k=1 I {ik = `, y`,jk = +1} p` + qjk \u2212 m\u2211 k=1 I {ik = `, y`,jk = \u22121} 2\u2212 p` \u2212 qjk ,\n\u2202 log P ( E0 \u2223\u2223\u2223 {pi, qi}|V |i=1) \u2202q`\n(4)\n= m\u2211 k=1 I {jk = `, yik,` = +1} pik + q` \u2212 m\u2211 k=1 I {jk = `, yik,` = \u22121} 2\u2212 pik \u2212 q` ,\nwhere I {\u00b7} is the indicator function of the event at argument. Unfortunately, equating (3) and (4) to zero, and solving for parameters {pi, qi}|V |i=1 gives rise to a hard set of nonlinear equations. Moreover, some such parameters may never occur in these equations, namely whenever Eout(i) or Ein(j) are not represented in E0 for some i, j \u2208 V . Our first approximation is therefore to replace the nonlinear equations resulting from (3) and (4) by the following set of linear equations5, one for each ` \u2208 V :\nm\u2211 k=1 I {ik = `, y`,jk = +1} (2\u2212 p` \u2212 qjk)\n= m\u2211 k=1 I {ik = `, y`,jk = \u22121} (p` + qjk)\nm\u2211 k=1 I {jk = `, yik,` = +1} (2\u2212 pik \u2212 q`)\n= m\u2211 k=1 I {jk = `, yik,` = \u22121} (pik + q`) .\nThe solution to these equations are precisely the points where the gradient w.r.t. (p, q) = {pi, qi}|V |i=1 of the quadratic function\nfE0(p, q) = \u2211\n(i,j)\u2208E0\n( 1 + yi,j\n2 \u2212 pi + qj 2 )2 vanishes. We follow a label propagation approach by adding to fE0 the corresponding test set function fE\\E0 ,\n5Details are provided in the supplementary material.\nand treat the sum of the two as the function to be minimized during training w.r.t. both (p, q) and all yi,j \u2208 [\u22121,+1] for (i, j) \u2208 E \\ E0, i.e.,\nmin (p,q),yi,j\u2208[\u22121,+1], (i,j)\u2208E\\E0\n( fE0(p, q) + fE\\E0(p, q) ) .\n(5) Binary \u00b11 predictions on the test set E \\ E0 are then obtained by thresholding the obtained values yi,j at 0.\nWe now proceed to solve (5) via label propagation [34] on the graph G\u2032\u2032 obtained through the G \u2192 G\u2032\u2032 reduction of Section 2.However, because of the presence of negative edge weights in G\u2032\u2032, we first have to symmetrize6 variables pi, qi, yi,j so as they all lie in the interval [\u22121,+1]. After this step, one can see that, once we get back to the original variables, label propagation computes the harmonic solution minimizing the function\nf\u0302 ( p, q, yi,j(i,j)\u2208E\\E0 ) = fE0(p, q) + fE\\E0(p, q)\n+ 1\n2 \u2211 i\u2208V ( dout(i) ( pi \u2212 1 2 )2 +din(i) ( qi \u2212 1 2 )2) .\nThe function f\u0302 is thus a regularized version of the target function fE0 + fE\\E0 in (5), where the regularization term tries to enforce the extra constraint that whenever a node i has a high out-degree then the corresponding pi should be close to 1/2. Thus, on any edge (i, j) departing from i, the Bayes optimal predictor y\u2217(i, j) = sgn(pi + qj \u2212 1) will mainly depend on qj being larger or smaller than 12 (assuming j has small in-degree). Similarly, if i has a high in-degree, then the corresponding qi should be close to 1/2 implying that on any edge (j, i) arriving at i the Bayes optimal predictor y\u2217(j, i) will mainly depend on pj (assuming j has small out-degree). Put differently, a node having a huge out-neighborhood makes each outgoing edge \u201ccount less\u201d than a node having only a small number of outgoing edges, and similarly for in-neighborhoods. The label propagation algorithm operating on G\u2032\u2032 does so (see again Figure 1 (c)) by iteratively updating as follows:\npi \u2190 \u2212 \u2211 j\u2208Nout(i) qj + \u2211 j\u2208Nout(i)(1 + yi,j)\n3 dout(i) \u2200i \u2208 V qj \u2190 \u2212 \u2211 i\u2208Nin(j) pi + \u2211 i\u2208Nin(j)(1 + yi,j)\n3 din(j) \u2200j \u2208 V\nyi,j \u2190 pi + qj\n2 \u2200(i, j) \u2208 E \\ E0 .\nThe algorithm is guaranteed to converge [34] to the\nminimizer of f\u0302 . Notice that the presence of negative\n6While we note here that such linear transformation of the variables does not change the problem, we provide more details in Section 1.3 of the supplementary material.\nweights on the edges of G\u2032\u2032 does not prevent label propagation from converging. This is the algorithm we will be championing in our experiments of Section 6.\nFurther related work. The vast majority of existing edge sign prediction algorithms for directed graphs are based on the computation of local features of the graph. These features are evaluated on the subgraph induced by the training edges, and the resulting values are used to train a supervised classification algorithm (e.g., logistic regression). The most basic set of local features used to classify a given edge (i, j) are defined by d+in(j), d \u2212 in(j), d + out(i), d \u2212 out(i) computed over the training set E0, and by the embeddedness coefficient\n\u2223\u2223Eout(i) \u2229 Ein(j)\u2223\u2223. In turn, these can be used to define more complicated features, such as d+in(j)+|E\n+|Uin(j) din(j)+Uin(j) and d+out(i)+p +Uout(i) dout(i)+Uout(i) introduced\nin [27], together with their negative counterparts, where |E+| is the overall fraction of positive edges, and Uin(j), Uout(i) are, respectively, the number of test edges outgoing from i and the number of test edges incoming to j. Other types of features are derived from social status theory (e.g., [19]), and involve the so-called triads; namely, the triangles formed by (i, j) together with (i, w) and (w, j) for any w \u2208 Nout(i) \u2229Nin(j). A third group of features is based on node ranking scores. These scores are computed using a variery of methods, including Prestige [35], exponential ranking [30], PageTrust [16], Bias and Deserve [22], TrollTrust [31], and generalizations of PageRank and HITS to signed networks [26]. Examples of features using such scores are reputation and optimism [26], defined for a node\ni by \u2211 j\u2208Nin(i)\nyj,i\u03c3(j)\u2211 j\u2208Nin(i) \u03c3(j) and \u2211 j\u2208Nout(i) Yi,j\u03c3(j)\u2211 j\u2208Nout(i) \u03c3(j) , where\n\u03c3(j) is the ranking score assigned to node j. Some of these algorithms will be used as representative competitors in our experimental study of Section 6."}, {"heading": "5 Algorithms in the Online Setting", "text": "For the online scenario, we have the following result.\nTheorem 2. There exists a randomized online prediction algorithm A whose expected number of mistakes satisfies EMA(Y ) = \u03a8G(Y ) + O (\u221a |V |\u03a8G(Y ) + |V | ) on any edge-labeled graph G(Y ) = (V,E(Y )).\nThe algorithm used in Theorem 2 is a combination of randomized Weighted Majority instances. Details are reported in the supplementary material. We complement the above result by providing a mistake lower bound. Like Theorem 2, the following result holds for all graphs, and for all label irregularity levels \u03a8G(Y ).\nTheorem 3. Given any edge-labeled graph G(Y ) = (V,E(Y )) and any integer K \u2264 \u230a |E|\n2\n\u230b , a randomized\nlabeling Y \u2208 {\u22121,+1}|E| exists such that \u03a8G(Y ) \u2264 K, and the expected number of mistakes that any online\nalgorithm A can be forced to make satisfies EMA(Y ) \u2265 K 2 . Moreover, as K |E| \u2192 0 then EMA(Y ) = K."}, {"heading": "6 Experimental Analysis", "text": "We now evaluate our edge sign classification methods on representative real-world datasets of varying density and label regularity, showing that our methods compete well against existing approaches in terms of both predictive and computational performance. We are especially interested in small training set regimes, and have restricted our comparison to the batch learning scenario since all competing methods we are aware of have been developed in that setting only.\nDatasets. We considered five real-world classification datasets. The first three are directed signed social networks widely used as benchmarks for this task (e.g.,[19, 26, 31]): In Wikipedia, there is an edge from user i to user j if j applies for an admin position and i votes for or against that promotion. In Slashdot, a news sharing and commenting website, member i can tag other members j as friends or foes. Finally, in Epinion, an online shopping website, user j reviews products and, based on these reviews, another user i can display whether he considers j to be reliable or not. In addition to these three datasets, we considered two other signed social networks where the signs are inferred automatically. In Wik. Edits [21], an edge from Wikipedia user i to user j indicates whether they edited the same article in a constructive manner or not.7 Finally, in the Citations [17] network, an author i cites another author j by either endorsing or criticizing j\u2019s work. The edge sign is derived by classifying the citation sentiment with a simple, yet powerful, keyword-based technique using a list of positive and negative words. See [17] for more details.8\nTable 1 summarizes statistics for these datasets. We note that most edge labels are positive. Hence, test set accuracy is not an appropriate measure of prediction performance. We instead evaluated our performance using the so-called Matthews Correlation Coefficient (MCC) (e.g., [1]), defined as\nMCC = tp\u00d7 tn\u2212 fp\u00d7 fn\u221a\n(tp+ fp)(tp+ fn)(tn+ fp)(tn+ fn) .\nMCC combines all the four quantities found in a binary confusion matrix (true positive, true negative, false positive and false negative) into a single metric which ranges from \u22121 (when all predictions are incorrect) to +1 (when all predictions are correct).\n7 This is the KONECT version of the \u201cWikisigned\u201d dataset, from which we removed self-loops.\n8 We again removed self-loops and merged multi-edges which are all of the same sign.\nAlthough the semantics of the edge signs is not the same across these networks, we can see from Table 1 that our generative model essentially fits all of them. Specifically, the last two columns of the table report the rate of label (ir)regularity, as measured by \u03a82G\u2032\u2032(Y )/|E| (second-last column) and \u03a8G(Y )/|E| (last column), where\n\u03a82G\u2032\u2032(Y ) = min (p,q)\n( fE0(p, q) + fE\\E0(p, q) ) ,\nfE0 and fE\\E0 being the quadratic criterions of Section 4.1, viewed as functions of both (p, q), and yi,j , and \u03a8G(Y ) is the label regularity measure adopted in the online setting, as defined in Section 2. It is reasonable to expect that higher label irregularity corresponds to lower prediction performance. This trend is in fact confirmed by our experimental findings: whereas Epinion tends to be easy, Citations tends to be hard, and this holds for all algorithms we tested, even if they do not explicitly comply with our inductive bias principles. Moreover, \u03a82G\u2032\u2032(Y )/|E| tends to be proportional to \u03a8G(Y )/|E| across datasets, hence confirming the anticipated connection between the two regularity measures.\nAlgorithms and parameter tuning. We compared the following algorithms:\n1. The label propagation algorithm of Section 4.1 (referred to as L. Prop.). The actual binarizing threshold was set by cross-validation on the training set.\n2. The algorithm analyzed at the beginning of Section 4, which we call blc(tr, un) (Bayes Learning Classifier based on trollness and untrustworthiness). After computing t\u0302r(i) and u\u0302n(i) on training set E0 for all i \u2208 V (or setting those values to 12 in case there is no outgoing or incoming edges for some node), we use Eq. (2) and estimate \u03c4 on E0. 3. A logistic regression model where each edge (i, j) is associated with the features [1\u2212 t\u0302r(i), 1\u2212 u\u0302n(j)] computed again on E0 (we call this method LogReg). Best binary thresholding is again computed on E0. Experimenting with this logistic model serves to support the claim we made in the introduction that our generative model in Section 3 is a good fit for the data.\n4. The solution obtained by directly solving the unregularized problem (5) through a fast constrained minimization algorithm (referred to as Unreg.). Again, the actual binarizing threshold was set by cross-validation\non the training set.9\n5. The matrix completion method from [9] based on LowRank matrix factorization. Since the authors showed their method to be robust to the choice of the rank parameter k, we picked k = 7 in our experiments.\n6. A logistic regression model built on 16 Triads features derived from status theory [19].\n7. The PageRank-inspired algorithm from [31], where a recursive notion of trollness is computed by solving a suitable set of nonlinear equations through an iterative method, and then used to assign ranking scores to nodes, from which (un)trustworthiness features are finally extracted for each edge. We call this method RankNodes. As for hyperparameter tuning (\u03b2 and \u03bb1 in [31]), we closely followed the authors\u2019 suggestion of doing cross validation.\n8. The last competitor is the logistic regression model whose features have been build according to [27]. We call this method Bayesian.\nThe above methods can be roughly divided into local and global methods. A local method hinges on building local predictive features, based on neighborhoods: blc(tr, un), LogReg, 16 Triads, and Bayesian essentially fall into this category. The remaining methods are global in that their features are designed to depend on global properties of the graph topology.\nResults. Our main results are summarized in Table 2, reporting MCC test set performance after training on sets of varying size (from 5% to 25%). Results have been averaged over 12 repetitions. Because scalability is a major concern on sizeable datasets, we also give an idea of relative training times (in milliseconds) by reporting the time it took to train a single run of each algorithm on a training set of size10 15% of |E|, and then predict on the test set. Though our experiments are not conclusive, some trends can be spotted:\n1. Global methods tend to outperform local methods in terms of prediction performance, but are also signifi-\n9 We have also tried to minimize (5) by removing the [\u22121,+1] constraints, but got similar MCC results as the ones we report for Unreg.\n10 Comparison of training time performances is fair since all algorithms have been carefully implemented using the same stack of Python libraries, and run on the same machine (16 Xeon cores and 192Gb Ram).\ncantly (or even much) slower (running times can differ by as much as three orders of magnitude). This is not surprising, and is in line with previous experimental findings (e.g., [26, 31]). Bayesian looks like an exception to this rule, but its running time is indeed in the same ballpark as global methods.\n2. L. Prop. always ranks first or at least second in this comparison when MCC is considered. On top of it, L. Prop. is fastest among the global methods (one or even two orders of magnitude faster), thereby showing the benefit of our approach to edge sign prediction.\n3. The regularized solution computed by L. Prop. is always better than the unregularized one computed by Unreg. in terms of both MCC and running time.\n4. As claimed in the introduction, our Bayes approximator blc(tr, un) closely mirrors in performance the more involved LogReg model. In fact, supporting our generative model of Section 3, the logistic regression weights for features 1 \u2212 t\u0302r(i) and 1 \u2212 u\u0302n(j) are almost equal (see Table 2 in the supplementary material), thereby suggesting that predictor (2), derived from the theoretical results at the beginning of Section 4, is also the best logistic model based on trollness and untrustworthiness."}, {"heading": "7 Conclusions and Ongoing Research", "text": "We have studied the edge sign prediction problem in directed graphs in both batch and online learning set-\ntings. In both cases, the underlying modeling assumption hinges on the trollness and (un)trustworthiness predictive features. We have introduced a simple generative model for the edge labels to craft this problem as a node sign prediction problem to be efficiently tackled by standard Label Propagation algorithms. Furthermore, we have studied the problem in an (adversarial) online setting providing upper and (almost matching) lower bounds on the expected number of prediction mistakes.\nFinally, we validated our theoretical results by experimentally assessing our methods on five real-world datasets in the small training set regime. Two interesting conclusions from our experiments are: i. Our generative model is robust, for it produces Bayes optimal predictors which tend to be empirically best also within the larger set of models that includes all logistic regressors based on trollness and trustworthiness alone; ii. our methods are in practice either strictly better than their competitors in terms of prediction quality or, when they are not, they are faster. We are currently engaged in extending our approach so as to incorporate further predictive features (e.g., side information, when available)."}, {"heading": "Acknowledgements", "text": "We would like to thank the reviewers for their comments which led improving the presentation of this paper."}, {"heading": "A Proofs from Section 4", "text": "A.1 Proof of Theorem 1\nThe following ancillary results will be useful.\nLemma 1 (Hoeffding\u2019s inequality for sampling without replacement). Let X = {x1, . . . , xN} be a finite subset of [0, 1] and let\n\u00b5 = 1\nN N\u2211 i=1 xi .\nIf X1, . . . , Xn is a random sample drawn at random from X without replacement, then, for every \u03b5 > 0,\nP (\u2223\u2223\u2223\u2223\u2223 1n n\u2211 t=1 Xt \u2212 \u00b5 \u2223\u2223\u2223\u2223\u2223 \u2265 \u03b5 ) \u2264 2e\u22122n\u03b5 2 .\nLemma 2. Let N1, . . . ,Nn be subsets of a finite set E. Let E0 \u2286 E be sampled uniformly at random without replacement from E, with |E0| = m. Then, for \u03b4 \u2208 (0, 1), Q > 0, and \u03b8 \u2265 2\u00d7max { Q, 4 ln n\u03b4 } , we have\nP ( \u2203i : |Ni| \u2265 \u03b8, |Ni \u2229 E0| < Q ) \u2264 \u03b4\nprovided |E| \u2265 m \u2265 2|E|\u03b8 \u00d7max { Q, 4 ln n\u03b4 } .\nProof of Lemma 2. Set for brevity pi = |Ni|/|E|. Then, due to the sampling without replacement, each random variable |Ni \u2229E0| is the sum of m dependent Bernoulli random variables Xi,1, . . . , Xi,m such that P(Xi,t = 1) = pi, for t = 1, . . . ,m. Let i be such that |Ni| \u2265 \u03b8. Then the condition m \u2265 2|E|Q\u03b8 implies\nQ \u2264 m\u03b8 2|E| \u2264 mpi 2\n= E [ |Ni \u2229 E0| ] 2 .\nSince the variables Xi,j are negatively associated, we may apply a (multiplicative) Chernoff bound [3, Section 3.1]. This gives\nP ( |Ni \u2229 E0| < Q ) \u2264 e\u2212 mpi 8 \u2264 e\u2212 m\u03b8 8|E|\nso that P ( \u2203i : |Ni| \u2265 \u03b8, |Ni \u2229 E0| < Q ) \u2264 n e\u2212 m\u03b8 8|E| , which is in turn upper bounded by \u03b4 whenever m \u2265 8|E| \u03b8 ln n \u03b4 .\nLet now E\u03b8 = {(i, j) \u2208 E : din(j) \u2265 \u03b8, dout(i) \u2265 \u03b8} \\ E0, where E0 \u2286 E is the set of edges sampled by the learning algorithm of Section 4. Then Theorem 1 in the main paper is an immediate consequence of the following lemma.\nLemma 3. Let G(Y ) = (V,E(Y )) be a directed graph with labels on the edges generated according to the model in Section 3. For all 0 < \u03b1, \u03b4 < 1 and 0 < \u03b5 < 116 , if the learning algorithm of Section 4 is run with parameters Q = 12\u03b52 ln 4|V | \u03b4 and \u03b1, then with probability at least 1 \u2212 11\u03b4 the predictions y\u0302(i, j) satisfy y\u0302(i, j) = y\u2217(i, j) for all (i, j) \u2208 E\u03b8 such that\n\u2223\u2223\u03b7(i, j)\u2212 12 \u2223\u2223 > 8\u03b5. Proof of Lemma 3. We apply Lemma 2 with \u03b8 = 2Q\u03b1 \u2265 2 \u00d7max { Q, 4 ln 2|V |+1\u03b4 } to the 2|V |+ 1 subsets of E consisting of EL and Ein(i), Eout(i), for i \u2208 V . We have that, with probability at least 1\u2212 \u03b4, at least Q edges of EL are sampled, at least Q edges of Ein(i) are sampled for each i such that Nin(i) \u2265 \u03b8, and at least Q edges of Eout(j) are sampled for each j such that Nout(j) \u2265 \u03b8. For all (i, j) \u2208 E\u03b8 let\npj = 1\ndin(j) \u2211 i\u2208Nin(j) pi and qi = 1 dout(i) \u2211 j\u2208Nout(i) qj\nand set for brevity \u03b4\u0302in(j) = 1 \u2212 u\u0302n(j) and \u03b4\u0302out(i) = 1 \u2212 t\u0302r(i). We now prove that \u03b4\u0302in(j) and \u03b4\u0302out(i) are concentrated around their expectations for all (i, j) \u2208 E\u03b8. Consider \u03b4\u0302out(i) (the same argument works for \u03b4\u0302in(j)). Let J1, . . . , JQ be the first Q draws in E0 \u2229 Nout(i) and define\n\u00b5\u0302p(i) = 1\nQ Q\u2211 t=1 pi + qJt 2 .\nApplying Lemma 1 to the set { pi+qj 2 : j \u2208 Nout(i) } ,\nand using our choice of Q, we get that \u2223\u2223\u00b5\u0302p(i)\u2212\u00b5p(i)\u2223\u2223 \u2264 \u03b5 holds with probability at least 1\u2212 \u03b4/(2|V |), where\n\u00b5p(i) = 1\ndout(i) \u2211 j\u2208Nout(i) pi + qj 2 = pi + qi 2 .\nNow consider the random variables Zt = I {yi,Jt = 1}, for t = 1, . . . , Q. Conditioned on J1, . . . , JQ, these are independent Bernoulli random variables with E[Zt | Jt] = pi+qJt 2 . Hence, applying a standard Hoeffding bound for independent variables and using our choice of Q, we get that\u2223\u2223\u2223\u2223\u2223 1Q Q\u2211 t=1 Zt \u2212 \u00b5\u0302p(i) \u2223\u2223\u2223\u2223\u2223 \u2264 \u03b5\nwith probability at least 1\u2212 \u03b4/(2|V |) for every realization of J1, . . . , JQ. Since \u03b4\u0302out(i) = (Z1 + \u00b7 \u00b7 \u00b7+ ZQ)/Q, we get that \u2223\u2223\u03b4\u0302out(i) \u2212 \u00b5p(i)\u2223\u2223 \u2264 2\u03b5 with probability at least 1 \u2212 2\u03b4/(2|V |). Applying the same argument to \u03b4\u0302in(j), and the union bound\n11 on the set{ \u03b4\u0302in(j), \u03b4\u0302out(i) : (i, j) \u2208 E\u03b8 } , we get that\u2223\u2223\u2223\u2223\u03b4\u0302out(i) + \u03b4\u0302in(j)\u2212 pi + qj2 \u2212 pj + qi2\n\u2223\u2223\u2223\u2223 \u2264 4\u03b5 (6) simultaneously holds for all (i, j) \u2208 E\u03b8 with probability at least 1\u2212 4\u03b4. Now notice that pj is a sample mean of Q i.i.d. [0, 1]-valued random variables drawn from\nthe prior marginal \u222b 1 0 \u00b5 ( \u00b7, q ) dq with expectation \u00b5p. Similarly, qi is a sample mean of Q i.i.d. [0, 1]-valued random variables independently drawn from the prior\nmarginal \u222b 1 0 \u00b5 ( p, \u00b7 ) dp with expectation \u00b5q. By applying Hoeffding bound for independent variables, together with the union bound to the set of pairs of random variables whose sample means are pj and qi for each (i, j) \u2208 E\u03b8 (there are at most 2|V | of them) we obtain that \u2223\u2223pj \u2212 \u00b5p\u2223\u2223 \u2264 \u03b5 and \u2223\u2223qi \u2212 \u00b5q\u2223\u2223 \u2264 \u03b5 hold simultaneously for all (i, j) \u2208 E\u03b8 with probability at least 1\u2212 2\u03b4. Combining with (6) we obtain that\u2223\u2223\u2223\u2223\u03b4\u0302out(i) + \u03b4\u0302in(j)\u2212 pi + qj2 \u2212 \u00b5p + \u00b5q2\n\u2223\u2223\u2223\u2223 \u2264 5\u03b5 (7) simultaneously holds for each (i, j) \u2208 E\u03b8 with probability at least 1 \u2212 6\u03b4. Next, let E\u2032L be the set of the first Q edges drawn in EL \u2229 E0. Then\nE [ \u03c4\u0302 ] = 1\nQ \u2211 (i,j)\u2208E\u2032L P ( yi,j = 1 ) = 1 Q \u2211 (i,j)\u2208E\u2032L pi + qj 2 ,\nwhere the expectation is w.r.t. the independent draws of the labels yi,j for (i, j) \u2208 E\u2032L. Hence, by applying again Hoeffding bound (this time without the union bound) to the Q = 12\u03b52 ln 2 \u03b4 independent Bernoulli random vari-\nables I {yi,j = 1}, (i, j) \u2208 E\u2032L, the event \u2223\u2223\u03c4\u0302 \u2212E[\u03c4\u0302]\u2223\u2223 \u2264 \u03b5 holds with probability at least 1\u2212 \u03b4. Now, introduce the function\nF (p, q) = E [ \u03c4\u0302 ] = 1\nQ \u2211 (i,j)\u2208E\u2032L pi + qj 2 .\nFor any realization q0 of q, the function F1(p) = F (p, q0) is a sample mean of Q = 1 2\u03b52 ln 4|V | \u03b4 i.i.d. [0, 1]- valued random variables {pi : (i, j) \u2208 E\u2032L} (recall that 11 The sample spaces for the ingoing and outgoing edges of the vertices occurring as endpoints in E\u03b8 overlap. Hence, in order to prove a uniform concentration result, we need to apply the union bound over the random variables defined over these sample spaces, which motivates the presence of the factor ln(2|V |) in the definition of Q.\nif i \u2208 V is the origin of an edge (i, j) \u2208 E\u2032L, then it is not the origin of any other edge (i, j\u2032) \u2208 E\u2032L). Using again the standard Hoeffding bound, we obtain that\u2223\u2223F (p, q)\u2212 Ep[F (p, q)]\u2223\u2223 \u2264 \u03b5 holds with probability at least 1\u2212\u03b4 for each q \u2208 [0, 1]|V |. With a similar argument, we obtain that\u2223\u2223Ep[F (p, q)]\u2212 Ep,q[F (p, q)]\u2223\u2223 \u2264 \u03b5 also holds with probability at least 1\u2212 \u03b4. Since\nEp,q [ F (p, q) ] = \u00b5p + \u00b5q\n2\nwe obtain that \u2223\u2223\u2223\u03c4\u0302 \u2212 \u00b5p + \u00b5q 2 \u2223\u2223\u2223 \u2264 3\u03b5 (8) with probability at least 1\u22123\u03b4. Combining (7) with (8) we obtain\u2223\u2223\u2223\u2223\u03b4\u0302out(i) + \u03b4\u0302in(j)\u2212 \u03c4\u0302 \u2212 p(i) + q(j)2\n\u2223\u2223\u2223\u2223 \u2264 8\u03b5 simultaneously holds for each (i, j) \u2208 E\u03b8 with probability at least 1\u2212 10\u03b4. Putting together concludes the proof.\nA.2 Derivation of the maximum likelihood equations Recall that the training set E0 ={( it, jt), yit,jt ) : t = 1, . . . ,m } is drawn uniformly at random from E without replacement. We can write\nP ( E0 \u2223\u2223\u2223 {pi, qi}|V |i=1) =\n1(|E| m ) m! m\u220f k=1 ( pik + qjk 2 )I{yik,jk=+1}\n\u00d7 m\u220f k=1 ( 1\u2212 pik + qjk 2 )I{yik,jk=\u22121}\n= 1(|E|\nm\n) m! |V |\u220f `=1 ( m\u220f k=1 ( p` + qjk 2 )I{ik=`, y`,jk=+1}\n\u00d7 m\u220f k=1 ( 1\u2212 p` + qjk 2 )I{ik=`, y`,jk=\u22121})\nso that logP ( E0 \u2223\u2223\u2223 {pi, qi}|V |i=1) is proportional to |V |\u2211 `=1 m\u2211 k=1 I {ik = `, y`,jk = +1} log ( p` + qjk 2 )\n+ |V |\u2211 `=1 m\u2211 k=1 I {ik = `, y`,jk = +1} log ( 1\u2212 p` + qjk 2 )\nand \u2202 logP ( E0 \u2223\u2223\u2223 {pi, qi}|V |i=1) \u2202p` = m\u2211 k=1 I {ik = `, y`,jk = +1} p` + qjk\n\u2212 m\u2211 k=1 I {ik = `, y`,jk = \u22121} 2\u2212 p` \u2212 qjk .\nBy a similar argument, P ( E0 \u2223\u2223\u2223 {pi, qi}|V |i=1) =\n1(|E| m ) m! |V |\u220f `=1 ( m\u220f k=1 ( pik + q` 2 )I{jk=`, yik,`=+1}\n\u00d7 m\u220f k=1 ( 1\u2212 pik + q` 2 )I{jk=`, yik,`=\u22121})\nso that \u2202 logP ( E0 \u2223\u2223\u2223 {pi, qi}|V |i=1) \u2202q` = m\u2211 k=1 I {jk = `, yik,` = +1} pik + q`\n\u2212 m\u2211 k=1 I {jk = `, yik,` = \u22121} 2\u2212 pik \u2212 q` .\nWe then derive the approximation presented in the main paper. Namely, equating to zero the gradient of the log likelihood w.r.t p` gives m\u2211 k=1 I {ik = `, y`,jk = +1} p` + qjk \u2212 I {ik = `, y`,jk = \u22121} 2\u2212 p` \u2212 qjk = 0\nTo simplify the notation, let ak = I {ik = `, y`,jk = +1}, bk = I {ik = `, y`,jk = +1} and ck = p` + qjk , we can rewrite the previous equation as\nm\u2211 k=1 ak ck \u2212 bk 2\u2212 ck = 0\nm\u2211 k=1 ak(2\u2212 ck)\u2212 bkck ck(2\u2212 ck) = 0\nThe approximation consists in assuming that the denominator ck(2 \u2212 ck) is a constant for all k, and can therefore be disregarded. Moving bkck to the right hand side and returning to the original variables, it yields the approximate equation presented in the main paper, namely\nm\u2211 k=1 I {ik = `, y`,jk = +1} (2\u2212 p` \u2212 qjk)\n= m\u2211 k=1 I {ik = `, y`,jk = \u22121} (p` + qjk)\nA.3 Label propagation on G\u2032\u2032\nHere we provide more details on the choice of weight for the edges of G\u2032\u2032, as well as an explanation on why we temporarily use symmetrized variables lying in [\u22121, 1] (which we will denote with primes, so that for instance p\u2032i = 2pi\u22121). Since only the ratio between the negative and positive weights matters, we fix the negative weight of the edges in E\u2032\u2032 \\ E\u2032 to be \u22121 and we denote by the weight of edges in E\u2032. With these notations, Label Propagation on G\u2032\u2032 seeks the harmonic minimizer of the following expression\n1\n16 \u2211 i,j\u2208E [ (yi,j \u2212 p\u2032i) 2 + ( yi,j \u2212 q\u2032j )2 + (p\u2032i + q \u2032 j) 2 ]\nwhich can be successively rewritten as\n1\n16 \u2211 i,j\u2208E [ (yi,j + 1\u2212 2pi)2 + (yi,j + 1\u2212 2qj)2\n+ (2pi + 2qj \u2212 2)2 ]\n= 1\n8 \u2211 i,j\u2208E\n[ 2 ( yi,j + 1\n2 \u2212 pi\n)2 + 2 ( yi,j + 1\n2 \u2212 qj\n)2\n+ 8\n( pi + qj \u2212 1\n2\n)2]\n= 1\n8 \u2211 i,j\u2208E\n[ 2 (( yi,j + 1\n2\n)2 \u2212 pi(1 + yi,j) + p2i ) +\n2\n(( yi,j + 1\n2\n)2 \u2212 qj(1 + yi,j) + q2j ) +\n8\n(( pi + qj\n2\n)2 \u2212 pi + qj\n2 +\n1\n4\n)]\n= 1\n8 \u2211 i,j\u2208E 4\n( ( yi,j + 1\n2\n)2 \u2212 2 ( yi,j + 1\n2\n)( pi + qj\n2\n)\n+ 2\n( pi + qj\n2 )2) + \u2211 i,j\u2208E [( 2 p2i \u2212 4pi + 1 ) + ( 2 q2j \u2212 4qj + 1\n)] By setting = 2, we can factor this expression into\u2211\ni,j\u2208E\n( yi,j + 1\n2 \u2212 pi + qj 2\n)2\n+ 1\n2 \u2211 i,j\u2208E\n(( pi \u2212 1\n2\n)2 + ( qj \u2212 1\n2\n)2) ."}, {"heading": "B Proofs from Section 5", "text": "Proof of Theorem 2. Let each node i \u2208 V host two instances of the randomized Weighted Majority (RWM)\nalgorithm [4] with an online tuning of their learning rate [2, 1]: one instance for predicting the sign of outgoing edges (i, j), and one instance for predicting the sign of incoming edges (j, i). Both instances simply compete against the two constant experts, predicting always +1 or always \u22121. Denote by M(i, j) the indicator function (zero-one loss) of a mistake on edge (i, j). Then the expected number of mistakes of each RWM instance satisfy [2, 1]:\u2211 j\u2208Nout(i) EM(i, j) = \u03a8out(i, Y )+O (\u221a \u03a8out(i, Y ) + 1 )\nand\u2211 i\u2208Nin(j) EM(i, j) = \u03a8in(j, Y ) +O (\u221a \u03a8in(j, Y ) + 1 ) .\nWe then define two meta-experts: an ingoing expert, which predicts yi,j using the prediction of the ingoing RWM instance for node j, and the outgoing expert, which predicts yi,j using the prediction of the outgoing RWM instance for node i. The number of mistakes of these two experts satisfy\u2211 i\u2208V \u2211 j\u2208Nout(i) EM(i, j)\n= \u03a8out(Y ) +O (\u221a |V |\u03a8out(Y ) + |V | ) \u2211 j\u2208V \u2211 i\u2208Nin(j) EM(i, j)\n= \u03a8in(Y ) +O (\u221a |V |\u03a8in(Y ) + |V | ) ,\nwhere we used \u2211 j\u2208V \u221a \u03a8in(j, Y ) \u2264 \u221a |V |\u03a8in(Y ), and similarly for \u03a8out(Y ). Finally, let the overall prediction of our algorithm be a RWM instance run on top of the ingoing and the outgoing experts. Then the expected number of mistakes of this predictor satisfies\n\u2211 (i,j)\u2208E EM(i, j) = \u03a8G(Y ) +O\n(\u221a |V |\u03a8G(Y ) + |V |\n+ \u221a( \u03a8G(Y ) + |V |+ \u221a |V |\u03a8G(Y ) )) = \u03a8G(Y ) +O (\u221a |V |\u03a8G(Y ) + |V | ) ,\nas claimed.\nProof sketch of Theorem 3. Let YK be the set of all labelings Y such that the total number of negative and positive edges are K and |E| \u2212 K, respectively (without loss of generality we will focus on negative edges). Consider the randomized strategy that draws a labeling Y \u2208 {\u22121,+1}|E| uniformly at random from YK . For each node i \u2208 V , we have \u03a8in(i, Y ) \u2264 d\u2212in(i),\nwhich implies \u03a8in(Y ) \u2264 K. A very similar argument applies to the outgoing edges, leading to \u03a8out(Y ) \u2264 K. The constraint \u03a8G(Y ) \u2264 K is therefore always satisfied.\nThe adversary will force on average 1/2 mistakes in each one of the first K rounds of the online protocol by repeating K times the following: (i) A label value ` \u2208 {\u22121,+1} is selected uniformly at random. (ii) An edge (i, j) is sampled uniformly at random from the set of all edges that were not previously revealed and whose labels are equal to `.\nThe learner is required to predict yi,j and, in doing so, 1/2 mistakes will be clearly made on average because of the randomized labeling procedure. Observe that this holds even when A knows the value of K and \u03a8G(Y ). Hence, we can conclude that the expected number of mistakes that A can be forced to make is always at least K/2, as claimed.\nWe now show that, as K|E| \u2192 0, the lower bound gets arbitrarily close to K for any G(Y ) and any constant K. Let E be the following event: There is at least one unrevealed negative label. The randomized iterative strategy used to achieve this result is identical to the one described above, except for the stopping criterion. Instead of repeating step (i) and (ii) only for the first K rounds, these steps are repeated until E is true. Let mr,c be defined as follows: For c = 1 it is equal to the expected number of mistakes forced in round r when K = 1. For c > 1 it is equal to the difference between the expected number of mistakes forced in round r when K = c and K = c\u2212 1. One can see that mr,c is null when r < c. When K = 1, the probability that E is true in round r is clearly equal to 12r\u22121 . Hence, the expected number of mistakes made by A when K = 1 in any round r is equal to 12 1 2r\u22121 = 1 2r . We can therefore conclude that mr,1 = 1 2r for all r. A simple calculation shows that if r = c then mr,c = 1 2r . Furthermore, when r > 1 and c > 1, we have the following recurrence:\nmr,c = mr\u22121,c +mr\u22121,c\u22121\n2 .\nIn order to calculate mr,c for all r and c, we will rest on the ancillary quantity sj(i), recursively defined as specified next.\nGiven any integer variable i, we have s0(i) = 1 and, for any positive integer j,\nsj(i) = i\u2211 k=1 sj\u22121(k) .\nIt is not difficult to verify that\nmr,c = sc\u22121(r \u2212 c+ 1)\n2r .\nSince sj(i) = \u3008i\u3009j j! , where \u3008i\u3009j is the rising factorial i(i+ 1)(i+ 2) . . . (i+ j \u2212 1), we have\nmr,c = \u3008r \u2212 c+ 1\u3009c\u22121\n(c\u2212 1)!2r .\nWhen K|E| \u2192 0, given any integer K \u2032 > 1, the difference between the expected number of mistakes forced when K = K \u2032 and K = K \u2032 \u2212 1 is equal to\n\u221e\u2211 r=K\u2032 mr,K\u2032 = 1 (K \u2032 \u2212 1)! \u221e\u2211 r=K\u2032 \u3008r \u2212K \u2032 + 1\u3009K\u2032\u22121 2r\n= 1 (K \u2032 \u2212 1)!2K\u2032\u22121 \u221e\u2211 r\u2032=1 \u3008r\u2032\u3009K\u2032\u22121 2r\u2032 ,\nwhere we set r\u2032 = r \u2212K \u2032 + 1. Setting i\u2032 = i \u2212 1 and recalling that\n\u3008i\u3009j = j! ( i+ j \u2212 1 i\u2212 1 ) ,\nwe have\n1\nj! \u221e\u2211 i=1 \u3008i\u3009j 2i = \u221e\u2211 i=1\n( i+j\u22121 i\u22121 ) 2i = \u221e\u2211 i\u2032=0 ( i\u2032+j i\u2032 ) 2i\u2032+1 .\nNow, using the identity( i\u2032 + j + 1\ni\u2032\n) = ( i\u2032 + j\ni\u2032\n) + ( i\u2032 + j\ni\u2032 \u2212 1\n) ,\nwe can easily prove by induction on j that\n\u221e\u2211 i\u2032=0 ( i\u2032+j i\u2032 ) 2i\u2032+1 = 2j .\nHence, we have\n\u221e\u2211 r=K\u2032 mr,K\u2032 = 1.\nMoreover, as shown earlier, mr,1 = 1 2r for all r. Hence we can conclude that when K|E| \u2192 0\nEMA(Y ) \u2265 \u221e\u2211 r=1 1 2r + K\u2211 K\u2032=2 \u221e\u2211 r=K\u2032 mr,K\u2032 = K\nfor any edge-labeled graph G(Y ) and any constant K, as claimed."}, {"heading": "C Further Experimental Results", "text": "This section contains more evidence related to the experiments in Section 6. In particular, we experimentally\ndemonstrate the alignment between blc(tr, un) and LogReg.\nAfter training on the two features 1\u2212t\u0302r(i) and 1\u2212u\u0302n(j), LogReg has learned three weights w0, w1 and w2, which allow to predict yi,j according to\nsgn (( w1(1\u2212 t\u0302r(i) ) + w2 ( 1\u2212 u\u0302n(j) ) + w0 ) .\nThis can be rewritten as sgn (( 1\u2212 t\u0302r(i) ) + w\u20322 ( 1\u2212 u\u0302n(j) ) \u2212 12 \u2212 \u03c4 \u2032 ) ,\nwith w\u20322 = w2 w1\nand \u03c4 \u2032 = \u2212 (\n1 2 + w0 w1\n) .\nAs shown in Table 3, and in accordance with the predictor built out of Equation (2), w\u20322 is almost 1 on all datasets, while \u03c4 \u2032 tends to be always close the fraction of positive edges in the dataset."}], "references": [{"title": "Adaptive and self-confident on-line learning algorithms", "author": ["P. Auer", "N. Cesa-Bianchi", "C. Gentile"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Helmbold", "R.E. Schapire", "M.K. Warmuth"], "venue": "J. ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Concentration of Measure for the Analysis of Randomized Algorithms", "author": ["D.P. Dubhashi", "A. Panconesi"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}], "referenceMentions": [{"referenceID": 3, "context": "Interest in signed networks can be traced back to the psychological theory of structural balance [4, 12] with its weak version [10].", "startOffset": 97, "endOffset": 104}, {"referenceID": 2, "context": ", [3, 34, 13, 14, 7].", "startOffset": 2, "endOffset": 20}, {"referenceID": 1, "context": ", [34, 2]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "Each node i \u2208 V is endowed with two latent parameters pi, qi \u2208 [0, 1], which we assume to be generated, for each node i, by an independent draw from a fixed but unknown joint prior distribution \u03bc(p, q) over [0, 1].", "startOffset": 63, "endOffset": 69}, {"referenceID": 0, "context": "Each node i \u2208 V is endowed with two latent parameters pi, qi \u2208 [0, 1], which we assume to be generated, for each node i, by an independent draw from a fixed but unknown joint prior distribution \u03bc(p, q) over [0, 1].", "startOffset": 207, "endOffset": 213}, {"referenceID": 0, "context": "1 Approximation to Maximum Likelihood via Label Propagation For simplicity, assume the joint prior distribution \u03bc(p, q) is uniform over [0, 1] with independent marginals, and suppose that we draw at random without replacement the training set E0 = ( (i1, j1), yi1,j1), ((i2, j2), yi2,j2), .", "startOffset": 136, "endOffset": 142}, {"referenceID": 0, "context": ", [1]), defined as", "startOffset": 2, "endOffset": 5}], "year": 2017, "abstractText": "In the problem of edge sign prediction, we are given a directed graph (representing a social network), and our task is to predict the binary labels of the edges (i.e., the positive or negative nature of the social relationships). Many successful heuristics for this problem are based on the troll-trust features, estimating at each node the fraction of outgoing and incoming positive/negative edges. We show that these heuristics can be understood, and rigorously analyzed, as approximators to the Bayes optimal classifier for a simple probabilistic model of the edge labels. We then show that the maximum likelihood estimator for this model approximately corresponds to the predictions of a Label Propagation algorithm run on a transformed version of the original social graph. Extensive experiments on a number of real-world datasets show that this algorithm is competitive against state-ofthe-art classifiers in terms of both accuracy and scalability. Finally, we show that trolltrust features can also be used to derive online learning algorithms which have theoretical guarantees even when edges are adversarially labeled.", "creator": "LaTeX with hyperref package"}}}