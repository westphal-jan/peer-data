{"id": "1606.01433", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2016", "title": "Brundlefly at SemEval-2016 Task 12: Recurrent Neural Networks vs. Joint Inference for Clinical Temporal Information Extraction", "abstract": "we applied four options to the 1991 - 2016 consortium 12 : clinical causal analyses, participating in phase planning, where we identified insufficient amounts of time and meaningful expressions discussing clinical chemistry termed phase 2, where we predicted a relation being an event and its causal document showing time.", "histories": [["v1", "Sat, 4 Jun 2016 23:22:41 GMT  (97kb,D)", "http://arxiv.org/abs/1606.01433v1", "NAACL HLT 2016, SemEval-2016 Task 12 submission"]], "COMMENTS": "NAACL HLT 2016, SemEval-2016 Task 12 submission", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jason alan fries"], "accepted": false, "id": "1606.01433"}, "pdf": {"name": "1606.01433.pdf", "metadata": {"source": "CRF", "title": "Brundlefly at SemEval-2016 Task 12: Recurrent Neural Networks vs. Joint Inference for Clinical Temporal Information Extraction", "authors": ["Jason Alan Fries"], "emails": ["jason-fries@stanford.edu"], "sections": [{"heading": null, "text": "We submitted two systems to the SemEval2016 Task 12: Clinical TempEval challenge, participating in Phase 1, where we identified text spans of time and event expressions in clinical notes and Phase 2, where we predicted a relation between an event and its parent document creation time.\nFor temporal entity extraction, we find that a joint inference-based approach using structured prediction outperforms a vanilla recurrent neural network that incorporates word embeddings trained on a variety of large clinical document sets. For document creation time relations, we find that a combination of date canonicalization and distant supervision rules for predicting relations on both events and time expressions improves classification, though gains are limited, likely due to the small scale of training data."}, {"heading": "1 Introduction", "text": "This work discusses two information extraction systems for identifying temporal information in clinical text, submitted to SemEval-2016 Task 12 : Clinical TempEval (Bethard et al., 2016). We participated in tasks from both phases: (1) identifying text spans of time and event mentions; and (2) predicting relations between clinical events and document creation time.\nTemporal information extraction is the task of constructing a timeline or ordering of all events in a given document. In the clinical domain, this is a key requirement for medical reasoning systems as well as longitudinal research into the progression of\ndisease. While timestamps and the structured nature of the electronic medical record (EMR) directly capture some aspects of time, a large amount of information on the progression of disease is found in the unstructured text component of the EMR where temporal structure is less obvious.\nWe examine a deep-learning approach to sequence labeling using a vanilla recurrent neural network (RNN) with word embeddings, as well as a joint inference, structured prediction approach using Stanford\u2019s knowledge base construction framework DeepDive (Zhang, 2015). Our DeepDive application outperformed the RNN and scored similarly to 2015\u2019s best-in-class extraction systems, even though it only used a small set of context window and dictionary features. Extraction performance, however lagged this year\u2019s best system submission. For document creation time relations, we again use DeepDive. Our system examined a simple temporal distant supervision rule for labeling time expressions and linking them to nearby event mentions via inference rules. Overall system performance was better than this year\u2019s median submission, but again fell short of the best system."}, {"heading": "2 Methods and Materials", "text": "Phase 1 of the challenge required parsing clinical documents to identify TIMEX3 and EVENT temporal entity mentions in text. TIMEX3 entities are expressions of time, ranging from concrete dates to phrases describing intervals like \u201cthe last few months.\u201d EVENT entities are broadly defined as anything relevant to a patient\u2019s clinical timeline, e.g., diagnoses, illnesses, procedures. Entity mentions\nar X\niv :1\n60 6.\n01 43\n3v 1\n[ cs\n.C L\n] 4\nJ un\n2 01\nare tagged using a document collection of clinic and pathology notes from the Mayo Clinic called the THYME (Temporal History of Your Medical Events) corpus (Styler IV et al., 2014).\nWe treat Phase 1 as a sequence labeling task and examine several models for labeling entities. We discuss our submitted tagger which uses a vanilla RNN1 and compare its performance to a DeepDivebased system, which lets us encode domain knowledge and sequence structure into a probabilistic graphic model.\nFor Phase 2, we are given all test set entities and asked to identify the temporal relationship between an EVENT mention and corresponding document creation time. This relation is represented as a classification problem, assigning event attributes from the label set {BEFORE, OVERLAP, BEFORE/OVERLAP, AFTER}. We use DeepDive to define several inference rules for leveraging neighboring pairs of EVENT and TIMEX3 mentions to better reason about temporal labels.\n2.1 Recurrent Neural Networks 2.1.1 Overview\nVanilla (or Elman-type) RNNs are recursive neural networks with a linear chain structure (Elman, 1990). RNNs are similar to classical feedforward neural networks, except that they incorporate an additional hidden context layer that forms a timelagged, recurrent connection (a directed cycle) to the\n1There was a bug in our original RNN submission that dramatically lowered recall. We report the original and corrected test set scores in order to better characterize the contributions of this work.\nprimary hidden layer. In the canonical RNN design, the output of the hidden layer at time step t \u2212 1 is retained in the context layer and fed back into the hidden layer at t; this enables the RNN to explicitly model some aspects of sequence history. (see Figure 1).\nEach word in our vocabulary is represented as an n-dimensional vector in a lookup table of |vocab| x n parameters (i.e., our learned embedding matrix). Input features then consist of a concatenation of these embeddings to represent a context window surrounding our target word. The output layer then emits a probability distribution in the dimension of the candidate label set. The lookup table is shared across all input instances and updated during training.\nFormally our RNN definition follows (Mesnil et al., 2013):\nh(t) = f(Ux(t) +Vh(t\u2212 1)) where x(t) is our concatenated context window of word embeddings, h(t) is our hidden layer, U is the input-to-hidden layer matrix, V is the hidden layerto-context layer matrix, and f(x) is the activation function (logistic in this work).\nf(x) = 1 1+e\u2212x\nThe output layer y(t) consists of a softmax activation function g(x)\ny(t) = g(Wh(t)) g(xi) = exi\u2211 k e xk\nwhere W is the output layer matrix. Training is done using batch gradient descent using one sentence per batch. Our RNN implementation is based on code available as part of Theano v0.7 (Bastien et al., 2012)."}, {"heading": "2.1.2 Word Embeddings", "text": "For baseline RNN models, all embedding parameters are initialized randomly in the range [-1.0, 1.0]. For all other word-based models, embedding vectors are initialized or pre-trained with parameters trained on different clinical corpora. Pre-training generally improves classification performance over random initialization and provides a mechanism to leverage large collections of unlabeled data for use in semi-supervised learning (Erhan et al., 2010).\nWe create word embeddings using two collections of clinical documents: the MIMIC-III database\ncontaining 2.4M notes from critical care patients at Beth Israel Deaconess Medical Center (Saeed et al., 2011); and the University of Iowa Hospitals and Clinics (UIHC) corpus, containing 15M predominantly inpatient notes (see Table 1). All word embeddings in this document are trained with word2vec (Mikolov et al., 2013) using the Skipgram model, trained with a 10 token window size. We generated 100 and 300 dimensional embeddings based on prior work tuning representation sizes in clinical domains (Fries, 2015)."}, {"heading": "2.1.3 RNN Taggers", "text": "We train RNN models for three tasks in Phase 1: a character-level RNN for tokenization; and two word-level RNNs for POS tagging and entity labeling. Word-level RNNs are pre-trained with the embeddings described above, while characterlevel RNNs are randomly initialized. All words are normalized by lowercasing tokens and replacing digits with N, e.g., 01-Apr-2016 becomes NN-apr-NNNN to improve generalizability and restrict vocabulary size. Characters are left as unnormalized input. In the test data set, unknown words/characters are represented using the special token <UNK> . All hyperparameters were selected using a randomized grid search. 2\nTokenization: Word tokenization and sentence boundary detection are done simultaneously using a character-level RNN. Each character is assigned a tag from 3 classes: WORD(W) if a character is a member of a token that does not end a sentence; END(E) for a token that does end a sentence, and whitespace O. We use IOB2 tagging to encode the range of token spans.\nModels are trained using THYME syntactic annotations from colon and brain cancer notes. Training\n2RNN parameter space: hidden units: [48 - 384]; L/R context window [2 - 6]; learning rate: (0.001, 0.01, 0.1, 0.25). Character-level RNN parameters: dim: (16, 32); sentence padding [0 - 5].\ndata consists of all sentences, padded with 5 characters from the left and right neighboring sentences. Each character is represented by a 16-dimensional embedding (from an alphabet of 90 characters) and an 11 character context window. The final prediction task input is one, long character sequence perdocument. We found that the tokenizer consistently made errors conflating E and W classes (e.g., B-W, I-E, I-E) so after tagging, we enforce an additional consistency constraint on B-* and I-* tags so that contiguous BEGIN/INSIDE spans share the same class.\nPart-of-speech Tagging: We trained a POS tagger using THYME syntactic annotations. A model using 100-dimensional UIHC-CN embeddings (clinic notes) and a context window of \u00b1 2 words performed best on held out test data, with an accuracy of 97.67% and F1= 0.973.\nTIMEX3 and EVENT Span Tagging: We train separate models for each entity type, testing different pre-training schemes using 100 and 300- dimensional embeddings trained on our large, unlabeled clinical corpora. Both tasks use context windows of \u00b1 2 words (i.e., concatenated input of 5 n-d word embeddings) and a learning rate of 0.01. We use 80 hidden units for 100-dimensional embeddings models and 256 units for 300-dimensional models. Output tags are in the IOB2 tagging format."}, {"heading": "2.2 DeepDive", "text": "DeepDive developers build domain knowledge into applications using a combination of distant supervision rules, which use heuristics to generate noisy training examples, and inference rules which use factors to define relationships between random variables. This design pattern allows us to quickly encode domain knowledge into a probabilistic graphical model and do joint inference over a large space of random variables.\nFor example, we want to capture the relationship between EVENT entities and their closest TIMEX3 mentions in text since that provides some information about when the EVENT occurred relative to document creation time. TIMEX3s lack a class DocRelTime, but we can use a distant supervision rule to generate a noisy label that we then leverage to predict neighboring EVENT labels. We also know that the set of all EVENT/TIMEX3 mentions\nwithin a given note section, such as patient history, provides discriminative information that should be shared across labels in that section. DeepDive lets us easily define these structures by linking random variables (in this case all entity class labels) with factors, directly encoding domain knowledge into our learning algorithm."}, {"heading": "2.2.1 TIMEX3 and EVENT Spans", "text": "Phase 1: Our baseline tagger consists of three inference rules: logistic regression, conditional random fields (CRF), and skip-chain CRF (Sutton and McCallum, 2006). In CRFs, factor edges link adjoining words in a linear chain structure, capturing label dependencies between neighboring words. Skip-chain CRFs generalize this idea to include skip edges, which can connect non-neighboring words. For example, we can link labels for all identical words in a given window of sentences. We use DeepDive\u2019s feature library, ddlib, to generate common textual features like context windows and dictionary membership. We explored combinations of left/right windows of 2 neighboring words and POS tags, letter case, and entity dictionaries for all vocabulary identified by the challenge\u2019s baseline memorization rule, i.e., all phrases that are labeled as true entities \u2265 50% of the time in the training set.\nFeature Ablation Tests We evaluate 3 feature set combinations to determine how each contributes predictive power to our system.\nRun 1: dictionary features, letter case Run 2: dictionary features, letter case, context window (\u00b1 2 normalized words) Run 3: dictionary features, letter case, context window (\u00b1 2 normalized words), POS tags"}, {"heading": "2.2.2 Document Creation Time Relations", "text": "Phase 2: In order to predict the relationship between an event and the creation time of its parent document, we assign a DocRelTime random variable to every TIMEX3 and EVENT mention. For EVENTs, these values are provided by the training data, for TIMEX3s we have to compute class labels. Around 42% of TIMEX3 mentions are simple dates (\u201c12/29/08\u201d, \u201cOctober 16\u201d, etc.) and can be naively canonicalized to a universal timestamp. This is done using regular expressions to identify common date patterns and heuristics to deal with incomplete dates.\nThe missing year in \u201cOctober 16\u201d, for example, can be filled in using the nearest preceding date mention; if that isn\u2019t available we use the document creation year. These mentions are then assigned a class using the parent document\u2019s DocTime value and any revision timestamps. Other TIMEX3 mentions are more ambiguous so we use a distant supervision approach. Phrases like \u201ccurrently\u201d and \u201ctoday\u2019s\u201d tend to occur near EVENTs that overlap the current document creation time, while \u201cago\u201d or \u201cX-years\u201d indicate past events. These dominant temporal associations can be learned from training data and then used to label TIMEX3s. Finally, we define a logistic regression rule to predict entity DocRelTime values as well as specify a linear skip-chain factor over EVENT mentions and their nearest TIMEX3 neighbor, encoding the baseline system heuristic directly as an inference rule."}, {"heading": "3 Results", "text": ""}, {"heading": "3.1 Phase 1", "text": "Word tokenization performance was high, F1=0.993 while sentence boundary detection was lower with F1 = 0.938 (document micro average F1 = 0.985). Tokenization errors were largely confined to splitting numbers and hyphenated words (\u201cex-smoker\u201d vs. \u201cex - smoker\u201d) which has minimal impact on upstream sequence labeling. Sentence boundary errors were largely missed terminal words, creating longer sentences, which is preferable to short, less informative sequences in terms of impact on RNN minibatches.\nTables 2 and 3 contain results for all sequence labeling models. For TIMEX3 spans, the best RNN ensemble model performed poorly compared to the winning system (0.706 vs. 0.795). DeepDive runs 2-3 performed as well as 2015\u2019s best system, but also fell short of the top system (0.730 vs. 0.795). EVENT spans were easier to tag and RNN models compared favorably with DeepDive, the former scoring higher recall and the latter higher precision. Both approaches scored below this year\u2019s best system (0.885 vs. 0.903)."}, {"heading": "3.2 Phase 2", "text": "Finally, Table 4 contains our DocRelTime relation extraction. Our simple distant supervision rule leads\nto better performance than then median system submission, but also falls substantially short of current state of the art."}, {"heading": "4 Discussion", "text": "Randomly initialized RNNs generally weren\u2019t competitive to our best performing structured prediction\nmodels (DeepDive runs 2-3) which isn\u2019t surprising considering the small amount of training data available compared to typical deep-learning contexts. There was a statistically significant improvement for RNNs pre-trained with clinical text word2vec embeddings, reflecting the consensus that embeddings capture some syntactic and semantic information that must otherwise be manually encoded as features. Performance was virtually the same across all embedding types, independent of corpus size, note type, etc. While embeddings trained on more data perform better in semantic tasks like synonym detection, its unclear if that representational strength is important here. Similar performance might also just reflect the general ubiquity with which temporal vocabulary occurs in all clinical note contexts. Alternatively, vanilla RNNs rarely achieve stateof-the-art performance in sequence labeling tasks due to well-known issues surrounding the vanishing or exploding gradient effect (Pascanu et al., 2012). More sophisticated recurrent architectures with gated units such as Long Short-Term Memory (LSTM), (Hochreiter and Schmidhuber, 1997) and gated recurrent unit (Chung et al., 2014) or recursive structures like Tree-LSTM (Tai et al., 2015) have shown strong representational power in other sequence labeling tasks. Such approaches might perform better in this setting.\nDeepDive\u2019s feature generator libraries let us easily create a large space of binary features and then let regularization address overfitting. In our extraction system, just using a context window of \u00b1 2 words and dictionaries representing the baseline memorization rules was enough to achieve median system performance. POS tag features had no statistically significant impact on performance in either EVENT/TIMEX3 extraction.\nFor classifying an EVENT\u2019s document creation time relation, our DeepDive application essentially implements the joint inference version of the baseline memorization rule, leveraging entity proximity to increase predictive performance. A simple distant supervision rule that canonicalizes TIMEX3 timestamps and predicts nearby EVENT\u2019s lead to a slight performance boost, suggesting that using a larger collection of unlabeled note data could lead to further increases.\nWhile our systems did not achieve current stateof-the-art performance, DeepDive matched last year\u2019s top submission for TIMEX3 and EVENT tagging with very little upfront engineering \u2013 around a week of dedicated development time. One of the primary goals of this work was to avoid an over-engineered extraction pipeline, instead relying on feature generation libraries or deep learning approaches to model underlying structure. Both systems explored in this work were successful to some extent, though future work remains in order to close the performance gap between these approaches and current state-of-the-art systems."}, {"heading": "Acknowledgments", "text": "This work was supported by the Mobilize Center, a National Institutes of Health Big Data to Knowledge (BD2K) Center of Excellence supported through Grant U54EB020405."}], "references": [{"title": "Nicolas Bouchard", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bastien et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "James Pustejovsky", "author": ["Steven Bethard", "Leon Derczynski", "Guergana Savova", "Guergana Savova"], "venue": "and Marc Verhagen.", "citeRegEx": "Bethard et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "James Pustejovsky", "author": ["Steven Bethard", "Guergana Savova", "Wei-Te Chen", "Leon Derczynski"], "venue": "and Marc Verhagen.", "citeRegEx": "Bethard et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Association for Computational Linguistics", "author": ["San Diego", "California", "June"], "venue": null, "citeRegEx": "2016. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "2016. et al\\.", "year": 2016}, {"title": "KyungHyun Cho", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre"], "venue": "and Yoshua Bengio.", "citeRegEx": "Chung et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Pascal Vincent", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol"], "venue": "and Samy Bengio.", "citeRegEx": "Erhan et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Modeling Words for Online Sexual Behavior Surveillance and Clinical Text Information Extraction", "author": ["Jason Fries"], "venue": "Ph.D. thesis,", "citeRegEx": "Fries.,? \\Q2015\\E", "shortCiteRegEx": "Fries.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Li Deng", "author": ["Gr\u00e9goire Mesnil", "Xiaodong He"], "venue": "and Yoshua Bengio.", "citeRegEx": "Mesnil et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Greg Corrado", "author": ["Tomas Mikolov", "Kai Chen"], "venue": "and Jeffrey Dean.", "citeRegEx": "Mikolov et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Tomas Mikolov", "author": ["Razvan Pascanu"], "venue": "and Yoshua Bengio.", "citeRegEx": "Pascanu et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Benjamin Moody", "author": ["Mohammed Saeed", "Mauricio Villarroel", "Andrew T Reisner", "Gari Clifford", "Li-Wei Lehman", "George Moody", "Thomas Heldt", "Tin H Kyaw"], "venue": "and Roger G Mark.", "citeRegEx": "Saeed et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Guergana Savova", "author": ["William F Styler IV", "Steven Bethard", "Sean Finan", "Martha Palmer", "Sameer Pradhan", "Piet C de Groen", "Brad Erickson", "Timothy Miller", "Chen Lin"], "venue": "et al.", "citeRegEx": "Styler IV et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to conditional random fields for relational learning. Introduction to statistical relational learning, pages 93\u2013128", "author": ["Sutton", "McCallum2006] Charles Sutton", "Andrew McCallum"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2006}, {"title": "Richard Socher", "author": ["Kai Sheng Tai"], "venue": "and Christopher D Manning.", "citeRegEx": "Tai et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "DeepDive: A Data Management System for Automatic Knowledge Base Construction", "author": ["Ce Zhang"], "venue": "Ph.D. thesis,", "citeRegEx": "Zhang.,? \\Q2015\\E", "shortCiteRegEx": "Zhang.", "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "We submitted two systems to the SemEval2016 Task 12: Clinical TempEval challenge, participating in Phase 1, where we identified text spans of time and event expressions in clinical notes and Phase 2, where we predicted a relation between an event and its parent document creation time. For temporal entity extraction, we find that a joint inference-based approach using structured prediction outperforms a vanilla recurrent neural network that incorporates word embeddings trained on a variety of large clinical document sets. For document creation time relations, we find that a combination of date canonicalization and distant supervision rules for predicting relations on both events and time expressions improves classification, though gains are limited, likely due to the small scale of training data.", "creator": "LaTeX with hyperref package"}}}