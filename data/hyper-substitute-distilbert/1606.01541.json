{"id": "1606.01541", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2016", "title": "Deep Reinforcement Learning for Dialogue Generation", "abstract": "passive neural models of collaboration generation offer great capability beyond social recall when conversational agents, but tend to report shortsighted, predicting utterances one back one head & ignoring their influence its external outcomes. addressing the future direction increases competitive dialogue strategy crucial to developing continuous, disciplined dialogues, a need which contemporary traditional nlp models of dialogue modeling draw on reinforcement feedback. toward this paper, we articulated motives to harness these languages, applying deep spatial learning to model better reward - chatbot capabilities. the latter simulates communications between two virtual agents, avoiding policy enforcement methods requiring reward sequences the induce three useful conversational properties : informativity ( non - repetitive mode ), coherence, and ease procedural perception ( related to forward - sensing function ). we analyze our model on diversity, thinking as predictable as with persistent beings, showing and the dominant technology generates specific interactive responses and dialogue must foster a more sustained development in dialogue simulation. scientific work marks a necessary step towards learning a neural conversational communication improved on the long - term success strategies confrontation.", "histories": [["v1", "Sun, 5 Jun 2016 17:59:23 GMT  (4623kb,D)", "http://arxiv.org/abs/1606.01541v1", null], ["v2", "Fri, 10 Jun 2016 05:44:47 GMT  (4625kb,D)", "http://arxiv.org/abs/1606.01541v2", null], ["v3", "Sat, 25 Jun 2016 04:49:39 GMT  (4633kb,D)", "http://arxiv.org/abs/1606.01541v3", null], ["v4", "Thu, 29 Sep 2016 15:02:32 GMT  (9575kb,D)", "http://arxiv.org/abs/1606.01541v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "will monroe", "alan ritter", "dan jurafsky", "michel galley", "jianfeng gao"], "accepted": true, "id": "1606.01541"}, "pdf": {"name": "1606.01541.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning for Dialogue Generation", "authors": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky"], "emails": ["jiweil@stanford.edu,", "wmonroe4@stanford.edu,", "jurafsky@stanford.edu,", "ritter.1492@osu.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural response generation (Li et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Wen et al., 2015; Shang et al., 2015; Yao et al., 2015; Xu et al., 2016; Wen et al., 2016; Li et al., 2016) is of growing interest. The LSTM sequence-to-sequence (SEQ2SEQ) model (Sutskever et al., 2014) is one type of neural generation model that maximizes the probability of generating a response given the previous dialogue turn. This approach enables the incorporation of rich context when mapping between consecutive dialogue\nturns (Sordoni et al., 2015) in a way not possible, for example, with MT-based dialogue models (Ritter et al., 2011).\nDespite the success of SEQ2SEQ models in dialogue generation, two problems emerge: First, SEQ2SEQ models are trained by predicting the next dialogue turn in a given conversational context using the maximum-likelihood estimation (MLE) objective function. However, it is not clear how well MLE approximates the real-world goal of chatbot development: teaching a machine to converse with humans, while providing interesting, diverse, and informative feedback that keeps users engaged. One concrete example is that SEQ2SEQ models tend to generate highly generic responses such as\u201cI don\u2019t know\u201d regardless of the input (Sordoni et al., 2015; Serban et al., 2015b; Serban et al., 2015c; Li et al., 2015). This can be ascribed to the high frequency of generic responses found in the training set and their compatibility with a diverse range of conversational contexts. Apparently \u201cI don\u2019t know\u201d is not a good action to take, since it closes the conversation down.\nAnother common problem, illustrated in Table 1 (the example in the bottom left), is when the system becomes stuck in an infinite loop of repetitive responses. This is due to MLE-based SEQ2SEQ models\u2019 inability to account for repetition. In example 2, the dialogue falls into an infinite loop after three turns, with both agents generating dull, generic utterances like i don\u2019t know what you are talking about and you don\u2019t know what you are saying. Looking at the entire conversation, utterance (2) i\u2019m 16 turns out to be a bad action to take. While it is an informative and coherent response to utterance (1) asking about age, it offers no way of continuing the conversation.1\n1A similar rule is often suggested in improvisational comedy: https://en.wikipedia.org/wiki/Yes,_and...\nar X\niv :1\n60 6.\n01 54\n1v 1\n[ cs\n.C L\n] 5\nJ un\n2 01\n6\nThese challenges suggest we need a conversation framework that has the ability to (1) integrate developer-defined rewards that better mimic the true goal of chatbot development and (2) model the longterm influence of a generated response in an ongoing dialogue.\nTo achieve these goals, we draw on the insights of reinforcement learning, which have been widely applied in MDP and POMDP dialogue systems (see Related Work section for details). We introduce a neural reinforcement learning (RL) generation method, which can optimize long-term rewards designed by system developers. Our model uses the encoderdecoder architecture as its backbone, and simulates conversation between two virtual agents to explore the space of possible actions while learning to maximize expected reward. We define simple heuristic approximations to rewards that characterize good conversations: good conversations are forward-looking (Allwood et al., 1992) or interactive (a turn suggests a following turn), informative, and coherent. The parameters of an encoder-decoder RNN define a policy over an infinite action space consisting of all possible utterances. The agent learns a policy by optimizing the long-term developer-defined reward from ongo-\ning dialogue simulations using policy gradient methods (Williams, 1992), rather than the MLE objective defined in standard SEQ2SEQ models.\nOur model thus integrates the power of SEQ2SEQ systems to learn compositional semantic meanings of utterances with the strengths of reinforcement learning in optimizing for long-term goals across a conversation. Experimental results (sampled results at the right panel of Table 1) demonstrate that our approach fosters a more sustained dialogue and manages to produce more interactive responses than standard SEQ2SEQ models trained using the MLE objective."}, {"heading": "2 Related Work", "text": "Efforts to build statistical dialog systems fall into two major categories.\nThe first treats dialogue generation as a sourceto-target transduction problem and learns mapping rules between input messages and responses from a massive amount of training data. Ritter et al. (2011) frames the response generation problem as a statistical machine translation (SMT) problem. Sordoni et al. (2015) improved Ritter et al.\u2019s system by rescoring the outputs of a phrasal SMT-based conversation\nsystem with a neural model that incorporates prior context. Recent progress in SEQ2SEQ models inspire several efforts (Vinyals and Le, 2015) to build endto-end conversational systems which first apply an encoder to map a message to a distributed vector representing its semantics and generate a response from the message vector. Serban et al. (2015a) propose a hierarchical neural model that captures dependencies over an extended conversation history. Li et al. (2015) propose mutual information between message and response as an alternative objective function in order to reduce the proportion of generic responses produced by SEQ2SEQ systems.\nThe other line of statistical research focuses on building task-oriented dialogue systems to solve domain-specific tasks. Efforts include statistical models including Markov Decision Processes (MDPs) (Levin et al., 1997; Levin et al., 2000; Walker et al., 2003; Pieraccini et al., 2009), POMDP (Young et al., 2010; Young et al., 2013) models, and models that statistically learn generation rules (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Nio et al., 2014). This dialogue literature thus widely appliess reinforcement learning (Walker, 2000; Schatzmann et al., 2006; Gasic et al., 2013; Singh et al., 1999; Singh et al., 2000; Singh et al., 2002) to train dialogue policies. But task-oriented RL dialogue systems often rely on carefully limited dialogue parameters, or hand-built templates with state, action and reward signals designed by humans for each new domain, making the paradigm difficult to extend to open-domain scenarios.\nOur goal is to integrate these two paradigms, drawing on the advantages of both. We are thus inspired by recent work like Wen et al. (2016) trains an endto-end task-oriented dialogue system that links input representations to slot-value pairs in a database."}, {"heading": "3 Reinforcement Learning for Open-Domain Dialogue", "text": "In this section, we describe in detail the components of the proposed RL model.\nThe learning system consists of two agents. We use p to denote sentences generated from the first agent and q to denote sentences from the second. The two agents take turns talking with each other. A dialogue can be represented as a alternating sequence of sentences generated by the two agents:\np1, q1, p2, q2, ..., pi, qi. We view the generated sentences as actions that are taken according to a policy defined a by an encoder-decoder recurrent neural network language model.\nThe parameters of the network are optimized to maximize expected future reward using policy search, as described in Section 4.3. Policy gradient methods are more appropriate for our scenario than Q-learning (Mnih et al., 2013), because we can initialize the encoder-decoder RNN using MLE parameters that already produce plausible responses, before changing the objective and tuning towards a policy that maximizes long-term reward. Q-learning, on the other hand, directly estimates the future expected reward of each action, which can differ from the MLE objective by orders of magnitude, thus making MLE parameters inapropriate for initialization. The components (states, actions, reward, etc.) of our sequential decision problem are summarized in the following sub-sections."}, {"heading": "3.1 Action", "text": "An action a is the dialogue utterance to generate. The action space is infinite since arbitrary-length sequences can be generated."}, {"heading": "3.2 State", "text": "A state is denoted by the previous two dialogue turns [pi, qi]. The dialogue history is further transformed to a vector representation by feeding the concatenation of pi and qi into an LSTM encoder model as described in Li et al. (2015)."}, {"heading": "3.3 Policy", "text": "A policy takes the form of an LSTM encoder-decoder (i.e., pRL(pi+1|pi, qi) ) and is defined by its parameters. Note that we use a stochastic representation of the policy (a probability distribution over actions given states). A deterministic policy would result in a discontinuous objective that is difficult to optimize using gradient-based methods."}, {"heading": "3.4 Reward", "text": "r denotes the reward obtained for each action. In this subsection, we discuss major factors that contribute to the success of a dialogue and describe how approximations to these factors can be operationalized in computable reward functions.\nEase of answering A turn generated by a machine should be easy to respond to. This aspect of a turn is related to its forward-looking function: the constraints a turn places on the next turn (Schegloff and Sacks, 1973; Allwood et al., 1992). We propose to measure the ease of answering a generated turn by using the negative log likelihood of reponding to that utterance with a dull response. We manually constructed a list of dull responses S consisting 8 turns such as \u201cI don\u2019t know what you are talking about\u201d, \u201cI have no idea\u201d, etc., that we and others have found occur very frequently in SEQ2SEQ models of conversations. The reward function is given as follows:\nr1 = \u2212 1\nNS \u2211 s\u2208S 1 Ns log pseq2seq(s|a) (1)\nwhere NS denotes the cardinality of NS and Ns denotes the number of tokens in the dull response s. Although of course there are more ways to generate dull responses than the list can cover, many of these responses are likely to fall into similar regions in the vector space computed by the model. A system less likely to generate utterances in the list is thus also less likely to generate other dull responses. pseq2seq represents the likelihood output by SEQ2SEQ models. It is worth noting that pseq2seq is different from the stochastic policy function pRL(pi+1|pi, qi), since the former is learned based on the MLE objective of the SEQ2SEQ model while the latter is the policy optimized for long-term future reward in the RL setting. r1 is further scaled by the length of target S.\nInformation Flow We want each agent to contribute new information at each turn to keep the dialogue moving and avoid repetitive sequences. We therefore propose penalizing semantic similarity between consecutive turns from the same agent. Let hpi and hpi+1 denote representations obtained from the encoder for two consecutive turns pi and pi+1. The reward is given by the negative log of the cosine similarity between them:\nr2 = \u2212 log cos(\u03b8) = \u2212 log hpi \u00b7 hpi+1 \u2016hpi\u2016\u2016hpi+1\u2016\n(2)\nSemantic Coherence We also need to measure the adequacy of responses to avoid situations in which the generated replies are highly rewarded but are\nungrammatical or not coherent. We therefore consider mutual information between the action a and previous turns in the history to ensure the generated responses are coherent and appropriate:\nr3 = 1\nNa log pseq2seq(a|qi, pi)+\n1\nNqi log pbackwardseq2seq (qi|a)\n(3) pseq2seq(a|pi, qi) denotes the probability of generating response a the given previous dialogue utterances [pi, qi]. pbackwardseq2seq (qt|a) denotes the backward probability of generating the previous dialogue utterance qt based on response a. pbackwardseq2seq is trained in a similar way as standard SEQ2SEQ models with sources and targets swapped. Again, to control for the influence of target length, both log pseq2seq(a|qi, pi) and log pbackwardseq2seq (qi|a) are scaled by the length of targets.\nThe final reward for action a is a weighted sum of the rewards discussed above:\nr(a, [pi, qi]) = \u03bb1r1 + \u03bb2r2 + \u03bb3r3 (4)\nwhere \u03bb1 + \u03bb2 + \u03bb3 = 1. We set \u03bb1 = 0.25, \u03bb2 = 0.25 and \u03bb3 = 0.5. A reward is observed after the agent reaches the end of each sentence."}, {"heading": "4 Simulation", "text": "The central idea behind our approach is to simulate the process of two virtual agents taking turns talking with each other, through which we can explore the state space and learn a policy pRL(pi+1|pi, qi) that leads to optimal expected reward."}, {"heading": "4.1 Supervised Learning", "text": "For the first stage of training, we build on prior work of predicting a generated target sequence given dialogue history from the supervised SEQ2SEQ model (Vinyals and Le, 2015). Results from supervised models will be later used for initialization.\nWe trained a SEQ2SEQ model on the Opensubtitle dataset, which consists of roughly 80 million sourcetarget pairs. We treated each turn in the dataset as a target and the concatenation of two previous sentences as source inputs."}, {"heading": "4.2 Mutual Information", "text": "Samples from SEQ2SEQ models are often times dull and generic, e.g., \u201ci don\u2019t know\u201d (Li et al., 2015) We thus do not want to initialize the policy model\nusing the pre-trained SEQ2SEQ models because this will lead to a lack of diversity in the RL models\u2019 experiences. Li et al. (2015) showed that modeling mutual information between sources and targets will significantly decrease the chance of generating dull responses and improve general response quality. We now show how we can obtain an encoder-decoder model which generates maximum mutual information responses.\nAs illustrated in Li et al. (2015), direct decoding from Eq 3 is infeasible since the second term requires the target sentence to be completely generated. Inspired by recent work on sequence level learning (Ranzato et al., 2015), we treat the problem of generating maximum mutual information response as a reinforcement learning problem in which a reward of mutual information value is observed when the model arrives at the end of a sequence.\nSimilar to Ranzato et al. (2015), we use policy gradient methods (Sutton et al., 1999; Williams, 1992) for optimization. We initialize the policy model pRL using a pre-trained pSEQ2SEQ(a|pi, qi) model. Given an input source [pi, qi], we generate a candidate list A = {a\u0302|a\u0302 \u223c pRL}. For each generated candidate a\u0302, we will obtain the mutual information score m(a\u0302, [pi, qi]) from the pre-trained pSEQ2SEQ(a|pi, qi) and pbackwardSEQ2SEQ(qi|a). This mutual information score will be used as reward and back propagate to the encoder-decoder model, tailoring it to generate sequences with higher rewards. We refer the readers to Zaremba and Sutskever (2015) and Williams (1992) for details. For each candidate a\u0302, the loss function is given by:\nLoss = \u2212pRL(a\u0302|[pi, qi]) \u00b7m(a\u0302, [pi, qi]) \u2206\u03b8 = m(a\u0302, [pi, qi]) \u2202pRL(a\u0302|[pi, qi])\n\u2202\u03b8\n(5)\nWe update the parameters in the encoder-decoder model using stochastic gradient descent."}, {"heading": "4.3 Dialogue Simulation between Two Agents", "text": "We simulate conversation between the two virtual agents and have them take turns talking with each other. The simulation proceeds as follows: at the initial step, a message from the training set is fed to the first agent. The agent encodes the input message to a vector representation and starts decoding to generate a response output. Combining the immediate\noutput from the first agent with the dialogue history, the second agent updates the state by encoding the dialogue history into a representation and uses the decoder RNN to generate responses, which are subsequently fed back to the first agent, and the process is repeated.\nOptimization We initialize the policy model pRL with parameters from the mutual information model described in the previous subsection. We then use policy gradient methods to find parameters that lead to a large expected future reward, by minimizing the following loss function:\nLoss =\u2212 Ea[R\u0302(a, [pi, qi])] =\u2212 \u2211 a pRL(a|pi, qi)[R\u0302(a, [pi, qi])] (6)\nwhere R\u0302(a, [pi, qi]) denotes future expected reward resulting from action a. R\u0302(a, [pi, qi]) consists of two parts, the immediate reward r(a, [pi, qi]) observed from action a computed using Eq 4 and the value of future reward denoted as rfuture(a, [pi, qi]) , which is equivalent to the expectation of the reward introduced by the sequence a\u2032 generated from the other agent in response to a.\nrfuture(a, [pi, qi]) = Ea\u2032 [R\u0302(a\u2032, [qi, a])] (7)\nIn Eq 7, [qi, a] denotes the new state the system arrives after taking the action a, which is represented by the two consecutive dialogue turns a and qi. We therefore have\nLoss =\u2212 Ea[R\u0302(a, [pi, qi])] =\u2212 \u2211 a pRL(a|pi, qi)r(a, [pi, qi])\n\u2212\u03b3 \u2211 a pRL(a|pi, qi) \u2211 a\u2032 R\u0302(a\u2032, [qi, a]) (8)\nwhere \u03b3 denotes the discount factor and is set to 0.99. Two practical issues arise regarding Eq 8. First, computing reward expectation requires summing over all possible actions which is infeasible given the infinite number of response candidates. We turn to a simplified strategy which approximates the entire action space by sampling a small list of candidates, denoted\nas C. Ea[R\u0302(a, [pi, qi])] can then be rewritten as:\nEa[R\u0302(a, [pi, qi])] \u2248 \u2211 a\u2208C pRL(a|pi, qi)\u2211 a\u2217\u2208C pRL(a|pi, qi) [R\u0302(a, [pi, qi])]\n(9) To avoid favoring shorter sequences, we replace pRL(a|pi, qi) with p\u2032RL(a|pi, qi) which is the geometric mean of word-predicting probability:\np\u2032RL(a|pi, qi) = [pRL(a|pi, qi)]1/Na (10)\nwhere Na denotes the length of the generated sequence. Eq 9 is differentiable and gradients can be computed using standard back-propagation techniques. We refer the readers to the appendix for the full derivation of the objective function.\nSince most of the generated translations in the Nbest list are similar, differing only by punctuation or minor morphological variations, candidates are generated by sampling from the distribution with additional Gaussian noise to foster more diverse candidates."}, {"heading": "4.4 Staged Training", "text": "We adopt the strategy of staged training in which we begin with simulating the dialogue for 2 turns, and gradually increase the number of simulated turns. We generate 5 turns at most, as the number of candidates to examine grows exponentially in the size of candidate list. Five candidate responses are generated at each step of the simulation."}, {"heading": "5 Experimental Results", "text": "In this section, we describe experimental results along with qualitative analysis. We evaluate dialogue generation systems using both human judgments and two automatic metrics: conversation length and diversity."}, {"heading": "5.1 Dataset", "text": "The dialogue simulation requires high-quality initial inputs fed to the agent. For example, an initial input of \u201cwhy ?\u201d is undesirable since it is unclear how the dialogue could proceed. We take a subset of 10 million messages from the Opensubtitle dataset and extract 0.8 million sequences with the lowest likelihood of generating the response \u201ci don\u2019t know what you are taking about\u201d to make ensure initial inputs are easy to respond to."}, {"heading": "5.2 Automatic Evaluation", "text": "Evaluating dialogue systems is difficult. Metrics such as BLEU (Papineni et al., 2002) and perplexity have been widely used for dialogue quality evaluation (Li et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015), but it is widely debated how well these automatic metrics are correlated with true response quality (Liu et al., 2016; Galley et al., 2015). Since the goal of the proposed system is not to predict the highest probability response, but rather the long-term success of the dialogue, we do not employ BLEU or\nperplexity for evaluation2.\nLength of the dialogue The first metric we propose is the length of the simulated dialogue. We say a dialogue ends when one of the agents starts generating dull responses such as \u201ci don\u2019t know\u201d 3 or two consecutive utterances from the same user are highly overlapping4.\nThe test set consists of 1,000 input messages. To reduce the risk of circular dialogues, we limit the number of simulated turns to be less than 8. Results are shown in Table 2. As can be seen, using mutual information leads to more sustained conversations between the two agents. The proposed RL model is first trained based on the mutual information objective and thus benefits from it in addition to the RL model. We observe that the RL model with dialogue simulation achieves the best evaluation score.\nDiversity We report degree of diversity by calculating the number of distinct unigrams and bigrams in generated responses. The value is scaled by the total number of generated tokens to avoid favoring long sentences as described in (Li et al., 2015). The resulting metric is thus a type-token ratio for unigrams and bigrams.\nFor both the standard SEQ2SEQ model and the proposed RL model, we use beam search with a beam size 10 to generate a response to a given input mes-\n2We found the RL model performs worse on BLEU score. On a random sample of 2,500 conversational pairs, single reference BLEU scores for RL models, mutual information models and vanilla SEQ2SEQ models are respectively 1.28, 1.44 and 1.17. BLEU is highly correlated with perplexity in generation tasks. Since the RL model is trained based on future reward rather than MLE, it is not surprising that the RL based models achieve lower BLEU score.\n3We use a simple rule matching method, with a list of 8 phrases that count as dull responses. Although this can lead to both false-positives and -negatives, it works pretty well in practice.\n4Two utterances are considered to be repetitive if they share more than 80 percent of their words.\nsage. For the mutual information model, we first generate n-best lists using pSEQ2SEQ(t|s) and then linearly re-rank them using pSEQ2SEQ(s|t). Results are presented in Table 4. We find that the proposed RL model generates more diverse outputs when compared against both the vanilla SEQ2SEQ model and the mutual information model.\nHuman Evaluation We explore three settings for human evaluation: the first setting is similar to what was described in Li et al. (2015), where we employ crowdsourced judges to evaluate a random sample of 500 items. We present both an input message and the generated output to 3 judges and ask them to decide which of the two outputs is better (denoted as singleturn general quality). Ties are permitted. Identical strings are assigned the same score. We measure the improvement achieved by the RL model over the mutual information model by the mean difference in scores between the models.\nFor the second setting, judges are again presented with input messages and system outputs, but are asked to decide which of the two outputs is easier to respond to (denoted as single-turn ease to answer). Again we evaluate a random sample of 500 items, each being assigned to 3 judges.\nFor the third setting, judges are presented with simulated conversations between the two agents (denoted as multi-turn general quality). Each conversation consists of 5 turns. We evaluate 200 simulated conversations, each being assigned to 3 judges, who are asked to decide which of the simulated conversations is of higher quality.\nResults for human evaluation are shown in Table 5. The proposed RL system does not introduce a significant boost in single-turn response quality with 95% CT [-0.02, 0.05], which is in line with our expectation, as the RL model is not optimized to predict the next utterance, but rather to increase long-term reward. The RL system produces responses that are significantly easier to answer than mutual information, as demonstrated by the single-turn ease to answer setting, and also higher quality multi-turn dialogues, as demonstrated by the multi-turn general quality setting.\nQualitative Analysis and Discussion We show a random sample of generated responses in Table 3 and simulated conversations in Table 1 at the beginning of the paper. From Table 3, we can see that the RL based agent indeed generates more interactive responses than the other baselines. We also find that the RL model has a tendency to end a sentence with another question and hand the conversation over to the user. From Table 1, we observe that the RL model manages to produce more interactive and sustained conversations than the mutual information model.\nDuring error analysis, we found that although we penalize repetitive utterances in consecutive turns, the dialogue sometimes enters a cycle with length greater than one, as shown in Table 6. This can be ascribed to the limited amount of conversational history we consider. Another issue observed is that the model sometimes starts a less relevant topic during the conversation. There is a tradeoff between relevance and less repetitiveness, as manifested in the reward function we define in Eq 4.\nThe fundamental problem, of course, is that the manually defined reward function can\u2019t possibly cover the crucial aspects that define an ideal conversation. While the heuristic rewards that we defined are amenable to automatic calculation, and do capture\nsome aspects of what makes a good conversation, ideally the system would instead receive real rewards from humans. Another problem with the current model is that we can only afford to explore a very small number of candidates and simulated turns since the number of cases to consider grow exponentially."}, {"heading": "6 Conclusion", "text": "We introduce a reinforcement learning framework for neural response generation by simulating dialogues between two agents, integrating the strengths of neural SEQ2SEQ systems and reinforcement learning for dialogue. Like earlier neural SEQ2SEQ models, our framework captures the compositional models of the meaning of a dialogue turn and generates semantically appropriate responses. Like reinforement learning dialogue systems, our framework is able to generate utterances that optimize future reward, successfully capturing global properties of a good conversation. Despite the fact that model uses very simple, operationable heuristics for capturing these global properties, the framework generates more diverse, interactive responses that foster a more sustained conversation."}, {"heading": "7 Acknowledgement", "text": "We would like to thank Michel Galley, Jianfeng Gao, Chris Brockett, Bill Dolan and other members of the NLP group at Microsoft Research for insightful comments and suggestions. We also want to thank Kelvin Guu, Percy Liang, Chris Manning, Sida Wang, Ziang Xie and other members of the Stanford NLP groups for useful discussions. Jiwei Li is supported by the Facebook Fellowship, to which we gratefully acknowledge. This work partially supported by NSF Award IIS-1514268. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF or Facebook."}], "references": [{"title": "On the semantics and pragmatics of linguistic feedback", "author": ["Jens Allwood", "Joakim Nivre", "Elisabeth Ahls\u00e9n."], "venue": "Journal of Semantics, 9:1\u201326.", "citeRegEx": "Allwood et al\\.,? 1992", "shortCiteRegEx": "Allwood et al\\.", "year": 1992}, {"title": "Iris: a chatoriented dialogue system based on the vector space model", "author": ["Rafael E Banchs", "Haizhou Li."], "venue": "Proceedings of the ACL 2012 System Demonstrations, pages 37\u201342. Association for Computational Linguistics.", "citeRegEx": "Banchs and Li.,? 2012", "shortCiteRegEx": "Banchs and Li.", "year": 2012}, {"title": "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint", "citeRegEx": "Galley et al\\.,? 2015", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "On-line policy optimisation of bayesian spoken dialogue systems via human interaction", "author": ["Milica Gasic", "Catherine Breslin", "Mike Henderson", "Dongkyu Kim", "Martin Szummer", "Blaise Thomson", "Pirros Tsiakoulis", "Stephanie Young."], "venue": "Acoustics, Speech and", "citeRegEx": "Gasic et al\\.,? 2013", "shortCiteRegEx": "Gasic et al\\.", "year": 2013}, {"title": "Learning dialogue strategies within the markov decision process framework", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert."], "venue": "Automatic Speech Recognition and Understanding, 1997. Proceedings., 1997 IEEE Workshop on, pages 72\u201379. IEEE.", "citeRegEx": "Levin et al\\.,? 1997", "shortCiteRegEx": "Levin et al\\.", "year": 1997}, {"title": "A stochastic model of human-machine interaction for learning dialog strategies", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert."], "venue": "Speech and Audio Processing, IEEE Transactions on, 8(1):11\u201323.", "citeRegEx": "Levin et al\\.,? 2000", "shortCiteRegEx": "Levin et al\\.", "year": 2000}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1510.03055.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1603.06155.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1603.08023.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Lstm based conversation models", "author": ["Yi Luan", "Yangfeng Ji", "Mari Ostendorf."], "venue": "arXiv preprint arXiv:1603.09457.", "citeRegEx": "Luan et al\\.,? 2016", "shortCiteRegEx": "Luan et al\\.", "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller."], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "Mnih et al\\.,? 2013", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Developing non-goal dialog system based on examples of drama television", "author": ["Lasguido Nio", "Sakriani Sakti", "Graham Neubig", "Tomoki Toda", "Mirna Adriani", "Satoshi Nakamura."], "venue": "Natural Interaction with Robots, Knowbots and Smartphones, pages 355\u2013361. Springer.", "citeRegEx": "Nio et al\\.,? 2014", "shortCiteRegEx": "Nio et al\\.", "year": 2014}, {"title": "Stochastic language generation for spoken dialogue systems", "author": ["Alice H Oh", "Alexander I Rudnicky."], "venue": "Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems-Volume 3, pages 27\u201332. Association for Computational Linguistics.", "citeRegEx": "Oh and Rudnicky.,? 2000", "shortCiteRegEx": "Oh and Rudnicky.", "year": 2000}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computa-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Are we there yet? research in commercial spoken dialog systems", "author": ["Roberto Pieraccini", "David Suendermann", "Krishna Dayanidhi", "Jackson Liscombe."], "venue": "Text, Speech and Dialogue, pages 3\u201313. Springer.", "citeRegEx": "Pieraccini et al\\.,? 2009", "shortCiteRegEx": "Pieraccini et al\\.", "year": 2009}, {"title": "Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Trainable approaches to surface natural language generation and their application to conversational dialog systems", "author": ["Adwait Ratnaparkhi."], "venue": "Computer Speech & Language, 16(3):435\u2013455.", "citeRegEx": "Ratnaparkhi.,? 2002", "shortCiteRegEx": "Ratnaparkhi.", "year": 2002}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the conference on empirical methods in natural language processing, pages 583\u2013593. Association for Computational Linguistics.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies", "author": ["Jost Schatzmann", "Karl Weilhammer", "Matt Stuttle", "Steve Young."], "venue": "The knowledge engineering review, 21(02):97\u2013126.", "citeRegEx": "Schatzmann et al\\.,? 2006", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2006}, {"title": "Opening up closings", "author": ["Emanuel A. Schegloff", "Harvey Sacks."], "venue": "Semiotica, 8(4):289\u2013327.", "citeRegEx": "Schegloff and Sacks.,? 1973", "shortCiteRegEx": "Schegloff and Sacks.", "year": 1973}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1507.04808.", "citeRegEx": "Serban et al\\.,? 2015a", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1507.04808.", "citeRegEx": "Serban et al\\.,? 2015b", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "A survey of available corpora for building data-driven dialogue systems", "author": ["Iulian Vlad Serban", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1512.05742.", "citeRegEx": "Serban et al\\.,? 2015c", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "arXiv preprint arXiv:1503.02364.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Reinforcement learning for spoken dialogue systems", "author": ["Satinder P Singh", "Michael J Kearns", "Diane J Litman", "Marilyn A Walker."], "venue": "Nips, pages 956\u2013962.", "citeRegEx": "Singh et al\\.,? 1999", "shortCiteRegEx": "Singh et al\\.", "year": 1999}, {"title": "Empirical evaluation of a reinforcement learning spoken dialogue system", "author": ["Satinder Singh", "Michael Kearns", "Diane J Litman", "Marilyn A Walker"], "venue": "In AAAI/IAAI,", "citeRegEx": "Singh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2000}, {"title": "Optimizing dialogue management with reinforcement learning: Experiments with the njfun system", "author": ["Satinder Singh", "Diane Litman", "Michael Kearns", "Marilyn Walker."], "venue": "Journal of Artificial Intelligence Research, pages 105\u2013133.", "citeRegEx": "Singh et al\\.,? 2002", "shortCiteRegEx": "Singh et al\\.", "year": 2002}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1506.06714.", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour"], "venue": "In NIPS,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "A trainable generator for recommendations in multimodal dialog", "author": ["Marilyn A Walker", "Rashmi Prasad", "Amanda Stent."], "venue": "INTERSPEECH. Citeseer.", "citeRegEx": "Walker et al\\.,? 2003", "shortCiteRegEx": "Walker et al\\.", "year": 2003}, {"title": "An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email", "author": ["Marilyn A. Walker."], "venue": "Journal of Artificial Intelligence Research, pages 387\u2013416.", "citeRegEx": "Walker.,? 2000", "shortCiteRegEx": "Walker.", "year": 2000}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-Hao Su", "David Vandyke", "Steve Young."], "venue": "arXiv preprint arXiv:1508.01745.", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young."], "venue": "arXiv preprint arXiv:1604.04562.", "citeRegEx": "Wen et al\\.,? 2016", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning, 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Incorporating loose-structured knowledge into lstm with recall gate for conversation modeling", "author": ["Zhen Xu", "Bingquan Liu", "Baoxun Wang", "Chengjie Sun", "Xiaolong Wang."], "venue": "arXiv preprint arXiv:1605.05110.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Attention with intention for a neural network conversation model", "author": ["Kaisheng Yao", "Geoffrey Zweig", "Baolin Peng."], "venue": "arXiv preprint arXiv:1510.08565.", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "The hidden information state model: A practical framework for pomdp-based spoken dialogue management", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Simon Keizer", "Fran\u00e7ois Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu."], "venue": "Computer Speech & Language, 24(2):150\u2013", "citeRegEx": "Young et al\\.,? 2010", "shortCiteRegEx": "Young et al\\.", "year": 2010}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["Stephanie Young", "Milica Gasic", "Blaise Thomson", "John D Williams."], "venue": "Proceedings of the IEEE, 101(5):1160\u20131179.", "citeRegEx": "Young et al\\.,? 2013", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1505.00521.", "citeRegEx": "Zaremba and Sutskever.,? 2015", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Neural response generation (Li et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Wen et al., 2015; Shang et al., 2015; Yao et al., 2015; Xu et al., 2016; Wen et al., 2016; Li et al., 2016) is of growing interest.", "startOffset": 27, "endOffset": 193}, {"referenceID": 30, "context": "Neural response generation (Li et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Wen et al., 2015; Shang et al., 2015; Yao et al., 2015; Xu et al., 2016; Wen et al., 2016; Li et al., 2016) is of growing interest.", "startOffset": 27, "endOffset": 193}, {"referenceID": 9, "context": "Neural response generation (Li et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Wen et al., 2015; Shang et al., 2015; Yao et al., 2015; Xu et al., 2016; Wen et al., 2016; Li et al., 2016) is of growing interest.", "startOffset": 27, "endOffset": 193}, {"referenceID": 33, "context": "Neural response generation (Li et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Wen et al., 2015; Shang et al., 2015; Yao et al., 2015; Xu et al., 2016; Wen et al., 2016; Li et al., 2016) is of growing interest.", "startOffset": 27, "endOffset": 193}, {"referenceID": 23, "context": "Neural response generation (Li et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Wen et al., 2015; Shang et al., 2015; Yao et al., 2015; Xu et al., 2016; Wen et al., 2016; Li et al., 2016) is of growing interest.", "startOffset": 27, "endOffset": 193}, {"referenceID": 37, "context": "Neural response generation (Li et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Wen et al., 2015; Shang et al., 2015; Yao et al., 2015; Xu et al., 2016; Wen et al., 2016; Li et al., 2016) is of growing interest.", "startOffset": 27, "endOffset": 193}, {"referenceID": 36, "context": "Neural response generation (Li et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Wen et al., 2015; Shang et al., 2015; Yao et al., 2015; Xu et al., 2016; Wen et al., 2016; Li et al., 2016) is of growing interest.", "startOffset": 27, "endOffset": 193}, {"referenceID": 34, "context": "Neural response generation (Li et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Wen et al., 2015; Shang et al., 2015; Yao et al., 2015; Xu et al., 2016; Wen et al., 2016; Li et al., 2016) is of growing interest.", "startOffset": 27, "endOffset": 193}, {"referenceID": 7, "context": "Neural response generation (Li et al., 2015; Vinyals and Le, 2015; Luan et al., 2016; Wen et al., 2015; Shang et al., 2015; Yao et al., 2015; Xu et al., 2016; Wen et al., 2016; Li et al., 2016) is of growing interest.", "startOffset": 27, "endOffset": 193}, {"referenceID": 28, "context": "The LSTM sequence-to-sequence (SEQ2SEQ) model (Sutskever et al., 2014) is one type of neural generation model that maximizes the probability of generating a response given the previous dialogue turn.", "startOffset": 46, "endOffset": 70}, {"referenceID": 27, "context": "This approach enables the incorporation of rich context when mapping between consecutive dialogue turns (Sordoni et al., 2015) in a way not possible, for example, with MT-based dialogue models (Ritter et al.", "startOffset": 104, "endOffset": 126}, {"referenceID": 17, "context": ", 2015) in a way not possible, for example, with MT-based dialogue models (Ritter et al., 2011).", "startOffset": 74, "endOffset": 95}, {"referenceID": 27, "context": "One concrete example is that SEQ2SEQ models tend to generate highly generic responses such as\u201cI don\u2019t know\u201d regardless of the input (Sordoni et al., 2015; Serban et al., 2015b; Serban et al., 2015c; Li et al., 2015).", "startOffset": 132, "endOffset": 215}, {"referenceID": 21, "context": "One concrete example is that SEQ2SEQ models tend to generate highly generic responses such as\u201cI don\u2019t know\u201d regardless of the input (Sordoni et al., 2015; Serban et al., 2015b; Serban et al., 2015c; Li et al., 2015).", "startOffset": 132, "endOffset": 215}, {"referenceID": 22, "context": "One concrete example is that SEQ2SEQ models tend to generate highly generic responses such as\u201cI don\u2019t know\u201d regardless of the input (Sordoni et al., 2015; Serban et al., 2015b; Serban et al., 2015c; Li et al., 2015).", "startOffset": 132, "endOffset": 215}, {"referenceID": 6, "context": "One concrete example is that SEQ2SEQ models tend to generate highly generic responses such as\u201cI don\u2019t know\u201d regardless of the input (Sordoni et al., 2015; Serban et al., 2015b; Serban et al., 2015c; Li et al., 2015).", "startOffset": 132, "endOffset": 215}, {"referenceID": 6, "context": "Baseline mutual information model (Li et al. 2015) Proposed reinforcement learning model A: Where are you going ? (1) A: Where are you going ? (1) B: I \u2019m going to the restroom .", "startOffset": 34, "endOffset": 50}, {"referenceID": 6, "context": "The output is generated using the mutual information model (Li et al., 2015) in which an N-best list is first obtained from beam search based on p(t|s) and reranked by linearly combining the backward probability p(s|t), where t and s respectively denote targets and sources.", "startOffset": 59, "endOffset": 76}, {"referenceID": 0, "context": "We define simple heuristic approximations to rewards that characterize good conversations: good conversations are forward-looking (Allwood et al., 1992) or interactive (a turn suggests a following turn), informative, and coherent.", "startOffset": 130, "endOffset": 152}, {"referenceID": 35, "context": "The agent learns a policy by optimizing the long-term developer-defined reward from ongoing dialogue simulations using policy gradient methods (Williams, 1992), rather than the MLE objective defined in standard SEQ2SEQ models.", "startOffset": 143, "endOffset": 159}, {"referenceID": 17, "context": "Ritter et al. (2011) frames the response generation problem as a statistical machine translation (SMT) problem.", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "Ritter et al. (2011) frames the response generation problem as a statistical machine translation (SMT) problem. Sordoni et al. (2015) improved Ritter et al.", "startOffset": 0, "endOffset": 134}, {"referenceID": 30, "context": "Recent progress in SEQ2SEQ models inspire several efforts (Vinyals and Le, 2015) to build endto-end conversational systems which first apply an encoder to map a message to a distributed vector representing its semantics and generate a response from the message vector.", "startOffset": 58, "endOffset": 80}, {"referenceID": 18, "context": "Serban et al. (2015a) propose a hierarchical neural model that captures dependencies over an extended conversation history.", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "Li et al. (2015) propose mutual information between message and response as an alternative objective function in order to reduce the proportion of generic responses produced by SEQ2SEQ systems.", "startOffset": 0, "endOffset": 17}, {"referenceID": 4, "context": "Efforts include statistical models including Markov Decision Processes (MDPs) (Levin et al., 1997; Levin et al., 2000; Walker et al., 2003; Pieraccini et al., 2009), POMDP (Young et al.", "startOffset": 78, "endOffset": 164}, {"referenceID": 5, "context": "Efforts include statistical models including Markov Decision Processes (MDPs) (Levin et al., 1997; Levin et al., 2000; Walker et al., 2003; Pieraccini et al., 2009), POMDP (Young et al.", "startOffset": 78, "endOffset": 164}, {"referenceID": 31, "context": "Efforts include statistical models including Markov Decision Processes (MDPs) (Levin et al., 1997; Levin et al., 2000; Walker et al., 2003; Pieraccini et al., 2009), POMDP (Young et al.", "startOffset": 78, "endOffset": 164}, {"referenceID": 14, "context": "Efforts include statistical models including Markov Decision Processes (MDPs) (Levin et al., 1997; Levin et al., 2000; Walker et al., 2003; Pieraccini et al., 2009), POMDP (Young et al.", "startOffset": 78, "endOffset": 164}, {"referenceID": 38, "context": ", 2009), POMDP (Young et al., 2010; Young et al., 2013) models, and models that statistically learn generation rules (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Nio et al.", "startOffset": 15, "endOffset": 55}, {"referenceID": 39, "context": ", 2009), POMDP (Young et al., 2010; Young et al., 2013) models, and models that statistically learn generation rules (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Nio et al.", "startOffset": 15, "endOffset": 55}, {"referenceID": 12, "context": ", 2013) models, and models that statistically learn generation rules (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Nio et al., 2014).", "startOffset": 69, "endOffset": 150}, {"referenceID": 16, "context": ", 2013) models, and models that statistically learn generation rules (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Nio et al., 2014).", "startOffset": 69, "endOffset": 150}, {"referenceID": 1, "context": ", 2013) models, and models that statistically learn generation rules (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Nio et al., 2014).", "startOffset": 69, "endOffset": 150}, {"referenceID": 11, "context": ", 2013) models, and models that statistically learn generation rules (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Nio et al., 2014).", "startOffset": 69, "endOffset": 150}, {"referenceID": 32, "context": "This dialogue literature thus widely appliess reinforcement learning (Walker, 2000; Schatzmann et al., 2006; Gasic et al., 2013; Singh et al., 1999; Singh et al., 2000; Singh et al., 2002) to train dialogue policies.", "startOffset": 69, "endOffset": 188}, {"referenceID": 18, "context": "This dialogue literature thus widely appliess reinforcement learning (Walker, 2000; Schatzmann et al., 2006; Gasic et al., 2013; Singh et al., 1999; Singh et al., 2000; Singh et al., 2002) to train dialogue policies.", "startOffset": 69, "endOffset": 188}, {"referenceID": 3, "context": "This dialogue literature thus widely appliess reinforcement learning (Walker, 2000; Schatzmann et al., 2006; Gasic et al., 2013; Singh et al., 1999; Singh et al., 2000; Singh et al., 2002) to train dialogue policies.", "startOffset": 69, "endOffset": 188}, {"referenceID": 24, "context": "This dialogue literature thus widely appliess reinforcement learning (Walker, 2000; Schatzmann et al., 2006; Gasic et al., 2013; Singh et al., 1999; Singh et al., 2000; Singh et al., 2002) to train dialogue policies.", "startOffset": 69, "endOffset": 188}, {"referenceID": 25, "context": "This dialogue literature thus widely appliess reinforcement learning (Walker, 2000; Schatzmann et al., 2006; Gasic et al., 2013; Singh et al., 1999; Singh et al., 2000; Singh et al., 2002) to train dialogue policies.", "startOffset": 69, "endOffset": 188}, {"referenceID": 26, "context": "This dialogue literature thus widely appliess reinforcement learning (Walker, 2000; Schatzmann et al., 2006; Gasic et al., 2013; Singh et al., 1999; Singh et al., 2000; Singh et al., 2002) to train dialogue policies.", "startOffset": 69, "endOffset": 188}, {"referenceID": 33, "context": "We are thus inspired by recent work like Wen et al. (2016) trains an endto-end task-oriented dialogue system that links input representations to slot-value pairs in a database.", "startOffset": 41, "endOffset": 59}, {"referenceID": 10, "context": "Policy gradient methods are more appropriate for our scenario than Q-learning (Mnih et al., 2013), because we can initialize the encoder-decoder RNN using MLE parameters that already produce plausible responses, before changing the objective and tuning towards a policy that maximizes long-term reward.", "startOffset": 78, "endOffset": 97}, {"referenceID": 6, "context": "The dialogue history is further transformed to a vector representation by feeding the concatenation of pi and qi into an LSTM encoder model as described in Li et al. (2015).", "startOffset": 156, "endOffset": 173}, {"referenceID": 19, "context": "This aspect of a turn is related to its forward-looking function: the constraints a turn places on the next turn (Schegloff and Sacks, 1973; Allwood et al., 1992).", "startOffset": 113, "endOffset": 162}, {"referenceID": 0, "context": "This aspect of a turn is related to its forward-looking function: the constraints a turn places on the next turn (Schegloff and Sacks, 1973; Allwood et al., 1992).", "startOffset": 113, "endOffset": 162}, {"referenceID": 30, "context": "For the first stage of training, we build on prior work of predicting a generated target sequence given dialogue history from the supervised SEQ2SEQ model (Vinyals and Le, 2015).", "startOffset": 155, "endOffset": 177}, {"referenceID": 6, "context": ", \u201ci don\u2019t know\u201d (Li et al., 2015) We thus do not want to initialize the policy model", "startOffset": 17, "endOffset": 34}, {"referenceID": 6, "context": "Li et al. (2015) showed that modeling mutual information between sources and targets will significantly decrease the chance of generating dull responses and improve general response quality.", "startOffset": 0, "endOffset": 17}, {"referenceID": 15, "context": "Inspired by recent work on sequence level learning (Ranzato et al., 2015), we treat the problem of generating maximum mutual information response as a reinforcement learning problem in which a reward of mutual information value is observed when the model arrives at the end of a sequence.", "startOffset": 51, "endOffset": 73}, {"referenceID": 6, "context": "As illustrated in Li et al. (2015), direct decoding from Eq 3 is infeasible since the second term requires the target sentence to be completely generated.", "startOffset": 18, "endOffset": 35}, {"referenceID": 29, "context": "(2015), we use policy gradient methods (Sutton et al., 1999; Williams, 1992) for optimization.", "startOffset": 39, "endOffset": 76}, {"referenceID": 35, "context": "(2015), we use policy gradient methods (Sutton et al., 1999; Williams, 1992) for optimization.", "startOffset": 39, "endOffset": 76}, {"referenceID": 15, "context": "Similar to Ranzato et al. (2015), we use policy gradient methods (Sutton et al.", "startOffset": 11, "endOffset": 33}, {"referenceID": 15, "context": "Similar to Ranzato et al. (2015), we use policy gradient methods (Sutton et al., 1999; Williams, 1992) for optimization. We initialize the policy model pRL using a pre-trained pSEQ2SEQ(a|pi, qi) model. Given an input source [pi, qi], we generate a candidate list A = {\u00e2|\u00e2 \u223c pRL}. For each generated candidate \u00e2, we will obtain the mutual information score m(\u00e2, [pi, qi]) from the pre-trained pSEQ2SEQ(a|pi, qi) and pbackward SEQ2SEQ(qi|a). This mutual information score will be used as reward and back propagate to the encoder-decoder model, tailoring it to generate sequences with higher rewards. We refer the readers to Zaremba and Sutskever (2015) and Williams (1992) for details.", "startOffset": 11, "endOffset": 651}, {"referenceID": 15, "context": "Similar to Ranzato et al. (2015), we use policy gradient methods (Sutton et al., 1999; Williams, 1992) for optimization. We initialize the policy model pRL using a pre-trained pSEQ2SEQ(a|pi, qi) model. Given an input source [pi, qi], we generate a candidate list A = {\u00e2|\u00e2 \u223c pRL}. For each generated candidate \u00e2, we will obtain the mutual information score m(\u00e2, [pi, qi]) from the pre-trained pSEQ2SEQ(a|pi, qi) and pbackward SEQ2SEQ(qi|a). This mutual information score will be used as reward and back propagate to the encoder-decoder model, tailoring it to generate sequences with higher rewards. We refer the readers to Zaremba and Sutskever (2015) and Williams (1992) for details.", "startOffset": 11, "endOffset": 671}, {"referenceID": 13, "context": "Metrics such as BLEU (Papineni et al., 2002) and perplexity have been widely used for dialogue quality evaluation (Li et al.", "startOffset": 21, "endOffset": 44}, {"referenceID": 6, "context": ", 2002) and perplexity have been widely used for dialogue quality evaluation (Li et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015), but it is widely debated how well these automatic metrics are correlated with true response quality (Liu et al.", "startOffset": 77, "endOffset": 138}, {"referenceID": 30, "context": ", 2002) and perplexity have been widely used for dialogue quality evaluation (Li et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015), but it is widely debated how well these automatic metrics are correlated with true response quality (Liu et al.", "startOffset": 77, "endOffset": 138}, {"referenceID": 27, "context": ", 2002) and perplexity have been widely used for dialogue quality evaluation (Li et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015), but it is widely debated how well these automatic metrics are correlated with true response quality (Liu et al.", "startOffset": 77, "endOffset": 138}, {"referenceID": 8, "context": ", 2015), but it is widely debated how well these automatic metrics are correlated with true response quality (Liu et al., 2016; Galley et al., 2015).", "startOffset": 109, "endOffset": 148}, {"referenceID": 2, "context": ", 2015), but it is widely debated how well these automatic metrics are correlated with true response quality (Liu et al., 2016; Galley et al., 2015).", "startOffset": 109, "endOffset": 148}, {"referenceID": 6, "context": "The value is scaled by the total number of generated tokens to avoid favoring long sentences as described in (Li et al., 2015).", "startOffset": 109, "endOffset": 126}, {"referenceID": 6, "context": "Human Evaluation We explore three settings for human evaluation: the first setting is similar to what was described in Li et al. (2015), where we employ crowdsourced judges to evaluate a random sample of 500 items.", "startOffset": 119, "endOffset": 136}], "year": 2017, "abstractText": "Recent neural models of dialogue generation offer great promise for generating responses for conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the future direction of a dialogue is crucial to generating coherent, interesting dialogues, a need which led traditional NLP models of dialogue to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to model future reward in chatbot dialogue. The model simulates dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity (non-repetitive turns), coherence, and ease of answering (related to forward-looking function). We evaluate our model on diversity, length as well as with human judges, showing that the proposed algorithm generates more interactive responses and manages to foster a more sustained conversation in dialogue simulation. This work marks a first step towards learning a neural conversational model based on the long-term success of dialogues.", "creator": "LaTeX with hyperref package"}}}