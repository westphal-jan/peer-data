{"id": "1606.02270", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Natural Language Comprehension with the EpiReader", "abstract": "we present simple mouse, a novel model for computer comprehension of text. simultaneous comprehension of unstructured, real - world text contains a possible research goal for natural resources production. serial modeling of machine comprehension compares objects whose answers regularly receive input past some supporting text, and evaluate relevant developer's responses to what idea. numerous challenges propose an array - so - many neural solution comprising 14 components : the first role proposes a small amount of realistic alternatives after comparing a question ( possible supporting theme, versus fourth outer component formulates responses. the above concept and the conditions, further create the scenarios based on their estimated effect with the supporting text. we present experiments demonstrating that novel epireader sets a new state - of - the - art on the adults and children's hospital based machine comprehension project, outperforming previous neural techniques by implying significant effect.", "histories": [["v1", "Tue, 7 Jun 2016 19:27:04 GMT  (92kb,D)", "http://arxiv.org/abs/1606.02270v1", "8 pages plus references. Submitted to EMNLP 2016"], ["v2", "Fri, 10 Jun 2016 15:43:51 GMT  (92kb,D)", "http://arxiv.org/abs/1606.02270v2", "8 pages plus references. Submitted to EMNLP 2016"]], "COMMENTS": "8 pages plus references. Submitted to EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["adam trischler", "zheng ye", "xingdi yuan", "philip bachman", "alessandro sordoni", "kaheer suleman"], "accepted": true, "id": "1606.02270"}, "pdf": {"name": "1606.02270.pdf", "metadata": {"source": "CRF", "title": "Natural Language Comprehension with the EpiReader", "authors": ["Adam Trischler", "Zheng Ye jeff.ye", "Xingdi Yuan eric.yuan", "Kaheer Suleman"], "emails": ["k.suleman@maluuba.com"], "sections": [{"heading": "1 Introduction", "text": "When humans reason about the world, we tend to formulate a variety of hypotheses and counterfactuals, then test them in turn by physical or thought experiments. The philosopher Epicurus first formalized this idea in his Principle of Multiple Explanations: if several theories are consistent with the observed data, retain them all until more data is observed. In this paper, we argue that the same principle can be applied to machine comprehension of natural language. We propose a deep, end-to-end, neural comprehension model that we call the EpiReader.\nComprehension of natural language by machines, at a near-human level, is a prerequisite for an extremely broad class of useful applications of artificial intelligence. Indeed, most human knowledge is collected in the natural language of text. Machine comprehension (MC) has therefore garnered significant attention from the machine learning research community. Machine comprehension is typically evaluated by posing a set of questions based on a supporting text passage, then scoring a system\u2019s answers to those questions. We all took similar tests in school. Such tests are objectively gradable and may assess a range of abilities, from basic understanding to causal reasoning to inference (Richardson et al., 2013).\nIn the past year, two large-scale MC datasets have been released: the CNN/Daily Mail corpus, consisting of news articles from those outlets (Hermann et al., 2015), and the Children\u2019s Book Test (CBT), consisting of short excerpts from books available through Project Gutenberg (Hill et al., 2015). The size of these datasets (on the order of 105 distinct questions) makes them amenable to data-intensive deep learning techniques. Both corpora use Clozestyle questions (Taylor, 1953), which are formulated by replacing a word or phrase in a given sentence with a placeholder token. The task is then to find the answer that \u201cfills in the blank\u201d.\nIn tandem with these corpora, a host of neural machine comprehension models has been developed (Weston et al., 2014; Hermann et al., 2015; Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016). We compare the EpiReader to these earlier models through training and evaluation on the CNN and CBT\nar X\niv :1\n60 6.\n02 27\n0v 1\n[ cs\n.C L\n] 7\nJ un\n2 01\ndatasets.1\nThe EpiReader factors into two components. The first component extracts a small set of potential answers based on a shallow comparison of the question with its supporting text; we call this the Extractor. The second component reranks the proposed answers based on deeper semantic comparisons with the text; we call this the Reasoner. We can summarize this process as Extract \u2192 Hypothesize \u2192 Test2. The semantic comparisons implemented by the Reasoner are based on the concept of recognizing textual entailment (RTE) (Dagan et al., 2006), also known as natural language inference. This process is computationally demanding. Thus, the Extractor serves the important function of filtering a large set of potential answers down to a small, tractable set of likely candidates for more thorough testing.\nThe Extractor follows the form of a pointer network (Vinyals et al., 2015), and uses a differentiable attention mechanism to indicate words in the text that potentially answer the question. This approach was used (on its own) for question answering with the Attention Sum Reader (Kadlec et al., 2016). The Extractor outputs a small set of answer candidates along with their estimated probabilities of correctness. The Reasoner forms hypotheses by inserting the candidate answers into the question, then estimates the concordance of each hypothesis with each sentence in the supporting text. We use these estimates as a measure of the evidence for a hypothesis, and aggregate evidence over all sentences. In the end, we combine the Reasoner\u2019s evidence with the Extractor\u2019s probability estimates to produce a final ranking of the answer candidates.\nThis paper is organized as follows. In Section 2 we formally define the problem to be solved and give some background on the datasets used in our tests. In Section 3 we describe the EpiReader, focusing on its two components and how they combine. Section 4 discusses related work, and Section 5 details our experimental results and analysis. We conclude in Section 6.\n1The CNN and Daily Mail datasets were released together and have the same form. The Daily Mail dataset is significantly larger, and our tests with this data are still in progress.\n2The Extractor performs extraction, while the Reasoner both hypothesizes and tests."}, {"heading": "2 Problem definition, notation, datasets", "text": "The task of the EpiReader is to answer a Cloze-style question by reading and comprehending a supporting passage of text. The training and evaluation data consist of tuples (Q, T , a\u2217, A), where Q is the question (a sequence of words {q1, ...q|Q|}), T is the text (a sequence of words {t1, ..., t|T |}), A is a set of possible answers {a1, ..., a|A|}, and a\u2217 \u2208 A is the correct answer. All words come from a vocabulary V , and A \u2282 T . In each question, there is a placeholder token indicating the missing word to be filled in."}, {"heading": "2.1 Datasets", "text": "CNN This corpus is built using articles scraped from the CNN website. The articles themselves form the text passages, and questions are generated synthetically from short summary statements that accompany each article. These summary points are (presumably) written by human authors. Each question is created by replacing a named entity in a summary point with a placeholder token. All named entities in the articles and questions are replaced with anonymized tokens that are shuffled for each (Q, T ) pair. This forces the model to rely only on the text, rather than learning world knowledge about the entities during training. The CNN corpus (henceforth CNN) was presented by Hermann et al. (2015).\nChildren\u2019s Book Test This corpus is constructed similarly to CNN, but from children\u2019s books available through Project Gutenberg. Rather than articles, the text passages come from book excerpts of 20 sentences. Since no summaries are provided, a question is generated by replacing a single word in the next (i.e. 21st) sentence. The corpus distinguishes questions based on the type of word that is replaced: named entity, common noun, verb, or preposition. Like Kadlec et al. (2016), we focus only on the first two classes since Hill et al. (2015) showed that standard LSTM language models already achieve humanlevel performance on the latter two. Unlike in the CNN corpora, named entities are not anonymized and shuffled in the Children\u2019s Book Test (CBT). The CBT was presented by Hill et al. (2015).\nDue to the construction of questions in each corpus, CNN and CBT assess different aspects of machine comprehension. The summary points of CNN are\ncondensed paraphrasings of information from the text, so determining the correct answer relies more on recognizing textual entailment. On the other hand, a CBT question, generated from a sentence which continues the text passage (rather than summarizes it), is more about making a prediction based on context."}, {"heading": "3 The EpiReader", "text": ""}, {"heading": "3.1 Overview and intuition", "text": "The EpiReader explicitly leverages the observation that the answer to a question is often a word or phrase from the related text passage. This condition holds for the CNN and CBT datasets. EpiReader\u2019s first module, the Extractor, can thus select a small set of candidate answers by pointing to their locations in the supporting passage. This mechanism is detailed in Section 3.2, and was used previously by the Attention Sum Reader (Kadlec et al., 2016). Pointing to candidate answers removes the need to apply a softmax over the entire vocabulary as in Weston et al. (2014), which is computationally more costly and uses less-direct information about the context of a predicted answer in the supporting text.\nEpiReader\u2019s second module, the Reasoner, begins by formulating hypotheses using the extracted answer candidates. It generates each hypothesis by replacing the placeholder token in the question with an answer candidate. Cloze-style questions are ideally-suited to this process, because inserting the correct answer at the placeholder location produces a well-formed, grammatical statement. Thus, the correct hypothesis will \u201cmake sense\u201d to a language model.\nThe Reasoner then tests each hypothesis individually. It compares a hypothesis to the text, split into sentences, to measure textual entailment, and then aggregates entailment over all sentences. This computation uses a pair of convolutional encoder networks followed by a recurrent neural network. The convolutional encoders generate abstract representations of the hypothesis and each text sentence; the recurrent network estimates and aggregates entailment. This is described formally in Section 3.3. The end-toend EpiReader model, combining the Extractor and Reasoner modules, is depicted in Figure 1.\nThroughout our model, words will be represented with trainable embeddings (Bengio et al., 2000). We represent these embeddings using a matrix W \u2208\nRD\u00d7|V |, where D is the embedding dimension and |V | is the vocabulary size."}, {"heading": "3.2 The Extractor", "text": "The Extractor is a Pointer Network (Vinyals et al., 2015). It uses a pair of bidirectional recurrent neural networks, f(\u03b8T ,T) and g(\u03b8Q,Q), to encode the text passage and the question. \u03b8T represents the parameters of the text encoder, and T \u2208 RD\u00d7N is a matrix representation of the text (comprising N words), whose columns are individual word embeddings ti. Likewise, \u03b8Q represents the parameters of the question encoder, and Q \u2208 RD\u00d7NQ is a matrix representation of the question (comprisingNQ words), whose columns are individual word embeddings qj .\nWe use a recurrent neural network with gated recurrent units (GRU) (Bahdanau et al., 2014) to scan over the columns (i.e. word embeddings) of the input matrix. We selected the GRU because it is computationally simpler than Long Short-Term Memory (Hochreiter and Schmidhuber, 1997), while still avoiding the problem of vanishing/exploding gradients often encountered when training recurrent networks.\nThe GRU\u2019s hidden state gives a representation of the ith word conditioned on preceding words. To include context from proceeding words, we run a second GRU over T in the reverse direction. We refer to the combination as a biGRU. At each step the biGRU outputs two d-dimensional encoding vectors, one for the forward direction and one for the backward direction. We concatenate these to yield a vector f(ti) \u2208 R2d. The question biGRU is similar, but we get a single-vector representation of the question by concatenating the final forward state with the initial backward state, which we denote g(Q) \u2208 R2d.\nAs in Kadlec et al. (2016), we model the probability that the ith word in text T answers question Q using\nsi \u221d exp(f(ti) \u00b7 g(Q)), (1)\nwhich takes the inner product of the text and question representations followed by a softmax. In many cases unique words repeat in a text. Therefore, we compute the total probability that word w is the correct answer using a sum:\nP (w | T ,Q) = \u2211\ni: ti=w\nsi. (2)\nThis probability is evaluated for each unique word in T . Finally, the Extractor outputs the set {p1, ..., pK} of the K highest word probabilities from 2, along with the corresponding set of K most probable answer words {a\u03021, ..., a\u0302K}."}, {"heading": "3.3 The Reasoner", "text": "The indicial selection involved in gathering {a\u03021, ..., a\u0302K} is not a smooth operation. To construct an end-to-end differentiable model, we bypass this by propagating the probability estimates of the Extractor directly through the Reasoner.\nThe Reasoner begins by inserting the answer candidates, which are single words or phrases, into the\nquestion sequence Q at the placeholder location. This forms K hypotheses {H1, ...,HK}. At this point, we consider each hypothesis to have probability p(Hk) \u2248 pk, as estimated by the Extractor. The Reasoner updates and refines this estimate.\nThe hypotheses represent new information in some sense\u2014they are statements we have constructed, albeit from words already present in the question and text passage. The Reasoner estimates entailment between the statements Hk and the passage T . We denote these estimates using ek = F (Hk, T ), with F to be defined. We start by reorganizing T into a sequence of Ns sentences: T = {t1, . . . , tN} \u2192 {S1, . . . ,SNs}, where Si is a sequence of words.\nFor each hypothesis and each sentence of the text, Reasoner input consists of two matrices: Si \u2208 RD\u00d7|Si|, whose columns are the embedding vectors for each word of sentence Si, and Hk \u2208 RD\u00d7|Hk|, whose columns are the embedding vectors for each word in the hypothesisHk. The embedding vectors themselves come from matrix W, as before.\nThese matrices feed into a convolutional architecture based on that of Severyn and Moschitti (2016). The architecture first augments Si with matrix M \u2208 R2\u00d7|Si|. The first row of M contains the inner product of each word embedding in the sentence with the candidate answer embedding, and the second row contains the maximum inner product of each sentence word embedding with any word embedding in the question. These word-matching features were inspired by similar approaches in Wang and Jiang (2015) and Trischler et al. (2016), where they were shown to improve entailment estimates.\nThe augmented Si is then convolved with a bank of filters FS \u2208 R(D+2)\u00d7m, while Hk is convolved with filters FH \u2208 RD\u00d7m, where m is the convolutional filter width. We add a bias term and apply a nonlinearity (we use a ReLU) following the convolution. Maxpooling over the sequences then yields two vectors: the representation of the text sentence, rSi \u2208 RNF , and the representation of the hypothesis, rHk \u2208 RNF , where NF is the number of filters.\nWe then compute a scalar similarity score between these vector representations using the bilinear form\n\u03c2 = rTSiRrHk , (3)\nwhere R \u2208 RNF\u00d7NF is a matrix of trainable parameters. We then concatenate the similarity score with the sentence and hypothesis representations to get a vector, xik = [\u03c2; rSi ; rHk ]\nT . There are more powerful models of textual entailment that could have been used in place of this convolutional architecture. We adopted the approach of Severyn and Moschitti (2016) for computational efficiency.\nThe resulting sequence of Ns vectors feeds into yet another GRU for synthesis, of hidden dimension dS . Intuitively, it is often the case that evidence for a particular hypothesis is distributed over several sentences. For instance, if we hypothesize that the football is in the park, perhaps it is because one sentence tells us that Sam picked up the football and a\nlater one tells us that Sam ran to the park.3 The Reasoner synthesizes distributed information by running a GRU network over xik, where i indexes sentences and represents the step dimension.4 The final hidden state of the GRU is fed through a fully-connected layer, yielding a single scalar yk. This value represents the collected evidence forHk based on the text. In practice, the Reasoner processes all K hypotheses in parallel and the estimated entailment of each is normalized by a softmax, ek \u221d exp(yk).\nThe reranking step performed by the Reasoner helps mitigate a significant weakness of most existing attention mechanisms. Specifically, these mechanisms blend representations of all possible outcomes together using \u201csoft\u201d attention, rather than considering them discretely using \u201chard\u201d attention. This is like exploring a maze by generating an average path out of the several before you, and then attempting to follow it by walking through a wall. Examining possibilities individually, as in the Reasoner module, is more natural."}, {"heading": "3.4 Combining components", "text": "Finally, we combine the evidence from the Reasoner with the probability from the Extractor. We compute the output probability of each hypothesis, \u03c0k, according to the product\n\u03c0k \u221d ekpk, (4)\nwhereby the evidence of the Reasoner can be interpreted as a correction to the Extractor probabilities, applied as an additive shift in log-space. We experimented with other combinations of the Extractor and Reasoner, but we found the multiplicative approach to yield the best performance.\nAfter combining results from the Extractor and Reasoner to get the probabilities \u03c0k described in Eq. 4, we optimize the parameters of the full EpiReader to minimize a cost comprising two terms, LE and LR. The first term is a standard negative loglikelihood objective, which encourages the Extractor to rate the correct answer above other answers. This\n3This example is characteristic of the bAbI dataset (Weston et al., 2015).\n4Note a benefit of forming the hypothesis: it renders bidirectional aggregation unnecessary, since knowing both the question and the putative answer \"closes the loop\" the same way that a bidirectional encoding would.\nis the same loss term used in Kadlec et al. (2016). This loss is given by:\nLE = E (Q,T ,a\u2217,A)\n[\u2212 logP (a\u2217 | T ,Q)] , (5)\nwhere P (a\u2217 | T ,Q) is as defined in Eq. 2, and a\u2217 denotes the true answer. The second term is a marginbased loss on the end-to-end probabilities \u03c0k. We define \u03c0\u2217 as the probability \u03c0k corresponding to the true answer word a\u2217. This term is given by:\nLR = E (Q,T ,a\u2217,A)  \u2211 a\u0302i\u2208{a\u03021,...,a\u0302K}\\a\u2217 [\u03b3 \u2212 \u03c0\u2217 + \u03c0a\u0302i ]+  , (6) where \u03b3 is a margin hyperparameter, {a\u03021, ..., a\u0302K} is the set of K answers proposed by the Extractor, and [x]+ indicates truncating x to be non-negative. Intuitively, this loss says that we want the end-to-end probability \u03c0\u2217 for the correct answer to be at least \u03b3 larger than the probability \u03c0a\u0302i for any other answer proposed by the Extractor. During training, the correct answer is occasionally missed by the Extractor, especially in early epochs. We counter this issue by forcing the correct answer into the top K set while training. When evaluating the model on validation and test examples we rely fully on the top K answers proposed by the Extractor.\nTo get the final loss term LER, minus `2 regularization terms on the model parameters, we take a weighted combination of LE and LR:\nLER = LE + \u03bbLR, (7)\nwhere \u03bb is a hyperparameter for weighting the relative contribution of the Extractor and Reasoner losses. In practice, we found that \u03bb should be fairly large (e.g. 10 < \u03bb < 100). Empirically, we observed that the output probabilities from the Extractor often peak and saturate the first softmax; hence, the Extractor term can come to dominate the Reasoner term without the weight \u03bb (we discuss the Extractor\u2019s propensity to overfit in Section 5)."}, {"heading": "4 Related Work", "text": "The Impatient and Attentive Reader models were proposed by Hermann et al. (2015). The Attentive Reader applies bidirectional recurrent encoders to the\nquestion and supporting text. It then uses the attention mechanism described in Bahdanau et al. (2014) to compute a fixed-length representation of the text based on a weighted sum of the text encoder\u2019s output, guided by comparing the question representation to each location in the text. Finally, a joint representation of the question and supporting text is formed by passing their separate representations through a feedforward MLP and an answer is selected by comparing the MLP output to a representation of each possible answer. The Impatient Reader operates similarly, but computes attention over the text after processing each consecutive word of the question. The two models achieved similar performance on the CNN and Daily Mail datasets.\nMemory Networks were first proposed by Weston et al. (2014) and later applied to machine comprehension by Hill et al. (2015). This model builds fixed-length representations of the question and of windows of text surrounding each candidate answer, then uses a weighted-sum attention mechanism to combine the window representations. As in the previous Readers, the combined window representation is then compared with each possible answer to form a prediction about the best answer. What distinguishes Memory Networks is how they construct the question and text window representations. Rather than a recurrent network, they use a specially-designed, trainable transformation of the word embeddings.\nMost of the details for the very recent AS Reader are provided in the description of our Extractor module in Section 3.2, so we do not summarize it further here. This model (Kadlec et al., 2016) set the previous state-of-the-art on the CBT dataset.\nDuring the write-up of this paper, another very recent model came to our attention. Chen et al. (2016) propose using a bilinear term instead of a tanh layer to compute the attention between question and passage words, and also uses the attended word encodings for direct, pointer-style prediction as in Kadlec et al. (2016). This model set the previous state-of-theart on the CNN dataset. However, this model used embedding vectors pretrained on a large external corpus (Pennington et al., 2014).\nThe EpiReader borrows ideas from other models as well. The Reasoner\u2019s convolutional architecture is based on Severyn and Moschitti (2016) and Kalchbrenner et al. (2014). Our use of word-level match-\ning was inspired by the Parallel-Hierarchical model of Trischler et al. (2016) and the natural language inference model of Wang and Jiang (2015). Finally, the idea of formulating and testing hypotheses for question-answering was used to great effect in IBM\u2019s DeepQA system for Jeopardy! (Ferrucci et al., 2010), although that was a more traditional information retrieval pipeline rather than an end-to-end neural model."}, {"heading": "5 Evaluation", "text": ""}, {"heading": "5.1 Implementation and training details", "text": "To train our model we used stochastic gradient descent with the ADAM optimizer (Kingma and Ba, 2014), with an initial learning rate of 0.001. The word embeddings were initialized randomly, drawing from the uniform distribution over [\u22120.05, 0.05). We used batches of 32 examples, and early stopping with a patience of 2 epochs. Our model was implement in Theano (Bergstra et al., 2010) using the Keras framework (Chollet, 2015).\nThe results presented below for the EpiReader were obtained by searching over a small grid of hyperparameter settings. We selected the model that, on each dataset, maximized accuracy on the validation set, then evaluated it on the test set. We record the best settings for each dataset in Table 1. As has been done previously, we train separate models on CBT\u2019s named entity (CBT-NE) and common noun (CBTCN) splits. All our models used `2-regularization at 0.001, \u03bb = 50, and \u03b3 = 0.04. We did not use dropout but plan to investigate its effect in the future. Hill et al. (2015) and Kadlec et al. (2016) also present results for ensembles of their models. Time did not\npermit us to generate an ensemble of EpiReaders on the CNN dataset so we omit those measures; however, EpiReader ensembles (of seven models) demonstrated improved performance on the CBT dataset."}, {"heading": "5.2 Results", "text": "In Table 2, we compare the performance of the EpiReader against that of several baselines, on the validation and test sets of the CBT and CNN corpora. We measure EpiReader performance at the output of both the Extractor and the Reasoner. The EpiReader achieves state-of-the-art performance across the board for both datasets. On CNN, we score 2.2% higher on test than the best previous model of Chen et al. (2016). Interestingly, an analysis of the CNN dataset by Chen et al. (2016) suggests that approximately 25% of the test examples contain coreference errors or questions which are \u201cambiguous/hard\u201d even for a human analyst. If this estimate is accurate, then the EpiReader, achieving an absolute test accuracy of 74.0%, is operating close to expected human performance. On the other hand, ambiguity is unlikely to be distributed evenly over entities, so a good model should be able to perform at better-thanchance levels even on questions where the correct answer is uncertain. If, on the 25% of \u201cnoisy\u201d questions, the model can shift its hit rate from, e.g., 1/10 to 1/3, then there is still a fair amount of performance to gain.\nOn CBT-CN our single model scores 4.0% higher than the previous best of the AS Reader. The improvement on CBT-NE is more modest at 1.1%. Looking more closely at our CBT-NE results, we found that the validation and test accuracies had relatively high variance even in late epochs of training. We discovered that many of the validation and test questions were asked about the same named entity, which may explain this issue."}, {"heading": "5.3 Analysis", "text": "Aside from achieving state-of-the-art results at its final output, the EpiReader framework gives a boost to its Extractor component through the joint training process. In Table 2, we provide accuracy scores evaluated at the output of the Extractor. These are all higher than the analogous scores reported for the AS Reader, which we verified ourselves to within negligible difference. Based on our own work with that\nTable 2: Model comparison on the CBT and CNN datasets. Results marked with 1 are from Hill et al. (2016), those marked with 2 are from Kadlec et al. (2016), those marked with 3 are from Hermann et al. (2015), and those marked with 4 are from Chen et al. (2016).\nCBT-NE CBT-CN\nModel valid test valid test\nHumans (context + query) 1 - 81.6 - 81.6\nLSTMs (context + query) 1 51.2 41.8 62.6 56.0\nMemNNs 1 70.4 66.6 64.2 63.0\nAS Reader 2 73.8 68.6 68.8 63.4\nEpiReader Extractor 73.2 69.4 69.9 66.7 EpiReader 75.3 69.7 71.5 67.4\nAS Reader (ensemble) 2 74.5 70.6 71.1 68.9 EpiReader (ensemble) 76.6 71.8 73.6 70.6\nCNN\nModel valid test\nDeep LSTM Reader 3 55.0 57.0 Attentive Reader 3 61.6 63.0 Impatient Reader 3 61.8 63.8\nMemNNs 1 63.4 66.8\nAS Reader 2 68.6 69.5\nStanford AR 4 71.5 71.8\nEpiReader Extractor 71.8 72.0 EpiReader 73.4 74.0\nmodel, we found it to overfit the training set rapidly and significantly, achieving training accuracy scores upwards of 98% after only 2 epochs. We suspect that the Reasoner module had a regularizing effect on the Extractor, but leave the verification for future work. An analysis by Kadlec et al. (2016) indicates that the trained AS Reader includes the correct answer among its five most probable candidates on approximately 95% of test examples for both datasets. We verified that our Extractor achieved a similar rate, and of course this is vital for performance of the full system, since the Reasoner cannot recover when the correct answer is not among its inputs.\nOur results demonstrate that the Reasoner often corrects erroneous answers from the Extractor. Figure 2 gives an example of this correction. In the text passage, from CBT-NE, Mr. Blacksnake is pursuing Mr. Toad, presumably to eat him. The dialogue in the question sentence refers to both: Mr. Toad is its subject, referred to by the pronoun \u201che\u201d, and Mr. Blacksnake is its object. In the preceding sentences, it is clear (to a human) that Jimmy is worried about Mr. Toad and his potential encounter with Mr. Blacksnake. The Extractor, however, points most strongly to \u201cToad\u201d, possibly because he has been referred to most recently. The Reasoner corrects this error and selects \u201cBlacksnake\u201d as the answer. This relies on a deeper understanding of the text. The named entity can, in this case, be inferred through an alternation of the entities most recently referred to. This kind alternation is typical of dialogues, when two actors\nMr. Blacksnake grinned and started after him, not very fast because he knew that he wouldn't have to run very fast to catch old Mr. Toad, and he thought the exercise would do him good. \u2026\n\u201cStill, the green meadows wouldn't be quite the same without old Mr. Toad. I should miss him if anything happened to him. I suppose it would be partly my fault, too, for if I hadn't pulled over that piece of bark, he probably would have stayed there the rest of the day and been safe.\u201d\nQUESTION: \u201cMaybe he won't meet Mr. XXXXX,\u201d said a little voice inside of Jimmy.\nEXTRACTOR: Toad REASONER: Blacksnake\n1.\n18.\n21.\n19. 20.\nFigure 2: An abridged example from CBT-NE demonstrating corrective reranking by the Reasoner.\ninteract in turns. The Reasoner can capture this behavior because it examines sentences in sequence."}, {"heading": "6 Conclusion", "text": "In this article we presented the novel EpiReader framework for machine comprehension, and evaluated it on two large, complex datasets: CNN and CBT. Our model achieves state-of-the-art results on these corpora, outperforming all previous approaches. In future work, we plan to augment our framework with a more powerful model for natural language inference, and explore the effect of pretraining such a model specifically on an inference task. We also plan to try simplifying the model by reusing the Extractor\u2019s biGRU encodings in the Reasoner."}], "references": [{"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "R\u00e9jean Ducharme", "author": ["Yoshua Bengio"], "venue": "and Pascal Vincent.", "citeRegEx": "Bengio et al.2000", "shortCiteRegEx": null, "year": 2000}, {"title": "and Y", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley"], "venue": "Bengio.", "citeRegEx": "Bergstra et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Christopher D", "author": ["Danqi Chen", "Jason Bolton"], "venue": "Manning.", "citeRegEx": "Chen et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "keras. https: //github.com/fchollet/keras", "author": ["Fran\u00e7ois Chollet"], "venue": null, "citeRegEx": "Chollet.,? \\Q2015\\E", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Oren Glickman", "author": ["Ido Dagan"], "venue": "and Bernardo Magnini.", "citeRegEx": "Dagan et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "John Prager", "author": ["David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya A Kalyanpur", "Adam Lally", "J William Murdock", "Eric Nyberg"], "venue": "et al.", "citeRegEx": "Ferrucci et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Mustafa Suleyman", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay"], "venue": "and Phil Blunsom.", "citeRegEx": "Hermann et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sumit Chopra", "author": ["Felix Hill", "Antoine Bordes"], "venue": "and Jason Weston.", "citeRegEx": "Hill et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Ondrej Bajgar", "author": ["Rudolf Kadlec", "Martin Schmid"], "venue": "and Jan Kleindienst.", "citeRegEx": "Kadlec et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Edward Grefenstette", "author": ["Nal Kalchbrenner"], "venue": "and Phil Blunsom.", "citeRegEx": "Kalchbrenner et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Richard Socher", "author": ["Jeffrey Pennington"], "venue": "and Christopher D Manning.", "citeRegEx": "Pennington et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Christopher JC Burges", "author": ["Matthew Richardson"], "venue": "and Erin Renshaw.", "citeRegEx": "Richardson et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Modeling relational information in question-answer pairs with convolutional neural networks. arXiv preprint arXiv:1604.01178", "author": ["Severyn", "Moschitti2016] Aliaksei Severyn", "Alessandro Moschitti"], "venue": null, "citeRegEx": "Severyn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2016}, {"title": "Cloze procedure: a new tool for measuring readability", "author": ["Wilson L Taylor"], "venue": "Journalism and Mass Communication Quarterly,", "citeRegEx": "Taylor.,? \\Q1953\\E", "shortCiteRegEx": "Taylor.", "year": 1953}, {"title": "Philip Bachman", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Jing He"], "venue": "and Kaheer Suleman.", "citeRegEx": "Trischler et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "2015", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly"], "venue": "Pointer networks. In Advances in Neural Information Processing Systems, pages 2674\u2013", "citeRegEx": "Vinyals et al.2015", "shortCiteRegEx": null, "year": 2682}, {"title": "Learning natural language inference with lstm", "author": ["Wang", "Jiang2015] Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1512.08849", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Sumit Chopra", "author": ["Jason Weston"], "venue": "and Antoine Bordes.", "citeRegEx": "Weston et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Sumit Chopra", "author": ["Jason Weston", "Antoine Bordes"], "venue": "and Tomas Mikolov.", "citeRegEx": "Weston et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model\u2019s response to the questions. The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that the EpiReader sets a new state-of-the-art on the CNN and Children\u2019s Book Test machine comprehension benchmarks, outperforming previous neural models by a significant margin.", "creator": "LaTeX with hyperref package"}}}