{"id": "1606.02555", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Improving Recurrent Neural Networks For Sequence Labelling", "abstract": "in this paper we weave different types into uniform neural networks ( rnn ) demonstrating geometric coloring decisions. laboratory uncover two new applications defining rnns integrating improvements for sequence labeling, and we compare them to the larger traditional oman and jordan rnns. we define hierarchical models, either essentially synthetic unconventional, on four distinct regions of sequence grading : two on spoken fluent understanding ( atis in media ) ; using two of pos tagging for identifying emerging varieties ( ip ) and portuguese tree valley ( pa ) corpora. the effects show that completely new uses of rnns should always more effective like the improved.", "histories": [["v1", "Wed, 8 Jun 2016 13:47:18 GMT  (188kb,D)", "http://arxiv.org/abs/1606.02555v1", "21 pages, 4 figures"]], "COMMENTS": "21 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["marco dinarelli", "isabelle tellier"], "accepted": false, "id": "1606.02555"}, "pdf": {"name": "1606.02555.pdf", "metadata": {"source": "CRF", "title": "Improving Recurrent Neural Networks For Sequence Labelling", "authors": ["Marco Dinarelli", "Isabelle Tellier"], "emails": ["marco.dinarelli@ens.fr,", "isabelle.tellier@univ-paris3.fr"], "sections": [{"heading": "1 Introduction", "text": "Recurrent Neural Networks (RNN) [1\u20133] are neural models able to take some context into account in their decision function. For this reason, they are particularly suitable for several NLP tasks, in particular sequential information prediction [4\u20138]. In RNNs, the contextual information is provided to the model by a loop connection in the network architecture. This connection allows to use at the current time step one or more pieces of information predicted at previous time steps. This architecture seems particularly effective for neural networks since it allows to combine the power of distributional representations (or embeddings) with the effectiveness of contextual information.\nIn the literature about RNNs for NLP, two main variants have been proposed, also called \u201csimple\u201d RNNs: the Elman [2] and the Jordan [1] RNN models. The difference between these models lies in the position of the loop connection giving the recurrent character to the network: in the Elman RNN, it is put in the hidden layer whereas in\nar X\niv :1\n60 6.\n02 55\n5v 1\nthe Jordan RNN it connects the output layer to the hidden layer. In this last case, the recurrent connection allows to use, at the current time step, the information predicted at previous time steps. In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].\nThe intuition at the origin of this article is that embeddings allow a fine and effective modeling not only of words, but also of labels and their dependencies, which are very important for sequence labeling. In this paper, we define two new variants of RNN to achieve this more effective modeling.\nIn the first variant, the recurrent connection is between the output and the input layers. In other words, this variant gives labels predicted at previous positions in a sequence as input to the network. Such contextual information is added to the usual input context made of words, and both are used to predict the label at the current position in the sequence. Moreover we modified the hidden layer activity computation with respect to Elman and Jordan RNN, so that to take the different information provided by words and labels into account. From our intuition, thanks to label embeddings and to features-learning abilities of the hidden layer, this variant models in a more effective way label dependencies. The second variant we propose combines an Elman RNN and our first variant. This variant can thus exploit both contextual information provided by the previous states of the hidden layer, and the labels predicted at previous positions of a sequence.\nA high-level schema of the Elman\u2019s, Jordan\u2019s and our first variant of RNN are shown in Figure 1. The schema of our second variant can be obtained by adding the recursion of the first one to the Elman architecture. In this figure, w is the input word, y is the predicted label,E,H ,O andR are the parameter matrices between each pair of layers: they will be described in details in the next section. Before, it is worth discussing the advantages our variants can bring with respect to traditional RNN architectures, and explaining why they are expected to provide better modeling abilities.\nFirst, since the output at previous positions is given as input to the network at the\ncurrent position, the contextual information flows across the whole network, affecting each layer\u2019s input and output, at both forward and backward phases. In contrast, in Elman and Jordan RNNs, not all layers are affected at both forward and backward phases.\nA second advantage of our variants is given by label embeddings. Indeed, the first layer of our RNNs is just a look-up table mapping sparse \u201cone-hot\u201d representations into distributional representations.1 Since in our variants the output of the network at previous steps is given as input at the current step, the mapping from sparse representations to embeddings involves both words and labels. Label embeddings can be pre-trained from data as it is usually done for words. Pre-trained word embeddings, e.g. with word2vec, have already shown their ability to capture very attractive syntactic and semantic properties [13,14]. Using label embeddings, the same properties can be learned also for labels. More importantly, using several predicted labels as embeddings provide a more effective modeling of label dependencies via the internal state of the network, which is the hidden layer\u2019s output.\nAnother advantage coming from the use of label embeddings and different previous labels as context, is an increased robustness of the model to prediction mistakes. This effect comes from the syntactic and semantic properties that embeddings can encode [14].\nAll these advantages are also supported by our second variant, which uses both a label embedding context, like the first variant, and the loop connection at the hidden layer, like an Elman RNN.\nAll RNNs in this article are studied in their forward, backward and bidirectional versions [3]. In order to have a fair and straightforward comparison, we give the results of our new variants of RNNs together with those obtained with our implementation of Elman and Jordan RNNs. These implementations are very close to state-of-the-art, even if we did not implement every optimization feature.\nAll models are evaluated on four tasks. Two are Spoken Language Understanding (SLU) tasks [15]: ATIS [16] and MEDIA [17], which can be both modeled as sequence labeling problems. Two are POS-tagging tasks, one on the French Treebank (FTB) [18, 19] and one on the Penn Treebank (PTB) [20]. The results we obtain on these tasks with our implementations, despite they are not always better than the state-of-the-art, provide a stable ranking of different RNN architectures: at least one of our variants, most of the time and surprisingly the simpler one, is always better than Jordan and Elman RNNs.\nIn the remainder of the paper, we introduce RNNs and we describe in more details the variants proposed in this work (section 2). In section 3, we describe the corpora used for evaluation and all the results obtained, in comparison with state-of-the-art models. In section 4, we draw our conclusions.\n1The \u201cone-hot\u201d representation of an element at position i in a dictionary V is a vector of size |V | where the i-th component has the value 1 whereas all the others are 0."}, {"heading": "2 Improving Recurrent Neural Networks", "text": "The RNNs we consider in this work have the same architecture also used for Feedforward Neural Network Language Models (NNLM), described in [21]. In this architecture, we have four layers: input, embedding, hidden and output. Words are given as input to the network as indexes, corresponding to their position in a dictionary V .\nThe index of a word is used to select its embedding (or distributional representation) in a real-valued matrix E \u2208R|V |XN , |V | being the size of the dictionary and N the dimensionality of the embeddings (which is a parameter to be chosen). We name E(v(wt)) the embedding of the word w given as input at the position t of a sequence. v(wt) = i is the index of the word wt in the dictionary, and it can be seen alternatively as a \u201cone-hot\u201d vector representation (the vector is zero everywhere except at position v(w), where it is 1).\nIn contrast to NNLM, RNNs have one more connection, the recursive connection, between two layers, depending on the type of RNNs. As mentioned previously, Elman RNNs have a recursion loop in the hidden layer. Since this layer encodes the internal representation of the input to the network, the recurrent connection of an Elman network allows to keep \u201cin memory\u201d words used as input at previous positions in the sequence. Jordan RNNs have instead a recursion between the output and the hidden layer. This means that a Jordan RNN can take previous predicted labels into account to predict the label at the current position in a sequence. For every type of RNN, we call R the matrix of parameters of the recursion connection.\nOur implementation of Jordan and Elman RNNs is like in the literature. [1\u20133]. 2"}, {"heading": "2.1 RNN Learning", "text": "Learning the described RNNs consists in learning the parameters \u0398 = (E,H,O,R) between each pair of layers (see Figure 1), and we omit biases to keep notations lighter. We use a cross-entropy cost function between the expected label ct and the predicted label yt at the position t in the sequence, plus a L2 regularization term [22]:\nC = \u2212ct \u00b7 log(yt) + \u03bb\n2 |\u0398|2 (1)\n\u03bb is an hyper-parameter of the model. Since yt is a probability distribution over output labels, we can also view the output of a RNN as the probability of the predicted label yt: P (yt|I,\u0398). I is the input given to the network plus the contextual information provided by the recurrent connection. For Elman RNN IElman = wt\u2212w...wt...wt+w, ht\u22121, that is the word input context and the output of the hidden layer at the previous position in the sequence. For Jordan RNN IJordan = wt\u2212w...wt...wt+w, yt\u22121, that is the same word input context as Elman RNN, and the label predicted at the previous position. We associate the following decision function to predict the label at position t in a sequence:\nl\u0303t = argmaxj\u22081,...,|L|P (y j t |I,\u0398) (2)\n2We also find [7] quite easy to understand, even for readers not familiar with RNNs.\nl\u0303t is a particular discrete label. We use the back-propagation algorithm and the stochastic gradient descent with\nmomentum [22] for learning the weights \u0398."}, {"heading": "2.2 Learning Variants", "text": "An important choice for learning RNN models concern the back-propagation algorithm. Indeed, because of the recurrent nature of their architecture, in order to properly learn RNNs the Back-Propagation Through Time algorithm (BPTT) should be used [23]. The BPTT algorithm constists basically in unfolding the recurrent architecture for a choosen number of steps, and then learning the network as a standard Feed-Forward network. This is supposed to allow RNNs to learn arbitrarily long past context. However, [10] has shown that RNNs for language modelling learn best with just 5 time steps in the past. This may be due to the fact that, at least in NLP tasks, the past information kept \u201cin memory\u201d by the network via the recurrent architecture, actually fades away after some time steps. Moreover, in many NLP tasks, using an arbitrarily-long context on either input or output side, doesn\u2019t garantee better performances, as increasing the context size also increases the noise. Since BPTT is quite more expensive than the traditional back-propagation algorithm, [7] has preferred to use explicit output context in Jordan RNNs and to learn the model with the traditional back-propagation algorithm, not surprisingly without loosing performance.\nIn this work we use the same variant as [7]. When using an explicit context of output labels from previous time steps, the hidden layer activity of a Jordan RNN is computed as:\nht = \u03a3(It \u00b7H + [yt\u2212c+1yt\u2212c+2...yt\u22121] \u00b7R) (3)\nwhere c is the size of the history of previous labels that we want to explicitly use as context to predict next label. [\u00b7] indicates the concatenation of vectors.\nAll the modifications applied to the Jordan RNN so far can be applied in a similar way to the Elman RNN."}, {"heading": "2.3 New Variants of RNN", "text": "As mentioned in the introduction, the new variants of RNN proposed in this work present two differences with respect to traditional RNNs: i) the recurrent connection is from the output to the input layer, meaning that predicted labels are converted into embeddings in the same way as words; ii) the hidden layer activities are computed in a slightly different way. Indeed, in Elman and Jordan RNNs the contextual information provided by the recurrent connection is summed to the input information (see equation 3 above). In our variants of RNNs instead, word and label embeddings are concatenated and provided to the hidden layer as different inputs.\nThe most interesting consequence of modifications in our RNN variants, is the fact that output labels are mapped into distributional representations, as it is usually done for input items. Indeed, the first layer of our network, is just a mapping from sparse \u201conehot\u201d representations to distributional representations. Such mapping results in fine\nfeatures and attractive syntactic and semantic properties, as shown by word2vec and similar works [13]. Such representations can be learnt from data the same way as for words. In the simplest case, this can be done by using sequences of output labels. When structured information is available, like syntactic parse trees or structured semantic labels such as named entities or entity relations, more sophosticated embeddings can be learnt. In this work, we learn label embeddings using sequences of output labels associated to word sequences in annotated data. It is worth noting that the idea of using label embeddings has been introduced by [24] in the context of dependency parsing. In this paper, we focus on the use of several label embeddings as context, thus encoding label dependencies, which are very important in sequence labeling tasks.\nUsing the same notation as above, we name Ew the embedding matrix for words, and El the embedding matrix for labels. We name\nIt = [Ew(v(wt\u2212w))...Ew(v(wt))...Ew(v(wt+w))] the concatenation of the vectors representing the input words when processing the\nposition t in a sequence, while Lt = [El(v(yt\u2212c+1))El(v(yt\u2212c+2))...El(v(yt\u22121))]\nis the concatenation of the vectors representing the output labels predicted at the previous c steps. The hidden layer activities are computed as:\nht = \u03a3([ItLt] \u00b7H) \u03a3 is the sigmoid activation function [22], [\u00b7] means the concatenation of the two matrices and we omit biases to keep notations lighter. The remainder of the layer activities, as well as the error computation and back-propagation are computed the same way as in traditional RNNs.\nNote that in this variant of RNN there is no R matrix at the recurrent connection. The recurrent connection here means that the output is given back as input to the network and it is thus converted explicitely from probability distribution given by the softmax into a label index, which is used in turn to select a label embedding from the matrix El. Basically the role played from matrix R in Elman and Jordan RNN, is played from matrix El in our variant.\nAnother important interest of having the recursion between output and input layers is robustness. This is a direct consequence of using embeddings for output labels. Since we use several predicted labels as context at each position t (see Lt above), at least in the later stages of learning (when the model is close to the final optimum), it is unlikely to have several mistakes in the same context. Even then, thanks to the properties of distributed representations [14], wrong labels have very similar representations to the correct ones. Taking an example cited in [14]: if we use Paris instead of Rome, it has no effect for many NLP tasks, as they are both proper nouns for POS-tagging, locations for named entity recognition etc. Distributed representations for labels provide the same robustness on the output side.3\nJordan RNNs cannot provide in general the same robustness. We can interpret hidden activity computation in a Jordan RNN in two ways.\nOn the one hand, if we interpret a Jordan RNN as using sparse label representations as input to the hidden layer, such representations are either \u201cone-hot\u201d representations of\n3Sometimes in POS-tagging, models mistake verbs and nouns. They make such errors because some particular verbs occur in the same context of nouns (e.g. \u201cthe sleep is important\u201d), and so have similar representations.\nlabels or probability distributions given as output by the softmax at the output layer. In the first case it is clear that a mistake may have more effect than in the second one, as the only value that is not zero is in a wrong position. But when probability distributions are used, we have found that most of the probability mass is \u201cpicked\u201d on one or few labels, which thus does not provide much more softness than a \u201cone-hot\u201d representation. In this interpretation, sparse labels are an additional input for the hidden layer, the matrix R on the recurrent connection plays the same role as H does for input words. The two matrices are then summed to compute the total input of the hidden layer.\nOn the other hand, we can see the multiplication of sparse representation of labels in equation 3 as the selection of an embedding for labels from matrix R.4 Even in this interpretation there is a substantial difference between the Jordan RNN and our variant of RNN, I-RNN henceforth (I stands for Improved). In order to understand in detail this difference, we focus on the equations for computing the hidden activities. For Jordan RNN we have:\nht = \u03a3(It \u00b7H + [yt\u2212c+1yt\u2212c+2...yt\u22121] \u00b7R) For I-RNN we have: ht = \u03a3([ItLt] \u00b7H) where Lt is [El(v(yt\u2212c+1))El(v(yt\u2212c+2))... El(v(yt\u22121))], that is the concatenation of c previous label embeddings. In this second interpretation, in Jordan RNN labels are not directly concerned in the computation of the total input for the hidden layer, since they are not directly multiplied by matrixH . Only input context It is multiplied byH . The result of this multiplication is summed to the result of label embedding selection, performed as y[t \u2212 i] \u00b7 R, i = 1 . . . c\u22121. Finally the hidden non-linear function \u03a3 is applied. Mixing input processing It \u00b7 H and label processing y[t \u2212 i] \u00b7 R with a sum, can make sense for the tasks for which the Jordan network was designed [1], as output units were of the same nature as input units (speech signal). However we believe that it doesn\u2019t express sufficiently well words and labels interactions in NLP tasks. Also, since y[t \u2212 i] \u00b7 R is an embedding selection for labels, labels and words are not processed in the same way: It is already made of word embeddings, which are further transformed by It \u00b7 H . This further transformation is not applied to label embeddings.\nIn I-RNN in contrast, sparse labels are first converted into embeddings withEl(v(yt\u2212i))], i = 1 . . . c, and their concatenation results in Lt. This matrix is further concatenated to the input context It. The result of the concatenation is multiplied by matrix H to compute the total input to the hidden layer.5 Finally the hidden non-linear function \u03a3 is applied. This means that information provided by the input context It is not mixed with Lt with a sum like in Jordan RNN. These two data are given to the hidden layer as separated inputs. More in particular, the concatenation of It and Lt is performed neuron-wise, that is each hidden neuron receives as input all context words and all context labels, encoding them as a network internal feature. Thus we let the hidden layer itself to learn labels interactions, and words-labels interactions. This is indeed in agreement with the \u201cphilosophy\u201d of neural networks, where features designing is turned into features learning. Since words and labels have different nature in sequence labeling\n4Multiplying a \u201cone-hot\u201d representation by a matrix is equivalent to selecting one row of the matrix. 5We remind the reader that we are omitting biases to keep notation lighter\ntasks, we believe that modeling interactions in this way is more effective. Also, with the I-RNN architecture, words and labels are processed in the same way, as both are first converted into embeddings, and then \u201ctransformed\u201d again via multiplication by H matrix. In order to make results obtained with different RNNs comparable, we used the same number of hidden neurons for all RNNs. In I-RNN thus, each hidden neuron receives as input much more information than Jordan and Elman hidden neurons.\nIn order to make the explanation more clear, I-RNN architecture is detailed in figure 2. Symbols have the same meaning as equations in the paper, the only exception is that labels are indicated in the figure with upper-case L. Matrix H is replicated at the hidden layer computation meaning that all neurons receive the whole [ItLt] input, which is made of 2 \u00b7w+ 1 + c D-dimensional embeddings: 2w+ 1 word embeddings and c label embeddings. CAT is the concatenation operator. Please note that the concatenations of embeddings are performed at two different steps just for the sake of clearness and to be coherent with equations in the paper, all concatenations can be performed in one step.\nOur second variant of RNN combines the characteristics of an Elman RNN and of our first variant. In this variant, the only difference with the first one is the computation of the hidden layer activities, where we use the concatenation of the c previous hidden layer states in addition to the information already used in the first variant:\nht = \u03a3([ItLt] \u00b7H + [ht\u2212c+1ht\u2212c+2...ht\u22121] \u00b7R)"}, {"heading": "2.4 Forward, Backward and Bidirectional RNNs", "text": "All RNNs described in this work are studied in their forward, backward and bidirectional versions [3]. Forward RNNs work as described so far. Backward RNNs have exactly the same architecture, the only difference being that they process sequences in the reverse order, from the end to the begin. Backward RNNs can be used to predict future labels in a sequence.\nBidirectional RNNs use both past and future information to predict the next label, which is both words and labels in our variants and in Jordan RNNs, or both words and hidden layers in Elman RNNs. When labeling sequences with bidirectional RNNs, a backward network is first used to predict labels backward. The bidirectional RNN then processes sequences in forward direction using past contextual information as usual, and future contextual information provided by the states and labels predicted by the backward network. The hidden layer of the bidirectional version of our fist variant of RNNs is thus computed as:\nht = \u03a3([ItL p tL f t ] \u00b7H)\nwhere Lpt = Lt introduced above, while Lft = [El(v(yt+1)) . . . El(v(yt+c\u22121))El(v(yt+c))]\nis the concatenation of the vectors representing the c future labels predicted by the backward model. It is very similar for our second variant, refer to [3] for details."}, {"heading": "2.5 Recurrent Neural Network Complexity", "text": "We provide here an analysis of the complexity in terms of the number of parameters involved in each model. In Jordan RNN we count:\n|V | \u00d7D + ((2w + 1)D + c|O|)\u00d7 |H|+ |H| \u00d7 |O| where |V | is the size of the input dictionary, D the dimensionality of the embeddings, |H| and |O| are the size of the hidden and output layers, respectively, w the size of the window of words used as context on the input side6 and c is the size of the context of labels, which is multiplied by the dimensionality of the output label dictionary |O|.\nWith the same symbols, in an Elman RNN and in our first variant we have, respectively:\n|V | \u00d7D + ((2w + 1)D + c|H|)\u00d7 |H|+ |H| \u00d7 |O| and\n|V | \u00d7D + |O| \u00d7D + ((2w + 1)D + cD)\u00d7 |H|+ |H| \u00d7 |O| 6The word input context is thus made of w words on the left and w on the right of the word at a given\nposition t, plus the word at t itself, which gives a total of 2w + 1 input words\nThe only difference between the Jordan and Elman RNNs lies in the factors c|O| and c|H|. Their difference in complexity depends thus on the size of the output layer (Jordan) with respect to the size of the hidden layer (Elman). Since in sequence labeling tasks the hidden layer is often bigger, Elman RNN is more complex than Jordan RNN. The difference between the Jordan RNN and our first variant is in the factors |O| \u00d7D and cD. The first is due to the label embeddings7, the second is due to the use of such embeddings as input to the hidden layer. Since often D and O have sizes in the same order of magnitude, and thanks to the use of vectorized operations on matrices, we didn\u2019t found a noticeable difference in terms of training and testing time between the Jordan RNN and our first variant. This simple analysis also shows that our first variant roughly needs the same number of connections in the hidden layer as a Jordan RNN. Our first variant is thus architecturally equivalent to a Jordan RNN.\nIn contrast, for the second variant we have: |V | \u00d7D + |O| \u00d7D + ((2w + 1)D + cD + c|H|)\u00d7 |H|+ |H| \u00d7 |O|\nThe additional term c|H|, is due to the same recurrent connection as in an Elman RNN. Using vectorized operations for matrix calculations, we found the second variant slower for both training and testing time by a factor 1.15 with respect to the other RNNs. The same complexity analysis holds for backward RNNs. But bidirectional RNNs are even more complex. Without deriving any new formula, we note that they are slower with respect to their corresponding forward/backward models by a factor of roughly 1.5."}, {"heading": "3 Evaluation", "text": ""}, {"heading": "3.1 Corpora", "text": "We used four distinct corpora:\nThe Air Travel Information System (ATIS) task [16] has been designed to automatically provide flight information in SLU systems. The semantic representation is frame-based and the goal is to find the correct frame and the corresponding semantic slots. For example, for the sentence \u201cI want the flights from Boston to Philadelphia today\u201d, the correct frame is FLIGHT and the words Boston, Philadelphia and today must be annotated with the concepts DEPARTURE.CITY, ARRIVAL.CITY and DEPARTURE.DATE, respectively.\nATIS is a relatively simple task dating from 1993. The training set is made of 4978 sentences taken from the \u201ccontext independent\u201d data in the ATIS-2 and ATIS-3 corpora. The test set is made of 893 sentences, taken from the ATIS-3 NOV93 and DEC94 datasets. There are no official development data provided with this corpus, we have thus taken a part of the training data at random to play this role. 8\nThe French corpus MEDIA [17] has been created to evaluate SLU systems providing tourist information, in particular hotel information in France. It is composed of 1250 dialogues acquired with a Wizard-of-OZ protocol where 250 speakers have applied 5 hotel reservation scenarios. The dialogues have been annotated following a rich\n7We use embeddings of the same size D for words and labels. 8 Please see [16] for more details on the ATIS corpus.\nsemantic ontology. Semantic components can be combined to create complex semantic labels.9 In addition to this rich annotation, another difficulty lies in the coreferences. Some words cannot be correctly annotated without information about previous dialog turns. For example in the sentence \u201cYes, the one at less than fifty euros per night\u201d, the one refers to an hotel previously introduced in the dialog. Statistics on training, development and test data from this corpus are shown in table 1.\nBoth ATIS and MEDIA can be modelled as sequence labeling tasks using the BIO chunking notation [25]. Several different works compared on ATIS [6\u20138,26]. [8] is the only work providing results on MEDIA with RNNs, it also provides results obtained with CRFs [27], allowing an interesting comparison.\nThe French Treebank (FTB) corpus is presented in [18]. The version we use for POS-tagging is exactly the same as in [19]. But, in contrast to them, who reach the best result on this task with an external lexicon, we don\u2019t use any external resource here 10. Statistics on the FTB corpus are shown in table 2.\nThe Penn Treebank (FTB) corpus is presented in [20]. In order to have a direct comparison with previous works [28] [29] [5], we split the data as they do: sections 0 \u2212 18 are used for training, 19 \u2212 21 for validation and 22 \u2212 24 for testing. Statistics on the PTB corpus are shown in table 3."}, {"heading": "3.2 RNNs Parameters Settings", "text": "In order to compare with some published works on the ATIS and MEDIA tasks, we use the same dimensionality settings used by [6], [7] and [8], that is embeddings have 200 dimensions, hidden layer has 100 dimensions. We also use the same context size for words, that is w = 3, and we use c = 6 as labels context size in our variants and in Jordan RNN. We use the same tokenization, basically consisting of words lowercasing.\nIn contrast, in our models the sigmoid activation function at the hidden layer and the L2 regularization. While [6], [7], [8] and [26] use the rectified linear activation function and the dropout regularization [22] [30].\n9For example the label localisation can be combined with city, relative-distance, general-relative-place, street etc.\n10 [19] also provides results without using the external lexicon\nFor the FTB POS-tagging task we have used 200-dimensional embeddings, 300- dimensional hidden layer, again w = 3 for the context on the input side, and 6 context labels on the output side. The bigger hidden layer gave better results during validation experiments, due to the larger word dictionary in this task with respect to the others, roughly 25000 for the FTB against 2000 for MEDIA and 1300 for ATIS. In contrast to [19], which has used several features of words (prefixes, suffixes, capitalisation information etc.), we only performed a simple tokenization for reducing the size of the input dictionary: all numbers have been mapped to a conventional symbol (NUM), and nouns not corresponding to proper names and starting with a capital letter have been converted to lowercase. We preferred this simple tokenisation without using rich features, because our goal in this work is not obtaining the best results ever, it is to compare Jordan and Elman RNNs with our variants of RNN and show that our variants works better for sequence labeling. Adding many features and/or building sophisticated models would make the message less clear, as results would be probably better but the improvements could be attributed to rich and sophisticated models, instead of to the model itself.\nFor the PTB POS-tagging task we use exactly the same settings and pre-processing as for the FTB task, except that we used 400 hidden neurons. During validation we found that this works better, again due to the size of the dictionary which is 45000 for this task (after pre-processing).\nWe trained all RNNs with exactly the same protocol: i) we first train neural language models to obtain word and label embeddings. This language model is like the one in [21], except it uses both words/labels in the past and in the future to predict next word/label. ii) we train all RNNs using the same embeddings trained at previous step. We train the RNN for word embeddings for 20 epochs, the RNN for label embeddings for 10 epochs, and we train the RNNs for sequence labeling for 20 epochs. The number of epochs has been roughly optimized on development data. At the end of training we keep the model which gave the best tagging accuracy on development data. Also we roughly optimized on development data the learning rate and the parameter \u03bb for regularization, the best values found are 0.5 and 0.003, respectively."}, {"heading": "3.3 Training and Tagging Time", "text": "Since implementations of RNNs use in this work are prototypes11, it does not make sense to compare them to state-of-the-art in terms of training and tagging time. However it is worth providing training times at least to have an idea and to have a comparison among different RNNs.\n11Our implementations are basically written in Octave https://www.gnu.org/software/octave/\nAs explained in section 2.5, our first variant I-RNN and the Jordan RNN have the same complexity. Also, since the size of the hidden layer is in the same order of magnitude as the size of the output layer (i.e. the number of labels), also Elman RNN has roughly the same complexity as the Jordan RNN. This is reflected in the training time.\nThe training time for label embeddings is always relatively short, as the size of the output layer, that is the number of labels, is always relatively small. This training time thus can vary from few minutes for ATIS and MEDIA, to less that 1 hour for the FTB corpus.\nTraining word embeddings is also very fast on ATIS and MEDIA, taking less than 30 minutes for ATIS and roughly 40 minutes for MEDIA. In contrast training word embeddings on the FTB corpus takes roughly 5 days, training time goes to roughly 6 days for the PTB word embeddings.\nTraining the RNN taggers takes roughly the same time as training the word embeddings, as the size of the word dictionary is the dimension that most affects the computational complexity at softmax used to predict next label.\nConcerning the second variant of RNN proposed in this work, I+E-RNN, this is slower as it is more complex in terms of number of parameters. Training I+E-RNN on ATIS and MEDIA takes roughly 45 minutes and 1 hour, respectively. In contrast training I+E-RNN on the FTB and PTB corpora takes roughly 6 days and 7 days, respectively.\nWe didn\u2019t keep track of tagging time, however it is always negligible with respect to training time, and it is always measured in few minutes. All times provided here are with a 1.7 GHz CPU, single process."}, {"heading": "3.4 Sequence Labeling Results", "text": "The evaluation of all models described here is shown in table 4 5 6 and 7, in terms of F1 measure for ATIS and MEDIA, Accuracy on the FTB and PTB. Our implementations of Elman and Jordan RNN are indicated in tables as E-RNN and J-RNN. Our new variants are indicated as I-RNN and I+E-RNN.\nTable 4 shows results on the ATIS corpus. In the higher part of the table we show results obtained using only words as input. In the lower part, results are obtained using both words and word-classes available for this task. Such classes concern city names, airports and time expressions. They allow the models to generalize from specific words triggering concepts.12\nNote that our results on the ATIS corpus are not always comparable with those published in the literature because: i) Models published in the literature use a rectified linear activation function at the hidden layer13 and the dropout regularization. Our models use the sigmoid activation function and the L2 regularization. ii) For experiments on ATIS we have used roughly 18% of the training data for development, we thus\n12For example the cities of Boston and Philadelphia in the example above are mapped to the class CITY-NAME. If a model has never seen Boston during the training phase, but has seen at least one city name, it will possibly annotate Boston as a departure city thanks to some discriminative context, such as the preposition from.\n13f(x) = max(0, x).\nused a smaller training set. iii) The works we compare with do not always give details on how classes available for the task have been integrated into their models. iv) Layer dimensionality and hyper-parameter settings do not always match those of published works. In fact, to avoid running too much experiments, we have based our settings on known works, but this doesn\u2019t allow a straightforward comparison with other published works.\nDespite this, the message we want to claim in this work is still true for two reasons: i) Some of the results obtained with Elman and Jordan RNNs are close, or even better, than state-of-the-art. Thus they are not weak baselines. ii) We provide a fair comparison of our variants with traditional Elman and Jordan RNNs.\nThe results in the higher part of table 4 show that the best model on the ATIS task, with these settings, is the Elman RNN of [26]. Note that it is not clear how the improvements of [26] with respect to [7] (in part due to same authors) have been obtained. Indeed, in [7] authors obtain the best result with a Jordan RNN, while in [26] an Elman RNN gets the best performance. During our experiments, using the same experimentation protocol as [26], we could not reach the same performances. We conclude that the difference between our results and those in [26] are due to reasons mentioned above. Beyond this, we note that our Elman and Jordan RNN implementations are equivalent to those of [7]. Also, our first variant of RNNs, I-RNN, obtains the second best result (93.84 in bold), which is the best result we could reach. Our second variant is roughly equivalent to a Jordan RNN on this task.\nThe results in the lower part of the table 4 (Classes), obtained with both words and classes as input, are quite better than those obtained with words only. They roughly follow the same behavior, except that in this case our Jordan RNN is slightly better than our second variant. The first variant I-RNN obtains again the best result among our implementations (95.21 in bold). In this case also, we attribute the differences with respect to published results to the different settings mentioned above. For comparison, in this part of the table we show also results obtained using CRF.\nOn the ATIS task, using either words or both words and classes as inputs, we can see that results are always quite high. This task is relatively simple. Label dependencies can be easily modeled, as there is basically no segmentation of concepts over different consecutive words (one concept corresponds to one word). In this settings, the potential of our new variants of RNNs cannot be fully exploited. This limitation is confirmed by the results obtained by [8] and [26] using CRFs. Indeed, RNNs don\u2019t take the whole sequence of labels into account in their decision function. In contrast, CRFs use a global decision function taking all the possible labelings of a given input sequence into account to predict the best sequence of labels. The fact that CRFs are less effective than RNN on ATIS is a clear sign that label dependency modeling is relatively simple in this task.\nTable 5 shows the results on the corpus MEDIA. As already mentioned, this task is more difficult because of its richer semantic annotation, and also because of the coreferences and of the segmentation of concepts over several words. This creates relatively long label dependencies, which cannot be taken into account by simple models. The difficulty of this task is confirmed by the magnitude of results (10 F1 points lower than on the ATIS task).\nAs can be seen in Table 5, CRFs are in general much more effective than Jordan\nand Elman RNNs on MEDIA. This outcome could be expected as RNNs use a local decision function not able to take long label dependencies into account. We can also see that our implementations of Elman and Jordan RNNs are comparable, even better in the case of Elman RNN, with state-of-the-art RNNs of [8].\nMore importantly, results on the MEDIA task shows that in this particular experimental settings where taking label dependencies into account is crucial, our new variants are remarkably more effective than both our implementations and state-of-the-art implementations [8] of Elman and Jordan RNNs. This holds for forward, backward and bidirectional RNNs. Moreover, the bidirectional version of our variants of RNNs outperforms even CRF. We attribute this effectiveness to a better modeling of label dependencies, due to label embeddings.\nTable 6 shows the results obtained on the POS-tagging task of the FTB corpus. On this task, we compare our RNN implementations to the state-of-the-art results obtained in [19] with the modelMElt0fr. We would like to underline thatMElt 0 fr, when it does not use external resources like in the model obtaining the best absolute result in [19], nevertheless uses several features associated with words that provide an advantage over features used in our RNNs. As can be expected thus, MElt0fr outperforms the RNNs. Results in table 6 shows that forward and backward RNNs are quite close to each other on this task, I-RNN, providing a little improvement. In contrast, the bidirectional version of I-RNN and I+E-RNN provide a significant improvement over Jordan and Elman RNNs.\nTable 7 shows the results obtained on the WSJ POS tagging task. We also provide the results of [28] and [29] for comparison with the state-of-the-art. This is just for a matter of comparison, as the results shown in those works were achieved with rich features and more sophisticated models. Instead, we can roughly compare our results with those of [5], which were also obtained with neural networks. In particular, we can compare with the model called NN+SLL. Note that this is a rough comparison as the model of [5], though not a RNN, integrates capitalisation features and uses a convolution and a max-over-time layer to encode large context information.\nResults for POS tagging on the PTB corpus roughly confirm the conclusions reached for FTB results, with the only difference that in this case I+E-RNN is slightly better than I-RNN. Our RNNs don\u2019t improve the state-of-the-art, but are all more effective than the model of [5]. This result is particularly important, as it shows that RNNs, even without using a sophisticated encoding of the context like the model NN+SLL in [5], are intrinsecally a better model for sequence labeling. This claim is enforced also by the fact that NN+SLL of [5] implements a global probability computation strategy similar to CRF (SLL stands for Sentence Level Likelihood), while all RNNs presented here use a local decision function (see equation 2). Again, the ranking of RNN models on the PTB POS-tagging task is stable, the variants of RNNs proposed in this work being more effective than traditional Elman and Jordan RNNs.\nWe would like to notify that we have also performed some experiments on ATIS and MEDIA without pre-training label embeddings. The results reached are not substantially different from those obtained with pre-training label embeddings. Indeed, on relatively small data, it is not rare to have similar or even better results without pretraining. This is due to the fact that learning effective embeddings requires a relatively large amount of data. [8] also shows results obtained with embeddings pre-trained with\nword2vec and without embedding pre-training. His conclusions are indeed very similar. More generally, reaching roughly the same results on a difficult task like MEDIA without label embedding pre-training is a clear sign that our variants of RNNs are superior to traditional RNNs because they use a context made of label embeddings. As a matter of fact, the gain with respect to Elman and Jordan RNNs cannot be attributed in this case to the use of pre-trained embeddings. On relatively larger corpora like FTB and PTB, label embedding pre-training seems to provide a slight improvement.\nFinally, we have run experiments also modifying Jordan and Elman RNNs so that to model words and labels interactions more like I-RNN does, that is word and label embeddings (or hidden states in Elman RNN) are not summed together, instead they are concatenated. Results obtained were not substantially different from those obtained with \u201ctraditional\u201d Jordan and Elman RNN and in any case I-RNN was always performing best, still keeping a large gain over Elman and Jordan RNN on the MEDIA task. The explanation of this outcome is that keeping word and label embeddings separated, and then multiplying both by matrix H to compute hidden activities, as we do in I-RNN, is more effective than concatenating It \u00b7 H and y[t \u2212 1] \u00b7 R, as we did for Jordan RNN, and analogously with previous hidden layer for Elman RNN. This is not surprising, as I-RNN also in this case is applying one transformation more when multiplying label embeddings by H to compute the total input to the hidden layer.\nIt is someway surprising that I-RNN systematically outperforms I+E-RNN, the latter model integrates more information at the hidden layer and thus should be able to take advantage of both Elman and I-RNN characteristics. While an analysis to explain this outcome is not trivial, our interpretation is that using two recursions in a RNN gives actually redundant information to the model. Indeed, the output of the hidden layer keeps the internal state of the network, which is the internal (distributed) representation of the input n-gram of words around position t and the previous c labels. The recursion at the hidden layer allows to keep this information \u201cin memory\u201d and to use it at the next step t + 1. However using the recursion of I-RNN the previous c labels are also given explicitely as input to the hidden layer. This may be redundant, and constrain the model to learn an increased amount of noise. A similar idea of hybrid RNN model has been tested in [26] without showing a clear advantage on Elman and Jordan RNNs.\nWhat can be said in general from the results obtained on all the presented tasks, is that RNN architectures using label embedding context can model label dependencies in a more effective way, even when these dependencies are relatively simple (like in ATIS and POS tagging tasks). The two variant of RNNs proposed in this work, in particular the I-RNN variant, are for this reason more effective than Elman and Jordan RNNs on sequence labeling tasks.\n3.5 Comparison of Jordan-RNN and I-RNN label representations We compare Jordan RNN and I-RNN label representations under the interpretation where Jordan RNN hidden activity computation uses sparse labels as an additional input to the hidden layer, as explained in section 2.3. As explained also in the same section, under the other interpretation I-RNN provides the advantage of performing an\nadditional transformation on labels, and gives words and labels embeddings as separated inputs to the hidden layer.\nUnder the first interpretation, the advantage of using label embeddings in I-RNN instead of \u201cone-hot\u201d or probability distribution representations like in Jordan RNNs, is an increased amount of signal flowing across the network. A semantic interpretation of the interaction of these two representations with the network is not trivial. Indeed, in the probability representation output by the softmax in a Jordan RNN, the different dimensions are just probabilities associated to different labels. In contrast, in label embeddings used in I-RNN, the dimensions are different distributional features, related to how a particular label is used in particular label contexts. A comparison between these two representation is thus not really meaningful.\nInstead, we performed a simple analysis of the magnitude of values found in the probability distribution used as representations in Jordan RNNs, when using development data of the MEDIA corpus. We summarize this analysis as follows:\n1. 7843 out of 11051 ( 71%) of the time, the maximum value is greater than 0.9\n2. 9928 out of 11051 ( 90%) of the time, the sum of the 3 highest probabilities is greater than 0.9\n3. Excluding the 3 highest probabilities, the remaining values in the distribution\nhave very small values (less than 0.001)\nThis simple analysis shows that the probability distributions used for label representations in Jordan RNNs do not provide much more information to the network than a \u201cone-hot\u201d representation, and not much signal into the network. This problem is someway similar to the \u201cvanishing gradient\u201d problem [31]: as the network learns, the probability gets concentrated on few dimensions and all the other values get very small, limiting network learning. This problem is all the more obvious that label dependency modeling is more important for the task. On an absolute scale however, this is less serious than the vanishing gradient problem, as Jordan RNNs still reach competitive performances."}, {"heading": "4 Conclusions", "text": "In this paper we have studied different architectures of Recurrent Neural Networks for sequence labeling tasks. We have proposed two new variants of RNNs to better model label dependencies, and we have compared these variants to the traditional architectures of Elman and Jordan RNNs. We have explained the advantages provided by the proposed variants with respect to previous RNNs. We have evaluated all RNNs, either new or traditional, on 4 different tasks: two of Spoken Language Understanding and two of POS-tagging. The results show that, even though RNNs don\u2019t always improve the state-of-the-art, our new variants of RNNs always outperform the traditional Elman and Jordan RNNs."}], "references": [{"title": "Serial order: A parallel, distributed processing approach", "author": ["M.I. Jordan"], "venue": "Advances in Connectionist Theory: Speech", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "COGNITIVE SCIENCE", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K. Paliwal"], "venue": "Trans. Sig. Proc", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th International Conference on Machine Learning. ICML \u201908,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "J. Mach. Learn. Res", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Recurrent neural networks for language understanding", "author": ["K. Yao", "G. Zweig", "M.Y. Hwang", "Y. Shi", "D. Yu"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Is it time to switch to word embedding and recurrent neural networks for spoken language understanding", "author": ["V. Vukotic", "C. Raymond", "G. Gravier"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernock", "S. Khudanpur"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "CCG supertagging with a recurrent neural network. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015", "author": ["W. Xu", "M. Auli", "S. Clark"], "venue": "July 26-31,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Unsupervised and lightly supervised partof-speech tagging using recurrent neural networks", "author": ["O. Zennaki", "N. Semmar", "L. Besacier"], "venue": "Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation, PACLIC 29,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR abs/1301.3781", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W. Yih", "G. Zweig"], "venue": "Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Spoken language understanding: A survey", "author": ["R. De Mori", "F. Bechet", "D. Hakkani-Tur", "M. McTear", "G. Riccardi", "G. Tur"], "venue": "IEEE Signal Processing Magazine", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Expanding the scope of the atis task: The atis-3 corpus", "author": ["D.A. Dahl", "M. Bates", "M. Brown", "W. Fisher", "K. Hunicke-Smith", "D. Pallett", "C. Pao", "A. Rudnicky", "E. Shriberg"], "venue": "Proceedings of the Workshop on Human Language Technology. HLT \u201994,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1994}, {"title": "Results of the french evalda-media evaluation campaign for literal understanding", "author": ["H. Bonneau-Maynard", "C. Ayache", "F. Bechet", "A. Denis", "A. Kuhn", "F. Lef\u00e8vre", "D. Mostefa", "M. Qugnard", "S. Rosset", "Servan", "J.S. Vilaneau"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Building a Treebank for French. In: Treebanks : Building and Using Parsed Corpora", "author": ["A. Abeill\u00e9", "L. Cl\u00e9ment", "F. Toussenel"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Coupling an annotated corpus and a lexicon for state-of-theart pos tagging", "author": ["P. Denis", "B. Sagot"], "venue": "Lang. Resour. Eval", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "B. Santorini", "M.A. Marcinkiewicz"], "venue": "COMPUTATIONAL LINGUISTICS", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1993}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "JOURNAL OF MACHINE LEARNING RESEARCH", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Y. Bengio"], "venue": "CoRR abs/1206.5533", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Backpropagation through time: what does it do and how to do it", "author": ["P. Werbos"], "venue": "Proceedings of IEEE. Volume", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1990}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Text chunking using transformation-based learning", "author": ["L. Ramshaw", "M. Marcus"], "venue": "Proceedings of the 3rd Workshop on Very Large Corpora,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1995}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "D. Hakkani-Tur", "X. He", "L. Heck", "G. Tur", "D. Yu", "G. Zweig"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning (ICML),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Guided learning for bidirectional sequence classification", "author": ["L. Shen", "G. Satta", "A. Joshi"], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "In Kremer, Kolen, eds.: A Field Guide", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Recurrent Neural Networks (RNN) [1\u20133] are neural models able to take some context into account in their decision function.", "startOffset": 32, "endOffset": 37}, {"referenceID": 1, "context": "Recurrent Neural Networks (RNN) [1\u20133] are neural models able to take some context into account in their decision function.", "startOffset": 32, "endOffset": 37}, {"referenceID": 2, "context": "Recurrent Neural Networks (RNN) [1\u20133] are neural models able to take some context into account in their decision function.", "startOffset": 32, "endOffset": 37}, {"referenceID": 3, "context": "For this reason, they are particularly suitable for several NLP tasks, in particular sequential information prediction [4\u20138].", "startOffset": 119, "endOffset": 124}, {"referenceID": 4, "context": "For this reason, they are particularly suitable for several NLP tasks, in particular sequential information prediction [4\u20138].", "startOffset": 119, "endOffset": 124}, {"referenceID": 5, "context": "For this reason, they are particularly suitable for several NLP tasks, in particular sequential information prediction [4\u20138].", "startOffset": 119, "endOffset": 124}, {"referenceID": 6, "context": "For this reason, they are particularly suitable for several NLP tasks, in particular sequential information prediction [4\u20138].", "startOffset": 119, "endOffset": 124}, {"referenceID": 7, "context": "For this reason, they are particularly suitable for several NLP tasks, in particular sequential information prediction [4\u20138].", "startOffset": 119, "endOffset": 124}, {"referenceID": 1, "context": "In the literature about RNNs for NLP, two main variants have been proposed, also called \u201csimple\u201d RNNs: the Elman [2] and the Jordan [1] RNN models.", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "In the literature about RNNs for NLP, two main variants have been proposed, also called \u201csimple\u201d RNNs: the Elman [2] and the Jordan [1] RNN models.", "startOffset": 132, "endOffset": 135}, {"referenceID": 8, "context": "In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].", "startOffset": 95, "endOffset": 102}, {"referenceID": 9, "context": "In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].", "startOffset": 95, "endOffset": 102}, {"referenceID": 5, "context": "In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].", "startOffset": 141, "endOffset": 154}, {"referenceID": 6, "context": "In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].", "startOffset": 141, "endOffset": 154}, {"referenceID": 7, "context": "In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].", "startOffset": 141, "endOffset": 154}, {"referenceID": 10, "context": "In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].", "startOffset": 141, "endOffset": 154}, {"referenceID": 11, "context": "In the last few years, these two types of RNNs have been very successful for language modeling [9, 10], and for some sequence labeling tasks [6\u20138, 11, 12].", "startOffset": 141, "endOffset": 154}, {"referenceID": 12, "context": "with word2vec, have already shown their ability to capture very attractive syntactic and semantic properties [13,14].", "startOffset": 109, "endOffset": 116}, {"referenceID": 13, "context": "with word2vec, have already shown their ability to capture very attractive syntactic and semantic properties [13,14].", "startOffset": 109, "endOffset": 116}, {"referenceID": 13, "context": "This effect comes from the syntactic and semantic properties that embeddings can encode [14].", "startOffset": 88, "endOffset": 92}, {"referenceID": 2, "context": "All RNNs in this article are studied in their forward, backward and bidirectional versions [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 14, "context": "Two are Spoken Language Understanding (SLU) tasks [15]: ATIS [16] and MEDIA [17], which can be both modeled as sequence labeling problems.", "startOffset": 50, "endOffset": 54}, {"referenceID": 15, "context": "Two are Spoken Language Understanding (SLU) tasks [15]: ATIS [16] and MEDIA [17], which can be both modeled as sequence labeling problems.", "startOffset": 61, "endOffset": 65}, {"referenceID": 16, "context": "Two are Spoken Language Understanding (SLU) tasks [15]: ATIS [16] and MEDIA [17], which can be both modeled as sequence labeling problems.", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "Two are POS-tagging tasks, one on the French Treebank (FTB) [18, 19] and one on the Penn Treebank (PTB) [20].", "startOffset": 60, "endOffset": 68}, {"referenceID": 18, "context": "Two are POS-tagging tasks, one on the French Treebank (FTB) [18, 19] and one on the Penn Treebank (PTB) [20].", "startOffset": 60, "endOffset": 68}, {"referenceID": 19, "context": "Two are POS-tagging tasks, one on the French Treebank (FTB) [18, 19] and one on the Penn Treebank (PTB) [20].", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "The RNNs we consider in this work have the same architecture also used for Feedforward Neural Network Language Models (NNLM), described in [21].", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "[1\u20133].", "startOffset": 0, "endOffset": 5}, {"referenceID": 1, "context": "[1\u20133].", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[1\u20133].", "startOffset": 0, "endOffset": 5}, {"referenceID": 21, "context": "We use a cross-entropy cost function between the expected label ct and the predicted label yt at the position t in the sequence, plus a L2 regularization term [22]:", "startOffset": 159, "endOffset": 163}, {"referenceID": 6, "context": "2We also find [7] quite easy to understand, even for readers not familiar with RNNs.", "startOffset": 14, "endOffset": 17}, {"referenceID": 21, "context": "We use the back-propagation algorithm and the stochastic gradient descent with momentum [22] for learning the weights \u0398.", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "Indeed, because of the recurrent nature of their architecture, in order to properly learn RNNs the Back-Propagation Through Time algorithm (BPTT) should be used [23].", "startOffset": 161, "endOffset": 165}, {"referenceID": 9, "context": "However, [10] has shown that RNNs for language modelling learn best with just 5 time steps in the past.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "Since BPTT is quite more expensive than the traditional back-propagation algorithm, [7] has preferred to use explicit output context in Jordan RNNs and to learn the model with the traditional back-propagation algorithm, not surprisingly without loosing performance.", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "In this work we use the same variant as [7].", "startOffset": 40, "endOffset": 43}, {"referenceID": 12, "context": "features and attractive syntactic and semantic properties, as shown by word2vec and similar works [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "It is worth noting that the idea of using label embeddings has been introduced by [24] in the context of dependency parsing.", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "The hidden layer activities are computed as: ht = \u03a3([ItLt] \u00b7H) \u03a3 is the sigmoid activation function [22], [\u00b7] means the concatenation of the two matrices and we omit biases to keep notations lighter.", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "distributed representations [14], wrong labels have very similar representations to the correct ones.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "Taking an example cited in [14]: if we use Paris instead of Rome, it has no effect for many NLP tasks, as they are both proper nouns for POS-tagging, locations for named entity recognition etc.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "Mixing input processing It \u00b7 H and label processing y[t \u2212 i] \u00b7 R with a sum, can make sense for the tasks for which the Jordan network was designed [1], as output units were of the same nature as input units (speech signal).", "startOffset": 148, "endOffset": 151}, {"referenceID": 2, "context": "All RNNs described in this work are studied in their forward, backward and bidirectional versions [3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "It is very similar for our second variant, refer to [3] for details.", "startOffset": 52, "endOffset": 55}, {"referenceID": 15, "context": "We used four distinct corpora: The Air Travel Information System (ATIS) task [16] has been designed to automatically provide flight information in SLU systems.", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "8 The French corpus MEDIA [17] has been created to evaluate SLU systems providing tourist information, in particular hotel information in France.", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "8 Please see [16] for more details on the ATIS corpus.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "Both ATIS and MEDIA can be modelled as sequence labeling tasks using the BIO chunking notation [25].", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "Several different works compared on ATIS [6\u20138,26].", "startOffset": 41, "endOffset": 49}, {"referenceID": 6, "context": "Several different works compared on ATIS [6\u20138,26].", "startOffset": 41, "endOffset": 49}, {"referenceID": 7, "context": "Several different works compared on ATIS [6\u20138,26].", "startOffset": 41, "endOffset": 49}, {"referenceID": 25, "context": "Several different works compared on ATIS [6\u20138,26].", "startOffset": 41, "endOffset": 49}, {"referenceID": 7, "context": "[8] is the only work providing results on MEDIA with RNNs, it also provides results obtained with CRFs [27], allowing an interesting comparison.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[8] is the only work providing results on MEDIA with RNNs, it also provides results obtained with CRFs [27], allowing an interesting comparison.", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "The French Treebank (FTB) corpus is presented in [18].", "startOffset": 49, "endOffset": 53}, {"referenceID": 18, "context": "The version we use for POS-tagging is exactly the same as in [19].", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "The Penn Treebank (FTB) corpus is presented in [20].", "startOffset": 47, "endOffset": 51}, {"referenceID": 27, "context": "In order to have a direct comparison with previous works [28] [29] [5], we split the data as they do: sections 0 \u2212 18 are used for training, 19 \u2212 21 for validation and 22 \u2212 24 for testing.", "startOffset": 57, "endOffset": 61}, {"referenceID": 28, "context": "In order to have a direct comparison with previous works [28] [29] [5], we split the data as they do: sections 0 \u2212 18 are used for training, 19 \u2212 21 for validation and 22 \u2212 24 for testing.", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "In order to have a direct comparison with previous works [28] [29] [5], we split the data as they do: sections 0 \u2212 18 are used for training, 19 \u2212 21 for validation and 22 \u2212 24 for testing.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "In order to compare with some published works on the ATIS and MEDIA tasks, we use the same dimensionality settings used by [6], [7] and [8], that is embeddings have 200 dimensions, hidden layer has 100 dimensions.", "startOffset": 123, "endOffset": 126}, {"referenceID": 6, "context": "In order to compare with some published works on the ATIS and MEDIA tasks, we use the same dimensionality settings used by [6], [7] and [8], that is embeddings have 200 dimensions, hidden layer has 100 dimensions.", "startOffset": 128, "endOffset": 131}, {"referenceID": 7, "context": "In order to compare with some published works on the ATIS and MEDIA tasks, we use the same dimensionality settings used by [6], [7] and [8], that is embeddings have 200 dimensions, hidden layer has 100 dimensions.", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "While [6], [7], [8] and [26] use the rectified linear activation function and the dropout regularization [22] [30].", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "While [6], [7], [8] and [26] use the rectified linear activation function and the dropout regularization [22] [30].", "startOffset": 11, "endOffset": 14}, {"referenceID": 7, "context": "While [6], [7], [8] and [26] use the rectified linear activation function and the dropout regularization [22] [30].", "startOffset": 16, "endOffset": 19}, {"referenceID": 25, "context": "While [6], [7], [8] and [26] use the rectified linear activation function and the dropout regularization [22] [30].", "startOffset": 24, "endOffset": 28}, {"referenceID": 21, "context": "While [6], [7], [8] and [26] use the rectified linear activation function and the dropout regularization [22] [30].", "startOffset": 105, "endOffset": 109}, {"referenceID": 29, "context": "While [6], [7], [8] and [26] use the rectified linear activation function and the dropout regularization [22] [30].", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": "10 [19] also provides results without using the external lexicon", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In contrast to [19], which has used several features of words (prefixes, suffixes, capitalisation information etc.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "This language model is like the one in [21], except it uses both words/labels in the past and in the future to predict next word/label.", "startOffset": 39, "endOffset": 43}, {"referenceID": 25, "context": "The results in the higher part of table 4 show that the best model on the ATIS task, with these settings, is the Elman RNN of [26].", "startOffset": 126, "endOffset": 130}, {"referenceID": 25, "context": "Note that it is not clear how the improvements of [26] with respect to [7] (in part due to same authors) have been obtained.", "startOffset": 50, "endOffset": 54}, {"referenceID": 6, "context": "Note that it is not clear how the improvements of [26] with respect to [7] (in part due to same authors) have been obtained.", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "Indeed, in [7] authors obtain the best result with a Jordan RNN, while in [26] an Elman RNN gets the best performance.", "startOffset": 11, "endOffset": 14}, {"referenceID": 25, "context": "Indeed, in [7] authors obtain the best result with a Jordan RNN, while in [26] an Elman RNN gets the best performance.", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "During our experiments, using the same experimentation protocol as [26], we could not reach the same performances.", "startOffset": 67, "endOffset": 71}, {"referenceID": 25, "context": "We conclude that the difference between our results and those in [26] are due to reasons mentioned above.", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "Beyond this, we note that our Elman and Jordan RNN implementations are equivalent to those of [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 7, "context": "This limitation is confirmed by the results obtained by [8] and [26] using CRFs.", "startOffset": 56, "endOffset": 59}, {"referenceID": 25, "context": "This limitation is confirmed by the results obtained by [8] and [26] using CRFs.", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "We can also see that our implementations of Elman and Jordan RNNs are comparable, even better in the case of Elman RNN, with state-of-the-art RNNs of [8].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "More importantly, results on the MEDIA task shows that in this particular experimental settings where taking label dependencies into account is crucial, our new variants are remarkably more effective than both our implementations and state-of-the-art implementations [8] of Elman and Jordan RNNs.", "startOffset": 267, "endOffset": 270}, {"referenceID": 18, "context": "On this task, we compare our RNN implementations to the state-of-the-art results obtained in [19] with the modelMEltfr.", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "We would like to underline thatMElt 0 fr, when it does not use external resources like in the model obtaining the best absolute result in [19], nevertheless uses several features associated with words that provide an advantage over features used in our RNNs.", "startOffset": 138, "endOffset": 142}, {"referenceID": 27, "context": "We also provide the results of [28] and [29] for comparison with the state-of-the-art.", "startOffset": 31, "endOffset": 35}, {"referenceID": 28, "context": "We also provide the results of [28] and [29] for comparison with the state-of-the-art.", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "Instead, we can roughly compare our results with those of [5], which were also obtained with neural networks.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "Note that this is a rough comparison as the model of [5], though not a RNN, integrates capitalisation features and uses a convolution and a max-over-time layer to encode large context information.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "Our RNNs don\u2019t improve the state-of-the-art, but are all more effective than the model of [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "This result is particularly important, as it shows that RNNs, even without using a sophisticated encoding of the context like the model NN+SLL in [5],", "startOffset": 146, "endOffset": 149}, {"referenceID": 4, "context": "This claim is enforced also by the fact that NN+SLL of [5] implements a global probability computation strategy similar to CRF (SLL stands for Sentence Level Likelihood), while all RNNs presented here use a local decision function (see equation 2).", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "[8] also shows results obtained with embeddings pre-trained with", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "A similar idea of hybrid RNN model has been tested in [26] without showing a clear advantage on Elman and Jordan RNNs.", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "Words [7] E-RNN 93.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "12% \u2013 [7] J-RNN 93.", "startOffset": 6, "endOffset": 9}, {"referenceID": 25, "context": "98% [26] E-RNN 94.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "Classes [8] E-RNN 96.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "16% \u2013 \u2013 [8] CRF \u2013 \u2013 95.", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "23% [6] E-RNN 96.", "startOffset": 4, "endOffset": 7}, {"referenceID": 25, "context": "04% \u2013 \u2013 [26] E-RNN 96.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "29% [26] CRF \u2013 \u2013 95.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "forward backward bidirectional [8] E-RNN 81.", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "94% \u2013 \u2013 [8] J-RNN 83.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "25% \u2013 \u2013 [8] CRF \u2013 \u2013 86.", "startOffset": 8, "endOffset": 11}, {"referenceID": 18, "context": "[19] MElt0fr \u2013 \u2013 97.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "forward backward bidirectional [28] \u2013 \u2013 97.", "startOffset": 31, "endOffset": 35}, {"referenceID": 28, "context": "24% [29] \u2013 \u2013 97.", "startOffset": 4, "endOffset": 8}, {"referenceID": 4, "context": "33% [5] NN+SLL \u2013 \u2013 96.", "startOffset": 4, "endOffset": 7}, {"referenceID": 30, "context": "This problem is someway similar to the \u201cvanishing gradient\u201d problem [31]: as the network learns, the probability gets concentrated on few dimensions and all the other values get very small, limiting network learning.", "startOffset": 68, "endOffset": 72}], "year": 2016, "abstractText": "In this paper we study different types of Recurrent Neural Networks (RNN) for sequence labeling tasks. We propose two new variants of RNNs integrating improvements for sequence labeling, and we compare them to the more traditional Elman and Jordan RNNs. We compare all models, either traditional or new, on four distinct tasks of sequence labeling: two on Spoken Language Understanding (ATIS and MEDIA); and two of POS tagging for the French Treebank (FTB) and the Penn Treebank (PTB) corpora. The results show that our new variants of RNNs are always more effective than the others.", "creator": "LaTeX with hyperref package"}}}