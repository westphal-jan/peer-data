{"id": "1606.02601", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "A Joint Model for Word Embedding and Word Morphology", "abstract": "seminal paper presents any joint model for performing unsupervised behavior analysis surrounding words, first has a character - level communication model matching familiarity between word embeddings. our technique splits individual words into segments, and studies each segment according however its pattern accurately predict entire words. initial computational analysis appears susceptible to typical meta analyzers at the task representing spatial collision recovery, and also none better than word - length embedding models at the task of syntactic conflict answering. today, we show that computational morphology explicitly utilizing character - level models requires them produce embeddings for each relatives : pair better with similarity judgments.", "histories": [["v1", "Wed, 8 Jun 2016 15:24:22 GMT  (386kb,D)", "http://arxiv.org/abs/1606.02601v1", "Submission for first Representation Learning for NLP workshop at ACL2016"]], "COMMENTS": "Submission for first Representation Learning for NLP workshop at ACL2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kris cao", "marek rei"], "accepted": false, "id": "1606.02601"}, "pdf": {"name": "1606.02601.pdf", "metadata": {"source": "CRF", "title": "A Joint Model for Word Embedding and Word Morphology", "authors": ["Kris Cao"], "emails": ["kc391@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Word embedding models associate each word in a corpus with a vector in a semantic space. These vectors can either be learnt to optimize performance in a downstream task (Bengio et al., 2003; Collobert et al., 2011) or learnt via the distributional hypothesis: words with similar contexts have similar meanings (Harris, 1954; Mikolov et al., 2013a). Current word embedding models treat words as atomic. However, words follow a power law distribution (Zipf, 1935), and word embedding models suffer from the problem of sparsity: a word like \u2018unbelievableness\u2019 does not appear at all in the first 17 million words of Wikipedia, even though it is derived from common morphemes. This leads to three problems:\n1. word representations decline in quality for\nrarely observed words (Bullinaria and Levy, 2007).\n2. word embedding models handle out-ofvocabulary words badly, typically as a single \u2018OOV\u2019 token.\n3. the word distribution has a long tail, and many parameters are needed to capture all of the words in a corpus (for an embedding size of 300 with a vocabulary of 10k words, 3 million parameters are needed)\nOne approach to smooth word distributions is to operate on the smallest meaningful semantic unit, the morpheme (Lazaridou et al., 2013; Botha and Blunsom, 2014). However, previous work on the morpheme level has all used external morphological analyzers. These require a separate preprocessing step, and cannot be adapted to suit the problem at hand.\nAnother is to operate on the smallest orthographic unit, the character (Ling et al., 2015; Kim et al., 2016). However, the link between shape and meaning is often complicated (de Saussure, 1916), as alphabetic characters carry no inherent semantic meaning. To account for this, the model has to learn complicated dependencies between strings of characters to accurately capture word meaning. We hypothesize that explicitly introducing morphology into character-level models can help them learn morphological features, and hence word meaning.\nIn this paper, we introduce a word embedding model that jointly learns word morphology and word embeddings. To the best of our knowledge, this is the first word embedding model that learns morphology as part of the model. Our guiding intuition is that the words with the same stem have similar contexts. Thus, when considering word segments in terms of context-predictive power, the\nar X\niv :1\n60 6.\n02 60\n1v 1\n[ cs\n.C L\n] 8\nJ un\n2 01\n6\nsegment corresponding to the stem will have the most weight.\nOur model \u2018reads\u2019 the word and outputs a sequence of word segments. We weight each segment, and then combine the segments to obtain the final word representation. These representations are trained to predict context words, as this has been shown to give word representations which capture word semantics well (Mikolov et al., 2013b). As the root morpheme has the most context-predictive power, we expect our model to assign high weight to this segment, thereby learning to separate root+affix structures.\nOne exciting feature of character-level models is their ability to represent open-vocabulary words. After training, they can predict a vector for any word, not just words that they have seen before. Our model has an advantage in that it can split unknown words into known and unknown components. Hence, it can potentially generalise better over seen morphemes and words and apply existing knowledge to new cases.\nTo evaluate our model, we evaluate its use as a morphological analyzer (\u00a74.1), test how well it learns word semantics, including for unseen words (\u00a74.2), and examine the structure of the embedding space (\u00a74.3)."}, {"heading": "2 Related Work", "text": "While words are often treated as the fundamental unit of language, they are in fact themselves compositional. The smallest unit of semantics is the morpheme, while the smallest unit of orthography is the grapheme, or character. Both have been used as a method to go beyond word-level models."}, {"heading": "2.1 Morphemic analysis and semantics", "text": "As word semantics is compositional, one might ask whether it is possible to learn morpheme representations, and compose them to obtain good word representations. Lazaridou et al. (2013) demonstrated precisely this: one can derive good representations of morphemes distributionally, and apply tools from compositional distributional semantics to obtain good word representations. Luong et al. (2013) also trained a morphological composition model based on recursive neural networks. Botha and Blunsom (2014) built a language model incorporating morphemes, and demonstrated improvements in language modelling and in machine translation. All of these\napproaches incorporated external morphological knowledge, either in the form of gold standard morphological analyses such as CELEX (Baayen et al., 1995) or an external morphological analyzer such as Morfessor (Creutz and Lagus, 2007).\nUnsupervised morphology induction aims to decide whether two words are morphologically related or to generate a morphological analysis for a word (Goldwater et al., 2005; Goldsmith, 2001). While they may use semantic insights to perform the morphological analysis (Soricut and Ochs, 2015), they typically are not concerned with obtaining a semantic representation for morphemes, nor of the resulting word."}, {"heading": "2.2 Character-level models", "text": "Another approach to go beyond words is based on on character-level neural network models. Both recurrent and convolutional architectures for deriving word representations from characters have been used, and results in downstream tasks such as language modelling and POS tagging have been promising, with reductions in word perplexity for language modelling and state-of-the-art English POS tagging accuracy (Ling et al., 2015; Kim et al., 2016). Ballesteros et al. (2015) train a character-level model for parsing. Zhang et al. (2015) do away with words completely, and train a convolutional neural network to do text classification directly from characters.\nExcitingly, character-level models seem to capture morphological effects. Examining nearest neighbours of morphologically complex words in character-aware models often shows other words with the same morphology (Ling et al., 2015; Kim et al., 2016). Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging (Xu et al., 2015; Toutanova et al., 2003). By explicitly modelling these features, one might expect good performance gains in many NLP tasks.\nWhat is less clear is how well these models learn word semantics. Classical word embedding models seem to capture word semantics, and the nearest neighbours of a given word are typically semantically related words (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013). In addition, the correlation between model word similarity scores and human similarity judgments is typically high (Levy et al., 2015). However, no previous work (to our knowledge) evaluates the similarity judgments\nof character-level models against human annotators."}, {"heading": "3 The Char2Vec model", "text": "We hypothesize that by incorporating morphological knowledge directly into a character-level model, one can improve the ability of characterlevel models to learn compositional word semantics. In addition, we hypothesize that incorporating morphological knowledge helps structure the embedding space in such a way that affixation corresponds to a regular shift in the embedding space. We test both hypotheses directly in \u00a74.2 and \u00a74.3 respectively.\nThe starting point for our model is the skipgram with negative sampling (SGNS) objective of Mikolov et al. (2013b). For a vocabulary V of size |V | and embedding size N , SGNS learns two embedding tables W,C \u2208 RN\u00d7|V |, the target and context vectors. Every time a word w is seen in the corpus with a context word c, the tables are updated to maximize\nlog \u03c3(w \u00b7 c) + k\u2211\ni=1\nEc\u0303i\u223cP (w)[log \u03c3(\u2212w \u00b7 c\u0303i)] (1)\nwhere P (w) is a noise distribution from which we draw k negative samples. In the end, the target\nvector for a word w should have high inner product with context vectors for words with which it is typically seen, and low inner products with context vectors for words it is not typically seen with. Figure 1 illustrates this for a particular example. In Mikolov et al. (2013b), the noise distribution P (w) is proportional to the unigram probability of a word raised to the 3/4th power (Mikolov et al., 2013b).\nOur innovation is to replace W with a trainable function f that accepts a sequence of characters and returns a vector of length N (i.e. f : A<\u03c9 \u2192 RN , where A is the alphabet we are considering and A<\u03c9 denotes the finite length strings over the alphabet A). We still keep the table of context embeddings C, and our model objective is still to minimize log \u03c3(f(w) \u00b7c)+ k\u2211\ni=1\nEc\u0303i\u223cP (w)[log \u03c3(\u2212f(w) \u00b7 c\u0303i)]\n(2) where we now treat w as a sequence of characters. After training, f can be used to produce an embedding for any sequence of characters, even if it was not previously seen in training.\nThe process of calculating f on a word is illustrated in Figure 2. We first pad the word with beginning and end of word tokens, and then pass the characters of the word into a character lookup table. As the link between characters and morphemes is non-compositional and requires essentially memorizing a sequence of characters, we use LSTMs (Hochreiter and Schmidhuber, 1997) to encode the letters in the word, as they have been shown to capture non-local and non-linear dependencies. We run a forward and a backward LSTM over the character embeddings. The forward LSTM reads the beginning of word symbol, but not the end of word symbol, and the backward LSTM reads the end of word symbol but not the beginning of word symbol. This is necessary to align the resulting embeddings, so that the LSTM hidden states taken together correspond to a partition of the word into two without overlap.\nThe LSTMs output two sequences of vectors hf0 , . . . , h f n and hbn, . . . , h b 0. We then concatenate the resulting vectors, and pass them through a shared feed-forward layer to obtain a final sequence of vectors hi. Each vector corresponds to two half-words: one half read by the forward LSTM, and the other by the backward LSTM.\nWe then learn an attention model over these hid-\nden states: given a hidden state hi, we calculate a weight \u03b1i = a(hi) such that \u2211 \u03b1i = 1, and then calculate the resulting vector for the word w as f(w) = \u2211 \u03b1ihi. Following Bahdanau et al. (2014), we calculate a as\na(hi) = exp(vT tanh(Whi))\u2211 j exp(v T tanh(Whj)) (3)\ni.e. a softmax over the hidden states."}, {"heading": "3.1 Capturing morphology via attention", "text": "Previous work on bidirectional LSTM characterlevel models used both LSTMs to read the entire word (Ling et al., 2015; Ballesteros et al., 2015). This can lead to redundancy, as both LSTMs are used to capture the full word. In contrast, our model is capable of splitting the words and optimizing the two LSTMs for modelling different halves. This means one of the LSTMs can specialize on word prefixes and roots, while the other memorizes possible suffixes. In addition, when dealing with an unknown word, it can be split into\nknown and unknown components. The model can then use the semantic knowledge it has learnt for a known component to predict a representation for the unknown word as a whole.\nWe hypothesize that the natural place to split words is on morpheme boundaries, as morphemes are the smallest unit of language which carry semantic meaning. We test the splitting capabilities of our model in \u00a74.1."}, {"heading": "4 Experiments", "text": "We evaluate our model on three tasks: morphological analysis (\u00a74.1), semantic similarity (\u00a74.2), and analogy retrieval (\u00a74.3). We trained all of the models once, and then use the same trained model for all three tasks \u2013 we do not perform hyperparameter tuning to optimize performance on each task.\nWe trained our Char2Vec model on the Text8 corpus, consisting of the first 100MB of a 2006\ncleaned-up dump of Wikipedia1. We only trained on words which appeared more than 5 times in our corpus. We used a context window size of 3 words either side of the target word, and took 11 negative samples per positive sample, using the same smoothed unigram distribution as word2vec. The model was trained for 3 epochs using the Adam optimizer (Kingma and Ba, 2015). All experiments were carried out using Keras (Chollet, 2015) and Theano (Bergstra et al., 2010; Bastien et al., 2012). We initialized the context lookup table using word2vec2, and kept it fixed during training. 3 In all character-level models, the character embeddings have dimension dC = 64, while the forward and backward LSTMs have dimension dLSTM = 256. The concatenation of both therefore has dimensionality d = 512. The concatenated LSTM hidden states are then compressed down to dword = 256 by a feed-forward layer.\nAs baselines, we trained a SGNS model on the same dataset with the same parameters. To test how much the attention model helps the characterlevel model to generalize, we also trained the Char2Vec model without the attention layer, but with the same parameters. In this model, the word embeddings are just the concatenation of the final forward and backward states, passed through a feedforward layer. We refer to this model as C2VNO-ATT. We also constructed count-based vectors using SVD on PPMI-weighted co-occurence counts, with a window size of 3. We kept the top 256 principal components in the SVD decomposition, to obtain embeddings with the same size as our other models."}, {"heading": "4.1 Morphological awareness", "text": "The main innovation of our Char2Vec model compared to existing recurrent character-level models is the capability to split words and model each half independently. Here we test whether our model segmentations correspond to gold-standard morphological analyses.\nWe obtained morphological analyses for all the words in our training vocabulary which were in the English Lexicon Project (Balota et al., 2007). We then converted these into surface-level segmenta-\n1available at mattmahoney.net/dc/text8 2We use the Gensim implementation: https://radimrehurek.com/gensim/ 3We experimented with updating the initialized context lookup tables, and with randomly initialized context lookups, but found they were influenced too much by orthographic similarity from the character encoder.\ntions using heuristic affix-matching, and used this as a gold-standard morphemic analysis. We ended up with 14682 words, of which 7867 have at least two morphemes and 1138 have at least three.\nEvaluating morphological segmentation is a long-debated issue (Cotterell et al., 2016). Traditional hard morphological analyzers are normally evaluated on border F1 \u2013 that is, how many morpheme borders are recovered. However, our model does not actually posit any hard morpheme borders. Instead, it just associates each character boundary with a weight. Therefore, we treat the problem of recovering intra-word morpheme boundaries as a ranking problem. We rank each inter-character boundary of a word according to our model weights, and then evaluate whether our model ranks morpheme boundaries above nonmorpheme boundaries.\nWe use mean average precision (MAP) as our evaluation metric. We first calculate precision at N for each word, until all the gold standard morpheme boundaries have been recovered. Then, we average over N to obtain the average precision (AP) for that word. We then calculate the mean of the APs across all words to obtain the MAP for the model.\nWe report results of a random baseline as a point of comparison, which randomly places morpheme boundaries inside the word. We also report the results of the Porter stemmer4, where we place a morpheme boundary at the end of the stem, then randomly thereafter.\nFinally, we trained Morfessor 2.05 (Creutz and Lagus, 2007) on our corpus, using an initial random split value of 0.9, and stopping training when the difference in loss between successive epochs is less than 0.1% of the total loss. We then used our trained Morfessor model to predict morpheme boundaries6, and randomly permuted the morpheme boundaries and ranked them ahead of randomly permuted non-morpheme boundaries to calculate MAP.\nAs the test set is dominated by words with simple morphology, we also extracted all the morphologically rich words with 3 or more morphemes, and created a separate evaluation on this subsection. We report the results in Table 1.\n4We used the NLTK implementation 5We used the Python implementation 6We found Morfessor to be quite conservative by default in its segmentations. The 2nd ranked segmentation gave better MAPs, which are the results we describe.\nAs the results show, our model performs the best out of all the methods at analysing morphologically rich words with multiple morphemes. On these words, our model even outperforms Morfessor, which is explicitly designed as a morphological analyzer. This shows that our model learns splits which correspond well to human morphological analysis, even though we build no morphological knowledge into our model. However, when evaluating on all words, the Porter stemmer has a great advantage, as it is rule-based and able to give just the stem of words with great precision, which is effectively giving a canonical segmentation for words with just 2 morphemes.\nWe show some model analyses against the gold standard in Table 2."}, {"heading": "4.2 Capturing semantic similarity", "text": "Next, we tested our model similarity scores against human similarity judgments. For these datasets, human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman\u2019s \u03c1) between model judgments and human judgments (Levy et al., 2015).\nWe use the WordSim353 dataset (Finkelstein et al., 2002), the test split of the MEN dataset (Bruni et al., 2014), and the Rare Word (RW) dataset (Luong et al., 2013). The word pairs in the Word-\nSim353 and MEN datasets are typically simple, commonly occurring words denoting basic concepts, whereas the RW dataset contains many morphologically derived words which have low corpus frequencies. This is reflected by how many of the test pairs in each dataset contain out of vocabulary (OOV) items: 3/353 and 6/1000 of the word pairs in WordSim353 and MEN, compared with 1083/2034 for the RW dataset.\nWe report results for in-corpus word pairs in Table 3, and for all word pairs for those models able to predict vectors for unseen words in Table 4.\nOverall, word-based embedding models learn vectors that correlate better with human judgments, particularly for morphologically simple words. However, character-based models are competitive with word-based models on the RW dataset. While the words in this dataset appear rarely in our corpus (of the in-corpus words, over half appear fewer than 100 times), each morpheme may be common, and the character-level models can use this information. We note that on the entire RW dataset (of which over half contain an OOV word), the character-based models still perform reasonably. We also note that on word pairs in the RW test containing at least one OOV word, the full Char2Vec model outperforms the C2V model without morphology. This suggests that characterbased embedding models are learning to morphologically analyse complex word forms, even on unseen words, and that giving the model the capability to learn word segments independently helps\nthis process. We also present some word nearest neighbours for our Char2Vec model in Table 5, both on the whole vocabulary and then filtering the nearest neighbours to only include words which appear 100 times or more in our corpus. This corresponds to keeping the top 10k words, which is common among language models (Ling et al., 2015; Kim et al., 2016). We note that nearest neighbour predictions include words that are orthographically distant but semantically similar, showing that our model has the capability to learn to compose characters into word meanings.\nWe also note that word nearest neighbours seem to be more semantically coherent when rarelyobserved words are filtered out of the vocabulary, and more based on orthographic overlap when the entire vocabulary is included. This suggests that for rarely-observed words, the model is basing its predictions on orthographic analysis, whereas for more commonly observed words it can \u2018memorize\u2019 the mapping between the orthography and word semantics."}, {"heading": "4.3 Capturing syntactic and semantic regularity", "text": "Finally, we evaluate the structure of the embedding space of our various models. In particular, we test whether affixation corresponds to regular linear shifts in the embedding space.\nTo do this, we use the Google analogy dataset (Mikolov et al., 2013a). This consists of 19544 questions of the form \u201cA is to B as C is to X\u201d. We split this collection into semantic and syntactic\nsections, based on whether the analogies between the words are driven by morphological changes or deeper semantic shifts. Example semantic questions are on capital-country relationships (\u201cParis is to France as Berlin is to X) and currencycountry relationships. Example syntactic questions are adjective-adverb relationships (\u201camazing is to amazingly as apparent is to X\u201d) and opposites formed by prefixing a negation particle (\u201cacceptable is to unacceptable as aware is to X\u201d). This results in 5537 semantic analogies and 10411 syntactic analogies.\nWe use the method of Mikolov et al. (2013a) to answer these questions. We first `2-normalize all of our word vectors. Then, to answer a question of the form \u201cA is to B as C is to X\u201d, we find the word w which satisfies\nw = argmax w\u2208V\u2212{a,b,c}\ncos(w, b\u2212 a+ c) (4)\nwhere a, b, c are the word vectors for the words A, B and C respectively.\nWe report the results in Table 6. The most intriguing result is that character-level models are competitive with word-level models for syntactic analogy, with our Char2Vec model holding the\nbest result for syntactic analogy answering. This suggests that incorporating morphological knowledge explicitly rather than latently helps the model learn morphological features. However, on the semantic analogies, the character-based models do much worse than the word-based models. This is perhaps unsurprising in light of the previous section, where we demonstrate that character-based models do worse at the semantic similarity task than word-level models."}, {"heading": "5 Discussion", "text": "We only report results for English. However, English is a morphologically impoverished language, with little inflection and relatively few productive patterns of derivation. Our morphology test set reflects this, with over half the words consisting of a simple morpheme, and over 90% having at most 2 morphemes.\nThis is unfortunate for our model, as it performs better on words with richer morphology. It gives consistently more accurate morphological analyses for these words compared to standard baselines, and matches word-level models for semantic similarity on rare words with rich morphology. In addition, it seems to learn morphosyntactic features to help solve the syntactic analogy task. Most of all, it is language-agnostic, and easy to port across different languages. We thus expect our model to perform even better for languages with a richer morphology than English, such as Turkish and German."}, {"heading": "6 Conclusion", "text": "In this paper, we present a model which learns morphology and word embeddings jointly. Given a word, it splits the word in to segments and ranks the segments based on their context-predictive power. Our model can segment words into morphemes, and also embed the word into a representation space.\nWe show that our model is competitive at the task of morpheme boundary recovery compared to a dedicated morphological analyzer, beating dedicated analyzers on words with a rich morphology. We also show that in the representation space word affixation corresponds to linear shifts, demonstrating that our model can learn morphological features.\nFinally, we show that character-level models, while outperformed by word-level models gener-\nally at the task of semantic similarity, are competitive at representing rare morphologically rich words. In addition, the character-level models can predict good quality representations for unseen words, with the morphologically aware characterlevel model doing slightly better."}], "references": [{"title": "Richard Piepenbrock", "author": ["Harald R. Baayen"], "venue": "and Leon Gulikers.", "citeRegEx": "Baayen et al.1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Chris Dyer", "author": ["Miguel Ballesteros"], "venue": "and Noah A Smith.", "citeRegEx": "Ballesteros et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Simpson", "author": ["David A. Balota", "Melvin J. Yap", "Keith A. Hutchison", "Michael J. Cortese", "Brett Kessler", "Bjorn Loftis", "James H. Neely", "Douglas L. Nelson", "Greg B"], "venue": "and Rebecca Treiman.", "citeRegEx": "Balota et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Nicolas Bouchard", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bastien et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Pascal Vincent", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme"], "venue": "and Christian Janvin.", "citeRegEx": "Bengio et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "David WardeFarley", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bergstra et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Compositional Morphology for Word Representations and Language Modelling", "author": ["Botha", "Blunsom2014] Jan A. Botha", "Phil Blunsom"], "venue": null, "citeRegEx": "Botha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Botha et al\\.", "year": 2014}, {"title": "Nam Khanh Tran", "author": ["Elia Bruni"], "venue": "and Marco Baroni.", "citeRegEx": "Bruni et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Bullinaria and Joseph P", "author": ["A John"], "venue": "Levy.", "citeRegEx": "Bullinaria and Levy2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Koray Kavukcuoglu", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen"], "venue": "and Pavel Kuksa.", "citeRegEx": "Collobert et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Tim Vieira", "author": ["Ryan Cotterell"], "venue": "and Hinrich Sch\u00fctze.", "citeRegEx": "Cotterell et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Creutz", "Lagus2007] Mathias Creutz", "Krista Lagus"], "venue": "ACM Trans. Speech Lang. Process.,", "citeRegEx": "Creutz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Creutz et al\\.", "year": 2007}, {"title": "Gadi Wolfman", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan"], "venue": "and Eytan Ruppin.", "citeRegEx": "Finkelstein et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Unsupervised learning of the morphology of a natural language", "author": ["John Goldsmith"], "venue": "Computational Linguistics,", "citeRegEx": "Goldsmith.,? \\Q2001\\E", "shortCiteRegEx": "Goldsmith.", "year": 2001}, {"title": "and Thomas L", "author": ["Sharon Goldwater", "Mark Johnson"], "venue": "Griffiths.", "citeRegEx": "Goldwater et al.2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "and Alexander M", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag"], "venue": "Rush.", "citeRegEx": "Kim et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization. ICLR", "author": ["Kingma", "Ba2015] Diederik P. Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Roberto Zamparelli", "author": ["Angeliki Lazaridou", "Marco Marelli"], "venue": "and Marco Baroni.", "citeRegEx": "Lazaridou et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Yoav Goldberg", "author": ["Omer Levy"], "venue": "and Ido Dagan.", "citeRegEx": "Levy et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Tiago Lu\u0131\u0301s", "author": ["Wang Ling"], "venue": "Lu\u0131\u0301s Marujo, Ram\u00f3n Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W. Black, and Isabel Trancoso.", "citeRegEx": "Ling et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Christopher D", "author": ["Minh-Thang Luong", "Richard Socher"], "venue": "Manning.", "citeRegEx": "Luong et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "ICLR Workshop", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Mnih", "Kavukcuoglu2013] Andriy Mnih", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Soricut", "Ochs2015] Radu Soricut", "Franz Ochs"], "venue": null, "citeRegEx": "Soricut et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soricut et al\\.", "year": 2015}, {"title": "Manning", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D"], "venue": "and Yoram Singer.", "citeRegEx": "Toutanova et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Michael Auli", "author": ["Wenduan Xu"], "venue": "and Stephen Clark.", "citeRegEx": "Xu et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Junbo Zhao", "author": ["Xiang Zhang"], "venue": "and Yann LeCun.", "citeRegEx": "Zhang et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "This paper presents a joint model for performing unsupervised morphological analysis on words, and learning a character-level composition function from morphemes to word embeddings. Our model splits individual words into segments, and weights each segment according to its ability to predict context words. Our morphological analysis is comparable to dedicated morphological analyzers at the task of morpheme boundary recovery, and also performs better than word-based embedding models at the task of syntactic analogy answering. Finally, we show that incorporating morphology explicitly into character-level models help them produce embeddings for unseen words which correlate better with human judgments.", "creator": "LaTeX with hyperref package"}}}