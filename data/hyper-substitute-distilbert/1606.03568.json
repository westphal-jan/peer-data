{"id": "1606.03568", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2016", "title": "Word Sense Disambiguation using a Bidirectional LSTM", "abstract": "with 2016 article we tested simulation model that builds a bidirectional hybrid short - term memory algorithm that learn word sense disambiguation preferences from data. the approach is end - ending - end trainable therefore ensures optimization solutions low word order. further, to improve the robustness of global model we introduce dropword, implicit regularization behavior that quickly removes light from surrounding stimuli. the model thus evaluated on our standard samples... achieves state - out - not - art results identifying both datasets, using gps sensor settings.", "histories": [["v1", "Sat, 11 Jun 2016 08:12:02 GMT  (24kb)", "http://arxiv.org/abs/1606.03568v1", null], ["v2", "Fri, 18 Nov 2016 22:47:07 GMT  (37kb)", "http://arxiv.org/abs/1606.03568v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["mikael k{\\aa}geb\\\"ack", "hans salomonsson"], "accepted": false, "id": "1606.03568"}, "pdf": {"name": "1606.03568.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["kageback@chalmers.se", "hans.salomonsson@chalmers.se"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n03 56\n8v 1\n[ cs\n.C L\n] 1\n1 Ju\nn 20\n16"}, {"heading": "1 Introduction", "text": "Words are in general ambiguous and can have several related or unrelated meanings depending on context. For instance, the word rock can refer to both a stone and a music genre, but in the sentence \u201dWithout the guitar, there would be no rock music\u201d the sense is no longer ambiguous. From this example it is easy to see that the context surrounding the word is what defines the sense. However, it may not be so obvious that this is a difficult task. To see this, consider instead the sentence \u201dHard rock crushes heavy metal\u201d where all words in themselves seems to indicate stone but together they actually define the word token as music. Hence, to do word sense disambiguation (WSD) well we need to go beyond bag of words and into the territory of sequence modeling.\nImproved WSD would be beneficial to many of the big natural language processing (NLP) problems, e.g. machine translation, information Retrieval, information Extraction (Vickrey et al., 2005; Navigli, 2009), and basic NLP tasks like sense aware word representations (Neelakantan et al., 2015;\nNieto Pin\u0303a and Johansson, 2015; Bovi et al., 2015). However, for word representations the benefit goes two ways which was shown by Taghipour and Ng (2015) who used word embeddings to improve on WSD, and later by Johansson and Pina (2015) who combined word embeddings with a knowledge graph approach. Though much progress has been done in the area, most current WSD systems suffer one of two problems. They either disregard the order of words or they rely on complicated hand crafted features. The first problem will lead to systems with low precision since many different contexts, potentially corresponding to different senses, will consist of the same bag of words. The second problem usually lead to high precision but will inevitable lack in coverage and therefore produce a low recall system. We aim to mitigate these problems by (1) modeling the sequence of words surrounding the target word instead of a bag, and (2) refrain from using any hand crafted features but let the model learn the features directly from the text during training.\nThe main contributions of this work include:\n\u2022 An end\u2013to\u2013end trainable WSD system that achieves state-of-the-art result on two standard datasets without any hand crafted features.\n\u2022 A novel regularization method for word sequences, dropword.\n\u2022 Empirical evidence that highlights the importance of word order for WSD."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Long short-term memory", "text": "Long short-term memory (LSTM) is a gated type of recurrent neural network (RNN). LSTMs were introduced by\nHochreiter and Schmidhuber (1997) to enable RNNs to better capture long term dependencies when used to model sequences. This is achieved by letting the model copy the state between timesteps without forceing the state through a non-linearity. The flow of information is instead regulated using multiplicative gates,\nin =\u03c3(W xixn +W hihn\u22121 +W cicn\u22121 + bi)\nfn =\u03c3(W xfxn +W hfhn\u22121 +W cfcn\u22121 + bf )\non =\u03c3(W xoxn +W hohn\u22121 +W cocn\u22121 + bo)\nthe state of the network is saved in a memory cell,\ncn = fncn\u22121+in tanh(W xcxn+W hchn\u22121+bc)\nand the output\nhn = on tanh(cn) (1)\nis computed using the state of the memory cell (cn) and the multiplicative gate (on). b[i|f |o|c] are bias units and \u03c3 is the logistic function. The behavior of all gates are learned from data together with the rest of the model, and captured in the model parameters: \u0398 = {W xi,W xf ,W xo,W hc,W xc,bc, W hi,W ci,bi,W hf ,W cf ,bf ,W ho,W co,bo}."}, {"heading": "2.2 Bidirectional LSTM", "text": "The bidirectional variant of LSTM, (BLSTM) (Graves and Schmidhuber, 2005) is a straightforward adaptation of the LSTM described in Section 2.1 where the state at each time step consist of the state of two LSTMs, one going left and one going right. For WSD this means that the state has information about both preceding words and succeeding words, which in many cases is absolutely necessary to correctly classify the sense."}, {"heading": "2.3 Global vectors for word representation", "text": "Global Vectors for Word Representation (GloVe), introduced by Pennington et al. (2014) is a hybrid approach to word representation that combine a log-linear model, made popular by Mikolov et al. (2013), with counting based cooccurrence statistics to more efficiently capture global statistics. GloVe vectors are trained in an unsupervised fashion, typically on large amounts of data, and is able to capture fine grained semantic and syntactic information about words. These vectors can subsequently be used to initialize the input layer of a neural network or some other NLP model."}, {"heading": "2.4 Regularization", "text": "When training powerful nonlinear classifiers like neural networks, good regularization is essential for generalization. One of the most basic regularization techniques is to add Gaussian noise to the inputs (Sietsma and Dow, 1991). This can be viewed as a type of data augmentation, i.e. distorting the training data to create slightly different examples for each pass through the data, making it harder for the model to simply memorize the training set. Another, more recent noise based regularization technique is dropout, introduced by Srivastava et al. (2014). Dropout lowers the capacity of the model by randomly erasing information from a layer of neurons by setting a subset of them to zero. This forces the network to learn independent sub networks which improves the generalization in a similar way as to train many separate models and combining the results."}, {"heading": "3 Dropword", "text": "Besides the regularization techniques described in Section 2.4 we present a novel regularizer referred to as dropword. The idea behind dropword is similar to that of dropout but instead of randomly removing a part of the activations, we randomly replace some words in the context with a <dropped> tag. The tag is treated just like any other word in the vocabulary and has a corresponding word embedding that is trained. This process is repeated over time, so that the words dropped change with time. The motivation for doing dropword is to decrease the dependency on individual words in the training context. This technique can be generalized to other kinds of sequential inputs, not only words."}, {"heading": "4 The Model", "text": "Given a document and the position of the target word, i.e. the word to disambiguate, the model computes a probability distribution over the possible senses corresponding to that word. The architecture of the model, depicted in Figure 1, consist of a softmax layer, a hidden layer, and a BLSTM. See Section 2.2 for more details regarding the BLSTM. The BLSTM and the hidden layer share parameters over all word types and senses, while the softmax is parameterized by word type and selects the corresponding weight matrix and bias vector for each word type respectively. This structure enables the model to share statistical strength\nacross different word types while remaining computationally efficient even for a large total number of senses."}, {"heading": "4.1 Model definition", "text": "The input to the BLSTM at position n in document D is computed as\nxn = W xv(wn), n \u2208 {1, . . . , |D|}. (2)\nHere, v(wn) is the one-hot representation of the word type corresponding to wn \u2208 D. This will have the effect of picking the column from W x corresponding to that word type. The resulting vector is referred to as word embedding. Further, W x can be initialized using pre-trained word embeddings, to leverage large unannotated datasets. In this work GloVe vectors were used for this purpose.\nThe model output, y(n), is a predicted sense distribution for a word at position n, and is computed via the hidden layer a as\na = W ha[hn\u22121;hn+1] + b ha (3) y(n) = softmax(W aywna+ b ay wn), (4)\nwhere [hn\u22121;hn+1] is the concatenated outputs of the right and left traversing LSTMs of the BLSTM at word n, see Equation (1). W ha and bha are the weights and biases for the hidden layer, and W aywn and baywn are the weights and biases for the softmax layer corresponding to the word at position n. Hence, each word type will have its own softmax parameters, with dimensions depending on the number of senses of that particular word type. The softmax is the multidimensional generalization of the logistic function defined as\nsoftmaxj(z(n)) = exp(zj(n))\n\u2211|S(wn)| k=1 exp(zk(n)) ,\nwhere z(n) = W aywna+ b ay wn , and S(wn) is the set of senses corresponding to the word type of wn."}, {"heading": "4.2 Loss function", "text": "The parameters of the model, \u2126 = {W x,\u0398BLSTM , W ha,bha, {W ayw ,b ay w }\u2200w\u2208V , }, are fitted by minimizing the cross entropy error\nL(I; \u2126) = \u2212 \u2211\ni\u2208I\n\u2211\nj\u2208S(wi)\nti,j log yj(i) (5)\nover a set of sense labeled tokens with indices I \u2282 {1, . . . , |C|} within a training corpus C, each labeled with a target sense distribution ti,\u2200i \u2208 I .\n."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Data and Task", "text": "To evaluate our proposed model we performed the lexical sample task of SensEval 2 (SE2) and SensEval 3 (SE3). For both of these tasks training and test data is supplied, as well as a standardized scorer to ensure fair comparisons."}, {"heading": "5.2 Experimental settings", "text": "A big challenge with these relatively small datasets is the risk of overfitting the model. To mitigate this problem we apply a 50% dropout to the word embeddings, the LSTM hidden state h, and the hidden layer a. Further, GloVe vectors trained on Wikipedia and Gigaword are used to initialize the word embeddings, and we use 10% dropword, 74+74 neurons in the BLSTM, embedding size of 100 and additive gaussian noise with zero mean and a standard deviation of 0.2\u03c3i, where \u03c3i is the standard deviation in embedding dimension i for all words in the embeddings matrix, W x. \u03c3i is updated after each weight update. To speed up training, we use a maximum context window length of 140 words. Allowing the model to use more distant data than this did not improve the results. Finally, to showcase the generality of the model we use identical hyperparameter settings for both SE2 and SE3 during test."}, {"heading": "5.3 Training details", "text": "The model is implementedin TensorFlow (Abadi et al., 2015), and optimized using Stochastic Gradient Descent with momentum (0.1) and batch size 100. The learning rate is initialized to 2.0 and then decreased exponentially with a decay factor of 0.96. All parameters in the model are initialized in U(\u22120.1, 0.1), with the exception of the biases that are initialized to zero. The data is cleaned by removing various tags. We do not remove uncommon words, but to keep the size of the vocabulary manageable we replaced numbers with a < number > tag. Words not in GloVe is initialized from N (0, 0.1). This resulted in a vocabulary size of |V | = 50817 for SE2 and |V | = 37998 for SE3. Words not present in the training set is considered as unknown words during test."}, {"heading": "5.4 Results", "text": "The results on the corresponding datasets can be seen in Table 1. 100JHU(R) was developed by Yarowsky et al. (2001) and achieved the best score on the English lexical sample task of SE2 with a F1 score of 64.2. Their system utilized a rich feature space based on raw words, lemmas, POS tags, bag-of-words, bi-gram, and tri-gram collocations, etc. as inputs to an ensemble classifier. Correspondingly, htsa3 by Grozea (2004) was the winner of the SE3 lexical sample task with a F1 score of 72.9. This system also used a rich set of features. These were used as inputs to a regularized least square classifier. IMS+adapted CW is a more recent system by Taghipour and Ng (2015) that introduces word embeddings in WSD, albeit separately trained, as a feature together with, again, a rich set of features including POS tags, collocations and surrounding words. A support vector machine is subsequently trained on the extracted features for each target word.\nOur proposed model outperform previous stateof-the-art on SE2 and are tied with IMS+adapted CW on SE3. Moreover, we see that dropword consistently improves the results on both SE2 and SE3. Randomizing the order of the input words yields a substantially worse result, which provides evidence for our hypothesis that the order of the words are significant. We also see that the system effectively makes use of the information in the pre-trained word embeddings and that they are essential to the performance of our system on these\ndatasets."}, {"heading": "6 Conclusions & future work", "text": "We presented a BLSTM based model for WSD that was able to effectively exploit word order and achieves state-of-the-art results on two WSD dataset. The model stands in stark contrast to previous work in that it is an end-to-end trainable system, all the way from the text to the sense prediction. Further, we showed that employing pre trained word vectors improved the generalization power of the model. However, it is likely that this effect would diminish if the model was trained on more sense tagged data, which is an experiment we hope to conduct as future work. Also, in part due to the limited amount of data, we developed the regularization technique dropword that we could show improved the results under these constrained circumstances. Another effect that stemmed from the limited data is that we needed to scale down and regularize the model heavily. Therefore we hold it as likely that it is possible to improve on the reported results simply by scaling up the model and training on more data."}, {"heading": "Acknowledgments", "text": "The authors would like to acknowledge the project Towards a knowledge-based culturomics supported by a framework grant from the Swedish Research Council (2012\u20132016; dnr 2012-5738)."}], "references": [{"title": "Knowledge base unification via sense embeddings and disambiguation", "author": ["Luis EspinosaAnke", "Roberto Navigli"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Bovi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bovi et al\\.", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Finding optimal parameter settings for high performance word sense disambiguation", "author": ["Cristian Grozea"], "venue": "In Proceedings of Senseval-3 Workshop", "citeRegEx": "Grozea.,? \\Q2004\\E", "shortCiteRegEx": "Grozea.", "year": 2004}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Combining relational and distributional knowledge for word sense disambiguation", "author": ["Johansson", "Pina2015] Richard Johansson", "Luis Nieto Pina"], "venue": "Nordic Conference of Computational Linguistics NODALIDA", "citeRegEx": "Johansson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johansson et al\\.", "year": 2015}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLTNAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Word Sense Disambiguation: a survey", "author": ["Roberto Navigli"], "venue": "ACM Computing Surveys,", "citeRegEx": "Navigli.,? \\Q2009\\E", "shortCiteRegEx": "Navigli.", "year": 2009}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space. arXiv preprint arXiv:1504.06654", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "A simple and efficient method to generate word sense representations", "author": ["Nieto Pi\u00f1a", "Johansson2015] Luis Nieto Pi\u00f1a", "Richard Johansson"], "venue": "In Proceedings of Recent Advances in Natural Language Processing,", "citeRegEx": "Pi\u00f1a et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pi\u00f1a et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Creating artificial neural networks that generalize", "author": ["Sietsma", "Dow1991] Jocelyn Sietsma", "Robert JF Dow"], "venue": "Neural networks,", "citeRegEx": "Sietsma et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Sietsma et al\\.", "year": 1991}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Semi-supervised word sense disambiguation using word embeddings in general and specific domains", "author": ["Taghipour", "Ng2015] Kaveh Taghipour", "Hwee Tou Ng"], "venue": "In The 2015 Annual Conference of the North American Chapter", "citeRegEx": "Taghipour et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Taghipour et al\\.", "year": 2015}, {"title": "Wordsense disambiguation for machine translation", "author": ["Luke Biewald", "Marc Teyssier", "Daphne Koller"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Lan-", "citeRegEx": "Vickrey et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Vickrey et al\\.", "year": 2005}, {"title": "The johns hopkins senseval2 system descriptions", "author": ["Silviu Cucerzan", "Radu Florian", "Charles Schafer", "Richard Wicentowski"], "venue": "In The Proceedings of the Second International Workshop", "citeRegEx": "Yarowsky et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 13, "context": "machine translation, information Retrieval, information Extraction (Vickrey et al., 2005; Navigli, 2009), and basic NLP tasks like sense aware word representations (Neelakantan et al.", "startOffset": 67, "endOffset": 104}, {"referenceID": 6, "context": "machine translation, information Retrieval, information Extraction (Vickrey et al., 2005; Navigli, 2009), and basic NLP tasks like sense aware word representations (Neelakantan et al.", "startOffset": 67, "endOffset": 104}, {"referenceID": 7, "context": ", 2005; Navigli, 2009), and basic NLP tasks like sense aware word representations (Neelakantan et al., 2015; Nieto Pi\u00f1a and Johansson, 2015; Bovi et al., 2015).", "startOffset": 82, "endOffset": 159}, {"referenceID": 0, "context": ", 2005; Navigli, 2009), and basic NLP tasks like sense aware word representations (Neelakantan et al., 2015; Nieto Pi\u00f1a and Johansson, 2015; Bovi et al., 2015).", "startOffset": 82, "endOffset": 159}, {"referenceID": 0, "context": ", 2015; Nieto Pi\u00f1a and Johansson, 2015; Bovi et al., 2015). However, for word representations the benefit goes two ways which was shown by Taghipour and Ng (2015) who used word embeddings to improve on WSD, and later by Johansson and Pina (2015) who combined word embeddings with a knowledge graph approach.", "startOffset": 40, "endOffset": 163}, {"referenceID": 0, "context": ", 2015; Nieto Pi\u00f1a and Johansson, 2015; Bovi et al., 2015). However, for word representations the benefit goes two ways which was shown by Taghipour and Ng (2015) who used word embeddings to improve on WSD, and later by Johansson and Pina (2015) who combined word embeddings with a knowledge graph approach.", "startOffset": 40, "endOffset": 246}, {"referenceID": 8, "context": "Global Vectors for Word Representation (GloVe), introduced by Pennington et al. (2014) is a hybrid approach to word representation that combine a log-linear model, made popular by Mikolov et al.", "startOffset": 62, "endOffset": 87}, {"referenceID": 5, "context": "(2014) is a hybrid approach to word representation that combine a log-linear model, made popular by Mikolov et al. (2013), with counting based cooccurrence statistics to more efficiently capture global statistics.", "startOffset": 100, "endOffset": 122}, {"referenceID": 11, "context": "Another, more recent noise based regularization technique is dropout, introduced by Srivastava et al. (2014). Dropout lowers the capacity of the model by randomly erasing information from a layer of neurons by setting a subset of them to zero.", "startOffset": 84, "endOffset": 109}, {"referenceID": 13, "context": "100JHU(R) was developed by Yarowsky et al. (2001) and achieved the best score on the English lexical sample task of SE2 with a F1 score of 64.", "startOffset": 27, "endOffset": 50}, {"referenceID": 2, "context": "Correspondingly, htsa3 by Grozea (2004) was the winner of the SE3 lexical sample task with a F1 score of 72.", "startOffset": 26, "endOffset": 40}, {"referenceID": 2, "context": "Correspondingly, htsa3 by Grozea (2004) was the winner of the SE3 lexical sample task with a F1 score of 72.9. This system also used a rich set of features. These were used as inputs to a regularized least square classifier. IMS+adapted CW is a more recent system by Taghipour and Ng (2015) that introduces word embeddings in WSD, albeit separately trained, as a feature together with, again, a rich set of features including POS tags, collocations and surrounding words.", "startOffset": 26, "endOffset": 291}], "year": 2016, "abstractText": "In this paper we present a model that leverages a bidirectional long short-term memory network to learn word sense disambiguation directly from data. The approach is end-to-end trainable and makes effective use of word order. Further, to improve the robustness of the model we introduce dropword, a regularization technique that randomly removes words from the text. The model is evaluated on two standard datasets and achieves stateof-the-art results on both datasets, using identical hyperparameter settings.", "creator": "LaTeX with hyperref package"}}}