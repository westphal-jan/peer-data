{"id": "1606.04165", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Using Virtual Humans to Understand Real Ones", "abstract": "human interactions but constrained by explicit displays strongly as figurative channels of stimulus. but the political picture transmits overt behaviors, the social videos transmit hidden something about the communicator ( e. h., himself / her threats and replies ). there is a growing researcher suggesting providing a group with the ability. experience several affective emotions then allow for a different meaningful and valuable way toward studying hidden non - verbal signals regarding human - human experiences, human - computer interactions. in this pilot study, we encountered a non - dynamic sensory - real interaction while manipulating three specific cyber - verbal channels controlling communication : gaze pattern, facial expression, and gesture. participants rated for virtual behaviors on affective dimensional markers ( pleasure, arousal, and cognition ) while decreasing physiological ranking ( electrodermal testing, eda ) was negative for the interaction. assessment of the statistical evidence demonstrated a significant nationally influential other - way interaction between gaze, gesture, and facial configuration / the dimension involving pleasure, as diverse as determining unique force of gesture on particular dimension of dominance. analogous results suggest a discourse group and different non - verbal behavior and are general context specific relationship they are applied. qualifying considerations over well as possible strategy steps are further pursued in texts about these different findings.", "histories": [["v1", "Mon, 13 Jun 2016 22:41:50 GMT  (727kb,D)", "http://arxiv.org/abs/1606.04165v1", null]], "reviews": [], "SUBJECTS": "cs.HC cs.AI", "authors": ["katie hoemann", "behnaz rezaei", "stacy c marsella", "sarah ostadabbas"], "accepted": false, "id": "1606.04165"}, "pdf": {"name": "1606.04165.pdf", "metadata": {"source": "CRF", "title": "Using Virtual Humans to Understand Real Ones", "authors": ["Katie Hoemann", "Behnaz Rezaei", "Stacy C. Marsella", "Sarah Ostadabbas"], "emails": ["bas@ece.neu.edu)."], "sections": [{"heading": null, "text": "Index Terms\u2014 Non-Verbal Behaviour, Virtual Human, Gaze Pattern, Gesture, Facial Expression\nI. INTRODUCTION\nNon-verbal signals are at the heart of human-to-human interaction and communication. We nod to indicate understanding or agreement, we shift our gaze to index attention and cognitive processing, and we alter our body movements and gestures as a function of our mood or level of autonomic arousal. The fluid production and navigation of these signals has implications for interpersonal dynamics [3], emotional intelligence [4], and cross-cultural communication [5]. Likewise, being able to naturalistically produce and follow these signals is core to creating a believable and trustworthy virtual agent [6]. Producing the full range of non-verbal signals may not be desirable in all situations, however. One of the hallmarks of Autism Spectrum Disorders (ASD) is a reluctance or inability to interact socially with other humans [7]. At the same time, individuals with ASD are known for their keen interest in technology, and readily interact with video games and non-human agents [8], [9], [10], [11].\n1Katie Hoemann is with the Interdisciplinary Affective Science Laboratory, Department of Psychology, Northeastern University, MA, USA.\n2Behnaz Rezaei and Sarah Ostadabbas are with the Augmented Cognition laboratory, Electrical and Computer Engineering Department, Northeastern University, MA, USA (corresponding author\u2019s e-mail: ostadabbas@ece.neu.edu).\n3Stacy C. Marsella is with the College of Computer and Information Science and Department of Psychology, Northeastern University, MA, USA.\nResearch to date suggests that non-verbal signaling may be a critical factor in engaging the attention of individuals with ASD, as these signals may induce aversive physiological changes (e.g. increased autonomic arousal) to a greater extent than in neurotypical individuals [12], [13], [14].\nWhile research has shown that non-human interaction partners such as virtual agents are more approachable for individuals with ASD, the exact properties that make them so have yet to be identified. The boundary conditions for human-like interaction are critical: what behaviors or characteristics tip an agent over the line, making it off-putting? What specific aspects of non-verbal signals are being evaluated, and how does changing their presence or intensity modulate the evaluation? The ultimate goal is to find a balance of behaviors or characteristics with which the individual with ASD feels comfortable, while also working toward maximally humanlike social exposure. In this manner, virtual agents can serve as pathways for teaching social interaction skills such as emotion recognition and face-to-face communication. Should non-verbal cues prove to be socio-affective roadblocks for individuals with ASD, the specific behaviors involved may feasibly also be used diagnostically, to identify levels or subclasses of the autism spectrum. If coupled with measurement of peripheral physiology, such a social-interactionbased training model could support an idiographic approach to broad-spectrum diagnosis and intervention.\nThe present research seeks to address these needs by employing virtual reality (VR) or virtual agent systems to first assess neurotypical individuals semantic evaluations of and physiological reactions to non-verbal behaviors. As a pilot study, this project looks to gain insight into the nuances of non-verbal signaling, in order to ultimately develop a model of real-time, dynamic social evaluation that can serve as a baseline or point of comparison for individuals on the spectrum. This work extends previous social psychology and affective computing research on interpersonal dynamics and socialaffective semantic evaluation to capitalize on several advantages of VR environments. VR environments allow for tight manipulation of agent non-verbal signals while preserving immediacy and embodiment both necessary elements to understanding the precise behavioral underpinnings and psychophysiological correlates of social interaction. Moreover, the use of virtual agents has the aforementioned advantage: the results of this investigation can be directly applied to interventions for individuals with ASD. ar X\niv :1\n60 6.\n04 16\n5v 1\n[ cs\n.H C\n] 1\n3 Ju\nn 20\n16"}, {"heading": "A. Related Work", "text": "Researchers have been doing work on how we make meaning out of social interactions and our environment for the better part of the past century. In particular, early work by Osgood focused on what he termed the semantic differential: a structured data-gathering procedure in which participants rate concepts on a variety of concrete and abstract verbal scales (e.g., wet vs. dry or good vs. bad); these ratings are then submitted to a factor analysis to determine what dimensions of meaning underlie the various judgments [15], [16]. Across concept categories and cultures, Osgood and his colleagues found that the same three affective dimensions Evaluation, Potency, and Activity consistently accounted for the variance in semantic judgments (though the exact loading of differential scales onto factors, and the variance accounted for by each dimension did shift) [17], [18]. Using similar methods, Mehrabian (later joined by Russell) interpreted these affective dimensions as Pleasure, Arousal, and Dominance [19]. In particular, Mehrabians work also covered the connection between specific non-verbal behaviors and how they are evaluated along these dimensions [20], [21]. In 1994, Bradley and Lang introduced the Self-Assessment Manikin (SAM), pictorial scales for quickly and intuitively capturing ratings on the dimensions of Pleasure, Arousal, and Dominance [22].\nThis type of dimensional model for rating stimuli has been applied widely, including in research on non-verbal behaviors and emotion perception. Critically, however, most studies to date have been based on human-to-human interaction. The use of humans as targets for socio-affective evaluation constrains experimental manipulations on what non-verbal behaviors can be naturalistically controlled, and limits the conclusions we can draw about what constitutes humanlike interaction. Although work in social psychology extends the methodological toolkit by incorporating virtual environments, agents, and avatars [6], it has yet to fully explore the impact of these VR representations on participants affective ratings of and reactions to their social environment. For example, Bailenson and colleagues used a shared virtual environment to study the effect of head movements (as a simple non-verbal cues) on individual task performance [23]. However, they failed to find a significant effect of this nonverbal channel on task performance, due to a floor effect: the task was too simple to demonstrate any improvements that may result from the knowledge of other interactants\u2019 head orientation. Work in affective computing has explored the impact of virtual agents\u2019 non-verbal signals on participant affect (e.g., [24], [25], [26], [27]). With the goal of developing naturalistic agent behaviors, this work often focuses on a single non-verbal channel (e.g., gaze) or single affective dimension (e.g., dominance), rather than investigating how a suite of behaviors functions when an agent is embedded in an interactive context. Similarly, although work on ASD uses virtual agents to connect with individuals [9], it has yet to investigate the precise non-verbal aspects that facilitate this communication. It is at this nexus of lines of inquiry that the\ncurrent project proposal takes form.\nThe secondary goal of this project is to investigate the usability of physiological modalities to improve affective computing methods. Embodiment theory states that the body is an integral factor is shaping the interpretation of and reaction to the environment [28], [29]. Likewise, theories of emotion such as psychological constructivism contend that the brain interprets the current physiological state through the process of interoception; this information is then used to predict the current emotional state [30], [31]. As mentioned above, this is particularly relevant for work on ASD, as one hypothesis posits that individuals on the spectrum may experience greater autonomic arousal in social interactions, which they in turn may experience as negative affect, and so they avoid these situations [12], [13], [14]. As autonomic arousal is indexed by physiological signals such as skin conductance, respiration rate, and cardiac variables, it can be measured using standard psychophysiological methods. Psychophysiology is the practice of measuring physiological signals from the body and inferring psychological, or mental, states from them [32]. One way to make this inference is to use machine learning algorithms to produce classifiers which detect patterns in physiological responses; these patterns can then, ideally, be associated with patterns in behavioral responses (e.g., self-report) to determine their validity. To date, machine based approaches to social emotional processing have focused on facial expression and/or speech. However, we believe that physiological signals have several advantages when compared to video and sound:\n\u2022 The sensors used to record those signals are generally placed directly on the user, reducing potential sources of noise and problems due to the unavailability of the signal (e.g., the user not turning their head in front of the camera or not speaking); \u2022 They have very good time responses: for instance, muscle activity can be detected earlier by using electromyography (EMG) than by using a camera; \u2022 They cannot be easily manipulated by the user, thus minimizing the presence of faked emotion signals; \u2022 In the case of impaired users that cannot move facial muscles or express themselves, many physiological signals, such as brain waves, are still usable for emotion assessment.\nIn addition, there is good evidence that the physiological activity associated with affective states can be differentiated and systematically organized [33]. Cardiovascular and electromyogram activity have been used to examine the dimension of pleasure, or valence (i.e, positive and negative affect) of human subjects [34], [35]. Electrodermal activity (EDA) has been shown to be associated with task engagement [36]. The variation of peripheral temperature caused by emotional stimuli was studied by [37]. In this work, we exploited the dependence of one physiological response - EDA - on the underlying affective state of arousal."}, {"heading": "II. APPROACH/METHODOLOGY", "text": "In this pilot study, we seek to develop a methodological framework for evaluating agent non-verbal behavior in a virtual reality environment. Using a semantic differential approach to gather participants affective ratings, and while continuously gathering physiological (EDA) data, we manipulate agent non-verbal signals in a series of short, scripted interactions."}, {"heading": "A. Participants", "text": "Participants were 10 university students recruited on volunteer-only basis from the Affective Computing course, as well as the investigators respective labs. Participants had normal or corrected-to-normal hearing and vision. We were able to balance participants by gender (5 female versus 5 male), but were not be able to control for culture or native language (4 Iranian, 3 American, 2 Indian, 1 Belgian). Mean age of the participants was 28.5 years (SD = 3.83)."}, {"heading": "B. Stimuli", "text": "Stimuli consisted of 8 short scenarios (about 30 seconds each) featuring a virtual agent who engages participants attention and responds to non-dynamic interactive feedback. For example, the agent may look at the participant while uttering a greeting and request (e.g., You must be here for the open house. Can I tell you more about this room?). Following a standard template, these scripted dialogues were written to be as neutral as possible in tone and content, and to be one-on-one interactions one might plausibly have with a (presumed) stranger. Dialogue content further corresponded with the setting in which the interaction occurred: either an indoor house/office scene, or an outdoor city street scene (see configuration details below). Although the initial plan was not to have any interaction between the participant and the virtual agent, we decided to incorporate this element in order to ensure continuous engagement of the participant, and long enough trials to gather EDA signal of adequate length.\nScenarios were developed to manipulate various aspects of the agents non-verbal behavior: facial configuration (smile, AU 4, or furrowed brow, AU 12); gaze pattern (looking at the participant, or looking away); and naturalistic gesture (nods and hand movements present or absent). These 3 conditions, with 2 levels each, were controlled to produce a maximum of 8 possible combinations (see Table I). Scenarios are presented in Table II. A random permutation of scenarios was targeted for the initial round of testing.\nStimulus generation was accomplished using the Unity game engine accompanied by Smartbody software. During design, we made use of existing assets created by the USC Institute for Creative Technologies (ICT) as a part of the VHtoolkit framework (the virtual agent, Brad, as well as the house scene), and a free asset in the Unity asset store (the street scene). Utterances to be spoken by the agent were recorded by a male speaker of American English; their corresponding lip-syncing files were generated separately and assigned to the Brad character game object. In order to\ncontrol the duration of each scenario for the temporal segmentation of the physiological signals, as well as to prevent problems with the VHtoolkit-embedded speech recognizer not recognizing the participant\u2019s voices (especially nonnative ones), we created a non-dynamic interaction by scripting the agent\u2019s speech at predetermined time marks. Namely, we specified dead time in proportion to the participant\u2019s utterances to allow him/her interact with the agent during each scenario. A sample dialogue, structured according to the general template and scripted by utterance time-mark, is provided in Appendix A.\nFor all the intended facial configurations and gestures, specific XML command executable by the SmartBody software were created and sent to the the SmartBody through the main function attached to the agent. The main function was scripted in C# language and was used to control all the behaviours of the agent (Brad).\nFig. 1 indicates a snapshot of one of the scenarios which take places in the house scene, the agent communicates with the participant as he smiles and looks directly toward the participant with natural gesturing."}, {"heading": "C. Procedure", "text": "Prior to the start of the task, participants were directed to sit at a desk, on which there is a computer monitor for presentation of the visual stimuli. Participants completed a brief demographic survey, and were then outfitted with a headset and a wristband on the non-dominant hand for peripheral physiology measurement. After establishing baseline readings of 2 minutes for the EDA signal, the researchers provided participants with written and verbal task instructions (see Appendix B for detailed instructions sheet).\nIn a within-subjects manipulation, participants were presented with the interactive VR scenarios. Scenarios were presented in a randomized order for a total of 8 trials. On each trial, participants were asked to attentively watch the vignette presented and speak the scripted answerer to the agent during the presentation. Afterward, they rated their reaction to the agent (i.e., how they felt based on their interaction with the agent) using the dimensional affective rating instrument provided to them on paper. For this study, the 9-point version of the Self-Assessment Manikin (SAM)\nwas used to capture ratings on the dimensions of pleasure, arousal, and dominance [22] (see Appendix C for a sample scenario dialogue and rating sheet). A between-trial resting period was included to facilitate a return to baseline for the physiological signal. The physiological activity of each participant was recorded for the duration of the task using the E4 wristband sensor from the Empatica company.\nFollowing stimulus presentation, participants completed a basic debriefing questionnaire, in which they provided feedback about the task and the stimuli. The total running time for the procedure was 20 to 30 minutes. Participant demographics and rating data were digitized for analysis, along with any study notes. Data from one trial (scenario D: gaze off, gesture off, face smiling) were dropped due to confusion on the part of the participant. The obtained EDA signals were then segmented in several trials (or epochs), each one being associated to a given stimulus."}, {"heading": "D. Hypotheses", "text": "Broadly, we hypothesize that each of the agent\u2019s nonverbal behaviors will impact the affective ratings provided by participants. More specifically, we predict there will be main effects such that the two levels of every condition have the opposite effect on affect. For example: a facial configuration of smile (level 1) will result in increased pleasure, decreased arousal, and increased dominance; whereas a facial configuration of furrowed brow (level 2) will result in decreased pleasure, increased arousal, and decreased dominance. These hypothesized are based on previous work in the social psychological, communication, and affective computing literature (e.g., [38], [39], [24], [40], [25], [41], [26], [27], [42], [43]). Specific per-condition by level hypotheses are summarized in Table III.\nWe further hypothesize that there will be a series of twoway interactions between levels of non-verbal behaviors. For example, in the dimension of pleasure, we expect a combination of direct gaze and a smile to result in more\npositive feeling, while direct gaze and a furrowed brow would result in more negative feeling [38]. We predict that gaze will likewise magnify the main effect of naturalistic gesture, and that gestures will enhance the valence set by facial configuration, as a sign of social/affective immediacy [42]. See Table IV for descriptions of additional two-way interactions. Based on a review of the above-mentioned literature, we were unable to form hypotheses about the effect the interaction of gesture and facial interaction will have on the dimensions of arousal and dominance. The results may also reveal three-way interactions, though we do not have strong a priori hypotheses on this point. As this is an exploratory pilot, we expect the findings and feedback we receive to inform these hypotheses for future studies.\nRelated to peripheral physiology, we predict that participants affective ratings will be positive correlated with autonomic arousal levels, as inferred from EDA. To wit, the mean skin resistance over a trial has been shown to be negatively correlated with the arousal of a stimulus [44]. An aroused emotion should also induce a decrease in the Galvanic Skin Resistance (GSR) signal. The increase and decrease of heart rate (HR) (here, as measured through PPG) is associated with many emotions, however an increase in heart rate indicates an overall increase in sympathetic nervous system activity and a decrease in heart rate indicates that the parasympathetic nervous system is recovering the person to a relaxation state [45]."}, {"heading": "III. EVALUATION/RESULTS", "text": ""}, {"heading": "A. Behavioral Data Results", "text": "The data from this 2x2x2 within-subjects design were analyzed using repeated measures ANOVAs with 3 withinsubjects factors: gaze (on vs. off), gesture (on vs. off), facial configuration (smile vs. furrowed brow). Participants pleasure, arousal, and dominance ratings were analyzed separately as the dependent variables.\nPleasure The three-way interaction between the three factors was found to be significant at F (1, 8) = 23.273, p = .001, \u03b72p = .744. See Fig. 2 for a means plot of this interaction. Follow-up 2x2 ANOVAs were conducted for each factor broken down by level:\n\u2022 A 2x2 ANOVA for gaze x gesture at face = smile revealed a significant two-way interaction at F (1, 8) = 12.250, p = .008, \u03b72p = .605.\n\u2013 Follow-up pairwise t-tests revealed that when gaze is on and the face is smiling, participants rate their experience as more pleasant when the agent gesture is off (M = 6.100,SD = .876), compared to when its on (M = 4.800, SD = .919): t(9) = \u22124.333, p = .002. In contrast, when gaze is off and the face is smiling, the effect of gesture is not significant (p > .05). \u2013 When gesture is off and face is smiling, participants rate their experience as more pleasant when the gaze is on (M = 6.111, SD = .928), compared to when its off (M = 5.222, SD = .972): t(8) = 2.530, p = .035. When the gesture is on and the face is smiling, there is no effect of gaze (p > .05).\n\u2022 A 2x2 ANOVA for gaze x gesture at face = furrowed brow revealed a significant 2-way interaction at F (1, 8) = 11.172, p = .009, \u03b72p = .554.\n\u2013 Follow-up pairwise t-tests revealed that when gaze is on and the face has a furrowed brow, participants rate their experience as more pleasant when the agent gesture is on (M = 5.000, SD = .943), compared to when its off (M = 4.100, SD = .738): t(9) = 3.857, p = .004. When the gaze is off and the brow is furrowed, there is no effect of gesture (p > .05). \u2013 When the brow is furrowed, regardless of the gesture is on or off, there is no effect of gaze (all ps > .05).\n\u2022 A 2x2 ANOVA for gaze by face at gesture = on did not reveal a significant interaction (p > .05). \u2022 A 2x2 ANOVA for gaze by face at gesture = off revealed a significant 2-way interaction at F (1, 8) = 11.636, p = .009, \u03b72p = .593.\n\u2013 Follow-up pairwise t-tests revealed that when gaze is on and gesture is off, participants rate their experience as more pleasant when the agent is\n3.00\n3.50\n4.00\n4.50\n5.00\n5.50\n6.00\n6.50\n7.00\nGesture on Gesture off Gesture on Gesture off\nGaze on Gaze off\nP le\nas u\nre\nSmile Furrow\nArousal A 2x2x2 ANOVA revealed no significant interactions or main effects (all ps > .2).\nDominance A 2x2x2 ANOVA revealed a significant main effect of gesture at F (1, 8) = 7.118, p = .028, \u03b72p = .471. Figure 3a: Participants indicated that they felt more dominant when interacting with the agent when gesture was off (M = 5.583, SE = .358) than when gesture was on (M = 4.667, SE = .144).\nTABLE IV HYPOTHESES: 2-WAY INTERACTIONS\nGaze x gesture direct gaze positive when relaxed, negative when tense Pleasure Facial configuration x gaze direct gaze positive when smiling, negative when furrowed brow\nGesture x facial configuration gestures enhance the valence of facial configuration Gaze x gesture averted gaze higher arousal when no gesture than with gesture\nArousal Facial configuration x direct gaze higher arousal when accompanied by furrowed brow Gesture x facial configuration ? Gaze x gesture greater dominance conveyed by greater kinesic involvement Dominance Facial configuration x gaze smile is more dominant with direct gaze, furrowed brow is moredominant with averted gaze Gesture x facial configuration ?\n3.00\n3.50\n4.00\n4.50\n5.00\n5.50\n6.00\n6.50\n7.00\nSmile Furrow\nD o\nm in\nan ce\nMain effect of face\n(a)\n3.00\n3.50\n4.00\n4.50\n5.00\n5.50\n6.00\n6.50\n7.00\nOn Off\nD o\nm in\nan ce\nMain effect of gesture\n(b)\nFig. 3. (a) Main effect of face on dominance. (b) Main effect of gesture on dominance.\nA main effect of face was also found to be approaching significance at F (1, 8) = 4.566, p = .065, \u03b72p = .363. Fig. 3b: Participants indicated that they felt more dominant when interacting with the agent when he was smiling (M = 5.278, SE = .214) than when he had a furrowed brow (M = 4.972, SE = .234)."}, {"heading": "B. Electrodermal Activity Evaluation", "text": "Electrodermal activity consists of two main components: tonic response and phasic response. Tonic skin conductance refers to the ongoing or the baseline level of skin conductance in the absent of any particular discrete environmental events. Phasic skin conductance refers to the event-related changes that are caused by momentary increase in skin conductance (resembling a peak superimposed on tonic skin conductance). In order to have a valid acquisition of the electrodermal activity, it is sampled at a higher frequency (more than 100 Hz), then filtered and down-sampled by a specific order. However, the E4 wristband EDA sensor that we used for this experiment has a frequency sampling of 4 Hz, which is too low to provide an accurate and reliable measurement of electrodermal activity.\nTo achieve good classification results with pattern recognition and machine learning, the set of input features is crucial. For EDA signals, the time domain is most often employed for feature extraction. Consequently, we have chosen a range of features from time domain: mean, absolute deviation, standard deviation (SD), variance, and skewness. To define an optimal set of features, a criterion function should be defined. However, no such criterion function was available in our case."}, {"heading": "C. Classification Results", "text": "A wide range of methods has been used to infer affective states. Most of them are part of the machine learning and pattern recognition techniques. Classifiers like k-Nearest Neighbors (k-NN), Linear Discriminant Analysis (LDA), neural networks, Support Vector Machines (SVMs) and others [46], [47] are useful to detect emotional classes of interest. Regression techniques [46] can also be used to obtain continuous estimation of emotions for instance, in the valence-arousal space. Prior to inferring emotional states it is important to define some physiological features of interest. It is very challenging to find with certainty some features in physiological signals that always correlate with the affective status of users. Those variables frequently differ from one user to another and they are also very sensitive to day-to-day variations, as well as to the context of the emotion induction. To perform this selection, researchers generally apply feature selection or projection algorithms like Sequential Floating Forward Search (SFFS) or Fisher projection. In this project we trained a SVM-based classifier for estimation of the arousal dimensions of the participants according to their EDA signals in three different levels.\nWe have used the MATLAB environment and an SVM and Kernel methods (KM) toolbox, for experimenting with SVMs. The kernel function of SVM characterizes the shapes of possible subsets of inputs classified into one category. Usually, in order to fit a better classifier to the data set, a polynomial kernel with dimensionality d, defined as Equation (1), is applied, but it gave us very low accuracy so we did not use any kernel for training the classifier.\nKp(xi, x l) = (xi.x l)d (1)\nwhere xi is a feature vector that has to be classified and xl is a feature vector assigned to a class (i.e., the training sample).\nWe trained the SVM classifier with 40 iterations to classify our data to three different classes for low, medium, and high arousal levels. In each iteration, we used a random subset of the recorded signals of 7 subjects as the training set, and the remaining 3 subjects as the test set. The average accuracy of the classifier was 57.2%."}, {"heading": "IV. DISCUSSION/FUTURE WORK", "text": "The results of the current work demonstrate that the relationship between different nonverbal cues can be a com-\nplex one. That is, the influence of each individual factor on the dimensions of pleasure, arousal, and dominance is not necessarily straightforward. In the present paradigm, the dimension of pleasure was most sensitive to influence from the agent\u2019s nonverbal behaviors. This is not surprising given the ease of assessment of pleasure or affective valence; anecdotally, participants readily rate stimuli on this dimension as opposed to others. A main effect of facial configuration on pleasure emerged from the data, such that participants found the agent more pleasant when he was smiling (M = 5.500, SE = .228) than when he had a furrowed brow (M = 4.611, SE = .229): F (1, 8) = 28.247, p = .001, \u03b72p = .779. This effect is consistent with our hypotheses and, moreover, serves as a sort of manipulation check that participants understood the task instructions and were paying attention to the stimuli. In contrast, our hypotheses for the main effects of gaze and gesture on pleasure were not borne out by the data. Returning to our predictions of two-way interactions for pleasure, we find partial support for the gaze x facial configuration hypothesis that eye contact would be considered positive when the agent is smiling, but negative when he has a furrowed brow. Although this interaction was observed, it was only when the agent was not gesturing. Similarly, the gaze x gesture prediction that eye contact would be positive when the agent is gesturing, but negative when the agent is not, was only supported when the agent had a furrowed brow. In fact, the opposite influence of gesture was also observed: participants reported actually feeling less pleasant when the agent smiled, maintained eye contact with them, and gestured (vs. did not gesture). Taken together, these results suggest that gesture has a distracting influence on the overall pleasantness - contrary to our hypothesis that gestures would rather enhance the valence set by e.g. the facial configuration.\nIn comparison with the dimension of pleasure, nonverbal behaviors\u2019 effects on the dimensions of arousal and dominance were less nuanced. Unfortunately, none of our hypotheses for the arousal dimension were borne out by the data, and no main effects of gaze, gesture, or facial configuration were observed. Given that this affective dimension was to be correlated with the EDA signal obtained during the task, these findings are particularly disappointing. They are not, however, particularly surprising: arousal manipulations are difficult to consistently induce in the lab, especially with short, similar, non-immersive stimuli. Further, participants find the dimension of arousal a less intuitive to grasp, and range of use issues are often anecdotally reported with arousal rating scales. For the dimension of dominance, we did see an expected main effect of face, and in the predicted direction: participants reported feeling more dominant when the agent was smiling than when he had a furrowed brow. This finding fits with our experience that interaction partners who are happy (as often inferred from a smile) are not as threatening as interaction partners who are angry (as often inferred from a furrowed brow). Considered from an approachavoid distinction, both happiness and anger are considered \u2019approach-motivated\u2019 emotions, and would thus\nengage more immediate feelings of dominance or submission within a social encounter [48]. Lastly, although we did see a trending main effect of gesture on dominance, it was opposite that predicted. In contrast to the hypothesis that participants would perceive a lack of gesture as more threatening, they reported feeling less dominant when naturalistic gesture was present. Despite going against our predictions, it is explained nicely by the association of referential gestures and nods with physical and social immediacy [42], which could be considered a form of threat.\nOverall, the present results offer null or somewhat inscrutable findings due not only to the multi-layered relationship between nonverbal behaviors, but also due to the nature of the present study. Pilot work involves a reduced sample size (here only n=10) and incompletely developed stimuli, both contributing to a small, noisy data set. As further confounds for interpretation, the current stimuli also incorporated elements other than the agent - namely, the dialogue content and the background scene - which undoubtedly contributed to participants\u2019 experience of the scenarios. For example, a conversation with a stranger about a lost dog is likely inherently more pleasant - even if the person is scowling the whole time - than a request from a pollster for your opinion. More generally, scenarios in which the agent is asking if he can do something for the participant (e.g., bring him/her a beverage) are presumably going to be more enjoyable than scenarios in which the agent needs something from the participant (e.g., borrow his/her phone). (Indeed, pairwise t-tests based on this property reveal that participants reported feeling significantly more pleasant when the agent was doing them a favor (M = 5.533, SD = .632) than when he was requesting something from them (M = 4.705, SD = .671): t(9) = 5.459, p < .001.) Moreover, the context in which an interaction occurs also influences one\u2019s interpretation. During the feedback process, in fact, one participant reported feeling more pleasant during the indoor house scenes than during the outdoor street scenes, as the indoor space was brighter and had a more protected feel (presumably because interactions inside a building are more of one\u2019s choosing). Future iterations of this work should seek to address these possible confounds, for example by randomizing the pairing of dialogue and nonverbal behavior, controlling for interaction type (favor vs. request), or separately norming background scenes for pleasure and arousal.\nFuture work should also seek to address procedural and technological shortcomings of the present work. Feedback from participants indicated that they found it difficult to focus on and fully evaluate the virtual agent because they were constantly looking back to the scripted dialogue on the paper in front of them. Though this item can be addressed in the short term by having the scripts available on the computer monitor, alongside the screen with the agent, ultimately this problem would be fully resolved by incorporating dynamic speech recognition and response within the VR environment. Dynamically generated speech would also resolve issues with timing of speech (e.g., sometimes the pre-programmed dead space was too long, sometimes too short) and would allow\nparticipants to have a fully immersed, authentic conversation with the agent. The use of a VR headset would complete the immersion in the environment (although rating instruments would somehow need to be incorporated). For more accurate measurement of electrodermal activity, the E4 wristband would either be augmented with a separate set of electrodes, or replaced by wet sensors as used in laboratory recordings. For fuller description and classification of psychophysiological data, additional channels such as heart rate or blood volume pulse could be captured - again, ideally with laboratory recording equipment rather than wearable technology. All told, the current work provides a useful starting point for various possible future studies, each able to address a different aspect of the same overarching goal: to use virtual humans to better understand real ones."}, {"heading": "V. ACKNOWLEDGEMENTS", "text": "Both Behnaz Rezaei and Katie Hoemann designed the procedure: Katie selected the psychological measures, and Behnaz selected the physiological measures. Both Katie and Behnaz designed and developed the virtual agent characteristics and VR scenarios. Katie wrote the scenario dialogues, recorded and cleaned the audio files, and contributed to asset configuration and BML code generation for the virtual character. Behnaz completed all coding and scripting of the scenarios, along with additional asset configuration.\nKatie was the primary author for the introduction, related work, psychological approach/methods, behavioral results and discussion sections. Behnaz was the primary author for the physiological approach/methods, and physiological data evaluation/results sections.\nThe authors would like to thank Dr. Stacy Marsella for his continued support and suggestions for the appropriate software platform in which to realize this project. Dan Feng was instrumental in providing support throughout the development and testing process, including generation of the BML files for naturalistic speech. This project would not have succeeded - and we would not have learned nearly as much - without her involvement."}], "references": [{"title": "Probabilistic assessment of user\u2019s emotions in educational games", "author": ["C. Conati"], "venue": "Applied Artificial Intelligence, vol. 16, no. 7-8, pp. 555\u2013575, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "The empathic companion: A character-based interface that addresses users\u2019affective states", "author": ["H. Prendinger", "M. Ishizuka"], "venue": "Applied Artificial Intelligence, vol. 19, no. 3-4, pp. 267\u2013285, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "The importance of nonverbal cues in judging rapport", "author": ["J.E. Grahe", "F.J. Bernieri"], "venue": "Journal of Nonverbal behavior, vol. 23, no. 4, pp. 253\u2013269, 1999.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Emotional intelligence", "author": ["P. Salovey", "J.D. Mayer"], "venue": "Imagination, cognition and personality, vol. 9, no. 3, pp. 185\u2013211, 1990.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Cross-cultural and intercultural communication", "author": ["W.B. Gudykunst"], "venue": "2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Immersive virtual environment technology as a methodological tool for social psychology", "author": ["J. Blascovich", "J. Loomis", "A.C. Beall", "K.R. Swinth", "C.L. Hoyt", "J.N. Bailenson"], "venue": "Psychological Inquiry, vol. 13, no. 2, pp. 103\u2013124, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "The autistic spectrum", "author": ["L. Wing"], "venue": "The lancet, vol. 350, no. 9093, pp. 1761\u20131766, 1997.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Computers in the treatment of nonspeaking autistic children.", "author": ["K.M. Colby", "D.C. Smith"], "venue": "Current psychiatric therapies,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1971}, {"title": "Development of a virtual agent based social tutor for children with autism spectrum disorders", "author": ["M. Milne", "M.H. Luerssen", "T.W. Lewis", "R.E. Leibbrandt", "D.M. Powers"], "venue": "Neural Networks (IJCNN), The 2010 International Joint Conference on. IEEE, 2010, pp. 1\u20139.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "An android for enhancing social skills and emotion recognition in people with autism", "author": ["G. Pioggia", "R. Igliozzi", "M. Ferro", "A. Ahluwalia", "F. Muratori", "D. De Rossi"], "venue": "Neural Systems and Rehabilitation Engineering, IEEE Transactions on, vol. 13, no. 4, pp. 507\u2013515, 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Robotic assistants in therapy and education of children with autism: can a small humanoid robot help encourage social interaction skills?", "author": ["B. Robins", "K. Dautenhahn", "R. Te Boekhorst", "A. Billard"], "venue": "Universal Access in the Information Society,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "The use of virtual characters to assess and train non-verbal communication in high-functioning autism", "author": ["A.L. Georgescu", "B. Kuzmanovic", "D. Roth", "G. Bente", "K. Vogeley"], "venue": "Front Hum Neurosci, vol. 8, no. 807, p. b24, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Cardiovascular arousal in individuals with autism", "author": ["M.S. Goodwin", "J. Groden", "W.F. Velicer", "L.P. Lipsitt", "M.G. Baron", "S.G. Hofmann", "G. Groden"], "venue": "Focus on Autism and Other Developmental Disabilities, vol. 21, no. 2, pp. 100\u2013123, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Physiology-based affect recognition for computer-assisted intervention of children with autism spectrum disorder", "author": ["C. Liu", "K. Conn", "N. Sarkar", "W. Stone"], "venue": "International journal of human-computer studies, vol. 66, no. 9, pp. 662\u2013677, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "The nature and measurement of meaning.", "author": ["C.E. Osgood"], "venue": "Psychological bulletin,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1952}, {"title": "Factor analysis of meaning.", "author": ["C.E. Osgood", "G.J. Suci"], "venue": "Journal of experimental psychology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1955}, {"title": "Semantic differential technique in the comparative study of cultures", "author": ["C.E. Osgood"], "venue": "American Anthropologist, vol. 66, no. 3, pp. 171\u2013 200, 1964.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1964}, {"title": "A cross-culture and crossconcept study of the generality of semantic spaces", "author": ["Y. Tanaka", "T. Oyama", "C.E. Osgood"], "venue": "Journal of Verbal Learning and Verbal Behavior, vol. 2, no. 5, pp. 392\u2013405, 1963.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1963}, {"title": "Evidence for a three-factor theory of emotions", "author": ["J.A. Russell", "A. Mehrabian"], "venue": "Journal of research in Personality, vol. 11, no. 3, pp. 273\u2013294, 1977.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1977}, {"title": "Relationship of attitude to seated posture, orientation, and distance.", "author": ["A. Mehrabian"], "venue": "Journal of personality and social psychology,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1968}, {"title": "A measure of emotional empathy1", "author": ["A. Mehrabian", "N. Epstein"], "venue": "Journal of personality, vol. 40, no. 4, pp. 525\u2013543, 1972.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1972}, {"title": "Measuring emotion: the selfassessment manikin and the semantic differential", "author": ["M.M. Bradley", "P.J. Lang"], "venue": "Journal of behavior therapy and experimental psychiatry, vol. 25, no. 1, pp. 49\u201359, 1994.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "Gaze and task performance in shared virtual environments", "author": ["J.N. Bailenson", "A.C. Beall", "J. Blascovich"], "venue": "The journal of visualization and computer animation, vol. 13, no. 5, pp. 313\u2013320, 2002.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Relations between facial display, eye gaze and head tilt: Dominance perception variations of virtual agents", "author": ["N. Bee", "S. Franke", "E. Andre"], "venue": "Affective Computing and Intelligent Interaction and Workshops (ACII), 2009, 3rd International Conference, pp. 1\u20137, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Realistic emotional gaze and head behavior generation based on arousal and dominance factors", "author": ["C. Cig", "Z. Kasap", "A. Egges", "N. Magnenat-Thalmann"], "venue": "Motion in Games, pp. 278\u2013289, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Emotionally expressive head and body movement during gaze shifts", "author": ["B. Lance", "S. Marsella"], "venue": "Intelligent virtual agents, pp. 72\u201385, 2007.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "The relation between gaze behavior and the attribution of emotion: An empirical study", "author": ["\u2014\u2014"], "venue": "Intelligent virtual agents, pp. 1\u201314, 2008.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Beyond the brain: How body and environment shape animal and human minds", "author": ["L. Barrett"], "venue": "2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Supersizing the mind: Embodiment, action, and cognitive extension", "author": ["A. Clark"], "venue": "2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "The conceptual act theory: A precis", "author": ["L.F. Barrett"], "venue": "Emotion Review, vol. 6, no. 4, pp. 292\u2013297, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "The embodiment of emotion", "author": ["L.F. Barrett", "K.A. Lindquist"], "venue": "Embodied grounding: Social, cognitive, affective, and neuroscientific approaches, pp. 237\u2013262, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Inferring psychological significance from physiological signals", "author": ["J.T. Cacioppo", "L.G. Tassinary"], "venue": "American Psychologist, vol. 45, no. 1, pp. 16\u201328, 1990.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1990}, {"title": "Emotion and motivation", "author": ["M.M. Bradley", "P.J. Lang"], "venue": "Handbook of psychophysiology, vol. 2, pp. 602\u2013642, 2000.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2000}, {"title": "Principles of psychophysiology: Physical, social and inferential elements, chapter the cardiovascular system", "author": ["J. Papillo", "D. Shapiro"], "venue": "1990.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1990}, {"title": "Social isolation and health, with an emphasis on underlying mechanisms", "author": ["J.T. Cacioppo", "L.C. Hawkley"], "venue": "Perspectives in biology and medicine, vol. 46, no. 3, pp. S39\u2013S52, 2003.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2003}, {"title": "The affective significance of skin conductance activity during a difficult problem-solving task", "author": ["A. Pecchinenda"], "venue": "Cognition & Emotion, vol. 10, no. 5, pp. 481\u2013504, 1996.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1996}, {"title": "Development of a skin temperature measuring system for non-contact stress evaluation", "author": ["H. Kataoka", "H. Kano", "H. Yoshida", "A. Saijo", "M. Yasuda", "M. Osumi"], "venue": "Engineering in Medicine and Biology Society, 1998. Proceedings of the 20th Annual International Conference of the IEEE, vol. 2. IEEE, 1998, pp. 940\u2013943.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1998}, {"title": "Effects of direct and averted gaze on the perception of facially communicated emotion", "author": ["R.B. Adams Jr.", "R.E. Kleck"], "venue": "Emotion, vol. 5, no. 1, p. 3, 2005.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2005}, {"title": "Gaze and mutual gaze", "author": ["M. Argyle", "M. Cook"], "venue": "1976.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1976}, {"title": "Nonverbal cues and interpersonal judgments: Participant and observer perceptions of intimacy, dominance, composure, and formality", "author": ["J.K. Burgoon", "B.A. Le Poire"], "venue": "Communications Monographs, vol. 66, no. 2, pp. 105\u2013124, 1999.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1999}, {"title": "Facial expressions of emotion influence interpersonal trait inferences", "author": ["B. Knutson"], "venue": "Journal of Nonverbal Behavior, vol. 20, no. 3, pp. 165\u2013182, 1996.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1996}, {"title": "Some referents and measures of nonverbal behavior", "author": ["A. Mehrabian"], "venue": " Behavior Research Methods & Instrumentation, vol. 1, no. 6, pp. 203\u2013 207, 1968.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1968}, {"title": "The many faces of a neutral face: Head tilt and perception of dominance and emotion", "author": ["A. Mignault", "A. Chaudhuri"], "venue": "Journal of Nonverbal Behavior, vol. 27, no. 2, pp. 111\u2013132, 2003.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2003}, {"title": "Looking at pictures: Affective, facial, visceral, and behavioral reactions", "author": ["P.J. Lang", "M.K. Greenwald", "M.M. Bradley", "A.O. Hamm"], "venue": "Psychophysiology, vol. 30, no. 3, pp. 261\u2013273, 1993.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1993}, {"title": "Physiological sensing of emotion", "author": ["J. Healey"], "venue": "The Oxford Handbook of Affective Computing, p. 204, 2014.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Pattern recognition", "author": ["C.M. Bishop"], "venue": "Machine Learning, 2006.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2006}, {"title": "Pattern classification. 2nd", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "Edition. New York, 2001.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "There is a growing consensus that providing a computer with the ability to manipulate implicit affective cues should allow for a more meaningful and natural way of studying particular non-verbal signals of human-human communications by human-computer interactions [1], [2].", "startOffset": 264, "endOffset": 267}, {"referenceID": 1, "context": "There is a growing consensus that providing a computer with the ability to manipulate implicit affective cues should allow for a more meaningful and natural way of studying particular non-verbal signals of human-human communications by human-computer interactions [1], [2].", "startOffset": 269, "endOffset": 272}, {"referenceID": 2, "context": "The fluid production and navigation of these signals has implications for interpersonal dynamics [3], emotional intelligence [4], and cross-cultural communication [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": "The fluid production and navigation of these signals has implications for interpersonal dynamics [3], emotional intelligence [4], and cross-cultural communication [5].", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "The fluid production and navigation of these signals has implications for interpersonal dynamics [3], emotional intelligence [4], and cross-cultural communication [5].", "startOffset": 163, "endOffset": 166}, {"referenceID": 5, "context": "signals is core to creating a believable and trustworthy virtual agent [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "One of the hallmarks of Autism Spectrum Disorders (ASD) is a reluctance or inability to interact socially with other humans [7].", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "for their keen interest in technology, and readily interact with video games and non-human agents [8], [9], [10], [11].", "startOffset": 98, "endOffset": 101}, {"referenceID": 8, "context": "for their keen interest in technology, and readily interact with video games and non-human agents [8], [9], [10], [11].", "startOffset": 103, "endOffset": 106}, {"referenceID": 9, "context": "for their keen interest in technology, and readily interact with video games and non-human agents [8], [9], [10], [11].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "for their keen interest in technology, and readily interact with video games and non-human agents [8], [9], [10], [11].", "startOffset": 114, "endOffset": 118}, {"referenceID": 11, "context": "than in neurotypical individuals [12], [13], [14].", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "than in neurotypical individuals [12], [13], [14].", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "than in neurotypical individuals [12], [13], [14].", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "dimensions of meaning underlie the various judgments [15], [16].", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "dimensions of meaning underlie the various judgments [15], [16].", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "accounted for by each dimension did shift) [17], [18].", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "accounted for by each dimension did shift) [17], [18].", "startOffset": 49, "endOffset": 53}, {"referenceID": 18, "context": "Using similar methods, Mehrabian (later joined by Russell) interpreted these affective dimensions as Pleasure, Arousal, and Dominance [19].", "startOffset": 134, "endOffset": 138}, {"referenceID": 19, "context": "In particular, Mehrabians work also covered the connection between specific non-verbal behaviors and how they are evaluated along these dimensions [20], [21].", "startOffset": 147, "endOffset": 151}, {"referenceID": 20, "context": "In particular, Mehrabians work also covered the connection between specific non-verbal behaviors and how they are evaluated along these dimensions [20], [21].", "startOffset": 153, "endOffset": 157}, {"referenceID": 21, "context": "In 1994, Bradley and Lang introduced the Self-Assessment Manikin (SAM), pictorial scales for quickly and intuitively capturing ratings on the dimensions of Pleasure, Arousal, and Dominance [22].", "startOffset": 189, "endOffset": 193}, {"referenceID": 5, "context": "Although work in social psychology extends the methodological toolkit by incorporating virtual environments, agents, and avatars [6], it has yet to fully explore the", "startOffset": 129, "endOffset": 132}, {"referenceID": 22, "context": "For example, Bailenson and colleagues used a shared virtual environment to study the effect of head movements (as a simple non-verbal cues) on individual task performance [23].", "startOffset": 171, "endOffset": 175}, {"referenceID": 23, "context": ", [24], [25], [26], [27]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": ", [24], [25], [26], [27]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": ", [24], [25], [26], [27]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 26, "context": ", [24], [25], [26], [27]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "virtual agents to connect with individuals [9], it has yet to investigate the precise non-verbal aspects that facilitate this communication.", "startOffset": 43, "endOffset": 46}, {"referenceID": 27, "context": "Embodiment theory states that the body is an integral factor is shaping the interpretation of and reaction to the environment [28], [29].", "startOffset": 126, "endOffset": 130}, {"referenceID": 28, "context": "Embodiment theory states that the body is an integral factor is shaping the interpretation of and reaction to the environment [28], [29].", "startOffset": 132, "endOffset": 136}, {"referenceID": 29, "context": "the process of interoception; this information is then used to predict the current emotional state [30], [31].", "startOffset": 99, "endOffset": 103}, {"referenceID": 30, "context": "the process of interoception; this information is then used to predict the current emotional state [30], [31].", "startOffset": 105, "endOffset": 109}, {"referenceID": 11, "context": "they avoid these situations [12], [13], [14].", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "they avoid these situations [12], [13], [14].", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "they avoid these situations [12], [13], [14].", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "states from them [32].", "startOffset": 17, "endOffset": 21}, {"referenceID": 32, "context": "activity associated with affective states can be differentiated and systematically organized [33].", "startOffset": 93, "endOffset": 97}, {"referenceID": 33, "context": "e, positive and negative affect) of human subjects [34], [35].", "startOffset": 51, "endOffset": 55}, {"referenceID": 34, "context": "e, positive and negative affect) of human subjects [34], [35].", "startOffset": 57, "endOffset": 61}, {"referenceID": 35, "context": "has been shown to be associated with task engagement [36].", "startOffset": 53, "endOffset": 57}, {"referenceID": 36, "context": "The variation of peripheral temperature caused by emotional stimuli was studied by [37].", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "was used to capture ratings on the dimensions of pleasure, arousal, and dominance [22] (see Appendix C for a sample", "startOffset": 82, "endOffset": 86}, {"referenceID": 37, "context": ", [38], [39], [24], [40], [25], [41], [26], [27], [42], [43]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 38, "context": ", [38], [39], [24], [40], [25], [41], [26], [27], [42], [43]).", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": ", [38], [39], [24], [40], [25], [41], [26], [27], [42], [43]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 39, "context": ", [38], [39], [24], [40], [25], [41], [26], [27], [42], [43]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 24, "context": ", [38], [39], [24], [40], [25], [41], [26], [27], [42], [43]).", "startOffset": 26, "endOffset": 30}, {"referenceID": 40, "context": ", [38], [39], [24], [40], [25], [41], [26], [27], [42], [43]).", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": ", [38], [39], [24], [40], [25], [41], [26], [27], [42], [43]).", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": ", [38], [39], [24], [40], [25], [41], [26], [27], [42], [43]).", "startOffset": 44, "endOffset": 48}, {"referenceID": 41, "context": ", [38], [39], [24], [40], [25], [41], [26], [27], [42], [43]).", "startOffset": 50, "endOffset": 54}, {"referenceID": 42, "context": ", [38], [39], [24], [40], [25], [41], [26], [27], [42], [43]).", "startOffset": 56, "endOffset": 60}, {"referenceID": 37, "context": "would result in more negative feeling [38].", "startOffset": 38, "endOffset": 42}, {"referenceID": 41, "context": "We predict that gaze will likewise magnify the main effect of naturalistic gesture, and that gestures will enhance the valence set by facial configuration, as a sign of social/affective immediacy [42].", "startOffset": 196, "endOffset": 200}, {"referenceID": 43, "context": "To wit, the mean skin resistance over a trial has been shown to be negatively correlated with the arousal of a stimulus [44].", "startOffset": 120, "endOffset": 124}, {"referenceID": 44, "context": "PPG) is associated with many emotions, however an increase in heart rate indicates an overall increase in sympathetic nervous system activity and a decrease in heart rate indicates that the parasympathetic nervous system is recovering the person to a relaxation state [45].", "startOffset": 268, "endOffset": 272}, {"referenceID": 45, "context": "Classifiers like k-Nearest Neighbors (k-NN), Linear Discriminant Analysis (LDA), neural networks, Support Vector Machines (SVMs) and others [46], [47] are useful to detect emotional classes of", "startOffset": 140, "endOffset": 144}, {"referenceID": 46, "context": "Classifiers like k-Nearest Neighbors (k-NN), Linear Discriminant Analysis (LDA), neural networks, Support Vector Machines (SVMs) and others [46], [47] are useful to detect emotional classes of", "startOffset": 146, "endOffset": 150}, {"referenceID": 45, "context": "Regression techniques [46] can also be used to obtain continuous estimation of emotions for instance, in the valence-arousal space.", "startOffset": 22, "endOffset": 26}, {"referenceID": 41, "context": "Despite going against our predictions, it is explained nicely by the association of referential gestures and nods with physical and social immediacy [42], which could be considered a form of threat.", "startOffset": 149, "endOffset": 153}], "year": 2016, "abstractText": "Human interactions are characterized by explicit as well as implicit channels of communication. While the explicit channel transmits overt messages, the implicit ones transmit hidden messages about the communicator (e.g., his/her intentions and attitudes). There is a growing consensus that providing a computer with the ability to manipulate implicit affective cues should allow for a more meaningful and natural way of studying particular non-verbal signals of human-human communications by human-computer interactions [1], [2]. In this pilot study, we created a non-dynamic human-computer interaction while manipulating three specific non-verbal channels of communication: gaze pattern, facial expression, and gesture. Participants rated the virtual agent on affective dimensional scales (pleasure, arousal, and dominance) while their physiological signal (electrodermal activity, EDA) was captured during the interaction. Assessment of the behavioral data revealed a significant and complex three-way interaction between gaze, gesture, and facial configuration on the dimension of pleasure, as well as a main effect of gesture on the dimension of dominance. These results suggest a complex relationship between different non-verbal cues and the social context in which they are interpreted. Qualifying considerations as well as possible next steps are further discussed in light of these exploratory findings.", "creator": "LaTeX with hyperref package"}}}