{"id": "1606.04275", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Efficient Pairwise Learning Using Kernel Ridge Regression: an Exact Two-Step Method", "abstract": "this approximation during dyadic search concerns the independently desired properties for pairs of objects. filtering can be seen as an umbrella covering functional machine learning contexts both as matrix completion, collaborative filtering, multi - task operations, concurrent learning, machine prediction and fixed - priority learning. given this work scholars analyze similarity - bound assumptions promoting pairwise learning, with we particular focus on a never - suggested 9 - step method. we get that if option seeks a appealing alternative containing fast - applied kronecker - klein methods that model rendering basically means about matching graphical representations and pairwise ordering. in a series of theoretical stages, we calculate correspondences between the two types on function transform terms into linear detection and shared filtering, and we adjust their statistical consistency. in particular, the two - shift method allows us to establish novel algorithmic shortcuts while implicit training machine validation on very large computers. putting those properties finally, we mean that its simple, consistently powerful improvement having become highly standard tool into many problems. extensive modelling developments for a range of practical settings are generated.", "histories": [["v1", "Tue, 14 Jun 2016 09:38:18 GMT  (826kb,D)", "http://arxiv.org/abs/1606.04275v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michiel stock", "tapio pahikkala", "antti airola", "bernard de baets", "willem waegeman"], "accepted": false, "id": "1606.04275"}, "pdf": {"name": "1606.04275.pdf", "metadata": {"source": "CRF", "title": "Efficient Pairwise Learning Using Kernel Ridge Regression: an Exact Two-Step Method", "authors": ["Michiel Stock", "Tapio Pahikkala", "Antti Airola", "Bernard De Baets", "Willem Waegeman"], "emails": ["michiel.stock@ugent.be", "aatapa@utu.fi", "ajairo@utu.fi", "bernard.debaets@ugent.be", "willem.waegeman@ugent.be"], "sections": [{"heading": null, "text": "Keywords: dyadic prediction, pairwise learning, transfer learning, kernel ridge regression, kernel methods, zero-shot learning"}, {"heading": "1. Introduction to pairwise prediction", "text": "Many real-world machine learning problems can be naturally represented as pairwise learning or dyadic prediction problems. In contrast to more traditional learning settings, the goal here consists of making predictions for pairs of objects, each of them being characterized by a feature representation. Amongst others, applications of that kind emerge in biology (e.g. predicting mRNA-miRNA interactions), medicine (e.g. design of personalized\nar X\niv :1\n60 6.\n04 27\n5v 1\ndrugs), chemistry (e.g. prediction of binding between two types of molecules), ecology (e.g. prediction of host-parasite interactions), social network analysis (e.g. finding links between persons) and recommender systems (e.g. recommending personalized products to users).\nPairwise learning has strong connections with many other machine learning settings. Especially a link with multi-task learning can be advocated, by calling the first object of the dyad an \u2018instance\u2019 and the second object a \u2018task\u2019. As a typical multi-task learning example, consider ten schools providing the grades for a selection of their students. Suppose we want to predict the scores for new students in a school. The most straightforward path would be to fit a different model for each school, which uses features of the students. Intuitively, we might do better by building one model which takes all information into account. This can be done by learning a general model which has both the student and the school as input. The prediction function is thus pairwise: it takes both an instance and a task as input. In multi-task learning, the underlying idea for making the distinction between instances and tasks is that the feature description of the instances is often considered as more informative for making a prediction, while the feature description for the tasks is mainly used to steer learning into the right direction. In the majority of multi-task learning methods, a feature description for tasks is even not given, though often the idea that the tasks can be clustered or are located on a low-dimensional manifold is exploited.\nIn this work we adopt a multi-task learning terminology for pairwise learning. Formally, the training set is assumed to consist of a set S = {(dh, th, yh) | h = 1, . . . , n} of labeled instance-task pairs. As such, each training input is a labeled dyad (dh, th), where dh \u2208 D and th \u2208 T are the feature representations of the instances and the tasks, respectively, and yh is the corresponding label that is either continuous or discrete. Similarly, D and T are the corresponding spaces of instances and tasks. In pairwise learning a model f(d, t) is trained to make predictions for (possibly new) instances and tasks.\nVarious types of learning methods can be used to solve prediction problems of that kind. Kernel methods are very popular in bioinformatics, as indicated by the large number of applications in bioinformatics. They can be easily employed for pairwise learning by defining so-called pairwise kernels \u0393((d, t), (d\u0304, t\u0304)), which measure the similarity between two dyads (d, t) and (d\u0304, t\u0304). Kernels of that kind can be used in tandem with any conventional kernelized learning algorithm, such as support vector machines, kernel ridge regression and kernel Fisher discriminant analysis. In case of regression or classification tasks, one obtains prediction functions of the following form:\nf(d, t) = n\u2211 h=1 \u03b1k\u0393((d, t), (dh, th)) ,\nwith \u03b11, . . . , \u03b1n the dual parameters which are obtained by the learning algorithm \u2013 see subsequent section for references and details.\nIn this work we analyze a simple, yet elegant two-step method as an alternative for models based on pairwise kernels. In the first step, a kernel ridge regression model is trained on auxiliary tasks, and adopted to predict labels for the related target task. Then, in a second step, a second model is constructed, by employing kernel ridge regression on the target data, augmented with the predictions of the first phase. We first presented this method in a conference paper (Pahikkala et al., 2014). One year later, the same method was independently proposed as a tool to solve zero-shot learning problems (Romera-Paredes\nand Torr, 2015). From a different but related perspective, Schrynemackers et al. (2015) recently also proposed a similar method for biological network inference, but here tree-based methods instead of kernel methods were used as base learners. Those three papers have confirmed that the two-step method can obtain state-of-the-art performance on standard benchmarks in various pairwise learning settings. In this work, we will not put the emphasis on demonstrating the performance of this method. In contrast, our main focus is rather on a theoretical, computational and experimental justification of the two-step method, since we believe that it has a lot of potential. It will become clear that our two-step method is much simpler to implement than efficient Kronecker models, while manifesting more flexible model selection capabilities. It will also be shown to be applicable to more heterogeneous transfer learning settings.\nThis work can be subdivided in several parts that describe different aspects of the twostep method. Before going into mathematical details, Section 2 gives an overview of related settings in pairwise learning. We identify four different prediction settings, for which it is crucial to make a subdivision further in this paper. In Section 3 different kernel ridge regression-based methods that can be used for pairwise learning are described, including the two-step method. Subsequently, we show in Section 4 via linear algebra and spectral filtering that this approach is closely related to other kernel methods, but with slightly different regularization mechanisms. We also formally prove universal approximation properties for two-step kernel ridge regression. In Section 5 we use a specific decomposition to derive novel algorithmic shortcuts for leave-one-out cross-validation and for updating existing models with new training instances or tasks. A very important merit of the two-step method will be that there are closed-form shortcuts for cross-validation for any of the prediction settings discussed in Section 2, in contrast to the other methods. In the experiments (Section 6) we consider several dyadic prediction problems, studying generalization for the different settings as well as efficient implementations for training and testing the models. Our results show that the two-step method can be highly beneficial when there is no labeled data at all, or only a small amount of labeled data available for the target task, while in settings where there is a significant amount of labeled data available for the target task, an independenttask model suffices. Furthermore, we showcase the tools for performing cross-validation for the different settings and updating the model with extra instances or tasks."}, {"heading": "2. The different prediction settings in pairwise learning", "text": "In this section we make an important distinction between several types of pairwise learning settings. This will allow us to give a brief overview of related methods, and to perform a more detailed analysis in the upcoming sections. In pairwise learning it is extremely important to implement appropriate training and evaluation procedures. For example, in a large-scale meta-study about biological network identification it was found that these concepts are vital to correctly evaluate pairwise learning models (Park and Marcotte, 2012). Given d and t as the feature representations of the instances and tasks, respectively, four settings can be distinguished:\n\u2022 Setting A: Both d and t are observed during training, as parts of different dyads, but the label of the dyad (d, t) must be predicted.\n\u2022 Setting B: Only t is known during training, while d is not observed in any dyad, and the label of the dyad (d, t) must be predicted.\n\u2022 Setting C: Only d is known during training, while t is not observed in any dyad, and the label of the dyad (d, t) must be predicted.\n\u2022 Setting D: Neither d nor t occur in any training dyad, but the label of the dyad (d, t) must be predicted.\nThese settings are represented graphically in Figure 1. Setting A, the matrix completion problem, is of all four settings by far the most studied setting in the machine learning literature. Motivated by applications in collaborative filtering and link prediction, matrix factorization and related techniques are often applied to complete partially observed matrices. Missing values represent (d, t) combinations that are not observed during training - see e.g. Larochelle et al. (2008) for a review. Many of these matrix completion algorithms do not incorporate side features (features of the instances and tasks) and make assumptions on the structure of the true label matrix by, for example, assuming that the completed matrix is low rank or has a low nuclear norm (Candes and Recht, 2008; Mazumder et al., 2010). Recently, a framework based on bipartite graphs was proposed, which exploits the network structure for transductive link prediction (Liu and Yang, 2015). If one uses the notion of \u2018latent features\u2019 (which can be implemented by means of a delta kernel), one can exploit\nboth the structure of the label matrix as well as the side-features. Some interesting work has been done to unify these two approaches, using both the structure of the matrix, as well as features, e.g. Basilico and Hofmann (2004); Abernethy et al. (2008); Menon and Elkan (2011).\nSettings B and C are very similar, and a variety of machine learning methods can be applied to these settings. From a recommender systems viewpoint, those settings resemble the cold start problem (new user or new item), for which hybrid and content-based filtering techniques are often applied \u2013 see e.g. Adams et al. (2010); Fang and Si (2011); Menon and Elkan (2010); Shan and Banerjee (2010); Zhou et al. (2012) for a non-exhaustive list. From a bioinformatics viewpoint, Settings B and C are often analyzed using graph-based methods that take the structure of a biological network into account \u2013 see e.g. Schrynemackers et al. (2013) for a recent review. When the features of t are negligible or unavailable, while those of d are informative, Setting B can be interpreted as a multi-label classification problem (in case of binary labels), a multi-output regression problem (in case of continuous labels) or, more generally, as a multi-task learning problem. Here, most techniques encode dependency in the tasks by means of a suitable loss function or by jointly regularizing the different tasks (Dembczynski et al., 2012). Prior knowledge on the tasks can be incorporated by using a feature description of the tasks that captures the relations between the tasks. Setting C is closer to transfer learning, in which one wants to generalize to new tasks. Here as well, a large number of applicable methods is available in the literature (Pan and Yang, 2010).\nMatrix factorization and hybrid filtering strategies are not applicable to Setting D. We will refer to this setting as the zero-shot learning problem. This setting finds important applications in domains such as bioinformatics and chemistry \u2013 see experiments. Compared to the other three settings, Setting D has received less attention in the literature (but it is gaining rapidly in popularity, see e.g. Larochelle et al. (2008); Park and Chu (2009); Menon and Elkan (2010); Palatucci et al. (2009); Pahikkala et al. (2013); Rohrbach et al. (2011)). In the experimental section, we will investigate the transition phase between Settings C and D, when t occurs very few times in the training dataset, while d of the dyad (d, t) is only observed in the prediction phase. We refer to this setting as an almost zero-shot learning problem.\nFull and almost zero-shot learning problems can only be solved by considering feature representations of dyads. For Setting D Kronecker-based kernel methods are often employed (Vert et al., 2007; Brunner and Fischer, 2012). They have been successfully applied in order to solve problems such as product recommendation (Basilico and Hofmann, 2004; Park and Chu, 2009), enzyme annotation (Stock et al., 2014), prediction of proteinprotein (Ben-Hur and Noble, 2005; Kashima et al., 2009) or protein-nucleic acid (Pelossof et al., 2015) interactions, drug design (Jacob and Vert, 2008), prediction of game outcomes (Pahikkala et al., 2010) and document retrieval (Pahikkala et al., 2013). For classification and regression problems, a standard recipe consists of plugging pairwise kernels in support vector machines, kernel ridge regression (KRR), or any other kernel method. Efficient optimization approaches based on gradient descent (Park and Chu, 2009; Kashima et al., 2009) and closed-form solutions (Pahikkala et al., 2013) have been proposed. In the next section, we will review those methods more in detail."}, {"heading": "3. Pairwise learning using ridge-regression-based methods", "text": "In this section an overview of kernel methods for pairwise learning is given, with a particular focus on the two-step method that will be further analyzed in upcoming sections. To this end, let us further extend the mathematical notation that was introduced before. Let D = {di | i = 1, . . . ,m} and T = {tj | j = 1, . . . , q} denote, respectively, the sets of distinct instances and tasks encountered in the training set with m = |D| and q = |T |. We say that the training set is complete if it contains every instance-task pair with instance in D and task in T exactly once. For complete training sets, we introduce a further notation for the matrix of labels Y \u2208 Rm\u00d7q, so that its rows are indexed by the instances in D and the columns by the tasks in T .\nThe prediction function will be denoted as f(d, t), with fj(d) the model for task tj . In some cases we will denote the learning algorithm in superscript, when this is omitted the model should be clear from the context or the statement is generally valid. For notational simplicity and generality, we will mostly use the dual form in this work. The matrix of parameters for the different models will be denoted as A = [aij ] \u2208 Rm\u00d7q, where each column corresponds to a parameter set for a different task. A> denotes the transpose of A. In some cases we need to work with the parameters transformed to a column vector: vec(A) = \u03b1 = [\u03b1k] \u2208 Rmq. Finally, let Bi. denote the i-th row and B.j the j-th column of the matrix B; we also use Bij to refer to the element at the i-th row and j-th column of B."}, {"heading": "3.1 Independent-task kernel ridge regression", "text": "Suppose that the training set is complete and that for each task we have m labeled dyads D = {di}mi=1. Let Y.j \u2208 Rm be the labels of task tj and k(\u00b7, \u00b7) be a suitable kernel function which quantifies the similarity between the different instances. Since a separate and independent model is trained for each task, we will denote this setting as independent task (IT) kernel ridge regression. For each task tj , one would like to learn a function of the form\nf ITj (d) = m\u2211 i=1 aITij k(d,di) ,\nwith aITij parameters that minimize a suitable objective function. In the case of kernel ridge regression (KRR), this objective function is the squared loss with an L2-complexity penalty. The parameters for the individual tasks using KRR can be found jointly by minimizing the following objective function (Bishop, 2006):\nJ(AIT) = tr[(KAIT \u2212Y)>(KAIT \u2212Y)] + \u03bbdtr[AIT > KAIT] , (1)\nwith AIT = [aITij ] \u2208 Rm\u00d7q and K \u2208 Rm\u00d7m the Gram matrix associated with the kernel function k(\u00b7, \u00b7) for the instances. For simplicity, we assume the same regularization parameter \u03bbd for each task, though extensions to different penalties for different tasks are straightforward. This basic setting assumes no crosstalk between the tasks as each model is fitted independently. The optimal coefficients that minimize Eq. (1) can be found by solving the following linear system:\n(K + \u03bbdI) A IT = Y . (2)\nUsing the singular value decomposition of the Gram matrix, this system can be solved for any value of \u03bbd with a time complexity of O(m3 +m2q)."}, {"heading": "3.2 Pairwise and Kronecker kernel ridge regression", "text": "Suppose one has prior knowledge about which tasks are more similar, quantified by a kernel function g(\u00b7, \u00b7) defined over the tasks. Several authors (see Alvarez et al. (2012); Baldassarre et al. (2012) and references therein) have extended KRR to incorporate task correlations via matrix-valued kernels. However, most of this literature concerns kernels for which the tasks are fixed at training time. An alternative approach, allowing for the generalization to new tasks more straightforwardly by means of such a task kernel, is to use a pairwise kernel \u0393 ( (d, t) , ( d, t )) . Pairwise kernels provide a prediction function of type\nf(d, t) = n\u2211 h=1 \u03b1h\u0393 ((d, t) , (dh, th)) , (3)\nwhere \u03b1 = [\u03b1h] are parameters that minimize the following objective function similar to the one used for independent task KRR:\nJ(\u03b1) = (\u0393\u03b1\u2212 y)>(\u0393\u03b1\u2212 y) + \u03bb\u03b1>\u0393\u03b1 . (4)\nThe minimizer can also be found by solving a system of linear equations:\n(\u0393 + \u03bbI)\u03b1 = y , (5)\nwith \u0393 the Gram matrix. The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013). This kernel is defined as\n\u0393KK ( (d, t) , ( d, t )) = k ( d,d ) g ( t, t )\n(6)\nas a product of the data kernel k(\u00b7, \u00b7) and the task kernel g(\u00b7, \u00b7). Many other variations of pairwise kernels have been considered to incorporate prior knowledge on the nature of the relations (e.g. Vert et al. (2007); Pahikkala et al. (2010); Waegeman et al. (2012); Pahikkala et al. (2013)) or for more efficient calculations in certain settings, e.g. (Kashima et al., 2010).\nLet G \u2208 Rq\u00d7q be the Gram matrix for the tasks. Then, for a complete training set, the Gram matrix for the instance-task pairs is the Kronecker product \u0393 = G\u2297K. Often it is infeasible to use this kernel directly due to its large size. When the dataset is complete, the prediction function (Eq. (3)) can be written as\nfKK(d, t) = m\u2211 i=1 q\u2211 j=1 aKKij k(d,di)g(t, tj) . (7)\nThe matrix F containing the predictions for the training data using a pairwise kernel can be obtained by a linear transformation of the training labels:\nvec(F) = \u0393 (\u0393 + \u03bbI)\u22121 vec(Y) (8)\n= H\u0393vec(Y) , (9)\nwith vec the vectorization operator which stacks the columns of a matrix into a vector. In the statistical literature H\u0393 = \u0393 (\u0393 + \u03bbI)\u22121 is denoted as the so-called hat matrix (Hastie et al., 2001), which transforms the measurements into estimates. As a special case of the Kronecker KRR, we also retrieve ordinary Kronecker kernel least-squares (OKKLS), when the objective function of Eq. (4) has no regularization term (i.e. \u03bb = 0).\nSeveral authors have pointed out that, while the size of the system in Eq. (5) is considerably large, its solutions for the Kronecker product kernel can be found efficiently via tensor algebraic optimization (Van Loan, 2000; Martin and Van Loan, 2006; Kashima et al., 2009; Raymond and Kashima, 2010; Pahikkala et al., 2013; Alvarez et al., 2012). This is because the eigenvalue decomposition of a Kronecker product of two matrices can easily be computed from the eigenvalue decomposition of the individual matrices. The time complexity scales roughly with O(m3 + q3), which is required for computing the singular value decomposition of both the instance and task kernel matrices, but the complexities can be scaled down even further by using sparse kernel matrix approximation.\nHowever, these computational short-cuts only concern the case in which the training set is complete. If some of the instance-task pairs in the training set are missing or if there are several occurrences of certain pairs, one has to resort, for example, to gradient-descent-based training approaches (Park and Chu, 2009; Kashima et al., 2009; Pahikkala et al., 2013). While the training can be accelerated via tensor algebraic optimization, those techniques still remain considerably slower than the approach based on eigenvalue decomposition."}, {"heading": "3.3 Two-step kernel ridge regression", "text": "Clearly, independent-task ridge regression can generalize to new instances, but not to new tasks as no dependence between these tasks is encoded in the model. Kronecker KRR on the other hand can be used for all four prediction settings depicted in Figure 1. But since our definition of \u2018instances\u2019 and \u2018tasks\u2019 is purely conventional, nothing is stopping us from building a model using the kernel function g(\u00b7, \u00b7) to generalize to new tasks for the same instances. By combining two ordinary kernel ridge regressions, one for generalizing to new instances and one that generalizes for new tasks, one can indirectly predict for new dyads.\nMore formally, suppose one wants to make a prediction for the dyad (d, t). Let k \u2208 Rm denote the vector of instance kernel evaluations between the instances in the training set and an instance in the test set, i.e. k(d) = (k(d,d1), . . . , k(d,dm))\n>. Likewise, g \u2208 Rq represents the vector of task kernel evaluations between the target task and the auxiliary tasks, i.e. g(t) = (g(t, t1), . . . , g(t, tq))\n>. Based on the parameters found by solving Eq. (2), we can make a prediction for the new instance d for all the auxiliary tasks:\nfT (d) = k > (K + \u03bbdI) \u22121 Y , (10)\nwith \u03bbd the specific regularization parameter for the instances. This vector of predictions fT (d) can be used as a set of labels in an intermediate step to train a second model for generalizing to new tasks for the same instance. Thus, using the task kernel and a regularization parameter for the tasks \u03bbt, one obtains:\nfTS(d, t) = g> (G + \u03bbtI) \u22121 fT (d) > ,\nor, by making use of Eq. (10), the prediction is given by\nfTS(d, t) = k> (K + \u03bbdI) \u22121 Y (G + \u03bbtI) \u22121 g (11)\n= k>ATSg , (12)\nwith ATS the dual parameters. We call this method two-step (TS) kernel ridge regression and it was independently proposed as embarrassingly simple zero-shot learning by RomeraParedes and Torr (2015). It is represented in Figure 2. Superficially, this approach resembles alternating least-squares (Zachariah and Sundin, 2012), though the latter is an iteratively trained model mainly used for Setting A to obtain a low-rank representation. Our method on the other hand has a closed-form solution for the model parameters and allows for some computational techniques discussed later in this work. Two-step KRR can be used for any of the settings discussed in Section 2. Note that in practice there is no need to explicitly calculate fT , nor does it matter if in the first step one uses a model for new tasks and in the second step for instances, or the other way around.\nThis model can be cast in a similar form as the pairwise prediction function of Eq. (7) by making use of the identity vec(MXN) = (N> \u2297M)vec(X). Thus for two-step kernel ridge regression the parameters are given by\nATS = (K + \u03bbdI) \u22121 Y (G + \u03bbtI) \u22121 . (13)\nThe time complexity for two-step kernel ridge regression is the same as for Kronecker KRR: O(m3 + q3). The parameters can also be found by computing the eigenvalue decomposition of the two Gram matrices. Starting from these eigenvalue decompositions, it is possible to directly obtain the dual parameters for any values of the regularization hyperparameters \u03bbd and \u03bbt. Because of its conceptual simplicity, it is quite straightforward to use two-step KRR for certain cases when the label matrix is not complete, in contrast to the Kronecker KRR, see experimental section. The computational advantages of this method will be discussed in Section 5. Table 1 gives an overview of the different learning methods considered in this section."}, {"heading": "4. Theoretical considerations", "text": "In this section we will show that two-step kernel ridge regression can be seen as using kernel ridge regression with special kinds of pairwise kernel matrices, depending on the prediction setting. We will show that Setting A is a transductive setting while Setting D is merely a special case of (Kronecker) kernel ridge regression. We will also study the different learning algorithms from a spectral filtering point of view, showing that two-step kernel ridge regression uses a special decomposable filter. From these observations we will prove the universality and admissibility of the methods."}, {"heading": "4.1 Equivalence between two-step and other kernel ridge regression methods", "text": "The relation between two-step ridge regression and independent-task ridge regression is given in the following theorem.\nIn-sample tasks\nOut-of-sample tasks\nTheorem 1 (Setting B) When the Gram matrix of the tasks G is full rank and \u03bbt is set to zero, independent-task KRR and two-step KRR return the same predictions for any given training task:\nf ITj (\u00b7) \u2261 fTS(\u00b7, tj) .\nProof The prediction for the independent-task KRR is given by:\nf ITj (d) = [k >(K + \u03bbdI) \u22121Y]j .\nFor two-step KKR, it follows from Eq. (11) that\nfTSj (d) = [k >(K + \u03bbdI) \u22121YG\u22121G]j\n= [k>(K + \u03bbdI) \u22121Y]j .\nWhen G is singular, the q outputs for the different tasks are projected on a lowerdimensional subspace by two-step KRR. This means that a dependence between the tasks is enforced, even with \u03bbt = 0.\nThe connection between two-step and Kronecker kernel ridge regression is established by the following results.\nTheorem 2 (Setting A) Consider the following pairwise kernel matrix:\n\u039e = G\u2297K (\u03bbd\u03bbtI\u2297 I + \u03bbtI\u2297K + \u03bbdG\u2297 I)\u22121 .\nThe predictions for the training data F using pairwise KRR (Eq. (8)) with the above pairwise kernel and regularization parameter \u03bb = 1 correspond to those obtained with two-step KRR using the kernel matrices K, G with respective regularization parameters \u03bbd and \u03bbt.\nProof\nWe will write the corresponding empirical risk minimization of Eq. (4) from the perspective of value regularization. Since Setting A is an imputation setting, we directly search for the optimal predicted label matrix F, rather than the optimal parameter matrix. Starting from the objective function for KRR, the predictions for the training data are obtained via\nminimizing the following variational problem:\nJ(F) = vec(F\u2212Y)>vec(F\u2212Y) + vec(F)>\u039e\u22121vec(F) (14) = vec(F\u2212Y)>vec(F\u2212Y)\n+ vec(F)> ( G\u2297K (\u03bbd\u03bbtI\u2297 I + \u03bbtI\u2297K + \u03bbdG\u2297 I)\u22121 )\u22121 vec(F)\n= vec(F\u2212Y)>vec(F\u2212Y) + vec(F)> ( G\u22121 \u2297K\u22121 (\u03bbd\u03bbtI\u2297 I + \u03bbtI\u2297K + \u03bbdG\u2297 I) ) vec(F) = vec(F\u2212Y)>vec(F\u2212Y) + vec(F)> ( \u03bbd\u03bbtG \u22121 \u2297K\u22121 + \u03bbdI\u2297K\u22121 + \u03bbtG\u22121 \u2297 I ) vec(F) = tr((F\u2212Y)>(F\u2212Y) + \u03bbd\u03bbtF>K\u22121FG\u22121 + \u03bbdF>K\u22121F + \u03bbtF>FG\u22121) .\nThe derivative with respect to F is:\n\u2202J \u2202F = 2(F\u2212Y + \u03bbd\u03bbtK\u22121FG\u22121 + \u03bbdK\u22121F + \u03bbtFG\u22121)\n= 2(\u03bbdK \u22121 + I)F(\u03bbtG \u22121 + I)\u2212 2Y .\nSetting it to zero and solving with respect to F yields:\nF = (\u03bbdK \u22121 + I)\u22121Y(\u03bbtG \u22121 + I)\u22121\n= K(K + \u03bbdI) \u22121Y(G + \u03bbtI) \u22121G .\nComparing with Eq. (13), we note that F = KATSG, which proves the theorem.\nHere, we have assumed that K and G are invertible. The kernel \u039e can always be obtained as long as K and G are positive semi-definite. The relevance of the above theorem is that it formulates two-step KRR as an empirical risk minimization problem for Setting A (Eq. (14)). It is important to note that the pairwise kernel matrix \u039e only appears in the regularization term of this variational problem. The loss function is only dependent on the predicted values F and the label matrix Y. Using two-step KRR for Setting A when dealing with incomplete data is thus well defined. The empirical risk minimization problem of Eq. (14) can be modified so that the squared loss only takes the observed dyads into account:\nJ(F) = \u2211\n(d,t,y)\u2208S\n(y \u2212 f(d, t)))2 + vec(F)>\u039e\u22121vec(F) ,\nwith S the training set of labeled dyads. In this case, one ends up with a transductive setting. This explains why Setting A is the most easy setting to predict for, as will be shown in the experiments. See Rifkin and Lippert (2007a); Johnson and Zhang (2008) for a more in-depth discussion.\nTwo-step and Kronecker KRR also coincide in an interesting way for zero-shot learning problems (e.g. the special case in which there is no labeled data available for the target task). This, in turn, allows us to show the consistency of two-step KRR via its universal approximation and spectral regularization properties. The theorem below shows the relation between two-step KRR and ordinary Kronecker kernel ridge regression for Setting D.\nTheorem 3 (Setting D) Let us consider a zero-shot learning setting with a complete training set. Let fTS(\u00b7, \u00b7) be a model trained with two-step KRR and fOKKLS(\u00b7, \u00b7) be a model trained with ordinary Kronecker kernel least-squares regression (OKKLS) using the following pairwise kernel function on D \u00d7 T :\n\u03a5 (( d, t), (d, t )) = ( k ( d,d ) + \u03bbd\u03b4 ( d,d )) ( g ( t, t) + \u03bbt\u03b4 ( t, t )))\n(15)\nwhere \u03b4 is the delta kernel whose value is 1 if the arguments are equal and 0 otherwise. Then for making predictions for instances d \u2208 D \\D and tasks t \u2208 T \\ T not seen in the training set, it holds that fTS(t,d) = fOKKLS(t,d).\nProof From Eq. (11) we have the following dual model for prediction:\nfTS(d, t) = m\u2211 i=1 q\u2211 j=1 aTSij k(d,di)g(t, tj) ,\nwith ATS = [aTSij ] the matrix of parameters. Similarly, the dual representation of the OKKLS (see Eq. (7)), using a parametrization AOKKLS = [aOKKLSij ], is given by\nfOKKLS(d, t) = m\u2211 i=1 q\u2211 j=1 aOKKLSij \u03a5 ((d, t), (di, tj)) ,\n= m\u2211 i=1 q\u2211 j=1 aOKKLSij (k (d,di) + \u03bbd\u03b4 (d,di)) (g (t, tj) + \u03bbt\u03b4 (t, tj))) ,\n= m\u2211 i=1 q\u2211 j=1 aOKKLSij k(d,di)g(t, tj) .\nIn the last step we used the fact that d 6= di and t 6= tj to drop the delta kernels. Hence, we need to show that ATS = AOKKLS.\nBy Eq. (13) and denoting G\u0303 = (G + \u03bbI)\u22121 and K\u0303 = (K + \u03bbI)\u22121, we observe that the model parameters ATS of two-step model can also be obtained from the following closed form:\nATS = K\u0303YG\u0303 . (16)\nThe kernel matrix of \u03a5 for Setting D can be expressed as: \u03a5 = (G + \u03bbtI) \u2297 (K + \u03bbdI) . The OKKLS problem with kernel \u03a5 being\nvec(AOKKLS) = argmin AOKKLS\u2208Rm\u00d7q\n( vec(Y)\u2212\u03a5vec(AOKKLS) )> ( vec(Y)\u2212\u03a5vec(AOKKLS) ) ,\nits minimizer can be expressed as vec(AOKKLS) = \u03a5\u22121vec(Y) = ( (G + \u03bbtI) \u22121 \u2297 (K + \u03bbdI)\u22121 ) vec(Y)\n= vec ( (K + \u03bbdI) \u22121 Y (G + \u03bbtI) \u22121 ) = vec ( K\u0303YG\u0303 ) . (17)\nHere, we again make use of the identity vec(MXN) = (N>\u2297M)vec(X), which holds for any conformable matrices M, X, and N. From Eq. (17) it then follows that ATS = AOKKLS, which proves the theorem."}, {"heading": "4.2 Universality of the Kronecker product pairwise kernel", "text": "Here we consider the universal approximation properties of two-step kernel ridge regression. This is a necessary step in showing the consistency of this method. We first recall the concept of universal kernel functions.\nDefinition 1 (Steinwart, 2002) A continuous kernel k(\u00b7, \u00b7) on a compact metric space X (i.e. X is closed and bounded) is called universal if the reproducing kernel Hilbert space (RKHS) induced by k(\u00b7, \u00b7) is dense in C(X ), where C(X ) is the space of all continuous functions f : X \u2192 R.\nThe universality property indicates that the hypothesis space induced by a universal kernel can approximate any continuous function on the input space X to be learned arbitrarily well, given that the available set of training data is large and representative enough, and the learning algorithm can efficiently find this approximation from the hypothesis space (Steinwart, 2002). In other words, the learning algorithm is consistent in the sense that, informally put, the hypothesis learned by it gets closer to the function to be learned while the size of the training set gets larger. The consistency properties of two-step KRR are considered in more detail in Section 4.3.\nNext, we consider the universality of the Kronecker product pairwise kernel. The following result is a straightforward modification of some of the existing results in the literature (e.g. Waegeman et al. (2012)), but we present it here for self-sufficiency. This theorem is mainly related to Setting D, while it also covers the other settings as special cases.\nTheorem 4 The kernel \u0393KK((\u00b7, \u00b7), (\u00b7, \u00b7)) on D \u00d7 T defined in Eq. (6) is universal if the instance kernel k(\u00b7, \u00b7) on D and the task kernel g(\u00b7, \u00b7) on T are both universal.\nProof Let us define\nA\u2297 B = {t | t(d, t) = u(d)v(t), u \u2208 A, v \u2208 B} (18)\nfor compact metric spaces D and T and sets of functions A \u2282 C(D) and B \u2282 C(T ). We observe that the RKHS of the kernel \u0393 can be written as H(k) \u2297 H(g), where H(k) and H(g) are the RKHS of the kernels k(\u00b7, \u00b7) and g(\u00b7, \u00b7), respectively.\nLet > 0 and let t \u2208 C(D) \u2297 C(T ) be an arbitrary function which can, according to Eq. (18), be written as t(d, t) = u(d)v(t), where u \u2208 C(D) and v \u2208 C(T ). By definition of the universality property, H(k) and H(g) are dense in C(D) and C(T ), respectively. Therefore, there exist functions u \u2208 H(k) and v \u2208 H(g) such that\nmax d\u2208D |u(d)\u2212 u(d)| \u2264 , max t\u2208T |v(t)\u2212 v(t)| \u2264 ,\nwhere is a constant for which it holds that\nmax d\u2208D,t\u2208T\n{ | u(d)|+ | v(t)|+ 2 } \u2264 .\nNote that, according to the extreme value theorem, the maximum exists due to the compactness of D and T and the continuity of the functions u(\u00b7) and v(\u00b7). Now we have\nmax d\u2208D,t\u2208T\n|t(d, t)\u2212 u(d)v(t)|\n\u2264 max d\u2208D,t\u2208T\n{ |t(d, t)\u2212 u(d)v(t)|+ | u(d)|+ | v(t)|+ 2 } = max\nd\u2208D,t\u2208T\n{ | u(d)|+ | v(t)|+ 2 } \u2264 ,\nwhich confirms the density of H(k)\u2297H(g) in C(D)\u2297 C(T ). The space D \u00d7 T is compact if both D and T are compact according to Tikhonov\u2019s theorem. It is straightforward to see that C(D) \u2297 C(T ) is a subalgebra of C(D \u00d7 T ), it separates points in D \u00d7 T , it vanishes at no point of C(D \u00d7 T ), and it is therefore dense in C(D \u00d7 T ) due to the Stone-Weierstra\u00df theorem. Thus, H(k) \u2297 H(g) is also dense in C(D \u00d7 T ), and \u0393 is a universal kernel on D \u00d7 T ."}, {"heading": "4.3 Spectral interpretation", "text": "In Theorem 3 we have shown the relation between two-step kernel ridge regression and (Kronecker) kernel ridge regression for Setting D. In this section we will study the difference between single-task, Kronecker and two-step kernel ridge regression from the point of view of spectral regularization. The above shown universal approximation properties of this kernel are also connected to the consistency properties of two-step KRR, as is elaborated in more detail in this section.\nLearning by spectral regularization originates from the theory of ill-posed problems. This paradigm is well studied in domains such as image analysis (Bertero and Boccacci, 1998) and, more recently, in machine learning \u2013 e.g. Lo Gerfo et al. (2008). Here, one wants to find the parameters \u03b1 of the data-generating process given a set of noisy measurements y such that\n\u0393\u03b1 \u2248 y , (19)\nwith \u0393 a Gram matrix with eigenvalue decomposition \u0393 = W\u039bW>. At first glance, one can find the parameters \u03b1 by inverting \u0393:\n\u03b1 = \u0393\u22121y\n= W\u039b\u22121W>y .\nIf \u0393 has small eigenvalues, the inverse becomes highly unstable: small changes in the feature description of the label vector will lead to huge changes in \u03b1. Spectral regularization deals with this problem by generalizing the inverse by a so-called filter function to make solving Eq. (19) well-posed. The following definition of a spectral filter-based regularizer is standard in the machine learning literature (see e.g. Lo Gerfo et al. (2008) and references therein). Note that we assume \u0393((\u00b7, \u00b7), (\u00b7, \u00b7)) being bounded with \u03ba > 0 such that supx\u2208X \u221a \u0393(x,x) \u2264 \u03ba, assuring that the eigenvalues of the Gram matrix \u0393 are in [0, \u03ba2].\nDefinition 2 (Admissible regularizer) A function \u03d5\u03bb : [0, \u03ba 2] \u2192 R, parameterized by 0 < \u03bb \u2264 \u03ba2, is an admissible regularizer if there exist constants D,B, \u03b3 \u2208 R and \u03bd\u0304, \u03b3\u03bd > 0 such that\nsup 0<\u03c3\u2264\u03ba2 |\u03c3\u03d5\u03bb(\u03c3)| \u2264 D, sup 0<\u03c3\u2264\u03ba2\n|\u03d5\u03bb(\u03c3)| \u2264 B\n\u03bb , sup 0<\u03c3\u2264\u03ba2 |1\u2212 \u03c3\u03d5\u03bb(\u03c3)| \u2264 \u03b3 ,\nand sup 0<\u03c3\u2264\u03ba2\n\u03bb\u03bd \u03c3\u03bd |1\u2212 \u03c3\u03d5\u03bb(\u03c3)| \u2264 \u03b3\u03bd , for any \u03bd \u2208 ]0, \u03bd\u0304] ,\nwhere the constant \u03b3\u03bd does not depend on \u03bb.\nThe constant \u03bd\u0304 is in the literature called the qualification of the regularizer and it is related to the consistency properties of the learning method as described in more detail below.\nThe spectral filter is a matrix function that acts as a stabilized generalization of a matrix inverse. Hence, Eq. (19) can be solved by\n\u03b1 = \u03d5\u03bb(\u0393)y\n= W\u03d5\u03bb(\u039b)W >vec(Y) .\nSimilarly, the noisy measurements can be filtered to obtain a better estimation of the true labels:\nf = \u0393\u03b1\n= W\u039bW>W\u03d5\u03bb(\u039b)W >vec(Y) = W\u039b\u03d5\u03bb(\u039b)W >vec(Y) .\nThe spectral interpretation allows for using a more general form of the hat matrix (Eq. (9)):\nH\u0393 = W\u039b\u03d5\u03bb(\u039b)W > .\nFor example, the filter function corresponding to the Tikhonov regularization, as used for independent-task kernel ridge regression, is given by\n\u03d5TIK\u03bb (\u03c3) = 1\n\u03c3 + \u03bb ,\nwith the ordinary least-squares approach corresponding to \u03bb = 0. Several other learning approaches, such as spectral cut-off, iterated Tikhonov and L2 Boosting, can also be expressed as filter functions, but cannot be expressed as a penalized empirical error minimization problem analogous to Eq. (4) (Lo Gerfo et al., 2008). The spectral interpretation can also be used to motivate novel learning algorithms.\nMany authors have expanded this framework to multi-task settings, e.g. Argyriou et al. (2007, 2010); Baldassarre et al. (2012). We will translate the pairwise learning methods from Section 3 to this spectral regularization context. Let us denote the eigenvalue decomposition of the instance and task kernel matrices as\nK = U\u03a3U> G = VSV> .\nLet ui denote the i-th eigenvector of K and vj the j-th eigenvector of G. The eigenvalues of the kernel matrix obtained with the Kronecker product kernel on a complete training set can be expressed as the Kronecker product \u039b = S \u2297\u03a3 of the eigenvalues \u03a3 and S of the instance and task kernel matrices. For the models of this work, it is opportune to define a pairwise filter function over the representation of the instances and tasks.\nWe also note that we assume both of the factor kernels to be bounded, and hence we can write that all the eigenvalues \u03c2 of the Kronecker product kernel can be factorized as the product of the eigenvalues of the instance and task kernels as follows:\n\u03c2 = \u03c3s with 0 \u2264 \u03c3, s \u2264 a \u221a \u03c2 and 1 \u2264 a <\u221e , (20)\nwhere \u03c3, s denote the eigenvalues of the factor kernels and a the constant determined as the product of supd\u2208D \u221a k(d,d) and supt\u2208T \u221a g(t, t). Definition 3 (Pairwise spectral filter) We say that a function \u03d5\u03bb : [0, \u03ba 2] \u2192 R, parameterized by 0 < \u03bb \u2264 \u03ba2, is a pairwise spectral filter if it can be written as\n\u03d5\u03bb(\u03c2) = \u03d1\u03bb(\u03c3, s)\nfor some function \u03d1\u03bb : [0, a \u221a \u03c2]2 \u2192 R with 1 \u2264 a < \u221e, and it is an admissible regularizer for all possible factorizations of the eigenvalues as in Eq. (20).\nSince the eigenvalues of a Kronecker product of two matrices are just the scalar product of the eigenvalues of the matrices, the filter function for Kronecker KRR is given by\n\u03d1KK\u03bb (s, \u03c3) = \u03d5 TIK \u03bb (\u03c3s) =\n1\n(\u03c3s+ \u03bb) , (21)\nwhere \u03c3 and s are the eigenvalues of K and G, respectively. The admissibility of this filter is a well-known result, since it is simply the Tikhonov regularizer for the pairwise Kronecker product kernel.\nInstead of considering two-step kernel ridge regression from the kernel point of view, one can also cast it into the spectral filtering regularization framework. We start from Eq. (13) in vectorized form:\nvec(A) = ( (G + \u03bbtI) \u22121 \u2297 (K + \u03bbdI)\u22121 ) vec(Y)\n= ( (VSV> + \u03bbtI) \u22121 \u2297 (U\u03a3U> + \u03bbdI)\u22121 ) vec(Y)\n= (\n(V\u03d5TIK\u03bbt (S)V >)\u2297 (U\u03d5TIK\u03bbd (\u03a3)U\n>) ) vec(Y)\n= (\n(V \u2297U)(\u03d5TIK\u03bbt (S)\u2297 \u03d5 TIK \u03bbd\n(\u03a3))(V \u2297U)> ) vec(Y) .\nHence, one can interpret two-step KRR with a complete training set for Setting D as a spectral filtering regularization based learning algorithm that uses the pairwise Kronecker product kernel with the following filter function.\n\u03d1TS\u03bb (s, \u03c3) = \u03d5 TIK \u03bbt (s)\u03d5 TIK \u03bbd (\u03c3)\n= 1\n(\u03c3 + \u03bbd)(s+ \u03bbt)\n= 1\n\u03c3s+ \u03bbt\u03c3 + \u03bbds+ \u03bbt\u03bbd . (22)\nThe validity of this filter is characterized by the following theorem.\nTheorem 5 The filter function \u03d1TS\u03bb (\u00b7, \u00b7) is admissible with D = B = \u03b3 = 1, \u03b3\u03bd = 2ab, and has qualification \u03bd\u0304 = 12 for all factorizations of \u03c2 and \u03bb as\n\u03c2 = \u03c3s and \u03bb = \u03bbt\u03bbd with 0 \u2264 \u03c3, s \u2264 a \u221a \u03c2 and 0 < \u03bbt, \u03bbd \u2264 b \u221a \u03bb , (23)\nwhere 1 \u2264 a, b <\u221e are constants that do not depend on \u03bb or \u03c2.\nProof Let us recollect the last condition in Definition 2:\nsup 0<\u03c2\u2264\u03ba2\n\u03c2\u03bd \u03bb\u03bd |1\u2212 \u03c2\u03d5\u03bb(\u03c2)| \u2264 \u03b3\u03bd , for any \u03bd \u2208 ]0, \u03bd\u0304] ,\nwhere \u03b3\u03bd does not depend on \u03bb. In order to show this for all cases covered by Eq. (23), we rewrite the condition by taking the supremum with respect to the factorizations of \u03c2 and \u03bb given the constants a and b:\nsup 0<\u03c2\u2264\u03ba2\n0<\u03bbt,\u03bbd\u2264b \u221a \u03bb\n0<\u03c3,s\u2264a\u221a\u03c2\n\u03c2\u03bd \u03bb\u03bd\n( 1\u2212 \u03c2\n\u03c2 + \u03bbt\u03c3 + \u03bbds+ \u03bb\n) \u2264 \u03b3\u03bd , for any \u03bd \u2208 ]0, \u03bd\u0304] .\nThe left-hand side then becomes\nsup 0<\u03c2\u2264\u03ba2\n\u03c2\u03bd \u03bb\u03bd\n( 1\u2212 \u03c2\n\u03c2 + 2ab \u221a \u03bb \u221a \u03c2 + \u03bb\n) = sup\n0<\u03c2\u2264\u03ba2\n( 2ab\u03bb 1 2 \u2212\u03bd\u03c2\u03bd+ 1 2 + \u03bb1\u2212\u03bd\u03c2\u03bd\n\u03c2 + 2ab \u221a \u03bb \u221a \u03c2 + \u03bb\n) .\nBy checking the extreme values of the latter expression with respect to (\u03c2, \u03bb, \u03bd) using standard differential calculus, we observe that it is bounded by \u03b3\u03bd = 2ab if \u03bd \u2208 ]0, 12 ]. With values of \u03bd\u0304 larger than 12 , the term 2ab\u03bb 1 2 \u2212\u03bd\u03c2\u03bd+ 1 2 in the numerator grows arbitrarily while \u03bb \u2192 0, and hence the qualification is \u03bd\u0304 = 12 . The other conditions in Definition 2 can be checked by direct computation.\nThus, Eq. (22) can be positioned within the spectral filtering regularization framework with separate regularization parameter values for instances and tasks. In contrast to Eq. (21), the filter of two-step KRR can be factorized into a component for the tasks and instances separately:\n\u03d1\u03bb(s, \u03c3) = \u03d5\u03bbd(\u03c3)\u03d5\u03bbt(s) . (24)\nThis decomposition gives rise to some computational shortcuts for performing crossvalidation, as will be discussed in Section 5.\nProviding a different regularization for instances and tasks also makes sense from a learning point of view. It is easy to imagine a setting in which the instance has a much larger influence in determining the label compared to the task or vice versa. For example, consider a collaborative filtering setting with the goal of recommending books for customers. Suppose that the sales of a book is for a very large part determined simply by being a bestseller novel or not, and less by individual customers\u2019 taste. When building a predictive model, one would give more freedom to the part concerning the books (hence a lower regularization). Less degrees of freedom are given to the inference of the users\u2019 personal\ntask, as this is harder to learn and explains less of the variance in the preferences. This can be extended even further, by choosing specific filter functions separately for the instances and tasks tuned to the application at hand. In a pairwise setting, the filter function to perform independent-task KRR arises as a special case with \u03bbt = 0:\n\u03d1IT\u03bbt (s, \u03c3) = 1\n(\u03c3 + \u03bbd)s ,\nwhen the task kernel is full rank (see Theorem 1). Next, we analyze the consistency properties of two-step KRR in setting D, given the above results about the universality of the pairwise Kronecker product kernel and the spectral filtering interpretation of the method. Let R denote the expected prediction error of a hypothesis f with respect to some unknown probability measure \u03c1(x, y) on the joint space X \u00d7 R of inputs and labels, that is,\nR(f) = \u222b X\u00d7R (f(x)\u2212 y)2d\u03c1(x, y) .\nGiven the input space X , the minimizer of the error is the so-called regression function:\nf\u03c1(x) = \u222b R y d\u03c1(y | x) .\nFollowing Bauer et al. (2007); Lo Gerfo et al. (2008); Baldassarre et al. (2012), we characterize the quality of a learning algorithm via its consistency properties. In particular, by considering whether the learning algorithm is consistent in the following sense:\nDefinition 4 A learning algorithm is consistent if the following holds with high probability\nlim n\u2192\u221e \u222b X ( f\u0302\u03bbn (x)\u2212 f\u03c1(x) )2 d\u03c1(x) = 0 ,\nwhere f\u0302\u03bbn denotes the hypothesis inferred by the learning algorithm from a training set having n independently and identically drawn training examples.\nThe following result is assembled from the existing literature concerning spectral filtering based regularization methods and we present it here only in a rather abstract form. For the exact details and further elaboration, we refer to Bauer et al. (2007); Lo Gerfo et al. (2008); Baldassarre et al. (2012).\nTheorem 6 If the filter function is admissible and the kernel function is universal, then the learning algorithm is consistent in the sense of Def. 4. Furthermore, if the regularization parameter is set as \u03bb = 1\nn2\u03bd\u0304+1 , where n denotes the number of independently and identically\ndrawn training examples, then the following holds with high probability: R(f\u0302\u03bb)\u2212R(f\u03c1(x)) = O ( n\u2212 \u03bd\u0304 2\u03bd\u0304+1 ) . (25)\nIntuitively put, the universality of the kernel ensures that the regression function belongs to the hypothesis space of the learning algorithm and the admissibility of the regularizer ensures that R(f\u0302\u03bb) converges to it when the size of the training set approaches infinity and the rate of convergence is reasonable.\nCorollary 5 Two-step KRR is consistent and the hypothesis it infers from the training set of size n = mq converges to the underlying regression function with a rate at least proportional to\nR(f\u0302\u03bb)\u2212R(f\u03c1(d, t)) = O ( min(m, q)\u2212 \u03bd\u0304 2\u03bd\u0304+1 ) . (26)\nProof The result follows from the admissibility of the pairwise filter function, the universality of the pairwise Kronecker product kernel and the fact that the training set consists of at least min(m, q) independently and identically drawn training examples."}, {"heading": "5. Efficient algorithms for two-step kernel ridge regression", "text": "In this section we derive some computational shortcuts for two-step kernel ridge regression. For Kronecker kernel ridge regression, it is well known that the huge system of Eq. (5) can be solved efficiently because the Gram matrix can be decomposed. Our two-step method takes this decomposition one step further, as the model can be seen as applying two consecutive regression steps. This allows to derive efficient algorithms for cross-validation for each of the four settings depicted in Figure 1, while the original Kronecker kernel ridge regression only allowed for a shortcut for Setting A. The same linear algebra can also be used to implement a scheme for online updating of the model with new instances or tasks. These shortcuts for cross-validating for Settings B, C and D and online updating cannot be derived for Kronecker ridge regression in general."}, {"heading": "5.1 Efficient hold-out computation", "text": "As indicated in Figure 1, making predictions in a dyadic setting is much more complex compared to the classical case when there is only one task. This implies that the correct way to assess the performance and to do model selection is also more complicated. To this end we suggest cross-validation settings which take the structure of the label matrix into account. Depending on the prediction setting of interest, one should withhold individual elements, rows, columns or both of the label matrix, as shown in Figure 3. These schemes have been discussed in other works, often in the context of predicting interactions in molecular biology, e.g. (Park and Marcotte, 2012; Pahikkala et al., 2015). In this work, we will only focus on deriving shortcuts for leave-one-out cross-validation for these settings. Extensions to general hold-out schemes can be obtained using similar reasonings as the one in this section.\nIt is well known that for independent-task kernel ridge regression one can efficiently compute the values for leave-one-out cross-validation, provided one has stored the hat matrix (Rifkin and Lippert, 2007b). For the instances, using the kernel k(\u00b7, \u00b7), the hat matrix is denoted as Hk = K(K +\u03bbdI) \u22121 (see Eq. (9)). As noted earlier, if this matrix is obtained\nSetting A Setting B\nusing an eigenvalue decomposition of K, Hk can be computed efficiently for any \u03bbd:\nHk = K(K + \u03bbdI) \u22121\n= U\u03a3U>(U\u03a3U> + \u03bbdI) \u22121 = U\u03a3U>U(\u03a3 + \u03bbdI) \u22121U> = U(\u03a3(\u03a3 + \u03bbdI) \u22121)U> .\nFirst, we will derive the well-known shortcut for independent-task kernel ridge regression for calculating the leave-one-out cross-validation values, using our notation. We start with the classical leave-one-out shortcut.\nTheorem 7 A single row of the matrix FIT,LOO containing the labels re-estimated by leaveone-out cross-validation using independent-task kernel ridge regression with associated hat matrix Hk, can be calculated as\nFIT,LOOi. = Hki.Y \u2212HkiiYi.\n1\u2212Hkii .\nProof This is merely a multivariate version of the leave-one-out shortcut, proven in texts such as Wahba (1990); Pahikkala et al. (2006); Rifkin and Lippert (2007b).\nThe above theorem can be applied to ridge regression and related models. The shortcut is relevant for both Settings A and B, as it is used to estimate how well the model can generalize to new instances, though kernel ridge regression does not use any information on the tasks. Starting from this general shortcut and using the connection between independent-task, two-step and Kronecker KRR, we can derive shortcuts for Setting A.\nCorollary 1 (Setting A) A single element of the matrix containing the labels re-estimated by leave-one-out cross-validation for Setting A, i.e. leaving out one dyad at a time, using Kronecker kernel ridge regression, can be calculated as\nFKK,LOO,Aij = HKKs. vec(Y)\u2212HKKss Yij\n1\u2212HKKss ,\nwith HKK = (G\u2297K)(G\u2297K + \u03bbI)\u22121 and s = mj + i.\nProof We have noted earlier that Kronecker kernel ridge regression is merely kernel ridge regression using the Gram matrix G\u2297K and vec(Y) as the single output label vector. Since HKK is the corresponding hat matrix, the proof follows directly from rephrasing Theorem 7 in this terminology.\nBy using the eigenvalue decomposition of the kernel matrices, the diagonal elements of HKK and HKKvec(Y) can be computed efficiently.\nCorollary 2 (Setting A) A single element of the matrix containing the labels re-estimated by leave-one-out cross-validation for Setting A, i.e. leaving out one dyad at a time, using two-step kernel ridge regression can be calculated as\nFTS,LOO,Aij = HTSs. vec(Y)\u2212HTSss Yij\n1\u2212HTSss ,\nwith HTS = (G + \u03bbtI) \u22121G\u2297 (K + \u03bbdI)\u22121K and s = mj + i.\nProof Theorem 2 states that two-step KRR is merely kernel ridge regression with the Gram matrix \u039e and vec(Y) as the single output label vector. From Eq. (13) it follows that HTS is the hat matrix for two-step kernel ridge regression in vectorized form:\nf = vec(KATSG)\n= vec(K (K + \u03bbdI) \u22121 Y (G + \u03bbtI) \u22121 G) = [(G + \u03bbtI) \u22121G\u2297 (K + \u03bbdI)\u22121K]vec(Y) .\nHence, the proof follows directly from rephrasing Theorem 7 in this terminology.\nThese two shortcuts are of interest when validating or tuning models for collaborative filtering or network inference. For example, recently a model for recipe completion was validated by iteratively withholding single elements from the label matrix (De Clercq et al., 2015). An interesting application would be in supervised biological network inference, e.g. predicting interactions between biomolecules, such as proteins, ligands or DNA or predicting interactions between species, such as plants and pollinators (see e.g. Rafferty and Ives\n(2013); Hadfield et al. (2014) for applications of pairwise learning in such a setting). Such datasets are often plagued with false negatives or false positives, which make them difficult to analyze, see Schrynemackers et al. (2013) for a discussion. Using the provided holdout tricks, it is possible to re-estimate each interaction extremely efficiently in order to screen for mislabeled observations.\nBecause two-step kernel ridge regression can be decomposed in two \u2018steps\u2019, it is possible to derive shortcuts for leave-one-out cross-validation for Settings B, C and D. Below is a shortcut for Setting B, which allows for cross-validation of one row (instance) at a time.\nCorollary 3 (Setting B) A single row of the matrix containing the labels re-estimated by leave-one-out cross-validation for Setting B, i.e. leaving out one instance at a time, using two-step kernel ridge regression, can be calculated as\nFTS,LOO,Bi. =\n( Hki.Y \u2212HkiiYi. ) Hg\n1\u2212Hkii .\nProof Using two-step KRR for Setting B can be thought of as just using independent-task KRR with Hk as the hat matrix and YHg as the label matrix. The proof follows directly from rephrasing Theorem 7 in this terminology.\nSimilarly, using the following shortcut one can perform cross-validation for Setting C, leaving out one column (task) at a time.\nCorollary 4 (Setting C) A single column of the matrix containing the labels re-estimated by leave-one-out cross-validation for Setting C, i.e. leaving out one task at a time, using two-step kernel ridge regression, can be calculated as\nFTS,LOO,C.j = Hk ( YHg.j \u2212Y.jH g jj ) 1\u2212Hgjj .\nProof Similarly as for Corollary 3 but using a \u2018transposed\u2019 dataset, two-step KRR for Setting C can be thought of as just performing independent-task KRR with Hg as the hat matrix and Y>Hk as the label matrix. We apply Theorem 7 using this terminology and transpose the obtained matrix.\nFinally, the theorem below gives the shortcut for Setting D, the zero-shot learning setting.\nCorollary 5 (Setting D) A single element of the matrix containing the labels re-estimated by leave-one-out cross-validation for Setting D, i.e. leaving out one dyad with the corresponding row and column and predicting for that dyad, using two-step kernel ridge regression can be calculated as\nFTS,LOO,Dij = Hki.\n( YHg.j \u2212Y.jH g jj ) \u2212Hkii ( Yi.H g .j \u2212YijH g jj ) (1\u2212Hkii)(1\u2212H g jj) .\nProof This can easily be proven by applying Theorem 7 twice. First, a column vector that predicts for task j is generated, using a model trained only based on the q \u2212 1 remaining\ntasks. Adopting a similar reasoning as in Corollary 4, we obtain\nFSTEP1.j = YHg.j \u2212Y.jH g jj\n1\u2212Hgjj .\nThis vector is subsequently used as a label vector for an \u2018unseen\u2019 task j. We apply Theorem 7 once more to obtain the leave-one-out value of instance (row) i:\nFTS,LOO,Dij = Hki.F STEP1 .j \u2212HkiiFSTEP1ij\n1\u2212Hkii\n=\nHki.\n( YHg.j\u2212Y.jH g jj\n1\u2212Hgjj\n) \u2212Hkii ( Yi.H g .j\u2212YijH g jj\n1\u2212Hgjj ) 1\u2212Hkii\n= Hki.\n( YHg.j \u2212Y.jH g jj ) \u2212Hkii ( Yi.H g .j \u2212YijH g jj ) (1\u2212Hkii)(1\u2212H g jj) .\nAll the derived shortcuts are summarized in a more compact matrix notation in Table 2. Each matrix of hold-out values is computed by dividing the predicted labels of the complete training set by an appropriate linear transformation of the diagonal elements of the hat\nmatrices. As noted earlier, using an eigenvalue decomposition with a time complexity of O(m3 + q3), these hat matrices and predictions can be obtained for any values of \u03bbd and \u03bbt by straightforward matrix manipulations. The shortcuts are in fact valid for all models with a pairwise spectral filter of the form in Eq. (24). Stated more boldly, after the eigenvalue decomposition, one can tune or validate the model for any of the four settings at essentially no computational cost, compared to the cost of the initial preprocessing. For Kronecker kernel ridge regression, the only computational shortcut of Table 2 possible is for Setting A, as this makes no use of this property. Since many interesting learning problems relate to the other settings, we consider the shortcuts possible for two-step kernel ridge regression a very strong merit of this method."}, {"heading": "5.2 Learning with mini batches", "text": "In some cases it is desirable to train a model in an online fashion, as opposed to the standard batch training. For example, when new instances for the different tasks become available, it is more desirable to update the model parameters rather than completely retrain the model. Similarly, new tasks could be added to the model, which also influences the performance of the older tasks. This is prevalent in online applications, such as recommender systems, in which data is added dynamically so that the systems should be updated swiftly. In the context of dealing with large-scale data, the required matrices might simply be too large to fit in main memory as a whole, thus an iterative approach is needed.\nIn addition to gradient-based methods such as stochastic gradient descent and conjugated gradient descent, closed-from solutions can be derived for updating the model parameters for two-step KRR. Since we assume that this is of particular interest for large-scale data applications, we will derive a shortcut for the primal form. Denote a feature vector for the instances by \u03c6 \u2208 Rd and for the tasks by \u03c8 \u2208 Rr. Here we assume that d m and r q. These can either be the primal features or be obtained from a decomposition of the kernel matrices, for example by means of the Nystro\u0308m method (Drineas and Mahoney, 2005). This leads to the associated feature matrices \u03a6 \u2208 Rm\u00d7d and \u03a8 \u2208 Rq\u00d7r. Hence, the primal notation boils down to:\nK = \u03a6\u03a6> k = \u03a6\u03c6 G = \u03a8\u03a8> g = \u03a8\u03c8 .\nThus the model of Eq. (11) translates to\nfTS(x) = \u03c6>\u03a6ATS\u03a8>\u03c8\n= \u03c6>WTS\u03c8 ,\nwith WTS the primal parameter matrix. Suppose that the dataset is again complete, then, starting from Eq. (13), the parameters are given by\nWTS = \u03a6(\u03a6\u03a6> + \u03bbdI) \u22121Y(\u03a8\u03a8> + \u03bbtI) \u22121\u03a8>\n= (\u03a6>\u03a6 + \u03bbdI) \u22121\u03a6>Y\u03a8(\u03a8>\u03a8 + \u03bbtI) \u22121 ,\nwhere we made use of the matrix inversion lemma (Bishop (2006), Eq. (C.7)). Suppose that in a first training phase, we use only the first m\u2212 l instances to train the first model\nAlgorithm 1 Update primal parameters WTSold of two-step KRR using batches of l new instances, assuming l < d. In addition to the old weight matrix, new instance features and labels, the algorithm requires precomputed matrices Mold = (\u03a6 > old\u03a6old + \u03bbdI)\n\u22121, Bg = \u03a8(\u03a8>\u03a8+\u03bbtI)\n\u22121 and \u03a6>oldYold. The algorithm also updates these matrices using the new data. INPUT: \u03a6new,Ynew,Wold,Mold,B g,\u03a6>oldYold\n1: Mnew = Mold \u2212Mold\u03a6>new(\u03a6newMold\u03a6>new + I)\u22121\u03a6newMold . O(l3 + d2l) 2: WTSnew = Mnew(\u03a6 > oldYold + \u03a6 > newYnew)B\ng . O(d3 + q3) 3: return Wnew,Mnew, (\u03a6 > oldYold + \u03a6 > newYnew)\nand later update the model with the remaining l instances. Without loss of generality, we divide the labels and instance features as follows in corresponding block matrices\n\u03a6 = ( \u03a61 \u03a62 ) and Y = ( Y1 Y2 ) ,\nwith \u03a61 \u2208 R(m\u2212l)\u00d7d, \u03a62 \u2208 Rl\u00d7d, Y1 \u2208 R(m\u2212l)\u00d7q and Y2 \u2208 Rl\u00d7q. The model parameters of the complete dataset can then be calculated as\nWTS = (\u03a6>1 \u03a61 + \u03a6 > 2 \u03a62 + \u03bbdI) \u22121(\u03a6>1 Y1 + \u03a6 > 2 Y2)\u03a8(\u03a8 >\u03a8 + \u03bbtI) \u22121\n= (M\u221211 + \u03a6 > 2 \u03a62) \u22121(\u03a6>1 Y1 + \u03a6 > 2 Y2)\u03a8(\u03a8 >\u03a8 + \u03bbtI) \u22121 = [M1 \u2212M1\u03a6>2 (I + \u03a62M1\u03a6>2 )\u22121\u03a62M1] (\u03a6>1 Y1 + \u03a6 > 2 Y2)\u03a8(\u03a8 >\u03a8 + \u03bbtI) \u22121 , (27)\nwhere M1 = (\u03a6 > 1 \u03a61 + \u03bbdI) \u22121. In Eq. (27), we have made use of the Woodbury identity. Thus, in order to update the model parameters, the l \u00d7 l matrix (I + \u03a6>2 M1\u03a62) has to be inverted. A practical implementation for this scheme is given in Algorithm 1. If l > d, it is useful to make use of the matrix identity\n(I + \u03a62M1\u03a6 > 2 ) \u22121\u03a6>2 M1 = \u03a6 > 2 (\u03a6 > 2 \u03a62 + M \u22121 1 ) \u22121\nin line 1 so that only a d \u00d7 d matrix has to be inverted. To update for new tasks, a very similar algorithm can be derived. In this case we assume that the data is divided as follows:\n\u03a8 = ( \u03a81 \u03a82 ) and Y = ( Y1 Y2 ) ,\nwith \u03a62 \u2208 Rl\u00d7r, \u03a81 \u2208 R(q\u2212l)\u00d7r, Y1 \u2208 Rm\u00d7(q\u2212l) and Y2 \u2208 Rm\u00d7l. Using this notation, Algorithm 2 updates the parameters for a set of l new tasks."}, {"heading": "6. Experiments", "text": "In the experiments we will demonstrate the learning properties of two-step KRR, compared to independent-task and Kronecker KRR. Furthermore, the experiments illustrate the efficient algorithms for training and evaluating the two-step KRR model, in contrast to the more limited toolkit for the Kronecker KRR model. To be more specific:\nAlgorithm 2 Update primal parameters WTSold of two-step KRR using batches of l new tasks, assuming l < q. In addition to the old weight matrix, new task features and labels, the algorithm requires precomputed matrices Nold = (\u03a8 > old\u03a8old + \u03bbtI)\n\u22121, Bk = (\u03a6>\u03a6 + \u03bbdI)\n\u22121\u03a6> and Yold\u03a8old. The algorithm also updates these matrices using the new data. INPUT: \u03a8new,Ynew,Wold,Nold,B k,Yold\u03a8old\n1: Nnew = Nold \u2212Nold\u03a8>new(\u03a8newNold\u03a8>new + I)\u22121\u03a8newNold . O(l3 + r2l) 2: WTSnew = B\nk(Yold\u03a8old + Ynew\u03a8new)Nnew . O(d3 + r3) 3: return Wnew,Nnew, (Yold\u03a8old + Ynew\u03a8new)\n\u2022 In Section 6.1 we study the performance of two-step and Kronecker KRR for the four different settings on four protein-ligands benchmarks. We illustrate the use of the analytical shortcuts for cross-validation.\n\u2022 In Section 6.2 we use a case study of protein-ligands interactions to study the differences between independent-task regression, multi-task learning and zero-shot learning. Some of the learning curves can only be made without resorting to slower gradientbased optimization for training the models with two-step KRR.\n\u2022 Finally, in Section 6.3 we demonstrate learning in mini-batches on a large-scale hierarchical text classification problem.\nWe refer to Romera-Paredes and Torr (2015) for some experimental results which show that two-step KRR is a competitive method compared to established zero-shot learning methods, including DAP (Lampert et al., 2014) and ZSRwUA (Jayaraman and Grauman, 2014), for some zero-shot learning benchmark datasets. We refer to that paper for a comparison with the state-of-the-art."}, {"heading": "6.1 Study of regularization for the different settings", "text": "In this section we investigate the influence of the regularization parameters \u03bb, \u03bbd and \u03bbt, of two-step and Kronecker KRR for the different Settings A, B, C and D. We will also demonstrate the scalability of the different shortcuts for cross-validation, described in Section 5. To this end, we use four drug-target classification datasets collected by Yamanishi et al. (2008)1. Each dataset concerns a different class of protein targets: enzymes (e), G proteincoupled receptors (gpcr), ion channels (ic) and nuclear receptors (nc). The properties of these datasets are given in Table 3. The interactions are given in the form of a binary adjacency matrix. Both the drugs and targets come along with a respective similarity matrix. For the drugs, common substructures are calculated using a graph alignment algorithm. The Jaccard similarity measure is used to obtain a drug similarity based on these substructures. The similarity matrix of the targets is a normalized version of the scores obtained by Smith-Waterman alignment. We rescored the labels, such that positive interactions have a value of N/N+, while negative interactions have a value of \u2212N/N\u2212, with N the number of\n1. Available at http://web.kuicr.kyoto-u.ac.jp/supp/yoshi/drugtarget\npairs and N+ and N\u2212 the number of positive and negative interactions, respectively. By using this relabeling, minimizing the squared loss becomes equivalent with Fisher discriminant analysis (Bishop, 2006), making our method more suitable for classification.\nFor each of the models, we perform leave-one-out cross-validation for new pairs, new targets and both, as described in the introduction. The cross-validated predictions are obtained using the computation short-cuts described in Section 5.1. For the four different cross-validation settings we use AUC as a performance measure:\n\u2022 For Settings A and D, i.e. imputation of pairs and new targets and ligands, we use the micro AUC. Here, the AUC is calculated over all the pairs and the model is evaluated for its ability to give a higher score to an arbitrary interacting dyad compared to an arbitrary non-interacting dyad.\n\u2022 For the setting where we leave out one target with all the drugs at a time (Setting B), the AUC is calculated over all the drugs for each target and this then averaged over the drugs.\n\u2022 Likewise, for the setting with new drugs with all the ligands for validation (Setting C), the AUC is calculated over all the ligands for each drug and then averaged over the targets.\nWe trained Kronecker and two-step KRR with the regularization parameters \u03bb, \u03bbd and \u03bbt each taken from the grid with values 10\n\u22127, 10\u22126, . . . , 105, 106. For each (combination of) regularizer(s), the performance was calculated. For Setting A for both methods and the other settings for two-step KRR, we used the analytic shortcuts to calculate the holdout\nvalues. For the other settings using Kronecker KRR, we calculated the eigenvalue decomposition of a submatrix of the Gram matrix K (G) for each row (column) for Setting B (Setting C). For Setting D, we computed the eigenvalue decomposition of a submatrix of K for each row of Y, while calculating the eigenvalue decomposition of a submatrix of G for each element in this row. This corresponds to performing the smallest number of computations, while only holding one eigenvalue decomposition of each Gram matrix in memory.\nTable 4 shows the best performances for both methods, as well as the running times for performing the complete cross-validation and hyperparameter grid search. For the different settings and datasets, we observe that both methods have a similar performance, with twostep KRR often slightly outperforming Kronecker KRR. For both methods, Setting D is the hardest and Setting A the easiest, as expected.\nIt is immediately clear that the optimal regularization depends both on the type of dataset, as well as on the type of predictions one is aiming at. For example, comparing the setting of new targets with new drugs for the datasets e, gpcr and ic, we see that for generalizing to new targets a low regularization for the drugs is needed, while the latter cases require the opposite. For these cases, it seems the models can predict better for new targets compared to drugs, indicating that the feature description for the drugs is more adequate. Hence, finding a suitable model for predicting for new targets in this case is harder, indicated by smaller regions of high performance. We see that for the nr data, the regions are quite irregular and dependent on the setting, likely because this is the smallest dataset. Depending on the dataset, Setting B or Setting C is the harder one. Likely this is determined by both the number of proteins and ligands in the training set, as well as the quality of the two kernel matrices.\nWhen comparing the running times for model selection, we observe the computational advantage of two-step KRR. For Setting A, both methods have a holdout shortcut, hence both are fast. Kronecker KRR has to iterate over a grid of 15 regularization values, while two-step KRR has to search a grid of 15 \u00d7 15 regularization parameters, making it slower. Both methods are very fast in practice though. For Settings B and C, there is only an efficient algorithm for two-step KRR. For datasets larger than dataset nr, two-step KRR is much faster than Kronecker KRR. Note that because for these settings we calculate the AUC for each row and column in each step, calculating the performance takes more computing time than calculating the leave-one-out values! If we would use the same performance metric for all settings (which would not make sense for the AUC), two-step KRR would have approximately the same running time for all settings. Finally, for Setting D two-step KRR is much faster compared to Kronecker KRR, where it was not even possible to do this cross-validation for the e dataset within three days.\nIn Figures 4 and 5 it is visualized how the performance depends on the regularization for the different settings. The performance is quite sensitive to the value of the regularization parameter(s), a fact well known in machine learning. The optimal regularization is also strongly determined by the cross-validation setting, especially for the two-step method. The contour plots of Figure 5 illustrate that for different settings a specific model has to be selected. Penalizing the instances and the tasks separately is natural for these types of learning problems, and the effect on the performance can be a valuable diagnostic tool to aid the model building. Two-step KRR allows for efficiently exploring this space for any setting, in contrast to Kronecker KRR."}, {"heading": "6.2 Comparison of different transfer learning settings", "text": "In this series of experiments, we compare different types of transfer learning settings in protein-ligand prediction. We simulate the zero-shot and almost zero-shot learning problem as follows. In each experiment, one ligand is considered to be the target task in question,\nwhere the task is to predict the interactions of proteins with respect to the target. Further, other tasks formed in the same way are provided as auxiliary information, leading to a zero-shot learning or almost zero-shot learning setting. The experiments are performed 100 times with different training/test set splits.\nThe experiments were performed using two different datasets. In the first experiment, the enzyme dataset of the previous section was reused. The goal is to learn to predict for the given ligand the binding with a set of proteins that were not encountered in the training phase. The performances are always computed over a test set of 164 protein targets for a given task, i.e. we assess whether for a given target we can discriminate between proteins with more or less affinity for this ligand. The performance was measured by calculating the AUC for the test set of proteins for each ligand or task separately (i.e. macro AUC).\nWe also used a different drug-target interaction prediction dataset2 (Davis et al., 2011; Pahikkala et al., 2015) consisting of 68 drug compounds and 442 protein targets. In contrast to the earlier protein-ligand datasets, this is a regression task with real-valued labels. The kernel between the drugs is based on the 3D Tanimoto similarity coefficient, and the sequence similarity between the protein targets was computed using the normalized version of the Smith-Waterman score. Further, for each drug-protein pair we have a real-valued label, the negative logarithm of the kinase disassociation constant Kd, which characterizes the interaction affinity between the drug and target in question. In each experiment, the task of interest corresponds to one of the drugs in the data set. The goal is to learn to predict for the given drug the Kd values for proteins unseen during the training phase. The performances are always computed over a test set of 192 protein targets for a given task, i.e. we assess whether for a given target we can discriminate between proteins with more or less affinity for this drug. The performances are averages over all repetitions and over all target tasks, and are measured using the concordance index (Go\u0308nen and Heller, 2005) (C-index), also known as the pairwise ranking accuracy:\n1 |{(i, j) | yi > yj}| \u2211 yi>yj H(fi \u2212 fj) ,\nwhere yi denotes the true and fi the predicted label, and H is the Heaviside step function. The C-index can be seen as a generalization of the area under the ROC curve (AUC). Regularization parameter selection is performed using leave-one-out cross-validation on the training data using the shortcuts of Section 5.1. The algorithms used in the experiments are implemented in the RLScore open source machine learning library3.\nFor each task, we vary the number of available training proteins. For the enzyme dataset the number of training proteins is increased from 25 to 500 in steps of 25 and for the drugtarget affinity the number of proteins is varied from 10 to 250 in steps of 5. In addition, we have available the training data for all training proteins for the auxiliary tasks. As summarized in Figure 6, we evaluate a number of different approaches:\n\u2022 Independent-task: KRR trained with data from the target task only. The regularization parameter is selected based on leave-one-out cross-validation for Setting A.\n2. http://users.utu.fi/aatapa/data/DrugTarget 3. Available at https://github.com/aatapa/RLScore\nAlmost zero-shot\nSingle-task Zero-shotMulti-task\n\u2022 Multi-task: both the target and auxiliary tasks have the same training data available (multi-output learning leveraging task correlations, tackled with Kronecker and twostep KRR). The regularization parameter is selected based on leave-one-out crossvalidation for Setting A (Kronecker and two-step KRR) and Setting B (two-step KRR).\n\u2022 Zero-shot learning: Kronecker KRR with no data for the target task using Kronecker and two-step KRR. The regularization parameter is selected based on leave-one-out cross-validation for Setting A (Kronecker and two-step KRR) and Setting D (two-step KRR).\n\u2022 Almost zero-shot learning: using a varying amount of data from the target task, and all the available data from auxiliary tasks (tackled with two-step KRR). Here the missing labels for the target task are imputed in a first step and in a second step the \u2018completed\u2019 data is used to predict for the new proteins. For both steps the regularization parameter is selected based on leave-one-out cross-validation for Setting A.\nWe do not consider Kronecker KRR in the almost full cold start experiment due to computational considerations. Unlike for two-step KRR, no closed-form solution exists for the method in this setting, and the iterative conjugate gradient based method has rather poor scalability. In Figure 7, we present the results for the experiments for the two datasets. The top two plots show the experiments where all the auxiliary tasks have the data for\nall training proteins, and the amount of data available for the target task is varied. It can be seen that learning is always possible even in the full zero-shot setting, where both two-step KRR and Kronecker KRR perform much better than random. For both datasets, the independent-task approach begins to outperform the full zero-shot setting after some point when one has access to enough training proteins. Combining these two sources of information leads to the best performance for the enzyme dataset and for the drug-target affinity dataset up until 150 training proteins. In both cases, using auxiliary tasks can greatly improve performance when there are only a few labeled instances of the target task. However, once there is enough data available for the target task, there is no longer any positive transfer from the auxiliary tasks, though there also seems to be no harm from negative transfer.\nIn the second row of Figure 7 we consider a setting in which there is the same amount of data available for both the auxiliary tasks and the target tasks. This setting corresponds closely to the traditional multi-output regression problem, the exception being that only the label for the target task is of interest during testing. For the enzyme dataset, Kronecker KRR slightly outperforms two-step KKR and both perform better than independent-task KRR. For the two-step method, it does not seem to matter whether the hyperparameters were selected for Setting A or Setting B. For the second dataset, we can see that the multi-task method that uses the task correlation information fails to outperform the simple independent-task method, suggesting that on this type of data one requires significantly more data in the auxiliary tasks compared to the target tasks in order for it to be helpful for learning.\nIn the bottom row of Figure 7 we consider the full zero-shot learning setting, while increasing the amount of data available for the auxiliary tasks. For the first dataset we observe that the two-step method slightly outperforms Kronecker KRR when the hyperparameters are optimized for Setting D. For the second dataset, there is no noteworthy difference between both methods. Both approaches generalize to the unknown target task, though the results are still much worse than when having a significant amount of data for the target task.\nHere, two-step KRR shows itself to be competitive compared to Kronecker KRR. Previously, Schrynemackers et al. (2013) have, in their overview article on dyadic prediction in the biological domain, made the observation that in terms of predictive accuracy there does not seem to be a clear winner between the independent-task and multi-task type of learning approaches. Based on these experiments, a deciding factor on whether one may expect positive transfer from related tasks seems to be the amount of data available for the target task. The two-step method performs well in the almost full zero-shot settings with availability of a significant amount of auxiliary data and only very little data for the target task. But when there is enough data available for the target task, auxiliary data is no longer helpful."}, {"heading": "6.3 Online-learning of hierarchical text classification", "text": "In a final experiment we study the online learning of a large-scale hierarchical text classification problem. We will demonstrate learning with mini-batches, showing that both independent-task and two-step KRR improve when iteratively adding more training data.\nWe used the Wikipedia benchmark dataset (Partalas et al., 2015) of the Large Scale Hierarchical Text Classification Challenge4. We used the dataset provided for the third track,\n4. http://lshtc.iit.demokritos.gr/\nwhich is a subset of a larger, better curated set of another challenge. This dataset contains over 380,000 Wikipedia articles, for which the goal is to assign one of 12,633 labels, denoting the category of the article. The articles are described by a sparse bag-of-words vector with a length of 833,482. Each article can belong to only one class and the classes are part of a directed acyclic graph, representing the hierarchy of the categories (e.g. \u2018restricted Boltzmann machine\u2019 is a subcategory of \u2018artificial neural networks\u2019, which is a subcategory of \u2018machine learning\u2019). For this subchallenge, the label space has been extended with 3000 novel labels. There are 6000 new articles that are labeled according to this new scheme. Here we will study the performance for both a test set of 10,000 articles with tasks seen in the training phase (Setting B or multi-task learning) and the performance on this dataset with novel labels (Setting D or zero-shot learning) as a function of the number of training articles.\nThe bag-of-words representation was compressed by using canonical correspondence analysis to obtain 1000 orthogonal components that are maximally correlated with the training labels. We also added a dummy feature so that the model could fit an intercept. For describing the tasks, we considered two approaches:\n\u2022 Considering all the tasks independently, i.e. using independent-task KRR or, equivalently, G = I for two-step KRR.\n\u2022 Using features describing the hierarchy between the classes. We used the Dijkstra algorithm to generate all pairwise distances dij between the nodes of the label graph. A kernel was constructed by using a radial basis kernel with the distance in the\nexponent: kij = e \u2212 dij 10 .\nThe model is initially trained using 5000 instances. Subsequently, the model is iteratively updated with batches of 1000 instances using Algorithm 5.2. The regularization parameters \u03bbd and \u03bbd are chosen by minimizing the mean squared error\n5 during leave-one-out crossvalidation for Setting B (see Section 5.1).\nThe accuracy of the predictions was measured either as AUC over the instances (i.e. the capacity of the model to discriminate between a relevant and irrelevant label for a given article) and the macro AUC (i.e. the capacity of the model to give a higher score to articles of a class compared to articles that are not of that class). These learning curves are presented in Figure 8. For Setting B, for both evaluation schemes, using task features greatly improves the performance when the number of training instances is low. With more training data, both methods converge to a similar performance. Note that for the macro AUC evaluation, the learning rate is much slower compared to instance-wise AUC, implying that the former task is harder. Note that using two-step kernel ridge regression, the performance is never worse than for ridge regression. It is thus advisable to start with this method when few data points are available and update using Algorithm 1 when more labeled instances become accessible.\nFor Setting D (Figure 8), we notice a different pattern. First note that the AUC values cannot be directly compared with those from Setting B, since the test set contains less and different labels. Here, for both evaluations, the test performance has not converged yet,\n5. We did model selection on mean squared error rather than accuracy under the curve to speed up this procedure.\nimplying that more training data would be beneficial for the model. This makes sense, as assigning novel labels is much harder than known labels. Nevertheless, using two-step kernel ridge regression one can both discriminate between relevant and irrelevant labels for a given article and between relevant and irrelevant articles for a given new label. For the macro AUC evaluation, the slope of the learning curve is again quite steep, indicating that for this problem more training data is required."}, {"heading": "7. Conclusions and perspectives", "text": "In this work we have studied a new two-step kernel ridge regression method, in comparison to independent-task kernel ridge regression and Kronecker kernel ridge regression. We have shown that these methods are very related: Kronecker KRR is a special case of ordinary kernel ridge regression, two-step KRR is a special case of (Kronecker) KRR, while independent-task KRR is again a special case of two-step KRR. This unifying framework has allowed us to study the spectral interpretation for all these methods. Two-step KRR, which was found both theoretically and experimentally to work very well, has additional computational advantages. Because the model building can conceptually be decomposed into two independent steps, efficient novel hold-out tricks and algorithms for online learning can be obtained. Using the shortcuts we were experimentally able to study the learning rate of our methods. All experiments illustrate that the use of task features can significantly improve performance, but careful tuning is required. An intriguing question for further research is whether it would be useful to combine other models than ridge regression or to even mix different types of spectral filters for the two steps.\nGiven the recent surge into fields such as zero-shot learning and extreme classification, two-step KRR has potential to become a standard tool for many problems. We believe that two-step KRR, as a special case of Kronecker KRR, is particularly useful in two specific\nsituations. Firstly, when dealing with rather small-scale interaction datasets (hundreds or thousands rows and columns) with a lot of domain knowledge. Such situations are often encountered in biological applications, e.g. molecular interaction prediction or the study of species-species interactions. In these domains, kernel-based methods are already well established to encode prior knowledge. For such datasets, two-step KRR allows for fast and flexible model selection and validation, so that the researcher can easily explore what is possible with the data at hand. A second application would be in large-scale data applications. When dealing with huge output spaces, two-step KRR is a simple method to enforce prior knowledge on the outputs, while the suggested learning in mini-batches is an attractive alternative for gradient-based optimization."}, {"heading": "Acknowledgements", "text": "Part of this work was carried out using the Stevin Supercomputer Infrastructure at Ghent University, funded by Ghent University, the Hercules Foundation and the Flemish Government - department EWI."}], "references": [{"title": "A new approach to collaborative filtering: operator estimation with spectral regularization", "author": ["J. Abernethy", "F. Bach", "T. Evgeniou", "J.-P. Vert"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Incorporating side information in probabilistic matrix factorization with Gaussian processes", "author": ["R.P. Adams", "G.E. Dahl", "I. Murray"], "venue": "In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Adams et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2010}, {"title": "Kernels for vector-valued functions: a review", "author": ["M.A. Alvarez", "L. Rosasco", "N.D. Lawrence"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Alvarez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alvarez et al\\.", "year": 2012}, {"title": "A spectral regularization framework for multi-task structure learning", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil", "Y. Ying"], "venue": "In Proceedings of the 21st Annual Conference on Neural Information Processing Systems,", "citeRegEx": "Argyriou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "On spectral learning", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil", "Y. Massimiliano"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Argyriou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2010}, {"title": "Multi-output learning via spectral filtering", "author": ["L. Baldassarre", "L. Rosasco", "A. Barla", "A. Verri"], "venue": "Machine Learning,", "citeRegEx": "Baldassarre et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baldassarre et al\\.", "year": 2012}, {"title": "A joint framework for collaborative and content filtering", "author": ["J. Basilico", "T. Hofmann"], "venue": "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Basilico and Hofmann.,? \\Q2004\\E", "shortCiteRegEx": "Basilico and Hofmann.", "year": 2004}, {"title": "On regularization algorithms in learning theory", "author": ["F. Bauer", "S. Pereverzev", "L. Rosasco"], "venue": "Journal of Complexity,", "citeRegEx": "Bauer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bauer et al\\.", "year": 2007}, {"title": "Kernel methods for predicting protein-protein interactions", "author": ["A. Ben-Hur", "W. Noble"], "venue": "Suppl 1:38\u201346,", "citeRegEx": "Ben.Hur and Noble.,? \\Q2005\\E", "shortCiteRegEx": "Ben.Hur and Noble.", "year": 2005}, {"title": "Introduction to Inverse Problems in Imaging", "author": ["M. Bertero", "P. Boccacci"], "venue": "CRC Press,", "citeRegEx": "Bertero and Boccacci.,? \\Q1998\\E", "shortCiteRegEx": "Bertero and Boccacci.", "year": 1998}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Kernel multi-task learning using task-specific features", "author": ["E.V. Bonilla", "F.V. Agakov", "C.K.I. Williams"], "venue": "In Proceedings of the 11th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Bonilla et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bonilla et al\\.", "year": 2007}, {"title": "Pairwise support vector machines and their application to large scale problems", "author": ["C. Brunner", "A. Fischer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brunner and Fischer.,? \\Q2012\\E", "shortCiteRegEx": "Brunner and Fischer.", "year": 2012}, {"title": "Exact low-rank matrix completion via convex optimization", "author": ["E. Candes", "B. Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Candes and Recht.,? \\Q2008\\E", "shortCiteRegEx": "Candes and Recht.", "year": 2008}, {"title": "Comprehensive analysis of kinase inhibitor selectivity", "author": ["M.I. Davis", "J.P. Hunt", "S. Herrgard", "P. Ciceri", "L.M. Wodicka", "G. Pallares", "M. Hocker", "D.K. Treiber", "P.P. Zarrinkar"], "venue": "Nature Biotechnology,", "citeRegEx": "Davis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2011}, {"title": "Data-driven recipe completion using machine learning methods", "author": ["M. De Clercq", "M. Stock", "B. De Baets", "W. Waegeman"], "venue": "Trends in Food Science & Technology,", "citeRegEx": "Clercq et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clercq et al\\.", "year": 2015}, {"title": "On label dependence and loss minimization in multi-label classification", "author": ["K. Dembczynski", "W. Waegeman", "W. Cheng", "E. H\u00fcllermeier"], "venue": "Machine Learning,", "citeRegEx": "Dembczynski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2012}, {"title": "On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M. Mahoney"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas and Mahoney.,? \\Q2005\\E", "shortCiteRegEx": "Drineas and Mahoney.", "year": 2005}, {"title": "Matrix co-factorization for recommendation with rich side information and implicit feedback", "author": ["Y. Fang", "L. Si"], "venue": "In The 2nd International Workshop on Information Heterogeneity and Fusion in Recommender Systems,", "citeRegEx": "Fang and Si.,? \\Q2011\\E", "shortCiteRegEx": "Fang and Si.", "year": 2011}, {"title": "Concordance probability and discriminatory power in proportional hazards", "author": ["M. G\u00f6nen", "G. Heller"], "venue": "regression. Biometrika,", "citeRegEx": "G\u00f6nen and Heller.,? \\Q2005\\E", "shortCiteRegEx": "G\u00f6nen and Heller.", "year": 2005}, {"title": "A tale of two phylogenies: comparative analyses of ecological interactions", "author": ["J.D. Hadfield", "B.R. Krasnov", "R. Poulin", "S. Nakagawa"], "venue": "The American Naturalist,", "citeRegEx": "Hadfield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hadfield et al\\.", "year": 2014}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "Self-measuring similarity for multi-task Gaussian processes", "author": ["K. Hayashi", "T. Takenouchi", "R. Tomioka", "H. Kashima"], "venue": "Transactions of the Japanese Society for Artificial Intelligence,", "citeRegEx": "Hayashi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hayashi et al\\.", "year": 2012}, {"title": "Protein-ligand interaction prediction: an improved chemogenomics", "author": ["L. Jacob", "J.-P. Vert"], "venue": "approach. Bioinformatics,", "citeRegEx": "Jacob and Vert.,? \\Q2008\\E", "shortCiteRegEx": "Jacob and Vert.", "year": 2008}, {"title": "Zero shot recognition with unreliable attributes", "author": ["D. Jayaraman", "K. Grauman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jayaraman and Grauman.,? \\Q2014\\E", "shortCiteRegEx": "Jayaraman and Grauman.", "year": 2014}, {"title": "Graph-based semi-supervised learning and spectral kernel design", "author": ["R. Johnson", "T. Zhang"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Johnson and Zhang.,? \\Q2008\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2008}, {"title": "Link propagation: a fast semi-supervised learning algorithm for link prediction", "author": ["H. Kashima", "T. Kato", "Y. Yamanishi", "M. Sugiyama", "K. Tsuda"], "venue": "In Proceedings of the SIAM International Conference on Data Mining,", "citeRegEx": "Kashima et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kashima et al\\.", "year": 2009}, {"title": "Cartesian kernel: an efficient alternative to the pairwise kernel", "author": ["H. Kashima", "S. Oyama", "Y. Yamanishi", "K. Tsuda"], "venue": "IEICE Transactions on Information and Systems,", "citeRegEx": "Kashima et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kashima et al\\.", "year": 2010}, {"title": "Attribute-based classification for zeroshot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Lampert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lampert et al\\.", "year": 2014}, {"title": "Zero-data learning of new tasks", "author": ["H. Larochelle", "D. Erhan", "Y. Bengio"], "venue": "In Proceedings of the 23rd National Conference on Artificial Intelligence,", "citeRegEx": "Larochelle et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2008}, {"title": "Bipartite edge prediction via transductive learning over product graphs", "author": ["H. Liu", "Y. Yang"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Liu and Yang.,? \\Q2015\\E", "shortCiteRegEx": "Liu and Yang.", "year": 2015}, {"title": "Spectral algorithms for supervised learning", "author": ["L. Lo Gerfo", "L. Rosasco", "F. Odone", "E. De Vito", "A. Verri"], "venue": "Neural Computation,", "citeRegEx": "Gerfo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gerfo et al\\.", "year": 2008}, {"title": "Shifted Kronecker product systems", "author": ["C.D. Martin", "C.F. Van Loan"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Martin and Loan.,? \\Q2006\\E", "shortCiteRegEx": "Martin and Loan.", "year": 2006}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "author": ["R. Mazumder", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mazumder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mazumder et al\\.", "year": 2010}, {"title": "Link prediction via matrix factorization", "author": ["A. Menon", "C. Elkan"], "venue": "Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Menon and Elkan.,? \\Q2011\\E", "shortCiteRegEx": "Menon and Elkan.", "year": 2011}, {"title": "A log-linear model with latent features for dyadic prediction", "author": ["A.K. Menon", "C. Elkan"], "venue": "In Proceedings of the 10th International Conference on Data Mining,", "citeRegEx": "Menon and Elkan.,? \\Q2010\\E", "shortCiteRegEx": "Menon and Elkan.", "year": 2010}, {"title": "Using feature conjunctions across examples for learning pairwise classifiers", "author": ["S. Oyama", "C. Manning"], "venue": "Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Oyama and Manning.,? \\Q2004\\E", "shortCiteRegEx": "Oyama and Manning.", "year": 2004}, {"title": "Fast $n$-fold cross-validation for regularized least-squares", "author": ["T. Pahikkala", "J. Boberg", "T. Salakoski"], "venue": "In Proceedings of the Nineth Scandinavian Conference on Artificial Intelligence,", "citeRegEx": "Pahikkala et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pahikkala et al\\.", "year": 2006}, {"title": "Learning intransitive reciprocal relations with kernel methods", "author": ["T. Pahikkala", "W. Waegeman", "E. Tsivtsivadze", "T. Salakoski", "B. De Baets"], "venue": "European Journal of Operational Research,", "citeRegEx": "Pahikkala et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pahikkala et al\\.", "year": 2010}, {"title": "Efficient regularized least-squares algorithms for conditional ranking on relational data", "author": ["T. Pahikkala", "A. Airola", "M. Stock", "B. De Baets", "W. Waegeman"], "venue": "Machine Learning,", "citeRegEx": "Pahikkala et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pahikkala et al\\.", "year": 2013}, {"title": "A two-step learning approach for solving full and almost full cold start problems in dyadic prediction", "author": ["T. Pahikkala", "M. Stock", "A. Airola", "T. Aittokallio", "B. De Baets", "W. Waegeman"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Pahikkala et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pahikkala et al\\.", "year": 2014}, {"title": "Toward more realistic drug-target interaction predictions", "author": ["T. Pahikkala", "A. Airola", "S. Pietila", "S. Shakyawar", "A. Szwajda", "J. Tang", "T. Aittokallio"], "venue": "Briefings in Bioinformatics,", "citeRegEx": "Pahikkala et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pahikkala et al\\.", "year": 2015}, {"title": "Zero-shot learning with semantic output codes", "author": ["M. Palatucci", "G. Hinton", "D. Pomerleau", "T.M. Mitchell"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Palatucci et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Palatucci et al\\.", "year": 2009}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Pan and Yang.,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "Pairwise preference regression for cold-start recommendation", "author": ["S.-T. Park", "W. Chu"], "venue": "In Proceedings of the Third ACM Conference on Recommender Systems,", "citeRegEx": "Park and Chu.,? \\Q2009\\E", "shortCiteRegEx": "Park and Chu.", "year": 2009}, {"title": "Flaws in evaluation schemes for pair-input computational predictions", "author": ["Y. Park", "E.M. Marcotte"], "venue": "Nature Methods,", "citeRegEx": "Park and Marcotte.,? \\Q2012\\E", "shortCiteRegEx": "Park and Marcotte.", "year": 2012}, {"title": "LSHTC: a benchmark for large-scale text classification", "author": ["I. Partalas", "A. Kosmopoulos", "N. Baskiotis", "T. Artieres", "G. Paliouras", "E. Gaussier", "I. Androutsopoulos", "M.-R. Amini", "P. Galinari"], "venue": "rXivpreprint arXiv:1503.08581,", "citeRegEx": "Partalas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Partalas et al\\.", "year": 2015}, {"title": "Affinity regression predicts the recognition code of nucleic acid-binding proteins", "author": ["R. Pelossof", "I. Singh", "J.L. Yang", "M.T. Weirauch", "T.R. Hughes", "C.S. Leslie"], "venue": "Nature Biotechnology,", "citeRegEx": "Pelossof et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pelossof et al\\.", "year": 2015}, {"title": "Phylogenetic trait-based analyses of ecological networks. Ecology", "author": ["N.E. Rafferty", "A.R. Ives"], "venue": null, "citeRegEx": "Rafferty and Ives.,? \\Q2013\\E", "shortCiteRegEx": "Rafferty and Ives.", "year": 2013}, {"title": "Fast and scalable algorithms for semi-supervised link prediction on static and dynamic graphs", "author": ["R. Raymond", "H. Kashima"], "venue": "Proceedings of the European Conference on Machine learning and Knowledge Discovery in Databases,", "citeRegEx": "Raymond and Kashima.,? \\Q2010\\E", "shortCiteRegEx": "Raymond and Kashima.", "year": 2010}, {"title": "Value regularization and Fenchel duality", "author": ["R. Rifkin", "R. Lippert"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rifkin and Lippert.,? \\Q2007\\E", "shortCiteRegEx": "Rifkin and Lippert.", "year": 2007}, {"title": "Notes on regularized least squares", "author": ["R. Rifkin", "R. Lippert"], "venue": "Technical Report MIT-CSAILTR-2007-025, Massachusetts Institute of Technology,", "citeRegEx": "Rifkin and Lippert.,? \\Q2007\\E", "shortCiteRegEx": "Rifkin and Lippert.", "year": 2007}, {"title": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Rohrbach et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2011}, {"title": "An embarrassingly simple approach to zero-shot learning", "author": ["B. Romera-Paredes", "P. Torr"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Romera.Paredes and Torr.,? \\Q2015\\E", "shortCiteRegEx": "Romera.Paredes and Torr.", "year": 2015}, {"title": "On protocols and measures for the validation of supervised methods for the inference of biological networks", "author": ["M. Schrynemackers", "R. K\u00fcffner", "P. Geurts"], "venue": "Frontiers in Genetics,", "citeRegEx": "Schrynemackers et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schrynemackers et al\\.", "year": 2013}, {"title": "Classifying pairs with trees for supervised biological network inference", "author": ["M. Schrynemackers", "L. Wehenkel", "M.M. Babu", "P. Geurts"], "venue": "Molecular Biosystems,", "citeRegEx": "Schrynemackers et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schrynemackers et al\\.", "year": 2015}, {"title": "Probabilistic matrix factorization for collaborative filtering", "author": ["H. Shan", "A. Banerjee"], "venue": "In Proceedings of the 10th IEEE International Conference on Data Mining,", "citeRegEx": "Shan and Banerjee.,? \\Q2010\\E", "shortCiteRegEx": "Shan and Banerjee.", "year": 2010}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["I. Steinwart"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Steinwart.,? \\Q2002\\E", "shortCiteRegEx": "Steinwart.", "year": 2002}, {"title": "Identification of functionally related enzymes by learning-torank methods", "author": ["M. Stock", "T. Fober", "E. H\u00fcllermeier", "S. Glinca", "G. Klebe", "T. Pahikkala", "A. Airola", "B. De Baets", "W. Waegeman"], "venue": "IEEE Transactions on Computational Biology and Bioinformatics,", "citeRegEx": "Stock et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stock et al\\.", "year": 2014}, {"title": "The ubiquitous Kronecker product", "author": ["C.F. Van Loan"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "Loan.,? \\Q2000\\E", "shortCiteRegEx": "Loan.", "year": 2000}, {"title": "A new pairwise kernel for biological network inference with support vector machines", "author": ["J.-P. Vert", "J. Qiu", "W.S. Noble"], "venue": "BMC Bioinformatics,", "citeRegEx": "Vert et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vert et al\\.", "year": 2007}, {"title": "A kernelbased framework for learning graded relations from data", "author": ["W. Waegeman", "T. Pahikkala", "A. Airola", "T. Salakoski", "M. Stock", "B. De Baets"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "Waegeman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Waegeman et al\\.", "year": 2012}, {"title": "Spline Models for Observational Data", "author": ["G. Wahba"], "venue": null, "citeRegEx": "Wahba.,? \\Q1990\\E", "shortCiteRegEx": "Wahba.", "year": 1990}, {"title": "Prediction of drugtarget interaction networks from the integration of chemical and genomic", "author": ["Y. Yamanishi", "M. Araki", "A. Gutteridge", "W. Honda", "M. Kanehisa"], "venue": "spaces. Bioinformatics,", "citeRegEx": "Yamanishi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yamanishi et al\\.", "year": 2008}, {"title": "Alternating least-squares for low-rank matrix reconstruction", "author": ["D. Zachariah", "M. Sundin"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "Zachariah and Sundin.,? \\Q2012\\E", "shortCiteRegEx": "Zachariah and Sundin.", "year": 2012}, {"title": "Kernelized probabilistic matrix factorization: exploiting graphs and side information", "author": ["T. Zhou", "H. Shan", "A. Banerjee", "G. Sapiro"], "venue": "In Proceedings of the 12th SIAM International Conference on Data Mining,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 40, "context": "We first presented this method in a conference paper (Pahikkala et al., 2014).", "startOffset": 53, "endOffset": 77}, {"referenceID": 54, "context": "From a different but related perspective, Schrynemackers et al. (2015) recently also proposed a similar method for biological network inference, but here tree-based methods instead of kernel methods were used as base learners.", "startOffset": 42, "endOffset": 71}, {"referenceID": 45, "context": "For example, in a large-scale meta-study about biological network identification it was found that these concepts are vital to correctly evaluate pairwise learning models (Park and Marcotte, 2012).", "startOffset": 171, "endOffset": 196}, {"referenceID": 13, "context": "Many of these matrix completion algorithms do not incorporate side features (features of the instances and tasks) and make assumptions on the structure of the true label matrix by, for example, assuming that the completed matrix is low rank or has a low nuclear norm (Candes and Recht, 2008; Mazumder et al., 2010).", "startOffset": 267, "endOffset": 314}, {"referenceID": 33, "context": "Many of these matrix completion algorithms do not incorporate side features (features of the instances and tasks) and make assumptions on the structure of the true label matrix by, for example, assuming that the completed matrix is low rank or has a low nuclear norm (Candes and Recht, 2008; Mazumder et al., 2010).", "startOffset": 267, "endOffset": 314}, {"referenceID": 30, "context": "Recently, a framework based on bipartite graphs was proposed, which exploits the network structure for transductive link prediction (Liu and Yang, 2015).", "startOffset": 132, "endOffset": 152}, {"referenceID": 28, "context": "Larochelle et al. (2008) for a review.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "Basilico and Hofmann (2004); Abernethy et al.", "startOffset": 0, "endOffset": 28}, {"referenceID": 0, "context": "Basilico and Hofmann (2004); Abernethy et al. (2008); Menon and Elkan (2011).", "startOffset": 29, "endOffset": 53}, {"referenceID": 0, "context": "Basilico and Hofmann (2004); Abernethy et al. (2008); Menon and Elkan (2011).", "startOffset": 29, "endOffset": 77}, {"referenceID": 16, "context": "Here, most techniques encode dependency in the tasks by means of a suitable loss function or by jointly regularizing the different tasks (Dembczynski et al., 2012).", "startOffset": 137, "endOffset": 163}, {"referenceID": 43, "context": "Here as well, a large number of applicable methods is available in the literature (Pan and Yang, 2010).", "startOffset": 82, "endOffset": 102}, {"referenceID": 1, "context": "Adams et al. (2010); Fang and Si (2011); Menon and Elkan (2010); Shan and Banerjee (2010); Zhou et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Adams et al. (2010); Fang and Si (2011); Menon and Elkan (2010); Shan and Banerjee (2010); Zhou et al.", "startOffset": 0, "endOffset": 40}, {"referenceID": 1, "context": "Adams et al. (2010); Fang and Si (2011); Menon and Elkan (2010); Shan and Banerjee (2010); Zhou et al.", "startOffset": 0, "endOffset": 64}, {"referenceID": 1, "context": "Adams et al. (2010); Fang and Si (2011); Menon and Elkan (2010); Shan and Banerjee (2010); Zhou et al.", "startOffset": 0, "endOffset": 90}, {"referenceID": 1, "context": "Adams et al. (2010); Fang and Si (2011); Menon and Elkan (2010); Shan and Banerjee (2010); Zhou et al. (2012) for a non-exhaustive list.", "startOffset": 0, "endOffset": 110}, {"referenceID": 1, "context": "Adams et al. (2010); Fang and Si (2011); Menon and Elkan (2010); Shan and Banerjee (2010); Zhou et al. (2012) for a non-exhaustive list. From a bioinformatics viewpoint, Settings B and C are often analyzed using graph-based methods that take the structure of a biological network into account \u2013 see e.g. Schrynemackers et al. (2013) for a recent review.", "startOffset": 0, "endOffset": 333}, {"referenceID": 29, "context": "Larochelle et al. (2008); Park and Chu (2009); Menon and Elkan (2010); Palatucci et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 29, "context": "Larochelle et al. (2008); Park and Chu (2009); Menon and Elkan (2010); Palatucci et al.", "startOffset": 0, "endOffset": 46}, {"referenceID": 29, "context": "Larochelle et al. (2008); Park and Chu (2009); Menon and Elkan (2010); Palatucci et al.", "startOffset": 0, "endOffset": 70}, {"referenceID": 29, "context": "Larochelle et al. (2008); Park and Chu (2009); Menon and Elkan (2010); Palatucci et al. (2009); Pahikkala et al.", "startOffset": 0, "endOffset": 95}, {"referenceID": 29, "context": "Larochelle et al. (2008); Park and Chu (2009); Menon and Elkan (2010); Palatucci et al. (2009); Pahikkala et al. (2013); Rohrbach et al.", "startOffset": 0, "endOffset": 120}, {"referenceID": 29, "context": "Larochelle et al. (2008); Park and Chu (2009); Menon and Elkan (2010); Palatucci et al. (2009); Pahikkala et al. (2013); Rohrbach et al. (2011)).", "startOffset": 0, "endOffset": 144}, {"referenceID": 60, "context": "For Setting D Kronecker-based kernel methods are often employed (Vert et al., 2007; Brunner and Fischer, 2012).", "startOffset": 64, "endOffset": 110}, {"referenceID": 12, "context": "For Setting D Kronecker-based kernel methods are often employed (Vert et al., 2007; Brunner and Fischer, 2012).", "startOffset": 64, "endOffset": 110}, {"referenceID": 6, "context": "They have been successfully applied in order to solve problems such as product recommendation (Basilico and Hofmann, 2004; Park and Chu, 2009), enzyme annotation (Stock et al.", "startOffset": 94, "endOffset": 142}, {"referenceID": 44, "context": "They have been successfully applied in order to solve problems such as product recommendation (Basilico and Hofmann, 2004; Park and Chu, 2009), enzyme annotation (Stock et al.", "startOffset": 94, "endOffset": 142}, {"referenceID": 58, "context": "They have been successfully applied in order to solve problems such as product recommendation (Basilico and Hofmann, 2004; Park and Chu, 2009), enzyme annotation (Stock et al., 2014), prediction of proteinprotein (Ben-Hur and Noble, 2005; Kashima et al.", "startOffset": 162, "endOffset": 182}, {"referenceID": 8, "context": ", 2014), prediction of proteinprotein (Ben-Hur and Noble, 2005; Kashima et al., 2009) or protein-nucleic acid (Pelossof et al.", "startOffset": 38, "endOffset": 85}, {"referenceID": 26, "context": ", 2014), prediction of proteinprotein (Ben-Hur and Noble, 2005; Kashima et al., 2009) or protein-nucleic acid (Pelossof et al.", "startOffset": 38, "endOffset": 85}, {"referenceID": 47, "context": ", 2009) or protein-nucleic acid (Pelossof et al., 2015) interactions, drug design (Jacob and Vert, 2008), prediction of game outcomes (Pahikkala et al.", "startOffset": 32, "endOffset": 55}, {"referenceID": 23, "context": ", 2015) interactions, drug design (Jacob and Vert, 2008), prediction of game outcomes (Pahikkala et al.", "startOffset": 34, "endOffset": 56}, {"referenceID": 38, "context": ", 2015) interactions, drug design (Jacob and Vert, 2008), prediction of game outcomes (Pahikkala et al., 2010) and document retrieval (Pahikkala et al.", "startOffset": 86, "endOffset": 110}, {"referenceID": 39, "context": ", 2010) and document retrieval (Pahikkala et al., 2013).", "startOffset": 31, "endOffset": 55}, {"referenceID": 44, "context": "Efficient optimization approaches based on gradient descent (Park and Chu, 2009; Kashima et al., 2009) and closed-form solutions (Pahikkala et al.", "startOffset": 60, "endOffset": 102}, {"referenceID": 26, "context": "Efficient optimization approaches based on gradient descent (Park and Chu, 2009; Kashima et al., 2009) and closed-form solutions (Pahikkala et al.", "startOffset": 60, "endOffset": 102}, {"referenceID": 39, "context": ", 2009) and closed-form solutions (Pahikkala et al., 2013) have been proposed.", "startOffset": 34, "endOffset": 58}, {"referenceID": 10, "context": "The parameters for the individual tasks using KRR can be found jointly by minimizing the following objective function (Bishop, 2006): J(A) = tr[(KA \u2212Y)>(KAIT \u2212Y)] + \u03bbdtr[A > KA] , (1) with AIT = [aIT ij ] \u2208 Rm\u00d7q and K \u2208 Rm\u00d7m the Gram matrix associated with the kernel function k(\u00b7, \u00b7) for the instances.", "startOffset": 118, "endOffset": 132}, {"referenceID": 2, "context": "Several authors (see Alvarez et al. (2012); Baldassarre et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 2, "context": "Several authors (see Alvarez et al. (2012); Baldassarre et al. (2012) and references therein) have extended KRR to incorporate task correlations via matrix-valued kernels.", "startOffset": 21, "endOffset": 70}, {"referenceID": 6, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013).", "startOffset": 80, "endOffset": 246}, {"referenceID": 36, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013).", "startOffset": 80, "endOffset": 246}, {"referenceID": 8, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013).", "startOffset": 80, "endOffset": 246}, {"referenceID": 44, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013).", "startOffset": 80, "endOffset": 246}, {"referenceID": 22, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013).", "startOffset": 80, "endOffset": 246}, {"referenceID": 11, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013).", "startOffset": 80, "endOffset": 246}, {"referenceID": 39, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013).", "startOffset": 80, "endOffset": 246}, {"referenceID": 27, "context": "(Kashima et al., 2010).", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013). This kernel is defined as \u0393 ( (d, t) , ( d, t )) = k ( d,d ) g ( t, t ) (6) as a product of the data kernel k(\u00b7, \u00b7) and the task kernel g(\u00b7, \u00b7). Many other variations of pairwise kernels have been considered to incorporate prior knowledge on the nature of the relations (e.g. Vert et al. (2007); Pahikkala et al.", "startOffset": 81, "endOffset": 543}, {"referenceID": 6, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013). This kernel is defined as \u0393 ( (d, t) , ( d, t )) = k ( d,d ) g ( t, t ) (6) as a product of the data kernel k(\u00b7, \u00b7) and the task kernel g(\u00b7, \u00b7). Many other variations of pairwise kernels have been considered to incorporate prior knowledge on the nature of the relations (e.g. Vert et al. (2007); Pahikkala et al. (2010); Waegeman et al.", "startOffset": 81, "endOffset": 568}, {"referenceID": 6, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013). This kernel is defined as \u0393 ( (d, t) , ( d, t )) = k ( d,d ) g ( t, t ) (6) as a product of the data kernel k(\u00b7, \u00b7) and the task kernel g(\u00b7, \u00b7). Many other variations of pairwise kernels have been considered to incorporate prior knowledge on the nature of the relations (e.g. Vert et al. (2007); Pahikkala et al. (2010); Waegeman et al. (2012); Pahikkala et al.", "startOffset": 81, "endOffset": 592}, {"referenceID": 6, "context": "The most commonly used pairwise kernel is the Kronecker product pairwise kernel (Basilico and Hofmann, 2004; Oyama and Manning, 2004; Ben-Hur and Noble, 2005; Park and Chu, 2009; Hayashi et al., 2012; Bonilla et al., 2007; Pahikkala et al., 2013). This kernel is defined as \u0393 ( (d, t) , ( d, t )) = k ( d,d ) g ( t, t ) (6) as a product of the data kernel k(\u00b7, \u00b7) and the task kernel g(\u00b7, \u00b7). Many other variations of pairwise kernels have been considered to incorporate prior knowledge on the nature of the relations (e.g. Vert et al. (2007); Pahikkala et al. (2010); Waegeman et al. (2012); Pahikkala et al. (2013)) or for more efficient calculations in certain settings, e.", "startOffset": 81, "endOffset": 617}, {"referenceID": 21, "context": "In the statistical literature H\u0393 = \u0393 (\u0393 + \u03bbI)\u22121 is denoted as the so-called hat matrix (Hastie et al., 2001), which transforms the measurements into estimates.", "startOffset": 87, "endOffset": 108}, {"referenceID": 26, "context": "(5) is considerably large, its solutions for the Kronecker product kernel can be found efficiently via tensor algebraic optimization (Van Loan, 2000; Martin and Van Loan, 2006; Kashima et al., 2009; Raymond and Kashima, 2010; Pahikkala et al., 2013; Alvarez et al., 2012).", "startOffset": 133, "endOffset": 271}, {"referenceID": 49, "context": "(5) is considerably large, its solutions for the Kronecker product kernel can be found efficiently via tensor algebraic optimization (Van Loan, 2000; Martin and Van Loan, 2006; Kashima et al., 2009; Raymond and Kashima, 2010; Pahikkala et al., 2013; Alvarez et al., 2012).", "startOffset": 133, "endOffset": 271}, {"referenceID": 39, "context": "(5) is considerably large, its solutions for the Kronecker product kernel can be found efficiently via tensor algebraic optimization (Van Loan, 2000; Martin and Van Loan, 2006; Kashima et al., 2009; Raymond and Kashima, 2010; Pahikkala et al., 2013; Alvarez et al., 2012).", "startOffset": 133, "endOffset": 271}, {"referenceID": 2, "context": "(5) is considerably large, its solutions for the Kronecker product kernel can be found efficiently via tensor algebraic optimization (Van Loan, 2000; Martin and Van Loan, 2006; Kashima et al., 2009; Raymond and Kashima, 2010; Pahikkala et al., 2013; Alvarez et al., 2012).", "startOffset": 133, "endOffset": 271}, {"referenceID": 44, "context": "If some of the instance-task pairs in the training set are missing or if there are several occurrences of certain pairs, one has to resort, for example, to gradient-descent-based training approaches (Park and Chu, 2009; Kashima et al., 2009; Pahikkala et al., 2013).", "startOffset": 199, "endOffset": 265}, {"referenceID": 26, "context": "If some of the instance-task pairs in the training set are missing or if there are several occurrences of certain pairs, one has to resort, for example, to gradient-descent-based training approaches (Park and Chu, 2009; Kashima et al., 2009; Pahikkala et al., 2013).", "startOffset": 199, "endOffset": 265}, {"referenceID": 39, "context": "If some of the instance-task pairs in the training set are missing or if there are several occurrences of certain pairs, one has to resort, for example, to gradient-descent-based training approaches (Park and Chu, 2009; Kashima et al., 2009; Pahikkala et al., 2013).", "startOffset": 199, "endOffset": 265}, {"referenceID": 64, "context": "Superficially, this approach resembles alternating least-squares (Zachariah and Sundin, 2012), though the latter is an iteratively trained model mainly used for Setting A to obtain a low-rank representation.", "startOffset": 65, "endOffset": 93}, {"referenceID": 49, "context": "See Rifkin and Lippert (2007a); Johnson and Zhang (2008) for a more in-depth discussion.", "startOffset": 4, "endOffset": 31}, {"referenceID": 25, "context": "See Rifkin and Lippert (2007a); Johnson and Zhang (2008) for a more in-depth discussion.", "startOffset": 32, "endOffset": 57}, {"referenceID": 57, "context": "Definition 1 (Steinwart, 2002) A continuous kernel k(\u00b7, \u00b7) on a compact metric space X (i.", "startOffset": 13, "endOffset": 30}, {"referenceID": 57, "context": "The universality property indicates that the hypothesis space induced by a universal kernel can approximate any continuous function on the input space X to be learned arbitrarily well, given that the available set of training data is large and representative enough, and the learning algorithm can efficiently find this approximation from the hypothesis space (Steinwart, 2002).", "startOffset": 360, "endOffset": 377}, {"referenceID": 57, "context": "Definition 1 (Steinwart, 2002) A continuous kernel k(\u00b7, \u00b7) on a compact metric space X (i.e. X is closed and bounded) is called universal if the reproducing kernel Hilbert space (RKHS) induced by k(\u00b7, \u00b7) is dense in C(X ), where C(X ) is the space of all continuous functions f : X \u2192 R. The universality property indicates that the hypothesis space induced by a universal kernel can approximate any continuous function on the input space X to be learned arbitrarily well, given that the available set of training data is large and representative enough, and the learning algorithm can efficiently find this approximation from the hypothesis space (Steinwart, 2002). In other words, the learning algorithm is consistent in the sense that, informally put, the hypothesis learned by it gets closer to the function to be learned while the size of the training set gets larger. The consistency properties of two-step KRR are considered in more detail in Section 4.3. Next, we consider the universality of the Kronecker product pairwise kernel. The following result is a straightforward modification of some of the existing results in the literature (e.g. Waegeman et al. (2012)), but we present it here for self-sufficiency.", "startOffset": 14, "endOffset": 1173}, {"referenceID": 9, "context": "This paradigm is well studied in domains such as image analysis (Bertero and Boccacci, 1998) and, more recently, in machine learning \u2013 e.", "startOffset": 64, "endOffset": 92}, {"referenceID": 9, "context": "This paradigm is well studied in domains such as image analysis (Bertero and Boccacci, 1998) and, more recently, in machine learning \u2013 e.g. Lo Gerfo et al. (2008). Here, one wants to find the parameters \u03b1 of the data-generating process given a set of noisy measurements y such that", "startOffset": 65, "endOffset": 163}, {"referenceID": 31, "context": "Lo Gerfo et al. (2008) and references therein).", "startOffset": 3, "endOffset": 23}, {"referenceID": 3, "context": "Argyriou et al. (2007, 2010); Baldassarre et al. (2012). We will translate the pairwise learning methods from Section 3 to this spectral regularization context.", "startOffset": 0, "endOffset": 56}, {"referenceID": 6, "context": "Following Bauer et al. (2007); Lo Gerfo et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 6, "context": "Following Bauer et al. (2007); Lo Gerfo et al. (2008); Baldassarre et al.", "startOffset": 10, "endOffset": 54}, {"referenceID": 5, "context": "(2008); Baldassarre et al. (2012), we characterize the quality of a learning algorithm via its consistency properties.", "startOffset": 8, "endOffset": 34}, {"referenceID": 6, "context": "For the exact details and further elaboration, we refer to Bauer et al. (2007); Lo Gerfo et al.", "startOffset": 59, "endOffset": 79}, {"referenceID": 6, "context": "For the exact details and further elaboration, we refer to Bauer et al. (2007); Lo Gerfo et al. (2008); Baldassarre et al.", "startOffset": 59, "endOffset": 103}, {"referenceID": 5, "context": "(2008); Baldassarre et al. (2012). Theorem 6 If the filter function is admissible and the kernel function is universal, then the learning algorithm is consistent in the sense of Def.", "startOffset": 8, "endOffset": 34}, {"referenceID": 45, "context": "(Park and Marcotte, 2012; Pahikkala et al., 2015).", "startOffset": 0, "endOffset": 49}, {"referenceID": 41, "context": "(Park and Marcotte, 2012; Pahikkala et al., 2015).", "startOffset": 0, "endOffset": 49}, {"referenceID": 55, "context": "Proof This is merely a multivariate version of the leave-one-out shortcut, proven in texts such as Wahba (1990); Pahikkala et al.", "startOffset": 99, "endOffset": 112}, {"referenceID": 37, "context": "Proof This is merely a multivariate version of the leave-one-out shortcut, proven in texts such as Wahba (1990); Pahikkala et al. (2006); Rifkin and Lippert (2007b).", "startOffset": 113, "endOffset": 137}, {"referenceID": 37, "context": "Proof This is merely a multivariate version of the leave-one-out shortcut, proven in texts such as Wahba (1990); Pahikkala et al. (2006); Rifkin and Lippert (2007b).", "startOffset": 113, "endOffset": 165}, {"referenceID": 20, "context": "(2013); Hadfield et al. (2014) for applications of pairwise learning in such a setting).", "startOffset": 8, "endOffset": 31}, {"referenceID": 20, "context": "(2013); Hadfield et al. (2014) for applications of pairwise learning in such a setting). Such datasets are often plagued with false negatives or false positives, which make them difficult to analyze, see Schrynemackers et al. (2013) for a discussion.", "startOffset": 8, "endOffset": 233}, {"referenceID": 17, "context": "These can either be the primal features or be obtained from a decomposition of the kernel matrices, for example by means of the Nystr\u00f6m method (Drineas and Mahoney, 2005).", "startOffset": 143, "endOffset": 170}, {"referenceID": 10, "context": "where we made use of the matrix inversion lemma (Bishop (2006), Eq.", "startOffset": 49, "endOffset": 63}, {"referenceID": 28, "context": "We refer to Romera-Paredes and Torr (2015) for some experimental results which show that two-step KRR is a competitive method compared to established zero-shot learning methods, including DAP (Lampert et al., 2014) and ZSRwUA (Jayaraman and Grauman, 2014), for some zero-shot learning benchmark datasets.", "startOffset": 192, "endOffset": 214}, {"referenceID": 24, "context": ", 2014) and ZSRwUA (Jayaraman and Grauman, 2014), for some zero-shot learning benchmark datasets.", "startOffset": 19, "endOffset": 48}, {"referenceID": 51, "context": "We refer to Romera-Paredes and Torr (2015) for some experimental results which show that two-step KRR is a competitive method compared to established zero-shot learning methods, including DAP (Lampert et al.", "startOffset": 12, "endOffset": 43}, {"referenceID": 63, "context": "To this end, we use four drug-target classification datasets collected by Yamanishi et al. (2008)1.", "startOffset": 74, "endOffset": 98}, {"referenceID": 10, "context": "By using this relabeling, minimizing the squared loss becomes equivalent with Fisher discriminant analysis (Bishop, 2006), making our method more suitable for classification.", "startOffset": 107, "endOffset": 121}, {"referenceID": 14, "context": "We also used a different drug-target interaction prediction dataset2 (Davis et al., 2011; Pahikkala et al., 2015) consisting of 68 drug compounds and 442 protein targets.", "startOffset": 69, "endOffset": 113}, {"referenceID": 41, "context": "We also used a different drug-target interaction prediction dataset2 (Davis et al., 2011; Pahikkala et al., 2015) consisting of 68 drug compounds and 442 protein targets.", "startOffset": 69, "endOffset": 113}, {"referenceID": 19, "context": "The performances are averages over all repetitions and over all target tasks, and are measured using the concordance index (G\u00f6nen and Heller, 2005) (C-index), also known as the pairwise ranking accuracy:", "startOffset": 123, "endOffset": 147}, {"referenceID": 54, "context": "Previously, Schrynemackers et al. (2013) have, in their overview article on dyadic prediction in the biological domain, made the observation that in terms of predictive accuracy there does not seem to be a clear winner between the independent-task and multi-task type of learning approaches.", "startOffset": 12, "endOffset": 41}, {"referenceID": 46, "context": "We used the Wikipedia benchmark dataset (Partalas et al., 2015) of the Large Scale Hierarchical Text Classification Challenge4.", "startOffset": 40, "endOffset": 63}], "year": 2016, "abstractText": "Pairwise learning or dyadic prediction concerns the prediction of properties for pairs of objects. It can be seen as an umbrella covering various machine learning problems such as matrix completion, collaborative filtering, multi-task learning, transfer learning, network prediction and zero-shot learning. In this work we analyze kernel-based methods for pairwise learning, with a particular focus on a recently-suggested two-step method. We show that this method offers an appealing alternative for commonly-applied Kronecker-based methods that model dyads by means of pairwise feature representations and pairwise kernels. In a series of theoretical results, we establish correspondences between the two types of methods in terms of linear algebra and spectral filtering, and we analyze their statistical consistency. In addition, the two-step method allows us to establish novel algorithmic shortcuts for efficient training and validation on very large datasets. Putting those properties together, we believe that this simple, yet powerful method can become a standard tool for many problems. Extensive experimental results for a range of practical settings are reported.", "creator": "LaTeX with hyperref package"}}}