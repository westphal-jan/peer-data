{"id": "1606.04486", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Lifted Convex Quadratic Programming", "abstract": "symmetry represent the essential element of lifted inference that has independently outs - strated the compiler to perform immensely efficient constructions in properly - connected, but symmetric empirical models models. this raises such question, and consistency holds for optimisation problems in general. later we show that at infinitely large corpus of hardware designs this is actually the case. more precisely, can specify the concept of scaled symmetries over convex quadratic programs ( qps ), which collapses below the heart within infinite machine optimization approaches, and consider, to help, we. e., to implement flexibility. these lifted programs can then be tackled aboard the corresponding optimization toolbox ( scratch - the - page algorithms, local plane detectors, complexity method approx. ). if thus affected qp exhibits bias, then the lifted one will only be fewer suitable, whereupon hence their repetition prevents modifications that get more efficient.", "histories": [["v1", "Tue, 14 Jun 2016 18:18:58 GMT  (933kb,D)", "http://arxiv.org/abs/1606.04486v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["martin mladenov", "leonard kleinhans", "kristian kersting"], "accepted": false, "id": "1606.04486"}, "pdf": {"name": "1606.04486.pdf", "metadata": {"source": "CRF", "title": "Lifted Convex Quadratic Programming", "authors": ["Martin Mladenov", "Leonard Kleinhans"], "emails": ["martin.mladenov@cs.tu-dortmund.de", "leonard.kleinhans@tu-dortmund.de", "kristian.kersting@cs.tu-dortmund.de"], "sections": [{"heading": null, "text": "Symmetry is the essential element of lifted inference that has recently demonstrated the possibility to perform very efficient inference in highly-connected, but symmetric probabilistic models models. This raises the question, whether this holds for optimisation problems in general. Here we show that for a large class of optimisation methods this is actually the case. More precisely, we introduce the concept of fractional symmetries of convex quadratic programs (QPs), which lie at the heart of many machine learning approaches, and exploit it to lift, i.e., to compress QPs. These lifted QPs can then be tackled with the usual optimization toolbox (off-the-shelf solvers, cutting plane algorithms, stochastic gradients etc.). If the original QP exhibits symmetry, then the lifted one will generally be more compact, and hence their optimization is likely to be more efficient."}, {"heading": "1 Introduction", "text": "Convex optimization is arguably one of the main motors behind the success of machine learning as it enables learning and inference in a wide variety of statistical machine learning models, such as SVMs and LASSO, as well as efficient approximations (e.g. variational approaches, convex NMF) to hard inference tasks. The language in which convex optimization problems are specified typically includes inequalities, matrix and tensor algebra, and software packages for convex optimization such as CVXPY [Diamond et al., 2014] recreate this language as an interface between the user and the solver. Unfortunately, these algebraic languages have one shortcoming: it is difficult\u2014if not impossible\u2014for the non-expert to directly make use of the discrete, combinatorial structure often underlying convex programs; pixels depend only on neighboring pixels; quantities can flow only along specified links; the reward of placing a cup on a table does not depend on whether the window in the next room is open. Having a richer representation such as first-order logic to express the combinatorial structure and an automatic way to utilize it in the solver is likely extend the reach and efficiency of machine learning even further.\nThis is akin to statistical relational learning (SRL) that has argued in favor of first-order languages when dealing with complex graphical models, see e.g. [De Raedt et al., 2016] for a recent overview. Moreover, due to the high-level nature of the relational probabilistic languages, the low-level (ground) model they produce might often contain redundancies in terms of symmetries: \u201cindistinguishable\u201d entities of the model. Lifted probabilistic inference [Poole, 2003, De Raedt et al., 2016] approaches exploits these symmetries to perform very efficient inference in highly-connected (and hence otherwise often intractable for traditional inference approach) but symmetric models. Intuitively, one infers which variables are indistinguishable in the ground model (if possible without actually grounding) and solves the model treating the indistinguishable variables as groups instead of individuals. This\nar X\niv :1\n60 6.\n04 48\n6v 1\n[ cs\n.A I]\n1 4\nJu n\ndimensionality reduction is triggered by the knowledge of the high-level structure. Unfortunately, SRL does not support convex quadratic optimization approaches commonly used in machine learning.\nHere, we demonstrate that the core idea of SRL can be transferred to convex quadratic optimization. As our main contribution, we formalize the notion of symmetries of convex quadratic programs (QPs). Specifically, we first show that unlike for graphical models, where the notion of indistinguishability of variables is that of exact symmetry (automorphisms of the factor graph), QPs admit a weaker (partitions of indistinguishable variables which are at least as coarse) notion of indistinguishability called a fractional automorphism (FA) resp. equitable partition (EP). This implies that more general lifted inference rules for QPs can be designed. This is surprising, as it was believed that FAs apply only to linear equations. Second, we investigate geometrically how FAs of quadratic forms arise. The existing theory of symmetry in convex quadratic forms states that an automorphism of xTQx corresponds to a rotational symmetry of the semidefinite factors of Q. We generalize this in that FA of xTQx can be related not only to rotations, but also to certain scalings (as well as other not yet characterized properties of the semidefintie factors). This then results in the first approximate FA approach based on standard clustering techniques and whitening. Finally, we tackle the question to which extend kernels might preserve fractional symmetry. All this is embedded in a novel relational QP language, which is not discussed due to space limitations.\nWe proceed as follows. After reviewing prior art, we start developing automorphisms of QPs, introducing the required background on the fly. Then, we generalize this to fractional symmetries. Before concluding, we illustrate our theoretical results empirically."}, {"heading": "2 Prior Art", "text": "Several expressive modeling languages for mathematical programming have been proposed, see e.g. [Wallace and Ziemba, 2005] for a recent overview. These modeling languages are mixtures of declarative and imperative programming styles using sets of objects to index multidimensional parameters and LP variables. Recently, Diamond et al. [2014] enabled an object-oriented approach to constructing optimization problems. However, following Kabjan et al. [2009], one can still argue that there is a need for languages that not only facilitates natural algebraic modeling but also provides integrated capabilities with logic programming. This is also witnessed by the growing need for relational mathematical modeling e.g. in natural language processing [Yih and Roth, 2007, Riedel et al., 2012] and the recent push to marry statistical analytic frameworks like R and Python with relational databases [R\u00e9 et al., 2015]. The present work is the first that introduces relational convex QPs and studies their symmetries. There are symmetry-breaking branch-and-bound approaches for (mixed\u2013)integer programming [Margot, 2010] that are also featured by commercial solvers. QPs, however, do not feature branch-and-bound solvers. For the special fragment of LPs, Kersting et al. [2015] have introduced a relational language and shown how to exploit fractional symmetries. (Relaxed) graph automorphisms and variants have been explored for graph kernels [Shervashidze and Borgwardt, 2009] and (I)LP-MAP inference approaches [Bui et al., 2013, Mladenov et al., 2014, Jernite et al., 2015]. Unfortunately, their techniques or proofs do not carry over to (convex) QPs. G\u00fcler and G\u00fcrtuna [2012] and references in there have studied automorphisms but not fractional ones of convex sets. Finally, our approximate FA approach generalizes Van den Broeck and Darwiche\u2019s [2013] approach of approximating evidence in probabilistic relational models to QPs using real-valued low-rank factorizations."}, {"heading": "3 Exact Symmetries of Convex Quadratic Programs", "text": "Let us start off with exact symmetries of convex QPs. Lifting convex quadratic programs essentially amount to reducing the size a model by grouping together \u201cindistinguishable\u201d variables and constraints. In other words, they exploit symmetries. To formalize the notion of lifting more concisely let us consider a convex program, i.e., an optimization problem of the form\nx\u02da \u201c arg minxPD Jpxq , (\u2663) over x P Rn, where J : Rn \u00d1 R is a convex function, and D is a subset of Rn, typically specified as the solution a system of convex inequalities f1pxq \u010f 0, . . . , fmpxq \u010f 0. A convex quadratic program (QP) is an instance of p\u2663q where Jpxq \u201c xTQx ` cTx is a quadratic function with Q P Rn\u02c6n is symmetric and positive semi-definite, and D \u201c tx : Ax \u010f bu is a system of linear\nequations. If Q is the zero matrix, the problem is known as a linear program (LP). If we add convex quadratic constrants to a quadratic program, we obtain a quadratically constrained quadratic program (QCQP). We will not deal explicitly with QCQPs in this paper, however, by the end of our discussion of quadratic functions, it will be evident that our results can easily be extended to such programs. We shall denote a QP by the tuple QP \u201c pQ, c, A, bq. We are now interested in partitioning the variables of the program by a partition P \u201c tP1, . . . , Ppu, Pi X Pj \u201c H, \u0164\ni Pi \u201c tx1, . . . , xnu, such that there exists at least one solution that respects the partition. More formally, P is a lifting partition of p\u2663q if p\u2663q admits an optimal solution with xi \u201c xj whenever xi and xj are in the same class in P . We call the linear subspace defined by the latter condition RP . Having apriori obtained a lifting partition of the QP, we can restrict the solution space to DXRP . That is, we constrain indistinguishable variables to be equal, knowing that at least one solution will be preserved in this space of lower dimension. Since ground variables of the same class are now equal, they can be replaced with a single aggregated (lifted) variable. The resulting lifted problem has one variable per equivalence class, thus, if the lifting partition is coarse enough, significant dimensionality reduction and in turn run-time savings can be achieved. To recover a ground solution, one assigns the value of the lifted variable to every ground variable in its class.\nOne way to demonstrate that a given partition P is a lifing partition for p\u2663q is by showing that averaging any feasilbe x over the partition classes (i.e. rxi \u201c 1| classpxiq| \u0159\nxjPclasspxiq xj) yields a new feasible rx with Jprxq \u010f Jpxq. As a consequence, by averaging any optimal solution we get another optimal solution which respects P , implying that P is a lifting partition. One bit of notation that is handy in the analysis averaging operations is the partition matrix. To any partition P we can associate a matrix XP P Qn\u02c6n such that XPij \u201c 1{| classpxiq| if xj P classpxiq or 0 otherwise. With XP defined thusly, averaging x over the classes of P is equivalent to multiplying by XP , i.e., rx \u201c XPx. Partition matrices are always doubly stochastic (XP1 \u201c 1), symmetric (pXPqT \u201c XP ), and idempotent (XPXP \u201c XP ) \u2013 as a consequence also semidefinite. Example. We seek to minimize the function xTQx over x P R4, subject to x \u011b 1, with Q given in Fig. 1a. As a lifting partition, we propose P \u201c ttx1, x3u, tx2, x4uu (in the next paragraph, we will explain how one could compute this lifting partition). The corresponding parition matrix XP is also shown on Fig. 1a. Let us demonstrate that averaging over the classes of P decreases the value of the solution. For example, for x0 \u201c r2, 1, 1, 2sT , xT0 Qx0 \u201c 3. On the other hand, the class-averaged rx0 \u201c XPx0 \u201c r1.5, 1.5, 1.5, 1.5sT yields a value of 0. In fact, one could notice that any feasible x respecting the partition yields a value of 0, so any such solution is optimal. Moreover, if all coordinates of x are already greater than or equal to 1, then the same holds for rx, as averages cannot be lower than the minimum of the averaged numbers. Thus, the compressed problem reduces to finding any two numbers that greater than or equal to 1. In a sense, lifting solves this problem without having to resort to numerical optimization. l An intuitive way to find lifting partitions is via automorphism groups of convex problems. We define the automorphism group of p\u2663q, Autp\u2663q, as the group of all pairs of permutations p\u03c3, \u03c0q with permutation matrices p\u03a3,\u03a0q, such that for all x, Jpxq \u201c Jp\u03a0xq and pf1p\u03a0xq \u010f 0, . . . , fmp\u03a0xq \u010f 0q \u201c pf\u03c3p1qpxq \u010f 0, . . . , f\u03c3pmqpxq \u010f 0q. In other words, renaming the variables yields the same constraints up to reordering. For linear programs (LPs), this is equivalent to \u03a3A \u201c A\u03a0 and \u03a3b \u201c b and cT\u03a0 \u201c cT . The partition that groups together xi with xj if some \u03a0 in Autp\u2663q exchanges them is called an orbit partition. An interesting fact is that if P is an orbit partition, XP is the symmetrizer\nmatrix of Autp\u2663q, XP \u201c 1|Autp\u2663q| \u0159 p\u03a3,\u03a0qPAutp\u2663q\u03a0. One way to detect renaming symmetries is by inspection of the parameters of the problem. E.g., for a convex quadratic program pQ, c, A, bq, a set of necessary conditions for the pair of permutations p\u03a3,\u03a0q to be a renaming symmetry is: (i) \u03a0Q \u201c Q\u03a0 (equivalently \u03a0Q\u03a0T \u201c Q), (ii) cT\u03a0 \u201c cT , (iii) \u03a3A \u201c A\u03a0, and (iv) \u03a3b \u201c b. Such automorphism groups, or rather, the orbit partitions thereof, can be computed via packages such as Saucy Codenotti et al. [2013]. The reason why orbit partitions are lifting partitions of a convex problem, is that JpXPxq \u201c Jp 1|Aut | \u0159 p\u03a3,\u03a0qPAut \u03a0xq \u010f 1 |Aut | \u0159\np\u03a3,\u03a0qPAut Jp\u03a0xq \u201c Jpxq, the inequality being due to convexity of J . Recalling our example on Fig. 1a, we notice that permutations renaming row/column 1 to 3 resp. 2 to 4 are automorphisms, and our proposed P is an orbit partition. For the special case of LPs, Grohe et al. [2014] have proven that equitable partitions act as lifting partitions. An equitable partition of a square symmetric n\u02c6 nmatrixM is a partition P of 1, . . . , n, such that XP satisfies XPM \u201c MXP . For rectangular matrices, we say that a partition P of the columns is equitable, if there exists a partition of the rows Q such that XQM \u201c MXP . For LPs, we say that a partition of the variables P is equitable if there exists a partition of the constraints Q such that: cTXP \u201c cT , XQb \u201c b, and XQA \u201c AXP . Equitable partitions and their corresponding partition matrices are refered to as fractional automorphisms or fractional symmetries, as they satisfy the same conditions as automorphisms from the previous paragraph, except that XP is a doubly stochastix matrix and not a permutation matrix. Moreover, equitable partitions have an equivalent combinatorial characterization. A partition P of M P n\u02c6 n is equitable if for all i, j in the same class P and every class P 1 (including P 1 \u201c P ), we have \u0159\nkPP 1 Mik \u201c \u0159 kPP 1 Mjk. In other words, if we reorder the rows and columns of M such that indices of the same class are next to eachother, M will take on a block-rectangular form where every row (and column) of the block has the same sum. One special flavor of equitable partitions are what we will call counting partitions, where a narrower condition holds, |tk P P 1|Mik \u201c cu| \u201c |tk P P 1|Mjk \u201c cu| for all c P R, and Mii \u201c Mjj if i, j are in the same class. They partition M into blocks where each row (and column) have the same count of each number. The equitable partition of our example is such a partition. In fact, any orbit partition of a permutation group is a counting partition as well. Equitable partitions have several very attractive properties when used as lifting partitions. First, the coarsest equitable partition (as well as the coarsest counting equitable partition) of a matrix is computable in Oppe` nq logpnqq time, where e is the number of non-zeroes in the matrix, via an elegant algorithm called color refinement. Second, the coarsest equitable partition is at least as coarse as the orbit partition of a matrix, hence it offers more compression."}, {"heading": "4 Fractional Symmetry of Convex Quadratic Programs", "text": "Having developed automorphisms of convex QPs, we now move on to our main contributions. We develop FA esp. EPs of a convex QP. We start off with showing that they are lifted partitions. Then, we provide a geometric interpretation and investigate whether . kernels preserve fractional symmetries.\nEquitable Partitions of Quadratic Programs: We start be proving that the lifting partition of a convex QP captures its symmetries. Theorem 1. Let QP \u201c pQ, c, A, bq be a convex quadratic program. If P is a partition of the variables of QP , such that: (a) XPQ \u201c QXP and cTXP \u201c cT , (b) there exists a partition Q of the constraints of QP such that XQb \u201c b and XQA \u201c AXP , then P is a lifting partition for QP . Proof. We proceed along the lines drawn out in the previous section and show that for any feasible x, x1 \u201c XPx, the class-averaged x, is both feasible and Jpx1q \u010f Jpxq. Let us start with the latter. Note that both Q and XP are diagonalizeable (i.e. admit an eigendecomposition). It is known that if two diagonalizeable matrices commute (as is our starting hypothesis, XPQ \u201c QXP ), then they are also simultaneously diagonalizeable. That is, there exists an orthonormal basis of vectors u1, . . . ,un such that Q \u201c \u0159 i \u03bbiuiu T i \u201c U\u039bUT and XQ \u201c \u0159 i \u03baiuiu T i \u201c UKUT , where the \u03bbi\u2019s and \u03bai\u2019s are nonnegative scalars. Now, Jpx1q \u201c JpXPxq \u201c xT pXPqTQXPx ` cTXPx. From our discussion so far and assumption (a), this is equal to xTUKTUTU\u039bUTUKUTx ` cTx \u201c xTU\u039bK2UTx ` cTx. The key observation is that because XP is doubly stochastic, |\u03bai| \u010f 1. Hence xTUK2\u039bx \u201c \u0159\ni \u03ba 2 i\u03bb ixTuiu T i x \u010f\n\u0159\ni \u03bb ixTuiu T i x as \u03bbix Tuiu T i x is a nonnegative\nquantity. This entails Jpxq \u011b Jpx1q. Regarding feasibility, because XQ is a matrix of nonnegative numbers, Ax \u010f b implies XQAx \u010f XQb. Due to (b), this becomes AXPx \u010f b, that is, Ax1 \u010f b, demonstrating the feasibility of x1.\nWe have thus satisfied the two sufficient conditions stated in the previous section and shown that any P satisfying our assumptions is a lifting partition for QP . l Example. Recall Q from our running example on Fig. 1a. However, this time we propose P 1 \u201c tx1, x2, x3, x4u as a lifting partition with XP\n1 \u201c 14 \u00a8 14, where 14 is the 4 \u02c6 4 matrix of ones. We observe that QXP\n1 \u201c XP 1Q \u201c 04, moreover, if we introduce the constraint partition Q1 \u201c ty1, ..., y4u with partition matrix XQ\n1 \u201c XP 1 , we have that XQ1A \u201c AXP 1 and XQ1b \u201c b. According to Thm. 1 P 1 is a lifting partition of the QP in question. l There are two interesting observations to be made here. First, we have gained even further compression over our previous attempt, having a compressed problem with 1 variable instead of 2. Second, there is no automorphism of Q that could possibly exchange x1 and x2. As fractional symmetries generalize exact symmetries, it is to be expected that coarser equitable partitions than the orbit partition Q could satisfy the conditions of Thm 1. Moreover, these observations allow one to gain insight into what fractional symmetry means geometrically for a dataset. This is important as the matrix Q relates to the data we feed into the optimization problem for many QPs. For example, in the SVM dual quadratic program, the entries of Q are inner products of the feature vectors of the training examples.\nGeometry of Fractionally-Symmetric QPs: Our investigation is inspired by the characterization of automorphisms of semidefinite matrices and quadratic forms. One way to think about a semidefinite matrix Q is as the Gram matrix of a set of vectors, i.e. Q \u201c BBT where B is an n\u02c6 k matrix and k \u011b rankpQq. In this light, the quadratic form xTQx can be seen as the squared Euclidean norm of a matrix-vector product. That is, xTQx \u201c xTBBTx \u201c pBTxqT pBTxq \u201c ||BTx||2. It is a basic fact that the Euclidean norm is invariant under orthonormal transformations, that is, for any orthonormal matrix O and any vector y, ||OTy|| \u201c y as yTOOTy \u201c yTy. Thus, suppose we have a rotational autmorphism of B, i.e., a pair of orthonormal matrix O and permutation matrix \u03a0, such that \u03a0B \u201c BO or also \u03a0BOT \u201c B. That is, rotating the tuple of vectors that are the rows of B together yields same tuple back, but in different order. Observe then, that \u03a0 would be a renaming automorphism forQ, since \u03a0Q\u03a0T \u201c \u03a0BOTOB\u03a0T \u201c BBT \u201c Q, implying \u03a0Q \u201c Q\u03a0. Moreover, if the right dimension (number of columns) B is held fixed, the converse is true as well Bremner et al. [2009]. That is, not only do rotational symmetries of B correspond to renaming symmetries of Q, but vice-versa, as for fixed k, the semidefinite factors of Q are unique up to rotations.\nExample. Our Q from Fig. 1a can be factored into BBT as shown on Fig. 1b. The Figure also shows the plot of these vectors. If we were to rotate them by 180\u02dd counter-clockwise, we would get back the same set of vectors, but in the order tx3, x4, x1, x2u. The permutation matrix according to this reordering is a renaming automorphism of Q. l Using the case of automorphisms as a motivation, we now turn to fractional automorphisms. More precisely, given a doubly stochastic and idempotent matrix X , such that XQ \u201c QX , we would like to derive a similar characterization of X in terms of B. As we prove now, this is indeed possible.\nTheorem 2. Let X be a symmetric and X is idempotent (as our usual color-refinement automorphisms are) matrix, and Q \u201c BBT be a positive semidefinite matrix with B having full column rank. Then XQ \u201c QX if and only if there exists a symmetric matrix R such that XB \u201c BR. Proof. (only if direction): Suppose there exists an R such that R \u201c RT and XB \u201c BR . Then, XQ \u201c XBBT \u201c BRBT . Making use of R \u201c RT this rewrites as BRTBT \u201c BpBRqT \u201c BpXBqT \u201c BBTXT \u201c QX , as X is also symmetric. (if direction): Let XQ \u201c QX with X being idempotent and symmetric. Then, let R \u201c BTXBpBTBq\u00b41. Observe that BpBTBq\u00b41 exists and is the right pseudoinverse of BT , i.e., BTBpBTBq\u00b41 \u201c Ik, as B has full column rank. Therefore, left multiplying by Ik yields XB \u201c XBBTBpBTBq\u00b41 \u201c BBTXBpBTBq\u00b41 \u201c BR . It remains to demonstrate that R is symmetric. Recall that RTR and pRTRq\u00b41 are symmetric matrices. Then, RTR \u201c \u201c\nBTXBpBTBq\u00b41 \u2030T BTXBpBTBq\u00b41 . Since, pBTBq\u00b4T \u201c pBTBq\u00b41 and XBBT \u201c BBTX , this simplifies to pBTBq\u00b41pBTBqXXBpBTBq\u00b41. Since XX \u201c X and using Ik, this simplifies to BTXBpBTBq\u00b41 \u201c R . Hence, as RTR \u201c R, R is symmetric. l This theorem holds the key to explaining why all 4 dimensions in our example are compressed together. To see why, consider the situation on Fig 1b.\nExample. Fig 1b shows the factor B of Q (as well as a sketch of its rows). It also shows an invertible matrix M , which consists of a clockwise rotation by 45\u02dd which aligns the vectors with the axes, a rescaling of the vectors along the axes, then a further 45\u02dd. Multiplying B by this matrix yields back the same row vectors modulo a cyclic permutation, exchanging x4 with x1, x1 with x2 and so on, i.e. \u03a3B \u201c BM . Moreover BMMTBT \u2030 Q. The group of tM,M2,M3,M4u is thus a group that does not correspond to any group of automorphisms of Q, yet, the symmetrizer matrix 14 \u01594 i\u201c1M\ni is symmetric (and equal to 02), so it qualifies under the conditions of Thm. 2. l From this we can conclude that certain scaling symmetries of B do not result in symmetries of Q, but do result in fractional symmetries of Q (Thm. 2 ). On the other hand, by Thm. 1, we can also infer that these symmetries can safely be compressed out when minimizing the quadratic form xTQx. Note finally that even these symmetries do not exhaust the possible matrices of Thm. 2 \u2013 Thm. 2 allows for partitions and matrices that do not correspond to any group. Characterizing them is an exciting avenue for future work.\nUnfortunately, the (rotational) automorphism group of most Euclidean datasets consists of the identity transformation alone. This follows from the same result for convex bodies, see e.g. [G\u00fcler and G\u00fcrtuna, 2012], and is to be expected, since the symmetry properties of a given dataset B can easily be destroyed by slightly perturbing the body. To bypass this, we propose the first approximate lifting approach for Euclidean datasets. Proposition 3. Let B be an Euclidean dataset and D its corresponding pairwise distance matrix. Then Bi\u00a8 and Bj\u00a8 are in the same (rotational) orbit if an only if Bi\u00a8 and Bj\u00a8 have the same sorted distances to all other data points.\nProof. The EP of D encodes the symmetries of B. To compute it, we represent it as a colored graph C of D. We note that C is a clique with edge colors encoding distances. We turn this into a node-colored graph by assigning the same color to all nodes that have identical edge-color signatures. Runing color-refinement on this graph does not add any new color since since C is a clique. l This suggest a simple way to compute proper approximations of (rotational) EPs of B: (1, optional) Whiten the data to capture some scalings, (2) compute the pairwise distance matrixD ofB (potentially using anchor points), (3) sort each row of D, and (4) run any cluster algorithm on the sorted distance matrix. This is illustrated in Fig. 2 and should be explored further in future work.\nKernels and Equitable Partitions: Finally, we touch upon the relationship between the fractional symmetry of data vectors and kernels. Kernel functions often appear in conjunction with quadratic optimization in machine learning problems as a means of enriching the hypothesis space of a learner. From an algebraic perspective, the essence approach is to replace the entries of the semidefinite matrix Q with the values of a kernel function, which represents the inner product of data vectors under some non-linear transformation in a high dimensional space. That is, in place of Qij \u201c \u3008B\u00a8i, B\u00a8j\u3009, we use Kij \u201c kpB\u00a8i, B\u00a8jq \u201c \u3008\u03c6pBi\u00a8q, \u03c6pBj\u00a8q\u3009, where \u03c6 : Rn \u00d1 Rm is some non-linear function with m much greater than n or even infinite. Due to the prevalence of kernels, it is important to understand whether kernels preserve or destroy symmetries. Here, we will examine two popular kernels, the polynomial kernel, kPOLYpx,yq \u201c p\u3008x,y\u3009` 1qg and kRBFpx,yq \u201c expp\u00b42\u03b32||x\u00b4 y||22q, where g is a positive integer and \u03b3 is a nonzero real number. We find that in both cases, if Q \u201c BBT admits a counting equitable partition, then K will admit the same partition as well, i.e., these two kernels preserve fractional symmetry of Q up to counting (recall, that includes rotational symmetry of B): Proposition 4. Let B be a matrix whose rows are data instances. Then, if Q \u201c BBT admits a counting equitable partiton P with partition matrix XP , then both kernel matrices (a) KPOLY and (b) KRBF of this set of vectors admit the same counting partition.\nProof. Recall that an equitable partition P is a counting partition for Q if for all xi, xj in the same class P , and for every class P 1 (including P 1 \u201c P ), |txk P P 1|Qik \u201c cu| \u201c |txk P P 1|Qjk \u201c cu| for all c P R, and Qii \u201c Qjj . (a) A direct consequence of this definition is that if P is a counting partition for Q, it will be a counting partition for every other matrix whose equality pattern respects that of Q, in other words, Qij \u201c Qpq \u00f1 Kij \u201c Kpq. KPOLY has exactly this property: KPOLYij \u201c p\u3008Bi\u00a8, Bj\u00a8\u3009 ` 1qg \u201c pQij ` 1qg. It is clear that if Qij and Qpq are equal, the values of the last expression would be equal as well. (b) First, we note KRBFij \u201c expp\u00b42\u03b32||Bi\u00a8||2qexpp\u00b42\u03b32||Bj\u00a8||2qexpp\u00b4\u03b32 \u3008Bi\u00a8, Bj\u00a8\u3009q. This allows one to rewrite KRBF in terms of Q: KRBFij \u201c expp\u00b42\u03b32Qiiq expp\u00b42\u03b32Qjjq expp\u00b4\u03b32Qijq. Now, let xi, xj P P and xp, xq P P 1 such that Qip \u201c Qjq. Since Qii \u201c Qjj (by virtue of being in P ) and Qpp \u201c Qqq (by virtue of P 1), we have that KRBFip \u201c KRBFjq hence counts across classes are preserved. l To summarize, in order to lift a convex QP, we compute its quotient model w.r.t its EP as illustrated in Fig. 3. For the two popular kernels\u2014polynomial and RBF\u2014this also leads to valid liftings."}, {"heading": "5 Empirical Illustration", "text": "Our intention here is to investigate the following question (Q): Can machine learning problems potentially benefit from fractional symmetries of QPs? Generally this is to be expected e.g. for classification as if all the data points of an orbit share the same label, then this symmetry effectively lowers the VC-dimension and sample complexity of the classifier [Abu-Mostafa, 1993].\nIn a first experiment, we considered SVM classifiers for varying amounts of overlap between two classes represented by spherical Gaussians. This dataset was chosen in order to depict the potential of approximate symmetries. We trained a lifted SVM (LSVM) with 200 approximate color classes and a conventional SVM, both with RBF kernels, on 2500 training examples per class. We used a grid search together with CV for selecting \u03b3 \u201c t0.25, 0.50, 1.00, 2.00, 4.00u and C \u201c t0.5, 1.0, 2.0u. The performance was measured on an independently drawn test set of 5000 data points per class. For approximate lifting we used k-Means using the Euclidean metric and 500 anchor points. For 4 units apart class centers, the SVM achieved an error of 0.02 in 20 secs (all numbers in this experiment are averaged over 10 reruns and rounded to the second digit), while the LSVM achieved 0.02 in 1.7 secs. An SVM using just the anchor points as training set achieved an error of 0.06 in 2.1 secs. For closer class centers, namely 2 units apart, the SVM took 98 secs achieving an error of 0.16, while the LSVM achieved 0.17 in 2.1 secs. The \u201danchor\u201d SVM achieved an error of 0.16 in 2.1 secs.\nIn a second experiment, we considered a relational classification task on the Cora dataset [Sen et al., 2008]. The Cora dataset consists of 2708 scientific papers classified into seven classes. Each paper is described by a binary word vector indicating the absence/presence of a word from a dictionary of 1433 words. The citation network of the papers consisting of 5429 links. The goal is to predict the class of the paper. For simplicity, we converted this problem to a binary classification problem by taking the largest of the 7 classes as a positive class. We compared four different learners on Cora. The base classifiers are an8-norm regularized SVM (LP-SVM) [Zhou et al., 2002] and a conventional SVM (QP-SVM) [Vapnik, 1998] formulated as a convex QP. Both use the word feature vectors and do standard linear prediction (no kernel used). Additionally we considered transductive, collective versions of both of them following Kersting et al. [2015], denoted as TC-LP-SVM resp. TC-QP-SVM. Both transductive approaches have access to the citation network and implement the following simple rule: whenever we have access to an unlabaled paper i, if there is a cited or citing labeled paper j, then assume the label of j as a label of i. To account for contradicting constraint (a paper citing both\npapers of and not of its class), we introduced separate slack variables for the transductive constraints and add them to the objective with a different penalty parameter. This can easily be implemented by adding a few lines to an existing standard QP-SVM formulation as illustrated in Fig. 4a. In order to investigate the performance, we varied the amount of labeled examples available. That is, we have four cases, where we restricted the amount of labeled examples to t \u201c 20%, 40%, 60%, and 80% of size of the dataset. We first randomly split the dataset into a labeled set L and an unlabeled test set B, according to t. Then, we split L randomly in half, leaving one half for training - A, the other half becoming a validation set C. The validation set was used to select the parameters of the TC-QP-SVM in a 5-fold cross-validation fashion. That is, we split the validation set into 5 subsets Ci of equal size. On these sets we selected the parameter using a grid search for each Ci on a A Y pCzCiq labeled and B Y Ci unlabeled examples, computing the prediction error on Ci and averaging it over all Cis. We then evaluated the selected parameters on the test set B whose labels were never revealed in training. We repeated this experiment 5 times (one for each Ci) for the TC-SVMs. For consistency, we followed the same protocol with QP-SVM and LP-SVM, except that the set B Y Ci did not appeared during training as the non-transductive learners have no use for unlabeled examples. That is, we selected parameters by training on AY pCzCiq and evaluating on Ci. The selected parameters were then evaluated on the test set B. For all SVM models, we also ran a ground and a lifted version. The results are summarized in Fig. 4. The QP-SVM outperforms the LP-SVM in terms of accuracy for each setting and in turn both are ouperformed by TC-QP-SVM. While there was no appreciable symmetry in either QP-SVM or LP-SVM, TC-QP-SVM exhibited significant variable and constraint reduction: the lifted problem was reduced to up to 78% of the variables, resp., 70% of the constraints of the ground problem, while computing the same labels and in turn accuracy.\nQualitatively similar results were obtained in a final experiment on the two-moons dataset with 150 additional features, each drawn randomly from a Gaussian per example, and using the 4-nearestneighbour graph as \"citation network\".\nOverall, the results of our experiments are clear evidence for an affirmative answer to question (Q)."}, {"heading": "6 Conclusions", "text": "We have deepen the understanding of symmetries in machine learning and significantly extend the scope of lifted inference. Specifically, we have introduced and studied a precise mathematical definition of fractional symmetry of convex QPs. Using the tool of fractional automorphism, orbits of optimization variables are obtained, and lifted solvers materialize as performing the corresponding optimisation problem in the space of per-orbit optimization variables. This enables the lifting of a large class of machine learning tasks and approaches. We here instantiated this for SVMs by developing the first lifted solver for SVMs and illustrating empirically its potential. In the future,\nother ML settings should be explored. One could also deepen our theoretical results on more datasets, investigate the connection to other data reduction methods, develop approximate WL graph kernels, and move beyond convex QPs. Most significantly, our framework offers a mathematical foundation for symmetry-based machine learning [Gens and Domingos, 2014].\nAcknowledgements The authors would like to thank the anonymous reviewers for their feedback. The work was partly supported by the DFG Collaborative Research Center SFB 876, project A6."}], "references": [{"title": "Hints and the VC dimension", "author": ["Y.S. Abu-Mostafa"], "venue": "Neural Computation,", "citeRegEx": "Abu.Mostafa.,? \\Q1993\\E", "shortCiteRegEx": "Abu.Mostafa.", "year": 1993}, {"title": "Polyhedral representation conversion up to symmetries", "author": ["D. Bremner", "M. Dutour Sikri\u0107", "A. Sch\u00fcrmann"], "venue": "In Proceedings of the 2006 CRM Workshop on Polyhedral Computations. AMS, Providence,", "citeRegEx": "Bremner et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bremner et al\\.", "year": 2009}, {"title": "Automorphism groups of graphical models and lifted variational inference", "author": ["H.H. Bui", "T.N. Huynh", "S. Riedel"], "venue": "In Proc. of the 29th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Bui et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bui et al\\.", "year": 2013}, {"title": "Conflict analysis and branching heuristics in the search for graph automorphisms", "author": ["P. Codenotti", "H. Katebi", "K.A. Sakallah", "I.L. Markov"], "venue": "In Proc. of ICATI,", "citeRegEx": "Codenotti et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Codenotti et al\\.", "year": 2013}, {"title": "Statistical Relational Artificial Intelligence: Logic, Probability, and Computation", "author": ["L. De Raedt", "K. Kersting", "S. Natarajan", "D. Poole"], "venue": null, "citeRegEx": "Raedt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Raedt et al\\.", "year": 2016}, {"title": "CVXPY: A Python-embedded modeling language for convex optimization, version 0.2", "author": ["S. Diamond", "E. Chu", "S. Boyd"], "venue": "http://cvxpy.org/,", "citeRegEx": "Diamond et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Diamond et al\\.", "year": 2014}, {"title": "Deep symmetry networks", "author": ["R. Gens", "P.M. Domingos"], "venue": "In Proc. of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Gens and Domingos.,? \\Q2014\\E", "shortCiteRegEx": "Gens and Domingos.", "year": 2014}, {"title": "Dimension reduction via colour refinement", "author": ["M. Grohe", "K. Kersting", "M. Mladenov", "E. Selman"], "venue": "In Proceedings of the 22th Annual European Symposium on Algorithms (ESA),", "citeRegEx": "Grohe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Grohe et al\\.", "year": 2014}, {"title": "Symmetry of convex sets and its applications to the extremal ellipsoids of convex bodies", "author": ["O. G\u00fcler", "F. G\u00fcrtuna"], "venue": "Optimization Methods and Software,", "citeRegEx": "G\u00fcler and G\u00fcrtuna.,? \\Q2012\\E", "shortCiteRegEx": "G\u00fcler and G\u00fcrtuna.", "year": 2012}, {"title": "A fast variational approach for learning markov random field language models", "author": ["Y. Jernite", "S. Rush", "D. Sontag"], "venue": "In Proc. of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "Jernite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jernite et al\\.", "year": 2015}, {"title": "Algebraic modeling in a deductive database language", "author": ["D. Kabjan", "R. Fourer", "J. Ma"], "venue": "http://dynresmanagement. com/uploads/3/3/2/9/3329212/datalog_modeling.pdf,", "citeRegEx": "Kabjan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kabjan et al\\.", "year": 2009}, {"title": "Relational linear programming", "author": ["K. Kersting", "M. Mladenov", "P. Tokmakov"], "venue": "Artificial Intelligence Journal (AIJ), OnlineFirst,", "citeRegEx": "Kersting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kersting et al\\.", "year": 2015}, {"title": "Symmetry in integer linear programming", "author": ["F. Margot"], "venue": "Years of Integer Programming 1958-2008: From the Early Years to the State-of-the-Art,", "citeRegEx": "Margot.,? \\Q2010\\E", "shortCiteRegEx": "Margot.", "year": 2010}, {"title": "Lifted message passing as reparametrization of graphical models", "author": ["M. Mladenov", "A. Globerson", "K. Kersting"], "venue": "In Proc. of the 30th Int. Conf. on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Mladenov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mladenov et al\\.", "year": 2014}, {"title": "First-order probabilistic inference", "author": ["D. Poole"], "venue": "In Proc. of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Poole.,? \\Q2003\\E", "shortCiteRegEx": "Poole.", "year": 2003}, {"title": "Machine learning and databases: The sound of things to come or a cacophony of hype", "author": ["C. R\u00e9", "D. Agrawal", "M. Balazinska", "M.I. Cafarella", "M.I. Jordan", "T. Kraska", "R. Ramakrishnan"], "venue": "In Proc. of the ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "R\u00e9 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "R\u00e9 et al\\.", "year": 2015}, {"title": "Parse, Price and Cut\u2013Delayed Column and Row Generation for Graph Based Parsers", "author": ["S. Riedel", "D.A. Smith", "A. McCallum"], "venue": "In Proc. of the EMNLP-CoNLL,", "citeRegEx": "Riedel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2012}, {"title": "Collective classification in network data", "author": ["P. Sen", "G. Namata", "M. Bilgic", "L. Getoor", "B. Gallagher", "T. Eliassi-Rad"], "venue": "AI Magazine,", "citeRegEx": "Sen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sen et al\\.", "year": 2008}, {"title": "Fast subtree kernels on graphs", "author": ["N. Shervashidze", "K.M. Borgwardt"], "venue": "In Proc. of the 23rd Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "Shervashidze and Borgwardt.,? \\Q2009\\E", "shortCiteRegEx": "Shervashidze and Borgwardt.", "year": 2009}, {"title": "On the complexity and approximation of binary evidence in lifted inference", "author": ["G. Van den Broeck", "A. Darwiche"], "venue": "In Proc. of the 27th Annual Conf. on Neural Information Processing Systems (NIPS),", "citeRegEx": "Broeck and Darwiche.,? \\Q2013\\E", "shortCiteRegEx": "Broeck and Darwiche.", "year": 2013}, {"title": "Statistical learning theory. Adaptive and learning systems for signal processing, communications and control series", "author": ["V.N. Vapnik"], "venue": "A Wiley-Interscience Publication,", "citeRegEx": "Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik.", "year": 1998}, {"title": "Global inference for entity and relation identification via a linear programming formulation", "author": ["W.-t. Yih", "D. Roth"], "venue": null, "citeRegEx": "Yih and Roth.,? \\Q2007\\E", "shortCiteRegEx": "Yih and Roth.", "year": 2007}, {"title": "Linear programming support vector machines", "author": ["W. Zhou", "L. Zhang", "L. Jiao"], "venue": "Pattern recognition,", "citeRegEx": "Zhou et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 5, "context": "The language in which convex optimization problems are specified typically includes inequalities, matrix and tensor algebra, and software packages for convex optimization such as CVXPY [Diamond et al., 2014] recreate this language as an interface between the user and the solver.", "startOffset": 185, "endOffset": 207}, {"referenceID": 15, "context": ", 2012] and the recent push to marry statistical analytic frameworks like R and Python with relational databases [R\u00e9 et al., 2015].", "startOffset": 113, "endOffset": 130}, {"referenceID": 12, "context": "There are symmetry-breaking branch-and-bound approaches for (mixed\u2013)integer programming [Margot, 2010] that are also featured by commercial solvers.", "startOffset": 88, "endOffset": 102}, {"referenceID": 18, "context": "(Relaxed) graph automorphisms and variants have been explored for graph kernels [Shervashidze and Borgwardt, 2009] and (I)LP-MAP inference approaches [Bui et al.", "startOffset": 80, "endOffset": 114}, {"referenceID": 4, "context": "Recently, Diamond et al. [2014] enabled an object-oriented approach to constructing optimization problems.", "startOffset": 10, "endOffset": 32}, {"referenceID": 4, "context": "Recently, Diamond et al. [2014] enabled an object-oriented approach to constructing optimization problems. However, following Kabjan et al. [2009], one can still argue that there is a need for languages that not only facilitates natural algebraic modeling but also provides integrated capabilities with logic programming.", "startOffset": 10, "endOffset": 147}, {"referenceID": 4, "context": "Recently, Diamond et al. [2014] enabled an object-oriented approach to constructing optimization problems. However, following Kabjan et al. [2009], one can still argue that there is a need for languages that not only facilitates natural algebraic modeling but also provides integrated capabilities with logic programming. This is also witnessed by the growing need for relational mathematical modeling e.g. in natural language processing [Yih and Roth, 2007, Riedel et al., 2012] and the recent push to marry statistical analytic frameworks like R and Python with relational databases [R\u00e9 et al., 2015]. The present work is the first that introduces relational convex QPs and studies their symmetries. There are symmetry-breaking branch-and-bound approaches for (mixed\u2013)integer programming [Margot, 2010] that are also featured by commercial solvers. QPs, however, do not feature branch-and-bound solvers. For the special fragment of LPs, Kersting et al. [2015] have introduced a relational language and shown how to exploit fractional symmetries.", "startOffset": 10, "endOffset": 962}, {"referenceID": 2, "context": "(Relaxed) graph automorphisms and variants have been explored for graph kernels [Shervashidze and Borgwardt, 2009] and (I)LP-MAP inference approaches [Bui et al., 2013, Mladenov et al., 2014, Jernite et al., 2015]. Unfortunately, their techniques or proofs do not carry over to (convex) QPs. G\u00fcler and G\u00fcrtuna [2012] and references in there have studied automorphisms but not fractional ones of convex sets.", "startOffset": 151, "endOffset": 317}, {"referenceID": 2, "context": "(Relaxed) graph automorphisms and variants have been explored for graph kernels [Shervashidze and Borgwardt, 2009] and (I)LP-MAP inference approaches [Bui et al., 2013, Mladenov et al., 2014, Jernite et al., 2015]. Unfortunately, their techniques or proofs do not carry over to (convex) QPs. G\u00fcler and G\u00fcrtuna [2012] and references in there have studied automorphisms but not fractional ones of convex sets. Finally, our approximate FA approach generalizes Van den Broeck and Darwiche\u2019s [2013] approach of approximating evidence in probabilistic relational models to QPs using real-valued low-rank factorizations.", "startOffset": 151, "endOffset": 494}, {"referenceID": 3, "context": "Such automorphism groups, or rather, the orbit partitions thereof, can be computed via packages such as Saucy Codenotti et al. [2013]. The reason why orbit partitions are lifting partitions of a convex problem, is that JpXPxq \u201c Jp 1 |Aut | \u0159 p\u03a3,\u03a0qPAut \u03a0xq \u010f 1 |Aut | \u0159", "startOffset": 110, "endOffset": 134}, {"referenceID": 7, "context": "For the special case of LPs, Grohe et al. [2014] have proven that equitable partitions act as lifting partitions.", "startOffset": 29, "endOffset": 49}, {"referenceID": 1, "context": "Moreover, if the right dimension (number of columns) B is held fixed, the converse is true as well Bremner et al. [2009]. That is, not only do rotational symmetries of B correspond to renaming symmetries of Q, but vice-versa, as for fixed k, the semidefinite factors of Q are unique up to rotations.", "startOffset": 99, "endOffset": 121}, {"referenceID": 8, "context": "[G\u00fcler and G\u00fcrtuna, 2012], and is to be expected, since the symmetry properties of a given dataset B can easily be destroyed by slightly perturbing the body.", "startOffset": 0, "endOffset": 25}, {"referenceID": 11, "context": "[Kersting et al., 2015].", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "for classification as if all the data points of an orbit share the same label, then this symmetry effectively lowers the VC-dimension and sample complexity of the classifier [Abu-Mostafa, 1993].", "startOffset": 174, "endOffset": 193}, {"referenceID": 17, "context": "In a second experiment, we considered a relational classification task on the Cora dataset [Sen et al., 2008].", "startOffset": 91, "endOffset": 109}, {"referenceID": 22, "context": "The base classifiers are an8-norm regularized SVM (LP-SVM) [Zhou et al., 2002] and a conventional SVM (QP-SVM) [Vapnik, 1998] formulated as a convex QP.", "startOffset": 59, "endOffset": 78}, {"referenceID": 20, "context": ", 2002] and a conventional SVM (QP-SVM) [Vapnik, 1998] formulated as a convex QP.", "startOffset": 40, "endOffset": 54}, {"referenceID": 11, "context": "Additionally we considered transductive, collective versions of both of them following Kersting et al. [2015], denoted as TC-LP-SVM resp.", "startOffset": 87, "endOffset": 110}, {"referenceID": 11, "context": "linked(I1, I2) = label(I1) & query(I2) & (cite(I1, I2) | cite(I2, I1)) # query for the transductive constraint slacks = sum{I in labeled(I)} slack(I); coslacks = sum{I1, I2 in linked(I1, I2)} slack(I1,I2) # inline definitions # QUADRATIC OBJECTIVE, the main novelty compared to [Kersting et al., 2015] minimize: sum{J in feature(I,J)} weight(J)**2 + c1 * slack + c2 * coslack; subject to forall {I in labeled(I)}: labeled(I)*predict(I) >= 1 - slack(I); # push labeled examples to the correct side subject to forall {I in labeled(I)}: slack(I) >= 0; # slacks are positive # TRANSDUCTIVE PART: cited instances should have the same labels.", "startOffset": 278, "endOffset": 301}, {"referenceID": 11, "context": "(a) TC-QP-SVM encoded in a novel QP extension of the relational LP language in [Kersting et al., 2015].", "startOffset": 79, "endOffset": 102}, {"referenceID": 6, "context": "Most significantly, our framework offers a mathematical foundation for symmetry-based machine learning [Gens and Domingos, 2014].", "startOffset": 103, "endOffset": 128}], "year": 2016, "abstractText": "Symmetry is the essential element of lifted inference that has recently demonstrated the possibility to perform very efficient inference in highly-connected, but symmetric probabilistic models models. This raises the question, whether this holds for optimisation problems in general. Here we show that for a large class of optimisation methods this is actually the case. More precisely, we introduce the concept of fractional symmetries of convex quadratic programs (QPs), which lie at the heart of many machine learning approaches, and exploit it to lift, i.e., to compress QPs. These lifted QPs can then be tackled with the usual optimization toolbox (off-the-shelf solvers, cutting plane algorithms, stochastic gradients etc.). If the original QP exhibits symmetry, then the lifted one will generally be more compact, and hence their optimization is likely to be more efficient.", "creator": "LaTeX with hyperref package"}}}