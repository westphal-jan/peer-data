{"id": "1606.04506", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Max-Margin Feature Selection", "abstract": "many density simulations applications such as in dynamics, biology og social networking deal here data encoding data dimensions. zone selection involves typically employed specifically resolve spatial sequences of patterns which im - proves generalization accuracy as criterion because offset considerable computational frustration of learning the model. evaluation of the criteria used whereas zone selection is also jointly minimize time redundancy should maximize the load - vance of the spatial elements. in this paper, we formulate the strategy of phase selection examining minimum one hundred svm problem in temporal space where features correspond neither two component points and concurrently correspond to temporal dimensions. the goal is to compensate for a representative configuration of the features ( boundary vectors ) typically describes homogeneous content linking the region whichever additional rest of the features ( data points ) apply. this leads to a joint optimization of relevance and redundancy in above principled max - margin query. additionally, multi context enables approaches to leverage existing techniques for optimizing various input functions resulting in highly computationally efficient solutions influencing the mode of data selection. specifically, will employ the dual coordinate descent algorithm ( li og al., 2008 ), example seen when cad, for our formulation. we use a sparse representation therefore deal with data regarding very high dimensions. spatial surveying diverse publicly available benchmark datasets from a respective scientific domains show as our interaction results in increases of magnitude faster fitting into remotely maintaining the greater speed of accuracy compared to the state of business art spatial selection techniques.", "histories": [["v1", "Tue, 14 Jun 2016 19:05:01 GMT  (311kb)", "http://arxiv.org/abs/1606.04506v1", "submitted to PR Letters"]], "COMMENTS": "submitted to PR Letters", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["yamuna prasad", "dinesh khandelwal", "k k biswas"], "accepted": false, "id": "1606.04506"}, "pdf": {"name": "1606.04506.pdf", "metadata": {"source": "META", "title": "Max-Margin Feature Selection", "authors": ["Yamuna Prasada", "Dinesh Khandelwala", "K. K. Biswasa"], "emails": ["yprasad@cse.iitd.ac.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n04 50\n6v 1\n[ cs\n.L G\n] 1\n4 Ju\nn 20\nMany machine learning applications such as in vision, biology and social networking deal with data in high dimensions. Feature selection is typically employed to select a subset of features which improves generalization accuracy as well as reduces the computational cost of learning the model. One of the criteria used for feature selection is to jointly minimize the redundancy and maximize the relevance of the selected features. In this paper, we formulate the task of feature selection as a one class SVM problem in a space where features correspond to the data points and instances correspond to the dimensions. The goal is to look for a representative subset of the features (support vectors) which describes the boundary for the region where the set of the features (data points) exists. This leads to a joint optimization of relevance and redundancy in a principled max-margin framework. Additionally, our formulation enables us to leverage existing techniques for optimizing the SVM objective resulting in highly computationally efficient solutions for the task of feature selection. Specifically, we employ the dual coordinate descent algorithm (Hsieh et al., 2008), originally proposed for SVMs, for our formulation. We use a sparse representation to deal with data in very high dimensions. Experiments on seven publicly available benchmark datasets from a variety of domains show that our approach results in orders of magnitude faster solutions even while retaining the same level of accuracy compared to the state of the art feature selection techniques.\nc\u00a9 2016 Elsevier Ltd. All rights reserved."}, {"heading": "1. Introduction", "text": "Many machine learning problems in vision, biology, social networking and several other domains need to deal with very high dimensional data. Many of these attributes may not be relevant for the final prediction task and act as noise during the learning process. A number of feature selection methods have already been proposed in the literature to deal with this problem. These can be broadly categorized into filter based, wrapper based and embedded methods.\nIn filter based methods, features (or subset of the features) are ranked based on their statistical importance and are oblivious to the classifier being used (Guyon and Elisseeff, 2003; Peng et al., 2005). Wrapper based methods select subset of features heuristically and classification accuracy is used to estimate the goodness of the selected subset (Kumar et al., 2012). These methods typically result in good accuracy while incur\n\u2217\u2217Corresponding author: Tel.: +91-965-069-0223 e-mail: yprasad@cse.iitd.ac.in (Yamuna Prasad)\nhigh computational cost because of the need to train the classifier multiple number of times. In the embedded methods, feature selection criteria is directly incorporated in the objective function of the classifier (Tan et al., 2010; Yiteng et al., 2012). Many filter and wrapper based methods fail on very high dimensional datasets due to their high time and memory requirements, and also because of inapplicability on sparse datasets (Guyon and Elisseeff, 2003; Yiteng et al., 2012).\nIn the literature, various max-margin formulation had been developed for many applications (Burges, 1998; Guo et al., 2007). Recently, we have proposed a hard margin primal formulation for feature selection using quadratic program (QP) slover (Prasad et al., 2013). This approach jointly minimizes redundancy and maximizes relevance in a max-margin framework. We have formulated the task of feature selection as a one class SVM problem (Scho\u0308lkopf et al., 2000) in the dual space where f eatures correspond to the data points and instances correspond to the dimensions. The goal is to search for a representative subset of the features (support vectors) which describes the boundary for the region in which the set of the fea-\n2 tures (data points) lies. This is equivalent to searching for a hyperplane which maximally separates the data points from the origin (Scho\u0308lkopf et al., 2000).\nIn this paper, we have extended the hard-margin formulation to develop a general soft-margin framework for feature selection. We have also modified the primal and dual formulations. We present the dual objective as unconstrained optimization problem. We employ the Dual Coordinate Descent (DCD) algorithm (Hsieh et al., 2008) for solving our formulation. The DCD algorithm simultaneously uses the information in the primal as well as in the dual to come up with a very fast solver for the SVM objective. In order to apply DCD approach, our formulation has been appropriately modified by including an additional term in the dual objective, which can be seen as a regularizer on the feature weights. The strength of this regularizer can be tuned to control the sparsity of the selected features weights. We adapt the liblinear implementation (Fan et al., 2008) for our proposed framework so that our approach is scalable to data in very high dimensions. We also show that the Quadratic Programming Feature Selection (QPFS) (Rodriguez-Lujan et al., 2010) falls out as a special case of our formulation in the dual space when using a hard margin.\nExperiments on seven publicly available datasets from a vision, biology and Natural Language Processing (NLP) domains show that our approach results in orders of magnitude faster solutions compared to the state of the art techniques while retaining the same level of accuracy.\nThe rest of the paper is organized as follows. We describe our proposed max-margin formulation for feature selection (MMFS) including the dual coordinate descent approach in Section 2. We present our experimental evaluation in Section 4. We conclude our work in Section 5."}, {"heading": "2. Proposed Max-Margin Framework", "text": "The key objective in feature selection is to select a subset of features which are highly relevant (that is high predictive accuracy) and non-redundant (that is uncorrelated). Relevance is captured either using an explicit metric (such as the correlation between a feature and the target variable) or implicitly using the classifier accuracy on the subset of features being selected. Redundancy is captured using metrics such as correlation coefficient or mutual information. Most of the existing feature selection methods rely on a pairwise notion of similarity to capture redundancy (Peng et al., 2005; Rodriguez-Lujan et al., 2010; Yu and Liu, 2003).\nWe try to answer the question \u201dIs there a principled approach to jointly capturing the relevance as well redundancy amongst the features?\u201d. To do this, we flip around the problem and examine the space where features themselves become the first class objects. In particular, we analyze the space where \u201dfeatures\u201d represent the data points and \u201dinstances\u201d represent the dimensions. Which boundary could describe well the set of features lying in this space? Locating the desired boundary is similar to one class SVM formulation (Scho\u0308lkopf et al., 2000). This equivalently can be formulated as the problem of searching for a hyperplane which maximally separates the features (data points) from the origin in the appropriate kernel space over the\nfeatures. In order to incorporate feature relevance, we construct a set of parallel marginal hyperplanes, one hyperplane for each feature. The margin of each separating hyperplane captures the relevance of the corresponding feature. Greater the relevance, higher the margin required (a greater margin increases the chances of a feature being a support vector). Redundancy among the features is captured implicitly in our framework. The support vectors which lie on respective margin boundaries constitute the desired subset of features to be selected. This leads to a principled max-margin framework for feature selection. The proposed formulation for MMFS is presented hereafter."}, {"heading": "2.1. Formulation", "text": "Let X represent the data matrix where each row vector xiT (i \u2208 1 . . . M) denotes an instance and each column vector f j ( j \u2208 1 . . .N) denotes a feature vector. We will use \u03c6 to denote a feature map such that the dot product between the data points can be computed via a kernel k(xi, x j) = \u03c6(xi)T\u03c6(x j), which can be interpreted as the similarly of xi and x j. We will use Y to denote the vector of class labels yi\u2019s (i \u2208 1 . . . M). Based on the above notations, we present the following formulation for feature selection in the primal:\nmin w,b 1 2\nwT w + b +C N \u2211\ni=1\n\u03bei\nsubject to wT\u03c6( fi) + b \u2265 ri \u2212 \u03bei, \u03bei \u2265 0, \u2200i = 1, . . . , N; (1) where, w represents a vector normal to the separating hyperplane(s) 1, b represents the bias term and \u03bei\u2019s represent slack variables. ri captures the relevance for the ith feature. The equation of the separating hyperplane is given by wT\u03c6( fi) + b = 0 with the distance of the hyperplane from the origin being \u2212b. Note that in this formulation the objective function is similar to the one class SVM (Scho\u0308lkopf et al., 2000). However, the constraints are very much different as our formulation includes the relevance of the features (r). The choice of \u03c6 determines the kind of similarity (correlation) to be captured among the features. The set of support vectors obtained after optimizing this problem i.e. { fi | wT\u03c6( fi) + b = ri} and the margin violators { fi | \u03bei > 0} constitute the set of features to be selected. In the dual space, this translates to those features being selected for which 0 < \u03b1i \u2264 C where \u03b1i is the Lagrange multiplier for fi. We will refer to our approach as Max-Margin Feature Selection (MMFS). Note that when dealing with hard margin (no noise) case and the term involving C disappears (since this enforces \u03bei = 0,\u2200i).\nFigure 1 illustrates the intuition behind our proposed framework in the linear dot product space (with hard margin). In the figure, wT f + b = 0 represents the separating hyperplane. The distance of this hyperplane from the origin is given by \u2212b/||w||. The first term in the objective of Equation 1 tries to minimize wT w i.e. maximize 1/||w||. The second term in the objective tries to minimize b i.e. maximize \u2212b. Hence, the overall objective tries to push the plane away from the origin. The ith\n1All the separating hyperplanes are parallel to each other in our framework.\n3 dashed plane represents the margin boundary for the ith feature. The distance of this marginal hyperplane from the separating hyperplane is given by ri/||w|| where ri is the pre-computed relevance of the ith feature. Therefore, minimizing wT w in the objective also amounts to maximizing this marginal distance (ri/||w||). Hence, the objective has the dual goal of pushing the hyperplane away from the origin while maximizing the margin for each feature (weighted by its relevance)as well. The features which lie on the respective marginal planes are the support features (encircled points). The redundancy is explicitly captured in the dual formulation of this problem."}, {"heading": "2.2. Dual Formulation", "text": "In order to solve the MMFS optimization efficiently by Dual Coordinate Descent strategy, we require both the primal and dual formulations. The dual formulation for Equation 1 can be derived using the Lagrangian method. The Lagrangian function L(w, b, \u03be, \u03b1, \u03b2) can be written as:\nL(w, b, \u03be, \u03b1, \u03b2) = min w,b 1 2\nwT w + b + C N \u2211\ni=1\n\u03bei\n+\nN \u2211\ni=1\n\u03b1i(ri \u2212 \u03bei \u2212 (wT\u03c6( fi) + b)) \u2212 N \u2211\ni=1\n\u03b2i\u03bei\nWhere, \u03b1i\u2019s and \u03b2i\u2019s are the Lagrange multipliers. Now, the Lagrangian dual can be written as:\nmax \u03b1,\u03b2:\u03b1i\u22650,\u03b2i\u22650 min w,b,\u03be L(w, b, \u03be, \u03b1, \u03b2) (2)\nAt the optimality, \u2207wL, \u2202L\u2202b and \u2202L \u2202\u03bei (for all i) will be 0 i.e.\n\u2207wL = w \u2212 N \u2211\ni=1\n\u03b1i\u03c6( fi) = 0; \u2202L \u2202b = 1 \u2212\nN \u2211\ni=1\n\u03b1i = 0\n\u2202L \u2202\u03bei = C \u2212 \u03b1i \u2212 \u03b2i = 0\n(3)\nBy substituting the values from Equation (3) into Equation (2) we get:\nf (\u03b1) = max \u03b1 rT\u03b1 \u2212 1 2 \u03b1T Q\u03b1\nSubject to 0 \u2264 \u03b1i \u2264 C, i = 1, ..., M; IT\u03b1 = 1. (4)\nThis is similar to the standard SVM dual derivation (Scho\u0308lkopf et al., 2000). The only difference is that while there is a single margin in standard SVM, the number of features here dictate the number of margins . We can equivalently rewrite the dual formulation of (4) as follows:\nf (\u03b1) = min \u03b1 1 2 \u03b1T Q\u03b1 \u2212 rT\u03b1\nSubject to 0 \u2264 \u03b1i \u2264 C, i = 1, ..., M; IT\u03b1 = 1. (5)\nHere, Q is the similarity matrix whose entries are given by Qi j = k( fi, f j) where k( fi, f j) = \u03c6( fi)T\u03c6( f j) is the kernel function corresponding to the dot product in the transformed feature space. r represents the vector of feature relevance. \u03b1\u2019s are the Lagrange multipliers. Note that the first term in the objective captures the redundancy between the features and the second term captures the relevance as in the case of QPFS formulation of (Rodriguez-Lujan et al., 2010). Hence, the connection between the redundancy and the relevance becomes explicit in the dual formulation. It should be noted that the dual objective bears a close similarity to the QPFS objective. We give the detailed comparison in Section 3. We can give relative importance to redundancy and relevance by incorporating a scaling parameter \u03b8 \u2208 (0, 1) in Equation (5) as follows:\nf (\u03b1) = min \u03b1 1 2 (1 \u2212 \u03b8)\u03b1T Q\u03b1 \u2212 \u03b8rT\u03b1\nSubject to 0 \u2264 \u03b1i \u2264 C, i = 1, ..., M; I T\u03b1 = 1.\n(6)\nIn the primal formulation (Equation (1)), this can be achieved by scaling the relevance scores by \u03b81\u2212\u03b8 , that is, replacing the constraints wT\u03c6( fi) + b \u2265 ri \u2212 \u03bei by wT\u03c6( fi) + b \u2265 \u03b81\u2212\u03b8 (ri \u2212 \u03bei)."}, {"heading": "2.3. Choice of Metrics", "text": "The relevance of a feature in our framework is captured using the correlation between the feature vector and the class label vector. In our experiments, we have normalized the data as well as the target vector (class labels) so that it has zero mean and unit variance. Hence, the dot product between the feature vector and the target vector (normalized) estimates the correlation between them i.e. relevance of the ith feature can be computed as ri = |YT\u03c6( fi)|. Some other appropriate metric which captures the predictive accuracy of a feature (such as mutual information(MI)) could also be used (Peng et al., 2005).\nThe redundancy is usually captured using correlation or mutual information in feature selection tasks (Peng et al., 2005). In our framework, the dot product space (kernel) captures the similarity (redundancy) among the features and the required similarity metric can be captured by selecting the appropriate kernel. The linear kernel ( f Ti f j) represents the correlation among the features when the features are normalized to zero mean and\n4 unit variance 2. Since the value of the correlation ranges between \u22121 and 1, a degree two homogeneous polynomial kernel defined over normalized data represents the squared correlation (i.e. \u03c6( fi)T\u03c6( f j) = ( f Ti f j)\n2). The choice of this kernel is quite intuitive for feature selection as it gives equal importance to the positive and negative correlations. A Gaussian kernel can also be used to approximate the mutual information (MI) (Gretton et al., 2005) which is the key metric for non-linear redundancy measure in feature selection problems (Peng et al., 2005; Rodriguez-Lujan et al., 2010). Since the MMFS formulation very closely matches the one class SVM formulation, any of the existing algorithms for SVM optimization either in primal or dual can be used. Next, we describe the use of Dual Coordinate Descent (DCD) algorithm (Hsieh et al., 2008) to obtain a highly computationally efficient solution for our feature selection formulation."}, {"heading": "2.4. Dual Coordinate Descent for MMFS", "text": "Following equation (1), the number of variables and the number of constraints in the primal formulation are M+1 and 2N, respectively, while from equation (6), it is seen that the corresponding numbers are N and 2N+1, respectively. Solving the primal (typically by using QP solvers) may be efficient (O(M3)) in the cases when M \u226a N (Shalev-Shwartz et al., 2007). Solving the dual using QP solvers requires O(N2) space and O(N3) time. Even solving the dual using sequential minimal optimization (SMO) based methods in practice has the complexity of O(N2) (Fan et al., 2005). These high time and memory complexities limit the scalability of directly solving the primal or dual for data with a very large number of instances and features.\nIn many cases when the data already lies in a rich feature space, the performance of linear SVMs is observed to be similar to that of non-linear SVMs. In such scenarios, it may be much more efficient to train the linear SVMs directly. The dual coordinate descent methods have been well studied for solving linear SVMs using unconstrained form of the primal as well as dual formulations (Hsieh et al., 2008) who have shown that dual coordinate descent algorithm is significantly faster than many other existing algorithms for solving the SVM problem. Since our formulation very closely resembles the one class SVM formulation (with the exception of having a separate margin for each feature), we can easily adapt the Dual Coordinate Descent (DCD) algorithm for our case.\nFollowing the unconstrained formulation for the SVM objective (Hsieh et al., 2008), the MMFS objective in the primal (using a linear kernel) can be written as:\nmin w 1 2 wT w + 1 \u03b3\nb2 2 +C\nN \u2211\ni=1\n\u03be(w; fi, ri) (7)\nwhere \u03be(w; fi, ri) denotes the loss function and \u03b3 is a control parameter. Assuming standard L1 loss, \u03be(w, xi, ri) = max(ri \u2212 (wTi fi + b), 0). Note the slightly changed form of the objective\n2It is typical to normalize the data to zero mean and unit variance for feature selection.\ncompared to Equation (1) where the bias term b has been replaced by a squared term b 2\n2 . The bias term can now be handled by introducing an additional dimension:\nf \u2032i \u2190 [ fi 1/\u03b3] w \u2032 i \u2190 [wi b] (8)\nEquation (7) can then be equivalently written as:\nmin w\u2032 1 2\nw\u2032T w\u2032 + C N \u2211\ni=1\n\u03be(w\u2032, f \u2032i , ri) (9)\nThe dual of this slightly modified problem becomes:\nf (\u03b1) = min \u03b1 1 2 ( \u03b1T Q\u2032\u03b1 + \u03b3 \u2217 (IT\u03b1)2 ) \u2212 r\u2032T\u03b1\nsubject to 0 \u2264 \u03b1i \u2264 C,\u2200i; (10)\nwhere Q\u2032 is (N+1)\u00d7(N+1) matrix such that Q\u2032i j = f \u2032T i f \u2032 j . Comparing Equation (10) with Equation (6), we note that the constraint requiring IT\u03b1 = 1 is no longer needed because of the slightly changed form of the objective. In the unconstrained form of the dual, we are minimizing an additional term (IT\u03b1)2 in the objective which is nothing but the square of the L1 regularizer over the feature weights. Note that this term in the objective effectively takes care of the original constraint IT\u03b1 = 1. The parameter \u03b3 controls the strength of this regularizer and can be tuned to control the sparsity of the solution. The gradient of the objective w.r.t to \u03b1i can be computed as follows:\nGi = (Q\u2032\u03b1)i + \u03b3 N \u2211\ni=1\n\u03b1i \u2212 r \u2032 i\nUsing the fact w\u2032 = \u2211N j=1 \u03b1 j f \u2032 j (set of Equations (3)), the gradient can be further reduced as:\nGi = f T i w \u2032 + \u03b3\nN \u2211\ni=1\n\u03b1i \u2212 r \u2032 i\nWe adapt the Dual Coordinate Descent algorithm (Hsieh et al., 2008) for our MMFS problem. This algorithm works by optimizing the dual objective by computing the gradient based on the weight vector w\u2032 in the primal. This process is repeated with respect to each \u03b1i in turn and the weight vector w\u2032 is updated accordingly. This translates into optimizing a one variable quadratic function at every step and can be done very efficiently. We name this approach MMFS-DCD in the paper, henceforth."}, {"heading": "2.5. Complexity", "text": "Following (Hsieh et al., 2008), the MMFS-DCD approach obtains an \u01eb-accurate solution in O(log(1/\u01eb)) number of iterations. Time complexity of a single iteration is O(MN). Memory complexity of the DCD algorithm is O(NM). For sparse datasets, the complexities depend on N\u0304 instead of N, where N\u0304 is the average number of non-zero feature values in an instance. The details about the proof of convergence are available in (Hsieh et al., 2008).\n5"}, {"heading": "3. Relationship to Existing Filter Based Methods", "text": "Quadratic Programming Feature Selection (QPFS) (Rodriguez-Lujan et al., 2010) is a filter based feature selection method which models the feature selection problem as a quadratic program jointly minimizing redundancy and maximizing relevance. Redundancy is captured using some kind of similarity score (such as MI or correlation) amongst the features. Relevance is captured using the correlation between a feature and the target variable. One norm of the feature weight vector \u03b1 is constrained to be 1. Formally, the quadratic program can written as:\nf (\u03b1) = min \u03b1 1 2 (1 \u2212 \u03b8)\u03b1T Q\u03b1 \u2212 \u03b8rT\u03b1\nSubject to \u03b1i \u2265 0, i = 1, ..., N; IT\u03b1 = 1. (11)\nQ is an N \u00d7 N matrix representing redundancy among the features, r is an N-sized vector representing the feature relevance and \u03b1 is an N-sized vector capturing feature weights. \u03b8 \u2208 [0, 1] is a scalar which controls the relative importance of redundancy (the Q term) and the relevance (the r term). QPFS objective closely resembles the minimal-redundancy-maximalrelevance (mRMR) (Peng et al., 2005) criterion. When \u03b8 = 1, only the relevance is considered (maximum Relevance) and when \u03b8 = 0 only redundancy among the features is captured. QPFS has also been shown to outperform many existing feature selection methods including mRMR and maxRel (Rodriguez-Lujan et al., 2010).\nThe form of the QPFS formulation above is exactly similar to our dual formulation (Equation 6) for an appropriate choice of kernel (similarity) function and C = \u221e (hard margin). Hence, the QPFS objective falls out as a special case of our max-margin framework in the dual problem space when dealing with hard margin. It should be noted that Lujan et al. (Rodriguez-Lujan et al., 2010) do not give any strong justification for the particular form of the objective used, other than the fact that it makes intuitive sense and seems to work well in practice. This is unlike our case where we present a max-margin based framework for jointly optimizing relevance and redundancy. Therefore, our formulation can be seen as providing a framework for the use of the QPFS objective and generalizing it further to handle noise (soft margin). Further, since no direct connection of the QPFS objective has been established with the SVM like formulation by Lujan et al. (Rodriguez-Lujan et al., 2010), the proposed approach for solving the objective is to simply use any of the standard quadratic programming implementations. Hence, the time complexity of QPFS approach is O(N3 + MN2) and space complexity is O(N2). To deal with cubic complexity, they propose combining it with the Nystro\u0308m method which works on subsamples of the data. This can partially alleviate the problem with the computational inefficiency of QPFS but comes at the cost of significant loss in accuracy, as shown by our experiments. In our case, because of the close connection with the SVM based max-margin formulation and the ability to use the information from the primal as well as the dual, we can utilize any of the highly optimized SVM solvers (such as DCD which has time complexity linear in N).\nFurther it may be noted that while our MMFS-DCD approach can handle sparse representation of very high dimensional datasets, other feature selection methods like QPFS, FCBF, mRMR etc. cannot do so directly."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Datasets", "text": "We demonstrate our experiments on seven publicly available benchmark datasets with medium to large number of dimensions. Out of these seven datasets Leukemia, RAOA and RAC are microarray datasets (Kumar et al., 2012), MNIST is a vision dataset (Tan et al., 2010) and REAL-SIM, Webspam and Kddb are the text classification datasets from NLP domain (Chang et al., 2010; Yiteng et al., 2012). Table 1 describes the details of the datasets. The last column represents the sparsity that is average number of non-zero features per instance in the dataset."}, {"heading": "4.2. Algorithms", "text": "We compared the performance of our proposed MMFS algorithm with FCBF3 (Yu and Liu, 2003), QPFS (Rodriguez-Lujan et al., 2010) and two other embedded feature selection methods, namely, Feature Generating Machine (FGM) (Tan et al., 2010) and Group Discovery Machine (GDM) (Yiteng et al., 2012). FGM uses cutting plane strategy for feature selection. GDM further tries to minimize the redundancy in FGM by incorporating the correlation among the features. QPFS, FGM and GDM have been shown to outperform a variety of existing feature selection methods including mRMR and MaxRel (Peng et al., 2005), FCBF (Yu and Liu, 2003), SVM-RFE (Guyon and Elisseeff, 2003), etc. For QPFS, we used mutual information (MI) as the similarity metric as it has been shown to give the best set of results (Rodriguez-Lujan et al., 2010). In MMFS-DCD, we use correlation of a feature vector with the target class vector to compute the feature relevance.\n3http://www.public.asu.edu/h\u0303uanliu/FCBF/FCBFsoftware.html\n6"}, {"heading": "4.3. Methodology", "text": "We compare all the approaches for feature selection in terms of their accuracy and execution time on each of the datasets. For all the datasets except Webspam and Kddb, we report the accuracies obtained at varying number of top K features (K = {2, 3, 4,. . . , 100}) selected for each of the methods. For webspam and kddb datasets,we report the accuracies obtained at varying number of top K features (K ={5, 10, 20, 30,. . . , 200}) selected by FGM, GDM and MMFS-DCD methods.\nWe also report the best accuracies obtained at any given value of K in the above range for all the datasets. We have normalized all the datasets except webspam and kddb to zero mean and unit variance. The zero mean and unit variance normalization for webspam and kddb datasets is very memory inefficient (very large memory (> 100GB)) as these two are very large sparse datasets. We have normalized these two datasets with unit variance (Yiteng et al., 2012). In the microarray datasets, the number of samples are small so we report the leave-one-out cross-validation (LOOCV) accuracy. For MNIST and REAL-SIM datasets, training and testing splits are provided in (Chang et al., 2010). We have followed the training and testing splits of (Yiteng et al., 2012) for webspam and kddb datasets. The results reported are averaged over 10 random splits.\nFor MMFS-DCD, \u03b3 parameter was tuned separately for each of the microarray datasets. The values of the parameters C and \u03b8 were set to 1 and 0.5 respectively in all the experiments. We used the default settings of the parameters for both FGM and GDM as reported in (Tan et al., 2010; Yiteng et al., 2012). After the top K features are selected, we used L2-regularized L2loss SVM (Fan et al., 2008) with default settings (that is cost parameter C=1) for classification for each of the algorithms and for each of the datasets. MMFS was implemented on top of the liblinear tool4. This implementation uses shrinking strategy (Hsieh et al., 2008). We used the publicly available implementation of QPFS (Rodriguez-Lujan et al., 2010). For FGM, we used the publicly available tool5. GDM was implemented as an extension of the FGM based on the details given in Yiteng et. al (Yiteng et al., 2012). Any additional required wrapper code was written in C/C++. All the experiments were run on a Intel CoreTM i7 3.10GHz machine with 16GB RAM under linux operating system."}, {"heading": "4.4. Results", "text": ""}, {"heading": "4.4.1. Accuracy", "text": "Table 2 presents the best set of average accuracies (varying the number of top-K features selected) for all the methods. QPFS method did not produce any results on RAOA and RAC dataset within 24 hours6. So, we used Nystro\u0308m approximation (Rodriguez-Lujan et al., 2010) with sampling rate(\u03c1=0.01) for these datasets. In the Figure 2(a), QPFS-N represents the QPFS with Nystro\u0308m approximation. The QPFS and FCBF methods\n4http://www.csie.ntu.edu.tw/ cjlin/liblinear 5http://www.c2i.ntu.edu.sg/mingkui/FGM.htm 6We put a dash \u2212 with corresponding entries in the Table 2.\ncan not handle the sparse data, so we compare FGM, GDM and MMFS-DCD for webspam and kddb datasets. MMFS-DCD reaches the best accuracy on a small number of top K features for all the microarray datasets. Further, MMFS-DCD produces significantly better accuracies compared to FCBF, QPFS, FGM and GDM on all the microarray datasets (FGM does equally well on RAC). On MNIST and webspam datasets, MMFS-DCD is marginally worse than the best performing algorithm. The plots for the average accuracies obtained as we vary the number of top K features are available in the supplementary file. Clearly, for most of the datasets, MMFS-DCD is able to achieve the best set of accuracies at early stages of feature selection compared to all algorithms. Further, the gene ontology and biological significance of top selected genes for leukemia dataset is provided in the supplementary file."}, {"heading": "4.4.2. Time", "text": "Figure 2 plots the average execution time for each of the methods. y-axis is plotted on a log scale. The time requirement for MMFS-DCD, FCBF and QPFS is independent of the number of features selected. For FGM and GDM, time requirement monotonically increases with K. For GDM, there is a sharp increase in the time required when K becomes greater than five7. It is obvious from Figure 2 that MMFS-DCD is upto several orders of magnitude faster than all the other algorithms on all the datasets8."}, {"heading": "4.4.3. Parameter Sensitivity Analysis", "text": "Figure 3 presents the variation in accuracy for MMFS-DCD on the Leukemia dataset, as we vary the regularizer parameter (\u03b3) with varying number of top k features. The accuracy is not very sensitive to \u03b3 as demonstrated by a large flat region in the graph."}, {"heading": "5. Conclusion and Future Work", "text": "We have presented a novel Max-Margin framework for Feature Selection (MMFS) similar to one class SVM formulation. Our framework provides a principled approach to jointly maximize relevance and minimize redundancy. It also enables us\n7For RAC, we run GDM upto 20 iterations. 8 Plots for remaining datasets are available in supplementary file.\n7\nto use existing SVM based optimization techniques leading to highly efficient solutions for the task of feature selection. Our experiments show that MMFS with dual coordinate decent approach is many orders of magnitude faster than existing state of the art techniques while retaining the same level of accuracy.\nOne of the key future directions includes exploring if there is some notion of a generalization bound for the task of feature selection in our framework as in the case of SVMs for the task of classification. In other words, what can we say about the quality of the features selected as we see more and more data. We would also like to explore the performance of our model with non-linear kernels. Lastly, exploring the trade-off as we vary the noise penalty would also be a direction to pursue in the future."}, {"heading": "Acknowledgment", "text": "The authors would like to thank Dr. Parag Singla, Dept. of CSE, I.I.T Delhi for his valuable suggestions and support in improving the paper."}], "references": [{"title": "A tutorial on support vector machines for pattern recognition", "author": ["C.J.C. Burges"], "venue": "Data Min. Knowl. Discov", "citeRegEx": "Burges,? \\Q1998\\E", "shortCiteRegEx": "Burges", "year": 1998}, {"title": "Training and testing low-degree polynomial data mappings via linear svm", "author": ["Y.W. Chang", "C.J. Hsieh", "K.W. Chang", "M. Ringgaard", "C.J. Lin"], "venue": "J. Mach. Learn. Res", "citeRegEx": "Chang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2010}, {"title": "Liblinear: A library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "J. Mach. Learn. Res", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Working set selection using second order information for training support vector machines", "author": ["R.E. Fan", "P.H. Chen", "C.J. Lin"], "venue": "J. Mach. Learn. Res", "citeRegEx": "Fan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2005}, {"title": "Kernel methods for measuring independence", "author": ["A. Gretton", "R. Herbrich", "A. Smola", "O. Bousquet", "B. Sch\u00f6lkopf"], "venue": "J. Mach. Learn. Res", "citeRegEx": "Gretton et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2005}, {"title": "A max margin framework on image annotation and multimodal image retrieval", "author": ["Z. Guo", "Z. Zhang", "E.P. Xing", "C. Faloutsos"], "venue": "in: ICME,", "citeRegEx": "Guo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2007}, {"title": "An intoduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Guyon and Elisseeff,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff", "year": 2003}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["C.J. Hsieh", "K.W. Chang", "C.J. Lin", "S.S. Keerthi", "S. Sundararajan"], "venue": "in: Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "Design of fuzzy expert system for microarray data classification using a novel genetic swarm algorithm", "author": ["P.G. Kumar", "A.T.A. Victoire", "P. Renukadevi", "D. Devaraj"], "venue": "Expert Syst. Appl", "citeRegEx": "Kumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2012}, {"title": "Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence", "citeRegEx": "Peng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2005}, {"title": "Feature selection using one class svm: A new perspective, in: MLCB", "author": ["Y. Prasad", "K. Biswas", "P. Singla"], "venue": null, "citeRegEx": "Prasad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2013}, {"title": "Quadratic programming feature selection", "author": ["I. Rodriguez-Lujan", "R. Huerta", "C. Elkan", "C.S. Cruz"], "venue": "J. Mach. Learn. Res", "citeRegEx": "Rodriguez.Lujan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rodriguez.Lujan et al\\.", "year": 2010}, {"title": "Support vector method for novelty detection", "author": ["B. Sch\u00f6lkopf", "R.C. Williamson", "A.J. Smola", "J. Shawe-Taylor", "J. Platt"], "venue": "Advances in neural information processing systems", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2000}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "in: Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Learning sparse SVM for feature selection on very high dimensional datasets", "author": ["M. Tan", "L. Wang", "I.W. Tsang"], "venue": "in: Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Tan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2010}, {"title": "Discovering support and affiliated features from very high dimensions", "author": ["Z. Yiteng", "T. Mingkui", "O. Yew S", "T. Ivor W"], "venue": "in: Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Yiteng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yiteng et al\\.", "year": 2012}, {"title": "Feature selection for high-dimensional data: A fast", "author": ["L. Yu", "H. Liu"], "venue": null, "citeRegEx": "Yu and Liu,? \\Q2003\\E", "shortCiteRegEx": "Yu and Liu", "year": 2003}], "referenceMentions": [{"referenceID": 7, "context": "Specifically, we employ the dual coordinate descent algorithm (Hsieh et al., 2008), originally proposed for SVMs, for our formulation.", "startOffset": 62, "endOffset": 82}, {"referenceID": 6, "context": "In filter based methods, features (or subset of the features) are ranked based on their statistical importance and are oblivious to the classifier being used (Guyon and Elisseeff, 2003; Peng et al., 2005).", "startOffset": 158, "endOffset": 204}, {"referenceID": 9, "context": "In filter based methods, features (or subset of the features) are ranked based on their statistical importance and are oblivious to the classifier being used (Guyon and Elisseeff, 2003; Peng et al., 2005).", "startOffset": 158, "endOffset": 204}, {"referenceID": 8, "context": "Wrapper based methods select subset of features heuristically and classification accuracy is used to estimate the goodness of the selected subset (Kumar et al., 2012).", "startOffset": 146, "endOffset": 166}, {"referenceID": 14, "context": "In the embedded methods, feature selection criteria is directly incorporated in the objective function of the classifier (Tan et al., 2010; Yiteng et al., 2012).", "startOffset": 121, "endOffset": 160}, {"referenceID": 15, "context": "In the embedded methods, feature selection criteria is directly incorporated in the objective function of the classifier (Tan et al., 2010; Yiteng et al., 2012).", "startOffset": 121, "endOffset": 160}, {"referenceID": 6, "context": "Many filter and wrapper based methods fail on very high dimensional datasets due to their high time and memory requirements, and also because of inapplicability on sparse datasets (Guyon and Elisseeff, 2003; Yiteng et al., 2012).", "startOffset": 180, "endOffset": 228}, {"referenceID": 15, "context": "Many filter and wrapper based methods fail on very high dimensional datasets due to their high time and memory requirements, and also because of inapplicability on sparse datasets (Guyon and Elisseeff, 2003; Yiteng et al., 2012).", "startOffset": 180, "endOffset": 228}, {"referenceID": 0, "context": "In the literature, various max-margin formulation had been developed for many applications (Burges, 1998; Guo et al., 2007).", "startOffset": 91, "endOffset": 123}, {"referenceID": 5, "context": "In the literature, various max-margin formulation had been developed for many applications (Burges, 1998; Guo et al., 2007).", "startOffset": 91, "endOffset": 123}, {"referenceID": 10, "context": "Recently, we have proposed a hard margin primal formulation for feature selection using quadratic program (QP) slover (Prasad et al., 2013).", "startOffset": 118, "endOffset": 139}, {"referenceID": 12, "context": "We have formulated the task of feature selection as a one class SVM problem (Sch\u00f6lkopf et al., 2000) in the dual space where f eatures correspond to the data points and instances correspond to the dimensions.", "startOffset": 76, "endOffset": 100}, {"referenceID": 12, "context": "This is equivalent to searching for a hyperplane which maximally separates the data points from the origin (Sch\u00f6lkopf et al., 2000).", "startOffset": 107, "endOffset": 131}, {"referenceID": 7, "context": "We employ the Dual Coordinate Descent (DCD) algorithm (Hsieh et al., 2008) for solving our formulation.", "startOffset": 54, "endOffset": 74}, {"referenceID": 2, "context": "We adapt the liblinear implementation (Fan et al., 2008) for our proposed framework so that our approach is scalable to data in very high dimensions.", "startOffset": 38, "endOffset": 56}, {"referenceID": 11, "context": "We also show that the Quadratic Programming Feature Selection (QPFS) (Rodriguez-Lujan et al., 2010) falls out as a special case of our formulation in the dual space when using a hard margin.", "startOffset": 69, "endOffset": 99}, {"referenceID": 9, "context": "Most of the existing feature selection methods rely on a pairwise notion of similarity to capture redundancy (Peng et al., 2005; Rodriguez-Lujan et al., 2010; Yu and Liu, 2003).", "startOffset": 109, "endOffset": 176}, {"referenceID": 11, "context": "Most of the existing feature selection methods rely on a pairwise notion of similarity to capture redundancy (Peng et al., 2005; Rodriguez-Lujan et al., 2010; Yu and Liu, 2003).", "startOffset": 109, "endOffset": 176}, {"referenceID": 16, "context": "Most of the existing feature selection methods rely on a pairwise notion of similarity to capture redundancy (Peng et al., 2005; Rodriguez-Lujan et al., 2010; Yu and Liu, 2003).", "startOffset": 109, "endOffset": 176}, {"referenceID": 12, "context": "Which boundary could describe well the set of features lying in this space? Locating the desired boundary is similar to one class SVM formulation (Sch\u00f6lkopf et al., 2000).", "startOffset": 146, "endOffset": 170}, {"referenceID": 12, "context": "Note that in this formulation the objective function is similar to the one class SVM (Sch\u00f6lkopf et al., 2000).", "startOffset": 85, "endOffset": 109}, {"referenceID": 12, "context": "This is similar to the standard SVM dual derivation (Sch\u00f6lkopf et al., 2000).", "startOffset": 52, "endOffset": 76}, {"referenceID": 11, "context": "Note that the first term in the objective captures the redundancy between the features and the second term captures the relevance as in the case of QPFS formulation of (Rodriguez-Lujan et al., 2010).", "startOffset": 168, "endOffset": 198}, {"referenceID": 9, "context": "Some other appropriate metric which captures the predictive accuracy of a feature (such as mutual information(MI)) could also be used (Peng et al., 2005).", "startOffset": 134, "endOffset": 153}, {"referenceID": 9, "context": "The redundancy is usually captured using correlation or mutual information in feature selection tasks (Peng et al., 2005).", "startOffset": 102, "endOffset": 121}, {"referenceID": 4, "context": "A Gaussian kernel can also be used to approximate the mutual information (MI) (Gretton et al., 2005) which is the key metric for non-linear redundancy measure in feature selection problems (Peng et al.", "startOffset": 78, "endOffset": 100}, {"referenceID": 9, "context": ", 2005) which is the key metric for non-linear redundancy measure in feature selection problems (Peng et al., 2005; Rodriguez-Lujan et al., 2010).", "startOffset": 96, "endOffset": 145}, {"referenceID": 11, "context": ", 2005) which is the key metric for non-linear redundancy measure in feature selection problems (Peng et al., 2005; Rodriguez-Lujan et al., 2010).", "startOffset": 96, "endOffset": 145}, {"referenceID": 7, "context": "Next, we describe the use of Dual Coordinate Descent (DCD) algorithm (Hsieh et al., 2008) to obtain a highly computationally efficient solution for our feature selection formulation.", "startOffset": 69, "endOffset": 89}, {"referenceID": 13, "context": "Solving the primal (typically by using QP solvers) may be efficient (O(M3)) in the cases when M \u226a N (Shalev-Shwartz et al., 2007).", "startOffset": 100, "endOffset": 129}, {"referenceID": 3, "context": "Even solving the dual using sequential minimal optimization (SMO) based methods in practice has the complexity of O(N2) (Fan et al., 2005).", "startOffset": 120, "endOffset": 138}, {"referenceID": 7, "context": "The dual coordinate descent methods have been well studied for solving linear SVMs using unconstrained form of the primal as well as dual formulations (Hsieh et al., 2008) who have shown that dual coordinate descent algorithm is significantly faster than many other existing algorithms for solving the SVM problem.", "startOffset": 151, "endOffset": 171}, {"referenceID": 7, "context": "Following the unconstrained formulation for the SVM objective (Hsieh et al., 2008), the MMFS objective in the primal (using a linear kernel) can be written as:", "startOffset": 62, "endOffset": 82}, {"referenceID": 7, "context": "We adapt the Dual Coordinate Descent algorithm (Hsieh et al., 2008) for our MMFS problem.", "startOffset": 47, "endOffset": 67}, {"referenceID": 7, "context": "Following (Hsieh et al., 2008), the MMFS-DCD approach obtains an \u01eb-accurate solution in O(log(1/\u01eb)) number of iterations.", "startOffset": 10, "endOffset": 30}, {"referenceID": 7, "context": "The details about the proof of convergence are available in (Hsieh et al., 2008).", "startOffset": 60, "endOffset": 80}, {"referenceID": 11, "context": "Quadratic Programming Feature Selection (QPFS) (Rodriguez-Lujan et al., 2010) is a filter based feature selection method which models the feature selection problem as a quadratic program jointly minimizing redundancy and maximizing relevance.", "startOffset": 47, "endOffset": 77}, {"referenceID": 9, "context": "QPFS objective closely resembles the minimal-redundancy-maximalrelevance (mRMR) (Peng et al., 2005) criterion.", "startOffset": 80, "endOffset": 99}, {"referenceID": 11, "context": "QPFS has also been shown to outperform many existing feature selection methods including mRMR and maxRel (Rodriguez-Lujan et al., 2010).", "startOffset": 105, "endOffset": 135}, {"referenceID": 11, "context": "(Rodriguez-Lujan et al., 2010) do not give any strong justification for the particular form of the objective used, other than the fact that it makes intuitive sense and seems to work well in practice.", "startOffset": 0, "endOffset": 30}, {"referenceID": 11, "context": "(Rodriguez-Lujan et al., 2010), the proposed approach for solving the objective is to simply use any of the standard quadratic programming implementations.", "startOffset": 0, "endOffset": 30}, {"referenceID": 8, "context": "Out of these seven datasets Leukemia, RAOA and RAC are microarray datasets (Kumar et al., 2012), MNIST is a vision dataset (Tan et al.", "startOffset": 75, "endOffset": 95}, {"referenceID": 14, "context": ", 2012), MNIST is a vision dataset (Tan et al., 2010) and REAL-SIM, Webspam and Kddb are the text classification datasets from NLP domain (Chang et al.", "startOffset": 35, "endOffset": 53}, {"referenceID": 1, "context": ", 2010) and REAL-SIM, Webspam and Kddb are the text classification datasets from NLP domain (Chang et al., 2010; Yiteng et al., 2012).", "startOffset": 92, "endOffset": 133}, {"referenceID": 15, "context": ", 2010) and REAL-SIM, Webspam and Kddb are the text classification datasets from NLP domain (Chang et al., 2010; Yiteng et al., 2012).", "startOffset": 92, "endOffset": 133}, {"referenceID": 16, "context": "We compared the performance of our proposed MMFS algorithm with FCBF3 (Yu and Liu, 2003), QPFS (Rodriguez-Lujan et al.", "startOffset": 70, "endOffset": 88}, {"referenceID": 11, "context": "We compared the performance of our proposed MMFS algorithm with FCBF3 (Yu and Liu, 2003), QPFS (Rodriguez-Lujan et al., 2010) and two other embedded feature selection methods, namely, Feature Generating Machine (FGM) (Tan et al.", "startOffset": 95, "endOffset": 125}, {"referenceID": 14, "context": ", 2010) and two other embedded feature selection methods, namely, Feature Generating Machine (FGM) (Tan et al., 2010) and Group Discovery Machine (GDM) (Yiteng et al.", "startOffset": 99, "endOffset": 117}, {"referenceID": 15, "context": ", 2010) and Group Discovery Machine (GDM) (Yiteng et al., 2012).", "startOffset": 42, "endOffset": 63}, {"referenceID": 9, "context": "QPFS, FGM and GDM have been shown to outperform a variety of existing feature selection methods including mRMR and MaxRel (Peng et al., 2005), FCBF (Yu and Liu, 2003), SVM-RFE (Guyon and Elisseeff, 2003), etc.", "startOffset": 122, "endOffset": 141}, {"referenceID": 16, "context": ", 2005), FCBF (Yu and Liu, 2003), SVM-RFE (Guyon and Elisseeff, 2003), etc.", "startOffset": 14, "endOffset": 32}, {"referenceID": 6, "context": ", 2005), FCBF (Yu and Liu, 2003), SVM-RFE (Guyon and Elisseeff, 2003), etc.", "startOffset": 42, "endOffset": 69}, {"referenceID": 11, "context": "For QPFS, we used mutual information (MI) as the similarity metric as it has been shown to give the best set of results (Rodriguez-Lujan et al., 2010).", "startOffset": 120, "endOffset": 150}, {"referenceID": 15, "context": "We have normalized these two datasets with unit variance (Yiteng et al., 2012).", "startOffset": 57, "endOffset": 78}, {"referenceID": 1, "context": "For MNIST and REAL-SIM datasets, training and testing splits are provided in (Chang et al., 2010).", "startOffset": 77, "endOffset": 97}, {"referenceID": 15, "context": "We have followed the training and testing splits of (Yiteng et al., 2012) for webspam and kddb datasets.", "startOffset": 52, "endOffset": 73}, {"referenceID": 14, "context": "We used the default settings of the parameters for both FGM and GDM as reported in (Tan et al., 2010; Yiteng et al., 2012).", "startOffset": 83, "endOffset": 122}, {"referenceID": 15, "context": "We used the default settings of the parameters for both FGM and GDM as reported in (Tan et al., 2010; Yiteng et al., 2012).", "startOffset": 83, "endOffset": 122}, {"referenceID": 2, "context": "After the top K features are selected, we used L2-regularized L2loss SVM (Fan et al., 2008) with default settings (that is cost parameter C=1) for classification for each of the algorithms and for each of the datasets.", "startOffset": 73, "endOffset": 91}, {"referenceID": 7, "context": "This implementation uses shrinking strategy (Hsieh et al., 2008).", "startOffset": 44, "endOffset": 64}, {"referenceID": 11, "context": "We used the publicly available implementation of QPFS (Rodriguez-Lujan et al., 2010).", "startOffset": 54, "endOffset": 84}, {"referenceID": 15, "context": "al (Yiteng et al., 2012).", "startOffset": 3, "endOffset": 24}, {"referenceID": 11, "context": "So, we used Nystr\u00f6m approximation (Rodriguez-Lujan et al., 2010) with sampling rate(\u03c1=0.", "startOffset": 34, "endOffset": 64}], "year": 2016, "abstractText": "Many machine learning applications such as in vision, biology and social networking deal with data in high dimensions. Feature selection is typically employed to select a subset of features which improves generalization accuracy as well as reduces the computational cost of learning the model. One of the criteria used for feature selection is to jointly minimize the redundancy and maximize the relevance of the selected features. In this paper, we formulate the task of feature selection as a one class SVM problem in a space where features correspond to the data points and instances correspond to the dimensions. The goal is to look for a representative subset of the features (support vectors) which describes the boundary for the region where the set of the features (data points) exists. This leads to a joint optimization of relevance and redundancy in a principled max-margin framework. Additionally, our formulation enables us to leverage existing techniques for optimizing the SVM objective resulting in highly computationally efficient solutions for the task of feature selection. Specifically, we employ the dual coordinate descent algorithm (Hsieh et al., 2008), originally proposed for SVMs, for our formulation. We use a sparse representation to deal with data in very high dimensions. Experiments on seven publicly available benchmark datasets from a variety of domains show that our approach results in orders of magnitude faster solutions even while retaining the same level of accuracy compared to the state of the art feature selection techniques. c \u00a9 2016 Elsevier Ltd. All rights reserved.", "creator": "LaTeX with hyperref package"}}}