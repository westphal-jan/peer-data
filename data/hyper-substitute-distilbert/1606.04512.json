{"id": "1606.04512", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Why is Compiling Lifted Inference into a Low-Level Language so Effective?", "abstract": "first - gen input capture technologies have established models for affirmative inference. they arrange a generic probability chart into a primary circuit about which full inference inquiries can be answered efficiently. early examples used exponential diagrams providing this target circuit. doing our 1960 - 2001 analyses, we demonstrates that compiling to a design - reliability question worthy of suitable data structure offers orders of magnitude reductions, resulting in the state - of - any - art lifted inquiry technique. in this phase, publishers conduct steps to address two questions regarding our kr - 2016 results : 1 - does the sample come a more efficient compilation or more intelligent reasoning with the applicable circuit?, and 2 - essentially are top - volume programs more efficient target circuits than data structures?", "histories": [["v1", "Tue, 14 Jun 2016 19:13:30 GMT  (440kb,D)", "http://arxiv.org/abs/1606.04512v1", "6 pages, 3 figures, accepted at IJCAI-16 Statistical Relational AI (StaRAI) workshop"]], "COMMENTS": "6 pages, 3 figures, accepted at IJCAI-16 Statistical Relational AI (StaRAI) workshop", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["seyed mehran kazemi", "david poole"], "accepted": false, "id": "1606.04512"}, "pdf": {"name": "1606.04512.pdf", "metadata": {"source": "CRF", "title": "Why is Compiling Lifted Inference into a Low-Level Language so Effective?\u2217", "authors": ["Seyed Mehran Kazemi", "David Poole"], "emails": ["poole}@cs.ubc.ca"], "sections": [{"heading": null, "text": "Probabilistic relational models (Getoor, 2007; De Raedt et al., 2016) (PRMs), or template-based models (Koller and Friedman, 2009), are extensions of Markov and belief networks (Pearl, 1988) that allow modelling of the dependencies among relations of individuals, and use a form of exchangeability: individuals about which there exists the same information are treated identically. The promise of lifted probabilistic inference (Poole, 2003; Kersting, 2012) is to carry out probabilistic inference for a PRM without needing to reason about each individual separately (grounding out the representation) by instead exploiting exchangeability to count undistinguished individuals.\nThe problem of lifted probabilistic inference was first explicitly proposed by Poole (2003), who formulated the problem in terms of parametrized random variables, introduced the use of splitting to complement unification, the parametric factor (parfactor) representation of intermediate results, and an algorithm for summing out parametrized random variables and multiplying parfactors in a lifted manner. This work was advanced by the introduction of counting formulae, the development of counting elimination algorithms, and lifting the aggregator functions (de Salvo Braz, Amir, and Roth, 2005; Milch et al., 2008; Kisynski and Poole, 2009; Choi, de Salvo Braz, and Bui, 2011; Taghipour, Davis, and Blockeel, 2014). The main problem with these proposals is that they are based on variable elimination. Variable\n\u2217In IJCAI-16 Statistical Relational AI Workshop. Copyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nelimination (Zhang and Poole, 1994) (VE) is a dynamic programming approach which requires a representation of the intermediate results, and the current representations for such results are not closed under all operations used for inference.\nAn alternative to VE is to use search-based methods based on conditioning such as recursive conditioning (Darwiche, 2001), AND-OR search (Dechter and Mateescu, 2007) and other related works (e.g., Bacchus, Dalmao, and Pitassi (2009)). While for lifted inference these methods require propositionalization in the same cases VE does, the advantage of these methods is that conditioning simplifies the representations rather than complicating them, and these methods exploit context specific independence (Boutilier et al., 1996) and determinism. The use of lifted search-based inference was proposed by Jha et al. (2010), Gogate and Domingos (2011) and Poole, Bacchus, and Kisynski (2011). These methods take as input a probabilistic relational model, a query, and some observations, and output the probability of the query given the observations.\nVan den Broeck et al. (2011) and Kazemi and Poole (2016) follow a knowledge compilation approach to lifted inference by evaluating a search-based lifted inference algorithm symbolically (instead of numerically) and extracting a target circuit on which many inference queries can be efficiently answered. While the target circuit used by Van den Broeck et al. (2011) is a data structure, Kazemi and Poole (2016) showed that using a low-level program as a target circuit is more efficient and results in orders of magnitude speedup. In a simultaneous work, Wu et al. (2016) showed that compiling to a low-level program is effective for BLOG (Milch et al., 2005) and offers substantial speedup.\nTwo issues remained unanswered in Kazemi and Poole (2016)\u2019s results: 1- they compared end-to-end (compiling to a target circuit and reasoning with the circuit) run-times of their work with Van den Broeck et al. (2011)\u2019s weighted first-order model counting, leaving the question of where exactly the speedup comes from, and 2- the actual reason behind the speedup gained by compiling to a program instead of a data structure remained untested. In this paper, we answer to these two issues. We conduct our experiments on Markov logic networks (MLNs) (Richardson and Domingos, 2006) and argue that our results also hold for other representations (e.g., (Jaeger, 1997; De Raedt, Kimmig, and Toivonen, 2007; Suciu et al., 2011; Kazemi et al., 2014)).\nar X\niv :1\n60 6.\n04 51\n2v 1\n[ cs\n.A I]\n1 4\nJu n\n20 16"}, {"heading": "Background and Notations", "text": "A population is a set of individuals (a.k.a. things, entities or objects). The population size is a nonnegative integer indicating the cardinality of the population. A logical variable is written in lower case and is typed with a population. For a logical variable x, we let \u2206x and |\u2206x| represent the population associated with x and its cardinality respectively. A lower case letter in bold represents a tuple of logical variables. Constants, denoting individuals, are written starting with an upper-case letter. A term is a logical variable or a constant.\nA parametrized random variable (PRV) consists of a k-ary predicate R and k terms ti and is represented as R(t1, . . . , tk). If every ti is a constant, the PRV corresponds to a random variable. When k = 0, we omit the parentheses. A grounding of a PRV can be achieved by replacing each of the logical variables with one of the individuals in their domains. A literal is an assignment of a value to a PRV. We represent R(. . .) = True and R(. . .) = False by r(. . .) and \u00acr(. . .) respectively. A world is a truth assignment to all groundings of all PRVs. A formula is made up of literals connected with logical connectives (conjunctions and disjunctions).\nFollowing Kazemi and Poole (2016), we represent a weighted formula (WF) as a triple \u3008L,F,w\u3009, where L is a set of logical variables, F is a formula whose logical variables are a subset of L, and w is a real-valued weight. For a given WF \u3008L,F,w\u3009 and a world \u03c9 , we let \u03b7(L,F,\u03c9) represent the number of assignments of individuals to the logical variables in L for which F holds in \u03c9 ."}, {"heading": "Markov Logic Networks", "text": "A Markov Logic Network (MLN) consists of a set \u03c8 of WFs. It induces the following probability distribution:\nProb(\u03c9) = 1 Z \u220f\u3008L,F,w\u3009\u2208\u03c8 exp(\u03b7(L,F,\u03c9)\u2217w) (1)\nwhere \u03c9 is a world and\nZ = \u2211 \u03c9 \u2032 ( \u220f \u3008L,F,w\u3009\u2208\u03c8\n(exp(\u03b7(L,F,\u03c9 \u2032)\u2217w)) (2)\nis the partition (normalization) function. In this paper, we focus on calculating the partition function as many inference queries on MLNs reduce to calculating the partition function. Following Kazemi and Poole (2016), we assume the formulae in WFs of MLNs are in conjunctive form. Example 1. Consider an MLN over three PRVs R(x,m), S(x,m) and T (x), where \u2206x = {X1,X2,X3,X4,X5} and \u2206m = {M1,M2}, with the following WFs: {\u3008{x,m},r(x,m)\u2227 s(x,m),1.2\u3009, \u3008{x,m},s(x,m)\u2227 t(x),0.2\u3009} and a world \u03c9 in which R(X1,M1), S(X1,M1), S(X1,M2) and T (X1) are True and the other ground PRVs are False. Then \u03b7({x,m},r(x,m)\u2227 s(x,m),\u03c9) = 1 as there is only one assignment of individuals to x and m (x = X1 and m = M1) for which r(x,m)\u2227 s(x,m) holds in \u03c9 and \u03b7({x,m},s(x,m)\u2227 t(x),\u03c9) = 2. Therefore:\nPr(\u03c9) = 1Z (exp(1\u22171.2)\u2217 exp(2\u22170.2))\nAn MLN can be conditioned on a PRV having no logical variables by replacing the PRV in all formulae of WFs with its observed value. Observations on individuals can be handled by a process called shattering.\nExample 2. Suppose for the MLN in Example 1 we observe that T (X1) and T (X2) are True. Since we have more information about X1 and X2 compared to other individuals in \u2206x, the individuals in \u2206x are no longer exchangeable. In order to handle this, we create two new logical variables x1 and x2 with \u2206x1 = {X1,X2} representing the individuals for which we have observed T is True, and \u2206x2 = {X3,X4,X5} representing the individuals for which we have not observed T . Then we create new WFs with our new logical variables as follows:\n{\u3008{x1,m},r(x1,m)\u2227 s(x1,m),1.2\u3009, \u3008{x2,m},r(x2,m)\u2227 s(x2,m),1.2\u3009, \u3008{x1,m},s(x1,m)\u2227 t(x1),0.2\u3009, \u3008{x2,m},s(x2,m)\u2227 t(x2),0.2\u3009}\nThen we replace t(x1) with True. For every logical variable x in the above MLN, the individuals in \u2206x are now exchangeable. This process is called shattering. We assume our input MLNs have been shattered based on the observations and refer interested readers to (de Salvo Braz, Amir, and Roth, 2005) for the details.\nAn MLN can be also conditioned on some counts: the number of times a PRV with one logical variable is True or False. For a PRV T (x), we let Obs(T (x), i) represent a count observation on T (x) indicating T is True for exactly i out of |\u2206x| individuals. Example 3. Suppose for the MLN in Example 1 we observe Obs(T (x),2). We create two new logical variables x1 and x2 representing the subset of x having T True and False respectively, with |\u2206x1 | = 2 and |\u2206x2 | = 3.1 Then we create new WFs as in Example 2, and replace t(x1) with True and t(x2) with False."}, {"heading": "Search-based Lifted Inference Rules", "text": "There are several rules that are used in search-based lifted inference algorithms. In this section, we describe some of these rules using examples."}, {"heading": "Lifted Decomposition", "text": "Example 4. Consider the MLN in Example 1. On the relational level, all PRVs are connected to each other and we only have one connected component. On the grounding, however, for every individual Xi \u2208 \u2206x, we have the following WFs mentioning Xi:\n{\u3008{},r(Xi,M1)\u2227 s(Xi,M1),1.2\u3009, \u3008{},s(Xi,M1)\u2227 t(Xi),0.2\u3009, \u3008{},r(Xi,M2)\u2227 s(Xi,M2),1.2\u3009, \u3008{},s(Xi,M2)\u2227 t(Xi),0.2\u3009}\nNotice that the WFs mentioning Xi in the grounding are totally disconnected from the other WFs. Therefore, we have\n1The domains can be assigned randomly due to the exchangeability of the individuals.\n|\u2206x| connected components that are equivalent up to renaming of the Xi individuals. In this case, x is called a decomposer of the network. Given the exchangeability of the individuals, the Z of all these connected components are the same. Therefore, we compute the Z for only one of these connected components, e.g., for an MLN with the following WFs, and raise it to the power of |\u2206x|.\n{\u3008{m},r(X1,m)\u2227 s(X1,m),1.2\u3009, \u3008{m},s(X1,m)\u2227 t(X1),0.2\u3009}\nIn the above MLN, x has been replaced by one of its individuals. We refer to this as decomposing the MLN on logical variable x. While in this example only one logical variable is the decomposer, note that in general a set of logical variables can be the decomposer of an MLN. We point interested readers to (Poole, Bacchus, and Kisynski, 2011) for a detailed analysis of when a set of logical variables x is a decomposer of a network."}, {"heading": "Lifted Case Analysis", "text": "Example 5. Consider the resulting MLN in Example 4 after being decomposed on x. We can find the partition function for this MLN by a case analysis on the values of a PRV. Suppose we do a case analysis on S(X1,m). Given that S(X1,m) represents |\u2206m| random variables in the grounding, one may think 2|\u2206m| cases must be considered: one for each assignment of values to the random variables. However, the individuals are exchangeable, i.e. we only care about the number of times S(X1,m) is True, not about the individuals that make it True. Thus, we only consider |\u2206m|+ 1 cases with the ith case being the case where for i out of |\u2206m| individuals S(X1,m) is True. We also multiply the ith case to (|\u2206m| i\n) to take into account the number of different assignments to the individuals in \u2206m for which S(X1,m) is exactly i times True. The case analysis for this PRV will then be: Z(M) = \u2211|\u2206m|i=0 (|\u2206m| i ) Z(M|Obs(S(X1,m), i)) where M|Obs(S(X1,m), i) has the following WFs: {\u3008{m1},True\u2227 r(X1,m1),1.2\u3009, \u3008{m2},False\u2227 r(X1,m2),1.2\u3009, \u3008{m1},True\u2227 t(X1),0.2\u3009, \u3008{m2},False\u2227 t(X1),0.2\u3009}"}, {"heading": "Removing False Formulae", "text": "Example 6. Consider the resulting MLN in Example 5 after the case analysis on S(X1,m). The formulae of the second and the fourth WFs are equivalent to False and can be removed from the MLN. However, removing these WFs causes the random variables in R(X1,m2) to be totally eliminated from the MLN. To address the effect of these variables, we calculate the Z of the MLN having only the first and third WFs and multiply it by 2|\u2206m2 |, i.e. the number of possible assignments to the |\u2206m2 | random variables in R(X1,m2)."}, {"heading": "Decomposition", "text": "Example 7. Consider the resulting MLN in Example 6 after removing the WFs whose formulae are equivalent to False. The resulting MLN has two WFs each mentioning different PRVs, i.e. the two WFs are disconnected. In this case, we\ncan find the Z of the first and second formulae (more generally: first and second connected components) separately and return the product."}, {"heading": "Case Analysis", "text": "Example 8. Consider the second connected component of the MLN in Example 7. The partition function of this MLN can be found by a case analysis on T (X1) as: Z(M) = Z(M | T (X1) = True)+Z(M | T (X1) = False)."}, {"heading": "Evaluating True Formulae", "text": "Example 9. Consider the MLN in Example 8 conditioned on T (X1) = True. This MLN has one WF:\n{\u3008{m2},True,1.2\u3009} Since the formula of the WF is equivalent to True, we can evaluate this WF as exp(1.2\u2217 |\u2206m2 |)."}, {"heading": "Caching", "text": "The above rules each generate new MLNs, find their partition functions, combine and return the results. As we apply the above rules, we keep the partition functions of the generated MLNs in a cache so we can potentially use them in future when the partition function of the same MLN is required.\nLifted Inference by Compiling into a"}, {"heading": "Low-Level Program", "text": "We explain Kazemi and Poole (2016)\u2019s LRC2CPP algorithm for compiling an MLN into a C++ program using an example. LRC2CPP is a recursive algorithm which takes as input an MLN M and a variable name vname, and outputs a C++ code which computes Z(M) and stores it in a variable called vname. Example 10. Consider compiling the MLN MLN1 in Example 1 to a C++ program by following LRC2CPP. Initially, LRC2CPP calls LRC2CPP(MLN1,\"v1\").\nAs explained in Example 4, x is a decomposer of MLN1. Let MLN2 denote decompose(MLN1,x) (i.e. the resulting MLN in Example 4 after decomposition). LRC2CPP generates the following C++ program:\nCode f or LRC2CPP(MLN2,\"v2\") v1 = pow(v2,5);\nwhere 5 represents |\u2206x|. For LRC2CPP(MLN2,\"v2\"), suppose we choose to do a case analysis on S(X1,m) as in Example 5. Assuming MLN3 represents MLN2 conditioned on Obs(S(X1,m), i), LRC2CPP generates a for loop as follows:\nv2 = 0; f or(int i = 0; i <= 2; i++){\nCode f or LRC2CPP(MLN3,\"v3\") v2 += Choose(2, i)\u2217 v3;\n} where 2 represents |\u2206m|, and Choose(2, i) computes (2 i ) . MLN3 has the WFs in the resulting MLN of Example 5. The formulae of the second and fourth WFs are False and will be removed from MLN3. However, as explained in Example 6, after removing these two WFs, R(X1,m2) will be totally eliminated. Assuming MLN4 represents MLN3 after\nremoving its second and fourth WFs, LRC2CPP generates: Code f or LRC2CPP(MLN4,\"v4\") v3 = pow(2,2\u2212 i)\u2217 v4; where 2\u2212 i refers to the number of ground variables in R(X1,m2) (i.e. |\u2206m2 |).\nMLN4 is disconnected as explained in Example 7. Let MLN41 and MLN42 represent the first and second connected components. LRC2CPP generates:\nCode f or LRC2CPP(MLN41,\"v5\") Code f or LRC2CPP(MLN42,\"v6\") v4 = v5\u2217 v6;\nThe first component requires a case analysis of a PRV with one logical variable which generates another for loop, and the second component requires a case analysis of a PRV with no logical variables. We can continue following LRC2CPP for these components and get the C++ program in Figure 1(a)."}, {"heading": "Optimizing C++ Programs", "text": "Since the program obtained from LRC2CPP is generated automatically (not by a developer), a post-pruning step might seem required to reduce the size of the program. For instance, one can remove lines 11 and 12 of the program in Figure 1(a) and replace line 13 with \"v6 = v9+ 1;\". The same can be done for lines 5 and 9. One may also notice that some variables are set to some values and are then being used only once. For example in the program of Figure 1(a), v7 and v9 are two such variables. The program can be pruned by removing these lines and replacing them with their values whenever they are being used. One can obtain the program in Figure 1(b) by pruning the program in Figure 1(a). Pruning can potentially save time and memory at run-time, but the pruning itself may be time-consuming.\nKazemi and Poole (2016) use available optimization packages for C++ programs which optimize the code at compile time. In particular, they use the \u2212O3 flag at compile time to optimize their generated programs before running them."}, {"heading": "Experiments and Results", "text": "Kazemi and Poole (2016) compared their end-to-end running times to those of WFOMC (Van den Broeck et al., 2011) and probabilistic theorem proving (PTP) (Gogate and Domingos, 2011) on six benchmarks. By varying the population sizes of the logical variables for these benchmarks, they showed that LRC2CPP beats these two approaches for most population sizes, especially when the population sizes are large. WFOMC was the closest rival of LRC2CPP. A question which remained unanswered in Kazemi and Poole (2016)\u2019s experiments was to whether LRC2CPP outperforms WFOMC because the compilation to a target circuit is faster in LRC2CPP, or because reasoning with the target circuit generated by LRC2CPP is more efficient than that of WFOMC.\nIn order to address the above question, we measured the time spent by LRC2CPP and WFOMC on each of the reasoning steps for three networks: 1- the network used in Figure 1(f) of (Kazemi and Poole, 2016), 2- a network with only one WF A(x)\u2227 B(x)\u2227C(x,m)\u2227D(m)\u2227 E(m)\u2227 F , and 3- another network with only with WF A(x)\u2227 B(x)\u2227C(x)\u2227 D(x,m)\u2227E(m)\u2227F(m)\u2227G(m)\u2227H. For LRC2CPP, we used the MinNestedLoops heuristic (Kazemi and Poole, 2016) to select the (lifted) case analysis order of PRVs. MinNestedLoops starts with the order obtained from MinTableSize (Kazemi and Poole, 2014) and tries to improve it in terms of the maximum number of nested loops it produces in the C++ program using stochastic local search. All experiments were conducted on a 2.8GH core with 4GB RAM under MacOSX. Unless stated otherwise, the C++ programs of LRC2CPP were compiled using g++ compiler.\nFor the three networks, it takes LRC2CPP 0.173, 0.029, and 0.138 seconds and it takes WFOMC 0.768, 0.373, and 0.512 seconds respectively to generate their target circuits. Figure 2(a), (b) represent the time spent by LRC2CPP and WFOMC for reasoning with their circuits2 for the first and second networks when the population of the logical variables varies at the same time (WFOMC could not solve the third circuit for population sizes \u2265 500, so we did not include the run-time diagram for the third network).\nObtained results represent that the compilation part takes almost the same amount of time in both LRC2CPP and WFOMC. For small population sizes, reasoning with WFOMC\u2019s circuit is more efficient because LRC2CPP\u2019s circuit needs a program compilation step. However, as the population size grows, the program compilation time becomes negligible and reasoning with the programs generated by LRC2CPP becomes much faster than reasoning with the data structures generated by WFOMC. As an example, it can be seen from the diagrams that reasoning with LRC2CPP\u2019s\n2Reasoning with LRC2CPP programs are considered as the time spent on compiling the C++ codes plus the run time.\nprogram offers about 163x speedup compared to WFOMC\u2019s data structure for the first network when the population sizes are 5000. It is also interesting to note that the slope of the diagrams are the same, meaning reasoning with both circuits has the same time complexity.\nOur first experiment indicates that the speedup in LRC2CPP is mostly due to the reasoning step. The next question to be answered is why reasoning with LRC2CPP\u2019s programs is more efficient than reasoning with WFOMC\u2019s data structures. Kazemi and Poole (2016) hypothesized that the speedup is due to the fact that LRC2CPP\u2019s programs can be compiled and optimized, while reasoning with WFOMC\u2019s data structures requires an interpreter: a virtual machine that executes the data structure node-by-node. Validating this hypothesis by comparing the runtimes of LRC2CPP and WFOMC softwares is not sensible as there might be several implementation or other differences (e.g., case analysis order) between the two softwares.\nIn order to test Kazemi and Poole (2016)\u2019s hypothesis in an implementation-independent way, we used LRC2CPP to generate programs for the three networks in our previous experiment. For the reasoning step, we ran the programs in three different ways: 1- compiling and optimizing the programs using \u2212O3 flag, 2- compiling without optimizing the programs, and 3- running the programs using\nCh 7.5.3 which is an interpreter for C++ programs3. Obtained results can be viewed in Figure 3. We also included the run time of WFOMC in the first two diagrams (as explained before, WFOMC failed on the third network for population sizes of 500 or more). It can be viewed that interpreting the C++ programs produces similar run times as working with WFOMC\u2019s data structures. The diagram for WFOMC is slightly below the diagram for interpreting the C++ program. One reason can be the non-optimality of the interpreter used for interpreting the C++ programs. It is interesting to note that by interpreting the C++ programs for small population sizes, and compiling and optimizing them for larger population sizes, in our benchmarks LRC2CPP\u2019s programs are always more efficient than the WFOMC\u2019s data structures.\nIn order to compare the speedup caused by compilation (instead of interpreting) with the speedup caused by optimization, we measured the percentage of speedup caused by each of them for our three benchmarks (we only considered the cases where both of them contributed to the speedup). We found that on average, 99.7% of the speedup is caused by compilation, and only 0.3% of it is caused by optimization. For the largest population where interpreting produced\n3Note that C++ interpreters are mostly used for teaching purposes and may not be highly optimized.\nan answer in less than 1000s, we found that compilation offers an average of 175x speedup compared to interpretation. Furthermore, for the largest population where compilation produced an answer, we found that optimization offers an average of 2.3x speedup compared to running the code without optimizing it."}, {"heading": "Conclusion", "text": "Compiling relational models into low-level programs for lifted probabilistic inference is a promising approach and offers huge speedups compared to the other approaches. In this paper, we conducted two experiments to explore the reasons behind the efficiency of this approach. In our first experiment, we compared compiling to low-level programs vs. WFOMC (which compiles into data structures) regarding the amount of time spent on different steps of the reasoning process. Our results indicated that the compilation step takes almost the same time in both approaches and almost all the speedup comes from reasoning with a low-level program instead of a data structure. In our second experiment, we explored why reasoning with a low-level program is more efficient than reasoning with a data structure. We designed an implementation-independent experiment using which we tested and validated Kazemi and Poole (2016)\u2019s hypothesis stating that low-level programs can be compiled and optimized, while reasoning with a data structure requires a virtual machine to interpret the computations, and compilers are known to be faster than interpreters."}], "references": [{"title": "Solving #SAT and Bayesian inference with backtracking search", "author": ["F. Bacchus", "S. Dalmao", "T. Pitassi"], "venue": "Journal of Artificial Intelligence Research 391\u2013442.", "citeRegEx": "Bacchus et al\\.,? 2009", "shortCiteRegEx": "Bacchus et al\\.", "year": 2009}, {"title": "Context-specific independence in Bayesian networks", "author": ["C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller"], "venue": "Proceedings of UAI, 115\u2013123.", "citeRegEx": "Boutilier et al\\.,? 1996", "shortCiteRegEx": "Boutilier et al\\.", "year": 1996}, {"title": "Efficient methods for lifted inference with aggregate factors", "author": ["J. Choi", "R. de Salvo Braz", "H.H. Bui"], "venue": null, "citeRegEx": "Choi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2011}, {"title": "Recursive conditioning", "author": ["A. Darwiche"], "venue": "Artificial Intelligence 126(1-2):5\u201341.", "citeRegEx": "Darwiche,? 2001", "shortCiteRegEx": "Darwiche", "year": 2001}, {"title": "Statistical relational artificial intelligence: Logic, probability, and computation", "author": ["L. De Raedt", "K. Kersting", "S. Natarajan", "D. Poole"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning 10(2):1\u2013189.", "citeRegEx": "Raedt et al\\.,? 2016", "shortCiteRegEx": "Raedt et al\\.", "year": 2016}, {"title": "Problog: A probabilistic prolog and its application in link discovery", "author": ["L. De Raedt", "A. Kimmig", "H. Toivonen"], "venue": "IJCAI, volume 7.", "citeRegEx": "Raedt et al\\.,? 2007", "shortCiteRegEx": "Raedt et al\\.", "year": 2007}, {"title": "Lifted first-order probabilistic inference", "author": ["R. de Salvo Braz", "E. Amir", "D. Roth"], "venue": "In IJCAI,", "citeRegEx": "Braz et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Braz et al\\.", "year": 2005}, {"title": "And/or search spaces for graphical models", "author": ["R. Dechter", "R. Mateescu"], "venue": "Artificial intelligence 171(2):73\u2013 106.", "citeRegEx": "Dechter and Mateescu,? 2007", "shortCiteRegEx": "Dechter and Mateescu", "year": 2007}, {"title": "Introduction to statistical relational learning", "author": ["L. Getoor"], "venue": "MIT press.", "citeRegEx": "Getoor,? 2007", "shortCiteRegEx": "Getoor", "year": 2007}, {"title": "Probabilistic theorem proving", "author": ["V. Gogate", "P. Domingos"], "venue": "UAI, 256\u2013265.", "citeRegEx": "Gogate and Domingos,? 2011", "shortCiteRegEx": "Gogate and Domingos", "year": 2011}, {"title": "Relational Bayesian networks", "author": ["M. Jaeger"], "venue": "UAI. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Jaeger,? 1997", "shortCiteRegEx": "Jaeger", "year": 1997}, {"title": "Lifted inference seen from the other side: The tractable features", "author": ["A. Jha", "V. Gogate", "A. Meliou", "D. Suciu"], "venue": "NIPS, 973\u2013981.", "citeRegEx": "Jha et al\\.,? 2010", "shortCiteRegEx": "Jha et al\\.", "year": 2010}, {"title": "Elimination ordering in first-order probabilistic inference", "author": ["S.M. Kazemi", "D. Poole"], "venue": "AAAI.", "citeRegEx": "Kazemi and Poole,? 2014", "shortCiteRegEx": "Kazemi and Poole", "year": 2014}, {"title": "Knowledge compilation for lifted probabilistic inference: Compiling to a low-level language", "author": ["S.M. Kazemi", "D. Poole"], "venue": "KR.", "citeRegEx": "Kazemi and Poole,? 2016", "shortCiteRegEx": "Kazemi and Poole", "year": 2016}, {"title": "Relational logistic regression", "author": ["S.M. Kazemi", "D. Buchman", "K. Kersting", "S. Natarajan", "D. Poole"], "venue": "KR.", "citeRegEx": "Kazemi et al\\.,? 2014", "shortCiteRegEx": "Kazemi et al\\.", "year": 2014}, {"title": "Lifted probabilistic inference", "author": ["K. Kersting"], "venue": "ECAI, 33\u201338.", "citeRegEx": "Kersting,? 2012", "shortCiteRegEx": "Kersting", "year": 2012}, {"title": "Constraint processing in lifted probabilistic inference", "author": ["J. Kisynski", "D. Poole"], "venue": "UAI, 293\u2013302.", "citeRegEx": "Kisynski and Poole,? 2009", "shortCiteRegEx": "Kisynski and Poole", "year": 2009}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT Press, Cambridge, MA.", "citeRegEx": "Koller and Friedman,? 2009", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "BLOG: Probabilistic models with unknown objects", "author": ["B. Milch", "B. Marthi", "S. Russell", "D. Sontag", "D.L. Ong", "A. Kolobov"], "venue": "Proc. IJCAI, 1352\u20131359.", "citeRegEx": "Milch et al\\.,? 2005", "shortCiteRegEx": "Milch et al\\.", "year": 2005}, {"title": "Lifted probabilistic inference with counting formulae", "author": ["B. Milch", "L.S. Zettlemoyer", "K. Kersting", "M. Haimes", "L.P. Kaelbling"], "venue": "AAAI, 1062\u20131068.", "citeRegEx": "Milch et al\\.,? 2008", "shortCiteRegEx": "Milch et al\\.", "year": 2008}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "San Mateo, CA: Morgan Kaumann.", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Towards completely lifted search-based probabilistic inference", "author": ["D. Poole", "F. Bacchus", "J. Kisynski"], "venue": "arXiv:1107.4035 [cs.AI].", "citeRegEx": "Poole et al\\.,? 2011", "shortCiteRegEx": "Poole et al\\.", "year": 2011}, {"title": "First-order probabilistic inference", "author": ["D. Poole"], "venue": "IJCAI, 985\u2013991.", "citeRegEx": "Poole,? 2003", "shortCiteRegEx": "Poole", "year": 2003}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine Learning 62:107\u2013136.", "citeRegEx": "Richardson and Domingos,? 2006", "shortCiteRegEx": "Richardson and Domingos", "year": 2006}, {"title": "Probabilistic databases", "author": ["D. Suciu", "D. Olteanu", "C. R\u00e9", "C. Koch"], "venue": "Synthesis Lectures on Data Management 3(2):1\u2013180.", "citeRegEx": "Suciu et al\\.,? 2011", "shortCiteRegEx": "Suciu et al\\.", "year": 2011}, {"title": "Generalized counting for lifted variable elimination", "author": ["N. Taghipour", "J. Davis", "H. Blockeel"], "venue": "Inductive Logic Programming. Springer Berlin Heidelberg 107\u2013122.", "citeRegEx": "Taghipour et al\\.,? 2014", "shortCiteRegEx": "Taghipour et al\\.", "year": 2014}, {"title": "Lifted probabilistic inference by first-order knowledge compilation", "author": ["G. Van den Broeck", "N. Taghipour", "W. Meert", "J. Davis", "L. De Raedt"], "venue": "IJCAI, 2178\u20132185.", "citeRegEx": "Broeck et al\\.,? 2011", "shortCiteRegEx": "Broeck et al\\.", "year": 2011}, {"title": "Swift: Compiled inference for probabilistic programs", "author": ["Y. Wu", "L. Li", "S. Russell", "R. Bodik"], "venue": "Proc. IJCAI.", "citeRegEx": "Wu et al\\.,? 2016", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "A simple approach to Bayesian network computations", "author": ["N.L. Zhang", "D. Poole"], "venue": "Proceedings of the 10th Canadian Conference on AI, 171\u2013178.", "citeRegEx": "Zhang and Poole,? 1994", "shortCiteRegEx": "Zhang and Poole", "year": 1994}], "referenceMentions": [{"referenceID": 8, "context": "Probabilistic relational models (Getoor, 2007; De Raedt et al., 2016) (PRMs), or template-based models (Koller and Friedman, 2009), are extensions of Markov and belief networks (Pearl, 1988) that allow modelling of the dependencies among relations of individuals, and use a form of exchangeability: individuals about which there exists the same information are treated identically.", "startOffset": 32, "endOffset": 69}, {"referenceID": 17, "context": ", 2016) (PRMs), or template-based models (Koller and Friedman, 2009), are extensions of Markov and belief networks (Pearl, 1988) that allow modelling of the dependencies among relations of individuals, and use a form of exchangeability: individuals about which there exists the same information are treated identically.", "startOffset": 41, "endOffset": 68}, {"referenceID": 20, "context": ", 2016) (PRMs), or template-based models (Koller and Friedman, 2009), are extensions of Markov and belief networks (Pearl, 1988) that allow modelling of the dependencies among relations of individuals, and use a form of exchangeability: individuals about which there exists the same information are treated identically.", "startOffset": 115, "endOffset": 128}, {"referenceID": 22, "context": "The promise of lifted probabilistic inference (Poole, 2003; Kersting, 2012) is to carry out probabilistic inference for a PRM without needing to reason about each individual separately (grounding out the representation) by instead exploiting exchangeability to count undistinguished individuals.", "startOffset": 46, "endOffset": 75}, {"referenceID": 15, "context": "The promise of lifted probabilistic inference (Poole, 2003; Kersting, 2012) is to carry out probabilistic inference for a PRM without needing to reason about each individual separately (grounding out the representation) by instead exploiting exchangeability to count undistinguished individuals.", "startOffset": 46, "endOffset": 75}, {"referenceID": 19, "context": "This work was advanced by the introduction of counting formulae, the development of counting elimination algorithms, and lifting the aggregator functions (de Salvo Braz, Amir, and Roth, 2005; Milch et al., 2008; Kisynski and Poole, 2009; Choi, de Salvo Braz, and Bui, 2011; Taghipour, Davis, and Blockeel, 2014).", "startOffset": 154, "endOffset": 311}, {"referenceID": 16, "context": "This work was advanced by the introduction of counting formulae, the development of counting elimination algorithms, and lifting the aggregator functions (de Salvo Braz, Amir, and Roth, 2005; Milch et al., 2008; Kisynski and Poole, 2009; Choi, de Salvo Braz, and Bui, 2011; Taghipour, Davis, and Blockeel, 2014).", "startOffset": 154, "endOffset": 311}, {"referenceID": 4, "context": "Probabilistic relational models (Getoor, 2007; De Raedt et al., 2016) (PRMs), or template-based models (Koller and Friedman, 2009), are extensions of Markov and belief networks (Pearl, 1988) that allow modelling of the dependencies among relations of individuals, and use a form of exchangeability: individuals about which there exists the same information are treated identically. The promise of lifted probabilistic inference (Poole, 2003; Kersting, 2012) is to carry out probabilistic inference for a PRM without needing to reason about each individual separately (grounding out the representation) by instead exploiting exchangeability to count undistinguished individuals. The problem of lifted probabilistic inference was first explicitly proposed by Poole (2003), who formulated the problem in terms of parametrized random variables, introduced the use of splitting to complement unification, the parametric factor (parfactor) representation of intermediate results, and an algorithm for summing out parametrized random variables and multiplying parfactors in a lifted manner.", "startOffset": 50, "endOffset": 770}, {"referenceID": 28, "context": "elimination (Zhang and Poole, 1994) (VE) is a dynamic programming approach which requires a representation of the intermediate results, and the current representations for such results are not closed under all operations used for inference.", "startOffset": 12, "endOffset": 35}, {"referenceID": 3, "context": "An alternative to VE is to use search-based methods based on conditioning such as recursive conditioning (Darwiche, 2001), AND-OR search (Dechter and Mateescu, 2007) and other related works (e.", "startOffset": 105, "endOffset": 121}, {"referenceID": 7, "context": "An alternative to VE is to use search-based methods based on conditioning such as recursive conditioning (Darwiche, 2001), AND-OR search (Dechter and Mateescu, 2007) and other related works (e.", "startOffset": 137, "endOffset": 165}, {"referenceID": 1, "context": "While for lifted inference these methods require propositionalization in the same cases VE does, the advantage of these methods is that conditioning simplifies the representations rather than complicating them, and these methods exploit context specific independence (Boutilier et al., 1996) and determinism.", "startOffset": 267, "endOffset": 291}, {"referenceID": 18, "context": "(2016) showed that compiling to a low-level program is effective for BLOG (Milch et al., 2005) and offers substantial speedup.", "startOffset": 74, "endOffset": 94}, {"referenceID": 23, "context": "We conduct our experiments on Markov logic networks (MLNs) (Richardson and Domingos, 2006) and argue that our results also hold for other representations (e.", "startOffset": 59, "endOffset": 90}, {"referenceID": 10, "context": ", (Jaeger, 1997; De Raedt, Kimmig, and Toivonen, 2007; Suciu et al., 2011; Kazemi et al., 2014)).", "startOffset": 2, "endOffset": 95}, {"referenceID": 24, "context": ", (Jaeger, 1997; De Raedt, Kimmig, and Toivonen, 2007; Suciu et al., 2011; Kazemi et al., 2014)).", "startOffset": 2, "endOffset": 95}, {"referenceID": 14, "context": ", (Jaeger, 1997; De Raedt, Kimmig, and Toivonen, 2007; Suciu et al., 2011; Kazemi et al., 2014)).", "startOffset": 2, "endOffset": 95}, {"referenceID": 2, "context": "An alternative to VE is to use search-based methods based on conditioning such as recursive conditioning (Darwiche, 2001), AND-OR search (Dechter and Mateescu, 2007) and other related works (e.g., Bacchus, Dalmao, and Pitassi (2009)).", "startOffset": 106, "endOffset": 233}, {"referenceID": 1, "context": "While for lifted inference these methods require propositionalization in the same cases VE does, the advantage of these methods is that conditioning simplifies the representations rather than complicating them, and these methods exploit context specific independence (Boutilier et al., 1996) and determinism. The use of lifted search-based inference was proposed by Jha et al. (2010), Gogate and Domingos (2011) and Poole, Bacchus, and Kisynski (2011).", "startOffset": 268, "endOffset": 384}, {"referenceID": 1, "context": "While for lifted inference these methods require propositionalization in the same cases VE does, the advantage of these methods is that conditioning simplifies the representations rather than complicating them, and these methods exploit context specific independence (Boutilier et al., 1996) and determinism. The use of lifted search-based inference was proposed by Jha et al. (2010), Gogate and Domingos (2011) and Poole, Bacchus, and Kisynski (2011).", "startOffset": 268, "endOffset": 412}, {"referenceID": 1, "context": "While for lifted inference these methods require propositionalization in the same cases VE does, the advantage of these methods is that conditioning simplifies the representations rather than complicating them, and these methods exploit context specific independence (Boutilier et al., 1996) and determinism. The use of lifted search-based inference was proposed by Jha et al. (2010), Gogate and Domingos (2011) and Poole, Bacchus, and Kisynski (2011). These methods take as input a probabilistic relational model, a query, and some observations, and output the probability of the query given the observations.", "startOffset": 268, "endOffset": 452}, {"referenceID": 1, "context": "While for lifted inference these methods require propositionalization in the same cases VE does, the advantage of these methods is that conditioning simplifies the representations rather than complicating them, and these methods exploit context specific independence (Boutilier et al., 1996) and determinism. The use of lifted search-based inference was proposed by Jha et al. (2010), Gogate and Domingos (2011) and Poole, Bacchus, and Kisynski (2011). These methods take as input a probabilistic relational model, a query, and some observations, and output the probability of the query given the observations. Van den Broeck et al. (2011) and Kazemi and Poole (2016) follow a knowledge compilation approach to lifted inference by evaluating a search-based lifted inference algorithm symbolically (instead of numerically) and extracting a target circuit on which many inference queries can be efficiently answered.", "startOffset": 268, "endOffset": 640}, {"referenceID": 1, "context": "While for lifted inference these methods require propositionalization in the same cases VE does, the advantage of these methods is that conditioning simplifies the representations rather than complicating them, and these methods exploit context specific independence (Boutilier et al., 1996) and determinism. The use of lifted search-based inference was proposed by Jha et al. (2010), Gogate and Domingos (2011) and Poole, Bacchus, and Kisynski (2011). These methods take as input a probabilistic relational model, a query, and some observations, and output the probability of the query given the observations. Van den Broeck et al. (2011) and Kazemi and Poole (2016) follow a knowledge compilation approach to lifted inference by evaluating a search-based lifted inference algorithm symbolically (instead of numerically) and extracting a target circuit on which many inference queries can be efficiently answered.", "startOffset": 268, "endOffset": 668}, {"referenceID": 1, "context": "While for lifted inference these methods require propositionalization in the same cases VE does, the advantage of these methods is that conditioning simplifies the representations rather than complicating them, and these methods exploit context specific independence (Boutilier et al., 1996) and determinism. The use of lifted search-based inference was proposed by Jha et al. (2010), Gogate and Domingos (2011) and Poole, Bacchus, and Kisynski (2011). These methods take as input a probabilistic relational model, a query, and some observations, and output the probability of the query given the observations. Van den Broeck et al. (2011) and Kazemi and Poole (2016) follow a knowledge compilation approach to lifted inference by evaluating a search-based lifted inference algorithm symbolically (instead of numerically) and extracting a target circuit on which many inference queries can be efficiently answered. While the target circuit used by Van den Broeck et al. (2011) is a data structure, Kazemi and Poole (2016) showed that using a low-level program as a target circuit is more efficient and results in orders of magnitude speedup.", "startOffset": 268, "endOffset": 977}, {"referenceID": 1, "context": "While for lifted inference these methods require propositionalization in the same cases VE does, the advantage of these methods is that conditioning simplifies the representations rather than complicating them, and these methods exploit context specific independence (Boutilier et al., 1996) and determinism. The use of lifted search-based inference was proposed by Jha et al. (2010), Gogate and Domingos (2011) and Poole, Bacchus, and Kisynski (2011). These methods take as input a probabilistic relational model, a query, and some observations, and output the probability of the query given the observations. Van den Broeck et al. (2011) and Kazemi and Poole (2016) follow a knowledge compilation approach to lifted inference by evaluating a search-based lifted inference algorithm symbolically (instead of numerically) and extracting a target circuit on which many inference queries can be efficiently answered. While the target circuit used by Van den Broeck et al. (2011) is a data structure, Kazemi and Poole (2016) showed that using a low-level program as a target circuit is more efficient and results in orders of magnitude speedup.", "startOffset": 268, "endOffset": 1022}, {"referenceID": 1, "context": "While for lifted inference these methods require propositionalization in the same cases VE does, the advantage of these methods is that conditioning simplifies the representations rather than complicating them, and these methods exploit context specific independence (Boutilier et al., 1996) and determinism. The use of lifted search-based inference was proposed by Jha et al. (2010), Gogate and Domingos (2011) and Poole, Bacchus, and Kisynski (2011). These methods take as input a probabilistic relational model, a query, and some observations, and output the probability of the query given the observations. Van den Broeck et al. (2011) and Kazemi and Poole (2016) follow a knowledge compilation approach to lifted inference by evaluating a search-based lifted inference algorithm symbolically (instead of numerically) and extracting a target circuit on which many inference queries can be efficiently answered. While the target circuit used by Van den Broeck et al. (2011) is a data structure, Kazemi and Poole (2016) showed that using a low-level program as a target circuit is more efficient and results in orders of magnitude speedup. In a simultaneous work, Wu et al. (2016) showed that compiling to a low-level program is effective for BLOG (Milch et al.", "startOffset": 268, "endOffset": 1183}, {"referenceID": 1, "context": "While for lifted inference these methods require propositionalization in the same cases VE does, the advantage of these methods is that conditioning simplifies the representations rather than complicating them, and these methods exploit context specific independence (Boutilier et al., 1996) and determinism. The use of lifted search-based inference was proposed by Jha et al. (2010), Gogate and Domingos (2011) and Poole, Bacchus, and Kisynski (2011). These methods take as input a probabilistic relational model, a query, and some observations, and output the probability of the query given the observations. Van den Broeck et al. (2011) and Kazemi and Poole (2016) follow a knowledge compilation approach to lifted inference by evaluating a search-based lifted inference algorithm symbolically (instead of numerically) and extracting a target circuit on which many inference queries can be efficiently answered. While the target circuit used by Van den Broeck et al. (2011) is a data structure, Kazemi and Poole (2016) showed that using a low-level program as a target circuit is more efficient and results in orders of magnitude speedup. In a simultaneous work, Wu et al. (2016) showed that compiling to a low-level program is effective for BLOG (Milch et al., 2005) and offers substantial speedup. Two issues remained unanswered in Kazemi and Poole (2016)\u2019s results: 1- they compared end-to-end (compiling to a target circuit and reasoning with the circuit) run-times of their work with Van den Broeck et al.", "startOffset": 268, "endOffset": 1361}, {"referenceID": 1, "context": "While for lifted inference these methods require propositionalization in the same cases VE does, the advantage of these methods is that conditioning simplifies the representations rather than complicating them, and these methods exploit context specific independence (Boutilier et al., 1996) and determinism. The use of lifted search-based inference was proposed by Jha et al. (2010), Gogate and Domingos (2011) and Poole, Bacchus, and Kisynski (2011). These methods take as input a probabilistic relational model, a query, and some observations, and output the probability of the query given the observations. Van den Broeck et al. (2011) and Kazemi and Poole (2016) follow a knowledge compilation approach to lifted inference by evaluating a search-based lifted inference algorithm symbolically (instead of numerically) and extracting a target circuit on which many inference queries can be efficiently answered. While the target circuit used by Van den Broeck et al. (2011) is a data structure, Kazemi and Poole (2016) showed that using a low-level program as a target circuit is more efficient and results in orders of magnitude speedup. In a simultaneous work, Wu et al. (2016) showed that compiling to a low-level program is effective for BLOG (Milch et al., 2005) and offers substantial speedup. Two issues remained unanswered in Kazemi and Poole (2016)\u2019s results: 1- they compared end-to-end (compiling to a target circuit and reasoning with the circuit) run-times of their work with Van den Broeck et al. (2011)\u2019s weighted first-order model counting, leaving the question of where exactly the speedup comes from, and 2- the actual reason behind the speedup gained by compiling to a program instead of a data structure remained untested.", "startOffset": 268, "endOffset": 1521}, {"referenceID": 12, "context": "Following Kazemi and Poole (2016), we represent a weighted formula (WF) as a triple \u3008L,F,w\u3009, where L is a set of logical variables, F is a formula whose logical variables are a subset of L, and w is a real-valued weight.", "startOffset": 10, "endOffset": 34}, {"referenceID": 12, "context": "Following Kazemi and Poole (2016), we assume the formulae in WFs of MLNs are in conjunctive form.", "startOffset": 10, "endOffset": 34}, {"referenceID": 12, "context": "We explain Kazemi and Poole (2016)\u2019s LRC2CPP algorithm for compiling an MLN into a C++ program using an example.", "startOffset": 11, "endOffset": 35}, {"referenceID": 12, "context": "Kazemi and Poole (2016) use available optimization packages for C++ programs which optimize the code at compile time.", "startOffset": 0, "endOffset": 24}, {"referenceID": 9, "context": ", 2011) and probabilistic theorem proving (PTP) (Gogate and Domingos, 2011) on six benchmarks.", "startOffset": 48, "endOffset": 75}, {"referenceID": 13, "context": "In order to address the above question, we measured the time spent by LRC2CPP and WFOMC on each of the reasoning steps for three networks: 1- the network used in Figure 1(f) of (Kazemi and Poole, 2016), 2- a network with only one WF A(x)\u2227 B(x)\u2227C(x,m)\u2227D(m)\u2227 E(m)\u2227 F , and 3another network with only with WF A(x)\u2227 B(x)\u2227C(x)\u2227 D(x,m)\u2227E(m)\u2227F(m)\u2227G(m)\u2227H.", "startOffset": 177, "endOffset": 201}, {"referenceID": 13, "context": "For LRC2CPP, we used the MinNestedLoops heuristic (Kazemi and Poole, 2016) to select the (lifted) case analysis order of PRVs.", "startOffset": 50, "endOffset": 74}, {"referenceID": 12, "context": "MinNestedLoops starts with the order obtained from MinTableSize (Kazemi and Poole, 2014) and tries to improve it in terms of the maximum number of nested loops it produces in the C++ program using stochastic local search.", "startOffset": 64, "endOffset": 88}, {"referenceID": 9, "context": ", 2011) and probabilistic theorem proving (PTP) (Gogate and Domingos, 2011) on six benchmarks. By varying the population sizes of the logical variables for these benchmarks, they showed that LRC2CPP beats these two approaches for most population sizes, especially when the population sizes are large. WFOMC was the closest rival of LRC2CPP. A question which remained unanswered in Kazemi and Poole (2016)\u2019s experiments was to whether LRC2CPP outperforms WFOMC because the compilation to a target circuit is faster in LRC2CPP, or because reasoning with the target circuit generated by LRC2CPP is more efficient than that of WFOMC.", "startOffset": 49, "endOffset": 405}, {"referenceID": 12, "context": "Kazemi and Poole (2016) hypothesized that the speedup is due to the fact that LRC2CPP\u2019s programs can be compiled and optimized, while reasoning with WFOMC\u2019s data structures requires an interpreter: a virtual machine that executes the data structure node-by-node.", "startOffset": 0, "endOffset": 24}, {"referenceID": 12, "context": "In order to test Kazemi and Poole (2016)\u2019s hypothesis in an implementation-independent way, we used LRC2CPP to generate programs for the three networks in our previous experiment.", "startOffset": 17, "endOffset": 41}, {"referenceID": 12, "context": "We designed an implementation-independent experiment using which we tested and validated Kazemi and Poole (2016)\u2019s hypothesis stating that low-level programs can be compiled and optimized, while reasoning with a data structure requires a virtual machine to interpret the computations, and compilers are known to be faster than interpreters.", "startOffset": 89, "endOffset": 113}], "year": 2016, "abstractText": "First-order knowledge compilation techniques have proven efficient for lifted inference. They compile a relational probability model into a target circuit on which many inference queries can be answered efficiently. Early methods used data structures as their target circuit. In our KR-2016 paper, we showed that compiling to a low-level program instead of a data structure offers orders of magnitude speedup, resulting in the state-of-the-art lifted inference technique. In this paper, we conduct experiments to address two questions regarding our KR-2016 results: 1does the speedup come from more efficient compilation or more efficient reasoning with the target circuit?, and 2why are low-level programs more efficient target circuits than data structures? Probabilistic relational models (Getoor, 2007; De Raedt et al., 2016) (PRMs), or template-based models (Koller and Friedman, 2009), are extensions of Markov and belief networks (Pearl, 1988) that allow modelling of the dependencies among relations of individuals, and use a form of exchangeability: individuals about which there exists the same information are treated identically. The promise of lifted probabilistic inference (Poole, 2003; Kersting, 2012) is to carry out probabilistic inference for a PRM without needing to reason about each individual separately (grounding out the representation) by instead exploiting exchangeability to count undistinguished individuals. The problem of lifted probabilistic inference was first explicitly proposed by Poole (2003), who formulated the problem in terms of parametrized random variables, introduced the use of splitting to complement unification, the parametric factor (parfactor) representation of intermediate results, and an algorithm for summing out parametrized random variables and multiplying parfactors in a lifted manner. This work was advanced by the introduction of counting formulae, the development of counting elimination algorithms, and lifting the aggregator functions (de Salvo Braz, Amir, and Roth, 2005; Milch et al., 2008; Kisynski and Poole, 2009; Choi, de Salvo Braz, and Bui, 2011; Taghipour, Davis, and Blockeel, 2014). The main problem with these proposals is that they are based on variable elimination. Variable \u2217In IJCAI-16 Statistical Relational AI Workshop. Copyright c \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. elimination (Zhang and Poole, 1994) (VE) is a dynamic programming approach which requires a representation of the intermediate results, and the current representations for such results are not closed under all operations used for inference. An alternative to VE is to use search-based methods based on conditioning such as recursive conditioning (Darwiche, 2001), AND-OR search (Dechter and Mateescu, 2007) and other related works (e.g., Bacchus, Dalmao, and Pitassi (2009)). While for lifted inference these methods require propositionalization in the same cases VE does, the advantage of these methods is that conditioning simplifies the representations rather than complicating them, and these methods exploit context specific independence (Boutilier et al., 1996) and determinism. The use of lifted search-based inference was proposed by Jha et al. (2010), Gogate and Domingos (2011) and Poole, Bacchus, and Kisynski (2011). These methods take as input a probabilistic relational model, a query, and some observations, and output the probability of the query given the observations. Van den Broeck et al. (2011) and Kazemi and Poole (2016) follow a knowledge compilation approach to lifted inference by evaluating a search-based lifted inference algorithm symbolically (instead of numerically) and extracting a target circuit on which many inference queries can be efficiently answered. While the target circuit used by Van den Broeck et al. (2011) is a data structure, Kazemi and Poole (2016) showed that using a low-level program as a target circuit is more efficient and results in orders of magnitude speedup. In a simultaneous work, Wu et al. (2016) showed that compiling to a low-level program is effective for BLOG (Milch et al., 2005) and offers substantial speedup. Two issues remained unanswered in Kazemi and Poole (2016)\u2019s results: 1they compared end-to-end (compiling to a target circuit and reasoning with the circuit) run-times of their work with Van den Broeck et al. (2011)\u2019s weighted first-order model counting, leaving the question of where exactly the speedup comes from, and 2the actual reason behind the speedup gained by compiling to a program instead of a data structure remained untested. In this paper, we answer to these two issues. We conduct our experiments on Markov logic networks (MLNs) (Richardson and Domingos, 2006) and argue that our results also hold for other representations (e.g., (Jaeger, 1997; De Raedt, Kimmig, and Toivonen, 2007; Suciu et al., 2011; Kazemi et al., 2014)). ar X iv :1 60 6. 04 51 2v 1 [ cs .A I] 1 4 Ju n 20 16 Background and Notations A population is a set of individuals (a.k.a. things, entities or objects). The population size is a nonnegative integer indicating the cardinality of the population. A logical variable is written in lower case and is typed with a population. For a logical variable x, we let \u2206x and |\u2206x| represent the population associated with x and its cardinality respectively. A lower case letter in bold represents a tuple of logical variables. Constants, denoting individuals, are written starting with an upper-case letter. A term is a logical variable or a constant. A parametrized random variable (PRV) consists of a k-ary predicate R and k terms ti and is represented as R(t1, . . . , tk). If every ti is a constant, the PRV corresponds to a random variable. When k = 0, we omit the parentheses. A grounding of a PRV can be achieved by replacing each of the logical variables with one of the individuals in their domains. A literal is an assignment of a value to a PRV. We represent R(. . .) = True and R(. . .) = False by r(. . .) and \u00acr(. . .) respectively. A world is a truth assignment to all groundings of all PRVs. A formula is made up of literals connected with logical connectives (conjunctions and disjunctions). Following Kazemi and Poole (2016), we represent a weighted formula (WF) as a triple \u3008L,F,w\u3009, where L is a set of logical variables, F is a formula whose logical variables are a subset of L, and w is a real-valued weight. For a given WF \u3008L,F,w\u3009 and a world \u03c9 , we let \u03b7(L,F,\u03c9) represent the number of assignments of individuals to the logical variables in L for which F holds in \u03c9 . Markov Logic Networks A Markov Logic Network (MLN) consists of a set \u03c8 of WFs. It induces the following probability distribution: Prob(\u03c9) = 1 Z \u220f \u3008L,F,w\u3009\u2208\u03c8 exp(\u03b7(L,F,\u03c9)\u2217w) (1) where \u03c9 is a world and Z = \u2211 \u03c9 \u2032 ( \u220f \u3008L,F,w\u3009\u2208\u03c8 (exp(\u03b7(L,F,\u03c9 \u2032)\u2217w)) (2) is the partition (normalization) function. In this paper, we focus on calculating the partition function as many inference queries on MLNs reduce to calculating the partition function. Following Kazemi and Poole (2016), we assume the formulae in WFs of MLNs are in conjunctive form. Example 1. Consider an MLN over three PRVs R(x,m), S(x,m) and T (x), where \u2206x = {X1,X2,X3,X4,X5} and \u2206m = {M1,M2}, with the following WFs: {\u3008{x,m},r(x,m)\u2227 s(x,m),1.2\u3009, \u3008{x,m},s(x,m)\u2227 t(x),0.2\u3009} and a world \u03c9 in which R(X1,M1), S(X1,M1), S(X1,M2) and T (X1) are True and the other ground PRVs are False. Then \u03b7({x,m},r(x,m)\u2227 s(x,m),\u03c9) = 1 as there is only one assignment of individuals to x and m (x = X1 and m = M1) for which r(x,m)\u2227 s(x,m) holds in \u03c9 and \u03b7({x,m},s(x,m)\u2227 t(x),\u03c9) = 2. Therefore: Pr(\u03c9) = 1 Z (exp(1\u22171.2)\u2217 exp(2\u22170.2)) An MLN can be conditioned on a PRV having no logical variables by replacing the PRV in all formulae of WFs with its observed value. Observations on individuals can be handled by a process called shattering. Example 2. Suppose for the MLN in Example 1 we observe that T (X1) and T (X2) are True. Since we have more information about X1 and X2 compared to other individuals in \u2206x, the individuals in \u2206x are no longer exchangeable. In order to handle this, we create two new logical variables x1 and x2 with \u2206x1 = {X1,X2} representing the individuals for which we have observed T is True, and \u2206x2 = {X3,X4,X5} representing the individuals for which we have not observed T . Then we create new WFs with our new logical variables as follows: {\u3008{x1,m},r(x1,m)\u2227 s(x1,m),1.2\u3009, \u3008{x2,m},r(x2,m)\u2227 s(x2,m),1.2\u3009, \u3008{x1,m},s(x1,m)\u2227 t(x1),0.2\u3009, \u3008{x2,m},s(x2,m)\u2227 t(x2),0.2\u3009} Then we replace t(x1) with True. For every logical variable x in the above MLN, the individuals in \u2206x are now exchangeable. This process is called shattering. We assume our input MLNs have been shattered based on the observations and refer interested readers to (de Salvo Braz, Amir, and Roth, 2005) for the details. An MLN can be also conditioned on some counts: the number of times a PRV with one logical variable is True or False. For a PRV T (x), we let Obs(T (x), i) represent a count observation on T (x) indicating T is True for exactly i out of |\u2206x| individuals. Example 3. Suppose for the MLN in Example 1 we observe Obs(T (x),2). We create two new logical variables x1 and x2 representing the subset of x having T True and False respectively, with |\u2206x1 | = 2 and |\u2206x2 | = 3.1 Then we create new WFs as in Example 2, and replace t(x1) with True and t(x2) with False. Search-based Lifted Inference Rules There are several rules that are used in search-based lifted inference algorithms. In this section, we describe some of these rules using examples. Lifted Decomposition Example 4. Consider the MLN in Example 1. On the relational level, all PRVs are connected to each other and we only have one connected component. On the grounding, however, for every individual Xi \u2208 \u2206x, we have the following WFs mentioning Xi: {\u3008{},r(Xi,M1)\u2227 s(Xi,M1),1.2\u3009, \u3008{},s(Xi,M1)\u2227 t(Xi),0.2\u3009, \u3008{},r(Xi,M2)\u2227 s(Xi,M2),1.2\u3009, \u3008{},s(Xi,M2)\u2227 t(Xi),0.2\u3009} Notice that the WFs mentioning Xi in the grounding are totally disconnected from the other WFs. Therefore, we have 1The domains can be assigned randomly due to the exchangeability of the individuals. |\u2206x| connected components that are equivalent up to renaming of the Xi individuals. In this case, x is called a decomposer of the network. Given the exchangeability of the individuals, the Z of all these connected components are the same. Therefore, we compute the Z for only one of these connected components, e.g., for an MLN with the following WFs, and raise it to the power of |\u2206x|. {\u3008{m},r(X1,m)\u2227 s(X1,m),1.2\u3009, \u3008{m},s(X1,m)\u2227 t(X1),0.2\u3009} In the above MLN, x has been replaced by one of its individuals. We refer to this as decomposing the MLN on logical variable x. While in this example only one logical variable is the decomposer, note that in general a set of logical variables can be the decomposer of an MLN. We point interested readers to (Poole, Bacchus, and Kisynski, 2011) for a detailed analysis of when a set of logical variables x is a decomposer of a network. Lifted Case Analysis Example 5. Consider the resulting MLN in Example 4 after being decomposed on x. We can find the partition function for this MLN by a case analysis on the values of a PRV. Suppose we do a case analysis on S(X1,m). Given that S(X1,m) represents |\u2206m| random variables in the grounding, one may think 2|\u2206m| cases must be considered: one for each assignment of values to the random variables. However, the individuals are exchangeable, i.e. we only care about the number of times S(X1,m) is True, not about the individuals that make it True. Thus, we only consider |\u2206m|+ 1 cases with the ith case being the case where for i out of |\u2206m| individuals S(X1,m) is True. We also multiply the ith case to (|\u2206m| i ) to take into account the number of different assignments to the individuals in \u2206m for which S(X1,m) is exactly i times True. The case analysis for this PRV will then be: Z(M) = \u2211m i=0 (|\u2206m| i ) Z(M|Obs(S(X1,m), i)) where M|Obs(S(X1,m), i) has the following WFs: {\u3008{m1},True\u2227 r(X1,m1),1.2\u3009, \u3008{m2},False\u2227 r(X1,m2),1.2\u3009, \u3008{m1},True\u2227 t(X1),0.2\u3009, \u3008{m2},False\u2227 t(X1),0.2\u3009} Removing False Formulae Example 6. Consider the resulting MLN in Example 5 after the case analysis on S(X1,m). The formulae of the second and the fourth WFs are equivalent to False and can be removed from the MLN. However, removing these WFs causes the random variables in R(X1,m2) to be totally eliminated from the MLN. To address the effect of these variables, we calculate the Z of the MLN having only the first and third WFs and multiply it by 2|\u2206m2 |, i.e. the number of possible assignments to the |\u2206m2 | random variables in R(X1,m2).", "creator": "LaTeX with hyperref package"}}}