{"id": "1606.04552", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "A New Approach to Dimensionality Reduction for Anomaly Detection in Data Traffic", "abstract": "essentially ubiquitous efficient management of high - volume feature - rich transactions in large networks offers significant challenges reducing delivery, transaction and transaction costs. as predominant thrust on reducing infrastructure costs is based thus performing dense binary mapping of the resource to his infinite - dimensional diagram such that a desired large block contains the terrain in the data is preserved in the low - norm representation. this browser - wise subspace approach to dimensionality reduction forces any fixed choice of the value of dimensions, is not responsive to real - world assumptions in observed traffic searches, something is incorrect : widespread gateway spoofing. based on theoretical problems proved against this chair, we propose a consistent distance - preserving boundary targeting dimensionality reduction motivated by the case that the real - madrid structural differences generated create linear characteristics of the sites within the normal routing pattern more relevant to anomaly detection than each structure encoding the computed data alone. our approach, basically the distance - based subspace method, allows a different use of reduced domains or different time windows and arrives at limitation the number of dimensions from making effective anomaly detection. manufacturers present centralized and distributed versions of any query additionally, using simulation intensive clustered traffic traces, supports the vast physical quantitative advantages of the terrain - based subspace approach.", "histories": [["v1", "Tue, 14 Jun 2016 20:29:50 GMT  (523kb)", "http://arxiv.org/abs/1606.04552v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.NI", "authors": ["tingshan huang", "harish sethu", "nagarajan kandasamy"], "accepted": false, "id": "1606.04552"}, "pdf": {"name": "1606.04552.pdf", "metadata": {"source": "CRF", "title": "A New Approach to Dimensionality Reduction for Anomaly Detection in Data Traffic", "authors": ["Tingshan Huang", "Harish Sethu", "Nagarajan Kandasamy"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n04 55\n2v 1\n[ cs\n.L G\n] 1\n4 Ju\nn 20\n16\nI. INTRODUCTION\nSecurity professionals who monitor communication networks for malware, errors and intrusions are increasingly dependent on real-time detection of anomalous behavior in the data traffic [1]. The volume of data one has to monitor and process for effective real-time management of networks and systems, however, poses significant Big Data challenges [2]. Further, many of the anomalies are apparent only upon an examination of the correlations in the traffic data from multiple locations. State-of-the-art network management in multiple application scenarios today, therefore, requires the constant monitoring of a very large number of features at hundreds of locations across a network and the ability to collect, transmit and process the data in real-time for near-instantaneous mitigation actions in case of anomalous patterns indicative of threats. For example, a data center may use embedded software and hardware sensors to measure various features of the data traffic across its hundreds of servers (along with other system states such as temperature, humidity, response time, throughput, processor utilization, disk I/O, memory, and other network activity) to react appropriately to anomalous events. Security administration of a large network may involve constant monitoring of its thousands of backbone systems, routers and links to detect a nascent denial-of-service attack or a developing outage event or a worm about to propagate itself.\nAuthor Huang is with Akamai Technologies, Cambridge, MA 02142. Authors Sethu and Kandasamy are with the Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA 19104, USA.\nThis work was partially supported by the National Science Foundation Award #1228847.\nA key step in network management and traffic analysis, therefore, is dimensionality reduction, which cuts down the number of observed features or variables into a smaller set that is sufficient to capture the information essential to the goals of network management. Principal Component Analysis (PCA) is a widely used tool that allows us to derive a reduced set of the most significant uncorrelated features that are linear combinations of the original set of features [3]. Given N features, one selects k \u226a N most significant principal components out of N to define a k-dimensional subspace based on observations of normal traffic (it is generally assumed that training or some reference traffic is available for purposes of comparing the observed traffic with it.) An underlying assumption in this approach is that a very small number of principal components capture most of the variance in the traffic. As a result, this approach typically chooses k as the number of principal components which capture a pre-defined percentage (say, 99%) of the variance in the normal traffic. Then, a significant deviation in the projection of the N-dimensional observed data onto this k-dimensional reference (normal) subspace can be defined as an anomaly for purposes of detection [4]. In this paper, we refer to this traditional approach to employing PCA for dimensionality reduction as the variance-based subspace method.\nThere are at least three weaknesses of the traditional variance-based subspace approach for anomaly detection: (i) the reduced number of principal components, k, is computed based on the structure of normal traffic when, actually, the structure of the changes between the observed and the normal traffic is more relevant for choosing the appropriate number of dimensions for anomaly detection; (ii) a static determination of k is inadequate at capturing real-time changes \u2014 the right number of dimensions is often different during different periods of time depending on the structure of both the normal and the observed traffic; (iii) the method allows only weak heuristics because the performance of anomaly detection is very sensitive to small changes in the number of dimensions chosen for the normal subspace [5]. Motivated by the need to address these weaknesses, this paper presents a new distance-based approach to dimensionality reduction for anomaly detection based on a new metric called the maximum subspace distance. In this paper, we refer to our approach as the distance-based subspace method.\nNormal and anomalous traffic tend to differ in the correlations between pairs of features of the traffic being monitored. These correlations, along with the variance of each monitored feature are entries in the covariance matrix. This work is additionally motivated by two observations: (i) anomalies lead to changes in the covariance matrix of the set of traffic features being monitored, and (ii) different types of anomalies cause different types of deviations in the covariance matrix allowing a categorization of the detected anomaly and an immediate prescription of actions toward threat mitigation [6]. Besides, use of the covariance matrix requires no assumptions about the distributions of the monitored features, rendering it a general method for traffic characterization. Rapid detection of structural differences between two covariance matrices, therefore, is an important goal in modern anomaly detection and is a key motivation behind the improved method of dimensionality reduction sought in this work.\nThe remainder of this paper is organized as follows. Section II defines the problem statement and introduces the contributions of this paper. Section III discusses related work in the use of covariance matrices and in subspace methods for anomaly detection. Section IV describes the assumptions and definitions underlying the problem and the solution approach used in the paper. Sections V and VI describe the centralized and distributed versions of our algorithms, respectively. Section VII describes simulation experiments using real traffic traces which illustrate the\nadvantages of anomaly detection using our distance-based subspace method. Finally, Section VIII concludes the paper."}, {"heading": "II. PROBLEM STATEMENT AND CONTRIBUTIONS", "text": "Let N denote the number of traffic features being monitored in the network. These N features could all be observations made at a single location in the network or could be from a multiplicity of locations where the traffic is being monitored. Assume that each feature collected or computed from the traffic corresponds to a designated time slot. We place no assumptions on the selection of traffic features.\nThe problem considered in this paper is one of finding an appropriately reduced set of k \u226a N dimensions in real-time, which may be different during different time intervals depending on the observed traffic at that time. The traditional approach using PCA determines k statically based only on the training data in order to extract a normal subspace of k dimensions representative of normal traffic, even though the best value of k may differ during different periods of time depending on the characteristics of the observed traffic. In contrast, our distance-based subspace method derives k based on the currently observed differential between the normal traffic deduced from training data and the monitored/observed traffic (as opposed to deriving both k and the normal subspace from training data alone). Before any traffic is observed, one may begin with the choice of a pre-determined k based on training data as in traditional methods \u2014 but, as soon as the covariance matrix of observed traffic is computed, our method allows the subsequent use of only the number of dimensions necessary at any given time instead of a static determination independent of current observations.\n1) Centralized approach: In the case in which all N features are observations at a single node, the covariance matrices can be readily calculated at the node itself. Also, in the case in which the N features are observations made at multiple nodes but transmitted and collected at a central monitoring station, the covariance matrices can be calculated at this station for analysis. In either of these two cases, we can characterize network traffic as a stream of covariance matrices, one for each designated window of time, and then use the observed changes in the covariance matrices to infer changes in the system status, anomalous or otherwise. Let \u03a3A and \u03a3B denote the two N \u00d7N covariance matrices that need to be compared to detect changes indicative of an anomaly. These two matrices could represent real traffic data during different time windows or one of them could be a reference matrix representing normal operation without anomalies. The problem becomes one of devising a measure of the structural difference using a parsimonious metric that allows efficient computation at reasonable accuracy while also serving as a reliable indicator of anomalies.\nThe contribution of this paper begins with the use of a new metric, which we call the maximum subspace distance, defined as the largest possible angle, \u03b8max, between the subspace composed of the first kA principal components of \u03a3A and the subspace composed of the first kB principal components of \u03a3B , for all possible values of kA and kB . The maximum subspace distance is defined in more detail in Section IV.\nFinding the subspace (i.e., all the principal components) given a covariance matrix is computationally expensive since it requires either singular value decomposition or eigenvalue decomposition. By avoiding the computation of the entire subspace, by using only the most significant principal components and by employing the theoretical insights proved in the Appendix, this\npaper contributes a fast approximate algorithm, called GETESD, to estimate a reduced number of principal components that is sufficiently effective at distinguishing between the two matrices being compared. This number of principal components \u2014 which we call the effective subspace dimension (ESD) \u2014 is such that it describes each of the two subspaces enough to gain a close estimate of the maximum subspace distance.\n2) Distributed approach: In the case in which the N features are sampled at multiple locations in the network, the covariance matrix of these features is not readily computable at any given location. If the feature data cannot all be transmitted to a central monitoring station due to communication costs or other reasons typical, we have only the raw features computed for every time slot based on which we need to estimate the maximum subspace distance. Let M denote the number of consecutive time slots for which these features have to be sampled in order to ascertain the second-order statistics of these features, i.e., the variances and correlations which populate the covariance matrix of these features. The input to the distributed algorithm at each node, therefore, is not a stream of covariance matrices but raw feature data in the form of a stream of vectors, each of length M .\nTo address this case, we present a distributed algorithm, called GETESD-D, which avoids the direct computation of the covariance matrices but nevertheless returns the effective subspace dimension and an estimate of the maximum subspace distance. The participating nodes in the GETESD-D algorithm deploy the gossip-based Gaussian mixture learning mechanism to estimate principal components of the traffic features [7]. The algorithm requires only local computation and some communication between neighboring nodes. The correctness of this algorithm is proved in the Appendix along with an analysis of its computational and communication complexity.\nBesides offering new theoretical insights into comparisons between covariance matrices, our work in this paper improves upon the traditional variance-based approaches to dimensionality reduction in three ways: (i) our attempt to reduce the number of dimensions is based on data from both of the two traffic streams being compared instead of depending only on some reference data for a normal traffic stream \u2014 this allows the use of a smaller and only the number of dimensions necessary instead of a static pre-determined number based on the training data alone, (ii) our methods allow a dynamic real-time computation of the structural changes in network traffic features and, therefore, allow better characterization and classification of attack traffic, and (iii) our method makes it significantly harder for attack traffic to spoof the structure of normal traffic in order to escape detection."}, {"heading": "III. RELATED WORK", "text": "The problem of anomaly detection often invites customized solutions particular to the type of anomaly that is of interest. For example, methods reported in [8] and [9] allow the detection of load anomalies in voice-over-IP traffic at the network or the application layer. Similarly, distributed denial-of-service (DDoS) attacks are the target of detection in [10], [11] and [12]. However, given that anomalies manifest themselves in multiple and sometimes uncommon or even unknown ways, a more practical solution to the problem of anomaly detection is a generic method that can detect anomalies of all kinds \u2014 common and uncommon, known and unknown.\nDeveloping general anomaly detection tools can be challenging, largely due to the difficulty of extracting anomalous patterns from huge volumes of high-dimensional data contaminated with anomalies. Early anomaly detection methods which used artificial intelligence, machine learning,\nor state machine modeling are reviewed in [13]. Examples of later work on developing general anomaly detection tools include [4], [6], [12], [14]\u2013[18]. There are two broad approaches to anomaly detection that are related to the work reported in this paper \u2014 those using covariance matrices directly and those using subspace methods, and occasionally those which use both these approaches together.\nWhile the covariance matrix plays a role in many anomaly detection methods, it was most directly used in [6] to detect flooding attacks based on comparisons between a covariance matrix under normal conditions (used as the reference matrix) and the observed covariance matrix. The covariance matrix is also used directly for anomaly detection in [15]. These methods do not necessarily compete but are complementary to the approach presented in this paper \u2014 while they address detection, our paper primarily addresses dimensionality reduction for detection. Given a reduced number of dimensions, one may choose any of many detection strategies which depend very strongly on the context and the application domain. Further, as opposed to methods in [6] and [15] which are based on detecting the changes in individual entries in the covariance matrix, our method is additionally able to exploit the underlying correlations between the changes in the entries to offer a more refined and a more reliable approach to anomaly detection.\nThe variance-based subspace method, based on PCA, was first proposed for anomaly detection in [4] and later improved in [14] to explore the deviation in the network-wide traffic volume and feature distributions caused by anomalies. To use this method online in real-time as described in [4], one processes each arrival of new traffic measurements using the matrix PP T , where P is composed of the top k principal components representing the normal traffic pattern. Therefore, for real-time detection using this method, it is necessary to use a training dataset to determine P before the detection process along with a certain choice of static k (as opposed to k being determined dynamically in our distance-based subspace method.) The scheme proposed in [4] would separate the high-dimensional space of network traffic into two subspaces: the normal subspace and the anomalous subspace. The normal subspace is low-dimensional and captures high variance of normal traffic data, thus modeling the normal behavior of a network. The projections of measurement data onto the anomalous subspace are used to signal, identify and classify anomalies.\nThe limitations of the variance-based subspace methods are discussed in [5]. The simulation results in [19] further confirm that the effectiveness of the subspace method depends strongly on the dimension chosen for the normal subspace. In addition, excessively large anomalies can contaminate the normal subspace and weaken the performance of the detector. Later work has improved upon the training process of the subspace method [20]\u2013[22], but choosing the appropriate dimension for the normal subspace has remained an unmet challenge. The distancebased methods used in the literature also present the same challenge where the reduced number of dimensions is not adaptive to real-time data [23]. The distance-based subspace method presented in this paper overcomes this difficulty by allowing the number of dimensions to be based on both the normal and the observed traffic under examination, thus adapting constantly to the changing patterns in the observed traffic to use only the number of dimensions necessary at any given time.\nIn other related work, PCA-based methods have been decentralized for a variety of purposes including anomaly detection [24]\u2013[28]. A distributed framework for PCA is proposed in [27] to achieve accurate detection of network anomalies through monitoring of only the local data. A\ndistributed implementation of PCA is developed for decomposable Gaussian graphical models in [28] to allow decentralized anomaly detection in backbone networks. Distributed gossip algorithms using only local communication for subspace estimation have been used in the context of sensor networks [29]\u2013[31]. Our work in this paper extends the distributed average consensus protocol proposed in [31] to estimate the principal subspace for the context of anomaly detection in network traffic data."}, {"heading": "IV. THE METRIC", "text": "Let N denote the number of features in the dataset of interest and let \u03a3A and \u03a3B denote the two N \u00d7N covariance matrices to be compared. Let a1, . . . , aN and b1, . . . , bN denote the eigenvectors of \u03a3A and \u03a3B , respectively. In the following, the operator symbol \u2018\u00d7\u2019 used between two matrices or vectors denotes the matrix product."}, {"heading": "A. The subspace distance", "text": "Let \u03b8kA,kB(A,B) denote the angle between the subspace composed of the first kA principal components of \u03a3A, a1, . . . , akA , and the subspace composed of the first kB principal components of \u03a3B , b1, . . . , bkB . We refer to this angle between the subspaces as the subspace distance, which has a range between 0 to 90 degrees. We have:\nsin \u03b8kA,kB(A,B) = \u2016 TkA,kB(A,B) \u2016 (1)\nwhere \u2016 \u00b7 \u2016 is the matrix norm and TkA,kB(A,B) is the part of [b1, . . . , bkB ] orthogonal to [a1, . . . , akA]. Therefore,\nTkA,kB(A,B) = (I \u2212\nkA \u2211\ni=1\nai \u00d7 a\u2032i)[b1, . . . , bkB ] (2)\n= ( N \u2211\ni=kA+1\nai \u00d7 a\u2032i)[b1, . . . , bkB ]. (3)\nThe sine of the subspace distance captures the norm of [b1, . . . , bkB ] not including its projection in [a1, . . . , akA]. The more distinguishable or orthogonal these two subspaces are to each other, the larger the subspace distance between them."}, {"heading": "B. The maximum subspace distance", "text": "To compare the two matrices, we quantify the difference between \u03a3A and \u03a3B as the maximum value of the angle \u03b8kA,kB(A,B), where 1 \u2264 kA \u2264 N and 1 \u2264 kB \u2264 N :\n\u03b8max = max 1\u2264kA,kB\u2264N \u03b8kA,kB(A,B) (4)\nIn this paper, we refer to \u03b8max as the maximum subspace distance, which serves as our metric for anomaly detection and quantifies the difference between the two matrices.\nWhen kA and kB hold values that maximize \u03b8kA,kB(A,B), the two sets of principal components, [a1, . . . , akA] and [b1, . . . , bkB ], can be thought of as the distinguishing characteristics of covariance matrices \u03a3A and \u03a3B . Once the maximum subspace distance and the corresponding\npair of dimensions are found, they can be used to characterize the two datasets with \u03a3A and \u03a3B as their covariance matrices, and distinguish these two datasets from each other. We show in Section VII how these two sets of characteristics can be employed for anomaly detection.\nOur proposed metric of subspace distance is new, and the closest related metric in the literature is the principal angle [32]. The subspace distance, as a metric, is different from principal angle in several ways. Most importantly, the subspace distance considers the order in importance of each principal component while the principal angle does not. In addition, unlike in the case of the subspace distance, the values of the principal angles are not necessarily dependent on the principal components.\nThe two sets of principal components used in the definition of the subspace distance capture the linear correlated pattern of the two datasets. In addition, the subspace distance depends on the order of these principal components which represents their order of importance. As a result, our metric of the maximum subspace distance serves as a dependable measure of the magnitude of changes in the pattern between any two datasets."}, {"heading": "V. THE CENTRALIZED ALGORITHM", "text": "Given the high computational cost of finding the maximum subspace distance, in this section, we present an algorithm which estimates the metric at sufficient accuracy for successful anomaly detection. Our solution is based on four key ideas: (i) allowing kA = kB in our search for the maximum subspace distance, \u03b8max, (ii) reducing the problem to one of always finding only the first principal component of a matrix, (iii) using the power iteration method to approximate the first principal components, and finally, (iv) using a heuristic to approximately ascertain the value of \u03b8max."}, {"heading": "A. The rationale behind allowing kA = kB", "text": "In the approach presented in this paper, we limit our search for \u03b8max to only the cases in which kA = kB . Our rationale is based on Theorem 1 below. The proof is in the Appendix.\nTheorem 1. If for some kA and kB, \u03b8kA,kB(A,B) = \u03b8max, then there exists k, 1 \u2264 k \u2264 N , such that \u03b8k,k(A,B) = \u03b8max.\nAllowing kA = kB to find the maximum subspace distance reduces the search space from N2 to N . We refer to the value of k for which \u03b8k,k(A,B) is the maximum subspace distance as the optimal subspace dimension."}, {"heading": "B. Subspace distance and the projection matrix", "text": "We define an N \u00d7N projection matrix P from {b1, . . . , bN} to {a1, . . . , aN} as:\nPi,j = \u3008ai, bj\u3009 = a\u2032ibj (5)\nwhere \u3008\u00b7, \u00b7\u3009 represents dot product of two vectors. According to Theorem 2 below, which we prove in the Appendix, the smallest singular value of its submatrix P1:k,1:k, consisting of the first k rows and k columns of P , is equal to cos(\u03b8k,k(A,B)).\nTheorem 2. If an N\u00d7N matrix P is built as in Eq. (5), and its submatrix P1:k,1:k has \u03c3k(P1:k,1:k) as its smallest singular value, then \u03c3k(P1:k,1:k) = cos(\u03b8k,k(A,B)).\nTo understand the result in Theorem 2, one can refer to the fact that the singular values of a projection matrix P1:k,1:k are the scaling factors of projecting b1, . . . , bk onto a1, . . . , ak via matrix P1:k,1:k along different axes. For example, the largest singular value of P1:k,1:k shows the largest projection from b1, . . . , bk onto a1, . . . , ak, which results in the smallest angle between these two subspaces. The axis in a1, . . . , ak corresponding to the largest singular value is most parallel to b1, . . . , bk. Similarly, the smallest singular value results in an axis in a1, . . . , ak that is most orthogonal to b1, . . . , bk, and the resulting angle is the exact definition of the subspace angle.\nIt can be shown, as stated in Theorem 3 below, that by increasing the subspace dimension k, the largest projection from b1, . . . , bk onto a1, . . . , ak is non-decreasing with k and has 1 as its maximum value. In other words, the axis in a1, . . . , ak becomes more parallel to b1, . . . , bk with increasing k.\nTheorem 3. If an N\u00d7N matrix P is built as in Eq. (5), and its submatrix P1:k,1:k has \u03c3k(P1:k,1:k) as its smallest singular value and \u03c31(P1:k,1:k) as its largest singular value, then:\n\u03c31(P1:k\u22121,1:k\u22121) \u2264 \u03c31(P1:k,1:k), k = 2, . . . , N (6)\n\u03c31(P1:k,1:k) \u2264 1, k = 1, . . . , N (7)\nThe proof of Theorem 3 is in the Appendix. The results in Eqs. (6) and (7) indicate that when \u03c31(P1:k,1:k) is already close enough to 1, further increasing the number of principal components can only slightly increase the ability to differentiate between the two subspaces using the maximum subspace distance as the metric."}, {"heading": "C. Estimating the optimal subspace dimension", "text": "Based on our results as stated in Theorems 1\u20133, we develop an estimation algorithm for the optimal subspace dimension. It is straightforward to find the optimal subspace dimension by obtaining a1, . . . , aN and b1, . . . , bN first, computing \u03b8k,k(A,B) for every k from 1 to N and determining the k for which \u03b8k,k(A,B) is the maximum. However, we are interested in a less computationally expensive method that does not require full knowledge of a1, . . . , aN and b1, . . . , bN . Furthermore, our interest is in searching for a smaller k, which we refer to as the effective subspace dimension (ESD), corresponding to a subspace distance close enough to the maximum such that this value of k is sufficiently effective at distinguishing between the two subspaces.\nOur algorithm, which we call the GETESD algorithm, relies on the results stated in Theorems 1-3. Firstly, because of Theorem 1, we are able to limit our search of the optimal subspace dimension to cases in which kA = kB = k. Secondly, since the values of subspace distance, \u03b8k,k(A,B), depend on singular values of submatrix P1:k,1:k, as stated in Theorem 2, we can compute the subspace distance \u03b8k,k(A,B) with only the knowledge of the first k eigenvectors of \u03a3A and \u03a3B . In other words, if we have already tried values of k from 1 to K, the pair of (K + 1)-th eigenvectors is all the additional information we need to get \u03b8K+1,K+1(A,B).\nFinally, because of the property of singular values of submatrix P1:k,1:k, as stated in Theorem 3, \u03c31(P1:k,1:k) is non-decreasing to 1. The closer \u03c31(P1:k,1:k) is to 1, the less distinguishable these\ntwo subspaces become. As a result, we set a threshold on \u03c31(P1:k,1:k), and stop the algorithm when \u03c31(P1:k,1:k) is above the threshold 1 \u2212 \u01eb. Of course, a higher threshold leads to a wider search range and a closer approximation of the optimal subspace dimension.\nOverall, our algorithm computes an approximate value of \u03b8k,k(A,B) beginning with k = 1 and increases k at each step; the algorithm stops when the subspace distance, \u03b8k,k(A,B), has dropped and the largest singular value of P1:k,1:k is in a pre-defined neighborhood of 1. This observed \u03b8k,k(A,B) is used as the estimated maximum subspace distance and the corresponding k becomes the effective subspace dimension.\nFig. 1 presents the pseudo-code of our algorithm, GETESD, which returns ESD, the effective subspace dimension, and \u03b8max, the estimated maximum subspace distance, given two covariance matrices, \u03a3A and \u03a3B , and \u01eb, which defines the threshold at which the algorithm stops. Note that the k-th eigenvectors of \u03a3A and \u03a3B are not computed until we are in the k-th iteration of the while loop. Our implementation of this algorithm uses the power iteration method [33]. For each iteration in our algorithm, we add one row and one column to the projection matrix along with the updated pair of eigenvectors. During the k-th iteration of the while loop, we compute ak and bk, i.e., the k-th eigenvectors of \u03a3A and \u03a3B , using the power iteration method. We then construct P1:k,1:k by adding row vector [a\u2032kb1, . . . , a \u2032 kbk\u22121] to the bottom of P1:k\u22121,1:k\u22121, and column vector [a\u20321b \u2032 k, . . . , a \u2032 kb \u2032 k] to the right. We then use the power iteration method again to calculate the singular values \u03c31(P1:k,1:k) and \u03c3k(P1:k,1:k). The algorithm is run until the aforementioned stopping criteria is met.\nThe GETESD algorithm achieves a significantly higher accuracy over our previous work, a heuristic [34]. As reported in [34], the heuristic is able to estimate the subspace distance with a percentage error which frequently reaches above 1% and sometimes close to 5%. For the same data, however, the percentage error using the GETESD algorithm never exceeds 0.051%, a significant improvement."}, {"heading": "D. Complexity analysis", "text": "By the naive approach, we would first calculate the two sets of eigenvectors and then the subspace distance for every possible pair of (kA, kB) to ascertain the maximum subspace distance. The complexity of calculating the two sets of eigenvectors is O(N3). The complexity of calculating the subspace distance for every possible pair of (kA, kB) is N2 \u2217 O(N3) = O(N5). The computational complexity of the naive method which computes all eigenvectors to find the optimal subspace dimension, therefore, is O(N5).\nTheorem 4 states the computational complexity of the GETESD algorithm.\nTheorem 4. The GETESD algorithm in Fig. 1 for N nodes has a computational complexity of O(Zk3+k2N), where k is the resulting effective subspace dimension and Z is the upper bound on the number of iterations in the power method.\nWe prove Theorem 4 in the Appendix. This shows that the GETESD algorithm is significantly more computationally efficient compared with the naive method which computes the exact optimal subspace dimension. If a variance-based approach were to be used in each time window to dynamically compute the number of dimensions, the time complexity (determined by having to compute the full eigenvector) would be O(N3), significantly higher than that for the GETESD\nalgorithm. We will demonstrate in Section VII that the approximate result computed by the GETESD algorithm suffices to detect anomalies."}, {"heading": "VI. DISTRIBUTED ALGORITHM", "text": "When a multiplicity of points in a network are being monitored for anomalies, the data required for the computation of covariance matrices has to be collected at multiple nodes, some of them geographically distant from each other. The centralized algorithm in Section V would require a monitoring station to collect all the data from the collection nodes, compute the covariance matrices, and then process the stream of matrices for anomalies. The prohibitive logging and communication costs in this case form the motivation behind our distributed algorithm to allow an estimation of the maximum subspace distance without a central station.\nIn the distributed case, no node can possess the knowledge of the entire covariance matrix (which requires all of the data from all of the nodes) but each node can update and maintain its corresponding entry in the eigenvector. As a result, each node will need to collaborate with its neighbors to perform eigenvector/eigenvalue estimation in a distributed fashion."}, {"heading": "A. Assumptions and system model", "text": "Let N be the total number of features being monitored. In practice, different nodes will have different numbers of features to monitor. However, for purposes of simplicity and clarity, we will illustrate our distributed algorithm assuming there is one feature monitored at each node \u2014 so, in this section, we will also assume that there are N nodes. As described in Section II-2, let M denote the number of consecutive time slots for which each of these N features is sampled to compute a covariance matrix (any two features have to be observed for a certain length of time to ascertain the correlation between them). The input to the distributed algorithm, therefore, is a stream of vectors of length M each, with each entry being a feature sampled at a certain time slot.\nCollectively, these vectors form an N \u00d7M matrix, X , with data collection of the n-th feature xn available only at node n (here, xn is the n-th row of raw data). For an eigenvector v of X , vn is estimated and stored at the n-th node.\nWe also assume a weighted connection matrix W associated with an undirected graph composed of these N nodes. Besides a self-loop for each node, an edge exists between any two nodes as long as they can communicate with each other directly (we make no assumptions on the protocol layer for this communication, although the detection algorithm will achieve better\nperformance at the IP layer than at the application layer). Each edge is associated with a weight, and the total weight of all edges connected to the same node is one. This makes the column sums and row sums of W equal to one.\nWe also make use of a distributed routine to compute the average of certain values at all nodes but one which runs only locally at each node. This routine, a straightforward extension of the distributed average consensus algorithm in [31] to accommodate for the constraints placed on row sums and column sums of W , is shown in Fig. 2. If each of the N nodes holds the value xn, n = 1, . . . , N , this routine computes the average m = 1N \u2211N n=1 xn at each node locally."}, {"heading": "B. Distributed power iteration method", "text": "The goal of a distributed power iteration method is to allow nodes that are remotely located to cooperate in their estimation of the most significant principal component of a dataset. Fig. 3 presents the pseudo-code for a centralized power iteration method which estimates the first principal component v given a covariance matrix C. The method rests on the fact that the first eigenvalue of a given matrix C can be estimated by limk\u2192\u221e C kv \u2016Ckv\u2016 , where v is a random vector. We refer readers to [33] for more details about the power iteration method. In the following, we develop and describe a distributed version of the power iteration method. First, we describe how power iteration can be performed without involving the covariance matrix C. In the k-th step of the centralized power iteration method shown in Fig. 3, v(k+1) needs to be updated by multiplying with C. In the absence of C, v(k+1) can be computed as\nv(k+1) = Cv(k) = 1\nM \u2212 1 XX \u2032v(k)\n= 1\nM \u2212 1 X [x\u20321 . . . x \u2032 N ]v\n(k) = 1\nM \u2212 1 X\nN \u2211\ni=1\nx\u2032iv (k) i\nThe n-th entry of the estimated principal component at (k + 1)-th step is:\nv(k+1)n = 1\nM \u2212 1 xnz(k) (8)\nwhere z(k) = \u2211N n=1 x \u2032 nv (k) n . Since the n-th node has access to xn and x\u2032nv (k) n , it is able to compute v (k+1) n after it uses the average consensus process to get z(k). The norm of v(k) and v(k+1) \u2212 v(k) can be achieved by applying average consensus on (v(k)n )2 and (v (k+1) n \u2212 v (k) n )2.\nOur distributed version of the power iteration method is shown in Fig. 4. Note that the input to this algorithm is xn, raw data visible to the node running the algorithm, with the method requiring no knowledge of covariance matrix C. Besides xn, the algorithm takes an additional input, x\u0302n, which we explain in Section VI-D. For now, it suffices to know that x\u0302n = xn for the estimation of vn. The estimated principal component is normalized before it is returned, and each node only needs to estimate one entry of v(k) in each step."}, {"heading": "C. Evaluation of convergence and accuracy", "text": "In this subsection, we briefly report on the convergence of the distributed power iteration method and its accuracy. We use real data traffic traces \u2014 anonymized passive traffic traces from one of CAIDA\u2019s monitors on high-speed Internet backbone links, equinix-chicago-dirA [35]. Use of real traffic traces allows an examination of the algorithms in real contexts where there is always some degree of both noise and unmarked anomalies present in the data.\nWe consider the histogram of packet sizes and protocol of the packets flowing over a single link within an interval of 25ms. We use 75 bins for packet sizes and 4 for protocol types, leading to a feature vector of length 79. For the trace lasting 1 minute, we obtain around 2400 examples of the feature vectors and use them to calculate a covariance matrix of these 79 features.\nFor lack of publicly available traffic traces paired with a large network graph, we now lay this dataset on a simulated network graph composed of 79 nodes, generated using the scale-free Baraba\u0301si\u2013Albert (BA) model where, after initialization, each new node added to the network is attached to 2 existing nodes. To clearly isolate the performance of the distributed power iteration method, we assume a simple scenario with each node responsible for the collection of one type of feature and the computation of one histogram. The weights for edges, including self loops,\nare randomly chosen to satisfy the requirement that the weight matrix has unit row sums and unit column sums.\nSince the family of average consensus algorithms is already well-studied [31], we focus here on the distributed power iteration method assuming that the distributed average consensus method generates an answer within an error margin of 5%.\nWe use two metrics to evaluate the estimation result of our algorithm for distributed power iteration. The first, projection bias, quantifies the bias of the estimated principal component in the direction of the actual principal component. The second metric, mean squared error (MSE), quantifies the closeness of the estimate to the actual principal component. Fig. 5 shows these metrics for the distributed power iteration method using simulation results for varying numbers of iterations.\nSince the actual principal component is not available to our algorithm, it does not know to stop when both the projection bias and MSE are the lowest (as at iteration 7 in Fig. 5), but continues on to converge to some value other than the better answer it had already found. However, the algorithm does converge at an estimate which is the same as that estimated by the centralized version when all of the data is available to all the nodes (in our example in Fig. 5, this happens after iteration 12.) Since the centralized version of our algorithm also stops at an approximation, neither of the two versions arrives at exactly the correct answer but they both achieve a very low error in their estimates. Note that Fig. 5 also offers a comparison in the accuracy achieved by the centralized and the distributed versions, since the estimation of the principal component is what differentiates the GETESD and GETESD-D algorithms. The accuracy of the distributed GETESD-D algorithm given enough iterations to allow convergence of distributed power iteration is the same as the accuracy achieved by the centralized GETESD algorithm.\nWhile the number of required iterations depends on the threshold for the stopping condition, we find that, on average, the result converges after around 10 distributed power iterations and that an equivalent or better result is achieved with as few as 6 iterations. Fig. 5 also shows that the estimated principal component is almost in the same direction as the actual principal component with a mean squared error below 1%. Finally, it is worth noting that by exploiting results from\niterations in previous time windows, one can effectively reduce the number of iterations per time window to an average of 1.\nD. The GETESD-D algorithm\nIn this section, we present the GETESD-D algorithm, the decentralized implementation of our algorithm in Fig. 1. In each iteration of the while loop, the first principal component and the corresponding eigenvalue are estimated by the distributed power iteration method.\nAfter obtaining ak and bk, P1:k,1:k can be constructed as follows. Since Pi,j = \u2211N\nn=1 ai(n)bj(n), entries of row vector [a\u2032kb1, . . . , a \u2032 kbk\u22121] and column vector [a \u2032 1bk, . . . , a \u2032 kbk]\n\u2032 can be calculated by calling the distributed algorithm for average consensus on ai(n)bj(n). Then we can add the row vector [a\u2032kb1, . . . , a \u2032 kbk\u22121] to the bottom of P1:k\u22121,1:k\u22121, and column vector [a \u2032 1bk, . . . , a \u2032 kbk] \u2032 to its right to construct P1:k,1:k. Note that at the end of each while loop of Fig. 1, the part corresponding to ak and bk are subtracted from covariance matrices \u03a3\u0302A and \u03a3\u0302B . Without the subtraction, the eigenvector v(k+1) can be equivalently estimated using distributed power iteration as follows. Let ak be the estimate for the k-th eigenvector of dataset X , then\nv(k+1) = (C \u2212 aka\u2032kC)v (k)\n= Cv(k) \u2212 1\nM \u2212 1 aka\u2032kXX \u2032v(k)\n= Cv(k) \u2212 1\nM \u2212 1 ak\n(\nN \u2211\ni=1\nak(i)xi\n)(\nN \u2211\nj=1\nx\u2032nv (k) n\n)\n= Cv(k) \u2212 1\nM \u2212 1 ak\n(\nN \u2211\ni=1\nak(i)xi\n)\nz(k),\nwhere z(k) is defined as in Eq. (8). In particular, the n-th entry of the eigenvector v(k+1) can be estimated by:\nv(k+1)n = ( Cv(k) ) n \u2212\n1\nM \u2212 1 ak(n)\n(\nN \u2211\ni=1\nak(i)xi\n)\nz(k)\n= 1\nM \u2212 1 xnz(k) \u2212\n1\nM \u2212 1 ak(n)\n(\nN \u2211\ni=1\nak(i)xi\n)\nz(k)\n= 1\nM \u2212 1 (xn \u2212 ak(n)f(ak, X))z(k)\nwhere f(ak, X) = \u2211N\ni=1 ak(i)xi. As a result, a copy of the readings at node n, denoted by x\u0302n, can be updated using xn \u2212 ak(n)f(ak, X) to allow equivalent estimation as in Fig. 1.\nThe distributed algorithm that combines the aforementioned computations is shown in Fig. 6."}, {"heading": "E. Complexity analysis", "text": "Theorem 5 below, proved in the Appendix, addresses the computational complexity the GETESDD algorithm.\nTheorem 5. The computational complexity of GETESD-D is O(kpM\u2206S) for M measurements at each node, where S and p are upper bounds on the number of steps it takes for the convergence of average consensus and the GETESD-D methods, respectively.\nNote that in a central setting, the computational cost for calculating covariance matrix is O(N2M), and is O(ZN2) for Z steps of the power method. The messages with information\nabout principal components to all nodes would be O(N3) in the best case and O(N4) in the worst case.\nTheorem 6 below, also proved in the Appendix, addresses the communication complexity the GETESD-D algorithm.\nTheorem 6. The communication complexity of GETESD-D is O(kM\u2206+kp\u2206S) for M measurements at each node in a network with a maximum degree of \u2206, where S and p are upper bounds on the number of steps it takes for the convergence of average consensus and the GETESD-D methods, respectively.\nNote that the number of steps for the average consensus process, S, depends on the ratio of \u03bb2(W )/\u03bb1(W ), the second and first eigenvalues of W : a smaller ratio results in a smaller S. For a sparse graph, such a condition can be easily met. Similarly, the upper bound on the number of iterations in the distributed power method, p, relies on the ratio of \u03bb2(C)/\u03bb1(C), the second and first eigenvalues of C. In our tests with CAIDA traffic data [35], the ratio is as small as 3E\u20134, which means that only a couple of iterations for a 7000-dimensional dataset are enough to reach convergence."}, {"heading": "VII. EFFECTIVENESS OF ANOMALY DETECTION", "text": "Assume the dimension of data is N . Using training data, a normal subspace Uk is then constructed with the top k \u226a N principal components. Since most of the normal traffic patterns lie within the normal subspace Uk, projections of data with no anomalies onto the normal subspace should retain most of the data. However, projections of anomalous data, whose characteristics do not fall entirely within the aforementioned normal subspace, would retain only a portion of the observed data. Therefore, for purposes of anomaly detection using the variance-based subspace method, the projection residual of any given length-N vector x, denoted by r(x), is calculated as follows to yield a metric of the anomaly:\nr(x) = \u2016 (I \u2212 UkUTk )x \u2016,\nwhere I is the identity matrix. The detection process simply compares the projection residual with a pre-defined threshold and triggers an alarm if it is above the threshold [14].\nThe projection residual is the residual of the data after it has been projected onto the normal subspace: if the data is normal, the projection residual should be small relative to the case when the data consists of anomalies. In this paper, we will use the projection residual as a measure of the degree of anomalousness in the traffic. Note that in the distributed scenario, the projection residual can be computed at each node by calling the average consensus method on Uk(n, :)xn."}, {"heading": "A. Anomaly detection using the projection residual", "text": "To test our distance-based subspace approach for anomaly detection, we need labeled traffic data with connections that are already identified as either normal or anomalous. We use the Kyoto2006+ dataset, which includes 24 features of raw traffic data obtained by honeypot systems deployed at Kyoto University [36]. Traffic features include, among others, source/destination IP address and port numbers of a connection, its duration, and bytes that are received/sent. Four additional features are included for each connection, indicating whether the session is part of an\nattack or not. In our simulation, we use these 4 features for verification and the other 20 features for data analysis.\nSince raw features often show low correlation for useful anomaly detection, we extract the entropy of each traffic feature within non-overlapping time windows and use the length-20 entropy vector for each time window as the new traffic feature. In the dataset, there are about 100 new connections every minute, with most connections lasting around 1 minute. As a result, we use a time window lasting 5 minutes and extract the entropy of around 500 connections in each time window. The choice of a window size for optimal effectiveness depends not only on the traffic intensity but also on the features being monitored \u2014 extracting some features may require more traffic history than certain others. However, a larger window size need not increase the delay in anomaly detection, except upon start-up. To detect anomalies as rapidly as possible, one can use our algorithms with sliding (overlapping) time windows to update the sample covariance matrix and run the detection every time the window moves.\nIn subspace methods, we use training data to construct a subspace consisting of its first few principal components, which is assumed to be the normal subspace. We then project the observed/test data, which may contain varying numbers of anomalous connections at any given time, onto this normal subspace. This yields the projection residual, our metric of anomalousness. In our experiments using the labeled Kyoto data, we generate the training data using only the connections that are not labeled as anomalous. The test data, on the other hand, contains a varying number of anomalous connections.\nFig. 7 plots the projection residual of both the training data and the test data, using both the variance-based and the distance-based subspace methods. In case of the variance-based approach, we use the number of dimensions required to capture 99.5% of the variance in the training data as the number of dimensions for the normal subspace. In case of the distance-based approach, for each test dataset (comprised of multiple time windows), we derive a new effective subspace dimension (ESD) to serve as the dimension of the normal subspace. In Fig. 7, the number of\ndimensions of normal subspace used for the projections is 11 in the case of the variance-based approach and a lower number, 8, in the case of our distance-based approach. As Fig. 7 shows, through qualitative observation, that both the variance-based and the distance-based subspace methods are able to distinguish between test data (which contains anomalous connections) and the training data (which contains no anomalous connections)."}, {"heading": "B. Hit rate and false alarm rate", "text": "To evaluate our distance-based algorithm for anomaly detection, we use the Receiver operating characteristic (ROC) metric to characterize the number of anomalous datapoints in the Kyoto2006+ dataset that can be detected using the variance-based and distance-based subspace method. We compare the projection residual of the Kyoto2006+ dataset with a threshold, and use the resulting hit rate and the false alarm rate (or the false positive rate) to make a quantitative assessment of the effectiveness of anomaly detection. The hit rate is defined as the percentage of anomalous connections whose projection residual exceeds a certain threshold. The false alarm rate is defined as the percentage of non-anomalous connections whose projection residual exceeds the threshold. The hit rate and false alarm rate varies when the threshold changes.\nWe illustrate the comparative performance of variance-based and distance-based subspace methods using ROCs in two ways. Note that, as shown in Fig. 7, the projection residuals between the anomalous connections and the normal connections are distinctive for both of these two subspace methods. As a result, we can achieve 100% hit rate with zero false alarm rate when a proper threshold is used for detection (between 2.33 and 2.70 for the variance-based method, and between 6.69 and 7.03 for the distance-base method.) Fig. 8 shows the ROC comparison between the variance-based and distance-based subspace methods for various thresholds in this range. The results show that the distance-based subspace method achieves a better performance since it yields the same hit rate and false alarm rates even though it uses a reduced number of dimensions (8 compared to 11.)\nA second way to compare ROCs is by choosing a threshold such that the false alarm rate is fixed at a certain percentage, say 1%, and then make comparisons between the hit rate under variance-based and distance-based methods. Fig. 9 shows the resulting hit rate corresponding to different numbers of normal subspace dimensions when the false alarm rate is fixed at 1%. When the normal subspace dimension lies between 8 and 14, the hit rate is 100%. However, when the number of dimensions used for the normal subspace lies outside this range, the performance\nis worse. In the variance-based subspace method, the normal subspace dimension used for projections and, consequently, the hit rate, can vary significantly with the choice of the percentage of variance in the training data that the normal subspace is designed to capture (as suggested by Fig. 9). On the other hand, the distance-based subspace method depends on the structural changes in the test data and uses the smallest number of dimensions that achieves good performance (as also suggested by Fig. 9).\nThes results in Figs. 8 and 9 demonstrate a key advantage of the distance-based subspace method over the variance-based subspace methods. Instead of using a pre-set number of dimensions based only on the training data, the distance-based approach uses only the number of dimensions that approximately maximizes the distinction between the test data and the normal subspace. For this real-traffic trace, the variance-based approach may use as many as 14 (in this example, it uses 11 to capture 99.5% of the variance), while the distance-based approach uses only 8 dimensions with no appreciable reduction in the hit rate. The advantage lies in the fact that, unlike the variance-based approach, the distance-based approach can adapt to the characteristics of the test data to use only the number of dimensions necessary.\nThe benefit of using only the number of dimensions necessary is the ability to achieve a reduced runtime without compromising anomaly detection. To show this advantage of the distancebased method for real-time detection, we evaluate the average running time for computing the projection residual for the distance-based and variance-based subspace methods. The simulation was implemented in MATLAB and executed on a server equipped with an AMD Athlon II 3.0 GHz processor. Using synthetic datasets with increasing number of dimensions, we show in Fig. 10 that as the dimension of the dataset increases, the distance-based method is more efficient in reducing the overhead of computing the projection residual. More specifically, the runtime\nwith the distance-based method is less than half that with variance-based method as the number of features in the dataset approaches 5000, a very realistic scenario in network management.\nC. Information content of residuals\nBesides the real-time adaptability, the distance-based subspace approach offers a second key advantage over the variance-based approach \u2014 the projection residual in the distance-based approach carries more useful information toward successful anomaly detection. To illustrate this, we will use the labels (which identify a connection as part of attack traffic or not) in the Kyoto dataset to calculate the percentage of anomalous connections within each time window, and refer to this percentage as the anomalous rate.\nFig. 11 offers a comparison between the projection residual computed for each time window in the test data with the anomalous rate within the same time window. One interesting and illustrative observation that emerges is that, in general, the anomalous rate tends to somewhat track the projection residual but not consistently. A sharp and prolonged increase in the anomalous rate during time windows 55\u201365 leads to a reduced projection residual in the case of distance-based approach, while in the case of variance-based approach, there is no appreciable change.\nFor a deeper understanding of this phenomenon, we examined each of the principal components of the test data and the normal subspace in the time windows 55\u201365. The second and the ninth principal components of the training data are pertinent here and projections of the test data on these principal components are plotted in Fig. 12. In time windows with a high anomalous rate, we find that the anomalous patterns are largely along the second principal component of the normal subspace. However, it also deviates from the normal subspace in the less significant principal components, as shown in the figure for the ninth principal component.\nThe variance-based approach, because of its static use of the number of dimensions based only on training data, does not offer any hint of the rise in anomalous traffic through the projection residual. The distance-based approach, on the other hand, shows a sudden change in the projection\nresidual offering the potential for a richer set of heuristics for anomaly detection. The distancebased subspace method, by capturing more information about the test data in its dimensionality reduction, also transfers some of this information into the pattern of changes in the projection residual.\nHeuristics for anomaly detection can be further enhanced in the case of distance-based subspace method because the algorithm, in the process of computing the effective subspace dimension, also computes the principal components of the test data. Periodic analysis of these principal components can offer deeper insights into anomalous behavior and can, in fact, as shown in Section VII-D, even help detect malicious traffic which tries to mimic normal traffic in its principal components."}, {"heading": "D. Overcoming normal traffic spoofing", "text": "Just like cryptographers are required to assume that attackers have knowledge of the cryptographic algorithm, system security engineers have to assume that attackers will have knowledge of the normal subspace used for anomaly detection. We define a traffic spoofing attack as one in which an attacker deliberately engineers the profile of the anomalous traffic so that its most significant principal components align with those of the normal subspace. The variancebased approach, because its dimensionality reduction is based only on the normal subspace, is vulnerable to this type of spoofing attack. The distance-based approach, however, is able to resist this attack better; because of its dependence on the principal components of the test data, an attacker needs to spoof not just the principal components of the normal subspace but also in the right order, which presents the attacker a higher hurdle to cross.\nSuppose an anomaly detection system uses the histogram of packet sizes and protocols as traffic features. In order to launch a denial-of-service attack that evades detection, the attacker can design the size of attack packets such that the attack traffic follows a subset of the principal components in the normal subspace. But, assume that the order of the principal components of the attack traffic differ from those of the normal traffic, since preserving the order would be a more difficult feat for an attacker.\nSuppose the normal dataset (BEFORE), without any anomalies, has N \u00d7N covariance matrix \u03a3B = U\u039bBU \u2032, where the principal components are given by U = V \u2032 = [u1, . . . , uN ] with random\northonormal columns, and \u039bB is a diagonal matrix. Assume that the variance-based method uses a threshold percentage of captured variance such that all four of these principal components are in the normal subspace. Using 99.5% as the minimum variance that should be captured in the variance-based approach, the normal subpace dimension with the variance-based approach is 8. Assume the injected anomalous traffic behaves as the 4-th principal component in the normal subspace, and the resulting covariance matrix (AFTER) becomes \u03a3A = V \u039bAV \u2032, where \u039bA is also a diagonal matrix. Let the principal components of \u03a3A be given by V = [v1, . . . , vN ], and\nvn =\n\n\n u4 if n = 3 u3 if n = 4 un otherwise.\nIn other words, the injected anomalous traffic changes the order of importance of these two principal components, u3 and u4, both of which resides in the normal subspace.\nUpon simulation of this setting, we find that the effective subspace dimension using the distance-based method is 3. Fig. 13 presents the projection residual before and after such anomalous traffic is injected. One thing we observe from Fig. 13 is that because the anomalous data resides in the normal subspace, there is no substantive difference between the projection residual of normal data and anomalous data (BEFORE and AFTER) when using the variancebased subspace method. This confirms that the variance-based subspace method is not able to detect the anomaly that exploits normal traffic spoofing.\nOn the other hand, the projection residual with the distance-based subspace method shows a distinct qualitative difference and a larger value with anomalous traffic. Besides showing that this detection is possible with only 3 dimensions (while the variance-based approach uses 8 dimensions without being able to detect it), the distance-based approach also offers additional information for classification or mitigation. The GETESD algorithm computes, along with the effective subspace dimension, the most significant principal components of the test traffic which can be examined for a deeper look into the anomaly identified by the higher projection residual.\nIn this case, one may be able to observe that the order of the principal components has changed in the test data and that the anomalous traffic may be attempting to spoof normal traffic."}, {"heading": "E. Comparison of centralized and distributed algorithms", "text": "In this subsection, we offer a comparison between the performance of the centralized (GETESD) and distributed (GETESD-D) versions of our algorithm. Note that both of these versions yield the same mathematical output \u2014 that is, they achieve the same accuracy and, therefore, the same level of success in anomaly detection. Table I summarizes the comparisons for detecting an anomaly using the same workstation as in Fig. 10 for our simulations. For our simulations, we use synthetic networks modeled after realistic campus-wide WiFi backbones, with N = 100 and N = 1000 and 1Gbps links.\nThe detection time shown in Table I is the total worst-case runtime for computing the projection residual of each data vector for detection (in the distributed case, different nodes may converge to the answer at different times). As one might expect in a distributed algorithm in which detection steps begin with incomplete information until convergence is achieved, the worst-case run time of the distributed algorithm is longer. However, the distributed algorithm comes with several distinct advantages over the centralized version as described below.\nIn the centralized version, the detection occurs at the monitoring station. In the distributed version, the detection occurs at each node in the network (which is a significant advantage in large networks since localized mitigation actions can be applied faster and with knowledge of a fuller local context unavailable to a distant monitoring station.) The distributed algorithm also removes the communication and storage burden at the one monitoring station and distributes it across all the nodes. As Table I shows, the communication at the monitoring station in the centralized case is significantly higher than the communication at each of the nodes in the distributed case \u2014 which is especially helpful in the midst of a severe congestion caused by a distributed denial-of-service attack.\nStorage needs for detection in both the centralized and distributed algorithms are primarily that for storing the normal subspace data. The only difference is that, in algorithm GETESD-D, the stored information is evenly distributed among the nodes in the network instead of being concentrated at the monitoring station. In general, the monitoring station in GETESD has to store N times as much data as each node in GETESD-D."}, {"heading": "VIII. CONCLUDING REMARKS", "text": "Given Big Data trends in both collection and storage, current state-of-the-art network traffic analysis invariably deals with high-dimensional datasets of increasingly larger size \u2014 thus, it is important to derive a low-dimensional structure as a compact representation of the original dataset. In this paper, supported by theoretical analysis and simulation results using real traffic traces, we have described a new distance-based approach to dimensionality reduction with distinct advantages over previously known methods. These advantages include (i) improved adaptability to changing patterns in test data so that we only use the number of dimensions necessary at any given time, (ii) improved ability to characterize and classify attack traffic since the dimensionality reduction is based on the observed traffic and not pre-determined, and (iii) improved resilience against attacks which try to spoof normal traffic patterns.\nEven though we illustrated our distance-based subspace approach through the end goal of anomaly detection in networks, our contribution can be employed in any other network management context where dimensionality reduction of live network traffic is useful. The primary technical contribution of this paper, therefore, is a new general distance-based method for realtime dimensionality reduction in order to ease the Big Data challenges of network and system management.\nAPPENDIX"}, {"heading": "A. Proof of Theorem 1", "text": "Without loss of generality, assume kA \u2265 kB . The statement of the theorem is proved if \u03b8kA,kB(A,B) \u2264 \u03b8kB ,kB(A,B) and \u03b8kA,kB(A,B) \u2264 \u03b8kA,kA(A,B). The proofs of each of these two cases follows.\nCase (i): Let x denote a column vector of length kB. Using the definition of matrix norm, we have:\n\u2016TkA,kB(A,B) \u2016 2\n= max \u2200x,\u2016x\u2016=1\n\u2016 TkA,kB(A,B)x \u2016 2\n= max \u2200x,\u2016x\u2016=1\nN \u2211\ni=kA+1\n\u2016 a\u2032i[b1, . . . , bkB ]x \u2016 2\n=\nN \u2211\ni=kA+1\n\u2016 a\u2032i[b1, . . . , bkB ]xmax \u2016 2\nwhere xmax is the column vector with unit norm that achieves the maximum matrix norm. Similarly, using the definition of the matrix norm again:\n\u2016TkB ,kB(A,B) \u2016 2\n= max \u2200x,\u2016x\u2016=1\nN \u2211\ni=kB+1\n\u2016 a\u2032i[b1, . . . , bkB ]x \u2016 2\n\u2265\nN \u2211\ni=kB+1\n\u2016 a\u2032i[b1, . . . , bkB ]xmax \u2016 2\n\u2265 N \u2211\ni=kA+1\n\u2016 a\u2032i[b1, . . . , bkB ]xmax \u2016 2=\u2016 TkA,kB(A,B) \u2016 2 .\nThus we have \u03b8kA,kB(A,B) \u2264 \u03b8kB ,kB(A,B). Case (ii): Let y denote a column vector of length kA. Using the definition of matrix norm:\n\u2016TkA,kA(A,B) \u2016 2= max\n\u2200y,\u2016y\u2016=1\nN \u2211\ni=kA+1\n\u2016 a\u2032i[b1, . . . , bkA]y \u2016 2\nLet z denote a column vector of length kA with xmax followed by kA \u2212 kB zeroes: z\u2032 = [xmax 0 \u00b7 \u00b7 \u00b70]\u2032. This vector has unit norm and [b1, . . . , bkA]z = [b1, . . . , bkB ]xmax. Therefore,\n\u2016 TkA,kA(A,B) \u2016 2 \u2265\nN \u2211\ni=kA+1\n\u2016 a\u2032i[b1, . . . , bkA]z \u2016 2\n=\nN \u2211\ni=kA+1\n\u2016 a\u2032i[b1, . . . , bkB ]xmax \u2016 2= \u2016 TkA,kB(A,B) \u2016 2 .\nThus we have \u03b8kA,kB(A,B) \u2264 \u03b8kA,kA(A,B)."}, {"heading": "B. Proof of Theorem 2", "text": "First, we use Pi,j to express Tk,k(A,B) and T \u2032k,k(A,B)Tk,k(A,B).\nTk,k(A,B) = (I \u2212 k \u2211\ni=1\nai \u00d7 a\u2032i)[b1, . . . , bk]\n= [b1, . . . , bk]\u2212 k \u2211\ni=1\nai \u00d7 [a\u2032ib1, . . . , a \u2032 ibk]\n= [b1, . . . , bk]\u2212 k \u2211\ni=1\nai[Pi,1, . . . , Pi,k]\nSimilarly,\nT \u2032k,k(A,B)Tk,k(A,B) = Ik\u00d7k \u2212 P \u2032 1:k,1:kP1:k,1:k\nA more detailed proof with all intermediate steps is provided in [37]. The value of sin(\u03b8k,k(A,B)), defined earlier as \u2016 Tk,k(A,B) \u2016, is the first singular value of Tk,k(A,B) and also the square root of the largest eigenvalue of T \u2032k,k(A,B)Tk,k(A,B). Following the previous analysis, therefore, we have:\nsin2(\u03b8k,k(A,B)) = \u2016 Tk,k(A,B) \u2016 2\n= \u03c321(Tk,k(A,B)) = \u03bb1(T \u2032 k,k(A,B)Tk,k(A,B)) = \u03bb1(Ik\u00d7k \u2212 P \u2032 1:k,1:kP1:k,1:k) = 1\u2212 \u03bbk(P \u2032 1:k,1:kP1:k,1:k) = 1\u2212 \u03c32k(P1:k,1:k)\nwhere \u03bbi stands for the i-th eigenvalue and \u03c3i for the i-th singular value. As a result, the following holds:\ncos(\u03b8k,k(A,B)) = \u03c3k(P1:k,1:k)"}, {"heading": "C. Proof of Theorem 3", "text": "Consider an N \u00d7 N matrix P built as in Eq. (5). Let ui denote an N \u00d7 1 vector with i-th entry being non-zero:\nui(k) = { 1 if k = i 0 if k 6= i\nNow, letting pi denote the i-th column of P , and letting IN\u00d7N denote an identity matrix of size N \u00d7N , we have:\nu\u2032iP \u2032Puj = p\u2032ipj\n=\nN \u2211\nk=1\nPk,iPk,j =\nN \u2211\nk=1\nb\u2032iaka \u2032 kbj\n= b\u2032i( N \u2211\nk=1\naka\u2032k)bj = b \u2032 iIN\u00d7Nbj = b \u2032 ibj\nTherefore, we have\nu\u2032iP \u2032Puj =\n{\n1 if i = j 0 if i 6= j\n(9)\nNow, for simplicity, denote P1:k,1:k as Pk. We prove the result in Theorem 3 by first proving that \u03c31(Pk) > \u03c31(Pk\u22121) and then proving that \u03c31(Pk) \u2264 1.\nTo prove that \u03c31(Pk) > \u03c31(Pk\u22121), note that:\nPk =\n[\nPk\u22121 u v\u2032 c\n]\n(10)\nwhere u\u2032 = [a\u20321bk, . . . , a \u2032 k\u22121bk], v \u2032 = [a\u2032kb1, . . . , a \u2032 kbk\u22121], and c = a \u2032 kbk. Then, P \u2032 kPk can be constructed as follows:\nP \u2032kPk =\n[ P \u2032k\u22121 v u\u2032 c ] [ Pk\u22121 u v\u2032 c ]\n=\n[ P \u2032k\u22121Pk\u22121 + vv \u2032 P \u2032k\u22121u + cv\nu\u2032Pk\u22121 + cv\u2032 u\u2032u + c2\n]\n(11)\nSince \u03c31(Pk) is the first singular value of Pk, it implies the following: first, \u03c321(Pk) is an eigenvalue of P \u2032kPk, i.e., P \u2032 kPkzk = \u03c3 2 1(Pk)zk, where zk is the corresponding eigenvector with unit norm; second, \u2016 Pkz \u2016\u2264 \u03c31(Pk) for any vector z with unit norm. Thus, we have:\n\u03c321(Pk) \u2265 [ z\u2032k\u22121 0 ] P \u2032kPk\n[\nzk\u22121 0\n]\n= [ z\u2032k\u22121 0 ]\n[ P \u2032k\u22121Pk\u22121zk\u22121 + vv \u2032zk\u22121\nu\u2032Pk\u22121zk\u22121 + cv\u2032zk\u22121\n]\n= z\u2032k\u22121P \u2032 k\u22121Pk\u22121zk\u22121 + z \u2032 k\u22121vv \u2032zk\u22121 = z\u2032k\u22121\u03c3 2 1(Pk\u22121)zk\u22121 + (v \u2032zk\u22121)2 \u2265 \u03c321(Pk\u22121)z \u2032 k\u22121zk\u22121 = \u03c321(Pk\u22121)\nSince \u03c31(Pk) is non-negative, we have \u03c31(Pk) \u2265 \u03c31(Pk\u22121). Now, to prove that \u03c31(Pk) \u2264 1, note that for any x \u2208 RN\u00d71 with unit norm, we can represent it as a linear combination of ui, x = \u2211N k=1 xiui, with \u2211N k=1 x 2 i = 1. Using the result from Equation (9), we have\n\u2016 Px \u20162 = x\u2032P \u2032Px\n=\n(\nN \u2211\ni=1\nxiu\u2032i\n)\nP \u2032P\nN \u2211\nj=1\nxjuj\n=\nN \u2211\ni=1\n(\nN \u2211\nj=1\nxixju\u2032iP \u2032Puj\n)\n=\nN \u2211\ni=1\n( x2i ) = 1\nTherefore, \u03c31(P ) = \u00b7 \u00b7 \u00b7 = \u03c3N (P ) and \u03c31(P ) = max \u2016 Px \u2016= 1. As a result,\n\u03c31(Pk) \u2264 \u03c31(PN) = \u03c31(P ) = 1"}, {"heading": "D. Proof of Theorem 4", "text": "The proof of Theorem 3 above demonstrates that one can construct a projection matrix in search of the optimal subspace dimension. First, based on Equations (10) and (11), let\u2019s look at the computational complexity of constructing P1:k,1:k and P \u20321:k,1:kP1:k,1:k with the knowledge of P \u20321:k\u22121,1:k\u22121P1:k\u22121,1:k\u22121 and a1, . . . , ak, b1, . . . , bk.\nThe complexity of calculating u, v and c is O(kN). The complexity of constructing P \u20321:k,1:kP1:k,1:k is O(k2).\nNow, the complexity of calculating the eigenvalue of P \u20321:k,1:kP1:k,1:k using power iteration method is O(Zk2) with Z being the number of iterations necessary for convergence. Since k < N , the overall complexity of algorithm GETESD in k loops is O(Zk3 + k2N)."}, {"heading": "E. Proof of Theorem 5", "text": "The most computationally expensive part is in calculating z(k)n . When the size of measurements is M , the length of x\u2032nv (k) n is also M . Assuming the maximum degree of each node is \u2206, then the process of average consensus in S steps has complexity of O(\u2206S) for each entry of x\u2032nv (k) n . Overall, the computational complexity of the distributed power iteration method running for p steps is O(pM\u2206S). As a result, the overall computational complexity of GETESD-D using O(k) power iterations is O(kpM\u2206S)."}, {"heading": "F. Proof of Theorem 6", "text": "We consider the number of messages that one node needs to send when the algorithm is running. Again, let the maximum degree of each node be \u2206. At the very step, the node needs to send its observation xn to its neighbors, leading to communication cost of O(M\u2206). After that, this node only needs to update its neighbors about its estimate v(k)n . The number of messages sent for each power iteration is O(\u2206S). Overall, the communication complexity of the distributed power iteration method running for p steps is O(M\u2206 + p\u2206S). The overall communication complexity of GETESD-D using O(k) power iterations is O(kM\u2206 + kp\u2206S)."}], "references": [{"title": "Network anomaly detection: Methods, systems and tools", "author": ["M. Bhuyan", "D. Bhattacharyya", "J. Kalita"], "venue": "IEEE Commun. Surveys Tuts., vol. 16, no. 1, pp. 303\u2013336, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Beehive: Large-scale log analysis for detecting suspicious activity in enterprise networks", "author": ["T.-F. Yen", "A. Oprea", "K. Onarlioglu", "T. Leetham", "W. Robertson", "A. Juels", "E. Kirda"], "venue": "ACM Ann. Comput. Security Appl. Conf., 2013, pp. 199\u2013208.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Principal Component Analysis, 2nd ed", "author": ["I.T. Jolliffe"], "venue": "New York: Springer,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Diagnosing network-wide traffic anomalies", "author": ["A. Lakhina", "M. Crovella", "C. Diot"], "venue": "SIGCOMM Comput. Commun. Rev., vol. 34, no. 4, pp. 219\u2013230, Aug. 2004.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Sensitivity of PCA for traffic anomaly detection", "author": ["H. Ringberg", "A. Soule", "J. Rexford", "C. Diot"], "venue": "SIGMETRICS Perform. Eval. Rev., vol. 35, no. 1, pp. 109\u2013120, Jun. 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Covariance-matrix modeling and detecting various flooding attacks", "author": ["D. Yeung", "S. Jin", "X. Wang"], "venue": "IEEE Trans. Syst., Man, Cybern. A, Syst., Humans, vol. 37, no. 2, pp. 157\u2013169, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Gossip-based greedy gaussian mixture learning", "author": ["N. Vlassis", "Y. Sfakianakis", "W. Kowalczyk"], "venue": "Advances in Informatics. Springer, 2005, pp. 349\u2013359.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Load characterization and anomaly detection for voice over IP traffic", "author": ["M. Mandjes", "I. Saniee", "A.L. Stolyar"], "venue": "IEEE Trans. Neural Netw., vol. 16, no. 5, pp. 1019\u20131026, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Detecting VoIP calls hidden in web traffic", "author": ["E. Freire", "A. Ziviani", "R. Salles"], "venue": "IEEE Trans. Netw. Service Manag., vol. 5, no. 4, pp. 204\u2013214, December 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Locating network domain entry and exit point/path for DDoS attack traffic", "author": ["V. Thing", "M. Sloman", "N. Dulay"], "venue": "IEEE Trans. Netw. Service Manag., vol. 6, no. 3, pp. 163\u2013174, September 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "A large-scale hidden semi-Markov model for anomaly detection on user browsing behaviors", "author": ["Y. Xie", "S. zheng Yu"], "venue": "IEEE/ACM Trans. Netw., vol. 17, no. 1, pp. 54\u201365, Feb 2009. 29", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Spatio-temporal network anomaly detection by assessing deviations of empirical measures", "author": ["I. Paschalidis", "G. Smaragdakis"], "venue": "IEEE/ACM Trans. Netw., vol. 17, no. 3, pp. 685\u2013697, June 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Anomaly detection in IP networks", "author": ["M. Thottan", "C. Ji"], "venue": "IEEE Trans. Signal Process., vol. 51, no. 8, pp. 2191\u20132204, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Mining anomalies using traffic feature distributions", "author": ["A. Lakhina", "M. Crovella", "C. Diot"], "venue": "SIGCOMM Comput. Commun. Rev., vol. 35, no. 4, pp. 217\u2013228, Aug. 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "A novel covariance matrix based approach for detecting network anomalies", "author": ["M. Tavallaee", "W. Lu", "S.A. Iqbal", "A. Ghorbani"], "venue": "IEEE Commun. Netw. Services Res. Conf., 2008, pp. 75\u201381.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Histogram-based traffic anomaly detection", "author": ["A. Kind", "M.P. Stoecklin", "X. Dimitropoulos"], "venue": "IEEE Trans. Netw. Service Manag., vol. 6, no. 2, pp. 110\u2013121, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "A distribution-based approach to anomaly detection and application to 3G mobile traffic", "author": ["A.D. Alconzo", "A. Coluccia", "F. Ricciato", "P. Romirer-Maierhofer"], "venue": "IEEE Glob. Telecom. Conf., 2009, pp. 1\u20138.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "A novel PCA-based network anomaly detection", "author": ["C. Callegari", "L. Gazzarrini", "S. Giordano", "M. Pagano", "T. Pepe"], "venue": "IEEE Intl. Conf. Commun., 2011, pp. 1\u20135.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "A comparative study of two network-based anomaly detection methods", "author": ["K. Nyalkalkar", "S. Sinha", "M. Bailey", "F. Jahanian"], "venue": "IEEE Intl. Conf. Comput. Commun. (INFOCOM), 2011, pp. 176\u2013180.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust feature selection and robust PCA for Internet traffic anomaly detection", "author": ["C. Pascoal", "M.R. de Oliveira", "R. Valadas", "P. Filzmoser", "P. Salvador", "A. Pacheco"], "venue": "IEEE Intl. Conf. Comput. Commun. (INFOCOM), 2012, pp. 1755\u20131763.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust PCA as bilinear decomposition with outlier-sparsity regularization", "author": ["G. Mateos", "G.B. Giannakis"], "venue": "IEEE Trans. Sig. Process., vol. 60, no. 10, pp. 5176\u20135190, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "PCA-based robust anomaly detection using periodic traffic behavior", "author": ["T. Kudo", "T. Morita", "T. Matsuda", "T. Takine"], "venue": "IEEE Intl. Conf. Commun. Wksp. (ICC), 2013, pp. 1330\u20131334.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Dimension reduction: A guided tour", "author": ["C.J.C. Burges"], "venue": "Now Publishers, Inc.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Asynchronous distributed power iteration with gossip-based normalization", "author": ["M. Jelasity", "G. Canright", "K. Eng\u00f8-Monsen"], "venue": "Euro-Par 2007 Parallel Process. Springer, 2007, pp. 514\u2013525.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Power iteration-based distributed total least squares estimation in ad hoc sensor networks", "author": ["A. Bertrand", "M. Moonen"], "venue": "IEEE Intl. Conf. Acoust., Speech, Signal Process. (ICASSP), 2012, pp. 2669\u20132672.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed principal component analysis on networks via directed graphical models", "author": ["Z. Meng", "A. Wiesel", "A.O. Hero III"], "venue": "IEEE Intl. Conf. Acoust., Speech, Signal Process. (ICASSP), 2012, pp. 2877\u20132880.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "In-network PCA and anomaly detection", "author": ["L. Huang", "X. Nguyen", "M. Garofalakis", "M.I. Jordan", "A. Joseph", "N. Taft"], "venue": "Adv. Neural Inf. Process. Sys., 2006, pp. 617\u2013624.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Decomposable principal component analysis", "author": ["A. Wiesel", "A.O. Hero"], "venue": "IEEE Trans. Signal Process., vol. 57, no. 11, pp. 4369\u20134377, 2009.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Randomized gossip algorithms", "author": ["S. Boyd", "A. Ghosh", "B. Prabhakar", "D. Shah"], "venue": "IEEE/ACM Trans. Netw. (TON), vol. 14, no. SI, pp. 2508\u20132530, 2006.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Gossip algorithms for distributed signal processing", "author": ["A.G. Dimakis", "S. Kar", "J.M. Moura", "M.G. Rabbat", "A. Scaglione"], "venue": "Proc. IEEE, vol. 98, no. 11, pp. 1847\u20131864, 2010.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1847}, {"title": "Distributed principal subspace estimation in wireless sensor networks", "author": ["L. Li", "A. Scaglione", "J.H. Manton"], "venue": "IEEE J. Sel. Topics Signal Process., vol. 5, no. 4, pp. 725\u2013738, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Numerical methods for computing angles between linear subspaces", "author": ["A. Bj\u00f6rck", "G.H. Golub"], "venue": "Math. comput., vol. 27, no. 123, pp. 579\u2013594, 1973.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1973}, {"title": "A fast algorithm for detecting anomalous changes in network traffic", "author": ["T. Huang", "H. Sethu", "N. Kandasamy"], "venue": "Intl. Conf. Netw. Service Manag.(CNSM), 2015.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive sampling and statistical inference for anomaly detection", "author": ["T. Huang"], "venue": "Ph.D. dissertation, Drexel University, 2015. 30", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Security professionals who monitor communication networks for malware, errors and intrusions are increasingly dependent on real-time detection of anomalous behavior in the data traffic [1].", "startOffset": 198, "endOffset": 201}, {"referenceID": 1, "context": "The volume of data one has to monitor and process for effective real-time management of networks and systems, however, poses significant Big Data challenges [2].", "startOffset": 157, "endOffset": 160}, {"referenceID": 2, "context": "Principal Component Analysis (PCA) is a widely used tool that allows us to derive a reduced set of the most significant uncorrelated features that are linear combinations of the original set of features [3].", "startOffset": 203, "endOffset": 206}, {"referenceID": 3, "context": "Then, a significant deviation in the projection of the N-dimensional observed data onto this k-dimensional reference (normal) subspace can be defined as an anomaly for purposes of detection [4].", "startOffset": 190, "endOffset": 193}, {"referenceID": 4, "context": "There are at least three weaknesses of the traditional variance-based subspace approach for anomaly detection: (i) the reduced number of principal components, k, is computed based on the structure of normal traffic when, actually, the structure of the changes between the observed and the normal traffic is more relevant for choosing the appropriate number of dimensions for anomaly detection; (ii) a static determination of k is inadequate at capturing real-time changes \u2014 the right number of dimensions is often different during different periods of time depending on the structure of both the normal and the observed traffic; (iii) the method allows only weak heuristics because the performance of anomaly detection is very sensitive to small changes in the number of dimensions chosen for the normal subspace [5].", "startOffset": 813, "endOffset": 816}, {"referenceID": 5, "context": "This work is additionally motivated by two observations: (i) anomalies lead to changes in the covariance matrix of the set of traffic features being monitored, and (ii) different types of anomalies cause different types of deviations in the covariance matrix allowing a categorization of the detected anomaly and an immediate prescription of actions toward threat mitigation [6].", "startOffset": 375, "endOffset": 378}, {"referenceID": 6, "context": "The participating nodes in the GETESD-D algorithm deploy the gossip-based Gaussian mixture learning mechanism to estimate principal components of the traffic features [7].", "startOffset": 167, "endOffset": 170}, {"referenceID": 7, "context": "For example, methods reported in [8] and [9] allow the detection of load anomalies in voice-over-IP traffic at the network or the application layer.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "For example, methods reported in [8] and [9] allow the detection of load anomalies in voice-over-IP traffic at the network or the application layer.", "startOffset": 41, "endOffset": 44}, {"referenceID": 9, "context": "Similarly, distributed denial-of-service (DDoS) attacks are the target of detection in [10], [11] and [12].", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "Similarly, distributed denial-of-service (DDoS) attacks are the target of detection in [10], [11] and [12].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "Similarly, distributed denial-of-service (DDoS) attacks are the target of detection in [10], [11] and [12].", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "or state machine modeling are reviewed in [13].", "startOffset": 42, "endOffset": 46}, {"referenceID": 3, "context": "Examples of later work on developing general anomaly detection tools include [4], [6], [12], [14]\u2013[18].", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "Examples of later work on developing general anomaly detection tools include [4], [6], [12], [14]\u2013[18].", "startOffset": 82, "endOffset": 85}, {"referenceID": 11, "context": "Examples of later work on developing general anomaly detection tools include [4], [6], [12], [14]\u2013[18].", "startOffset": 87, "endOffset": 91}, {"referenceID": 13, "context": "Examples of later work on developing general anomaly detection tools include [4], [6], [12], [14]\u2013[18].", "startOffset": 93, "endOffset": 97}, {"referenceID": 17, "context": "Examples of later work on developing general anomaly detection tools include [4], [6], [12], [14]\u2013[18].", "startOffset": 98, "endOffset": 102}, {"referenceID": 5, "context": "While the covariance matrix plays a role in many anomaly detection methods, it was most directly used in [6] to detect flooding attacks based on comparisons between a covariance matrix under normal conditions (used as the reference matrix) and the observed covariance matrix.", "startOffset": 105, "endOffset": 108}, {"referenceID": 14, "context": "The covariance matrix is also used directly for anomaly detection in [15].", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "Further, as opposed to methods in [6] and [15] which are based on detecting the changes in individual entries in the covariance matrix, our method is additionally able to exploit the underlying correlations between the changes in the entries to offer a more refined and a more reliable approach to anomaly detection.", "startOffset": 34, "endOffset": 37}, {"referenceID": 14, "context": "Further, as opposed to methods in [6] and [15] which are based on detecting the changes in individual entries in the covariance matrix, our method is additionally able to exploit the underlying correlations between the changes in the entries to offer a more refined and a more reliable approach to anomaly detection.", "startOffset": 42, "endOffset": 46}, {"referenceID": 3, "context": "The variance-based subspace method, based on PCA, was first proposed for anomaly detection in [4] and later improved in [14] to explore the deviation in the network-wide traffic volume and feature distributions caused by anomalies.", "startOffset": 94, "endOffset": 97}, {"referenceID": 13, "context": "The variance-based subspace method, based on PCA, was first proposed for anomaly detection in [4] and later improved in [14] to explore the deviation in the network-wide traffic volume and feature distributions caused by anomalies.", "startOffset": 120, "endOffset": 124}, {"referenceID": 3, "context": "To use this method online in real-time as described in [4], one processes each arrival of new traffic measurements using the matrix PP T , where P is composed of the top k principal components representing the normal traffic pattern.", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": ") The scheme proposed in [4] would separate the high-dimensional space of network traffic into two subspaces: the normal subspace and the anomalous subspace.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "The limitations of the variance-based subspace methods are discussed in [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 18, "context": "The simulation results in [19] further confirm that the effectiveness of the subspace method depends strongly on the dimension chosen for the normal subspace.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "Later work has improved upon the training process of the subspace method [20]\u2013[22], but choosing the appropriate dimension for the normal subspace has remained an unmet challenge.", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "Later work has improved upon the training process of the subspace method [20]\u2013[22], but choosing the appropriate dimension for the normal subspace has remained an unmet challenge.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "The distancebased methods used in the literature also present the same challenge where the reduced number of dimensions is not adaptive to real-time data [23].", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "In other related work, PCA-based methods have been decentralized for a variety of purposes including anomaly detection [24]\u2013[28].", "startOffset": 119, "endOffset": 123}, {"referenceID": 27, "context": "In other related work, PCA-based methods have been decentralized for a variety of purposes including anomaly detection [24]\u2013[28].", "startOffset": 124, "endOffset": 128}, {"referenceID": 26, "context": "A distributed framework for PCA is proposed in [27] to achieve accurate detection of network anomalies through monitoring of only the local data.", "startOffset": 47, "endOffset": 51}, {"referenceID": 27, "context": "distributed implementation of PCA is developed for decomposable Gaussian graphical models in [28] to allow decentralized anomaly detection in backbone networks.", "startOffset": 93, "endOffset": 97}, {"referenceID": 28, "context": "Distributed gossip algorithms using only local communication for subspace estimation have been used in the context of sensor networks [29]\u2013[31].", "startOffset": 134, "endOffset": 138}, {"referenceID": 30, "context": "Distributed gossip algorithms using only local communication for subspace estimation have been used in the context of sensor networks [29]\u2013[31].", "startOffset": 139, "endOffset": 143}, {"referenceID": 30, "context": "Our work in this paper extends the distributed average consensus protocol proposed in [31] to estimate the principal subspace for the context of anomaly detection in network traffic data.", "startOffset": 86, "endOffset": 90}, {"referenceID": 31, "context": "Our proposed metric of subspace distance is new, and the closest related metric in the literature is the principal angle [32].", "startOffset": 121, "endOffset": 125}, {"referenceID": 32, "context": "The GETESD algorithm achieves a significantly higher accuracy over our previous work, a heuristic [34].", "startOffset": 98, "endOffset": 102}, {"referenceID": 32, "context": "As reported in [34], the heuristic is able to estimate the subspace distance with a percentage error which frequently reaches above 1% and sometimes close to 5%.", "startOffset": 15, "endOffset": 19}, {"referenceID": 30, "context": "This routine, a straightforward extension of the distributed average consensus algorithm in [31] to accommodate for the constraints placed on row sums and column sums of W , is shown in Fig.", "startOffset": 92, "endOffset": 96}, {"referenceID": 30, "context": "Since the family of average consensus algorithms is already well-studied [31], we focus here on the distributed power iteration method assuming that the distributed average consensus method generates an answer within an error margin of 5%.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "The detection process simply compares the projection residual with a pre-defined threshold and triggers an alarm if it is above the threshold [14].", "startOffset": 142, "endOffset": 146}, {"referenceID": 33, "context": "A more detailed proof with all intermediate steps is provided in [37].", "startOffset": 65, "endOffset": 69}], "year": 2016, "abstractText": "The monitoring and management of high-volume feature-rich traffic in large networks offers significant challenges in storage, transmission and computational costs. The predominant approach to reducing these costs is based on performing a linear mapping of the data to a low-dimensional subspace such that a certain large percentage of the variance in the data is preserved in the low-dimensional representation. This variance-based subspace approach to dimensionality reduction forces a fixed choice of the number of dimensions, is not responsive to real-time shifts in observed traffic patterns, and is vulnerable to normal traffic spoofing. Based on theoretical insights proved in this paper, we propose a new distance-based approach to dimensionality reduction motivated by the fact that the real-time structural differences between the covariance matrices of the observed and the normal traffic is more relevant to anomaly detection than the structure of the training data alone. Our approach, called the distance-based subspace method, allows a different number of reduced dimensions in different time windows and arrives at only the number of dimensions necessary for effective anomaly detection. We present centralized and distributed versions of our algorithm and, using simulation on real traffic traces, demonstrate the qualitative and quantitative advantages of the distance-based subspace approach.", "creator": "LaTeX with hyperref package"}}}