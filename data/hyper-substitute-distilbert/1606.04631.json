{"id": "1606.04631", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Bidirectional Long-Short Term Memory for Video Description", "abstract": "video captioning is been attracting increased research attention amidst speech coding. critically, most existing guidelines either ignore temporal ambiguity among video frames would just employ human - temporal knowledge. designing its segment, ieee developed my novel video captioning framework, identifying as \\ object { complete long - short term memory } ( bilstm ), which immediately enhance bidirectional global temporal structure comprising video. first, creators initially propose a joint quantitative modelling task thus encode quantitative images compression combining robust forward lstm pass, a digital lstm pass, together with visual manipulation from convolutional neural networks ( figures ). generally, processors inject structurally same video representation - grouped subsequent language components providing integration. other benefits are including two folds : 1 ) comprehensively preserving sequential hierarchical visual aspects ; and 2 ) consciously learning repetitive visual features and sparse posterior ambiguity along videos and sentences, respectively. we verify the adoption of enterprise collaboration video captioning methods on single commonly - met benchmark, 2. e., microsoft cognitive description ( rr ) corpus, theoretically using experimental process confirming that overall superiority of those proposed approach significantly compared to comparable state - just - the - art methods.", "histories": [["v1", "Wed, 15 Jun 2016 03:26:53 GMT  (225kb,D)", "http://arxiv.org/abs/1606.04631v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.MM cs.CL", "authors": ["yi bin", "yang yang", "zi huang", "fumin shen", "xing xu", "heng tao shen"], "accepted": false, "id": "1606.04631"}, "pdf": {"name": "1606.04631.pdf", "metadata": {"source": "CRF", "title": "Bidirectional Long-Short Term Memory for Video Description", "authors": ["Yi Bin", "Yang Yang", "Zi Huang", "Fumin Shen", "Xing Xu", "Heng Tao Shen"], "emails": ["yi.bin@hotmail.com,", "dlyyang@gmail.com,", "huang@itee.uq.edu.au", "fumin.shen@gmail.com,", "xing.xu@uestc.edu.cn,", "shenht@itee.uq.edu.au"], "sections": [{"heading": null, "text": "CCS Concepts \u2022Computing methodologies \u2192 Video summarization;\nKeywords Video caption; bidirectional long-short term memory"}, {"heading": "1. INTRODUCTION", "text": "With the development of digital media technology and popularity of Mobile Internet, online visual content has increased rapidly in recent couple of years. Subsequently, visual content analysis for retrieving [31, 18] and understanding becomes a fundamental problem in the area of multimedia research, which has motivated world-wide researchers\nACM ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nto develop advanced techniques. Most previous works, however, have focused on classification task, such as annotating an image [9, 19] or video [5, 17, 21, 30] with given fixed label sets. With some pioneering methods [4, 14] tackling the challenge of describing images with natural language proposed, visual content understanding has attracted more and more attention. State-of-the-art techniques for image captioning have been surpassed by new advanced approaches in succession [2, 3, 8, 25, 29]. Recent researches [15, 24, 32, 11, 23] have been focusing on describing videos with more comprehensive sentences instead of simple keywords. Different from image, video is sequential data with temporal structure, which may pose significant challenge to video caption. Most of the existing works in video description employed max or mean pooling across video frames to obtain videolevel representation, which failed to capture temporal knowledge. To address this problem, Yao et al. proposed to use 3- D Convolutional Neural Networks to explore local temporal information in video clips, where the most relevant temporal fragments were automatically chosen for generating natural language description with attention mechanism [32]. In [23], Venugopanlan et al. implemented a Long-Short Term Memory (LSTM) network, a variant of Recurrent Neural Networks (RNNs), to model the global temporal structure in whole video snippet. However, these methods failed to exploit bidirectional global temporal structure, which could benefit from not only previous video frames, but also information in future frames. Also, existing video captioning schemes cannot adaptively learn dense video representation and generate sparse semantic sentences.\nIn this work, we propose to construct a novel bidirectional LSTM (BiLSTM) network for video captioning. More specifically, we design a joint visual modelling to comprehensively explore bidirectional global temporal information in video data by integrating a forward LSTM pass, a backward LSTM pass, together with CNNs features. In order to enhance the subsequent sentence generation, the obtained visual representations are then fed into LSTM-based language model as initialization. We summarize the main contributions of this work as follows: (1) To our best knowledge, our approach is one of the first to utilize bidirectional recurrent neural networks for exploring bidirectional global temporal structure in video captioning; (2) We construct two sequential processing models for adaptive video representation learning and language description generation, respectively, rather than using the same LSTM for both video frames encoding and text decoding in [23]; and (3) Extensive experiments on a real-world video corpus illustrate the ar X\niv :1\n60 6.\n04 63\n1v 1\n[ cs\n.M M\n] 1\n5 Ju\nn 20\n16\nsuperiority of our proposal as compared to state-of-the-arts."}, {"heading": "2. THE PROPOSED APPROACH", "text": "In this section, we elaborate the proposed video captioning framework, including an introduction of the overall flowchart (as illustrated in Figure 1), a brief review of LSTM-based Sequential Model, the joint visual modelling with bidirectional LSTM and CNNs, as well as the sentence generation process."}, {"heading": "2.1 LSTM-based Sequential Model", "text": "With the success in speech recognition and machine translation tasks, recurrent neural structure, especially LSTM and its variants, have dominated sequence processing field. LSTM has been demonstrated to be able to effectively address the gradients vanishing or explosion problem [6] during back-propagation through time (BPTT) [26] and to exploit temporal dependencies in very long temporal structure. LSTM incorporates several control gates and a constant memory cell, the details of which are following:\nit = \u03c3 (Wixxt +Wihht\u22121) (1)\nft = \u03c3 (Wfxxt +Wihht\u22121) (2)\not = \u03c3 (Woxxt +Wohht\u22121) (3)\nct = ft ct\u22121 + it \u03c6 (Wcxxt +Wchht\u22121) (4)\nht = ot \u03c6 (ct) (5)\nwhere Wmn-like matrices are LSTM weight parameters, \u03c3 and \u03c6 are denote the sigmoid and hyperbolic non-linear functions, respectively, and indicates element-wise multiplication operation. Inspired by the success of LSTM, we devise an LSTM-based network to investigate the video temporal structure for video representation. Then initializing language model with video representation to generate video description."}, {"heading": "2.2 Bidirectional Video Modelling", "text": "Different from other video description approaches that represent video by implementing pooling across frames [24] or 3-D CNNs with local temporal structure [15], we apply BiLSTM networks to exploit the bidirectional temporal structure of video clips. Convolutional Neural Networks (CNNs) has demonstrated overwhelming performance on image recognition, classification [9] and video content analysis [3, 23]. Therefore, we extract caffe [7] fc7 layer of each frame through VGG-16 layers [20] caffemodel. Following [23, 24], we sample one frame from every ten frames in the video and extract the fc7 layer, the second fully-connected layer, to express selected frames. Then a T -by-4096 feature matrix generated to denote given video clip, where T is the number of frames we sampled in the video. As in Figure 1, we then implement two LSTMs, forward pass and backward pass, to encode CNNs features of video frames, and then merge the output sequences at each time point with a learnt weight matrix. What is interesting is that at each time point in bidirectional structure, we not only \u201csee\u201d the past frames, but also \u201cpeek\u201d at the future frames. In other words, our bidirectional LSTM structure encodes video by scanning the entire video sequence several times (same as the number of time steps at encoding stage), and each scan is relevant to its adjacent scans. To investigate the effect of reinforcement of original CNNs feature, we combine the merged hidden states of BiLSTM structure and fc7 representation time step-wise. We further employ another forward pass LSTM network with incorporated sequence to generate our video representation. In [27, 28], Wu et al. had demonstrated that using the output of the last step could perform better than pooling approach across outputs of all the time steps in video classification task. Similarly, we represent the entire video clip using the state of memory cell and output of the last time point, and feed them into description generator as initialization of memory cell and hidden unit respectively."}, {"heading": "2.3 Generating Video Description", "text": "Existing video captioning approaches usually share common part of visual model and language model as representation [23, 15], which may lead to severe information loss. Besides, they also input the same pooled visual vector of the whole video into every sentence processing unit, thereby ignoring temporal structure. Such methods may easily result in undesirable outputs due to the duplicate inputs in every time point of the new sequence [24]. To address these issues, we generate descriptions for video clips using a sequential model initialized with visual representation. Inspired by the superior performance of probabilistic sequence generation machine, we generate each word recurrently at each time point. Then the log probability of sentence S can be expressed as below:\nlog p (S|V ) = t=N\u2211 t=1 log p (wt|V,w1, ...wt\u22121; \u03b8) (6)\nwhere \u03b8 denotes all parameters in sentence generation model and V is the representation of given video, and N indicates the number of words in sentence. We identify the most likely sentence by maximizing the log likelihood in Equation (6),\nthen our object function can be described as:\n\u03b8\u2217 = argmax \u03b8 t=N\u2211 t=1 log p (S|V ; \u03b8) (7)\nThe optimizer updates \u03b8 with \u03b8\u2217 across the entire training process applying Stochastic Gradient Descent (SGD). During training phrase, the loss is back propagated through time and each LSTM unit learns to derive an appropriate hidden representation ht from input sequence. We then implement the Softmax function to get the probability distribution over the words in the entire vocabulary.\nAt the beginning of the sentence generation, as depicted in Figure 1, an explicit starting token (<BOS>) is needed and we terminate each sentence when the end-of-sentence token (<EOS>) is feeding in. During test phrase, similar to [23], our language model takes the word wt\u22121 with maximum likelihood as input at time t repeatedly until the <EOS> token is emitted."}, {"heading": "3. EXPERIMENTS", "text": ""}, {"heading": "3.1 Dataset", "text": "Video Dataset: We evaluate our approach by conducting experiments on the Microsoft Research Video Description (MSVD) [1] corpus, which is description for a collection of 1,970 video clips. Each video clip depicts a single action or a simple event, such as \u201cshooting\u201d, \u201ccutting\u201d, \u201cplaying the piano\u201d and \u201ccooking\u201d, which with the duration between 8 seconds to 25 seconds. There are roughly 43 available sentences per video and 7 words in each sentence at average. Following the majority of prior works [15, 24, 23, 32], we split entire dataset into training, validation and test set with 1200, 100 and 670 snippets, respectively.\nImage Dataset: Comparing to other LSTM structure and deep networks, the size of video dataset for caption task is small, thereby we apply transferring learning from image description. COCO 2014 image description dataset [13] has been used to perform experiments frequently [8, 3, 2, 29], which consists of more than 120,000 images, about 82,000 and 40,000 images for training and test respectively. We pre-train our language model on COCO 2014 training set first, then transfer learning on MSVD with integral video description model."}, {"heading": "3.2 Experimental Setup", "text": "3.2.1 Preprocessing Description Processing: Some minimal preprocessing\nhas been implemented to the descriptions in both MSVD and COCO 2014 datasets. We first employ word tokenize operation in NLTK toolbox1 to obtain individual words, and then convert all words to lower-case. All punctuation are removed, and then we start each sentence with <BOS> and end with <EOS>. Finally, we combine the sets of words in MSVD with COCO 2014, and generate a vocabulary with 12,984 unique words. Each word input to our system is represented by one-hot vector.\nVideo Preprocessing: As previous video description works [24, 23, 15] , we sample video frames once in every ten frames, then these frames could represent given video and\n1http://www.nltk.org\n28.5 frames for each video averagely. We extract frame-wise caffe fc7 layer features using VGG-16 layers model, then feed the sequential feature into our video caption system.\n3.2.2 Model We employ a bidirectional S2VT [23] and a joint bidi-\nrectional LSTM structure to investigate the performance of our bidirectional approach. For convenient comparison, we set the size of hidden unit of all LSTMs in our system to 512 as [15, 23], except for the first video encoder in unidirectional joint LSTM. During training phrase, we set 80 as maximum number of time steps of LSTM in all our models and a mini-batch with 16 video-sentence pairs. We note that over 99% of the descriptions in MSVD and COCO 2014 contain no more than 40 words, and in [23], Venugopalan et al. pointed out that 94% of the YouTube training videos satisfy our maximum length limit. To ensure sufficient visual content, we adopt two ways to truncate the videos and sentences adaptively when the sum of the number of frames and words exceed the limit. If the number of words is within 40, we arbitrarily truncate the frames to satisfy the maximum length. When the length of sentence is more than 40, we discard the words that beyond the length and take video frames with a maximum number of 40.\nBidirectional S2VT: Similar to [23], we implement several S2VT-based models: S2VT, bidirectional S2VT and reinforced S2VT with bidirectional LSTM video encoder. We conduct experiment on S2VT using our video features and LSTM structure instead of the end-to-end model in [23], which need original RGB frames as input. For bidirectional S2VT model, we first pre-train description generator on COCO 2014 for image caption. We next implement forward and backward pass for video encoding and merge the hidden states step-wise with a learnt weight while the language layer receives merged hidden representation with null padded as words. We also pad the inputs of forward LSTM and backward LSTM with zeros at decoding stage, and concatenate the merged hidden states to embedded words. In the last model, we regard merged bidirectional hidden states as complementary enhancement and concatenate to original fc7 features to obtain a reinforced representation of video, then derive sentence from new feature using the last LSTM. The loss is computed only at decoding stage in all S2VTbased models.\nJoint-BiLSTM: Different from S2VT-based models, we employ a joint bidirectional LSTM networks to encode video sequence and decode description applying another LSTM respectively rather than sharing the common one. We stack two layers of LSTM networks to encode video and pre-train language model as in S2VT-based models. Similarly, unidirectional LSTM, bidirectional LSTM and reinforced BiLSTM are executed to investigate the performance of each structure. We set 1024 hidden units of the first LSTM in unidirectional encoder so that the output could pass to the second encoder directly, and the memory cell and hidden state of the last time point are applied to initialize description decoder. Bidirectional structure and reinforced BiLSTM in encoder are implemented similarly to the corresponding type structure in S2VT-based models, respectively, and then feed the video representation into description generator as the unidirectional model aforementioned.\n3.3 Results and Analysis\nBLEU [16], METEOR [10], ROUGE-L [12] and CIDEr [22] are common evaluation metrics in image and video description, the first three were originally proposed to evaluate machine translation at the earliest and CIDEr was proposed to evaluate image description with sufficient reference sentences. To quantitatively evaluate the performance of our bidirectional recurrent based approach, we adopt METEOR metric because of its robust performance. Contrasting to the other three metrics, METEOR could capture semantic aspect since it identifies all possible matches by extracting exact matcher, stem matcher, paraphrase matcher and synonym matcher using WordNet database, and compute sentence level similarity scores according to matcher weights. The authors of CIDEr also argued for that METEOR outperforms CIDEr when the reference set is small [22].\nWe first compare our unidirectional, bidirectional structures and reinforced BiLSTM. As shown in Table 1, in S2VTbased model, bidirectional structure performs very little lower score than unidirectional structure while it shows the opposite results in joint LSTM case. It may be caused by the pad at description generating stage in S2VT-based structure. We note that BiLSTM reinforced structure gains more than 3% improvement than unidirectional-only model in both S2VTbased and joint LSTMs structures, which means that combining bidirectional encoding of video representation is beneficial to exploit some additional temporal structure in video encoder (Figure 2). On structure level, Table 1 illustrates that our Joint-LSTMs based models outperform all S2VT based models correspondingly. It demonstrates our JointLSTMs structure benefits from encoding video and decoding natural language separately.\nWe also evaluate our Joint-BiLSTM structure by comparing with several other state-of-the-art baseline approaches, which exploit either local or global temporal structure. As shown in Table 2, our Joint-BiLSTM reinforced model outperforms all of the baseline methods. The result of \u201cLSTM\u201d in first row refer from [15] and the last row but one denotes the best model combining local temporal structure using C3D with global temporal structure utilizing temporal attention in [32]. From the first two rows, our unidirectional joint LSTM shows rapid improvement, and comparing with S2VT-VGG model in line 3, it also demonstrates some superiority. Even LSTM-E jointly models video and descriptions representation by minimizing the distance between video and corresponding sentence, our Joint-BiLSTM reinforced obtains better performance from bidirectional encoding and separated visual and language models.\nWe observed that while our unidirectional S2VT has the same deployment as [23], our model gives a little poorer performance(line 1, Table 1 and line 3, Table 2). As mentioned in Section 3.2.2, they employed an end-to-end model reading original RGB frames and fine-tuning on the VGG caffemodel. The features of frames from VGG fc7 layer are more compatible to MSVD dataset and the description task. However, our joint LSTM demonstrates better performance with general features rather than specific ones for data, even superior to their model with multiple feature aspects (RGB + Flow, line 4, Table 2), which means that our Joint-BiLSTM could show more powerful descriptive ability in end-to-end case. Certainly, We would investigate effect of end-to-end type of our Joint-BiLSTM in future works."}, {"heading": "4. CONCLUSION AND FUTURE WORKS", "text": "In this paper, we introduced a sequence to sequence approach to describe video clips with natural language. The core of our method was, we applied two LSTM networks for the visual encoder and natural language generator component of our model. In particular, we encoded video sequences with a bidirectional Long-Short Term Memory (BiLSTM) network, which could effectively capture the bidirectional global temporal structure in video. Experimental results on MSVD dataset demonstrated the superior performance over many other state-of-the-art methods.\nWe also note some limitations in our model, such as endto-end framework employed in [23] and distance measured in [15]. In the future we will make more effort to fix these limitations and exploit the linguistic domain knowledge in visual content understanding."}, {"heading": "5. REFERENCES", "text": "[1] D. L. Chen and W. B. Dolan. Collecting highly\nparallel data for paraphrase evaluation. In ACL, 2011.\n[2] X. Chen and C. Lawrence Zitnick. Mind\u2019s eye: A recurrent visual representation for image caption generation. In CVPR, 2015.\n[3] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, 2015.\n[4] A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every picture tells a story: Generating sentences from images. In ECCV. Springer, 2010.\n[5] C. Gan, N. Wang, Y. Yang, D.-Y. Yeung, and A. G. Hauptmann. Devnet: A deep event network for multimedia event detection and evidence recounting. In CVPR, 2015.\n[6] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.\n[7] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACM Multimedia, 2014.\n[8] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015.\n[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.\n[10] M. D. A. Lavie. Meteor universal: language specific translation evaluation for any target language. ACL, 2014.\n[11] G. Li, S. Ma, and Y. Han. Summarization-based video caption via deep neural networks. In ACM Multimedia, 2015.\n[12] C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In ACL, 2004.\n[13] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dolla\u0301r, and C. L. Zitnick. Microsoft coco: Common objects in context. In ECCV. Springer, 2014.\n[14] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describing images using 1 million captioned photographs. In NIPS, 2011.\n[15] Y. Pan, T. Mei, T. Yao, H. Li, and Y. Rui. Jointly modeling embedding and translation to bridge video and language. arXiv preprint arXiv:1505.01861, 2015.\n[16] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002.\n[17] V. Ramanathan, P. Liang, and L. Fei-Fei. Video event understanding using natural language descriptions. In ICCV, 2013.\n[18] F. Shen, W. Liu, S. Zhang, Y. Yang, and H. Tao Shen. Learning binary codes for maximum inner product search. In ICCV, pages 4148\u20134156, December 2015.\n[19] F. Shen, C. Shen, W. Liu, and H. T. Shen. Supervised discrete hashing. In CVPR, pages 37\u201345, 2015.\n[20] K. Simonyan and A. Zisserman. Very deep\nconvolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014.\n[21] K. Tang, B. Yao, L. Fei-Fei, and D. Koller. Combining the right features for complex event recognition. In ICCV, 2013.\n[22] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015.\n[23] S. Venugopalan, M. Rohrbach, J. Donahue, R. Mooney, T. Darrell, and K. Saenko. Sequence to sequence-video to text. In ICCV, 2015.\n[24] S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. Mooney, and K. Saenko. Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729, 2014.\n[25] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR, 2015.\n[26] P. J. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550\u20131560, 1990.\n[27] Z. Wu, Y.-G. Jiang, X. Wang, H. Ye, X. Xue, and J. Wang. Fusing multi-stream deep networks for video classification. arXiv preprint arXiv:1509.06086, 2015.\n[28] Z. Wu, X. Wang, Y.-G. Jiang, H. Ye, and X. Xue. Modeling spatial-temporal clues in a hybrid deep learning framework for video classification. In ACM Multimedia, 2015.\n[29] K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2015.\n[30] Y. Yang, Z.-J. Zha, Y. Gao, X. Zhu, and T.-S. Chua. Exploiting web images for semantic video indexing via robust sample-specific loss. TMM, 16(6):1677\u20131689, 2014.\n[31] Y. Yang, H. Zhang, M. Zhang, F. Shen, and X. Li. Visual coding in a semantic hierarchy. In ACM Multimedia, pages 59\u201368, 2015.\n[32] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and A. Courville. Describing videos by exploiting temporal structure. In ICCV, 2015."}], "references": [{"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["D.L. Chen", "W.B. Dolan"], "venue": "In ACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C. Lawrence Zitnick"], "venue": "In CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "In ECCV. Springer,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Devnet: A deep event network for multimedia event detection and evidence recounting", "author": ["C. Gan", "N. Wang", "Y. Yang", "D.-Y. Yeung", "A.G. Hauptmann"], "venue": "In CVPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In ACM Multimedia,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Meteor universal: language specific translation evaluation for any target", "author": ["M.D.A. Lavie"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Summarization-based video caption via deep neural networks", "author": ["G. Li", "S. Ma", "Y. Han"], "venue": "In ACM Multimedia,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.-Y. Lin"], "venue": "In ACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In ECCV. Springer,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": "arXiv preprint arXiv:1505.01861,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "In ACL,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Video event understanding using natural language descriptions", "author": ["V. Ramanathan", "P. Liang", "L. Fei-Fei"], "venue": "In ICCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Learning binary codes for maximum inner product search", "author": ["F. Shen", "W. Liu", "S. Zhang", "Y. Yang", "H. Tao Shen"], "venue": "In ICCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Supervised discrete hashing", "author": ["F. Shen", "C. Shen", "W. Liu", "H.T. Shen"], "venue": "In CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Very deep  convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Combining the right features for complex event recognition", "author": ["K. Tang", "B. Yao", "L. Fei-Fei", "D. Koller"], "venue": "In ICCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Sequence to sequence-video to text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "In ICCV,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": "arXiv preprint arXiv:1412.4729,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1990}, {"title": "Fusing multi-stream deep networks for video classification", "author": ["Z. Wu", "Y.-G. Jiang", "X. Wang", "H. Ye", "X. Xue", "J. Wang"], "venue": "arXiv preprint arXiv:1509.06086,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Modeling spatial-temporal clues in a hybrid deep learning framework for video classification", "author": ["Z. Wu", "X. Wang", "Y.-G. Jiang", "H. Ye", "X. Xue"], "venue": "In ACM Multimedia,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Exploiting web images for semantic video indexing via robust sample-specific", "author": ["Y. Yang", "Z.-J. Zha", "Y. Gao", "X. Zhu", "T.-S. Chua"], "venue": "loss. TMM,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Visual coding in a semantic hierarchy", "author": ["Y. Yang", "H. Zhang", "M. Zhang", "F. Shen", "X. Li"], "venue": "In ACM Multimedia,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "In ICCV,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}], "referenceMentions": [{"referenceID": 30, "context": "Subsequently, visual content analysis for retrieving [31, 18] and understanding becomes a fundamental problem in the area of multimedia research, which has motivated world-wide researchers", "startOffset": 53, "endOffset": 61}, {"referenceID": 17, "context": "Subsequently, visual content analysis for retrieving [31, 18] and understanding becomes a fundamental problem in the area of multimedia research, which has motivated world-wide researchers", "startOffset": 53, "endOffset": 61}, {"referenceID": 8, "context": "Most previous works, however, have focused on classification task, such as annotating an image [9, 19] or video [5, 17, 21, 30] with given fixed label sets.", "startOffset": 95, "endOffset": 102}, {"referenceID": 18, "context": "Most previous works, however, have focused on classification task, such as annotating an image [9, 19] or video [5, 17, 21, 30] with given fixed label sets.", "startOffset": 95, "endOffset": 102}, {"referenceID": 4, "context": "Most previous works, however, have focused on classification task, such as annotating an image [9, 19] or video [5, 17, 21, 30] with given fixed label sets.", "startOffset": 112, "endOffset": 127}, {"referenceID": 16, "context": "Most previous works, however, have focused on classification task, such as annotating an image [9, 19] or video [5, 17, 21, 30] with given fixed label sets.", "startOffset": 112, "endOffset": 127}, {"referenceID": 20, "context": "Most previous works, however, have focused on classification task, such as annotating an image [9, 19] or video [5, 17, 21, 30] with given fixed label sets.", "startOffset": 112, "endOffset": 127}, {"referenceID": 29, "context": "Most previous works, however, have focused on classification task, such as annotating an image [9, 19] or video [5, 17, 21, 30] with given fixed label sets.", "startOffset": 112, "endOffset": 127}, {"referenceID": 3, "context": "With some pioneering methods [4, 14] tackling the challenge of describing images with natural language proposed, visual content understanding has attracted more and more attention.", "startOffset": 29, "endOffset": 36}, {"referenceID": 13, "context": "With some pioneering methods [4, 14] tackling the challenge of describing images with natural language proposed, visual content understanding has attracted more and more attention.", "startOffset": 29, "endOffset": 36}, {"referenceID": 1, "context": "State-of-the-art techniques for image captioning have been surpassed by new advanced approaches in succession [2, 3, 8, 25, 29].", "startOffset": 110, "endOffset": 127}, {"referenceID": 2, "context": "State-of-the-art techniques for image captioning have been surpassed by new advanced approaches in succession [2, 3, 8, 25, 29].", "startOffset": 110, "endOffset": 127}, {"referenceID": 7, "context": "State-of-the-art techniques for image captioning have been surpassed by new advanced approaches in succession [2, 3, 8, 25, 29].", "startOffset": 110, "endOffset": 127}, {"referenceID": 24, "context": "State-of-the-art techniques for image captioning have been surpassed by new advanced approaches in succession [2, 3, 8, 25, 29].", "startOffset": 110, "endOffset": 127}, {"referenceID": 28, "context": "State-of-the-art techniques for image captioning have been surpassed by new advanced approaches in succession [2, 3, 8, 25, 29].", "startOffset": 110, "endOffset": 127}, {"referenceID": 14, "context": "Recent researches [15, 24, 32, 11, 23] have been focusing on describing videos with more comprehensive sentences instead of simple keywords.", "startOffset": 18, "endOffset": 38}, {"referenceID": 23, "context": "Recent researches [15, 24, 32, 11, 23] have been focusing on describing videos with more comprehensive sentences instead of simple keywords.", "startOffset": 18, "endOffset": 38}, {"referenceID": 31, "context": "Recent researches [15, 24, 32, 11, 23] have been focusing on describing videos with more comprehensive sentences instead of simple keywords.", "startOffset": 18, "endOffset": 38}, {"referenceID": 10, "context": "Recent researches [15, 24, 32, 11, 23] have been focusing on describing videos with more comprehensive sentences instead of simple keywords.", "startOffset": 18, "endOffset": 38}, {"referenceID": 22, "context": "Recent researches [15, 24, 32, 11, 23] have been focusing on describing videos with more comprehensive sentences instead of simple keywords.", "startOffset": 18, "endOffset": 38}, {"referenceID": 31, "context": "proposed to use 3D Convolutional Neural Networks to explore local temporal information in video clips, where the most relevant temporal fragments were automatically chosen for generating natural language description with attention mechanism [32].", "startOffset": 241, "endOffset": 245}, {"referenceID": 22, "context": "In [23], Venugopanlan et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "We summarize the main contributions of this work as follows: (1) To our best knowledge, our approach is one of the first to utilize bidirectional recurrent neural networks for exploring bidirectional global temporal structure in video captioning; (2) We construct two sequential processing models for adaptive video representation learning and language description generation, respectively, rather than using the same LSTM for both video frames encoding and text decoding in [23]; and (3) Extensive experiments on a real-world video corpus illustrate the ar X iv :1 60 6.", "startOffset": 475, "endOffset": 479}, {"referenceID": 5, "context": "LSTM has been demonstrated to be able to effectively address the gradients vanishing or explosion problem [6] during back-propagation through time (BPTT) [26] and to exploit temporal dependencies in very long temporal structure.", "startOffset": 106, "endOffset": 109}, {"referenceID": 25, "context": "LSTM has been demonstrated to be able to effectively address the gradients vanishing or explosion problem [6] during back-propagation through time (BPTT) [26] and to exploit temporal dependencies in very long temporal structure.", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "Different from other video description approaches that represent video by implementing pooling across frames [24] or 3-D CNNs with local temporal structure [15], we apply BiLSTM networks to exploit the bidirectional temporal structure of video clips.", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "Different from other video description approaches that represent video by implementing pooling across frames [24] or 3-D CNNs with local temporal structure [15], we apply BiLSTM networks to exploit the bidirectional temporal structure of video clips.", "startOffset": 156, "endOffset": 160}, {"referenceID": 8, "context": "Convolutional Neural Networks (CNNs) has demonstrated overwhelming performance on image recognition, classification [9] and video content analysis [3, 23].", "startOffset": 116, "endOffset": 119}, {"referenceID": 2, "context": "Convolutional Neural Networks (CNNs) has demonstrated overwhelming performance on image recognition, classification [9] and video content analysis [3, 23].", "startOffset": 147, "endOffset": 154}, {"referenceID": 22, "context": "Convolutional Neural Networks (CNNs) has demonstrated overwhelming performance on image recognition, classification [9] and video content analysis [3, 23].", "startOffset": 147, "endOffset": 154}, {"referenceID": 6, "context": "Therefore, we extract caffe [7] fc7 layer of each frame through VGG-16 layers [20] caffemodel.", "startOffset": 28, "endOffset": 31}, {"referenceID": 19, "context": "Therefore, we extract caffe [7] fc7 layer of each frame through VGG-16 layers [20] caffemodel.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "Following [23, 24], we sample one frame from every ten frames in the video and extract the fc7 layer, the second fully-connected layer, to express selected frames.", "startOffset": 10, "endOffset": 18}, {"referenceID": 23, "context": "Following [23, 24], we sample one frame from every ten frames in the video and extract the fc7 layer, the second fully-connected layer, to express selected frames.", "startOffset": 10, "endOffset": 18}, {"referenceID": 26, "context": "In [27, 28], Wu et al.", "startOffset": 3, "endOffset": 11}, {"referenceID": 27, "context": "In [27, 28], Wu et al.", "startOffset": 3, "endOffset": 11}, {"referenceID": 22, "context": "Existing video captioning approaches usually share common part of visual model and language model as representation [23, 15], which may lead to severe information loss.", "startOffset": 116, "endOffset": 124}, {"referenceID": 14, "context": "Existing video captioning approaches usually share common part of visual model and language model as representation [23, 15], which may lead to severe information loss.", "startOffset": 116, "endOffset": 124}, {"referenceID": 23, "context": "Such methods may easily result in undesirable outputs due to the duplicate inputs in every time point of the new sequence [24].", "startOffset": 122, "endOffset": 126}, {"referenceID": 22, "context": "During test phrase, similar to [23], our language model takes the word wt\u22121 with maximum likelihood as input at time t repeatedly until the <EOS> token is emitted.", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "Video Dataset: We evaluate our approach by conducting experiments on the Microsoft Research Video Description (MSVD) [1] corpus, which is description for a collection of 1,970 video clips.", "startOffset": 117, "endOffset": 120}, {"referenceID": 14, "context": "Following the majority of prior works [15, 24, 23, 32], we split entire dataset into training, validation and test set with 1200, 100 and 670 snippets, respectively.", "startOffset": 38, "endOffset": 54}, {"referenceID": 23, "context": "Following the majority of prior works [15, 24, 23, 32], we split entire dataset into training, validation and test set with 1200, 100 and 670 snippets, respectively.", "startOffset": 38, "endOffset": 54}, {"referenceID": 22, "context": "Following the majority of prior works [15, 24, 23, 32], we split entire dataset into training, validation and test set with 1200, 100 and 670 snippets, respectively.", "startOffset": 38, "endOffset": 54}, {"referenceID": 31, "context": "Following the majority of prior works [15, 24, 23, 32], we split entire dataset into training, validation and test set with 1200, 100 and 670 snippets, respectively.", "startOffset": 38, "endOffset": 54}, {"referenceID": 12, "context": "COCO 2014 image description dataset [13] has been used to perform experiments frequently [8, 3, 2, 29], which consists of more than 120,000 images, about 82,000 and 40,000 images for training and test respectively.", "startOffset": 36, "endOffset": 40}, {"referenceID": 7, "context": "COCO 2014 image description dataset [13] has been used to perform experiments frequently [8, 3, 2, 29], which consists of more than 120,000 images, about 82,000 and 40,000 images for training and test respectively.", "startOffset": 89, "endOffset": 102}, {"referenceID": 2, "context": "COCO 2014 image description dataset [13] has been used to perform experiments frequently [8, 3, 2, 29], which consists of more than 120,000 images, about 82,000 and 40,000 images for training and test respectively.", "startOffset": 89, "endOffset": 102}, {"referenceID": 1, "context": "COCO 2014 image description dataset [13] has been used to perform experiments frequently [8, 3, 2, 29], which consists of more than 120,000 images, about 82,000 and 40,000 images for training and test respectively.", "startOffset": 89, "endOffset": 102}, {"referenceID": 28, "context": "COCO 2014 image description dataset [13] has been used to perform experiments frequently [8, 3, 2, 29], which consists of more than 120,000 images, about 82,000 and 40,000 images for training and test respectively.", "startOffset": 89, "endOffset": 102}, {"referenceID": 23, "context": "Video Preprocessing: As previous video description works [24, 23, 15] , we sample video frames once in every ten frames, then these frames could represent given video and", "startOffset": 57, "endOffset": 69}, {"referenceID": 22, "context": "Video Preprocessing: As previous video description works [24, 23, 15] , we sample video frames once in every ten frames, then these frames could represent given video and", "startOffset": 57, "endOffset": 69}, {"referenceID": 14, "context": "Video Preprocessing: As previous video description works [24, 23, 15] , we sample video frames once in every ten frames, then these frames could represent given video and", "startOffset": 57, "endOffset": 69}, {"referenceID": 22, "context": "We employ a bidirectional S2VT [23] and a joint bidirectional LSTM structure to investigate the performance of our bidirectional approach.", "startOffset": 31, "endOffset": 35}, {"referenceID": 14, "context": "For convenient comparison, we set the size of hidden unit of all LSTMs in our system to 512 as [15, 23], except for the first video encoder in unidirectional joint LSTM.", "startOffset": 95, "endOffset": 103}, {"referenceID": 22, "context": "For convenient comparison, we set the size of hidden unit of all LSTMs in our system to 512 as [15, 23], except for the first video encoder in unidirectional joint LSTM.", "startOffset": 95, "endOffset": 103}, {"referenceID": 22, "context": "We note that over 99% of the descriptions in MSVD and COCO 2014 contain no more than 40 words, and in [23], Venugopalan et al.", "startOffset": 102, "endOffset": 106}, {"referenceID": 22, "context": "Bidirectional S2VT: Similar to [23], we implement several S2VT-based models: S2VT, bidirectional S2VT and reinforced S2VT with bidirectional LSTM video encoder.", "startOffset": 31, "endOffset": 35}, {"referenceID": 22, "context": "We conduct experiment on S2VT using our video features and LSTM structure instead of the end-to-end model in [23], which need original RGB frames as input.", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "BLEU [16], METEOR [10], ROUGE-L [12] and CIDEr [22] are common evaluation metrics in image and video description, the first three were originally proposed to evaluate machine translation at the earliest and CIDEr was proposed to evaluate image description with sufficient reference sentences.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "BLEU [16], METEOR [10], ROUGE-L [12] and CIDEr [22] are common evaluation metrics in image and video description, the first three were originally proposed to evaluate machine translation at the earliest and CIDEr was proposed to evaluate image description with sufficient reference sentences.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "BLEU [16], METEOR [10], ROUGE-L [12] and CIDEr [22] are common evaluation metrics in image and video description, the first three were originally proposed to evaluate machine translation at the earliest and CIDEr was proposed to evaluate image description with sufficient reference sentences.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "BLEU [16], METEOR [10], ROUGE-L [12] and CIDEr [22] are common evaluation metrics in image and video description, the first three were originally proposed to evaluate machine translation at the earliest and CIDEr was proposed to evaluate image description with sufficient reference sentences.", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "The authors of CIDEr also argued for that METEOR outperforms CIDEr when the reference set is small [22].", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "S2VT [23]", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "LSTM-E (VGG) [15] 29.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "5 LSTM-E (C3D) [15] 29.", "startOffset": 15, "endOffset": 19}, {"referenceID": 31, "context": "[32] 29.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The result of \u201cLSTM\u201d in first row refer from [15] and the last row but one denotes the best model combining local temporal structure using C3D with global temporal structure utilizing temporal attention in [32].", "startOffset": 45, "endOffset": 49}, {"referenceID": 31, "context": "The result of \u201cLSTM\u201d in first row refer from [15] and the last row but one denotes the best model combining local temporal structure using C3D with global temporal structure utilizing temporal attention in [32].", "startOffset": 206, "endOffset": 210}, {"referenceID": 22, "context": "We observed that while our unidirectional S2VT has the same deployment as [23], our model gives a little poorer performance(line 1, Table 1 and line 3, Table 2).", "startOffset": 74, "endOffset": 78}, {"referenceID": 22, "context": "We also note some limitations in our model, such as endto-end framework employed in [23] and distance measured in [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "We also note some limitations in our model, such as endto-end framework employed in [23] and distance measured in [15].", "startOffset": 114, "endOffset": 118}], "year": 2016, "abstractText": "Video captioning has been attracting broad research attention in multimedia community. However, most existing approaches either ignore temporal information among video frames or just employ local contextual temporal knowledge. In this work, we propose a novel video captioning framework, termed as Bidirectional Long-Short Term Memory (BiLSTM), which deeply captures bidirectional global temporal structure in video. Specifically, we first devise a joint visual modelling approach to encode video data by combining a forward LSTM pass, a backward LSTM pass, together with visual features from Convolutional Neural Networks (CNNs). Then, we inject the derived video representation into the subsequent language model for initialization. The benefits are in two folds: 1) comprehensively preserving sequential and visual information; and 2) adaptively learning dense visual features and sparse semantic representations for videos and sentences, respectively. We verify the effectiveness of our proposed video captioning framework on a commonlyused benchmark, i.e., Microsoft Video Description (MSVD) corpus, and the experimental results demonstrate that the superiority of the proposed approach as compared to several state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}