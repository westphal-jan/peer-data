{"id": "1606.04754", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "A Correlational Encoder Decoder Architecture for Pivot Based Sequence Generation", "abstract": "consensus based machine method ( mt ) aims to encode unique meanings allowing creating token morphological token and then decode sentences through unique target expressions resembling regular representation. while human work we explore this idea towards the emergence of neural mapped decoder architectures, albeit gaining a smaller hardware and without mt as the key goal. specifically, please report by instance that three maps or varieties x, z and k besides we fully interested in generating sequences in texts starting from positions available in x. however, there emerges no real training data transmission between nc and y both, training data is available making rm & amp ; rc and bass & amp ; y ( as is often the effect in many real linguist applications ). z thus acts well auditory reward / correlation. under obvious response, however is perhaps infinitely elegant but works very whilst adding depth is to train as two stage model which first converts from x at z output then from bb to cf. next we explore an automatically inspired solution which jointly learns to do the procedure ( i ) merge x and yi to provide common representation and ( ii ) decode y up this common representation. we evaluate our problems on two tasks : ( by ) bridge initiation and ( ii ) file propagation. we report exceptional behaviour in both these applications that believe suggest this is a right process towards having interlingua interactive devices for rendering.", "histories": [["v1", "Wed, 15 Jun 2016 13:27:16 GMT  (1711kb,D)", "http://arxiv.org/abs/1606.04754v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amrita saha", "mitesh m khapra", "sarath chandar", "janarthanan rajendran", "kyunghyun cho"], "accepted": false, "id": "1606.04754"}, "pdf": {"name": "1606.04754.pdf", "metadata": {"source": "CRF", "title": "A Correlational Encoder Decoder Architecture for Pivot Based Sequence Generation", "authors": ["Amrita Saha", "Mitesh M Khapra", "Sarath Chandar", "Janarthanan Rajendran", "Kyunghyun Cho"], "emails": ["amrsaha4@in.ibm.com", "mikhapra@in.ibm.com", "apsarathchandar@gmail.com", "rsdjjana@gmail.com", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Interlingua based MT (Nirenburg, 1994; Dorr et al., 2010) relies on the principle that every language in the world can be mapped to a common linguistic\nrepresentation. Further, given this representation, it should be possible to decode a target sentence in any language. This implies that given n languages we just need n decoders and n encoders to translate between these nC2 language pairs. Note that even though we take inspiration from interlingua based MT, the focus of this work is not on MT. We believe that this idea is not just limited to translation but could be applicable to any kind of conversion involving multiple source and target languages and/or modalities (for example, transliteration, multilingual image captioning, multilingual image Question Answering, etc.). Even though this idea has had limited success, it is still fascinating and considered by many as the holy grail of multilingual multimodal processing.\nIt is interesting to consider the implications of this idea when viewed in the statistical context. For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al., 2015b; Xu et al., 2015), etc. require parallel data between the source and target views (where a view could be a language or some other modality like image). Thus, given n views, we require nC2 parallel datasets to build systems which can convert from any source view to any target view. Obviously, this does not scale well in practice because it is hard to find parallel data between all nC2 views. For example, publicly available parallel datasets for transliteration (Zhang et al., 2012) cater to < 20 languages. Similarly, publicly available image caption datasets are available only for ar X iv :1\n60 6.\n04 75\n4v 1\n[ cs\n.C L\n] 1\n5 Ju\nn 20\nEnglish1 and German2. This problem of resource scarcity could be alleviated if we could learn only n statistical encoders and n statistical decoders wherein (i) the encoded representation is common across languages and (ii) the decoders can decode from this common representation (akin to interlingua based conversion). As a small step in this direction, we consider a scaled down version of this generic nC2 conversion problem. Specifically, we consider the case where we have three views X , Z, Y but parallel data is available only between XZ and ZY (instead of all 3C2 parallel datasets). At test time, we are interested in generating natural language sequences in Y starting from information available in X . We refer to this as the bridge setup as the language Z here can be considered to be a bridge/pivot between X and Y .\nAn obvious solution to the above problem is to train a two-stage system which first converts fromX to Z and then from Z to Y . While this solution may work very well in practice (as our experiments indeed suggest) it is perhaps less elegant and becomes tedious as the number of views increases. For example, consider the case of converting from X to Z to R to Y . Instead, we suggest a neural network based model which simultaneously learns the following (i) a common representation for X and Z and (ii) decoding Y from this common representation. In other words, instead of training two independent models using the datasets between XZ and ZY , the model jointly learns from the two datasets. The resulting common representation learned for X and Z can be viewed as a vectorial analogue of the linguistic representation sought by interlingua based approaches. Of course, by no means do we suggest that this vectorial representation is a substitute for the rich linguistic representation but its easier to learn from parallel data (as opposed to a linguistic representation which requires hand crafted resources).\nNote that our work should not be confused with the recent work of (Firat et al., 2016), (Zoph and Knight, 2016) and (Elliott et al., 2015). The last two works in fact require 3-way parallel data betweenX , Z and Y and learn to decode sequences in Y given both X and Z. For example, at test time, (Elliott et\n1http://mscoco.org/dataset/$#$download 2http://www.statmt.org/wmt16/\nmultimodal-task.html\nal., 2015) generate captions in German, given both (i) the image and (ii) its corresponding English caption. This is indeed very different from the problem addressed in this paper. Similarly, even though (Firat et al., 2016) learn a single encoder per language and a single decoder per language they do not learn shared representations for multiple languages (only the attention mechanism is shared). Further, in all their experiments they require parallel data between the two languages of interest. Specifically, they do not consider the case of generating sentences in Y given a sentence in X when no parallel data is available between X and Y .\nWe present an empirical comparison of jointly trained models which explicitly aim for shared encoder representations with two-stage architectures. We consider two downstream applications (i) bridge transliteration and (ii) bridge caption generation. We use the standard NEWS 2012 dataset (Zhang et al., 2012) for transliteration. We consider transliteration between 12 languages pairs (XY ) using English as the bridge (Z). Bridge caption generation is a new task introduced in this paper where the aim is to generate French captions for an image when no ImageFrench(XY ) parallel data is available for training. Instead training data is available between ImageEnglish (XZ) and English-French (ZY ). In both these tasks we report promising results. In fact, in our multilingual transliteration experiments we are able to beat the strong two-stage baseline in many cases. These results show potential for further research in interlingua inspired neural network architectures. We do acknowledge that a successful interlingua based statistical solution requiring only n encoders and n decoders is a much harder task whereas our work is only a small step in that direction."}, {"heading": "2 Related Work", "text": "Encoder decoder based architectures for sequence to sequence generation were initially proposed in (Cho et al., 2014; Sutskever et al., 2014) in the context of Machine Translation (MT) and have also been successfully used for generating captions for images (Vinyals et al., 2015b). However, such sequence to sequence models are often difficult to train as they aim to encode the entire source sequence using a fixed encoder representation. Bahdanau et al.\n(2014) introduced attention based models wherein a different representation is fed to the decoder at each time step by focusing the attention on different parts of the input sequence. Such attention based models have been more successful than vanilla encoderdecoder models and have been used successfully for MT (Bahdanau et al., 2014), parsing (Vinyals et al., 2015a), speech recognition (Chorowski et al., 2015), image captioning (Xu et al., 2015) among other applications. All the above mentioned works focus only on the case when there is one source and one target. The source can be image, text, or speech signal but the target is always a text sequence.\nEncoder decoder models in a multi-source, single target setting have been explored by (Elliott et al., 2015) and (Zoph and Knight, 2016). Specifically, Elliott et al. (2015) try to generate a German caption from an image and its corresponding English caption. Similarly, Zoph and Knight (2016) focus on the problem of generating English translations given the same sentence in both French and German. We would like to highlight that both these models require three-way parallel data while we are focusing on situations where such data is not available. Single source, multi-target and multi-source, single target settings have been considered in (Luong et al., 2015a). Recent work by Firat et al. (2016) explores multi-source to multi-target encoder decoder models in the context of Machine Translation. However, Firat et al. (2016) focus on multi-task learning with a shared attention mechanism and the goal is to improve the MT performance for a pair of languages for which parallel data is available. This is clearly different from the goal of this paper which is to design encoder decoder models for a pair of languages for which no parallel data is available but data is available only between each of these languages and a bridge language.\nOf course, in general the idea of pivot/bridge/interlingua based conversion is not new and has been used previously in several non-neural network settings. For example (Khapra et al., 2010) use a bridge language or pivot language to do machine transliteration. Similarly, (Wu and Wang, 2007; Zhu et al., 2014) do pivot based machine translation. Lastly, we would also like to mention the work in interlingua based Machine Translation (Nirenburg, 1994; Dorr et al., 2010)\nwhich is clearly the inspiration for this work even though the focus of this work is not on MT.\nThe main theme explored in this paper is to learn a common representation for two views with the end goal of generating a target sequence in a third view. The idea of learning common representations for multiple views has been explored well in the past (Klementiev et al., 2012; Chandar et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2016; Rajendran et al., 2015). For example, Andrew et al. (2013) propose Deep CCA for learning a common representation for two views. (Chandar et al., 2014; Chandar et al., 2016) propose correlational neural networks for common representation learning and Rajendran et al. (2015) propose bridge correlational networks for multilingual multimodal representation learning. From the point of view of representation learning, the work of Rajendran et al. (2015) is very similar to our work except that it focuses only on representation learning and does not consider the end goal of generating sequences in a target language."}, {"heading": "3 Models", "text": "As mentioned earlier, one of the aims of this work is to compare a jointly trained model with a two stage model. We first briefly describe such a two stage encoder decoder architecture and then describe our model which is a correlation based jointly trained encoder decoder model."}, {"heading": "3.1 A two stage encoder-decoder model", "text": "A two stage encoder-decoder is a straight-forward extension of sequence to sequence models (Cho et al., 2014; Sutskever et al., 2014) to the bridge setup. Given parallel data between XZ and ZY , a two stage model will learn a generative model for each of the pairs independently. For the purpose of this work, the source can be an image or text but the target is always a natural language text. For encoding an image, we simply take its feature representation obtained from one of the fully connected layers of a convolutional neural network and pass it through a feed-forward layer. On the other hand, for encoding the source text sequence, we use a recurrent neural network. The decoder is always a recurrent neural network which generates the text sequence, one to-\nken at a time.\nLet the two training sets be D1 = {xi, zi}N1i=1 and D2 = {zi, yi}N2i=1 where xi \u2208 X , yi \u2208 Y and zi \u2208 Z. Given D1, the first encoder learns to encode xi and decode the corresponding zi from this encoded representation. The second encoder is trained independently of the first encoder and usesD2 to encode zi \u2208 Z and decode the corresponding yi \u2208 Y from this encoded representation. These independent training processes are indicated by the dotted arrows in Figure 1. At test time, the two stages are run sequentially. In other words, given xj , we first encode it and decode zj from it using the first encoder-decoder model. This decoded zj is then fed to the second encoder-decoder model to generate yj . This sequential test process is indicated by solid arrows in Figure 1.\nWhile this two stage encoder-decoder model is a trivial extension of a single encoder-decoder model, it serves as a very strong baseline as we will see later in the experiments section."}, {"heading": "3.2 A correlation based joint encoder-decoder model", "text": "While the above model works well in practice, it becomes cumbersome when more views are involved (for example, when converting from U to X to Y to Z). We desire a more elegant solution which could scale even when more views are involved (although for the purpose of this work, we restrict ourselves to 3 views only). To this end, we propose a joint\nmodel which uses the parallel data D1 (as defined above) to learn one encoder each for X and Z such that the representations of xi and zi are correlated. In addition and simultaneously the model uses D2 and learns to decode yj from zj . Note that this joint training has the advantage that the encoder for Z benefits from instances in D1 and D2.\nHaving given an intuitive explanation of the model, we now formally define the objective functions used during training. Given D1 = {xi, zi}N1i=1, the model tries to maximize the correlation between the encoded representations of xi and zi as defined below.\nJcorr(\u03b8) = \u2212\u03bb corr(s(hX(X)), s(hZ(Z))) (1)\nwhere hX is the representation computed by the encoder for X and hZ is the representation computed by the encoder for Z. As mentioned earlier, these encoders could be RNN encoders (in the case of text) and simple feedforward encoders (in the case of images). s() is a standardization function which adjusts the hidden representations hX and hY so that they have zero-mean and unit-variance. Further, \u03bb is a scaling hyper-parameter and corr is the correlation function as defined below:\nN\u2211 i=1 s(hX(xi))s(hZ(zi)) T (2)\nWe would like to emphasize that s() ensures that the representations already have zero mean and unit variance and hence no separate standardization is required while computing the correlation.\nIn addition to the above loss function, givenD2 = {zi, yi}N2i=1, the model minimizes the following cross entropy loss:\nJce(\u03b8) = 1\nN2 N2\u2211 k=1 P (yk|zk) (3)\nwhere\nP (yk|zk) = L\u220f i=1 P (yki |yk<i , zk) (4)\nwhere L is the number of tokens in yk. The dotted lines in Figure 2 show the joint training process where the model simultaneously learns\nto compute correlated representations for xi and zi and decode yi from zi. The testing process is shown by the solid lines wherein the model computes a hidden representation for xi and then decodes yi from it directly without transitioning through zi.\nWhile training, we alternately pick mini-batches from D1 and D2 and use the corresponding objective function. Means and variances for the representations computed by the two encoders are updated at the end of every epoch based on the hidden representations of all instances in the training data. During the first epoch we assume the mean and variance to be 0 and 1. Note that \u03bb rescales the value of the correlation loss term so that it is in the same range as the value of the cross-entropy loss term."}, {"heading": "4 Experiment 1: Bridge Transliteration", "text": "We consider the task of transliteration between two languages X and Y when no direct data is available between them but parallel data is available between X & Z and Z & Y . In the following subsections we describe the datasets used for our task, the hyperparameters considered for our experiments and results."}, {"heading": "4.1 Datasets", "text": "We consider transliteration between 4 languages, viz., Hindi, Kannada, Tamil and Marathi resulting in 4C2 = 12 language pairs. However, we do not\nuse any direct parallel data between any of these languages. Instead we use the standard datasets available between English and each of these languages which were released as part of the NEWS 2012 shared task (Zhang et al., 2012). Just to be explicit, for the task of transliterating from Hindi to Kannada, we construct D1 from the English-Hindi dataset and D2 from the English-Kannada dataset. Table 1 summarizes the sizes of these parallel datasets. Fortunately, the English portion of the test set was common across all the 4 language pairs mentioned in Table 1. This allowed us to easily create test sets for all the 12 language pairs. For example, if hi is the transliteration of the English word ei in the EnglishHindi test set and ki is the transliteration of the same English word ei in the English-Kannada test set then we add (hi, ki) as a transliteration pair in our HindiKannada test set. In this way, we created test sets containing 1000 words for all the 12 language pairs."}, {"heading": "4.2 Hyperparameters", "text": "For the two stage encoder decoder model, we considered the following hyperparameters: embedding size \u2208 {1024, 2048} for characters, rnn hidden unit size \u2208 {1024, 2048}, initial learning rate \u2208 {0.01, 0.001} and batch size \u2208 {32, 64}. The numbers in bracket indicate the distinct values that we considered for each hyperparameter. Note that the embedding size and rnn size are always kept equal. All these parameters were tuned independently for the two stages using their respective validation sets. For the correlated encoder decoder model, in addition to the above hyperparamaters we also had \u03bb \u2208 [0.1, 1.0] as a hyperparameter. Here, we tuned the hyperparameters based on the performance on the validation set available between ZY (since the correlated encoder decoder can also decode yi \u2208 Y from zi \u2208 Z). Note that we do not use any parallel data between XY for tuning the hyperparame-\nnumber in this table signifies that for that specific language pair the corresponding system is performing better than the Two Stage\nPBSMT model and the best performing system for any of the language-pairs is represented in bold font\nters because the general assumption is that no parallel data is available between XY . We used Adam (Kingma and Ba, 2014) as the optimizer for all our experiments."}, {"heading": "4.3 Results", "text": "We compare our model with the following systems:\n1. Two Stage PBSMT: Here, we train two PBSMT (Koehn et al., 2003) based transliteration systems using D1 and D2. This is an additional baseline to see how well an encoder decoder architecture compares to a conventional PBSMT based system. We used Moses (Koehn et al., 2007) for building our PBSMT systems. The decoder parameters were tuned using the validation sets. Language model was trained on the target portion of the parallel corpus.\n2. Two Stage Encoder Decoder: Here, we train two encoder decoder based transliteration systems using D1 and D2 as described in Section 4.\nTable 2 summarizes the accuracy (% of correct transliterations) of the three systems in the bridge setup. We observe that in 6 out of the 12 language pairs our correlated model does better than the 2 stage encoder decoder model. Further, it does better than the two-stage PBSMT baseline in 11 out of the 12 language pairs. This is very encouraging especially because such 2-stage approaches are considered to be very strong baselines for these tasks (Khapra et al., 2010). In general, the encoder decoder based approaches do better than PBSMT based systems. This is indeed the case even when we compare the performance of the PBSMT based system and the Encoder Decoder based system independently on the two stages (Table 3)."}, {"heading": "5 Experiment 2: Bridge Captioning", "text": "We now introduce the task of bridge caption generation. The purpose of introducing this task is twofold. Firstly, we feel that it is important to put things in perspective and demonstrate that while interlingua inspired encoder decoder architectures are a step in the right direction, much more work is needed when dealing with different modalities in a bridge setup. Secondly, we think that this is an important task which has not received any attention in the past. We would like to formally define and report some initial baselines to motivate further research in this area. The formal task definition is as follows: Generate captions for images in language L1 (say, French) when no parallel data is available between images and L1 but parallel data is available between Image-L2 (D1) and between L1-L2 (D2) where L2 is another language (say, English). In the following subsection we describe the datasets used for this task, the hyperparameters considered for our experiments and the results."}, {"heading": "5.1 Datasets", "text": "Even though we do not have direct training data between Image-French, we need some test data to evaluate our model. For this, we use the Image-French test set recently released by (Rajendran et al., 2015). To create this data, they first merged the 80K images from the standard train split and 40K images from the standard valid split of MSCOCO data3. They then randomly split the merged 120K images into train(118K), validation (1K) and test set (1K). They then collect French translations for all the 5 captions for each image in the test set using crowdsourcing. CrowdFlower4 was used as the crowdsourcing platform and they solicited one French and one German translation for each of the 5000 captions using native speakers. Note that (Rajendran et al., 2015) report results for cross modal search and do not address the problem of crosslingual image captioning.\nIn our model, for D1 we use the same train(118K), validation (1K) and test sets (1K) as defined in (Rajendran et al., 2015) and explained above. Choosing D2 was a bit more tricky. Initially we considered the corpus released as part of WMT\u201912 (Callison-Burch et al., 2012) which contains roughly 44M English-French parallel sentences from various sources including News, parliamentary proceedings, etc. However, our initial small scale experiments showed that this does not work well because there is a clear mismatch between the vocabulary of this corpus and the vocabulary that we need for generating captions. Also the vocabulary is much larger (at least an order higher than what we need for image captioning) and it thus hampers training. Further, the average length and structure of these sentences is also very different from captions. Domain shift in MT is itself a challenging problem (not to mention the added complexity in a multimodal bridge setup). It was unrealistic to ex-\n3http://mscoco.org/dataset/$#$download 4https://make.crowdflower.com\npect our model to work in the presence of these orthogonal complexities.\nTo isolate these issues and evaluate our model in a controlled environment, we needed a parallel corpus which had very similar characteristics to that observed in captions. Since we did not have such a corpus at our disposal we decided to follow (Rajendran et al., 2015) and use a pseudo parallel corpus between English-French. Specifically, we take the English captions from the MSCOCO data and translate them to French using the publicly available translation system provided by IBM5. Note that our model still does not see direct parallel data between Image and French during training. We acknowledge that this is not the ideal thing to do but it is good enough to do a proof-of-concept evaluation of our model and understand its potential. We, of course, account for the liberty taken here by comparing with equally strong baselines as discussed later in the results section."}, {"heading": "5.2 Hyperparameters", "text": "Our model has the following hyperparameters: embedding size, batch size, hidden representation size, \u03bb and learning rate. Based on experiments involving direct Image-to-English caption generation we observed that the following parameters work well : embedding size = 512, batch size = 80, rnn hidden unit size = 512, and learning rate = 4e-4 with Adam (Kingma and Ba, 2014) as the optimizer. We just retained these hyper-parameters and did not tune them again for the bridge setup. We tuned the value of \u03bb by evaluating the correlation loss on the ImageEnglish validation set. Again, we do not use any Image-French data for tuning any hyperparameters."}, {"heading": "5.3 Results", "text": "We now present the results of our experiments where we compare with the following strong baselines.\n5http://www.ibm.com/smarterplanet/us/en/ ibmwatson\n1. Two Stage : Here we use a Show & Tell model (Vinyals et al., 2015b) trained using D1 to generate an English caption for the image. We then translate this caption into French using IBM\u2019s translation system as described above. 2. Pseudo Im-Fr : Here we train an Image-toFrench Show & Tell model (Vinyals et al., 2015b) by pairing the images in the MSCOCO dataset with their pseudo French captions generated by translating the English captions into French (using IBM\u2019s translation system).\nWe observe that our model is unable to beat the two strong baselines described above but still comes close to their performance. We believe this reinforces our belief in this line of research and hopefully more powerful models (perhaps attention based) could eventually surpass these two baselines.\nAs a qualitative evaluation of our model, Table 5 shows the captions generated by our model. It is exciting that even in a complex multimodal bridge setup the model is able to capture correlations between Images and English sentences and further decode relevant French captions from a given image."}, {"heading": "6 Conclusion", "text": "In this paper, we considered the problem of pivot based sequence generation. Specifically, we are in-\nterested in generating sequences in a target language starting from information in a source view. However, no direct training data is available between the source and target views but training data is available between each of these views and a pivot view. To this end, we take inspiration from interlingua based MT and propose a neural network based model which explicitly maximizes the correlation between the source and pivot view and simultaneously learns to decode target sequences from this correlated representation. We evaluate our model on the task of bridge transliteration and show that it outperforms a strong two-stage baseline for many language pairs. Finally, we introduce the task of bridge caption generation and report promising initial results. We hope this new task will fuel further research in this area.\nAs future work, we would like to go beyond simple encoder decoder based correlational models. For example, we would like to apply the idea of correlation to attention based encoder decoder models. The ideas expressed here can also be applied to other tasks such as bridge translation, bridge Image QA, etc. However, for these tasks, additional issues such as larger vocabulary sizes, complex sentence structures, non-monotonic alignments between source and target language pairs need to be addressed. The model proposed here is just a beginning and much\nmore work is needed to cater to these complex tasks."}], "references": [{"title": "Deep canonical correlation analysis", "author": ["Andrew et al.2013] Galen Andrew", "Raman Arora", "Jeff Bilmes", "Karen Livescu"], "venue": null, "citeRegEx": "Andrew et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Findings of the 2012 workshop on statistical machine translation", "author": ["Philipp Koehn", "Christof Monz", "Matt Post", "Radu Soricut", "Lucia Specia"], "venue": "In Seventh Workshop on Statistical Machine Translation,", "citeRegEx": "Callison.Burch et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2012}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Stanislas Lauly", "Hugo Larochelle", "Mitesh M Khapra", "Balaraman Ravindran", "Vikas Raykar", "Amrita Saha"], "venue": "Proceedings of NIPS", "citeRegEx": "Chandar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "Correlational neural networks", "author": ["Mitesh M. Khapra", "Hugo Larochelle", "Balaraman Ravindran"], "venue": "Neural Computation,", "citeRegEx": "Chandar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2016}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang"], "venue": "ACL", "citeRegEx": "Chiang.,? \\Q2005\\E", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural In-", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Interlingual annotation", "author": ["Dorr et al.2010] Bonnie J. Dorr", "Rebecca J. Passonneau", "David Farwell", "Rebecca Green", "Nizar Habash", "Stephen Helmreich", "Eduard H. Hovy", "Lori S. Levin", "Keith J. Miller", "Teruko Mitamura", "Owen Rambow", "Advaith Siddharthan"], "venue": null, "citeRegEx": "Dorr et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dorr et al\\.", "year": 2010}, {"title": "Multi-language image description with neural sequence models. CoRR, abs/1510.04709", "author": ["Stella Frank", "Eva Hasler"], "venue": null, "citeRegEx": "Elliott et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Neural network transduction models in transliteration generation", "author": ["Finch et al.2015] Andrew Finch", "Lemao Liu", "Xiaolin Wang", "Eiichiro Sumita"], "venue": "Proceedings of NEWS 2015 The Fifth Named Entities Workshop,", "citeRegEx": "Finch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Finch et al\\.", "year": 2015}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Firat et al.2016] Orhan Firat", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Hermann", "Blunsom2014] Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Everybody loves a rich cousin: An empirical study of transliteration through bridge languages", "author": ["A. Kumaran", "Pushpak Bhattacharyya"], "venue": "In Human Language Technologies: Conference of the North American Chapter of the As-", "citeRegEx": "Khapra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Khapra et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization. CoRR, abs/1412.6980", "author": ["Kingma", "Ba2014] Diederik P. Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Inducing Crosslingual Distributed Representations of Words", "author": ["Ivan Titov", "Binod Bhattarai"], "venue": "In Proceedings of the International Conference on Computational Linguistics (COLING)", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In HLT-NAACL", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Multi-task sequence to sequence learning. CoRR, abs/1511.06114", "author": ["Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation. CoRR, abs/1508.04025", "author": ["Hieu Pham", "Christopher D. Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Multiple system combination for transliteration", "author": ["Bradley Hauer", "Mohammad Salameh", "Adam St Arnaud", "Ying Xu", "Lei Yao", "Grzegorz Kondrak"], "venue": "Proceedings of NEWS 2015 The Fifth Named Entities Workshop,", "citeRegEx": "Nicolai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nicolai et al\\.", "year": 2015}, {"title": "Pangloss: A machine translation project. In Human Language Technology, Proceedings of a Workshop held at Plainsboro, New Jerey", "author": ["Sergei Nirenburg"], "venue": null, "citeRegEx": "Nirenburg.,? \\Q1994\\E", "shortCiteRegEx": "Nirenburg.", "year": 1994}, {"title": "Bridge correlational neural networks for multilingual multimodal representation learning. CoRR, abs/1510.03519", "author": ["Mitesh M. Khapra", "Sarath Chandar", "Balaraman Ravindran"], "venue": null, "citeRegEx": "Rajendran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rajendran et al\\.", "year": 2015}, {"title": "Boosting english-chinese machine transliteration via high quality alignment and multilingual resources", "author": ["Shao et al.2015] Yan Shao", "J\u00f6rg Tiedemann", "Joakim Nivre"], "venue": "Proceedings of NEWS 2015 The Fifth Named Entities Workshop,", "citeRegEx": "Shao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shao et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "2015a. Grammar as a foreign language", "author": ["Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "2015b. Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Pivot language approach for phrase-based statistical machine translation", "author": ["Wu", "Wang2007] Hua Wu", "Haifeng Wang"], "venue": "ACL", "citeRegEx": "Wu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2007}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Report of news 2012 machine transliteration shared task", "author": ["Zhang et al.2012] Min Zhang", "Haizhou Li", "A. Kumaran", "Ming Liu"], "venue": "In Proceedings of the 4th Named Entity Workshop,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Improving pivot-based statistical machine translation by pivoting the co-occurrence count of phrase pairs", "author": ["Zhu et al.2014] Xiaoning Zhu", "Zhongjun He", "Hua Wu", "Conghui Zhu", "Haifeng Wang", "Tiejun Zhao"], "venue": "In Proceedings of the 2014 Conference on Empiri-", "citeRegEx": "Zhu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "Interlingua based MT (Nirenburg, 1994; Dorr et al., 2010) relies on the principle that every language in the world can be mapped to a common linguistic representation.", "startOffset": 21, "endOffset": 57}, {"referenceID": 8, "context": "Interlingua based MT (Nirenburg, 1994; Dorr et al., 2010) relies on the principle that every language in the world can be mapped to a common linguistic representation.", "startOffset": 21, "endOffset": 57}, {"referenceID": 16, "context": "For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al.", "startOffset": 65, "endOffset": 120}, {"referenceID": 5, "context": "For example, current state of the art statistical systems for MT (Koehn et al., 2003; Chiang, 2005; Luong et al., 2015b), transliteration (Finch et al.", "startOffset": 65, "endOffset": 120}, {"referenceID": 10, "context": ", 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al.", "startOffset": 26, "endOffset": 87}, {"referenceID": 22, "context": ", 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al.", "startOffset": 26, "endOffset": 87}, {"referenceID": 19, "context": ", 2015b), transliteration (Finch et al., 2015; Shao et al., 2015; Nicolai et al., 2015), image captioning (Vinyals et al.", "startOffset": 26, "endOffset": 87}, {"referenceID": 27, "context": ", 2015), image captioning (Vinyals et al., 2015b; Xu et al., 2015), etc.", "startOffset": 26, "endOffset": 66}, {"referenceID": 28, "context": "For example, publicly available parallel datasets for transliteration (Zhang et al., 2012) cater to < 20 languages.", "startOffset": 70, "endOffset": 90}, {"referenceID": 11, "context": "Note that our work should not be confused with the recent work of (Firat et al., 2016), (Zoph and Knight, 2016) and (Elliott et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 9, "context": ", 2016), (Zoph and Knight, 2016) and (Elliott et al., 2015).", "startOffset": 37, "endOffset": 59}, {"referenceID": 6, "context": "Encoder decoder based architectures for sequence to sequence generation were initially proposed in (Cho et al., 2014; Sutskever et al., 2014) in the context of Machine Translation (MT) and have also been successfully used for generating captions for images (Vinyals et al.", "startOffset": 99, "endOffset": 141}, {"referenceID": 23, "context": "Encoder decoder based architectures for sequence to sequence generation were initially proposed in (Cho et al., 2014; Sutskever et al., 2014) in the context of Machine Translation (MT) and have also been successfully used for generating captions for images (Vinyals et al.", "startOffset": 99, "endOffset": 141}, {"referenceID": 1, "context": "els have been more successful than vanilla encoderdecoder models and have been used successfully for MT (Bahdanau et al., 2014), parsing (Vinyals et al.", "startOffset": 104, "endOffset": 127}, {"referenceID": 7, "context": ", 2015a), speech recognition (Chorowski et al., 2015), image captioning (Xu et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 27, "context": ", 2015), image captioning (Xu et al., 2015) among other ap-", "startOffset": 26, "endOffset": 43}, {"referenceID": 9, "context": "Encoder decoder models in a multi-source, single target setting have been explored by (Elliott et al., 2015) and (Zoph and Knight, 2016).", "startOffset": 86, "endOffset": 108}, {"referenceID": 9, "context": "Encoder decoder models in a multi-source, single target setting have been explored by (Elliott et al., 2015) and (Zoph and Knight, 2016). Specifically, Elliott et al. (2015) try to generate a German caption", "startOffset": 87, "endOffset": 174}, {"referenceID": 11, "context": "Recent work by Firat et al. (2016) explores", "startOffset": 15, "endOffset": 35}, {"referenceID": 11, "context": "However, Firat et al. (2016) focus on multi-task learning with a shared attention mechanism and the goal is to improve the MT performance for a pair of languages for which parallel data is available.", "startOffset": 9, "endOffset": 29}, {"referenceID": 13, "context": "For example (Khapra et al., 2010) use a bridge language or pivot language to do machine transliteration.", "startOffset": 12, "endOffset": 33}, {"referenceID": 29, "context": "Similarly, (Wu and Wang, 2007; Zhu et al., 2014) do pivot based machine translation.", "startOffset": 11, "endOffset": 48}, {"referenceID": 20, "context": "Lastly, we would also like to mention the work in interlingua based Machine Translation (Nirenburg, 1994; Dorr et al., 2010) which is clearly the inspiration for this work even though the focus of this work is not on MT.", "startOffset": 88, "endOffset": 124}, {"referenceID": 8, "context": "Lastly, we would also like to mention the work in interlingua based Machine Translation (Nirenburg, 1994; Dorr et al., 2010) which is clearly the inspiration for this work even though the focus of this work is not on MT.", "startOffset": 88, "endOffset": 124}, {"referenceID": 15, "context": "The idea of learning common representations for multiple views has been explored well in the past (Klementiev et al., 2012; Chandar et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2016; Rajendran et al., 2015).", "startOffset": 98, "endOffset": 218}, {"referenceID": 3, "context": "The idea of learning common representations for multiple views has been explored well in the past (Klementiev et al., 2012; Chandar et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2016; Rajendran et al., 2015).", "startOffset": 98, "endOffset": 218}, {"referenceID": 4, "context": "The idea of learning common representations for multiple views has been explored well in the past (Klementiev et al., 2012; Chandar et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2016; Rajendran et al., 2015).", "startOffset": 98, "endOffset": 218}, {"referenceID": 21, "context": "The idea of learning common representations for multiple views has been explored well in the past (Klementiev et al., 2012; Chandar et al., 2014; Hermann and Blunsom, 2014; Chandar et al., 2016; Rajendran et al., 2015).", "startOffset": 98, "endOffset": 218}, {"referenceID": 3, "context": "(Chandar et al., 2014; Chandar et al., 2016) propose correlational neural networks for common representation learning and Rajendran et al.", "startOffset": 0, "endOffset": 44}, {"referenceID": 4, "context": "(Chandar et al., 2014; Chandar et al., 2016) propose correlational neural networks for common representation learning and Rajendran et al.", "startOffset": 0, "endOffset": 44}, {"referenceID": 3, "context": "(Chandar et al., 2014; Chandar et al., 2016) propose correlational neural networks for common representation learning and Rajendran et al. (2015) propose bridge correlational", "startOffset": 1, "endOffset": 146}, {"referenceID": 21, "context": "From the point of view of representation learning, the work of Rajendran et al. (2015) is very similar to our work except that it focuses only on representation learning and does not consider the", "startOffset": 63, "endOffset": 87}, {"referenceID": 6, "context": "A two stage encoder-decoder is a straight-forward extension of sequence to sequence models (Cho et al., 2014; Sutskever et al., 2014) to the bridge setup.", "startOffset": 91, "endOffset": 133}, {"referenceID": 23, "context": "A two stage encoder-decoder is a straight-forward extension of sequence to sequence models (Cho et al., 2014; Sutskever et al., 2014) to the bridge setup.", "startOffset": 91, "endOffset": 133}, {"referenceID": 28, "context": "shared task (Zhang et al., 2012).", "startOffset": 12, "endOffset": 32}, {"referenceID": 16, "context": "SMT (Koehn et al., 2003) based transliteration systems using D1 and D2.", "startOffset": 4, "endOffset": 24}, {"referenceID": 13, "context": "This is very encouraging especially because such 2-stage approaches are considered to be very strong baselines for these tasks (Khapra et al., 2010).", "startOffset": 127, "endOffset": 148}, {"referenceID": 21, "context": "test set recently released by (Rajendran et al., 2015).", "startOffset": 30, "endOffset": 54}, {"referenceID": 21, "context": "Note that (Rajendran et al., 2015) report results for cross modal search and do not address the problem of crosslingual image captioning.", "startOffset": 10, "endOffset": 34}, {"referenceID": 21, "context": "In our model, for D1 we use the same train(118K), validation (1K) and test sets (1K) as defined in (Rajendran et al., 2015) and explained above.", "startOffset": 99, "endOffset": 123}, {"referenceID": 2, "context": "Initially we considered the corpus released as part of WMT\u201912 (Callison-Burch et al., 2012) which", "startOffset": 62, "endOffset": 91}, {"referenceID": 21, "context": "Since we did not have such a corpus at our disposal we decided to follow (Rajendran et al., 2015) and use a pseudo parallel corpus between English-French.", "startOffset": 73, "endOffset": 97}], "year": 2016, "abstractText": "Interlingua based Machine Translation (MT) aims to encode multiple languages into a common linguistic representation and then decode sentences in multiple target languages from this representation. In this work we explore this idea in the context of neural encoder decoder architectures, albeit on a smaller scale and without MT as the end goal. Specifically, we consider the case of three languages or modalities X , Z and Y wherein we are interested in generating sequences in Y starting from information available in X . However, there is no parallel training data available between X and Y but, training data is available between X & Z and Z & Y (as is often the case in many real world applications). Z thus acts as a pivot/bridge. An obvious solution, which is perhaps less elegant but works very well in practice is to train a two stage model which first converts from X to Z and then from Z to Y . Instead we explore an interlingua inspired solution which jointly learns to do the following (i) encodeX and Z to a common representation and (ii) decode Y from this common representation. We evaluate our model on two tasks: (i) bridge transliteration and (ii) bridge captioning. We report promising results in both these applications and believe that this is a right step towards truly interlingua inspired encoder decoder architectures.", "creator": "LaTeX with hyperref package"}}}