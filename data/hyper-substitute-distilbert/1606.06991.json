{"id": "1606.06991", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Toward Word Embedding for Personalized Information Retrieval", "abstract": "experimental case studies proposed perspectives on semantic letter embedding ( mate ) targeting query expansion in the context of personalized link encoding. usually, their variants cannot learned on a general corpus, like wikipedia. on this literature we commonly then achieve the word embeddings learning, via recording the learning on the platforms'social browser. your word embeddings network then afforded the same context than shared user interests. our proposal been evaluated on the clef social book model 2016 algorithm. original results obtained show that some comments nonetheless be made against the way to apply semantic types in the context of personalized information retrieval.", "histories": [["v1", "Wed, 22 Jun 2016 15:53:29 GMT  (406kb,D)", "http://arxiv.org/abs/1606.06991v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["nawal ould-amer", "philippe mulhem", "mathias gery"], "accepted": false, "id": "1606.06991"}, "pdf": {"name": "1606.06991.pdf", "metadata": {"source": "CRF", "title": "Toward Word Embedding for Personalized Information Retrieval", "authors": ["Nawal OULD AMER", "Philippe MULHEM"], "emails": ["amer@imag.fr", "philippe.mulhem@imag.fr", "mathias.gery@univ-st-"], "sections": [{"heading": null, "text": "CCS Concepts \u2022Information systems \u2192 Personalization;\nKeywords Word Embedding, word2vec, Personalization, Social Book Search, Query expansion"}, {"heading": "1. INTRODUCTION", "text": "Recent works investigate the use of Word Embedding for enhancing IR effectiveness [1, 3, 8] or classification [5]. Word Embedding [7] is the generic name of a set of NLP-related learning techniques that seek to embed representations of words, leading to a richer representation: words are represented as vectors of more elementary components or features. Similarly to Latent Semantic Analysis (LSA) [2], Word Embedding maps the words to low-dimensional (w.r.t. vocabulary size) vectors of real numbers. For example, two vectors \u2212\u2192 t0 and \u2212\u2192 t1 , corresponding to the words t0 and t1, are close in a N-dimensional space if they have similar contexts and vice-versa, i.e. if the contexts in turn have similar words [4]. In this vector space of embedded words, the cosine similarity measure is classically used to identify words occur-\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\nNeu-IR\u201916 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy c\u00a9 2016 Copyright held by the owner/author(s).\nACM ISBN . . . $00.00\nDOI:\nring in similar contexts. In addition, the arithmetic operations between vectors reflect a type of semantic-composition, e.g. bass + guitar = bass guitar [10].\nIn this paper, we present an approach using Word Embedding for Personalized Information Retrieval. The goal of our works is to provide clues about the following questions:\n\u2022 Can Word Embedding be used for query expansion in the context of social collection ?\n\u2022 Can Word Embedding be used to personalize query expansion?\nThe first question is motivated by our participation in the CLEF Social Book Search task in 2016 (similar to the Social Book Search task in 2015 [6]). Our concern was related to the fact that the topics provided contain non-topical terms that may impact the usage of Word Embedding.\nThe second question tackles more specifically the usage of Word Embedding when the learning is based on the user\u2019s profile in order to select personalized words for query expansion. The idea is to select words that occur in the same context as the terms of the query. We compare then Word Embedding learned on the whole collection of Social Book Search, called the Non Personalized Query Expansion, versus Word Embedding learned on the user\u2019s profiles, called the Personalized Query Expansion.\nThe paper is organized as follows: Section 2 presents the proposed approach. The experiments on the official CLEF Social Book Search collection are presented and discussed in Section 3, and the results are commented in Section 4. We conclude this work in Section 5."}, {"heading": "2. PERSONALIZED QUERY EXPANSION", "text": ""}, {"heading": "2.1 User Modeling", "text": "In the context of Social Book Search, each user is represented by his catalog (i.e. set of books) and other information such as tags and the ratings that he assigns to the books [6]. All these information describe the interests of the user.\nWe represent a user u as one document du which is the concatenation of all the documents present in his catalog.\nar X\niv :1\n60 6.\n06 99\n1v 1\n[ cs\n.I R\n] 2\n2 Ju\nn 20\n16\nThe profile of u, noted pu, is then represented by the set of words in du:\npu = {w1, w2, w3, ..., wn} (1)"}, {"heading": "2.2 Term Filtering", "text": "As stated before, Word Embedding can potentially be helpful in the selection of terms related to the query. Usually, embedded terms are used for query expansion, as extensions of one of the query term. Despite the effectiveness of Word Embedding to select embedded terms, it could happen that their use for query expansion decreases the effectiveness of the system.\nIn fact, if an extended term is a noisy term (because of its ambiguity for instance, or because it is not a topical term of the query), then the set of its resulting word embeddings will increase the non-topical noise in the extended query. For example, in the queries (taken from the topics of Social Book Search 2016)\u201cHelp! I Need more books\u201d, \u201cNew releases from authors that literally make you swoon...\u201d or \u201cFavorite Christmas Books to read to young children\u201d, the majority of theses words are not useful for expansion, like \u201cnew, good, recommend, make, you, etc.\u201d. Therefore, these words need to be filtered out of the queries before the expansion process. We chose to remove all the adjectives words from the queries. To do that, we use an English stop-adjective list in addition to the standard English stop-list.\nThen, from a user query q = {t1, t2, ..., tm}, we note qf = {t1, t2, ..., to} the filtered query."}, {"heading": "2.3 Word Embedding Selection", "text": "Once we have a filtered query qf as described above (cf. subsection 2.2), we select the top-k word embeddings to be used as extensions. This selection is achieved in three steps for each term t of the filtered query qf :\ni) Building a set of word embeddings for t using the cosine similarity between t and all words in the training corpus;\nii) Filtering out from the set of word embeddings of i) the terms that have the same stem than t using English Porter Stemmer, in order to avoid overemphasizing the variations of t;\niii) Selecting the top-k word embeddings of t from the filtered set of word embeddings of ii).\nThen, the output of the selection of word embeddings is:\nWordEmbeddingw2v(qf ) =  em t11, em t12, ..., em t1k, em t21, em t22, ..., em t2k, ... em to1, em tm2, ..., em tok  (2)\nWhere WordEmbeddingw2v(qf ) denotes the function that returns a set of word embedding for a given filtered query qf , and em tij denotes the j\nth element of the top-k word embeddings of ti."}, {"heading": "2.4 Ranking Model", "text": "The final expanded query qnew is the union of the original user query q and the word embeddings set as follow:\nqnew = q \u222aWordEmbeddingw2v(qf ) (3)\nThe score for each document d according to the expanded query qnew and the user u is computed according to a classical Language Model with Dirichlet smoothing."}, {"heading": "3. EXPERIMENTS", "text": ""}, {"heading": "3.1 Dataset", "text": "Our experiments are conducted on Social Book Search dataset [6].\n\u2022 Documents: The documents collection consists of 2.8 millions of books descriptions with meta-data from Amazon and LibraryThing. Each document is represented by book-title, author, publisher, publication year, library classification codes and user-generated content in the form of user ratings and reviews.1\n\u2022 Users: The collection provides profiles of 120 users. Each user is described by his catalog (i.e. a set of books), tags, and rating.\n\u2022 Queries: The collection contains 120 user queries. Due to the nature of the queries, we chose 28 queries for our experiments: these queries have topical content (i.e. a classical IR system has then chances to get relevant results) and the profile of the query issuer is not empty."}, {"heading": "3.2 Learning of Word Embedding", "text": "Here, we describe the process of learning word vectors (see section 1). We use two training sets.\n\u2022 The first one is called The Non Personalized Corpus: the train is built on the whole Social Book Search Corpus.\n\u2022 The second one is called The Personalized Corpus: we build a personalized train corpus for each user.\nThe training process is the same for the two corpora:\n1. Non Personalized Corpus train process: We train word2vec [7] on the Social Book Search corpus. word2vec represents each word w of the training set as a vector of features, where this vector is supposed to capture the contexts in which w appears. Therefore, we chose the Social Book Search corpus to be the training set, as the training set is expected to be consistent with the data on which we want to test. To construct the training set from the corpus, we simply concatenate the content of all the documents, without any particular pre-processing such as stemming or stop-words removal, except some text normalization and cleaning operations such as lower-case normalization, removing HTML tags, etc. The concatenated document is the input training set of word2vec. The size of the training set is \u223c 12.5GB; it contains 2.3 billions words, and the vocabulary size is about 600K.\nThe training parameters of word2vec are set as follows:\n\u2022 continuous bag of words model instead of the skipgram (word2vec options: cbow=1);\n1http://social-book-search.humanities.uva.nl/#/suggestion\n\u2022 the output vectors size or the number of features of resulting word-vectors is set to 500 (word2vec options: size=500);\n\u2022 the width of the word-context window is set to 8 (word2vec options: window=8);\n\u2022 the number of negative samples is set to 25 (word2vec options: negative=25).\n2. Personalized Corpus train process: For this corpus, each user is described as a document du (containing the documents belonging to his catalog, cf. Section 2.1) and we train word2vec on each user document du using the same process than the Non Personalized Corpus train process."}, {"heading": "3.3 Parameters and Tested Configurations", "text": "The ranking model is achieved using the Language Model with Dirichlet smoothing. All documents are retrieved using Terrier search engine [9] with \u00b5 = 50.\nWe compare the following variations:\n1. Query filtering: the query is filtered or not, by removing the stop-word and the adjectives as stated in section 2.2;\n2. Query expansion: the query is expanded or not using the Word Embedding function;\n3. Personalization: the query is personalized (using the Personalized Corpus) or not (using the Non Personalized Corpus).\nThe tested configurations are presented in Table 3.3."}, {"heading": "4. RESULTS", "text": "In this section, we present and comment the results of the above configurations. All of these configurations lead to quite low MAP values, but this is consistent with the official CLEF Social Book Search results."}, {"heading": "4.1 Query Filtering", "text": "Table 4.1 reports the Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and precision at 10 documents (P@10) evaluation measures obtained with the configuration Conf1 (without query filtering and without query expansion) and the configuration Conf2 (with query filtering and without query expansion). As we can see, filtering the query terms with the stop-adjective list improves the MAP values, but surprisingly nor the MRR neither the P@10. The choice of filtering the adjectives may be not the most effective way to deal with noisy terms."}, {"heading": "4.2 Personalized Query Expansion (with filtering)", "text": "Figure 1 reports the MAP effectiveness over the number of word embeddings. As we can see, in most of the cases the non personalized approach outperforms the personalized approach. The personalized approach shows a better result only when the word embeddings are limited to the top-2 terms. We also remark that both the two approaches underperfom the filtered non expanded approach Conf2, which shows that inadequate words are added. For Conf3, we see that adding more that 8 terms lead to better results, which is not really the case to the personalized configuration Conf4 where the MAP evolution is quite flat. In this case, no added term seems to play a positive role."}, {"heading": "4.3 Personalized Query Expansion (without filtering)", "text": "The figure 2 reports the MAP value evolution over the number of word embeddings for the configurations Conf5 (non personalized query expansion, without filtering) and the Conf6 (personalized query expansion, without filtering). As we can see, in most of the cases, the non personalized approach outperforms the personalized one. However, these two curves in figure 2 behave quite differently: the personalized approach is better only when adding the top-1 and top-2 words, leading to consider that the first two personalized terms are interesting, whereas the non personalized expansion behaves better and better as the number of terms increases, leading to consider that \u201cgood\u201d terms (according to the user) appear in the non personalized case."}, {"heading": "4.4 Discussion", "text": "The figure 3 reports the MAP value over the number of word embeddings for the six configurations. As we can see, the results of the configurations: 1) Conf3 and Conf5, and 2) Conf4 and Conf6 are quite similar.\nWe observe that the configurations with query expansion\n(Conf1 and Conf2) have still the best results, and outperform all the configurations with query expansion (i.e. Conf3, Conf4, Conf5 and Conf6).\nAs presented above, in most of the cases, the expanded approach fails to improve results. We may explain these results by the quality of the corpus. In fact, the documents collection describes the reviews of users for books. Therefore, it\u2019s still difficult to extract the context of terms and select the similar terms in the similar context.\nThe personalized Word Embedding fails to improve the results comparing to any configurations. We first can explain the results by the same quality problem than for the documents collection. In fact, the user is represented by the documents that appear in his catalog. So, these documents present the reviews of the user about the books. Secondly, the learning of word2vec is effective if a large amount of data is available. However, the users\u2019 profiles correspond to short documents. Therefore, the amount of learning data may not reach the limit under which no convergence is possible for word2vec."}, {"heading": "5. CONCLUSION", "text": "The focus of this paper was to study the integration of word embeddings for the Social Book Suggestion task of CLEF 2016, according to non-personalized and to personalized query expansion.\nWe found that the nature of the queries poses a great challenge to an effective use of Word Embedding in this context. Future works may help to better understand this behavior.\nThe second point is related to the fact that the personalization using word embeddings did not lead to good results. The reasons could be multiple: the quality of the description of the user\u2019s profiles, the lack of data that we get to describe the profile which does not allow word embeddings to be used effectively. Here also, future works may help to find solutions to these problems. Especially, is it possible to counteract this limitation by adding other inputs (user neighbors for instance)? This question is largely open for now."}, {"heading": "6. REFERENCES", "text": "[1] M. Almasri, C. Berrut, and J. Chevallet. A\ncomparison of deep learning based query expansion with pseudo-relevance feedback and mutual information. In European Conference on IR Research, ECIR 2016, Padua, Italy, pages 709\u2013715, 2016.\n[2] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391\u2013407, 1990.\n[3] D. Ganguly, D. Roy, M. Mitra, and G. J. Jones. Word embedding based generalized language model for information retrieval. In Conference on Research and Development in Information Retrieval, SIGIR\u201915, pages 795\u2013798, New York, USA, 2015.\n[4] Y. Goldberg and O. Levy. word2vec explained: deriving mikolov et al.\u2019s negative-sampling word-embedding method. CoRR, abs/1402.3722, 2014.\n[5] Y. Kim. Convolutional neural networks for sentence classification. CoRR, abs/1408.5882, 2014.\n[6] M. Koolen, T. Bogers, M. Ga\u0308de, M. A. Hall, H. C. Huurdeman, J. Kamps, M. Skov, E. Toms, and D. Walsh. Overview of the CLEF 2015 social book search lab. In Conference and Labs of the Evaluation Forum, CLEF\u201915, pages 545\u2013564, Toulouse, France, 2015.\n[7] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013.\n[8] E. Nalisnick, B. Mitra, N. Craswell, and R. Caruana. Improving document ranking with dual word embeddings. In Conference Companion on World Wide Web, WWW\u201916 Companion, pages 83\u201384, Monteal, Quebec, Canada, 2016.\n[9] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, and C. Lioma. Terrier: A High Performance and Scalable Information Retrieval Platform. In SIGIR\u201906 Workshop on Open Source Information Retrieval, OSIR\u201906), 2006.\n[10] D. Widdows. Geometry and Meaning. Center for the Study of Language and Information/SRI, 2004."}], "references": [{"title": "A comparison of deep learning based query expansion with pseudo-relevance feedback and mutual information", "author": ["M. Almasri", "C. Berrut", "J. Chevallet"], "venue": "European Conference on IR Research, ECIR 2016, Padua, Italy, pages 709\u2013715", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science, 41(6):391\u2013407", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1990}, {"title": "Word embedding based generalized language model for information retrieval", "author": ["D. Ganguly", "D. Roy", "M. Mitra", "G.J. Jones"], "venue": "Conference on Research and Development in Information Retrieval, SIGIR\u201915, pages 795\u2013798, New York, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "word2vec explained: deriving mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Y. Goldberg", "O. Levy"], "venue": "CoRR, abs/1402.3722,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "CoRR, abs/1408.5882", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Overview of the CLEF 2015 social book search lab", "author": ["M. Koolen", "T. Bogers", "M. G\u00e4de", "M.A. Hall", "H.C. Huurdeman", "J. Kamps", "M. Skov", "E. Toms", "D. Walsh"], "venue": "Conference and Labs of the Evaluation Forum, CLEF\u201915, pages 545\u2013564, Toulouse, France", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, abs/1301.3781", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving document ranking with dual word embeddings", "author": ["E. Nalisnick", "B. Mitra", "N. Craswell", "R. Caruana"], "venue": "Conference Companion on World Wide Web, WWW\u201916 Companion, pages 83\u201384, Monteal, Quebec, Canada", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Terrier: A High Performance and Scalable Information Retrieval Platform", "author": ["I. Ounis", "G. Amati", "V. Plachouras", "B. He", "C. Macdonald", "C. Lioma"], "venue": "SIGIR\u201906 Workshop on Open Source Information Retrieval, OSIR\u201906)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Geometry and Meaning", "author": ["D. Widdows"], "venue": "Center for the Study of Language and Information/SRI", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Recent works investigate the use of Word Embedding for enhancing IR effectiveness [1, 3, 8] or classification [5].", "startOffset": 82, "endOffset": 91}, {"referenceID": 2, "context": "Recent works investigate the use of Word Embedding for enhancing IR effectiveness [1, 3, 8] or classification [5].", "startOffset": 82, "endOffset": 91}, {"referenceID": 7, "context": "Recent works investigate the use of Word Embedding for enhancing IR effectiveness [1, 3, 8] or classification [5].", "startOffset": 82, "endOffset": 91}, {"referenceID": 4, "context": "Recent works investigate the use of Word Embedding for enhancing IR effectiveness [1, 3, 8] or classification [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "Word Embedding [7] is the generic name of a set of NLP-related learning techniques that seek to embed representations of words, leading to a richer representation: words are represented as vectors of more elementary components or features.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "Similarly to Latent Semantic Analysis (LSA) [2], Word Embedding maps the words to low-dimensional (w.", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "if the contexts in turn have similar words [4].", "startOffset": 43, "endOffset": 46}, {"referenceID": 9, "context": "bass + guitar = bass guitar [10].", "startOffset": 28, "endOffset": 32}, {"referenceID": 5, "context": "The first question is motivated by our participation in the CLEF Social Book Search task in 2016 (similar to the Social Book Search task in 2015 [6]).", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "set of books) and other information such as tags and the ratings that he assigns to the books [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "Our experiments are conducted on Social Book Search dataset [6].", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "Non Personalized Corpus train process: We train word2vec [7] on the Social Book Search corpus.", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "All documents are retrieved using Terrier search engine [9] with \u03bc = 50.", "startOffset": 56, "endOffset": 59}], "year": 2016, "abstractText": "This paper presents preliminary works on using Word Embedding (word2vec) for query expansion in the context of Personalized Information Retrieval. Traditionally, word embeddings are learned on a general corpus, like Wikipedia. In this work we try to personalize the word embeddings learning, by achieving the learning on the user\u2019s profile. The word embeddings are then in the same context than the user interests. Our proposal is evaluated on the CLEF Social Book Search 2016 collection. The results obtained show that some efforts should be made in the way to apply Word Embedding in the context of Personalized Information Retrieval. CCS Concepts \u2022Information systems \u2192 Personalization;", "creator": "LaTeX with hyperref package"}}}