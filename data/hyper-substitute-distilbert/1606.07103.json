{"id": "1606.07103", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2016", "title": "Deep Feature Fusion Network for Answer Quality Prediction in Community Question Answering", "abstract": "frequent request answering ( jar ) forums largely become a popular medium for ensuring consistent answers to specific questions of users bordering other or independently experienced users on a suitable resource. but, for a given relevance, users sometimes have access sift through a large number of low - end email irrelevant searches to find satisfying the answer but satisfies their privacy constraints. attempts address this, web approach of answer sampling determination ( es ) aims to index the quantity of an answer sample in response to a forum question. current sql systems either largely models as - lc ) conventional adaptive - crafted matching ( hcf ) approaches b ) use deep learning ( dl ) networks which either mask the required feature representations.", "histories": [["v1", "Wed, 22 Jun 2016 20:58:08 GMT  (425kb,D)", "https://arxiv.org/abs/1606.07103v1", null], ["v2", "Sun, 26 Jun 2016 05:54:51 GMT  (305kb,D)", "http://arxiv.org/abs/1606.07103v2", "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy"]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["sai praneeth suggu", "kushwanth n goutham", "manoj k chinnakotla", "manish shrivastava"], "accepted": false, "id": "1606.07103"}, "pdf": {"name": "1606.07103.pdf", "metadata": {"source": "CRF", "title": "Deep Feature Fusion Network for Answer Quality Prediction in Community Question Answering", "authors": ["Sai Praneeth Suggu", "Kushwanth N. Goutham", "Manoj K. Chinnakotla", "Manish Shrivastava"], "emails": ["suggusai.praneeth@research.iiit.ac.in", "kushwanth.naga@research.iiit.ac.in", "m.shrivastava@iiit.ac.in", "manojc@microsoft.com"], "sections": [{"heading": null, "text": "In this paper, we propose a novel approach for AQP known as -\u201cDeep Feature Fusion Network (DFFN)\u201dwhich leverages the advantages of both hand-crafted features and deep learning based systems. Given a question-answer pair along with its metadata, DFFN independently - a) learns deep features using a Convolutional Neural Network (CNN) and b) computes hand-crafted features using various external resources and then combines them using a deep neural network trained to predict the final answer quality. DFFN achieves stateof-the-art performance on the standard SemEval-2015 and SemEval-2016 benchmark datasets and outperforms baseline approaches which individually employ either HCF or DL based techniques alone.\nKeywords Community Question Answering, Answer Quality Prediction, Answer Selection, Deep Learning, Convolutional Neural Networks, Feature Engineering\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\nNeu-IR \u201916 SIGIR Workshop on Neural Information Retrieval, July"}, {"heading": "21, 2016, Pisa, Italy.", "text": "c\u00a92016 Copyright held by the owner/author(s)."}, {"heading": "1. INTRODUCTION", "text": "Community Question Answering (cQA) forums (such as Yahoo! Answers, Stack Overflow, etc.) have become a popular medium for many internet users to get precise answers or opinions to their questions from experts or other experienced users in the topic. Such forums are usually open, allowing any user to respond to a given question. As a result, for a given question posed by the user, the quality of response often varies a lot ranging from highly precise and detailed answers from authentic users to highly imprecise or non-comprehensible one-word, single line answers or answers which are completely unrelated to the topic posted by spammy and other non-serious users. This severely hampers the effectiveness of the cQA forums as users will have to sift through a large number of irrelevant posts to find the answers satisfying their information needs. To alleviate this problem, cQA forums often include feedback mechanisms such as votes, ratings etc. for rating the quality of answers and users which could also be used as signals for ranking the answers given a question. However, such popularity based signals (votes, ratings) are often prone to spam due to users who may artificially inflate their ratings, votes with the help of other users whom they know. To overcome the above problems, recent approaches [6, 7, 16, 18, 22, 23, 26\u201328] have focused on automatically ranking answers for a given question based on their quality.\nThe problem of answer quality prediction is defined as follows: Given a question Q and its set of community answers C = {A1, A2, . . . , An}, rate the answers corresponding to their quality. The cQA tasks of SemEval-2015 (Task A) [14] and SemEval-2016 (Task A) [15] provide a universal benchmark for evaluating research on this problem. In SemEval2015, the answers are to be rated as {good,potentially useful or bad} and in SemEval-2016, the answers are to be rated as either {good or bad}.\nRecent approaches for answer quality prediction can be categorized into - a) Hand-crafted Feature (HCF) based approaches [6, 16, 22, 23, 26] or b) Deep Learning (DL) based approaches [7, 18, 27, 28]. HCF based approaches mainly rely on capturing various semantic and syntactic similarities between the question and answer, behavior of users using feature engineering. For computing these similarities, recent approaches have leveraged external knowledge\nar X\niv :1\n60 6.\n07 10\n3v 2\n[ cs\n.I R\n] 2\n6 Ju\nn 20\nresources such as WordNet and other text corpora. DL based approaches, on the other hand, automatically learn the feature representations while learning the target quality scoring function. As a result, they are language-agnostic and don\u2019t require feature engineering or any external resources except for a large training corpus.\nIn this paper, we propose \u201cDeep Feature Fusion Network (DFFN)\u201d - a novel approach which combines HCF into a Convolutional Neural Network (CNN) model for improving answer quality prediction. DFFN leverages the advantage of both HCF and DL based approaches i.e. ability to - a) encode similarities between question-answer pair using external knowledge resources such as Wikipedia, Google CrossLingual Dictionary (GCD), Clickthrough data and b) automatically learn features and the target function. Given a question, answer pair along with its metadata, DFFN independently learns deep features from CNN, computes handcrafted features using various external resources and then combines the deep features and hand-crafted features using a deep neural network trained to predict the quality rating of the answer. DFFN achieves state-of-the-art performance on the standard SemEval-2015 and SemEval-2016 benchmark datasets and shows better performance than baseline approaches which individually employ either HCF or DL based techniques. In this context, the following are our main contributions:\n\u2022 We propose a novel approach to combine hand-crafted features into a CNN for the answer quality prediction task\n\u2022 We achieve state-of-the-art performance on SemEval 2015 and SemEval 2016 cQA answer quality prediction tasks\nThe rest of the paper is organized as follows: Section 2 discusses the related work in this area. Section 3 presents our contribution DFFN in detail. Section 4 discusses our experimental set-up. Section 5 presents our results and finally Section 6 concludes the paper."}, {"heading": "2. RELATED WORK", "text": "AQP in cQA forums has been researched a lot in the IR community. Jeon et al. [9] employ non-textual features such as clicks, print counts, copy counts etc. to predict the quality of a answer in a cQA forum. Liu et al. [12] investigate a slightly related problem i.e. predicting whether an asker would be satisfied with the answers provided so far to the given question. Burel et al. [1] have used a combination of content, user and thread related features for predicting answer quality. Dalip et al. [2] propose a learning to rank approach for AQP using eight different groups of features. Li et al. [11] studied the various factors such as shorter length, authors reputation which lead to a high answer quality rating as rated by peers.\nMore recently, Tran et al. [22] made use of topic models, word vectors and other hand crafted rules to train a SVM classifier for AQP. Hou et al. [6] made use of statistics like avg. word length of a sentence (question or answer), sentence length with other hand-crafted features to train an ensemble of classifiers for AQP. Wang et.al [23] use Bayesian logistic regression and link prediction models for AQP. Yu et al. [27] used CNN to learn a distributional sentence model for AQP from bag of words and bigram based word representations. Nicosia et.al [16] have used lexical similarity between word n-grams, tree kernels, word-embeddings and other hand crafted features for AQP. Hsu et al. [7] used a LSTM encoder with neural attention mechanism to auto-\nmate feature engineering process. Severyn et al. [18] used a CNN to automatically learn features for matching short text pairs. Zhou et al. [28] used a 2-dimensional CNN to represent a question-answer pair and ranked the representations using a Recurrent Neural Network.\nOur work resembles the work by Wu et al. [24] who employ the idea of combining hand-crafted features and deep features for person re-identification task in computer vision. However, in our case, the idea of using hand-crafted features is driven by the availability of large similarity resources such as Wikipedia, Google Cross-Lingual Dictionary and Clickthrough data which could be leveraged to infer richer syntactic and semantic similarities between textual elements."}, {"heading": "3. DEEP FEATURE FUSION NETWORK (DFFN)", "text": "Figure 1 shows the architecture of our DFFN model. The input to DFFN is the question, answer and metadata (question category, question author, answer author etc.) and the output is a relevance score depicting the quality of the answer. DFFN is a two-staged deep Neural Network (NN) model. In the first stage, DFFN has two parallel CNN based sentence models for the question and answer which are used to learn their feature representations (lets call it CNNFR). In parallel, DFFN also generates hand-crafted feature representations from the question, answer and metadata information using the Wikipedia and other similarity models (lets call it HCFR). In the second stage, CNNFR and HCFR along with Metadata (question author,answer author and question category) are joined and passed through one more deep Neural Network (NN) which predicts the score representing answer quality. We will now discuss each stage of DFNN in detail."}, {"heading": "3.1 Sentence Model", "text": "The sentence model projects a sentence (question/answer) into the semantic space and learns a good intermediate representation of the given question/answer. The sentence model is a deep convolutional neural network (CNN). Our CNN mainly consists of sentence matrix and multiple convolutional, pooling and non-linearity layers as in Figure 1.\nThe input to the sentence matrix S is a vector of words from the sentence (question/answer) s = {w1, w2, ....w|s|}. We build the sentence matrix by mapping each word wi in the question/answer to its corresponding word embedding in d dimensions. We use GLoVE [17] based embeddings of 300 dimensions to map the words in the question and answer. We limit the size of the sentence upto certain threshold. We ignore the words in the sentence after a certain threshold if the length of the sentence is greater than the threshold and pad zeros upto the threshold if the length of the sentence is less than the threshold. The sentence matrix is then convolved through multiple convolution, pooling and non-linearity layers to get the feature representations of the question/answer. We perform convolution in 2 dimensions i.e. horizontally and vertically. We use max-pooling for the pooling layer and Randomized Leaky Rectified Linear Unit (RReLU) [25], a randomized version of leakyReLU [25], as the non-linearity layer. RReLU for a value x is computed as follows in training phase:\nf(x) = { x, if x \u2265 0 ax, ifx < 0\nwhere\na \u223c U(l, u), l < u and l, u \u2208 [0, 1)\ni.e. a is a random number drawn from a uniform random distribution U(l, u). In testing phase it is computed as:\nf(x) = x\nl+u 2\nUsing sentence models, we get the individual feature representations (270 dimensions) of the question and answer which are then concatenated to produce a combined feature representation (540 dimensions). These are concatenated with hand crafted features and metadata and are given as input to the second stage Neural Network. We describe in detail regarding them in the following subsections."}, {"heading": "3.2 Hand Crafted Features (HCF)", "text": "The question and answer text usually consist of several Named Entities (NEs) and concepts along with their various variants. For example, the cricketer Sachin Tendulkar could be referred to as Sachin, Tendulkar, The Little Master etc. Such variants are hard to capture using CNN based features alone. Hence, we make use of resources such as Wikipedia, Google Cross-Lingual Dictionary (GCD), Named Entity Recognizers (NER) and Clickthorugh data to come up with hand-crafted features which can capture such rich similarities. We also observe that user behaviour and specific patterns on metadata and question-answer text are useful. We use these features to compute individual similarity scores between question and answer and combine these scores as Hand Crafted Features to give them as input to the second stage NN. We describe the details of the features below:\n3.2.1 Wikipedia Based Features In this section, we describe the similarity features which\nare computed based on using Wikipedia as a resource. TagMe Similarity: We extract TagMe concepts of the question and answer by mapping them to their corresponding Wikipedia page titles using TagMe [4]. TagMe identifies meaningful substrings in an unstructured text and links them to their relevant wikipedia pages. We compute the similarity between two TagMe concepts using wikipedia Miner [13]. Wikipedia Miner computes similarity between two wikipedia pages based on the number of common inlinks and outlinks between them.\nSimilarity between question and answer represented by TagMe concepts using Wikipedia Miner is computed as the mean average of the similarity between pairs of TagMe concepts (one each from the question and the answer) as in Equation 1 \u2211n\ni=1 \u2211m j=1 sim(ci, cj)\nnm (1)\nwhere n,m are the number of TagMe concepts in the question and answer respectively, ci, cj are the i\nth and jth TagMe concepts in the question and answer respectively, sim(ci, cj) is the similarity between ci and cj calculated using Wikipedia Miner.\nGCD Similarity: Google Cross-Lingual Dictionary (GCD) [20] is a string to concept mapping on the vast link structure of the web, created using anchor text from various pages across the web. A concept is an individual Wikipedia document. The text strings constitute the anchor texts that refer to these concepts. Thus, each anchor text string represents a concept.\nWe extract common and proper nouns from the question and answer using Stanford CoreNLP POS Tagger [21] and query them individually on GCD anchor texts to get top ten unique concepts related to question and answer. We calculate the similarity between two GCD concepts using Wiki Miner. The similarity between question and answer represented by GCD concepts is calculated as in Equation 1 where we use GCD concepts instead of TagMe concepts.\nNamed Entities Similarity: We extract Named Entities from the question and answer, using Stanford CoreNLP POS Tagger. We compute the similarity between two Named Entities using a GCD based similarity feature.\nThe GCD based similarity between two Named Entities is computed as the ratio of number of wikipedia documents in which these two terms co-occur in the top k retrieved documents when queried on GCD. For our experiments, we set k to 100. We calculated the co-occurrence of two terms in a document by checking if both the terms match with any of the words in the document with a string edit distance greater than a threshold.\nSimilarity between question and answer represented by Named Entities is calculated as in Equation 1 where we use Named Entities instead of TagMe concepts and GCD based similarity feature instead of Wikipedia Miner to calculate the similarity between two Named Entities.\n3.2.2 Sentence-Vector Features\nParagraph2vec Similarity: Paragraph2Vec [10] allows to model vectors for text of any arbitrary length. It learns continuous distributed vector representations for pieces of texts. We train the para2vec model on the training data of the particular tasks only (SemEval\u201915 and SemEval\u201916) by treating each question-answer pair as a single document. We train only on the good question-answer pairs from the training data. A good question-answer pair is a pair in which answer is rated as a \u201cgood\u201d answer for that question.\nWe map the question and answer to vectors using para2vec and compute the similarity between the question and answer as the cosine similarity between their para2vec vectors.\nSent2Vec Similarity: Sent2Vec maps a pair of short texts to a pair of feature vectors in a continuous, low-dimensional space. Sent2Vec performs the mapping using the Deep Structured Semantic Model (DSSM) built using Clickthrough data [8], or the DSSM with convolutional-pooling structure (CDSSM) [5,19].\nWe map the question and answer to vectors using both DSSM and CDSSM. We compute the Sent2Vec DSSM similarity between the question and answer as the cosine similarity between the vectors of question and answer obtained by using Sent2Vec performing the mapping of vectors using DSSM. Similarly by using CDSSM instead of DSSM we also compute the Sent2Vec CDSSM similarity between the question and answer.\n3.2.3 Metadata Based Features\nAuthor Reputation Score: We observed that the reputation of an answer author, within a forum plays a key role in determining the quality of answer. We capture this through a author reputation feature. We have two reputation features namely Good Reputation and Bad Reputation. Good reputation of an author is computed as the ratio of the number of good answers given by that author to the maximum number of good answers given by any individual author in the entire forum. Similarly, by using the number of bad answers instead of good answers, we also compute a score for the bad reputation of an author.\nIs Answer Seeker?: We have a boolean feature to represent whether the answer (comment) is written by the person who has asked the question.\nQuestion Authors\u2019 Response Pattern: We compute features based on whether the question author has commented before or after the present answer and if that comment by the question author is a question. Usually, the question author posts comments/questions below an answer if one is not satisfied with the current answer. These features capture the behavior.\nMiscellaneous: Besides, we extract and add features related to - a) statistics of each question category (number of good, potential and bad answers in that category ) b) position of the answer. c) presence of URL, e-mail in the answer d) presence of question marks, exclamation marks in the answer e) boolean features for the presence of various emoticons such as happy ( eg: \u201c:)\u201d, \u201c:D\u201d ), sad ( eg: \u201c:(\u201d , \u201c:\u2019(\u201d ) in the answer.\nWe obtain the similarity scores and together call them as Hand-crafted features (28 dimensions). We join them as a vector and give them as input to second stage NN along with Metadata as described in the following subsections."}, {"heading": "3.3 Metadata", "text": "We observe that category of the question plays an important role in computing the answer quality score as it is easy to write good answers for some categories and difficult for some other. We also include author information of the question and answer. We encode the question category, question author and answer author using a logarithmic function and give them as input to second stage NN."}, {"heading": "3.4 Second Stage Neural Network", "text": "As discussed, the vector representations from the sentence models (540 dimensions), the feature representations from HCF (28 dimensions) and direct inputs from Metadata (33 dimensions) are combined to get a single feature vector of 601 dimensions. This vector is given as input to the second stage NN consisting of fully connected layers. These layers model various interactions between the features present in the vector and finally output a score predicting the answer quality."}, {"heading": "3.5 Training", "text": "The parameters of the network are learnt with an objective to maximize the accuracy of prediction given the target categories. For example, in SemEval-2015, the target categories were {good, potentially useful, bad} and {good, bad} in SemEval-2016 . For training, we used the training data provided in the SemEval 2015 [14] and 2016 [15] tasks which consists of question, answer, metadata along with their ideal quality rating. We tuned the DFFN parameters on the cor-\nresponding development sets of SemEval 2015 and 2016. We used Adagrad [3] to speed up the convergence rate of stochastic gradient descent (SGD).\nGiven an input (p, t) where p is the predicted answer quality score by DFFN and t is the true label depicting answer quality, we used SmoothL1 as the loss criterion which is computed as:\nloss(p, t) = 1\nn \u00d7 { 0.5\u00d7 (p\u2212 t)2, if |p\u2212 t| < 1 |p\u2212 t| \u2212 0.5, if |p\u2212 t| \u2265 1\nt is 1 for good question-answer pair (answer labeled as good for that question) and -1 for bad question answer pair (answer labeled as bad for that question). The model is trained by minimizing the loss function in a batch of size n."}, {"heading": "4. EXPERIMENTAL SETUP", "text": "We use the SemEval 2016 [15] and SemEval 2015 [14] datasets for our experiments as it exactly matches our problem description. SemEval 2016 consists of 36198 training question-answer (QA) pairs and 2440 for dev and 3270 for testing purposes. SemEval 2015 consists of 16541 training QA pairs and 1645 dev and 1976 for testing.\nTo evaluate the performance, we use standard evaluation metrics - Mean Average Precision (MAP), F1 score and Accuracy. We compare our approach with the top two best performing systems from SemEval 2015 - JAIST [22] and HITSZ-ICRC [6]. JAIST and HITSZ-ICRC use handcrafted feature based models. We also compare with ICRCHIT [28] as it uses a purely deep learning based model. Similarly, for SemEval 2016, we compare with their corresponding top two best performing systems - Kelp [15] and ConvKN [15]. We do not know their system descriptions and exact algorithms since the conference proceedings are not yet out."}, {"heading": "5. RESULTS AND DISCUSSION", "text": "Table 1 shows the overall results of DFFN on SemEval 2015 and SemEval 2016 datasets. DFFN performs better than the top systems across all the metrics. The improvement is higher in SemEval 2015 although the task is more harder due to lesser training data and more granularity in target labels to be predicted. We also observe that DFFN performs better than a single CNN alone (DFFN w/o HCF) or single hand-crafted feature based model alone (DFFN w/o CNN). Hence, the fusion of deep features and hand-crafted features helps in boosting the performance.\n5.1 Qualitative Analysis\nIn Table 2, we present a qualitative analysis of DFFN results comparing it other baselines. The first three examples are cases where we correctly predicted the target label. In the last two examples, we incorrectly predict the label.\nIn the first example, all the baseline systems predict incorrectly while DFFN is able to predict correctly as Good as the GCD similarity feature captures that post office, DHL, courier are linked to similar pages when they occur as anchor texts. In the second example, JAIST and HITSZ-ICRC output incorrectly while DFFN does it correctly as Sent2Vec has good similarity score, as embassy, passport co-occur in search queries. CNN based features also contributed to the score. In the third example, JAIST and ICRC-HIT get it incorrectly whereas DFFN is able to predict correctly as TagMe links azureus, limewire, utorrent, to their wiki pages due to which it can find out that they all belong to movie torrent software class.\nIn the fourth example, all the systems predict it wrongly as Good instead of Bad. In the case of DFFN, this is due to Sent2Vec scoring a high similarity score as regn. vehicles, license plate, car co-occur in search queries. In the last example HITSZ-ICRC and ICRC-HIT predict it correctly while DFFN predicts it wrongly as Potential instead of Bad. For this example Author Reputation feature gave a neutral similarity score as the answer author had written very less answers in the forum and had almost equal number of Good and Bad answers (Good:4, Potential:1, Bad:5). Question Authors\u2019 Response Pattern feature has also given a neutral similarity score as question author commented before and after this answer. Wikipedia based and Sentence Vector features have given high similarity scores since question and answer are exactly the same except for one word as this was answered in a sarcastic way. Thus DFFN predicted Potential in this example."}, {"heading": "6. CONCLUSION", "text": "We present a novel approach \u201cDeep Feature Fusion Networks (DFFN)\u201d which combines HCF features into a CNN model for improving answer quality prediction. DFFN enriches the feature representations learnt through a CNN by introducing more similarity features computed using external resources such as Wikipedia, Google Cross-Lingual Dictionary (GCD), Clickthrough Data. As a result, we show that DFFN achieves state-of-the-art performance on the standard SemEval-2015 and SemEval-2016 benchmark datasets and shows better performance than baseline approaches which individually employ either HCF or DL based techniques. In future work, we would like to investigate the difference between features learnt using DFFN and a stand alone CNN."}, {"heading": "7. REFERENCES", "text": "[1] Gre\u0301goire Burel, Yulan He, and Harith Alani.\nAutomatic Identification of Best Answers in Online Enquiry Communities. In The Semantic Web: Research and Applications: 9th Extended Semantic Web Conference, ESWC 2012.\n[2] Daniel Hasan Dalip, Marcos Andre\u0301 Gonc\u0327alves, Marco Cristo, and Pavel Calado. Exploiting User Feedback to Learn to Rank Answers in QA Forums: A Case Study with Stack Overflow. In SIGIR \u201913. ACM.\n[3] John Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. J. Mach. Learn. Res., 2011.\n[4] Paolo Ferragina and Ugo Scaiella. TAGME: On-the-fly Annotation of Short Text Fragments (by Wikipedia Entities). CIKM \u201910, pages 1625\u20131628. ACM, 2010.\n[5] Jianfeng Gao, Patrick Pantel, Michael Gamon, Xiaodong He, Li Deng, and Yelong Shen. Modeling Interestingness with Deep Neural Networks. EMNLP, 2014.\n[6] Yongshuai Hou, Cong Tan, Xiaolong Wang, Yaoyun Zhang, Jun Xu, and Qingcai Chen. HITSZ-ICRC:\nExploiting Classification Approach for Answer Selection in Community Question Answering. In SemEval 2015, pages 196\u2013202. ACL.\n[7] Wei-Ning Hsu, Yu Zhang, and James R. Glass. Recurrent Neural Network Encoder with Attention for Community Question Answering. CoRR, 2016.\n[8] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning Deep Structured Semantic Models for Web Search using Clickthrough Data, 2013.\n[9] Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon Park. A Framework to Predict the Quality of Answers with Non-textual Features. In SIGIR \u201906. ACM.\n[10] Quoc V. Le and Tomas Mikolov. Distributed Representations of Sentences and Documents. CoRR, abs/1405.4053, 2014.\n[11] Lei Li, Daqing He, Wei Jeng, Spencer Goodwin, and Chengzhi Zhang. Answer Quality Characteristics and Prediction on an Academic QA Site: A Case Study on ResearchGate. In Proceedings of the 24th International Conference on World Wide Web, WWW \u201915\nCompanion. ACM, 2015.\n[12] Yandong Liu, Jiang Bian, and Eugene Agichtein. Predicting Information Seeker Satisfaction in Community Question Answering. In SIGIR \u201908. ACM.\n[13] David Milne and Ian H. Witten. An Open-source Toolkit for Mining Wikipedia. Artif. Intell., 13.\n[14] Preslav Nakov, Llu\u0301\u0131s Ma\u0300rquez, Walid Magdy, Alessandro Moschitti, Jim Glass, and Bilal Randeree. Semeval-2015 Task 3: Answer Selection in Community Question Answering. In SemEval \u201915. ACL.\n[15] Preslav Nakov, Llu\u0301\u0131s Ma\u0300rquez, Alessandro Moschitti, Walid Magdy, Hamdy Mubarak, Abed Alhakim Freihat, Jim Glass, and Bilal Randeree. SemEval-2016 Task 3: Community Question Answering. SemEval \u201916. ACL.\n[16] Massimo Nicosia, Simone Filice, Alberto Barro\u0301n-Ceden\u0303o, Iman Saleh, Hamdy Mubarak, Wei Gao, Preslav Nakov, Giovanni Da San Martino, Alessandro Moschitti, Kareem Darwish, Llu\u0301\u0131s Ma\u0300rquez, Shafiq Joty, and Walid Magdy. QCRI: Answer Selection for Community Question Answering - Experiments for Arabic and English. In SemEval 2015, pages 203\u2013209. ACL.\n[17] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global Vectors for Word Representation. In EMNLP, 2014.\n[18] Aliaksei Severyn and Alessandro Moschitti. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. SIGIR \u201915, pages 373\u2013382. ACM.\n[19] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gregoire Mesnil. A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval. CIKM, 2014.\n[20] Valentin I. Spitkovsky and Angel X. Chang. A Cross-Lingual Dictionary for English Wikipedia Concepts. In LREC\u201912. ELRA.\n[21] Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. Feature-rich Part-of-speech Tagging with a Cyclic Dependency Network. NAACL \u201903, pages 173\u2013180. ACL.\n[22] Quan Hung Tran, Vu Tran, Tu Vu, Minh Nguyen, and Son Bao Pham. JAIST: Combining multiple features for Answer Selection in Community Question Answering. In SemEval 2015, pages 215\u2013219. ACL.\n[23] Xin-Jing Wang, Xudong Tu, Dan Feng, and Lei Zhang. Ranking Community Answers by Modeling Question-answer Relationships via Analogical Reasoning. In SIGIR \u201909. ACM.\n[24] Shangxuan Wu, Ying-Cong Chen, Xiang Li, An-Cong Wu, Jin-Jie You, and Wei-Shi Zheng, editors. An Enhanced Deep Feature Representation for Person Re-identification. http://arxiv.org/abs/1604.07807.\n[25] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical Evaluation of Rectified Activations in Convolutional Network. CoRR, abs/1505.00853, 2015.\n[26] Liang Yi, JianXiang Wang, and Man Lan. ECNU: Using Multiple Sources of CQA-based Information for Answers Selection and YES/NO Response Inference. In SemEval 2015, pages 236\u2013241. ACL.\n[27] Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. Deep Learning for Answer Sentence\nSelection. CoRR, abs/1412.1632, 2014.\n[28] Xiaoqiang Zhou, Baotian Hu, Jiaxin Lin, Yang xiang, and Xiaolong Wang. ICRC-HIT: A Deep Learning based Comment Sequence Labeling System for Answer Selection Challenge. In SemEval 2015. ACL."}], "references": [{"title": "Automatic Identification of Best Answers in Online Enquiry Communities", "author": ["Gr\u00e9goire Burel", "Yulan He", "Harith Alani"], "venue": "In The Semantic Web: Research and Applications: 9th Extended Semantic Web Conference,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "TAGME: On-the-fly Annotation of Short Text Fragments (by Wikipedia Entities)", "author": ["Paolo Ferragina", "Ugo Scaiella"], "venue": "CIKM \u201910,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Modeling Interestingness with Deep", "author": ["Jianfeng Gao", "Patrick Pantel", "Michael Gamon", "Xiaodong He", "Li Deng", "Yelong Shen"], "venue": "Neural Networks. EMNLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "HITSZ-ICRC:  Exploiting Classification Approach for Answer Selection in Community Question Answering", "author": ["Yongshuai Hou", "Cong Tan", "Xiaolong Wang", "Yaoyun Zhang", "Jun Xu", "Qingcai Chen"], "venue": "SemEval", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Recurrent Neural Network Encoder with Attention for Community Question Answering", "author": ["Wei-Ning Hsu", "Yu Zhang", "James R. Glass"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Learning Deep Structured Semantic Models for Web Search using Clickthrough", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Quoc V. Le", "Tomas Mikolov"], "venue": "CoRR, abs/1405.4053,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Answer Quality Characteristics and Prediction on an Academic QA Site: A Case Study on ResearchGate", "author": ["Lei Li", "Daqing He", "Wei Jeng", "Spencer Goodwin", "Chengzhi Zhang"], "venue": "In Proceedings of the 24th International Conference on World Wide Web,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Semeval-2015 Task 3: Answer Selection in Community Question Answering", "author": ["Preslav Nakov", "Ll\u00fa\u0131s M\u00e0rquez", "Walid Magdy", "Alessandro Moschitti", "Jim Glass", "Bilal Randeree"], "venue": "In SemEval \u201915", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "SemEval-2016 Task 3: Community Question Answering. SemEval \u201916", "author": ["Preslav Nakov", "Ll\u00fa\u0131s M\u00e0rquez", "Alessandro Moschitti", "Walid Magdy", "Hamdy Mubarak", "Abed Alhakim Freihat", "Jim Glass", "Bilal Randeree"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "QCRI: Answer Selection for Community Question Answering - Experiments for Arabic and English", "author": ["Massimo Nicosia", "Simone Filice", "Alberto Barr\u00f3n-Cede\u00f1o", "Iman Saleh", "Hamdy Mubarak", "Wei Gao", "Preslav Nakov", "Giovanni Da San Martino", "Alessandro Moschitti", "Kareem Darwish", "Ll\u00fa\u0131s M\u00e0rquez", "Shafiq Joty", "Walid Magdy"], "venue": "SemEval", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gregoire Mesnil"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "JAIST: Combining multiple features for Answer Selection in Community Question Answering", "author": ["Quan Hung Tran", "Vu Tran", "Tu Vu", "Minh Nguyen", "Son Bao Pham"], "venue": "SemEval", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Empirical Evaluation of Rectified Activations in Convolutional Network", "author": ["Bing Xu", "Naiyan Wang", "Tianqi Chen", "Mu Li"], "venue": "CoRR, abs/1505.00853,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "ECNU: Using Multiple Sources of CQA-based Information for Answers Selection and YES/NO Response Inference", "author": ["Liang Yi", "JianXiang Wang", "Man Lan"], "venue": "SemEval", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Deep Learning for Answer Sentence  Selection", "author": ["Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman"], "venue": "CoRR, abs/1412.1632,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "ICRC-HIT: A Deep Learning based Comment Sequence Labeling System for Answer Selection Challenge", "author": ["Xiaoqiang Zhou", "Baotian Hu", "Jiaxin Lin", "Yang xiang", "Xiaolong Wang"], "venue": "SemEval", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "To overcome the above problems, recent approaches [6, 7, 16, 18, 22, 23, 26\u201328] have focused on automatically ranking answers for a given question based on their quality.", "startOffset": 50, "endOffset": 79}, {"referenceID": 5, "context": "To overcome the above problems, recent approaches [6, 7, 16, 18, 22, 23, 26\u201328] have focused on automatically ranking answers for a given question based on their quality.", "startOffset": 50, "endOffset": 79}, {"referenceID": 11, "context": "To overcome the above problems, recent approaches [6, 7, 16, 18, 22, 23, 26\u201328] have focused on automatically ranking answers for a given question based on their quality.", "startOffset": 50, "endOffset": 79}, {"referenceID": 14, "context": "To overcome the above problems, recent approaches [6, 7, 16, 18, 22, 23, 26\u201328] have focused on automatically ranking answers for a given question based on their quality.", "startOffset": 50, "endOffset": 79}, {"referenceID": 16, "context": "To overcome the above problems, recent approaches [6, 7, 16, 18, 22, 23, 26\u201328] have focused on automatically ranking answers for a given question based on their quality.", "startOffset": 50, "endOffset": 79}, {"referenceID": 17, "context": "To overcome the above problems, recent approaches [6, 7, 16, 18, 22, 23, 26\u201328] have focused on automatically ranking answers for a given question based on their quality.", "startOffset": 50, "endOffset": 79}, {"referenceID": 18, "context": "To overcome the above problems, recent approaches [6, 7, 16, 18, 22, 23, 26\u201328] have focused on automatically ranking answers for a given question based on their quality.", "startOffset": 50, "endOffset": 79}, {"referenceID": 9, "context": "The cQA tasks of SemEval-2015 (Task A) [14] and SemEval-2016 (Task A) [15] provide a universal benchmark for evaluating research on this problem.", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "The cQA tasks of SemEval-2015 (Task A) [14] and SemEval-2016 (Task A) [15] provide a universal benchmark for evaluating research on this problem.", "startOffset": 70, "endOffset": 74}, {"referenceID": 4, "context": "Recent approaches for answer quality prediction can be categorized into - a) Hand-crafted Feature (HCF) based approaches [6, 16, 22, 23, 26] or b) Deep Learning (DL) based approaches [7, 18, 27, 28].", "startOffset": 121, "endOffset": 140}, {"referenceID": 11, "context": "Recent approaches for answer quality prediction can be categorized into - a) Hand-crafted Feature (HCF) based approaches [6, 16, 22, 23, 26] or b) Deep Learning (DL) based approaches [7, 18, 27, 28].", "startOffset": 121, "endOffset": 140}, {"referenceID": 14, "context": "Recent approaches for answer quality prediction can be categorized into - a) Hand-crafted Feature (HCF) based approaches [6, 16, 22, 23, 26] or b) Deep Learning (DL) based approaches [7, 18, 27, 28].", "startOffset": 121, "endOffset": 140}, {"referenceID": 16, "context": "Recent approaches for answer quality prediction can be categorized into - a) Hand-crafted Feature (HCF) based approaches [6, 16, 22, 23, 26] or b) Deep Learning (DL) based approaches [7, 18, 27, 28].", "startOffset": 121, "endOffset": 140}, {"referenceID": 5, "context": "Recent approaches for answer quality prediction can be categorized into - a) Hand-crafted Feature (HCF) based approaches [6, 16, 22, 23, 26] or b) Deep Learning (DL) based approaches [7, 18, 27, 28].", "startOffset": 183, "endOffset": 198}, {"referenceID": 17, "context": "Recent approaches for answer quality prediction can be categorized into - a) Hand-crafted Feature (HCF) based approaches [6, 16, 22, 23, 26] or b) Deep Learning (DL) based approaches [7, 18, 27, 28].", "startOffset": 183, "endOffset": 198}, {"referenceID": 18, "context": "Recent approaches for answer quality prediction can be categorized into - a) Hand-crafted Feature (HCF) based approaches [6, 16, 22, 23, 26] or b) Deep Learning (DL) based approaches [7, 18, 27, 28].", "startOffset": 183, "endOffset": 198}, {"referenceID": 0, "context": "[1] have used a combination of content, user and thread related features for predicting answer quality.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[11] studied the various factors such as shorter length, authors reputation which lead to a high answer quality rating as rated by peers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[22] made use of topic models, word vectors and other hand crafted rules to train a SVM classifier for AQP.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[6] made use of statistics like avg.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[27] used CNN to learn a distributional sentence model for AQP from bag of words and bigram based word representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "al [16] have used lexical similarity between word n-grams, tree kernels, word-embeddings and other hand crafted features for AQP.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "[7] used a LSTM encoder with neural attention mechanism to auto-", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[28] used a 2-dimensional CNN to represent a question-answer pair and ranked the representations using a Recurrent Neural Network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "We use GLoVE [17] based embeddings of 300 dimensions to map the words in the question and answer.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "We use max-pooling for the pooling layer and Randomized Leaky Rectified Linear Unit (RReLU) [25], a randomized version of leakyReLU [25], as the non-linearity layer.", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "We use max-pooling for the pooling layer and Randomized Leaky Rectified Linear Unit (RReLU) [25], a randomized version of leakyReLU [25], as the non-linearity layer.", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "TagMe Similarity: We extract TagMe concepts of the question and answer by mapping them to their corresponding Wikipedia page titles using TagMe [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 7, "context": "Paragraph2vec Similarity: Paragraph2Vec [10] allows to model vectors for text of any arbitrary length.", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "Sent2Vec performs the mapping using the Deep Structured Semantic Model (DSSM) built using Clickthrough data [8], or the DSSM with convolutional-pooling structure (CDSSM) [5,19].", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "Sent2Vec performs the mapping using the Deep Structured Semantic Model (DSSM) built using Clickthrough data [8], or the DSSM with convolutional-pooling structure (CDSSM) [5,19].", "startOffset": 170, "endOffset": 176}, {"referenceID": 13, "context": "Sent2Vec performs the mapping using the Deep Structured Semantic Model (DSSM) built using Clickthrough data [8], or the DSSM with convolutional-pooling structure (CDSSM) [5,19].", "startOffset": 170, "endOffset": 176}, {"referenceID": 9, "context": "For training, we used the training data provided in the SemEval 2015 [14] and 2016 [15] tasks which consists of question, answer, metadata along with their ideal quality rating.", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "For training, we used the training data provided in the SemEval 2015 [14] and 2016 [15] tasks which consists of question, answer, metadata along with their ideal quality rating.", "startOffset": 83, "endOffset": 87}, {"referenceID": 1, "context": "We used Adagrad [3] to speed up the convergence rate of stochastic gradient descent (SGD).", "startOffset": 16, "endOffset": 19}, {"referenceID": 10, "context": "We use the SemEval 2016 [15] and SemEval 2015 [14] datasets for our experiments as it exactly matches our problem description.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "We use the SemEval 2016 [15] and SemEval 2015 [14] datasets for our experiments as it exactly matches our problem description.", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "We compare our approach with the top two best performing systems from SemEval 2015 - JAIST [22] and HITSZ-ICRC [6].", "startOffset": 91, "endOffset": 95}, {"referenceID": 4, "context": "We compare our approach with the top two best performing systems from SemEval 2015 - JAIST [22] and HITSZ-ICRC [6].", "startOffset": 111, "endOffset": 114}, {"referenceID": 18, "context": "We also compare with ICRCHIT [28] as it uses a purely deep learning based model.", "startOffset": 29, "endOffset": 33}, {"referenceID": 10, "context": "Similarly, for SemEval 2016, we compare with their corresponding top two best performing systems - Kelp [15] and ConvKN [15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "Similarly, for SemEval 2016, we compare with their corresponding top two best performing systems - Kelp [15] and ConvKN [15].", "startOffset": 120, "endOffset": 124}], "year": 2016, "abstractText": "Community Question Answering (cQA) forums have become a popular medium for soliciting direct answers to specific questions of users from experts or other experienced users on a given topic. However, for a given question, users sometimes have to sift through a large number of low-quality or irrelevant answers to find out the answer which satisfies their information need. To alleviate this, the problem of Answer Quality Prediction (AQP) aims to predict the quality of an answer posted in response to a forum question. Current AQP systems either learn models using a) various hand-crafted features (HCF) or b) use deep learning (DL) techniques which automatically learn the required feature representations. In this paper, we propose a novel approach for AQP known as -\u201cDeep Feature Fusion Network (DFFN)\u201dwhich leverages the advantages of both hand-crafted features and deep learning based systems. Given a question-answer pair along with its metadata, DFFN independently a) learns deep features using a Convolutional Neural Network (CNN) and b) computes hand-crafted features using various external resources and then combines them using a deep neural network trained to predict the final answer quality. DFFN achieves stateof-the-art performance on the standard SemEval-2015 and SemEval-2016 benchmark datasets and outperforms baseline approaches which individually employ either HCF or DL based techniques alone.", "creator": "LaTeX with hyperref package"}}}