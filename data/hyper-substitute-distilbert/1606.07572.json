{"id": "1606.07572", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Enriching Linked Datasets with New Object Properties", "abstract": "multiple periodic rdf performance tables are available along the lod initiative, often related data sources remain isolated, lacking functionality both relational to other structures. while there are important works available verge on establishing that two resources are identical and avoided adding unexpected instances of connecting already existing content, the problem function finding optimal access between them two given datasets has not been simplified in detail. in this paper, given two primary approaches, we provide an unsupervised model regarding enhancing conventional lod records with new relations upon them by exploiting semantic linking corpus. during field initiation phase we gather prospective relations from the corpus through pattern extraction and flaw detection. and further second phase, we discover actual enrichment by extracting events'linking relations. learners prefer empirically evaluated multiple designs on several relation pairs \u2026 found content source system l sometimes be used for editing frequently available datasets with new relations.", "histories": [["v1", "Fri, 24 Jun 2016 06:00:42 GMT  (34kb)", "https://arxiv.org/abs/1606.07572v1", "Technical report"], ["v2", "Fri, 24 Mar 2017 04:49:49 GMT  (30kb)", "http://arxiv.org/abs/1606.07572v2", "Technical report"], ["v3", "Mon, 4 Sep 2017 11:12:42 GMT  (55kb)", "http://arxiv.org/abs/1606.07572v3", "Technical report"]], "COMMENTS": "Technical report", "reviews": [], "SUBJECTS": "cs.DB cs.AI cs.CL", "authors": ["subhashree s", "p sreenivasa kumar"], "accepted": false, "id": "1606.07572"}, "pdf": {"name": "1606.07572.pdf", "metadata": {"source": "META", "title": "Enriching Linked Datasets with New Object Properties", "authors": [], "emails": ["ssshree@cse.iitm.ac.in", "psk@cse.iitm.ac.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n07 57\n2v 3\n[ cs\n.D B\n] 4\nS ep\n2 01\n7\nAlthough several RDF knowledge bases are available through the LOD initiative, the ontology schema of such linked datasets is not very rich. In particular, they lack object properties. The problem of finding new object properties (and their instances) between any two given classes has not been investigated in detail in the context of Linked Data. In this paper, we present DART (Detecting ArbitraryRelations for enriching T-Boxes of Linked Data) - an unsupervised solution to enrich the LOD cloud with new object properties between two given classes. DART exploits contextual similarity to identify text patterns from the web corpus that can potentially represent relations between individuals. These text patterns are then clustered by means of paraphrase detection to capture the object properties between the two given LOD classes. DART also performs fully automated mapping of the discovered relations to the properties in the linked dataset. This serves many purposes such as identification of completely new relations, elimination of irrelevant relations, and generation of prospective property axioms. We have empirically evaluated our approach on several pairs of classes and found that the system can indeed be used for enriching the linked datasets with new object properties and their instances. We compared DART with newOntExt system which is an offshoot of the NELL (Never-Ending Language Learning) effort. Our experiments reveal that DART gives better results than newOntExt with respect to both the correctness, as well as the number of relations.\nKEYWORDS\nLinked Data, LOD enrichment, arbitrary relations, object properties, grounding\nACM Reference Format: Subhashree S and P Sreenivasa Kumar. 2017. Enriching Linked Datasets withNewObject Properties. In Proceedings ofACMK-CAP conference, Austin, Texas USA, Dec 2017 (K-CAP 2017), 9 pages. https://doi.org/10.475/123_4"}, {"heading": "1 INTRODUCTION", "text": "The Linked Data initiative provides a set of guidelines and best practices for publishing structured data and representing attribute values and relations among a set of entities. The Linking Open\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this workmust be honored. For all other uses, contact the owner/author(s). K-CAP 2017, Dec 2017, Austin, Texas USA \u00a9 2017 Copyright held by the owner/author(s). ACM ISBN 123-4567-24-567/08/06. . . $15.00 https://doi.org/10.475/123_4\nData (LOD) community project1 works with the main objective of publishing open datasets as RDF triples and establishing RDF links between entities (aka objects) from different datasets. LOD complements the world wide web with a data space of entities connected to one another with labelled edges, which represent the relations among entity pairs (or entities and literal values). Many organizations have built systems to exploit the power of Linked Data for specific purposes. For example, the British Broadcasting Corporation (BBC) uses linked datasets such as DBpedia [18] to enable cross-domain navigation and enhanced search2 in their websites. IBM has been using Linked Data as an integration technology for several years and their new cognitive system, Watson, has DBpedia and YAGO [36] as part of its major data sources [11].\nCurrently, most linked datasets are rich in A-Box assertions but poor in T-Box information i.e they have a very weak ontology schema. They especially lack object properties. For example, the linked dataset YAGO has 488,469 classes [20]. Among such a huge number of classes, surprisingly there are only 32 object properties3 and hence looking for more object properties to connect these classes becomes an interesting task. Adding more object properties to the ontology schema will help in enriching the domain being represented in the linked dataset. Question answering systems can make use of these additional relations to answer more number and also a wider range of questions. To realize the full potential of Linked Data in various applications, it is important to enrich LOD with as many appropriate ontological axioms and assertions as possible.\nMost of the Linked Data enrichment works (surveyed in [33]) focus on adding more instances to existing object properties (in this paper, the term \u2018relation\u2019 is used as a synonym of \u2018object property\u2019). There are not many techniques available in the literature that identify new relations, given two LOD classes.\nThe systems proposed in ([25], [4]) for the purpose of extending the NELL ontology, OntExt and newOntExt respectively, can be adapted to the Linked Data settings to discover new object properties between given LOD classes. However, we found the following issues with their working: newOntExt tends to miss out important relations. It seeks to represent relations as text patterns and cluster patterns based on how frequently they co-occur with a pair of entities in a text corpus. The system tends to group dissimilar patterns into the same cluster and finally only the representative\n1http://www.w3.org/wiki/SweoIG/TaskForces/CommunityProjects/LinkingOpenData 2https://www.w3.org/2001/sw/sweo/public/UseCases/BBC/ 3http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/statistics/ - totally there are 60 object properties, but 28 of them connect the domain class to the class http://dbpedia.org/class/yago/YagoLiteral\nrelation of the cluster is output by the system as a newly discovered relation. For example, given the classes athletes and sportsleagues as inputs, newOntExt places the relations \u201cdoesn\u2019t play at\" (currently not playing) and \u201cwants to play at\" (wish to play) in the same cluster [31] because these two relations occur between the same subject-object pairs with a high frequency. Hence, only one of them gets selected as the cluster\u2019s representative relation, though both of them are correct relations, but with different meanings. Also, newOntExt does not do any contextual check to see if the pattern actually fits the context of the given two classes. For example, between the classes Languages and Countries, an incorrect pattern \u201care people living in\" is obtained from the web corpus as \u201cChinese\" can refer to both the language as well as the ethnic group. newOntExt does not perform any contextual check to eliminate such a pattern.\nIn this paper, we present DART (Detecting Arbitrary Relations for enriching T-Boxes of Linked Data) which adopts an unsupervised approach in order to discover and add new object properties and their instances to a linked dataset. DART exploits contextual similarity tools and paraphrase detection in order to identify the correct set of text patterns which are most-likely to be useful as object properties between the two given LOD classes. Additionally, it grounds the relations to the linked dataset in order to identify the completely new relations and is also capable of generating candidate property axioms. By grounding, we mean mapping of discovered relations to existing LOD object properties.\nTo summarize, our contributions include the following:\n(1) Given two classes belonging to a linked dataset, the proposed system DART discovers relations between them by exploiting text patterns from the web corpus, hence enriching the T-Box of the linked dataset. For example, given the two classes, Religions4 and Countries5, DART generates relations such as \u201cbecame the official religion in\", \u201cis the predominant religion in\" etc. (2) For each generated relation, a set of paraphrases are also generated that can be used to extract additional instances of the relation. (3) DART produces instances of the newly generated relations, leading to the enrichment of the A-Box. Continuing with the above example, it can add triples of the form (Hinduism, became the official religion in, Nepal), (Christianity, is the predominant religion in, Australia) etc. (4) A completely automated technique for grounding of the generated relations in the linked dataset has been proposedwhich also suggests T-Box axioms for the newly generated relations. For example, in the case of Empires6 and Rulers7, DART infers that the newly generated relation \u201cwas ruler of\" might be a sub-property of the YAGO property \u201cisLeaderOf\". (5) Through the process of grounding, DART also eliminates irrelevant and ambiguous relations.\nOur experiments show that DART gives much better results than newOntExt in terms of both precision and recall on input classes\n4http://dbpedia.org/class/yago/Religion105946687 5http://dbpedia.org/class/yago/Country108544813 6http://dbpedia.org/class/yago/Empire108557482 7http://dbpedia.org/class/yago/Ruler110541229\nbelonging to different domains. DART is also capable of suggesting insightful property axioms. The rest of the paper is structured as follows: Section 2 describes the related works from the literature. Section 3 gives an account of the working of DART with each phase of the approach explained in detail. The experiments conducted by us in order to evaluate the effectiveness of the approach are presented in Section 4 along with the comparison ofDARTwith newOntExt. Conclusions drawn from the work are given in Section 5."}, {"heading": "2 RELATED WORKS", "text": "Relation enrichment (of those other than the owl:sameAs links) of the linked datasets for the purpose of the overall growth of the LOD cloud has been the major focus in many recent works (surveyed in [33]). Most of the relation enrichment approaches surveyed in [33] focus on extracting more instances (subject-object pairs) of existing relations in the linked datasets. Works such as ([27], [26]), [35], ([38], [29], [28] and [30]) and [19] use the technique of interpreting web tables for this purpose and a few other works such as [37] and [16] propose using various semi-supervised approaches for the same. Distant supervision is another new paradigm which has been recently adopted by many works ([17], [24], [1], [2], [32]) in order to extractmore instances of existing relations. Distant supervision is the technique of utilizing a large number of known facts (from a huge linked dataset such as Freebase) for automatically labeling mentions of these facts in an unannotated text corpus, hence generating training data. A classifier is learnt based on this weakly labeled training data in order to classify unseen instances [17].\nApart from enriching the datasets with additional instances of already existing relations, two other less-explored problems of relation enrichment are: (1)finding instances of specified new relations and (2) discovering arbitrary new relations.\nBy instances of specified new relations, we mean that the relation is not present in the dataset currently but the name of the relation is given to the system and the system needs to add instances of such a relation to the dataset. The technique proposed in [15] to detect instances of \u201cpart-of\u201d (partonomy) relation between linked data instances falls under this category. Similarly, the SILK link discovery framework [6] which is primarily used to detect owl:sameAs links is also capable of detecting instances of user-specified relations. It uses its own declarative language, Silk - Link Specification Language (Silk-LSL) in order to specify the two datasets between which the links ought to be found and to give the link type. Coming to the second problem of discovering arbitrary new relations, it can be defined as the task of finding any or all possible relations between two given classes. We find that it has not been tackled by many works. It is precisely this problem we address in this paper. It is to be noted that the system is not aware of the possible relations between the concerned classes before-hand and hence such relations are termed as arbitrary relations (as defined in [9]). There are two systems, OntExt [25] and newOntExt ([4], [5]) which have been proposed in the context of helping NELL to extend its ontology by means of discovering new relations between the ontology classes. They are described below:\nOntExt: Given two noun categories ([7] calls classes as noun categories), and their instances, OntExt discovers relations between them by exploiting the notion that similar patterns occur between the same subject-object pairs. For example, if the patterns \u201cGanges flows through Allahabad\" and \u201cGanges in the heart of Allahabad\" occur in the web corpus with a very high frequency then this can be taken as an indicator that the patterns, \u201cflows through\" and \u201cin the heart of\" are similar to each other. When such an evidence is shown by many number of subject-object pairs, OntExt gives a very high similarity score between the two patterns. In general, OntExt works in the following manner: given a pair of categories and a set of sentences-each containing a pair of instances known to belong to the given categories, OntExt collects the words in between the instances from each sentence and calls these words a \u201ccontext-pattern\". Then it builds a context-pattern by contextpattern co-occurrence matrix based on the frequencies of occurrence of these context-patterns with the same subject-object instance pairs. For example, in the above case of finding relations between Rivers and Cities, if the pair \u201cGanges\" and \u201cAllahabad\" occurs with the context-pattern \u201cflows through\" with a frequency f1 and the pair occurs with the pattern \u201cin the heart of\" with a frequency f2, then the matrix entry corresponding to these two context-patternswill be given a value of (f1+ f2). In case there is another subject-object pair (for example- Thames, London) occurring with both these context-patterns with frequencies f3 and f4 respectively, then thematrix cell value becomes (f1+ f2+ f3+ f4). K-means clustering is applied on the normalized matrix to group the related context-patterns together. The centroid of each cluster is proposed as a new relation. Then the subject-object pairs are ranked based on how often they occur alongwith each context-pattern using the formula in equation (1). Finally, the top 50 subject-object pairs are given as seed instances of the new relation to NELL [7]. Weight of a (subject,object) pair \u201cs\"\n=\n\u2211\nc \u2208cluster\nOcc(c, s) 1 + sd(c) (1)\nWhere, cluster is the cluster of pattern contexts for the given new relation, Occ(c,s) is the number of times instance \u201cs\" co-occurs with the context pattern \u201cc\", sd(c) is the standard deviation of the context pattern from the centroid of the pattern cluster\nAs more than half of the relations generated by OntExt were invalid (determined manually in [25]), the authors of OntExt have proposed a classifier which can differentiate between valid and invalid relations to some extent.\nnewOntExt: newOntExt which was developed based on OntExt had a few changes in its working [4]: instead of considering all the words in between the two input instances as a pattern, newOntExt used ReVerb [9] for extracting the patterns in order to reduce the number of noisy patterns obtained; for optimising the computational cost, a more elegant file structure was used for searching through the sentences; instead of considering every pair of categories as input to this system, reduced category groups of interest were formed to pick the input category pairs.\nA major difference between DART and newOntExt is that the latter takes co-occurrence values of the patterns to be an indicator of the semantic similarity between them whereas DART computes the semantic similarity by means of paraphrase detection techniques. It should be noted that DART does not rely upon the lexical similarity of the patterns i.e DART can detect the semantic similarity even if the two patterns have disjoint set of words. In addition to this, DART also performs grounding of relations and generation of candidate property axioms. Comparison of DART with newOntExt is described in Section 4.1."}, {"heading": "3 WORKING OF DART", "text": ""}, {"heading": "3.1 Pre-processing", "text": "Given two classes D1 and D2, we need patterns occurring in the web corpus along with the instances of D1 and D2, in order to discover the possible relations between them. Hence for this purpose, we obtain (subject, predicate, object) triples - known as a triple corpus C from the RCE 1.1 file 8, such that the subject and object belong to D1 (D2) and D2 (D1) respectively. We have used the RCE dataset in our experiments as newOntExt employs ReVerb and we wished to maintain uniform set of inputs for both DART and newOntExt for a fair comparison. However we can also replace this step in the following manner: use a web corpus such as ClueWeb and extract sentences containing instances of D1 and D2; then apply any triplification tool such as ClausIE [8], Ollie [21] etc to obtain the input triples corpus C.\nWe also store the direction of these triples in C, i.e if the subject of the triple belongs to D1 and the object belongs to D2, then the direction is marked as \u201cforward\". If subject belongs to D2 and object belongs to D1, the direction is marked as \u201creverse\"."}, {"heading": "3.2 Relation discovery phase", "text": "Relation discovery phase, given in Algorithm 1, takes the corpus C as input and outputs clusters of synonymous relations. We collect all the unique predicates in C (let us call them \u201cpatterns\") and filter them based on whether they are suitable for the given input domain or not (Lines 1-7) i.e a contextual similarity check is performed in the following manner: in each pattern, all the function words 9 are removed (as they are not context-specific words) and the remaining words are checked for similarity with the domain name. For example, let us assume that the user intends to find the relations between a set of rivers and a set of cities, and the user-specified domain name is \u201criver\". If the pattern under consideration is \u201crises in\", DART checks the similarity of \u201crises\" (as the other word \u201cin\" is a functional word) with \u201criver\" and if this similarity crosses a certain threshold (more details on how this threshold was fixed are given in Section 4), DART includes this pattern else discards it. We use the Word2Vec [23] model proposed and trained by Google 10 for finding the contextual similarity. The intuition behind this step is that, patterns not relevant to the domain obtained from the web corpus can be eliminated by checking if the contexts of the pattern and the domain name are close to each other, i.e this\n8ReVerb ClueWeb Extractions 1.1: dataset consisting of 15 million triples produced by running ReVerb on the English portion of ClueWeb09 corpus 9http://www.sequencepublishing.com/academic.html 10https://code.google.com/archive/p/word2vec/\nserves as a pseudo disambiguation step. The filtered patterns are then subjected to single pass clustering [13]. Single pass clustering works as follows (Lines 8-33): Take each pattern \u201cp\" and check its semantic similarity with the representative relations of all the clusters. Place \u201cp\" in the cluster whose representative relation has themaximum similaritywith it. Now recompute the representative relation for this augmented cluster in the following manner - representative relation is the patternwhich has the maximum average similarity with the other patterns in that cluster. If the maximum similarity value is lesser than a fixed threshold value (=0.5), place \u201cp\" in a new cluster.\nIn order to determine the semantic similarity between two patterns, we modified the paraphrase detection technique proposed by Mihalcea et al. [22]: We have eliminated the word specificity weights. In [22], the individual word-to-word similarity valueswere weighted using a word specificity measure so that higher importance can be given to a semantic matching identified between two specific words such as \u201ccollie\" and \u201csheepdog\" when compared to a matching identified between words such as \u201cget\" and \u201cbecome\". In the context of DART, words such as \u201cget\" and \u201cbecome\" (any verb in general) have a good chance of occurring in the input patterns as the aim of DART is to extract relations between classes. Hence, giving a low weight to such words (as done in [22]) is not appropriate in the context of DART. The formula used to determine similarity of patterns in our work is given in equation (2).\nsim(T1,T2) = 1/2(\n\u2211 w \u2208T1 (maxSim(w,T2))\nlen(T1) +\n\u2211 w \u2208T2 (maxSim(w,T1))\nlen(T2) )\n(2)\nwhere, T1 and T2 represent the input text segments, maxSim(w,Ti ) refers to the similarity value of theword inTi which is most similar to the wordw in the other text segment, len(Ti) refers to the number of words in Ti\nIn our implementation, the threshold value chosen to consider two segments T1 and T2 to be similar is 0.5 (adopted from [22]).\nLESK [3] has been used to perform the word-to-word similarity component of equation (2), as it works for all combinations of parts of speech. The representative relations of the clusters obtained at the end of this phase form the relations between the two given classes."}, {"heading": "3.3 Grounding of relations", "text": "Once the relation discovery phase generates relations between the two input classes, the system needs to check if these relations can be grounded in the linked dataset, i.e whether they can be mapped to some existing property in the linked dataset. Only if a relation cannot be grounded (mapped to existing LODproperties), it is added as a new relation leading to T-Box enrichment of the linked dataset. In order tomap a representative relationwith existing LOD properties, DART checks the semantic similarity between an LOD property p and the representative relation r (using equation (2), but with an increased threshold of 0.75 as we want to avoid spurious mappings). If the similarity value crosses 0.75, then the similarity\nAlgorithm 1: Relation-Discovery\nInput: C , the corpus; dname , user-specified domain name; cThreshold , the threshold for contextual similarity; sThreshold , the threshold for semantic similarity; Output: clusts , clusters of relations; repRels , representative relations;\n1 allPatterns\u2190 set of all patterns in C\n2 f ilteredPatterns\u2190 \u2205\n3 for each pattern p \u2208 allPatterns do 4 if any word in p has wordToVec similarity with dname >=\ncThreshold then\n5 add p to f ilteredPatterns\n6 end\n7 end\n8 clusts \u2190 \u2205\n9 repRels \u2190 \u2205\n10 for each pattern p \u2208 f ilteredPatterns do"}, {"heading": "11 maxRel \u2190 null", "text": "12 if repRels = \u2205 then 13 add p to a new cluster cl\n14 add p to repRels\n15 add cl to clusts"}, {"heading": "16 end", "text": ""}, {"heading": "17 else", "text": "18 maxRel \u2190 the representative relation which has the maximum similaritymaxSim with p 19 if maxSim >= sThreshold then 20 add p to cl1, the cluster containingmaxRel ;\n21 for each pattern p1 \u2208 cl1 do 22 calculate p1\u2019s average similarity with other patterns in cl1"}, {"heading": "23 end", "text": "24 rep \u2190 pattern in cl1 having maximum average similarity 25 add rep to repRels"}, {"heading": "26 end", "text": ""}, {"heading": "27 else", "text": "28 add p to a new cluster cl\n29 add p to repRels\n30 add cl to clusts"}, {"heading": "31 end", "text": ""}, {"heading": "32 end", "text": ""}, {"heading": "33 end", "text": "34 return clusts , repRels\nbetween p and every relation in the cluster of r is determined. Finally, if more than 50% of the relations in r \u2019s cluster have a similarity value >= 0.75 with p, then r is said to be grounded and matched to p.\nIn order to determine the domain and range of the LOD property to which the relation was grounded, we use the ontology of the linked dataset (if the ontology lacks this information, we use the\nsystem proposed in [39]). The domain and range of the grounded relation are the 2 input classes. Then these grounded relations are handled by DART in two ways: If the grounded relation r and the matched LOD property p have the same domain and range then it means we have detected a new equivalent property to p. If the domain of r is the range of p and if the range of r happens to be the domain of p, then it means r is a new inverse property for p. If the domain (and/or range) of r is a subclass of the domain (and/or range) of p, then it means we have discovered a new sub-property for the LOD propertyp. Hence in these cases, we don\u2019t discard the relation completely. More relation instances of r are produced by DART in the Triple-Finding phase (Section 3.4). However if the domain and range do not match in any of the above mentioned ways, then we consider the grounded relation \u201cr\" as an ambiguous or irrelevant (noisy) relation and hence completely discard it. In this paper, we call a relation ambiguous if it holds between 2 or more pairs of classes. For example, the relation \u201ccaused by\" is an ambiguous relation as it is a meaningful relation between the classes (Event, Event) as well as between the classes (Disease, Drug).\nNote that there are a few other works in the literature (such as [10]) which focus mainly on the grounding of relations in a Knowledge Base (KB). However, the goal of such systems and the goal of DART are very different from each other with respect to grounding - the former kind of systems extract triples from an external source (such as text) and attempt to ground the relations. They retain only the grounded relations and consider the non-grounded relations as irrelevant to the schema and hence discard them. In our system, we attempt to ground the discovered relations in a linked dataset in order to achieve three things: identify irrelevant relations among the grounded relations and discard them; align the remaining grounded relations to the ontology schema and generate prospective axioms such as inverse, subproperty etc.; identify the non-grounded (new) relations and add them to the T-Box. Moreover in systems such as [10], grounding of relations is based on the grounding of entities. We do not use this approach as it will not help us to identify irrelevant and ambiguous relations. Also, the method in [10] is semi-automatic i.e a human is involved to decide whether \u201cbuy\" can be mapped to the relation \u201cacquired\". On the contrary, DART performs this phase in an automated fashion."}, {"heading": "3.4 Triple finding phase", "text": "In this phase, we intend to find all triples (s,p,o) where p is a relation found in the previous phase, hence enriching the A-Box of the linked dataset. In order to obtain the instances of a new relation (let us call this \u201cp\"), each relation \u201cr\" in p\u2019s cluster is looked up in the corpus C, and the subject-object pair found in C for the relation \u201cr\" is given as an instance to \u201cp\". Hence (subject ,p,object) becomes the final triple. One thing to be noted here is if the relation looked up in the corpus (\u201cr\") is of forward direction and the relation \u201cp\" is of reverse direction (notion of directions explained in the Section 3.1), then the final triple given as output becomes (object ,p, subject)."}, {"heading": "4 EXPERIMENTS AND RESULTS", "text": "The proposed system, DART, has been implemented in Java 1.7 and all experiments have been conducted on a Linux system equipped\nwith an Intel 3.20 GHz quad-core processor and 32 GB main memory. All details regarding the input classes, the relations and relation instances obtained can be found in our project web page11.\nThe experiments conducted on the NELL Knowledge Base for the purpose of comparing DART with newOntExt, and the observations made are given in Section 4.1. The details about the experiments held to gauge DART\u2019s performance on LOD classes are given in Section 4.2."}, {"heading": "4.1 Comparison with newOntExt", "text": "Though the primary aim of DART is to enrich linked datasets such as DBpedia, YAGO etc., in this subsection, we have conducted experiments on collections of entities belonging to theNELLKnowledgeBase12 in order to compare DART against the newOntExt system. We have used the implementation of newOntExt provided by the authors of [4]13. The systems have been compared using two measures, accuracy and the number of meaningful (the terms \u201cmeaningful\" and \u201ccorrect\" have been used synonymously in the paper) relations obtained. Accuracy is taken as the ratio of the correct relations (as determined by human evaluators) to the total number of relations obtained.\nFor newOntExt, the value of k used in the k-means clustering of patterns (see Section 2) affects the quality of the relations obtained to a large extent. Since the value of k used is not mentioned in ([4], [5]) and has been fixed in a dataset-specific manner in [25], we have applied the Elbow method [34] to determine the best k value (from a range of k=3 to k=29) for clustering the patterns, for each experiment. For DART, the threshold used for checking the contextual similarity using Word2Vec has an impact on the quality of the relations obtained. Hence we conducted experiments for each input class pair for 5 different thresholds - 0.1, 0.2, 0.3, 0.5 and 0.7. We observed that the thresholds of 0.3, 0.5 and 0.7 give very meaningful but very less number of relations. On the other hand, setting the threshold to 0.1 gives very high number of relations (around 130) but most of them are noisy, irrelevant relations. Therefore we decided to use the threshold of 0.2 uniformly for our experiments in order to maintain a good trade-off between the correctness and the number of relations obtained (however, the user can choose to vary this threshold depending on the requirements of the application). For the evaluation, the relations were presented in this format: <classname> relation <classname>(for example, <rivers> flows through <cities>) and three ontology engineers were assigned to evaluate them on a two-valued scale: correct, and incorrect. We required that all the three evaluators agree that a relation is correct in order for it to be counted as correct. Table 1 gives details about the input categories and a few sample relations obtained through DART.We have chosen the input categories such that they belong to different domains (Geography, Industries and Medicine) in order to demonstrate the versatility of the proposed system. Also, these particular categories were chosen from their respective domains to ease the process of manual evaluation. Table\n2 gives the accuracy and the number of correct relations obtained through DART and newOntExt.\nFromTable 2we can see that DARTperforms better than newOntExt both as a recall-oriented system and as a precision-oriented system. Since clustering of patterns in newOntExt is based on cooccurrence values, dissimilar(but meaningful) patterns tend to get grouped together and hence many meaningful patterns get lost, leading to lower number of correct relations from newOntExt. For example, in the experiment conducted on the classes CEOand Company, newOntExt places the patterns \u201cis the ceo of\" and \u201cis the founder of\" into the same cluster because the two patterns occur between the same set of subject-object pairs. Only one pattern from a cluster gets chosen as the centroid of the cluster and output by newOntExt and hence the other pattern is dropped though it is a meaningful relation between the given classes. Also, theWord2Vec model used by DART has eliminated irrelevant patterns such as \u201care people living in\" (in the case of Languages and Countries) leading to a better accuracy value of DART.\n4.1.1 Grounding in the context of NELL relations. The convention followed by NELL and the LOD for naming the relations are different. In NELL, the domain and/or range names are appended to the actual relation to form the relation name. For example, the relation \u201cflows through\" which holds between the classes Rivers14 and Cities15 is named \u201criverflowsthroughcity\" (in LOD, such a relation would be named \u201cflowsThrough\"). Similarly, the relation \u201cside effect caused by\" which holds between the classes Physiological Condition and Drugs is named \u201csideeffectcausedbydrug\" in NELL. The advantage of using such a naming technique is that every\n11https://sites.google.com/site/ontoworks/projects 12NELL.08m.1050.esv.csv \u201cevery belief in the KB\" file downloaded from http://rtw.ml.cmu.edu/rtw/resources on 26th April 2017 13https://github.com/MaLL-UFSCar/ontext 14http://rtw.ml.cmu.edu/rtw/kbbrowser/pred:river 15http://rtw.ml.cmu.edu/rtw/kbbrowser/pred:city\nsense of the relation can be captured through its name itself, thus giving no room for ambiguity. Hence there is no necessity for grounding the generated relations in the NELL Knowledge Base. Also, the main goal of DART is to enrich the LOD and hence we exclude the process of grounding in our experiments on the NELL KB. We compare the number of correct relations obtained through DART and those obtained through newOntExt (see Table 2) irrespective of whether they are already present in the NELL KB. This is to demonstrate the efficacy of DART vs newOntExt in the context of discovering relations between given classes.\n4.1.2 Complexity of DART vs newOntExt. Table 3 gives the details about the time taken by DART and newOntExt for the four experiments.\nAs newOntExt follows co-occurrence based clustering of patterns and DART performs semantic similarity check for clustering of patterns, the time taken by DART would be inherently higher than newOntExt. However we have attempted to reduce the computational complexity in two ways: by employing Word2Vec to filter patterns and by using single-pass clustering to cluster the patterns(as opposed to clustering algorithms like k-means which perform several iterations). For example, in the case of CEOs and Companies the initial number of patterns was 339, whereas after\nfiltering through Word2Vec the number of patterns remarkably reduced to 51. Hence the final number of patterns subjected to clustering is low leading to a reduced consumption of time (even lesser than newOntExt). In most of the cases DART takes only around few seconds to 1 minute to perform its task (except for the case of Languages and Countries where 230 patterns are output by the Word2Vec stage and subjected to clustering). It is an interesting piece of future work to further optimize the working of DART."}, {"heading": "4.2 Evaluation of DART on linked datasets", "text": "In this Section, we give an account of the experiments held to demonstrate the enrichment of LOD through DART, i.e we have chosen classes from linked datasets such as YAGO and DBpedia as our input classes. Table 4 gives the details of the input classes taken and a few sample relations obtained through DART. Here again, we have chosen these classes from different domains (Geography, Literature, History and Music) to prove that our approach is versatile. Table 5 gives an account of few relationswhich weremapped to the LOD properties in each experiment, and the action performed by DART on the grounded relations. The full list of all the grounded relations is available in our project web page11 .\nTable 6 shows the accuracy and the number of correct relations obtained for the input classes in Table 4. As done in Section 4.1, three ontology engineers were asked to evaluate the relations manually and a relation was considered correct only if all the three experts agreed that it is correct.\n4.2.1 Value of grounding. Following the grounding technique explained in Section 3.3, DART discarded or retained the grounded relations appropriately. It should be noted that if the discarded irrelevant relations (such as \u201cis the father of\" in the case of Religions and Countries) had been included in the output of DART, then the accuracy of DART would have decreased. Hence, the grounding phase improves the performance of DART. The grounding phase also suggests candidate equivalence, sub-property and inverse property axioms between the relations and existing LOD properties. These property axioms can further be validated through techniques that are based on determining the support from the instances [12] and then added to the T-Box. We intend to do the validation and enrichment process as a part of our future work. DART has not been compared with any of the property alignment systems (such as those surveyed in [14]) since the main goal of DART is to generate relations between two given classes only. DART suggests candidate property axioms which are yet to be supported by evidence from the A-Box. In that sense, DART can also be seen as a system which is capable of extracting new prospective inverse relations from text. For example, if one needs to find the inverse of the DBpedia property \u201cauthor\", the domain and range of \u201cauthor\", namely the classes Person and Book can be given as inputs to DART and DART would produce the relations both in the forward direction (the same direction as \u201cauthor\") as well as the reverse. If any of the relations in the reverse direction get grounded to the property \u201cauthor\" (i.e the relation\u2019s direction is opposite to that of \u201cauthor\" but its meaning is similar to \u201cauthor\"), then that relation is a prospective inverse property to the \u201cauthor\" property."}, {"heading": "5 CONCLUSIONS AND FUTUREWORK", "text": "The central idea behind this paper is to propose a completely automated and unsupervised technique to identify possible arbitrary relations between two classes of Linked Data. For this purpose, we have built a system, DART, whose working connects the techniques of contextual similarity checking and paraphrase detection into a unified framework for discovering new relations from the web patterns. DART then attempts to ground the discovered relations in the linked dataset in order to discard irrelevant relations and identify new relations. The fully automated grounding technique proposed in this paper also generates prospective property axioms for the enrichment of the linked dataset.\nThe results gathered reveal the potential of DART to unearth many interesting relations between a given pair of classes thus leading to the growth of a relationship-rich LOD. DART outperforms the state-of-the-art system with respect to the validity as well as the number of relations. As a part of our future work, we intend to validate the grounding phase to improve its accuracy and efficiency. We would also like to propose methods to validate the prospective property axioms generated through DART by means of gathering evidence from the generated relation instances."}], "references": [{"title": "Extending the Coverage of DBpedia Properties using Distant Supervision over Wikipedia", "author": ["Alessio Palmero Aprosio", "ClaudioGiuliano", "Alberto Lavelli"], "venue": "In NLP-DBPEDIA@ISWC (CEUR Workshop Proceedings),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Distant Supervision for Relation Extraction Using Ontology Class Hierarchy-Based Features", "author": ["PedroH.R. Assis", "MarcoA. Casanova"], "venue": "In The Semantic Web: ESWC", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet", "author": ["Satanjeev Banerjee", "Ted Pedersen"], "venue": "In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing (CICLing \u201902)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Never-ending ontology extension through machine reading", "author": ["P.H. Barchi", "E. Rafael Hruschka"], "venue": "In 2014 14th International Conference on Hybrid Intelligent Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Two different approaches to Ontology Extension Through Machine Reading", "author": ["P.H. Barchi", "E. Rafael Hruschka"], "venue": "Journal of Network and Innovative Computing", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Silk - A Link Discovery Framework for the Web of Data", "author": ["Christian Bizer", "Julius Volz", "Georgi Kobilarov", "Martin Gaedke"], "venue": "In 18th International World Wide Web Conference", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Toward an Architecture for Never-Ending Language Learning", "author": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R. Hruschka Jr.", "TomM.Mitchell"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "ClausIE: Clause-basedOpen Information Extraction", "author": ["Luciano Del Corro", "Rainer Gemulla"], "venue": "In Proceedings of the 22Nd International Conference on World Wide Web (WWW", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Open Information Extraction: The Second Generation", "author": ["Oren Etzioni", "Anthony Fader", "Janara Christensen", "Stephen Soderland", "Mausam Mausam"], "venue": "In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume One (IJCAI\u201911)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Towards Monitoring of Novel Statements in the News", "author": ["Michael F\u00e4rber", "Achim Rettinger", "Andreas Harth"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "The AI Behind Watson \u2013 The Technical Article", "author": ["David Ferrucci", "Eric Brown", "Jennifer Chu-Carroll", "James Fan", "David Gondek", "Aditya A. Kalyanpur", "Adam Lally", "J. William Murdock", "Eric Nyberg", "John Prager", "Nico Schlaefer", "Chris Welty"], "venue": "The AI Magazine", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Mining RDF Data for Property Axioms", "author": ["Daniel Fleischhacker", "Johanna V\u00f6lker", "Heiner Stuckenschmidt"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Information Retrieval: Data Structures and Algorithms", "author": ["William B. Frakes", "Ricardo Baeza-Yates"], "venue": "Rulersiv", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1992}, {"title": "Alignment and dataset identification of linked data in Semantic Web. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery", "author": ["Kalpa Gunaratna", "Sarasi Lalithsena", "Amit Sheth"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Moving Beyond SameAs with PLATO: Partonomy Detection for Linked Data", "author": ["Prateek Jain", "Pascal Hitzler", "Kunal Verma", "Peter Z. Yeh", "Amit P. Sheth"], "venue": "In Proceedings of the 23rd ACM Conference on Hypertext and Social Media (HT \u201912)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Large-Scale Learning of Relation-Extraction Rules with Distant Supervision from the Web", "author": ["Sebastian Krause", "Hong Li", "Hans Uszkoreit", "Feiyu Xu"], "venue": "In The Semantic Web - ISWC", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Large-Scale Learning of Relation-Extraction Rules with Distant Supervision from the Web", "author": ["Sebastian Krause", "Hong Li", "Hans Uszkoreit", "Feiyu Xu"], "venue": "In The Semantic Web - ISWC", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "DBpedia - A large-scale, multilingual knowledge base extracted from Wikipedia", "author": ["Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo N. Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick van Kleef", "S\u00c3\u0171ren Auer", "Christian Bizer"], "venue": "Semantic Web", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Annotating and Searching Web Tables Using Entities, Types and Relationships", "author": ["Girija Limaye", "Sunita Sarawagi", "Soumen Chakrabarti"], "venue": "Proc. VLDB Endow", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "YAGO3: A Knowledge Base from Multilingual Wikipedias", "author": ["Farzaneh Mahdisoltani", "Joanna Biega", "FabianM. Suchanek"], "venue": "CIDR", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Open Language Learning for Information Extraction", "author": ["Mausam", "Michael Schmitz", "Robert Bart", "Stephen Soderland", "Oren Etzioni"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Corpus-based and Knowledge-based Measures of Text Semantic Similarity", "author": ["Rada Mihalcea", "Courtney Corley", "Carlo Strapparava"], "venue": "In Proceedings of  the 21st National Conference on Artificial Intelligence", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems 26,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Distant Supervision for Relation Extraction Without Labeled Data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2 (ACL \u201909)", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Discovering Relations Between Noun Categories", "author": ["Thahir P. Mohamed", "Estevam R. Hruschka", "Jr.", "Tom M. Mitchell"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "TriplifyingWikipedia\u2019s Tables", "author": ["Emir Mu\u00f1oz", "Aidan Hogan", "Alessandra Mileo"], "venue": "In LD4IE@ISWC (CEUR Workshop Proceedings),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Using Linked Data to Mine RDF from Wikipedia\u2019s Tables", "author": ["Emir Mu\u00f1oz", "Aidan Hogan", "Alessandra Mileo"], "venue": "In Proceedings of the 7th ACM International Conference on Web Search and Data Mining (WSDM \u201914)", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "T2LD: Interpreting and Representing Tables as Linked Data", "author": ["Varish Mulwad", "Tim Finin", "Zareen Syed", "Anupam Joshi"], "venue": "In Proceedings of the ISWC 2010 Posters & Demonstrations Track: Collected Abstracts,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Using Linked Data to Interpret Tables", "author": ["VarishMulwad", "Tim Finin", "Zareen Syed", "Anupam Joshi"], "venue": "In Proceedings of the First International Workshop on Consuming Linked Data, Shanghai,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Mining Ontologies to Extract Implicit Knowledge", "author": ["Lucas Fonseca Navarro"], "venue": "Ph.D. Dissertation. Federal University of Sao Carlos", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "End-to-end Relation Extraction Using Distant Supervision from External Semantic Repositories. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2 (HLT \u201911)", "author": ["Truc-Vien T. Nguyen", "Alessandro Moschitti"], "venue": "Association for Computational Linguistics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Knowledge graph refinement: A survey of approaches and evaluation methods", "author": ["Heiko Paulheim"], "venue": "Semantic Web 8,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2017}, {"title": "Mining of Massive Datasets", "author": ["Anand Rajaraman", "Jeffrey David Ullman"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Matching HTML Tables to DBpedia. In Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics (WIMS \u201915)", "author": ["Dominique Ritze", "Oliver Lehmberg", "Christian Bizer"], "venue": "ACM, Article 10,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Yago: A Core of Semantic Knowledge", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of the 16th International Conference on World Wide Web (WWW", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "SOFIE: A Selforganizing Framework for Information Extraction", "author": ["Fabian M. Suchanek", "Mauro Sozio", "Gerhard Weikum"], "venue": "In Proceedings of the 18th International Conference on World Wide Web (WWW \u201909)", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Exploiting a Web of Semantic Data for Interpreting Tables", "author": ["Zareen Syed", "Tim Finin", "VarishMulwad", "Anupam Joshi"], "venue": "Proceedings of the Second Web Science Conference", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "DBpedia Ontology Enrichment for Inconsistency Detection", "author": ["Gerald T\u00f6pper", "Magnus Knuth", "Harald Sack"], "venue": "In Proceedings of the 8th International Conference on Semantic Systems (I-SEMANTICS \u201912)", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "For example, the British Broadcasting Corporation (BBC) uses linked datasets such as DBpedia [18] to enable cross-domain navigation and enhanced search2 in their websites.", "startOffset": 93, "endOffset": 97}, {"referenceID": 34, "context": "IBM has been using Linked Data as an integration technology for several years and their new cognitive system, Watson, has DBpedia and YAGO [36] as part of its major data sources [11].", "startOffset": 139, "endOffset": 143}, {"referenceID": 10, "context": "IBM has been using Linked Data as an integration technology for several years and their new cognitive system, Watson, has DBpedia and YAGO [36] as part of its major data sources [11].", "startOffset": 178, "endOffset": 182}, {"referenceID": 19, "context": "linked dataset YAGO has 488,469 classes [20].", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "Most of the Linked Data enrichment works (surveyed in [33]) focus on adding more instances to existing object properties (in this paper, the term \u2018relation\u2019 is used as a synonym of \u2018object property\u2019).", "startOffset": 54, "endOffset": 58}, {"referenceID": 24, "context": "The systems proposed in ([25], [4]) for the purpose of extending the NELL ontology, OntExt and newOntExt respectively, can be adapted to the Linked Data settings to discover new object properties between given LOD classes.", "startOffset": 25, "endOffset": 29}, {"referenceID": 3, "context": "The systems proposed in ([25], [4]) for the purpose of extending the NELL ontology, OntExt and newOntExt respectively, can be adapted to the Linked Data settings to discover new object properties between given LOD classes.", "startOffset": 31, "endOffset": 34}, {"referenceID": 29, "context": "For example, given the classes athletes and sportsleagues as inputs, newOntExt places the relations \u201cdoesn\u2019t play at\" (currently not playing) and \u201cwants to play at\" (wish to play) in the same cluster [31] because these two relations occur between the same subject-object pairs with a high frequency.", "startOffset": 200, "endOffset": 204}, {"referenceID": 31, "context": "the linked datasets for the purpose of the overall growth of the LOD cloud has been the major focus in many recent works (surveyed in [33]).", "startOffset": 134, "endOffset": 138}, {"referenceID": 31, "context": "Most of the relation enrichment approaches surveyed in [33] focus on extracting more instances (subject-object pairs) of existing relations in the linked datasets.", "startOffset": 55, "endOffset": 59}, {"referenceID": 26, "context": "Works such as ([27], [26]), [35], ([38], [29], [28] and [30]) and [19] use the technique of interpreting web tables for this purpose and a few other works such as [37] and [16] propose using various semi-supervised approaches for the same.", "startOffset": 15, "endOffset": 19}, {"referenceID": 25, "context": "Works such as ([27], [26]), [35], ([38], [29], [28] and [30]) and [19] use the technique of interpreting web tables for this purpose and a few other works such as [37] and [16] propose using various semi-supervised approaches for the same.", "startOffset": 21, "endOffset": 25}, {"referenceID": 33, "context": "Works such as ([27], [26]), [35], ([38], [29], [28] and [30]) and [19] use the technique of interpreting web tables for this purpose and a few other works such as [37] and [16] propose using various semi-supervised approaches for the same.", "startOffset": 28, "endOffset": 32}, {"referenceID": 36, "context": "Works such as ([27], [26]), [35], ([38], [29], [28] and [30]) and [19] use the technique of interpreting web tables for this purpose and a few other works such as [37] and [16] propose using various semi-supervised approaches for the same.", "startOffset": 35, "endOffset": 39}, {"referenceID": 27, "context": "Works such as ([27], [26]), [35], ([38], [29], [28] and [30]) and [19] use the technique of interpreting web tables for this purpose and a few other works such as [37] and [16] propose using various semi-supervised approaches for the same.", "startOffset": 41, "endOffset": 45}, {"referenceID": 28, "context": "Works such as ([27], [26]), [35], ([38], [29], [28] and [30]) and [19] use the technique of interpreting web tables for this purpose and a few other works such as [37] and [16] propose using various semi-supervised approaches for the same.", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "Works such as ([27], [26]), [35], ([38], [29], [28] and [30]) and [19] use the technique of interpreting web tables for this purpose and a few other works such as [37] and [16] propose using various semi-supervised approaches for the same.", "startOffset": 66, "endOffset": 70}, {"referenceID": 35, "context": "Works such as ([27], [26]), [35], ([38], [29], [28] and [30]) and [19] use the technique of interpreting web tables for this purpose and a few other works such as [37] and [16] propose using various semi-supervised approaches for the same.", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "Works such as ([27], [26]), [35], ([38], [29], [28] and [30]) and [19] use the technique of interpreting web tables for this purpose and a few other works such as [37] and [16] propose using various semi-supervised approaches for the same.", "startOffset": 172, "endOffset": 176}, {"referenceID": 16, "context": "Distant supervision is another new paradigm which has been recently adopted by many works ([17], [24], [1], [2], [32]) in order to extractmore instances of existing relations.", "startOffset": 91, "endOffset": 95}, {"referenceID": 23, "context": "Distant supervision is another new paradigm which has been recently adopted by many works ([17], [24], [1], [2], [32]) in order to extractmore instances of existing relations.", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "Distant supervision is another new paradigm which has been recently adopted by many works ([17], [24], [1], [2], [32]) in order to extractmore instances of existing relations.", "startOffset": 103, "endOffset": 106}, {"referenceID": 1, "context": "Distant supervision is another new paradigm which has been recently adopted by many works ([17], [24], [1], [2], [32]) in order to extractmore instances of existing relations.", "startOffset": 108, "endOffset": 111}, {"referenceID": 30, "context": "Distant supervision is another new paradigm which has been recently adopted by many works ([17], [24], [1], [2], [32]) in order to extractmore instances of existing relations.", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "A classifier is learnt based on this weakly labeled training data in order to classify unseen instances [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "The technique proposed in [15] to detect instances of \u201cpart-of\u201d (partonomy) relation between linked data instances falls under this category.", "startOffset": 26, "endOffset": 30}, {"referenceID": 5, "context": "Similarly, the SILK link discovery framework [6] which is primarily used to detect owl:sameAs links is also capable of detecting instances of user-specified relations.", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "It is to be noted that the system is not aware of the possible relations between the concerned classes before-hand and hence such relations are termed as arbitrary relations (as defined in [9]).", "startOffset": 189, "endOffset": 192}, {"referenceID": 24, "context": "There are two systems, OntExt [25] and newOntExt ([4], [5]) which have been proposed in the context of helping NELL to extend its ontology by means of discovering new relations between the ontology classes.", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "There are two systems, OntExt [25] and newOntExt ([4], [5]) which have been proposed in the context of helping NELL to extend its ontology by means of discovering new relations between the ontology classes.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "There are two systems, OntExt [25] and newOntExt ([4], [5]) which have been proposed in the context of helping NELL to extend its ontology by means of discovering new relations between the ontology classes.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "OntExt: Given two noun categories ([7] calls classes as noun categories), and their instances, OntExt discovers relations between them by exploiting the notion that similar patterns occur between the same subject-object pairs.", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "Finally, the top 50 subject-object pairs are given as seed instances of the new relation to NELL [7].", "startOffset": 97, "endOffset": 100}, {"referenceID": 24, "context": "As more than half of the relations generated by OntExt were invalid (determined manually in [25]), the authors of OntExt have proposed a classifier which can differentiate between valid and invalid relations to some extent.", "startOffset": 92, "endOffset": 96}, {"referenceID": 3, "context": "newOntExt: newOntExt which was developed based on OntExt had a few changes in its working [4]: instead of considering all the words in between the two input instances as a pattern, newOntExt used ReVerb [9] for extracting the patterns in order to reduce the number of noisy patterns obtained; for optimising the computational cost, a more elegant file structure was used for searching through the sentences; instead of considering every pair of cate-", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "newOntExt: newOntExt which was developed based on OntExt had a few changes in its working [4]: instead of considering all the words in between the two input instances as a pattern, newOntExt used ReVerb [9] for extracting the patterns in order to reduce the number of noisy patterns obtained; for optimising the computational cost, a more elegant file structure was used for searching through the sentences; instead of considering every pair of cate-", "startOffset": 203, "endOffset": 206}, {"referenceID": 7, "context": "However we can also replace this step in the following manner: use a web corpus such as ClueWeb and extract sentences containing instances of D1 and D2; then apply any triplification tool such as ClausIE [8], Ollie [21] etc to obtain the input triples corpus C.", "startOffset": 204, "endOffset": 207}, {"referenceID": 20, "context": "However we can also replace this step in the following manner: use a web corpus such as ClueWeb and extract sentences containing instances of D1 and D2; then apply any triplification tool such as ClausIE [8], Ollie [21] etc to obtain the input triples corpus C.", "startOffset": 215, "endOffset": 219}, {"referenceID": 22, "context": "We use the Word2Vec [23] model proposed and trained by Google 10 for finding the contextual similarity.", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "The filtered patterns are then subjected to single pass clustering [13].", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "[22]: We have eliminated the word specificity weights.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In [22], the individual word-to-word similarity valueswere weighted using a word specificity measure so that higher importance can be given to a semantic matching identified between two specific words such as \u201ccollie\" and \u201csheepdog\" when compared to a matching identified between words such as \u201cget\" and \u201cbecome\".", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "ing a low weight to such words (as done in [22]) is not appropriate in the context of DART.", "startOffset": 43, "endOffset": 47}, {"referenceID": 21, "context": "5 (adopted from [22]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 2, "context": "LESK [3] has been used to perform the word-to-word similarity component of equation (2), as it works for all combinations of parts of speech.", "startOffset": 5, "endOffset": 8}, {"referenceID": 37, "context": "system proposed in [39]).", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "Note that there are a few other works in the literature (such as [10]) which focus mainly on the grounding of relations in a Knowledge Base (KB).", "startOffset": 65, "endOffset": 69}, {"referenceID": 9, "context": "Moreover in systems such as [10], grounding of relations is based on the grounding of entities.", "startOffset": 28, "endOffset": 32}, {"referenceID": 9, "context": "Also, the method in [10] is semi-automatic i.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "We have used the implementation of newOntExt provided by the authors of [4]13.", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "Since the value of k used is not mentioned in ([4], [5]) and has been fixed in a dataset-specific manner in [25], we have applied the Elbow method [34] to determine the best k value (from a range of k=3 to k=29) for clustering the patterns, for each experiment.", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "Since the value of k used is not mentioned in ([4], [5]) and has been fixed in a dataset-specific manner in [25], we have applied the Elbow method [34] to determine the best k value (from a range of k=3 to k=29) for clustering the patterns, for each experiment.", "startOffset": 52, "endOffset": 55}, {"referenceID": 24, "context": "Since the value of k used is not mentioned in ([4], [5]) and has been fixed in a dataset-specific manner in [25], we have applied the Elbow method [34] to determine the best k value (from a range of k=3 to k=29) for clustering the patterns, for each experiment.", "startOffset": 108, "endOffset": 112}, {"referenceID": 32, "context": "Since the value of k used is not mentioned in ([4], [5]) and has been fixed in a dataset-specific manner in [25], we have applied the Elbow method [34] to determine the best k value (from a range of k=3 to k=29) for clustering the patterns, for each experiment.", "startOffset": 147, "endOffset": 151}, {"referenceID": 11, "context": "These property axioms can further be validated through techniques that are based on determining the support from the instances [12] and then added to the T-Box.", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "DART has not been compared with any of the property alignment systems (such as those surveyed in [14]) since the main goal of DART is to gener-", "startOffset": 97, "endOffset": 101}], "year": 2017, "abstractText": "Although several RDF knowledge bases are available through the LOD initiative, the ontology schema of such linked datasets is not very rich. In particular, they lack object properties. The problem of finding new object properties (and their instances) between any two given classes has not been investigated in detail in the context of Linked Data. In this paper, we present DART (Detecting ArbitraryRelations for enriching T-Boxes of Linked Data) an unsupervised solution to enrich the LOD cloud with new object properties between two given classes. DART exploits contextual similarity to identify text patterns from the web corpus that can potentially represent relations between individuals. These text patterns are then clustered by means of paraphrase detection to capture the object properties between the two given LOD classes. DART also performs fully automated mapping of the discovered relations to the properties in the linked dataset. This serves many purposes such as identification of completely new relations, elimination of irrelevant relations, and generation of prospective property axioms. We have empirically evaluated our approach on several pairs of classes and found that the system can indeed be used for enriching the linked datasets with new object properties and their instances. We compared DART with newOntExt system which is an offshoot of the NELL (Never-Ending Language Learning) effort. Our experiments reveal that DART gives better results than newOntExt with respect to both the correctness, as well as the number of relations.", "creator": "LaTeX with hyperref package"}}}