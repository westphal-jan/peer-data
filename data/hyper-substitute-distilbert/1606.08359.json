{"id": "1606.08359", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "Lifted Rule Injection for Relation Embeddings", "abstract": "entity based or representation learning may hold the state - a - than - art framework many data modelling extensions and knowledge base inference traditions. while, still major variation in controlling others ultimately construct relevance constraints like such data. hybrid meta approach regularizes relation and entity representations by broadly preserving logical - order logic rules. however, processing does not run beyond domains explaining specific few entities distinct behaviors. in this paper investigators present a highly efficient achievement in incorporating implication rules over distributed mapping toward automated knowledge base arithmetic. we map entity - tuple embeddings into an indirect boolean perspective and encourage a causal ordering over relation embeddings based on basic rules mined from wordnet. essentially, we find that constant relation constraint facing the entity - tuple embedding space substantially precisely hurt the expressiveness amongst the model and just acts as a regularizer necessarily prefers generalization. since incorporating few objective beliefs, we gain an increase of 2 percentage points mean intrinsic precision over a reasonable factorization baseline, offering conversely a negligible increase in runtime.", "histories": [["v1", "Mon, 27 Jun 2016 16:39:23 GMT  (292kb,D)", "http://arxiv.org/abs/1606.08359v1", null], ["v2", "Fri, 23 Sep 2016 20:40:25 GMT  (294kb,D)", "http://arxiv.org/abs/1606.08359v2", "Camera-ready version for EMNLP 2016 Conference"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL", "authors": ["thomas demeester", "tim rockt\u00e4schel", "sebastian riedel"], "accepted": true, "id": "1606.08359"}, "pdf": {"name": "1606.08359.pdf", "metadata": {"source": "CRF", "title": "Lifted Rule Injection for Relation Embeddings", "authors": ["Thomas Demeester", "Tim Rockt\u00e4schel", "Sebastian Riedel"], "emails": ["tdmeeste@intec.ugent.be", "t.rocktaschel@cs.ucl.ac.uk", "s.riedel@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Current successful methods for automated knowledge base construction tasks heavily rely on learned distributed vector representations (Nickel et al., 2012; Riedel et al., 2013; Socher et al., 2013; Chang et al., 2014; Neelakantan et al., 2015; Toutanova et al., 2015; Nickel et al., 2015; Verga et al., 2016;\nVerga and McCallum, 2016). Although these models are able to learn robust representations from large amounts of data, they often lack commonsense knowledge. Such knowledge is rarely explicitly stated in texts but can be found in resources like PPDB (Ganitkevitch et al., 2013) or WordNet (Miller, 1995).\nCombining neural methods with symbolic commonsense knowledge, for instance in the form of implication rules, is in the focus of current research (Rockta\u0308schel et al., 2014; Wang et al., 2014; Bowman et al., 2015; Wang et al., 2015; Vendrov et al., 2016; Hu et al., 2016; Rockta\u0308schel and Riedel, 2016; Cohen, 2016). A recent approach (Rockta\u0308schel et al., 2015) regularizes entity-tuple and relation embeddings via first-order logic rules. To this end, every first-order rule is propositionalized based on observed entity-tuples, and a differentiable loss term is added for every propositional rule. This approach does not scale beyond only a few entity-tuples and rules. For example, propositionalizing the rule \u2200x : isMan(x)\u21d2 isMortal(x) would result in a very large number of loss terms on a large database.\nIn this paper, we introduce a new method to impose such rules in a highly efficient way, entirely independent from the number of entities in the domain. This is achieved by minimizing an upper bound of the loss that encourages the implication between relations to hold. It only involves representations of the relations that are mentioned in rules, as well as a general rule-independent constraint on the entity-tuple embedding space. In the example given above, if we require that every component of the vector representation of isMan is lower than the\nar X\niv :1\n60 6.\n08 35\n9v 1\n[ cs\n.L G\n] 2\n7 Ju\nn 20\ncorresponding component of relation isMortal, then we can show that the rule holds for any nonnegative representation of an entity-tuple. Hence our method avoids the need for separate loss terms for every ground atom resulting from propositionalizing rules. In statistical relational learning this type of approach is often referred to as lifted inference or learning (Poole, 2003; Braz, 2007) because it deals with groups of random variables at a first-order level. In this sense our approach is a lifted form of rule injection. This allows for imposing large numbers of rules while learning distributed representations of relations and entity-tuples. Besides drastically lower computation time, an important advantage of our method over Rockta\u0308schel et al. (2015) is that when these constraints are satisfied, the injected rules always hold, even for unseen but inferred facts.\nOur contributions are fourfold: (i) we develop a very efficient way of regularizing relation representations to incorporate first-order logic implications (\u00a73), (ii) we reveal that, against expectation, mapping entity-tuple embeddings to non-negative space does not hurt but instead improves the generalization ability of our model (\u00a75.1) (iii) we show improvements on a knowledge base completion task by injecting mined commonsense rules from WordNet (\u00a75.3), and finally (iv) we give a qualitative analysis of the results, demonstrating that implication constraints are indeed satisfied in an asymmetric way and result in a substantially increased structuring of the relation embedding space (\u00a75.6)."}, {"heading": "2 Background", "text": "In this section we revisit the matrix factorization relation extraction model by Riedel et al. (2013) and introduce the notation used throughout the paper. We choose the matrix factorization model for its simplicity as the base on which we develop implication injection.\nRiedel et al. (2013) represent every relation r \u2208 R (selected from Freebase (Bollacker et al., 2008) or extracted as textual surface pattern) by a kdimensional latent representation r \u2208 Rk. A particular relation instance or fact is the combination of a relation r and a tuple t of entities that are engaged in that relation, and is written as \u3008r, t\u3009. We write O as the set of all such input facts available\nfor training. Furthermore, every entity-tuple t \u2208 T is represented by a latent vector t \u2208 Rk (with T the set of all entity-tuples in O).\nModel F by Riedel et al. (2013) measures the compatibility between a relation r and an entitytuple t using the dot product r>t of their respective vector representations. During training, the representations are learned such that valid facts receive high scores, whereas negative ones receive low scores. Typically no negative evidence is available at training time, and therefore a Bayesian Personalized Ranking (BPR) objective (Rendle et al., 2009) is used. Given a pair of facts fp := \u3008rp, tp\u3009 6\u2208 O and fq := \u3008rq, tq\u3009 \u2208 O, this objective requires that\nr>p tp \u2264 r>q tq. (1)\nThe embeddings can be trained by minimizing a convex loss function `R that penalizes violations of that requirement when iterating over the training set. In practice, each positive training fact \u3008r, tq\u3009 is compared with a randomly sampled unobserved fact \u3008r, tp\u3009 for the same relation. The overall loss can hence be written as\nLR = \u2211 \u3008r,tq\u3009\u2208O\ntp\u2208T , \u3008r,tp\u30096\u2208O\n`R ( r>[tp \u2212 tq] ) . (2)\nand measures how well observed valid facts are ranked above unobserved facts, thus reconstructing the ranking of the training data. We will henceforth call LR the reconstruction loss, to make a distinction with the implication loss that we will introduce later. Riedel et al. (2013) use the logistic loss `R(s) := \u2212 log \u03c3(\u2212s), where \u03c3(s) := (1 + e\u2212x)\u22121 denotes the sigmoid function. In order to avoid overfitting, an L2 regularization term on the r and t embeddings is added to the reconstruction loss. The overall objective to minimize hence is\nLF = LR + \u03b1 (\u2211 r\u2016r\u2016 2 2 + \u2211 t\u2016t\u2016 2 2 ) (3)\nwhere \u03b1 is the regularization strength."}, {"heading": "3 Lifted Injection of Implications", "text": "In this section, we show how an implication\n\u2200t \u2208 T : \u3008rp, t\u3009 \u21d2 \u3008rq, t\u3009, (4)\ncan be imposed independently of the entity-tuples. For simplicity, we abbreviate such implications as rp \u21d2 rq (e.g., professorAt\u21d2 employeeAt)."}, {"heading": "3.1 Grounded Loss Formulation", "text": "The implication rule can be imposed by requiring that every tuple t \u2208 T is at least as compatible with relation rp as with rq. Written in terms of the latent representations, eq. (4) therefore becomes\n\u2200t \u2208 T : r>p t \u2264 r>q t (5)\nIf \u3008rp, t\u3009 is a true fact with a high score r>p t, and the fact \u3008rq, t\u3009 has an even higher score, it must also be true, but not vice versa. We can therefore inject an implication rule by minimizing a loss term with a separate contribution from every t \u2208 T , adding up to the total loss if the corresponding inequality is not satisfied. In order to make the contribution of every tuple t to that loss independent of the magnitude of the tuple embedding, we divide both sides of the above inequality by \u2016t\u20161. With t\u0303 := t/\u2016t\u20161, the implication loss for the rule rp \u21d2 rq can be written as\nLI = \u2211 \u2200t\u2208T `I ( [rp \u2212 rq]>t\u0303 ) (6)\nfor an appropriate convex loss function `I , similarly to eq. (2). In practice, the summation can be reduced to those tuples that occur in combination with rp or rq in the training data. Still, the propositionalization in terms of training facts leads to a heavy computational cost for imposing a single implication, similar to the technique introduced in Rockta\u0308schel et al. (2015). Moreover, with that simplification there is no guarantee that the implication between both relations would generalize towards inferred facts not seen during training."}, {"heading": "3.2 Lifted Loss Formulation", "text": "The problems mentioned above can be avoided if instead of LI , a tuple-independent upper bound is minimized. Such a bound can be constructed, provided all components of t are restricted to a nonnegative embedding space, i.e., T \u2286 Rk,+. If this holds, Jensen\u2019s inequality allows us to transform eq. (6) as follows\nLI = \u2211 \u2200t\u2208T `I ( k\u2211 i=1 t\u0303i [rp \u2212 rq]>1i )\n(7)\n\u2264 k\u2211\ni=1\n`I ( [rp \u2212 rq]>1i ) \u2211 \u2200t\u2208T t\u0303i (8)\nwhere 1i is the unit vector along dimension i in tuple-space. This is allowed because the {t\u0303i}ki=1 form convex coefficients (t\u0303i > 0, and \u2211 i t\u0303i = 1), and `I is a convex function. If we define\nLUI := k\u2211\ni=1\n`I ( [rp \u2212 rq]>1i ) (9)\nwe can write LI \u2264 \u03b2LUI (10)\nin which \u03b2 is an upper bound on \u2211\nt t\u0303i. One such bound is |T |, but others are conceivable too. In practice we rescale \u03b2 to a hyper-parameter \u03b2\u0303 that we use to control the impact of the upper bound to the overall loss. We call LUI the lifted loss, as it no longer depends on any of the entity-tuples; it is grounded over the unit tuples 1i instead.\nThe implication rp \u21d2 rq can thus be imposed by minimizing the lifted loss LUI . Note that by minimizing LUI , the model is encouraged to satisfy the constraint rp \u2264 rq on the relation embeddings, where \u2264 denotes the component-wise comparison. In fact, a sufficient condition for eq. (5) to hold, is\nrp \u2264 rq and \u2200t \u2208 T : t \u2265 0 (11)\nwith 0 the k-dimensional null vector. This corresponds to a single relation-specific loss term, and the general restriction T \u2286 Rk,+ on the tupleembedding space."}, {"heading": "3.3 Approximately Boolean Entity Tuples", "text": "In order to impose implications by minimizing a lifted loss LUI , the tuple-embedding space needs to be restricted to Rk,+. We have chosen to restrict the tuple space even more than required, namely to the hypercube t \u2208 [0, 1]k, as approximately Boolean embeddings (Kruszewski et al., 2015). The tuple embeddings are constructed from real-valued vectors e, using the component-wise sigmoid function\nt = \u03c3(e), e \u2208 Rk. (12)\nFor minimizing the loss, the gradients are hence computed with respect to e, and the L2 regularization is applied to the components of e instead of t.\nOther choices for ensuring the restriction t \u2265 0 in eq. (11) are possible, but we found that our approach works better in practice than those (e.g., the\nexponential transformation proposed by Demeester et al. (2016)). It can also be observed that the unit tuples over which the implication loss is grounded, form a special case of approximately Boolean embeddings.\nIn order to investigate the impact of this restriction even when not injecting any rules, we introduce model FS: the original model F, but with sigmoidal entity-tuples:\nLFS = \u2211 \u3008r,tq\u3009\u2208O\ntp\u2208T , \u3008r,tp\u30096\u2208O\n`R ( r>[\u03c3(ep)\u2212 \u03c3(eq)] ) + \u03b1 (\u2211 r\u2016r\u2016 2 2 + \u2211 e\u2016e\u2016 2 2 ) (13)\nHere, ep and eq are the real-valued representations as in eq. (12), for tuples tp and tq, respectively.\nWith the above choice of a non-negative tupleembedding space we can now state the full lifted rule injection model (FSL):\nLFSL = LFS + \u03b2\u0303 \u2211 I\u2208I LUI (14)\nLUI denotes a lifted loss term for every rule in a set I of implication rules that we want to inject."}, {"heading": "3.4 Convex Implication Loss", "text": "The logistic loss `R (see \u00a72) is not suited for imposing implications because once the inequality in eq. (11) is satisfied, the components of rp and rq do not need to be separated any further. However, with `R this would continue to happen due to the small non-zero gradient. In the reconstruction loss LR this is a desirable effect which further separates the scores for positive from negative examples. However, if an implication is imposed between two relations that are almost equivalent according to the training data, we still want to find almost equivalent embedding vectors. Hence, we propose to use the loss\n`I(s) = max(0, s+ \u03b4) (15)\nwith \u03b4 a small positive margin to ensure that the gradient does not disappear before the inequality is actually satisfied. We use \u03b4 = 0.01 in all experiments.\nThe main advantage of the presented approach over earlier methods that impose the rules in a grounded way (Rockta\u0308schel et al., 2015; Wang et\nal., 2015) is the computational efficiency of imposing the lifted loss. Evaluating LUI or its gradient for one implication rule is comparable to evaluating the reconstruction loss for one pair of training facts. In typical applications there are much fewer rules than training facts and the extra computation time needed to inject these rules is therefore negligible."}, {"heading": "4 Related Work", "text": "Recent research on combining rules with learned vector representations has been important for new developments in the field of knowledge base completion. Wang et al. (2015) demonstrated how different types of rules can be incorporated using an Integer Linear Programming approach. Rockta\u0308schel et al. (2015) provided a framework to jointly maximize the probability of observed facts and first-order logic rules. These approaches ground the rules in the training data, limiting their scalability towards large rule sets and KBs with many entities. As argued in the introduction, this forms an important motivation for the current work.\nWu et al. (2015) proposed to use a path ranking approach for capturing long-range interactions between entities, and to add these as an extra loss term, besides the loss that models pairwise relations. Our model FSL differs substantially from their approach, in that we consider tuples instead of separate entities, and we inject a given set of rules. Yet, by creating a partial ordering in the relation embeddings as a result of injecting implication rules, model FSL can also capture interactions beyond direct relations. This will be demonstrated in \u00a75.3 by injecting rules between surface patterns only and still measuring an improvement on predictions for structured Freebase relations.\nCombining logic and distributed representations is also an active field of research outside of automated knowledge base completion. Recent advances include the work by Faruqui et al. (2014), who injected ontological knowledge from WordNet into word representations. Furthermore, Vendrov et al. (2016) proposed to enforce a partial ordering in an embeddings space of images and phrases. Our method is related to such order embeddings since we define a partial ordering on relation embeddings. However, to ensure that implications hold for all\nentity-tuples we also need a restriction on the entitytuple embedding space and derive bounds on the loss. Another important contribution is the recent work by Hu et al. (2016), who proposed a framework for injecting rules into general neural network architectures, by jointly training on the actual targets and on the rule-regularized predictions provided by a teacher network. Although quite different at first sight, their work could offer a way to use our model in various neural network architectures, by integrating the proposed lifted loss into the teacher network.\nThis paper builds upon our previous workshop paper (Demeester et al., 2016). In that work, we tested different tuple embedding transformations in an ad-hoc manner. We used approximately Boolean representations of relations instead of entity-tuples, strongly reducing the model\u2019s degrees of freedom. We now derive the FSL model from a carefully considered mathematical transformation of the grounded loss. The FSL model only restricts the tuple embedding space, whereby relation vectors remain real valued. Furthermore, previous experiments were performed on small-scale artificial datasets, whereas we now test on a real-world relation extraction benchmark."}, {"heading": "5 Experiments and Results", "text": "We now present our experimental results. We start by describing the experimental setup and hyperparameters. Before turning to the injection of rules, we compare model F with model FS, and show that restricting the tuple embedding space has a regularization effect, rather than limiting the expressiveness of the model (\u00a75.1). We then demonstrate that model FSL is capable of zero-shot learning (\u00a75.2), and show that injecting high-quality WordNet rules leads to an improved precision (\u00a75.3). We proceed with a visual illustration of the relation embeddings with and without injected rules (\u00a75.4), provide details on time efficiency of the lifted rule injection method (\u00a75.5), and show that it correctly captures the asymmetry of implication rules (\u00a75.6).\nAll models were implemented in TensorFlow (Abadi et al., 2015). We use the hyperparameters of Riedel et al. (2013), with k = 100 hidden dimensions and a weight of \u03b1 = 0.01 for the L2 regularization loss. We use ADAM (Kingma and\nBa, 2014) for optimization with an initial learning rate of 0.005 and a mini-batch size of 8192. The embeddings are initialized by sampling uniformly from [\u22120.1, 0.1] and we use \u03b2\u0303 = 0.1 for the implication loss throughout our experiments."}, {"heading": "5.1 Restricted Embedding Space", "text": "Before incorporating external commonsense knowledge into relation representations, we were curious how much we loose by restricting the entity-tuple space to approximately Boolean embeddings. We evaluate our models on the New York Times dataset introduced by Riedel et al. (2013). Surprisingly, we find that the expressiveness of the model does not suffer from this strong restriction. From Table 1 we see that restricting the tuple-embedding space seems to perform slightly better (FS) as opposed to a realvalued tuple-embedding space (F), suggesting that this restriction has a regularization effect that improves generalization. We also provide the original results for model F by Riedel et al. (2013) (denoted as R13-F) for comparison. Due to a different implementation and optimization procedure, the results for our model F and R13-F are not identical.\nInspecting the top relations for a sampled dimen-\nsion in the embedding space reveals that the relation space of model FS more closely resembles clusters than that of model F (Table 2). We hypothesize that this might be caused by approximately Boolean entity-tuple representations in model FS, resulting in attribute-like entity-tuple vectors that capture which relation clusters they belong to."}, {"heading": "5.2 Zero-shot Learning", "text": "The zero-shot learning experiment performed in Rockta\u0308schel et al. (2015) leads to an important finding: when injecting implications with right-hand sides for Freebase relations for which no or very limited training facts are available, the model should be able to infer the validity of Freebase facts for those relations based on rules and correlations between textual surface patterns.\nWe inject the same hand-picked relations as used by Rockta\u0308schel et al. (2015), after removing all Freebase training facts. The lifted rule injection (model FSL) reaches a weighted MAP of 0.35, comparable with 0.38 by the Joint model from Rockta\u0308schel et al. (2015) (denoted R15-Joint). Note that for this experiment we initialized the Freebase relations implied by the rules with negative random vectors (sampled uniformly from [\u22127.9,\u22128.1]). The reason is that without any negative training facts for these relations, their components can only go up due to the implication loss, and we do not want to get values that are too high before optimization.\nFigure 1 shows how the relation extraction performance improves when more Freebase relation training facts are added. It effictively measures how well the proposed models, matrix factorization (F), propositionalized rule injection (R15-Joint), and our model (FSL), can make use of the provided rules and correlations between textual surface form patterns and increased fractions of Freebase training facts. Although FSL starts at a lower performance\nthan R15-Joint when no Freebase training facts are present, it outperforms R15-Joint and a plain matrix factorization model by a substantial margin when provided with more than 7.5% of Freebase training facts. This indicates that, in addition to being much faster than R15-Joint, it can make better use of provided rules and few training facts. We attribute this to the Bayesian personalized ranking loss instead of the logistic loss used in Rockta\u0308schel et al. (2015). The former is compatible with our rule-injection method, but not with the approach of maximizing the expectation of propositional formulae used by R15-Joint."}, {"heading": "5.3 Injecting Knowledge from WordNet", "text": "The main purpose of this work is to be able to incorporate rules from external resources for aiding relation extraction. We use WordNet hypernyms to generate rules for the NYT dataset. To this end we iterate over all surface form patterns in the dataset and attempt to replace words in the pattern by their hypernyms. If the result-\ning pattern is contained in the dataset, we generate the corresponding rule. For instance, we generate a rule appos->diplomat->amod \u21d2 appos->official->amod since both patterns are contained in the NYT dataset and we know from WordNet that a diplomat is an official. This leads to 427 rules from WordNet that we subsequently annotate manually to obtain 36 high-quality rules. Note that none of these rules directly imply a Freebase relation. Although the test relations all originate from Freebase, we still hope to see improvements by transitive effects, i.e., better surface form representations that in turn help to predict Freebase facts.\nWe show results obtained by injecting these WordNet rules in Table 1 (column FSL). The weighted MAP measure increases by 2% with respect to model FS, and 4% compared to our reimplementation of the matrix factorization model F. This demonstrates that imposing a partial ordering based on implication rules can be used to incorporate logical commonsense knowledge and increase the quality of information extraction systems."}, {"heading": "5.4 Visualizing Relation Embeddings", "text": "We provide a visual inspection of how the structure of the relation embedding space changes when rules are imposed. We select all relations involved in the WordNet rules, and gather them as columns in a single matrix, sorted by increasing `1 norm (values in the 100 dimensions are similarly sorted). Figures 2a and 2b show the difference between model F (without injected rules) and FSL (with rules). The values of the embeddings in model FSL are more polarized, i.e., we observe stronger negative or positive components than for model F. Furthermore, FSL also reveals a clearer difference between the leftmost (mostly negative, more specific) and right-most (predominantly positive, more general) embeddings (i.e., a clearer separation between positive and negative values in the plot), which results from imposing the order relation in eq. (11) when injecting implications."}, {"heading": "5.5 Efficiency of Lifted Injection of Rules", "text": "In order to get an idea of the time efficiency of injecting rules, we measure the time per epoch when restricting the program execution to a single 2.4GHz CPU core. We measure on average 6.33s per epoch\nwithout rules (model FS), against 6.76s and 6.97s when injecting the 36 high-quality WordNet rules and the unfiltered 427 rules (model FSL), respectively. Increasing the amount of injected rules from 36 to 427 leads to an increase of only 3% in computation time, even though in our setup all rule losses are used in every training batch. This confirms the high efficiency of our lifted rule injection method."}, {"heading": "5.6 Asymmetric Character of Implications", "text": "In order to demonstrate that injecting implications conserves their asymmetric nature, we perform the following experiment. After incorporating highquality Wordnet rules rp \u21d2 rq into model FSL we select all of the tuples tp that occur with relation rp in a training fact \u3008rp, tp\u3009. Matching these with relation rq should result in high values for the scores r>q tp, if the implication holds. If however the tuples tq are selected from the training facts \u3008rq, tq\u3009, and matched with relation rp, the scores r>p tq should be much lower if the inverse implication does not hold (in other words, if rq and rp are not equivalent). Table 3 lists the averaged results for 5 example rules, and the average over all relations in WordNet rules, both for the case with injected rules (model FSL), and without rules (model FS). For easier comparison, the scores are mapped to the unit interval\nvia the sigmoid function. This quantity \u03c3(r>t) is often interpreted as the probability that the corresponding fact holds (Riedel et al., 2013), but because of the BPR-based training, only differences between scores play a role here. After injecting rules, the average scores of facts inferred by these rules (i.e., column \u03c3(r>q tp) for model FSL) are always higher than for facts (incorrectly) inferred by the inverse rules (column \u03c3(r>p tq) for model FSL). In the fourth example, the inverse rule leads to high scores as well (on average 0.79, vs. 0.98 for the actual rule). This is due to the fact that the daily and newspaper relations are more or less equivalent, such that the components of rp are not much below those of rq. For the last example (the ambassador \u21d2 diplomat rule), the asymmetry in the implication is maintained, although the absolute scores are rather low for these two relations.\nThe results for model FS reflect how strongly the implications in either direction are latently present in the training data. We can only conclude that model FS manages to capture the similarity between relations, but not the asymmetric character of implications. For example, purely based on the training data, it appears to be more likely that the parent relation implies the father relation, than vice versa. This again demonstrates the importance and added value of injecting external rules capturing commonsense knowledge."}, {"heading": "6 Conclusions", "text": "We presented a novel, fast approach for incorporating first-order implication rules into distributed representations of relations. We termed our approach \u2018lifted rule injection\u2019, as it avoids the costly grounding of first-order implication rules and is thus independent of the size of the domain of entities. By\nconstruction, these rules are satisfied for any observed or unobserved fact. The presented approach requires a restriction on the entity-tuple embedding space. However, experiments on a real-world dataset show that this does not impair the expressiveness of the learned representations. On the contrary, it appears to have a beneficial regularization effect.\nBy incorporating rules generated from WordNet hypernyms, our model improved over a matrix factorization baseline for knowledge base completion. Especially for domains where annotation is costly and only small amounts of training facts are available, our approach provides a way to leverage external knowledge sources for inferring facts.\nIn future work, we want to extend the proposed ideas beyond implications towards general firstorder logic rules. We believe that supporting conjunctions, disjunctions and negations would enable to debug and improve representation learning based knowledge base completion. Furthermore, we want to integrate these ideas into neural methods beyond matrix factorization approaches."}], "references": [{"title": "Tim Sturge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh"], "venue": "and Jamie Taylor.", "citeRegEx": "Bollacker et al.2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Christopher Potts", "author": ["Samuel R Bowman"], "venue": "and Christopher D Manning.", "citeRegEx": "Bowman et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Lifted Firstorder Probabilistic Inference", "author": ["Rodrigo De Salvo Braz"], "venue": "Ph.D. thesis,", "citeRegEx": "Braz.,? \\Q2007\\E", "shortCiteRegEx": "Braz.", "year": 2007}, {"title": "Bishan Yang", "author": ["Kai-Wei Chang", "Wen-tau Yih"], "venue": "and Christopher Meek.", "citeRegEx": "Chang et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "W", "author": ["William"], "venue": "Cohen.", "citeRegEx": "Cohen2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Tim Rockt\u00e4schel", "author": ["Thomas Demeester"], "venue": "and Sebastian Riedel.", "citeRegEx": "Demeester et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Eduard Hovy", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K Jauhar", "Chris Dyer"], "venue": "and Noah A Smith.", "citeRegEx": "Faruqui et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Benjamin Van Durme", "author": ["Juri Ganitkevitch"], "venue": "and Chris Callison-Burch.", "citeRegEx": "Ganitkevitch et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Eduard Hovy", "author": ["Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu"], "venue": "and Eric Xing.", "citeRegEx": "Hu et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Denis Paperno", "author": ["German Kruszewski"], "venue": "and Marco Baroni.", "citeRegEx": "Kruszewski et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Benjamin Roth", "author": ["Arvind Neelakantan"], "venue": "and Andrew McCallum.", "citeRegEx": "Neelakantan et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Volker Tresp", "author": ["Maximilian Nickel"], "venue": "and Hans-Peter Kriegel.", "citeRegEx": "Nickel et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Volker Tresp", "author": ["Maximilian Nickel", "Kevin Murphy"], "venue": "and Evgeniy Gabrilovich.", "citeRegEx": "Nickel et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "First-order probabilistic inference", "author": ["David Poole"], "venue": "In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Poole.,? \\Q2003\\E", "shortCiteRegEx": "Poole.", "year": 2003}, {"title": "Zeno Gantner", "author": ["Steffen Rendle", "Christoph Freudenthaler"], "venue": "and Lars Schmidt-Thieme.", "citeRegEx": "Rendle et al.2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Andrew McCallum", "author": ["Sebastian Riedel", "Limin Yao"], "venue": "and Benjamin M Marlin.", "citeRegEx": "Riedel et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning knowledge base inference with neural theorem provers", "author": ["Rockt\u00e4schel", "Riedel2016] Tim Rockt\u00e4schel", "Sebastian Riedel"], "venue": "In NAACL Workshop on Automated Knowledge Base Construction (AKBC)", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "Sameer Singh", "author": ["Tim Rockt\u00e4schel", "Matko Bosnjak"], "venue": "and Sebastian Riedel.", "citeRegEx": "Rockt\u00e4schel et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Sameer Singh", "author": ["Tim Rockt\u00e4schel"], "venue": "and Sebastian Riedel.", "citeRegEx": "Rockt\u00e4schel et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Christopher D Manning", "author": ["Richard Socher", "Danqi Chen"], "venue": "and Andrew Ng.", "citeRegEx": "Socher et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Pallavi Choudhury", "author": ["Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon"], "venue": "and Michael Gamon.", "citeRegEx": "Toutanova et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sanja Fidler", "author": ["Ivan Vendrov", "Ryan Kiros"], "venue": "and Raquel Urtasun.", "citeRegEx": "Vendrov et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Row-less universal schema", "author": ["Verga", "McCallum2016] Patrick Verga", "Andrew McCallum"], "venue": null, "citeRegEx": "Verga et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Verga et al\\.", "year": 2016}, {"title": "Benjamin Roth", "author": ["Patrick Verga", "David Belanger", "Emma Strubell"], "venue": "and Andrew McCallum.", "citeRegEx": "Verga et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Kathryn Mazaitis", "author": ["William Yang Wang"], "venue": "and William W Cohen.", "citeRegEx": "Wang et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Bin Wang", "author": ["Quan Wang"], "venue": "and Li Guo.", "citeRegEx": "Wang et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Zhongfei Zhang", "author": ["Fei Wu", "Jun Song", "Yi Yang", "Xi Li"], "venue": "and Yueting Zhuang.", "citeRegEx": "Wu et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.", "creator": "LaTeX with hyperref package"}}}