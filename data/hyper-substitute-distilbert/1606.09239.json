{"id": "1606.09239", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2016", "title": "Learning Concept Taxonomies from Multi-modal Data", "abstract": "mathematicians study complicated problem of aggressively building computational languages from textual and visual references. empirical considerations in taxonomy too effectively ignore the increasingly inconsistent metadata data, although create important perceptual semantics. instead, we propose balanced probabilistic model performing taxonomy induction by jointly leveraging vocabulary and images. two combine bias - crafted coding engineering, we conduct end - to - end vocabulary based online distributed groups of images displaying words. how model is being trained exploring a huge cast of organizational theories using skills worthy of building full taxonomies from scratch for specialized vocabulary reflecting unseen conceptual vocabulary items documenting associated images. specialists tailor our model and features examining the wordnet hierarchies, where expert system outperforms previous approaches fills a large gap.", "histories": [["v1", "Wed, 29 Jun 2016 19:52:53 GMT  (630kb,D)", "http://arxiv.org/abs/1606.09239v1", "To appear in ACL 2016"]], "COMMENTS": "To appear in ACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.LG", "authors": ["hao zhang", "zhiting hu", "yuntian deng", "mrinmaya sachan", "zhicheng yan", "eric p xing"], "accepted": true, "id": "1606.09239"}, "pdf": {"name": "1606.09239.pdf", "metadata": {"source": "CRF", "title": "Learning Concept Taxonomies from Multi-modal Data", "authors": ["Hao Zhang", "Zhiting Hu", "Yuntian Deng", "Mrinmaya Sachan", "Zhicheng Yan", "Eric P. Xing"], "emails": ["hao@cs.cmu.edu", "zhitingh@cs.cmu.edu", "yuntiand@cs.cmu.edu", "mrinmays@cs.cmu.edu", "epxing@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Human knowledge is naturally organized as semantic hierarchies. For example, in WordNet (Miller, 1995), specific concepts are categorized and assigned to more general ones, leading to a semantic hierarchical structure (a.k.a taxonomy). A variety of NLP tasks, such as question answering (Harabagiu et al., 2003), document clustering (Hotho et al., 2002) and text generation (Biran and McKeown, 2013) can benefit from the conceptual relationship present in these hierarchies.\nTraditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowledge or time intensive, and the results have limited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both\nNLP and computer vision. On one hand, a number of methods have been developed to build hierarchies based on lexical patterns in text (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2015). These works generally ignore the rich visual data which encode important perceptual semantics (Bruni et al., 2014) and have proven to be complementary to linguistic information and helpful for many tasks (Silberer and Lapata, 2014; Kiela and Bottou, 2014; Zhang et al., 2015; Chen et al., 2013). On the other hand, researchers have built visual hierarchies by utilizing only visual features (Griffin and Perona, 2008; Yan et al., 2015; Sivic et al., 2008). The resulting hierarchies are limited in interpretability and usability for knowledge transfer.\nHence, we propose to combine both visual and textual knowledge to automatically build taxonomies. We induce is-a taxonomies by supervised learning from existing entity ontologies where each concept category (entity) is associated with images, either from existing dataset (e.g. ImageNet (Deng et al., 2009)) or retrieved from the web using search engines, as illustrated in Fig 1. Such a scenario is realistic and can be extended to a variety of tasks; for example, in knowledge base\nar X\niv :1\n60 6.\n09 23\n9v 1\n[ cs\n.C L\n] 2\n9 Ju\nn 20\n16\nconstruction (Chen et al., 2013), text and image collections are readily available but label relations among categories are to be uncovered. In largescale object recognition, automatically learning relations between labels can be quite useful (Deng et al., 2014; Zhao et al., 2011).\nBoth textual and visual information provide important cues for taxonomy induction. Fig 1 illustrates this via an example. The parent category seafish and its two child categories shark and ray are closely related as: (1) there is a hypernym-hyponym (is-a) relation between the words \u201cseafish\u201d and \u201cshark\u201d/\u201cray\u201d through text descriptions like \u201c...seafish, such as shark and ray...\u201d, \u201c...shark and ray are a group of seafish...\u201d; (2) images of the close neighbors, e.g., shark and ray are usually visually similar and images of the child, e.g. shark/ray are similar to a subset of images of seafish. To effectively capture these patterns, in contrast to previous works that rely on various hand-crafted features (Chen et al., 2013; Bansal et al., 2014), we extract features by leveraging the distributed representations that embed images (Simonyan and Zisserman, 2014) and words (Mikolov et al., 2013) as compact vectors, based on which the semantic closeness is directly measured in vector space. Further, we develop a probabilistic framework that integrates the rich multi-modal features to induce \u201cis-a\u201d relations between categories, encouraging local semantic consistency that each category should be visually and textually close to its parent and siblings.\nIn summary, this paper has the following contributions: (1) We propose a novel probabilistic Bayesian model (Section 3) for taxonomy induction by jointly leveraging textual and visual data. The model is discriminatively trained and can be directly applied to build a taxonomy from scratch for a collection of semantic labels. (2) We design novel features (Section 4) based on generalpurpose distributed representations of text and images to capture both textual and visual relations between labels. (3) We evaluate our model and features on the ImageNet hierarchies with two different taxonomy induction tasks (Section 5). We achieve superior performance on both tasks and improve the F1 score by 2x in the taxonomy construction task, compared to previous approaches. Extensive comparisons demonstrate the effectiveness of integrating visual features with language features for taxonomy induction. We also provide\nqualitative analysis on our features, the learned model, and the taxonomies induced to provide further insights (Section 5.3)."}, {"heading": "2 Related Work", "text": "Many approaches have been recently developed that build hierarchies purely by identifying either lexical patterns or statistical features in text corpora (Yang and Callan, 2009; Snow et al., 2006; Kozareva and Hovy, 2010; Navigli et al., 2011; Zhu et al., 2013; Fu et al., 2014; Bansal et al., 2014; Tuan et al., 2014; Tuan et al., 2015; Kiela et al., 2015). The approaches in Yang and Callan (2009) and Snow et al. (2006) assume a starting incomplete hierarchy and try to extend it by inserting new terms. Kozareva and Hovy (2010) and Navigli et al. (2011) first find leaf nodes and then use lexical patterns to find intermediate terms and all the attested hypernymy links between them. In (Tuan et al., 2014), syntactic contextual similarity is exploited to construct the taxonomy, while Tuan et al. (2015) go one step further to consider trustiness and collective synonym/contrastive evidence. Different from them, our model is discriminatively trained with multi-modal data. The works of Fu et al. (2014) and Bansal et al. (2014) use similar language-based features as ours. Specifically, in (Fu et al., 2014), linguistic regularities between pretrained word vectors (Mikolov et al., 2013) are modeled as projection mappings. The trained projection matrix is then used to induce pairwise hypernym-hyponym relations between words. Our features are partially motivated by Fu et al. (2014), but we jointly leverage both textual and visual information. In Kiela et al. (2015), both textual and visual evidences are exploited to detect pairwise lexical entailments. Our work is significantly different as our model is optimized over the whole taxonomy space rather than considering only word pairs separately. In (Bansal et al., 2014), a structural learning model is developed to induce a globally optimal hierarchy. Compared with this work, we exploit much richer features from both text and images, and leverage distributed representations instead of hand-crafted features.\nSeveral approaches (Griffin and Perona, 2008; Bart et al., 2008; Marsza\u0142ek and Schmid, 2008) have also been proposed to construct visual hierarchies from image collections. In (Bart et al., 2008), a nonparametric Bayesian model is developed to group images based on low-level features.\nIn (Griffin and Perona, 2008) and (Marsza\u0142ek and Schmid, 2008), a visual taxonomy is built to accelerate image categorization. In (Chen et al., 2013), only binary object-object relations are extracted using co-detection matrices. Our work differs from all of these as we integrate textual with visual information to construct taxonomies.\nAlso of note are several works that integrate text and images as evidence for knowledge base autocompletion (Bordes et al., 2011) and zeroshot recognition (Gan et al., 2015; Gan et al., ; Socher et al., 2013). Our work is different because our task is to accurately construct multilevel hyponym-hypernym hierarchies from a set of (seen or unseen) categories."}, {"heading": "3 Taxonomy Induction Model", "text": "Our model is motivated by the key observation that in a semantically meaningful taxonomy, a category tends to be closely related to its children as well as its siblings. For instance, there exists a hypernym-hyponym relation between the name of category shark and that of its parent seafish. Besides, images of shark tend to be visually similar to those of ray, both of which are seafishes. Our model is thus designed to encourage such local semantic consistency; and by jointly considering all categories in the inference, a globally optimal structure is achieved. A key advantage of the model is that we incorporate both visual and textual features induced from distributed representations of images and text (Section 4). These features capture the rich underlying semantics and facilitate taxonomy induction. We further distinguish the relative importance of visual and textual features that could vary in different layers of a taxonomy. Intuitively, visual features would be increasingly indicative in the deeper layers, as sub-categories under the same category of specific objects tend to be visually similar. In contrast, textual features would be more important when inducing hierarchical relations between the categories of general concepts (i.e. in the near-root layers) where visual characteristics are not necessarily similar."}, {"heading": "3.1 The Problem", "text": "Assume a set of N categories x = {x1, x2, . . . , xN}, where each category xn consists of a text term tn as its name, as well as a set of images in = {i1, i2, . . . }. Our goal\nis to construct a taxonomy tree T over these categories1, such that categories of specific object types (e.g. shark) are grouped and assigned to those of general concepts (e.g. seafish). As the categories in x may be from multiple disjoint taxonomy trees, we add a pseudo category x0 as the hyper-root so that the optimal taxonomy is ensured to be a single tree. Let zn \u2208 {1, . . . , N} be the index of the parent of category xn, i.e. xzn is the hypernymic category of xn. Thus the problem of inducing a taxonomy structure is equivalent to inferring the conditional distribution p(z|x) over the set of (latent) indices z = {z1, . . . , zn}, based on the images and text."}, {"heading": "3.2 Model", "text": "We formulate the distribution p(z|x) through a model which leverages rich multi-modal features. Specifically, let cn be the set of child nodes of category xn in a taxonomy encoded by z. Our model is defined as pw(z,\u03c0|x,\u03b1) \u221d p(\u03c0|\u03b1)\nN\u220f n=1 \u220f xn\u2032\u2208cn \u03c0ngw(xn, xn\u2032 , cn\\xn\u2032)\n(1)where gw(xn, xn\u2032 , cn\\xn\u2032), defined as\ngw(xn, xn\u2032 , cn\\xn\u2032) = exp{w>d(xn\u2032 )fn,n\u2032,cn\\xn\u2032 },\nmeasures the semantic consistency between category xn\u2032 , its parent xn as well as its siblings indexed by cn\\xn\u2032 . The function gw(\u00b7) is loglinear with respect to fn,n\u2032,cn\\xn\u2032 , which is the feature vector defined over the set of relevant categories (xn, xn\u2032 , cn\\xn\u2032), with cn\\xn\u2032 being the set of child categories excluding xn\u2032 (Section 4). The simple exponential formulation can effectively encourage close relations among nearby categories in the induced taxonomy. The function has combination weights w = {w1, . . . ,wL}, where L is the maximum depth of the taxonomy, to capture the importance of different features, and the function d(xn\u2032) to return the depth of xn\u2032 in the current taxonomy. Each layer l (1 \u2264 l \u2264 L) of the taxonomy has a specific wl thereby allowing varying weights of the same features in different layers. The parameters are learned in a supervised manner. In eq 1, we also introduce a weight \u03c0n for each node xn, in order to capture the varying popularity of different categories (in terms of being a parent category). For example, some categories like\n1We assume T to be a tree. Most existing taxonomies are modeled as trees (Bansal et al., 2014), since a tree helps simplify the construction and ensures that the learned taxonomy is interpretable. With minor modifications, our model also works on non-tree structures.\nplant can have a large number of sub-categories, while others such as stone have less. We model \u03c0 as a multinomial distribution with Dirichlet prior \u03b1 = (\u03b11, . . . , \u03b1N ) to encode any prior knowledge of the category popularity2; and the conjugacy allows us to marginalize out \u03c0 analytically to get\npw(z|x,\u03b1) \u221d \u222b p(\u03c0|\u03b1) N\u220f n=1 \u220f xn\u2032\u2208cn \u03c0ngw(xn, xn\u2032 , cn\\xn\u2032)d\u03c0\n\u221d \u220f n \u0393(qn + \u03b1n) \u220f\nxn\u2032\u2208cn\ngw(xn, xn\u2032 , cn\\xn\u2032)\n(2) where qn is the number of children of category xn.\nNext, we describe our approach to infer the expectation for each zn, and based on that select a particular taxonomy structure for the category nodes x. As z is constrained to be a tree (i.e. cycle without loops), we include with eq 2, an indicator factor 1(z) that takes 1 if z corresponds a tree and 0 otherwise. We modify the inference algorithm appropriately to incorporate this constraint. Inference. Exact inference is computationally intractable due to the normalization constant of eq 2. We therefore use Gibbs Sampling, a procedure for approximate inference. Here we present the sampling formula for each zn directly, and defer the details to the supplementary material. The sampling procedure is highly efficient because the normalization term and the factors that are irrelevant to zn are cancelled out. The formula is\np(zn =m|z\\zn, \u00b7) \u221d 1(zn = m,z\\zn) \u00b7 ( q\u2212nm + \u03b1m ) \u00b7\u220f\nxn\u2032\u2208cm\u222a{xn} gw(xm, xn\u2032 , cm \u222a {xn})\u220f\nxn\u2032\u2208cm\\xn gw(xm, xn\u2032 , cm\\xn)\n,\n(3)\nwhere qm is the number of children of category m; the superscript\u2212n denotes the number excluding xn. Examining the validity of the taxonomy structure (i.e. the tree indicator) in each sampling step can be computationally prohibitive. To handle this, we restrict the candidate value of zn in eq 3, ensuring that the new zn is always a tree. Specifically, given a tree T , we define a structure operation as the procedure of detaching one node xn in T from its parent and appending it to another node xm which is not a descendant of xn.\nProposition 1. (1) Applying a structure operation on a tree T will result in a structure that is still a tree. (2) Any tree structure over the node set x that has the same root node with tree T can be achieved by applying structure operation on T a finite number of times.\n2\u03b1 could be estimated using training data.\nThe proof is straightforward and we omit it due to space limitations. We also add a pseudo node x0 as the fixed root of the taxonomy. Hence by initializing a tree-structured state rooted at x0 and restricting each updating step as a structure operation, our sampling procedure is able to explore the whole valid tree space. Output taxonomy selection. To apply the model to discover the underlying taxonomy from a given set of categories, we first obtain the marginals of z by averaging over the samples generated through eq 3, then output the optimal taxonomy z\u2217 by finding the maximum spanning tree (MST) using the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Bansal et al., 2014). Training. We need to learn the model parameters wl of each layer l, which capture the relative importance of different features. The model is trained using the EM algorithm. Let `(xn) be the depth (layer) of category xn; and z\u0303 (siblings c\u0303n) denote the gold structure in training data. Our training algorithm updates w through maximum likelihood estimation, wherein the gradient of wl is (see the supplementary materials for details): \u03b4wl = \u2211\nn:`(xn)=l\n{f(xz\u0303n , xn, c\u0303n\\xn)\u2212Ep[f(xzn , xn, cn\\xn)]} ,\nwhich is the net difference between gold feature vectors and expected feature vectors as per the model. The expectation is approximated by collecting samples using the sampler described above and averaging them."}, {"heading": "4 Features", "text": "In this section, we describe the feature vector f used in our model, and defer more details in the supplementary material. Compared to previous taxonomy induction works which rely purely on linguistic information, we exploit both perceptual and textual features to capture the rich spectrum of semantics encoded in images and text. Moreover, we leverage the distributed representations of images and words to construct compact and effective features. Specifically, each image i is represented as an embedding vector vi \u2208 Ra extracted by deep convolutional neural networks. Such image representation has been successfully applied in various vision tasks. On the other hand, the category name t is represented by its word embedding vt \u2208 Rb, a low-dimensional dense vector induced by the Skip-gram model (Mikolov et\nal., 2013) which is widely used in diverse NLP applications too. Then we design f(xn, xn\u2032 , cn\\xn\u2032) based on the above image and text representations. The feature vector f is used to measure the local semantic consistency between category xn\u2032 and its parent category xn as well as its siblings cn\\xn\u2032 ."}, {"heading": "4.1 Image Features", "text": "Sibling similarity. As mentioned above, close neighbors in a taxonomy tend to be visually similar, indicating that the embedding of images of sibling categories should be close to each other in the vector space Ra. For a category xn and its image set in, we fit a Gaussian distribution N (vin ,\u03a3n) to the image vectors, where vin \u2208 Ra is the mean vector and \u03a3n \u2208 Ra\u00d7a is the covariance matrix. For a sibling category xm of xn, we define the visual similarity between xn and xm as\nvissim(xn, xm)=[N (vim ;vin ,\u03a3n)+N (vin ;vim ,\u03a3m)]/2\nwhich is the average probability of the mean image vector of one category under the Gaussian distribution of the other. This takes into account not only the distance between the mean images, but also the closeness of the images of each category. Accordingly, we compute the visual similarity between xn\u2032 and the set cn\\xn\u2032 by averaging:\nvissim(xn\u2032 , cn\\xn\u2032) = \u2211 xm\u2208cn\\xn\u2032 vissim(xn\u2032 , xm)\n|cn| \u2212 1 .\nWe then bin the values of vissim(xn\u2032 , cn\\xn\u2032) and represent it as an one-hot vector, which constitutes f as a component named as siblings imageimage relation feature (denoted as S-V13). Parent prediction. Similar to feature S-V1, we also create the similarity feature between the image vectors of the parent and child, to measure their visual similarity. However, the parent node is usually a more general concept than the child, and it usually consists of images that are not necessarily similar to its child. Intuitively, by narrowing the set of images to those that are most similar to its child improves the feature. Therefore, different from S-V1, when estimating the Gaussian distribution of the parent node, we only use the top K images with highest probabilities under the Gaussian distribution of the child node. We empirically show in section 5.3 that choosing an appropriate K consistently boosts the performance. We name this feature as parent-child image-image relation feature (denoted as PC-V1).\n3S: sibling, PC: parent-child, V: visual, T: textual.\nFurther, inspired by the linguistic regularities of word embedding, i.e. the hypernym-hyponym relationship between words can be approximated by a linear projection operator between word vectors (Mikolov et al., 2013; Fu et al., 2014), we design a similar strategy to (Fu et al., 2014) between images and words so that the parent can be \u201cpredicted\u201d given the image embedding of its child category and the projection matrix. Specifically, let (xn, xn\u2032) be a parent-child pair in the training data, we learn a projection matrix \u03a6 which minimizes the distance between \u03a6vin\u2032 (i.e. the projected mean image vector vin\u2032 of the child) and vtn (i.e. the word embedding of the parent):\n\u03a6\u2217 = argmin \u03a6\n1 N \u2211 n \u2016\u03a6vin\u2032 \u2212 vtn\u2016 2 2 + \u03bb\u2016\u03a6\u20161,\nwhere N is the number of parent-child pairs in the training data. Once the projection matrix has been learned, the similarity between a child node xn\u2032 and its parent xn is computed as \u2016\u03a6vin\u2032 \u2212 vtn\u2016, and we also create an one-hot vector by binning the feature value. We call this feature as parentchild image-word relation feature (PC-V2)."}, {"heading": "4.2 Word Features", "text": "We briefly introduce the text features employed. More details about the text feature extraction could be found in the supplementary material. Word embedding features.d PC-V1, We induce features using word vectors to measure both sibling-sibling and parent-child closeness in text domain (Fu et al., 2014). One exception is that, as each category has only one word, the sibling similarity is computed as the cosine distance between two word vectors (instead of mean vectors). This will produce another two parts of features, parentchild word-word relation feature (PC-T1) and siblings word-word relation feature (S-T1). Word surface features. In addition to the embedding-based features, we further leverage lexical features based on the surface forms of child/parent category names. Specifically, we employ the Capitalization, Ends with, Contains, Suffix match, LCS and Length different features, which are commonly used in previous works in taxonomy induction (Yang and Callan, 2009; Bansal et al., 2014)."}, {"heading": "5 Experiments", "text": "We first disclose our implementation details in section 5.1 and the supplementary material for bet-\nter reproducibility. We then compare our model with previous state-of-the-art methods (Fu et al., 2014; Bansal et al., 2014) with two taxonomy induction tasks. Finally, we provide analysis on the weights and taxonomies induced."}, {"heading": "5.1 Implementation Details", "text": "Dataset. We conduct our experiments on the ImageNet2011 dataset (Deng et al., 2009), which provides a large collection of category items (synsets), with associated images and a label hierarchy (sampled from WordNet) over them. The original ImageNet taxonomy is preprocessed, resulting in a tree structure with 28231 nodes. Word embedding training. We train word embedding for synsets by replacing each word/phrase in a synset with a unique token and then using Google\u2019s word2vec tool (Mikolov et al., 2013). We combine three public available corpora together, including the latest Wikipedia dump (Wikipedia, 2014), the One Billion Word Language Modeling Benchmark (Chelba et al., 2013) and the UMBC webbase corpus (Han et al., 2013), resulting in a corpus with total 6 billion tokens. The dimension of the embedding is set to 200. Image processing. we employ the ILSVRC12 pre-trained convolutional neural networks (Simonyan and Zisserman, 2014) to embed each image into the vector space. Then, for each category xn with images, we estimate a multivariate Gaussian parameterized by Nxn = (\u00b5xn ,\u03a3xn), and constrain \u03a3xn to be diagonal to prevent overfitting. For categories with very few images, we only estimate a mean vector \u00b5xn . For nodes that do not have images, we ignore the visual feature. Training configuration. The feature vector is a concatenation of 6 parts, as detailed in section 4. All pairwise distances are precomputed and stored in memory to accelerate Gibbs sampling. The initial learning rate for gradient descent in the M step is set to 0.1, and is decreased by a fraction of 10 every 100 EM iterations."}, {"heading": "5.2 Evaluation", "text": ""}, {"heading": "5.2.1 Experimental Settings", "text": "We evaluate our model on three subtrees sampled from the ImageNet taxonomy. To collect the subtrees, we start from a given root (e.g. consumer goods) and traverse the full taxonomy using BFS, and collect all descendant nodes within a depth h (number of nodes in the longest path). We vary h\nto get a series of subtrees with increasing heights h \u2208 {4, 5, 6, 7} and various scales (maximally 1326 nodes) in different domains. The statistics of the evaluation sets are provided in Table 1. To avoid ambiguity, all nodes used in ILSVRC 2012 are removed as the CNN feature extractor is trained on them.\nWe design two different tasks to evaluate our model. (1) In the hierarchy completion task, we randomly remove some nodes from a tree and use the remaining hierarchy for training. In the test phase, we infer the parent of each removed node and compare it with groundtruth. This task is designed to figure out whether our model can successfully induce hierarchical relations after learning from within-domain parent-child pairs. (2) Different from the previous one, the hierarchy construction task is designed to test the generalization ability of our model, i.e. whether our model can learn statistical patterns from one hierarchy and transfer the knowledge to build a taxonomy for another collection of out-of-domain labels. Specifically, we select two trees as the training set to learn w. In the test phase, the model is required to build the full taxonomy from scratch for the third tree.\nWe use Ancestor F1 as our evaluation metric (Kozareva and Hovy, 2010; Navigli et al., 2011; Bansal et al., 2014). Specifically, we measure F1 = 2PR/(P +R) values of predicted \u201cis-a\u201d relations where the precision (P) and recall (R) are:\nP = |isapredicted \u2229 isagold| |isapredicted| , R = |isapredicted \u2229 isagold| |isagold| .\nWe compare our method to two previously state-of-the-art models by Fu et al. (2014) and Bansal et al. (2014), which are closest to ours."}, {"heading": "5.2.2 Results", "text": "Hierarchy completion. In the hierarchy completion task, we split each tree into 70% nodes for training and 30% for test, and experiment with different h. We compare the following three systems: (1) Fu20144 (Fu et al., 2014); (2) Ours (L): Our model with only language features enabled (i.e. surface features, parent-child word-word relation feature and siblings word-word relation feature); (3) Ours (LV): Our model with both language features and visual features 5. The average performance on three trees are reported at Table 2. We observe that the performance gradually drops when h increases, as more nodes are inserted when the tree grows higher, leading to a more complex and difficult taxonomy to be accurately constructed. Overall, our model outperforms Fu2014 in terms of the F1 score, even without visual features. In the most difficult case with h = 7, our model still holds an F1 score of 0.42 (2\u00d7 of Fu2014), demonstrating the superiority of our model. Hierarchy construction. The hierarchy construction task is much more difficult than hierarchy completion task because we need to build a taxonomy from scratch given only a hyper-root. For this task, we use a leave-one-out strategy, i.e. we train our model on every two trees and test on the third, and report the average performance in Table 2. We compare the following methods: (1) Fu2014, (2) Ours (L), and (3) Ours (LV), as described above; (4) Bansal2014: The model by Bansal et al. (2014)\n4We tried different parameter settings for the number of clusters C and the identification threshold \u03b4, and reported the best performance we achieved.\n5In the comparisons to (Fu et al., 2014) and (Bansal et al., 2014), we simply set K = \u221e, i.e. we use all available images of the parent category to estimate the PC-V1 feature.\nretrained using our dataset; (5) Ours (LB): By excluding visual features, but including other language features from Bansal et al. (2014); (6) Ours (LVB): Our full model further enhanced with all semantic features from Bansal et al. (2014); (7) Ours (LVB - E): By excluding word embeddingbased language features from Ours (LVB).\nAs shown, on the hierarchy construction task, our model with only language features still outperforms Fu2014 with a large gap (0.30 compared to 0.18 when h = 7), which uses similar embeddingbased features. The potential reasons are two-fold. First, we take into account not only parent-child relations but also siblings. Second, their method is designed to induce only pairwise relations. To build the full taxonomy, they first identify all possible pairwise relations using a simple thresholding strategy and then eliminate conflicted relations to obtain a legitimate tree hierarchy. In contrast, our model is optimized over the full space of all legitimate taxonomies by taking the structure operation in account during Gibbs sampling.\nWhen comparing to Bansal2014, our model with only word embedding-based features underperforms theirs. However, when introducing visual features, our performance is comparable (pvalue = 0.058).Furthermore, if we discard visual features but add semantic features from Bansal et al. (2014), we achieve a slight improvement of 0.02 over Bansal2014 (p-value = 0.016), which is largely attributed to the incorporation of word embedding-based features that encode high-level linguistic regularity. Finally, if we enhance our full model with all semantic features from Bansal et al. (2014), our model outperforms theirs by a gap of 0.04 (p-value < 0.01), which justifies our intuition that perceptual semantics underneath visual contents are quite helpful."}, {"heading": "5.3 Qualitative Analysis", "text": "In this section, we conduct qualitative studies to investigate how and when the visual information helps the taxonomy induction task. Contributions of visual features. To evaluate the contribution of each part of the visual features to the final performance, we train our model jointly with textual features and different combinations of visual features, and report the ancestorF1 scores. As shown in Table 3. When incorporating the feature S-V1, the performance is substantially boosted by a large gap at all heights, show-\ning that visual similarity between sibling nodes is a strong evidence for taxonomy induction. It is intuitively plausible, as it is highly likely that two specific categories share a common (and more general) parent category if similar visual contents are observed between them. Further, adding the PC-V1 feature gains us a better improvement than adding PC-V2, but both minor than S-V1.\nCompared to that of siblings, the visual similarity between parents and children does not strongly holds all the time. For example, images of Terrestrial animal are only partially similar to those of Feline, because the former one contains the later one as a subset. Our feature captures this type of \u201ccontain\u201d relation between parents and children by considering only the top-K images from the parent category that have highest probabilities under the Gaussian distribution of the child category. To see this, we vary K while keep all other settings, and plot the F1 scores in Fig 2. We observe a trend that when we gradually increase K, the performance goes up until reaching some maximal; It then slightly drops (or oscillates) even when more images are available, which confirms with our feature design that only top images should be considered in parent-child visual similarity.\nOverall, the three visual features complement each other, and achieve the highest performance when combined. Visual representations. To investigate how the image representations affect the final performance, we compare the ancestor-F1 score when different pre-trained CNNs are used for visual feature extraction. Specifically, we employ both the CNN-128 model (128 dimensional feature with 15.6% top-5 error on ILSVRC12) and the VGG16 model (4096 dimensional feature with 7.5% top-5 error) by Simonyan and Zisserman (2014), but only observe a slight improvement of 0.01 on the ancestor-F1 score for the later one. Relevance of textual and visual features v.s. depth of tree. Compared to Bansal et al. (2014),\na major difference of our model is that different layers of the taxonomy correspond to different weightswl, while in (Bansal et al., 2014) all layers share the same weights. Intuitively, introducing layer-wisew not only extends the model capacity, but also differentiates the importance of each feature at different layers. For example, the images of two specific categories, such as shark and ray, are very likely to be visually similar. However, when the taxonomy goes from bottom to up (specific to general), the visual similarity is gradually undermined \u2014 images of fish and terrestrial animal are not necessarily similar any more. Hence, it is necessary to privatize the weightsw for different layers to capture such variations, i.e. the visual features become more and more evident from shallow to deep layers, while the textual counterparts, which capture more abstract concepts, relatively grow more indicative oppositely from specific to general.\nTo visualize the variations across layers, for each feature component, we fetch its correspond-\nmillipede\ninvertebrate critter\nanimal\ncaterpillar\ndomestic animal\nstarfish\nchordate\narrowwormarthropod nematode\ntrichinaplanarian polyp\nechinodermannelid\nworm\ntussock\ncaterpillar\ntent\ncaterpillar\ncephalochordate scavenger\nlarvaceansagitta\nstocker\nlancelet archiannelid\nlarva\nfoodstuff\nmeal, repast\nfood, nutrient\nnutriment\nliquid\ndiet\ndietary\ningredientflour grain\nbeef stew cows\u2019 milk\njuice waterboiled\negg\nbarley spring\nwater\nstew\nfish stew\ndiary\nproduct\nwheatsoybean\nmeal\nwheat flour\nhard-boiled\negg\nbrunch breakfast\nwater\ndrinking\nwater\njuice\nFigure 4: Excerpts of the prediction taxonomies, compared to the groundturth. Edges marked as red and green are false predictions and unpredicted groundtruth links, respectively.\ning block in w as V . Then, we average |V | and observe how its values change with the layer depth h. For example, for the parent-child word-word relation feature, we first fetch its corresponding weights V from w as a 20 \u00d7 6 matrix, where 20 is the feature dimension and 6 is the number of layers. We then average its absolute values6 in column and get a vector v with length 6. After `2 normalization, the magnitude of each entry in v directly reflects the relative importance of the feature as an evidence for taxonomy induction. Fig 3(b) plots how their magnitudes change with h for every feature component averaged on three train/test splits. It is noticeable that for both wordword relations (S-T1, PC-T1), their corresponding weights slightly decrease as h increases. On the contrary, the image-image relation features (S-V1, PC-V1) grows relatively more prominent. The results verify our conjecture that when the category hierarchy goes deeper into more specific classes, the visual similarity becomes relatively more indicative as an evidence for taxonomy induction.\nVisualizing results. Finally, we visualize some excerpts of our predicted taxonomies, as compared to the groundtruth in Fig 4.\n6We take the absolute value because we only care about the relevance of the feature as an evidence for taxonomy induction, but note that the weight can either encourage (positive) or discourage (negative) connections of two nodes."}, {"heading": "6 Conclusion", "text": "In this paper, we study the problem of automatically inducing semantically meaningful concept taxonomies from multi-modal data. We propose a probabilistic Bayesian model which leverages distributed representations for images and words. We compare our model and features to previous ones on two different tasks using the ImageNet hierarchies, and demonstrate superior performance of our model, and the effectiveness of exploiting visual contents for taxonomy induction. We further conduct qualitative studies and distinguish the relative importance of visual and textual features in constructing various parts of a taxonomy."}, {"heading": "Acknowledgements", "text": "We would like to thank anonymous reviewers for their valuable feedback. We would also like to thank Mohit Bansal for helpful suggestions. We thank NVIDIA for GPU donations. The work is supported by NSF Big Data IIS1447676."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We study the problem of automatically building hypernym taxonomies from textual and visual data. Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics. Instead, we propose a probabilistic model for taxonomy induction by jointly leveraging text and images. To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representations of images and words. The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images. We evaluate our model and features on the WordNet hierarchies, where our system outperforms previous approaches by a large gap.", "creator": "TeX"}}}