{"id": "1607.05002", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jul-2016", "title": "Geometric Mean Metric Learning", "abstract": "\u2022 revisit common task of learning a structured metric system information. we approach descriptive term from first principles \u2026 model it as \u201c surprisingly simple optimization function. indeed, our system even admits a closed form solution. nonlinear analysis illustrates several very notable options : ( to ) an innate robust approximation through the integral geometry of positive definite entries ; ( ii ) properties of interpretability ; and ( iii ) additive function scale orders de magnitude faster than the widely used lmnn * rd methods. specifically, involving standard analytical datasets, our closed - variety solution necessarily ensures unprecedented classification accuracy.", "histories": [["v1", "Mon, 18 Jul 2016 10:14:46 GMT  (38kb)", "http://arxiv.org/abs/1607.05002v1", "7 pages, 4 figures"]], "COMMENTS": "7 pages, 4 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["pourya zadeh", "reshad hosseini", "suvrit sra"], "accepted": true, "id": "1607.05002"}, "pdf": {"name": "1607.05002.pdf", "metadata": {"source": "CRF", "title": "Geometric Mean Metric Learning", "authors": ["Suvrit Sra"], "emails": ["reshad.hosseini}@ut.ac.ir", "suvrit@mit.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 7.\n05 00\n2v 1\n[ st\nat .M\nL ]\n1 8\nJu l 2\n01 6\nI. INTRODUCTION\nMany machine learning algorithms require computing distances between input data points, be it for clustering, classification, or search. Selecting the distance measure is, therefore, an important concern; though the answer is task specific. When supervised or weakly supervised information is available, selection of the distance function can itself be cast as a learning problem called \u201cmetric learning\u201d [1, 2].\nIn its most common form, metric learning seeks to learn a Euclidean metric. An abstract approach is to take input data in Rn and learn a linear map \u03a6 : Rn \u2192 Rm, so that the Euclidean distance \u2016\u03a6(x) \u2212 \u03a6(y)\u2016 can be used to measure the distance between points x,y \u2208 Rn. More generally, the map \u03a6 can also be nonlinear.\nThe problem of learning linear maps was introduced in [3] as \u201cMahalanobis metric learning.\u201d Since then metric learning has witnessed a sequence of improvements both in modeling and algorithms (see related work). More broadly, the idea of linearly transforming input features is a bigger theme across machine learning and statistics; encompassing whitening transforms, linear dimensionality reduction, Euclidean metric learning, and more [1, 4].\nWe revisit the task of learning a Euclidean metric. Like most Euclidean metric learning methods, we also seek to learn a Mahalanobis distance1\ndA(x,x \u2032) = (x\u2212 x\u2032)TA(x\u2212 x\u2032), (1)\nwhere x,x\u2032 \u2208 Rd are input vectors, and A is a d \u00d7 d real, symmetric positive definite (SPD) matrix2. Like other metric learning approaches we also assume weak-supervision, which\nPourya Habib Zadeh and Reshad Hosseini are with the School of ECE, College of Engineering, University of Tehran. E-mail: {p.habibzadeh, reshad.hosseini}@ut.ac.ir\nSuvrit Sra is with the Massachusetts Institute of Technology. E-mail: suvrit@mit.edu\n1This is actually a squared distance. The true metric is \u221a dA; but in accord\nwith metric learning literature we call (1) a distance. 2Do not confuse SPD with positive semi-definite matrices.\nis provided through the sets of pairs\nS := {(xi,xj) | xi and xj are in the same class} D := {(xi,xj) | xi and xj are in different classes}.\nUnlike other Euclidean metric learning methods, however, we follow a much simpler yet fresh new approach.\nSpecifically, we make the following main contributions:\n\u2013 Formulation. We formulate Euclidean metric learning from first principles following intuitive geometric reasoning; we name our setup \u201cGeometric Mean Metric Learning\u201d (GMML) and cast it as an unconstrained smooth, strictly convex optimization problem.\n\u2013 Solution & insights. We show that our formulation admits a closed form solution, which not only also enjoys connections to the Riemannian geometry of SPD matrices (and thus explains the name GMML) but also has important empirical consequences.\n\u2013 Validation. We consider multi-class classification using the learned metrics, and validate GMML by comparing it against widely used metric learning methods. GMML runs up to three orders of magnitude faster while consistently delivering equal or higher classification accuracy."}, {"heading": "A. Related work", "text": "We recall below some related work to help place GMML in perspective. We omit a discussion of nonlinear methods, and other variations of the basic Euclidean task outlined above; for these, we refer the reader to both kernelized metric learning [5] and other techniques as summarized in the recent surveys of Kulis [1] and Bellet et al. [6].\nProbably the earliest work to formulate metric learning is [3], sometimes referred to as MMC. This method minimizes the sum of distances over similar points while trying to ensure that dissimilar points are far away from each other. Using the sets S and D, MMC solves the optimization problem\nmin A 0\n\u2211\n(xi,xj)\u2208S\ndA(xi,xj)\nsuch that \u2211\n(xi,xj)\u2208D\n\u221a dA(xi,xj) \u2265 1. (2)\nXing et al. [3] use \u221a dA instead of the distance dA because under dA, problem (2) has a trivial rank-one solution. To optimize (2), they use a gradient-descent algorithm combined with a projection onto the set of positive semi-definite matrices. The term \u2211\n(xi,xj)\u2208S dA(xi,xj) is also used in the other metric\nlearning methods like LMNN [2] and MCML [7] as a part of their cost functions.\n2 Information-Theoretic Metric Learning (ITML) [5], aims to satisfy the similarity and dissimilarity constraints while staying as \u201cclose\u201d as possible to a predefined matrix. This closeness is measured using the LogDet divergence Dld(A,A0) := tr(AA\u221210 ) \u2212 log det(AA\u221210 ) \u2212 d; and ITML is formulated as follows:\nmin A 0 Dld(A,A0)\nsuch that dA(x,y) \u2264 u, (x,y) \u2208 S, dA(x,y) \u2265 l, (x,y) \u2208 D,\n(3)\nwhere u, v \u2208 R are threshold parameters, chosen to encourage distance between similar points to be small and between dissimilar points be large. Similar to ITML, Meyer et al. [8] propose the formulation\nmin A 0\n\u2211\n(xi,xj)\u2208S\nmax ( 0 , l \u2212 dA(xi,xj) )2\n+ \u2211\n(xi,xj)\u2208D\nmax ( 0 , dA(xi,xj)\u2212 u )2 ,\n(4)\nfor which they use Riemannian techniques to minimize the cost function. Although (4) does not use any regularizer, the authors observed good classification performance.\nThere exist several attempts for achieving high scalability with both the dimensionality and the number of constraints in the metric learning methods; some examples include [9, 10, 11, 12].\nHowever, the focus of our paper is different: we are concerned with the formulation of Euclidean metric learning. Remarkably, our new formulation admits a closed form solution, which turns out to be 3 orders of magnitude faster than established competing methods."}, {"heading": "II. GMML: FORMULATION AND SOLUTION", "text": "As discussed above, the guiding idea behind Euclidean metric learning is to ultimately obtain a metric that yields \u201csmall\u201d distances for similar points and \u201cbig\u201d ones for dissimilar ones. Different metric learning methods try to fulfill this guideline either implicitly or explicitly.\nThe main idea that we introduce below is in how we choose to include the impact of the dissimilar points. Like one of earliest metric learning methods MMC, we propose to find a matrix A that decreases the sum of distances over all the similar points, but unlike all previous methods, instead of treating dissimilar points asymmetrically, we propose to measure their interpoint distances using A\u22121, and to add their contribution to the overall objective. More precisely, we propose the following novel objective function:\n\u2211\n(xi,xj)\u2208S\ndA(xi,xj) + \u2211\n(xi,xj)\u2208D\ndA\u22121(xi,xj). (5)\nIn the sequel, we write d\u0302A \u2261 dA\u22121 for brevity.\nA. Insights\nLet us provide some intuition behind our proposed objective (5). These insights are motivated by the idea that we may increase the Mahalanobis distance between dissimilar\npoints dA(x,y) by decreasing d\u0302A(x,y). The first idea is the simple observation that the distance dA(x,y) increases monotonically in A, whereas the distance d\u0302A(x,y) decreases monotonically in A. This observation follows from the following well-known result:\nProposition 1. Let A,B be (strictly) positive definite matrices such that A \u227b B. Then, A\u22121 \u227a B\u22121.\nThe second idea (which essentially reaffirms the first) is that the gradients of dA and d\u0302A point in nearly opposite directions. Therefore, infinitesimally decreasing dA leads to an increase in d\u0302A. Indeed, the (Euclidean) gradient of dA(x,y) is\n\u2202dA \u2202A = uuT ,\nwhere u = x \u2212 y; this is a rank-one positive semi-definite matrix. The gradient of d\u0302A(x,y) is\n\u2202d\u0302A \u2202A = \u2212A\u22121uuTA\u22121,\nwhich is a rank-one matrix with a negative eigenvalue. It is easy to see that the inner product of these two gradients is negative, as desired."}, {"heading": "B. Optimization problem and its solution", "text": "In the following, we further simplify the objective in (5). Rewriting the Mahalanobis distance using traces, we turn (5) into the optimization problem\nmin A\u227b0\n\u2211\n(xi,xj)\u2208S\ntr(A(xi \u2212 xj)(xi \u2212 xj)T )\n+ \u2211\n(xi,xj)\u2208D\ntr(A\u22121(xi \u2212 xj)(xi \u2212 xj)T ). (6)\nWe define now the following two important matrices:\nS := \u2211\n(xi,xj)\u2208S\n(xi \u2212 xj)(xi \u2212 xj)T ,\nD := \u2211\n(xi,xj)\u2208D\n(xi \u2212 xj)(xi \u2212 xj)T , (7)\nwhich denote the similarity and dissimilarity matrices, respectively. The matrices S and D are scaled second sample moments of the differences between similar points and the differences between dissimilar points. In the rest of this subsection, we assume that S is a SPD matrix, which is a realistic assumption in many situations. For the cases where S is just a positive semi-definite matrix, the regularized version can be used; we treat this case in Section II-C.\nUsing (7), the minimization problem (6) yields the basic optimization formulation of GMML, namely\nmin A\u227b0\nh(A) := tr(AS) + tr(A\u22121D). (8)\nThe GMML cost function (8) has several remarkable properties, which may not be apparent at first sight. Below we highlight some of these to help build greater intuition, as well as to help us minimize it.\nThe first key property of h(A) is that it is both strictly convex and strictly geodesically convex. Therefore, if \u2207h(A) = 0\n3 has a solution, that solution will be the global minimizer. Before proving this key property of h, let us recall some material that is also helpful for the remainder of the section.\nGeodesic convexity is the generalization of ordinary (linear) convexity to (nonlinear) manifolds and metric spaces [13, 14]. On Riemannian manifolds, geodesics are curves with zero acceleration that at the same time locally minimize the Riemannian distance between two points. The set of SPD matrices forms a Riemannian manifold of nonpositive curvature [15, Ch. 6]. We denote this manifold by S+. The geodesic curve joining A to B on the SPD manifold is denoted by\nA\u266ftB = A 1/2\n( A\u22121/2BA\u22121/2 )t A1/2, t \u2208 [0, 1].\nThis notation for geodesic is customary, and in the literature, \u03b3(t) is also used. Moreover, the entire set of SPD matrices is geodesically convex, as there is a geodesic between every two points in the set. On this set, one defines geodesically convex functions as follows.\nDefinition 2. A function f on a geodesically convex subset of a Riemannian manifold is geodesically convex, if for all points A and B in this set, it satisfies\nf(A\u266ftB) \u2264 tf(A) + (1\u2212 t)f(B), t \u2208 [0, 1].\nIf for t \u2208 (0, 1) the above inequality is strict, the function is called strictly geodesically convex.\nWe refer the reader to [16] for more on geodesic convexity for SPD matrices. We are ready to state a simple but key convexity result.\nTheorem 3. The cost function h in (8) is both strictly convex and strictly geodesically convex on the SPD manifold.\nProof: The first term in (8) is linear, hence convex, while the second term is strictly convex [17, Ch. 3], viewing SPD matrices as a convex cone [see 18, Thm. 2.6]. Thus, strict convexity of h(A) is obvious. Therefore, we concentrate on proving its strict geodesic convexity. Using continuity, it suffices to show midpoint strict convexity, namely\nh(A\u266f1/2B) < 1 2h(A) + 1 2h(B).\nIt is well-known [15, Ch. 4] that for two distinct SPD matrices, we have the operator inequality\nA\u266f1/2B \u227a 12A+ 12B. (9)\nSince S is SPD, is immediately follows that\ntr ( (A\u266f1/2B)S) < 1 2 tr(AS) + 1 2 tr(BS). (10)\nFrom the definition of \u266ft, a brief manipulation shows that\n(A\u266ftB) \u22121 = A\u22121\u266ftB \u22121.\nThus, in particular for the midpoint (with t = 1/2) we have\ntr ( (A\u266f1/2B) \u22121D) < 12 tr(A \u22121D) + 12 tr(B \u22121D). (11)\nAdding (10) and (11), we obtained the desired result.\nSolution via geometric mean. The optimal solution to (8) will reveal one more reason why we invoke geodesic convexity. Since the constraint set of (8) is open and the objective is strictly convex, to find its global minimum, it is enough to find a point where the gradient \u2207h vanishes. Differentiating with respect to A, this yields\n\u2207h(A) = S \u2212A\u22121DA\u22121.\nSetting this gradient to zero results in the equation\n\u2207h(A) = 0 =\u21d2 ASA = D. (12)\nEquation (12) is a Riccati equation whose unique solution is nothing but the midpoint of the geodesic joining S\u22121 to D (see e.g., Bhatia [15, 1.2.13]). Indeed,\nA = S\u22121\u266f1/2 D = S \u22121/2(S1/2DS1/2)1/2S\u22121/2.\nObserve by construction this solution is SPD, therefore, the constraint of optimization is satisfied.\nIt is this fact that the solution to GMML is given by the midpoint of the geodesic joining the inverse of the second moment matrix of similar points to the second moment matrix of dissimilar points, which gives GMML its name: the midpoint of this geodesic is known as the matrix geometric mean and is a very important object in the study of SPD matrices [15, Ch. 6]."}, {"heading": "C. Regularized version", "text": "We have seen that the solution of our method is the geometric mean between S\u22121 and D. However, in practice the matrix S might sometimes be non-invertible or near-singular. To address this concern, we propose to add a regularizing term to the objective function. This regularizer term can also be used to incorporate prior knowledge about the distance function. In particular, we propose to use\nmin A\u227b0\n\u03bbDsld(A,A0) + tr(AS) + tr(A \u22121D), (13)\nwhere A0 is the \u201cprior\u201d (SPD matrix) and Dsld(A,A0) is the symmetrized LogDet divergence: Dld(A,A0) + Dld(A0,A), which is equal to\nDsld(A,A0) := tr(AA \u22121 0 ) + tr(A \u22121A0)\u2212 2d, (14)\nwhere d is the dimensionality of the data. Interestingly, using (14) and following the argument as above, we see that the minimization problem in (13) with this regularizer also has a closed form solution. After straightforward computations, we obtain the following solution\nAreg = (S + \u03bbA \u22121 0 ) \u22121\u266f1/2 (D + \u03bbA0), (15)\nthe regularized geometric mean of suitably modified S and D matrices. Observe that as the regularization parameter \u03bb \u2265 0 increases, Areg becomes more similar to A0.\n4 S+ \u03b3(t) S\u22121 D +A\nFig. 1. The solution of GMML is located in the geodesic between matrices S\u22121 and D on the manifold of SPD matrices."}, {"heading": "D. Extension to weighted geometric mean", "text": "The geodesic viewpoint is also key to deciding how one may assign different \u201cweights\u201d to the matrices S and D when computing the GMML solution. This viewpoint is important because merely scaling the cost in (8) to change the balance between S and D is not meaningful as it only scales the resulting solution A by a constant.\nGiven the geometric nature of the GMML\u2019s solution, we replace the linear cost in (8) by a nonlinear one guided by Riemannian geometry of the SPD manifold. The key insight into obtaining a weighted version of GMML comes from a crucial geometric observation. The minimum of (8) is also the minimum to the following optimization problem:\nmin A\u227b0\n\u03b42R(A,S \u22121) + \u03b42R(A,D), (16)\nwhere \u03b4R denotes the Riemannian distance\n\u03b4R(X,Y ) := \u2016log(Y \u22121/2XY \u22121/2)\u2016F for X,Y \u227b 0, on SPD matrices and \u2016\u00b7\u2016F denotes the Frobenius norm.\nOnce we identify the solution of (8) with that of (16), the generalization to the weighted case becomes transparent. We introduce a parameter that characterizes the degree of balance between the cost terms of similarity and dissimilarity data. The weighted GMML formulation is then\nmin A\u227b0\nht(A) := (1\u2212 t) \u03b42R(A,S\u22121) + t \u03b42R(A,D), (17)\nwhere t is a parameter that determines the balance. Unlike (8), which we observed to be strictly convex as well as strictly geodesically convex, problem (17) is not (Euclidean) convex. Fortunately, it is still geodesically convex, because \u03b4R itself is geodesically convex. The proof of the geodesic convexity of \u03b4R is more involved than that of Theorem 3, and we refer the reader to [15, Ch. 6] for complete details.\nIt can be shown, see e.g., [15, Ch. 6] that the unique solution to (17) is the weighted geometric mean\nA = S\u22121\u266ft D, (18)\nthat is, a point on the geodesic from S\u22121 and D. Figure 1 illustrates this fact about the solution of GMML.\nThe regularized form of the previous solution is given by\nAreg = (S + \u03bbA \u22121 0 ) \u22121\u266ft (D + \u03bbA0),\nAlgorithm 1 Geometric Mean Metric Learning Input: S: set of similar pairs, D: set of dissimilar pairs, t: step length of geodesic, \u03bb: regularization parameter, A0: prior knowledge Compute the similarity and dissimilarity matrices:\nS = \u2211\n(xi,xj)\u2208S\n(xi \u2212 xj)(xi \u2212 xj)T\nD = \u2211\n(xi,xj)\u2208D\n(xi \u2212 xj)(xi \u2212 xj)T\nReturn the distance matrix:\nA = (S + \u03bbA\u221210 ) \u22121\u266ft (D + \u03bbA0)\nfor t \u2208 [0, 1]. In the cases where t = 1/2, it is equal to (15). This solution is our final and complete proposed solution to the linear metric learning problem. The summary of our GMML algorithm for metric learning is presented in Algorithm 1. Empirically, we have observed that the generalized solution (with free t) can significantly outperform the ordinary solution.\nThere are several approaches for fast computation of Riemannian geodesics for SPD matrices, for instance, CholeskySchur and scaled Newton methods [19]. We use CholeskySchur method in our paper to expedite the computation of Riemannian geodesics."}, {"heading": "III. RESULTS", "text": "In this section, we compare the performance of the proposed method GMML (Algorithm 1) to some well-known metric learning algorithms:\n\u2022 ITML [5]; \u2022 LMNN [2]; and \u2022 FlatGeo with batch flat geometry [8].\nWe exploit the commonly used criterion for comparing the performance of different methods, that is, the rate of the classification error for a k-NN classifier on different datasets. We choose k = 5, and estimate a full-rank matrix A in all methods."}, {"heading": "A. Experiment 1", "text": "Assume c to be the number of classes, it is common in practice to generate 40c(c \u2212 1) number of constraints by randomly choosing 40c(c\u2212 1) pairs of points in a dataset. In our first experiment, shown in Figure 2, we use this number of constraints in our method in addition to ITML and FlatGeo methods. The LMNN method does not have this number of constraints parameter and we used a new version of its toolbox that uses Bayesian optimization for optimizing the model hyper-parameters. We use the default parameters used in ITML and FlatGeo, except we also use a minimum iterations of 104 for the FlatGeo method, because we observed that sometimes FlatGeo stops prematurely leading to a very poor performance. ITML has a regularization parameter that is set by using crossvalidation.\nFigure 2 reports the results for the smaller datasets. The datasets are obtained from the well-known UCI repository [20]. In the plot, the baseline of using Euclidean distance for classification is shown in yellow. It can be seen that GMML outperforms the other three metric learning methods.\nThe figure reports 40 runs of a two-fold splitting of the data. In each run, the data is randomly divided into two equal sets. The regularization parameter \u03bb is set to zero for most of the datasets. We only add a small value of \u03bb when the similarity matrix S becomes singular. For example, since the similarity matrix of the Segment data is near singular, we use the regularized version of our method with \u03bb = 0.1 and A0 equals to the identity matrix.\nWe use five-fold cross-validation for choosing the best parameter t. We tested 18 different values for t in a two-step method. In the first step the best t is chosen among the values {0.1, 0.3, 0.5, 0.7, 0.9}. Then in the second step, 12 values of t are tested within an interval of length 0.02 in the window around the previously selected point.\nFigure 3 shows the effect of the parameter t on the average accuracy of k-NN classifier for five datasets. These datasets are also appeared in Figure 2. It is obvious that in some datasets, going from the ordinary version to the extended version can make the GMML\u2019s performance substantially better. Observe that each curve has a convex-like shape with some wiggling. That is why we choose the above approach for finding the best t, and we can verify its precision by comparing Figures 2 and 3."}, {"heading": "B. Experiment 2", "text": "To evaluate the performance of our method on larger datasets, we conduct a second set of experiments. The results can be summarized in Figure 4. The datasets in this experiment\nare Isolet, Letters [20], MNIST3 [21] and USPS [22]. Figure 4 reports the average classification error over 5 runs of random splitting of the data. We use three-fold crossvalidation for adjusting the parameter t. Since the similarity matrices of the MNIST data were not invertible, we use the regularized version of our method with regularization parameter \u03bb = 0.1. The prior matrix A0 is set to the identity matrix.\nOn two of the large datasets, Letters and USPS, our method achieves the same performance as the best competing method that is LMNN. For one of the datasets our method significantly outperforms LMNN, and in one dataset it is significantly\n3We used a smaller version of the MNIST dataset available in www.cad.zju.edu.cn/home/dengcai/Data/MLData.html\n6 USPS d = 256, c = 10\nn = 9298\nMNIST d = 784, c = 10\nn = 4000\nIsolet d = 617, c = 26\nn = 7797\nLetters d = 16, c = 26 n = 20000\n5\n10\n15 20 C la ss ifi ca tio n E rr or (% ) GMML LMNN ITML FlatGeo Euclidean\nFig. 4. Classification error rates of k-nearest neighbor classifier via different learned metrics for large datasets.\noutdone by LMMN. We also observed that by using more data pairs for generating the similarity and dissimilarity matrices, the performance of our method on Isolet and MNIST datasets improves. We tested 1000c(c \u2212 1) for these two datasets, with which we achieve about 1 percent better accuracy for Isolet leading to slightly better performance than FlatGeo approach. For MNIST data, we achieved about 0.5 percent better accuracy.\nThe average running times of the methods on all large data sets and one small dataset are shown in Table I. The running time of different methods is reported for only one run of each algorithm for fixed values of hyper-parameters; that means, the reported run times do not include the time required to select the hyper-parameters. All methods were implemented on MATLAB R2014a (64-bit), and the simulations were run on a personal laptop with an Intel Core i5 (2.5Ghz) processor under the OS X Yosemite operating system.\nIt can be seen that our method is several order of magnitudes faster than other methods. In addition to obtaining good classification accuracy using the proposed method, the computational complexity of our method is another nice property making it an interesting candidate for large-scale metric learning."}, {"heading": "IV. CONCLUSION AND FUTURE WORK", "text": "We revisited the task of learning a Euclidean metric from weakly supervised data given as pairs of similar and dissimilar points. Building on geometric intuition, we approached the task of learning a symmetric positive definite matrix by formulating it as a smooth, strictly convex optimization problem (thus, ensuring a unique solution). Remarkably, our formulation was shown to have a closed form solution. We also viewed our formulation as an optimization problem on the Riemannian manifold of SPD matrices, a viewpoint that\nproved crucial to obtaining a weighted generalization of the basic formulation. We also presented a regularized version of our problem. In all cases, the solution could be obtained as a closed form \u201cmatrix geometric mean\u201d, thus explaining our choice of nomenclature.\nWe experimented with several datasets, both large and small, in which we compared the classification accuracy of a k-NN classifier using metric learned via various competing methods. In addition to good classification accuracy and global optimality, our proposed method for solving the metric learning problem has other nice properties like being fast and being scalable with regard to both the dimensionality d and the number of training samples n.\nGiven the importance of metric learning to a vast number of applications, we believe that the new understanding offered by our formulation, its great simplicity, and its tremendous speedup over widely used methods make it attractive."}, {"heading": "A. Future work", "text": "Several avenues of future work are worth pursuing. We list some most promising directions below:\n\u2022 To view our metric learning methods as a dimensionality reduction method; here the connections in [4] may be helpful.\n\u2022 Extensions of our simple geometric framework to learn nonlinear and local metrics.\n\u2022 Applying the idea of using concurrently the Mahalanobis distance dA with its counterpart d\u0302A on the other machine learning problems."}], "references": [{"title": "Metric learning: A survey", "author": ["B. Kulis"], "venue": "Foundations and Trends in Machine Learning, vol. 5, no. 4, pp. 287\u2013364, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 207\u2013244, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "M.I. Jordan", "S. Russell", "A.Y. Ng"], "venue": "Advances in neural information processing systems, 2002, pp. 505\u2013512.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Linear dimensionality reduction: Survey, insights, and generalizations", "author": ["J.P. Cunningham", "Z. Ghahramani"], "venue": "Journal of Machine Learning Research, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "Proceedings of the 24th international conference on Machine learning. ACM, 2007, pp. 209\u2013216.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "arXiv preprint arXiv:1306.6709, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S.T. Roweis"], "venue": "Advances in neural information processing systems, 2005, pp. 451\u2013458.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Regression on fixed-rank positive semidefinite matrices: A riemannian  7 approach", "author": ["G. Meyer", "S. Bonnabel", "R. Sepulchre"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 593\u2013625, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Online and batch learning of pseudo-metrics", "author": ["S. Shalev-Shwartz", "Y. Singer", "A.Y. Ng"], "venue": "Proceedings of the twenty-first international conference on Machine learning, 2004, p. 94.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Online metric learning and fast similarity search", "author": ["P. Jain", "B. Kulis", "I.S. Dhillon", "K. Grauman"], "venue": "Advances in neural information processing systems, 2009, pp. 761\u2013 768.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast solvers and efficient implementations for distance metric learning", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Proceedings of the 25th international conference on Machine learning, 2008, pp. 1160\u20131167.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Online learning in the embedded manifold of low-rank matrices", "author": ["U. Shalit", "D. Weinshall", "G. Chechik"], "venue": "The Journal of Machine Learning Research, vol. 13, no. 1, pp. 429\u2013458, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Metric spaces, convexity and nonpositive curvature", "author": ["A. Papadopoulos"], "venue": "European Mathematical Society,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Geodesic convexity in nonlinear optimization", "author": ["T. Rapcs\u00e1k"], "venue": "Journal of Optimization Theory and Applications, vol. 69, no. 1, pp. 169\u2013183, 1991.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1991}, {"title": "Positive definite matrices", "author": ["R. Bhatia"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Conic geometric optimization on the manifold of positive definite matrices", "author": ["S. Sra", "R. Hosseini"], "venue": "SIAM Journal on Optimization, vol. 25, no. 1, pp. 713\u2013739, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Convex analysis", "author": ["R.T. Rockafellar"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1970}, {"title": "The geometric mean of two matrices from a computational viewpoint", "author": ["B. Iannazzo"], "venue": "arXiv preprint arXiv:1201.0101, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "UCI machine learning repository", "author": ["A. Asuncion", "D. Newman"], "venue": "2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u2013 2324, 1998.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["B.B. Le Cun", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Advances in neural information processing systems, 1990.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1990}], "referenceMentions": [{"referenceID": 0, "context": "When supervised or weakly supervised information is available, selection of the distance function can itself be cast as a learning problem called \u201cmetric learning\u201d [1, 2].", "startOffset": 164, "endOffset": 170}, {"referenceID": 1, "context": "When supervised or weakly supervised information is available, selection of the distance function can itself be cast as a learning problem called \u201cmetric learning\u201d [1, 2].", "startOffset": 164, "endOffset": 170}, {"referenceID": 2, "context": "The problem of learning linear maps was introduced in [3] as \u201cMahalanobis metric learning.", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "More broadly, the idea of linearly transforming input features is a bigger theme across machine learning and statistics; encompassing whitening transforms, linear dimensionality reduction, Euclidean metric learning, and more [1, 4].", "startOffset": 225, "endOffset": 231}, {"referenceID": 3, "context": "More broadly, the idea of linearly transforming input features is a bigger theme across machine learning and statistics; encompassing whitening transforms, linear dimensionality reduction, Euclidean metric learning, and more [1, 4].", "startOffset": 225, "endOffset": 231}, {"referenceID": 4, "context": "We omit a discussion of nonlinear methods, and other variations of the basic Euclidean task outlined above; for these, we refer the reader to both kernelized metric learning [5] and other techniques as summarized in the recent surveys of Kulis [1] and Bellet et al.", "startOffset": 174, "endOffset": 177}, {"referenceID": 0, "context": "We omit a discussion of nonlinear methods, and other variations of the basic Euclidean task outlined above; for these, we refer the reader to both kernelized metric learning [5] and other techniques as summarized in the recent surveys of Kulis [1] and Bellet et al.", "startOffset": 244, "endOffset": 247}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Probably the earliest work to formulate metric learning is [3], sometimes referred to as MMC.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "[3] use \u221a dA instead of the distance dA because under dA, problem (2) has a trivial rank-one solution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "(xi,xj)\u2208S dA(xi,xj) is also used in the other metric learning methods like LMNN [2] and MCML [7] as a part of their cost functions.", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "(xi,xj)\u2208S dA(xi,xj) is also used in the other metric learning methods like LMNN [2] and MCML [7] as a part of their cost functions.", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "Information-Theoretic Metric Learning (ITML) [5], aims to satisfy the similarity and dissimilarity constraints while staying as \u201cclose\u201d as possible to a predefined matrix.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "[8] propose the formulation", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "There exist several attempts for achieving high scalability with both the dimensionality and the number of constraints in the metric learning methods; some examples include [9, 10, 11, 12].", "startOffset": 173, "endOffset": 188}, {"referenceID": 9, "context": "There exist several attempts for achieving high scalability with both the dimensionality and the number of constraints in the metric learning methods; some examples include [9, 10, 11, 12].", "startOffset": 173, "endOffset": 188}, {"referenceID": 10, "context": "There exist several attempts for achieving high scalability with both the dimensionality and the number of constraints in the metric learning methods; some examples include [9, 10, 11, 12].", "startOffset": 173, "endOffset": 188}, {"referenceID": 11, "context": "There exist several attempts for achieving high scalability with both the dimensionality and the number of constraints in the metric learning methods; some examples include [9, 10, 11, 12].", "startOffset": 173, "endOffset": 188}, {"referenceID": 12, "context": "Geodesic convexity is the generalization of ordinary (linear) convexity to (nonlinear) manifolds and metric spaces [13, 14].", "startOffset": 115, "endOffset": 123}, {"referenceID": 13, "context": "Geodesic convexity is the generalization of ordinary (linear) convexity to (nonlinear) manifolds and metric spaces [13, 14].", "startOffset": 115, "endOffset": 123}, {"referenceID": 0, "context": "A\u266ftB = A 1/2 ( ABA t A, t \u2208 [0, 1].", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "f(A\u266ftB) \u2264 tf(A) + (1\u2212 t)f(B), t \u2208 [0, 1].", "startOffset": 34, "endOffset": 40}, {"referenceID": 15, "context": "We refer the reader to [16] for more on geodesic convexity for SPD matrices.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "for t \u2208 [0, 1].", "startOffset": 8, "endOffset": 14}, {"referenceID": 17, "context": "There are several approaches for fast computation of Riemannian geodesics for SPD matrices, for instance, CholeskySchur and scaled Newton methods [19].", "startOffset": 146, "endOffset": 150}, {"referenceID": 4, "context": "In this section, we compare the performance of the proposed method GMML (Algorithm 1) to some well-known metric learning algorithms: \u2022 ITML [5]; \u2022 LMNN [2]; and \u2022 FlatGeo with batch flat geometry [8].", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "In this section, we compare the performance of the proposed method GMML (Algorithm 1) to some well-known metric learning algorithms: \u2022 ITML [5]; \u2022 LMNN [2]; and \u2022 FlatGeo with batch flat geometry [8].", "startOffset": 152, "endOffset": 155}, {"referenceID": 7, "context": "In this section, we compare the performance of the proposed method GMML (Algorithm 1) to some well-known metric learning algorithms: \u2022 ITML [5]; \u2022 LMNN [2]; and \u2022 FlatGeo with batch flat geometry [8].", "startOffset": 196, "endOffset": 199}, {"referenceID": 18, "context": "The datasets are obtained from the well-known UCI repository [20].", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "are Isolet, Letters [20], MNIST3 [21] and USPS [22].", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": "are Isolet, Letters [20], MNIST3 [21] and USPS [22].", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "are Isolet, Letters [20], MNIST3 [21] and USPS [22].", "startOffset": 47, "endOffset": 51}], "year": 2016, "abstractText": "We revisit the task of learning a Euclidean metric from data. We approach this problem from first principles and formulate it as a surprisingly simple optimization problem. Indeed, our formulation even admits a closed form solution. This solution possesses several very attractive properties: (i) an innate geometric appeal through the Riemannian geometry of positive definite matrices; (ii) ease of interpretability; and (iii) computational speed several orders of magnitude faster than the widely used LMNN and ITML methods. Furthermore, on standard benchmark datasets, our closed-form solution consistently attains higher classification accuracy.", "creator": "LaTeX with hyperref package"}}}