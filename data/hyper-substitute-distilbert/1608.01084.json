{"id": "1608.01084", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Aug-2016", "title": "To Swap or Not to Swap? Exploiting Dependency Word Pairs for Reordering in Statistical Machine Translation", "abstract": "sts removes a physical challenge in sound navigation ( mt ) between two languages recognizing significant differences in specific meaning. in 1 paper, we present to remarkable structural option representing new features conditional on dependency word connectivity. promising instance of these features remains whether two words, which became related by a dependency link without the source sentence sequence parse tree, correspond within same order or are neglected in the translation process. experiments reviewing catalan - to - english translation show markedly statistically modest improvement of 102. 21 bleu point using hm approach, compared to simplified state - of - da - art, mt system suggesting incorporates large reordering effects.", "histories": [["v1", "Wed, 3 Aug 2016 06:24:01 GMT  (2718kb,D)", "http://arxiv.org/abs/1608.01084v1", "7 pages, 1 figures, Proceedings of AAAI-16"]], "COMMENTS": "7 pages, 1 figures, Proceedings of AAAI-16", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["christian hadiwinoto", "yang liu", "hwee tou ng"], "accepted": true, "id": "1608.01084"}, "pdf": {"name": "1608.01084.pdf", "metadata": {"source": "META", "title": "To Swap or Not to Swap? Exploiting Dependency Word Pairs for Reordering in Statistical Machine Translation", "authors": ["Christian Hadiwinoto", "Yang Liu", "Hwee Tou Ng"], "emails": ["chrhad@comp.nus.edu.sg", "nght@comp.nus.edu.sg", "liuyang2011@tsinghua.edu.cn"], "sections": [{"heading": "Introduction", "text": "Reordering in machine translation (MT) is a crucial process to get the correct translation output word order given an input source sentence, as word order reflects meaning. It remains a major challenge, especially for language pairs with a significant word order difference. Phrase-based MT systems (Koehn, Och, and Marcu 2003) generally adopt a reordering model that predicts reordering based on the span of a phrase and that of the adjacent phrase (Tillmann 2004; Xiong, Liu, and Lin 2006; Galley and Manning 2008; Cherry 2013).\nThe above methods do not explicitly preserve the relationship between words in the source sentence, which reflects the sentence meaning. Word relationship in a sentence can be captured by its dependency parse tree, in which each word w is a tree node connected to its head node hw, another word, indicating that the former is a dependent (child) of the latter.\nDependency parsing has been used for reordering in statistical machine translation (SMT). Its usage is well-known in the pre-ordering approach, where a source sentence is reordered before the actual translation. Dependency-based pre-ordering can be performed either by a rule-based approach based on manually specified human linguistic knowledge (Xu et al. 2009; Cai et al. 2014), or by a learning approach (Xia and McCord 2004; Habash 2007; Genzel 2010; Yang et al. 2012; Lerner and Petrov 2013; Jehl et al.\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n2014). Dependency parsing has also been used in reordering approaches integrated with decoding to determine the next source phrase to translate after translating the current source phrase (Cherry 2008; Bach, Gao, and Vogel 2009; Chang et al. 2009).\nIn this paper, we propose a novel reordering approach integrated with translation. We propose sparse feature functions based on the pre-ordering rules of (Cai et al. 2014). However, in contrast to the manual rule-based pre-ordering approach of (Cai et al. 2014), the weights of our sparse feature functions are automatically learned and used during the actual translation process, without an explicit pre-ordering step. Our approach detects and exploits the reordering of each dependency word pair in the source sentence during phrase-based decoding."}, {"heading": "Dependency Word Pair Features", "text": "We define a set of sparse features based on dependency tree word pairs to be learned and used in a phrase-based SMT beam search decoding algorithm."}, {"heading": "Capturing Word Pair Ordering in Phrase-Based Beam Search", "text": "The phrase-based approach is a state-of-the-art approach for SMT, taking phrases, defined as a sequence of one or more words, as its translation units. It is performed by a beam search algorithm (Koehn 2004a), in which the search process produces translation from left to right in the translation output. The search is organized into hypotheses, each of which represents an input sentence phrase covered and its possible translation.\nAs the beam search can choose input phrases in any order, the target-language phrase sequence in the translation output may not follow the original source sentence order. The sequence determines the translation output order and enables translation output reordering for language pairs with differences in word order, such as Chinese and English.\nWhen a word fi in a source sentence f = fN1 is covered by a hypothesis, it is known that the words {fl|fl \u2208 f i\u221211 \u2227 \u00actranslated(fl)} on the left of fi and the words {fr|fr \u2208 fNi+1 \u2227 \u00actranslated(fr)} on the right of fi that have not been translated will be translated after (appearing on the right of) the translation of fi. As fl is before fi in the\nar X\niv :1\n60 8.\n01 08\n4v 1\n[ cs\n.C L\n] 3\nA ug\n2 01\n6\nsource sentence, but the translation of fl is after the translation of fi, the translations of fi and fl are swapped. Meanwhile, fr is after fi both in the source sentence and in the translation, therefore the translations of fi and fr are inorder. Internally within a phrase, the ordering of each of its words in the translation depends on the phrasal word alignment, which is stored in the phrase table.\nAs each word in the source sentence is a node of the source dependency parse tree, the above notion can be used for reordering based on the source dependency parse tree. Instead of capturing all pairwise relations, we are only interested in the relations between a word and its related words, defined collectively as its head, sibling, and child words in the dependency parse tree."}, {"heading": "Dependency Swap Features", "text": "We define our dependency swap features, following the rule template definition for dependency swap rules (Cai et al. 2014), which defines rule instances based on the word pairs with head-child or sibling relationship. However, the difference is that our approach does not require manually specifying which dependency labels are the conditions to swap words, but learns them automatically.\nIn our approach, each rule instance based on the above template becomes a Boolean sparse feature function (Chiang, Knight, and Wang 2009). The function parameters are the word pair specification and output order. While Cai et al. (2014) defined the rules only by the dependency labels, we define our feature functions for each word pair by the dependency labels, the POS tags, and the combination of both, resulting in a group of four feature functions for every word pair ordering. The dependency link label of a word x is defined as the label of the link connecting x to its head word. Henceforth for each word x, L(x) and T (x) denote the dependency link label and POS tag of x respectively.\nFollowing the dependency swap rule template, we define two types of feature function templates, namely head-child and sibling. The head-child feature functions are equal to 1 if a head word xh and its child word xc (where xh is on the p \u2208 {left, right} of xc in the source sentence) take a\ncertain ordering o (which can be in-order (io) or swapped (sw)) in the translation output, and 0 otherwise. This group of feature functions is defined as\nHhc(xh, xc, p, o) =  hhc(L(xh), L(xc), p, o)hhc(T (xh), T (xc), p, o)hhc(L(xh), T (xc), p, o) hhc(T (xh), L(xc), p, o)  (1) Similarly, the sibling feature functions are equal to 1 if siblings xl and xr (xl on the left of xr in the source sentence) take a certain ordering o in the translation output, and 0 otherwise. This group of feature functions is defined as\nHsib(xl, xr, o) =  hsib(L(xl), L(xr), o)hsib(T (xl), T (xr), o)hsib(L(xl), T (xr), o) hsib(T (xl), L(xr), o)  (2) Each dependency swap feature that is set to 1 when a hypothesis is generated captures two source words, one is covered by the current hypothesis, while the other has not yet been translated. These word pairs have either head-child or sibling relationship as defined above. Figure 1 is an illustration of how word order is detected during beam search decoding and the swap features involved. When \u201c\u53d1\u8868\u8bb2\u8bdd \u2192 made a speech\u201d is translated after \u201c\u4f50\u79d1\u5a01 \u2192 Jokowi\u201d, it is known that the head \u201c\u53d1\u8868\u201d is before the child \u201c\u8bb2\u8bdd\u201d (p = left) and their translations also follow the same word order, setting the value of the following four features to 1:\nHhc(\u53d1\u8868,\u8bb2\u8bdd, left, io) =  hhc(root, dobj, left, io)hhc(V V,NN, left, io)hhc(root,NN, left, io) hhc(V V, dobj, left, io)  On the other hand, the head \u201c\u53d1\u8868\u201d is after the child \u201c\u5728\u201d in the source sentence (p = right), but \u201c\u5728\u201d has not been translated. Therefore, the translation of \u201c\u53d1\u8868\u201d will be swapped with that of \u201c\u5728\u201d, setting the four features in Hhc(\u53d1\u8868,\u5728, right, sw) to 1. Similarly, \u201c\u8bb2\u8bdd\u201d is after its sibling \u201c\u5728\u201d in the source sentence, but \u201c\u5728\u201d\nhas not been translated, resulting in the translation of the two words being swapped and setting the four features in Hsib(\u5728,\u8bb2\u8bdd, sw) to 1. The features for \u201c\u4f50\u79d1\u5a01\u201d such as Hhc(\u53d1\u8868,\u4f50\u79d1\u5a01, right, io) and Hsib(\u4f50\u79d1\u5a01,\u8bb2\u8bdd, io) are not set to 1 when the hypothesis \u201c\u53d1\u8868 \u8bb2\u8bdd \u2192 made a speech\u201d is generated. Those features were set to 1 with the previous hypothesis \u201c\u4f50\u79d1\u5a01\u2192 Jokowi\u201d."}, {"heading": "Dependency Distortion Penalty", "text": "To encourage the translation output that conforms to the dependency parse structure, we impose a penalty feature that discourages translation candidates not conforming to the dependency parse subtree (Cherry 2008). This assigns a penalty if the translation of the current phrase results in a source dependency parse subtree to be split in the output translation.\nThe translation output shown in Figure 1a has each dependency parse subtree grouped together, therefore incurring no penalty. However, there can be a case when the translation has produced \u201cJokowi made a speech in\u201d, then translate \u201c\u6628 \u5929\u201d to \u201cyesterday\u201d. This translation will break the cohesion of the source dependency parse subtree \u201c\u5728 \u5317\u4eac\u201d and is bad. This is the case when a dependency distortion penalty is incurred."}, {"heading": "Modified Future Cost", "text": "Phrase-based SMT beam search decoding involves future cost, not accumulated over the translation hypotheses but used to reduce search space (Koehn 2004a). This predicts the cost to translate the remaining untranslated phrases.\nBased on our dependency swap features, we incorporate the future cost for each untranslated dependency word pairs. The future cost assumes that the untranslated words will be ordered in the most likely ordering. However, if an untranslated word x is an ancestor (in the dependency parse tree) of a word covered by the current hypothesis, the future cost assumes that x precedes all of its related words. This follows the assumption that each subtree will have a contiguous translation (Cherry 2008)."}, {"heading": "Other Sparse Features", "text": "This section describes other sparse features from previous work to compare to our method."}, {"heading": "Sparse Reordering Orientation Features", "text": "We incorporate sparse reordering orientation features (Cherry 2013). The features are derived from the reordering orientation model, capturing the source position of the current phrase being translated with respect to the previously translated phrase(s), for which three orientation types are defined: \u2022 Monotone (M), if the current phrase and the previously\ntranslated unit are adjacent in the input sentence and the former follows the latter in it, e.g., \u201cBeijing\u201d with respect to the previous phrase \u201cin\u201d in Figure 1.\n\u2022 Swapped (S), if the current phrase and the previously translated unit are adjacent in the input sentence but the former precedes the latter in it (example below).\n\u2022 Discontinuous (D), if the current phrase and the previously translated unit are not adjacent in the input sentence, e.g., \u201cmade a speech\u201d with respect to the previous phrase \u201cJokowi\u201d in Figure 1. Cherry (2013) designed his sparse reordering orientation model following the hierarchical reordering (HR) model (Galley and Manning 2008), capturing the relative position of the current phrase (covered by the current hypothesis) with respect to the largest chunk of contiguous source phrases that form a contiguous translation before this phrase. Therefore, in Figure 1a, when the decoding produces a phrase \u201cyesterday\u201d after \u201cBeijing\u201d, the orientation of \u201cyesterday\u201d is swapped instead of discontinuous, as \u201cmade a speech in Beijing\u201d is formed by contiguous phrases \u201c\u5728\u5317 \u4eac\u53d1\u8868\u8bb2\u8bdd\u201d, which is adjacent to \u201c\u6628\u5929\u201d.\nWhile the original (non-sparse) reordering orientation model is based on the phrase orientation probability in the parallel training data and defines a single feature function on it, the sparse model defines one feature function for each reordering phenomenon during decoding. We define the sparse feature functions taking into account the phrase orientation o \u2208 {M,S,D} during decoding and important locations loc pertaining to the current phrase and the previous phrase with the following template:\nhs hr(loc := rep(loc), o) (3)\nwhere locations loc are the first and last words of the current source phrase (sfirst,slast), the previously translated unit (pfirst,plast), i.e., the largest contiguous chunk of phrases forming a contiguous translation, and the span between the current and the previous source phrase, or gap (gfirst,glast) only for discontinuous orientation. Each word in loc is represented, rep(loc), by its POS tag1, and the surface lexical form if it belongs to the 80-most frequent words in the training data.\nAssuming that only \u201c\u5728\u201d belongs to the top-80 words, when the phrase \u201cyesterday\u201d is generated after \u201cBeijing\u201d, the sparse reordering orientation features that are equal to 1 are shown in Figure 2.\nWhile the approach leverages the reordering orientation model by defining features on the context information, not just the current phrase, it does not capture dependency relation, by which the important relation between words in the source sentence is captured. Therefore, we introduce our sparse dependency swap features to enable the translation system to arrive at reordering decisions based on the source dependency relations.\n1Cherry (2013) substituted POS tags with mkcls unsupervised word clusters."}, {"heading": "Dependency Path Features", "text": "We also utilize dependency path features (Chang et al. 2009) for phrase-based SMT, defined over the shortest path of dependency parse tree links bridging the current and the previous source phrase. Chang et al. (2009) defined the dependency path features on a maximum entropy phrase orientation classifier, trained on their word-aligned parallel text and labeled by the two possible phrase orderings in the translation output: in-order and swapped. Meanwhile, we use the features as sparse decoding features, with the following template:\nhpath(shortest path(plast, sfirst); o) (4)\nwhere o \u2208 {in order, swapped} denotes the orientation of the two phrases in the translation.\nGiven a source sentence and its translation output, a path is defined between the last word of the previous source phrase plast and the first word of the current source phrase sfirst. Path traversal is always from left to right based on the source sentence word position. Therefore, if the current source phrase being translated is to the right of the previous source phrase, the traversal is from plast to sfirst. Otherwise, if the current source phrase is to the left of the previous source phrase, it is from sfirst to plast. In addition, path edges going against the dependency label arrow are distinguished from those following the arrow.\nAs an example, in Figure 1a, when the translation generates \u201c\u53d1\u8868 \u8bb2\u8bdd \u2192 made a speech\u201d after \u201c\u4f50\u79d1\u5a01 \u2192 Jokowi\u201d, the path is from \u201c\u4f50\u79d1\u5a01\u201d to \u201c\u53d1\u8868\u201d, consisting of a direct link following the arrow of nsubj, resulting in the feature hpath(nsubj, in order) = 1. However, when the translation generates \u201c\u6628\u5929 \u2192 yesterday\u201d after \u201c\u5317\u4eac \u2192 Beijing\u201d, as \u201c\u6628\u5929\u201d is before \u201c\u5317\u4eac\u201d in the source sentence, the traversal is from \u201c\u6628\u5929\u201d to \u201c\u5317 \u4eac\u201d, consisting of the link sequence tmod, prepR, pobjR. As it goes against the arrows of prep and pobj, the suffix R is added to distinguish it. This results in the feature hpath(tmod, prepR, pobjR; in order) = 1.\nThe approach leverages the phrase-based reordering by guiding the ordering of two adjacent phrases using dependency parse. However, the features do not capture the pairwise ordering of every word with its related word, as the features are induced only when the words are used in the two adjacent translation phrases."}, {"heading": "Experimental Setup", "text": ""}, {"heading": "Data Set and Toolkits", "text": "We built a phrase-based Chinese-to-English SMT system by using Moses (Koehn et al. 2007). Our parallel training text is a collection of parallel corpora from LDC, which we divide into older corpora2 and newer corpora3. Due to the dominant older data, we duplicate the newer corpora of various\n2LDC2002E18, LDC2003E14, LDC2004E12, LDC2004T08, LDC2005T06, and LDC2005T10.\n3LDC2007T23, LDC2008T06, LDC2008T08, LDC2008T18, LDC2009T02, LDC2009T06, LDC2009T15, LDC2010T03, LDC2013T11, LDC2013T16, LDC2014T04, LDC2014T11, LDC2014T15, LDC2014T20, and LDC2014T26.\ndomains by 10 times to achieve better domain balance. To reduce the possibility of alignment errors, parallel sentences in the corpora that are longer than 85 words in either Chinese (after word segmentation) or English are discarded. In the end, the final parallel text consists of around 8.8M sentence pairs, 228M Chinese tokens, and 254M English tokens (a token can be a word or punctuation symbol). We also added two dictionaries4 by concatenating them to our training parallel text. The total number of words in these two corpora is 1.81M for Chinese and 2.03M for English.\nAll Chinese sentences in the training, development, and test data are first word-segmented using a maximum entropy-based Chinese word segmenter (Low, Ng, and Guo 2005) trained on the Chinese Treebank (CTB) scheme. Then the parallel corpus is word-aligned by GIZA++ (Och and Ney 2003) using IBM Models 1, 3, and 4 (Brown et al. 1993)5. For building the phrase table, which follows word alignment, the maximum length of a phrase pair is set to 7 words for both the source and target sides.\nThe language model (LM) is a 5-gram model trained on the English side of the FBIS parallel corpus (LDC2003E14) and the monolingual corpus English Gigaword version 4 (LDC2009T13), consisting of 107M sentences and 3.8G tokens altogether. Each individual Gigaword sub-corpus6 is used to train a separate language model and so is the English side of FBIS. These individual language models are then interpolated to build one single large LM, via perplexity tuning on the development set.\nOur translation development set is MTC corpus version 1 (LDC2002T01) and version 3 (LDC2004T07). This development set has 1,928 sentence pairs in total, 49K Chinese tokens and 58K English tokens on average across the four reference translations. Weight tuning is done by using the pairwise ranked optimization (PRO) algorithm (Hopkins and May 2011), which is also used to obtain weights of the sparse features to help determine the reordering.\nFor dependency sparse features, we parse the Chinese side of our development and test sets by the Mate parser, which jointly performs POS tagging and dependency parsing (Bohnet and Nivre 2012), trained on Chinese Treebank (CTB) version 8.0 (LDC2013T21).\nOur test set consists of the NIST MT evaluation sets from 2002 to 2006 and 2008 (LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14, LDC2010T17, LDC2010T21)."}, {"heading": "Baseline System", "text": "We build a phrase-based baseline SMT system, which uses non-sparse phrase-based lexicalized reordering (PBLR), in which the reordering probability depends on the phrase being translated and its position with respect to the source position of the previously translated phrase (Tillmann 2004; Koehn et al. 2005), and non-sparse hierarchical reordering (HR), in which the previous unit is not only the previous phrase, but the largest chunk of contiguous source phrases\n4LDC2002L27 and LDC2005T34. 5The default when running GIZA++ with Moses. 6AFP, APW, CNA, LTW, NYT, and Xinhua.\nhaving contiguous translation (Galley and Manning 2008). In addition, a distortion limit is set such that the reordering cannot be longer than a certain distance. We set punctuation symbols as reordering constraint across which phrases cannot be reordered, as they form the natural boundaries between different clauses. We also use n-best Minimum Bayes Risk (MBR) decoding (Kumar and Byrne 2004) instead of the default maximum a-posteriori (MAP) decoding."}, {"heading": "Our Approach", "text": "To accommodate our sparse feature functions, our Moses code has been modified to read dependency-parsed input sentences and incorporate additional decoding features on top of our baseline, namely dependency distortion penalty (DDP) (Cherry 2008), sparse dependency path features (Path) (Chang et al. 2009), sparse reordering orientation following hierarchical reordering orientation (SHR) (Cherry 2013), and our sparse dependency swap features (DS). DDP feature is a single penalty feature similar to the distortion penalty for distance-based reordering model (Koehn, Och, and Marcu 2003), while Path, SHR, and DS are sparse features, each instance of which captures a specific phenomenon during translation (Chiang, Knight, and Wang 2009).\nWe always couple DS with DDP. However, as the original Path features did not use DDP, we experiment with Path features in one setting that does not incorporate DDP and another that does. The SHR features are not coupled with DDP, following the original design, as the features are not defined on a dependency-parsed input sentence."}, {"heading": "Experimental Results", "text": "The translation quality of the system outputs is measured by case-insensitive BLEU (Papineni et al. 2002), for which the brevity penalty is computed based on the shortest reference (NIST-BLEU)7. Statistical significance testing between systems is conducted by bootstrap resampling (Koehn 2004b).\n7ftp://jaguar.ncsl.nist.gov/mt/resources/ mteval-v11b.pl\nTable 1 shows the experimental results. The distortion limit of all the systems is set to 14, which yields the best result on the development set for the baseline system. As shown in the table, the system with our DS features and DDP on top of the baseline is able to improve over the baseline system without and with DDP, by +1.09 and +0.93 BLEU points respectively. The individual contribution of the other dependency-based features (Path), without or with DDP, is inferior to our DS features. Nevertheless, coupling our DS features with Path features yields the best result (+1.21 and +1.05 BLEU points over the baseline without and with DDP).\nThe SHR features yield more improvement than Path features without DDP, and are comparable to the Path features with DDP. However, our DS features yield more improvement than SHR features. Our preliminary experiments indicate that adding these features on top of the system with DS does not improve over it."}, {"heading": "Discussions", "text": "The reordering orientation models, i.e., PBLR and HR, only take into account the phrase pair generated by a hypothesis and not the related word properties. Sparse reordering orientation features (Cherry 2013) leverage this by capturing the previous phrase (or contiguous chunk) properties. Therefore, they are able to improve over the baseline. However, as the results suggest, dependency parse provides a more useful guidance to reorder a source sentence.\nAs shown in Figure 3, the baseline phrase-based SMT system with the two reordering orientation models (PBLR and HR) produces an incorrect translation output that \u201cthe export is the main market\u201d, which is not what the source sentence means. This is also the case with the system added with prior reordering approaches, which includes DDP (Cherry 2008), Path (Chang et al. 2009), their combination DDP+Path, and SHR (Cherry 2013). Meanwhile, our reordering approach with DS features is able to output the correct translation \u201cthe main markets for the export\u201d, as it penalizes the swap between the subject head \u201cmarket\u201d and the copula, which\nshould not be swapped. The dependency parse of a sentence can capture the relationship among the words (nodes) in it. Between each related word pair, there is a dependency label which specifies the head-modifier relationship. While the head-modifier relationships in a sentence hold across languages, their ordering may differ. For example, in Chinese, prepositional phrase (modifier) comes before the predicate (head) verb, while in English, they come in the other way round. This particular clue, provided by the source dependency parse, is useful in deciding the word order in the translation output, corresponding to a word in the source sentence.\nCombining our sparse dependency swap features with sparse dependency path features (Chang et al. 2009) achieves the best experimental result. This can be attributed to the complementary nature of both types of sparse features. Path features are able to capture the subsequence of translation output phrases, i.e., which phrase follows another but not the relative position (right or left) of a source dependency word with respect to all its related words, while our swap features are designed to capture them."}, {"heading": "Related Work", "text": "Source dependency trees have been exploited in phrasebased SMT, by modeling transition among dependency subtrees during translation (Bach, Gao, and Vogel 2009). However, this does not take into account the dependency label\nand the POS tag. Another work exploits source and target dependency trees for phrase-based MT output reranking (Gimpel and Smith 2014), instead of for translation decoding.\nChang et al. (2009) introduced dependency path as a soft constraint based on the sequence of source dependency links traversed in phrase-based translation. It is used as features on a maximum entropy phrase orientation classifier, whose probability output is used as a decoding feature function. As the path can be arbitrarily long, it may not be represented sufficiently in the training samples. Our sparse feature definition can alleviate this as features are defined on two words. In addition, capturing word pairs instead of paths enables incorporation of other word properties such as POS tags.\nHunter and Resnik (2010) proposed a probability model to capture the offset of a word with respect to its head position in phrase-based MT. Their model does not take into account two sibling words sharing the same head. They reported negative result.\nMeanwhile, Gao, Koehn, and Birch (2011) defined soft constraints on hierarchical phrase-based MT, which produce translation by bottom-up constituency parsing algorithm instead of beam search. Their soft constraint is also defined on a maximum entropy classifier instead of sparse features.\nPrior work has also used constituency parse to guide reordering, by manually defining pre-ordering rules to reorder input sentences into the target language order before translation (Collins, Koehn, and Kuc\u030cerova\u0301 2005; Wang, Collins, and Koehn 2007), or to automatically learn those rules (Li et al. 2007; Khalilov and Fonollosa 2011). Each constituency parse tree node represents the phrase nesting instead of a word, resulting in a deeper structure, which is generally slower to produce."}, {"heading": "Conclusion", "text": "We have presented a reordering approach for phrase-based SMT, guided by sparse dependency swap features. We have contributed a new approach for learning and performing reordering in phrase-based MT by the incorporation of dependency-based features. From our experiments, we have shown that utilizing source dependency parse for reordering sentences helps to significantly improve translation quality over a phrase-based baseline system with state-of-the-art reordering orientation models."}, {"heading": "Acknowledgments", "text": "This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office."}], "references": [{"title": "Source-side dependency tree reordering models with subtree movements and constraints", "author": ["N. Bach", "Q. Gao", "S. Vogel"], "venue": "Proceedings of MT Summit XII. Bohnet, B., and Nivre, J. 2012. A transition-based system for joint part-of-speech tagging and labeled non-projective", "citeRegEx": "Bach et al\\.,? 2009", "shortCiteRegEx": "Bach et al\\.", "year": 2009}, {"title": "The mathematics of statistical machine translation", "author": ["P.F. Brown", "S.A. Della Pietra", "V.J. Della Pietra", "R.L. Mercer"], "venue": "Computational Linguistics 19(2):263\u2013311.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Dependency-based pre-ordering for Chinese-English machine translation", "author": ["J. Cai", "M. Utiyama", "E. Sumita", "Y. Zhang"], "venue": "Proceedings of ACL 2014 (Short Papers), 155\u2013160.", "citeRegEx": "Cai et al\\.,? 2014", "shortCiteRegEx": "Cai et al\\.", "year": 2014}, {"title": "Discriminative reordering with Chinese grammatical relations features", "author": ["P.-C. Chang", "H. Tseng", "D. Jurafsky", "C.D. Manning"], "venue": "Proceedings of SSST-3, 51\u201359.", "citeRegEx": "Chang et al\\.,? 2009", "shortCiteRegEx": "Chang et al\\.", "year": 2009}, {"title": "Cohesive phrase-based decoding for statistical machine translation", "author": ["C. Cherry"], "venue": "Proceedings of ACL-08: HLT, 72\u201380.", "citeRegEx": "Cherry,? 2008", "shortCiteRegEx": "Cherry", "year": 2008}, {"title": "Improved reordering for phrase-based translation using sparse features", "author": ["C. Cherry"], "venue": "Proceedings of NAACL HLT 2013, 22\u201331.", "citeRegEx": "Cherry,? 2013", "shortCiteRegEx": "Cherry", "year": 2013}, {"title": "11,001 new features for statistical machine translation", "author": ["D. Chiang", "K. Knight", "W. Wang"], "venue": "Proceedings of NAACL HLT 2009, 218\u2013226.", "citeRegEx": "Chiang et al\\.,? 2009", "shortCiteRegEx": "Chiang et al\\.", "year": 2009}, {"title": "Clause restructuring for statistical machine translation", "author": ["M. Collins", "P. Koehn", "I. Ku\u010derov\u00e1"], "venue": "Proceedings of ACL 2005, 531\u2013540.", "citeRegEx": "Collins et al\\.,? 2005", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "A simple and effective hierarchical phrase reordering model", "author": ["M. Galley", "C.D. Manning"], "venue": "Proceedings of EMNLP 2008, 848\u2013856.", "citeRegEx": "Galley and Manning,? 2008", "shortCiteRegEx": "Galley and Manning", "year": 2008}, {"title": "Soft dependency constraints for reordering in hierarchical phrase-based translation", "author": ["Y. Gao", "P. Koehn", "A. Birch"], "venue": "Proceedings of EMNLP 2011, 857\u2013868.", "citeRegEx": "Gao et al\\.,? 2011", "shortCiteRegEx": "Gao et al\\.", "year": 2011}, {"title": "Automatically learning source-side reordering rules for large scale machine translation", "author": ["D. Genzel"], "venue": "Proceedings of COLING 2010, 376\u2013384.", "citeRegEx": "Genzel,? 2010", "shortCiteRegEx": "Genzel", "year": 2010}, {"title": "Phrase dependency machine translation with quasi-synchronous tree-to-tree features", "author": ["K. Gimpel", "N.A. Smith"], "venue": "Computational Linguistics 40(2):349\u2013401.", "citeRegEx": "Gimpel and Smith,? 2014", "shortCiteRegEx": "Gimpel and Smith", "year": 2014}, {"title": "Syntactic preprocessing for statistical machine translation", "author": ["N. Habash"], "venue": "Proceedings of MT Summit XI, 215\u2013 222.", "citeRegEx": "Habash,? 2007", "shortCiteRegEx": "Habash", "year": 2007}, {"title": "Tuning as ranking", "author": ["M. Hopkins", "J. May"], "venue": "Proceedings of EMNLP 2011, 1352\u20131362.", "citeRegEx": "Hopkins and May,? 2011", "shortCiteRegEx": "Hopkins and May", "year": 2011}, {"title": "Exploiting syntactic relationships in a phrase-based decoder: an exploration", "author": ["T. Hunter", "P. Resnik"], "venue": "Machine Translation 24(2):123\u2013140.", "citeRegEx": "Hunter and Resnik,? 2010", "shortCiteRegEx": "Hunter and Resnik", "year": 2010}, {"title": "Source-side preordering for translation using logistic regression and depth-first branch-and-bound search", "author": ["L. Jehl", "A. de Gispert", "M. Hopkins", "W. Byrne"], "venue": "In Proceedings of EACL", "citeRegEx": "Jehl et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jehl et al\\.", "year": 2014}, {"title": "Syntax-based reordering for statistical machine translation", "author": ["M. Khalilov", "J.A.R. Fonollosa"], "venue": "Computer Speech and Language 25(4):761\u2013788.", "citeRegEx": "Khalilov and Fonollosa,? 2011", "shortCiteRegEx": "Khalilov and Fonollosa", "year": 2011}, {"title": "Edinburgh system description for the 2005 IWSLT speech translation evaluation", "author": ["P. Koehn", "A. Axelrod", "A.B. Mayne", "C. Callison-Burch", "M. Osborne", "D. Talbot"], "venue": "Proceedings of IWSLT 2005.", "citeRegEx": "Koehn et al\\.,? 2005", "shortCiteRegEx": "Koehn et al\\.", "year": 2005}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of the ACL 2007 Demo and Poster Sessions, 177\u2013180.", "citeRegEx": "Zens et al\\.,? 2007", "shortCiteRegEx": "Zens et al\\.", "year": 2007}, {"title": "Statistical phrasebased translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "Proceedings of HLT-NAACL 2003, 48\u2013", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Pharaoh: A beam search decoder for phrase-based statistical machine translation models", "author": ["P. Koehn"], "venue": "Proceedings of AMTA 2004, 115\u2013124.", "citeRegEx": "Koehn,? 2004a", "shortCiteRegEx": "Koehn", "year": 2004}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["P. Koehn"], "venue": "Proceedings of EMNLP 2004, 388\u2013395.", "citeRegEx": "Koehn,? 2004b", "shortCiteRegEx": "Koehn", "year": 2004}, {"title": "Minimum Bayes-Risk decoding for statistical machine translation", "author": ["S. Kumar", "W. Byrne"], "venue": "Proceedings of HLT-NAACL 2004, 169\u2013176.", "citeRegEx": "Kumar and Byrne,? 2004", "shortCiteRegEx": "Kumar and Byrne", "year": 2004}, {"title": "Source-side classifier preordering for machine translation", "author": ["U. Lerner", "S. Petrov"], "venue": "Proceedings of EMNLP 2013, 512\u2013523.", "citeRegEx": "Lerner and Petrov,? 2013", "shortCiteRegEx": "Lerner and Petrov", "year": 2013}, {"title": "A probabilistic approach to syntax-based reordering for statistical machine translation", "author": ["C.-H. Li", "D. Zhang", "M. Li", "M. Zhou", "M. Li", "Y. Guan"], "venue": "Proceedings of ACL 2007, 720\u2013727.", "citeRegEx": "Li et al\\.,? 2007", "shortCiteRegEx": "Li et al\\.", "year": 2007}, {"title": "A maximum entropy approach to Chinese word segmentation", "author": ["J.K. Low", "H.T. Ng", "W. Guo"], "venue": "Proceedings of SIGHAN4, 161\u2013164.", "citeRegEx": "Low et al\\.,? 2005", "shortCiteRegEx": "Low et al\\.", "year": 2005}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney"], "venue": "Computational Linguistics 29(1):19\u201351.", "citeRegEx": "Och and Ney,? 2003", "shortCiteRegEx": "Och and Ney", "year": 2003}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of ACL-02, 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A unigram orientation model for statistical machine translation", "author": ["C. Tillmann"], "venue": "Proceedings of HLT-NAACL 2004: Short Papers, 101\u2013104.", "citeRegEx": "Tillmann,? 2004", "shortCiteRegEx": "Tillmann", "year": 2004}, {"title": "Chinese syntactic reordering for statistical machine translation", "author": ["C. Wang", "M. Collins", "P. Koehn"], "venue": "Proceedings of EMNLP-CoNLL 2007, 737\u2013745.", "citeRegEx": "Wang et al\\.,? 2007", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Improving a statistical MT system with automatically learned rewrite patterns", "author": ["F. Xia", "M. McCord"], "venue": "Proceedings of COLING 2004, 508\u2013514.", "citeRegEx": "Xia and McCord,? 2004", "shortCiteRegEx": "Xia and McCord", "year": 2004}, {"title": "Maximum entropy based phrase reordering model for statistical machine translation", "author": ["D. Xiong", "Q. Liu", "S. Lin"], "venue": "Proceedings of COLING/ACL 2006, 521\u2013528.", "citeRegEx": "Xiong et al\\.,? 2006", "shortCiteRegEx": "Xiong et al\\.", "year": 2006}, {"title": "Using a dependency parser to improve SMT for subject-object-verb languages", "author": ["P. Xu", "J. Kang", "M. Ringgaard", "F. Och"], "venue": "Proceedings of NAACL HLT 2009, 245\u2013253.", "citeRegEx": "Xu et al\\.,? 2009", "shortCiteRegEx": "Xu et al\\.", "year": 2009}, {"title": "A rankingbased approach to word reordering for statistical machine translation", "author": ["N. Yang", "M. Li", "D. Zhang", "N. Yu"], "venue": "Proceedings of ACL 2012, 912\u2013920.", "citeRegEx": "Yang et al\\.,? 2012", "shortCiteRegEx": "Yang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 28, "context": "Phrase-based MT systems (Koehn, Och, and Marcu 2003) generally adopt a reordering model that predicts reordering based on the span of a phrase and that of the adjacent phrase (Tillmann 2004; Xiong, Liu, and Lin 2006; Galley and Manning 2008; Cherry 2013).", "startOffset": 175, "endOffset": 254}, {"referenceID": 8, "context": "Phrase-based MT systems (Koehn, Och, and Marcu 2003) generally adopt a reordering model that predicts reordering based on the span of a phrase and that of the adjacent phrase (Tillmann 2004; Xiong, Liu, and Lin 2006; Galley and Manning 2008; Cherry 2013).", "startOffset": 175, "endOffset": 254}, {"referenceID": 5, "context": "Phrase-based MT systems (Koehn, Och, and Marcu 2003) generally adopt a reordering model that predicts reordering based on the span of a phrase and that of the adjacent phrase (Tillmann 2004; Xiong, Liu, and Lin 2006; Galley and Manning 2008; Cherry 2013).", "startOffset": 175, "endOffset": 254}, {"referenceID": 32, "context": "Dependency-based pre-ordering can be performed either by a rule-based approach based on manually specified human linguistic knowledge (Xu et al. 2009; Cai et al. 2014), or by a learning approach (Xia and McCord 2004; Habash 2007; Genzel 2010; Yang et al.", "startOffset": 134, "endOffset": 167}, {"referenceID": 2, "context": "Dependency-based pre-ordering can be performed either by a rule-based approach based on manually specified human linguistic knowledge (Xu et al. 2009; Cai et al. 2014), or by a learning approach (Xia and McCord 2004; Habash 2007; Genzel 2010; Yang et al.", "startOffset": 134, "endOffset": 167}, {"referenceID": 4, "context": "Dependency parsing has also been used in reordering approaches integrated with decoding to determine the next source phrase to translate after translating the current source phrase (Cherry 2008; Bach, Gao, and Vogel 2009; Chang et al. 2009).", "startOffset": 181, "endOffset": 240}, {"referenceID": 3, "context": "Dependency parsing has also been used in reordering approaches integrated with decoding to determine the next source phrase to translate after translating the current source phrase (Cherry 2008; Bach, Gao, and Vogel 2009; Chang et al. 2009).", "startOffset": 181, "endOffset": 240}, {"referenceID": 2, "context": "We propose sparse feature functions based on the pre-ordering rules of (Cai et al. 2014).", "startOffset": 71, "endOffset": 88}, {"referenceID": 2, "context": "However, in contrast to the manual rule-based pre-ordering approach of (Cai et al. 2014), the weights of our sparse feature functions are automatically learned and used during the actual translation process, without an explicit pre-ordering step.", "startOffset": 71, "endOffset": 88}, {"referenceID": 20, "context": "It is performed by a beam search algorithm (Koehn 2004a), in which the search process produces translation from left to right in the translation output.", "startOffset": 43, "endOffset": 56}, {"referenceID": 2, "context": "We define our dependency swap features, following the rule template definition for dependency swap rules (Cai et al. 2014), which defines rule instances based on the word pairs with head-child or sibling relationship.", "startOffset": 105, "endOffset": 122}, {"referenceID": 2, "context": "We define our dependency swap features, following the rule template definition for dependency swap rules (Cai et al. 2014), which defines rule instances based on the word pairs with head-child or sibling relationship. However, the difference is that our approach does not require manually specifying which dependency labels are the conditions to swap words, but learns them automatically. In our approach, each rule instance based on the above template becomes a Boolean sparse feature function (Chiang, Knight, and Wang 2009). The function parameters are the word pair specification and output order. While Cai et al. (2014) defined the rules only by the dependency labels, we define our feature functions for each word pair by the dependency labels, the POS tags, and the combination of both, resulting in a group of four feature functions for every word pair ordering.", "startOffset": 106, "endOffset": 626}, {"referenceID": 4, "context": "To encourage the translation output that conforms to the dependency parse structure, we impose a penalty feature that discourages translation candidates not conforming to the dependency parse subtree (Cherry 2008).", "startOffset": 200, "endOffset": 213}, {"referenceID": 20, "context": "Phrase-based SMT beam search decoding involves future cost, not accumulated over the translation hypotheses but used to reduce search space (Koehn 2004a).", "startOffset": 140, "endOffset": 153}, {"referenceID": 4, "context": "This follows the assumption that each subtree will have a contiguous translation (Cherry 2008).", "startOffset": 81, "endOffset": 94}, {"referenceID": 5, "context": "We incorporate sparse reordering orientation features (Cherry 2013).", "startOffset": 54, "endOffset": 67}, {"referenceID": 8, "context": "Cherry (2013) designed his sparse reordering orientation model following the hierarchical reordering (HR) model (Galley and Manning 2008), capturing the relative position of the current phrase (covered by the current hypothesis) with respect to the largest chunk of contiguous source phrases that form a contiguous translation before this phrase.", "startOffset": 112, "endOffset": 137}, {"referenceID": 3, "context": "We also utilize dependency path features (Chang et al. 2009) for phrase-based SMT, defined over the shortest path of dependency parse tree links bridging the current and the previous source phrase.", "startOffset": 41, "endOffset": 60}, {"referenceID": 3, "context": "We also utilize dependency path features (Chang et al. 2009) for phrase-based SMT, defined over the shortest path of dependency parse tree links bridging the current and the previous source phrase. Chang et al. (2009) defined the dependency path features on a maximum entropy phrase orientation classifier, trained on their word-aligned parallel text and labeled by the two possible phrase orderings in the translation output: in-order and swapped.", "startOffset": 42, "endOffset": 218}, {"referenceID": 26, "context": "Then the parallel corpus is word-aligned by GIZA++ (Och and Ney 2003) using IBM Models 1, 3, and 4 (Brown et al.", "startOffset": 51, "endOffset": 69}, {"referenceID": 1, "context": "Then the parallel corpus is word-aligned by GIZA++ (Och and Ney 2003) using IBM Models 1, 3, and 4 (Brown et al. 1993)5.", "startOffset": 99, "endOffset": 118}, {"referenceID": 13, "context": "Weight tuning is done by using the pairwise ranked optimization (PRO) algorithm (Hopkins and May 2011), which is also used to obtain weights of the sparse features to help determine the reordering.", "startOffset": 80, "endOffset": 102}, {"referenceID": 28, "context": "We build a phrase-based baseline SMT system, which uses non-sparse phrase-based lexicalized reordering (PBLR), in which the reordering probability depends on the phrase being translated and its position with respect to the source position of the previously translated phrase (Tillmann 2004; Koehn et al. 2005), and non-sparse hierarchical reordering (HR), in which the previous unit is not only the previous phrase, but the largest chunk of contiguous source phrases", "startOffset": 275, "endOffset": 309}, {"referenceID": 17, "context": "We build a phrase-based baseline SMT system, which uses non-sparse phrase-based lexicalized reordering (PBLR), in which the reordering probability depends on the phrase being translated and its position with respect to the source position of the previously translated phrase (Tillmann 2004; Koehn et al. 2005), and non-sparse hierarchical reordering (HR), in which the previous unit is not only the previous phrase, but the largest chunk of contiguous source phrases", "startOffset": 275, "endOffset": 309}, {"referenceID": 4, "context": "Dataset Base (Cherry 2008) (Chang et al.", "startOffset": 13, "endOffset": 26}, {"referenceID": 3, "context": "Dataset Base (Cherry 2008) (Chang et al. 2009) +DDP+Path (Cherry 2013) Ours +DDP +Path +SHR +DDP+DS +DDP+Path+DS", "startOffset": 27, "endOffset": 46}, {"referenceID": 5, "context": "2009) +DDP+Path (Cherry 2013) Ours +DDP +Path +SHR +DDP+DS +DDP+Path+DS", "startOffset": 16, "endOffset": 29}, {"referenceID": 4, "context": ", dependency distortion penalty (+DDP) (Cherry 2008), sparse dependency path features (+Path) (Chang et al.", "startOffset": 39, "endOffset": 52}, {"referenceID": 3, "context": ", dependency distortion penalty (+DDP) (Cherry 2008), sparse dependency path features (+Path) (Chang et al. 2009), the combination of both (+DDP+Path), as well as sparse reordering orientation features (+SHR) (Cherry 2013).", "startOffset": 94, "endOffset": 113}, {"referenceID": 5, "context": "2009), the combination of both (+DDP+Path), as well as sparse reordering orientation features (+SHR) (Cherry 2013).", "startOffset": 101, "endOffset": 114}, {"referenceID": 8, "context": "having contiguous translation (Galley and Manning 2008).", "startOffset": 30, "endOffset": 55}, {"referenceID": 22, "context": "We also use n-best Minimum Bayes Risk (MBR) decoding (Kumar and Byrne 2004) instead of the default maximum a-posteriori (MAP) decoding.", "startOffset": 53, "endOffset": 75}, {"referenceID": 4, "context": "To accommodate our sparse feature functions, our Moses code has been modified to read dependency-parsed input sentences and incorporate additional decoding features on top of our baseline, namely dependency distortion penalty (DDP) (Cherry 2008), sparse dependency path features (Path) (Chang et al.", "startOffset": 232, "endOffset": 245}, {"referenceID": 3, "context": "To accommodate our sparse feature functions, our Moses code has been modified to read dependency-parsed input sentences and incorporate additional decoding features on top of our baseline, namely dependency distortion penalty (DDP) (Cherry 2008), sparse dependency path features (Path) (Chang et al. 2009), sparse reordering orientation following hierarchical reordering orientation (SHR) (Cherry 2013), and our sparse dependency swap features (DS).", "startOffset": 286, "endOffset": 305}, {"referenceID": 5, "context": "2009), sparse reordering orientation following hierarchical reordering orientation (SHR) (Cherry 2013), and our sparse dependency swap features (DS).", "startOffset": 89, "endOffset": 102}, {"referenceID": 27, "context": "The translation quality of the system outputs is measured by case-insensitive BLEU (Papineni et al. 2002), for which the brevity penalty is computed based on the shortest reference (NIST-BLEU)7.", "startOffset": 83, "endOffset": 105}, {"referenceID": 21, "context": "Statistical significance testing between systems is conducted by bootstrap resampling (Koehn 2004b).", "startOffset": 86, "endOffset": 99}, {"referenceID": 5, "context": "Sparse reordering orientation features (Cherry 2013) leverage this by capturing the previous phrase (or contiguous chunk) properties.", "startOffset": 39, "endOffset": 52}, {"referenceID": 4, "context": "This is also the case with the system added with prior reordering approaches, which includes DDP (Cherry 2008), Path (Chang et al.", "startOffset": 97, "endOffset": 110}, {"referenceID": 3, "context": "This is also the case with the system added with prior reordering approaches, which includes DDP (Cherry 2008), Path (Chang et al. 2009), their combination DDP+Path, and SHR (Cherry 2013).", "startOffset": 117, "endOffset": 136}, {"referenceID": 5, "context": "2009), their combination DDP+Path, and SHR (Cherry 2013).", "startOffset": 43, "endOffset": 56}, {"referenceID": 4, "context": "Base, +DDP (Cherry 2008), +Path (Chang et al.", "startOffset": 11, "endOffset": 24}, {"referenceID": 3, "context": "Base, +DDP (Cherry 2008), +Path (Chang et al. 2009): identical output The export of high-tech products in Guangdong Province is the", "startOffset": 32, "endOffset": 51}, {"referenceID": 5, "context": "+SHR (Cherry 2013)", "startOffset": 5, "endOffset": 18}, {"referenceID": 3, "context": "Combining our sparse dependency swap features with sparse dependency path features (Chang et al. 2009) achieves the best experimental result.", "startOffset": 83, "endOffset": 102}, {"referenceID": 11, "context": "Another work exploits source and target dependency trees for phrase-based MT output reranking (Gimpel and Smith 2014), instead of for translation decoding.", "startOffset": 94, "endOffset": 117}, {"referenceID": 24, "context": "Prior work has also used constituency parse to guide reordering, by manually defining pre-ordering rules to reorder input sentences into the target language order before translation (Collins, Koehn, and Ku\u010derov\u00e1 2005; Wang, Collins, and Koehn 2007), or to automatically learn those rules (Li et al. 2007; Khalilov and Fonollosa 2011).", "startOffset": 288, "endOffset": 333}, {"referenceID": 16, "context": "Prior work has also used constituency parse to guide reordering, by manually defining pre-ordering rules to reorder input sentences into the target language order before translation (Collins, Koehn, and Ku\u010derov\u00e1 2005; Wang, Collins, and Koehn 2007), or to automatically learn those rules (Li et al. 2007; Khalilov and Fonollosa 2011).", "startOffset": 288, "endOffset": 333}, {"referenceID": 3, "context": "Chang et al. (2009) introduced dependency path as a soft constraint based on the sequence of source dependency links traversed in phrase-based translation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Chang et al. (2009) introduced dependency path as a soft constraint based on the sequence of source dependency links traversed in phrase-based translation. It is used as features on a maximum entropy phrase orientation classifier, whose probability output is used as a decoding feature function. As the path can be arbitrarily long, it may not be represented sufficiently in the training samples. Our sparse feature definition can alleviate this as features are defined on two words. In addition, capturing word pairs instead of paths enables incorporation of other word properties such as POS tags. Hunter and Resnik (2010) proposed a probability model to capture the offset of a word with respect to its head position in phrase-based MT.", "startOffset": 0, "endOffset": 625}, {"referenceID": 3, "context": "Chang et al. (2009) introduced dependency path as a soft constraint based on the sequence of source dependency links traversed in phrase-based translation. It is used as features on a maximum entropy phrase orientation classifier, whose probability output is used as a decoding feature function. As the path can be arbitrarily long, it may not be represented sufficiently in the training samples. Our sparse feature definition can alleviate this as features are defined on two words. In addition, capturing word pairs instead of paths enables incorporation of other word properties such as POS tags. Hunter and Resnik (2010) proposed a probability model to capture the offset of a word with respect to its head position in phrase-based MT. Their model does not take into account two sibling words sharing the same head. They reported negative result. Meanwhile, Gao, Koehn, and Birch (2011) defined soft constraints on hierarchical phrase-based MT, which produce translation by bottom-up constituency parsing algorithm instead of beam search.", "startOffset": 0, "endOffset": 891}], "year": 2016, "abstractText": "Reordering poses a major challenge in machine translation (MT) between two languages with significant differences in word order. In this paper, we present a novel reordering approach utilizing sparse features based on dependency word pairs. Each instance of these features captures whether two words, which are related by a dependency link in the source sentence dependency parse tree, follow the same order or are swapped in the translation output. Experiments on Chinese-to-English translation show a statistically significant improvement of 1.21 BLEU point using our approach, compared to a state-of-the-art statistical MT system that incorporates prior reordering approaches.", "creator": "TeX"}}}