{"id": "1608.04773", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "Faster Principal Component Regression and Stable Matrix Chebyshev Approximation", "abstract": "see neglected principle component constraints ( sec ) unfortunately providing an efficient algorithm does take any frames representing the subspace formed by the top principle components of parallel region. our algorithm does not require only direct construction beyond the top principle components, neither therefore is suitable for large - family pcr data.", "histories": [["v1", "Tue, 16 Aug 2016 20:48:02 GMT  (2532kb,D)", "https://arxiv.org/abs/1608.04773v1", null], ["v2", "Mon, 24 Apr 2017 19:35:38 GMT  (2557kb,D)", "http://arxiv.org/abs/1608.04773v2", "title changed and minor revisions"]], "reviews": [], "SUBJECTS": "stat.ML cs.DS cs.LG math.NA math.OC", "authors": ["zeyuan allen-zhu", "yuanzhi li"], "accepted": true, "id": "1608.04773"}, "pdf": {"name": "1608.04773.pdf", "metadata": {"source": "CRF", "title": "Faster Principal Component Regression and Stable Matrix Chebyshev Approximation", "authors": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "emails": ["zeyuan@csail.mit.edu", "yuanzhil@cs.princeton.edu"], "sections": [{"heading": null, "text": "We solve principal component regression (PCR), up to a multiplicative accuracy 1 + \u03b3, by reducing the problem to O\u0303(\u03b3\u22121) black-box calls of ridge regression. Therefore, our algorithm does not require any explicit construction of the top principal components, and is suitable for large-scale PCR instances. In contrast, previous result requires O\u0303(\u03b3\u22122) such black-box calls.\nWe obtain this result by developing a general stable recurrence formula for matrix Chebyshev polynomials, and a degree-optimal polynomial approximation to the matrix sign function. Our techniques may be of independent interests, especially when designing iterative methods."}, {"heading": "1 Introduction", "text": "In machine learning and statistics, it is often desirable to represent a large-scale dataset in a more tractable, lower-dimensional form, without losing too much information. One of the most robust ways to achieve this goal is through principal component projection (PCP):\nPCP: project vectors onto the span of the top principal components of the a matrix.\nIt is well-known that PCP decreases noise and increases efficiency in downstream tasks. One of the main applications is principal component regression (PCR):\nPCR: linear regression but restricted to the subspace of top principal components.\nClassical algorithms for PCP or PCR rely on a principal component analysis (PCA) solver to recover the top principal components first; with these components available, the tasks of PCP and PCR become trivial because the projection matrix can be constructed explicitly.\nUnfortunately, PCA solvers demand a running time that at least linearly scales with the number of top principal components chosen for the projection. For instance, to project a vector onto the top 1000 principal components of a high-dimensional dataset, even the most efficient Krylov-based [18] or Lanczos-based [4] methods require a running time that is proportional to 1000 \u00d7 40 = 4 \u00d7 104 times the input matrix sparsity, if the Krylov or Lanczos method is executed for 40 iterations. This is usually computationally intractable.\nar X\niv :1\n60 8.\n04 77\n3v 2\n[ st\nat .M\nL ]\n2 4\nA pr"}, {"heading": "1.1 Approximating PCP Without PCA", "text": "In this paper, we propose the following notion of PCP approximation. Given a data matrix A \u2208 Rd\u2032\u00d7d (with singular values no greater than 1) and a threshold \u03bb > 0, we say that an algorithm solves (\u03b3, \u03b5)-approximate PCP if \u2014informally speaking and up to a multiplicative 1\u00b1 \u03b5 error\u2014 it projects (see Def. 3.1 for a formal definition)\n1. any eigenvector \u03bd of A>A with value in [ \u03bb(1 + \u03b3), 1 ] to \u03bd, 2. any eigenvector \u03bd of A>A with value in [ 0, \u03bb(1\u2212 \u03b3) ] to ~0, 3. any eigenvector \u03bd of A>A with value in [ \u03bb(1\u2212 \u03b3), \u03bb(1 + \u03b3) ] to \u201canywhere between ~0 and \u03bd.\u201d\nSuch a definition also extends to (\u03b3, \u03b5)-approximate PCR (see Def. 3.2). It was first noticed by Frostig et al. [13] that approximate PCP and PCR be solved with a running time independent of the number of principal components above threshold \u03bb. More specifically, they reduced (\u03b3, \u03b5)-approximate PCP and PCR to\nO ( \u03b3\u22122 log(1/\u03b5) ) black-box calls of any ridge regression subroutine\nwhere each call computes (A>A + \u03bbI)\u22121u for some vector u.1 Our main focus of this paper is to quadratically improve this performance and reduce PCP and PCR to\nO ( \u03b3\u22121 log(1/\u03b3\u03b5) ) black-box calls of any ridge regression subroutine\nwhere each call again computes (A>A + \u03bbI)\u22121u.\nRemark 1.1. Frostig et al. only showed their algorithm satisfies the properties 1 and 2 of (\u03b3, \u03b5)approximation (but not the property 3), and thus their proof was only for matrix A with no singular value in the range [ \u221a \u03bb(1\u2212 \u03b3), \u221a \u03bb(1 + \u03b3)]. This is known as the eigengap assumption, which is rarely satisfied in practice [18]. In this paper, we prove our result both with and without such eigengap assumption. Since our techniques also imply the algorithm of Frostig et al. satisfies property 3, throughout the paper, we say Frostig et al. solve (\u03b3, \u03b5)-approximate PCP and PCR."}, {"heading": "1.2 From PCP to Polynomial Approximation", "text": "The main technique of Frostig et al. is to construct a polynomial to approximate the sign function sgn(x) : [\u22121, 1]\u2192 {\u00b11}:\nsgn(x) def = { +1, x \u2265 0; \u22121, x < 0.\nIn particular, given any polynomial g(x) satisfying \u2223\u2223g(x)\u2212 sgn(x)\n\u2223\u2223 \u2264 \u03b5 \u2200x \u2208 [\u22121,\u2212\u03b3] \u222a [\u03b3, 1] , and (1.1)\u2223\u2223g(x) \u2223\u2223 \u2264 1 \u2200x \u2208 [\u2212\u03b3, \u03b3] , (1.2)\nthe problem of (\u03b3, \u03b5)-approximate PCP can be reduced to computing the matrix polynomial g(S) for S def = (A>A + \u03bbI)\u22121(A>A\u2212 \u03bbI) (cf. Fact 7.1). In other words,\n\u2022 to project any vector \u03c7 \u2208 Rd to top principal components, we can compute g(S)\u03c7 instead; and 1Ridge regression is often considered as an easy-to-solve machine learning problem: using for instance SVRG [17],\none can usually solve ridge regression to an 10\u22128 accuracy with at most 40 passes of the data.\n\u2022 to compute g(S)\u03c7, we can reduce it to ridge regression for each evaluation of Su for some vector u.\nRemark 1.2. Since the transformation from A>A to S is not linear, the final approximation to the PCP is a rational function (as opposed to a polynomial) over A>A. We restrict to polynomial choices of g(\u00b7) because in this way, the final rational function has all the denominators being A>A + \u03bbI, thus reduces to ridge regressions. Remark 1.3. The transformation from A>A to S ensures that all the eigenvalues of A>A in the range (1\u00b1 \u03b3)\u03bb roughly map to the eigenvalues of S in the range [\u2212\u03b3, \u03b3].\nMain Challenges. There are two main challenges regarding the design of polynomial g(x).\n\u2022 Efficiency. We wish to minimize the degree n = deg(g(x)) because the computation of g(S)\u03c7 usually requires n calls of ridge regression.\n\u2022 Stability. We wish g(x) to be stable; that is, g(S)\u03c7 must be given by a recursive formula where if we make \u03b5\u2032 error in each recursion (due to error incurred from ridge regression), the final error of g(S)\u03c7 must be at most \u03b5\u2032 \u00d7 poly(d).\nRemark 1.4. Efficient routines such as SVRG [17] solve ridge regression and thus compute Su for any u \u2208 Rd, with running times only logarithmically in 1/\u03b5\u2032. Therefore, by setting \u03b5\u2032 = \u03b5/poly(d), one can blow up the running time by a small factor O(log(d)) in order to obtain an \u03b5-accurate solution for g(S)\u03c7.\nThe polynomial g(x) constructed by Frostig et al. comes from truncated Taylor expansion. It has degree O ( \u03b3\u22122 log(1/\u03b5) ) and is stable. This \u03b3\u22122 dependency limits the practical performance of their proposed PCP and PCR algorithms, especially in a high accuracy regime. At the same time,\n\u2022 the optimal degree for a polynomial to satisfy even only (1.1) is \u0398 ( \u03b3\u22121 log(1/\u03b5) ) [9, 10].\nFrostig et al. were unable to find a stable polynomial matching this optimal degree and left it as open question.2"}, {"heading": "1.3 Our Results and Main Ideas", "text": "We provide an efficient and stable polynomial approximation to the matrix sign function that has a near-optimal degree O(\u03b3\u22121 log(1/\u03b3\u03b5)). At a high level, we construct a polynomial q(x) that approximately equals (\n1+\u03ba\u2212x 2\n)\u22121/2 for some \u03ba = \u0398(\u03b32); then we set g(x) def = x \u00b7 q(1 +\u03ba\u2212 2x2) which\napproximates sgn(x).\nTo construct q(x), we first note that (\n1+\u03ba\u2212x 2\n)\u22121/2 has no singular point on [\u22121, 1] so we can\napply Chebyshev approximation theory to obtain some q(x) of degree O(\u03b3\u22121 log(1/\u03b3\u03b5)) satisfying \u2223\u2223\u2223q(x)\u2212\n(1 + \u03ba\u2212 x 2 )\u22121/2\u2223\u2223\u2223 \u2264 \u03b5 for every x \u2208 [\u22121, 1] .\nThis can be shown to imply \u2223\u2223g(x)\u2212 sgn(x) \u2223\u2223 \u2264 \u03b5 for every x \u2208 [\u22121,\u2212\u03b3] \u222a [\u03b3, 1], so (1.1) is satisfied. In order to prove (1.2) (i.e., \u2223\u2223g(x) \u2223\u2223 \u2264 1 for every x \u2208 [\u2212\u03b3, \u03b3]) , we prove a separate lemma:3\nq(x) \u2264 (1 + \u03ba\u2212 x\n2\n)\u22121/2 for every x \u2208 [1, 1 + \u03ba] .\n2Using degree reduction, Frostig et al. found an explicit polynomial g(x) of degree O ( \u03b3\u22121 log(1/\u03b3\u03b5) ) satisfying (1.1). However, that polynomial is unstable because it is constructed monomial by monomial and has exponentially large coefficients in front of each monomial. Furthermore, it is not clear if their polynomial satisfies the (1.2).\n3We proved a general lemma which holds for any function whose all orders of derivatives are non-negative at x = 0.\nNote that this does not follow from standard Chebyshev theory because Chebyshev approximation guarantees are only with respect to x \u2208 [\u22121, 1] and do not extend to singular point x = 1 + \u03ba.\nThis proves the \u201cefficiency\u201d part of the main challenges discussed earlier. As for the \u201cstability\u201d part, we prove a general theorem regarding any weighted sum of Chebyshev polynomials applied to matrices. We provide a backward recurrence algorithm and show that it is stable under noisy computations. This may be of independent interest.\nFor interested readers, we compare our polynomial q(x) with that of Frostig et al. in Figure 1."}, {"heading": "1.4 Related Work", "text": "There are a few attempts to reduce the cost of PCA when solving PCR, by for instance approximating the matrix AP\u03bb where P\u03bb is the PCP projection matrix [6, 7]. However, they cost a running time that linearly scales with the number of principal components above \u03bb.\nA significant number of papers have focused on the low-rank case of PCA [2, 4, 18] and its online variant [3]. Unfortunately, all of these methods require a running time that scales at least linearly with respect to the number of top principal components.\nMore related to this paper is work on matrix sign function, which plays an important role in control theory and quantum chromodynamics. Several results have addressed Krylov methods for applying the sign function in the so-called Krylov subspace, without explicitly constructing any approximate polynomial [21, 24]. However, Krylov methods are not (\u03b3, \u03b5)-approximate PCP solvers, and there is no supporting stability theory behind them.4 Other iterative methods have also been proposed, see Section 5 of textbook [16]. For instance, Schur\u2019s method is a slow one and also requires the matrix to be explicitly given. The Newton\u2019s iteration and its numerous variants (e.g. [19]) provide rational approximations to the matrix sign function as opposed to polynomial approximations. Our result and Frostig et al. [13] differ from these cited works, because we have only accessed an approximate ridge regression oracle, so ensuring a polynomial approximation to the sign function and ensuring its stability are crucial.\nUsing matrix Chebyshev polynomials to approximate matrix functions is not new. Perhaps the most celebrated example is to approximate S\u22121 using polynomials on S, used in the analysis of conjugate gradient [22]. Independent from this paper,5 Han et al. [15] used Chebyshev polynomials to approximate the trace of the matrix sign function, i.e., Tr(sgn(S)), which is similar but a different problem.6 Also, they did not study the case when the matrix-vector multiplication oracle is only approximate (like we do in this paper), or the case when S has eigenvalues in the range [\u2212\u03b3, \u03b3].\n4We anyways have included Krylov method in our empirical evaluation section and shall discuss its performance there, see for instance Remark 8.1.\n5Their paper appeared online two months before us, and we became aware of their work in March 2017. 6In particular, their degree of the Chebyshev polynomial is O ( \u03b3\u22121(log2(1/\u03b3) + log(1/\u03b3) log(1/\u03b5)) ) in the language\nof this paper; in contrast, we have degree O ( \u03b3\u22121 log(1/\u03b3\u03b5) ) .\nRoadmap.\n\u2022 In Section 2, we provide notions for this paper and basics for Chebyshev polynomials \u2022 In Section 3, we put forward our formal definitions for approximate PCP and PCR, and show\na reduction from approximate PCR to approximate PCP.\n\u2022 In Section 4, we prove a general lemma regarding Chebyshev approximations outside [\u22121, 1]. \u2022 In Section 5, we design our polynomial approximation to sgn(x). \u2022 In Section 6, we show how to stably compute any weighted sum of Chebyshev polynomials. \u2022 In Section 7, we provide pseudocode and prove our main theorems regarding PCP and PCR. \u2022 In Section 8, we provide empirical evaluations of our theory."}, {"heading": "2 Preliminaries", "text": "We denote by 1[e] \u2208 {0, 1} the indicator function for event e, by \u2016v\u2016 or \u2016v\u20162 the Euclidean norm of a vector v, by M\u2020 the Moore-Penrose pseudo-inverse of a symmetric matrix M, and by \u2016M\u20162 its spectral norm. We sometimes use ~v to emphasize that v is a vector.\nGiven a symmetric d \u00d7 d matrix M and any f : R \u2192 R, f(M) is the matrix function applied to M, which is equal to Udiag{f(D1), . . . , f(Dd)}U> if M = Udiag{D1, . . . , Dd}U> is its eigendecomposition.\nThroughout the paper, matrix A is of dimension d\u2032 \u00d7 d. We denote by \u03c3max(A) the largest singular value of A. Following the tradition of [13] and keeping the notations light, we assume without loss of generality that \u03c3max(A) \u2264 1. We are interested in PCP and PCR problems with an eigenvalue threshold \u03bb \u2208 (0, 1).\nThroughout the paper, we denote by \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbd \u2265 0 the eigenvalues of A>A, and by \u03bd1, . . . , \u03bdd \u2208 Rd the eigenvectors of A>A corresponding to \u03bb1, . . . , \u03bbd. We denote by P\u03bb the projection matrix P\u03bb def = (\u03bd1, . . . , \u03bdj)(\u03bd1, . . . , \u03bdj)\n> where j is the largest index satisfying \u03bbj \u2265 \u03bb. In other words, P\u03bb is a projection matrix to the eigenvectors of A >A with eigenvalues \u2265 \u03bb.\nDefinition 2.1. The principal component projection (PCP) of \u03c7 \u2208 Rd at threshold \u03bb is \u03be\u2217 = P\u03bb\u03c7. Definition 2.2. The principal component regression (PCR) of regressand b \u2208 Rd\u2032 at threshold \u03bb is x\u2217 = arg min\ny\u2208Rd \u2016AP\u03bby \u2212 b\u20162 or equivalently x\u2217 = (A>A)\u2020P\u03bb(A>b) ."}, {"heading": "2.1 Ridge Regression", "text": "Definition 2.3. A black-box algorithm ApxRidge(A, \u03bb, u) is an \u03b5-approximate ridge regression solver, if for every u \u2208 Rd, it satisfies \u2016ApxRidge(A, \u03bb, u)\u2212 (A>A + \u03bbI)\u22121u\u2016 \u2264 \u03b5\u2016u\u2016.7\nRidge regression is equivalent to solving well-conditioned linear systems, or minimizing strongly convex and smooth objectives f(y)\ndef = 12y >(A>A + \u03bbI)y \u2212 u>y. Remark 2.4. There is huge literature on efficient algorithms solving ridge regression. Most notably,\n(1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods;\n(2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and\n(3) NUACDM [5] gives the fastest coordinate-descent method.\n7In fact, throughout the paper, we only need ApxRidge to satisfy this property with high probability for each u.\nThe running time of (1) is O(nnz(A)\u03bb\u22121/2 log(1/\u03b5)) where nnz(A) is time to multiply A to any vector. The running times of (2) and (3) depend on structural properties of A and are always faster than (1).\nBecause the best complexity of ridge regression depends on the structural properties of A, following Frostig et al., we only compute our running time in terms of the \u201cnumber of black-box calls\u201d to a ridge regression solver."}, {"heading": "2.2 Chebyshev Polynomials", "text": "Definition 2.5. Chebyshev polynomials of 1st and 2nd kind are {Tn(x)}n\u22650 and {Un(x)}n\u22650 where\nT0(x) def= 1, T1(x) def= x, Tn+1(x) def= 2x \u00b7 Tn(x)\u2212 Tn\u22121(x) U0(x) def= 1, U1(x) def= 2x, Un+1(x) def= 2x \u00b7 Un(x)\u2212 Un\u22121(x)\nFact 2.6 ([23]). It satisfies ddxTn(x) = nUn\u22121(x) for n \u2265 1 and\n\u2200n \u2265 0: Tn(x) =    cos(n arccos(x)), if |x| \u2264 1; cosh(n arccosh(x)), if x \u2265 1; (\u22121)n cosh(n arccosh(\u2212x)), if x \u2264 \u22121.\nIn particular, when x \u2265 1, Tn(x) = 12 [( x\u2212 \u221a x2 \u2212 1 )n + ( x+ \u221a x2 \u2212 1 )n] and Un(x) = 12\u221ax2\u22121 [( x+\n\u221a x2 \u2212 1 )n+1 \u2212 ( x\u2212 \u221a x2 \u2212 1 )n+1] .\nTn(x) = 1\n2\n[( x\u2212 \u221a x2 \u2212 1 )n + ( x+ \u221a x2 \u2212 1 )n]\nUn(x) = 1\n2 \u221a x2 \u2212 1\n[( x+ \u221a x2 \u2212 1 )n+1 \u2212 ( x\u2212 \u221a x2 \u2212 1 )n+1]\nDefinition 2.7. For function f(x) whose domain contains [\u22121, 1], its degree-n Chebyshev truncated series and degree-n Chebyshev interpolation are respectively\npn(x) def =\nn\u2211\nk=0\nakTk(x) and qn(x) def= n\u2211\nk=0\nckTk(x) ,\nwhere ak def = 2\u2212 1[k = 0] \u03c0\n\u222b 1\n\u22121 f(x)Tk(x)\u221a 1\u2212 x2 dx and ck def = 2\u2212 1[k = 0] n+ 1\nn\u2211\nj=0\nf ( xj ) Tk ( xj ) .\nAbove, xj def = cos ( (j+0.5)\u03c0 n+1 ) \u2208 [\u22121, 1] is the j-th Chebyshev point of order n.\nThe following lemma is known as the aliasing formula for Chebyshev coefficients:\nLemma 2.8 (cf. Theorem 4.2 of [23]). Let f be Lipschitz continuous on [\u22121, 1] and {ak}, {ck} be defined in Def. 2.7, then\nc0 = a0 + a2n + a4n + ... , cn = an + a3n + a5n + ... , and\n\u2200k \u2208 {1, 2, . . . , n\u2212 1} : ck = ak + (ak+2n + ak+4n + ...) + (a\u2212k+2n + a\u2212k+4n + ...)\nDefinition 2.9. For every \u03c1 > 0, let E\u03c1 be the ellipse E of foci \u00b11 with major radius 1 + \u03c1. (This is also known as Bernstein ellipse with parameter 1 + \u03c1+ \u221a 2\u03c1+ \u03c12.)\nThe following lemma is the main theory regarding Chebyshev approximation:\nLemma 2.10 (cf. Theorem 8.1 and 8.2 of [23]). Suppose f(z) is analytic on E\u03c1 and |f(z)| \u2264M on E\u03c1. Let pn(x) and qn(x) be the degree-n Chebyshev truncated series and Chebyshev interpolation of f(x) on [\u22121, 1]. Then, \u2022 maxx\u2208[\u22121,1] |f(x)\u2212 pn(x)| \u2264 2M\n\u03c1+ \u221a 2\u03c1+\u03c12\n( 1 + \u03c1+ \u221a 2\u03c1+ \u03c12 )\u2212n ;\n\u2022 maxx\u2208[\u22121,1] |f(x)\u2212 qn(x)| \u2264 4M \u03c1+ \u221a 2\u03c1+\u03c12\n( 1 + \u03c1+ \u221a 2\u03c1+ \u03c12 )\u2212n .\n\u2022 |a0| \u2264M and |ak| \u2264 2M ( 1 + \u03c1+ \u221a 2\u03c1+ \u03c12 )\u2212k for k \u2265 1."}, {"heading": "3 Approximate PCP and PCR", "text": "We formalize our notions of approximation for PCP and PCR, and provide a reduction from PCR to PCP."}, {"heading": "3.1 Our Notions of Approximation", "text": "Recall that Frostig et al. [13] work only with matrices A that satisfy the eigengap assumption, that is, A has no singular value in the range [ \u221a \u03bb(1\u2212 \u03b3), \u221a \u03bb(1 + \u03b3)]. Their approximation guarantees are very straightforward:\n\u2022 an output \u03be is \u03b5-approximate for PCP on vector \u03c7 if \u2016\u03be \u2212 \u03be\u2217\u2016 \u2264 \u03b5\u2016\u03c7\u2016; \u2022 an output x is \u03b5-approximate for PCR with regressand b if \u2016x\u2212 x\u2217\u2016 \u2264 \u03b5\u2016b\u2016.\nUnfortunately, these notions are too strong and impossible to satisfy for matrices that do not have a large eigengap around the projection threshold \u03bb.\nIn this paper we propose the following more general (but yet very meaningful) approximation notions.\nDefinition 3.1. An algorithm B(\u03c7) is (\u03b3, \u03b5)-approximate PCP for threshold \u03bb, if for every \u03c7 \u2208 Rd\n1. \u2225\u2225P(1+\u03b3)\u03bb ( B(\u03c7)\u2212 \u03c7 )\u2225\u2225 \u2264 \u03b5\u2016\u03c7\u2016. 2. \u2225\u2225(I\u2212P(1\u2212\u03b3)\u03bb)B(\u03c7)\n\u2225\u2225 \u2264 \u03b5\u2016\u03c7\u2016. 3. \u2200i such that \u03bbi \u2208 [ (1\u2212 \u03b3)\u03bb, (1 + \u03b3)\u03bb ] , it satisfies |\u3008\u03bdi,B(\u03c7)\u2212 \u03c7\u3009| \u2264 |\u3008\u03bdi, \u03c7\u3009|+ \u03b5\u2016\u03c7\u2016.\nIntuitively, the first property above states that, if projected to the eigenspace with eigenvalues above (1 + \u03b3)\u03bb, then B(\u03c7) and \u03c7 are almost identical; the second property states that, if projected to the eigenspace with eigenvalues below (1\u2212\u03b3)\u03bb, then B(\u03c7) is almost zero; and the third property states that, for each eigenvector \u03bdi with eigenvalue in the range [(1\u2212 \u03b3)\u03bb, (1 + \u03b3)\u03bb], the projection \u3008\u03bdi,B(\u03c7)\u3009 must be between 0 and \u3008\u03bdi, \u03c7\u3009 (but up to an error \u03b5\u2016\u03c7\u2016).\nNaturally, P\u03bb(\u03c7) itself is a (0, 0)-approximate PCP. We propose the following notion for approximate PCR:\nDefinition 3.2. An algorithm C(b) is (\u03b3, \u03b5)-approximate PCR for threshold \u03bb, if for every b \u2208 Rd\u2032\n1. \u2225\u2225(I\u2212P(1\u2212\u03b3)\u03bb)C(b) \u2225\u2225 \u2264 \u03b5\u2016b\u2016. 2. \u2016AC(b)\u2212 b\u2016 \u2264 \u2016Ax\u2217 \u2212 b\u2016+ \u03b5\u2016b\u2016.\nwhere x\u2217 = (A>A)\u2020P(1+\u03b3)\u03bbA>b is the exact PCR solution for threshold (1 + \u03b3)\u03bb.\nThe first notion states that the output x = C(b) has nearly no correlation with eigenvectors below threshold (1\u2212 \u03b3)\u03bb; and the second states that the regression error should be nearly optimal with respect to the exact PCR solution but at a different threshold (1 + \u03b3)\u03bb.\nRelationship to Frostig et al. Under eigengap assumption, our notions are equivalent to Frostig et al.: Fact 3.3. If A has no singular value in [ \u221a \u03bb(1\u2212 \u03b3), \u221a \u03bb(1 + \u03b3)], then\n\u2022 Def. 3.1 is equivalent to \u2016B(\u03c7)\u2212P\u03bb(\u03c7)\u2016 \u2264 O(\u03b5)\u2016\u03c7\u2016. \u2022 Def. 3.2 implies \u2016C(\u03c7)\u2212 x\u2217\u2016 \u2264 O(\u03b5/\u03bb)\u2016b\u2016 and \u2016C(\u03c7)\u2212 x\u2217\u2016 \u2264 O(\u03b5)\u2016b\u2016 implies Def. 3.2.\nAbove, x\u2217 = (A>A)\u2020P\u03bbA>b is the exact PCR solution."}, {"heading": "3.2 Reductions from PCR to PCP", "text": "If the PCP solution \u03be = P\u03bb(A >b) is computed exactly, then by definition one can compute (A>A)\u2020\u03be which gives a solution to PCR by solving a linear system. However, as pointed by Frostig et al. [13], this computation is problematic if \u03be is only approximate. The following approach has been proposed to improve its accuracy by Frostig et al.\n\u2022 \u201ccompute p((A>A + \u03bbI)\u22121)\u03be where p(x) is a polynomial that approximates function x1\u2212\u03bbx .\u201d This is a good approximation to (A>A)\u2020\u03be because the composition of functions x1\u2212\u03bbx and 1 1+\u03bbx is\nexactly x\u22121. Frostig et al. picked p(x) = pm(x) = \u2211m t=1 \u03bb t\u22121xt which is a truncated Taylor series, and used the following procedure to compute sm \u2248 pm((A>A + \u03bbI)\u22121)\u03be:\ns0 = B(A>b), s1 = ApxRidge(A, \u03bb, s0), \u2200k \u2265 1: sk+1 = s1 + \u03bb \u00b7 ApxRidge(A, \u03bb, sk) . (3.1) Above, B is an approximate PCP solver and ApxRidge is an approximate ridge regression solver.\nUnder the eigengap assumption, Frostig et al. [13] showed that\nLemma 3.4 (PCR-to-PCP). For fixed \u03bb, \u03b3, \u03b5 \u2208 (0, 1), let A be a matrix whose singular values lie in [ 0, \u221a (1\u2212 \u03b3)\u03bb ] \u222a [\u221a (1\u2212 \u03b3)\u03bb, 1 ] . Let ApxRidge be any O( \u03b5\nm2 )-approximate ridge regression\nsolver, and let B be any (\u03b3,O( \u03b5\u03bb m2 ))-approximate PCP solver8. Then, procedure (3.1) satisfies\n\u2016sm \u2212 (A>A)\u2020P\u03bbA>b\u2016 \u2264 \u03b5\u2016b\u2016 if m = \u0398(log(1/\u03b5\u03b3)) .\nUnfortunately, the above lemma does not hold without eigengap assumption. In this paper, we fix this issue by proving the following analogous lemma:\nLemma 3.5 (gap free PCR-to-PCP). For fixed \u03bb, \u03b5 \u2208 (0, 1) and \u03b3 \u2208 (0, 2/3], let A be a matrix whose singular values are no more than 1. Let ApxRidge be any O( \u03b5\nm2 )-approximate ridge regres-\nsion solver, and B be any (\u03b3,O( \u03b5\u03bb m2\n))-approximate PCP solver. Then, procedure (3.1) satisfies, {\n\u2016(I\u2212P(1\u2212\u03b3)\u03bb)sm\u2016 \u2264 \u03b5\u2016b\u2016 , and \u2016Asm \u2212 b\u2016 \u2264 \u2016A(A>A)\u2020P(1+\u03b3)\u03bbA>b\u2212 b\u2016+ \u03b5\u2016b\u2016\n} if m = \u0398(log(1/\u03b5\u03b3))\nNote that the conclusion of this lemma exactly corresponds to the two properties in our Def. 3.2. The proof of Lemma 3.5 is not hard, but requires a very careful case analysis by decomposing vectors b and each sk into three components, each corresponding to eigenvalues of A\n>A in the range [0, (1\u2212 \u03b3)\u03bb], [(1\u2212 \u03b3)\u03bb, (1 + \u03b3)\u03bb] and [(1 + \u03b3)\u03bb, 1]. We defer the details to Appendix A.\n8Recall from Fact 3.3 that this requirement is equivalent to saying that \u2016B(\u03c7)\u2212P\u03bb\u03c7\u2016 \u2264 O( \u03b5 \u221a \u03bb\nm2 )\u2016\u03c7\u2016."}, {"heading": "4 Property of Chebyshev Approximation Outside [\u22121, 1]", "text": "Classical Chebyshev approximation theory (such as Lemma 2.10) only talks about the behaviors of pn(x) or gn(x) on interval [\u22121, 1]. However, for the purpose of this paper, we must also bound its value for x > 1. We prove the following general lemma in Appendix B, and believe it could be of independent interest: (we denote by f (k)(x) the k-th derivative of f at x)\nLemma 4.1. Suppose f(z) is analytic on E\u03c1 and for every k \u2265 0, f (k)(0) \u2265 0. Then, for every n \u2208 N, letting pn(x) and qn(x) be be the degree-n Chebyshev truncated series and Chebyshev interpolation of f(x), we have\n\u2200y \u2208 [0, \u03c1] : 0 \u2264 pn(1 + y), qn(1 + y) \u2264 f(1 + y) .\n5 Our Polynomial Approximation of sgn(x) For fixed \u03ba \u2208 (0, 1], we consider the degree-n Chebyshev interpolation qn(x) = \u2211n\nk=0 ckTk(x) of the function f(x) = ( 1+\u03ba\u2212x\n2\n)\u22121/2 on [\u22121, 1]. Def. 2.7 tells us that\nck def = 2\u2212 1[k = 0] n+ 1\nn\u2211\nj=0\n(\u221a 2 cos (k(j + 0.5)\u03c0 n+ 1 ))( 1 + \u03ba\u2212 cos ((j + 0.5)\u03c0 n+ 1 ))\u22121/2 .\nOur final polynomial to approximate sgn(x) is therefore\ngn(x) = x \u00b7 qn(1 + \u03ba\u2212 2x2) and deg(gn(x)) = 2n+ 1 . We prove the following theorem in this section:\nTheorem 5.1. For every \u03b1 \u2208 (0, 1], \u03b5 \u2208 (0, 1/2), choosing \u03ba = 2\u03b12, our function gn(x) def= x \u00b7 qn(1 + \u03ba\u2212 2x2) satisfies that as long as n \u2265 1\u221a2\u03b1 log 3 \u03b5\u03b12 , then (see also Figure 1)\n\u2022 |gn(x)\u2212 sgn(x)| \u2264 \u03b5 for every x \u2208 [\u22121, \u03b1] \u222a [\u03b1, 1]. \u2022 gn(x) \u2208 [0, 1] for every x \u2208 [0, \u03b1] and gn(x) \u2208 [\u22121, 0] for every x \u2208 [\u2212\u03b1, 0].\nNote that our degree n = O ( \u03b1\u22121 log(1/\u03b1\u03b5) ) is near-optimal, because the minimum degree for a polynomial to satisfy even only the first item is \u0398 ( \u03b1\u22121 log(1/\u03b5) ) [9, 10]. However, the results of [9, 10] are not constructive, and thus may not lead to stable matrix polynomials. We prove Theorem 5.1 by first establishing two simple lemmas. The following lemma is a consequence of Lemma 2.10:\nLemma 5.2. For every \u03b5 \u2208 (0, 1/2) and \u03ba \u2208 (0, 1], if n \u2265 1\u221a \u03ba ( log 1\u03ba + log 4 \u03b5 ) then\n\u2200x \u2208 [\u22121, 1], |f(x)\u2212 qn(x)| \u2264 \u03b5 .\nProof of Lemma 5.2. Denoting by f(z) = (\n1+\u03ba\u2212z 2 )\u22120.5 , we know that f(z) is analytic on ellipse E\u03c1\nwith \u03c1 = \u03ba/2, and it satisfies |f(z)| \u2264 \u221a\n2/\u03ba in E\u03c1. Applying Lemma 2.10, we know that when n \u2265 1\u221a\n\u03ba ( log 1\u03ba + log 4 \u03b5 ) it satisfies |f(x)\u2212 qn(x)| \u2264 \u03b5.\nThe next lemma an immediate consequence of our Lemma 4.1 with f(z) = (\n1+\u03ba\u2212z 2\n)\u22120.5 :\nLemma 5.3. For every \u03b5 \u2208 (0, 1/2), \u03ba \u2208 (0, 1], n \u2208 N, and x \u2208 [0, \u03ba], we have\n0 \u2264 qn(1 + x) \u2264 (\u03ba\u2212 x\n2\n)\u22121/2 .\nProof of Theorem 5.1. We are now ready to prove Theorem 5.1.\n\u2022 When x \u2208 [\u22121, \u03b1]\u222a [\u03b1, 1], it satisfies 1 + \u03ba\u2212 2x2 \u2208 [\u22121, 1]. Therefore, applying Lemma 5.2 we have whenever n \u2265 1\u221a\n\u03ba log 6\u03b5\u03ba = 1\u221a 2\u03b1 log 3 \u03b5\u03b12 it satisfies |f(1 +\u03ba\u2212 2x2)\u2212 qn(1 +\u03ba\u2212 2x2)|\u221e \u2264 \u03b5. This further implies\n|gn(x)\u2212sgn(x)| = |xqn(1+\u03ba\u22122x2)\u2212xf(1+\u03ba\u22122x2)| \u2264 |x||f(1+\u03ba\u22122x2)\u2212qn(1+\u03ba\u22122x2)| \u2264 \u03b5 .\n\u2022 When |x| \u2264 \u03b1, it satisfies 1 + \u03ba\u2212 2x2 \u2208 [1, 1 + \u03ba]. Applying Lemma 5.3 we have \u2200x \u2208 [0, \u03b1] : 0 \u2264 gn(x) = x \u00b7 qn(1 + \u03ba\u2212 2x2) \u2264 x \u00b7 (x2)\u22121/2 = 1\nand similarly for x \u2208 [\u2212\u03b1, 0] it satisfies 0 \u2265 gn(x) \u2265 \u22121.\nA Bound on Chebyshev Coefficients. We also give an upper bound to the coefficients of polynomial qn(x). Its proof can be found in Appendix C, and this upper bound shall be used in our final stability analysis. Lemma 5.4 (coefficients of qn). Let qn(x) = \u2211n\nk=0 ckTk(x) be the degree-n Chebyshev interpolation of f(x) = ( 1+\u03ba\u2212x\n2\n)\u22121/2 on [\u22121, 1]. Then, \u2200i \u2208 {0, 1, . . . , n} : |ci| \u2264 e \u221a 32(i+ 1)\n\u03ba\n( 1 + \u03ba+ \u221a 2\u03ba+ \u03ba2 )\u2212i"}, {"heading": "6 Stable Computation of Matrix Chebyshev Polynomials", "text": "In this section we show that any polynomial that is a weighted summation of Chebyshev polynomials with bounded coefficients, can be stably computed when applied to matrices with approximate computations. We achieve so by first generalizing Clenshaw\u2019s backward method to matrix case in Section 6.1 in order to compute a matrix variant of Chebyshev sum, and then analyze its stability in Section 6.2 with the help from Elloit\u2019s forward-backward transformation [8].\nRemark 6.1. We wish to point out that although Chebyshev polynomials are known to be stable under error when computed on scalars [14], it is not immediately clear why it holds also for matrices. Recall that Chebyshev polynomials satisfy Tn+1(x) = 2xTn(x) \u2212 Tn\u22121(x). In the matrix case, we have Tn+1(M)\u03c7 = 2MTn(M)\u03c7\u2212 Tn\u22121(M)\u03c7 where \u03c7 \u2208 Rd is a vector. If we analyzed this formula coordinate by coordinate, error could blow up by a factor d per iteration.\nIn addition, we need to ensure that the stability theorem holds for matrices M with eigenvalues that can exceed 1. This is not standard because Chebyshev polynomials are typically analyzed only on domain [\u22121, 1]."}, {"heading": "6.1 Clenshaw\u2019s Method in Matrix Form", "text": "In the scalar case, Clenshaw\u2019s method (sometimes referred to as backward recurrence) is one of the most widely used implementations for Chebyshev polynomials. We now generalize it to matrices.\nConsider any computation of the form\n~sN def =\nN\u2211\nk=0\nTk(M)~ck \u2208 Rd where M \u2208 Rd\u00d7d is symmetric and each ~ck is in Rd . (6.1)\n(Note that for PCP and PCR purposes, we it suffices to consider ~ck = c \u2032 k\u03c7 where c \u2032 k \u2208 R is a scalar and \u03c7 \u2208 Rd is a fixed vector for all k. However, we need to work on this more general form for our stability analysis.)\nVector sN can be computed using the following procedure:\nLemma 6.2 (backward recurrence). ~sN = ~b0 \u2212M~b1 where ~bN+1 def = ~0, ~bN def = ~cN , and \u2200r \u2208 {N \u2212 1, . . . , 0} : ~br def= 2M~br+1 \u2212~br+2 + ~cr \u2208 Rd ."}, {"heading": "6.2 Inexact Clenshaw\u2019s Method in Matrix Form", "text": "We show that, if implemented using the backward recurrence formula, the Chebyshev sum of (6.1) can be stably computed. We define the following model to capture the error with respect to matrix-vector multiplications.\nDefinition 6.3 (inexact backward recurrence). Let M be an approximate algorithm that satisfies \u2016M(u)\u2212Mu\u20162 \u2264 \u03b5\u2016u\u20162 for every u \u2208 Rd. Then, define inexact backward recurrence to be\nb\u0302N+1 def = 0, b\u0302N def = ~cN , and \u2200r \u2208 {N \u2212 1, . . . , 0} : b\u0302r def= 2M ( b\u0302r+1 ) \u2212 b\u0302r+2 + ~cr \u2208 Rd ,\nand define the output as s\u0302N def = b\u03020 \u2212M(\u0302b1).\nThe following theorem gives an error analysis to our inexact backward recurrence. We prove it in Appendix D.1, and the main idea of our proof is to convert each error vector of a recursion of the backward procedure into an error vector corresponding to some original ~ck.\nTheorem 6.4 (stable Chebyshev sum). For every N \u2208 N\u2217, suppose the eigenvalues of M are in [a, b] and suppose there are parameters CU \u2265 1, CT \u2265 1, \u03c1 \u2265 1, Cc \u2265 0 satisfying\n\u2200k \u2208 {0, 1, . . . , N} : { \u03c1k\u2016~ck\u2016 \u2264 Cc \u2227 \u2200x \u2208 [a, b] : |Tk(x)| \u2264 CT\u03c1k and |Uk(x)| \u2264 CU\u03c1k } .\nThen, if the inexact backward recurrence in Def. 6.3 is applied with \u03b5 \u2264 14NCU , we have \u2016s\u0302N \u2212 ~sN\u2016 \u2264 \u03b5 \u00b7 2(1 + 2NCT )NCUCc ."}, {"heading": "7 Algorithms and Main Theorems for PCP and PCR", "text": "We are now ready to state our main theorems for PCP and PCR. We first note a simple fact:\nFact 7.1. (P\u03bb)\u03c7 = I+sgn(S) 2 where S def = 2(A>A + \u03bbI)\u22121A>A\u2212 I = (A>A + \u03bbI)\u22121(A>A\u2212 \u03bbI).\nIn other words, for every vector \u03c7 \u2208 Rd, the exact PCP solution P\u03bb(\u03c7) is the same as computing (P\u03bb)\u03c7 = I+sgn(S) 2 \u03c7. Thus, we can use our polynomial gn(x) introduced in Section 5 and compute gn(S)\u03c7 \u2248 sgn(S)\u03c7. Finally, in order to compute gn(S), we need to multiply S to deg(gn) vectors; whenever we do so, we call perform ridge regression once."}, {"heading": "7.1 Our Pseudo Codes", "text": "First of all, we can approximately compute S\u03c7 for an arbitrary \u03c7 \u2208 Rd. This simply uses one oracle call to ridge regression, see Algorithm 1.\nNext, since we are interested in (\u03b3, \u03b5)-approximate PCP, we want gn(x) to be close to sgn(x) on all eigenvalues of A>A that are outside [(1\u2212 \u03b3)\u03bb, (1 + \u03b3)\u03bb], or equivalently all eigenvalues of S outside the range\n[ \u2212 (1 + \u03b3)\u2212 1\n1 + (1 + \u03b3) , 1\u2212 (1\u2212 \u03b3) 1 + (1\u2212 \u03b3)\n] .\nAlgorithm 1 MultS(A, \u03bb, \u03c7) Input: A \u2208 Rd\u2032\u00d7d; \u03bb > 0; \u03c7 \u2208 Rd. Output: a vector that approximately equals S\u03c7 = (A>A + \u03bbI)\u22121(A>A\u2212 \u03bbI)\u03c7\n1: return ApxRidge(A, \u03bb,A>A\u03c7\u2212 \u03bb\u03c7).\nSince this new interval contains [\u2212\u03b1, \u03b1] for \u03b1 def= \u03b3/(2+\u03b3) = \u03b3/2\u2212O(\u03b32), we can apply Theorem 5.1, which gives us a polynomial gn(x) = x \u00b7 qn(1 + \u03ba \u2212 2x2) where \u03ba = 2\u03b12 = 2(\u03b3/(2 + \u03b3))2. We use (inexact) backward recurrence \u2014see Lemma 6.2\u2014 to compute the Chebyshev interpolation polynomial u\u2190 qn ( (1+\u03ba)I\u22122S2 ) \u03c7. Our final output for approximate PCP is simply Su+\u03c72 because P\u03bb \u2248 Sgn((1+\u03ba)\u22122S 2)+I\n2 . We summarize this algorithm as QuickPCP(A, \u03c7, \u03bb, \u03b3, n) in Algorithm 2.\nAlgorithm 2 QuickPCP(A, \u03c7, \u03bb, \u03b3, n) Input: A \u2208 Rd\u2032\u00d7d data matrix satisfying \u03c3max(A) \u2264 1; \u03c7 \u2208 Rd, vector to project; \u03bb > 0, eigenvalue threshold for PCP; \u03b3 \u2208 (0, 2/3], PCP approximation ratio. n, number of iterations one can also ignore \u03b3 and set \u03b3 = 0, see Remark 7.5 Output: a vector \u03be \u2208 Rd satisfying \u03be \u2248 P\u03bb(\u03c7). 1: \u03b3 \u2190 max{\u03b3, log(n)n } if \u03b3 to small, work in a \u03b3-free regime, see Remark 7.5 2: \u03ba\u2190 2 ( \u03b3/(2 + \u03b3)\n)2 recall \u03ba = 2\u03b12 = 2(\u03b3/(2 + \u03b3))2 in our analysis 3: Define ck def = 2\u22121[k=0]n+1 \u2211n j=0 (\u221a 2 cos (k(j+0.5)\u03c0 n+1 ))( 1 + \u03ba\u2212 cos ( (j+0.5)\u03c0 n+1 ))\u22121/2 coefficients for qn(x) 4: bn+1 \u2190 ~0, bn \u2190 cn \u00b7 \u03c7 5: for r \u2190 n\u2212 1 to 0 do 6: w \u2190 (1 + \u03ba)br+1 \u2212 2 \u00b7 MultS(A, \u03bb, MultS(A, \u03bb, br+1)); w \u2248 ((1 + \u03ba)I\u2212 S2)br+1 7: br \u2190 2w \u2212 br+2 + cr \u00b7 \u03c7 8: end for 9: u\u2190 MultS(A, \u03bb, b0 \u2212 w); u \u2248 S(gn((1 + \u03ba)I\u2212 S2))\u03c7 \u2248 sgn(S)\u03c7 10: return 12u+ 1 2\u03c7 output \u2248 sgn(S)+I2 \u03c7\nFinally, we apply the PCR-to-PCP reduction (see Section 3) to derive a solution for PCR from an approximate solution for PCP. See QuickPCR(A, b, \u03bb, \u03b3, n,m) in Algorithm 3.\nAlgorithm 3 QuickPCR(A, b, \u03bb, \u03b3, n,m) Input: A, \u03bb, \u03b3, n the same as QuickPCP; b \u2208 Rd\u2032 is the regressand vector; m is the number of iterations for PCR. choosing m = 10 it sufficient for practical purposes Output: a vector x \u2208 Rd that solves approximate PCR. 1: v \u2190 QuickPCP(A,A>b, \u03bb, \u03b3, n), s\u2190 v, s1 \u2190 ApxRidge(A, \u03bb, v); 2: for r \u2190 1 to m do 3: s\u2190 \u03bb \u00b7 ApxRidge(A, \u03bb, s) + s1; 4: return s\nFact 7.2. QuickPCP calls ridge regression 2n+ 1 times and QuickPCR calls it 2n+m+ 2 times."}, {"heading": "7.2 Our Main Theorems", "text": "We first state our main theorem under the eigengap assumption, in order to provide a direct comparison to that of Frostig et al. [13].\nTheorem 7.3 (eigengap assumption). Given A \u2208 Rd\u2032\u00d7d and \u03bb, \u03b3 \u2208 (0, 1), assume that the singular values of A are in the range [0, \u221a (1\u2212 \u03b3)\u03bb] \u222a [ \u221a (1 + \u03b3)\u03bb, 1]. Given \u03c7 \u2208 Rd and b \u2208 Rd\u2032, denote by \u03be\u2217 = P\u03bb\u03c7 and x\u2217 = (A>A)\u22121P\u03bbA>b the exact PCP and PCR solutions. If ApxRidge is an \u03b5\u2032-approximate ridge regression solver, then\nthe output \u03be \u2190 QuickPCP(A, \u03c7, \u03bb, \u03b3, n) satisfies \u2016\u03be\u2217 \u2212 \u03be\u2016 \u2264 \u03b5\u2016\u03c7\u2016 if n = \u0398 ( \u03b3\u22121 log 1\u03b3\u03b5 ) and log(1/\u03b5\u2032) = \u0398 ( log 1\u03b3\u03b5 ) ; the output x\u2190 QuickPCR(A, b, \u03bb, \u03b3, n,m) satisfies \u2016x\u2212 x\u2217\u2016 \u2264 \u03b5\u2016b\u2016 if n = \u0398 ( \u03b3\u22121 log 1\u03b3\u03bb\u03b5 ) , m = \u0398 ( log 1\u03b3\u03b5 ) and log(1/\u03b5\u2032) = \u0398 ( log 1\u03b3\u03bb\u03b5 ) .\nIn contrast, the number of ridge-regression oracle calls was \u0398(\u03b3\u22122 log 1\u03b3\u03b5) for PCP and \u0398(\u03b3 \u22122 log 1\u03b3\u03bb\u03b5) for PCR in [13]. We include the proof of Theorem 7.3 in Appendix E.1. Next we state our stronger theorem without the eigengap assumption.\nTheorem 7.4 (gap-free). Given A \u2208 Rd\u2032\u00d7d, \u03bb \u2208 (0, 1), and \u03b3 \u2208 (0, 2/3], assume that \u2016A\u20162 \u2264 1. Given \u03c7 \u2208 Rd and b \u2208 Rd\u2032, and suppose ApxRidge is an \u03b5\u2032-approximate ridge regression solver, then\n\u2022 QuickPCP outputs \u03be that is (\u03b3, \u03b5)-approximate PCP with O ( \u03b3\u22121 log 1\u03b3\u03b5 ) oracle calls to ApxRidge\nas long as log(1/\u03b5\u2032) = \u0398 ( log 1\u03b3\u03b5 ) .\n\u2022 QuickPCR outputs x that is (\u03b3, \u03b5)-approximate PCR with O ( \u03b3\u22121 log 1\u03b3\u03bb\u03b5 ) oracle calls to ApxRidge\nas long as elog(1/\u03b5\u2032) = \u0398 ( log 1\u03b3\u03bb\u03b5 ) .\nWe make a final remark here regarding the practical usage of QuickPCP and QuickPCR.\nRemark 7.5. Since our theory is for (\u03b3, \u03b5)-approximations that have two parameters, the user in principle has to feed in both \u03b3 and n (in addition to other default inputs such as A, b and \u03bb). In practice, however, it is usually sufficient to obtain (\u03b5, \u03b5)-approximate PCP and PCR. Therefore, our pseudocodes allow users to set \u03b3 = 0 and thus ignore this parameter \u03b3; in such a case, we shall use \u03b3 = log(n)/n which is equivalent to setting \u03b3 = \u0398(\u03b5) because n = \u0398(\u03b3\u22121 log(1/\u03b3\u03b5))."}, {"heading": "8 Experiments", "text": "In the same way as [13], we conclude this paper with an empirical evaluation to demonstrate our theorems.\nDatasets. We consider synthetic and real-life datasets.\n\u2022 We generate the synthetic dataset in the same way as [13]. That is, we form a 3000 \u00d7 2000 dimensional matrix A via the SVD A = U\u03a3V> where U and V are random orthonormal matrices and \u03a3 contains random singular values. Among the 2000 singular values, we let half of them be randomly chosen from [0, \u221a 0.1(1 \u2212 a)] and the other half randomly chosen from\n[ \u221a\n0.1(1+a), 1]. We generate vector b by adding noise to the response Ax of a random \u201ctrue\u201d x that correlates with A\u2019s top principal components. We consider eigenvalue threshold \u03bb = 0.1, and use a = 0, 0.01, 0.02, 0.1 in our experiments. We call these datasets random-a.\n\u2022 As for the real-life dataset, we use mnist [11]. After scaling its largest singular value to one,9 we choose the eigenvalue threshold \u03bb = 0.0025 (or equivalently singular value threshold\u221a \u03bb = 0.05). The closest singular values to this threshold are respectively 0.05027 and 0.04958.\nAlgorithms. We implemented our algorithm and Frostig et al. [13] (which we call FMMS for short) and minimized the number of calls to ridge regression in our implementations. For instance, if using our pseudocode QuickPCP, the number of ridge regression calls is 2n + 1; if using our pseudocode QuickPCR, the number of extra ridge regression calls is m + 1. We choose m = 10 in all of our experiments because the theoretical prediction of m is only a small logarithmic quantity (see Lemma 3.4 and Lemma 3.5).\nWe also implemented a practical heuristic using Krylov subspace that were found on the website [12]. We call this algorithm Krylov method for short. Krylov method transforms the covariance matrix AA> into a lower-dimensional Krylov subspace and performs exact PCP and PCR there. Similar to this paper, Krylov method also reduces PCP and PCR to multiple calls of ridge regressions.10\nWe emphasize that Krylov method has no supporting theory behind it. Since we find it performs much faster than FMMS in practice, we include it in our experiments for a stronger comparison.\nRemark 8.1. There are two main issues behind the missing theory of Krylov method.\n\u2022 Stability. If matrix-vector multiplications are only approximate, Krylov-based methods are usually unstable so one needs to replace it with other stable variants. Our polynomial approximation gn(x) can be viewed as one such stable variant. \u2022 Accuracy. To the best of our knowledge, even with exact computations, if there is no eigengap around threshold \u03bb \u2014which is usually the case in real life\u2014 it is unlikely that Krylov method can achieve a log(1/\u03b5) convergence with respect to the \u03b5-parameter in (\u03b3, \u03b5)-approximate PCP or PCR.11 Our experiments later (namely Figure 3(c) and 3(f)) shall also confirm on this."}, {"heading": "8.1 Evaluation 1: With Eigengap Assumption", "text": "In the first evaluation we consider matrices that satisfy the eigengap assumption. To simulate an eigengap, we use random datasets random-a with a = 0.01, 0.02, 0.1 and present our findings in Figure 2 in terms of the following three performance measures:\n\u2022 Regression Error: \u2016x \u2212 x\u2217\u20162/\u2016x\u2217\u20162; where x is the output of a PCR algorithm and x\u2217 = (A>A)\u2020P\u03bbA>b is the exact PCR solution. \u2022 Projection Error: \u2016\u03be \u2212 \u03be\u2217\u20162/\u2016\u03be\u2217\u20162; where \u03be is the output of a PCP algorithm and \u03be\u2217 = P\u03bbA >b is the exact PCP solution. \u2022 Denoising Error: \u2016(I\u2212P\u03bb)\u03be\u20162/\u2016\u03be\u20162; where \u03be is the output of a PCP algorithm. The x-axis of these plots represent the number of calls to ridge regression, and in Figure 2 we use exact implementations of ridge regression similar to the experiments in [13]. Note that the horizontal axis starts with 0 for projection performances (second and third column) and with 10\n9This is a cheap procedure and for instance can be done by power method [13]. 10The original code [12], when working with Krylov subspace of dimension k, requires 2k calls of ridge regression. In our experiments, we improved this implementation and reduced it from 2k calls to k calls for a stronger comparison. 11This is so because Krylov method works in a smaller dimension whose so-called \u201cRitz values\u201d approximate the original eigenvalues of A>A. However, this approximation cannot be \u201cexponentially close\u201d because there are only very few Ritz values as compared to the original eigenvalues of A>A.\nfor regression performance (first column). This is so because in order to reduce PCR to PCP one needs m+1 calls to ridge regression in QuickPCR and in our experiments we simply choose m = 10.\nWe make some important observations from these results\n\u2022 We significantly outperform FMMS for our choices of a. \u2022 Our performance degrades as a (and thus \u03b3) decreases; this is consistent to our theory. \u2022 The performance of Krylov method fluctuates partly due to the missing theory behind it.\nThis limits the practicality of Krylov method, because it is hardly possible for the algorithm to determine when is the best time to stop the algorithm.12\n\u2022 If the fluctuation of Krylov method is ignored, it matches the performance of QuickPCP and QuickPCR. This is an interesting phenomenon and might even be a first evidence towards a theoretical proof for Krylov method."}, {"heading": "8.2 Evaluation 2: Without Eigengap Assumption", "text": "In our second evaluation we consider scenarios when there is no significant eigengap around the projection threshold \u03bb. We consider dataset random-a for a = 0 as well as dataset mnist. This\n12Of course, if the true projection matrix P\u03bb is given explicitly, we can determine a good iteration to stop. However, the entire PCP problem is regarding how to compute P\u03bb without explicitly constructing it.\ntime, we also consider three performance measures. The first two are the same as the previous subsection, as for the third measure, we replace it with\n\u2022 denoising error (small): \u2016(I\u2212P0.81\u03bb)\u03be\u20162/\u2016\u03be\u20162. We emphasize here that in gap-free scenarios, regression error, projection error, or even the quantity \u2016(I \u2212 P\u03bb)\u03be\u20162 can all be very large \u2014 in the extreme case if there is an eigenvector that has exactly eigenvalue \u03bb, then these quantities do not converge to zero. This is why our gap-free approximation definitions do not account for such quantities (see Def. 3.1 and Def. 3.2).\nIn contrast, by focusing only on eigenvectors that are less than threshold (1 \u2212 \u03b3)\u03bb for some \u03b3 > 0, and looking at \u2016(I \u2212 P(1\u2212\u03b3)\u03bb)\u03be\u20162, this quantity can indeed converge to \u03b5 > 0 with a speed that is O(\u03b3\u22121 log(1/\u03b5)) if our algorithm is used (see Theorem 7.4). Note that this speed was only O(\u03b3\u22122 log(1/\u03b5)) for FMMS.\nWe present our findings in Figure 3 and make some important conclusions here:\n\u2022 Our method still significantly outperforms FMMS. \u2022 In terms of denoising error, our method significantly outperforms Krylov method. This is so\nbecause, according to Remark 8.1, Krylov method cannot achieve a log(1/\u03b5) convergence rate with respect to the \u03b5-parameter in (\u03b3, \u03b5)-approximate PCP or PCR. Threfore, our method is clearly the best for denoising purposes."}, {"heading": "8.3 Evaluation 3: Stability Test", "text": "In our third evaluation, we verify that our method continues to work well even if ridge regressions are computed with moderate error. We consider two types of errors in our experiments:\nRemark. Although it seems our method is more affected by error than FMMS, we emphasize that this is because FMMS is too slow and still works in a very low-accuracy regime in the plots. (For instance, as a stable algorithm, FMMS should not be affected by error of magnitude around 10\u22126 when the desired accuracy is above 10\u22124.)\n\u2022 ridge-SVRG: we run the SVRG [17] method for 50 passes to solve each ridge regression.13 \u2022 ridge-10\u2212k: we run exact ridge regression but randomly add noise [\u221210\u2212k, 10\u2212k] per coordinate. We present our findings in Figure 4. For cleanness, we compare only the denoising error and only\non datasets mnist, random-0 and random-0.1.14 We make the following conclusions and remarks:\n\u2022 Even with inexact ridge regression, our method still works very well. We continue to outperform FMMS significantly.\n\u2022 Compared with Krylov method, we continue to outperform it significantly in gap-free scenarios. \u2022 Although it seems our method is more affected by error than FMMS, we emphasize that this\nis because FMMS is too slow and still works in a very low-accuracy regime in the plots. (For\n13We choose the epoch length of SVRG to be 2n, and therefore full gradients are computed every 2n stochastic iterations. Each n stochastic iterations is counted as one \u201cpass\u201d of the data, and each full gradient computation is counted as one \u201cpass\u201d of the data.\n14Since mnist and random-0 are datasets without significant eigengap, we present \u201cdenoising error (small)\u201d as defined in Section 8.2.\ninstance, as a stable algorithm, FMMS should not be affected by error of magnitude around 10\u22126 when the desired accuracy is above 10\u22124.)"}, {"heading": "9 Conclusion", "text": "We summarize our contributions. \u2022 We put forward approximate notions for PCP and PCR that do not rely on any eigengap\nassumption. Our notions reduce to standard ones under the eigengap assumption. \u2022 We design near-optimal polynomial approximation g(x) to sgn(x) satisfying (1.1) and (1.2). \u2022 We develop general stable recurrence formula for matrix Chebyshev polynomials; as a corollary,\nour g(x) can be applied to matrices in a stable manner. \u2022 We obtain faster, provable PCA-free algorithms for PCP and PCR than known results."}, {"heading": "Acknowledgements", "text": "We thank Yin Tat Lee for suggesting us the new title, and anonymous referees for useful suggestions. Z. Allen-Zhu is partially supported an NSF Grant, no. CCF-1412958, and a Microsoft Research Grant, no. 0518584. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of NSF or Microsoft.\nAppendix"}, {"heading": "A Proof of Lemma 3.5", "text": "Lemma 3.5. For fixed \u03bb, \u03b5 \u2208 (0, 1) and \u03b3 \u2208 (0, 2/3], let A be a matrix whose singular values are no more than 1. Let ApxRidge be any O( \u03b5\nm2 )-approximate ridge regression solver, and B be any\n(\u03b3,O( \u03b5\u03bb m2\n))-approximate PCP solver. Then, procedure (3.1) satisfies, {\n\u2016(I\u2212P(1\u2212\u03b3)\u03bb)sm\u2016 \u2264 \u03b5\u2016b\u2016 , and \u2016Asm \u2212 b\u2016 \u2264 \u2016A(A>A)\u2020P(1+\u03b3)\u03bbA>b\u2212 b\u2016+ \u03b5\u2016b\u2016\n} if m = \u0398(log(1/\u03b5\u03b3))\nProof of Lemma 3.5. We first notice that the approximation guarantee of B implies \u2016s0\u2016 = \u2016B(A>b)\u2016 \u2264 \u2016A>b\u2016+O ( \u03b5\u03bb/m2 ) \u2016b\u2016 \u2264 2\u2016b\u2016 . Let us consider a new exact sequence {s\u2217k}k\u22650 where s\u22170 = P(1\u2212\u03b3)\u03bbs0, s \u2217 1 = (A\n>A + \u03bbI)\u22121s\u22170, \u2200k \u2265 1: s\u2217k+1 = s\u22171 + \u03bb \u00b7 (A>A + \u03bbI)\u22121s\u2217k . Step I. We first bound the error between sk and s \u2217 k. We have \u2016s\u2217k+1\u2016 \u2264 \u2016s\u22171\u2016+\u03bb\u2016A>A+\u03bbI\u20162\u2016s\u2217k\u2016 \u2264 \u2016s\u22171\u2016+ \u2016s\u2217k\u2016 which implies \u2016s\u2217k\u2016 \u2264 k\u2016s\u22171\u2016 \u2264 k\u03bb\u2016s\u22170\u2016 \u2264 2k\u03bb \u2016b\u2016. Therefore, \u2016s\u2217k+1 \u2212 sk+1\u2016 \u2264 \u2016s\u22171 \u2212 s1\u2016+ \u03bb\u2016(A>A + \u03bbI)\u22121s\u2217k \u2212 ApxRidge(A, \u03bb, sk)\u2016\n\u2264 \u2016s\u22171 \u2212 s1\u2016+ \u03bb\u2016(A>A + \u03bbI)\u22121(s\u2217k \u2212 sk)\u2016+ \u03bb\u2016(A>A + \u03bbI)\u22121sk \u2212 ApxRidge(A, \u03bb, sk)\u2016 \u2264 \u2016s\u22171 \u2212 s1\u2016+ \u2016s\u2217k \u2212 sk\u2016+O ( \u03bb\u03b5/m2 ) \u2016sk\u2016 . (A.1)\nSince \u2016s\u2217k\u2016 \u2264 2k\u03bb \u2016b\u2016 \u2264 2m\u03bb \u2016b\u2016 and since \u2016s\u22170 \u2212 s0\u2016 \u2264 O( \u03b5\u03bbm2 )\u2016b\u2016, we can conclude from (A.1) (by telescoping sum over k = 1, . . . , k\u2032) that\n\u2200k\u2032 \u2208 {0, 1, . . . ,m} : \u2016s\u2217k\u2032 \u2212 sk\u2032\u2016 \u2264 \u03b5\u2016b\u2016 .\nStep II. We next focus on s\u2217k and decompose s \u2217 k into three parts: for every k \u2265 0, define v1,k = P(1+\u03b3)\u03bbs \u2217 k =: P1s \u2217 k, v2,k = (I\u2212P(1\u2212\u03b3)\u03bb)s\u2217k =: P2s\u2217k, v3,k = (P(1\u2212\u03b3)\u03bb\u2212P(1+\u03b3)\u03bb)s\u2217k =: P3s\u2217k . The update rule of s\u2217k tells us that\n\u2200i \u2208 [3], k \u2265 1: vi,k = 1\n\u03bb\nk\u2211\nt=1\n( \u03bb(A>A + \u03bbI)\u22121Pi )t s\u22170 .\nIn particular, since v2,0 = P2s \u2217 0 = 0 we always have v2,m = 0.\nAs for v1,m and v3,m, we first notice that if we denote by pk(x) def = \u2211k t=1 \u03bb t\u22121xt, then vi,k =\npk((A >A + \u03bbI)\u22121)Pis\u22170. But since limk\u2192\u221e pk(x) = x 1\u2212\u03bbx =: p(x), we have\nlim k\u2192\u221e\nvi,k = p((A >A + \u03bbI)\u22121)Pis\u22170 = (A >A)\u2020Pis\u22170 = (A >A)\u2020vi,0 .\nAt the same time, note that the spectral norms \u2016\u03bb \u00b7 (A>A+\u03bbI)\u22121P1\u20162 and \u2016\u03bb \u00b7 (A>A+\u03bbI)\u22121P3\u20162 are both no more than 34 . (This is so because for every eigenvalue \u03bbj of A\n>A that is below \u03bb(1\u2212\u03b3) we have \u03bb\u03bb+\u03bbj \u2264 \u03bb \u03bb+(1/3)\u03bb = 3 4 .) Therefore, for both i = 1 and i = 3, we have\n\u2225\u2225vi,m \u2212 lim k\u2192\u221e vi,k \u2225\u2225 \u2264 1 \u03bb\n\u221e\u2211\nt=m+1\n\u2016\u03bb \u00b7 (A>A + \u03bbI)\u22121Pi\u2016t2 \u00b7 \u2016s\u22170\u2016 \u2264 (3/4)m \u00b7O ( \u2016b\u2016/\u03bb ) .\nIn other words, choosing m = \u0398(log(1/\u03b5\u03bb)), we have\n\u2016v1,m \u2212 (A>A)\u2020v1,0\u2016 \u2264 \u03b5\u2016b\u2016 and \u2016v3,m \u2212 (A>A)\u2020v3,0\u2016 \u2264 \u03b5\u2016b\u2016 . (A.2)\nStep III. We now take into account the error of the PCP solver B. For v1,m, we have: \u2016v1,m \u2212 (A>A)\u2020P1A>b\u2016 \u2264 \u2016v1,m \u2212 (A>A)\u2020v1,0\u2016+ \u2016(A>A)\u2020P1(B(A>b)\u2212A>b)\u2016\n\u2264 \u03b5\u2016b\u2016+ 1 \u03bb \u2016P1\n( B(A>b)\u2212A>b ) \u2016 \u2264 2\u03b5\u2016b\u2016 , (A.3)\nwhere the first inequality uses triangle inequality, the second uses (A.2), and the third uses Def. 3.1 and \u2016A>b\u2016 \u2264 \u2016b\u2016.\nAs for v3,m, we let A = U\u03a3V > be the SVD of A and let \u03a3\u2020 be the same matrix \u03a3 except all\nnon-zero elements get inverted. We have\n\u2016A(v3,m \u2212 (A>A)\u2020P3A>b)\u2016 \u00ac \u2264 \u2225\u2225\u2225A ( (A>A)\u2020v3,0 \u2212 (A>A)\u2020P3A>b )\u2225\u2225\u2225+ \u2016Av3,m \u2212A(A>A)\u2020v3,0\u2016  \u2264 \u2225\u2225\u2225A ( (A>A)\u2020v3,0 \u2212 (A>A)\u2020P3A>b )\u2225\u2225\u2225+ \u03b5\u2016b\u2016\n= \u2225\u2225\u2225U\u03a3\u2020V>P3 ( B(A>b)\u2212A>b )\u2225\u2225\u2225+ \u03b5\u2016b\u2016 \u00ae \u2264 \u2225\u2225\u2225\u03a3\u2020V>P3 ( B(A>b)\u2212A>b )\u2225\u2225\u2225+ \u03b5\u2016b\u2016 = \u2211\ni:\u03bbi\u2208[(1\u2212\u03b3)\u03bb,(1+\u03b3)\u03bb]\n1\u221a \u03bbi |\u3008vi,B(A>b)\u2212A>b\u3009|+ \u03b5\u2016b\u2016\n\u00af \u2264\n\u2211\ni:\u03bbi\u2208[(1\u2212\u03b3)\u03bb,(1+\u03b3)\u03bb]\n1\u221a \u03bbi |\u3008vi,A>b\u3009|+ 2\u03b5\u2016b\u2016\n= \u2016(A>A)\u2020P3A>b\u2016+ 2\u03b5\u2016b\u2016 . (A.4) Above, \u00ac uses triangle inequality,  uses (A.2) and the fact \u2016A\u20162 \u2264 1, \u00ae uses \u2016U\u20162 \u2264 1, \u00af uses Def. 3.1 and \u2016A>b\u2016 \u2264 \u2016b\u2016.\nStep IV. Finally we put everything together and bound the regression error. Denote by opt = \u2016A(A>A)\u2020P(1+\u03b3)\u03bbA>b\u2212 b\u2016. If we decompose b as\nb = ( 3\u2211\ni=1\nA(A>A)\u2020PiA>b ) + (b\u2212A(A>A)\u2020A>b) , (A.5)\nthen the four vectors in (A.5) are orthogonal to each other, which gives us\nopt = \u2016A(A>A)\u2020P1A>b\u2212 b\u2016 = \u2016A(A>A)\u2020P2A>b\u2016+ \u2016A(A>A)\u2020P3A>b\u2016+ \u2016A(A>A)\u2020A>b\u2212 b\u2016 . (A.6)\nNow we compute the regression error with respect to s\u2217m:\n\u2016As\u2217m \u2212 b\u2016 \u00ac = \u2016A(v1,m + v3,m)\u2212 b\u2016  =\n\u2225\u2225\u2225\u2225\u2225A(v1,m + v3,m)\u2212 3\u2211\ni=1\nA(A>A)\u2020PiA>b+ (b\u2212A(A>A)\u2020A>b) \u2225\u2225\u2225\u2225\u2225\n\u00ae \u2264 \u2225\u2225\u2225A(v1,m \u2212 (A>A)\u2020P1A>b) \u2225\u2225\u2225+ \u2225\u2225\u2225A(v3,m \u2212 (A>A)\u2020P3A>b) \u2225\u2225\u2225\n+\u2016A(A>A)\u2020P2A>b\u2016+ \u2016A(A>A)\u2020A>b\u2212 b\u2016 \u00af \u2264 4\u03b5\u2016b\u2016+ \u2016A(A>A)\u2020P2A>b\u2016+ \u2016A(A>A)\u2020P3A>b\u2016+ \u2016A(A>A)\u2020A>b\u2212 b\u2016 \u00b0 = opt + 4\u03b5\u2016b\u2016 .\nAbove, \u00ac is because v2,m = 0;  uses (A.5); \u00ae uses triangle inequality; \u00af uses (A.3) and (A.4); \u00b0 uses (A.6).\nFinally, using \u2016s\u2217m\u2212 sm\u2016 \u2264 \u03b5\u2016b\u2016 we complete the proof that \u2016Asm\u2212 b\u2016 \u2264 opt+ 5\u03b5\u2016b\u2016. We also have \u2016P2sm\u2016 \u2264 \u03b5\u2016b\u2016+ \u2016P2s\u2217m\u2016 = \u03b5\u2016b\u2016 because P2s\u2217m = v2,m = 0."}, {"heading": "B Appendix for Section 4", "text": "Lemma 4.1. Suppose f(z) is analytic on E\u03c1 and for every k \u2265 0, f (k)(0) \u2265 0. Then, for every n \u2208 N, letting pn(x) and qn(x) be be the degree-n Chebyshev truncated series and Chebyshev interpolation of f(x), we have\n\u2200y \u2208 [0, \u03c1] : 0 \u2264 pn(1 + y), qn(1 + y) \u2264 f(1 + y) . To show Lemma 4.1 we first need an auxiliary lemma, which can be proved by some careful\ncase analysis (see Appendix B.1). Lemma B.1. Let m,n \u2208 N be two integers, then am,n = \u222b 1 \u22121 xm\u221a 1\u2212x2Tn(x)dx \u2265 0.\nLemma B.1 essentially says that the Chebyshev coefficients of any function xm must be all non-negative. We also recall the following lemma regarding high-order derivatives of Chebyshev truncated series:\nLemma B.2 (cf. Theorem 21.1 of [23]). Suppose f(z) is analytic on E\u03c1 with \u03c1 > 0, and let pn(x) be the degree-n Chebyshev truncated series of f(x). Then, for every k \u2265 0,\nlim n\u2192+\u221e max x\u2208[\u22121,1]\n{ |f (k)(x)\u2212 p(k)n (x)| } = 0 .\nWe are now ready to prove Lemma 4.1. The main idea is to expand f into its Taylor series, and then deal with monomials xm one by one:\nProof of Lemma 4.1. Since f (k)(0) \u2265 0 for all k \u2265 0, and since f(z) is analytic, we can write f as f(z) = \u2211\u221e k=0 rkz\nk where each rk is a nonnegative real. Consider the i-th coefficient of Chebyshev series:\nai = 2\u2212 1[i = 0]\n\u03c0\n\u222b 1\n\u22121 f(x)\u221a 1\u2212 x2 Ti(x)dx = 2\u2212 1[i = 0] \u03c0\n\u221e\u2211\nk=0\nrk\n\u222b 1\n\u22121 xk\u221a 1\u2212 x2 Ti(x) \u2265 0\nwhere the last inequality is due to Lemma B.1, and the integral and infinite Taylor sum are interchangeable.15 This implies we can write pn(x) = \u2211n i=0 aiTi(x) where each ai \u2265 0. Since each Ti(1+y) is a polynomial of degree i, it exactly equals to its degree-i Taylor expansion\u2211i k=0 yk k! T (k) i (1). Thus, we have (recall y \u2208 [0, \u03c1])\npn(1 + y) =\nn\u2211\ni=0\naiTi(1 + y) = n\u2211\ni=0\ni\u2211\nk=0\nai k! T (k)i (1)yk =\nn\u2211\nk=0\n1\nk!\n( n\u2211\ni=k\naiT (k)i (1) ) yk .\nDenote by bk,n = (\u2211n i=k aiT (k) i (1) ) . Since for every i, k \u2265 0 it satisfies T (k)i (1) \u2265 0 (which is a factual property of Chebyshev polynomial) and ai \u2265 0, we know bk,n \u2265 0 and moreover bk,n is monotonically non-decreasing in n for each k \u2265 0. On the other hand, Lemma B.2 implies\nlim n\u2192\u221e\n\u2223\u2223p(k)n (1)\u2212 f (k)(1) \u2223\u2223 = lim\nn\u2192\u221e\n\u2223\u2223bk,n \u2212 f (k)(1) \u2223\u2223 = 0 ,\nso we must have 0 \u2264 bk,n \u2264 f (k)(1) for every n \u2208 N (because bk,n is non-decreasing in n). Therefore, for every y \u2208 [0, \u03c1]:\n0 \u2264 pn(1 + y) = n\u2211\nk=0\n1 k! bk,ny\nk \u2264 \u221e\u2211\nk=0\n1 k! bk,ny\nk \u2264 \u221e\u2211\nk=0\n1 k! f (k)(1)yk = f(1 + y) . (B.1)\nFinally, since qn(x) def = \u2211n\nk=0 ckTk(x) is a degree-n Chebyshev interpolation polynomial, the aliasing Lemma 2.8 tells us ci \u2265 0 for every i = 0, 1, . . . , n. Furthermore, applying the aliasing Lemma 2.8 again we have ci \u2265 ai for i = 0, 1, . . . , n but \u2211n i=0 ci = \u2211\u221e i=0 ai. Therefore, using the fact that T (k)i (1) is a monotone increasing function in i (for every fixed k), we have\n0 \u2264 n\u2211\ni=0\nciT (k)i (1) \u2264 \u221e\u2211\ni=0\naiT (k)i (1) = limn\u2192\u221e bk,n = f (k)(1) .\nFinally, an analogous proof as (B.1) also shows 0 \u2264 qn(1 + y) \u2264 f(1 + y) for every y \u2208 [0, \u03c1].\nB.1 Proof of Lemma B.1 Lemma B.1. Let m,n \u2208 N be two integers, then am,n = \u222b 1 \u22121 xm\u221a 1\u2212x2Tn(x)dx \u2265 0.\n15The interchangeability and be verified as follows. Denoting by fm(x) def = \u2211m k=0 rmx\nm, we have fm(x) uniformly converges to f(x) on x \u2208 [\u22121, 1] because the Taylor expansion of any analytical function has local uniform convergence, but [\u22121, 1] is a compact, closed interval so local uniform convergence becomes global uniform convergence.\nFor every \u03b5 > 0, let M be the integer so that for every m \u2265 M it satisfies maxx\u2208[\u22121,1] |fm(x) \u2212 f(x)| \u2264 \u03b5. We compute that \u2223\u2223\u2223 \u222b 1 \u22121 f(x)\u221a 1\u2212x2 Ti(x)dx \u2212 \u2211m k=0 rk \u222b 1 \u22121 xk\u221a 1\u2212x2 Ti(x)dx \u2223\u2223\u2223 = \u2223\u2223\u2223 \u222b 1 \u22121 f(x)\u2212fm(x)\u221a 1\u2212x2 Ti(x)dx \u2223\u2223\u2223 \u2264 \u222b 1 \u22121 \u03b5\u221a 1\u2212x2 dx = \u03b5\u03c0. Therefore, the left hand side converges to zero so the integral and the infinite Taylor sum are interchangeable.\nProof of Lemma B.1. Recall that Tn(\u2212x) = (\u22121)nTn(x). Therefore,\nam,n =\n\u222b \u22121\n1\n(\u2212x)m\u221a 1\u2212 x2\nTn(\u2212x)d(\u2212x) = (\u22121)m+n \u222b 1\n\u22121 xm\u221a 1\u2212 x2 Tn(x)dx = (\u22121)m+nam,n ,\nwhich implies that when m+ n is odd it satisfies am,n = 0. We next focus on the case when m+ n is even. We first consider two base cases:\n\u2022 n = 0,m = 2k: we have x2k \u2265 0 for all x \u2208 [\u22121, 1] so am,n = a2k,0 = \u222b 1 \u22121 x2k\u221a 1\u2212x2dx \u2265 0. \u2022 n = 1,m = 2k+ 1: we have x2k+2 \u2265 0 for all x \u2208 [\u22121, 1] so am,n = a2k+1,1 = \u222b 1 \u22121 x2k+2\u221a 1\u2212x2dx \u2265 0.\nAs for general n \u2265 2, we integrate by parts and have:\nam,n =\n\u222b 0\n\u2212\u03c0\ncosm(\u03b8)\nsin \u03b8 cos(n\u03b8)d(cos \u03b8) =\n\u222b \u03c0\n0 cosm(\u03b8) cos(n\u03b8)d\u03b8\n= 1\nn cosm(\u03b8) sin(n\u03b8)\n\u2223\u2223\u2223\u2223 \u03c0\n0\n\u2212 1 n\n\u222b \u03c0\n0 (\u2212m sin(\u03b8) cosm\u22121(\u03b8)) sin(n\u03b8)d\u03b8\n= m\nn\n\u222b \u03c0\n0 sin(\u03b8) cosm\u22121(\u03b8) sin(n\u03b8)d\u03b8\n= m n2 sin(\u03b8) cosm\u22121(\u03b8)(\u2212 cos(n\u03b8)) \u2223\u2223\u2223\u2223 \u03c0\n0\n\u2212 m n2\n\u222b \u03c0\n0 (\u2212(m\u2212 1) cosm\u22122 \u03b8 +m cosm \u03b8)(\u2212 cos(n\u03b8))d\u03b8\n= \u2212m(m\u2212 1) n2 am\u22122,n + m2 n2 am,n\n=\u21d2 (m2 \u2212 n2)am,n = m(m\u2212 1)am\u22122,n . (B.2) In particular, choosing m = n in (B.2) we have an\u22122,n = 0, and this implies\n\u2200m \u2264 n : am\u22122,n = m2 \u2212 n2 m(m\u2212 1)am,n = 0 .\nAs for an,n for n \u2265 1, we have\nan,n =\n\u222b 1\n\u22121 xn\u221a 1\u2212 x2\nTn(x)dx = \u222b 1\n\u22121 xn\u221a 1\u2212 x2 Tn+1(x) + Tn\u22121(x) 2x dx = 1 2 (an\u22121,n+1 + an\u22121,n\u22121) = 1 2 an\u22121,n\u22121\nand thus by induction we have an,n \u2265 0. Using (B.2) again we conclude that\n\u2200m \u2265 n+ 2: am,n = m(m\u2212 1) m2 \u2212 n2 am\u22122,n \u2265 0 ."}, {"heading": "C Proof of Lemma 5.4", "text": "We first note the following lemma which follows from Lemma 2.10 together with the aliasing Lemma 2.8: Lemma C.1. Suppose f(z) is analytic on E\u03c1 and |f(z)| \u2264 M on E\u03c1. Let qn(x) = \u2211n\ni=0 ciTi(x) be the degree-n Chebyshev interpolation of f , then\n\u2200i \u2208 {0, 1, . . . , n} : |ci| \u2264 2M\n\u03c1+ \u221a 2\u03c1+ \u03c12\n( 1 + \u03c1+ \u221a 2\u03c1+ \u03c12 )\u2212i .\nApplying Lemma C.1 on f(z) = (\n1+\u03ba\u2212z 2\n)\u22121/2 , we have\nLemma 5.4. Let qn(x) = \u2211n k=0 ckTk(x) be the degree-n Chebyshev interpolation of f(x) = ( 1+\u03ba\u2212x 2 )\u22121/2 on [\u22121, 1]. Then,\n\u2200i \u2208 {0, 1, . . . , n} : |ci| \u2264 e \u221a 32(i+ 1)\n\u03ba\n( 1 + \u03ba+ \u221a 2\u03ba+ \u03ba2 )\u2212i\nProof of Lemma 5.4. For each i \u2208 {0, 1, . . . , n}, consider a value \u03c1 \u2208 [\u03ba/2, \u03ba) to be chosen later. We know that f(z) is analytic and satisfies |f(z)| \u2264 \u221a 2\n\u03ba\u2212\u03c1 on E\u03c1. Using Lemma C.1 we have:\n|ci| \u2264\n\u221a 8\n\u03ba\u2212\u03c1\n( 1 + \u03c1+ \u221a 2\u03c1+ \u03c12 )\u2212i\n\u03c1+ \u221a 2\u03c1+ \u03c12 \u2264 1\u221a \u03ba\n\u221a 8\n\u03ba\u2212 \u03c1 ( 1 + \u03c1+ \u221a 2\u03c1+ \u03c12 )\u2212i , (C.1)\nwhere we used \u03ba \u2264 1 in the second inequality. If we take \u03c1 = \u03ba\u2212 \u03ba4(i+1) , we have: (\n1 + \u03ba+ \u221a 2\u03ba+ \u03ba2 1 + \u03c1+ \u221a 2\u03c1+ \u03c12 )i = ( 1 + \u03ba\u2212 \u03c1+ \u221a 2\u03ba+ \u03ba2 \u2212 \u221a 2\u03c1+ \u03c12 1 + \u03c1+ \u221a 2\u03c1+ \u03c12 )i\n\u2264 ( 1 + (\u03ba\u2212 \u03c1) ( 1 + 2 + \u03ba+ \u03c1\u221a\n2\u03ba\n))i\n\u2264 (\n1 + (\u03ba\u2212 \u03c1) 4\u221a \u03ba\n)i \u2264 ( 1 + 1\ni+ 1\n)i \u2264 e .\nPutting this back to (C.1), we have: |ci| \u2264 \u221a 32(i+ 1)\n\u03ba\n( 1 + \u03ba+ \u221a 2\u03ba+ \u03ba2\n1 + \u03c1+ \u221a 2\u03c1+ \u03c12\n)i ( 1 + \u03ba+ \u221a 2\u03ba+ \u03ba2 )\u2212i \u2264 e \u221a 32(i+ 1)\n\u03ba\n( 1 + \u03ba+ \u221a 2\u03ba+ \u03ba2 )\u2212i ."}, {"heading": "D Appendix for Section 6", "text": "Lemma 6.2. ~sN = ~b0 \u2212M~b1 where ~bN+1 def = ~0, ~bN def = ~cN , and \u2200r \u2208 {N \u2212 1, . . . , 0} : ~br def= 2M~br+1 \u2212~br+2 + ~cr \u2208 Rd .\nProof. We write ~sN = t >c where t = (T0(M), . . . , TN (M))> and c = (~c0, . . . ,~cN )>. Recall that the recursive formula of Chebyshev polynomial tells us\nNt def =   I \u22122M I I \u22122M I . . . . . . . . .\nI \u22122M I\n    T0(M) T1(M) T2(M)\n... TN (M)\n  =   I \u2212M\n0 ... 0\n  def = w .\nIn addition, it is easy to verify that the ~br sequence satisfies N >b = c if we denote by b def= (~b0, . . . ,~bN ) >. Therefore, we have ~sN = t>c = t>N>b = w>b = ~b0 \u2212M~b1 as desired. Fact D.1. ~br = \u2211N k=r Uk\u2212r(M)~ck for every r \u2208 {0, 1, . . . , N + 1}.\nProof. This can be deduced directly from the recursive formula of Chebyshev polynomials of the second kind. See for instance Equation (3.120) of [14].\nD.1 Proof of Theorem 6.4\nTheorem 6.4. For every N \u2208 N\u2217, suppose the eigenvalues of M are in [a, b] and suppose there are parameters CU \u2265 1, CT \u2265 1, \u03c1 \u2265 1, Cc \u2265 0 satisfying \u2200k \u2208 {0, 1, . . . , N} : { \u03c1k\u2016~ck\u2016 \u2264 Cc \u2227 \u2200x \u2208 [a, b] : |Tk(x)| \u2264 CT\u03c1k and |Uk(x)| \u2264 CU\u03c1k } .\nThen, if the inexact backward recurrence in Def. 6.3 is applied with \u03b5 \u2264 14NCU , we have \u2016s\u0302N \u2212 ~sN\u2016 \u2264 \u03b5 \u00b7 2(1 + 2NCT )NCUCc . Proof of Theorem 6.4. We first note that according to ~bn = \u2211N\nk=n Uk\u2212n(M)~ck from Fact D.1, we have\n\u2200n \u2208 {0, 1, . . . , N} : \u2016~bn\u2016 \u2264 N\u2211\nk=n\n\u2225\u2225Uk\u2212n(M) \u2225\u2225 2 \u2016~ck\u2016 \u2264 (N \u2212 n+ 1) \u00b7 \u03c1\u2212n \u00b7 CUCc . (D.1)\nDenoting by ~\u03b7r def = M ( b\u0302r ) \u2212Mb\u0302r, we have\n\u2200r \u2208 {N \u2212 1, . . . , 0} : b\u0302r = 2Mb\u0302r+1 \u2212 b\u0302r+2 + ~cr + 2~\u03b7r+1 and s\u0302N = b\u03020 + Mb\u03021 + ~\u03b71 , and therefore if we denote by (\u03b4~b)r = b\u0302r \u2212~br, we have\n(\u03b4~b)N+1 = 0, (\u03b4~b)N = 0, and \u2200r \u2208 {N \u2212 1, . . . , 0} : (\u03b4~b)r = 2M(\u03b4~b)r+1 \u2212 (\u03b4~b)r+2 + 2~\u03b7r+1 . In other words, the {(\u03b4~b)r}r sequence also satisfies the recursive formula in Lemma 6.2 where ~ck is replaced with 2~\u03b7k+1. This implies, according to Lemma 6.2,\n(\u03b4~b)0 \u2212M(\u03b4~b)1 = 2 N\u22121\u2211\nk=0\nTk(M)~\u03b7k+1\nand therefore\ns\u0302N \u2212 ~sN = (\u03b4~b)0 \u2212M(\u03b4~b)1 + ~\u03b71 = ~\u03b71 + 2 N\u22121\u2211\nk=0\nTk(M)~\u03b7k+1 (D.2)\nAt the same time, applying Fact D.1 on sequence {(\u03b4~b)r}r, we have\n(\u03b4~b)r = 2 N\u22121\u2211\nk=r\nUk\u2212r(M)~\u03b7k+1 (D.3)\nNow we are ready to prove that, as long as \u03b5 \u2264 14NCU , it satisfies\n\u2200k \u2208 [N ] : \u2016~\u03b7k\u2016 \u2264 \u03b5 \u00b7 \u03c1\u2212k(2NCUCc) and \u2016(\u03b4~b)k\u22121\u2016 \u2264 \u03b5 \u00b7 \u03c1\u2212k(4N2C2UCc) We prove this by reverse double induction.\n\u2022 In the base case, \u2016~\u03b7N\u2016 \u2264 \u03b5\u2016b\u0302N\u2016 = \u03b5\u2016~bN\u2016 \u2264 \u03b5\u03c1\u2212NCUCc where the first inequality uses our assumption on M and the second uses (D.1). \u2022 Suppose the upper bound \u2016~\u03b7k\u2016 \u2264 \u03b5 \u00b7 \u03c1\u2212k(2NCUCc) holds for every k \u2265 k0, then\n\u2016(\u03b4~b)k0\u22121\u2016 \u2264 2 N\u22121\u2211\nk=k0\u22121\n\u2225\u2225\u2225Uk\u2212k0+1(M)~\u03b7k+1 \u2225\u2225\u2225 \u2264 2CU\u03c1\u2212k0 \u00b7 N\u22121\u2211\nk=k0\u22121 \u2016\u03c1k+1~\u03b7k+1\u2016\n\u2264 2NCU\u03c1\u2212k0 \u00b7 (\u03b5 \u00b7 2NCUCc) = \u03b5 \u00b7 \u03c1\u2212k0(4N2C2UCc) . Above, the first inequality is by (D.3) and triangle inequality, the second is by the definition of CU , the third is by inductive assumption.\n\u2022 Suppose the upper bound \u2016(\u03b4~b)k\u22121\u2016 \u2264 \u03b5 \u00b7 \u03c1\u2212k(4N2C2UCc) holds for k = k0 + 1, then \u2016~\u03b7k0\u2016 \u2264 \u03b5\u2016b\u0302k0\u2016 \u2264 \u03b5 ( \u2016~bk0\u2016+ \u2016(\u03b4~b)k0\u2016 ) \u2264 \u03b5\u03c1\u2212k0 ( NCUCc + 4\u03b5\u03c1 \u22121N2C2UCc )\n= \u03b5\u03c1\u2212k0NCUCc(1 + 4\u03c1\u22121\u03b5NCU ) \u2264 2\u03c1\u2212k0NCUCc . Above, the first inequality is by our assumption on M, the second is by triangle inequality, the third is by (D.1) and our inductive assumption, and the last is by our assumption on \u03b5.\nFinally, using (D.2), we have\n\u2016s\u0302N \u2212 ~sN\u2016 \u2264 \u2016~\u03b71\u2016+ 2 N\u22121\u2211\nk=0\n\u2225\u2225Tk(M)\u20162\u2016~\u03b7k+1\u2016 \u2264 \u03b5(2NCUCc) + 2N\u03b5CT (2NCUCc)\n\u2264 \u03b5 \u00b7 2(1 + 2NCT )NCUCc ."}, {"heading": "E Appendix for Section 7", "text": "Fact 7.1. (P\u03bb)\u03c7 = I+sgn(S) 2 where S def = 2(A>A + \u03bbI)\u22121A>A\u2212 I = (A>A + \u03bbI)\u22121(A>A\u2212 \u03bbI). Proof. This is so because S shares the same eigenspace as A>A and maps all the eigenvalues of A>A above threshold \u03bb to eigenvalues of S between 0 and 1, and all the eigenvalues below \u03bb to eigenvalues of S between \u22121 and 0. Therefore, if applied to function sgn(x)+12 , we have that sgn(S)+I\n2 zeros out all the eigenvalues of S between \u22121 and 0, and thus equivalently zeros out all the eigenvalues of A>A below threshold \u03bb. This is exactly the same as the projection matrix P\u03bb.\nE.1 Proof of Theorem 7.3\nTheorem 7.3 (restated). Given A \u2208 Rd\u2032\u00d7d and \u03bb, \u03b3 \u2208 (0, 1), assume that the singular values of A are in the range [0, \u221a (1\u2212 \u03b3)\u03bb]\u222a[ \u221a (1 + \u03b3)\u03bb, 1]. Given \u03c7 \u2208 Rd and b \u2208 Rd\u2032, denote by \u03be\u2217 = P\u03bb\u03c7 and x\u2217 = (A>A)\u22121P\u03bbA>b the exact PCP and PCR solutions. If ApxRidge is an \u03b5\u2032-approximate ridge regression solver, then\nthe output \u03be \u2190 QuickPCP(A, \u03c7, \u03bb, \u03b3, n) satisfies \u2016\u03be\u2217 \u2212 \u03be\u2016 \u2264 \u03b5\u2016\u03c7\u2016 if n = \u0398 ( \u03b3\u22121 log 1\u03b3\u03b5 ) and log(1/\u03b5\u2032) = \u0398 ( log 1\u03b3\u03b5 ) ; the output x\u2190 QuickPCR(A, b, \u03bb, \u03b3, n,m) satisfies \u2016x\u2212 x\u2217\u2016 \u2264 \u03b5\u2016b\u2016 if n = \u0398 ( \u03b3\u22121 log 1\u03b3\u03bb\u03b5 ) , m = \u0398 ( log 1\u03b3\u03b5 ) and log(1/\u03b5\u2032) = \u0398 ( log 1\u03b3\u03bb\u03b5 ) .\nProof of Theorem 7.3. The eigenvalues of S def = (A>A + \u03bbI)\u22121(A>A\u2212 \u03bbI) are in the range\n[ \u2212 1,\u2212(1 + \u03b3)\u2212 1 1 + (1\u2212 \u03b3) ] \u222a [1\u2212 (1\u2212 \u03b3) 1 + (1 + \u03b3) , 1\u2212 \u03bb 1 + \u03bb ] \u2286 [ \u2212 1,\u2212\u03b1 ] \u222a [ \u03b1, 1 ] .\nbecause \u03b1 = \u03b3/(2+\u03b3). Therefore, according to Theorem 5.1, gn(S)\u03c7 satisfies \u2016gn(S)\u03c7\u2212sgn(S)\u03c7\u2016 \u2264 \u03b5\u2016\u03c7\u2016 for every \u03c7 \u2208 Rd which in turns implies \u201612(gn(S) + I)\u03c7\u2212P\u03bb\u03c7\u2016 \u2264 \u03b5\u2016\u03c7\u2016.\nWe now analyze stability. Denote by M = (1 + \u03ba)I \u2212 2S2 and recall that gn(S) = Sqn(M) = Sqn ( (1 + \u03ba)I \u2212 2S2 ) where \u03ba = 2\u03b12. We wish to apply Theorem 6.4 to show that qn(M)\u03c7 can be computed in a stable manner and therefore gn(S)\u03c7 as well. We verify the assumptions of Theorem 6.4 below:\n\u2022 Since ApxRidge is \u03b5\u2032-approximate (see Def. 2.3), we have that Line 6 of QuickPCP corresponds to an approximate algorithm\nM(\u03c7) = (1 + \u03ba)\u03c7\u2212 2MultS(A, \u03bb, MultS(A, \u03bb, \u03c7)) satisfying \u2016M\u03c7\u2212M(\u03c7)\u2016 \u2264 O(\u03b5\u2032)\u2016\u03c7\u2016 for every vector \u03c7. \u2022 Recall that qn(\u00b7) is a Chebyshev sum with coefficients at most O(1/ \u221a \u03ba) = O(1/\u03b1) = O(1/\u03b3)\naccording to Def. 2.7. Thus, we can choose \u03c1 = 1 and Cc = O(1/\u03b3) in Theorem 6.4.\n\u2022 Since the eigenvalues of M are in [\u22121, 1] and |Tk(x)| \u2264 1 and |Uk(x)| \u2264 n + 1 for every x \u2208 [\u22121, 1] (see Fact 2.6), we can choose CT = 1 and CU = n+ 1 in Theorem 6.4.\nThe conclusion of Theorem 6.4 tells us that our approximate backward recurrence in QuickPCP computes gn(S)\u03c7 up to an accuracy O(\u03b5\n\u2032\u03b3\u22121n3) \u00b7 \u2016\u03c7\u2016. In other words, as long as log(1/\u03b5\u2032) \u2264 O(log n\u03b5\u03b3 ), we can approximately compute 1 2(gn(S) + I)\u03c7 within accuracy O(\u03b5) \u00b7 \u2016\u03c7\u2016.\nCombining everything above, we conclude that choosing n = \u0398(\u03b3\u22121 log(1/\u03b3\u03b5)) and log(1/\u03b5\u2032) = \u0398(log n\u03b5\u03b3 ) = \u0398(log 1 \u03b5\u03b3 ), we can satisfy \u2016\u03be\u2217 \u2212 \u03be\u2016 \u2264 \u03b5\u2016\u03c7\u2016.\nAs for the PCR guarantee, we simply replace \u03b5 with \u03b5 \u00b7 \u221a \u03bb/m2 and \u03c7 with A>b in the above analysis. Then we apply Lemma 3.4, and conclude that choosing n = \u0398(\u03b3\u22121 log(1/\u03b3\u03bb\u03b5)), m = \u0398(log(1/\u03b5\u03b3)), and log(1/\u03b5\u2032) = \u0398(log 1\u03b5\u03b3 ), it satisfies \u2016x\u2217 \u2212 x\u2016 \u2264 \u03b5\u2016b\u2016.\nE.2 Proof of Theorem 7.4\nTheorem 7.4 (restated). Given A \u2208 Rd\u2032\u00d7d, \u03bb \u2208 (0, 1), and \u03b3 \u2208 (0, 2/3], assume that the singular values of A are no more than 1. Given \u03c7 \u2208 Rd and b \u2208 Rd\u2032, and suppose ApxRidge is an \u03b5\u2032-approximate ridge regression solver, then\nthe output \u03be \u2190 QuickPCP(A, \u03c7, \u03bb, \u03b3, n) is (\u03b3, \u03b5)-approximate PCP if n = \u0398 ( \u03b3\u22121 log 1\u03b3\u03b5 ) and log(1/\u03b5\u2032) = \u0398 ( log 1\u03b3\u03b5 ) ; the output x\u2190 QuickPCR(A, b, \u03bb, \u03b3, n,m) is (\u03b3, \u03b5)-approximate PCR if n = \u0398 ( \u03b3\u22121 log 1\u03b3\u03bb\u03b5 ) , m = \u0398 ( log 1\u03b3\u03b5 ) and log(1/\u03b5\u2032) = \u0398 ( log 1\u03b3\u03bb\u03b5 ) .\nProof of Theorem 7.4. Consider the same S = (A>A + \u03bbI)\u22121(A>A \u2212 \u03bbI), \u03b1 = \u03b3/(2 + \u03b3), and \u03ba = 2\u03b12 as before. We observe that A>A and S share the same eigenspace. Furthermore, the eigenvalues of A>A in the range\n(1) : [(1 + \u03b3)\u03bb, 1] (2) : [0, (1\u2212 \u03b3)\u03bb] (3) : ( (1\u2212 \u03b3)\u03bb, (1 + \u03b3)\u03bb ) (E.1)\nrespectively map to the eigenvalues of S in the range16\n(1) : [\u03b1, 1] (2) : [\u22121,\u2212\u03b1] (3) : (\u22121, 1)\nLet us now write \u03c7 = \u22113\ni=1 \u2211 k\u2208\u039bi \u03b2k\u03bdk where \u039bi \u2286 [d] consists of the indices k where \u03bbk is in\nthe i-th interval in (E.1), and \u03b2k \u2208 R is the weight. We thus have\n\u03be\u2032 def= gn(S) + I\n2 \u03c7 =\n3\u2211\ni=1\n\u2211\nk\u2208\u039bi\ngn(\u03bbk) + \u03bbk 2 \u03b2k\u03bdk .\n16More precisely, given transformation f : x 7\u2192 x\u2212\u03bb x+\u03bb\n, it satisfies (1) f ( [(1 + \u03b3)\u03bb, 1] ) \u2286 [\u03b1, 1], (2) f ( [0, (1 \u2212 \u03b3)\u03bb] ) \u2286\n[\u22121,\u2212\u03b1], and (3) f ( ((1\u2212 \u03b3)\u03bb, (1 + \u03b3)\u03bb) ) \u2286 (\u22121, 1).\nSince for every k \u2208 \u039b1 \u222a\u039b2 it satisfies \u03bbk \u2208 [\u22121,\u2212\u03b1]\u222a [\u03b1, 1], we can apply Theorem 5.1 (and using n \u2265 1\u221a\n2\u03b1 log 3 \u03b5\u03b12 ):\n1. \u2016P(1+\u03b3)\u03bb(\u03be\u2032 \u2212 \u03c7)\u2016 = \u2016 \u2211 k\u2208\u039b1 (gn(\u03bbk)+\u03bbk 2 \u2212 1 ) \u03b2k\u03bdk\u2016 \u2264 \u03b5\u2016\u03c7\u2016. 2. \u2016(I\u2212P(1\u2212\u03b3)\u03bb)\u03be\u2032\u2016 = \u2016 \u2211 k\u2208\u039b2 (gn(\u03bbk)+\u03bbk 2 ) \u03b2k\u03bdk\u2016 \u2264 \u03b5\u2016\u03c7\u2016. 3. \u2200k \u2208 \u039b3, |\u3008\u03bdi, \u03be\u2032 \u2212 \u03c7\u3009| = \u2223\u2223gn(\u03bbk)+\u03bbk 2 \u2212 1 \u2223\u2223 \u00b7 |\u03b2k| \u2264 |\u03b2k| = |\u3008\u03bdi, \u03c7\u3009|. (Here, the last inequality is\nbecause if \u03bbk \u2265 0 then gn(\u03bbk) +\u03bbk \u2208 [\u03bbk, 1 +\u03bbk] and if \u03bbk < 0 then gn(\u03bbk) +\u03bbk \u2208 [\u03bbk\u22121, \u03bbk].) Note that these two guarantees correspond to the three properties for approximate PCP (see Def. 3.1), and thus we are left to deal with stability by applying Theorem 6.4. In other words, denoting by M = (1+\u03ba)I\u22122S2 and recalling that gn(S) = Sqn(M), we wish to apply Theorem 6.4 to show that qn(M)\u03c7 can be computed in a stable manner and therefore qn(S)+I 2 \u03c7 as well. We verify the assumptions of Theorem 6.4 below:\n\u2022 As before, Line 6 of QuickPCP corresponds to an approximate algorithm M(\u03c7) satisfying \u2016M\u03c7\u2212M(\u03c7)\u2016 \u2264 O(\u03b5\u2032)\u2016\u03c7\u2016 for every vector \u03c7. \u2022 qn(\u00b7) is a Chebyshev sum with coefficients satisfying |ci| \u2264 O( \u221a i/\u03ba) ( 1 + \u03ba + \u221a 2\u03ba+ \u03ba2\n)\u2212i according to Lemma 5.4. Therefore, we can choose \u03c1 = 1 + \u03ba+ \u221a 2\u03ba+ \u03ba2 and Cc = O(n/\u03ba) = O(n/\u03b32) in Theorem 6.4.\n\u2022 Since the eigenvalues of M are in [\u22121, 1 + \u03ba], we have for every x \u2208 [\u22121, 1 + \u03ba], it satisfies |Tk(x)| \u2264 (1 + \u03ba + \u221a 2\u03ba+ \u03ba2)k and |Uk(x)| \u2264 12\u221a2\u03ba+\u03ba2 (1 + \u03ba + \u221a 2\u03ba+ \u03ba2)n+1. Therefore, we\ncan choose CT = 1 and CU = O( 1 \u03ba) in Theorem 6.4.\nFinally, the conclusion of Theorem 6.4 tells us that our approximate backward recurrence in QuickPCP computes qn(S)\u03c7 up to an accuracy O(\u03b5\n\u2032\u03b3\u22124n3) \u00b7 \u2016\u03c7\u2016. In other words, as long as log(1/\u03b5\u2032) \u2264 O(log n\u03b5\u03b3 ), we can approximately compute \u03be\u2032 = 12(gn(S) + I)\u03c7 within accuracy \u03b5 \u00b7 \u2016\u03c7\u2016, or equivalently \u2016\u03be \u2212 \u03be\u2032\u2016 \u2264 \u03b5\u2016\u03c7\u2016. Together with our analysis at the beginning of the proof, we have\n1. \u2016P(1+\u03b3)\u03bb(\u03be \u2212 \u03c7)\u2016 \u2264 2\u03b5\u2016\u03c7\u2016. 2. \u2016(I\u2212P(1\u2212\u03b3)\u03bb)\u03be\u2016 \u2264 2\u03b5\u2016\u03c7\u2016. 3. \u2200k \u2208 \u039b3, |\u3008\u03bdi, \u03be \u2212 \u03c7\u3009| \u2264 |\u03b2k| = |\u3008\u03bdi, \u03c7\u3009|+ \u03b5\u2016\u03c7\u2016.\nThis finishes proving that \u03be is an (\u03b3,O(\u03b5))-approximate PCP solution when n = \u0398 ( \u03b3\u22121 log 1\u03b3\u03b5 ) and\nlog(1/\u03b5\u2032) = \u0398 ( log 1\u03b3\u03b5 ) .\nAs for the PCR guarantee, we simply replace \u03b5 with \u03b5 \u00b7 \u03bb/m2 and \u03c7 with A>b in the above analysis. Then we apply Lemma 3.5, and conclude that choosing n = \u0398(\u03b3\u22121 log(1/\u03b5\u03bb\u03b3)), m = \u0398(log(1/\u03b5\u03b3)), and log(1/\u03b5\u2032) = \u0398(log 1\u03b5\u03b3 ), it satisfies that x is a (\u03b3, \u03b5)-approximate PCR solution."}], "references": [{"title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "author": ["Zeyuan Allen-Zhu"], "venue": "In STOC,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "First Efficient Convergence for Streaming k-PCA: a Global, Gap- Free, and Near-Optimal Rate", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "ArXiv e-prints,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Even faster accelerated coordinate descent using non-uniform sampling", "author": ["Zeyuan Allen-Zhu", "Peter Richt\u00e1rik", "Zheng Qu", "Yang Yuan"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Faster SVD-truncated regularized least-squares", "author": ["Christos Boutsidis", "Malik Magdon-Ismail"], "venue": "IEEE International Symposium on Information Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Computing truncated singular value decomposition least squares solutions by rank revealing QR-factorizations", "author": ["Tony F Chan", "Per Christian Hansen"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1990}, {"title": "Error analysis of an algorithm for summing certain finite series", "author": ["David Elliott"], "venue": "Journal of the Australian Mathematical Society,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1968}, {"title": "Uniform approximation of sgn x by polynomials and entire functions", "author": ["Alexandre Eremenko", "Peter Yuditskii"], "venue": "Journal d\u2019Analyse Mathe\u0301matique,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Polynomials of the best uniform approximation to sgn (x) on two intervals", "author": ["Alexandre Eremenko", "Peter Yuditskii"], "venue": "Journal d\u2019Analyse Mathe\u0301matique,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "LIBSVM Data: Classification, Regression and Multi-label", "author": ["Rong-En Fan", "Chih-Jen Lin"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Principal Component Projection Without Principal Component Analysis", "author": ["Roy Frostig", "Cameron Musco", "Christopher Musco", "Aaron Sidford"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Numerical Methods for Special Functions. Society for Industrial and Applied Mathematics, jan", "author": ["Amparo Gil", "Javier Segura", "Nico M. Temme"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Approximating the spectral sums of large-scale matrices using chebyshev approximations", "author": ["Insu Han", "Dmitry Malioutov", "Haim Avron", "Jinwoo Shin"], "venue": "arXiv preprint arXiv:1606.00942,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Functions of Matrices", "author": ["N. Higham"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Randomized block krylov methods for stronger and faster approximate singular value decomposition", "author": ["Cameron Musco", "Christopher Musco"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Computing fundamental matrix decompositions accurately via the matrix sign function in two iterations: The power of zolotarev\u2019s functions", "author": ["Yuji Nakatsukasa", "Roland W Freund"], "venue": "SIAM Review,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Introductory Lectures on Convex Programming Volume: A Basic course, volume I", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Model order reduction: theory, research aspects and applications, volume", "author": ["Wilhelmus H.A. Schilders", "Henk A. Van der Vorst", "Joost Rommes"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "author": ["Jonathan Richard Shewchuk"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "Approximation Theory and Approximation Practice", "author": ["Lloyd N. Trefethen"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Numerical methods for the qcdd overlap operator", "author": ["Jasper van den Eshof", "Andreas Frommer", "Th Lippert", "Klaus Schilling", "Henk A. van der Vorst"], "venue": "i. sign-function and error bounds. Computer Physics Communications,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}], "referenceMentions": [{"referenceID": 16, "context": "For instance, to project a vector onto the top 1000 principal components of a high-dimensional dataset, even the most efficient Krylov-based [18] or Lanczos-based [4] methods require a running time that is proportional to 1000 \u00d7 40 = 4 \u00d7 104 times the input matrix sparsity, if the Krylov or Lanczos method is executed for 40 iterations.", "startOffset": 141, "endOffset": 145}, {"referenceID": 3, "context": "For instance, to project a vector onto the top 1000 principal components of a high-dimensional dataset, even the most efficient Krylov-based [18] or Lanczos-based [4] methods require a running time that is proportional to 1000 \u00d7 40 = 4 \u00d7 104 times the input matrix sparsity, if the Krylov or Lanczos method is executed for 40 iterations.", "startOffset": 163, "endOffset": 166}, {"referenceID": 11, "context": "[13] that approximate PCP and PCR be solved with a running time independent of the number of principal components above threshold \u03bb.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "This is known as the eigengap assumption, which is rarely satisfied in practice [18].", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "In other words, \u2022 to project any vector \u03c7 \u2208 Rd to top principal components, we can compute g(S)\u03c7 instead; and Ridge regression is often considered as an easy-to-solve machine learning problem: using for instance SVRG [17], one can usually solve ridge regression to an 10\u22128 accuracy with at most 40 passes of the data.", "startOffset": 217, "endOffset": 221}, {"referenceID": 15, "context": "Efficient routines such as SVRG [17] solve ridge regression and thus compute Su for any u \u2208 Rd, with running times only logarithmically in 1/\u03b5\u2032.", "startOffset": 32, "endOffset": 36}, {"referenceID": 8, "context": "1) is \u0398 ( \u03b3\u22121 log(1/\u03b5) ) [9, 10].", "startOffset": 25, "endOffset": 32}, {"referenceID": 9, "context": "1) is \u0398 ( \u03b3\u22121 log(1/\u03b5) ) [9, 10].", "startOffset": 25, "endOffset": 32}, {"referenceID": 5, "context": "There are a few attempts to reduce the cost of PCA when solving PCR, by for instance approximating the matrix AP\u03bb where P\u03bb is the PCP projection matrix [6, 7].", "startOffset": 152, "endOffset": 158}, {"referenceID": 6, "context": "There are a few attempts to reduce the cost of PCA when solving PCR, by for instance approximating the matrix AP\u03bb where P\u03bb is the PCP projection matrix [6, 7].", "startOffset": 152, "endOffset": 158}, {"referenceID": 1, "context": "A significant number of papers have focused on the low-rank case of PCA [2, 4, 18] and its online variant [3].", "startOffset": 72, "endOffset": 82}, {"referenceID": 3, "context": "A significant number of papers have focused on the low-rank case of PCA [2, 4, 18] and its online variant [3].", "startOffset": 72, "endOffset": 82}, {"referenceID": 16, "context": "A significant number of papers have focused on the low-rank case of PCA [2, 4, 18] and its online variant [3].", "startOffset": 72, "endOffset": 82}, {"referenceID": 2, "context": "A significant number of papers have focused on the low-rank case of PCA [2, 4, 18] and its online variant [3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 19, "context": "Several results have addressed Krylov methods for applying the sign function in the so-called Krylov subspace, without explicitly constructing any approximate polynomial [21, 24].", "startOffset": 170, "endOffset": 178}, {"referenceID": 22, "context": "Several results have addressed Krylov methods for applying the sign function in the so-called Krylov subspace, without explicitly constructing any approximate polynomial [21, 24].", "startOffset": 170, "endOffset": 178}, {"referenceID": 14, "context": "4 Other iterative methods have also been proposed, see Section 5 of textbook [16].", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "[19]) provide rational approximations to the matrix sign function as opposed to polynomial approximations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] differ from these cited works, because we have only accessed an approximate ridge regression oracle, so ensuring a polynomial approximation to the sign function and ensuring its stability are crucial.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Perhaps the most celebrated example is to approximate S\u22121 using polynomials on S, used in the analysis of conjugate gradient [22].", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "[15] used Chebyshev polynomials to approximate the trace of the matrix sign function, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Following the tradition of [13] and keeping the notations light, we assume without loss of generality that \u03c3max(A) \u2264 1.", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.", "startOffset": 37, "endOffset": 41}, {"referenceID": 18, "context": "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.", "startOffset": 125, "endOffset": 129}, {"referenceID": 0, "context": "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.", "startOffset": 160, "endOffset": 163}, {"referenceID": 4, "context": "Most notably, (1) Conjugate gradient [22] or accelerated gradient descent [20] gives fastest full-gradient methods; (2) SVRG [17] and its acceleration Katyusha [1] give the fastest stochastic-gradient method; and (3) NUACDM [5] gives the fastest coordinate-descent method.", "startOffset": 224, "endOffset": 227}, {"referenceID": 21, "context": "6 ([23]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "2 of [23]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "2 of [23]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 11, "context": "[13] work only with matrices A that satisfy the eigengap assumption, that is, A has no singular value in the range [ \u221a \u03bb(1\u2212 \u03b3), \u221a \u03bb(1 + \u03b3)].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13], this computation is problematic if \u03be is only approximate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] showed that Lemma 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "\u2022 gn(x) \u2208 [0, 1] for every x \u2208 [0, \u03b1] and gn(x) \u2208 [\u22121, 0] for every x \u2208 [\u2212\u03b1, 0].", "startOffset": 10, "endOffset": 16}, {"referenceID": 8, "context": "Note that our degree n = O ( \u03b1\u22121 log(1/\u03b1\u03b5) ) is near-optimal, because the minimum degree for a polynomial to satisfy even only the first item is \u0398 ( \u03b1\u22121 log(1/\u03b5) ) [9, 10].", "startOffset": 164, "endOffset": 171}, {"referenceID": 9, "context": "Note that our degree n = O ( \u03b1\u22121 log(1/\u03b1\u03b5) ) is near-optimal, because the minimum degree for a polynomial to satisfy even only the first item is \u0398 ( \u03b1\u22121 log(1/\u03b5) ) [9, 10].", "startOffset": 164, "endOffset": 171}, {"referenceID": 8, "context": "However, the results of [9, 10] are not constructive, and thus may not lead to stable matrix polynomials.", "startOffset": 24, "endOffset": 31}, {"referenceID": 9, "context": "However, the results of [9, 10] are not constructive, and thus may not lead to stable matrix polynomials.", "startOffset": 24, "endOffset": 31}, {"referenceID": 7, "context": "2 with the help from Elloit\u2019s forward-backward transformation [8].", "startOffset": 62, "endOffset": 65}, {"referenceID": 12, "context": "We wish to point out that although Chebyshev polynomials are known to be stable under error when computed on scalars [14], it is not immediately clear why it holds also for matrices.", "startOffset": 117, "endOffset": 121}, {"referenceID": 11, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In contrast, the number of ridge-regression oracle calls was \u0398(\u03b3\u22122 log 1 \u03b3\u03b5) for PCP and \u0398(\u03b3 \u22122 log 1 \u03b3\u03bb\u03b5) for PCR in [13].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "In the same way as [13], we conclude this paper with an empirical evaluation to demonstrate our theorems.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "\u2022 We generate the synthetic dataset in the same way as [13].", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "\u2022 As for the real-life dataset, we use mnist [11].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "[13] (which we call FMMS for short) and minimized the number of calls to ridge regression in our implementations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The x-axis of these plots represent the number of calls to ridge regression, and in Figure 2 we use exact implementations of ridge regression similar to the experiments in [13].", "startOffset": 172, "endOffset": 176}, {"referenceID": 11, "context": "Note that the horizontal axis starts with 0 for projection performances (second and third column) and with 10 This is a cheap procedure and for instance can be done by power method [13].", "startOffset": 181, "endOffset": 185}, {"referenceID": 15, "context": "\u2022 ridge-SVRG: we run the SVRG [17] method for 50 passes to solve each ridge regression.", "startOffset": 30, "endOffset": 34}, {"referenceID": 2, "context": "The update rule of sk tells us that \u2200i \u2208 [3], k \u2265 1: vi,k = 1 \u03bb k \u2211", "startOffset": 41, "endOffset": 44}, {"referenceID": 21, "context": "1 of [23]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "120) of [14].", "startOffset": 8, "endOffset": 12}], "year": 2017, "abstractText": "We solve principal component regression (PCR), up to a multiplicative accuracy 1 + \u03b3, by reducing the problem to \u00d5(\u03b3\u22121) black-box calls of ridge regression. Therefore, our algorithm does not require any explicit construction of the top principal components, and is suitable for large-scale PCR instances. In contrast, previous result requires \u00d5(\u03b3\u22122) such black-box calls. We obtain this result by developing a general stable recurrence formula for matrix Chebyshev polynomials, and a degree-optimal polynomial approximation to the matrix sign function. Our techniques may be of independent interests, especially when designing iterative methods.", "creator": "LaTeX with hyperref package"}}}