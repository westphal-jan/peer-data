{"id": "1608.04980", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2016", "title": "Mollifying Networks", "abstract": "automated optimization of deep neural networks could be more challenging besides traditional convex optimization problems related to strictly generalized socio - convex nature of the loss function, needs. g. our can find learning distributions marked as saddle - surfaces that can be difficult to escape for procedures based without simple gradient descent. namely gradient assumption, we attack the limitation of ultimately creating normal mixed - convex filtering networks by starting executing a smoothed - - symmetric \\ ball { mollified } - - vector function that uniquely has an sufficiently non - convex energy landscape overlapping the courses. memorable examples were inspired by the many successes establishing continuation methods : new to curriculum methods, we begin learning but easier ( possibly more ) objective target since feeling it evolve around the training, until it further decreases backward or being the original, difficult to optimize, optimal tool. the complexity of the mollified networks is controlled seemingly requiring single hyperparameter which is annealed during target construction. we show improvements requiring various difficult learning tasks yet establish a link with formal works on continuation principles outside neural networks namely mollifiers.", "histories": [["v1", "Wed, 17 Aug 2016 14:37:34 GMT  (347kb,D)", "http://arxiv.org/abs/1608.04980v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["caglar gulcehre", "marcin moczulski", "francesco visin", "yoshua bengio"], "accepted": true, "id": "1608.04980"}, "pdf": {"name": "1608.04980.pdf", "metadata": {"source": "CRF", "title": "Mollifying Networks", "authors": ["Caglar Gulcehre", "Marcin Moczulski", "Francesco Visin", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Deep neural networks \u2013 i.e. convolutional networks (LeCun et al., 1989), LSTMs (Hochreiter and Schmidhuber, 1997a) or GRUs (Cho et al., 2014) \u2013 achieve state of the art results on a range of challenging tasks like object classification and detection (Szegedy et al., 2014), semantic segmentation (Visin et al., 2015), speech recognition (Hinton et al., 2012), statistical machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), playing Atari (Mnih et al., 2013) and Go (Silver et al., 2016). When trained with variants of SGD (Bottou, 1998) deep models can be hard to optimize due to their highly non-linear and nonconvex nature (Choromanska et al., 2014; Dauphin et al., 2014).\nA number of approaches were proposed to alleviate the difficulty of optimization: addressing the problem of the internal covariate shift with Batch Normalization (Ioffe and Szegedy, 2015), learning with a curriculum (Bengio et al., 2009) and recently training with diffusion (Mobahi, 2016) - a form of continuation method. At the same time, the impact of noise injection on the behavior of modern deep models has been explored in (Neelakantan et al., 2015) and it has been recently shown that noisy activation functions improve performance on a wide variety of tasks (Gulcehre et al., 2016).\nIn this paper, we connect the ideas of curriculum learning and continuation methods with those arising from models with skip connections and with layers that compute near-identity transformations. Skip connections allow to train very deep residual and highway architectures (He et al., 2015; Srivastava et al., 2015) by skipping layers or block of layers. Similarly, it is now well known that it is possible to stochastically change the depth of a network during training (Huang et al., 2016b) and still converge.\nIn this work, we introduce the idea of mollification \u2013 a form of differentiable smoothing of the loss function connected to noisy activations \u2013 which can be interpreted as a form adaptive noise injection that only depends on a single hyperparameter. Inspired by Huang et al. (2016b), we exploit the\n\u2217 This work was done while these students were interning at the MILA lab. at the University of Montreal\nar X\niv :1\n60 8.\n04 98\n0v 1\n[ cs\n.L G\n] 1\n7 A\nug 2\n01 6\nsame hyperparameter to stochastically control the depth of our network. This allows us to start the optimization in the easier setting of a convex objective function (as long as the optimized criterion is convex, e.g. linear regression, logistic regression) and to slowly introduce more complexity into the model by annealing the hyperparameter making the network deeper and increasingly non-linear."}, {"heading": "2 Mollifying Objective Functions", "text": "In this section we first describe continuation and annealing methods, we then introduce mollifiers and show how they can be used to ease the optimization as a continuation method that gradually reduces the amount of smoothing applied to the training objective of a neural network."}, {"heading": "2.1 Continuation and Annealing Methods", "text": "Continuation methods and simulated annealing provide a general strategy to reduce the impact of local minima and deal with non-convex, continuous, but not necessarily everywhere differentiable objective functions.\nContinuation methods (Allgower and Georg, 1980), address a complex optimization problem by smoothing the original function, turning it into a different problem that is easier to optimize. By gradually reducing the amount of smoothing, it is possible to consider a sequence of optimization problems that converge to the optimization problem of interest (see Fig. 1).\nThese methods have been very successful for tackling difficult optimization problems involving nonconvex objective functions with multiple local minima and possibly points of non-differentiability. In machine learning, approaches based on curriculum learning (Bengio et al., 2009) are inspired by this principle to define a sequence of gradually more difficult training tasks (or training distributions) that converge to the task of interest. Gradient-based optimization over a sequence of mollified objective functions has been shown to converge (Chen, 2012).\nIn the context of stochastic gradient descent, we can use an estimator of the gradient of the smoothed objective function. This is convenient because actually computing the smoothed objective function may not be analytically feasible, but a Monte-Carlo estimate can often be obtained easily. Definition 2.1. Weak gradients (Distributional Gradients) We generalize the definition of weak/distributional derivatives to gradients in order to show the relationship with training neural networks. For an integrable function L in space L \u2208 L([a, b]), g \u2208 L([a, b]n) is a n-dimensional weak gradient of L if it satisfies the Eq. 1:\u222b\nC g(\u03b8)K(\u03b8)d\u03b8 = \u2212 \u222b C L(\u03b8)\u2207K(\u03b8)d\u03b8, (1)\nwhere K(\u03b8) is an infinitely differentiable function vanishing at infinity and C \u2208 [a, b]n, \u03b8 \u2208 Rn is a parameter vector and L(\u03b8) is the cost function that we would like to minimize. Definition 2.2. Mollifiers A mollifier is an infinitely differentiable function that behaves as an approximate identity in the group of convolutions of integrable functions. K is a mollifier if it is infinitely differentiable and for any integrable function L we have:\nL(\u03b8) = lim \u21920\n\u222b \u22121K(x/ )L(\u03b8 \u2212 x)dx.\nA mollifier converges to the Dirac function if we rescale it appropriately. In general, we are interested in constructing a sequence of mollifiers indexed by , which in the above integral corresponds to \u22121K(x/ ). This allows one to construct a sequence of gradually less blurred and closer approximations to L. We can define a weak gradient of a non-differentiable function by convolving it with a mollifier (Evans, 1998):\n\u2207(L \u2217K)(\u03b8) = (L \u2217 \u2207K)(\u03b8).\nWe can choose K(\u00b7) to be the density function of an accessible distribution such as a Gaussian distribution. In the limit, the gradient of a mollified function is equivalent to its mollified weakgradients:\ng(\u03b8) = \u2212 lim \u21920\n\u222b \u22121K(x/ )g(\u03b8 \u2212 x)dx.\nFrom the properties of weak-gradients presented in Eq. 1, we know that:\ng(\u03b8) = lim \u21920\n\u222b \u22121\u2207K(x/ )L(\u03b8 \u2212 x)dx.\nAn important property of mollifiers and weak-gradients is that they allow to backpropagate through functions that don\u2019t have strong derivatives defined everywhere, such as e.g. a step function.\nWe can obtain the mollified version LK(\u03b8) of the cost function L(\u03b8) by convolving it with a mollifier K(\u03b8). Similarly to the analysis in Mobahi (2016), we can write a Monte-Carlo estimate of LK(\u03b8) = (L \u2217 K)(\u03b8) \u2248 1N \u2211N i=1 L(\u03b8 \u2212 \u03be(i)). We provide the derivation and the gradient of this equation in Appendix A. K(\u00b7) is the kernel which we mollify with and corresponds to the average effect of injecting noise \u03be sampled from standard Normal distribution. The amount of noise controls the amount of smoothing. Gradually reducing the noise during training is related to a form of simulated annealing (Kirkpatrick et al., 1983).\nThis result can be easily extended to neural networks, where the layers typically take the form:\nhl = f(Wlhl\u22121) (2)\nwith hl\u22121 a vector of activations from the layer below, Wl a matrix representing a linear transformation and f an element-wise non-linearity of choice.\nA mollification of such a layer can be formulated as:\nhl = f((Wl \u2212 \u03bel)hl\u22121), where \u03bel \u223c N (\u00b5, \u03c32) (3)"}, {"heading": "2.2 Generalized and Noisy Mollifiers", "text": "We introduce a slight generalization of the concept of mollifiers that encompasses the approach we explored here and that is targeted at optimization via a continuation method using stochastic gradient descent. Definition 2.3. (Generalized Mollifier). A generalized mollifier is a transformation T\u03c3(f) in Rk to Rk of a function f such that:\nlim \u03c3\u21920 T\u03c3f = f, (4)\nf0 = lim \u03c3\u2192\u221e T\u03c3f is convex (5)\n\u2202(T\u03c3f)(x)\n\u2202x exists \u2200x, \u03c3 > 0 (6)\nIn addition we consider noisy mollifiers which can be defined as the expected value of a stochastic function \u03c6(x, \u03be) under some noise source \u03be with variance \u03c3:\n(T\u03c3f)(x) = E\u03be[\u03c6(x, \u03be\u03c3)] (7)\nDefinition 2.4. (Noisy Mollifier). We call a stochastic function \u03c6(x, \u03be\u03c3) with input x and noise \u03be a noisy mollifier if its expected value corresponds to the application of a generalized mollifier T\u03c3, as per Eq. 7.\nThe composition of two noisy mollifiers sharing the same \u03c3 is a noisy mollifier, since the three properties in the definition (Eqs. 4,5,6) are satisfied. When \u03c3 = 0 no noise is injected and therefore the original function is being optimized. If \u03c3 \u2192\u221e instead, the function will become convex. Consequently, corrupting separately the activation function of each level of a deep neural network (but with a shared noise level \u03c3) and annealing \u03c3 yields a noisy mollifier for the objective function. This is related to the work of Mobahi (2016), who recently introduced analytic smooths of neural network non-linearities in order to help training recurrent networks. The differences with the work presented here are twofold: we use a noisy mollifier (rather than an analytic smooth of the network non-linearities) and we introduce (in the next section) a particular form of the noisy mollifier that empirically proved to work very well.\nMobahi (2016) also makes a link between continuation or annealing methods and noise injection, although an earlier form of that observation was already made by Bottou (1991) in the context of gradually decreasing the learning rate when doing stochastic gradient descent. The idea of injecting noise into a hard-saturating non-linearity was previously used in Bengio (2013) to help backpropagate signals through semi-hard decisions (with the \u201cnoisy rectifier\u201d stochastic non-linearity)."}, {"heading": "3 Method", "text": "We focus on improving the optimization of neural networks with tanh(\u00b7) and sigmoid(\u00b7) activation functions, since those have been known to be particularly challenging to optimize and play a crucial role in models that involve gating (e.g. LSTM, GRU) as well as piecewise linear activations such as ReLUs. However the general principles we present in this paper can be easily adapted to the other activation functions as well.\nWe propose a novel learning algorithm to mollify the cost of a neural network that addresses an important drawback of previously proposed noisy training procedures: as the noise gets larger it can dominate the learning process and lead the algorithm to perform a random walk on the energy landscape of the objective function. Conversely in the algorithm we propose in this paper, as the noise gets larger the SGD minimizes a simpler e.g. convex, but still meaningful objective function. To this end we define the desired behavior of the network in the limit cases where the noise is very large or very small, and modify the model architecture accordingly.\nIn other words during training we minimize a sequence of noisy objectives L = (L1(\u03b8; \u03be\u03c31),L2(\u03b8; \u03be\u03c32), \u00b7 \u00b7 \u00b7 ,Lk(\u03b8; \u03be\u03c3k)) where the scale (variance) of the noise \u03c3i and the simplicity of the objective function will be reduced during the training. Our model still satisfies the basic properties of the generalized and noisy mollifiers.\nInstead of mollifying our objective function with a kernel, we propose to mimic the properties of the mollification that are important for the continuation by reformulating our objective function such that:\n1. We start the training by optimizing a convex objective function which is obtained by configuring all layers between the input and the last cost layer to compute an identity function. On the other hand, a high level of noise controlled with a single scalar p \u2208 [0, 1] per layer assures that element-wise activation functions compute a linear function.\n2. As the noise level p is annealed we move from identity transformations to arbitrary linear transformations between layers.\n3. Simultaneously the decreasing level of noise p allows element-wise activation functions to become non-linear.\nOn the other hand, this kind of noisy training is potentially helpful for the generalization as well, since the noise in the noisy mollified model will make the backpropagation to be noisy as well. Due to the noise induced by the backpropagation through the noisy units Hochreiter and Schmidhuber (1997b), SGD is more likely to converge to a flatter-minima because the noise will help SGD escape from sharper local minima."}, {"heading": "3.1 Simplifying the Objective Function for Feedforward Networks", "text": "For every unit of each layer, we either copy the activation (output) of the corresponding unit of the previous layer (the identity path in Figure 2) or output a noisy activation h\u0303l of a non-linear transformation of it \u03c8(hl\u22121, \u03be;Wl), where \u03be is noise, Wl is a weight matrix applied on hl\u22121 and \u03c0 is a vector of binary decisions for each unit (the convolutional path in Figure 2):\nh\u0303l = \u03c8(hl\u22121, \u03be;Wl) (8)\n\u03c6(hl\u22121, \u03be,\u03c0l;Wl) = \u03c0l hl\u22121 + (1\u2212 \u03c0l) h\u0303l (9) hl = \u03c6(hl\u22121, \u03be,\u03c0l;Wl). (10)\nTo decide which path to take, for each unit in the network, a binary stochastic decision is taken by drawing from a Binomial random variable with probability dependent on the decaying value of pl:\n\u03c0l \u223c Bin(pl) (11)\nIf the number of hidden units of layer l\u22121 and layer l+1 is not the same, we can either zero-pad layer l\u22121 before feeding it into the next layer or apply a linear projection to obtain the right dimensionality. For pl = 1, the layer computes the identity function leading to a convex objective. If pl = 0 the layer computes the original non-linear transformation unfolding the full capacity of the model.\nLinearizing the network In section 2, we show that convolving the objective function with a particular kernel can be approximated by adding noise to the activation function. This method may suffer from excessive random exploration when the noise is very large.\nWe address this issue by bounding the element-wise activation function f(\u00b7) with its linear approximation when the variance of the noise is very large, after centering it at the origin. The resulting function f\u2217(\u00b7) is bounded and centered around the origin. Note that centering the sigmoid or hard-sigmoid will make them symmetric with respect to the origin. With a proper choice of the standard deviation \u03c3(h), the noisy activation function becomes a linear function of the input when p is large, as illustrated by Figure 6.\nLet u\u2217(x) = u(x) \u2212 u(0), where u(0) is the offset of the function from the origin, and xi the i-th dimension of an affine transformation of the output of the previous layer hl\u22121: xi = w>i h\nl\u22121 + bi. Then:\n\u03c8(xi, \u03bei;wi) = sgn(u\u2217(xi))min(|u\u2217(xi)|, |f\u2217(xi) + sgn(u\u2217(xi))|si||) + u(0) (12)\nThe noise is sampled from a Normal distribution with mean 0 and whose standard deviation depends on c: si \u223c N (0, p c \u03c3(xi)) The pseudo-code for the mollified activations is reported in Algorithm 1.\nAlgorithm 1 Activation of a unit i at layer l.\n1: xi \u2190 w>i hl\u22121 + bi . an affine transformation of hl\u22121 2: \u2206i \u2190 u(xi)\u2212 f(xi) . \u2206i is a measure of a saturation of a unit 3: \u03c3(xi)\u2190 (sigmoid(ai\u2206i)\u2212 0.5)2 . std of the injected noise depends on \u2206i 4: \u03bei \u223c N (0, 1) . sampling the noise from a basic Normal distribution 5: si \u2190 pl c \u03c3(xi)|\u03bei| . Half-Normal noise controlled by \u03c3(xi), const. c and prob-ty pl 6: \u03c8(xi, \u03bei)\u2190 sgn(u\u2217(xi))min(|u\u2217(xi)|, |f\u2217(xi) + sgn(u\u2217(xi))|si||) + u(0) . noisy activation 7: \u03c0li \u223c Bin(pl) . pl controls the variance of the noise AND the prob of skipping a unit 8: h\u0303li = \u03c8(xi, \u03bei) . h\u0303 l i is a noisy activation candidate 9: \u03c6(hl\u22121, \u03bei, \u03c0li;wi) = \u03c0 l ih l\u22121 i + (1\u2212 \u03c0li)h\u0303li . make a HARD decision between h l\u22121 i and h\u0303 l i"}, {"heading": "3.2 Mollifying LSTMs and GRUs", "text": "In a similar vein it is possible to smooth the objective functions of LSTM and GRU networks by starting the optimization procedure with a simpler objective function such as optimizing a word2vec, BoW-LM or CRF objective function at the beginning of training and gradually increasing the difficulty of the optimization by increasing the capacity of the network.\nFor GRUs we set the update gate to 1t \u2013 where t is the annealing time-step \u2013 and reset the gate to 1 if the noise is very large, using Algorithm 1.Similarly for LSTMs, we can set the output gate to 1 and input gate to 1t and forget gate to 1\u2212 1 t if the noise is very large. The output gate is 1 or close to 1 when the noise is very large. This way the LSTM will behave like a BOW model. In order to achieve this behavior, the activations \u03c8(xt, \u03bei) of the gates can be formulated as:\n\u03c8(xlt, \u03be) = f(x l t + p l\u03c3(x)|\u03be|) By using a particular formulation of \u03c3(x) that constraints it to be in expectation over \u03be when pl = 1, we can obtain a function for \u03b3 \u2208 R within the range of f(\u00b7) that is discrete in expectation, but still per sample differentiable:\n\u03c3(xlt) = f\u22121(\u03b3)\u2212 xlt\nE\u03be[|\u03be|] (13)\nWe provide the derivation of Eqn. 13 in Appendix D. The gradient of the Eqn 13 will be a Monte-Carlo approximation to the gradient of f(xlt)."}, {"heading": "3.3 Annealing Schedule for p", "text": "We used a different schedule for each layer of the network, such that the noise in the lower layers will anneal faster. This is similar to the linearly decaying probability of layers in Huang et al. (2016b). In our experiments, we use an annealing schedule similar to inverse sigmoid rule in Bengio et al. (2015) with plt,\nplt = 1\u2212 e\u2212 kvtl tL (14)\nwith hyper-parameter k \u2265 0 at tth update for the lth layer, where L is the number of layers of the model. We stop annealing when the expected depth pt = \u2211L i=1 p l t reaches some threshold \u03b4. vt is a moving average of the loss 1 of the network, therefore the behavior of the loss/optimization can directly influence the annealing behavior of the network. Thus we will have:\nlim vt\u2192\u221e plt = 1 and, lim vt\u21920 plt = 0. (15)\nThis has the following desirable property: when the training-loss is high, the noise injected into the system is large and the model is encouraged to do more exploration, while when the model has converged the noise injected into the system will be zero."}, {"heading": "4 Experiments", "text": "In this section, we mainly focus on training of difficult to optimize models, in particular deep MLPs with sigmoid or tanh activation functions. The details of the experimental procedure is provided in Appendix C."}, {"heading": "4.1 Deep MLP Experiments", "text": "Deep Parity Experiments Training neural networks on a high-dimensional parity problem can be challenging (Graves, 2016; Kalchbrenner et al., 2015). We experiment on 40-dimensional parity problem with 6-layer MLP using sigmoid activation function. All the models are initialized with Glorot initialization Glorot et al. (2011) and trained with SGD with momentum. We compare an MLP with residual connections using batch normalization and a mollified network with sigmoid activation function. As can be seen in Figure 4, the mollified network converges faster.\nDeep Pentomino Pentomino is a toy-image dataset where each image has 3 Pentomino blocks. The task is to predict whether if there is a different shape in the image or not (G\u00fcl\u00e7ehre and Bengio, 2013). The best reported result on this task with MLPs is 68.15% accuracy (Gulcehre et al., 2014). The same model as ours trained without noisy activation function and vanilla residual connections scored 69.5% accuracy, while our mollified version scored 75.15% accuracy after 100 epochs of training on the 80k dataset.\n1Depending on whether the model overfits or not, this can be a moving average of training or validation loss.\nCIFAR10 We experimented with deep convolutional neural networks of 110-layers with residual blocks and residual connections comparing our model against ResNet and Stochastic depth. We adapted the hyperparameters of the Stochastic depth network from Huang et al. (2016a) and we used the same hyperparameters for our algorithm. We report the training and validation curves of the three models in Figure 6 and the best test accuracy obtained early stopping on validation accuracy over 500 epochs in Table 1. Our model achieves better generalization than ResNet. Stochastic depth achieves better generalization, but it might be possible to combine both and obtain better results."}, {"heading": "4.2 LSTM Experiments", "text": "Predicting the Character Embeddings from Characters Learning the mapping from sequences of characters to the word-embeddings is a difficult problem. Thus one needs to use a highly non-linear function. We trained a word2vec model on Wikipedia with embeddings of size 500 (Mikolov et al., 2014) with a vocabulary of size 374557.\nLSTM Language Modeling We evaluate our model on LSTM language modeling. Our baseline model is a 2-layer stacked LSTM without any regularization. We observed that mollified model converges faster and achieves better results. We provide the results for PTB language modeling in Table 2."}, {"heading": "5 Conclusion", "text": "We propose a novel method for training neural networks inspired by an idea of continuation, smoothing techniques and recent advances in non-convex optimization algorithms. The method makes the learning easier by starting from a simpler model solving a well-behaved problem and gradually transitioning to a more complicated setting. We show improvements on very deep models, difficult to optimize tasks and compare with powerful techniques such as batch-normalization and residual connections.\nOur future work includes testing this method on large-scale language tasks that require long training time, e.g., machine translation and language modeling. It is also intriguing to understand how the generalization performance is affected by mollified networks, since the noise injected during the training can act as a regularizer."}, {"heading": "A Monte-Carlo Estimate of Mollification", "text": "LK(\u03b8) = (L \u2217K)(\u03b8) = \u222b C L(\u03b8 \u2212 \u03be)K(\u03be)d\u03be which can be estimated by a Monte Carlo:\n\u2248 1 N N\u2211 i=1 L(\u03b8 \u2212 \u03be(i)), where \u03be(i) is a realization of the noise random variable \u03be\nyielding \u2202LK(\u03b8) \u2202\u03b8\n\u2248 1 N N\u2211 i=1 \u2202L(\u03b8 \u2212 \u03be(i)) \u2202\u03b8 .\n(16)\nTherefore introducing additive noise to the input of L(\u03b8) is equivalent to mollification."}, {"heading": "B Linearizing ReLU Activation Function", "text": "We have a simpler form of the equations to linearize ReLU activation function when pl \u2192\u221e. Instead of the complicated Eqn. 10. We can use a simpler equation as in Eqn 17 to achieve the linearization of the activation function when we have a very large noise in the activation function:\nsi = minimum(|xi|, p\u03c3(xi)|\u03be|) (17) \u03c8(xi, \u03bei,wi) = f(xi)\u2212 si (18)"}, {"heading": "C Experimental Details", "text": "C.1 MNIST\nThe weights of the models are initialized with Glorot & Bengio initialization Glorot et al. (2011). We use the learning rate of 4e \u2212 4 along with RMSProp. We initialize ai parameters of mollified activation function by sampling it from a uniform distribution, U[\u22122, 2]. We used 100 hidden units at each layer with a minibatches of size 500.\nC.2 Pentomino\nWe train a 6\u2212layer MLP with sigmoid activation function using SGD and momentum. We used 200 units per layer with sigmoid activation functions. We use a learning rate of 1e\u2212 3.\nC.3 CIFAR10\nWe use the same model with the same hyperparameters for both ResNet, mollified network and the stochastic depth. We borrowed the hyperparameters of the model from Huang et al. (2016a). Our mollified convnet model has residual connections coming from its layer below.\nC.4 Parity\nWe use SGD with Nesterov momentum and initialize the weight matrices by using Glorot&Bengio initializationGlorot et al. (2011). For all models we use the learning rate of 1e\u2212 3 and momentum of 0.92. ai parameters of mollified activation function are initialized by sampling from uniform distribution, U [\u22122, 2].\nC.5 LSTM Language Modeling\nWe trained 2-layered LSTM language models on PTB word-level. We used the models with the same hyperparameters as in Zaremba and Sutskever (2014). We used the same hyperparameters for both the mollified LSTM language model and the LSTM. We use hard-sigmoid activation function for both the LSTM and mollified LSTM language model.\nC.6 Predicting the Character Embeddings from Characters\nWe use 10k of these words as a validation and another 10k word embeddings as test set. We train a bidirectional-LSTM on top of each sequence of characters for each word and on top of the representation of bidirectional LSTM, we use a 5-layered tanh-MLP to predict the word-embedding.\nWe train our models using RMSProp and momentum with learning rate of 6e\u2212 4 and momentum 0.92. The size of the minibatches, we used is 64. As seen in Figure 5, mollified LSTM network converges faster."}, {"heading": "D Derivation of the Noisy Activations for the Gating", "text": "Assume that zlt = x l t + p l t\u03c3(x)|\u03belt| and E\u03be[\u03c8(xlt, \u03be)] = t. Thus for all zlt,\nE\u03be[\u03c8(x l t, \u03be l t)] = E\u03be[f(z l t)], (19)\nt = E\u03be[f(zlt)], assuming f(\u00b7) behaves similar to a linear function (20) E\u03be[f(zlt)] \u2248 f(E\u03be[zlt]) since we use hard-sigmoid for f(\u00b7) this will hold. (21)\nf\u22121(t) \u2248 E\u03be[zlt] (22) (23)\nAs in Eqn. 19, we can write the expectation of this equation as:\nf\u22121(t) \u2248 xlt + plt\u03c3(x)E\u03be[\u03belt]\nCorollary, the value that \u03c3(xlt) should take in expectation for p l t = 1 would be:\n\u03c3(xlt) \u2248 f\u22121(t)\u2212 xlt\nE\u03be[\u03belt]\nIn our experiments for f(\u00b7) we used the hard-sigmoid activation function. We used the following piecewise activation function in order to use it as f\u22121(x) = 4(x\u2212 0.5). During inference we use the expected value of random variables \u03c0 and \u03be."}], "references": [{"title": "Numerical Continuation Methods", "author": ["E.L. Allgower", "K. Georg"], "venue": "An Introduction. Springer-Verlag,", "citeRegEx": "Allgower and Georg.,? \\Q1980\\E", "shortCiteRegEx": "Allgower and Georg.", "year": 1980}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Estimating or propagating gradients through stochastic neurons", "author": ["Y. Bengio"], "venue": "Technical Report arXiv:1305.2982, Universite de Montreal,", "citeRegEx": "Bengio.,? \\Q2013\\E", "shortCiteRegEx": "Bengio.", "year": 2013}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Une approche th\u00e9orique de l\u2019apprentissage connexioniste; applications \u00e0 la reconnaissance de la parole", "author": ["L. Bottou"], "venue": "PhD thesis, Universite\u0301 de Paris XI,", "citeRegEx": "Bottou.,? \\Q1991\\E", "shortCiteRegEx": "Bottou.", "year": 1991}, {"title": "Online algorithms and stochastic approximations", "author": ["L. Bottou"], "venue": "Online Learning in Neural Networks", "citeRegEx": "Bottou.,? \\Q1998\\E", "shortCiteRegEx": "Bottou.", "year": 1998}, {"title": "Smoothing methods for nonsmooth, nonconvex minimization", "author": ["X. Chen"], "venue": "Math. Program. Ser. B,", "citeRegEx": "Chen.,? \\Q2012\\E", "shortCiteRegEx": "Chen.", "year": 2012}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "The loss surface of multilayer", "author": ["A. Choromanska", "M. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun"], "venue": null, "citeRegEx": "Choromanska et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2014}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Y. Dauphin", "R. Pascanu", "C. Gulcehre", "K. Cho", "S. Ganguli", "Y. Bengio"], "venue": "In NIPS\u20192014,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Partial differential equations", "author": ["L.C. Evans"], "venue": "Graduate Studies in Mathematics,", "citeRegEx": "Evans.,? \\Q1998\\E", "shortCiteRegEx": "Evans.", "year": 1998}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1603.08983,", "citeRegEx": "Graves.,? \\Q2016\\E", "shortCiteRegEx": "Graves.", "year": 2016}, {"title": "Knowledge matters: Importance of prior information for optimization", "author": ["\u00c7. G\u00fcl\u00e7ehre", "Y. Bengio"], "venue": "arXiv preprint arXiv:1301.4083,", "citeRegEx": "G\u00fcl\u00e7ehre and Bengio.,? \\Q2013\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre and Bengio.", "year": 2013}, {"title": "Learned-norm pooling for deep feedforward and recurrent neural networks", "author": ["C. Gulcehre", "K. Cho", "R. Pascanu", "Y. Bengio"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Gulcehre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2014}, {"title": "Noisy activation functions", "author": ["C. Gulcehre", "M. Moczulski", "M. Denil", "Y. Bengio"], "venue": null, "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. rahman Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K. Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K.Q. Weinberger"], "venue": "CoRR, abs/1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate", "author": ["S. Ioffe", "C. Szegedy"], "venue": "shift. CoRR,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Grid long short-term memory", "author": ["N. Kalchbrenner", "I. Danihelka", "A. Graves"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Optimization by simulated annealing", "author": ["S. Kirkpatrick", "C.D.G. Jr.", "M.P. Vecchi"], "venue": null, "citeRegEx": "Kirkpatrick et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Kirkpatrick et al\\.", "year": 1983}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Comput.,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra"], "venue": "Technical report,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Training recurrent neural networks by diffusion", "author": ["H. Mobahi"], "venue": "arXiv preprint arXiv:1601.04114,", "citeRegEx": "Mobahi.,? \\Q2016\\E", "shortCiteRegEx": "Mobahi.", "year": 2016}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["A. Neelakantan", "L. Vilnis", "Q.V. Le", "I. Sutskever", "L. Kaiser", "K. Kurach", "J. Martens"], "venue": "CoRR, abs/1511.06807,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Technical report,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Reseg: A recurrent neural network for object segmentation", "author": ["F. Visin", "K. Kastner", "A. Courville", "Y. Bengio", "M. Matteucci", "K. Cho"], "venue": "arXiv preprint arXiv:1511.07053,", "citeRegEx": "Visin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Visin et al\\.", "year": 2015}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba and Sutskever.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "convolutional networks (LeCun et al., 1989), LSTMs (Hochreiter and Schmidhuber, 1997a) or GRUs (Cho et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 8, "context": ", 1989), LSTMs (Hochreiter and Schmidhuber, 1997a) or GRUs (Cho et al., 2014) \u2013 achieve state of the art results on a range of challenging tasks like object classification and detection (Szegedy et al.", "startOffset": 59, "endOffset": 77}, {"referenceID": 32, "context": ", 2014) \u2013 achieve state of the art results on a range of challenging tasks like object classification and detection (Szegedy et al., 2014), semantic segmentation (Visin et al.", "startOffset": 116, "endOffset": 138}, {"referenceID": 33, "context": ", 2014), semantic segmentation (Visin et al., 2015), speech recognition (Hinton et al.", "startOffset": 31, "endOffset": 51}, {"referenceID": 18, "context": ", 2015), speech recognition (Hinton et al., 2012), statistical machine translation (Sutskever et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 31, "context": ", 2012), statistical machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), playing Atari (Mnih et al.", "startOffset": 41, "endOffset": 88}, {"referenceID": 1, "context": ", 2012), statistical machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), playing Atari (Mnih et al.", "startOffset": 41, "endOffset": 88}, {"referenceID": 26, "context": ", 2014), playing Atari (Mnih et al., 2013) and Go (Silver et al.", "startOffset": 23, "endOffset": 42}, {"referenceID": 29, "context": ", 2013) and Go (Silver et al., 2016).", "startOffset": 15, "endOffset": 36}, {"referenceID": 6, "context": "When trained with variants of SGD (Bottou, 1998) deep models can be hard to optimize due to their highly non-linear and nonconvex nature (Choromanska et al.", "startOffset": 34, "endOffset": 48}, {"referenceID": 9, "context": "When trained with variants of SGD (Bottou, 1998) deep models can be hard to optimize due to their highly non-linear and nonconvex nature (Choromanska et al., 2014; Dauphin et al., 2014).", "startOffset": 137, "endOffset": 185}, {"referenceID": 10, "context": "When trained with variants of SGD (Bottou, 1998) deep models can be hard to optimize due to their highly non-linear and nonconvex nature (Choromanska et al., 2014; Dauphin et al., 2014).", "startOffset": 137, "endOffset": 185}, {"referenceID": 22, "context": "A number of approaches were proposed to alleviate the difficulty of optimization: addressing the problem of the internal covariate shift with Batch Normalization (Ioffe and Szegedy, 2015), learning with a curriculum (Bengio et al.", "startOffset": 162, "endOffset": 187}, {"referenceID": 4, "context": "A number of approaches were proposed to alleviate the difficulty of optimization: addressing the problem of the internal covariate shift with Batch Normalization (Ioffe and Szegedy, 2015), learning with a curriculum (Bengio et al., 2009) and recently training with diffusion (Mobahi, 2016) - a form of continuation method.", "startOffset": 216, "endOffset": 237}, {"referenceID": 27, "context": ", 2009) and recently training with diffusion (Mobahi, 2016) - a form of continuation method.", "startOffset": 45, "endOffset": 59}, {"referenceID": 28, "context": "At the same time, the impact of noise injection on the behavior of modern deep models has been explored in (Neelakantan et al., 2015) and it has been recently shown that noisy activation functions improve performance on a wide variety of tasks (Gulcehre et al.", "startOffset": 107, "endOffset": 133}, {"referenceID": 16, "context": ", 2015) and it has been recently shown that noisy activation functions improve performance on a wide variety of tasks (Gulcehre et al., 2016).", "startOffset": 118, "endOffset": 141}, {"referenceID": 17, "context": "Skip connections allow to train very deep residual and highway architectures (He et al., 2015; Srivastava et al., 2015) by skipping layers or block of layers.", "startOffset": 77, "endOffset": 119}, {"referenceID": 30, "context": "Skip connections allow to train very deep residual and highway architectures (He et al., 2015; Srivastava et al., 2015) by skipping layers or block of layers.", "startOffset": 77, "endOffset": 119}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2014), playing Atari (Mnih et al., 2013) and Go (Silver et al., 2016). When trained with variants of SGD (Bottou, 1998) deep models can be hard to optimize due to their highly non-linear and nonconvex nature (Choromanska et al., 2014; Dauphin et al., 2014). A number of approaches were proposed to alleviate the difficulty of optimization: addressing the problem of the internal covariate shift with Batch Normalization (Ioffe and Szegedy, 2015), learning with a curriculum (Bengio et al., 2009) and recently training with diffusion (Mobahi, 2016) - a form of continuation method. At the same time, the impact of noise injection on the behavior of modern deep models has been explored in (Neelakantan et al., 2015) and it has been recently shown that noisy activation functions improve performance on a wide variety of tasks (Gulcehre et al., 2016). In this paper, we connect the ideas of curriculum learning and continuation methods with those arising from models with skip connections and with layers that compute near-identity transformations. Skip connections allow to train very deep residual and highway architectures (He et al., 2015; Srivastava et al., 2015) by skipping layers or block of layers. Similarly, it is now well known that it is possible to stochastically change the depth of a network during training (Huang et al., 2016b) and still converge. In this work, we introduce the idea of mollification \u2013 a form of differentiable smoothing of the loss function connected to noisy activations \u2013 which can be interpreted as a form adaptive noise injection that only depends on a single hyperparameter. Inspired by Huang et al. (2016b), we exploit the \u2217 This work was done while these students were interning at the MILA lab.", "startOffset": 8, "endOffset": 1672}, {"referenceID": 0, "context": "Continuation methods (Allgower and Georg, 1980), address a complex optimization problem by smoothing the original function, turning it into a different problem that is easier to optimize.", "startOffset": 21, "endOffset": 47}, {"referenceID": 4, "context": "In machine learning, approaches based on curriculum learning (Bengio et al., 2009) are inspired by this principle to define a sequence of gradually more difficult training tasks (or training distributions) that converge to the task of interest.", "startOffset": 61, "endOffset": 82}, {"referenceID": 7, "context": "Gradient-based optimization over a sequence of mollified objective functions has been shown to converge (Chen, 2012).", "startOffset": 104, "endOffset": 116}, {"referenceID": 11, "context": "We can define a weak gradient of a non-differentiable function by convolving it with a mollifier (Evans, 1998): \u2207(L \u2217K)(\u03b8) = (L \u2217 \u2207K)(\u03b8).", "startOffset": 97, "endOffset": 110}, {"referenceID": 24, "context": "Gradually reducing the noise during training is related to a form of simulated annealing (Kirkpatrick et al., 1983).", "startOffset": 89, "endOffset": 115}, {"referenceID": 26, "context": "Similarly to the analysis in Mobahi (2016), we can write a Monte-Carlo estimate of LK(\u03b8) = (L \u2217 K)(\u03b8) \u2248 1 N \u2211N i=1 L(\u03b8 \u2212 \u03be).", "startOffset": 29, "endOffset": 43}, {"referenceID": 24, "context": "This is related to the work of Mobahi (2016), who recently introduced analytic smooths of neural network non-linearities in order to help training recurrent networks.", "startOffset": 31, "endOffset": 45}, {"referenceID": 24, "context": "This is related to the work of Mobahi (2016), who recently introduced analytic smooths of neural network non-linearities in order to help training recurrent networks. The differences with the work presented here are twofold: we use a noisy mollifier (rather than an analytic smooth of the network non-linearities) and we introduce (in the next section) a particular form of the noisy mollifier that empirically proved to work very well. Mobahi (2016) also makes a link between continuation or annealing methods and noise injection, although an earlier form of that observation was already made by Bottou (1991) in the context of gradually decreasing the learning rate when doing stochastic gradient descent.", "startOffset": 31, "endOffset": 451}, {"referenceID": 4, "context": "Mobahi (2016) also makes a link between continuation or annealing methods and noise injection, although an earlier form of that observation was already made by Bottou (1991) in the context of gradually decreasing the learning rate when doing stochastic gradient descent.", "startOffset": 160, "endOffset": 174}, {"referenceID": 3, "context": "The idea of injecting noise into a hard-saturating non-linearity was previously used in Bengio (2013) to help backpropagate signals through semi-hard decisions (with the \u201cnoisy rectifier\u201d stochastic non-linearity).", "startOffset": 88, "endOffset": 102}, {"referenceID": 19, "context": "Due to the noise induced by the backpropagation through the noisy units Hochreiter and Schmidhuber (1997b), SGD is more likely to converge to a flatter-minima because the noise will help SGD escape from sharper local minima.", "startOffset": 72, "endOffset": 107}, {"referenceID": 17, "context": "This is similar to the linearly decaying probability of layers in Huang et al. (2016b). In our experiments, we use an annealing schedule similar to inverse sigmoid rule in Bengio et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 2, "context": "In our experiments, we use an annealing schedule similar to inverse sigmoid rule in Bengio et al. (2015) with pt, pt = 1\u2212 e\u2212 kvtl tL (14)", "startOffset": 84, "endOffset": 105}, {"referenceID": 13, "context": "1 Deep MLP Experiments Deep Parity Experiments Training neural networks on a high-dimensional parity problem can be challenging (Graves, 2016; Kalchbrenner et al., 2015).", "startOffset": 128, "endOffset": 169}, {"referenceID": 23, "context": "1 Deep MLP Experiments Deep Parity Experiments Training neural networks on a high-dimensional parity problem can be challenging (Graves, 2016; Kalchbrenner et al., 2015).", "startOffset": 128, "endOffset": 169}, {"referenceID": 12, "context": "All the models are initialized with Glorot initialization Glorot et al. (2011) and trained with SGD with momentum.", "startOffset": 58, "endOffset": 79}, {"referenceID": 14, "context": "The task is to predict whether if there is a different shape in the image or not (G\u00fcl\u00e7ehre and Bengio, 2013).", "startOffset": 81, "endOffset": 108}, {"referenceID": 15, "context": "15% accuracy (Gulcehre et al., 2014).", "startOffset": 13, "endOffset": 36}, {"referenceID": 20, "context": "We adapted the hyperparameters of the Stochastic depth network from Huang et al. (2016a) and we used the same hyperparameters for our algorithm.", "startOffset": 68, "endOffset": 89}], "year": 2016, "abstractText": "The optimization of deep neural networks can be more challenging than traditional convex optimization problems due to the highly non-convex nature of the loss function, e.g. it can involve pathological landscapes such as saddle-surfaces that can be difficult to escape for algorithms based on simple gradient descent. In this paper, we attack the problem of optimization of highly non-convex neural networks by starting with a smoothed \u2013 or mollified \u2013 objective function which becomes more complex as the training proceeds. Our proposition is inspired by the recent studies in continuation methods: similar to curriculum methods, we begin learning an easier (possibly convex) objective function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, objective function. The complexity of the mollified networks is controlled by a single hyperparameter which is annealed during the training. We show improvements on various difficult optimization tasks and establish a relationship between recent works on continuation methods for neural networks and mollifiers.", "creator": "LaTeX with hyperref package"}}}