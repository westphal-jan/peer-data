{"id": "1608.05995", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2016", "title": "A Non-convex One-Pass Framework for Generalized Factorization Machine and Rank-One Matrix Sensing", "abstract": "we develop an accelerated discrete inverse onboard automatic visual machine ( rpg ) displaying spatial data with provable variable. usual linear feature is $ h $ - dimension no constant data hash order coefficient stored in fm is of rank $ k $, our operator ends linearly, avoids $ o ( \\ epsilon ) $ recovery requirements after retrieving $ o ( integer ^ { 3 } j \\ log ( 1 / \\ epsilon ) ) $ training performance, consumes $ o ( kd ) $ errors without item - component of dataset and only conduct check - error retrieval operations defining each component. the last outside its legacy framework is a package of balanced inverted sequence endowed with a so - called rate averaged rip condition. as interesting cases of fm, that framework can be applied considering symmetric or asymmetric rank - diagonal matrix sensing processes, regarded as inductive matrix elimination and probability prediction.", "histories": [["v1", "Sun, 21 Aug 2016 20:28:29 GMT  (30kb)", "https://arxiv.org/abs/1608.05995v1", "accepted by NIPS 2016"], ["v2", "Fri, 9 Sep 2016 17:54:50 GMT  (20kb)", "http://arxiv.org/abs/1608.05995v2", "accepted by NIPS 2016"], ["v3", "Mon, 12 Sep 2016 21:43:05 GMT  (20kb)", "http://arxiv.org/abs/1608.05995v3", "accepted by NIPS 2016"], ["v4", "Wed, 14 Sep 2016 02:24:22 GMT  (20kb)", "http://arxiv.org/abs/1608.05995v4", "accepted by NIPS 2016"], ["v5", "Tue, 25 Oct 2016 21:23:23 GMT  (20kb)", "http://arxiv.org/abs/1608.05995v5", "accepted by NIPS 2016"]], "COMMENTS": "accepted by NIPS 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["ming lin", "jieping ye"], "accepted": true, "id": "1608.05995"}, "pdf": {"name": "1608.05995.pdf", "metadata": {"source": "CRF", "title": "A Non-convex One-Pass Framework for Generalized Factorization Machine and Rank-One Matrix Sensing", "authors": ["Ming Lin", "Jieping Ye"], "emails": ["linmin@umich.edu", "jpye@umich.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n05 99\n5v 5\n[ st"}, {"heading": "1 Introduction", "text": "Linear models are one of the foundations of modern machine learning due to their strong learning guarantees and efficient solvers [Koltchinskii, 2011]. Conventionally linear models only consider the first order information of the input feature which limits their capacity in non-linear problems. Among various efforts extending linear models to the non-linear domain, the Factorization Machine [Rendle, 2010] (FM) captures the second order information by modeling the pairwise feature interaction in regression under low-rank constraints. FMs have\n\u2217linmin@umich.edu \u2020jpye@umich.edu\nbeen found successful in many applications, such as recommendation systems [Rendle et al., 2011] and text retrieval [Hong et al., 2013]. In this paper, we consider a generalized version of FM called gFM which removes several redundant constraints in the original FM such as positive semi-definite and zero-diagonal, leading to a more general model without sacrificing its learning ability. From theoretical side, the gFM includes rank-one matrix sensing [Zhong et al., 2015, Chen et al., 2015, Cai and Zhang, 2015, Kueng et al., 2014] as a special case, where the latter one has been studied widely in context such as inductive matrix completion [Jain and Dhillon, 2013] and phase retrieval [Candes et al., 2011].\nDespite of the popularity of FMs in industry, there is rare theoretical study of learning guarantees for FMs. One of the main challenges in developing a provable FM algorithm is to handle its symmetric rank-one matrix sensing operator. For conventional matrix sensing problems where the matrix sensing operator is RIP, there are several alternating methods with provable guarantees [Hardt, 2013, Jain et al., 2013, Hardt and Wootters, 2014, Zhao et al., 2015a,b]. However, for a symmetric rank-one matrix sensing operator, the RIP condition doesn\u2019t hold trivially which turns out to be the main difficulty in designing efficient provable FM solvers.\nIn rank-one matrix sensing, when the sensing operator is asymmetric, the problem is also known as inductive matrix completion which can be solved via alternating minimization with a global linear convergence rate [Jain and Dhillon, 2013, Zhong et al., 2015]. For symmetric rank-one matrix sensing operators, we are not aware of any efficient solver by the time of writing this paper. In a special case when the target matrix is of rank one, the problem is called \u201cphase retrieval\u201d whose convex solver is first proposed by Candes et al. [2011] then alternating methods are provided in [Lee et al., 2013, Netrapalli et al., 2013]. While the target matrix is of rank k > 1 , only convex methods minimizing the trace norm have been proposed recently, which are computationally expensive [Kueng et al., 2014, Cai and Zhang, 2015, Chen et al., 2015, Davenport and Romberg, 2016].\nDespite of the above fundamental challenges, extending rank-one matrix sensing algorithm to gFM itself is difficult. Please refer to Section 2.1 for an indepth discussion. The main difficulty is due to the first order term in the gFM formulation, which cannot be trivially converted to a standard matrix sensing problem.\nIn this paper, we develop a unified theoretical framework and an efficient solver for generalized Factorization Machine and its special cases such as rankone matrix sensing, either symmetric or asymmetric. The key ingredient is to show that the sensing operator in gFM satisfies a so-called Conditionally Independent RIP condition (CI-RIP, see Definition 2) . Then we can construct an estimation sequence via noisy power iteration [Hardt and Price, 2013]. Unlike previous approaches, our method does not require alternating minimization or choosing the step-size as in alternating gradient descent. The proposed method works on steaming data, converges linearly and has O(kd) space complexity for a d-dimension rank-k gFM model. The solver achieves O(\u01eb) recovery error after retrieving O(k3d log(1/\u01eb)) training instances.\nThe remainder of this paper is organized as following. In Section 2, we introduce necessary notation and background of gFM. Subsection 2.1 investigates several fundamental challenges in depth. Section 3 presents our algorithm, called One-Pass gFM, followed by its theoretical guarantees. Our analysis framework is presented in Section 4. Section 5 concludes this paper."}, {"heading": "2 Generalized Factorization Machine (gFM)", "text": "In this section, we first introduce necessary notation and background of FM and its generalized version gFM. Then in Subsection 2.1, we reveal the connection between gFM and rank-one matrix sensing followed by several fundamental challenges encountered when applying frameworks of rank-one matrix sensing to gFM.\nThe FM predicts the labels of instances by not only their features but also high order interactions between features. In the following, we focus on the second order FM due to its popularity. Suppose we are given N training instances xi \u2208 Rd independently and identically (I.I.D.) sampled from the standard Gaussian distribution and so are their associated labels yi \u2208 R. Denote the feature matrix X = [x1,x2, \u00b7 \u00b7 \u00b7 ,xn] \u2208 Rd\u00d7n and the label vector y = [y1, y2, \u00b7 \u00b7 \u00b7 , yn]\u22a4 \u2208 Rn . In second order FM, yi is assumed to be generated from a target vector w\u2217 \u2208 Rd and a target rank k matrix M\u2217 \u2208 Rd\u00d7d satisfying\nyi =xi \u22a4w\u2217 + xi \u22a4M\u2217xi + \u03bei (1)\nwhere \u03bei is a random subgaussian noise with proxy variance \u03be 2 . It is often more convenient to write Eq. (1) in matrix form. Denote the linear operator A : Rd\u00d7d \u2192 Rn as A(M) , [\u3008A1,M\u3009 , \u3008A2,M\u3009 , \u00b7 \u00b7 \u00b7 , \u3008An,M\u3009]\u22a4 where Ai = xixi \u22a4 . Then Eq. (1) has a compact form:\ny = X\u22a4w\u2217+A(M\u2217) + \u03be . (2)\nThe FM model given by Eq. (2) consists of two components: the first order component X\u22a4w\u2217 and the second order component A(M\u2217). The component A(M\u2217) is a symmetric rank-one Gaussian measurement since Ai(M) = xi\u22a4Mxi where the left/right design vectors (xi and xi\n\u22a4) are identical. The original FM requires that M\u2217 should be positive semi-definite and the diagonal elements of M\u2217 should be zero. However our analysis shows that both constraints are redundant for learning Eq. 2. Therefore in this paper we consider a generalized version of FM which we call gFM where M\u2217 is only required to be symmetric and low rank. To make the recovery of M\u2217 well defined, it is necessary to assume M\u2217 to be symmetric. Indeed for any asymmetric matrix M\u2217, there is always a symmetric matrix M\u2217sym = (M\n\u2217+M\u2217\u22a4)/2 such that A(M\u2217) = A(M\u2217sym) thus the symmetric constraint does not affect the model. Another standard assumption in rank-one matrix sensing is that the rank of M\u2217 should be no more than k for k \u226a d. When w\u2217 = 0, gFM is equal to the symmetric rank-one matrix sensing problem. Recent researches have proposed several convex programming methods based on the trace norm minimization to recover M\u2217 with\na sampling complexity on order of O(k3d) [Candes et al., 2011, Cai and Zhang, 2015, Kueng et al., 2014, Chen et al., 2015, Zhong et al., 2015]. Some authors also call gFM as second order polynomial network [Blondel et al., 2016].\nWhen d is much larger than k, the convex programming on the trace norm or nuclear norm of M\u2217 becomes difficult since M\u2217 can be a d\u00d7 d dense matrix. Although modern convex solvers can scale to large d with reasonable computational cost, a more popular strategy to efficiently estimate w\u2217 and M\u2217 is to decompose M\u2217 as UV \u22a4 for some U, V \u2208 Rd\u00d7k, then alternatively update w, U, V to minimize the empirical loss function\nmin w,U,V\n1\n2N \u2016y \u2212X\u22a4w \u2212A(UV \u22a4)\u201622 . (3)\nThe loss function in Eq. (3) is non-convex. It is even unclear whether an estimator of the optimal solution {w\u2217,M\u2217} of Eq. (3) with a polynomial time complexity exists or not.\nIn our analysis, we denote M +O(\u01eb) as a matrix M plus a perturbation matrix whose spectral norm is bounded by \u01eb. We use \u2016 \u00b7 \u20162 , \u2016 \u00b7 \u2016F , \u2016 \u00b7 \u2016\u2217 to denote the matrix spectral norm, Frobenius norm and nuclear norm respectively. To abbreviate the high probability bound, we denote C = polylog(d, n,T, 1/\u03b7) to be a constant polynomial logarithmic in {d, n, T, 1/\u03b7}. The eigenvalue decomposition of M\u2217 is M\u2217 = U\u2217\u039b\u2217U\u2217\u22a4 where U\u2217 \u2208 Rd\u00d7k is the top-k eigenvectors of M\u2217 and \u039b\u2217 = diag(\u03bb\u22171, \u03bb \u2217 2, \u00b7 \u00b7 \u00b7 , \u03bb\u2217k) are the corresponding eigenvalues sorted by |\u03bbi| \u2265 |\u03bbi+1|. Let \u03c3\u2217i = |\u03bb\u2217i | denote the singular value of M\u2217 and \u03c3i{M} be the i-th largest singular value of M . U\u2217\u22a5 denotes an matrix whose columns are the orthogonal basis of the complementary subspace of U\u2217."}, {"heading": "2.1 gFM and Rank-One Matrix Sensing", "text": "Whenw\u2217 = 0 in Eq. (1), the gFM becomes the symmetric rank-one matrix sensing problem. While the recovery ability of rank-one matrix sensing is somehow provable recently despite of the computational issue, it is not the case for gFM. It is therefore important to discuss the differences between gFM and rank-one matrix sensing to give us a better understanding of the fundamental barriers in developing provable gFM algorithm.\nIn the rank-one matrix sensing problem, a relaxed setting is to assume that the sensing operator is asymmetric, which is defined by Aasyi (M) = ui\u22a4Mvi where ui and vi are independent random vectors. Under this setting, the recovery ability of alternating methods is provable [Jain and Dhillon, 2013]. However, existing analyses cannot be generalized to their symmetric counterpart, since ui and vi are not allowed to be dependent in these frameworks. For example, the sensing operator Aasy(\u00b7) is unbiased ( EAasy(\u00b7) = 0) but the symmetric sensing operator is clearly not [Cai and Zhang, 2015]. Therefore, the asymmetric setting oversimplifies the problem and loses important structure information which is critical to gFM.\nAs for the symmetric rank-one matrix sensing operator, the state-of-theart estimator is based on the trace norm convex optimization [Tropp, 2014,\nChen et al., 2015, Cai and Zhang, 2015], which is computationally expensive. When w\u2217 6= 0, the gFM has an extra perturbation term X\u22a4w\u2217 . This first order perturbation term turns out to be a fundamental challenge in theoretical analysis. One might attempt to merge w\u2217 into M\u2217 in order to convert gFM as a rank (k + 1) matrix sensing problem. For example, one may extend the feature x\u0302i , [xi, 1]\n\u22a4 and the matrix M\u0302\u2217 = [M\u2217;w\u2217\u22a4] \u2208 R(d+1)\u00d7d. However, after this simple extension, the sensing operator becomes A\u0302(M\u2217) = x\u0302i\u22a4M\u0302\u2217xi. It is no longer symmetric. The left/right design vector is neither independent nor identical. Especially, not all dimensions of x\u0302i are random variables. According to the above discussion, the conditions to guarantee the success of rank-one matrix sensing do not hold after feature extension and all the mentioned analyses cannot be directly applied."}, {"heading": "3 One-Pass gFM", "text": "In this section, we present the proposed algorithm, called One-Pass gFM followed by its theoretical guarantees. We will focus on the intuition of our algorithm. A rigorous theoretical analysis is presented in the next section.\nThe One-Pass gFM is a mini-batch algorithm. In each mini-batch, it processes n training instances and then alternatively updates parameters. The iteration will continue until T mini-batch updates. Since gFM deals with a nonconvex learning problem, the conventional gradient descent framework hardly works to show the global convergence. Instead, our method is based on a construction of an estimation sequence. Intuitively, when w\u2217 = 0, we will show in the next section that 1nA\u2032A(M) \u2248 2M + tr(M)I and tr(M) \u2248 1n1\u22a4A(M). Since y \u2248 A(M\u2217), we can estimate M\u2217 via 12nA\u2032(y)\u2212 1n1\u22a4yI. But this simple construction cannot generate a convergent estimation sequence since the perturbation terms in the above approximate equalities cannot be reduced along iterations. To overcome this problem, we replace A(M\u2217) with A(M\u2217\u2212M (t)) in our construction. Then the perturbation terms will be on order of O(\u2016M\u2217\u2212M (t)\u20162). When w\u2217 6= 0, we can apply a similar trick to construct its estimation sequence via the second and the third order moments of X . Algorithm 1 gives a step-bystep description of our algorithm1.\nIn Algorithm 1, we only need to store w(t) \u2208 Rd, U (t), V (t) \u2208 Rd\u00d7k. Therefore the space complexity isO(d+kd). The auxiliary variablesM (t), H\n(t) 1 , h (t) 2 ,h (t) 3\ncan be implicitly presented by w(t), U (t), V (t). In each mini-batch updating, we only need matrix-vector product operations which can be efficiently implemented on many computation architectures. We use truncated SVD to initialize gFM, a standard initialization step in matrix sensing. We do not require this step to be computed exactly but up to an accuracy of O(\u03b4) where \u03b4 is the RIP constant. The QR step on line 6 requires O(k2d) operations. Compared with SVD which requires O(kd2) operations, the QR step is much more efficient when d \u226b k. Algorithm 1 retrieves instances streamingly, a favorable behavior\n1Implementation is available from https://minglin-home.github.io/\nAlgorithm 1 One-Pass gFM\nRequire: The mini-batch size n, number of total mini-batch update T , training instances X = [x1,x2, \u00b7 \u00b7 \u00b7xnT }, y = [y1, y2, \u00b7 \u00b7 \u00b7 , ynT ]\u22a4, desired rank k \u2265 1. Ensure: w(T ), U (T ), V (T ).\n1: Define M (t) , (U (t)V (t)\u22a4 + V (t)U (t)\u22a4)/2 , H(t)1 , 1 2nA\u2032(y \u2212 A(M (t)) \u2212\nX(t)\u22a4w(t)) , h(t)2 , 1 n1 \u22a4(y \u2212 A(M (t)) \u2212 X(t)\u22a4w(t)) , h(t)3 , 1nX(t)(y \u2212 A(M (t))\u2212X(t)\u22a4w(t)) .\n2: Initialize: w(0) = 0, V (0) = 0. U (0) = SVD(H (0) 1 \u2212 12h (0) 2 I, k), that is, the\ntop-k left singular vectors. 3: for t = 1, 2, \u00b7 \u00b7 \u00b7 , T do 4: Retrieve n training instances X(t) = [x(t\u22121)n+1, \u00b7 \u00b7 \u00b7 ,x(t\u22121)n+n] . Define\nA(M) , [X(t)i \u22a4MX (t) i ] n i=1.\n5: U\u0302 (t) = (H (t\u22121) 1 \u2212 12h (t\u22121) 2 I +M (t\u22121)\u22a4)U (t\u22121) .\n6: Orthogonalize U\u0302 (t) via QR decomposition: U (t) = QR ( U\u0302 (t) ) .\n7: w(t) = h (t\u22121) 3 +w (t\u22121) . 8: V (t) = (H (t\u22121) 1 \u2212 12h (t\u22121) 2 I +M (t\u22121))U (t) 9: end for\n10: Output: w(T ), U (T ), V (T ) .\non systems with high speed cache. Finally, we export w(T ), U (T ), V (T ) as our estimation of w\u2217 \u2248 w(T ) and M\u2217 \u2248 U (T )V (T )\u22a4.\nOur main theoretical result is presented in the following theorem, which gives the convergence rate of recovery and sampling complexity of gFM when M\u2217 is low rank and the noise \u03be = 0.\nTheorem 1. Suppose xi\u2019s are independently sampled from the standard Gaussian distribution. M\u2217 is a rank k matrix. The noise \u03be = 0. Then with a probability at least 1 \u2212 \u03b7, there exists a constant C and a constant \u03b4 < 1 such that\n\u2016w\u2217 \u2212w(t)\u20162 + \u2016M\u2217 \u2212M (t)\u20162 \u2264\u03b4t(\u2016w\u2217\u20162 + \u2016M\u2217\u20162)\nprovided n \u2265 C(4 \u221a 5\u03c3\u22171/\u03c3 \u2217 k + 3) 2k3d/\u03b42, \u03b4 \u2264 (4 \u221a 5\u03c3\u22171/\u03c3 \u2217 k +3)\u03c3\u2217 k 4 \u221a 5\u03c3\u2217\n1 +3\u03c3\u2217 k +4\n\u221a 5\u2016w\u2217\u20162\n2\n.\nTheorem 1 shows that {w(t),M (t)} will converge to {w\u2217,M\u2217} linearly. The convergence rate is controlled by \u03b4, whose value is on order of O(1/ \u221a n). A small \u03b4 will result in a fast convergence rate but a large sampling complexity. To reduce the sampling complexity, a large \u03b4 is preferred. The largest allowed \u03b4 is bounded by O(1/(\u2016M\u2217\u20162 + \u2016w\u2217\u20162)). The sampling complexity is O((\u03c3\u22171/\u03c3 \u2217 k) 2k3d). If M\u2217 is not well conditioned, it is possible to remove (\u03c3\u22171/\u03c3 \u2217 k) 2 in the sampling complexity by a procedure called \u201csoft-deflation\u201d [Jain et al., 2013, Hardt and Wootters, 2014]. By theorem 1, gFM achieves \u01eb recovery error after retrieving nT = O(k3d log ((\u2016w\u2217\u20162 + \u2016M\u2217\u20162)/\u01eb)) instances.\nThe noisy case where M\u2217 is not exactly low rank and \u03be > 0 is more intricate therefore we postpone it to Subsection 4.1. The main conclusion is similar to the noise-free case Theorem 1 under a small noise assumption."}, {"heading": "4 Theoretical Analysis", "text": "In this section, we give the sketch of our proof of Theorem 1. Omitted details are postponed to appendix.\nFrom high level, our proof constructs an estimation sequence {w\u0303(t), M\u0303 (t), \u01ebt} such that \u01ebt \u2192 0 and \u2016w\u2217 \u2212 w\u0303(t)\u20162 + \u2016M\u2217 \u2212 M\u0303 (t)\u20162 \u2264 \u01ebt . In conventional matrix sensing, this construction is possible when the sensing matrix satisfies the Restricted Isometric Property (RIP) [Cande\u0300s and Recht, 2009]:\nDefinition 2 (\u21132-norm RIP). A sensing operator A is \u21132-norm \u03b4k-RIP if for any rank k matrix M ,\n(1 \u2212 \u03b4k)\u2016M\u2016F \u2264 1\nn \u2016A(M)\u201622 \u2264 (1 + \u03b4k)\u2016M\u2016F .\nWhen A is \u21132-norm \u03b4k-RIP for any rank k matrix M , A\u2032A is nearly isometric [Jain et al., 2012], which implies \u2016M\u2212A\u2032A(M)/n\u20162 \u2264 \u03b4. Then we can construct our estimation sequence as following:\nM\u0303 (t) = 1 n A\u2032A(M\u2217 \u2212 M\u0303 (t\u22121)) + M\u0303 (t\u22121) , w\u0303(t) = (I \u2212 1 n XX\u22a4)(w\u2217 \u2212 w\u0303(t\u22121)) + w\u0303(t\u22121) .\nHowever, in gFM and symmetric rank-one matrix sensing, the \u21132-norm RIP condition cannot be satisfied with high probability [Cai and Zhang, 2015]. To establish an RIP-like condition for rank-one matrix sensing, several variants have been proposed, such as the \u21132/\u21131-RIP condition [Cai and Zhang, 2015, Chen et al., 2015]. The essential idea of these variants is to replace the \u21132norm \u2016A(M)\u20162 with \u21131-norm \u2016A(M)\u20161 then a similar norm inequality can be established for all low rank matrix again. However, even using these \u21131-norm RIP variants, we are still unable to design an efficient alternating algorithm. All these \u21131-norm RIP variants have to deal with trace norm programming problems. In fact, it is impossible to construct an estimation sequence based on \u21131-norm RIP because we require \u21132-norm bound on A\u2032A during the construction.\nA key ingredient of our framework is to propose a novel \u21132-norm RIP condition to overcome the above difficulty. The main technique reason for the failure of conventional \u21132-norm RIP is that it tries to bound A\u2032A(M) over all rank k matrices. This is too aggressive to be successful in rank-one matrix sensing. Regarding to our estimation sequence, what we really need is to make the RIP hold for current low rank matrix M (t). Once we update our estimation M (t+1), we can regenerate a new sensing operator independent of M (t) to avoid bounding A\u2032A over all rank k matrices. To this end, we propose the Conditionally Independent RIP (CI-RIP) condition.\nDefinition 3 (CI-RIP). A matrix sensing operator A is Conditionally Independent RIP with constant \u03b4k, if for a fixed rank k matrix M , A is sampled independently regarding to M and satisfies\n\u2016(I \u2212 1 n A\u2032A)M\u201622 \u2264 \u03b4k . (4)\nAn \u21132-norm or \u21131-norm RIP sensing operator is naturally CI-RIP but the reverse is not true. In CI-RIP,A is no longer a fixed but random sensing operator independent of M . In one-pass algorithm, this is achievable if we always retrieve new instances to construct A in one mini-batch updating. Usually Eq. (4) doesn\u2019t hold in a batch method since M (t+1) depends on A(M (t)).\nAn asymmetric rank-one matrix sensing operator is clearly CI-RIP due to the independency between left/right design vectors. But a symmetric rank-one matrix sensing operator is not CI-RIP. In fact it is a biased estimator since E(x\u22a4Mx) = tr(M) . To this end, we propose a shifted version of CI-RIP for symmetric rank-one matrix sensing operator in the following theorem. This theorem is the key tool in our analysis.\nTheorem 4 (Shifted CI-RIP). Suppose xi are independent standard random Gaussian vectors, M is a fixed symmetric rank k matrix independent of xi and w is a fixed vector. Then with a probability at least 1\u2212\u03b7, provided n \u2265 Ck3d/\u03b42 ,\n\u2016 1 2n A\u2032A(M)\u2212 1 2 tr(M)I \u2212M\u20162 \u2264 \u03b4\u2016M\u20162 .\nTheorem 4 shows that 12nA\u2032A(M) is nearly isometric after shifting by its expectation 12 tr(M)I. The RIP constant \u03b4 = O( \u221a k3d/n) . In gFM, we choose M = M\u2217 \u2212M (t) therefore M is of rank 3k . Under the same settings of Theorem 4, suppose that d \u2265 C then the following lemmas hold true with a probability at least 1\u2212 \u03b7 for fixed w and M . Lemma 5. | 1n1\u22a4A(M))\u2212 tr(M)| \u2264 \u03b4\u2016M\u20162 provided n \u2265 Ck/\u03b42 . Lemma 6. | 1n1\u22a4X\u22a4w| \u2264 \u2016w\u20162\u03b4 provided n \u2265 C/\u03b42 . Lemma 7. \u2016 1nA\u2032(X\u22a4w)\u20162 \u2264 \u2016w\u20162\u03b4 provided n \u2265 Cd/\u03b42 . Lemma 8. \u2016 1nX\u22a4A(M)\u20162 \u2264 \u2016M\u20162\u03b4 provided n \u2265 Ck2d/\u03b42 . Lemma 9. \u2016I \u2212 1nXX\u22a4\u20162 \u2264 \u03b4 provided n \u2265 Cd/\u03b42 .\nEquipping with the above lemmas, we construct our estimation sequence as following.\nLemma 10. Let M (t), H (t) 1 , h (t) 2 ,h (t) 3 be defined as in Algorithm 1. Define \u01ebt = \u2016w\u2217 \u2212w(t)\u20162 + \u2016M\u2217 \u2212M (t)\u20162 . Then with a probability at least 1\u2212 \u03b7, provided n \u2265 Ck3d/\u03b42 ,\nH (t) 1 =M \u2217 \u2212M (t) + tr(M\u2217 \u2212M (t))I +O(\u03b4\u01ebt) , h(t)2 = tr(M\u2217 \u2212M (t)) +O(\u03b4\u01ebt) h (t) 3 =w \u2217 \u2212w(t) +O(\u03b4\u01ebt) .\nSuppose by construction, \u01ebt \u2192 0 when t \u2192 \u221e. Then H(t)1 \u2212 h (t) 2 I +M (t) \u2192 M\u2217 and h(t)3 +w\n(t) \u2192 w\u2217 and then the proof of Theorem 1 is completed. In the following we only need to show that Lemma 10 constructs an estimation sequence with \u01ebt = O(\u03b4\nt) \u2192 0. To this end, we need a few things from matrix perturbation theory.\nBy Theorem 1, U (t) will converge to U\u2217 up to column order perturbation. We use the largest canonical angle to measure the subspace distance spanned by U (t) and U\u2217, which is denoted as \u03b8t = \u03b8(U (t), U\u2217). For any matrix U , it is well known [Zhu and Knyazev, 2013] that\nsin \u03b8(U,U\u2217) = \u2016U\u2217\u22a5\u22a4U\u20162, cos \u03b8(U,U\u2217) = \u03c3k{U\u2217\u22a4U}, tan \u03b8(U,U\u2217) = \u2016U\u2217\u22a5\u22a4U(U\u2217\u22a4U)\u22121\u20162 .\nThe last tangent equality allows us to bound the canonical angle after QR decomposition. Suppose U (t)R = U\u0302 (t) in the QR step of Algorithm 1, we have\ntan \u03b8(U\u0302 (t), U\u2217) = \u2016U\u2217\u22a5\u22a4U\u0302 (t)(U\u2217\u22a4U\u0302 (t))\u22121\u20162 = \u2016U\u2217\u22a5\u22a4U (t)R(U\u2217\u22a4U (t)R)\u22121\u20162 = \u2016U\u2217\u22a5\u22a4U (t)(U\u2217\u22a4U (t))\u22121\u20162 = tan \u03b8(U (t), U\u2217) .\nTherefore, it is more convenient to measure the subspace distance by tangent function.\nTo show \u01ebt \u2192 0, we recursively define the following variables:\n\u03b1t , tan \u03b8t, \u03b2t , \u2016w\u2217 \u2212w(t)\u20162, \u03b3t , \u2016M\u2217 \u2212M (t)\u20162, \u01ebt , \u03b2t + \u03b3t .\nThe following lemma derives the recursive inequalities regarding to {\u03b1t, \u03b2t, \u03b3t} .\nLemma 11. Under the same settings of Theorem 1, suppose \u03b1t \u2264 2, \u03b4\u01ebt \u2264 4 \u221a 5\u03c3\u2217k, then \u03b1t+1 \u2264 4 \u221a 5\u03b4\u03c3\u2217\u22121k (\u03b2t + \u03b3t), \u03b2t+1 \u2264 \u03b4(\u03b2t + \u03b3t), \u03b3t+1 \u2264 \u03b1t+1\u2016M\u2217\u20162 + 2\u03b4(\u03b2t + \u03b3t) .\nIn Lemma 11, when we choose n such that \u03b4 = O(1/ \u221a n) is small enough,\n{\u03b1t, \u03b2t, \u03b3t} will converge to zero. The only question is the initial value {\u03b10, \u03b20, \u03b30}. According to the initialization step of gFM, \u03b20 \u2264 \u2016w\u2217\u20162 and \u03b30 \u2264 \u2016M\u2217\u20162 . To bound \u03b10 , we need the following lemma which directly follows Wely\u2019s and Wedin\u2019s theorems [Stewart and Sun, 1990].\nLemma 12. Denote U and U\u0303 as the top-k left singular vectors of M and M\u0303 = M + O(\u01eb) respectively. The i-th singular value of M is \u03c3i. Suppose that \u01eb \u2264 \u03c3k\u2212\u03c3k+14 . Then the largest canonical angle between U and U\u0303 , denoted as \u03b8(U, U\u0303), is bounded by sin \u03b8(U, U\u0303) \u2264 2\u01eb/(\u03c3k \u2212 \u03c3k+1) .\nAccording to Lemma 12, when 2\u03b4(\u2016w\u2217\u20162+\u2016M\u2217\u20162) \u2264 \u03c3\u2217k/4, we have sin \u03b80 \u2264 4\u03b4(\u2016w\u2217\u20162+\u2016M\u2217\u20162)/\u03c3\u2217k. Therefore, \u03b10 \u2264 2 provided \u03b4 \u2264 \u03c3\u2217k/[8(\u2016w\u2217\u20162+\u2016M\u2217\u20162)] .\nProof of Theorem 1. Suppose that at step t, \u03b1t \u2264 2, \u03b4\u01ebt \u2264 4 \u221a 5\u03c3\u2217k, from Lemma 11, \u03b2t+1 + \u03b3t+1 \u2264\u03b2t+1 + \u03b1t+1\u2016M\u2217\u20162 + 2\u03b4(\u03b2t + \u03b3t) \u2264 \u03b4\u01ebt + 4 \u221a 5\u03b4\u03c3\u2217\u22121k \u01ebt\u2016M\u2217\u20162 + 2\u03b4\u01ebt\n=(4 \u221a 5\u03c3\u22171/\u03c3 \u2217 k + 3)\u03b4\u01ebt .\nTherefore,\n\u01ebt = \u03b2t + \u03b3t \u2264 [(4 \u221a 5\u03c3\u22171/\u03c3 \u2217 k + 3)\u03b4] t(\u03b20 + \u03b30)\n\u03b1t+1 \u2264 4 \u221a 5\u03b4\u03c3\u2217\u22121k (\u03b2t + \u03b3t) \u2264 4 \u221a 5\u03b4\u03c3\u2217\u22121k [(4 \u221a 5\u03c3\u22171/\u03c3 \u2217 k + 3)\u03b4] t(\u03b20 + \u03b30) .\nClearly we need (4 \u221a 5\u03c3\u22171/\u03c3 \u2217 k+3)\u03b4 < 1 to ensure convergence, which is guaranteed by \u03b4 < \u03c3\u2217 k\n4 \u221a 5\u03c3\u2217\n1 +3\u03c3\u2217 k\n. To ensure the recursive inequality holds for any t, we require\n\u03b1t+1 \u2264 2, which is guaranteed by\n4 \u221a 5(\u03b20 + \u03b30)\u03b4/\u03c3 \u2217 k \u2264 2 \u21d4 \u03b4 \u2264 \u03c3\u2217k 2 \u221a 5(\u03c3\u22171 + \u03b20) .\nTo ensure the condition \u03b4\u01ebt \u2264 4 \u221a 5\u03c3\u2217k,\n\u03b4 \u2264 4 \u221a 5\u03c3\u2217k/\u01eb0 = 4 \u221a 5\u03c3\u2217k/(\u03c3 \u2217 1 + \u03b20) \u21d2 \u03b4 \u2264 4 \u221a 5\u03c3\u2217k/\u01ebt .\nIn summary, when\n\u03b4 \u2264 min {\n\u03c3\u2217k 4 \u221a 5(\u03c3\u22171 + \u03b20) , \u03c3\u2217k 4 \u221a 5\u03c3\u22171 + 3\u03c3 \u2217 k , \u03c3\u2217k 2 \u221a 5(\u03c3\u22171 + \u03b20) , \u03c3\u2217k 8(\u03c3\u22171 + \u03b20)\n}\n\u21d0\u03b4 \u2264 \u03c3 \u2217 k\n4 \u221a 5\u03c3\u22171 + 3\u03c3 \u2217 k + 4 \u221a 5\u03b20 .\nwe have \u01ebt = [(4 \u221a 5\u03c3\u22171/\u03c3 \u2217 k + 3)\u03b4] t(\u03c3\u22171 + \u03b30) . To simplify the result, replace \u03b4 with \u03b41 = (4 \u221a 5\u03c3\u22171/\u03c3 \u2217 k + 3)\u03b4. The proof is completed."}, {"heading": "4.1 Noisy Case", "text": "In this subsection, we analyze the performance of gFM under noisy setting. Suppose that M\u2217 is no longer low rank, M\u2217 = U\u2217\u039b\u2217U\u2217\u22a4 + U\u2217\u22a5\u039b \u2217 \u22a5U \u2217 \u22a5 \u22a4 where \u039b\u2217\u22a5 = diag(\u03bbk+1, \u00b7 \u00b7 \u00b7 , \u03bbd) is the residual spectrum. Denote M\u2217k = U\u2217\u039b\u2217U\u2217\u22a4 to be the best rank k approximation of M\u2217 and M\u2217\u22a5 = M\n\u2217 \u2212 M\u2217k . The additive noise \u03bei\u2019s are independently sampled from subgaussian with proxy variance \u03be.\nFirst we generalize the above theorems and lemmas to noisy case.\nLemma 13. Suppose that in Eq. (1) xi\u2019s are independent standard random Gaussian vectors. M is a fixed rank k matrix. M\u2217\u22a5 6= 0 and \u03be > 0. Then provided n \u2265 Ck3d/\u03b42, with a probability at least 1\u2212 \u03b7,\n\u2016 1 2n A\u2032A(M\u2217 \u2212M)\u2212 1 2 tr(M\u2217k \u2212M)I \u2212 (M\u2217k \u2212M)\u20162 \u2264 \u03b4\u2016M\u2217k \u2212M\u20162 + C\u03c3\u2217k+1d2/\n\u221a n\n(5)\n| 1 n 1\u22a4A(M\u2217 \u2212M)\u2212 tr(M\u2217k \u2212M)| \u2264 \u03b4\u2016M\u2217k \u2212M\u20162 + C\u03c3\u2217k+1d2/\n\u221a n (6)\n\u2016 1 n X\u22a4A(M\u2217 \u2212M)\u20162 \u2264 \u03b4\u2016M\u2217k \u2212M\u20162 + C\u03c3\u2217k+1d2/\n\u221a n (7)\n\u2016 1 n A\u2032(X\u22a4w)\u20162 \u2264 \u03b4\u2016w\u20162, \u2016 1 n 1\u22a4X\u22a4w\u20162 \u2264 \u03b4\u2016w\u20162 . (8)\nDefine \u03b3t = \u2016M\u2217k\u2212M (t)\u20162 similar to the noise-free case. According to Lemma 13, when \u03be = 0, for n \u2265 Ck3d/\u03b42,\nH (t) 1 =M \u2217 k \u2212M (t) +\n1 2 tr(M\u2217k \u2212M (t))I +O(\u03b4\u01ebt + C\u03c3\u2217k+1d2/\n\u221a n)\nh (t) 2 =tr(M \u2217 \u2212M (t)) +O(\u03b4\u01ebt + C\u03c3\u2217k+1d2/ \u221a n) h (t) 3 =w \u2217 \u2212w(t) +O(\u03b4\u01ebt + C\u03c3\u2217k+1d2/ \u221a n) .\nDefine r = C\u03c3\u2217k+1d 2/ \u221a n. If \u03be > 0, it is easy to check that the perturbation\nbecomes r\u0302 = r + O(\u03be/ \u221a n) . Therefore we uniformly use r to present the perturbation term. The recursive inequalities regarding to the recovery error is constructed in Lemma 14.\nLemma 14. Under the same settings of Lemma 13, define \u03c1 , 2\u03c3\u2217k+1/(\u03c3 \u2217 k + \u03c3\u2217k+1). Suppose that at any step i, 0 \u2264 i \u2264 t , \u03b1i \u2264 2 . When provided 4 \u221a 5(\u03b4\u01ebt + r) \u2264 \u03c3\u2217k \u2212 \u03c3\u2217k+1, \u03b1t+1 \u2264\u03c1\u03b1t + 4 \u221a 5\n\u03c3\u2217k + \u03c3 \u2217 k+1\n\u03b4\u01ebt + 4 \u221a 5\n\u03c3\u2217k + \u03c3 \u2217 k+1\nr , \u03b2t+1 \u2264 \u03b4\u01ebt + r , \u03b3t+1 \u2264 \u03b1t+1\u2016M\u2217\u20162 + 2\u03b4\u01ebt + 2r .\nThe solution to the recursive inequalities in Lemma 14 is non-trivial. Comparing to the inequalities in Lemma 11, \u03b1t+1 is bounded by \u03b1t in noisy case. Therefore, if we simply follow Lemma 11 to construct recursive inequality about \u01ebt , we will quickly be overloaded by recursive expansion terms. The key construction of our solution is to bound the term \u03b1t + 8 \u221a 5/(\u03c3\u2217k + \u03c3 \u2217 k+1)\u03b4\u01ebt . The solution is given in the following theorem.\nTheorem 15. Define constants\nc =4 \u221a 5/(\u03c3\u2217k + \u03c3 \u2217 k+1) , b = 3+ 4 \u221a 5\u03c3\u22171/(\u03c3 \u2217 k + \u03c3 \u2217 k+1) , q = (1 + \u03c1)/2 ."}, {"heading": "Then for any t \u2265 0,", "text": "\u03b1t + 2c\u03b4\u01ebt \u2264qt ( 2\u2212 (1 + \u03c1)cr\n1\u2212 q\n) + (1 + \u03c1)cr\n1\u2212 q . (9)\nprovided\n\u03b4 \u2264 min{ 1\u2212 \u03c1 4\u03c1\u03c3\u22171c , \u03c1 2b } , (2 + c(\u03c3\u2217k \u2212 \u03c3\u2217k+1))\u03b4\u01eb0 + r \u2264 (\u03c3\u2217k \u2212 \u03c3\u2217k+1) (10)\n4 \u221a 5 ( 4 + 2c(\u03c3\u2217k \u2212 \u03c3\u2217k+1) ) \u03b4\u01eb0 + 4 \u221a 5 ( 4 + (\u03c3\u2217k \u2212 \u03c3\u2217k+1) ) r \u2264 (\u03c3\u2217k \u2212 \u03c3\u2217k+1)2 .\nTheorem 15 gives the convergence rate of gFM under noisy settings. We bound \u03b1t + 2c\u03b4\u01ebt as the index of recovery error, whose convergence rate is linear. The convergence rate is controlled by q, a constant depends on the eigen gap \u03c3\u2217k+1/\u03c3 \u2217 k . The final recovery error is bounded by O(r/(1 \u2212 q)) . Eq. (10) is the small noise condition to ensure the noisy recovery is possible. Generally speaking, learning a d\u00d7d matrix with O(d) samples is an ill-conditioned problem when the target matrix is full rank. The small noise condition given by Eq. (10) essentially says that M\u2217 can be slightly deviated from low rank manifold and the noise shouldn\u2019t be too large to blur the spectrum ofM\u2217. When the noise is large, Eq. (10) will be satisfied with n = O(d2) which is the information-theoretical lower bound for recovering a full rank matrix."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a provable efficient algorithm to solve generalized Factorization Machine (gFM) and rank-one matrix sensing. Our method is based on an one-pass alternating updating framework. The proposed algorithm is able to learn gFM within O(kd) memory on steaming data, has linear convergence rate and only requires matrix-vector product implementation. The algorithm takes no more than O(k3d log (1/\u01eb)) instances to achieve O(\u01eb) recovery error."}, {"heading": "A Preliminary", "text": "In this section, we present several important theorems and lemmas in our analysis. The following concentration inequalities are well known.\nLemma 16. Let xi be zero-mean sub-Gaussian distribution with variance proxy \u03c3 2. Denote Sn = \u2211n i=1 aixi for a fixed sequence {ai}. Then\nPr(|Sn| > t) \u2264 2 exp(\u2212 t 2\n2\u03c32( \u2211n i=1 a 2 i ) ) .\nThat is, with a probability at least 1\u2212 \u03b7,\n|Sn| \u2264 \u03c3\n\u221a \u221a \u221a \u221a n \u2211\ni=1\na2i \u221a 2 log(2/\u03b7) .\nCorollary 17. Let xi \u223c N (0, 1) be a standard Gaussian distribution. Then with a probability at least 1\u2212 \u03b7,\nn \u2211\ni=1\nai(x 2 i \u2212 1) \u22642\n\u221a \u221a \u221a \u221a n \u2211\ni=1\nai \u221a 2 log(2/\u03b7) .\nFor random matrix, we have matrix concentration inequalities [?].\nTheorem 18 (Matrix Bernstein\u2019s Inequality [?]). Suppose {Si}ni=1 are set of independent random matrices of dimension d1 \u00d7 d2,\n\u2016Si \u2212 ESi\u2016 \u2264 L ."}, {"heading": "Define", "text": "Z = n \u2211\ni=1\nSi, \u03c3 2 =\n1 n max(E\u2016(Z \u2212 EZ)(Z \u2212 EZ)\u22a4\u20162, E\u2016(Z \u2212 EZ)\u22a4(Z \u2212 EZ)\u20162) .\nThe with a probability at least 1\u2212 \u03b4, for any 0 < \u01eb < 1, 1\nn \u2016Z \u2212 EZ\u20162 \u2264 9\u01eb\n\u221a\nlog((d1 + d2)/\u03b4)\nprovided n \u2265 max(\u03c32, L)/\u01eb2 . And for any n,\n1 n \u2016Z \u2212EZ\u20162 \u22644 3 L n log((d1 + d2)/\u03b4) + 3\n\u221a\n2 \u03c32\nn log((d1 + d2)/\u03b4) .\nUsing matrix Bernstein\u2019s inequality, we can bound the covariance estimator.\nCorollary 19 (Matrix Bernstein\u2019s Inequality for Covariance Estimator [?]). Suppose xi \u2208 Rd, i = 1, 2, \u00b7 \u00b7 \u00b7 , n are independent random variables with zero mean. \u2016xi\u20162 \u2264 B, A = E(xixi\u22a4) Then with a probability at least 1\u2212 \u03b4,\n\u2016 1 n\nn \u2211\ni=1\nxix \u22a4 i \u2212 A\u20162 \u2264 9\u01eb \u221a log(2d/\u03b4)/n\nprovided\nn \u2265 max(B\u2016A\u2016, B)/\u01eb2 ."}, {"heading": "B Proof of Lemmas", "text": ""}, {"heading": "B.1 Proof of Lemma 5", "text": "Proof. Denote the eigenvalue decomposition of M as\nM = U\u039bU\u22a4 = Udiag(\u03bb1, \u03bb2, \u00b7 \u00b7 \u00b7 , \u03bbk)U\u22a4\nSince Gaussian distribution is rotation invariant, x\u0302i = U \u22a4 xi also follows standard Gaussian distribution.\nxi \u22a4Mxi =xi \u22a4U\u039bU\u22a4xi = |x\u0302i\u22a4\u039bx\u0302i| = k \u2211\nj=1\n\u03bbjx\u0302 2 i,j .\nIt is easy to see that E(xi \u22a4Mxi) = \u2211\nj \u03bbj = tr(M). Define\nai ,xi \u22a4Mxi \u2212 tr(M) =\nd \u2211\nj=1\n\u03bbj(x\u0302 2 i,j \u2212 1)\nAccording to Corollary 17, for a fixed i, with a probability at least 1\u2212 \u03b7,\n|ai| \u2264 2\u2016M\u2016F \u221a 2 log(2/\u03b7) .\nThen for any i, with a probability at least 1\u2212 \u03b7,\n|ai| \u2264 2\u2016M\u2016F \u221a 2 log(2n/\u03b7) .\nApply Corollary 17 again, with a probability at least 1\u2212 2\u03b7,\n| 1 n\nn \u2211\ni=1\nai \u2212 tr(M)| \u22642\u2016M\u2016F \u221a 2 log(2n/\u03b7) \u221a 2 log(2/\u03b7)/n\n\u22642 \u221a k\u2016M\u20162 \u221a 2 log(2n/\u03b7) \u221a 2 log(2/\u03b7)/n .\nDenote \u03b4 = 2 \u221a k \u221a 2 log(2n/\u03b7) \u221a 2 log(2/\u03b7)/n. Then when n \u2265 Ck/\u03b42,\n| 1 n\nn \u2211\ni=1\nai \u2212 tr(M)| \u2264 \u2016M\u20162\u03b4 ."}, {"heading": "B.2 Proof of Lemma 6", "text": "Proof. Define random variable\nai =xi \u22a4 w, Eai = 0\nEa2i \u2264\u2016w\u201622 Then according to Lemma 16, with a probability at least 1\u2212 \u03b7,\n| 1 n\nn \u2211\ni=1\nai| \u2264 \u2016w\u20162 \u221a 2 log(2/\u03b7)/n ."}, {"heading": "B.3 Proof of Lemma 8", "text": "Proof. Define random vector\nai = xixi \u22a4Mxi, Eai = 0 .\nWith a probability at least (1\u2212 \u03b71)(1\u2212 \u03b72),\n\u2016ai\u20162 =\u2016xixi\u22a4Mxi\u20162 \u2264 \u2016xi\u22a4Mxi\u20162\u2016xi\u20162 \u2264(|tr(M)|+ 2\u2016M\u2016F \u221a 2 log(2n/\u03b71)) \u221a 2d log(2n/\u03b72)\n,c1 \u221a 2d log(2n/\u03b72) .\n\u2016Eai\u22a4ai\u20162 =\u2016xi\u22a4Mxixi\u22a4xixi\u22a4Mxi\u20162 \u2264(xi\u22a4Mxi)2\u2016xi\u201622 \u22642c21d log(2n/\u03b72) .\nBy matrix Bernstein\u2019s inequality, the concentration holds when\nn \u2265 1 \u01eb2 max{c1 \u221a 2d log(2n/\u03b72), 2c 2 1d log(2n/\u03b72)}\n= 1\n\u01eb2 O(k2d\u2016M\u201622) .\nTherefore, after taking the union bound, there exists some constant C2 = O(log(2n/\u03b7)),\n\u2016 1 n\nn \u2211\ni=1\nai\u20162 \u2264 \u01eb\nn \u2265 C2k2d\u2016M\u201622 log(2(d+ 1)/\u03b7)/\u01eb2 . Denote \u03b4 = \u2016M\u20162/\u01eb. Then when n \u2265 Ck2d/\u03b4,\n\u2016 1 n\nn \u2211\ni=1\nai\u20162 \u2264 \u2016M\u20162\u03b4 .\nB.4 Proof of Lemma 7\n\u2016 1 n A\u2032(X\u22a4w)\u20162 =\u2016 1 n\nn \u2211\ni=1\nxixi \u22a4 wxi \u22a4\u20162 .\nE{xixi\u22a4wxi\u22a4} = 0\n\u2016xixi\u22a4wxi\u22a4\u20162 \u2264\u2016xi\u22a4w\u20162\u2016xi\u201622 \u22642\u2016w\u20162 \u221a 2 log(2/\u03b7)(d+ 2 \u221a 2d log(2n/\u03b7))\n\u22644\u2016w\u20162 \u221a 2 log(2/\u03b7)d\nprovided d \u2265 8 log(2n/\u03b7). Now considering\n{Exixi\u22a4wxi\u22a4xiw\u22a4xixi\u22a4}pq =E{( \u2211 xpxqwi1xi1wi2xi2x 2 i3)}\nWhen p 6= q,\nE{( \u2211 xpxqwi1xi1wi2xi2x 2 i3)} = E{(2 \u2211\ni3\nxpxqwpxpwqxqx 2 i3)}\n= E{(2 \u2211\ni3\nx2px 2 qwpwqx 2 i3)}\n= 2wpwqE{( \u2211\ni3\nx2px 2 qx 2 i3)}\n= 2wpwqd\nWhen p = q,\nE{( \u2211 xpxqwi1xi1wi2xi2x 2 i3)} =E{( \u2211 x2pwi1xi1wi2xi2x 2 i3)}\n=E{( \u2211 x2pwpxpwpxpx 2 i3 + \u2211 x2pwjxjwjxjx 2 i3 + \u2211 x2pwi3xi3wi3xi3x 2 i3)}\n=E{( \u2211 i36=p x4pw 2 px 2 i3 + \u2211 j 6=i36=p x2pw 2 jx 2 jx 2 i3 + \u2211 i36=p x2pw 2 i3x 4 i3)} =w2p(d\u2212 1) + \u2211\nj 6=p w2j (d\u2212 1) +\n\u2211 i36=p w2i3\n=w2p(d\u2212 1) + \u2211 j 6=p w2jd = w 2 p(d\u2212 1) +\nd \u2211\nj=1\nw2jd\u2212 w2pd\n= d \u2211\nj=1\nw2jd\u2212 w2p\nTherefore,\nExixi \u22a4 wxi \u22a4 xiw \u22a4 xixi \u22a4 =ddiag{\u2016w\u201622} \u2212 diag{w \u25e6w}+ 2dww\u22a4\n\u2016Exixi\u22a4wxi\u22a4xiw\u22a4xixi\u22a4\u20162 \u2264 4d\u2016w\u201622 Using matrix Bernstein\u2019s inequality,\n\u2016 1 n\nn \u2211\ni=1\nxixi \u22a4 wxi \u22a4\u20162 \u22644 3\n4\u2016w\u20162 \u221a 2 log(2/\u03b7)d\nn log(2d/\u03b7)\n+ 3\n\u221a\n2 4d\u2016w\u201622\nn log(2d/\u03b7)\n\u2264C\u2016w\u20162 \u221a d\nn\nDenote \u03b4 = C \u221a d/n, when n \u2265 Cd/\u03b42, d \u2265 8 log(2n/\u03b7),\n\u2016 1 n\nn \u2211\ni=1\nxixi \u22a4 wxi \u22a4\u20162 \u2264 \u2016w\u20162\u03b4"}, {"heading": "B.5 Proof of Lemma 9", "text": "According to Corollary 19, when d \u2265 8 log(2n/\u03b7),\n\u2016xi\u20162 \u2264 2d\nTherefore, with a probability at least 1\u2212 \u03b7,\n\u2016I \u2212 1 n XX\u22a4\u20162 \u22649\u01eb \u221a log(2d/\u03b7)/n\nfor n \u2265 2d/\u01eb2. Denote \u03b4 = 9\u01eb \u221a log(2d/\u03b7)/n, then when n \u2265 Cd/\u03b42,\n\u2016I \u2212 1 n XX\u22a4\u20162 \u2264\u03b4 ."}, {"heading": "B.6 Proof of Lemma 11", "text": "To derive \u03b1t+1 ,\n\u2016U\u2217\u22a5\u22a4[M\u2217 +O(2\u03b4\u01ebt)]U (t)\u20162 \u2264\u2016U\u2217\u22a5\u22a4M\u2217U (t)\u20162 + 2\u03b4\u01ebt \u22642\u03b4\u01ebt\n\u03c3k{U\u2217\u22a4[M\u2217 +O(2\u03b4\u01ebt)]U (t)} \u2265U\u2217\u22a4M\u2217U (t) \u2212 2\u03b4\u01ebt \u2265\u03c3\u2217k\u03c3k{U\u2217\u22a4U (t)} \u2212 2\u03b4\u01ebt =\u03c3\u2217k cos \u03b8t \u2212 2\u03b4\u01ebt\n\u03b1t+1 = tan \u03b8t+1 = \u2016U\u2217\u22a5\u22a4[M\u2217 +O(2\u03b4\u01ebt)]U (t)\u20162 \u03c3k{U\u2217\u22a4[M\u2217 +O(2\u03b4\u01ebt)]U (t)}\n\u2264 2\u03b4\u01ebt \u03c3\u2217k cos \u03b8t \u2212 2\u03b4\u01ebt .\nAccording to the assumption, cos \u03b8t \u2265 1\u221a5 , 2\u03b4\u01ebt \u2264 1 2 \u221a 5 \u03c3\u2217k, therefore\n\u03b1t+1 \u22642 \u221a 5\u01ebt/\u03c3 \u2217 k = 4 \u221a 5\u03b4(\u03b2t + \u03b3t)/\u03c3 \u2217 k .\nTo derive \u03b3t+1,\n\u03b3t+1 =\u2016M\u2217 \u2212M (t+1)\u20162 =\u2016M\u2217 \u2212 (U (t+1)U (t+1)\u22a4(H(t)1 \u2212H(t)2 +M (t))\u22a4)\u20162 =\u2016M\u2217 \u2212 U (t+1)U (t+1)\u22a4(M\u2217 +O(2\u03b4(\u03b3t + \u03b2t)))\u22a4\u20162 =\u2016(I \u2212 U (t+1)U (t+1)\u22a4)M\u2217 + U (t+1)U (t+1)\u22a4O(2\u03b4(\u03b3t + \u03b2t)))\u22a4\u20162 \u2264\u2016(I \u2212 U (t+1)U (t+1)\u22a4)M\u2217\u20162 +O(2\u03b4(\u03b3t + \u03b2t)) \u2264 tan \u03b8t+1\u2016M\u2217\u20162 + 2\u03b4(\u03b3t + \u03b2t) =\u03b1t+1\u2016M\u2217\u20162 + 2\u03b4(\u03b3t + \u03b2t) .\nThe recursive inequality of \u03b2t is trivial."}, {"heading": "C Proof of Theorem 4", "text": "Proof. Denote \u03c31 = \u2016M\u20162. Define random matrix\nBi =xixi \u22a4Mxixi \u22a4 .\nIt is easy to check that\nEBi =2M + tr(M)I .\n\u2016Bi \u2212 EBi\u20162 =\u2016xixi\u22a4Mxixi\u22a4 \u2212 2M \u2212 tr(M)I\u20162 \u2264\u2016xixi\u22a4Mxixi\u22a4\u20162 + \u20162M \u2212 tr(M)I\u20162 \u2264\u2016xixi\u22a4Mxixi\u22a4\u20162 + 2\u2016M\u20162 + |tr(M)| .\nAccording to Lemma 5, with a probability at least 1\u2212 \u03b72, for any i \u2208 {1, \u00b7 \u00b7 \u00b7 , n},\n|xi\u22a4Mxi| \u2264|tr(M)|+ 2\u2016M\u2016F \u221a 2 log(2n/\u03b72) , c1 .\nTherefore we have, with a probability at least (1\u2212 \u03b71)(1\u2212 \u03b72),\n\u2016Bi \u2212 EBi\u20162 \u2264\u2016xi\u201622 |xi\u22a4Mxi|+ 2\u2016M\u20162 + |tr(M)| \u22642d log(2n/\u03b71)|tr(M)|+ 2\u2016M\u2016F \u221a\n2 log(2n/\u03b72) + 2\u2016M\u20162 + |tr(M)| \u2264Cdk\u03c31 .\nNext we need to bound\n\u2016E(Bi \u2212 EBi)(Bi \u2212EBi)\u22a4\u20162 = \u2016E(B2i )\u2212 (EBi)2\u20162 \u2264 \u2016E(B2i )\u20162 + \u2016EBi\u201622 \u2264 \u2016E(B2i )\u20162 + 2|tr(M)|2 + 2\u2016M\u201622\nTo bound \u2016E(B2i )\u20162, denote the eigenvalue decomposition of M as\nM = U\u039bU\u22a4 = Udiag(\u03bb1, \u03bb2, \u00b7 \u00b7 \u00b7 , \u03bbk)U\u22a4\nLet U\u22a5 be the complementary basis matrix of U . Define random variables ui , U \u22a4 xi, vi , U\u22a5 \u22a4 xi. Since xi are standard random Gaussian, u and v should also be jointly random Gaussian thus independent.\n\u2016E(B2i )\u20162 =\u2016E(xixi\u22a4Mxixi\u22a4xixi\u22a4Mxixi\u22a4)\u20162\n=\u2016E( [ ui\nvi\n]\nu \u22a4 i \u039bui(\u2016ui\u201622 + \u2016vi\u201622)ui\u22a4\u039bui\n[\nui\nvi\n]\n\u22a4)\u20162\n\u2264\u2016E(uiu\u22a4i \u039bui(\u2016ui\u201622 + \u2016vi\u201622)ui\u22a4\u039buiui\u22a4\u20162 + 2\u2016E(uiu\u22a4i \u039bui(\u2016ui\u201622 + \u2016vi\u201622)ui\u22a4\u039buivi\u22a4\u20162 + \u2016E(viu\u22a4i \u039bui(\u2016ui\u201622 + \u2016vi\u201622)ui\u22a4\u039buivi\u22a4\u20162 \u2264\u2016E(uiu\u22a4i \u039bui\u2016ui\u201622ui\u22a4\u039buiui\u22a4)\u20162 + \u2016E(uiu\u22a4i \u039bui\u2016vi\u201622ui\u22a4\u039buiui\u22a4)\u20162 + 2\u2016E(uiu\u22a4i \u039bui\u2016ui\u201622ui\u22a4\u039buivi\u22a4)\u20162 + 2\u2016E(uiu\u22a4i \u039bui\u2016vi\u201622ui\u22a4\u039buivi\u22a4)\u20162 + \u2016E(viu\u22a4i \u039bui\u2016ui\u201622ui\u22a4\u039buivi\u22a4)\u20162 + \u2016E(viu\u22a4i \u039bui\u2016vi\u201622ui\u22a4\u039buivi\u22a4)\u20162 .\nLet us bound the above 6 terms respectively. Recall that with a probability at least 1\u2212 \u03b72,\n|u\u22a4i \u039bui| =|xi\u22a4Mxi| \u2264 c1 .\nWith a probability at least 1\u2212\u03b73, for any i \u2208 {1, \u00b7 \u00b7 \u00b7 , n}, \u2016ui\u20162 \u2264 2 \u221a k log(4n/\u03b73),\u2016vi\u20162 \u2264 2 \u221a d log(4n/\u03b73). Then:\n\u2016E(uiu\u22a4i \u039bui\u2016ui\u201622ui\u22a4\u039buiui\u22a4)\u20162 =\u2016E{ ( (u\u22a4i \u039bui) 2\u2016ui\u201622 ) uiui \u22a4}\u20162\n\u2264(u\u22a4i \u039bui)2\u2016ui\u201642 \u226432c21k2 log2(2n/\u03b73) .\n\u2016E(uiu\u22a4i \u039bui\u2016vi\u201622ui\u22a4\u039buiui\u22a4)\u20162 =\u2016E(\u2016vi\u201622)E(uiu\u22a4i \u039buiui\u22a4\u039buiui\u22a4)\u20162 \u22644d log(4n/\u03b73)\u2016E(uiu\u22a4i \u039buiui\u22a4\u039buiui\u22a4)\u20162 \u22644d log(4n/\u03b73)\u2016ui\u201622(u\u22a4i \u039bui)2 \u22644d log(4n/\u03b73)c21(4k log(4n/\u03b73)) .\n2\u2016E(uiu\u22a4i \u039bui\u2016ui\u201622ui\u22a4\u039buivi\u22a4)\u20162 =2\u2016E(uiu\u22a4i \u039bui\u2016ui\u201622ui\u22a4\u039bui)E(vi\u22a4)\u20162 = 0\n2\u2016E(uiu\u22a4i \u039bui\u2016vi\u201622ui\u22a4\u039buivi\u22a4)\u20162 =2\u2016E(ui(u\u22a4i \u039bui)2)E(\u2016vi\u201622vi\u22a4)\u20162 =2\u2016E(ui(u\u22a4i \u039bui)2)E(vi\u22a4vivi\u22a4)\u20162 = 0\n\u2016E(viu\u22a4i \u039bui\u2016ui\u201622ui\u22a4\u039buivi\u22a4)\u20162 =\u2016E(u\u22a4i \u039bui\u2016ui\u201622ui\u22a4\u039bui)E(vivi\u22a4)\u20162 =\u2016E(u\u22a4i \u039bui\u2016ui\u201622ui\u22a4\u039bui)\u20162 \u2264(u\u22a4i \u039bui)2\u2016ui\u201622 \u22644c21k log(4n/\u03b73) .\n\u2016E(viu\u22a4i \u039bui\u2016vi\u201622ui\u22a4\u039buivi\u22a4)\u20162 =\u2016E{(u\u22a4i \u039bui)2}E(vi\u2016vi\u201622vi\u22a4)\u20162 =\u2016E{(u\u22a4i \u039bui)2}(d+ 2)I\u20162 \u2264(d+ 2)(u\u22a4i \u039bui)2 \u2264c21(d+ 2)\nAdd all above together, we have\n\u2016E(B2i )\u20162 \u226432c21k2 log2(2n/\u03b73) + 4d log(4n/\u03b73)c21(4k log(4n/\u03b73)) + 4c21k log(4n/\u03b73) + c 2 1(d+ 2)\n\u2264Ck3d\u03c31 .\nApply matrix Bernsterin\u2019s inequality, the proof is completed."}, {"heading": "D Proof of Lemma 15", "text": "We assume that n \u2265 Ck3d/\u03b42 . To prove Eq. (5)\n\u2016 1 2n A\u2032A(M\u2217 \u2212M)\u2212 1 2 tr(M\u2217k \u2212M)I \u2212 (M\u2217k \u2212M)\u20162\n\u2264\u2016 1 2n A\u2032A(M\u2217k \u2212M)\u2212 1 2 tr(M\u2217k \u2212M)I \u2212 (M\u2217k \u2212M)\u20162 + \u2016 1 2n A\u2032A(M\u2217\u22a5)\u20162 \u2264\u2016 1 2n A\u2032A(M\u2217\u22a5)\u20162 + \u03b4\u2016M\u2217k \u2212M\u20162 .\nThe last inequality is because of Theorem 4. To bound the first term in the last inequality, define random matrix\nBi = xixi \u22a4M\u2217\u22a5xixi \u22a4\nAs proved in Theorem 4, EBi = 2M \u2217 \u22a5 + tr(M \u2217 \u22a5)I .\n\u2016(Bi \u2212 EBi)\u20162 =\u2016xixi\u22a4M\u2217\u22a5xixi\u22a4 \u2212 2M\u2217\u22a5 + tr(M\u2217\u22a5)I\u20162 \u2264\u2016xixi\u22a4M\u2217\u22a5xixi\u22a4\u20162 + 2\u2016M\u2217\u22a5\u20162 + \u2016tr(M\u2217\u22a5)I\u20162 =\u2016xixi\u22a4M\u2217\u22a5xixi\u22a4\u20162 + 2\u03c3\u2217k+1 + |tr(M\u2217\u22a5)|\nWhile\n\u2016xixi\u22a4M\u2217\u22a5xixi\u22a4\u20162 \u2264\u2016M\u2217\u22a5\u20162\u2016xi\u201642 \u2264\u03c3\u2217k+1(d+ 2 \u221a 2d log(2n/\u03b7))2\n\u2264Cd2\u03c3\u2217k+1\nApplying matrix Bernstein\u2019s inequality, with a probability at least 1\u2212 \u03b7, we have\n\u2016 1 n\nn \u2211\ni=1\n(Bi \u2212 EBi)\u20162 \u2264C\u03c3\u2217k+1d2/ \u221a n .\nTherefore\n\u2016 1 2n A\u2032A(M\u2217 \u2212M)\u2212 1 2 tr(M\u2217k \u2212M)I \u2212 (M\u2217k \u2212M)\u20162 \u2264\u03b4\u2016M\u2217k \u2212M\u20162 + C\u03c3\u22172k+1d4/\n\u221a n .\nThe other inequalities can be similarly proved."}, {"heading": "E Proof of Lemma 14", "text": "First we bound \u03b1t+1. According to assumption, when\n2(\u03b4\u01ebt + r) \u2264 \u03c3\u2217k \u2212 \u03c3\u2217k+1\n2\u03c3\u2217k \u03c3\u2217k/\n\u221a 5\nwe have\n\u03b1t+1 \u2264 \u03c3\u2217k+1 sin \u03b8t + 2(\u03b4\u01ebt + r)\n\u03c3\u2217k cos \u03b8t \u2212 2(\u03b4\u01ebt + r)\n\u2264 2\u03c3 \u2217 k\n\u03c3\u2217k + \u03c3 \u2217 k+1\n\u03c3\u2217k+1 sin \u03b8t + 2(\u03b4\u01ebt + r)\n\u03c3\u2217k cos \u03b8t\n\u2264 2\u03c3 \u2217 k+1\n\u03c3\u2217k + \u03c3 \u2217 k+1\ntan \u03b8t + 2\n\u03c3\u2217k + \u03c3 \u2217 k+1\n2(\u03b4\u01ebt + r)\ncos \u03b8t\n\u2264 2\u03c3 \u2217 k+1\n\u03c3\u2217k + \u03c3 \u2217 k+1\ntan \u03b8t + 4 \u221a 5\n\u03c3\u2217k + \u03c3 \u2217 k+1\n(\u03b4\u01ebt + r)\n\u2264\u03c1\u03b1t + 4 \u221a 5\n\u03c3\u2217k + \u03c3 \u2217 k+1\n\u03b4\u01ebt + 4 \u221a 5\n\u03c3\u2217k + \u03c3 \u2217 k+1\nr .\nTo bound \u03b2t+1. Clearly \u03b2t+1 \u2264 \u03b4\u01ebt + r. To bound \u03b3t+1, following the noise-free case,\n\u03b3t+1 \u2264\u03b1t+1\u2016M\u2217\u20162 + 2\u03b4\u01ebt + 2r ."}, {"heading": "F Proof of Lemma 15", "text": "Abbreviate\nc = 4 \u221a 5\n\u03c3\u2217k + \u03c3 \u2217 k+1\nThen\n\u03b1t+1 \u2264\u03c1\u03b1t + c\u03b4\u01ebt + cr .\nAccording to Lemma 14,\n\u03b2t+1 + \u03b3t+1 \u2264\u03b4\u01ebt + r + \u03b1t+1\u2016M\u2217\u20162 + 2\u03b4\u01ebt + 2r =\u03c3\u22171\u03b1t+1 + 3\u03b4\u01ebt + 3r\n\u2264\u03c3\u22171(\u03c1\u03b1t + c\u03b4\u01ebt + cr) + 3\u03b4\u01ebt + 3r =\u03c1\u03c3\u22171\u03b1t + (\u03c3 \u2217 1c+ 3)\u03b4\u01ebt + (\u03c3 \u2217 1c+ 3)r\nTherefore, abbreviate b , (\u03c3\u22171c+ 3), {\n\u03b1t+1 \u2264 \u03c1\u03b1t + c\u03b4\u01ebt + cr \u01ebt+1 \u2264 \u03c1\u03c3\u22171\u03b1t + b\u03b4\u01ebt + br\ndefine\nft =\u03b1t + 2c\u03b4\u01ebt\nft+1 =at+1 + 2c\u03b4\u01ebt+1\n\u2264\u03c1\u03b1t + c\u03b4\u01ebt + cr + 2c\u03b4(\u03c1\u03c3\u22171\u03b1t + b\u03b4\u01ebt + br) =\u03c1\u03b1t + c\u03b4\u01ebt + cr + 2c\u03b4\u03c1\u03c3 \u2217 1\u03b1t + 2c\u03b4b\u03b4\u01ebt + 2c\u03b4br =(\u03c1+ 2c\u03b4\u03c1\u03c3\u22171)\u03b1t + (c+ 2c\u03b4b)\u03b4\u01ebt + (1 + 2\u03b4b)cr\nWhen\n\u03b4 \u2264 1\u2212 \u03c1 4\u03c1\u03c3\u22171c\n\u21d2\u03c1+ 2c\u03b4\u03c1\u03c3\u22171 \u2264 1 + \u03c1\n2\nAnd when\n\u21d2\u03b4 \u2264 \u03c1 2b \u21d22\u03b4b \u2264 \u03c1 \u21d22c\u03b4b \u2264 \u03c1c \u21d2c+ 2c\u03b4b \u2264 (1 + \u03c1)c\n\u21d2c+ 2c\u03b4b \u2264 1 + \u03c1 2 2c\nThen abbreviate R , (c+ 2c\u03b4b)\u03b4\u01ebt + (1 + 2\u03b4b)cr we have\nft+1 \u2264 1 + \u03c1 2 ft + (1 + 2\u03b4b)cr \u2264 1 + \u03c1 2 ft + (1 + \u03c1)cr\nAbbreviate q = (1 + \u03c1)/2,\nft \u2264 (1 + \u03c1)cr 1\u2212 q + q t(f0 \u2212 (1 + \u03c1)cr 1\u2212 q )\nTo ensure \u03b1t+1 \u2264 2, we require\nf0 \u2264 2 \u21d0\u03b10 + 2c\u03b4\u01eb0 \u2264 2\nAccording to Lemma 12,\n\u03b10 \u2264 2 \u03c3\u2217k \u2212 \u03c3\u2217k+1 2(\u03b4\u01eb0 + r) = 4 \u03c3\u2217k \u2212 \u03c3\u2217k+1 (\u03b4\u01eb0 + r)\n\u03b10 + 2c\u03b4\u01eb0 \u2264 2\n\u21d0 4 \u03c3\u2217k \u2212 \u03c3\u2217k+1 (\u03b4\u01eb0 + r) + 2c\u03b4\u01eb0 \u2264 2 \u21d0(4 + 2c(\u03c3\u2217k \u2212 \u03c3\u2217k+1))\u03b4\u01eb0 + 4r \u2264 2(\u03c3\u2217k \u2212 \u03c3\u2217k+1) \u21d0(2 + c(\u03c3\u2217k \u2212 \u03c3\u2217k+1))\u03b4\u01eb0 + r \u2264 (\u03c3\u2217k \u2212 \u03c3\u2217k+1)\nIn summary,\n\u03b1t + 2c\u03b4\u01ebt \u2264qt(f0 \u2212 (1 + \u03c1)cr 1\u2212 q ) + (1 + \u03c1)cr\n1\u2212 q provided\n\u03b4 \u2264min{ 1\u2212 \u03c1 4\u03c1\u03c3\u22171c , \u03c1 2b }\nand\n(2 + c(\u03c3\u2217k \u2212 \u03c3\u2217k+1))\u03b4\u01eb0 + r \u2264 (\u03c3\u2217k \u2212 \u03c3\u2217k+1) 4 \u221a 5(\u03b4max\nt \u01ebt + r) \u2264 \u03c3\u2217k \u2212 \u03c3\u2217k+1\nTo ensure the last inequality,\n\u03b4max t \u01ebt \u2264f0 \u2264 \u03b10 + 2c\u03b4\u01eb0 \u2264 4 \u03c3\u2217k \u2212 \u03c3\u2217k+1 (\u03b4\u01eb0 + r) + 2c\u03b4\u01eb0\n=( 4\n\u03c3\u2217k \u2212 \u03c3\u2217k+1 + 2c)\u03b4\u01eb0 +\n4\n\u03c3\u2217k \u2212 \u03c3\u2217k+1 r\nTherefore we need the condition\n4 \u221a 5 (\n4\n\u03c3\u2217k \u2212 \u03c3\u2217k+1 + 2c\n) \u03b4\u01eb0 + 4 \u221a 5 (\n4\n\u03c3\u2217k \u2212 \u03c3\u2217k+1 + 1\n)\nr \u2264 \u03c3\u2217k \u2212 \u03c3\u2217k+1\n\u21d04 \u221a 5 (4 + 2c(\u03c3\u2217k \u2212 \u03c3\u2217k+1)) \u03b4\u01eb0 + 4 \u221a 5 (4 + (\u03c3\u2217k \u2212 \u03c3\u2217k+1)) r \u2264 (\u03c3\u2217k \u2212 \u03c3\u2217k+1)2"}], "references": [{"title": "Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms", "author": ["Mathieu Blondel", "Masakazu Ishihata", "Akinori Fujino", "Naonori Ueda"], "venue": null, "citeRegEx": "Blondel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2016}, {"title": "ROP: Matrix recovery via rank-one projections", "author": ["T. Tony Cai", "Anru Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "Cai and Zhang.,? \\Q2015\\E", "shortCiteRegEx": "Cai and Zhang.", "year": 2015}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "Phase Retrieval via Matrix Completion", "author": ["Emmanuel J. Candes", "Yonina Eldar", "Thomas Strohmer", "Vlad Voroninski"], "venue": null, "citeRegEx": "Candes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2011}, {"title": "Exact and stable covariance estimation from quadratic sampling via convex programming", "author": ["Yuxin Chen", "Yuejie Chi", "Andrea J. Goldsmith"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "An overview of low-rank matrix recovery from incomplete observations", "author": ["Mark A. Davenport", "Justin Romberg"], "venue": null, "citeRegEx": "Davenport and Romberg.,? \\Q2016\\E", "shortCiteRegEx": "Davenport and Romberg.", "year": 2016}, {"title": "The Noisy Power Method: A Meta Algorithm", "author": ["Moritz Hardt", "Eric Price"], "venue": null, "citeRegEx": "Hardt and Price.,? \\Q2013\\E", "shortCiteRegEx": "Hardt and Price.", "year": 2013}, {"title": "Low-rank Matrix Completion", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "Low rank matrix recovery", "author": ["Richard Kueng", "Holger Rauhut", "Ulrich Terstiege"], "venue": null, "citeRegEx": "Kueng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kueng et al\\.", "year": 2011}, {"title": "Matrix Perturbation Theory", "author": ["G.W. Stewart", "Ji-guang Sun"], "venue": null, "citeRegEx": "Stewart and Sun.,? \\Q1990\\E", "shortCiteRegEx": "Stewart and Sun.", "year": 1990}], "referenceMentions": [{"referenceID": 3, "context": ", 2014] as a special case, where the latter one has been studied widely in context such as inductive matrix completion [Jain and Dhillon, 2013] and phase retrieval [Candes et al., 2011].", "startOffset": 164, "endOffset": 185}, {"referenceID": 6, "context": "Then we can construct an estimation sequence via noisy power iteration [Hardt and Price, 2013].", "startOffset": 71, "endOffset": 94}, {"referenceID": 1, "context": ", 2015, Cai and Zhang, 2015, Kueng et al., 2014] as a special case, where the latter one has been studied widely in context such as inductive matrix completion [Jain and Dhillon, 2013] and phase retrieval [Candes et al., 2011]. Despite of the popularity of FMs in industry, there is rare theoretical study of learning guarantees for FMs. One of the main challenges in developing a provable FM algorithm is to handle its symmetric rank-one matrix sensing operator. For conventional matrix sensing problems where the matrix sensing operator is RIP, there are several alternating methods with provable guarantees [Hardt, 2013, Jain et al., 2013, Hardt and Wootters, 2014, Zhao et al., 2015a,b]. However, for a symmetric rank-one matrix sensing operator, the RIP condition doesn\u2019t hold trivially which turns out to be the main difficulty in designing efficient provable FM solvers. In rank-one matrix sensing, when the sensing operator is asymmetric, the problem is also known as inductive matrix completion which can be solved via alternating minimization with a global linear convergence rate [Jain and Dhillon, 2013, Zhong et al., 2015]. For symmetric rank-one matrix sensing operators, we are not aware of any efficient solver by the time of writing this paper. In a special case when the target matrix is of rank one, the problem is called \u201cphase retrieval\u201d whose convex solver is first proposed by Candes et al. [2011] then alternating methods are provided in [Lee et al.", "startOffset": 8, "endOffset": 1421}, {"referenceID": 0, "context": "Some authors also call gFM as second order polynomial network [Blondel et al., 2016].", "startOffset": 62, "endOffset": 84}, {"referenceID": 1, "context": "For example, the sensing operator Aasy(\u00b7) is unbiased ( EAasy(\u00b7) = 0) but the symmetric sensing operator is clearly not [Cai and Zhang, 2015].", "startOffset": 120, "endOffset": 141}, {"referenceID": 2, "context": "In conventional matrix sensing, this construction is possible when the sensing matrix satisfies the Restricted Isometric Property (RIP) [Cand\u00e8s and Recht, 2009]:", "startOffset": 136, "endOffset": 160}, {"referenceID": 1, "context": "However, in gFM and symmetric rank-one matrix sensing, the l2-norm RIP condition cannot be satisfied with high probability [Cai and Zhang, 2015].", "startOffset": 123, "endOffset": 144}, {"referenceID": 9, "context": "To bound \u03b10 , we need the following lemma which directly follows Wely\u2019s and Wedin\u2019s theorems [Stewart and Sun, 1990].", "startOffset": 93, "endOffset": 116}], "year": 2016, "abstractText": "We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from d dimensional random Gaussian vectors and the target second order coefficient matrix in gFM is of rank k, our algorithm converges linearly, achieves O(\u01eb) recovery error after retrieving O(kd log(1/\u01eb)) training instances, consumes O(kd) memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval.", "creator": "LaTeX with hyperref package"}}}