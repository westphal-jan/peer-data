{"id": "1608.07076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2016", "title": "A Context-aware Natural Language Generator for Dialogue Systems", "abstract": "ipa present a uniquely natural pattern generation concept for spoken interaction algorithms capable regarding assimilation ( mapping ) to users'way of speaking, providing contextually intelligent responses. their generator is based on recurrent neural networks and any sequence - word - repetition step. each is allowing trainable human data which include preceding context along past responses to be generated. others note that the context - aware grammar yields significant improvements over expected baseline in both automatic monitoring nor a full generalized language test.", "histories": [["v1", "Thu, 25 Aug 2016 10:43:56 GMT  (125kb,D)", "http://arxiv.org/abs/1608.07076v1", "Accepted as a short paper for SIGDIAL 2016"]], "COMMENTS": "Accepted as a short paper for SIGDIAL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ond\\v{r}ej du\\v{s}ek", "filip jur\\v{c}\\'i\\v{c}ek"], "accepted": false, "id": "1608.07076"}, "pdf": {"name": "1608.07076.pdf", "metadata": {"source": "CRF", "title": "A Context-aware Natural Language Generator for Dialogue Systems", "authors": ["Ond\u0159ej Du\u0161ek"], "emails": ["odusek@ufal.mff.cuni.cz", "jurcicek@ufal.mff.cuni.cz"], "sections": [{"heading": "1 Introduction", "text": "In a conversation, speakers are influenced by previous utterances of their counterparts and tend to adapt (align, entrain) their way of speaking to each other, reusing lexical items as well as syntactic structure (Reitter et al., 2006). Entrainment occurs naturally and subconsciously, facilitates successful conversations (Friedberg et al., 2012; Nenkova et al., 2008), and forms a natural source of variation in dialogues. In spoken dialogue systems (SDS), users were reported to entrain to system prompts (Parent and Eskenazi, 2010).\nThe function of natural language generation (NLG) components in task-oriented SDS typically is to produce a natural language sentence from a dialogue act (DA) (Young et al., 2010) representing an action, such as inform or request, along with one or more attributes (slots) and their values (see Fig. 1). NLG is an important component of SDS which has a great impact on the perceived naturalness of the system; its quality can also influence the overall task success (Stoyanchev and Stent, 2009; Lopes et al., 2013). However, typical\nNLG systems in SDS only take the input DA into account and have no way of adapting to the user\u2019s way of speaking. To avoid repetition and add variation into the outputs, they typically alternate between a handful of preset variants (Jurc\u030c\u0131\u0301c\u030cek et al., 2014) or use overgeneration and random sampling from a k-best list of outputs (Wen et al., 2015b). There have been several attempts at introducing entrainment into NLG in SDS, but they are limited to rule-based systems (see Section 4).\nWe present a novel, fully trainable contextaware NLG system for SDS that is able to entrain to the user and provides naturally variable outputs because generation is conditioned not only on the input DA, but also on the preceding user utterance (see Fig. 1). Our system is an extension of Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek (2016b)\u2019s generator based on sequence-to-sequence (seq2seq) models with attention (Bahdanau et al., 2015). It is, to our knowledge, the first fully trainable entrainment-enabled NLG system for SDS. We also present our first results on the dataset of Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek (2016a), which includes the preceding user utterance along with each data instance (i.e., pair of input meaning representation and output sentence), and we show that our context-aware system outperforms the baseline in both automatic metrics and a human pairwise preference test. ar X iv :1\n60 8.\n07 07\n6v 1\n[ cs\n.C L\n] 2\n5 A\nug 2\n01 6\nIn the following, we first present the architecture of our generator (see Section 2), then give an account of our experiments in Section 3. We include a brief survey of related work in Section 4. Section 5 contains concluding remarks and plans for future work."}, {"heading": "2 Our generator", "text": "Our seq2seq generator is an improved version of Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek (2016b)\u2019s generator, which itself is based on the seq2seq model with attention (Bahdanau et al., 2015, see Fig. 2) as implemented in the TensorFlow framework (Abadi et al., 2015).1 We first describe the base model in Section 2.1, then list our context-aware improvements in Section 2.2."}, {"heading": "2.1 Baseline Seq2seq NLG with Attention", "text": "The generation has two stages: The first, encoder stage uses a recurrent neural network (RNN) composed of long-short-term memory (LSTM) cells (Hochreiter and Schmidhuber, 1997; Graves, 2013) to encode a sequence of input tokens2 x = {x1, . . . , xn} into a sequence of hidden states h = {h1, . . . , hn}:\nht = lstm(xt, ht\u22121) (1)\nThe second, decoder stage then uses the hidden states h to generate the output sequence y = {y1, . . . , ym}. Its main component is a second LSTM-based RNN, which works over its own internal state st and the previous output token yt\u22121:\nst = lstm((yt\u22121 \u25e6 ct)WS , st\u22121) (2)\nIt is initialized by the last hidden encoder state (s0 = hn) and a special starting symbol. The generated output token yt is selected from a softmax distribution:\np(yt|yt\u22121 . . . ,x) = softmax((st \u25e6 ct)WY ) (3)\nIn (2) and (3), ct represents the attention model \u2013 a sum over all encoder hidden states, weighted by a feed-forward network with one tanh hidden layer; WS and WY are linear projection matrices and \u201c\u25e6\u201d denotes concatenation.\nDAs are represented as sequences on the encoder input: a triple of the structure \u201cDA type, slot,\n1See (Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek, 2016b) and (Bahdanau et al., 2015) for a more formal description of the base model.\n2Embeddings are used (Bengio et al., 2003), i.e., xt and yt are vector representations of the input and output tokens.\nvalue\u201d is created for each slot in the DA and the triples are concatenated (see Fig. 2). 3 The generator supports greedy decoding as well as beam search which keeps track of top k most probable output sequences at each time step (Sutskever et al., 2014; Bahdanau et al., 2015).\nThe generator further features a simple content classification reranker to penalize irrelevant or missing information on the output. It uses an LSTM-based RNN to encode the generator outputs token-by-token into a fixed-size vector. This is then fed to a sigmoid classification layer that outputs a 1-hot vector indicating the presence of all possible DA types, slots, and values. The vectors for all k-best generator outputs are then compared to the input DA and the number of missing and irrelevant elements is used to rerank them."}, {"heading": "2.2 Making the Generator Context-aware", "text": "We implemented three different modifications to our generator that make its output dependent on the preceding context: 4\nPrepending context. The preceding user utterance is simply prepended to the DA and fed into the encoder (see Fig. 2). The dictionary for context utterances is distinct from the DA tokens dictionary.\nContext encoder. We add another, separate encoder for the context utterances. The hidden states of both encoders are concatenated, and the decoder then works with double-sized vectors both on the input and in the attention model (see Fig. 2).\nn-gram match reranker. We added a second reranker for the k-best outputs of the generator that promotes outputs that have a word or phrase overlap with the context utterance. We use geometric mean of modified n-gram precisions (with n \u2208 {1, 2}) as a measure of context overlap, i.e., BLEU-2 (Papineni et al., 2002) without brevity penalty. The log probability l of an output sequence on the generator k-best list is updated as follows:\nl = l + w \u00b7 \u221ap1p2 (4)\n3While the sequence encoding may not necessarily be the best way to obtain a vector representation of DA, it was shown to work well (Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek, 2016b).\n4For simplicity, we kept close to the basic seq2seq architecture of the generator; other possibilities for encoding the context, such as convolution and/or max-pooling, are possible.\nIn (4), p1 and p2 are modified unigram and bigram precisions of the output sequence against the context, and w is a preset weight. We believe that any reasonable measure of contextual match would be viable here, and we opted for modified n-gram precisions because of simple computation, welldefined range, and the relation to the de facto standard BLEU metric.5 We only use unigrams and bigrams to promote especially the reuse of single words or short phrases.\nIn addition, we combine the n-gram match reranker with both of the two former approaches.\nWe used gold-standard transcriptions of the immediately preceding user utterance in our experiments in order to test the context-aware capabilities of our system in a stand-alone setting; in a live SDS, 1-best speech recognition hypotheses and longer user utterance history can be used with no modifications to the architecture."}, {"heading": "3 Experiments", "text": "We experiment on the publicly available dataset of Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek (2016a)6 for NLG in the pub-\n5We do not use brevity penalty as we do not want to demote shorter output sequences. However, adding it to the formula in our preliminary experiments yielded similar results to the ones presented here.\n6The dataset is released at http://hdl.handle. net/11234/1-1675; we used a more recent version from GitHub (https://github.com/UFAL-DSG/alex\nlic transport information domain, which includes preceding context along with each pair of input DA and target natural language sentence. It contains over 5,500 utterances, i.e., three paraphrases for each of the over 1,800 combinations of input DA and context user utterance. The data concern bus and subway connections on Manhattan, and comprise four DA types (iconfirm, inform, inform no match, request). They are delexicalized for generation to avoid sparsity, i.e., stop names, vehicles, times, etc., are replaced by placeholders (Wen et al., 2015a). We applied a 3:1:1 split of the set into training, development, and test data. We use the three paraphrases as separate instances in training data, but they serve as three references for a single generated output in validation and evaluation.\nWe test the three context-aware setups described in Section 2.2 and their combinations, and we compare them against the baseline noncontext-aware seq2seq generator. Same as Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek (2016b), we train the seq2seq models by minimizing cross-entropy on the training set using the Adam optimizer (Kingma and Ba, 2015), and we measure BLEU on the development set after each pass over the training data, selecting the best-performing parameters.7 The content classification reranker is trained in a similar fashion, measuring misclassification on both training and development set after each pass.8 We use 5 different random initializations of the networks and\ncontext nlg dataset), which contains several small fixes. 7Based on our preliminary experiments on development data, we use embedding size 50, LSTM cell size 128, learning rate 0.0005, and batch size 20. Training is run for at least 50 and up to 1000 passes, with early stopping if the top 10 validation BLEU scores do not change for 100 passes.\n8We use the same settings except for the number of passes over the training data, which is at least 20 and 100 at most. For validation, development set is given 10 times more importance than the training set.\naverage the results. Decoding is run with a beam size of 20 and the penalty weight for content classification reranker set to 100. We set the n-gram match reranker weight based on experiments on development data.9"}, {"heading": "3.1 Evaluation Using Automatic Metrics", "text": "Table 1 lists our results on the test data in terms of the BLEU and NIST metrics (Papineni et al., 2002; Doddington, 2002). We can see that while the n-gram match reranker brings a BLEU score improvement, using context prepending or separate encoder results in scores lower than the baseline.10 However, using the n-gram match reranker together with context prepending or separate encoder brings significant improvements of about 2.8 BLEU points in both cases, better than using the n-gram match reranker alone.11 We believe that adding the context information into the decoder does increase the chances of contextually appropriate outputs appearing on the decoder kbest lists, but it also introduces a lot more uncertainty and therefore, the appropriate outputs may not end on top of the list based on decoder scores alone. The n-gram match reranker is then able to promote the relevant outputs to the top of the k-best list. However, if the generator itself does not have access to context information, the n-gram match reranker has a smaller effect as contextually appropriate outputs may not appear on the k-best lists at all. A closer look at the generated outputs confirms that entrainment is present in sentences generated by the context-aware setups (see Fig. 2).\nIn addition to BLEU and NIST scores, we measured the slot error rate ERR (Wen et al., 2015b), i.e., the proportion of missing or superfluous slot placeholders in the delexicalized generated outputs. For all our setups, ERR stayed around 3%."}, {"heading": "3.2 Human Evaluation", "text": "We evaluated the best-performing setting based on BLEU/NIST scores, i.e., prepending context with n-gram match reranker, in a blind pairwise preference test with untrained judges recruited on\n9w is set to 5 when the n-gram match reranker is run by itself or combined with the separate encoder, 10 if combined with prepending context.\n10In our experiments on development data, all three methods brought a mild BLEU improvement.\n11Statistical significance at 99% level has been assessed using pairwise bootstrap resampling (Koehn, 2004).\nthe CrowdFlower crowdsourcing platform.12 The judges were given the context and the system output for the baseline and the context-aware system, and they were asked to pick the variant that sounds more natural. We used a random sample of 1,000 pairs of different system outputs over all 5 random initializations of the networks, and collected 3 judgments for each of them. The judges preferred the context-aware system output in 52.5% cases, significantly more than the baseline.13\nWe examined the judgments in more detail and found three probable causes for the rather small difference between the setups. First, both setups\u2019 outputs fit the context relatively well in many cases and the judges tend to prefer the overall more frequent variant (e.g., for the context \u201cstarting from Park Place\u201d, the output \u201cWhere do you want to go?\u201d is preferred over \u201cWhere are you going to?\u201d). Second, the context-aware setup often selects a shorter response that fits the context well (e.g., \u201cIs there an option at 10:00 am?\u201d is confirmed simply with \u201cAt 10:00 am.\u201d), but the judges seem to prefer the more eloquent variant. And third, both setups occasionally produce non-fluent outputs, which introduces a certain amount of noise."}, {"heading": "4 Related Work", "text": "Our system is an evolutionary improvement over the LSTM seq2seq system of Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek (2016b) and as such, it is most related in terms of architecture to other recent RNN-based approaches to NLG, which are not context-aware: RNN generation with a convolutional reranker by Wen et al. (2015a) and an improved LSTM-based version (Wen et al., 2015b), as well as the LSTM encoder-aligner-decoder NLG system of Mei et al. (2015). The recent end-to-end trainable SDS of Wen et al. (2016) does have an implicit access to previous context, but the authors do not focus on its influence on the generated responses.\nThere have been several attempts at modelling entrainment in dialogue (Brockmann et al., 2005; Reitter et al., 2006; Buschmeier et al., 2010) and even successful implementations of entrainment models in NLG systems for SDS, where entrainment caused an increase in perceived naturalness of the system responses (Hu et al., 2014) or increased naturalness and task success (Lopes et al.,\n12http://crowdflower.com 13The result is statistically significant at 99% level accord-\ning to the pairwise bootstrap resampling test.\n2013; Lopes et al., 2015). However, all of the previous approaches are completely or partially rulebased. Most of them attempt to model entrainment explicitly, focus on specific entrainment phenomena only, and/or require manually selected lists of variant expressions, while our system learns synonyms and entrainment rules implicitly from the corpus. A direct comparison with previous entrainment-capable NLG systems for SDS is not possible in our stand-alone setting since their rules involve the history of the whole dialogue whereas we focus on the preceding utterance in our experiments."}, {"heading": "5 Conclusions and Further Work", "text": "We presented an improvement to our natural language generator based on the sequence-tosequence approach (Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek, 2016b), allowing it to exploit preceding context user utterances to adapt (entrain) to the user\u2019s way of speaking and provide more contextually accurate and less repetitive responses. We used two different ways of feeding previous context into the generator and a reranker based on n-gram match against the context. Evaluation on our context-aware dataset (Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek, 2016a) showed a significant BLEU score improvement for the combination of the two approaches, which was confirmed in a subsequent human pairwise preference test. Our generator is available on GitHub at the following URL:\nhttps://github.com/UFAL-DSG/tgen\nIn future work, we plan on improving the ngram matching metric to allow fuzzy matching (e.g., capturing different forms of the same word), experimenting with more ways of incorporating context into the generator, controlling the output\neloquence and fluency, and most importantly, evaluating our generator in a live dialogue system. We also intend to evaluate the generator with automatic speech recognition hypotheses as context and modify it to allow n-best hypotheses as contexts. Using our system in a live SDS will also allow a comparison against previous handcrafted entrainment-capable NLG systems."}, {"heading": "Acknowledgments", "text": "This work was funded by the Ministry of Education, Youth and Sports of the Czech Republic under the grant agreement LK11221 and core research funding, SVV project 260 333, and GAUK grant 2058214 of Charles University in Prague. It used language resources stored and distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (project LM2015071). The authors would like to thank Ondr\u030cej Pla\u0301tek and Miroslav Vodola\u0301n for helpful comments."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "International Conference on Learning Representations. arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin."], "venue": "Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Modelling alignment for affective dialogue", "author": ["C. Brockmann", "A. Isard", "J. Oberlander", "M. White."], "venue": "Workshop on Adapting the Interaction Style to Affective Factors at the 10th International Conference on User Modeling.", "citeRegEx": "Brockmann et al\\.,? 2005", "shortCiteRegEx": "Brockmann et al\\.", "year": 2005}, {"title": "Modelling and evaluation of lexical and syntactic alignment with a priming-based microplanner", "author": ["H. Buschmeier", "K. Bergmann", "S. Kopp."], "venue": "Empirical Methods in Natural Language Generation, number 5790 in Lecture Notes in Computer", "citeRegEx": "Buschmeier et al\\.,? 2010", "shortCiteRegEx": "Buschmeier et al\\.", "year": 2010}, {"title": "Automatic evaluation of machine translation quality using N-gram cooccurrence statistics", "author": ["G. Doddington."], "venue": "Proceedings of the Second International Conference on Human Language Technology Research, pages 138\u2013145.", "citeRegEx": "Doddington.,? 2002", "shortCiteRegEx": "Doddington.", "year": 2002}, {"title": "Jur\u010d\u0131\u0301\u010dek. 2016a. A context-aware natural language generation dataset for dialogue systems", "author": ["F.O. Du\u0161ek"], "venue": "In Workshop on Collecting and Generating Resources for Chatbots and Conversational Agents - Development and Evaluation,", "citeRegEx": "Du\u0161ek,? \\Q2016\\E", "shortCiteRegEx": "Du\u0161ek", "year": 2016}, {"title": "Jur\u010d\u0131\u0301\u010dek. 2016b. Sequence-tosequence generation for spoken dialogue via deep syntax trees and strings. arXiv:1606.05491", "author": ["F.O. Du\u0161ek"], "venue": "To appear in Proceedings of ACL", "citeRegEx": "Du\u0161ek,? \\Q2016\\E", "shortCiteRegEx": "Du\u0161ek", "year": 2016}, {"title": "Lexical entrainment and success in student engineering groups", "author": ["H. Friedberg", "D. Litman", "S.B.F. Paletz."], "venue": "Proc. of SLT, pages 404\u2013409.", "citeRegEx": "Friedberg et al\\.,? 2012", "shortCiteRegEx": "Friedberg et al\\.", "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves."], "venue": "arXiv:1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long shortterm memory", "author": ["S. Hochreiter", "J. Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Entrainment in pedestrian direction giving: How many kinds of entrainment", "author": ["Z. Hu", "G. Halberg", "C. Jimenez", "M. Walker."], "venue": "Proc. of IWSDS, pages 90\u2013101.", "citeRegEx": "Hu et al\\.,? 2014", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Alex: A statistical dialogue systems framework", "author": ["F. Jur\u010d\u0131\u0301\u010dek", "O. Du\u0161ek", "O. Pl\u00e1tek", "L. \u017dilka"], "venue": "In Proc. of Text, Speech and Dialogue,", "citeRegEx": "Jur\u010d\u0131\u0301\u010dek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jur\u010d\u0131\u0301\u010dek et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba."], "venue": "International Conference on Learning Representations. arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["P. Koehn."], "venue": "Proceedings of EMNLP, pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Automated two-way entrainment to improve spoken dialog system performance", "author": ["J. Lopes", "M. Eskenazi", "I. Trancoso."], "venue": "Proc. of ICASSP, pages 8372\u20138376.", "citeRegEx": "Lopes et al\\.,? 2013", "shortCiteRegEx": "Lopes et al\\.", "year": 2013}, {"title": "From rule-based to data-driven lexical entrainment models in spoken dialog systems", "author": ["J. Lopes", "M. Eskenazi", "I. Trancoso."], "venue": "Computer Speech & Language, 31(1):87\u2013112.", "citeRegEx": "Lopes et al\\.,? 2015", "shortCiteRegEx": "Lopes et al\\.", "year": 2015}, {"title": "What to talk about and how? selective generation using LSTMs with coarse-to-fine alignment", "author": ["H. Mei", "M. Bansal", "M.R. Walter."], "venue": "arXiv:1509.00838.", "citeRegEx": "Mei et al\\.,? 2015", "shortCiteRegEx": "Mei et al\\.", "year": 2015}, {"title": "High frequency word entrainment in spoken dialogue", "author": ["A. Nenkova", "A. Gravano", "J. Hirschberg."], "venue": "Proc. of ACL-HLT, pages 169\u2013172.", "citeRegEx": "Nenkova et al\\.,? 2008", "shortCiteRegEx": "Nenkova et al\\.", "year": 2008}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu."], "venue": "Proc. of ACL, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Lexical entrainment of real users in the Let\u2019s Go spoken dialog system", "author": ["G. Parent", "M. Eskenazi."], "venue": "Proc. of Interspeech, pages 3018\u20133021.", "citeRegEx": "Parent and Eskenazi.,? 2010", "shortCiteRegEx": "Parent and Eskenazi.", "year": 2010}, {"title": "Computational modelling of structural priming in dialogue", "author": ["D. Reitter", "F. Keller", "J.D. Moore."], "venue": "Proc. of NAACL-HLT: Short Papers, pages 121\u2013 124.", "citeRegEx": "Reitter et al\\.,? 2006", "shortCiteRegEx": "Reitter et al\\.", "year": 2006}, {"title": "Lexical and syntactic priming and their impact in deployed spoken dialog systems", "author": ["S. Stoyanchev", "A. Stent."], "venue": "Proc. of NAACL-HLT, pages 189\u2013 192.", "citeRegEx": "Stoyanchev and Stent.,? 2009", "shortCiteRegEx": "Stoyanchev and Stent.", "year": 2009}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q. VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112. arXiv:1409.3215.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking", "author": ["T.-H. Wen", "M. Gasic", "D. Kim", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young."], "venue": "Proc. of SIGDIAL, pages 275\u2013284.", "citeRegEx": "Wen et al\\.,? 2015a", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["T.-H. Wen", "M. Gasic", "N. Mrk\u0161i\u0107", "P.-H. Su", "D. Vandyke", "S. Young."], "venue": "Proc. of EMNLP, pages 1711\u20131721.", "citeRegEx": "Wen et al\\.,? 2015b", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "A network-based endto-end trainable task-oriented dialogue system", "author": ["T.-H. Wen", "M. Ga\u0161i\u0107", "N. Mrk\u0161i\u0107", "L.M. RojasBarahona", "P.-H. Su", "S. Ultes", "D. Vandyke", "S. Young."], "venue": "arXiv:1604.04562.", "citeRegEx": "Wen et al\\.,? 2016", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "The hidden information state model: A practical framework for POMDP-based spoken dialogue management", "author": ["S. Young", "M. Ga\u0161i\u0107", "S. Keizer", "F. Mairesse", "J. Schatzmann", "B. Thomson", "K. Yu."], "venue": "Computer Speech & Language, 24(2):150\u2013", "citeRegEx": "Young et al\\.,? 2010", "shortCiteRegEx": "Young et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 20, "context": "In a conversation, speakers are influenced by previous utterances of their counterparts and tend to adapt (align, entrain) their way of speaking to each other, reusing lexical items as well as syntactic structure (Reitter et al., 2006).", "startOffset": 213, "endOffset": 235}, {"referenceID": 7, "context": "Entrainment occurs naturally and subconsciously, facilitates successful conversations (Friedberg et al., 2012; Nenkova et al., 2008), and forms a natural source of variation in dialogues.", "startOffset": 86, "endOffset": 132}, {"referenceID": 17, "context": "Entrainment occurs naturally and subconsciously, facilitates successful conversations (Friedberg et al., 2012; Nenkova et al., 2008), and forms a natural source of variation in dialogues.", "startOffset": 86, "endOffset": 132}, {"referenceID": 19, "context": "In spoken dialogue systems (SDS), users were reported to entrain to system prompts (Parent and Eskenazi, 2010).", "startOffset": 83, "endOffset": 110}, {"referenceID": 26, "context": "The function of natural language generation (NLG) components in task-oriented SDS typically is to produce a natural language sentence from a dialogue act (DA) (Young et al., 2010) representing an action, such as inform or request, along with one or more attributes (slots) and their values (see Fig.", "startOffset": 159, "endOffset": 179}, {"referenceID": 21, "context": "NLG is an important component of SDS which has a great impact on the perceived naturalness of the system; its quality can also influence the overall task success (Stoyanchev and Stent, 2009; Lopes et al., 2013).", "startOffset": 162, "endOffset": 210}, {"referenceID": 14, "context": "NLG is an important component of SDS which has a great impact on the perceived naturalness of the system; its quality can also influence the overall task success (Stoyanchev and Stent, 2009; Lopes et al., 2013).", "startOffset": 162, "endOffset": 210}, {"referenceID": 11, "context": "To avoid repetition and add variation into the outputs, they typically alternate between a handful of preset variants (Jur\u010d\u0131\u0301\u010dek et al., 2014) or use overgeneration and random sampling from a k-best list of outputs (Wen et al.", "startOffset": 118, "endOffset": 142}, {"referenceID": 24, "context": ", 2014) or use overgeneration and random sampling from a k-best list of outputs (Wen et al., 2015b).", "startOffset": 80, "endOffset": 99}, {"referenceID": 0, "context": "Our system is an extension of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b)\u2019s generator based on sequence-to-sequence (seq2seq) models with attention (Bahdanau et al., 2015).", "startOffset": 132, "endOffset": 155}, {"referenceID": 4, "context": "Our system is an extension of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b)\u2019s generator based on sequence-to-sequence (seq2seq) models with attention (Bahdanau et al.", "startOffset": 30, "endOffset": 58}, {"referenceID": 0, "context": "Our system is an extension of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b)\u2019s generator based on sequence-to-sequence (seq2seq) models with attention (Bahdanau et al., 2015). It is, to our knowledge, the first fully trainable entrainment-enabled NLG system for SDS. We also present our first results on the dataset of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016a), which includes the preceding user utterance along with each data instance (i.", "startOffset": 133, "endOffset": 328}, {"referenceID": 4, "context": "Our seq2seq generator is an improved version of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b)\u2019s generator, which itself is based on the seq2seq model with attention (Bahdanau et al.", "startOffset": 48, "endOffset": 76}, {"referenceID": 9, "context": "The generation has two stages: The first, encoder stage uses a recurrent neural network (RNN) composed of long-short-term memory (LSTM) cells (Hochreiter and Schmidhuber, 1997; Graves, 2013) to encode a sequence of input tokens2 x = {x1, .", "startOffset": 142, "endOffset": 190}, {"referenceID": 8, "context": "The generation has two stages: The first, encoder stage uses a recurrent neural network (RNN) composed of long-short-term memory (LSTM) cells (Hochreiter and Schmidhuber, 1997; Graves, 2013) to encode a sequence of input tokens2 x = {x1, .", "startOffset": 142, "endOffset": 190}, {"referenceID": 0, "context": "See (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2016b) and (Bahdanau et al., 2015) for a more formal description of the base model.", "startOffset": 37, "endOffset": 60}, {"referenceID": 1, "context": "Embeddings are used (Bengio et al., 2003), i.", "startOffset": 20, "endOffset": 41}, {"referenceID": 22, "context": "3 The generator supports greedy decoding as well as beam search which keeps track of top k most probable output sequences at each time step (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 140, "endOffset": 187}, {"referenceID": 0, "context": "3 The generator supports greedy decoding as well as beam search which keeps track of top k most probable output sequences at each time step (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 140, "endOffset": 187}, {"referenceID": 18, "context": ", BLEU-2 (Papineni et al., 2002) without brevity penalty.", "startOffset": 9, "endOffset": 32}, {"referenceID": 5, "context": "We experiment on the publicly available dataset of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016a)6 for NLG in the pub-", "startOffset": 51, "endOffset": 79}, {"referenceID": 23, "context": ", are replaced by placeholders (Wen et al., 2015a).", "startOffset": 31, "endOffset": 50}, {"referenceID": 12, "context": "Same as Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b), we train the seq2seq models by minimizing cross-entropy on the training set using the Adam optimizer (Kingma and Ba, 2015), and we measure BLEU on the development set after each pass over the training data, selecting the best-performing parameters.", "startOffset": 138, "endOffset": 159}, {"referenceID": 5, "context": "Same as Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b), we train the seq2seq models by minimizing cross-entropy on the training set using the Adam optimizer (Kingma and Ba, 2015), and we measure BLEU on the development set after each pass over the training data, selecting the best-performing parameters.", "startOffset": 8, "endOffset": 36}, {"referenceID": 18, "context": "Table 1 lists our results on the test data in terms of the BLEU and NIST metrics (Papineni et al., 2002; Doddington, 2002).", "startOffset": 81, "endOffset": 122}, {"referenceID": 4, "context": "Table 1 lists our results on the test data in terms of the BLEU and NIST metrics (Papineni et al., 2002; Doddington, 2002).", "startOffset": 81, "endOffset": 122}, {"referenceID": 24, "context": "In addition to BLEU and NIST scores, we measured the slot error rate ERR (Wen et al., 2015b), i.", "startOffset": 73, "endOffset": 92}, {"referenceID": 13, "context": "Statistical significance at 99% level has been assessed using pairwise bootstrap resampling (Koehn, 2004).", "startOffset": 92, "endOffset": 105}, {"referenceID": 24, "context": "(2015a) and an improved LSTM-based version (Wen et al., 2015b), as well as the LSTM encoder-aligner-decoder NLG system of Mei et al.", "startOffset": 43, "endOffset": 62}, {"referenceID": 5, "context": "Our system is an evolutionary improvement over the LSTM seq2seq system of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b) and as such, it is most related in terms of architecture to other recent RNN-based approaches to NLG, which are not context-aware: RNN generation with a convolutional reranker by Wen et al.", "startOffset": 74, "endOffset": 102}, {"referenceID": 5, "context": "Our system is an evolutionary improvement over the LSTM seq2seq system of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b) and as such, it is most related in terms of architecture to other recent RNN-based approaches to NLG, which are not context-aware: RNN generation with a convolutional reranker by Wen et al. (2015a) and an improved LSTM-based version (Wen et al.", "startOffset": 74, "endOffset": 300}, {"referenceID": 5, "context": "Our system is an evolutionary improvement over the LSTM seq2seq system of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b) and as such, it is most related in terms of architecture to other recent RNN-based approaches to NLG, which are not context-aware: RNN generation with a convolutional reranker by Wen et al. (2015a) and an improved LSTM-based version (Wen et al., 2015b), as well as the LSTM encoder-aligner-decoder NLG system of Mei et al. (2015). The recent end-to-end trainable SDS of Wen et al.", "startOffset": 74, "endOffset": 432}, {"referenceID": 5, "context": "Our system is an evolutionary improvement over the LSTM seq2seq system of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b) and as such, it is most related in terms of architecture to other recent RNN-based approaches to NLG, which are not context-aware: RNN generation with a convolutional reranker by Wen et al. (2015a) and an improved LSTM-based version (Wen et al., 2015b), as well as the LSTM encoder-aligner-decoder NLG system of Mei et al. (2015). The recent end-to-end trainable SDS of Wen et al. (2016) does have an implicit access to previous context, but the authors do not focus on its influence on the generated responses.", "startOffset": 74, "endOffset": 490}, {"referenceID": 2, "context": "There have been several attempts at modelling entrainment in dialogue (Brockmann et al., 2005; Reitter et al., 2006; Buschmeier et al., 2010) and even successful implementations of entrainment models in NLG systems for SDS, where entrainment caused an increase in perceived naturalness of the system responses (Hu et al.", "startOffset": 70, "endOffset": 141}, {"referenceID": 20, "context": "There have been several attempts at modelling entrainment in dialogue (Brockmann et al., 2005; Reitter et al., 2006; Buschmeier et al., 2010) and even successful implementations of entrainment models in NLG systems for SDS, where entrainment caused an increase in perceived naturalness of the system responses (Hu et al.", "startOffset": 70, "endOffset": 141}, {"referenceID": 3, "context": "There have been several attempts at modelling entrainment in dialogue (Brockmann et al., 2005; Reitter et al., 2006; Buschmeier et al., 2010) and even successful implementations of entrainment models in NLG systems for SDS, where entrainment caused an increase in perceived naturalness of the system responses (Hu et al.", "startOffset": 70, "endOffset": 141}, {"referenceID": 10, "context": ", 2010) and even successful implementations of entrainment models in NLG systems for SDS, where entrainment caused an increase in perceived naturalness of the system responses (Hu et al., 2014) or increased naturalness and task success (Lopes et al.", "startOffset": 176, "endOffset": 193}], "year": 2016, "abstractText": "We present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users\u2019 way of speaking, providing contextually appropriate responses. The generator is based on recurrent neural networks and the sequence-to-sequence approach. It is fully trainable from data which include preceding context along with responses to be generated. We show that the context-aware generator yields significant improvements over the baseline in both automatic metrics and a human pairwise preference test.", "creator": "LaTeX with hyperref package"}}}