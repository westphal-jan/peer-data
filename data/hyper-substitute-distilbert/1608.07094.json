{"id": "1608.07094", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2016", "title": "A Novel Term_Class Relevance Measure for Text Categorization", "abstract": "in this paper, we introduce a new measure called attribute _ bearer relevance together compute the relevancy of a unit in accepting a definition into determining particular class. the proposed measure estimates the degree of relevance of a given attribute, in placing an open semantic database be their member representing a known class, as a projection of term _ common capacity and common _ scope index ; where either class _ term weight is its ratio of the multitude of documents of the class limiting the term to the total proportion of documents containing additional term \u2014 then whole _ term density is the relative density considering events possessing the term to either class this find total occurrence of the term nearest the current population. examining the conventional existing term weighting tasks such as tf - gene platforms broadly similar, the proposed product only captures into account the degree of adaptive efficacy comprising the taxon across all documents denying it class to the entire internet. to demonstrate high comparative effectiveness the particular measure experimentation has been conducted on highly common newsgroups dataset. further, careful identification of both novel measure is brought possible through a comparative analysis.", "histories": [["v1", "Thu, 25 Aug 2016 11:46:06 GMT  (477kb)", "http://arxiv.org/abs/1608.07094v1", "12 pages, 6 figures, 2 tables"], ["v2", "Wed, 14 Sep 2016 12:51:50 GMT  (311kb)", "http://arxiv.org/abs/1608.07094v2", "12 pages, 6 figures, 2 tables"]], "COMMENTS": "12 pages, 6 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["d s guru", "mahamad suhil"], "accepted": false, "id": "1608.07094"}, "pdf": {"name": "1608.07094.pdf", "metadata": {"source": "CRF", "title": "A Novel Term_Class Relevance Measure for Text Categorization", "authors": ["D S Guru", "Mahamad Suhil"], "emails": [], "sections": [{"heading": null, "text": "the relevancy of a term in classifying a document into a particular class. The proposed measure estimates the degree of relevance of a given term, in placing an unlabeled document to be a member of a known class, as a product of Class_Term weight and Class_Term density; where the Class_Term weight is the ratio of the number of documents of the class containing the term to the total number of documents containing the term and the Class_Term density is the relative density of occurrence of the term in the class to the total occurrence of the term in the entire population. Unlike the other existing term weighting schemes such as TF-IDF and its variants, the proposed relevance measure takes into account the degree of relative participation of the term across all documents of the class to the entire population. To demonstrate the significance of the proposed measure experimentation has been conducted on the 20 Newsgroups dataset. Further, the superiority of the novel measure is brought out through a comparative analysis.\nKeywords: text categorization, term weight, term-document relevance, term_class relevance."}, {"heading": "1. Introduction", "text": "For the few decades automatic content based classification of documents from huge collections has become an active area of research due to the fact that electronic data over the internet has become unmanageably big and day by day it is increasing exponentially. Manual, tag based classification have lost their significance because of the huge size of the data that need to be processed and inability of the tags in describing the content of the documents. Varieties of applications of text classification which are of current demand such as spam filtering in E-mails, classification of E-Books, classification of news documents, classification of text data from social networks and so on have also made the researchers to explore various ways of analyzing and representing these data so that quick and efficient retrieval and management of this huge data can be done."}, {"heading": "1.1 A review of the available term weighting schemes", "text": "As our work focuses on proposal of a new term weighting scheme, but not on classification framework, here we consider the literature on only different term weighting schemes.\nTerms are the basic information units of any text document. So, all weighting schemes developed in the literature measure the weight of a term in representing the content of a document [1-5]. Based on whether the membership of the document in predefined categories is provided to\nmeasure the weight of a term or not, term weighting schemes are broadly classified into two classes namely, unsupervised term weighting schemes and supervised term weighting schemes. In the following subsections we provide a review of both the weighting scheme along with the techniques which have adopted them."}, {"heading": "1.1.1 Unsupervised term weighting schemes", "text": "Most of the unsupervised term weighting schemes are from the information retrieval field. These methods are very useful when the training documents are not labeled by their class labels. The traditional term weighting methods borrowed from IR, such as binary, term frequency (TF), TFIDF, and its various variants are unsupervised schemes [2]. The TF-IDF proposed by Jones [6, 7] and its variants are the most widely used term weighting schemes for text classification. Some of the variants of TF are Raw term frequency, log(TF), log(TF+1), or log(TF)+1[1-2]. If ni is the number of documents containing the term and N is the number of documents in the collection then, the variants of IDF are 1/ni, log(1/ni), log(N/ni), log(N/ni)+1 and log(N/ni-1)[1]. In [18], a novel inverse corpus frequency (ICF) based technique is proposed which computes the document representation in linear time."}, {"heading": "1.1.2 Supervised term weighting schemes", "text": "Supervised term weighting schemes were developed especially for text categorization because of the fact that a supervised knowledge on the class labels of the training samples is provided [1-4]. All the supervised term weighting schemes make use of this class information in different ways. Supervised term weighting schemes are further classified into subcategories, based on whether the weight estimates relevancy of a term in preserving document content or the relevancy of a term in placing a document as a member of a class. So, it will be more effective to call the weighting schemes which are used to measure the relevance of a term in preserving the document content as term-document relevance measures and those which can be used to measure the term relevance in categorizing a document as term_class relevance measures.\nTerm-Document Relevance measure\nThese measures are useful to select a discriminating subset of terms for representing a document by weighting the terms according to their relevance in preserving the content of the document. These are created by replacing the IDF component of the TF-IDF scheme. Most frequently used techniques to replace IDF include chi-square measure (X 2), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12]. From past few years, many researchers have proposed alternative term-document relevance schemes [1, 13-16]. All these are basically feature selection techniques used in term weighting schemes. In [14], a comparison of corpusbased and class-based keyword selection is proposed by using TF-IDF as weighting scheme. In [4], a class-indexing-based term weighting for automatic text classification is proposed. An\ninverse class space density frequency ( ICS F ) is used along with TF-IDF method that provides a positive discrimination on infrequent and frequent terms.\nTerm_class Relevance measures\nThese measures compute the ability of a term in classifying a document as a member of a class. To the best of our knowledge, only one work of this category has been proposed by Isa et al., [20] using Bayes posterior probability. Though, some works make use of Bayes probability for representation, they have not clearly stated the advantage of the measure in classification [11, 18]. After [20], this measure was extensively used for term weighting [21, 22]. The beauty of this measure lies in the fact that, instead of computing the weight of a term in preserving the content of a document, the relevancy of the term in categorizing the document as a member of a class can be measured directly. Which is computed as the Bayes posterior probability P(Cj/ ti) for a class Cj and term ti as given by,\n( / ) ( ) ( / )\n( )\ni j j\nj i\ni\nP t C P C P C t\nP t \nwhere,\n_ _ _ _ ( )\n_ _ _ _ _\nj\nj\nTotal of Words in C P C\nTotal of Words in Training Dataset  ,\n_ _ _ _ _ ( )\n_ _ _ _ _ _\ni\ni\noccurrence of t in all categories P t\noccurrence of all terms in all categories    , and\n_ _ _ _ ( / )\n_ _ _ _ _\ni j\ni j\nj\noccurence of t in C P t C\noccurrence of all terms in C  \nTo make use of the complete advantage of the proposed relevance measure, Isa et al., [20] also propose a text representation scheme which works with the reduced dimension for each document at the time of representation itself. This work happened to be the very first of its kind in the literature of text classification where, a document is represented only with number of dimensions equal to the number of classes in the corpus without any dimensionality reduction technique applied.\nIn this representation scheme, first, a matrix F of size m X k is created for every document\nwhere, m is the number of terms assumed to be available in the document and k is the number of classes. Then, every entry ( , )F i j of the matrix is filled by the relevancy of the corresponding\nterm ti in classifying the corresponding document as a member of class Cj . Then, a feature vector\nf of dimension k is created as a representative for the document where, f(j) is the average of relevancy of every term to a class Cj. It shall be carefully observed here that, a document with any number of terms is represented with a feature vector of dimension equal to the number of classes in the population which is very small in contrast to the feature vector that is created in any other vector space representation scheme where the dimension is equal to the total number of terms due to all documents of the population. Therefore, a great amount of dimensionality reduction is achieved at the time of representation itself without the application of any dimensionality reduction technique. However, the classification accuracy accomplished is not of that high. Motivated by this work, in this paper we propose a novel term_class relevance measure with the following objectives,\n Exploiting the complete advantage of text representation scheme proposed by Isa et\nal.,[20].\n Comparison of the effectiveness of the proposed term_class relevance measure with that\nof Bayes posterior probability based measure.\n Isa et al., [20] make use of SVM as the classifier. So we are also investigating the effect\nSVM on our proposed relevance measure and also compare it with other available classifiers.\nThe rest of the paper is organized as follows. The proposed Class_Term relevance measure is presented in the Section 2. In section 3, presents the results and discussion on the experimentation. A comparative analysis of the proposed relevance measure with other contemporary works is given in the Section 4. Finally, section 6 presents the conclusion and future enhancements."}, {"heading": "2. A New Term_Class Relevance Measure", "text": "In this section, we propose a novel measure called term_class relevance measure. Term_class relevancy is defined as the ability of a term \u2018ti\u2019 in classifying a document \u2018D\u2019 as a member of a class \u2018Cj\u2019. We begin with introducing two new concepts which decide the role of a term in a class, namely, Class_Term Weight and Class_Term Density.\nClass_Term Weight: It is the relative weight of the term with respect to a class of interest which is computed by counting only those documents of the class of interest that are containing the term of interest against that of the entire corpus. That is, the class_term weight of a term \u2018ti\u2019 in the class \u2018Cj\u2019 is computed as the ratio of ( , )i jClassFrequency t C to the ( )iCorpusFrequency t . It\nis given by the equation below.\n( , ) _ ( , )\n( )\ni j\ni j\ni\nClassFrequency t C Class TermWeight t C\nCorpusFrequency t \nwhere, ( , )i jClassFrequency t C is the number of documents of \u2018Cj\u2019 containing \u2018ti\u2019 at least once\nand ( )iCorpusFrequency t is the number of documents of the entire corpus containing \u2018ti\u2019 at least once.\nIf the class_term weight of a term ti with respect to the class Cj is very high then the probability that the document D which contains ti is most likely a member of the class Cj is also high. Therefore, the relevancy of a term which we call it as _ Re ( , )i jTerm Class levancy t C in\ndeciding the class of a document is directly proportional to the class_term weight of the term. i.e.,\n_ Re ( , ) _ ( , ) (1)i j i jTerm Class levancy t C Class TermWeight t C\nClass_Term Density: It is the relative density of a term of interest with respect to the class of interest. It is computed as the ratio of the number of occurrences of the term in the class of interest to that of the entire corpus. That is, the class_term density of a term \u2018ti\u2019 with respect to the class \u2018Cj\u2019 is computed as the ratio of frequency of ti in Cj to its frequency in the corpus. It is given by the equation below.\n1\n( , ) _ ( , )\n( , )\ni j\ni j k\ni j\nj\nTermFrequency t C Class TermDensity t C\nTermFrequency t C \n\n\nwhere, ( , )i jTermFrequency t C is the frequency of \u2018ti\u2019 in the class \u2018Cj\u2019 which is computed as the\nsum of the frequencies of \u2018ti\u2019 in every document of \u2018Cj\u2019 as shown by the equation below.\n1\n( , ) ( , ) jd\ni j i doc\ndoc TermFrequency t C Frequency t D  \nwhere, is the frequency of occurrence of the term \u2018ti\u2019 in document D and dj is the number of documents in the class Cj .\nIt shall be noticed that, if the class_term density of a term ti in a class Cj is very high then the probability that a document D which contains ti is most likely a member of the class Cj is also high. Therefore, the relevancy of a term in deciding the class of a document is directly proportional to the class_term density of the term. i.e.,\n_ Re ( , ) _ ( , ) (2)i j i jTerm Class levancy t C Class TermDensity t C\nBy combining (1) and (2), the term_class relevancy is directly proportional to the product of the class_term weight and class_term density of the term,\n_ Re ( , ) _ ( , )* _ ( , )i j i j i jTerm Class levancy t C Class TermWeight t C Class TermDensity t C\n( , )iFrequency t D\n_ Re ( , ) * _ ( , )* _ ( , )i j i j i jTerm Class levancy t C c Class TermWeight t C Class TermDensity t C\ni.e.,\nwhere, c is the proportionality constant, which we decide based on the class weight with respect to the entire population.\nClass Weight ( c ): It is the weight of the jth class Cj in the corpus which is computed as the ratio of the number of documents in Cj denoted by _ ( )jSize of C to the total number of documents in\nthe entire corpus as given by,\n1\n_ ( ) ( )\n_ ( )\nj\nj k\nj\nj\nSize of C ClassWeight C\nSize of C \n\n\nWhere, k is the number of classes.\nIf each class has equal number of documents, then the class-weight serves as a scaling factor in computing the relevance of a term and it increases or decreases the relevancy of a term to a class when the size of the class compared to the size of other classes is larger or smaller respectively.\nTherefore, the proposed relevancy measure of a term ti in placing a document D as a member of a class Cj is given by the product of the three aspects namely, Class weight, Class_Term weight and Class_Term Density as given by the formula below.\n_ Re ( , ) * _ ( , )* _ ( , )i j i j i jTerm Class levancy t C c Class TermWeight t C Class TermDensity t C\nThe main advantages of the proposed term_class relevancy measure are as follows,\n\u2022 It directly computes the relevancy of the term with respect to a class of interest; which can itself be used as a clue to identify the possible class to which a document may belong without the need of a classifier.\n\u2022 The measure uses class as well as corpus information together as opposed to the conventional TF-IDF scheme, which utilizes the document frequency from only the corpus.\n\u2022 It shall be observed that, the relevancy of a term to a class is high only if the three factors class_term weight, class_term density and class_weight are high. This helps in properly deciding the weight of a term without any bias towards a particular class, which in turn helps in deciding the class for a classifier.\nOnce the term_class relevance of all terms of the training set of documents is computed with respect to every class present, each training document is then represented using the representation scheme proposed by Isa et al., [20] as explained in section 1.1.2. A document is first represented as a matrix of size , where, m is the number of terms assumed to be available in the document and k is the number of classes. Then, every entry of the matrix is filled by the relevancy of the corresponding term ti with respect to the class Cj. Then, a feature vector f of\ndimension k is created as a representative for the document where, f(j) is the average relevancy of all terms with respect to a class Cj. The feature matrix of size thus created for the n training documents is used for learning process. A similar vector of k dimension is created for the test documents and given to the learning algorithm or a classifier for labeling. The process of training and testing the classifiers is explained in the next section."}, {"heading": "3. Classification with SVM and k-NN classifiers", "text": "To evaluate the applicability of the proposed term_class relevance measure, we make use SVM as learning algorithm to perform classification because of its good generalization ability. Moreover, the training burden for SVM is very less even though, the time required for training is directly proportional to the training dataset, because the representative feature vectors are of dimension equal to the number of classes only. So, to test the effectiveness of the proposed relevance measure we have experimented with the SVM classifier with Linear, Gaussian radial basis function (RBF) and Polynomial kernels.\nWe consider the 20 Newsgroups data set for our experimentation. It consists of approximately 20,000 newsgroup documents consisting 20 classes with each class bearing nearly equal number of samples. It has become a popular data set for text classification and clustering applications. Some of the documents are closely related to each other while others are highly unrelated. We conduct experiments with various proportion of training set to validate the performance of the proposed relevancy measure.\nFig 1 shows the overall classification accuracy of the system with various percentages of training samples using SVM classifier with different kernels. Fig 2 shows the precision of the SVM classifier with different kernels and Recall is shown in Fig 3. In Fig 4, the overall Fmeasure is presented. It can be observed from the figures (1-4) that, the SVM classifier with RBF kernel is working well when compared to the other kernels. The results are also presented graphically in figures below.\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n10 20 30 40 50 60 70 80\nLinear\nRBF\nPolynomial\npercentage of training\nA c c u r a c y\nFigure 4. F-measure of the SVM classifier with Linear, RBF and Polynomial kernels\nFurther, the k-NN classifier is also adapted to test the proposed method because of its simplicity in classification. We performed the experimentation with various values of k from 1 to 20 and the performance of the classifier was high for k=10. Table 2 shows the results of k-NN classifier for k=10 and a comparison with the best results of SVM is also given.\nTo compare the class-wise performance of each classifier we show the variation of F-measure vs. class in Figure 5 and 6. Figure 5, shows the values of F-measure vs. each class using k-NN classifier with k=10 and 10 percent of training. It can be noticed that, the performance is relatively low for classes 2, 3, 4, 7, 13 and 20. Further, the F-measure of SVM classifier vs. each class with RBF kernel and 10 % of training is shown in Figure 6. Though, the results of SVM are poor when compared to k-NN, SVM also has shown relatively low performance for the same classes as in the case of k-NN.\n0\n0.2\n0.4\n0.6\n0.8\n1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nFm\ne as\nu re\nClass Number\nFigure 5. Classification performance vs. class for k-NN classifier with 10 % training\nFigure 6. Classification performance vs. class for SVM classifier with RBF kernel and 10 % training"}, {"heading": "4. Comparative Analysis", "text": "In this section, we provide a quantitative comparative analysis of the proposed term_class relevance measure with the results of Isa et al.,[20] in Table 3. The results corresponding to [20] have been extracted directly from the paper as the representation scheme is same in both the works and they also have provided the results on the same 20Newsgroups dataset using only SVM classifier with different kernels. We can notice from the Table 3 that, the proposed term_class relevance measure outperforms the measure used by Isa et al.,[20]. Along with SVM, we compare the results using results of k-NN classifier with k=10. It can also be noticed from the Table 3 that, k-NN classifier with k=10 is showing enhanced results when compared to SVM with all the kernels for both the relevance measures. So, we recommend using k-NN as the classifier for better classification performance.\n0\n0.2\n0.4\n0.6\n0.8\n1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nFm\ne as\nu re\nClass Number\nConclusion\nIn this paper, a novel term_class relevance measure to compute the relevance of a term in classifying an unknown document as a member of a particular class is proposed. The proposed term_class relevance measure is a product of three aspects namely class_term weight, class_term density and class_weight. Experiments are conducted on 20 Newsgroups dataset using the SVM and k-NN classifiers. An effective text representation scheme which allows representation of text documents in reduced dimension is adapted to test the proposed term_class relevance measure. The comparative analysis of the results of the proposed work with the other contemporary research works shows the superiority of the proposed term_class relevance measure."}], "references": [{"title": "Term-Weighting Approaches in Automatic Text Retrieval, Information", "author": ["G. Salton", "C. Buckley"], "venue": "Processing and Management,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "Supervised Term Weighting for Automated Text Categorization", "author": ["F Debole", "F. Sebastiani"], "venue": "Proceedings of the 2003 ACM symposium on applied computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Class-indexing-based term weighting for automatic text classification", "author": ["F Ren", "G. Sohrab M"], "venue": "Information Sciences", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Representation and Classification of Text Documents: A Brief Review", "author": ["S. Harish B", "S. Guru D", "S. Manjunath"], "venue": "IJCA Special Issue on \u201cRecent Trends in Image Processing and Pattern Recognition\u201d", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["K.S. Jones"], "venue": "Journal of Documentation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1972}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["K.S. Jones"], "venue": "Journal of Documentation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Analytical evaluation of term weighting schemes for text categorization", "author": ["H Alt\u0131n\u00e7ay", "Z. Erenel"], "venue": "Pattern Recognition Letters", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Imbalanced text classification: A term weighting approach", "author": ["Y. Liu", "H.T. Loh", "A. Sun"], "venue": "Expert Systems with Applications", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Feature selection on hierarchy of web documents", "author": ["D. Mladenic", "M. Grobelnik"], "venue": "Decision Support Syst", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Machine learning in automated text categorization", "author": ["F. Sebastiani"], "venue": "ACM Comput. Surveys", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "A comparative study on feature selection in text categorization", "author": ["Y. Yang", "J.O. Pedersen"], "venue": "Proc. ICML\u201997, 14th Internat. Conf. on Machine Learning", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Toward integrating feature selection algorithms for classification and clustering", "author": ["H. Liu", "L. Yu"], "venue": "IEEE Trans. Knowledge Data Eng", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Text categorization with class-based and corpusbased keyword selection", "author": ["A. Ozgur", "L. Ozgur", "T. Gungor"], "venue": "Proc. 20th Internat. Symp. on Computer and Information Sciences. Lecture Notes in Computer Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Exploiting likely-positive and unlabeled data to improve the identification of protein\u2013protein interaction articles", "author": ["R.T. Tsai", "H. Hung", "H. Dai", "Y. Lin", "W. Hsu"], "venue": "BMC Bioinform", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Inverse-Category-Frequency Based Supervised Term Weighting Schemes for Text Categorization", "author": ["D Wang", "H. Zhang"], "venue": "Journal of Information Science and Engineering", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "TF-ICF: A New Term Weighting Scheme for Clustering Dynamic Data Streams", "author": ["J Reed", "Y. Jiao", "E. Potok T", "B Klump", "M Elmore", "A Hurson"], "venue": "5th International Conference on Machine Learning and Applications. pp.258-263", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "AIR/X \u2013 A rule-based multistage indexing system for large subject fields", "author": ["N. Fuhr", "S. Hartmann", "G. Lustig", "M. Schwantner", "K. Tzeras", "Darmstadt", "T. H"], "venue": "Proceedings of the proceedings of RIAO(pp", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1991}, {"title": "Mineau, \u201cBeyond tfidf Weighting for Text Categorization in the Vector Space Model,\u201dProc", "author": ["G.W.P. Soucy"], "venue": "Int\u2019l Joint Conf. Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Text document preprocessing with the Bayes formula for classification using the support vector machine", "author": ["D. Isa", "L.H. Lee", "V.P. Kallimani", "R. Raj Kumar"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Using the self-organizing map for clustering of text documents", "author": ["D. Isa", "V.P. Kallimani", "L.H. Lee"], "venue": "Expert Systems with Applications", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Symbolic representation of text documents", "author": ["S. Guru D", "S. Harish B", "S. Manjunath"], "venue": "In Proceedings of Third Annual ACM Bangalore Conference", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "So, all weighting schemes developed in the literature measure the weight of a term in representing the content of a document [1-5].", "startOffset": 125, "endOffset": 130}, {"referenceID": 1, "context": "So, all weighting schemes developed in the literature measure the weight of a term in representing the content of a document [1-5].", "startOffset": 125, "endOffset": 130}, {"referenceID": 2, "context": "So, all weighting schemes developed in the literature measure the weight of a term in representing the content of a document [1-5].", "startOffset": 125, "endOffset": 130}, {"referenceID": 3, "context": "So, all weighting schemes developed in the literature measure the weight of a term in representing the content of a document [1-5].", "startOffset": 125, "endOffset": 130}, {"referenceID": 0, "context": "The traditional term weighting methods borrowed from IR, such as binary, term frequency (TF), TFIDF, and its various variants are unsupervised schemes [2].", "startOffset": 151, "endOffset": 154}, {"referenceID": 4, "context": "The TF-IDF proposed by Jones [6, 7] and its variants are the most widely used term weighting schemes for text classification.", "startOffset": 29, "endOffset": 35}, {"referenceID": 5, "context": "The TF-IDF proposed by Jones [6, 7] and its variants are the most widely used term weighting schemes for text classification.", "startOffset": 29, "endOffset": 35}, {"referenceID": 0, "context": "Some of the variants of TF are Raw term frequency, log(TF), log(TF+1), or log(TF)+1[1-2].", "startOffset": 83, "endOffset": 88}, {"referenceID": 16, "context": "In [18], a novel inverse corpus frequency (ICF) based technique is proposed which computes the document representation in linear time.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Supervised term weighting schemes were developed especially for text categorization because of the fact that a supervised knowledge on the class labels of the training samples is provided [1-4].", "startOffset": 188, "endOffset": 193}, {"referenceID": 1, "context": "Supervised term weighting schemes were developed especially for text categorization because of the fact that a supervised knowledge on the class labels of the training samples is provided [1-4].", "startOffset": 188, "endOffset": 193}, {"referenceID": 2, "context": "Supervised term weighting schemes were developed especially for text categorization because of the fact that a supervised knowledge on the class labels of the training samples is provided [1-4].", "startOffset": 188, "endOffset": 193}, {"referenceID": 0, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 1, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 2, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 6, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 7, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 8, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 9, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 10, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 11, "context": "From past few years, many researchers have proposed alternative term-document relevance schemes [1, 13-16].", "startOffset": 96, "endOffset": 106}, {"referenceID": 12, "context": "From past few years, many researchers have proposed alternative term-document relevance schemes [1, 13-16].", "startOffset": 96, "endOffset": 106}, {"referenceID": 13, "context": "From past few years, many researchers have proposed alternative term-document relevance schemes [1, 13-16].", "startOffset": 96, "endOffset": 106}, {"referenceID": 14, "context": "From past few years, many researchers have proposed alternative term-document relevance schemes [1, 13-16].", "startOffset": 96, "endOffset": 106}, {"referenceID": 12, "context": "In [14], a comparison of corpusbased and class-based keyword selection is proposed by using TF-IDF as weighting scheme.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [4], a class-indexing-based term weighting for automatic text classification is proposed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 18, "context": ", [20] using Bayes posterior probability.", "startOffset": 2, "endOffset": 6}, {"referenceID": 9, "context": "Though, some works make use of Bayes probability for representation, they have not clearly stated the advantage of the measure in classification [11, 18].", "startOffset": 145, "endOffset": 153}, {"referenceID": 16, "context": "Though, some works make use of Bayes probability for representation, they have not clearly stated the advantage of the measure in classification [11, 18].", "startOffset": 145, "endOffset": 153}, {"referenceID": 18, "context": "After [20], this measure was extensively used for term weighting [21, 22].", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "After [20], this measure was extensively used for term weighting [21, 22].", "startOffset": 65, "endOffset": 73}, {"referenceID": 20, "context": "After [20], this measure was extensively used for term weighting [21, 22].", "startOffset": 65, "endOffset": 73}, {"referenceID": 18, "context": ", [20] also propose a text representation scheme which works with the reduced dimension for each document at the time of representation itself.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": ",[20].", "startOffset": 1, "endOffset": 5}, {"referenceID": 18, "context": ", [20] make use of SVM as the classifier.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": ", [20] as explained in section 1.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": ",[20] in Table 3.", "startOffset": 1, "endOffset": 5}, {"referenceID": 18, "context": "The results corresponding to [20] have been extracted directly from the paper as the representation scheme is same in both the works and they also have provided the results on the same 20Newsgroups dataset using only SVM classifier with different kernels.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": ",[20].", "startOffset": 1, "endOffset": 5}, {"referenceID": 18, "context": ",[20].", "startOffset": 1, "endOffset": 5}, {"referenceID": 18, "context": "Percentage of Training Results from [20] with SVM Results of Proposed Method SVM k-NN Linear RBF Polynomial Linear RBF Polynomial 30 83.", "startOffset": 36, "endOffset": 40}], "year": 2016, "abstractText": "In this paper, we introduce a new measure called Term_Class relevance to compute the relevancy of a term in classifying a document into a particular class. The proposed measure estimates the degree of relevance of a given term, in placing an unlabeled document to be a member of a known class, as a product of Class_Term weight and Class_Term density; where the Class_Term weight is the ratio of the number of documents of the class containing the term to the total number of documents containing the term and the Class_Term density is the relative density of occurrence of the term in the class to the total occurrence of the term in the entire population. Unlike the other existing term weighting schemes such as TF-IDF and its variants, the proposed relevance measure takes into account the degree of relative participation of the term across all documents of the class to the entire population. To demonstrate the significance of the proposed measure experimentation has been conducted on the 20 Newsgroups dataset. Further, the superiority of the novel measure is brought out through a comparative analysis.", "creator": "Microsoft\u00ae Word 2013"}}}