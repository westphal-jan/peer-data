{"id": "1608.07328", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2016", "title": "Fundamental Limits of Budget-Fidelity Trade-off in Label Crowdsourcing", "abstract": "criterion signatures ( cs ) \u2022 fairly standardized approach to perform comprehensive digital searches using small contributions of a matched crowd. through cs, a taskmaster typically breaks down the project very small batches of tasks nor assigns appropriately using so - that individuals which lower real numbers. the company then extracts and analyzes the results for surveying and outputs their purpose of predicting project. concerning this framework, the cs problem, as a plug - in - the - program computation problem, is estimated and adjusted among mixed expert theoretic rate - determining framework. the purpose is to identify who ultimate fidelity desired audiences can build by any form of reward from balanced crowd and intelligent decoding ( inference ) algorithm with reasonably given budget. positive results must established employing a joint source channel ( sql ) inspection process, which represent arbitrary query scheme making inference, over parallel noisy channels, which represents workers with imperfect experience levels. we also register and analyze a code scheme dubbed $ k $ - aaa incidence coding due use of flexible pricing affecting this setting.", "histories": [["v1", "Thu, 25 Aug 2016 22:43:46 GMT  (84kb,D)", "http://arxiv.org/abs/1608.07328v1", "Accepted for NIPS 2016, Barcelona, Spain"]], "COMMENTS": "Accepted for NIPS 2016, Barcelona, Spain", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT", "authors": ["farshad lahouti", "babak hassibi"], "accepted": true, "id": "1608.07328"}, "pdf": {"name": "1608.07328.pdf", "metadata": {"source": "CRF", "title": "Fundamental Limits of Budget-Fidelity Trade-off in Label Crowdsourcing", "authors": ["Farshad Lahouti"], "emails": ["lahouti@caltech.edu", "hassibi@caltech.edu"], "sections": [{"heading": "1 Introduction", "text": "Digital crowdsourcing (CS) is a modern approach to perform certain large projects using small contributions of a large crowd. Crowdsourcing is usually used when the tasks involved may better suite humans rather than machines or in situations where they require some form of human participation. As such crowdsourcing is categorized as a form of human-based computation or human-in-the-loop computation system. This article examines the fundamental performance limits of crowdsourcing and sheds light on the design of optimized crowdsourcing systems.\nCrowdsourcing is used in many machine learning projects for labeling of large sets of unlabeled data and Amazon Mechanical Turk (AMT) serves as a popular platform to this end. Crowdsourcing is also useful in very subjective matters such as rating of different goods and services, as is now widely popular in different online rating platforms and applications such as Yelp. Another example is if we wish to classify a large number of images as suitable or unsuitable for children. In so-called citizen research projects, a large number of \u2013often human deployed or operated\u2013 sensors contribute to accomplish a wide array of crowdsensing objectives, e.g., [2] and [3].\nIn crowdsourcing, a taskmaster typically breaks down the project into small batches of tasks, recruits so-called workers and assigns them the tasks accordingly. The crowdsourcer then collects and analyzes the results collectively to address the purpose of the project. The worker\u2019s pay is often low or non-existent. In cases such as labeling, the work is typically tedious and hence the workers usually handle only a small batch of work in a given project. The workers are often non-specialists\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 8.\n07 32\n8v 1\n[ cs\n.L G\n] 2\n5 A\nand as such there may be errors in their completion of assigned tasks. Due to the nature of task assignment by the taskmaster, the workers and the skill level are typically unknown a priori. In case of rating systems, such as Yelp, there is no pay for regular reviewers but only non-monetary personal incentives; however, there are illegal reviewers who are paid to write fake reviews. In many cases of crowdsourcing, the ground truth is not known at all. The transitory or fleeting characteristic of workers, their unknown and imperfect skill levels and their possible motivations for spamming makes the design of crowdsourcing projects and the analysis of the obtained results particularly challenging.\nResearchers have studied the optimized design of crowdsourcing systems within the setting described for enhanced reliability. Most research reported so far is devoted to optimized design and analysis of aggregation and inference schemes and possibly using redundant task assignment. In AMT-type crowdsourcing, two popular approaches for aggregation are namely majority voting and the Dawid and Skene (DS) algorithm [5]. The former sets the estimate based on what the majority of the crowd agrees on, and is provably suboptimal [8]. Majority voting is susceptible to error, when there are spammers in the crowd, as it weighs the opinion of everybody in the crowd the same. The DS algorithm, within a probabilistic framework, aims at joint estimation of the workers\u2019 skill levels and a reliable label based on the data collected from the crowd. The scheme runs as an expectation maximization (EM) formulation in an iterative manner. More recent research with similar EM formulation and a variety of probabilistic models are reported in [9, 12, 10]. In [8], a label inference algorithm for CS is presented that runs iteratively over a bipartite graph. In [1], the CS problem is posed as a so-called bandit survey problem for which the trade offs of cost and reliability in the context of worker selection is studied. Schemes for identifying workers with low skill levels is studied in, e.g., [6]. In [13], an analysis of the DS algorithm is presented and an improved inference algorithm is presented. Another class of works on crowdsourcing for clustering relies on convex optimization formulations of inferring clusters within probabilistic graphical models, e.g., [11] and the references therein.\nIn this work, a crowdsourcing problem is modeled and analyzed in an information theoretic setting. The purpose is to seek ultimate performance bounds, in terms of the CS budget (or equivalently the number of queries per item) and a CS fidelity, that one can achieve by any form of query from the workers and any inference algorithm. Two particular scenarios of interest include the case where the workers\u2019 skill levels are unknown both to the taskmaster and the crowdsourcer, or the case where the skill levels are perfectly estimated during inference by the crowdsourcer. Within the presented framework, we also investigate a class of query schemes dubbed k-ary incidence coding and analyze its performance. At the end, we comment on an associated query pricing strategy."}, {"heading": "2 Modeling Crowdsourcing", "text": "In this Section, we present a communication system model for crowdsourcing. The model, as depicted in Figure 1a, then enables the analysis of the fundamental performance limits of crowdsourcing."}, {"heading": "2.1 Data Set: Source", "text": "Consider a dataset X = {X1, . . . , XL} composed of L items, e.g., images. In practice, there is certain functionB(X) \u2208 B(X ) of the items that is of interest in crowdsourcing and is here considered as the source. The value of this function is to be determined by the crowd for the given dataset. In the case of crowdsourced clustering, B(Xi) = Bj \u2208 B(X ) = {B1, . . . , BN} indicates the bin or cluster to which the item Xi ideally belongs. We have B(X1, . . . , Xn) = B(Xn) = (B(X1), . . . , B(Xn)). The number of clusters, |B(X )| = N , may or may not be known a priori."}, {"heading": "2.2 Crowd Workers: Channels", "text": "The crowd is modeled by a set of parallel noisy channels in which each channel Ci, i = 1, . . . ,W, represents the ith worker. The channel input is a query that is designed based on the source. The channel output is what a user perceives or responds to a query. The output may or may not be the correct answer to the query depending on the skill level of the worker and hence the noisy channel is meant to model possible errors by the worker.\nA suitable model for Ci is a discrete channel model. The channels may be assumed independent, on the basis that different individuals have different knowledge sets. Related probabilistic models representing the possible error in completion of a task by a worker are reviewed in [8]. Formally, a channel (worker) is represented by a probability distribution P (v|u), u \u2208 U , v \u2208 V , where U is the set of possible responses to a query and V is the set of choices offered to the worker in responding to a query. For the example of images suitable for children, in general we may consider a shade of possible responses to the query, U , including the extremes of totally suitable and totally unsuitable; As possible choices offered to the worker to answer the query, V , we may consider two options of suitable and unsuitable. As described below, in this work we consider two channel models representing possibly erroneous responses of the workers: an M -ary symmetric channel model (MSC) and a spammer-hammer channel model (SHC).\nAn MSC model with parameter , is a symmetric discrete memoryless channel without feedback [4] and with input u \u2208 U and output v \u2208 V (|U| = |V| = M ), that is characterized by the following transition probability\nP (v|u) = {\nM\u22121 v 6= u 1\u2212 v = u. (1)\nIf we consider a sequence of channel inputs un = (u1, . . . , un) and the corresponding output sequence vn, we have P (vn|un) = \u220fn i=1 P (vi|ui), which holds because of the memoryless and no feedback assumptions. In case of clustering and MSC, the probability of misclassifying any input from a given cluster in another cluster only depends on the worker and not the corresponding clusters.\nIn the spammer-hammer channel model with the probability of being a hammer of q (SHC(q)), a spammer randomly and uniformly chooses a valid response to a query, and a hammer perfectly answers the query [8]. The corresponding discrete memoryless channel model without feedback, with input u \u2208 U and output v \u2208 V and state C \u2208 {S,H}, P (C = H) = q is described as follows\nP (v|u,C) =  0 C = H and v 6= u 1 C = H and v = u 1 |V| C = S\n(2)\nwhere C \u2208 {S,H} indicates whether the worker (channel) is a spammer or a hammer. In the case of our current interest |U| = |V| = M , and we have P (vn|un, Cn) = \u220fn i=1 P (vi|ui, Ci).\nIn the sequel, we consider the following two scenarios: when the workers\u2019 skill levels are unknown (SL-UK) and when it is perfectly known by the crowdsourcer (SL-CS). In both cases, we assume that the skill levels are not known at the taskmaster (transmitter).\nThe presented framework can also accommodate other more general scenarios of interest. For example, the feedforward link in Figure 1a could be used to model a channel whose state is affected by the input, e.g., difficulty of questions. These extensions remain for future studies."}, {"heading": "2.3 Query Scheme and Inference: Coding", "text": "In the system model presented in Figure 1a, encoding shows the way the queries are posed. A basic query is that the worker is asked of the value of B(X). In the example of crowdsourcing for labeling images that are suitable for children, the query is \"This image suits children; true or false?\" The decoder or the crowdsourcer collects the responses of workers to the queries and attempts to infer the right label (cluster) for each of the images. This is while the collected responses could be in general incomplete or erroneous.\nIn the case of crowdsourcing for labeling a large set of dog images with their breeds, a query may be formed by showing two pictures at once and inquiring whether they are from the same breed [11]. The queries in fact are posed as showing the elements of a binary incidence matrix, A, whose rows and columns correspond to X . In this case, A(X1, X2) = 1 indicates that the two are members of the same cluster (breed) and A(X1, X2) = 0 indicates otherwise. The matrix is symmetric and its diagonal is 1. We refer to this query scheme as Binary Incidence Coding. If we show three pictures at once and ask the user to classify them (put the pictures in similar or distinct bins); it is as if we ask about three elements of the same matrix, i.e., A(X1, X2),A(X1, X3) and A(X2, X3) (Ternary Incidence Coding). In general, if we show k pictures as a single query, it is equivalent to inquiring about C(k, 2) (choose 2 out of k elements) entries of the matrix (k-ary Incidence Coding or kIC). As we elaborate below, out of the 2C(k,2) possibilities, a number of the choices remain invalid and this provides an error correction capability for kIC.\nFigures 1b and 1c show the graphical representation of 3IC, and the choices a worker would have in clustering with this code. The nodes denote the items and the edges indicate whether they are in the same cluster. In 3IC, if X1 and X2 are in the same cluster as X3, then all three of them are in the same cluster. It is straightforward to see that in 3IC and for N = 2, we only have four valid responses (Figure 1b) to a query as opposed to 2C(3,2) = 8. The first item in Figure 1c is invalid because there are only two clusters (N = 2) (in case we do not know the number of clusters or N \u2265 3, then this would remain a valid response). In this setting, the encoded signal u can be one of the four valid symbols in the set U ; and similarly what the workers may select v (decoded signal over the channel) is from the set V , where U = V . As such, since in kIC the obviously erroneous answers are removed from the choices a worker can make in responding to a query, one expects an improved overall CS performance, i.e., an error correction capability for kIC. In Section 4, we study the performance of this code in greater details. Note that in clustering with kIC (k \u2265 2) described above, the code would identify clusters up to their specific labellings.\nWhile we presented kIC as a concrete example, there may be many other forms of query or coding schemes. Formally, the code is composed of encoder and decoder mappings:\nC : B(X )n \u2192 U \u2211W i=1mi , C\u2032 : V \u2211W i=1mi \u2192 B\u0302(X ) n , (3)\nwhere n is the block size or number of items in each encoding (we assume n|L), andmi is the number of uses of channel Ci or queries that worker i, 1 \u2264 i \u2264 W, handles. In many practical cases of interest, we have B\u0302(X ) = B(X ) and we may have n = L. The rate of the code is R = \u2211W i=1mi/n queries per item. In this setting, C\u2032(C(B(Xn))) = B\u0302(Xn). Depending on the availability of feedback from the decoder to the encoder, the code can adapt for optimized performance. The feedback could provide the encoder with the results of prior queries. We here focus on non-adaptive codes in (3) that are static and remain unchanged over the period of crowdsourcing project. We will elaborate on code design in Section 3.\nDepending on the type of code in use and the objectives of crowdsourcing, one may design different decoding schemes. For instance, in the simple case of directly inquiring workers about the function B(Xi), with multiple queries for each item, popular approaches are majority voting, and EM style decoding due to Dawid and Skene [5], where it attempts to jointly estimate the workers\u2019 skill levels and decode B(X). In the case of clustering with 2IC, an inference scheme based on convex optimization is presented in [11].\nThe rate of the code is proportional to the CS budget and we use the rate as a proxy for budget throughout this analysis. However, since different types of query have different costs both financially (in crowdsourcing platforms) and from the perspective of time or effort it takes from the user to process it, one needs to be careful in comparing the results of different coding schemes. We shall elaborate on this issue for the case of kIC in Appendix E."}, {"heading": "2.4 Distortion and the Design Problem", "text": "In the framework of Figure 1a, we are interested to design the CS code, i.e., the query and inference schemes, such that with a given budget a certain CS fidelity is optimized. We consider the fidelity as an average distortion with respect to the source (dataset). For a distance function d(B(x), B\u0302(x)), for which d(B(xn), B\u0302(xn)) = 1n \u2211n i=1 d(B(xi), B\u0302(xi)), the average distortion is\nD(B(X), B\u0302(X)) = Ed(B(Xn), B\u0302(Xn)) = \u2211 Xn P (B(X n))P (B\u0302(Xn)|B(Xn))d(B(Xn), B\u0302(Xn)), (4)\nwhere P (B(Xn)) = P (B(X))n for iid B(X). The design problem is therefore one of CS fidelityquery budget optimization (or distortion-rate, D(R), optimization) and may be expressed as follows\nD\u2217(Rt) = min C,C\u2032,R\u2264Rt D(B(X), B\u0302(X)) (5)\nwhere Rt is a target rate or query budget. The optimization is with respect to the coding and decoding schemes, the type of feedback (if applicable), and query assignment and rate allocation. The optimum solution to the above problem is referred to as the distortion-rate function, D\u2217(Rt) (or CS fidelity-query budget function). A basic distance function, for the case where B(X) is discrete, is l0(B(X), B\u0302(X)), or the Hamming distance. In this case, the average distortion D(B(X), B\u0302(X)) reflects the average probability of error. As such, the D(R) optimization problem may be rewritten as follows\nD\u2217(Rt) = min C,C\u2032,R\u2264Rt P (E : B\u0302(X) 6= B(X)). (6)\nIn case of crowdsourcing for clustering, this quantifies the performance in terms of the overall probability of error in clustering. For other crowdsourcing problems, we may consider other distortion functions. Equivalently, we may consider minimizing the rate subject to a constrained distortion in crowdsourcing. The R(D) problem is expressed as follows\nR\u2217(Dt) = min C,C\u2032,D(B(X),B\u0302(X))\u2264Dt R = min C,C\u2032,D(B(X),B\u0302(X))\u2264Dt W\u2211 i=1 mi/n (7)\nwhere Dt is a target distortion or average probability of error. The optimum solution to the above problem is referred to as the rate-distortion function, R\u2217(Dt) (CS query budget-fidelity function). In case, the taskmaster does not know the skill level of the workers, different users -disregarding their skill levels- would receive the same number of queries (mi = m\u2032,\u2200i); and the code design involves designing the query and inference schemes."}, {"heading": "3 Information Theoretic CS Budget-Fidelity Limits", "text": "In the CS budget-fidelity optimization problem in (5), the code providing the optimized solution indeed needs to balance two opposing design criteria to meet the target CS fidelity: On one hand the design aims at efficiency of the query and making as small number of queries as possible; On the other hand, the code needs to take into account the imperfection of worker responses and incorporate sufficient redundancy. In information theory (coding theory) realm, the former corresponds to source coding (compression) and the latter corresponds to channel coding (error control coding) and coding to serve both purposes is a joint source channel code.\nIn this Section, we first present a brief overview on joint source channel coding and related results in information theory. Next, we present the CS budget-fidelity function in two cases of SL-UK and SL-CS described in Section 2.2."}, {"heading": "3.1 Background", "text": "Consider the communication of a random source Z from a finite alphabet Z over a discrete memoryless channel. The source is first processed by an encoder C and whose output is communicated over the channel. The channel output is processed by a decoder C\u2032, which reconstructs the source as Z\u0302 \u2208 Z\u0302 and we often have Z = Z\u0302 .\nFrom a rate-distortion theory perspective, we first consider the case where the channel is error free. The source is iid distributed with probability mass function P (Z) and based on Shannon\u2019s source coding theorem is characterized by a rate-distortion function,\nR\u2217(Dt) = min C,C\u2032:D(Z,Z\u0302)\u2264Dt I(Z, Z\u0302), (8)\nwhere I(., .) indicates mutual information between two random variables. The source coding is defined by the following two mappings:\nC : Zn \u2192 {1, . . . , 2nR}, C\u2032 : {1, . . . , 2nR} \u2192 Z\u0302n (9) The average distortion is defined in (4) and Dt is the target performance. The optimization in source coding with distortion is with respect to the source coding or compression scheme, that is described probabilistically as P (Z\u0302|Z) in information theory. The proof of the source coding theorem follows in two steps: In the first step, we prove that any rate R \u2265 R\u2217(Dt) is achievable in the sense that there exists a family (as a function of n) of codes {Cn, C\u2032n} for which as n grows to infinity the resulting average distortion satisfies the desired constraint. In the second step or the converse, we prove that any code with rate R < R\u2217(Dt) results in an average distortion that violates the desired constraint. This establishes the described rate-distortion function as the fundamental limit for lossy compression of a source with a desired maximum average distortion.\nFrom the perspective of Shannon\u2019s channel coding theorem, we consider the source as an iid uniform source and the channel as a discrete memoryless channel characterized by P (V |U), where U \u2208 U is the channel input and, V \u2208 V is the channel output. The channel coding is defined by the following two mappings: C : {1, . . . , |Z|} \u2192 Un C\u2032 : Vn \u2192 {1, . . . , |Z|} (10) The theorem establishes the capacity of the channel as C = maxC,C\u2032 I(Z, Z\u0302) and states that for a rate R, there exists a channel code that provides a reliable communication over the noisy channel if and only if R \u2264 C. Again the proof follows in two steps: First, we establish achievability, i.e., we show that for any rate R \u2264 C, there exists a family of codes (as a function of length n) for which the average probability of error P (Z\u0302 6= Z) goes to zero as n grows to infinity. Next, we prove the converse, i.e., we show that for any rate R > C, the probability of error is always greater than zero and grows exponentially fast to 1/2 as R goes beyond C. This establishes the described capacity as the fundamental limit for transmission of an iid uniform source over a discrete memoryless channel.\nFor the problem of our interest, i.e., the transmission of an iid source (not necessarily uniform) over a discrete memoryless channel, the joint source channel coding theorem, aka source-channel separation theorem, is instrumental. The theorem states that in this setting a code exists that can facilitate reconstruction of the source with distortion D(Z, Z\u0302) \u2264 Dt if and only if R\u2217(Dt) < C. For completeness, we reproduce the theorem form [4] below.\nTheorem 1 Let Z be a finite alphabet iid source which is encoded as a sequence of n input symbols Un of a discrete memoryless channel with capacity C. The output of the channel V n is mapped onto the reconstruction alphabet Z\u0302n = C\u2032(V n). Let D(Zn, Z\u0302n) be the average distortion achieved by this joint source and channel coding scheme. Then distortion D is achievable if and only if C > R\u2217(Dt).\nThe proof follows a similar two step approach described above and assumes large block length (n\u2192\u221e). The result is important from a communication theoretic perspective as a concatenation of a source code, which removes the redundancy and produces an iid uniform output at a rate R > R\u2217(Dt), and a channel code, which communicates this reliably over the noisy channel at a rate R < C, can achieve the same fundamental limit."}, {"heading": "3.2 Basic Information Theoretic Bounds", "text": "We here consider crowdsourcing within the presented framework, and derive basic information theoretic bounds. Following Section 2.1, we examine the case where a large dataset X (L\u2192\u221e) and a function of interest B(X) with associated probability mass function, P (B(X)), are available.\nWe consider the MSC worker pool model described in Section 2.2, where the skill set of workers are from a discrete set E = { 1, 2, . . . , W \u2032} with probability P ( ), \u2208 E . The number of workers in each skill level class is assumed large. We here study the two scenarios of SL-UK and SL-CS.\nAt any given instance, a query is posed to a random worker with a random skill level within the set, E . We assume there is no feedback available from the decoder (non-adaptive coding) and the queries do not influence the channel probabilities (no feedforward). Extensions remain for future work.\nThe following theorem identifies the information theoretic minimum number of queries per item to perform at least as good as a target fidelity in case the skill levels are not known (SL-UK). The bound is oblivious to the type of code used and serves as an ultimate performance bound.\nTheorem 2 In crowdsourcing for a large dataset of N -ary discrete source B(X) \u223c P (B(X)) with Hamming distortion, when a large number of unknown workers with skill levels \u2208 E , \u223c P ( ) from an MSC population participate (SL-UK), the minimum number of queries per item to obtain an overall error probability of at most \u0302, is given by\nRmin =\n{ H(B(X))\u2212HN (\u0302) log2M\u2212HM (E( ))\n\u0302 \u2264 min{1\u2212 pmax, 1\u2212 1N } 0 otherwise,\n(11)\nin which HN ( ) , H(1 \u2212 , /(N \u2212 1), . . . , /(N \u2212 1)), and pmax = maxB(X)\u2208B(X ) P (B(X)). For any rate R above this threshold, there exists a query scheme (code) that achieves the desired fidelity \u0302, and for any rate below the threshold no such scheme exists.\nThe proof is provided in Appendix A. Another interesting scenario is when the crowdsourcer attempts to estimate the worker skill levels from the data it has collected as part of the inference. In case this estimation is done perfectly, the next theorem identifies the corresponding fundamental limit on the crowdsourcing rate. The proof is provided in Appendix B.\nTheorem 3 In crowdsourcing for a large dataset of N -ary discrete source B(X) \u223c P (B(X)) with Hamming distortion, when a large number of workers with skill levels \u2208 E , \u223c P ( ) -known to the crowdsourcer (SL-CS)- from an MSC population participate, the minimum number of queries per item to obtain an overall error probability of at most \u0302, is given by\nRmin =\n{ H(B(X))\u2212HN (\u0302) log2M\u2212E(HM ( ))\n\u0302 \u2264 min{1\u2212 pmax, 1\u2212 1N } 0 otherwise.\n(12)\nFor any rate R above this threshold, there exists a query scheme (code) that achieves the desired fidelity \u0302, and for any rate below the threshold no such scheme exists.\nComparing the results in Theorems 2 and 3 the following interesting observation can be made. In case the worker skill levels are unknown, the CS system provides an overall work quality (capacity) of an average worker; whereas when the skill levels are known at the crowdsourcer, the system provides an overall work quality that pertains to the average of the work quality of the workers.\n4 k-ary Incidence Coding\nIn this Section, we examine the performance of the k-ary incidence coding introduced in Section 2.3. The k-ary incidence code poses a query as a set of k \u2265 2 items and inquires the workers to identify those with the same label. In the sequel, we begin with deriving a lower-bound on the performance of kIC with a spammer-hammer worker pool. We then presents numerical results along with the information theoretic lower bounds presented in the previous Section.\n4.1 Performance of kIC with SHC Worker Pool\nWe consider kIC for crowdsourcing in the following setting. The items X in the dataset are iid with N = 2. There is no feedback from the decoder to the task manager (encoder), i.e., the code is non-adaptive. Since the task manager has no knowledge of the workers\u2019 skill levels, it queries the workers at the same fixed rate of R queries per item. To compose a query, the items are drawn uniformly at random from the dataset. We assume that the workers are drawn from the SHC(q) model elaborated in Section 2.2. The purpose is to obtain a lower-bound on the performance assuming an Oracle decoder that can perfectly identify the workers\u2019 skill levels (here a spammer or a hammer)\nand perform an optimal decoding. Specifically, we consider the following:\nmin C\u2032,C:kIC\nP (E : B\u0302(X) 6= B(X)) (13)\nwhere minimization is with respect to the choice of a decoder for a given kIC code. We note that the code length is governed by how the decoder operates, and often could be as long as the dataset. As evident in (2), in the SHC model, the channel error rate (worker reliability) is explicitly influenced by the code and parameter, k. In the model of Figure 1a, this implies that a certain static feedforward exists in this setting. We first present a lemma, which is used later to establish a Theorem 4 on kIC performance. The proofs are respectively provided in Appendix C and Appendix D.\nLemma 1 In crowdsourcing for binary labeling (N = 2) of a uniformly distributed dataset, with kIC and a SHC worker pool, the probability of error in labeling of an item by a spammer (C = S), is given by\n\u0304S = P (E : B\u0302(X) 6= B(X)|C = S) =  1 k2k\u22121 \u2211b(k\u22121)/2c i=0 i\u00d7 ( k i ) k odd\n1 k2k\u22121 [\u2211b(k\u22121)/2c i=0 i\u00d7 ( k i ) + k4 ( k k/2 )] k even.\nTheorem 4 Assuming crowdsourcing using a non-adaptive kIC over a uniformly distributed dataset (k \u2265 2), if the number of queries per item, R, is less than 1k ln(1\u2212q) ln \u0302 \u0304S , then no decoder can achieve an average probability of labeling error less than \u0302 for any L under the SHC(q) worker model.\nTo interpret and use the result in Theorem 4, we consider the following points: (i) The theorem presents a necessary condition, i.e., the minimum rate (budget) requirement identified here for kIC with a given fidelity is a lower bound. This is due to the fact that we are considering an oracle CS decoder that can perfectly identify the workers\u2019 skill levels and correctly label the item if the item is at least labeled by one hammer out of R\u2032 times it is processed by the workers. (ii) In the current setting, where the taskmaster does not know the workers\u2019 skill levels, each item is included in exactly R\u2032 \u2208 Z+ k-ary queries. That is due to the nature of the code R\u2032. (iii) As discussed in Appendix E, Theorem 4 can also be used to establish an approximate rule of thumb for pricing. Specifically, considering two query schemes k1IC and k2IC, the query price \u03c0 is to be set as\n\u03c0(k1) \u03c0(k2) \u2248 k1k2 ."}, {"heading": "4.2 Numerical Results", "text": "To obtain an information theoretic benchmark, the next corollary specializes Theorem 3 to the setting of interest in this Section.\nCorollary 1 In crowdsourcing for binary labeling of a uniformly distributed dataset with a SHC(q) worker pool -known to the crowdsourcer (SL-CS)- and number of choices in responding to a query of M , the minimum rate for any given coding scheme to obtain a probability of error of at most \u0302, is\nRmin =\n{ 1\u2212Hb(\u0302) q log2M\n, 0 \u2264 \u0302 \u2264 0.5 0 otherwise.\nqueries per item (14)\nFigure 2 shows the information theoretic limit of Corollary 1 and the bound obtained in Theorem 4. For rates (budgets) greater than the former bound, there exist a code which provides crowdsourcing with the desired fidelity; and for rates below this bound no such code exists. The coding theoretic lower bounds for kIC depend on k, q and fidelity, and improve as k and q grow. The kIC bounds for k = 1 is equivalent to the analysis leading to Lemma 1 of [8]."}, {"heading": "A Proof of Theorem 2", "text": "The parallel MSC channels representing workers with random skill levels may be viewed as a channel with random state [7]. The proof relies on the optimality of separate source and channel coding [7] (according to Shannon) in large dataset size and worker pool regime, which holds with a discrete memoryless source and discrete memoryless channel with random state. Here, we need a lossy source coder to bring the rate to R(D(B\u0302(X), B(X)) = \u0302). If the source is memoryless, then we have\nR(D(B\u0302(X), B(X)) = \u0302) = min I(B(X), B\u0302(X)) = H(P (B(X)))\u2212H(D = \u0302), (15)\nwhere H denotes the entropy and the minimization is with respect to the choice of a mapping (source coding scheme) P (B\u0302(X)|B(X)) (disregarding the channel or worker possible errors). With Hamming distance, the second term amounts to HN (\u0302), when \u0302 \u2264 min{1 \u2212 pmax, 1 \u2212 1N }. If \u0302 \u2265 1\u2212 pmax, then we can achieve the desired average distortion with rate zero and by a decoder that simply outputs argmaxP (B(X)). Again, if \u0302 \u2265 1\u2212 1N then we can achieve the desired average distortion with rate zero and by a decoder that uniformly outputs a B(X) at random. We need the channel to be able to provide reliable communication for such a rate. In case the skill levels are unknown to both the taskmaster and the crowdsourcer, we have\nnR(D = \u0302) \u2264 W\u2211 i miCSL\u2212UK . (16)\nand CSL\u2212UK = max\nP (u) I(U ;V ) = log2M \u2212HM (E( )) (17)\nwhere the maximum occurs when P (u) = 1/M and E(.) denotes the expectation operator. Note that since the taskmaster does not know about the skill levels, it can only communicate the queries to workers with an identical rate. Combining (15), (16) and (17), the proof is complete. Note that the optimality of the separate source and channel coding scheme invoked here relies on proving an achievable scheme and a converse, which is the basis for the latter part of the Theorem statement. This can be done in line with what is presented in sections 3.9 and 7.4 of [7] and is omitted for brevity."}, {"heading": "B Proof of Theorem 3", "text": "The proof follows the same steps as in that of Theorem 2. However, the capacity when the skill levels are known at the crowdsourcer is given by\nCSL\u2212CS = max P (u) I(U ;V | ) = log2M \u2212 E(HM ( )) (18)\nwhere the maximum occurs when P (u) = 1/M . Note again that since the taskmaster does not know about the skill levels, it can only communicate the queries to workers with an identical rate."}, {"heading": "C Proof of Lemma 1", "text": "The probability of error, when the query is posed to a spammer in a SHC, can be quantified as follows,\nP (E|C = S) = P (E : B\u0302(X) 6= B(X)|C = S) = \u2211 u\u2208U \u2211 v\u2208V P (u, v|C = S)P (E|U = u, V = v, C = S)\n= \u2211 u\u2208U \u2211 v\u2208V P (u)P (v|u,C = S)P (E|U = u, V = v, C = S)\n= \u2211 u\u2208U \u2211 v\u2208V 1 M2P (E|U = u, V = v)\n(19)\nThe last equality follows in part because of the uniform distribution of the dataset (P (B(X)) \u223c uniform) and that |U| = |V| = M .\nEach kIC symbol can be represented by a vector of length k, whose elements are in the set {0, 1, . . . , N \u2212 1} (here N = 2). As such, for kIC with arbitrary k \u2265 2 and N = 2, the number of valid responses to a query can be counted as\nk\u2032 =  \u2211b(k\u22121)/2c i=0 ( k i ) k odd\u2211b(k\u22121)/2c\ni=0\n( k i ) + 0.5 ( k k/2 ) k even\n(20)\nor alternatively k\u2032 = 2k\u22121. This is obtained noting that for k \u2265 2, kIC does not recover specific labellings (e.g., in clustering it does put items in their associated bins but does not label the bins). In the same direction, each query of kIC that is posed to a worker, is equivalent to transmission of a symbol over a k\u2032-ary discrete channel (M = k\u2032), or alternatively a codeword of k bits over an equivalent binary discrete channel. As such, any linear combination of two codewords over the corresponding field would create another codeword (up to labeling). As a result,\u2211\nu\u2208U \u2211 v\u2208V P (E|U = u, V = v) = k \u2032 k \u2211 u\u2032\u2208U wH(u \u2032)\nwhere wH is the Hamming weight. Noting (20) and setting M = k\u2032 in (19) the proof is complete."}, {"heading": "D Proof of Theorem 4", "text": "Under the SHC model, we consider an oracle decoder who only makes a mistake on task i if it is only assigned to spammers. Formally, the average error probability at the decoder, if in all R\u2032 queries per task (codeword) only spammers are selected, is given by\nP (E : B\u0302(X) 6= B(X)) = \u2211 C P (E : B\u0302(X) 6= B(X)|C1, . . . , CR\u2032)P (C1, . . . , CR\u2032) =\nP (E|C = S)\u00d7 (1\u2212 q)R \u2032 = P (E|C = S)\u00d7 (1\u2212 q)R \u2032\nThe last equality follows, since when the decoder observes R\u2032 instances of spammers, it does not have any more information as observing a single spammer. In kIC, the number of queries per item X , R, is given by R = 1kR \u2032. As such, we have the following result for an arbitrary decoder\n\u0302 = P (E : B\u0302(X) 6= B(X)|C = S)\u00d7 (1\u2212 q)kR (21)\nUsing Lemma 1, the desired result is obtained. Note that as evident in the lemma \u0304S = P (E|C = S) is also a function of k."}, {"heading": "E Pricing Strategy", "text": "Using Equation (21) (in proof of) Theorem KICBound, one can draw some insights into the design of a CS system based on kIC. Below we consider this in the context of pricing queries. Specifically, in crowdsourcing with kIC query scheme with a spammer-hammer worker distribution of hammer probability q, we consider two scenarios: (i) the case where the price of the query per item is fixed at \u03c0 and does not change with k and (ii) the case where the query price is a function of k, \u03c0(k). In the former case, one can readily examine that since P (E|C = S) in Lemma 1 grows very slowly with k, any increase in k directly translates to a smaller rate R and hence crowdsourcing cost \u03c0nR for a given crowdsourcing fidelity \u0302. In the latter case, however, the analysis sheds light on the appropriate pricing range. Specifically, for a given crowdsourcing budget and two query schemes k1IC and k2IC with rates R1 and R2, we have \u03c0(k1)R1 = \u03c0(k2)R2; and for a given \u0302 from (21), we have k1R1 \u2248 k2R2. This indicates that the pricing in the two scenarios compare as \u03c0(k1)\u03c0(k2) \u2248 k1 k2\n. This sets a threshold (range) for crowdsourcing pricing strategy: If we are to use a k2IC as opposed to k1IC (k2 > k1), then we would have to pay at most \u03c0(k2) .\n\u03c0(k1)k2 k1 . Note that due to said nature of P (E|C = S), this approximation is more accurate for larger values of k1 and k2 and when their difference is small."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Digital crowdsourcing (CS) is a modern approach to perform certain large projects<lb>using small contributions of a large crowd. In CS, a taskmaster typically breaks<lb>down the project into small batches of tasks and assigns them to so-called workers<lb>with imperfect skill levels. The crowdsourcer then collects and analyzes the results<lb>for inference and serving the purpose of the project. In this work, the CS problem,<lb>as a human-in-the-loop computation problem, is modeled and analyzed in an<lb>information theoretic rate-distortion framework. The purpose is to identify the<lb>ultimate fidelity that one can achieve by any form of query from the crowd and any<lb>decoding (inference) algorithm with a given budget. The results are established<lb>by a joint source channel (de)coding scheme, which represent the query scheme<lb>and inference, over parallel noisy channels, which model workers with imperfect<lb>skill levels. We also present and analyze a query scheme dubbed k-ary incidence<lb>coding and study optimized query pricing in this setting.", "creator": "LaTeX with hyperref package"}}}