{"id": "1608.07685", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2016", "title": "KSR: A Semantic Representation of Knowledge Graph within a Novel Unsupervised Paradigm", "abstract": "knowledge representation appears a big concern in ai, architecture currently known as some intrinsic capability of knowledge representation takes the original form inside its parameter relations to fit the computation components. however, most embedding methods merely drew on the structure similarity approaches require the explicit semantic expression, leading to this uninterpretable representation form. thus, traditional embedding methods - not accurately degrade the performance, but ultimately uncover many potential structures. continuing this sake, this paper proposes a double ranking method for achieving graph \\ textbf { ( ksr ) }, which imposes a two - level relational generative process that closely extracts possible associations to then locally assigns, further category in each aspect for every triple. if both quantitative aspects and categories constitute semantics - connected, lower complexity of categories in this aspect is treated as providing principal representation of another scheme. extensive statistics yielded our studies eliminating relatively state - perform - the - job baselines in the substantial extent.", "histories": [["v1", "Sat, 27 Aug 2016 09:53:38 GMT  (242kb,D)", "http://arxiv.org/abs/1608.07685v1", null], ["v2", "Tue, 25 Oct 2016 02:48:01 GMT  (220kb,D)", "http://arxiv.org/abs/1608.07685v2", "submitting to AAAI.2017"], ["v3", "Tue, 13 Jun 2017 02:06:16 GMT  (146kb,D)", "http://arxiv.org/abs/1608.07685v3", "submitting to NIPS.2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["han xiao", "minlie huang", "xiaoyan zhu"], "accepted": false, "id": "1608.07685"}, "pdf": {"name": "1608.07685.pdf", "metadata": {"source": "CRF", "title": "Knowledge Semantic Representation A Generative Model for Interpretable Knowledge Graph Embedding", "authors": ["Han Xiao", "Minlie Huang", "Xiaoyan Zhu"], "emails": ["bookman@vip.163.com;", "aihuang@tsinghua.edu.cn", "zxy-dcs@tsinghua.edu.cn"], "sections": [{"heading": "Introduction", "text": "Knowledge is always leveraged by many tasks such as semantic analysis, question answering and information retrieval. For offering a numerical representation framework to joint the statistical learning methods, knowledge graph embedding is proposed, which usually represents the entities and relations in a continuous low-dimensional vector space. In detail, a basic fact in knowledge graph is usually represented as a symbolic triple (h, r, t), where h, r, t are the representation vectors of the head entity, the relation and the tail entity, respectively. To this end, a lot of embedding methods have been proposed, such as TransE (Bordes et al. 2013), TransG (Xiao, Huang, and Zhu 2016b), ManifoldE (Xiao, Huang, and Zhu 2016a), etc.\nTo be the most influential branch of embedding models, translation-based methods, such as TransE, adopt the principle of translating the head entity to the tail one by a relationspecific vector, mathematically speaking h + r \u2248 t. Intuitively, the corresponding objective is fitting the translationbased principle with the representations by taking a minimization over the fitting error. Geometrically, the representations correspond to the points in Euclidean space Rn.\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nThe embedding models based on triple fitting certainly achieve the success, but geometrical positions as knowledge representation could hardly explicitly indicate the semantics. Widely agreed in knowledge embedding community, it\u2019s difficult to exactly map between a specific point to a specific semantics. For example, given the entity Table, its planeembedding representation (0.82, 0.51) could hardly tell anything semantic, such as being a furniture, being a daily tool, not an animal and so on. However, without explicit semantic expression, the gap between knowledge and language remains, limiting the incorporation of knowledge representation and natural language understanding (NLU). Thus, developing a semantics-specific representation triggers an urgent task.\nHowever, bridging symbolic triples and human-level understanding is an extremely challenging task. There is an instance: the entity Stanford University is recorded as an incomprehensible symbol /m/06pwq in Freebase, while what we expect semantically represent the knowledge (this entity) as is (University:Yes, Animal:No, Location:California, ...). For a first sight, there is no hint, making it a fantasy task. But, conversely this task is worthy to motivate our work, because knowledge representation better be composed by some basic semantics-relevant concepts such as University:Yes, Animal:No. At least, in this way, it is more elegant to joint language and knowledge.\nIn the scenario of question answering, based on semantic knowledge representation, there is a very naive method to answer the query (What private university is most famous in California?): first extracting the keywords (private, university, ...), then mapping the keywords to corresponding knowledge features (University:Yes, Animal:No, Location:California, Type:Private, Famous:Very, ...), and finally inferring the possible entity as the answer by searching for the matched representations of entities. Notably, knowledge feature is a term we introduced for describing some knowledge semantic aspects, such as being a university or not (University:Yes/No), what location (Location:California/...), etc. This potential application joints the language and knowledge with semantic knowledge embedding, which further motivates our work.\nConsequently, we propose a novel branch of knowledge representation, called Knowledge Semantic Analysis (KSA), which is a knowledge representation methodology that is\nar X\niv :1\n60 8.\n07 68\n5v 1\n[ cs\n.L G\n] 2\n7 A\nug 2\n01 6\nsupposed to explicitly provide human-comprehensive or at least semantics-relevant representation. A well-fitting model for knowledge graph is encouraging, but is still insufficient for more possibilities.\nPrevious work could not focus on the Knowledge Semantic Analysis (KSA) subject, leading to a semantically uninterpretable representation. However, our model Knowledge Semantic Representation (KSR) 1 concentrates on this subject to bridge the gap between knowledge representations and semantics. KSR leverages a two-level hierarchical generative process (Fig.1) to semantically represent the entities, relations and triples. In the first level of our model, we generate some knowledge features such as University(Yes/No), Animal Type, Location, etc. In the second level of our model, we assign a corresponding category in each knowledge feature for every triple. For the example of Stanford University, we assign Yes in the University feature, California in Location feature and so on. Naturally, knowledge are semantically organized in a multi-view clustering form. For an instance as Fig.1, clustering by every semantic aspects such as Location, University(Y/N), etc. could categorize the entities. Thus, by taking advantage of the multi-view clustering nature, KSR is semantically interpretable. Though the semantics are learned as a latent form, at least we could trivially involve little hand-craft analysis to map the latent features and categories to the human-comprehensive semantics.\nWe evaluate the effectiveness of our model Knowledge Semantic Representation (KSR) on two tasks that are knowledge graph completion and entity classification, for three benchmark datasets that are the subsets of Wordnet (Miller\n1KSA is a subject, and KSR is a method for this subject.\n1995) and Freebase (Bollacker et al. 2008). Experimental results on real-world datasets show that our model consistently outperforms the other baselines with an extensive improvement. Also, the most attractive part that the semantic analysis is presented in the \u201cExperiments\u201d section.\nContributions. We propose the subject of Knowledge Semantic Analysis (KSA). For the purpose to fulfill KSA, our model KSR is proposed as a two-level hierarchical generative process, which globally extracts many knowledge features and then locally assigns a specific category in each feature for every triple. Besides, our method outperforms all the state-of-the-art baselines on the tasks of knowledge graph completion and entity classification, which justifies our effectiveness and efficiency."}, {"heading": "Related Work", "text": "TransE (Bordes et al. 2013) is a seminal work for the translation-based principle, which translates the head entity to the tail one by the relation vector, mathematically h + r \u2248 t. Commonly, the scale of the loss vector is the score function, which represents the possibility of triples and a smaller score is better.\nThe next variants transform entities into different subspaces to play different roles. TransH (Wang et al. 2014b) leverages the relation-specific hyperplane to pose the entities. TransR (Lin et al. 2015) utilizes the relation-related matrix to rotate the embedding space. Similar researches also contain TransG (Xiao, Huang, and Zhu 2016b), TransA (Xiao et al. 2015), TransD (Ji et al. ) and TransM (Fan et al. 2014).\nFurther researches take extra structural information into embedding. PTransE (Lin, Liu, and Sun 2015) starts a line\nof path-based models, simultaneously involving the information and confidence level of the path in the knowledge graph. (Wang, Wang, and Guo 2015) leverages the rules to concentrate on the embeddings for the complex relation types such as 1-N, N-1 and N-N. SSE (Guo et al. 2015) aims at analyzing the geometric structure of embedding topologies then based on these discoveries, designs a semantically smooth score function. Also, KG2E (He et al. 2015) involves Gaussian analysis to characterize the uncertain concepts of knowledge graph. (Wang et al. 2014a) attempts to align the knowledge graph with the corpus then jointly conduct knowledge embedding and word embedding. However, the necessity of the alignment information limits this method both in performance and practical application. Thus, (Zhong et al. 2015) proposes \u201cJointly\u201d method that only aligns the freebase entity to the corresponding wiki-page. DKRL (Xie et al. 2016) extends the translation-based embedding methods from the triple-specific one to the \u201cText-Aware\u201d model. It\u2019s noteworthy that, ManifoldE (Xiao, Huang, and Zhu 2016a) is the seminal work of manifold-based principle to alleviate the ill-posed algebraic system and over-restricted geometric form of the traditional methods, which holds the state-of-the-art performance. There are also some pioneering work such as HOLE (Nickel, Rosasco, and Poggio 2015), SE (Bordes et al. 2011), LFM (Jenatton et al. 2012), NTN (Socher et al. 2013) and RESCAL (Nickel, Tresp, and Kriegel 2011), HOLE (Nickel, Rosasco, and Poggio 2015) etc."}, {"heading": "Methodology", "text": ""}, {"heading": "Model Description", "text": "We leverage a two-level hierarchical generative process to semantically represent the knowledge elements (entities/relations/triples), as following:\nFor each triple (h, r, t) \u2208 \u2206: (First-Level) Draw a knowledge feature fi from P(fi|r):\n1. (Second-Level) Draw a subject-specific category zi from\nP(zi) \u221d P(zi|h)P(zi|r)P(zi|t, fi)\n2. (Second-Level) Draw an object-specific category yi from\nP(yi) \u221d P(yi|t)P(yi|r)P(yi|zi, fi)\nAbove, E,R is the set of entities and relations, and \u2206 is the set of golden triples. All the parameters of P(fi|r), P(zi|h), P(zi|r), P(yi|t), P(zi|r), P(yi|r) are learned by training procedure, whereas all of P(f), P(h), P(r), P(t) are uniformly distributed, indicating they are safely omitted for being constant.\nRegarding the separated categories, the head-specific (zi) and tail-specific (yi) semantics are discriminated as the active and passive forms, or the subject- and object-relevant expressions. For example, \u201cShakespeare Did Write\u201d(headrelated) and \u201cMacbeth Was Written By\u201d(tail-related) of\n(Shakespeare, Write, Macbeth) are semantically distinct as subject- and object-specific. Thus, it better be to sample the category respectively from the head- and tail-part of the triple.\nHowever, for a single entity e, it is indifferent for being head or tail, mathematically P(zi|e) = P(yi|e). For example, the entity (Standford University) could be a subject or an object with the identical semantics. Also, it is noteworthy that the terms related with relations are inequivalent for being subject- or objective-related, stated in the last paragraph.\nRegarding P(zi, yi|fi), since one triple is too short to indicate more facts, the head- and tail-specific semantics or both the distributions over categories should be proximal enough to represent this one exact triple fact. To this end, we constrain the categories generation and pose a Laplace prior for the category distributions.\nFirstly, we expect zi and yi correspond to the same category. Thus, the case that zi 6= yi is forbidden in our model, so P(zi|yi, fi) \u221d \u03b4zi,yi \u2217 . For the example of Location feature of (Yangtze River, Event, Battle of Red Cliffs), the situation where zi is China and yi is American, is disallowed in our model, because one triple is so short that it could only talk about one exact thing as usual. Thus, only the case such as zi is China and yi is the same as zi (China), is accepted. Notably, though both the categories are consistent as zi = yi , the head- and tail-specific semantics could still be discriminated as different probabilities P(zi|h, r) and P(yi|t, r), respectively. Moreover, the cross-categories situations are beyond this seminal paper as a future work.\nSecondly, as an example, the head suggests the location is the category of China with probability 95% and American with 5%, then the tail is supposed to suggest China category (yi=zi=China) with higher probability than American (yi=zi=American). Thus, a Laplace prior is posed to approximate both the distribution, mathematically: P(zi|t, fi, \u03c3) \u221d exp ( \u2212 |P(zi)\u2212P(yi|t)|\u03c3 ) \u03b4zi,yi ,\nP(yi|zi, fi, \u03c3) \u221d exp ( \u2212 |P(yi)\u2212P(zi)|\u03c3 ) \u03b4zi,yi . where \u03c3 is a hyper-parameter for Laplace Distribution and P(zi), P(yi) are presented in the generative process.\nFig.2 is the corresponding probabilistic graph model, with which we could work out the joint probability. Notably, as some statistical literature introduced, for brevity, we replace P(a|b) .= [a|b] . Formulated in the equations of (1)-(3), n\nis the total knowledge feature number and d is the category number for each feature. Notably, the generative probability [h, r, t] of the triple (h, r, t) is our score function.\nNaturally, we adopt the most possible category in the specific knowledge features as the semantic representation. Suggested by the probabilistic graph (Fig.2), the exactly inferred representation for an entity Se = (Se,1, Se,2, ..., Se,n) or a relation Sr = (Sr,1, Sr,2, ..., Sr,n) is\nSe,i = arg d\nmax c=1\n[zi = c|e]\nSr,i = arg d\nmax c=1\n[zi = c|r][yi = c|r]\n* \u03b4zi,yj is 1 only if zi = yj , otherwise, it is 0\n[h, r, t, zk, yk|fk, \u03c3] = [zk|h][zk|r][yk|t][yk|r][zk, yk|fk, r, \u03c3] (1)\n[h, r, t] = n\u2211 k=1 [f = k|\u03c3]  d\u2211 i,j=1 [h, r, t, zk = i, yk = j|f = k, \u03c3]  (2)\n= First\u2212Level:Feature Mixture\ufe37 \ufe38\ufe38 \ufe37 n\u2211 k=1 [f = k|\u03c3] Second\u2212Level:Category Mixture\ufe37 \ufe38\ufe38 \ufe37 d\u2211 i,j=1 [zk = i, yk = j|f = k, \u03c3][h, r, t|zk = i, yk = j, f = k, \u03c3]  (3)"}, {"heading": "Objective & Training", "text": "The maximum data likelihood principle is applied for training. To be better distinguished, we maximize the ratio of likelihood of the true triples to that of the false ones. Our objective is as:\u2211\n(h,r,t)\u2208\u2206\nln[h, r, t]\u2212 \u2211\n(h\u2032,r\u2032,t\u2032)\u2208\u2206\u2032 ln[h\u2032, r\u2032, t\u2032] (4)\nwhere \u2206 is the set of golden triples and \u2206\u2032 is the set of false triples. The specific formula for [h, r, t] is presented in the previous subsection. This training procedure is very similar to (Xiao, Huang, and Zhu 2016a).\nAs to the efficiency, theoretically, the time complexity of our training algorithm is O(nd) where n is the feature number and d is the category number for each feature. If nd \u2248 d\u2032 where d\u2032 is the embedding dimension of TransE, our method is comparative to TransE in efficiency and this condition is practically satisfied. In real-word dataset FB15K, regarding the training time, TransE costs 11.3m and KSR costs 13.4m, which is proximal. Also, for a comparison, in the same setting, TransR needs 485.0m and KG2E costs 736.7m. Note that TransE is almost the fastest embedding method, which demonstrates that our method is nearly the most efficient."}, {"heading": "Clustering Perspective", "text": "Essentially, regarding the mixture form of equations (1) - (3), in both the first- and second-level, our method takes\nthe spirit of mixture model, which could be further analyzed from the clustering perspective. The second-level generative process clusters the knowledge elements (entities/relations/triples) according to some knowledge-feature associated aspects. These aspects stem from the first-level, mathematically according to all the probabilistic terms involved with fi. Furthermore, the first-level generative process adjusts different knowledge feature spaces with the feed-back from the second-level. Mathematically, the feed-back corresponds to [z1..n, y1..n, f |h, r, t]. In essence, knowledge are semantically organized in a multi-view clustering form, Thus, by modeling the multi-view clustering nature, KSR is semantically interpretable.\nFor clarity, we have visualized this process in Fig.1, besides a basic idea is discussed in the appendix, which we strongly suggest the readers to read first. To start, there is a pool of knowledge elements, which contains all the entities and relations. The simple clustering of these elements is ambiguous, because there are always many clustering forms, such as clustering by location, by being an animal or not, etc. However, once the first-level generates different semantic aspects that the knowledge features such as University and Location, clustering of knowledge elements in the second-level could be addressed according to one exact semantic aspect within this corresponding feature space. For example, Tsinghua University in University feature space belongs to the Yes cluster rather than No, while that in the Location one belongs to the Beijing cluster rather than Shanghai. Finally, summarizing each feature space, our model represents the entity/relation semantically, as Tsinghua University = (University:Yes, Location:Beijing, ...)."}, {"heading": "Experiments", "text": ""}, {"heading": "Settings", "text": "Datasets. Our experiments are conducted on public benchmark datasets that are the subsets of Wordnet and Freebase. About the statistics of these datasets, we refer the readers to (Xiao, Huang, and Zhu 2016a) and (Xie et al. 2016). The entity descriptions of FB15K are the same as DKRL (Xie et al. 2016), each of which is a small part of the corresponding wiki-page. The textual information of WN18 is the definitions that we extract from the Wordnet. Implementation. We implemented TransE, TransH, TransR and ManifoldE for comparison, we directly reproduce the\nclaimed results with the reported optimal parameters. Note that some results are directly reported from related literature due to the same task. The optimal settings of KSR is learning factor \u03b1 = 0.0004, margin \u03b3 = 2.5 and Laplace hyperparameter \u03c3 = 0.04. For a fair comparison within the same parameter quantity, we adopt three settings for dimensions: S1(n = 10, d = 10), S2(n = 20, d = 10) and S3(n = 90, d = 10). We train the model until convergence but at most 2000 rounds."}, {"heading": "Entity Classification", "text": "Motivations. To testify our semantics-specific performance, we conduct the entity classification prediction. Since the entity type such as Human Language, Artist and book Author represents some semantics-relevant sense, thus this task could justify KSR indeed addressed the semantic representation.\nEvaluation Protocol. Overall, this is a multi-label classification task with 25/50/75 classes, which means for each entity, the method should provide a set of types rather one specific type. In the classifier training process, we adopt the concatenation of category distribution ([z1|e], [z1|e], ..., [zn|e]) as entity representation, where [zi|e] is a distribution implemented as a vector. The entity representation is the feature for the classifier. For a fair comparison, our front-end classifier is identically the Logistic Regression in a one-versus-rest setting for multi-label classification. The evaluation is following (Neelakantan and Chang 2015), which applies the mean average precision (MAP) that is commonly used in multi-label classification. Type@N means the task is involved with N types to be predicted.\nResults. Evaluation results are reported in Tab.1, we could observe that: KSR outperforms all the baselines in a large degree, demonstrating the effectiveness of KSR. Entity types represent some level of semantics, thus the better result illustrates our method is indeed semantics-specific."}, {"heading": "Knowledge Graph Completion", "text": "Motivation. This task is a benchmark task, a.k.a \u201cLink Prediction\u201d, which concerns the identification ability for triples. Many NLP tasks could benefit from Link Prediction, such as relation extraction (Hoffmann et al. 2011).\nEvaluation Protocol. The same protocol used in previous studies, is adopted. First, for each testing triple (h, r, t), we replace the tail t (or the head h) with every entity e in the knowledge graph. Then, a probabilistic score of this corrupted triple is calculated with the score function fr(h, t). By ranking these scores in the ascending order, we then get the rank of the original triple. The evaluation metrics are the average of the ranks as Mean Rank and the proportion of testing triple whose rank is not larger than 10 (as HITS@10). This is called \u201cRaw\u201d setting. When we filter out the corrupted triples that exist in the training, validation, or test datasets, this is the \u201cFilter\u201d setting. If a corrupted triple exists in the knowledge graph, ranking it ahead the original triple is also correct. To eliminate this effect, the \u201cFilter\u201d setting is more preferred. In both settings, a higher HITS@10 and a lower Mean Rank mean better performance.\nResults. Evaluation results are reported in Tab.3, we could observe that: (1) KSR outperforms all the baselines substantially, justifying the effectiveness of our model. Theoretically, the effectiveness originates from the semanticsspecific modeling of KSR. (2) Within the same parameter scalability, compared to TransE, KSR improves 15% relatively while compared to TransR, KSR improves 27%. The comparison illustrates KSR prefer high-dimensional setting. Because one continuous variable such as age (10 \u223c 99), should be represented as many discrete variables (at most 90 boolean variables). Practically, sufficient dimensions are supposed to be provided for KSR to attempt the best performance."}, {"heading": "Semantic Analysis: Case Study", "text": "Suggested by our motivation, we conduct case study to analyze the semantics of our model. For brevity, we explore the FB15K datasets with KSR (n = 10, d = 3) which generates 10 features and for each feature assign three categories. Actually, FB15K is more complex than this setting, thus many minor features and categories would be suppressed. The consideration of this setting is to facilitate the research and presentation.\nFirstly, we analyze the specific semantics of each feature. We leverage the entity descriptions to calculate the joint probability of word w in the textual descriptions of an entity\nTable 2: Features with Significant Semantics in Semantic Analysis\nNo. Semantics Categories(Significant Words) 1 Film-Related Yes (Film, Director, Season, Writer), Yes (Awarded, Producer, Actor), No 2 American-Related No, No, Yes (United, States, Country, Population, Area) 3 Sports-Related No, No, Yes (Football, Club, League, Basketball, World Cup) 4 Art-Related Yes(Drama, Music, Voice, Acting), Yes (Film, Story, Screen Play), No 5 Persons-Related No, Multiple (Team, League, Roles), Single(She, Actress, Director, Singer) 6 Location-Related Yes (British, London, Canada, Europe, England), No, No\nand the inferred feature-category of that one. Then, we list the top words in each category for each feature. In this way, the semantics of the features and categories could be explicitly expressed. We directly list the results in Tab.2. There are six significant features, which are presented with categories and top words as evidence. This result strongly proves our motivation about KSR/KSA. Notably, the other four features are too puzzled to be recognized, because KSR is a latent space method such as LDA and most of these methods, would produce some contents that is hard to interpreted.\nSecondly, we present the semantic representations for three entities with different types: Film, Sports and Person. (1.) (Star Trek) =(Film:Related, American:Related, Sports:Unrelated, Person:Unrelated, Location :Unrelated, Drama:Related). Star Trek is a television series produced in American, Thus our semantic representations are fully satisfied. (2.) (Football Club Illichivets Mariupol) = (Film:Unrelated, American:Unrelated, Sports:Related, Art:Unrelated, Persons:Multiple, Location:Related). Its textual description is \u201cFootball Club Illichivets Mariupol is a Ukrainian professional football club based in Mariupol\u201d, which satisfies all the semantic representation. Note that, football club as a team or a league is composed by multiple persons, which is the reason for Multiple Persons Related. (3.) (Johnathan Glickman)=(File:Related, American:Unrelated, Sports:Unrelated, Art:Unrelated, Person:Single, Location:Unrelated). This person is a film producer, while we could not search out any nationality information about this man, so our semantic representation could still be totally correct.\nFinally, we also present the semantic representations for relation. For example, (Country Capital) = (Film:Unrelated, American:Unrelated, Sports:Unrelated, Art:Unrelated, Person:Unrelated, Location:Related). As a common sense, a capital is a location, not sports or art, thus our semantic representations are reasonable."}, {"heading": "Semantic Analysis: Statistical Justification", "text": "We conduct two statistical analysis in the same setting as \u201cCase Study\u201d subsection.\nFirstly, we randomly select 100 entities and manually check out the correctness of semantic represents by common knowledge. There are 68 entities, the semantic representations for which are totally correct and also 19 entities, the representations for which are incorrect at only one feature. There are just 13 entities, the corresponding representations of which are incorrect at more than one feature. Thus, the result proves the strong semantic expressive ability of KSR.\nFigure 3: The heatmap of correlations between knowledge features in Semantic Analysis. Dark color corresponds to the high-correlation and light-color indicates the weakcorrelation.\nSecondly, if two features (both with category Yes) cooccur in a semantic representation of an entity/relation, this knowledge element (entity/relation) contributes to the correlation between the two features. We make a statistics of the correlation and draw a heatmap in Fig.4, where the dark color corresponds to the high-correlation and the light color indicates the weak-correlation. Diving into the details, those Sports:Related entities would distribute among the world, so it is almost American:Unrelated. Thus, the block is light. Note that Location indicates the outside of American, so they are basically exclusive and share a light block. On the contrary, if a concept is Art:Related, it is very likely to be a film in FB15K. Hence, that block is dark. This result also justifies the semantic expressive ability of KSR."}, {"heading": "Conclusion", "text": "In this paper, we propose the knowledge semantic analysis (KSA) subject and then develop the Knowledge Semantic Representation (KSR) method for it, which is a two-level hierarchical generative process to explicitly represent knowledge in a semantic way. We also analyze our method from the perspectives of clustering and identification. Experimental results justify the effectiveness and the semantic expressive ability of our method."}], "references": [{"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Bollacker"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "Bollacker,? \\Q2008\\E", "shortCiteRegEx": "Bollacker", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Bordes"], "venue": "In Proceedings of the Twenty-fifth AAAI Conference on Artificial Intelligence", "citeRegEx": "Bordes,? \\Q2011\\E", "shortCiteRegEx": "Bordes", "year": 2011}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bordes,? \\Q2013\\E", "shortCiteRegEx": "Bordes", "year": 2013}, {"title": "T", "author": ["M. Fan", "Q. Zhou", "E. Chang", "Zheng"], "venue": "F.", "citeRegEx": "Fan et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantically smooth knowledge graph embedding", "author": ["Guo"], "venue": "In Proceedings of ACL", "citeRegEx": "Guo,? \\Q2015\\E", "shortCiteRegEx": "Guo", "year": 2015}, {"title": "Learning to represent knowledge graphs with gaussian embedding", "author": ["He"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "He,? \\Q2015\\E", "shortCiteRegEx": "He", "year": 2015}, {"title": "D", "author": ["R. Hoffmann", "C. Zhang", "X. Ling", "L. Zettlemoyer", "Weld"], "venue": "S.", "citeRegEx": "Hoffmann et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "G", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "Obozinski"], "venue": "R.", "citeRegEx": "Jenatton et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin"], "venue": "In Proceedings of the TwentyNinth AAAI Conference on Artificial Intelligence", "citeRegEx": "Lin,? \\Q2015\\E", "shortCiteRegEx": "Lin", "year": 2015}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Liu Lin", "Y. Sun 2015] Lin", "Z. Liu", "M. Sun"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "G", "author": ["Miller"], "venue": "A.", "citeRegEx": "Miller 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "and Chang", "author": ["A. Neelakantan"], "venue": "M.-W.", "citeRegEx": "Neelakantan and Chang 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Holographic embeddings of knowledge graphs. arXiv preprint arXiv:1510.04935", "author": ["Rosasco Nickel", "M. Poggio 2015] Nickel", "L. Rosasco", "T. Poggio"], "venue": null, "citeRegEx": "Nickel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2015}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Tresp Nickel", "M. Kriegel 2011] Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "In Proceedings of the 28th international conference on machine learning (ICML-11),", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "C", "author": ["R. Socher", "D. Chen", "Manning"], "venue": "D.; and Ng, A.", "citeRegEx": "Socher et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Knowledge graph and text jointly embedding", "author": ["Wang"], "venue": "Citeseer", "citeRegEx": "Wang,? \\Q2014\\E", "shortCiteRegEx": "Wang", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Wang,? \\Q2014\\E", "shortCiteRegEx": "Wang", "year": 2014}, {"title": "Knowledge base completion using embeddings and rules", "author": ["Wang Wang", "Q. Guo 2015] Wang", "B. Wang", "L. Guo"], "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "TransA: An adaptive approach for knowledge graph embedding", "author": ["Xiao"], "venue": "arXiv preprint arXiv:1509.05490", "citeRegEx": "Xiao,? \\Q2015\\E", "shortCiteRegEx": "Xiao", "year": 2015}, {"title": "2016a. From one point to a manifold: Knowledge graph embedding for precise link prediction", "author": ["Huang Xiao", "H. Zhu 2016a] Xiao", "M. Huang", "X. Zhu"], "venue": "In Proceedings of the 25th International Joint Conference on Artificial Intelligence", "citeRegEx": "Xiao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Transg : A generative model for knowledge graph embedding", "author": ["Huang Xiao", "H. Zhu 2016b] Xiao", "M. Huang", "X. Zhu"], "venue": "In Proceedings of the 29th international conference on computational linguistics", "citeRegEx": "Xiao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Representation learning of knowledge graphs with entity descriptions", "author": ["Xie"], "venue": null, "citeRegEx": "Xie,? \\Q2016\\E", "shortCiteRegEx": "Xie", "year": 2016}, {"title": "Aligning knowledge and text embeddings by entity descriptions", "author": ["Zhong"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Zhong,? \\Q2015\\E", "shortCiteRegEx": "Zhong", "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Knowledge representation is a critical topic in AI, and currently embedding as a key branch of knowledge representation takes the numerical form of entities and relations to joint the statistical models. However, most embedding methods merely concentrate on the triple fitting and ignore the explicit semantic expression, leading to an uninterpretable representation form. Thus, traditional embedding methods do not only degrade the performance, but also restrict many potential applications. For this end, this paper proposes a semantic representation method for knowledge graph (KSR), which imposes a two-level hierarchical generative process that globally extracts many aspects and then locally assigns a specific category in each aspect for every triple. Because both the aspects and categories are semantics-relevant, the collection of categories in each aspect is treated as the semantic representation of this triple. Extensive experiments justify our model outperforms other state-of-the-art baselines in a substantial", "creator": "LaTeX with hyperref package"}}}