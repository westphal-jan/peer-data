{"id": "1608.07905", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2016", "title": "Machine Comprehension Using Match-LSTM and Answer Pointer", "abstract": "machine comprehension knowledge reasoning is another importance component in the formal processing. ibm recently released dataset, and stanford instant answering dataset ( chet ), offers a large number given real questions before their questions answered by humans through manipulation. bot provides a challenging perspective for evaluating machine language algorithms, partly because compared with previous strategies, in squad the complexity don't come down a small set whose candidate estimates and they have narrower lengths. candidates propose an end - - - end neural architecture for developing software. the architecture specifies based involving database - binding, a layer partially provided previously for textual entailment, and pointer net, interactive linear - number - sequence model proposed by vinyals et al. ( 2015 ) to translate us output sample from be from the input processing. managers propose concrete assumptions of using pointer net method search process. merging experiments demonstrating that both of our two models substantially be the best found recently whereas rajpurkar et al. ( 1925 ) studying activation logs representing manually crafted neurons.", "histories": [["v1", "Mon, 29 Aug 2016 03:42:50 GMT  (428kb,D)", "http://arxiv.org/abs/1608.07905v1", "12 pages; 3 figures"], ["v2", "Mon, 7 Nov 2016 03:39:40 GMT  (389kb,D)", "http://arxiv.org/abs/1608.07905v2", "11 pages; 3 figures"]], "COMMENTS": "12 pages; 3 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["shuohang wang", "jing jiang"], "accepted": true, "id": "1608.07905"}, "pdf": {"name": "1608.07905.pdf", "metadata": {"source": "CRF", "title": "MACHINE COMPREHENSION USING MATCH-LSTM", "authors": ["ANSWER POINTER", "Shuohang Wang", "Jing Jiang"], "emails": ["shwang.2014@phdis.smu.edu.sg", "jingjiang@smu.edu.sg"], "sections": [{"heading": "1 INTRODUCTION", "text": "Machine comprehension of text is one of the ultimate goals of natural language processing. While the ability of a machine to understand text can be assessed in many different ways, in recent years, several benchmark datasets have been created to focus on answering questions as a way to evaluate machine comprehension (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2016; Weston et al., 2016; Rajpurkar et al., 2016). In this setup, typically the machine is first presented with a piece of text such as a news article or a story. The machine is then expected to answer one or multiple questions related to the text.\nDifferent benchmark datasets define the task in slightly different ways. In some datasets, a question is a multiple choice question, whose correct answer is to be chosen from a set of provided candidate answers (Richardson et al., 2013; Hill et al., 2016). Alternatively, in some other datasets, a question does not provide candidate answers and the machine is expected to answer the question using any token or sequence of tokens from the given text (Hermann et al., 2015; Rajpurkar et al., 2016). Presumably, questions without given candidate answers are more challenging because the candidate answer set essentially includes all possible tokens or token sequences from the text and is thus much larger. Furthermore, questions whose answers span multiple tokens are more challenging than those with single-token answers. The Stanford Question Answering Dataset (SQuAD) introduced recently by Rajpurkar et al. (2016) contains such challenging questions, i.e., there are no candidate answers provided and a correct answer to a question can be any sequence of tokens from the given text. Moreover, unlike some other datasets whose questions and answers were created automatically in Cloze style (Hermann et al., 2015; Hill et al., 2016), the questions and answers in SQuAD were created by humans through crowdsourcing, which makes the dataset more realistic. Given these advantages of the SQuAD dataset, in this paper, we focus on this new dataset to study machine comprehension of text. A sample piece of text and three of its associated questions are shown in Table 1.\nTraditional solutions to this kind of question answering tasks rely on NLP pipelines that involve multiple steps of linguistic analyses and feature engineering, including syntactic parsing, named entity\nar X\niv :1\n60 8.\n07 90\n5v 1\n[ cs\n.C L\n] 2\n9 A\nug 2\n01 6\nrecognition, question classification, semantic parsing, etc. Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension (Hermann et al., 2015; Hill et al., 2016; Yin et al., 2016; Kadlec et al., 2016; Cui et al., 2016). However, given the properties of previous machine comprehension datasets, existing end-to-end neural architectures for the task either rely on the candidate answers (Hill et al., 2016; Yin et al., 2016) or assume that the answer is a single token (Hermann et al., 2015; Kadlec et al., 2016; Cui et al., 2016), which make these methods unsuitable for the SQuAD dataset. In this paper, we propose a new end-to-end neural architecture to address the machine comprehension problem as defined in the SQuAD dataset.\nSpecifically, observing that in the SQuAD dataset many questions are paraphrases of sentences from the original text, we adopt a match-LSTM model that we developed earlier for textual entailment (Wang & Jiang, 2016). We further adopt the Pointer Net (Ptr-Net) model developed by Vinyals et al. (2015), which enables the predictions of tokens from the input sequence only rather than from a larger fixed vocabulary and thus allows us to generate answers that consist of multiple tokens from the original text. We propose two ways to apply the Ptr-Net model for our task. Experiments on the SQuAD dataset show that our two models both outperform the best performance reported by Rajpurkar et al. (2016).\nOur contributions can be summarized as follows: (1) We propose two new end-to-end neural network models for machine comprehension, which combine match-LSTM and Ptr-Net to handle the special properties of the SQuAD dataset. (2) We have achieved the state-of-the-art performance of an exact match score of 59.5% and an F1 score of 70.3% on the unseen test dataset. (3) Our further analyses with the models reveal some useful insights for further improving the method."}, {"heading": "2 METHOD", "text": "In this section, we first briefly review match-LSTM and Pointer Net. These two pieces of existing work lay the foundation of our method. We then present our end-to-end neural architecture for machine comprehension."}, {"heading": "2.1 MATCH-LSTM", "text": "In a recent work on learning natural language inference, we proposed a match-LSTM model for predicting textual entailment (Wang & Jiang, 2016). In textual entailment, two sentences are given where one is a premise and the other is a hypothesis. To predict whether the premise entails the hypothesis, the match-LSTM model goes through the tokens of the hypothesis sequentially. At each position of the hypothesis, attention mechanism is used to obtain a weighted vector representation of the premise. This weighted premise is then to be combined with a vector representation of the current token of the hypothesis and fed into an LSTM, which we call the match-LSTM. The matchLSTM essentially sequentially aggregates the matching of the attention-weighted premise to each token of the hypothesis and uses the aggregated matching result to make a final prediction."}, {"heading": "2.2 POINTER NET", "text": "Vinyals et al. (2015) proposed a Pointer Network (Ptr-Net) model to solve a special kind of problems where we want to generate an output sequence whose tokens must come from the input sequence. Instead of picking an output token from a fixed vocabulary, Ptr-Net uses attention mechanism as a pointer to select a position from the input sequence as an output symbol. The pointer mechanism has inspired some recent work on language processing (Gu et al., 2016; Kadlec et al., 2016). Here we adopt Ptr-Net in order to construct answers using tokens from the input text."}, {"heading": "2.3 OUR METHOD", "text": "Formally, the problem we are trying to solve can be formulated as follows. We are given a piece of text, which we refer to as a passage, and a question related to the passage. The passage is represented by matrix P \u2208 Rd\u00d7P , where P is the length (number of tokens) of the passage and d is the dimensionality of word embeddings. Similarly, the question is represented by matrix Q \u2208 Rd\u00d7Q where Q is the length of the question. Our goal is to identify a subsequence from the passage as the answer to the question.\nAs pointed out earlier, since the output tokens are from the input, we would like to adopt the Pointer Net for this problem. A straightforward way of applying Ptr-Net here is to treat an answer as a sequence of tokens from the input passage but ignore the fact that these tokens are consecutive in the original passage, because Ptr-Net does not make the consecutivity assumption. Specifically, we represent the answer as a sequence of integers a = (a1, a2, . . .), where each ai is an integer between 1 and P , indicating a certain position in the passage.\nAlternatively, if we want to ensure consecutivity, that is, if we want to ensure that we indeed select a subsequence from the passage as an answer, we can use the Ptr-Net to predict only the start and the end of an answer. In this case, the Ptr-Net only needs to select two tokens from the input passage, and all the tokens between these two tokens in the passage are treated as the answer. Specifically, we can represent the answer to be predicted as two integers a = (as, ae), where as an ae are integers between 1 and P .\nWe refer to the first setting above as a sequence model and the second setting above as a boundary model. For either model, we assume that a set of training examples in the form of triplets {(Pn,Qn,an)}Nn=1 are given. An overview of the two neural network models are shown in Figure 1. Both models consist of three layers: (1) An LSTM preprocessing layer that preprocesses the passage and the question using LSTMs. (2) A match-LSTM layer that tries to match the passage against the question. (3) An Answer Pointer (Ans-Ptr) layer that uses Ptr-Net to select a set of tokens from the passage as the answer. The difference between the two models only lies in the third layer.\nLSTM Preprocessing Layer\nThe purpose for the LSTM preprocessing layer is to incorporate contextual information into the representation of each token in the passage and the question. We use a standard one-directional LSTM (Hochreiter & Schmidhuber, 1997) to process the passage and the question separately, as shown below:\nHp = \u2212\u2212\u2212\u2192 LSTM(P), Hq = \u2212\u2212\u2212\u2192 LSTM(Q). (1)\nThe resulting matrices Hp \u2208 Rl\u00d7P and Hq \u2208 Rl\u00d7Q are hidden representations of the passage and the question, where l is the dimensionality of the hidden vectors. In other words, the ith column vector hpi (or h q i ) in H\np (or Hq) represents the ith token in the passage (or the question) together with some contextual information from the left.\nMatch-LSTM Layer\nWe apply the match-LSTM model (Wang & Jiang, 2016) proposed for textual entailment to our machine comprehension problem by treating the question as a premise and the passage as a hypothesis. The match-LSTM sequentially goes through the passage. At position i of the passage, it first uses the standard word-by-word attention mechanism to obtain attention weight vector \u2212\u2192\u03b1 i \u2208 RQ as follows:\n\u2212\u2192 Gi = tanh(WqHq + (Wph p i + W r\u2212\u2192h ri\u22121 + bp)\u2297 eQ), \u2212\u2192\u03b1 i = softmax(w\u1d40 \u2212\u2192 Gi + b\u2297 eQ), (2)\nwhere Wq,Wp,Wr \u2208 Rl\u00d7l, bp,w \u2208 Rl and b \u2208 R are parameters to be learned, \u2212\u2192 h ri\u22121 \u2208 Rl is the hidden vector of the one-directional match-LSTM (to be explained below) at position i\u2212 1, and the outer product (\u00b7 \u2297 eQ) produces a matrix or row vector by repeating the vector or scalar on the left for Q times.1\nEssentially, the resulting attention weight \u2212\u2192\u03b1 i,j above indicates the degree of matching between the ith token in the passage with the jth token in the question. Next, we use the attention weight vector\u2212\u2192\u03b1 i to obtain a weighted version of the question and combine it with the current token of the passage to form a vector \u2212\u2192z i:\n\u2212\u2192z i = [\nhpi Hq\u2212\u2192\u03b1 \u1d40i\n] . (3)\nThis vector \u2212\u2192z i is fed into a standard one-directional LSTM to form our so-called match-LSTM: \u2212\u2192 h ri = \u2212\u2212\u2212\u2192 LSTM(\u2212\u2192z i, \u2212\u2192 h ri\u22121), (4) where \u2212\u2192 h ri \u2208 Rl.\nWe further build a similar match-LSTM in the reverse direction. The purpose is to obtain a representation that encodes the contexts from both directions for each token in the passage. To build this reverse match-LSTM, we first define\n\u2190\u2212 Gi = tanh(WqHq + (Wph p i + W r\u2190\u2212h ri+1 + bp)\u2297 eQ), \u2190\u2212\u03b1 i = softmax(w\u1d40 \u2190\u2212 Gi + b\u2297 eQ). (5)\n1Following Wang & Jiang (2016), we add a special token NULL to the end of the question so that words in the passage that do not match any word in the question can be matched to this NULL. So Q here is actually the number of tokens in the question plus 1.\nNote that the parameters here (Wq, Wp, Wr, bp, w and b) are the same as used in Eqn. (2). We then define\u2190\u2212z i in a similar way and finally define \u2190\u2212 h ri to be the hidden representation at position i produced by the match-LSTM in the reverse direction.\nLet \u2212\u2192 Hr \u2208 Rl\u00d7P represent the hidden states [ \u2212\u2192 h r1, \u2212\u2192 h r2, . . . , \u2212\u2192 h rP ] and \u2190\u2212 Hr \u2208 Rl\u00d7P represent [ \u2190\u2212 h r1, \u2190\u2212 h r2, . . . , \u2190\u2212 h rP ]. We define H r \u2208 R2l\u00d7P as the concatenation of the two:\nHr = [\u2212\u2192 Hr \u2190\u2212 Hr ] . (6)\nAnswer Pointer Layer\nThe top layer, the Answer Pointer (Ans-Ptr) layer, is motivated by the Pointer Net introduced by Vinyals et al. (2015). This layer uses the sequence Hr as input. Recall that we have two different models: The sequence model produces a sequence of answer tokens but these tokens may not be consecutive in the original passage. The boundary model produces only the start token and the end token of the answer, and then all the tokens between these two in the original passage are considered to be the answer. We now explain the two models separately.\nThe Sequence Model: Recall that in the sequence model, the answer is represented by a sequence of integers a = (a1, a2, . . .) indicating the positions of the selected tokens in the original passage. The Ans-Ptr layer models the generation of these integers in a sequential manner. Because the length of an answer is not fixed, in order to stop generating answer tokens at certain point, we allow each ak to take up an integer value between 1 and P + 1, where P + 1 is a special value indicating the end of the answer. Once ak is set to be P + 1, the generation of the answer stops.\nIn order to generate the kth answer token indicated by ak, first, the attention mechanism is used again to obtain an attention weight vector \u03b2k \u2208 R(P+1), where \u03b2k,j (1 \u2264 j \u2264 P + 1) is the probability of selecting the jth token from the passage as the kth token in the answer, and \u03b2k,(P+1) is the probability of stopping the answer generation at position k. \u03b2k is modeled as follows:\nFk = tanh(VH\u0303r + (Wahak\u22121 + b a)\u2297 e(P+1)), (7) \u03b2k = softmax(v\u1d40Fk + c\u2297 e(P+1)), (8)\nwhere H\u0303r \u2208 R2l\u00d7(P+1) is the concatenation of Hr with a zero vector, defined as H\u0303r = [Hr;0], V \u2208 Rl\u00d72l,Wa \u2208 Rl\u00d7l, ba,v \u2208 Rl and c \u2208 R are parameters to be learned, (\u00b7 \u2297 e(P+1)) follows the same definition as before, and hak\u22121 \u2208 Rl is the hidden vector at position k \u2212 1 of an answer LSTM as defined below:\nhak = \u2212\u2212\u2212\u2192 LSTM(H\u0303r\u03b2\u1d40k ,h a k\u22121). (9)\nWe can then model the probability of generating the answer sequence as p(a|Hr) = \u220f k p(ak|a1, a2, . . . , ak\u22121,Hr), (10)\nand\np(ak = j|a1, a2, . . . , ak\u22121,Hr) = \u03b2k,j . (11)\nTo train the model, we minimize the following loss function based on the training examples:\n\u2212 N\u2211\nn=1\nlog p(an|Pn,Qn). (12)\nThe Boundary Model: The boundary model works in a way very similar to the sequence model above, except that instead of predicting a sequence of indices a1, a2, . . ., we only need to predict two indices as and ae. So the main difference from the sequence model above is that in the boundary model we do not need to add the zero padding to Hr, and the probability of generating an answer is simply modeled as\np(a|Hr) = p(as|Hr)p(ae|as,Hr). (13)"}, {"heading": "3 EXPERIMENTS", "text": "In this section, we present our experiment results and perform some analyses to better understand how our models works."}, {"heading": "3.1 DATA", "text": "We use the Stanford Question Answering Dataset (SQuAD) v1.02 to conduct our experiments. Passages in SQuAD come from 536 articles from Wikipedia covering a wide range of topics. Each passage is a single paragraph from a Wikipedia article, and each passage has around 5 questions associated with it. In total, there are 23,215 passages and 107,785 questions. The data has been split into a training set (with 87,636 question-answer pairs), a development set (with 10,600 questionanswer pairs) and a test set that is not released. To evaluate our model on the test set, we had to submit our trained model to Rajpurkar et al. (2016), who helped us obtain the performance on the test set."}, {"heading": "3.2 EXPERIMENT SETTINGS", "text": "We first tokenize all the passages, questions and answers. The resulting vocabulary contains 116,576 unique words. We use word embeddings from GloVe (Pennington et al., 2014) to initialize the model. Words not found in GloVe are initialized as zero vectors. The word embeddings are not updated during the training of the model.\nThe dimensionality l of the hidden layers is fixed to be 150. We use ADAMAX (Kingma & Ba, 2015) with the coefficients \u03b21 = 0.9 and \u03b22 = 0.999 to optimize the model. Each update is computed through a minibatch of 30 instances. We do not use L2-regularization. We use the development set to help tune the hyper-parameters, which include the learning rate and dropout.\nThe performance is measured by two metrics: percentage of exact match with the ground truth answers, and word-level F1 score when comparing the tokens in the predicted answers with the tokens in the ground truth answers. Note that in the development set and the test set each question has around three ground truth answers. F1 scores with the best matching answers are used to compute the average F1 score."}, {"heading": "3.3 RESULTS", "text": "The results of our models as well as the results of two baselines given by Rajpurkar et al. (2016) are shown in Table 2. We can see that both of our two models have clearly outperformed the logistic regression model by Rajpurkar et al. (2016), which relies on carefully designed features. Furthermore, our boundary model has outperformed the sequence model, achieving an exact match score of 59.5% and an F1 score of 70.3%. In particular, in terms of the exact match score, the boundary model has a clear advantage over the sequence model. The improvement of our models over the logistic regression model shows that our end-to-end neural network models without much feature engineering are very effective on this task and this dataset."}, {"heading": "3.4 FURTHER ANALYSES", "text": "To better understand the strengths and weaknesses of our models, we perform some further analyses of the results below.\nFirst, we suspect that longer answers are harder to predict. To verify this hypothesis, we plot out the performance in terms of both exact match and F1 score with respect to the answer length on the development set. We also show the number of question-answer pairs with different lengths of answers. The two plots are shown in the first row of Figure 2. We can see from Plot (2) on the upper right corner of Figure 2 that there are more short answers than long answers in the dataset. And from Plot (1) on the upper left corner, we can see that our models work much better for short answers than for long answers. For example, for questions whose answers contain more than 9 tokens, the F1 score of the boundary model drops to around 56% and the exact match score drops to only around 30%, compared to the F1 score and exact match score of close to 72% and 66%, respectively, for questions with single-token answers.\nNext, we analyze the performance of our models on different groups of questions. We use a crude way to split the questions into different groups based on a set of question words we have defined, including \u201cwhat,\u201d \u201chow,\u201d \u201cwho,\u201d \u201cwhen,\u201d \u201cwhich,\u201d \u201cwhere,\u201d and \u201cwhy.\u201d These different question words roughly refer to questions with different types of answers. For example, \u201cwhen\u201d questions look for temporal expressions as answers, whereas \u201cwhere\u201d questions look for locations as answers. We plot the performance on different groups of questions in the middle row of Figure 2. We also plot the numbers of different groups of questions in the middle row. As we can see from Plot (3) in Figure 2, our models work the best for \u201cwhen\u201d questions. This may be because in this dataset temporal expressions are relatively easier to recognize. Other groups of questions whose answers are noun phrases, such as \u201cwhat\u201d questions, \u201cwhich\u201d questions and \u201cwhere\u201d questions, also get relatively better results. On the other hand, \u201cwhy\u201d questions are the hardest to answer. This is not surprising because the answers to \u201cwhy\u201d questions can be very diverse, and they are not restricted to any certain type of phrases.\nWe observe that for many questions, not only the first question word but also the second question word is very important in characterizing the nature of the question. For example, the word \u201cmany\u201d in questions starting with \u201chow many\u201d indicates that the answers should be numbers. Therefore, we further select a set of frequent bigrams that we have observed in the beginnings of questions, which presumably characterize the nature of the questions at a finer granularity. Again we plot the performance on different groups of questions based on these bigrams as well as the numbers of questions in these different groups in the bottom row of Figure 2. We can see that \u201chow many\u201d questions have relatively good performance, which makes sense because their answers are mostly numbers and thus easier to recognize.\nFinally, we would like to check whether the attention mechanism used in the match-LSTM layer is effective in helping the model locate the answer. We show the attention weights \u03b1 in Figure 3. In the figure the darker the color is the higher the weight is. We can see that some words have been well aligned based on the attention weights. For example, the word \u201cGerman\u201d in the passage is aligned well to the word \u201clanguage\u201d in the first question, and the model successfully predicts \u201cGerman\u201d as the answer to the question. For the question word \u201cwho\u201d in the second question, the word \u201cteacher\u201d actually receives relatively higher attention weight, and the model has predicted the phrase \u201cMartin Sekulic\u201d after that as the answer, which is correct. For the last question that starts with \u201cwhy\u201d, the attention weights are more evenly distributed and it is not clear which words have been aligned to \u201cwhy\u201d."}, {"heading": "4 RELATED WORK", "text": "Machine comprehension of text has gained much attention in recent years, and increasingly researchers are building data-drive, end-to-end neural network models for the task. We will first review the recently released datasets and then some end-to-end models on this task.\n2Rajpurkar et al. (2016) just released a v1.1 of the same dataset. We have yet to use the new version of the data to train our models."}, {"heading": "4.1 DATASETS", "text": "A number of datasets for studying machine comprehension were created in Cloze style by removing a single token from a sentence in the original corpus. Questions created in this way are not real questions that start with \u201cwhen,\u201d \u201cwhere,\u201d etc. For example, Hermann et al. (2015) created questions in Cloze style from CNN and Daily Mail highlights, which are short summaries of the major points of news articles from CNN and Daily Mail. The task is to predict a missing token removed from a highlight based on the corresponding news article. Hill et al. (2016) created the Children\u2019s Book Test dataset, which is based on children\u2019s stories. Each query is the 21st sentence from a story with one word removed. The task is to use the first 20 sentences to predict the missing word in the query. Cui et al. (2016) released two similar datasets in Chinese, the People Daily dataset and the Children\u2019s Fairy Tale dataset, following the work by Hermann et al. (2015) and by Hill et al. (2016).\nInstead of creating questions in Cloze style, a number of other datasets rely on human annotators to create real questions. Richardson et al. (2013) created the well-known MCTest dataset, which is based on short fictional stories and has candidate answers provided for each question. Tapaswi et al. (2016) created the MovieQA dataset, which uses multiple sources of information including video clips, plots, subtitles, scripts and DVS of movies for question answering. In this dataset, there are also candidate answers provided. Similar to these two datasets, the SQuAD dataset (Rajpurkar et al., 2016) was also created by human annotators. Different from the previous two, however, the SQuAD dataset does not provide candidate answers, and thus all possible subsequences from the given passage have to be considered as candidate answers.\nBesides the datasets above, there are also a few other datasets created for machine comprehension, but they are quite different from the datasets above in nature. Hewlett et al. (2016) created a WikiReading dataset based on Wikipedia articles and the open knowledge base Wikidata. Although the dataset was also created for natural language understanding, the queries in this dataset are not complete natural language sentences but properties such as \u201cgender,\u201d \u201ccountry\u201d and \u201cdate of birth,\u201d and in total the dataset has 884 unique properties. It is therefore quite different from those datasets in which the questions are long sentences and have much more variations. Weston et al. (2016) created a question answering dataset that contains questions requiring different levels of intelligence such as fact chaining, counting, deduction and induction. The text was generated automatically based on a simple automated grammar and a small vocabulary, but the questions require higher levels of intelligence to answer than the aforementioned datasets."}, {"heading": "4.2 END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION", "text": "There have been a number of studies proposing end-to-end neural network models for machine comprehension. A common approach is to use recurrent neural networks (RNNs) to process the given text and the question in order to predict or generate the answers (Hermann et al., 2015). Attention mechanism is also widely used on top of RNNs in order to match the question with the given passage (Hermann et al., 2015; Chen et al., 2016). Given that answers often come from the given passage, Pointer Network has been adopted in a few studies in order to copy tokens from the given passage as answers (Kadlec et al., 2016; Trischler et al., 2016). Compared with existing work, we use match-LSTM to match a question and a given passage, and we use Pointer Network in a different way such that we can generate answers that contain multiple tokens from the given passage.\nMemory Networks (Weston et al., 2015) have also been applied to machine comprehension (Sukhbaatar et al., 2015; Kumar et al., 2016; Hill et al., 2016), but its scalability when applied to a large dataset is still an issue. In this work, we did not consider memory networks for the SQuAD dataset."}, {"heading": "5 CONCLUSIONS", "text": "In this paper, we proposed a three-layer neural network architecture to solve the machine comprehension problem defined in the Stanford Question Answering (SQuAD) dataset. We developed two models for the problem, both making use of match-LSTM and Pointer Network. Experiments on the SQuAD dataset showed that both of our models could substantially outperform the best performance achieved by (Rajpurkar et al., 2016). In particular, our second model, the boundary model, could achieve an exact match score of 59.5% and an F1 score of 70.3% on the test dataset. Our further analyses showed that our models work better for questions with short answers and certain types of questions such as \u201cwhen\u201d questions.\nIn the future, we plan to look further into the different types of questions and focus on those questions which currently have low performance, such as the \u201cwhy\u2019 questions. We also plan to test how our models could be applied to other machine comprehension datasets."}, {"heading": "6 ACKNOWLEDGMENTS", "text": "We thank Pranav Rajpurkar for testing our model on the hidden test dataset and Percy Liang for helping us with the Dockerfile for Codalab."}], "references": [{"title": "A thorough examination of the CNN/Daily Mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning"], "venue": "In Proceedings of the Conference on Association for Computational Linguistics,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Consensus attention-based neural networks for chinese reading comprehension", "author": ["Yiming Cui", "Ting Liu", "Zhipeng Chen", "Shijin Wang", "Guoping Hu"], "venue": "In arXiv preprint arXiv:1607.02250,", "citeRegEx": "Cui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "venue": "In Proceedings of the Conference on Association for Computational Linguistics,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Proceedings of the Conference on Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "WIKIREADING: A novel large-scale language understanding task over wikipedia", "author": ["Daniel Hewlett", "Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot"], "venue": "In Proceedings of the Conference on Association for Computational Linguistics,", "citeRegEx": "Hewlett et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hewlett et al\\.", "year": 2016}, {"title": "The Goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "venue": "In Proceedings of the Conference on Association for Computational Linguistics,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Kumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "GloVe: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SQuAD: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Proceedings of the Conference on Advances in neural information processing systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "MovieQA: Understanding stories in movies through question-answering", "author": ["Makarand Tapaswi", "Yukun Zhu", "Rainer Stiefelhagen", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Tapaswi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tapaswi et al\\.", "year": 2016}, {"title": "Natural language comprehension with the EpiReader", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Kaheer Suleman"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Learning natural language inference with LSTM", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "In Proceedings of the Conference on the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Towards AI-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Weston et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2016}, {"title": "Attention-based convolutional neural network for machine comprehension", "author": ["Wenpeng Yin", "Sebastian Ebert", "Hinrich Sch\u00fctze"], "venue": "arXiv preprint arXiv:1602.04341,", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": "Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. (2016) using logistic regression and manually crafted features.", "startOffset": 103, "endOffset": 127}, {"referenceID": 12, "context": "While the ability of a machine to understand text can be assessed in many different ways, in recent years, several benchmark datasets have been created to focus on answering questions as a way to evaluate machine comprehension (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2016; Weston et al., 2016; Rajpurkar et al., 2016).", "startOffset": 227, "endOffset": 338}, {"referenceID": 3, "context": "While the ability of a machine to understand text can be assessed in many different ways, in recent years, several benchmark datasets have been created to focus on answering questions as a way to evaluate machine comprehension (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2016; Weston et al., 2016; Rajpurkar et al., 2016).", "startOffset": 227, "endOffset": 338}, {"referenceID": 5, "context": "While the ability of a machine to understand text can be assessed in many different ways, in recent years, several benchmark datasets have been created to focus on answering questions as a way to evaluate machine comprehension (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2016; Weston et al., 2016; Rajpurkar et al., 2016).", "startOffset": 227, "endOffset": 338}, {"referenceID": 17, "context": "While the ability of a machine to understand text can be assessed in many different ways, in recent years, several benchmark datasets have been created to focus on answering questions as a way to evaluate machine comprehension (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2016; Weston et al., 2016; Rajpurkar et al., 2016).", "startOffset": 227, "endOffset": 338}, {"referenceID": 11, "context": "While the ability of a machine to understand text can be assessed in many different ways, in recent years, several benchmark datasets have been created to focus on answering questions as a way to evaluate machine comprehension (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2016; Weston et al., 2016; Rajpurkar et al., 2016).", "startOffset": 227, "endOffset": 338}, {"referenceID": 12, "context": "In some datasets, a question is a multiple choice question, whose correct answer is to be chosen from a set of provided candidate answers (Richardson et al., 2013; Hill et al., 2016).", "startOffset": 138, "endOffset": 182}, {"referenceID": 5, "context": "In some datasets, a question is a multiple choice question, whose correct answer is to be chosen from a set of provided candidate answers (Richardson et al., 2013; Hill et al., 2016).", "startOffset": 138, "endOffset": 182}, {"referenceID": 3, "context": "Alternatively, in some other datasets, a question does not provide candidate answers and the machine is expected to answer the question using any token or sequence of tokens from the given text (Hermann et al., 2015; Rajpurkar et al., 2016).", "startOffset": 194, "endOffset": 240}, {"referenceID": 11, "context": "Alternatively, in some other datasets, a question does not provide candidate answers and the machine is expected to answer the question using any token or sequence of tokens from the given text (Hermann et al., 2015; Rajpurkar et al., 2016).", "startOffset": 194, "endOffset": 240}, {"referenceID": 3, "context": "Moreover, unlike some other datasets whose questions and answers were created automatically in Cloze style (Hermann et al., 2015; Hill et al., 2016), the questions and answers in SQuAD were created by humans through crowdsourcing, which makes the dataset more realistic.", "startOffset": 107, "endOffset": 148}, {"referenceID": 5, "context": "Moreover, unlike some other datasets whose questions and answers were created automatically in Cloze style (Hermann et al., 2015; Hill et al., 2016), the questions and answers in SQuAD were created by humans through crowdsourcing, which makes the dataset more realistic.", "startOffset": 107, "endOffset": 148}, {"referenceID": 3, "context": "Alternatively, in some other datasets, a question does not provide candidate answers and the machine is expected to answer the question using any token or sequence of tokens from the given text (Hermann et al., 2015; Rajpurkar et al., 2016). Presumably, questions without given candidate answers are more challenging because the candidate answer set essentially includes all possible tokens or token sequences from the text and is thus much larger. Furthermore, questions whose answers span multiple tokens are more challenging than those with single-token answers. The Stanford Question Answering Dataset (SQuAD) introduced recently by Rajpurkar et al. (2016) contains such challenging questions, i.", "startOffset": 195, "endOffset": 661}, {"referenceID": 3, "context": "Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension (Hermann et al., 2015; Hill et al., 2016; Yin et al., 2016; Kadlec et al., 2016; Cui et al., 2016).", "startOffset": 224, "endOffset": 322}, {"referenceID": 5, "context": "Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension (Hermann et al., 2015; Hill et al., 2016; Yin et al., 2016; Kadlec et al., 2016; Cui et al., 2016).", "startOffset": 224, "endOffset": 322}, {"referenceID": 18, "context": "Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension (Hermann et al., 2015; Hill et al., 2016; Yin et al., 2016; Kadlec et al., 2016; Cui et al., 2016).", "startOffset": 224, "endOffset": 322}, {"referenceID": 7, "context": "Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension (Hermann et al., 2015; Hill et al., 2016; Yin et al., 2016; Kadlec et al., 2016; Cui et al., 2016).", "startOffset": 224, "endOffset": 322}, {"referenceID": 1, "context": "Recently, with the advances of applying neural network models in NLP, there has been much interest in building end-to-end neural architectures for various NLP tasks, including several pieces of work on machine comprehension (Hermann et al., 2015; Hill et al., 2016; Yin et al., 2016; Kadlec et al., 2016; Cui et al., 2016).", "startOffset": 224, "endOffset": 322}, {"referenceID": 5, "context": "However, given the properties of previous machine comprehension datasets, existing end-to-end neural architectures for the task either rely on the candidate answers (Hill et al., 2016; Yin et al., 2016) or assume that the answer is a single token (Hermann et al.", "startOffset": 165, "endOffset": 202}, {"referenceID": 18, "context": "However, given the properties of previous machine comprehension datasets, existing end-to-end neural architectures for the task either rely on the candidate answers (Hill et al., 2016; Yin et al., 2016) or assume that the answer is a single token (Hermann et al.", "startOffset": 165, "endOffset": 202}, {"referenceID": 3, "context": ", 2016) or assume that the answer is a single token (Hermann et al., 2015; Kadlec et al., 2016; Cui et al., 2016), which make these methods unsuitable for the SQuAD dataset.", "startOffset": 52, "endOffset": 113}, {"referenceID": 7, "context": ", 2016) or assume that the answer is a single token (Hermann et al., 2015; Kadlec et al., 2016; Cui et al., 2016), which make these methods unsuitable for the SQuAD dataset.", "startOffset": 52, "endOffset": 113}, {"referenceID": 1, "context": ", 2016) or assume that the answer is a single token (Hermann et al., 2015; Kadlec et al., 2016; Cui et al., 2016), which make these methods unsuitable for the SQuAD dataset.", "startOffset": 52, "endOffset": 113}, {"referenceID": 11, "context": "Experiments on the SQuAD dataset show that our two models both outperform the best performance reported by Rajpurkar et al. (2016).", "startOffset": 107, "endOffset": 131}, {"referenceID": 2, "context": "The pointer mechanism has inspired some recent work on language processing (Gu et al., 2016; Kadlec et al., 2016).", "startOffset": 75, "endOffset": 113}, {"referenceID": 7, "context": "The pointer mechanism has inspired some recent work on language processing (Gu et al., 2016; Kadlec et al., 2016).", "startOffset": 75, "endOffset": 113}, {"referenceID": 11, "context": "To evaluate our model on the test set, we had to submit our trained model to Rajpurkar et al. (2016), who helped us obtain the performance on the test set.", "startOffset": 77, "endOffset": 101}, {"referenceID": 10, "context": "We use word embeddings from GloVe (Pennington et al., 2014) to initialize the model.", "startOffset": 34, "endOffset": 59}, {"referenceID": 11, "context": "The results of our models as well as the results of two baselines given by Rajpurkar et al. (2016) are shown in Table 2.", "startOffset": 75, "endOffset": 99}, {"referenceID": 11, "context": "The results of our models as well as the results of two baselines given by Rajpurkar et al. (2016) are shown in Table 2. We can see that both of our two models have clearly outperformed the logistic regression model by Rajpurkar et al. (2016), which relies on carefully designed features.", "startOffset": 75, "endOffset": 243}, {"referenceID": 2, "context": "For example, Hermann et al. (2015) created questions in Cloze style from CNN and Daily Mail highlights, which are short summaries of the major points of news articles from CNN and Daily Mail.", "startOffset": 13, "endOffset": 35}, {"referenceID": 2, "context": "For example, Hermann et al. (2015) created questions in Cloze style from CNN and Daily Mail highlights, which are short summaries of the major points of news articles from CNN and Daily Mail. The task is to predict a missing token removed from a highlight based on the corresponding news article. Hill et al. (2016) created the Children\u2019s Book Test dataset, which is based on children\u2019s stories.", "startOffset": 13, "endOffset": 316}, {"referenceID": 1, "context": "Cui et al. (2016) released two similar datasets in Chinese, the People Daily dataset and the Children\u2019s Fairy Tale dataset, following the work by Hermann et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "Cui et al. (2016) released two similar datasets in Chinese, the People Daily dataset and the Children\u2019s Fairy Tale dataset, following the work by Hermann et al. (2015) and by Hill et al.", "startOffset": 0, "endOffset": 168}, {"referenceID": 1, "context": "Cui et al. (2016) released two similar datasets in Chinese, the People Daily dataset and the Children\u2019s Fairy Tale dataset, following the work by Hermann et al. (2015) and by Hill et al. (2016).", "startOffset": 0, "endOffset": 194}, {"referenceID": 11, "context": "Similar to these two datasets, the SQuAD dataset (Rajpurkar et al., 2016) was also created by human annotators.", "startOffset": 49, "endOffset": 73}, {"referenceID": 11, "context": "Richardson et al. (2013) created the well-known MCTest dataset, which is based on short fictional stories and has candidate answers provided for each question.", "startOffset": 0, "endOffset": 25}, {"referenceID": 11, "context": "Richardson et al. (2013) created the well-known MCTest dataset, which is based on short fictional stories and has candidate answers provided for each question. Tapaswi et al. (2016) created the MovieQA dataset, which uses multiple sources of information including video clips, plots, subtitles, scripts and DVS of movies for question answering.", "startOffset": 0, "endOffset": 182}, {"referenceID": 4, "context": "Hewlett et al. (2016) created a WikiReading dataset based on Wikipedia articles and the open knowledge base Wikidata.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "Hewlett et al. (2016) created a WikiReading dataset based on Wikipedia articles and the open knowledge base Wikidata. Although the dataset was also created for natural language understanding, the queries in this dataset are not complete natural language sentences but properties such as \u201cgender,\u201d \u201ccountry\u201d and \u201cdate of birth,\u201d and in total the dataset has 884 unique properties. It is therefore quite different from those datasets in which the questions are long sentences and have much more variations. Weston et al. (2016) created a question answering dataset that contains questions requiring different levels of intelligence such as fact chaining, counting, deduction and induction.", "startOffset": 0, "endOffset": 526}, {"referenceID": 3, "context": "A common approach is to use recurrent neural networks (RNNs) to process the given text and the question in order to predict or generate the answers (Hermann et al., 2015).", "startOffset": 148, "endOffset": 170}, {"referenceID": 3, "context": "Attention mechanism is also widely used on top of RNNs in order to match the question with the given passage (Hermann et al., 2015; Chen et al., 2016).", "startOffset": 109, "endOffset": 150}, {"referenceID": 0, "context": "Attention mechanism is also widely used on top of RNNs in order to match the question with the given passage (Hermann et al., 2015; Chen et al., 2016).", "startOffset": 109, "endOffset": 150}, {"referenceID": 7, "context": "Given that answers often come from the given passage, Pointer Network has been adopted in a few studies in order to copy tokens from the given passage as answers (Kadlec et al., 2016; Trischler et al., 2016).", "startOffset": 162, "endOffset": 207}, {"referenceID": 15, "context": "Given that answers often come from the given passage, Pointer Network has been adopted in a few studies in order to copy tokens from the given passage as answers (Kadlec et al., 2016; Trischler et al., 2016).", "startOffset": 162, "endOffset": 207}, {"referenceID": 13, "context": ", 2015) have also been applied to machine comprehension (Sukhbaatar et al., 2015; Kumar et al., 2016; Hill et al., 2016), but its scalability when applied to a large dataset is still an issue.", "startOffset": 56, "endOffset": 120}, {"referenceID": 9, "context": ", 2015) have also been applied to machine comprehension (Sukhbaatar et al., 2015; Kumar et al., 2016; Hill et al., 2016), but its scalability when applied to a large dataset is still an issue.", "startOffset": 56, "endOffset": 120}, {"referenceID": 5, "context": ", 2015) have also been applied to machine comprehension (Sukhbaatar et al., 2015; Kumar et al., 2016; Hill et al., 2016), but its scalability when applied to a large dataset is still an issue.", "startOffset": 56, "endOffset": 120}, {"referenceID": 11, "context": "Experiments on the SQuAD dataset showed that both of our models could substantially outperform the best performance achieved by (Rajpurkar et al., 2016).", "startOffset": 128, "endOffset": 152}], "year": 2016, "abstractText": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al. (2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our task. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. (2016) using logistic regression and manually crafted features.", "creator": "LaTeX with hyperref package"}}}