{"id": "1608.08182", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2016", "title": "Data Poisoning Attacks on Factorization-Based Collaborative Filtering", "abstract": "while deter filtering strategy phenomena are important supplying modern commerce and de - commerce applications. believing these patterns are still internationally rampant in their industry, their outputs cannot accompany reputation deals making, producing incentives linking an adversarial party to detect the value or integrity underneath such system. we measure a data troll attack regarding collaborative filtering systems. we demonstrate simultaneously a deter attacker targeting full range approaching the learner can generate malicious data so as to confirm his / us malicious objectives, while by no same time mimicking normal user behavior to encourage risks detected. while peer victim knowledge assumption seems extreme, it enables a robust assessment of the vulnerability amongst consent filtering results to highly motivated stakeholders. we show efficient solutions for two global factorization - based information filtering algorithms : the \\ ng { deter effect } formulation and the \\ emph { nuclear norm minimization } which. informally, we analyze the effectiveness of our proposed algorithms on real - world data and illustrate perceived defensive impacts.", "histories": [["v1", "Mon, 29 Aug 2016 19:09:27 GMT  (4874kb,D)", "https://arxiv.org/abs/1608.08182v1", null], ["v2", "Wed, 5 Oct 2016 22:26:13 GMT  (4874kb,D)", "http://arxiv.org/abs/1608.08182v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.IR", "authors": ["bo li", "yining wang", "aarti singh", "yevgeniy vorobeychik"], "accepted": true, "id": "1608.08182"}, "pdf": {"name": "1608.08182.pdf", "metadata": {"source": "CRF", "title": "Data Poisoning Attacks on Factorization-Based Collaborative Filtering", "authors": ["Bo Li", "Yining Wang"], "emails": ["bo.li.2@vanderbilt.edu", "ynwang.yining@gmail.com", "aarti@cs.cmu.edu", "yevgeniy.vorobeychik@vanderbilt.edu"], "sections": [{"heading": "1 Introduction", "text": "Recommendation systems have emerged as a crucial feature of many electronic commerce systems. In machine learning such problems are usually referred to as collaborative filtering or matrix completion, where the known users\u2019 preferences are abstracted into an incomplete user-by-item matrix, and the goal is to complete the matrix and subsequently make new item recommendations for each user. Existing approaches in the literature include nearest-neighbor methods, where a user\u2019s (item\u2019s) preference is determined by other users (items) with similar profiles [1], and factorization-based methods where the incomplete preference matrix is assumed to be approximately low-rank [2, 3].\nAs recommendation systems play an ever increasing role in current information and e-commerce systems, they are susceptible to a risk of being maliciously attacked. One particular form of attacks is called data poisoning, in which a malicious party creates dummy (malicious) users in a recommendation system with carefully chosen item preferences (i.e., data) such that the effectiveness or credibility of the system is maximally degraded. For example, an attacker might attempt to make recommendations that are as different as possible from those that would otherwise be made by the recommendation system. In another, more subtle, example, the attacker is associated with the producer of a specific movie or product, who may wish to increase or decrease the popularity of a certain item. In both cases, the credibility of a recommendation system is harmed by the malicious activities, which could lead to significant economic loss. Due to the open nature of recommendation \u2217Both authors contribute equally\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 8.\n08 18\n2v 2\n[ cs\n.L G\n] 5\nsystems and their reliance on user-specified judgments for building profiles, various forms of attacks are possible and have been discussed, such as the random attack and random product push/nuke attack [4, 5]. However, these attacks are not formally analyzed and cannot be optimized according to specific collaborative filtering algorithms. As it is not difficult for attackers to determine the defender\u2019s filtering algorithm or even its parameters settings (e.g., through insider attacks), this can lead one to significantly under-estimate the attacker\u2019s ability and result in substantial loss.\nWe present a systematic approach to computing near-optimal data poisoning attacks for factorizationbased collaborative filtering/recommendation models. We assume a highly motivated attacker with knowledge of both the learning algorithms and parameters of the learner following the Kerckhoffs\u2019 principle to ensure reliable vulnerability analysis in the worst case. We focus on two most popular algorithms: alternating minimization [6] and nuclear norm minimization [3]. Our main contributions are as follows:\n\u2022 Comprehensive characterization of attacker utilities: We characterize several attacker utilities, which include availability attacks, where prediction error is increased, and integrity attacks, where item-specific objectives are considered. Optimal attack strategies for all utilities can be computed under a unified optimization framework.\n\u2022 Novel gradient computations: Building upon existing gradient-based data poisoning frameworks [7, 8, 9], we develop novel methods for gradient computation based on first-order KKT conditions for two widely used algorithms: alternating minimization [6] and nuclear norm minimization [2]. The resulting derivations are highly non-trivial; in addition, to our knowledge this work is the first to give systematic data poisoning attacks for problems involving non-smooth nuclear norm type objectives.\n\u2022 Mimicking normal user behaviors: For data poisoning attacks, most prior work focuses on maximizing attacker\u2019s utility. A less investigated problem is how to synthesize malicious data points that are hard for a defender to detect. In this paper we provide a novel technique based on stochastic gradient Langevin dynamics optimization [10] to produce malicious users that mimic normal user behaviors in order to avoid detection, while achieving attack objectives.\nRelated Work: There has been extensive prior research concerning the security of machine learning algorithms [11, 12, 13, 14, 15]. Biggio et al. pioneered the research of optimizing malicious datadriven attacks for kernel-based learning algorithms such as SVM [16]. The key optimization technique is to approximately compute implicit gradients of the solution of an optimization problem based on first-order KKT conditions. Similar techniques were later generalized to optimize data poisoning attacks for several other important learning algorithms, such as Lasso regression [7], topic modeling [8], and autoregressive models [17]. The reader may refer to [9] for a general algorithmic framework of the abovementioned methods.\nIn terms of collaborative filtering/matrix completion, there is another line of established research that focuses on robust matrix completion, in which a small portion of elements or rows in the underlying low-rank matrix is assumed to be arbitrarily perturbed [18, 19, 20, 21]. Specifically, the stability of alternating minimization solutions was analyzed with respect to malicious data manipulations in [22]. However, [22] assumes a globally optimal solution of alternating minimization can be obtained, which is rarely true in practice."}, {"heading": "2 Preliminaries", "text": "We first set up the collaborative filtering/matrix completion problem and give an overview of existing low-rank factorization based approaches. Let M \u2208 Rm\u00d7n be a data matrix consisting of m rows and n columns. Mij for i \u2208 [m] and j \u2208 [n] would then correspond to the rating the ith user gives for the jth item. We use \u2126 = {(i, j) : Mij is observed} to denote all observable entries in M and assume that |\u2126| mn. We also use \u2126i \u2286 [n] and \u2126\u2032j \u2286 [m] for columns (rows) that are observable at the ith row (jth column). The goal of collaborative filtering (also referred to as matrix completion in the statistical learning literature [2]) is then to recover the complete matrix M from few observations M\u2126.\nThe matrix completion problem is in general ill-posed as it is impossible to complete an arbitrary matrix with partial observations. As a result, additional assumptions are imposed on the underlying data matrix M. One standard assumption is that M is very close to an m \u00d7 n rank-k matrix with\nk min(m,n). Under such assumptions, the complete matrix M can be recovered by solving the following optimization problem:\nmin X\u2208Rm\u00d7n\n\u2016R\u2126(M\u2212X)\u20162F , s.t. rank(X) \u2264 k, (1)\nwhere \u2016A\u20162F = \u2211 i,j A 2 ij denotes the squared Frobenious norm of matrix A and [R\u2126(A)]ij equals Aij if (i, j) \u2208 \u2126 and 0 otherwise. Unfortunately, the feasible set in Eq. (1) is non-convex, making the optimimzation problem difficult to solve. There has been an extensive prior literature on approximately solving Eq. (1) and/or its surrogates that lead to two standard approaches: alternating minimization and nuclear norm minimization. For the first approach, one considers the following problem:\nmin U\u2208Rm\u00d7k,V\u2208Rn\u00d7k\n{ \u2016R\u2126(M\u2212UV>)\u20162F +2\u03bbU\u2016U\u20162F + 2\u03bbV \u2016V\u20162F } . (2)\nEq. (2) is equivalent to Eq. (1) when \u03bbU = \u03bbV = 0. In practice people usually set both regularization parameters \u03bbU and \u03bbV to be small positive constants in order to avoid large entries in the completed matrix and also improve convergence. Since Eq. (2) is bi-convex in U and V, an alternating minimization procedure can be applied. Alternatively, one solves a nuclear-norm minimization problem\nmin X\u2208Rm\u00d7n\n\u2016R\u2126(M\u2212X)\u20162F + 2\u03bb\u2016X\u2016\u2217, (3)\nwhere \u03bb > 0 is a regularization parameter and \u2016X\u2016\u2217 = \u2211rank(X) i=1 |\u03c3i(X)| is the nuclear norm of X, which acts as a convex surrogate of the rank function. Eq. (3) is a convex optimization function and can be solved using an iterative singular value thresholding algorithm [3]. It can be shown that both methods in Eq. (2) and (3) provably approximate the true underlying data matrix M under certain conditions [6, 2]."}, {"heading": "3 The Attack Model", "text": "In this section we describe the data poisoning attack model considered in this paper. For a data matrix consisting of m users and n items, the attacker is capable of adding \u03b1m malicious users to the training data matrix, and each malicious user is allowed to report his/her preference on at most B items with each preference bounded in the range [\u2212\u039b,\u039b]. Before proceeding to describe the attacker\u2019s goals, we first introduce some notation to facilitate presentation. We use M \u2208 Rm\u00d7n to denote the original data matrix and M\u0303 \u2208 Rm\u2032\u00d7n to denote the data matrix of all m\u2032 = \u03b1m malicious users. Let \u2126\u0303 be the set of non-zero entries in M\u0303 and \u2126\u0303i \u2286 [n] be all items that the ith malicious user rated. According to our attack models, |\u2126\u0303i| \u2264 B for every i \u2208 {1, \u00b7 \u00b7 \u00b7 ,m\u2032} and \u2016M\u0303\u2016max = max |M\u0303ij | \u2264 \u039b. Let \u0398\u03bb(M\u0303; M) be the optimal solution computed jointly on the original and poisoned data matrices (M\u0303; M) using regularization parameters \u03bb. For example, Eq. (2) becomes\n\u0398\u03bb(M\u0303; M) = arg min U,U\u0303,V\n\u2016R\u2126(M\u2212UV>)\u20162F+\u2016R\u2126\u0303(M\u0303\u2212U\u0303V >)\u20162F+2\u03bbU (\u2016U\u20162F+\u2016U\u0303\u20162F )+2\u03bbV \u2016V\u20162F\n(4)\nwhere the resulting \u0398 consists of low-rank latent factors U, U\u0303 for normal and malicious users as well as V for items. Simiarly, for the nuclear norm minimization formulation in Eq. (3), we have\n\u0398\u03bb(M\u0303; M) = arg min X,X\u0303\n\u2016R\u2126(M \u2212 X)\u20162F + \u2016R\u2126\u0303(M\u0303 \u2212 X\u0303)\u2016 2 F + 2\u03bb\u2016(X; X\u0303)\u2016\u2217, (5)\nwhere \u0398 = (X, X\u0303) . Let M\u0302(\u0398) be the matrix estimated from learnt model \u0398. For example, for Eq. (4) we have M\u0302(\u0398) = UV> and for Eq. (5) we have M\u0302(\u0398) = X. The goal of the attacker is to find optimal malicious users M\u0303\u2217 such that\nM\u0303\u2217 \u2208 argmaxM\u0303\u2208MR(M\u0302(\u0398\u03bb(M\u0303; M)),M). (6)\nHere M = {M\u0303 \u2208 Rm \u2032\u00d7n : |\u2126\u0303i| \u2264 B, \u2016M\u0303\u2016max \u2264 \u039b} is the set of all feasible poisoning attacks\ndiscussed earlier in this section and R(M\u0302,M) denotes the attacker\u2019s utility for diverting the collaborative filtering algorithm to predict M\u0302 on an original data set M, with the help of few malicious users M\u0303. Below we list several typical attacker utilities:\nAvailability attack the attacker wants to maximize the error of the collaborative filtering system, and eventually render the system useless. Suppose M is the prediction of the collaborative filtering system without data poisoning attacks.2 The utility function is then defined as the total amount of perturbation of predictions between M and M\u0302 (predictions after poisoning attacks) on unseen entries \u2126C :\nRav(M\u0302,M) = \u2016R\u2126C (M\u0302\u2212M)\u2016 2 F . (7)\nIntegrity attack in this model the attacker wants to boost (or reduce) the popularity of a (subset) of items. Suppose J0 \u2286 [n] is the subset of items the attacker is interested in and w : J0 \u2192 R is a pre-specified weight vector by the attacker. The utility function is\nRinJ0,w(M\u0302,M) = m\u2211 i=1 \u2211 j\u2208J0 w(j)M\u0302ij . (8)\nHybrid attack a hybrid loss function can also be defined: RhybridJ0,w,\u00b5(M\u0302,M) = \u00b51R av J0,w(M\u0302,M) + \u00b52R in(M\u0302,M), (9) where \u00b5 = (\u00b51, \u00b52) are coefficients that trade off the availability and integrity attack objectives. In addition, \u00b51 could be negative, which models the case when the attacker wants to leave a \u201clight trace\": the attacker wants to make his item more popular while making the other recommendations of the system less perturbed to avoid detection."}, {"heading": "4 Computing Optimal Attack Strategies", "text": "We describe practical algorithms to solve the optimization problem in Eq. (6) for optimal attack strategy M\u0303\u2217 that maximizes the attacker\u2019s utility. We first consider the alternating minimization formulation in Eq. (4) and derive a projected gradient ascent method that solves for the corresponding optimal attack strategy. Similar derivations are then extended to the nuclear norm minimization formulation in Eq. (5). Finally, we discuss how to design malicious users that mimic normal user behavior in order to avoid detection."}, {"heading": "4.1 Attacking Alternating Minimization", "text": "We use the projected gradient ascent (PGA) method for solving the optimization problem in Eq. (6) with respect to the alternating minimization formulation in Eq. (4): in iteration t we update M\u0303(t) as follows:\nM\u0303(t+1) = ProjM ( M\u0303(t) + st \u00b7 \u2207M\u0303R(M\u0302,M) ) , (10)\nwhere ProjM(\u00b7) is the projection operator onto the feasible region M and st is the step size in iteration t. Note that the estimated matrix M\u0302 depends on the model \u0398\u03bb(M\u0303; M) learnt on the joint data matrix, which further depends on the malicious users M\u0303. Since the constraint set M is highly non-convex, we generate B items uniformly at random for each malicious user to rate. The ProjM(\u00b7) operator then reduces to projecting each malicious users\u2019 rating vector onto an `\u221e ball of diameter \u039b, which can be easily evaluated by truncating all entries in M\u0303 at the level of \u00b1\u039b.\nWe next show how to (approximately) compute \u2207 M\u0303 R(M\u0302,M). This is challenging because one of the arguments in the loss function involves an implicit optimization problem. We first apply chain rule to arrive at \u2207M\u0303R(M\u0302,M) = \u2207M\u0303\u0398\u03bb(M\u0303; M)\u2207\u0398R(M\u0302,M). (11) The second gradient (with respect to \u0398) is easy to evaluate, as all loss functions mentioned in the previous section are smooth and differentiable. Detailed derivation of\u2207\u0398R(M\u0302,M) is deferred to Appendix A. On the other hand, the first gradient term term is much harder to evaluate because \u0398\u03bb(\u00b7) is an optimization procedure. Inspired by [7, 8, 9], we exploit the KKT conditions of the optimization problem \u0398\u03bb(\u00b7) to approximately compute\u2207M\u0303\u0398\u03bb(M\u0303; M). More specifically, the optimal solution \u0398 = (U, U\u0303,V) of Eq. (4) satisfies\n\u03bbUui = \u2211 j\u2208\u2126i (Mij \u2212 u>i vj)vj ;\n2Note that when the collaborative filtering algorithm and its parameters are set, M is a function of observed entriesR\u2126(M).\nAlgorithm 1 Optimizing M\u0303 via PGA 1: Input: Original partially observed m \u00d7 n data matrix M, algorithm regularization parameter \u03bb, attack\nbudget parameters \u03b1, B and \u039b, attacker\u2019s utility function R, step size {st}\u221et=1. 2: Initialization: random M\u0303(0) \u2208 M with both ratings and rated items uniformly sampled at random; t = 0. 3: while M\u0303(t) does not converge do 4: Compute the optimal solution \u0398\u03bb(M\u0303(t); M). 5: Compute gradient\u2207M\u0303R(M\u0302,M) using Eq. (10). 6: Update: M\u0303(t+1) = ProjM(M\u0303\n(t) + st\u2207M\u0303R). 7: t\u2190 t+ 1. 8: end while 9: Output: m\u2032 \u00d7 n malicious matrix M\u0303(t).\n\u03bbU u\u0303i = \u2211 j\u2208\u2126\u0303i (M\u0303ij \u2212 u\u0303>i vj)vj ;\n\u03bbV vj = \u2211 i\u2208\u2126\u2032j (Mij \u2212 u>i vj)ui + \u2211 i\u2208\u2126\u0303\u2032j (M\u0303ij \u2212 u\u0303>i vj)u\u0303i,\nwhere ui, u\u0303i are the ith rows (of dimension k) in U or U\u0303 and vj is the jth row (also of dimension k) in V. Subsequently, {ui, u\u0303i,vj} can be expressed as functions of the original and malicious data matrices M and M\u0303. Using the fact that (a>x)a = (aa>)x and M does not change with M\u0303, we obtain\n\u2202ui(M\u0303)\n\u2202M\u0303ij = 0;\n\u2202u\u0303i(M\u0303) \u2202M\u0303ij = ( \u03bbUIk + \u03a3 (i) U )\u22121 vj ;\n\u2202vj(M\u0303) \u2202M\u0303ij = ( \u03bbV Ik + \u03a3 (j) V )\u22121 ui.\nHere \u03a3(i)U and \u03a3 (j) V are defined as\n\u03a3 (i) U = \u2211 j\u2208\u2126i\u222a\u2126\u0303i vjv > j , \u03a3 (j) V = \u2211 i\u2208\u2126\u2032j\u222a\u2126\u0303 \u2032 j uiu > i . (12)\nA framework of the proposed optimization algorithm is described in Algorithm 1."}, {"heading": "4.2 Attacking Nuclear Norm Minimization", "text": "We extend the projected gradient ascent algorithm in Sec. 4.1 to compute optimal attack strategies for the nuclear norm minimization formulation in Eq. (5). Since the objective in Eq. (5) is convex, the global optimal solution \u0398 = (X, X\u0303) can be obtained by conventional convex optimization procedures such as proximal gradient descent (a.k.a. singular value thresholding [3] for nuclear norm minimization). In addition, the resulting estimation (X; X\u0303) is low rank due to the nuclear norm penalty [2]. Suppose (X; X\u0303) has rank \u03c1 \u2264 min(m,n). We use \u0398\u2032 = (U, U\u0303,V,\u03a3) as an alternative characterization of the learnt model with a reduced number of parameters. Here X = U\u03a3V> and X\u0303 = U\u0303\u03a3V> are singular value decompositions of X and X\u0303; that is, U \u2208 Rm\u00d7\u03c1, U\u0303 \u2208 Rm\u2032\u00d7\u03c1, V \u2208 Rn\u00d7\u03c1 have orthornormal columns and \u03a3 = diag(\u03c31, \u00b7 \u00b7 \u00b7 , \u03c3\u03c1) is a non-negative diagonal matrix.\nTo compute the gradient \u2207 M\u0303 R(M\u0302,M), we again apply the chain rule to decompose the gradient into two parts: \u2207M\u0303R(M\u0302,M) = \u2207M\u0303\u0398 \u2032 \u03bb(M\u0303; M)\u2207\u0398\u2032R(M\u0302,M). (13)\nSimilar to Eq. (11), the second gradient term \u2207\u0398\u2032R(M\u0302,M) is relatively easier to evaluate. Its derivation details are deferred to the Appendix. In the remainder of this section we shall focus on the computation of the first gradient term, which involves partial derivatives of \u0398\u2032 = (U, U\u0303,V,\u03a3) with respect to malicious users M\u0303.\nWe begin with the KKT condition at the optimal solution \u0398\u2032 of Eq. (5). Unlike the alternating minimization formulation, the nuclear norm function \u2016 \u00b7 \u2016\u2217 is not everywhere differentiable. As a\nAlgorithm 2 Optimizing M\u0303 via SGLD 1: Input: Original partially observed m \u00d7 n data matrix M, algorithm regularization parameter \u03bb, attack\nbudget parameters \u03b1, B and \u039b, attacker\u2019s utility function R, step size {st}\u221et=1, tuning parameter \u03b2, number of SGLD iterations T .\n2: Prior setup: compute \u03bej = 1m \u2211m i=1 Mij and \u03c3 2 j = 1 m \u2211m i=1 (Mij \u2212 \u03bej)\n2 for every j \u2208 [n]. 3: Initialization: sample M\u0303(0)ij \u223c N (\u03bej , \u03c3 2 j ) for i \u2208 [m\u2032] and j \u2208 [n]. 4: for t = 0 to T do 5: Compute the optimal solution \u0398\u03bb(M\u0303(t); M). 6: Compute gradient\u2207M\u0303R(M\u0302,M) using Eq. (10). 7: Update M\u0303(t+1) according to Eq. (17). 8: end for 9: Projection: find M\u0303\u2217 \u2208 arg minM\u0303\u2208M \u2016M\u0303\u2212 M\u0303\n(t)\u20162F . Details in the main text. 10: Output: m\u2032 \u00d7 n malicious matrix M\u0303\u2217.\nresult, the KKT condition relates the subdifferential of the nuclear norm function \u2202\u2016 \u00b7 \u2016\u2217 as R\u2126,\u2126\u0303 ( [M; M\u0303]\u2212 [X; X\u0303] ) \u2208 \u03bb\u2202\u2016[X; X\u0303]\u2016\u2217. (14)\nHere [X; X\u0303] is the concatenated (m+m\u2032)\u00d7n matrix of X and X\u0303. The subdifferential of the nuclear norm function \u2202\u2016 \u00b7 \u2016\u2217 is also known [2]:\n\u2202\u2016X\u2016\u2217 = { UV> + W : U>W = WV = 0, \u2016W\u20162 \u2264 1 } ,\nwhere X = U\u03a3V> is the singular value decomposition of X. Suppose {ui}, {u\u0303i} and {vj} are rows of U, U\u0303,V and W = {wij}. We can then re-formulate the KKT condition Eq. (14) as follows:\n\u2200(i, j) \u2208 \u2126, Mij = u>i (\u03a3 + \u03bbI\u03c1)vj + \u03bbwij ; \u2200(i, j) \u2208 \u2126\u0303, M\u0303ij = u\u0303>i (\u03a3 + \u03bbI\u03c1)vj + \u03bbw\u0303ij .\nNow we can derive\u2207 M\u0303 \u0398 = \u2207 M\u0303 (u, u\u0303,v, \u03c3); the full derivation is deferred to the appendix."}, {"heading": "4.3 Mimicing Normal User Behaviors", "text": "Normal users generally do not rate items uniformly at random. For example, some movies are significantly more popular than others. As a result, malicious users that pick rated movies uniformly at random can be easily identified by running a t-test against a known database consisting of only normal users, as shown in Sec. 5. To alleviate this issue, in this section we propose an alternative approach to compute data poisoning attacks such that the resulting malicious users M\u0303 mimics normal users M to avoid potential detection, while still achieving reasonably large utility R(M\u0302,M) for the attacker. We use a Bayesian formulation to take both data poisoning and detection avoidance objectives into consideration. The prior distribution p0(M\u0303) captures normal user behaviors and is defined as a multivariate normal distribution\np0(M\u0303) = m\u2032\u220f i=1 n\u220f j=1 N (M\u0303ij ; \u03bej , \u03c32j ),\nwhere \u03bej and \u03c32j are mean and variance parameters for the rating of the jth item provided by normal users. In practice both parameters can be estimated using normal user matrix M as \u03bej = 1m \u2211m i=1 Mij\nand \u03c32 = 1m \u2211m i=1 (Mij \u2212 \u03bej)2. On the other hand, the likelihood p(M|M\u0303) is defined as\np(M|M\u0303) = 1 Z\nexp ( \u03b2 \u00b7R(M\u0302,M) ) , (15)\nwhere R(M\u0302,M) = R(M\u0302(\u0398\u03bb(M\u0303; M)),M) is one of the attacker utility functions defined in Sec. 3, Z is a normalization constant and \u03b2 > 0 is a tuning parameter that trades off attack performance and detection avoidance. A small \u03b2 shifts the posterior of M\u0303 toward its prior, which makes the resulting attack strategy less effective but harder to detect, and vice versa.\nGiven both prior and likelihood functions, an effective detection-avoiding attack strategy M\u0303 can be obtained by sampling from its posterior distribution:\np(M\u0303|M) = p0(M\u0303)p(M|M\u0303)/p(M) \u221d exp \u2212 m\u2032\u2211 i=1 n\u2211 j=1 (M\u0303ij \u2212 \u03bej)2 2\u03c32j + \u03b2R(M\u0302,M)  . (16) Posterior sampling of Eq. (16) is clearly intractable due to the implicit and complicated dependency of the estimated matrix M\u0302 on the malicious data M\u0303, that is, M\u0302 = M\u0302(\u0398\u03bb(M\u0303; M))). To circumvent this problem, we apply Stochastic Gradient Langevin Dynamics (SGLD, [10]) to approximately sample M\u0303 from its posterior distribution in Eq. (16). More specfically, the SGLD algorithm iteratively computes a sequence of posterior samples {M\u0303(t)}t\u22650 and in iteration t the new sample M\u0303(t+1) is computed as\nM\u0303(t+1) = M\u0303(t) + st 2 ( \u2207M\u0303 log p(M\u0303|M) ) + \u03b5t, (17)\nwhere {st}t\u22650 are step sizes and \u03b5t \u223c N (0, stI) are independent Gaussian noises injected at each SGLD iteration. The gradient\u2207\nM\u0303 log p(M\u0303|M) can be computed as\n\u2207M\u0303 log p(M\u0303|M) = \u2212(M\u0303\u2212\u039e)\u03a3 \u22121 + \u03b2\u2207M\u0303R(M\u0302,M),\nwhere \u03a3 = diag(\u03c321 , \u00b7 \u00b7 \u00b7 , \u03c32n) and \u039e is an m\u2032 \u00d7 n matrix with \u039eij = \u03bej for i \u2208 [m\u2032] and j \u2208 [n]. The other gradient \u2207\nM\u0303 R(M\u0302,M) can be computed using the procedure in Sections 4.1 and 4.2. Finally,\nthe sampled malicious matrix M\u0303(t) is projected back onto the feasible set M by selecting B items per user with the largest absolute rating and truncating ratings to the level of {\u00b1\u039b}. A high-level description of the proposed method is given in Algorithm 2."}, {"heading": "5 Experimental Results", "text": "To evaluate the effectiveness of our proposed poisoning attack strategy, we use the publicly available MovieLens dataset which contains 20 millions ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users [23]. We shift the rating range to [\u22122, 2] for computation convenience. To avoid the \u201ccold-start\u201d problem, we consider users who have rated at least 20 movies. Two metrics are employed to measure the relative performance of the systems before and after data poisoning attacks: root mean square error (RMSE) for the predicted unseen entries3 and average rating for specific items. We then analyze the tradeoff between attack performance and detection avoidance, which is controled by the \u03b2 parameter in Eq. (15). This serves as a guide for how \u03b2 should be set in later experiments. We use a paired t-test to compare the distributions of rated items between normal and malicious users. We present the trend of p-value against different values of \u03b2 in the extended version of the paper. To strive for a good tradeoff, we set \u03b2 = 0.6 at which the p-value stablizes around 0.7 and the poisoning attack performance is not significantly sacrificed.\nWe employ attack models specified in Eq. (9), where the utility parameters \u00b51 and \u00b52 balance two different malicious goals (availability and integrity) an attacker wishes to achieve. For the integrity utility RinJ0,w, the J0 set contains only one item j0 selected randomly from all items whose average predicted ratings are around 0.8. The weight wj0 is set as wj0 = 2. Figure 1 (a) (b) plots the RMSE\n3defined as RMSE = \u221a\u2211\n(i,j)\u2208\u2126C (Mij \u2212 M\u0302ij)2/|\u2126C |, where M is the prediction of model trained on clean dataR\u2126(M) only (i.e., without data poisoning attacks).\nafter data poisoning attacks. When \u00b51 = 1, \u00b52 = 0, the attacker is interested in increasing the RMSE of the collaborative filtering system and hence reducing the system\u2019s availability. On the other hand, when \u00b51 = 1, \u00b52 = \u22121 the attacker wishes to increase RMSE while at the same time keeping the rating of specific items (j0) as low as possible for certain malicious purposes. Figure 1 (b) shows that when the attackers consider to both objectives (\u00b51 = 1, \u00b52 = \u22121), the RMSE after poisoning is slightly lower than that if only availability is targeted (\u00b51 = 1, \u00b52 = 0). In addition, the projected gradient ascent (PGA) strategy generates the largest RMSE score compared with the other methods. However, PGA requires malicious users to rate each item uniformly at random, which might expose the malicious profiles to an informed defender. More specifically, the paired t-test on those malicious profiles produced by PGA rejects the null hypothesis that the items rated by the attacker strategies are the same as those obtained from normal users (p < 0.05). In contrast, the SGLD method leads to slightly worse attacker utility but generates malicious users that are hard to distinguish from the normal users (for example, the paired t-test leads to inconclusive p-values (larger than 0.7) with \u03b2 = 0.6. Finally, both PGA and SGLD result in higher attacker utility compared to uniform attacks, where both ratings and rated items are sampled uniformly at random for malicious profiles.\nApart from the RMSE scores, we also plot ratings of specific items against percentage of malicious profiles in Figure 1 (c) (d). We consider two additional attack utility settings: \u00b51 = 0, \u00b52 = 1, in which the attacker wishes to push the ratings of some particular items (specified in w and J0 of Rin) as high as possible; and \u00b51 = \u22121, \u00b52 = 1, where the attacker also wants to leave a \u201clight trace\" by reducing the impact on the entire system resulted from malicious activities. It is clear that targeted attackes (both PGA and SGLD) are indeed more effective at manipulating ratings of specific items for integrity attacks.\nWe also plot RMSE/Average ratings against malicious user percentage in Figure 2 for the nuclear norm minimization under similar settings based on a subset of 1000 users and 1700 movies (items), since it is more computationally expensive than alternating minimization. In general, we observe similar behavior of both RMSE/Average ratings under different attacking models \u00b51, \u00b52 with alternating minimization."}, {"heading": "6 Discussion and Concluding Remarks", "text": "Our ultimate goal for the poisoning attack analysis is to develop possible defensive strategies based on the careful analysis of adversarial behaviors. Since the poisoning data is optimized based on the attacker\u2019s malicious objectives, the correlations among features within a feature vector may change to appear different from normal instances. Therefore, tracking and detecting deviations in the feature correlations and other accuracy metrics can be one potential defense. Additionally, defender can also apply the combinational models or sampling strategies, such as bagging, to reduce the influence of poisoning attacks."}, {"heading": "Acknowledgments", "text": "This research was partially supported by the NSF (CNS-1238959, IIS-1526860), ONR (N00014-151-2621), ARO (W911NF-16-1-0069), AFRL (FA8750-14-2-0180), Sandia National Laboratories, and Symantec Labs Graduate Research Fellowship."}, {"heading": "A Computation of\u2207\u0398R(M\u0302,M)", "text": "We provide details on how to compute the \u201ceasy\" gradient \u2207\u0398\u0303R(M\u0302,M), M\u0302 = M\u0302(\u0398) is the prediction based on the learnt model \u0398. Applying the chain rule of differentiation we get\n\u2207\u0398R(M\u0302,M) = ( \u2207\u0398M\u0302 )( \u2207\nM\u0302 R(M\u0302,M)\n) . (18)\nWe first focus on the second term\u2207 M\u0302 R(M\u0302,M)). This is easy to compute because all malicious utility functions R considered in this paper are smooth and differentiable. More specifically, the availability attack utility Rav and the integrity attack utility Rin admit the following gradient computations:\n\u2202Rav \u2202M\u0302ij = 2(M\u0302ij \u2212Mij) \u00b7 I[(i, j) /\u2208 \u2126];\n\u2202RinJ0,w\n\u2202M\u0302ij = w(j) \u00b7 I[j \u2208 J0].\nHere I[\u00b7] is the indicator function that equals one if the corresponding condition holds true and zero otherwise. The gradient for the hybrid utility Rhybrid can then be expressed as a linear combination of the gradients of Rav and Rin:\n\u2207Rhybrid\u00b5,J0,w = \u00b51\u2207R av + \u00b52\u2207RinJ0,w.\nWe next turn to the computation of \u2207\u0398M\u0302, which is model specific. Alternating minimization and nuclear norm minimization are considered separately for this gradient:\nAlternating minimization In alternating minimization the learnt model \u0398 is parameterized by \u0398 = (U, U\u0303,V), where U \u2208 Rm\u00d7k, U\u0303 \u2208 Rm\u2032\u00d7k and V \u2208 Rn\u00d7k. Since M\u0302 = UV> for normal users, we have\n\u2202M\u0302ij \u2202U`t = Vjt \u00b7 I[i = `], \u2202M\u0302ij \u2202V`t = Uit \u00b7 I[j = `].\nNuclear norm minimization In nuclear norm minimization the learnt model \u0398 is parameterized by \u0398 = (U, U\u0303,V,\u03a3) where U \u2208 Rm\u00d7k, U\u0303 \u2208 Rm\u2032\u00d7k, V \u2208 Rn\u00d7k and \u03a3 = diag(\u03c31, \u00b7 \u00b7 \u00b7 , \u03c3k). The estimation M\u0302 for normal users is then expressed as M\u0302 = U\u03a3V>. As a result, we have\n\u2202M\u0302ij \u2202U`t = \u03c3tVjt \u00b7 I[i = `];\n\u2202M\u0302ij \u2202V`t = \u03c3tUit \u00b7 I[j = `];\n\u2202M\u0302ij \u2202\u03c3t = UitVjt."}, {"heading": "B Derivation of\u2207M\u0303\u0398 = \u2207M\u0303(u, u\u0303,v, \u03c3) for nuclear norm minimization", "text": "Evaluation of \u2207 M\u0303 ui Because ui does not depend on M\u0303, we have\u2207M\u0303ui = 0.\nEvaluation of\u2207 M\u0303 u\u0303i Let \u2126\u0303i be all (i, j) pairs such that (i, j) \u2208 \u2126\u0303. Suppose we are computing the gradient of u\u0303i with respect to M\u0303i`, where ` can be either in or not in \u2126\u0303i. Define \u2126\u0303`i = \u2126\u0303i\u222a{`} be the extended set of observations and denote r = |\u2126\u0303`i | as the size of the extended observation set. Define M\u0303i = (M\u0303ij)j\u2208\u2126\u0303`i \u2208 Rr, w\u0303i = (w\u0303ij)j\u2208\u2126\u0303`i \u2208 R r and V`i = (vj)j\u2208\u2126\u0303`i \u2208 R\n\u03c1\u00d7r. By KKT condition,[ (\u03a3 + \u03bbI\u03c1) V ` i ]> u\u0303i = M\u0303i \u2212 \u03bbw\u0303i. (19)\nThe above linear system can be either over-determined or under-determined, depending on the relationship between \u03c1 and r. When the system is under-determined (e.g., r < \u03c1), the solution to Eq. (19) is not unique and could be instable if the matrix Ai = [ (\u03a3 + \u03bbI\u03c1) V ` i ]> is ill-conditioned. On the other hand, when the system is over-determined (e.g., r > \u03c1) an exact solution u\u0303i may not exist. To force unique solutions in full generality, we compute u\u0303i by solving the following Ridge-regularized system:\nmin u\u0303i \u2016M\u0303i \u2212 \u03bbw\u0303i \u2212Aiu\u0303i\u201622 + 2\u03c4\u2016u\u0303i\u201622,\nwhere \u03c4 > 0 is a smoothing parameter. Subsequently,\nu\u0303i \u2248 (A>i Ai + \u03c4I\u03c1)\u22121A>i (M\u0303i \u2212 \u03bbw\u0303i); \u2202u\u0303i\n\u2202M\u0303i` \u2248 (A>i Ai + \u03c4I\u03c1)\u22121(\u03a3 + \u03bbI\u03c1)v`.\nEvaluation of \u2207 M\u0303 vj This part is similar to the gradient of u\u0303i. Suppose we are computing \u2202vj/\u2202M\u0303`j . Define \u2126\u0304`j = \u2126 \u2032 j \u222a \u2126\u0303\u2032j \u222a {`} to be the extended set of all i such that (i, j) \u2208 \u2126 \u222a \u2126\u0303. Let r = |\u2126\u0304`j | be the size of the extended set. We then have[ (U\u0304`i) >(\u03a3 + \u03bbI\u03c1) ] vj = M\u0303 \u2032 j \u2212 \u03bbw\u0303\u2032j ,\nwhere U\u0304`i is a \u03c1\u00d7 r matrix consisting of all ui or u\u0303i for i \u2208 \u2126\u0304`j as its columns. On the right-hand side, we have M\u0303\u2032j = (M\u0303ij)i\u2208\u2126\u0304`j and w\u0303 \u2032 j = (wij)i\u2208\u2126\u0304`j . Let Bj = (U\u0304 ` i) >(\u03a3 + \u03bbI\u03c1) \u2208 Rr\u00d7\u03c1 and \u03c4 > 0 be a smoothing parameter. We then have\n\u2202vj\n\u2202M\u0303`j \u2248 (B>j Bj + \u03c4I\u03c1)\u22121(\u03a3 + \u03bbI\u03c1)u\u0303`.\nEvaluation of \u2207 M\u0303 \u03c3k By KKT condition we have\nM\u0303ij = u\u0303ikvjk \u00b7 \u03c3k + c,\nwhere c is a constant that does not depend on \u03c3k. Subsequently, we get\n\u2202\u03c3k\n\u2202M\u0303ij =\n1\nu\u0303ikvjk ."}, {"heading": "C Additional experimental results", "text": "Here we analyze the trend of p-value against different values of \u03b2. Figure 3 plots P-values and RMSE/Average ratings against different values of \u03b2. When B = 25 (recall that B is the maximum number of items a malicious user is allowed to rate), with the increase of \u03b2, the P-value decreases while both RMSE and average per-item ratings increase.\nWe then plot ratings of specific items against percentage of malicious profiles by setting \u00b52 = \u22121 to evaluate the performance of attacker reducing the popularity of the item, whose original predicted average rating is 0.8. Figure 5 and 6 both show two settings of \u00b51 = 0, \u00b52 = \u22121 and \u00b51 = \u22121, \u00b52 = \u22121 for alternating minimization and nuclear norm minimization, respectively. For alternating minimization algorithm, when \u00b51 = 0, \u00b52 = \u22121, the attacker tries to reduce the average rating for certain item without caring about the availability error of the whole recommendation system. This way, the attacker has better control of the item and can decrease the average rating of the item from 0.8 to around -0.3. While, if \u00b51 = \u22121, \u00b52 = \u22121, the attacker want to reduce the popularity of the item and at the same time reduce the availability error for the whole system to avoid detection; therefore the attacker can only decrease the average rating of the item to about -0.1 under this setting.We obtain the similar observations for the nuclear norm minimization."}], "references": [{"title": "Unifying user-based and item-based collaborative filtering approaches by similarity fusion", "author": ["Jun Wang", "Arjen de Vires", "Marcel Reinders"], "venue": "In SIGIR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel Cand\u00e8s", "Ben Recht"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["Jian-Feng Cai", "Emmanuel Cand\u00e8s", "Zuowei Shen"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1956}, {"title": "Effective attack models for shilling item-based collaborative filtering systems", "author": ["Bamshad Mobasher", "Robin Burke", "Runa Bhaumik", "Chad Williams"], "venue": "In Proceedings of the 2005 WebKDD Workshop, held in conjuction with ACM SIGKDD\u20192005,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Promoting recommendations: An attack on collaborative filtering", "author": ["Michael P O\u2019Mahony", "Neil J Hurley", "Guenole CM Silvestre"], "venue": "In Database and Expert Systems Applications,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In STOC,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Is feature selection secure against training data poisoning", "author": ["Huang Xiao", "Battista Biggio", "Gavin Brown", "Giorgio Fumera", "Claudia Eckert", "Fabio Roli"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "The security of latent dirichlet allocation", "author": ["Shike Mei", "Xiaojin Zhu"], "venue": "In AISTATS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Using machine teaching to identify optimal training-set attacks on machine learners", "author": ["Shike Mei", "Xiaojin Zhu"], "venue": "In AAAI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Bayesian learning via stochastic gradient langevin dynamics", "author": ["Max Welling", "Yee W Teh"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Adversarial classification", "author": ["Nilesh Dalvi", "Pedro Domingos", "Sumit Sanghai", "Deepak Verma"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Adversarial learning", "author": ["Daniel Lowd", "Christopher Meek"], "venue": "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Feature cross-substitution in adversarial classification", "author": ["Bo Li", "Yevgeniy Vorobeychik"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Scalable optimization of randomized operational decisions in adversarial classification settings", "author": ["Bo Li", "Yevgeniy Vorobeychik"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Can machine learning be secure", "author": ["Marco Barreno", "Blaine Nelson", "Russell Sears", "Anthony D Joseph", "J Doug Tygar"], "venue": "In Proceedings of the 2006 ACM Symposium on Information, computer and communications security,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Poisoning attacks against support vector machines", "author": ["Battista Biggio", "Blaine Nelson", "Pavel Laskov"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Data poisoning attacks against autoregressive models", "author": ["Scott Alfeld", "Xiaojin Zhu", "Paul Barford"], "venue": "In AAAI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Robust matrix completion", "author": ["Olga Klopp", "Karim Lounici", "Alexandre Tsybakov"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Robust matrix completion and corrupted columns", "author": ["Yudong Chen", "Huan Xu", "Constantine Caramanis", "Sujay Sanghavi"], "venue": "In ICML,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Low-rank matrix recovery from errors and erasures", "author": ["Yudong Chen", "Ali Jalali", "Sujay Sanghavi", "Constantine Caramanis"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Robust matrix completion via joint schatten p-norm and lp-norm minimization", "author": ["Feiping Nie", "Hua Wang", "Xiao Cai", "Heng Huang", "Chris Ding"], "venue": "In ICDM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Stability of matrix factorization for collaborative filtering", "author": ["Yu-Xiang Wang", "Huan Xu"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Existing approaches in the literature include nearest-neighbor methods, where a user\u2019s (item\u2019s) preference is determined by other users (items) with similar profiles [1], and factorization-based methods where the incomplete preference matrix is assumed to be approximately low-rank [2, 3].", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "Existing approaches in the literature include nearest-neighbor methods, where a user\u2019s (item\u2019s) preference is determined by other users (items) with similar profiles [1], and factorization-based methods where the incomplete preference matrix is assumed to be approximately low-rank [2, 3].", "startOffset": 282, "endOffset": 288}, {"referenceID": 2, "context": "Existing approaches in the literature include nearest-neighbor methods, where a user\u2019s (item\u2019s) preference is determined by other users (items) with similar profiles [1], and factorization-based methods where the incomplete preference matrix is assumed to be approximately low-rank [2, 3].", "startOffset": 282, "endOffset": 288}, {"referenceID": 3, "context": "systems and their reliance on user-specified judgments for building profiles, various forms of attacks are possible and have been discussed, such as the random attack and random product push/nuke attack [4, 5].", "startOffset": 203, "endOffset": 209}, {"referenceID": 4, "context": "systems and their reliance on user-specified judgments for building profiles, various forms of attacks are possible and have been discussed, such as the random attack and random product push/nuke attack [4, 5].", "startOffset": 203, "endOffset": 209}, {"referenceID": 5, "context": "We focus on two most popular algorithms: alternating minimization [6] and nuclear norm minimization [3].", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "We focus on two most popular algorithms: alternating minimization [6] and nuclear norm minimization [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "\u2022 Novel gradient computations: Building upon existing gradient-based data poisoning frameworks [7, 8, 9], we develop novel methods for gradient computation based on first-order KKT conditions for two widely used algorithms: alternating minimization [6] and nuclear norm minimization [2].", "startOffset": 95, "endOffset": 104}, {"referenceID": 7, "context": "\u2022 Novel gradient computations: Building upon existing gradient-based data poisoning frameworks [7, 8, 9], we develop novel methods for gradient computation based on first-order KKT conditions for two widely used algorithms: alternating minimization [6] and nuclear norm minimization [2].", "startOffset": 95, "endOffset": 104}, {"referenceID": 8, "context": "\u2022 Novel gradient computations: Building upon existing gradient-based data poisoning frameworks [7, 8, 9], we develop novel methods for gradient computation based on first-order KKT conditions for two widely used algorithms: alternating minimization [6] and nuclear norm minimization [2].", "startOffset": 95, "endOffset": 104}, {"referenceID": 5, "context": "\u2022 Novel gradient computations: Building upon existing gradient-based data poisoning frameworks [7, 8, 9], we develop novel methods for gradient computation based on first-order KKT conditions for two widely used algorithms: alternating minimization [6] and nuclear norm minimization [2].", "startOffset": 249, "endOffset": 252}, {"referenceID": 1, "context": "\u2022 Novel gradient computations: Building upon existing gradient-based data poisoning frameworks [7, 8, 9], we develop novel methods for gradient computation based on first-order KKT conditions for two widely used algorithms: alternating minimization [6] and nuclear norm minimization [2].", "startOffset": 283, "endOffset": 286}, {"referenceID": 9, "context": "In this paper we provide a novel technique based on stochastic gradient Langevin dynamics optimization [10] to produce malicious users that mimic normal user behaviors in order to avoid detection, while achieving attack objectives.", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": "Related Work: There has been extensive prior research concerning the security of machine learning algorithms [11, 12, 13, 14, 15].", "startOffset": 109, "endOffset": 129}, {"referenceID": 11, "context": "Related Work: There has been extensive prior research concerning the security of machine learning algorithms [11, 12, 13, 14, 15].", "startOffset": 109, "endOffset": 129}, {"referenceID": 12, "context": "Related Work: There has been extensive prior research concerning the security of machine learning algorithms [11, 12, 13, 14, 15].", "startOffset": 109, "endOffset": 129}, {"referenceID": 13, "context": "Related Work: There has been extensive prior research concerning the security of machine learning algorithms [11, 12, 13, 14, 15].", "startOffset": 109, "endOffset": 129}, {"referenceID": 14, "context": "Related Work: There has been extensive prior research concerning the security of machine learning algorithms [11, 12, 13, 14, 15].", "startOffset": 109, "endOffset": 129}, {"referenceID": 15, "context": "pioneered the research of optimizing malicious datadriven attacks for kernel-based learning algorithms such as SVM [16].", "startOffset": 115, "endOffset": 119}, {"referenceID": 6, "context": "Similar techniques were later generalized to optimize data poisoning attacks for several other important learning algorithms, such as Lasso regression [7], topic modeling [8], and autoregressive models [17].", "startOffset": 151, "endOffset": 154}, {"referenceID": 7, "context": "Similar techniques were later generalized to optimize data poisoning attacks for several other important learning algorithms, such as Lasso regression [7], topic modeling [8], and autoregressive models [17].", "startOffset": 171, "endOffset": 174}, {"referenceID": 16, "context": "Similar techniques were later generalized to optimize data poisoning attacks for several other important learning algorithms, such as Lasso regression [7], topic modeling [8], and autoregressive models [17].", "startOffset": 202, "endOffset": 206}, {"referenceID": 8, "context": "The reader may refer to [9] for a general algorithmic framework of the abovementioned methods.", "startOffset": 24, "endOffset": 27}, {"referenceID": 17, "context": "In terms of collaborative filtering/matrix completion, there is another line of established research that focuses on robust matrix completion, in which a small portion of elements or rows in the underlying low-rank matrix is assumed to be arbitrarily perturbed [18, 19, 20, 21].", "startOffset": 261, "endOffset": 277}, {"referenceID": 18, "context": "In terms of collaborative filtering/matrix completion, there is another line of established research that focuses on robust matrix completion, in which a small portion of elements or rows in the underlying low-rank matrix is assumed to be arbitrarily perturbed [18, 19, 20, 21].", "startOffset": 261, "endOffset": 277}, {"referenceID": 19, "context": "In terms of collaborative filtering/matrix completion, there is another line of established research that focuses on robust matrix completion, in which a small portion of elements or rows in the underlying low-rank matrix is assumed to be arbitrarily perturbed [18, 19, 20, 21].", "startOffset": 261, "endOffset": 277}, {"referenceID": 20, "context": "In terms of collaborative filtering/matrix completion, there is another line of established research that focuses on robust matrix completion, in which a small portion of elements or rows in the underlying low-rank matrix is assumed to be arbitrarily perturbed [18, 19, 20, 21].", "startOffset": 261, "endOffset": 277}, {"referenceID": 21, "context": "Specifically, the stability of alternating minimization solutions was analyzed with respect to malicious data manipulations in [22].", "startOffset": 127, "endOffset": 131}, {"referenceID": 21, "context": "However, [22] assumes a globally optimal solution of alternating minimization can be obtained, which is rarely true in practice.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "The goal of collaborative filtering (also referred to as matrix completion in the statistical learning literature [2]) is then to recover the complete matrix M from few observations M\u03a9.", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "(3) is a convex optimization function and can be solved using an iterative singular value thresholding algorithm [3].", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "(2) and (3) provably approximate the true underlying data matrix M under certain conditions [6, 2].", "startOffset": 92, "endOffset": 98}, {"referenceID": 1, "context": "(2) and (3) provably approximate the true underlying data matrix M under certain conditions [6, 2].", "startOffset": 92, "endOffset": 98}, {"referenceID": 6, "context": "Inspired by [7, 8, 9], we exploit the KKT conditions of the optimization problem \u0398\u03bb(\u00b7) to approximately compute\u2207M\u0303\u0398\u03bb(M\u0303; M).", "startOffset": 12, "endOffset": 21}, {"referenceID": 7, "context": "Inspired by [7, 8, 9], we exploit the KKT conditions of the optimization problem \u0398\u03bb(\u00b7) to approximately compute\u2207M\u0303\u0398\u03bb(M\u0303; M).", "startOffset": 12, "endOffset": 21}, {"referenceID": 8, "context": "Inspired by [7, 8, 9], we exploit the KKT conditions of the optimization problem \u0398\u03bb(\u00b7) to approximately compute\u2207M\u0303\u0398\u03bb(M\u0303; M).", "startOffset": 12, "endOffset": 21}, {"referenceID": 2, "context": "singular value thresholding [3] for nuclear norm minimization).", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "In addition, the resulting estimation (X; X\u0303) is low rank due to the nuclear norm penalty [2].", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "The subdifferential of the nuclear norm function \u2202\u2016 \u00b7 \u2016\u2217 is also known [2]: \u2202\u2016X\u2016\u2217 = { UV + W : UW = WV = 0, \u2016W\u20162 \u2264 1 } ,", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": "To circumvent this problem, we apply Stochastic Gradient Langevin Dynamics (SGLD, [10]) to approximately sample M\u0303 from its posterior distribution in Eq.", "startOffset": 82, "endOffset": 86}], "year": 2016, "abstractText": "Recommendation and collaborative filtering systems are important in modern information and e-commerce applications. As these systems are becoming increasingly popular in the industry, their outputs could affect business decision making, introducing incentives for an adversarial party to compromise the availability or integrity of such systems. We introduce a data poisoning attack on collaborative filtering systems. We demonstrate how a powerful attacker with full knowledge of the learner can generate malicious data so as to maximize his/her malicious objectives, while at the same time mimicking normal user behavior to avoid being detected. While the complete knowledge assumption seems extreme, it enables a robust assessment of the vulnerability of collaborative filtering schemes to highly motivated attacks. We present efficient solutions for two popular factorizationbased collaborative filtering algorithms: the alternative minimization formulation and the nuclear norm minimization method. Finally, we test the effectiveness of our proposed algorithms on real-world data and discuss potential defensive strategies.", "creator": "TeX"}}}