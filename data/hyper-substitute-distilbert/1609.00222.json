{"id": "1609.00222", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2016", "title": "Ternary Neural Networks for Resource-Efficient AI Applications", "abstract": "the output and storage requirements into deep neural networks ( dnns ) come usually high. this issue limit. possibilities include ubiquitous networking devices such as medical phones or wearables. in extensive paper, we use ternary neural networks ( ct ) using order. ensure deep learning programmes resource - sufficient. parents train these tnns on a teacher - aware approach. using typical hierarchical branches identify ternary blocks, with a step activation component of concentration - thresholds, as student dominated network learns / mimic the behaviour about its traditional network. we propose a novel, layer - function greedy method for all operations. during training, no ternary digital network inherently holds the smaller databases because setting cycles to zero. this decreases but they seem compact looking more eco - friendly. developers build a purpose - implemented syntax design for libraries thus implement classes on fpga. compiler optimization results with that intentionally - built hardware based tnns reveal memory, with only 42. 24 microjoules hd image, we can establish 21. 76 % accuracy x 5. 37 microsecond latency rate with computed rate of 255k memory generated second by mnist.", "histories": [["v1", "Thu, 1 Sep 2016 13:08:47 GMT  (81kb,D)", "http://arxiv.org/abs/1609.00222v1", null], ["v2", "Sun, 26 Feb 2017 09:44:34 GMT  (84kb,D)", "http://arxiv.org/abs/1609.00222v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["hande alemdar", "vincent leroy", "adrien prost-boucle", "fr\\'ed\\'eric p\\'etrot"], "accepted": false, "id": "1609.00222"}, "pdf": {"name": "1609.00222.pdf", "metadata": {"source": "CRF", "title": "Ternary Neural Networks for Resource-Efficient AI Applications", "authors": ["Hande Alemdar", "Nicholas Caldwell", "Vincent Leroy", "Adrien Prost-Boucle", "Fr\u00e9d\u00e9ric P\u00e9trot"], "emails": ["name.surname@imag.fr"], "sections": [{"heading": "1 Introduction", "text": "Deep neural networks (DNNs) have achieved state-of-the-art results on a wide range of AI tasks including computer vision [1], speech recognition [2] and natural language processing [3]. As DNNs become more complex, their number of layers, number of weights, and computational cost increase. While DNNs are generally trained on powerful servers with the support of GPUs, they can be used for classification tasks on a variety of hardware. Our goal in this paper is to train DNNs that are able to classify at a high throughput on low-power devices.\nIn recent years, two main directions of research have been explored to reduce the cost of DNNs classifications. The first one preserves the floating point precision of DNNs, but drastically increases sparsity and weights sharing for\nar X\niv :1\n60 9.\n00 22\n2v 1\n[ cs\n.L G\n] 1\ncompression [4, 5]. This has the advantage of preserving compatibility with most standard training algorithms, while significantly diminishing memory and power consumption. However, the power savings are limited by the need for floating-point operation. The second direction completely eliminates the need for floating-point operations using weights discretization [6, 7, 8], with extreme cases such as binary neural networks eliminating the need for multiplications. The main drawbacks of these approaches are, a significant degradation in the classification accuracy, and the need for custom training methods that deviate from well-known back-propagation.\nThis paper addresses these issues and makes the following contributions:\n\u2022 We propose a teacher-student approach for training Ternary NNs with weights constrained to {\u22121, 0, 1}. The teacher network is trained with stochastic firing using back-propagation, and can benefit from all techniques that exist in the literature such as dropout [9], batch normalization [10], and convolutions. The student network has the same architecture and, for each neuron, mimics the behavior of the equivalent neuron in the teacher network.\n\u2022 We present a FPGA-based architecture that is able to process ternary NNs at 255K images per second with an energy cost of 1.24 \u00b5J."}, {"heading": "2 Training Ternary Neural Networks", "text": "We use a teacher-student approach for training TNNs. First, we train the real-valued teacher network with stochastically firing ternary neurons. Then, we let the student network learn how to imitate the teacher\u2019s behavior using a layer-wise greedy algorithm. Both the teacher and the student network have the same architecture. The student network\u2019s weights are the ternarized version of the teacher network\u2019s weights. The student network uses a step function with two thresholds as the activation function. In Table 1, we provide our notation and descriptions. In general, we denote the discrete values with a bold font. Real-valued parameters are denoted by normal font. We use [.] to denote a matrix or a vector. We describe the details of the two stages in the following subsections."}, {"heading": "2.1 The Teacher Network", "text": "The teacher network is trained as a real-valued neural network, it has stochastically firing ternary neurons with output values of \u22121, 0, or 1. In order to achieve a ternary output for teacher neuron nti , we add a stochastic firing step after the hyperbolic tangent, tanh, activation function, as described in Table 1. Although we use tanh for obtaining the range (\u22121, 1) before ternarization, any nonlinear function such as hard tanh or soft-sign that has the same range is applicable. We do not impose any restrictions to the weights of the teacher network, nor do we ternarize them at this stage. The benefit of this approach is that we can use any technique that already exists for efficient NN training, such as batch normalization [10], dropout [9], etc. The teacher network can have any architecture with any number of neurons, and can be trained using any of the standard training algorithms."}, {"heading": "2.2 The Student Network", "text": "After the teacher network is trained, we begin the training of the student network. The goal of the student network is to predict the output of the teacher realvalued network. Since we use the same architecture for both networks, there is a one-to-one correspondence between the neurons of both. Each student neuron nsi learns to mimic the behavior of the corresponding teacher neuron nti individually and independently from the other neurons. In order to achieve this, a student neuron uses the corresponding teacher neuron\u2019s weights as a guide to determine its own ternary weights using two thresholds tloi and thii on the teacher neuron\u2019s weights. This step is called the weight ternarization. In order to have a ternary neuron output, we have a step activation function of two thresholds bilo and bi\nhi . The output ternarization step determines these. Figure 1 depicts the ternarization procedure for a sample neuron. In the top row, we plot the distributions of the weights, activations and ternary output of a sample neuron in the teacher network respectively. The student neuron\u2019s weight distribution that is determined by tloi and thii is plotted below the teacher\u2019s weight distribution. We use the transfer function output of the student neuron, grouped according to the teacher neuron\u2019s output on the same input, to determine the thresholds for the step activation function. In this way, the resulting output distribution for both the teacher and the student neurons are similar. In the following sub sections we provide the details of each step."}, {"heading": "2.2.1 Output Ternarization", "text": "The student network uses a two-thresholded step activation function to have ternary output as described in Table 1. Output ternarization finds the step activation function\u2019s thresholds bilo and bihi , for a ternary neuron i, for a given set of ternary weights W. In order to achieve this, we compute three different transfer function output distributions for the student neuron, using the teacher neuron\u2019s ternary output value on the same input. We use y\u2212 to denote the set\ntlo thi 0\n50\n100\n150\n200 W\n-1 -0.5 0 0.5 1 0\n500\n1000\n1500\n2000 tanh(WTx+ b)\n-1 0 1 0\n1\n2\n3\n4 #10 4 nt\n1000 W\n2000 y!;y0;y+\n4 #10 4 ns\nof transfer function outputs of the student neuron for which the teacher neuron\u2019s output value is \u22121. y0 and y+ are defined in the same way for teacher neuron output values 0 and 1, respectively.\nWe use a simple classifier to find the boundaries between these three clusters of student neuron transfer function outputs, and use the boundaries as the two thresholds bilo and bihi of the step activation function. The classification is done by using a linear discriminant on the kernel density estimates of the three distributions. The discriminant between y+ and y0 is selected as the bihi , and the discriminant between y\u2212 and y0 gives the bilo ."}, {"heading": "2.2.2 Weight Ternarization", "text": "During weight ternarization, the order and the sign of the teacher network\u2019s weights are preserved. We ternarize the weights of the ith neuron of the teacher network using two thresholds tloi and thii such that min(Wi) 6 tloi 6 0 and 0 6 thii 6 max(Wi). The weights for the ith student neuron are obtained by weight ternarization as follows\nternarize(Wi|tloi , thii ) = Wi = [wj] (1)\nwhere\nwj =  \u22121 if wj < tloi 0 if tloi > wj > t hi i\n1 if wj > thii\n(2)\nWe find the optimal threshold values for the weights by evaluating the quality of ternarization with a score function. For a given neuron with p positive weights and n negative weights, the total number of possible ternarization schemes is np since we respect the original sign and order of weights. For a given configuration,\nfor the positive and negative threshold values thi and tlo , we calculate the following score for assessing the performance of the ternary network, mimicking the original network.\nStlo ,thi = \u2211 d p(nt = \u00b11|xtd)I(n s=\u00b11|xsd)p(nt = 0|xtd)I(n s=0|xsd) (3)\nwhere nt and ns denote the output of the teacher neuron and student neuron, respectively. xtd is the input d of the layer to the teacher network, and x s d is the input to the student network. Note that xtd 6= xsd after the of the first layer. Since we ternarize the network in a feed-forward manner, in order to prevent ternarization errors from propagating to upper layers, we always use the teacher\u2019s original input to determine its output probability distribution. p(nt|xtd) is calculated using stochastic firing as described in Table 1. p(ns|xsd) is calculated using the ternary weights W with the current configuration of tlo , thi , and the step activation function thresholds. These thresholds, bhi and blo are selected according to the current ternary weight configuration W.\nThe optimal ternarization of weights is determined by selecting the configuration with the maximum score.\nW\u2217 = argmax tlo ,thi Stlo ,thi (4)\nThe worst case time complexity of the algorithm is O(\u2016W\u20162). We propose using a greedy dichotomic search strategy instead of a fully exhaustive one. We make a search grid over n candidate values for tlo by p values for thi . We select two equally spaced pivot points along one of the dimensions, n or p. Using these pivot points, we calculate the maximum score along the other axis. We reduce the search space by selecting the region maximum point lies in. Since we have two points, we reduce the search space to two-thirds at each step. Then, we repeat the search procedure in the reduced search space. This faster strategy runs in O(log2\u2016W\u2016), and when there are no local maxima it is guaranteed to find the optimal solution. When there are multiple local extremum, it may get stuck. Fortunately, we can detect the possible sub-optimal solutions, using the score values we obtain for the student neuron. By using a threshold on the output score for a student neuron, we can selectively use exhaustive search on a subset of neurons. Empirically, we find these cases to be rare. We provide a detailed analysis in Section 4.1.\nThe ternarization of the output layer is slightly different since it is a soft-max classifier. In the ternarization process, instead of using the teacher network\u2019s output, we use the actual labels in the training set. Again, we treat neurons independently but we make several iterations over each output neuron in a round-robin fashion. After each iteration we check against convergence. In our experiments, we observed that the method converges after a few passes over all neurons.\nOur layer-wise approach allows us to update the weights of the teacher network before ternarization of any layer. For this optional weight update, we\nuse a staggered retraining approach in which only the non-ternarized layers are modified. After the teacher network\u2019s weights are updated, input to a layer for both teacher and student networks become equal, xtd = x s d. We use early stopping during this optional retraining and we find that a few dozen of iterations suffice."}, {"heading": "3 Hardware", "text": "We devised and implemented on FPGA, a hardware architecture that exploits the fact that input values and neuron weights are restricted to ternary values, {\u22121, 0,+1} . These values are represented on 2 bits using usual two\u2019s complement encoding. Figure 2 illustrates the implementation of a 3-layer NN. The design is split into several blocks connected to each other. These blocks form a pipeline that correspond to the sequence of the NN processing steps. We assume that a given NN configuration is used for a large number of classification operations. Thus, for area and power efficiency reasons, we can exploit embedded memory blocks for storing at run-time, the neuron weights and output ternarization thresholds blo and bhi . The compute part of each neuron is an instance of a small component containing a ternary multiplier (two logic gates) and a small accumulator (a few tens of gates). All neurons work in parallel so that one new item is processed per clock cycle. Since layers are pipelined, each of them simultaneously work on a different set of inputs, i.e. layer 2 processes image n while layer 1 processes image n + 1. The ternarization block processes the neuron outputs sequentially, so it simply consists of two signed comparators and a multiplexer.\nWhen using FPGA technology, the actual board can be chosen according to the accuracy/throughput/power/price trade-off required. In this paper, we use the Sakura board [11] for experimenting. It can accommodate a 1024 neuron, 3-layer NN using 81% of the Kintex-7 160T FPGA. With a 200 MHz clock frequency and frames of size 784 (MNIST dataset), the throughput (here limited by the number of neurons) is 195K images/s with a power consumption of 3.8W and a classification latency of 20.5 \u00b5s."}, {"heading": "4 Experiments", "text": "We perform our experiments on the MNIST database of handwritten digits [12], a well-studied database for benchmarking methods on real-world data. MNIST has a training set of 60K examples, and a test set of 10K examples of 28x28 gray-scale images. We use the last 10K samples of the training set as a validation set for early stopping and model selection.\nWe experiment with both multi-layer perceptrons (MLP) in a permutationinvariant manner and convolutional neural networks (CNN). For the MLPs, we experiment with different architectures in terms of depth and neuron count. We use 250, 500, 750, and 1000 neurons per layer for 2, 3, and 4 layer networks. For the CNNs, we use a LENET-like architecture with 10 and 15 filters in the first and second convolutional layers, followed by two fully connected layers with 100 and 10 neurons. Our main goals of the experiments are to demonstrate, (i) the performance of the ternarization procedure with respect to the real-valued teacher network, (ii) the classification performance of TNNs on MNIST, and (iii) the resource-efficiency, speed and throughput of TNNs deployed on our purpose-built hardware.\nFor that reason, we only use vanilla versions of the networks. We minimize cross entropy loss using stochastic gradient descent with a mini-batch size of 100. During training we use random rotations up to \u00b110 degrees. We report the test error rate associated with the best validation error rate after 1000 epochs. We do not preform any preprocessing on the dataset, other than binarization which is a requirement of our hardware."}, {"heading": "4.1 Ternarization Performance", "text": "The ternarization performance, is the ability of the student network to imitate the behavior of it\u2019s teacher. We measure this by using the accuracy difference between the teacher network and the student network. Table 2 shows this difference between the teacher and student networks on training and test sets for three different exhaustive search threshold values. \u03b5 = 1 corresponds to the fully exhaustive search case whereas \u03b5 = 0 represents fully dichotomic search. The results show that the ternarization performance is better for deeper networks. Since we always use the teacher network\u2019s original output as a reference, errors are not amplified in the network. On the contrary, deeper networks allow the student network to correct some of the mistakes in the upper layers, dampening the errors. Also, we perform a retraining step with early stopping before ternarizing a layer, since it slightly improves the performance. The ternarization performance generally decreases with lower \u03b5 threshold values, but the decrease is marginal. On occasion, performance has been seen to increase. These are due to teacher network\u2019s weight update, that allows the network to escape from a local minima. In order to demonstrate the effect of \u03b5 in terms of run-time and classification performance, we conduct a detailed analysis without the optional staggered retraining. Figure 3 shows the distribution of the ratio of neurons that are ternarized exhaustively with different \u03b5, together with the performance gaps\n0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 \"\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 100 N eu ro n s (% )\nPercentage of Neurons for Exhaustive Search\n0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 \"\n0\n0.1\n0.2\nE rr\no r \"\nPerformance on Training Data\n0.83 0.84 0.85 0.86 0.87 0.88 0.89 0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 \"\n-0.1\n0\n0.1\n0.2\nE rr\no r \"\nPerformance on Test Data\nFigure 3: The effect of threshold values on run-time and classification performance\non training and test datasets. The optimal trade-off is achieved with \u03b5 = 0.95. Exhaustive search is used for only 20% of the neurons, and the expected value of accuracy gaps is practically 0. For the largest layer with 1000 neurons, the ternarization operations take 2min and 63min for dichotomic and exhaustive search, respectively, on a 40-core Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz server with 128 GB RAM. For the output layer, the ternarization time is reduced to 21min with exhaustive search."}, {"heading": "4.2 Classification Performance", "text": "The classification performance on the MNIST dataset is provided in Table 3. We also compare ternary NNs performance to other solutions that exist in the literature that we discuss in Section 5 in more detail. We trained Binarized NNs using the code provided by the authors. In order to allow a fair comparison, we binarize the input to their algorithm as required by our purpose-built hardware. We also cite the reported performance of Bitwise NNs [6] with 1024 neurons in 3 layers. For EBP, we use the results provided in [13] and map the results to the closest architecture in our experiments. Note that the reported results are\nobtained with normalized real-valued input and real-valued weights. We know that using real-valued input can be a game changer in terms of classification performance although it is not fully energy-efficient. For TrueNorth [8], we only cite the relevant accuracy results in Table 3. A more detailed comparison is provided in terms of energy efficiency in the following section.\nThe results show that the other methods fail to perform well with smaller networks. Ternary NNs, however, consistently outperform other techniques, with the exception of the Bitwise NN with 3 layers of 1024 neurons. The sole reason for this, is that our teacher network\u2019s performance for the largest configuration is already worse than other configurations. Since we do not impose any restrictions on the weights during initial training, bigger networks decrease the generalization performance due to over-fitting. With more appropriately sized networks, our final performance is much better. In that respect, we argue that TNNs avoid the need for over-parametrization and performs equally well even with the smaller networks, which are more suited for MNIST. For instance, the maxout networks\u2019 best accuracy on MNIST is achieved with a two layer network that has 240 neurons in each of them [14].\nFor the convolutional TNNs, the teacher network\u2019s performance is 97.84%, whereas the student TNN achieves 96.58%. The performance gap between the teacher and the student in CNN case is higher than MLPs. This is due to the weight sharing concept in CNNs. The number of neurons and the weights are much smaller than MLPs but they are replicated all over the input space. This makes student network\u2019s job slightly harder in mimicing the original CNNs behavior. We plot example filters for convolutional TNNs in Figure 4. Like their real-valued counterparts, convolutional TNNs also learn more generic shape descriptors in the first layer and more specialized filters in the second layer.\nThe performance of TNNs for both MLPs and CNNs are bounded by the performance of the teacher network. Since we do not impose any restrictions on the weights, state-of-the-art techniques can be used to further improve the performance. In this study, we consider only the vanilla versions of the networks and leave improving the performance of the teacher networks using data preprocessing techniques and optimized hyper-parameters as a future work."}, {"heading": "4.3 Hardware Performance", "text": "The performance of our hardware solution in terms of latency, throughput and energy efficiency is given in Table 4. We know that TrueNorth can operate at the two extremes of power consumption and accuracy. It consumes 0.268 \u00b5J with a network of low accuracy (92.7%), and consumes as high as 108 \u00b5J with a committee of 64 networks that achieves 99.4%. Our hardware cannot operate at these two extremes, yet in the middle operating zone, we outperform TrueNorth both in terms of energy-efficiency - accuracy trade-off and speed. TrueNorth consumes 4 \u00b5J with 95% accuracy with a throughput of 1000 images/s, and with 1ms latency. Our TNN hardware, consuming 3.63 \u00b5J achieves 98.14% accuracy at a rate of 255 102 images/s, and a latency of 8.09 \u00b5s. Moreover, if our FPGA design was built as an ASIC, it could use even less power by an order of magnitude [15]. Finally, we note that our hardware is capable of running any binarized network since TNNs are a more general form of their binary counterparts."}, {"heading": "5 Discussion and Related Work", "text": "Courbariaux et al. [16] propose the BinaryConnect (BC) method for binarizing only the weights, leaving the inputs and the activations as real-values. They use the back-propagation algorithm with an additional weight binarization step. In the forward pass, weights are binarized either deterministically using their sign, or stochastically. Stochastic binarization converts the real-valued weights\nto probabilities with the use of the hard-sigmoid function, then decides the final value of the weight with this. In the back-propagation phase, they use a quantization mechanism so that the multiplication operations are converted to bit-shift operations [17]. While this binarization scheme helps reducing the number of multiplications during training and testing, it is not fully hardwarefriendly. More recently, the same authors extend their idea to the activations of the neurons also [7]. In Binarized NN, they use sign activation function for obtaining binary neurons.\nSoudry et al. [18] propose Expectation Backpropagation (EBP), an algorithm for learning the weights of a binary network using a variational Bayes technique. The algorithm can be used to train the network such that, each weight can be restricted to be binary or ternary values. The strength of this approach is that the training algorithm does not require any tuning of hyper-parameters, such as learning rate as in the standard back-propagation algorithm. Also, the neurons in the middle layers are binary, making it hardware-friendly. However, this approach assumes the bias is real and it is not currently applicable to CNNs.\nKim and Smaragdis propose Bitwise NN [6] which is a completely binary approach, where all the inputs, weights, and the outputs are binary. They use a straightforward extension of back-propagation to learn bitwise network\u2019s weights. First, they train a real-valued network by constraining the weights of the network using tanh. They use a tanh non-linearity for the activations to constrain the neuron output to (\u22121, 1). Then, they have a second training step for the binary network, using the real-valued network together with a global sparsity parameter. In each epoch during forward propagation, they binarize the weights and the activations of this binary network using the sign function on the original constrained real-valued parameters and activations.\nRecently, IBM announced an energy efficient TrueNorth chip, designed for spiking neural network architectures [19]. Esser et al. [8] propose an algorithm for training networks that are compatible with IBM TrueNorth chip. Their algorithm is based on backpropagation with two modifications. First, they use Gaussian approximation for the summation of several Bernoulli neurons, and second, they clip the values to satisfy the boundary requirements of TrueNorth chip. They obtain ternary weights by introducing a synaptic connection parameter that determines whether a connection exits. If the connection exists, the sign of the weight is used. They use a threshold activation function to obtain binary neuron outputs.\nIn Table 5, we provide a comparison between the related works and our approach by summarizing the constraints put on the inputs, weights and the activations during training and testing. Unlike other studies, we ternarize the weights and neurons using a step function with two thresholds. In this way, we allow the network to prune the less important connections, and use only the most important weights\u2019 sign. For that reason, our method performs better on the smaller networks also, unlike the other methods. Since they use the sign function for binarization, small weights and larger weights have the same value in the binary network. In order to compensate the effects of this imbalance, they generally use over-parametrized networks to obtain good results. Our Ternary\nNN shows that we can perform nearly as good as the real-valued valued network, even with smaller networks."}, {"heading": "6 Conclusion", "text": "In this study, we proposed TNNs for resource-efficient applications of deep learning. Our TNNs have shown to outperform referenced resource-efficient DNNs with regards to accuracy. In tandem with our TNNs, our hardware, offers significant throughput and latency improvements too. Where both optimal classification accuracy and energy efficiency is required, we surpass previous works. We present a pipelined FPGA-based architecture that takes advantage of the assumption that the same NN configuration is reused for many operations. In doing so, embedded memory is exploited and parallel execution is performed on input streams. Resulting throughput of ternary DNNs exhibit 255 times the rate of TrueNorth. At the same time, it exhibits a lower power consumption per classification. We propose a teacher-student approach for training TNNs with weights constrained to {\u22121, 0, 1}. We allow each neuron to choose a sparsity parameter for itself, an opportunity to remove the weights that have very little contribution. In that respect, a TNN inherently prunes the unnecessary connections. This scheme helps to prevent over-parametrization observed in other variants of resource-efficient DNNs. Future research will focus on training better teacher networks using state-of-the-art techniques to further improve the accuracy of TNNs."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel rahman Mohamed", "Geoffrey Hinton"], "venue": "In ICASSP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "In ICLR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Bitwise neural networks", "author": ["Minje Kim", "Paris Smaragdis"], "venue": "In International Conference on Machine Learning (ICML) Workshop on Resource-Efficient Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1", "author": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Backpropagation for energy-efficient neuromorphic computing", "author": ["Steve K Esser", "Rathinakumar Appuswamy", "Paul Merolla", "John V Arthur", "Dharmendra S Modha"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1929}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Training binary multilayer neural networks for image classification using expectation backpropagation", "author": ["Zhiyong Cheng", "Daniel Soudry", "Zexi Mao", "Zhenzhong Lan"], "venue": "arXiv preprint arXiv:1503.03562,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Measuring the Gap Between FPGAs and ASICs", "author": ["Ian Kuon", "Jonathan Rose"], "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Neural networks with few multiplications", "author": ["Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["Daniel Soudry", "Itay Hubara", "Ron Meir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["Paul A Merolla", "John V Arthur", "Rodrigo Alvarez-Icaza", "Andrew S Cassidy", "Jun Sawada", "Filipp Akopyan", "Bryan L Jackson", "Nabil Imam", "Chen Guo", "Yutaka Nakamura", "Bernard Brezzo", "Ivan Vo", "Steven K. Esser", "Rathinakumar Appuswamy", "Brian Taba", "Arnon Amir", "Myron D. Flickner", "William P. Risk", "Rajit Manohar", "Dharmendra S. Modha"], "venue": "Science, 345(6197):668\u2013673,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNNs) have achieved state-of-the-art results on a wide range of AI tasks including computer vision [1], speech recognition [2] and natural language processing [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "Deep neural networks (DNNs) have achieved state-of-the-art results on a wide range of AI tasks including computer vision [1], speech recognition [2] and natural language processing [3].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "Deep neural networks (DNNs) have achieved state-of-the-art results on a wide range of AI tasks including computer vision [1], speech recognition [2] and natural language processing [3].", "startOffset": 181, "endOffset": 184}, {"referenceID": 3, "context": "compression [4, 5].", "startOffset": 12, "endOffset": 18}, {"referenceID": 4, "context": "compression [4, 5].", "startOffset": 12, "endOffset": 18}, {"referenceID": 5, "context": "The second direction completely eliminates the need for floating-point operations using weights discretization [6, 7, 8], with extreme cases such as binary neural networks eliminating the need for multiplications.", "startOffset": 111, "endOffset": 120}, {"referenceID": 6, "context": "The second direction completely eliminates the need for floating-point operations using weights discretization [6, 7, 8], with extreme cases such as binary neural networks eliminating the need for multiplications.", "startOffset": 111, "endOffset": 120}, {"referenceID": 7, "context": "The second direction completely eliminates the need for floating-point operations using weights discretization [6, 7, 8], with extreme cases such as binary neural networks eliminating the need for multiplications.", "startOffset": 111, "endOffset": 120}, {"referenceID": 8, "context": "The teacher network is trained with stochastic firing using back-propagation, and can benefit from all techniques that exist in the literature such as dropout [9], batch normalization [10], and convolutions.", "startOffset": 159, "endOffset": 162}, {"referenceID": 9, "context": "The teacher network is trained with stochastic firing using back-propagation, and can benefit from all techniques that exist in the literature such as dropout [9], batch normalization [10], and convolutions.", "startOffset": 184, "endOffset": 188}, {"referenceID": 9, "context": "The benefit of this approach is that we can use any technique that already exists for efficient NN training, such as batch normalization [10], dropout [9], etc.", "startOffset": 137, "endOffset": 141}, {"referenceID": 8, "context": "The benefit of this approach is that we can use any technique that already exists for efficient NN training, such as batch normalization [10], dropout [9], etc.", "startOffset": 151, "endOffset": 154}, {"referenceID": 10, "context": "We perform our experiments on the MNIST database of handwritten digits [12], a well-studied database for benchmarking methods on real-world data.", "startOffset": 71, "endOffset": 75}, {"referenceID": 5, "context": "We also cite the reported performance of Bitwise NNs [6] with 1024 neurons in 3 layers.", "startOffset": 53, "endOffset": 56}, {"referenceID": 11, "context": "For EBP, we use the results provided in [13] and map the results to the closest architecture in our experiments.", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "11 Binarized NN [7] 6.", "startOffset": 16, "endOffset": 19}, {"referenceID": 5, "context": "74 Bitwise NN [6] 1.", "startOffset": 14, "endOffset": 17}, {"referenceID": 11, "context": "36 EBP [13] 4.", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "08 TrueNorth [8] 7.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "For TrueNorth [8], we only cite the relevant accuracy results in Table 3.", "startOffset": 14, "endOffset": 17}, {"referenceID": 12, "context": "Moreover, if our FPGA design was built as an ASIC, it could use even less power by an order of magnitude [15].", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "[16] propose the BinaryConnect (BC) method for binarizing only the weights, leaving the inputs and the activations as real-values.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "In the back-propagation phase, they use a quantization mechanism so that the multiplication operations are converted to bit-shift operations [17].", "startOffset": 141, "endOffset": 145}, {"referenceID": 6, "context": "More recently, the same authors extend their idea to the activations of the neurons also [7].", "startOffset": 89, "endOffset": 92}, {"referenceID": 15, "context": "[18] propose Expectation Backpropagation (EBP), an algorithm for learning the weights of a binary network using a variational Bayes technique.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Kim and Smaragdis propose Bitwise NN [6] which is a completely binary approach, where all the inputs, weights, and the outputs are binary.", "startOffset": 37, "endOffset": 40}, {"referenceID": 16, "context": "Recently, IBM announced an energy efficient TrueNorth chip, designed for spiking neural network architectures [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 7, "context": "[8] propose an algorithm for training networks that are compatible with IBM TrueNorth chip.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 51, "endOffset": 54}, {"referenceID": 15, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 137, "endOffset": 140}, {"referenceID": 0, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 192, "endOffset": 198}, {"referenceID": 7, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 251, "endOffset": 254}, {"referenceID": 0, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 255, "endOffset": 261}, {"referenceID": 0, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 270, "endOffset": 276}], "year": 2017, "abstractText": "The computation and storage requirements for Deep Neural Networks (DNNs) are usually high. This issue limit their deployability on ubiquitous computing devices such as smart phones or wearables. In this paper, we propose ternary neural networks (TNNs) in order to make deep learning more resource-efficient. We train these TNNs using a teacher-student approach. Using only ternary weights and ternary neurons, with a step activation function of two-thresholds, the student ternary network learns to mimic the behaviour of its teacher network. We propose a novel, layer-wise greedy methodology for training TNNs. During training, a ternary neural network inherently prunes the smaller weights by setting them to zero. This makes them even more compact thus more resource-friendly. We devise a purpose-built hardware design for TNNs and implement it on FPGA. The benchmark results with our purpose-built hardware running TNNs reveal that, with only 1.24\u03bcJ per image, we can achieve 97.76% accuracy with 5.37\u03bcs latency and with a rate of 255K images per second on MNIST.", "creator": "LaTeX with hyperref package"}}}