{"id": "1609.02036", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "Deep Markov Random Field for Image Modeling", "abstract": "markov design fields ( mrfs ), early formulation widely cited involving systems image modeling, have long ignored plagued by excessive failure of expressive power. this burden is primarily addressed to the incentive that computer image formulations evolve to use simplistic factors helping improve local patterns. within this paper, we stretch beyond such limitations, eventually reveal a novel mrf hypothesis namely targets mirror - connected neurons to express geographically complex combinations among surfaces. despite theoretical analysis, we reveal little inherent connection connects this model and recurrent modeling representations, and thereon derive an approximated feed - forward analogy representation couples multiple pairs along opposite sheets. his formulation combines more expressive power of deep neural networks and the cyclic dependency structure creating mrf in truly robust approach, bringing the modeling field to a larger goal. the feed - forward approximation package allows research help improve efficiently learned from depth. implementation attempts on magnitude scale versus low - level vision tasks show continual improvement representing state - of - the - moment.", "histories": [["v1", "Wed, 7 Sep 2016 15:56:36 GMT  (8113kb,D)", "http://arxiv.org/abs/1609.02036v1", "Accepted at ECCV 2016"]], "COMMENTS": "Accepted at ECCV 2016", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["zhirong wu", "dahua lin", "xiaoou tang"], "accepted": false, "id": "1609.02036"}, "pdf": {"name": "1609.02036.pdf", "metadata": {"source": "CRF", "title": "Deep Markov Random Field for Image Modeling", "authors": ["Zhirong Wu", "Dahua Lin", "Xiaoou Tang"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Generative image model, MRF, RNN"}, {"heading": "1 Introduction", "text": "Generative image models play a crucial role in a variety of image processing and computer vision tasks, such as denoising [1], super-resolution [2], inpainting [3], and image-based rendering [4]. As repeatedly shown by previous work [5], the success of image modeling, to a large extent, hinges on whether the model can successfully capture the spatial relations among pixels.\nExisting image models can be roughly categorized as global models and lowlevel models. Global models [6,7,8] usually rely on compressed representations to capture the global structures. Such models are typically used for describing objects with regular structures, e.g. faces. For generic images, low-level models are more popular. Thanks to their focus on local patterns instead of global appearance, low-level models tend to generalize much better, especially when there can be vast variations in the image content.\nOver the past decades, Markov Random Fields (MRFs) have evolved into one of the most popular models for low-level vision. Specifically, the clique-based structure makes them particularly well suited for capturing local relations among pixels. Whereas MRFs as a generic mathematical framework are very flexible and provide immense expressive power, the performance of many MRF-based methods still leaves a lot to be desired when faced with challenging conditions.\nar X\niv :1\n60 9.\n02 03\n6v 1\n[ cs\n.C V\n] 7\nMarkov Random Field Deep Neural Networks Applications Texture Synthesis\nThis occurs due to the widespread use of simplistic potential functions that largely restrict the expressive power of MRFs.\nIn recent years, the rise of Deep Neural Networks (DNN) has profoundly reshaped the landscape of many areas in computer vision. The success of DNNs is primarily attributed to its unparalleled expressive power, particularly their strong capability of modeling complex variations. However, DNNs in computer vision are mostly formulated as end-to-end convolutional networks (CNN) for classification or regression. The modeling of local interactions among pixels, which is crucial for many low-level vision tasks, has not been sufficiently explored.\nThe respective strengths of MRFs and DNNs inspire us to explore a new approach to low-level image modeling, that is, to bring the expressive power of DNNs to an MRF formulation. Specifically, we propose a generative image model comprised of a grid of hidden states, each corresponding to a pixel. These latent states are connected to their neighbors \u2013 together they form an MRF. Unlike in classical MRF formulations, we use fully connected layers to express the relationship among these variables, thus substantially improving the model\u2019s ability to capture complex patterns.\nThrough theoretical analysis, we reveal an inherent connection between our MRF formulation and the RNN [9], which opens an alternative way to MRF formulation. However, they still differ fundamentally: the dependency structure of an RNN is acyclic, while that of an MRF is cyclic. Consequently, the hidden states cannot be inferred in a single feed-forward manner as in a RNN. This posts a significant challenge \u2013 how can one derive the back-propagation procedure without a well-defined forward function?\nOur strategy to tackle this difficulty is to unroll an iterative inference procedure into a feed-forward function. This is motivated by the observation that while the inference is iterative, each cycle of updates is still a feed-forward procedure. Following a carefully devised scheduling policy, which we call the Coupled\nAcyclic Passes (CAP), the inference can be unrolled into multiple RNNs operating along opposite directions that are coupled together. In this way, local information can be effectively propagated over the entire network, where each hidden state can have a complete picture of its context from all directions.\nThe primary contribution of this work is a new generative model that unifies MRFs and DNNs in a novel way, as well as a new learning strategy that makes it possible to learn such a model using mainstream deep learning frameworks. It is worth noting that the proposed method is generic and can be adapted to a various problems. In this work, we test it on a variety of low-level vision tasks, including texture synthesis, image super-resolution, and image synthesis."}, {"heading": "2 Related Works", "text": "In this paper, we develop a generative image model that incorporates the expressive power of deep neural networks with an MRF. This work is related to several streams of research efforts, but moves beyond their respective limitations.\nGenerative image models. Generative image models generally fall into two categories: parametric models and non-parametric models. Parametric models typically use a compressed representation to capture an image\u2019s global appearance. In recent years, deep networks such as autoencoders [10] and adversarial networks [11,12] have achieved substantial improvement in generating images with regular structures such as faces or digits. Non-parametric models, including pixel-based sampling [13,14,15] and patch-based sampling [16,17,18], instead rely on a large set of exemplars to capture local patterns. Whereas these methods can produce high quality images with local patterns directly sampled from realistic images. Exhaustive search over a large exemplar set limits their scalability and often leads to computational difficulties. Our work draws inspiration from both lines of work. By using DNNs to express local interactions in an MRF, our model can capture highly complex patterns while maintaining strong scalability.\nMarkov random fields. For decades, MRFs have been widely used for low-level vision tasks, including texture synthesis [19], segmentation [20,21], denoising [1], and super-resolution [2]. Classical MRF models in earlier work [22] use simple hand-crafted potentials (e.g., Ising models [23], Gaussian MRFs [24]) to link neighboring pixels. Later, more flexible models such as FRAME [25] and Fields of Experts [26] were proposed, which allow the potential functions to be learned from data. However, in these methods, the potential functions are usually parameterized as a set of linear filters, and therefore their expressive power remains very limited.\nRecurrent neural networks. Recurrent neural networks (RNNs), a special family of deep models, use a chain of nonlinear units to capture sequential relations. In computer vision, RNNs are primarily used to model sequential changes in videos [27], visual attention [28,29], and hand-written digit recognition [30]. Previous work explores multi-dimensional RNNs [31] for scene labeling [32] as well as object detections [33]. The most related work is perhaps the use of 2D\nRNNs for generating gray-scale textures [34] or color images [35]. A key distinction of these models from ours is that 2D RNNs rely on an acyclic graphs to model spatial dependency, e.g. each pixel depends only on its left and upper neighbors \u2013 this severely limits the spatial coherence. Our model, instead, allows dependencies from all directions via iterative inference unrolling.\nMRF and neural networks. Connections between both models have been discussed long ago [36]. With the rise of deep learning, recent work on image segmentation [37,38] uses mean field method to approximate a conditional random field (CRF) with CNN layers. A hybrid model of CNN and MRF has also been proposed for human pose estimation [39]. These works primarily target prediction problems (e.g. segmentation) and are not as effective at capturing complex pixel patterns in a purely generative way."}, {"heading": "3 Deep Markov Random Field", "text": "The primary goal of this work is to develop a generative model for images that can express complex local relationships among pixels while being tractable for inference and learning. Formally, we consider an image, denoted by x, as an undirected graph with a grid structure, as shown in Figure 2 left. Each node u corresponds to a pixel xu. To capture the interactions among pixels, we introduce, hu, a hidden variable for each pixel denoting the hidden state corresponding to the pixel xu. In the graph, each node u has a neighborhood, denoted by Nu. Particularly, we use the 4-connected neighborhood of a 2D grid in this work.\nJoint Distribution. We consider three kinds of dependencies: (1) the dependency between a pixel xu and its corresponding hidden state hu, (2) the dependency between a hidden state hu and a neighbor hv with v \u2208 Nu, and (3) the dependency between a hidden state hu and a neighboring pixel xv. They are respectively captured by factors \u03b6(xu, hu), \u03c6(hu, hv), and \u03c8(hu, xv). In addition, we introduce a regularization factor \u03bb(hu) for each hidden state, which gives us the leeway to encourage certain distribution over the state values. Bringing these\nfactors together, we formulate an MRF to express the joint distribution:\np(x,h) = 1\nZ \u220f u\u2208V \u03b6(xu, hu) \u220f (u,v)\u2208E (\u03c6(hu, hv)\u03c8(hu, xv)\u03c8(hv, xu)) \u220f u\u2208V \u03bb(hu). (1)\nHere, V and E are respectively the set of vertices and that of the edges in the image graph, Z is a normalizing constant. Figure 2 shows it structure.\nChoices of Factors. Whereas the MRF provides a principled way to express the dependency structure, the expressive power of the model still largely depends on the specific forms of the factors that we choose. For example, the modeling capacity of classical MRF models are limited by their simplistic factors.\nBelow, we discuss the factors that we choose for the proposed model. First, the factor \u03b6(xu, hu) determines how the pixel values are generated from the hidden states. Considering the stochastic nature of natural images, we formalize this generative process as a Gaussian mixture model (GMM). The rationale behind is that pixel values are on a low-dimensional space, where a GMM with a small number of components can usually provide a good approximation to an empirical distribution. Specifically, we fix the number of components to be K, and consider the concatenation of component parameters as the linear transform of the hidden state, hTuQ = ((\u03c0 c u, \u00b5 c u, \u03a3 c u)) K c=1, where Q is a weight matrix of model parameters. In this way, the factor \u03b6(xu, hu) can be written as\n\u03b6(xu, hu) , pGMM(xu|hu) = K\u2211 c=1 \u03c0cuN(xu|\u00b5cu, \u03a3cu). (2)\nTo capture the rich interactions among pixels and their neighbors, we formulate the relational factors \u03c6(hu, hv) and \u03c8(hu, xv) with fully connected forms:\n\u03c6(hu, hv) = exp ( hTuWhv ) , \u03c8(hu, xv) = exp ( hTuRxv ) . (3)\nFinally, to control the value distribution of the hidden states, we further incorporate a regularization term over hu, as\n\u03bb(hu) = exp ( \u22121T \u03b7(hu) ) = exp ( \u2212\u03b7(h(1)u )\u2212 \u00b7 \u00b7 \u00b7 \u2212 \u03b7(h(d)u ) ) . (4)\nHere, \u03b7 is an element-wise nonlinear function and d is the dimension of hu. In summary, the use of GMM in \u03b6(xu, hu) effectively accounts for the variations in pixel generation, the fully-connected factors \u03c6(hu, hv) and \u03c8(hu, xv) enable the modeling of complex interactions among neighbors, while the regularization term \u03bb(hu) provides a way to explicitly control the distribution of hidden states. Together, they substantially increase the capacity of the MRF model.\nInference of Hidden States. With this MRF formulation, the posterior distribution of the hidden state hu, conditioned on all other variables, is given by\np (hu | xu, xNu , hNu) \u221d \u03b6(xu, hu)\u03bb(hu) \u00b7 \u220f\nv\u2208Nu\n\u03c6(hu, hv)\u03c8(hu, xv). (5)\nHere, hu depends on its neighboring states, the corresponding pixel values, as well as that of its neighbors. Since the pixel xu and its neighboring pixels xNu are highly correlated, to simplify our later computations, we approximate the posterior distribution as,\np (hu | xu, xNu , hNu) ' p (hu | xNu , hNu) \u221d \u03bb(h) \u220f\nv\u2208Nu\n\u03c6(h, hv)\u03c8(h, xv). (6)\nWe performed numerical simulations for this approximation. They are indeed very close to each other, as illustrated in Figure 3. Consequently, the MAP estimate of hu can be approximately computed from its neighbors. It turns out that this optimization problem has an analytic solution given by,\nh\u0303u = \u03c3 ( \u2211 v\u2208Nu Whv + Rxv ) . (7)\nHere, \u03c3 is an element-wise function that is related to \u03b7 as \u03c3\u22121(z) = \u03b7\u2032(z), where \u03b7\u2032 is the first-order derivative w.r.t. \u03b7, and \u03c3\u22121 the inverse function of \u03c3.\nConnections to RNNs. We observe that Eq.(7) has a form that is similar to the feed-forward computations in Recurrent Neural Networks (RNN) [9]. In this sense, we can view the feed-forward RNN as an MAP inference process for MRF models. Particularly, given the RNN computations in the form of Eq.(7), one can formulate an MRF as in Eq.(1), where regularization function \u03b7 can be derived from \u03c3 according to the relation \u03c3\u22121(z) = \u03b7\u2032(z), as\n\u03b7(h) = \u222b h b \u03c3\u22121(z)dz + C. (8)\nHere, b is the minimum of the domain of h, which can be \u2212\u221e, and C is an arbitrary constant. This connection provides an alternative way to formulate an MRF model. More importantly, in this way, RNN models that have been proven to be successful can be readily transferred to an MRF formulation. Figure 3 shows the regularization functions \u03b7(h) corresponding to popular activation functions in RNNs, such as sigmoid and ReLU [40]."}, {"heading": "4 Learning via Coupled Recurrent Networks", "text": "Except for special cases [41], inference and learning on MRFs is generally intractable. Conventional estimation methods [42,8,43] either take overly long time to train or tend to yield poor estimates, especially for models with a highdimensional parameter space. In this work, we consider an alternative approach to MRF learning, which allows us to draw on deep learning techniques [44,45] that have been proven to be highly effective [40].\nVariational Learning Principle. Estimation of probabilistic models based on the maximum likelihood principle is often intractable when the model contains hidden variables. Expectation-maximization [46] is one of the most widely used ways to tackle this problem, which iteratively calculates the posterior distribution of hi (in E-steps) and then optimizes \u03b8 (in M-steps) as\n\u03b8\u0302 = argmax \u03b8\n1\nn n\u2211 i=1 Ep(hi|xi,\u03b8) {log p(xi,hi|\u03b8)} . (9)\nHere, \u03b8 = {W,Q,R} is the model parameter, xi is the i-th image, and hi is the corresponding hidden state. As exact computation of this posterior expectation is intractable, we approximate it based on h\u0303i, the MAP estimate of hi, as below:\n\u03b8\u0302 = argmax \u03b8\n1\nn n\u2211 i=1 log p(xi|h\u0303i,\u03b8), with h\u0303i , f(xi,\u03b8). (10)\nThis is the learning objective of our model. Here, f is the function that approximately infers the latent state h\u0303i given an observed image xi. When the posterior distribution p(hi|xi,\u03b8) is highly concentrated, which is often the case in vision tasks, this is a good approximation. For an image x, log p(x|h\u0303,\u03b8) can be further expanded as a sum of terms defined on individual pixels:\nlog p(x|h\u0303,\u03b8) = \u2211 u log pGMM(xu|h\u0303) = \u2211 u log K\u2211 c=1 \u03c0cuN(xu|\u00b5\u0303cu, \u03a3cu), (11)\nwhere \u00b5\u0303cu = \u00b5 c u +\u03a3 c u( \u2211 v h T v )R. For our problem, this learning principle can be interpreted in terms of encoding/decoding \u2013 the hidden states h\u0303 = f(x,\u03b8) can be understood as an representation that encodes the observed patterns in an image x, while log p(x|h\u0303,\u03b8) measures how well h\u0303 explains the observations.\nCoupled Acyclic Passes. In the proposed model, the dependencies among neighbors are cyclic. Hence, the MAP estimate h\u0303 = f(x,\u03b8) cannot be computed in a single forward pass. Instead, Eq.(7) needs to be applied across the graph in multiple iterations. Our strategy is to unroll this iterative inference procedure into multiple feed-forward passes along opposite directions, such that these passes together provide a complete context to each local estimate.\nSpecifically, we decompose the underlying dependency graph G = (V,E), which is undirected, into two acyclic directed graphs Gf = (V,Ef ) and Gb = (V,Eb), as illustrated in Figure 4, such that each undirected edge {u, v} \u2208 E corresponds uniquely to an edge (u, v) \u2208 Ef and an opposite edge (v, u) \u2208 Eb. It can be proved that such a decomposition always exists and that for each node u \u2208 V , the neighborhood Nu can be expressed as Nu = N f (u) \u222a N b(u), where N f (u) and N b(u) are the set of parents of u respectively along Gf and Gb.\nGiven such a decomposition, we can derive an iterative computational procedure, where each cycle couples a forward pass that applies Eq.(7) along Gf and a backward pass1 along Gb. After the t-th cycle, the state hu is updated to\nh(t)u = \u03c3 ( \u2211 v\u2208N f (u) ( Wh(t\u22121)v + Rxv ) + \u2211 v\u2208N b(u) ( Wh(t)v + Rxv )) . (12)\nAs states above, we have Nu = N f (u) \u222a N b(u). Therefore, over a cycle, the updated state hu would incorporate information from all its neighbors. Note that a given graph G can be decomposed in many different ways. In this work, we specifically choose the one that forms the zigzag path. The advantage over a simple raster line order is that zigzag path traverses all the nodes continuously, so that it conserves spatial coherence by making dependence of each node to all the previous nodes that have been visited before. The forward and backward passes resulted from such decomposition are shown in Figure 4.\nThis algorithm has two important properties: First, the acyclic decomposition allows feed-forward computation as in Eq.(7) to be applied. As a result, the entire inference procedure can be viewed as a feed-forward network that couples multiple RNNs operating along different directions. Therefore, it can be learned in a way similar to other deep neural networks, using Stochastic Gradient Descent (SGD). Second, the feedback mechanism embodied by the backward pass facil-\n1 The word forward and backward here means the sequential order in the graph. They are not feed-forward and back-propagation in the context of deep neural networks.\nitates the propagation of local information and thus the learning of long-range dependencies.\nDiscussions with 2D-RNN. Previous work has explored two-dimensional extensions of RNN [31], often referred to as 2D-RNN. Such extensions, however, are formulated upon an acyclic graph, and can be considered as a trimmed down version of our algorithm. A major drawback of 2D-RNN is that it scans the image in a raster line order and it is not able to provide a feedback path. Therefore, the inference of each hidden state can only take into account 1/4 of the context, and there is no way to recover from a poor inference. As we will show in our experiments, this may cause undesirable effects. Whereas bidirectional RNNs [47] may partly mitigate this problem, they decouple the hidden states into multiple ones that are independent apriori, which would lead to consistency issues. Recent work [48] also finds it difficult to use in generative modeling.\nImplementation Details For inference and learning, to make the computation feasible, we just take one forward pass and one backward pass. Thus, each node is only updated twice while being able to use the information from all possible contexts. The training patch size varies from 15 to 25 depending on the specific experiment. Overall, if we unroll the full inference procedure, our model2 is more than thousands of layers deep. We use rmsprop [45] for optimization and we don\u2019t use dropout for regularization, as we find it oscillates the training."}, {"heading": "5 Experiments", "text": "In the following experiments, we test the proposed deep MRF on 3 scenarios for modeling natural images. We first study its basic properties on texture synthesis, and then we apply it on a prediction problem, image super-resolution. Finally, we integrate global CNN models with local deep MRF for natural image synthesis."}, {"heading": "5.1 Texture Synthesis", "text": "The task of texture synthesis is to synthesize new texture images that possess similar patterns and statistical characteristics as a given texture sample. The study of this problem originated from graphics [13,14]. The key to successful texture reproduction, as we learned from previous work, is to effectively capture the local patterns and variations. Therefore, this task is a perfect testbed to assess a model\u2019s capability of modeling visual patterns.\nOur model works in a purely generative way. Given a sample texture, we train the model on randomly extracted patches of size 25\u00d7 25, which are larger than most texels in natural images. We set K = 20, initialize x and h to zeros, and train the model with back-propagation along the coupled acyclic graph. With a trained model, we can generate textures by running the RNN to derive the\n2 code available at https://github.com/zhirongw/deep-mrf\nlatent states and at the same time sampling the output pixels. As our model is stationary, it can generate texture images of arbitrary sizes.\nWe work on two texture datasets, Brodatz [49] for grayscale images, and VisTex [50] for color images. From the results shown in Figure 5, our synthesis visually resembles to high resolution natural images, and the quality is close to the non-parametric approach [13]. We also compare with the 2D-RNN. [34]. As we can see, the results obtained using 2D-RNN, which synthesizes based only on the left and upper regions, exhibit undesirable effects and often evolve into blacks in the bottom-right parts.\nTwo fundamental parameters control the behaviors of our texture model. The training patch size decides the farthest spatial relationships that could be learned from data. The number of gaussian mixtures control the dynamics of the texture landscape. We analyze our model by changing the two parameters. As shown in Figure 6, bigger training patch size and bigger number of mixtures consistently improves the results. For non-parametric approaches, bigger patch size would dramatically bring up the computation cost. While for our model, the inference time holds the same regardless of the patch size that the model is trained on. Moreover, our parametric model is able to scale to large dataset without bringing additional computations."}, {"heading": "5.2 Image Super-Resolution", "text": "Image super-resolution is a task to produce a high resolution image given a single low resolution one. Whereas previous MRF-based models [2,55] work reasonably,\nthe quality of their products is inferior to the state-of-the-art models based on deep learning [52,54]. With deep MRF, we wish to close the gap.\nUnlike in texture synthesis, the generation of this task is driven by a lowresolution image. To incorporate this information, we introduce additional connections between the hidden states and corresponding pixels of the low-resolution image, as shown in Figure 7. It is noteworthy that we just input a single pixel (instead of a patch) at each site, and in this way, we can test whether the model can propagate information across the spatial domain. As the task is deterministic, we use a GMM with a single component and fix its variance. In the testing stage, we output the mean of the Gaussian component at each location as the inferred high-resolution pixel. This approach is very generic \u2013 the model is not specifically tuned for the task and no pre- and post-processing steps are needed.\nWe train our model on a widely used super-resolution dataset [56] which contains 91 images, and test it on Set5, Set14, and BSD100 [57]. The training is on patches of size 16\u00d7 16 and rmsprop with momentum 0.95 is used. We use PSNR for quantitative evaluation. Following previous work, we only consider the luminance channel in the YCrCb color space. The two chrominance channels are upsampled with bicubic interpolation.\nAs shown in Table 1 and Table 2, our approach outperforms the CNN-based baseline [52] and compares favorably with the state-of-the-art methods dedicated to this task [53,54]. One possible explanation for the success is that our model not only learns the mapping, but also learns the image statistics for high reso-\nlution images. The training procedure which unrolls the RNN into thousands of steps that share parameters also reduces the risk of overfitting. The results also demonstrate the particular strength of our model in handling large upscaling factors and difficult images. Figure 8 shows several examples visually."}, {"heading": "5.3 Natural Image Synthesis", "text": "Images can be roughly considered as a composition of textures with the guidance of scene and object structures. In this task, we move beyond the synthesis of homogeneous textures, and try to generate natural images with structural guidance.\nWhile our model excels in capturing spatial dependencies, learning weak dependencies across the entire image is both computationally infeasible and analytically inefficient. Instead, we adopt a global model to capture the overall structure and use it to provide contextual guidance to MRF. Specifically, we incorporate the variational auto-encoder (VAE) [10] for this purpose \u2013 VAE generates feature maps at each location and our model uses that feature to render the final image (see Figure 7). Such features may contain information of scene layouts, objects, and texture categories.\nWe train the joint model end-to-end from scratch. During each iteration, the VAE first encodes the image into a latent vector, then decodes it to a feature map with the same size of the input image. We then connect this feature map\nto the latent states of the deep MRF. The total loss is defined as the addition of gaussian mixtures at image space and KL divergence at high-level VAE latent space. For training, we randomly extracts patches from the feature map. The gradients from the deep MRF back to the VAE thus only cover the patches being extracted. During testing, VAE randomly samples from the latent space and decodes it to generate the global feature maps. The output pixels are sampled from the GMM with 10 mixtures along the coupled acyclic graph.\nWe work on the MSRC [58] and SUN database [59] and select some scene categories with rich natural textures, such as Mountains and Valleys. Each category contains about a hundred images. As we will see, our approach generalizes much better than the data-hungry CNN approaches. We train the model on images of size 64\u00d7 64 with a batch size of 4. For each image, we extract 16 patches of size 15\u00d7 15 for training. Figure 9 shows several images generated from our models, in comparison with those obtained from the baselines, namely raw VAE [10] and DCGAN [60]. The CNN architecture is shared for all methods described in the\nDCGAN paper [60] to ensure fair comparison. We can see our model successfully captures a variety of local patterns, such as water, clouds, wall and trees. The global appearance also looks coherent, real and dynamic. The state-of-the-art CNN based models, which focuses too much on global structures, often yield sub-optimal local effects."}, {"heading": "6 Conclusions", "text": "We present a new class of MRF model whose potential functions are expressed by powerful fully-connected neurons. Through theoretical analysis, we draw close connections between probabilistic deep MRFs and end-to-end RNNs. To tackle the difficulty of inference in cyclic graphs, we derive a new framework that decouples a cyclic graph with multiple coupled acyclic passes. Experimental results show state-of-the-art results on a variety of low-level vision problems, which demonstrate the strong capability of MRFs with expressive potential functions.\nAcknowledgment. This work is supported by the Big Data Collaboration Research grant (CUHK Agreement No. TS1610626) and the Early Career Scheme (ECS) grant (No: 24204215). We also thank Aditya Khosla for helpful discussions and comments on a draft of the manuscript."}], "references": [{"title": "Image denoising using scale mixtures of gaussians in the wavelet domain", "author": ["J. Portilla", "V. Strela", "M.J. Wainwright", "E.P. Simoncelli"], "venue": "Image Processing, IEEE Transactions on 12(11)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning low-level vision", "author": ["W.T. Freeman", "E.C. Pasztor", "O.T. Carmichael"], "venue": "International journal of computer vision 40(1)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Image inpainting", "author": ["M. Bertalmio", "G. Sapiro", "V. Caselles", "C. Ballester"], "venue": "Proceedings of the 27th annual conference on Computer graphics and interactive techniques, ACM Press/Addison-Wesley Publishing Co.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Plenoptic modeling: An image-based rendering system", "author": ["L. McMillan", "G. Bishop"], "venue": "Proceedings of the 22nd annual conference on Computer graphics and interactive techniques, ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Statistics of natural images and models", "author": ["J. Huang", "D. Mumford"], "venue": "Computer Vision and Pattern Recognition, 1999. IEEE Computer Society Conference on. Volume 1., IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Face recognition using eigenfaces", "author": ["M.A. Turk", "A.P. Pentland"], "venue": "Computer Vision and Pattern Recognition, 1991. Proceedings CVPR\u201991., IEEE Computer Society Conference on, IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1991}, {"title": "Sparse representation for computer vision and pattern recognition", "author": ["J. Wright", "Y. Ma", "J. Mairal", "G. Sapiro", "T.S. Huang", "S. Yan"], "venue": "Proceedings of the IEEE 98(6)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural computation 14(8)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE 78(10)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1990}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "R Fergus"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Texture synthesis by non-parametric sampling", "author": ["A. Efros", "Leung", "T.K"], "venue": "Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on. Volume 2., IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Fast texture synthesis using tree-structured vector quantization", "author": ["L.Y. Wei", "M. Levoy"], "venue": "Proceedings of the 27th annual conference on Computer graphics and interactive techniques, ACM Press/Addison-Wesley Publishing Co.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Image analogies", "author": ["A. Hertzmann", "C.E. Jacobs", "N. Oliver", "B. Curless", "D.H. Salesin"], "venue": "Proceedings of the 28th annual conference on Computer graphics and interactive techniques, ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Image quilting for texture synthesis and transfer", "author": ["A.A. Efros", "W.T. Freeman"], "venue": "Proceedings of the 28th annual conference on Computer graphics and interactive techniques, ACM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Scene completion using millions of photographs", "author": ["J. Hays", "A.A. Efros"], "venue": "ACM Transactions on Graphics (TOG) 26(3)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Photo clip art", "author": ["J.F. Lalonde", "D. Hoiem", "A.A. Efros", "C. Rother", "J. Winn", "A. Criminisi"], "venue": "ACM transactions on graphics (TOG) 26(3)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Markov random field texture models", "author": ["G.R. Cross", "A.K. Jain"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on (1)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1983}, {"title": "Interactive graph cuts for optimal boundary & region segmentation of objects in nd images", "author": ["Y.Y. Boykov", "M.P. Jolly"], "venue": "Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on. Volume 1., IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Multiscale conditional random fields for image labeling", "author": ["X. He", "R.S. Zemel", "M.\u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "Computer vision and pattern recognition, 2004. CVPR 2004. Proceedings of the 2004 IEEE computer society conference on. Volume 2., IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on (6)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1984}, {"title": "Beitrag zur theorie des ferromagnetismus", "author": ["E. Ising"], "venue": "Zeitschrift f\u00fcr Physik A Hadrons and Nuclei 31(1)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1925}, {"title": "Gaussian Markov random fields: theory and applications", "author": ["H. Rue", "L. Held"], "venue": "CRC Press", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Filters, random fields and maximum entropy (frame): Towards a unified theory for texture modeling", "author": ["S.C. Zhu", "Y. Wu", "D. Mumford"], "venue": "International Journal of Computer Vision 27(2)", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "Fields of experts: A framework for learning image priors", "author": ["S. Roth", "M.J. Black"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. Volume 2., IEEE", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A Graves"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "arXiv preprint arXiv:1502.04623", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Advances in neural information processing systems.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-dimensional recurrent neural networks", "author": ["A. Graves", "S. Fernandez", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:0705.2011", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Scene labeling with lstm recurrent neural networks", "author": ["W. Byeon", "T.M. Breuel", "F. Raue", "M. Liwicki"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks", "author": ["S. Bell", "C.L. Zitnick", "K. Bala", "R. Girshick"], "venue": "arXiv preprint arXiv:1512.04143", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative image modeling using spatial lstms", "author": ["L. Theis", "M. Bethge"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Pixel recurrent neural networks", "author": ["Oord", "A.v.d.", "N. Kalchbrenner", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1601.06759", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Markov random fields and neural networks with applications to early vision problems", "author": ["A. Rangarajan", "R. Chellappa", "B. Manjunath"], "venue": "Citeseer", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1991}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H. Torr"], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L.C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "arXiv preprint arXiv:1412.7062", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint training of a convolutional network and a graphical model for human pose estimation", "author": ["J.J. Tompson", "A. Jain", "Y. LeCun", "C. Bregler"], "venue": "Advances in neural information processing systems.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Markov random field modeling in image analysis", "author": ["S.Z. Li"], "venue": "Springer Science & Business Media", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning in markov random fields using tempered transitions", "author": ["R.R. Salakhutdinov"], "venue": "Advances in neural information processing systems.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research 12", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2011}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society 39(1)", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1977}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on 45(11)", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1997}, {"title": "Bidirectional recurrent neural networks as generative models-reconstructing gaps in time series", "author": ["M. Berglund", "T. Raiko", "M. Honkala", "L. K\u00e4rkk\u00e4inen", "A. Vetek", "J. Karhunen"], "venue": "arXiv preprint arXiv:1504.01575", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "A photographic album for artists and designers", "author": ["P. Brodatz", "A. Textures"], "venue": "1966. Images downloaded in July", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "Vision Texture Database", "author": ["M.M. Lab"], "venue": "http://http://vismod.media.mit.edu", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2002}, {"title": "A+: Adjusted anchored neighborhood regression for fast super-resolution", "author": ["R. Timofte", "V. De Smet", "L. Van Gool"], "venue": "Computer Vision\u2013ACCV 2014. Springer", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning a deep convolutional network for image super-resolution", "author": ["C. Dong", "C.C. Loy", "K. He", "X. Tang"], "venue": "Computer Vision\u2013ECCV 2014. Springer", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}, {"title": "Single image super-resolution from transformed self-exemplars", "author": ["J.B. Huang", "A. Singh", "N. Ahuja"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, IEEE", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep networks for image superresolution with sparse prior", "author": ["Z. Wang", "D. Liu", "J. Yang", "W. Han", "T. Huang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Markov random fields for super-resolution and texture synthesis", "author": ["W. Freeman", "C. Liu"], "venue": "Advances in Markov Random Fields for Vision and Image Processing 1", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2011}, {"title": "Low-complexity single-image super-resolution based on nonnegative neighbor embedding", "author": ["M. Bevilacqua", "A. Roumy", "C. Guillemot", "M.L. Alberi-Morel"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2012}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring 18 Zhirong Wu, Dahua Lin, Xiaoou Tang ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "Proc. 8th Int\u2019l Conf. Computer Vision. Volume 2.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2001}, {"title": "Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context", "author": ["J. Shotton", "J. Winn", "C. Rother", "A. Criminisi"], "venue": "International Journal of Computer Vision 81(1)", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2009}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba"], "venue": "Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, IEEE", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2010}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Generative image models play a crucial role in a variety of image processing and computer vision tasks, such as denoising [1], super-resolution [2], inpainting [3], and image-based rendering [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "Generative image models play a crucial role in a variety of image processing and computer vision tasks, such as denoising [1], super-resolution [2], inpainting [3], and image-based rendering [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "Generative image models play a crucial role in a variety of image processing and computer vision tasks, such as denoising [1], super-resolution [2], inpainting [3], and image-based rendering [4].", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "Generative image models play a crucial role in a variety of image processing and computer vision tasks, such as denoising [1], super-resolution [2], inpainting [3], and image-based rendering [4].", "startOffset": 191, "endOffset": 194}, {"referenceID": 4, "context": "As repeatedly shown by previous work [5], the success of image modeling, to a large extent, hinges on whether the model can successfully capture the spatial relations among pixels.", "startOffset": 37, "endOffset": 40}, {"referenceID": 5, "context": "Global models [6,7,8] usually rely on compressed representations to capture the global structures.", "startOffset": 14, "endOffset": 21}, {"referenceID": 6, "context": "Global models [6,7,8] usually rely on compressed representations to capture the global structures.", "startOffset": 14, "endOffset": 21}, {"referenceID": 7, "context": "Global models [6,7,8] usually rely on compressed representations to capture the global structures.", "startOffset": 14, "endOffset": 21}, {"referenceID": 8, "context": "Through theoretical analysis, we reveal an inherent connection between our MRF formulation and the RNN [9], which opens an alternative way to MRF formulation.", "startOffset": 103, "endOffset": 106}, {"referenceID": 9, "context": "In recent years, deep networks such as autoencoders [10] and adversarial networks [11,12] have achieved substantial improvement in generating images with regular structures such as faces or digits.", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "In recent years, deep networks such as autoencoders [10] and adversarial networks [11,12] have achieved substantial improvement in generating images with regular structures such as faces or digits.", "startOffset": 82, "endOffset": 89}, {"referenceID": 11, "context": "In recent years, deep networks such as autoencoders [10] and adversarial networks [11,12] have achieved substantial improvement in generating images with regular structures such as faces or digits.", "startOffset": 82, "endOffset": 89}, {"referenceID": 12, "context": "Non-parametric models, including pixel-based sampling [13,14,15] and patch-based sampling [16,17,18], instead rely on a large set of exemplars to capture local patterns.", "startOffset": 54, "endOffset": 64}, {"referenceID": 13, "context": "Non-parametric models, including pixel-based sampling [13,14,15] and patch-based sampling [16,17,18], instead rely on a large set of exemplars to capture local patterns.", "startOffset": 54, "endOffset": 64}, {"referenceID": 14, "context": "Non-parametric models, including pixel-based sampling [13,14,15] and patch-based sampling [16,17,18], instead rely on a large set of exemplars to capture local patterns.", "startOffset": 54, "endOffset": 64}, {"referenceID": 15, "context": "Non-parametric models, including pixel-based sampling [13,14,15] and patch-based sampling [16,17,18], instead rely on a large set of exemplars to capture local patterns.", "startOffset": 90, "endOffset": 100}, {"referenceID": 16, "context": "Non-parametric models, including pixel-based sampling [13,14,15] and patch-based sampling [16,17,18], instead rely on a large set of exemplars to capture local patterns.", "startOffset": 90, "endOffset": 100}, {"referenceID": 17, "context": "Non-parametric models, including pixel-based sampling [13,14,15] and patch-based sampling [16,17,18], instead rely on a large set of exemplars to capture local patterns.", "startOffset": 90, "endOffset": 100}, {"referenceID": 18, "context": "For decades, MRFs have been widely used for low-level vision tasks, including texture synthesis [19], segmentation [20,21], denoising [1], and super-resolution [2].", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "For decades, MRFs have been widely used for low-level vision tasks, including texture synthesis [19], segmentation [20,21], denoising [1], and super-resolution [2].", "startOffset": 115, "endOffset": 122}, {"referenceID": 20, "context": "For decades, MRFs have been widely used for low-level vision tasks, including texture synthesis [19], segmentation [20,21], denoising [1], and super-resolution [2].", "startOffset": 115, "endOffset": 122}, {"referenceID": 0, "context": "For decades, MRFs have been widely used for low-level vision tasks, including texture synthesis [19], segmentation [20,21], denoising [1], and super-resolution [2].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "For decades, MRFs have been widely used for low-level vision tasks, including texture synthesis [19], segmentation [20,21], denoising [1], and super-resolution [2].", "startOffset": 160, "endOffset": 163}, {"referenceID": 21, "context": "Classical MRF models in earlier work [22] use simple hand-crafted potentials (e.", "startOffset": 37, "endOffset": 41}, {"referenceID": 22, "context": ", Ising models [23], Gaussian MRFs [24]) to link neighboring pixels.", "startOffset": 15, "endOffset": 19}, {"referenceID": 23, "context": ", Ising models [23], Gaussian MRFs [24]) to link neighboring pixels.", "startOffset": 35, "endOffset": 39}, {"referenceID": 24, "context": "Later, more flexible models such as FRAME [25] and Fields of Experts [26] were proposed, which allow the potential functions to be learned from data.", "startOffset": 42, "endOffset": 46}, {"referenceID": 25, "context": "Later, more flexible models such as FRAME [25] and Fields of Experts [26] were proposed, which allow the potential functions to be learned from data.", "startOffset": 69, "endOffset": 73}, {"referenceID": 26, "context": "In computer vision, RNNs are primarily used to model sequential changes in videos [27], visual attention [28,29], and hand-written digit recognition [30].", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "In computer vision, RNNs are primarily used to model sequential changes in videos [27], visual attention [28,29], and hand-written digit recognition [30].", "startOffset": 105, "endOffset": 112}, {"referenceID": 28, "context": "In computer vision, RNNs are primarily used to model sequential changes in videos [27], visual attention [28,29], and hand-written digit recognition [30].", "startOffset": 105, "endOffset": 112}, {"referenceID": 29, "context": "In computer vision, RNNs are primarily used to model sequential changes in videos [27], visual attention [28,29], and hand-written digit recognition [30].", "startOffset": 149, "endOffset": 153}, {"referenceID": 30, "context": "Previous work explores multi-dimensional RNNs [31] for scene labeling [32] as well as object detections [33].", "startOffset": 46, "endOffset": 50}, {"referenceID": 31, "context": "Previous work explores multi-dimensional RNNs [31] for scene labeling [32] as well as object detections [33].", "startOffset": 70, "endOffset": 74}, {"referenceID": 32, "context": "Previous work explores multi-dimensional RNNs [31] for scene labeling [32] as well as object detections [33].", "startOffset": 104, "endOffset": 108}, {"referenceID": 33, "context": "RNNs for generating gray-scale textures [34] or color images [35].", "startOffset": 40, "endOffset": 44}, {"referenceID": 34, "context": "RNNs for generating gray-scale textures [34] or color images [35].", "startOffset": 61, "endOffset": 65}, {"referenceID": 35, "context": "Connections between both models have been discussed long ago [36].", "startOffset": 61, "endOffset": 65}, {"referenceID": 36, "context": "With the rise of deep learning, recent work on image segmentation [37,38] uses mean field method to approximate a conditional random field (CRF) with CNN layers.", "startOffset": 66, "endOffset": 73}, {"referenceID": 37, "context": "With the rise of deep learning, recent work on image segmentation [37,38] uses mean field method to approximate a conditional random field (CRF) with CNN layers.", "startOffset": 66, "endOffset": 73}, {"referenceID": 38, "context": "A hybrid model of CNN and MRF has also been proposed for human pose estimation [39].", "startOffset": 79, "endOffset": 83}, {"referenceID": 8, "context": "(7) has a form that is similar to the feed-forward computations in Recurrent Neural Networks (RNN) [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 39, "context": "Figure 3 shows the regularization functions \u03b7(h) corresponding to popular activation functions in RNNs, such as sigmoid and ReLU [40].", "startOffset": 129, "endOffset": 133}, {"referenceID": 40, "context": "Except for special cases [41], inference and learning on MRFs is generally intractable.", "startOffset": 25, "endOffset": 29}, {"referenceID": 41, "context": "Conventional estimation methods [42,8,43] either take overly long time to train or tend to yield poor estimates, especially for models with a highdimensional parameter space.", "startOffset": 32, "endOffset": 41}, {"referenceID": 7, "context": "Conventional estimation methods [42,8,43] either take overly long time to train or tend to yield poor estimates, especially for models with a highdimensional parameter space.", "startOffset": 32, "endOffset": 41}, {"referenceID": 42, "context": "Conventional estimation methods [42,8,43] either take overly long time to train or tend to yield poor estimates, especially for models with a highdimensional parameter space.", "startOffset": 32, "endOffset": 41}, {"referenceID": 43, "context": "In this work, we consider an alternative approach to MRF learning, which allows us to draw on deep learning techniques [44,45] that have been proven to be highly effective [40].", "startOffset": 119, "endOffset": 126}, {"referenceID": 44, "context": "In this work, we consider an alternative approach to MRF learning, which allows us to draw on deep learning techniques [44,45] that have been proven to be highly effective [40].", "startOffset": 119, "endOffset": 126}, {"referenceID": 39, "context": "In this work, we consider an alternative approach to MRF learning, which allows us to draw on deep learning techniques [44,45] that have been proven to be highly effective [40].", "startOffset": 172, "endOffset": 176}, {"referenceID": 45, "context": "Expectation-maximization [46] is one of the most widely used ways to tackle this problem, which iteratively calculates the posterior distribution of hi (in E-steps) and then optimizes \u03b8 (in M-steps) as", "startOffset": 25, "endOffset": 29}, {"referenceID": 30, "context": "Previous work has explored two-dimensional extensions of RNN [31], often referred to as 2D-RNN.", "startOffset": 61, "endOffset": 65}, {"referenceID": 46, "context": "Whereas bidirectional RNNs [47] may partly mitigate this problem, they decouple the hidden states into multiple ones that are independent apriori, which would lead to consistency issues.", "startOffset": 27, "endOffset": 31}, {"referenceID": 47, "context": "Recent work [48] also finds it difficult to use in generative modeling.", "startOffset": 12, "endOffset": 16}, {"referenceID": 44, "context": "We use rmsprop [45] for optimization and we don\u2019t use dropout for regularization, as we find it oscillates the training.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "The study of this problem originated from graphics [13,14].", "startOffset": 51, "endOffset": 58}, {"referenceID": 13, "context": "The study of this problem originated from graphics [13,14].", "startOffset": 51, "endOffset": 58}, {"referenceID": 2, "context": "2 D R N N [3 4 ]", "startOffset": 10, "endOffset": 16}, {"referenceID": 3, "context": "2 D R N N [3 4 ]", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": "g ra p h ic s[ 1 3 ]", "startOffset": 13, "endOffset": 20}, {"referenceID": 2, "context": "g ra p h ic s[ 1 3 ]", "startOffset": 13, "endOffset": 20}, {"referenceID": 48, "context": "We work on two texture datasets, Brodatz [49] for grayscale images, and VisTex [50] for color images.", "startOffset": 41, "endOffset": 45}, {"referenceID": 49, "context": "We work on two texture datasets, Brodatz [49] for grayscale images, and VisTex [50] for color images.", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "From the results shown in Figure 5, our synthesis visually resembles to high resolution natural images, and the quality is close to the non-parametric approach [13].", "startOffset": 160, "endOffset": 164}, {"referenceID": 33, "context": "[34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Whereas previous MRF-based models [2,55] work reasonably,", "startOffset": 34, "endOffset": 40}, {"referenceID": 54, "context": "Whereas previous MRF-based models [2,55] work reasonably,", "startOffset": 34, "endOffset": 40}, {"referenceID": 51, "context": "the quality of their products is inferior to the state-of-the-art models based on deep learning [52,54].", "startOffset": 96, "endOffset": 103}, {"referenceID": 53, "context": "the quality of their products is inferior to the state-of-the-art models based on deep learning [52,54].", "startOffset": 96, "endOffset": 103}, {"referenceID": 55, "context": "We train our model on a widely used super-resolution dataset [56] which contains 91 images, and test it on Set5, Set14, and BSD100 [57].", "startOffset": 61, "endOffset": 65}, {"referenceID": 56, "context": "We train our model on a widely used super-resolution dataset [56] which contains 91 images, and test it on Set5, Set14, and BSD100 [57].", "startOffset": 131, "endOffset": 135}, {"referenceID": 51, "context": "As shown in Table 1 and Table 2, our approach outperforms the CNN-based baseline [52] and compares favorably with the state-of-the-art methods dedicated to this task [53,54].", "startOffset": 81, "endOffset": 85}, {"referenceID": 52, "context": "As shown in Table 1 and Table 2, our approach outperforms the CNN-based baseline [52] and compares favorably with the state-of-the-art methods dedicated to this task [53,54].", "startOffset": 166, "endOffset": 173}, {"referenceID": 53, "context": "As shown in Table 1 and Table 2, our approach outperforms the CNN-based baseline [52] and compares favorably with the state-of-the-art methods dedicated to this task [53,54].", "startOffset": 166, "endOffset": 173}, {"referenceID": 9, "context": "Specifically, we incorporate the variational auto-encoder (VAE) [10] for this purpose \u2013 VAE generates feature maps at each location and our model uses that feature to render the final image (see Figure 7).", "startOffset": 64, "endOffset": 68}, {"referenceID": 50, "context": "Dataset Bicubic A+ [51] CNN [52] SE [53] CSCN [54] Ours Set5 30.", "startOffset": 19, "endOffset": 23}, {"referenceID": 51, "context": "Dataset Bicubic A+ [51] CNN [52] SE [53] CSCN [54] Ours Set5 30.", "startOffset": 28, "endOffset": 32}, {"referenceID": 52, "context": "Dataset Bicubic A+ [51] CNN [52] SE [53] CSCN [54] Ours Set5 30.", "startOffset": 36, "endOffset": 40}, {"referenceID": 53, "context": "Dataset Bicubic A+ [51] CNN [52] SE [53] CSCN [54] Ours Set5 30.", "startOffset": 46, "endOffset": 50}, {"referenceID": 57, "context": "We work on the MSRC [58] and SUN database [59] and select some scene categories with rich natural textures, such as Mountains and Valleys.", "startOffset": 20, "endOffset": 24}, {"referenceID": 58, "context": "We work on the MSRC [58] and SUN database [59] and select some scene categories with rich natural textures, such as Mountains and Valleys.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "Figure 9 shows several images generated from our models, in comparison with those obtained from the baselines, namely raw VAE [10] and DCGAN [60].", "startOffset": 126, "endOffset": 130}, {"referenceID": 59, "context": "Figure 9 shows several images generated from our models, in comparison with those obtained from the baselines, namely raw VAE [10] and DCGAN [60].", "startOffset": 141, "endOffset": 145}, {"referenceID": 59, "context": "DCGAN paper [60] to ensure fair comparison.", "startOffset": 12, "endOffset": 16}], "year": 2016, "abstractText": "Markov Random Fields (MRFs), a formulation widely used in generative image modeling, have long been plagued by the lack of expressive power. This issue is primarily due to the fact that conventional MRFs formulations tend to use simplistic factors to capture local patterns. In this paper, we move beyond such limitations, and propose a novel MRF model that uses fully-connected neurons to express the complex interactions among pixels. Through theoretical analysis, we reveal an inherent connection between this model and recurrent neural networks, and thereon derive an approximated feed-forward network that couples multiple RNNs along opposite directions. This formulation combines the expressive power of deep neural networks and the cyclic dependency structure of MRF in a unified model, bringing the modeling capability to a new level. The feed-forward approximation also allows it to be efficiently learned from data. Experimental results on a variety of low-level vision tasks show notable improvement over state-of-the-arts.", "creator": "LaTeX with hyperref package"}}}