{"id": "1609.04621", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2016", "title": "Factored Neural Machine Translation", "abstract": "we present second new approach for graphical machine modelling ( pet ) using the morphological and psychological decomposition of the dots ( ts ) in the image side of the image array. this architecture highlighted two practical problems realized in mt, firstly dealing with increasing localized target language vocabulary and spelling out finite vocabulary ( oov ) words. by technical means research methodology, we are able mentally specify small vocabulary and reduce processing training time ( for systems applying equivalent target language vocabulary size ). in fact, grammar can produce irrelevant categories, completely missing in the vocabulary. my mandate a morphological analyser also get her single representation of each word ( document, part meaning all tag, event, class, area and number ). we have extended the nmt xml user attention mechanism in order effectively have two different outputs, one ; the translation overlapping neither other for whatever object of my project. the software translation is built using some \\ textit { a priori } linguistic information. specialists compare our extension with a word - based nmt system. the experiments, available on theoretical linguist'77 dataset translating from english near indonesian, report that while the performance do nearly totally increase, the system can manage adding second larger vocabulary and consistently reduce the auxiliary functionality. scientists observe up to 2 % for context improvement based human simulated dictionary context character development setup.", "histories": [["v1", "Thu, 15 Sep 2016 13:15:01 GMT  (356kb,D)", "http://arxiv.org/abs/1609.04621v1", "8 pages, 3 figures"]], "COMMENTS": "8 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mercedes garc\\'ia-mart\\'inez", "lo\\\"ic barrault", "fethi bougares"], "accepted": false, "id": "1609.04621"}, "pdf": {"name": "1609.04621.pdf", "metadata": {"source": "CRF", "title": "Factored Neural Machine Translation", "authors": ["Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Fethi Bougares"], "emails": ["FirstName.LastName@lium.univ-lemans.fr"], "sections": [{"heading": "1 Introduction", "text": "Neural Machine Translation (NMT) has been further developed in the last years (Bahdanau et al., 2014). In contrast to the traditional phrased-based statistical machine translation (Koehn et al., 2007) that automatically translates subparts of the sentences, NMT uses the sequence to sequence of words approach (Cho et al., 2014).\nRecently, NMT has improved the results of the phrased-based systems (Bahdanau et al., 2014). Besides these improvements in NMT, some problems still remain. One problem is the high computational cost of the target word probability due to the softmax that requires to normalize all the output values, see Equation 1:\npi = e oi/\nN\u2211\nr=1\neor for i \u2208 {1, . . . , N} (1)\nwhere oi are the outputs, pi their softmax normalization and N the total number of outputs. In order to solve this issue, a standard technique is to define a short-list containing the most frequent words only. This has the disadvantage of increasing the out of vocabulary (OOV) rate. OOV words correspond to those unseen in the training dataset or which are not included in the vocabulary. They are all considered as unknown words and mapped to the special UNK token.\nJean et al. (2014), proposed to carefully organise the batches so that only a subset of the target vocabulary is possibly generated at training time. This allows the system to perform the softmax only on this subset during training (the complexity remains the same at test time). Another possibility is to define a structured output layer (SOUL) to handle the words not appearing in the shortlist. This allows the system to always apply the softmax normalization on a layer with reduced size (Le et al., 2011).\nRecently, some works have used subword units to translate instead of words. In Sennrich et al. (2015), the rare and some unknown words are encoded as subword units with the Byte Pair Encoding (BPE)\nar X\niv :1\n60 9.\n04 62\n1v 1\n[ cs\n.C L\n] 1\n5 Se\np 20\n16\nmethod. The authors show that this can also generate words which are unseen at training time. As an extreme case, the character-level neural machine translation has been presented in several works (Chung et al., 2016; Ling et al., 2015; Costa-Jussa\u0300 and Fonollosa, 2016) and showed very promising results.\nIn this work we propose an approach using factors as unit level in the output side of the neural network. The factors are referring to the linguistic annotation at word level like the Part of Speech (POS) tags. Moses toolkit (Haddow and Koehn, 2012) for statistical machine translation is able to manage factors information in addition to the words to be able to improve the translation. Some works have used factors as additional information for language modeling (Bilmes and Kirchhoff, 2003; Alexandrescu, 2006). Recently, factors have been used as linguistic input features to improve NMT (Sennrich and Haddow, 2016) as well.\nOur approach differs from previous works in the sense that we use only the linguistic decomposition of the words in the output side. Each word is represented by its lemma along its linguistic factors (POS tag, tense, gender, number and person). By these means, the target vocabulary size is reduced because we do not have to keep all the derived forms of the verbs, nouns, adjectives, etc. Furthermore, we are able to produce new words that are not in the vocabulary using all the derived forms of the lemmas.\nWe use two different outputs for the translation at word level, one output is the lemma of the word and the other output is the rest of the factors mentioned earlier. Multiple output neural networks have been used before (Firat et al., 2016) with the difference that in our approach the system produces both outputs at the same time instead of scheduling them. With both outputs (lemma and factors) we are able to generate the final word using linguistic resources."}, {"heading": "2 Neural Machine Translation", "text": "The encoder-decoder architecture used for NMT consists of two recurrent neural networks (RNN), one for the encoder and the other for the decoder. The encoder maps a source sequence into a continuous space representation and the decoder maps the representation back to a target sequence. Our trained neural translation models are based on a bidirectional encoder-decoder deep neural network equipped with an attention mechanism (Bahdanau et al., 2014), as described in Figure 2.\nThis architecture consists of a bidirectional RNN as an encoder (as seen in stage 1 of Figure 2). An input sentence is encoded in a sequence of annotations (one for each input word), corresponding to the concatenation of the outputs of a forward and a backward RNN. Each annotation represents the full sentence with a strong focus on the current word. The decoder is composed of a conditional RNN as provided for the DL4MT winter school1 (see stage 3 of Figure 2), equipped with an attention mechanism\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/\n1https://github.com/nyu-dl/dl4mt-tutorial\n(stage 2). The attention mechanism aims at providing weights for each annotation in order to generate a context vector (by performing a weighted sum over the annotations). The attention mechanism uses the hidden state at timestep j of the decoder RNN along with the annotation hi to generate a coefficient eij . A softmax operation is performed over those coefficients to generate the annotation weights \u03b1ij . As described in (Bahdanau et al., 2014), the annotation weights can be used to align the input words to the output words. The RNN takes as input the context vector, the embedding of the previous output word (stage 4), and of course its hidden state. Finally, on stage 5 of the Figure 2, the output probabilities of the target vocabulary are computed. The word with the highest probability is selected to be the translation at each timestep. The encoder and the decoder are trained jointly to maximize the conditional probability of the correct translation."}, {"heading": "3 Factors in Neural Machine Translation", "text": "zj\npj\nuj\n+\nlemma factors\nAs we can see, lemmas and factors are generated separately, which in some cases, lead to sequences with different length. To solve this problem, we give priority to the length of the lemmas. Consequently, we constraint the length of the factors sequence to be equal to the length of the lemma sequence. This is motivated by the fact that the lemmas are closer to the final objective (a sequence of words) and that they are the symbols carrying most of the meaning.\nAnother issue is the feedback that the RNN receives. In the word based model, this feedback is the embedding of the previous generated word. Since we have two outputs, we have to decide what will be given to the decoder RNN. Several options are possible and will be explored in this paper (details in section 4.3.2). For the first set of experiments, only the previous lemma embedding was used as feedback (no information from the factors output)."}, {"heading": "3.1 Handling beam search with factors", "text": "The beam search procedure has also been extended with respect to the original approach, since we are actually facing two beams (one for lemmas and one for factors). We need to deal with the multiple outputs because we do not want to rely solely on the lemma sequence to decide which are the best sequences. Then, we merge the two beams. Once the best lemma and factors hypotheses are generated for each partial hypothesis, the cross product of those output spaces is performed. By this mean, each lemma hypothesis is associated with each factors hypothesis. Afterwards, we keep the k-best combinations for each sample, with k being the beam size. Finally, the number of best hypotheses is reduced again to the beam size for further processing."}, {"heading": "3.2 From factors to word", "text": "Once we obtain the factorized outputs from the neural network, we need to fall back to the word representation. This operation is also performed with the MACAON tool, which, given a lemma and some factors, provides the word candidate."}, {"heading": "4 Experiments", "text": "We performed several sets of experiments trying different architectures and vocabulary sizes for Factored NMT (FNMT) and comparing them with the NMT system."}, {"heading": "4.1 Data processing and selection", "text": "We evaluate our approach on the English to French Spoken Language Translation task from IWSLT 2015 evaluation campaign2. A selection method (Rousseau, 2013) has been applied using the available parallel corpora (news-commentary, united-nations, europarl, wikipedia, and two crawled corpora) and Technology Entertainment Design (TED3) corpus as in-domain corpus. We also do a preprocessing to convert html entities and filter out the sentences with more than 50 words for both source and target languages. We finally end with a selected corpus of 2M sentences, 147K unique words for English side and 266K unique words for French side."}, {"heading": "4.2 Training", "text": "We chose the following hyperparameters to train the systems. The embedding and recurrent layers have a dimensionality of 620 and 1000 respectively. We use a minibatch size of 80 sentences trained with Adadelta algorithm. The norm of the gradient is clipped to be no more than 1 (Pascanu et al., 2012) and the weights are initialized with Xavier (Glorot and Bengio, 2010). The validations start at the second epoch and are performed every 5000 updates. Early stopping is based on BLEU with a patience set to 10 (early stopping occurs after 10 evaluations without improvement in BLEU). The vocabulary size of the source languages is set to 30K. We varied the output layer size from 5K to 30K in order to simulate different levels of out of domain data. Once the model is trained, we set the beam size to 12 (as this is the standard value for NMT, (Bahdanau et al., 2014)) when translating the development corpus."}, {"heading": "4.3 Factors models and results", "text": "The Factored NMT system aims at integrating linguistic knowledge into the decoder in order to obtain better performance when facing out of domain data and/or a low resource setup. To assess the feasibility and estimate the potential gain of our approach, we performed a set of experiments reducing the output vocabulary size, simulating such an environment. The results are presented in Table 1.\nThe FNMT system obtains a similar performance compared to the NMT system (first two rows) in terms of word level BLEU score, despite the increased complexity of the architecture of our model (and in particular the two outputs).\nIn order to estimate the capacity of such a model, we computed the oracle which corresponds to ignore the errors caused by the factors, i.e. if we produce the correct lemma, then the correct word is generated (see last column of Table 1). We can see that a potential gain of more than 1.5% BLEU points can be achieved with a perfect modeling of the factors, which is encouraging.\n2IWSLT\u201915: https://sites.google.com/site/iwsltevaluation2015 3TED: https://www.ted.com\nThe first comment is that the Factored NMT approach is able to model a bigger word vocabulary while preserving manageable output layers size. This is due to the fact that the factors-to-word tool is able to generate words which are unseen in the training corpus, augmenting the expressiveness of our model. For the sake of comparison, we provide the target vocabulary size for the standard NMT and the FNMT systems. For example, with an output layer size of 30K, the NMT system can model 30K words against 172K words for the FNMT system. This is an almost 6 times larger word vocabulary.\nOne consequence is that the word coverage is higher for the FNMT than for the NMT system, as shown in column 4. However, for the first two systems (first two rows), we see that the difference between the coverages is small. When decreasing the output layer size, we can observe that the coverage decreases slowly for FNMT systems compared to the word based system. The FNMT approach surpasses standard NMT when the coverage difference becomes higher. This proves that the approach is sound and well performing, when dealing with out-of-domain data. This is of course dependent on the linguistic knowledge available in the factors-to-word tool. This is exactly the sought behavior: by integrating a priori linguistic knowledge, we reduce the impact of the training conditions (domain, data availability, etc.) on the performance of the system.\nThe reduction of the out of vocabulary (OOV) rate of about 47% is a promising result which is not always well reflected by the BLEU score. These results would be better highlighted if performing a human evaluation (this point will not be addressed further in this paper). To make things clear, the OOV rate corresponds to the number of UNK tokens generated by our system. In those experiments, we did not use any specific method to replace them (e.g. put source words aligned to them, use a dictionary, etc.)\nMoreover, the number of parameters to train also decreases according to the size of the output layer, as shown in column 6, allowing a simpler training because we have to learn less weights in the model. For example, using a lemma output layer size of 10K instead of 30K (3 times smaller) for factored model, we obtain a small drop of 0.67 points in BLEU. By contrast, in NMT base model we observe a drop of 2.27 points in BLEU comparing the same output sizes 30K and 10K.\nAnother interesting remark is that the scores evaluating in lemmas and factors are higher than the BLEU in words for both systems, this is due to the difficulty of the final step to generate the words. Nevertheless, the BLEU for factors are pretty low considering that the output layer size for this is only 142. This can be due to two different causes. First, the neural network is not able to correctly model this small output. Second, the task of translating from English words to French factors is complex."}, {"heading": "4.3.1 Evaluating each output", "text": "We evaluated BLEU at different levels (word, lemma or factors) using the base NMT system with only one output (see Table 2). We compare the values with the Factored NMT system results which models lemmas and factors at the same time. We observe that the difference between the results in BLEU for lemmas using the FNMT are similar to the NMT system. However, the differences evaluating factors are big between the two systems (2.44 difference of %BLEU). This experiment confirms that the task to predict factors managing very different output sizes respect to the source words is not easy. In future we will implement factors also in the input side of the neural network to verify this hypothesis. Also, we have to take into account that we are giving more priority to the length of the lemmas sequence than the factors one during beam search. This also suggests that we adapt our architecture so that factors are better predicted to obtain a final better BLEU at word evaluation."}, {"heading": "4.3.2 Feedback", "text": "As explained in section 2, the decoder RNN is a conditional-GRU which is fed by the input context vector, its hidden state and the feedback (i.e. the previous generated symbol). Since we now have two outputs, we need to define what kind of feedback is more suitable for the Factored NMT system. Several solutions are possible.\nThe first assumption we made is highly dependent on the design of the considered factors, i.e. the lemmas are the most informative factors among all. Then, we tried using only the output lemma embedding as feedback (see equation 2).\nLemma feedback : EL[yj ] (2)\nwhere EL is the target language lemma lookup table and EL[yj ] is the embedding of the lemma used to generate the output word yi.\nAnother straightforward operation is to sum the embeddings of the previous lemma with the embedding of the previous factors, as described in equation 3.\nSum feedback : EL[yj ] + EF [yj ] (3)\nwhere yi is the target output word, and EL[yi] and EF [yi] are its corresponding lemma and factors embeddings. While this could seem unnatural, by doing this, we hope to obtain a joint vector representation of both the lemma and the factors.\nFinally, we investigated whether the neural network can learn a better combination of the lemmas and factors embeddings using a linear (eq. 4) or non-linear (eq. 5) operation instead of a simple sum.\nLinear feedback : EL[yj ] \u00b7WL + EF [yj ] \u00b7WF (4) Tanh feedback : tanh (EL[yj ] \u00b7WL + EF [yj ] \u00b7WF ) (5)\nwhere WL and WF are the parameters to be learned.\nTable 3 presents the results obtained with systems integrating the different output embedding combinations as feedback. We can see that all systems perform similarly regarding BLEU score on words with a better result for the lemma feedback. As expected, when using only lemma as feedback, the system better estimates the lemmas probabilities, as a consequence, there is a significant reduction of the performance on factors. The comparison between the lemma %BLEU (fourth column of Table 3) and the number of OOVs (sixth column) shows a correlation between those two values, except when using non-linear combination which has the lowest value of OOVs. This tends to prove that modeling the lemmas better is important to reduce the OOV rate (confirming our assumption that lemmas are more informative) but not sufficient. In the future we would like to explore the combination of the two embeddings using its concatenation to see if we can get better results."}, {"heading": "4.3.3 Dependency model", "text": "One observation that can be made is that while generating factors could seem easier due to the small number of the possible outputs (only 142), the BLEU score is not as high as what we could expect. However, one could argue that generating a sequence of factors in French from a sequence of English words is not an easy task. In order to help the factors prediction, we contextualized the corresponding output with the lemma being generated. This creates a dependency between the lemma output and the factors output. The dependency has been implemented by including a transformer (see Figure 3) which projects the lemma embeddings into the hidden layer used to generate factors. The results by applying those two techniques are presented in Table 4.\nzj\npj\nuj\n+\nlemma factors\nT\nFigure 3: Dependency model\n%BLEU Model Feedback word lemma factors #OOV NMT - 34.88 - - 1775 FNMT with dependency Lemma 34.45 37.45 42.15 770 FNMT with dependency Sum 34.65 37.34 44.35 800 FNMT with dependency Linear 34.25 37.02 43.57 822 FNMT with dependency Tanh 34.38 37.09 43.82 915\nTable 4: Results for dependency model\nIn Table 4, we can observe that the dependency model does not improve the results in terms of %BLEU score on words from Table 3 using lemma, linear and tanh feedback. However, it improves using the sum feedback. For the sum feedback dependency model, we see that lemma BLEU output improves with respect to the same model without dependency. By contrast, the factors output obtains lower BLEU. This can occur because factors output receives more information from lemma and when the factors cost is back-propagated, the lemma output can improve the learning. We can also observe that if we improve lemma output it is more correlated to the word evaluation than if we improve factors output. Moreover, the number of the OOV are reduced for all the feedback combination excepting tanh feedback, which is not reflected by the automatic score."}, {"heading": "4.3.4 Qualitative analysis", "text": "We have observed some of the translation outputs to better understand in what cases our FNMT system performs better or worse than the NMT system.\nTranslation examples with better BLEU performance In the first two examples of Table 5, the FNMT system obtains better BLEU score than the NMT system.\nFirst example shows when our factored system can generate words when the NMT base system predicts unknown words. Firstly, the word lineage in source sentence is translated as the reference (lignee\u0301) by the FNMT system and mapped to UNK by the NMT base system. Secondly, the word adaptive is translated as adaptatifs by the FNMT system, the reference translation is adapte\u0301s, but we can consider the FNMT choice a better translation. NMT system also mapped the word adaptive to UNK.\nIn the second example, FNMT translation performs as the reference. We are able to generate the new word actualise\u0301e (actualiser+past participle+feminine+singular) that it is not in the shortlist of the NMT system vocabulary. This is due, on one hand, because the word actualise\u0301e appears 40 times in the word vocabulary of the NMT system so it is excluded from the shortlist. On the other hand, the lemma actualiser appears 172 times in the lemmas shortlist so it is included and we are able to generate\nactualise\u0301e from the lemma and factors outputs. These examples can show the potential of our FNMT system generating new words and reducing unknown words.\nTranslations with lower BLEU performance\nWe also have extracted some translations where we have seen a lower BLEU from the FNMT system with respect to the NMT base system (see Table 5).\nExample 3 shows a problem with the factors output, from the correct lemma pouvoir, the FNMT system has generated the word pourrais instead of pouvais. We can consider both translations as correct but BLEU score penalizes the FNMT translation.\nFinally, in the last example, we saw that the translation of the FNMT system is more correct than the NMT system because it translated the word and to et but in the reference is not included. In addition, FNMT system translated easy to a synonym (facile) of simple. Consequently, BLEU score penalizes this example in FNMT system being a correct translation."}, {"heading": "5 Conclusion", "text": "In this paper, we have proposed an NMT architecture which produces a factored representation of the target language words. Those factors are based on linguistics a priori knowledge. We showed that we are able to train Factored NMT systems with similar performance to word based systems but with the advantage of modeling an almost 6 times bigger word vocabulary with only a slight increase of the computational cost. A consequence of that is the OOV rate reduction observed with the FNMT system. Also, the use of additional linguistic resources allows us to generate new word forms that would not be included in the standard NMT system shortlist.\nBy reducing the target language vocabulary, we simulated an out-of-domain setup, and we showed that our factored NMT method performs better than the basic NMT system in this case.\nAs future work, we would like to include linguistic features at the input. It is known that this can be helpful for NMT (Sennrich and Haddow, 2016). Extending the approach with input factors could make the target language factors generation simpler. This will be investigated in the future. The proposed Factored NMT method could even show better performance if applied on highly inflected languages like German, Arabic, Czech, Russian or Hindi on the target side."}], "references": [{"title": "Factored neural language models", "author": ["Andrei Alexandrescu"], "venue": null, "citeRegEx": "Alexandrescu.,? \\Q2006\\E", "shortCiteRegEx": "Alexandrescu.", "year": 2006}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Factored language models and generalized parallel backoff", "author": ["Bilmes", "Kirchhoff2003] Jeff A. Bilmes", "Katrin Kirchhoff"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Companion Volume of the Proceedings of HLT-NAACL 2003\u2013short Papers - Volume 2, NAACL-Short", "citeRegEx": "Bilmes et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bilmes et al\\.", "year": 2003}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR, abs/1406.1078", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation. CoRR, abs/1603.06147", "author": ["Chung et al.2016] Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Character-based neural machine translation. CoRR, abs/1603.00810", "author": ["Costa-Juss\u00e0", "Jos\u00e9 A.R. Fonollosa"], "venue": null, "citeRegEx": "Costa.Juss\u00e0 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Costa.Juss\u00e0 et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Firat et al.2016] Orhan Firat", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Bengio2010] Xavier Glorot", "Yoshua Bengio"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS\u201910). Society for Artificial Intelligence and Statistics", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Interpolated backoff for factored translation models. Association for Machine Translation in the Americas, AMTA", "author": ["Haddow", "Koehn2012] Barry Haddow", "Philipp Koehn"], "venue": null, "citeRegEx": "Haddow et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Haddow et al\\.", "year": 2012}, {"title": "On using very large target vocabulary for neural machine translation. CoRR, abs/1412.2007", "author": ["Jean et al.2014] S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": null, "citeRegEx": "Jean et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst"], "venue": "In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Large vocabulary SOUL neural network language models", "author": ["Le et al.2011] Hai-Son. Le", "Ilya Oparin", "Abdel. Messaoudi", "Alexandre Allauzen", "Jean-Luc Gauvain", "Fran\u00e7ois Yvon"], "venue": "In INTERSPEECH", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Character-based neural machine translation. CoRR, abs/1511.04586", "author": ["Ling et al.2015] Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Macaon, an nlp tool suite for processing word lattices", "author": ["Nasr et al.2011] Alexis Nasr", "Fred\u00e9ric B\u00e9chet", "Jean-Fran\u00e7ois Rey", "Beno\u0131\u0302t Favre", "Joseph Le Roux"], "venue": "In Proceedings of the ACL-HLT 2011 System Demonstrations,", "citeRegEx": "Nasr et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nasr et al\\.", "year": 2011}, {"title": "Understanding the exploding gradient problem. CoRR, abs/1211.5063", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "XenC: An open-source tool for data selection in natural language processing", "author": ["Anthony Rousseau"], "venue": "The Prague Bulletin of Mathematical Linguistics,", "citeRegEx": "Rousseau.,? \\Q2013\\E", "shortCiteRegEx": "Rousseau.", "year": 2013}, {"title": "Linguistic input features improve neural machine translation. CoRR, abs/1606.02892", "author": ["Sennrich", "Haddow2016] Rico Sennrich", "Barry Haddow"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units. CoRR, abs/1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "We have extended the NMT approach with attention mechanism (Bahdanau et al., 2014) in order to have two different outputs , one for the lemmas and the other for the rest of the factors.", "startOffset": 59, "endOffset": 82}, {"referenceID": 1, "context": "Neural Machine Translation (NMT) has been further developed in the last years (Bahdanau et al., 2014).", "startOffset": 78, "endOffset": 101}, {"referenceID": 10, "context": "In contrast to the traditional phrased-based statistical machine translation (Koehn et al., 2007) that automatically translates subparts of the sentences, NMT uses the sequence to sequence of words approach (Cho et al.", "startOffset": 77, "endOffset": 97}, {"referenceID": 3, "context": ", 2007) that automatically translates subparts of the sentences, NMT uses the sequence to sequence of words approach (Cho et al., 2014).", "startOffset": 117, "endOffset": 135}, {"referenceID": 1, "context": "Recently, NMT has improved the results of the phrased-based systems (Bahdanau et al., 2014).", "startOffset": 68, "endOffset": 91}, {"referenceID": 11, "context": "This allows the system to always apply the softmax normalization on a layer with reduced size (Le et al., 2011).", "startOffset": 94, "endOffset": 111}, {"referenceID": 9, "context": "Jean et al. (2014), proposed to carefully organise the batches so that only a subset of the target vocabulary is possibly generated at training time.", "startOffset": 0, "endOffset": 19}, {"referenceID": 9, "context": "Jean et al. (2014), proposed to carefully organise the batches so that only a subset of the target vocabulary is possibly generated at training time. This allows the system to perform the softmax only on this subset during training (the complexity remains the same at test time). Another possibility is to define a structured output layer (SOUL) to handle the words not appearing in the shortlist. This allows the system to always apply the softmax normalization on a layer with reduced size (Le et al., 2011). Recently, some works have used subword units to translate instead of words. In Sennrich et al. (2015), the rare and some unknown words are encoded as subword units with the Byte Pair Encoding (BPE) ar X iv :1 60 9.", "startOffset": 0, "endOffset": 613}, {"referenceID": 4, "context": "As an extreme case, the character-level neural machine translation has been presented in several works (Chung et al., 2016; Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016) and showed very promising results.", "startOffset": 103, "endOffset": 175}, {"referenceID": 12, "context": "As an extreme case, the character-level neural machine translation has been presented in several works (Chung et al., 2016; Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016) and showed very promising results.", "startOffset": 103, "endOffset": 175}, {"referenceID": 0, "context": "Some works have used factors as additional information for language modeling (Bilmes and Kirchhoff, 2003; Alexandrescu, 2006).", "startOffset": 77, "endOffset": 125}, {"referenceID": 6, "context": "Multiple output neural networks have been used before (Firat et al., 2016) with the difference that in our approach the system produces both outputs at the same time instead of scheduling them.", "startOffset": 54, "endOffset": 74}, {"referenceID": 1, "context": "Our trained neural translation models are based on a bidirectional encoder-decoder deep neural network equipped with an attention mechanism (Bahdanau et al., 2014), as described in Figure 2.", "startOffset": 140, "endOffset": 163}, {"referenceID": 1, "context": "As described in (Bahdanau et al., 2014), the annotation weights can be used to align the input words to the output words.", "startOffset": 16, "endOffset": 39}, {"referenceID": 13, "context": "The morphological and grammatical analysis is performed with the MACAON toolkit (Nasr et al., 2011).", "startOffset": 80, "endOffset": 99}, {"referenceID": 15, "context": "A selection method (Rousseau, 2013) has been applied using the available parallel corpora (news-commentary, united-nations, europarl, wikipedia, and two crawled corpora) and Technology Entertainment Design (TED3) corpus as in-domain corpus.", "startOffset": 19, "endOffset": 35}, {"referenceID": 14, "context": "The norm of the gradient is clipped to be no more than 1 (Pascanu et al., 2012) and the weights are initialized with Xavier (Glorot and Bengio, 2010).", "startOffset": 57, "endOffset": 79}, {"referenceID": 1, "context": "Once the model is trained, we set the beam size to 12 (as this is the standard value for NMT, (Bahdanau et al., 2014)) when translating the development corpus.", "startOffset": 94, "endOffset": 117}], "year": 2016, "abstractText": "We present a new approach for neural machine translation (NMT) using the morphological and grammatical decomposition of the words (factors) in the output side of the neural network. This architecture addresses two main problems occurring in MT, namely dealing with a large target language vocabulary and the out of vocabulary (OOV) words. By the means of factors, we are able to handle larger vocabulary and reduce the training time (for systems with equivalent target language vocabulary size). In addition, we can produce new words that are not in the vocabulary. We use a morphological analyser to get a factored representation of each word (lemmas, Part of Speech tag, tense, person, gender and number). We have extended the NMT approach with attention mechanism (Bahdanau et al., 2014) in order to have two different outputs , one for the lemmas and the other for the rest of the factors. The final translation is built using some a priori linguistic information. We compare our extension with a word-based NMT system. The experiments, performed on the IWSLT\u201915 dataset translating from English to French, show that while the performance do not always increase, the system can manage a much larger vocabulary and consistently reduce the OOV rate. We observe up to 2% BLEU point improvement in a simulated out of domain translation setup.", "creator": "LaTeX with hyperref package"}}}