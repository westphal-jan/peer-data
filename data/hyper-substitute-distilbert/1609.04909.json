{"id": "1609.04909", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "An Iterative Transfer Learning Based Ensemble Technique for Automatic Short Answer Grading", "abstract": "automatic short phrase delivery ( ac ) techniques are refined to finely assess short answers distinguishing phrases besides natural language, having a drop of a nine words to a few verb. supervised asag techniques were been demonstrated to evolve although typically suffer from a couple of costly conceptual limitations. they may greatly reliant - instructor provided independent approaches and need labeled raw data in the form of paired student answers for multiple learners task. sometimes overcome such, in this situation, studies establish automated automatic variation providing dramatically novel features. two propose an accuracy attack on repeated ensemble of ( a ) defined text classifier of student proficiency and ( b ) a metric highlighting numeric coefficients derived from local similarity measures with respect to model answers. second, we predict sequential correlation grid based linear method requires highly common spatial paradigm of query the second ensemble for not having no labelled data. the proposed technique tracks correlated search winning topic entries on the scientsbank dataset like the student response analysis task base semeval 2013. additionally, we demonstrate confidence and benefits of analyzing proposed technique through evaluation on multiple asag datasets from different subject topics and departments.", "histories": [["v1", "Fri, 16 Sep 2016 04:58:54 GMT  (2515kb,D)", "https://arxiv.org/abs/1609.04909v1", null], ["v2", "Mon, 19 Sep 2016 01:28:17 GMT  (2510kb,D)", "http://arxiv.org/abs/1609.04909v2", null], ["v3", "Mon, 21 Nov 2016 13:44:09 GMT  (1442kb,D)", "http://arxiv.org/abs/1609.04909v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shourya roy", "himanshu s bhatt", "y narahari"], "accepted": false, "id": "1609.04909"}, "pdf": {"name": "1609.04909.pdf", "metadata": {"source": "CRF", "title": "An Iterative Transfer Learning Based Ensemble Technique for Automatic Short Answer Grading", "authors": ["Shourya Roy", "Himanshu S. Bhatt"], "emails": ["shourya.roy@xerox.com", "himanshu.bhatt@xerox.com", "hari@csa.iisc.ernet.in"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nComputer assisted assessment has been prevalent in schools and colleges for many years albeit primarily for questions with constrained answers. To answer such recognition questions e.g. multiple choice questions (MCQs), students typically have to choose the correct answer(s) from a given list of options. Prior work reported in many papers has questioned the effectiveness of such questions to assess knowledge, scholarship, and depth of understanding gathered by students [1], [2]. On the other hand, open-ended questions or recall questions that seek constructed responses from students, reveal their ability to integrate, synthesize, design, and communicate ideas in natural language. An example is shown in Table I. However, automatic assessment of such answers on various scales (binary, ordinal, nominal) has remained a non-trivial challenge owing to multiple reasons. These include linguistic variations of student answers to a question (same answer could be articulated in different ways); lack of uniformity in how instructors provide model answers across questions and datasets (detailed, brief, representative); subjective nature of assessment (multiple possible correct answers or no correct answer); lack of consistency in human rating, etc. Consequently, the task of assessment of answers to recall questions has predominantly remained a repetitive and tedious manual job for teaching\ninstructors. This paper dwells on a computational technique for automatically grading such answers and particularly focuses on short answers which are a few words to a few sentences long (everything in between fill-in-the-gap and essay type answers [3]). This task of automatically grading short answers is referred to as Automatic Short Answer Grading (ASAG).\nA large fraction of prior work in ASAG has been based on supervised learning techniques viz. classification and regression. These techniques extract various features from model and instructor graded student answers using natural language processing (NLP) techniques reflecting similarity (synonymously, overlap, correspondence, entailment etc.) between them. For example, Dzikovska et. al. proposed four lexical similarity metrics viz. the raw number of overlapping words, F1 score, Lesk score and cosine score between student and model answers as features [5]. These features are then fed to various classification or regression techniques to train models which can subsequently be applied to score new student answers automatically. While classification techniques can predict scores directly, continuous valued regression output needs to be discretized based on some thresholding logic (e.g. ceil, floor). Such supervised techniques trained solely on features derived with respect to model answers immediately suffer from a couple of intuitive shortcomings. Firstly, the nature of model answers varies across questions. Consider the two examples shown in Table I: the first question has a very brief model answer compared to the second one. Consequently, the same type of features may not be able to effectively measure similarity of student answers with respective model answers for both questions. Secondly, student answers can be (very) different from the corresponding model answers but could still be correct. Consider an example-seeking question: Give an example of a Java primitive Wrapper class. The model answer may not exhaustively list all possible answers (in fact, it may be impossible in some cases) e.g. Byte, Short, Integer, Long, Float etc. students may write.\nEnsemble Based Supervised ASAG\nTo address the above shortcomings (which leads to the first contribution of this work), we propose a novel supervised ASAG technique based on an ensemble of two classifiers. The first classifier is a text classifier trained using the classical TFIDF representation [6] of bag-of-words (BoW) features\nar X\niv :1\n60 9.\n04 90\n9v 3\n[ cs\n.C L\n] 2\n1 N\nov 2\n01 6\n.of student answers. It is independent of model answers and learns good textual features (words and n-grams) from graded student answers to discriminate between student answers belonging to different scores. The second classifier has features expressed as real numbers indicating similarity of student answers with the corresponding model answer (analogous to model answer based classifiers). We take various lexical, semantic, and vector space based measures to compute these features (a.k.a similarity values). The classifiers complement each other splendidly since the first (text based) classifier is independent of the model answer whereas the second classifier is based on the similarity between the model answer and the student answers. By exploiting student answers directly in the first classifier, additionally, the ensemble can overcome the shortcomings mentioned earlier in this paragraph. While stacking of classifiers has been used in ASAG [7] towards \u201coneshot\u201d combination of predictions from multiple classifiers, the proposed technique is designed in a different (iterative) manner to eliminate the need for extensive labelled data for new questions (will be explained next). We empirically demonstrate that this iterative ensemble outperforms (significantly, in many cases) either of the constituent classifiers (\u00a7 IV-C2).\nTransfer Learning for ASAG\nWhile supervised models have been applied in many reallife scenarios to automate human activities, we opine that ASAG does not readily fit into the same train-once-and-applyforever model. Every assessment task is unique and hence, graded answers from one assessment task cannot readily be used to train a model for another. In today\u2019s world, repetition of questions across different groups of students is a rarity owing to proliferation of sharing and communication channels. Consequently, application of supervised ASAG techniques would require ongoing instructor involvement to create labelled data (by grading 12 to 2 3\nrd of student answers as per typical train-test split guidelines) for every question and assessment task. Requirement of such continuous involvement of instructors limits the benefit of automation and thereby\nposes a hindrance to practical adoption. Towards addressing this limitation, we propose a transfer learning based approach for ASAG.\nTransfer learning techniques, in contrast to traditional supervised techniques, work on the principle of transferring learned knowledge across domains. In transfer learning parlance, a domain D consists of two components: an n-dimensional feature space X and a marginal probability distribution P (X) where X = {x1, x2, . . . , xn} \u2208 X . Two domains, commonly referred to as source (with labeled data; typically aplenty) and target (with less or no labelled data), are said to be different if they have different feature spaces or different marginal probability distributions [8]. Supervised models trained on data from the source domain cannot be applied to the target domain data as it would violate the fundamental assumptions that training and test data must be in the same feature space and have the same distribution. In such cases, transfer learning techniques have been shown to be effective (to reduce new labeling efforts) for various tasks such as sentiment classification [9], named entity recognition (NER) [10] and social media analytics [11]. Surprisingly, we found only one prior work [12] where domain adaptation was used for ASAG based on the technique proposed in [10]. We will review this along with other prior work in Section II-C and empirically compare against in Section IV-C1.\nAs the second contribution of this work, we employ a novel transfer learning based technique for ASAG by considering answers to different questions as different domains. The technique leverages canonical correlation analysis (CCA) for building the classifier ensemble for target questions requiring graded answers only from the source question; this eliminates the need for graded answers for the former. It transfers the trained model of the second classifier (model answer based) of source question ensemble by learning a common shared representation of features which minimizes domain divergence and misclassification error. The transferred model is applied on answers to target questions to predict scores and confidently predicted answers are considered as pseudo labelled data to\ntrain the corresponding first classifier. This, along with the transferred second classifier, constitutes the ensemble for the target question. The ensemble is then applied to the remaining (other than the pseudo labelled data) student answers. In an analogous manner, confidently predicted instances from the ensemble are added to the pseudo labelled data pool to update the first classifier. The ensemble is then iteratively applied and used to augment the pseudo labelled data pool till all answers are confidently classified or some predefined stopping criteria is met. It is imperative to note that we do not require any instructor graded student answers for the target question in this entire iterative process. Secondly, a similar transfer would have been less meaningful to be applied to the first (text) classifier. Between answers to the two questions, the feature space and distributions of features are both expected to be different. For example, the perfect scoring student answer of (Q1) in Table I would be a totally incorrect answer for (Q2).\nContributions\nWe propose a novel supervised ASAG technique using an ensemble of a text and a numerical classifier (\u00a7 III). We introduce a transfer learning technique for ASAG towards reducing continuous labeling effort needed for the task, thus taking a step towards making supervised ASAG techniques practical (\u00a7 III).\nWe empirically demonstrate superior performance of the proposed method in comparison to [12] on the dataset released by [13] for the joint task of student response analysis in SemEval 2013 Task 7. Additionally, we provide a detailed quantitative analysis on the dataset collected as a part of an undergraduate computer science course [14] towards bringing out insights on why and when transfer learning in ASAG produces superior performance. (\u00a7 IV)\nWe believe that this is one of the first efforts in ASAG which reports empirical results of a technique across multiple datasets towards filling an important gap in this field. 1 (\u00a7 IV)"}, {"heading": "II. PRIOR ART", "text": "Two recently written survey papers by Burrows et. al. [3] and Roy et. al. [15] provide comprehensive views of prior research in ASAG. In this section, we review relevant topics for our technique viz. supervised ASAG, transfer learning and transfer learning for ASAG ."}, {"heading": "A. Supervised ASAG", "text": "Most prior work in supervised ASAG took the approach of designing novel task and dataset specific features to feed to standard classification and regression algorithms. Sukkarieh used features based on lexical constructs such as presence/absence of concepts, the order in which concepts appear etc. [16], [17]; CAM (Content Assessment Module) used types of overlap including word unigrams and n-grams,\n1Quoting from a recent survey paper [3], \u201c[Finally, concerning the effectiveness scores in Table 7,] the meaningful comparisons that can be performed are limited, as the majority of evaluations have been performed in a bubble. That is, the data sets that are common between two or more publications are relatively few.\u201d\nnoun-phrase chunks, parts of speech [18]; Madnani et. al. applied BLEU score (commonly used for evaluating machine translation systems), ROUGE (a recall based metric that measures the lexical and phrasal overlap between two pieces of text) for summary assessment [19] and Nielsen et. al. used carefully crafted lexical and syntactic features [20]. Horbach et. al. demonstrated an interesting variation for assessment of reading comprehension questions where they used the original reading text as a feature [21]. Sukkarieh et. al. compared different classification techniques such as k-Nearest Neighbor, Inductive Logic Programming, Decision Tree and Na\u0131\u0308ve Bayes to compare two sets of experiments viz. on raw text answers and annotated answers [22], [23].\nDzikovska et. al. [13] floated a task, \u201cStudent Response Analysis\u201d (SRA) in the Semantic Evaluation (SemEval) workshop in 2013, where participating teams had to categorize student answers into 2-,3- and 5-way categorization.2 As a part of this task, they also released a dataset of student answers split into train and test format. This is one of the few well annotated datasets in ASAG though for the 5-way categorization it has an atypical characteristic of nominal grades (labels) viz. \u2018Correct\u2019, \u2018Partially correct incomplete\u2019,\u2018Contradictory\u2019 (student answer contradicts the model answer), \u2018Irrelevant\u2019 and \u2018non domain\u2019 unlike commonly used ordinal grades. In the end-of-workshop report, Dzikovska et. al. discussed and compared submissions from 9 participating teams [13]. The trend of feature design continued with most submissions [24], [25] employing various text similarity based features which were heavily tuned towards the dataset (with more emphasis on winning the task and less on generalizability). Multiple participants [12], [26], [27] used some form of \u201cone-shot\u201d system combination approach, with several components feeding into a final decision made by a stacked classifier. One team later built on their submission and employed the idea of stacking on a reading comprehension dataset [7]. While our proposed ensemble-based technique also uses multiple classifiers, the gradual iterative transfer from the similarity based (second) classifier to the answer-text based (first) classifier makes it more robust as opposed to the existing one-shot stacking techniques (based on empirical evidence reported in Section IV-C). Kaggle, a platform for sharing data analytics problems and data, hosted a similar challenge to develop a scoring algorithm for short answers to 10 questions (reading comprehension and science) written by 10th grade students.3 This dataset is one of the largest among public ASAG datasets in terms of number of students and also unique owing to the presence of well defined scoring schemes. Winning participants\u2019 reports, though not archived, again demonstrate prevalence of feature engineering with stacking of supervised methods.\nWe note that the features used for supervised ASAG techniques in different prior art are extensively tuned towards datasets used in respective papers. Rarely, a technique proposed in a paper is also tested on datasets referred to in prior\n2https://www.cs.york.ac.uk/semeval-2013/task7/ 3https://www.kaggle.com/c/asap-sas\npapers. This lack of comparative analysis is also observed by both the recent survey papers [3], [15] who have independently emphasized the importance of sharing of data and ushered in the era of evolution in ASAG. In this paper, we deliberately stayed away from dataset specific feature engineering for the second classifier (which depends on model answer) and rather used generic similarity measures at different types of textual representation (along the lines of unsupervised ASAG pioneered by [4]). Experimental results show that these measure work well across multiple datasets but we acknowledge that specific results may be improved by conducting focused dataset specific feature engineering (with explicit mentioning how it can be done in Section III-B)."}, {"heading": "B. Transfer Learning", "text": "Transfer learning [8] in text analysis (a.k.a. domain adaptation4) has shown promising results in recent years. A large body of domain adaptation literature are around techniques which are based on learning common feature representation [9], [10], [28]. The intuitive idea behind most of these techniques is to learn a transformed feature space where if source and target domain instances are projected they follow a similar distribution. Consequently, a standard supervised learning algorithm can be trained on the former (projected source domain instances) to predict the latter (projected target domain instances). Structural Correspondence Learning (SCL) [9], being one of the most widely used techniques, aims to learn the co-occurrence between features expressing similar meaning in different domains. In 2008, Pan et. al. [29] proposed a dimensionality reduction method Maximum Mean Discrepancy Embedding to identify a shared latent space. A similar approach, based on co-clustering [30] was proposed by Dai et al. [31] to leverage common words as bridge between two domains. Daume\u0301 [10] proposed a heuristic based nonlinear mapping of source and target data to a high dimensional space. In this work, we used a classical feature mapping technique, canonical correlation analysis [32], [33], towards learning a joint subspace where both the source and target domains features are mapped to have maximum correlation."}, {"heading": "C. Transfer Learning and ASAG", "text": "Heilman and Madnani discussed about the use of domain adaptation for ASAG [12] by applying the technique from [10] to support generalization across questions and domains. For each feature, they maintained multiple copies with potentially different weights: a generic copy, a domain-specific copy, and an item-specific copy. For answers to a new question, only the generic features get active but for answers to questions in the training data, all copies of the feature would be active and contribute to the score. Apparently, for their submission in the SRA challenge, they used feature copying only for a subset of features. Phandi et. al. proposed a novel domain adaptation technique that uses Bayesian linear ridge regression\n4We use the terminologies transfer learning and domain adaptation interchangeably in this paper ignoring subtle, but irrelevant for our work, differences they bear.\nfor a related task of automated essay scoring [34]. Recently, Sultan et. al. [35] proposed a hierarchical Bayesian model for domain adaptation of short text where they mentioned possible application to short answer grading."}, {"heading": "III. OUR APPROACH", "text": "A. Intuition\nIn this section, we explain the proposed technique in an intuitive manner before describing the same formally in the next. Following transfer learning terminology, we refer to the questions for which graded answers are available as source questions and questions for which no graded answers are available as target questions. Philosophy of our algorithm is gradual transfer of knowledge from a source to a target question while accounting for question specific variations. The technique has two salient features: an ensemble of two classifiers and an iterative transfer based on a common shared representation:\nEnsemble of classifiers: We model ASAG as a supervised learning task where we employ an ensemble of two classifiers to predict student scores. In the ensemble, the first classifier is a text classifier trained on a bag of word (BoW) model of student answers. It is trained on the corpus of student answers only and does not require any model answer. The second classifier is based on real-valued features capturing similarity of student answers with respect to the model answer. While prior work have used many features towards capturing the same, we found often they were designed and tuned specifically for proprietary datasets. In our endeavor towards generalizability of the proposed technique, we employ five generic state of the art short-text similarity measures to compute similarity between the model and student answers covering lexical, semantic and vector-space measures. Additionally, the model of the first classifier is question specific (i.e. a word which is a good feature for a question is not necessarily a good feature for another question), whereas features for the second classifier are more question agnostic (i.e. high similarity with respective model answer is indicative of high scores irrespective of question). The two classifiers thus capture complementary information useful for grading student answers. Finally, these two classifiers are combined in a weighted manner to form an ensemble which is used to predict the final score (label).\nTransfer based on a common representation: The ensemble of classifiers can be developed as described above for the source question based on instructor graded answers. The question is how do we do the same for target questions in absence of graded answers? It is done in two steps - (i) obtaining the second classifier through a feature based transfer of the model from the source to the target question, followed by (ii) iteratively building the first classifier and the ensemble using pseudo labeled data from the target question.\nLearning a common representation for ASAG task is based on finding a common projection of the question agnostic features (used in the second classifier) for the source and target questions. The common representation between the source and the target questions should be such that a model trained\non this representation using graded student answers to the source question generalizes well for predicting the grades for the student answers to the target questions. For numeric features, we used the classical canonical correlation analysis (CCA) [32], [33] which aims to obtain a joint correlation subspace such that the projected features from the source and target domains are maximally correlated, as shown in Eq 1. Consider two random variables Xs and Xt such that Xs = [xs1, ...., x s n] \u2208 Rds\u00d7n and Xt = [xt1, ...., xtn] \u2208 Rdt\u00d7n. CCA is solved using generalized eigenvalue decomposition to obtain two projection vectors, 1) ps for the source and 2) pt for the target questions.\nmax ps,pt\n\u03c1 = ps\n\u2032 XsXt \u2032 pt\u221a ps\u2032XsXs\u2032ps \u221a pt\u2032XtXt\u2032pt\n= ps\n\u2032 \u2211 st p\nt\u221a ps\u2032 \u2211 ss p s \u221a pt\u2032 \u2211 tt p t\n(1)\nwhere \u2211\ntt = X tXt\n\u2032 , \u2211\nst = X sXt\n\u2032 , and \u2211 sst = X sXs \u2032\nand x\u2032 stands for transpose of x. Features extracted from the student answers to the source question are then projected onto this subspace (using ps) to learn a model which is subsequently used to predict labels for the student answers to the target question projected onto the same subspace (using pt).\nThe newly trained classifier on CCA-based transformed features is the second classifier of target question. It is applied to all student answers to the target question and confidently predicted answers are chosen as pseudo-labeled data to train the first classifier for the target question. We call this training data pool as pseudo as these are not labeled by the instructor rather based on (confident) predictions from the second classifier. The first classifier, trained on the text features using the pseudo labeled data, along with the transferred second classifier are combined as an ensemble (as described above) and applied on the remaining student answers to the target question (i.e. which were not in pseudo labeled training data). Confidently predicted instances from the ensemble are subsequently iteratively used to re-train the text classifier and boost up the overall prediction accuracy of the ensemble. The iteration continues till all the examples are correctly predicted or a specified number of iterations are performed."}, {"heading": "B. The Technique", "text": "In this section, we describe the proposed technique considering two questions qs and qt as the source and target questions respectively. Notations used are shown in Table 2 and the block diagram depicting key steps is shown in Figure 1.\n1) Process graded student answers {xqsi , t qs i } of qs and\nungraded answers {xqti } to create input vectors for two classifiers. 2) TFIDF-Vectorizer for the graded answers of qs takes a bag-of-word (BoW) representations of student answers and converts to TFIDF vectors {uqsi } with corresponding grades (labels), {tqsi }. Prior to vectorization, we perform basic NLP pre-processing of stemming and stopword removal. We also perform question word demoting (i.e.\nconsidering words appearing in the question as stopwords while vectoring student answers) to avoid giving importance to parrot answering.\n3) Train the first classifier Cqs1 on {u qs i , t qs i } using the\ngraded answers of the source question (qs). 4) Generate features for the second classifier using the fol-\nlowing five similarity measures between student answers and model answer for a given question. All values are normalized between 0\u22121 using min-max normalization leading to real valued vectors. Further, this classifier can be easily extended to include additional features that capture specific characteristics of the underlying dataset for enhanced performance. Many of such features are discussed in [13] (and the references there in); however, we restricted our proposed technique to general similarity based features rather than using features tailored for specific datasets. \u2022 LO: First we consider lexical overlap (LO) between\nmodel and student answers. It is a simple baseline measure which looks for exact match for every content words (post pre-processing e.g. stopword removal and stemming) between student and model answers. \u2022 JC and SP: These are two semantic similarity measures based on Wordnet [36]. For each word in student answer, maximum word-to-word similarity scores are obtained with respect to words in model answers which are then summed up and normalized by the length of the two responses as described by Mohler and Mihalcea [4]. They compared eight options for computing word-to-word similarities; of which we select the two best performing ones viz. the measure proposed by Jiang and Conrath (JC) [37] and Shortest Path (SP). \u2022 LSA and W2V: These are the measures in vector space similarity category. In this category we first chose the most popular measure for measuring semantic similarity viz. Latent Semantic Analysis\n(LSA) [38] trained on a Wikipedia dump. We also use the recently popular word2vec tool (W2V) [39] to obtain vector representation of words which are trained on 100 billion words of Google news dataset and are of length 300. Word-to-word similarity measures obtained using euclidean distance between word vectors are summed up and normalized in a manner similar to JC and SP.\n5) Compute {vqsi , t qs i } for the ith student answer to source\nquestion qs, and {vqti } for the ith student answer to the target question qt. 6) Learn CCA-based projection vectors ps and pt to transform the real valued features from the source and target questions respectively to have maximum correlation, as shown in Eq 1. 7) Train Cqst2 using CCA transformed features on the graded answers to the source question, {psvqsi , t qs i }. 8) Use Cqst2 to predict labels, t\u0302 qt i , of student answers to qt\non the CCA-based representation ptvqti . 9) Move instances which are predicted with confidence\ngreater than a pre-defined threshold, \u03b81, to a pseudo training data-pool T . 10) TFIDF vectorized representation of instances in T are selected to train the first classifier (text based) Cqt1 for the target question (Same as Steps 2 & 3 for qs). 11) The two classifiers Cqt1 & C qst 2 are combined to form an\nensemble E(\u00b7)\u2192 w1Cqt1 (\u00b7) +w2C qst 2 (\u00b7). The ensemble E is used to predict the remaining instances and the instances now predicted with a confidence greater than another predefined threshold \u03b82 are again added to T . 12) Update/re-train the first classifier Cqt1 using additional pseudo-labeled instances (added in the previous step). 13) Step-13: Update ensemble weights based on the error of individual classifiers such that the better classifier gets more weight mass.\nwl+11 = 1\u2212 wl1 \u2217 I(C qt 1 )\nwl1 \u2217 I(C qt 1 ) + w l 2 \u2217 I(C qt 2 )\n;wl+12 = 1\u2212w l+1 1\n(2) I(\u00b7) is absolute error of individual classifiers w.r.t. to the pseudo labels obtained by the ensemble over all confidently predicted instances in lth iteration. 14) Repeat steps 10 to 12 till all instances are confidently predicted or a specified number of iterations are performed.\nAlgorithm 1 summarizes the proposed algorithm for automatic short answer grading. The iterative algorithm converges when no more student answers to the target question can be confidently predicted or maximum number of iterations are performed (iterMAX = 10 in our experiments). The transfer of knowledge occurs within the ensemble where the first classifier trained on the CCA-based shared representation provides pseudo-labeled training data to initialize the first classifier on the TFIDF representations of student answers. These two complementary classifiers are further combined in an ensemble where the CCA-based classifier helps in better learning the first classifier. Finally, the ensemble is used for predicting the labels for the ungraded student answers for enhanced ASAG performance. The underlying classifiers used in the proposed algorithm in the logistic regression (LR) classifier and the weights for the individual classifiers in the ensemble are initialized to 0.5 and are further updated as shown in Eq. 2. The thresholds \u03b81 & \u03b82 are empirically set to 2 / (number of class-labels) where number of class-labels > 2 for all questions.\nAn intrigued reader may find the proposed iterative approach similar to the classical co-training algorithm proposed by Blum and Mitchell [40]. However, there are significant differences between the proposed iterative technique and cotraining. Firstly, in co-training the assumption is that one has labeled data, albeit in small number, according to both views of the data. In the proposed technique we do not require any labeled data for the target question. In fact, the proposed technique can be deployed for a large number of target\nAlgorithm 1 The Proposed Algorithm for ASAG INPUT: Cqst2 trained using {psv qs i , t qs i } from qs, thresholds\n\u03b81 & \u03b82, T = \u2205, n: count of student answers to qt. 1: INITIALIZE TFIDF-based CLASSIFIER in TARGET: for i = 1 to n do\nCqst2 (p tvqti ) \u2192 t\u0302 qt i & calculate confidence of pre-\ndiction (\u03b1i). if \u03b1i > \u03b81 then\nMove xqti to T with pseudo label as t\u0302 qt i .\nend if. end for Initialize Cqt1 using pseudo-labeled instances in T\n2: ITERATIVE LEARNING: Iterate: l = 0 : till l \u2264 iterMax Process: Construct E(\u00b7)\u2192 w1Cqt1 (\u00b7) + w2C qst 2 (\u00b7) for i = 1 to n\u2212 ||T || do Predict labels: E (ptvqti )\u2192 t\u0302i\nqt ; calculate \u03b1i: confidence of prediction. if \u03b1i > \u03b82 then\nAdd xqti to T with pseudo label t\u0302 qt i .\nend if. end for. Retrain Cqt1 on T & update ensemble weights. end iterate. OUTPUT: Updated TFIDF-based classifier Cqt1 .\nquestions requiring graded answers from only a few source questions. Secondly, in co-training both classifiers get updated in an iterative manner whereas in the proposed technique only the text classifier gets updated."}, {"heading": "IV. EXPERIMENTAL EVALUATION", "text": ""}, {"heading": "A. Datasets", "text": "Prior work in (supervised) ASAG has not presented evaluation results on multiple datasets. In fact, the recent survey papers referred to in Section II ([3], [15]) have emphasized the need for sharing of datasets and structured evaluations of techniques across multiple datasets. Towards that, we have evaluated the proposed technique on four datasets covering varying subject matter (sciences and literature) as well as standards (high school and college).\nSE2013:5 This dataset is a part of the \u201cStudent Response Analysis\u201d (SRA) in the Semantic Evaluation (SemEval) workshop in 2013 [13]. The task released two datasets: BEETLE data, based on transcripts of students interacting with BEETLE II tutorial dialogue system [41], and SCIENTSBANK data based on the corpus of student answers to assessment questions collected by [42]. For this work, we only consider SCIENTSBANK dataset as it had exactly one model answer for every question. Our technique could be extended in future for BEETLE dataset as well where questions have varying\n5https://www.cs.york.ac.uk/semeval-2013/task7/index.php%3Fid=data.html\nnumber of model answers. The SCIENTSBANK training corpus contains approximately 10,000 answers to 197 assessment questions from 15 different science domains. The answers were graded by multiple annotators on a nominal scale viz. \u2018Correct\u2019, \u2018Partially correct incomplete\u2019,\u2018Contradictory\u2019 (student answer contradicts the reference answer), \u2018Irrelevant\u2019 and \u2018non domain\u2019. Test subsets are of three types: \u2022 Unseen answers (UA): A held-out set of student answers\nto the questions contained in the training set. \u2022 Unseen questions (UQ): A test set created by holding\nback all student answers to a subset of randomly selected questions. These questions were not present in the training set but they are from the same domain. \u2022 Unseen domains (UD): Same as UQ but test set questions are from different domains than training set.\nCSD:6 This is one of the earliest ASAG datasets comprising of a set of questions, model answers and student answers taken from an undergraduate computer science course [4]. The data set consists of 21 questions (7 questions from 3 assignments each) from introductory assignments in the course with answers provided by a class of abut 30 undergraduate students. Student answers were independently evaluated by two annotators on a scale of 0-5 and automatic techniques are measured against their average. All our detail analysis are reported based on this dataset.\nX-CSD:7 This is an extended version of CSD with 87 questions from the same course [14].\nRCD: We created a new dataset on a reading comprehension assignment for Standard-12 students in a Central Board of Secondary Education (CBSE) school in India. The dataset contains 14 questions based on a passage which were answered by 58 students. The answers were graded by two human raters based on model answers and an optional scoring scheme."}, {"heading": "B. Performance Metrics", "text": "Depending on the nature of labels (grades) i.e. ordinal or nominal, we used two different performance metrics. Most ASAG datasets (in our case CSD, X-CSD, and RCD) have ordinal class labels; hence we used mean absolute error (MAE) as the metric for quantitative evaluation. MAE for a question is the absolute difference between the groundtruth and predicted scores averaged over all students and is given by 1n \u2211n i=1|ti \u2212 yi|, where ti and yi are respectively the groundtruth and predicted scores of the ith student\u2019s answer. For reporting aggregate performances over a dataset, question wise MAE values are averaged for all questions.\nThe SE2013 dataset has nominal class labels. Following the evaluation metrics used in the SRA task, we report two confusion matrix based evaluation metrics viz. the macroaverage F1 (= 1/Nc \u2211 c F1(c)) and weighted average F1\n(= 1/N \u2211\nc |c|\u00d7F1(c)) as described in the end-of-workshop report [13]. Here Nc is the number of classes (e.g. \u2018correct\u2019,\n6http://web.eecs.umich.edu/\u223cmihalcea/downloads/ShortAnswerGrading v1.0.tar.gz\n7http://web.eecs.umich.edu/\u223cmihalcea/downloads/ShortAnswerGrading v2.0.zip\n\u2018contradictory\u2019 etc.), N is the total number of test items, |c| is the number of items labeled as c in gold-standard data and F1(c) is class specific F1 score for class c. As described in [13], we ignore the \u2018nondomain\u2019 class as it is severely underrepresented and report macro-averaged F1 over 4 classes for consistent comparison."}, {"heading": "C. Quantitative Results", "text": "In this section, we first present aggregate results for all the datasets followed by fine grained results and insights on the CSD dataset.\n1) Aggregate Results: Table III shows performance of the proposed technique on SE2013 dataset against the entry \u201cETS\u201d [12] (the only ASAG technique based on transfer learning as reviewed in Section II-C) as well as the best performances obtained for the SRA task in SemEval workshop reported in [13]. We report two runs of \u201cETS\u201d as their performances varied significantly between them. The proposed technique performs better than \u201cETS\u201d as well as the best performing entries across all test sets in terms of both the metrics. It is important to note that all techniques use labeled data in supervised learning mode whereas the proposed technique requires labeled data only from the source question. This is a significant feature of the proposed technique, demonstrating that using labeled data from the source question along with generic similarity measures between the student and model answers can result in efficient ASAG in the target question without any labeled data.8\nFor the other three datasets with ordinal labels, there was no prior work based on transfer learning approach to ASAG. We followed the convention in transfer learning literature of comparing against a skyline and a baseline: \u2022 Baseline (Sup-BL): Supervised models are built using\nlabeled data from a source question and applied as-it-is to a target question. \u2022 Skyline (Sup-SL): Supervised models are built assuming labeled data is available for all questions (including target). Performance is measured by training a model on every question and applied on the same.\n8While we focused on the finer 5-way categorization, we observed similar results for 2-way and 3-way tasks in SRA (created by combining \u2018Partially correct incomplete\u2019, \u2018Irrelevant\u2019, and \u2018non domain\u2019 (and \u2018Contradictory\u2019 for 2-way)).\nPerformance of transfer learning techniques should be in between the baseline and skyline - closer to the skyline, better it is. As shown in Table IV, the proposed method beats the baseline for all datasets handsomely (differences being 0.35, 0.03 and 0.86 for CSD, X-CSD, and RCD respectively) whereas coming much closer to the skyline (differences being 0.01, 0.28 and 0.05 for CSD, X-CSD, and RCD respectively).9\n2) Detailed Results: Most prior work in supervised ASAG has reported only aggregated performances over all questions. However, we note that performance of ASAG techniques varies significantly across questions as well as depends on other factors such as the choice of classifier or source question. In this section, we present detailed results with explanation and insights. For lack of space, we present the detailed results only for the CSD dataset.\nQuestion-wise performance: Table V compares the question-wise MAE of the proposed algorithm against the skyline and baseline on all the 21 questions in the CSD dataset.\n9We exclude questions which do not have short answers viz. questions marked with # sign by authors in X-CSD and questions 6 and 11 in RCD.\nFor Sup-BL and the proposed technique, for each question we consider all remaining 20 questions as source one at a time and report the best MAE obtained. Firstly, we note that all methods exhibit performance variations across questions. Variance of SUP-BL, CCA classifier, SUP-SL and the proposed technique are 0.12, 4.46, 0.16 and 0.24 respectively. Secondly, for all questions the proposed technique gives MAE between those of Sup-BL and Sup-SL while being closer to the Sup-SL as observed in aggregate result (Table IV). Thirdly, the proposed algorithm which combines the text and numeric classifier in a weighted ensemble yields significantly lower error rates than the constituent classifiers. This demonstrates the fact that the ensemble exploits their complimentary nature effectively towards improving the overall performance.\nEffect of Iterative Learning: Figure 2 shows the effect of iteratively building the first (text) classifier for the target question based on the pseudo-labeled training instances provided by the second (numeric) classifier. Results suggest that the iterative learning monotonically reduces MAE on all the questions. This further validates our assertion that exploiting textual features along with features derived from model answers in an iterative manner is an important aspect in ASAG. We observed that for most questions the iterative algorithm converged in 5\u2212 6 iterations.\nEffect of Different Classifiers: To explore if the proposed technique generalizes across different classifiers, we experimented with multiple classifiers to build the ensemble viz. logistic regression (LR), support vector machines (SVM), AdaBoost and linear discriminant analysis (LDA). Table VI shows that the performance of the proposed algorithm with respect to underlying classifiers. It is observed that performance is slightly better with LR (Mean MAE=1.07) and SVM (1.29) as compared to LDA (1.63) and Adaboost (1.58). Another implication of this result is that one can tune the second classifier with more number of dataset specific features (as reported in Section II) and still be able to use the proposed technique as a framework. While we deliberately avoided such feature engineering in this paper, it would be an interesting study as a future work.\nEffect of Different Grading Schemes: Each question in the CSD dataset are graded in the range 0 \u2212 5 leading to 11 possible scores (0, 0.5, . . . , 4.5, 5). Hence this is a 11-class classification problem with only 30 student answers as dataset.\nEven with a leave-one-out experimental protocol this is an extremely sparse training dataset with most classes having no training examples. We analyze the effect of different grading schemes on the performance of the proposed algorithm under three different granularity of grading schemes: 1) 11-class grading scheme ranging from 0-5 with a step size of 0.5, 2) 6-class grading scheme ranging from 0-5 with a step size of 1 and 3) 2-class grading scheme with score > 3 as correct and \u2264 3 as incorrect. Results in Figure 3 reports the performance of the proposed algorithm with the three grading schemes and it suggests that the MAE reduce get better with coarser scoring schemes. While such reduction in MAE with coarser class labels is expected, we believe that with larger amount of student data we will be able to further reduce MAE even at finer class label structure."}, {"heading": "V. CONCLUSION", "text": "In this paper, we presented a novel ASAG technique based on an ensemble of a text and a numeric classifier of complementary nature. The technique used a canonical correlation analysis based transfer learning to bootstrap an iterative algorithm to obtain the ensemble for target questions without requiring any labeled data. We demonstrated efficacy of the proposed technique by empirical evaluation on multiple datasets from different subject matters and standards. In future, we intend to conduct studies towards comparing various feature representation techniques along with prior art in supervised ASAG. Additionally, it will be interesting to compare deep learning techniques against heavy feature engineering based approaches prevalent in supervised ASAG prior art. Another interesting question that came up during the course of this work, is that certain types of questions are perhaps more amenable to be recipient of transfer than others. If yes, then how do we characterize those based on questions and model answers? Finally, through this work, we introduced the potential of application of transfer learning in supervised ASAG towards making it practical which hopefully would bring in more novel work in this direction."}], "references": [{"title": "A review of computer-assisted assessment", "author": ["G. Conole", "B. Warburton"], "venue": "Research in learning technology, vol. 13, no. 1, 2005.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "A Review of Automatically Scorable Constructed-response Item Types for Large-scale Assessment", "author": ["M.E. Martinez", "R.E. Bennett"], "venue": "ETS Research Report Series, vol. 1992, no. 2, pp. i\u201334, 1992.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1992}, {"title": "The Eras and Trends of Automatic Short Answer Grading", "author": ["S. Burrows", "I. Gurevych", "B. Stein"], "venue": "International Journal of Artificial Intelligence in Education, vol. 25, no. 1, pp. 60\u2013117, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Text-to-text semantic similarity for automatic short answer grading", "author": ["M. Mohler", "R. Mihalcea"], "venue": "Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL), 2009, pp. 567\u2013575.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Effective Tutorial Feedback for Explanation Questions: A Dataset and Baselines.", "author": ["M. Dzikovska", "R.D. Nielsen", "C. Brew", "\u201cTowards"], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Term-weighting Approaches in Automatic Text Retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "Information Processing and Management, vol. 24, no. 5, pp. 513\u2013523, 1988.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1988}, {"title": "Effective Feature Integration for Automated Short Answer Scoring", "author": ["K. Sakaguchi", "M. Heilman", "N. Madnani"], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL- HLT), 2015, pp. 1049\u20131054.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345\u2013 1359, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Domain Adaptation with Structural Correspondence Learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 2006, pp. 120\u2013128.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Frustratingly Easy Domain Adaptation", "author": ["III H. Daum\u00e9"], "venue": "arXiv preprint arXiv:0907.1815. [Online]. Available: https://arxiv.org/abs/0907.1815", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1815}, {"title": "An Iterative Similarity based Adaptation Technique for Cross-domain Text Classification.", "author": ["H.S. Bhatt", "D. Semwal", "S. Roy"], "venue": "Proceedings of the Conference of Natural Language and Learning (CoNLL),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "ETS: Domain Adaptation and Stacking for Short Answer Scoring", "author": ["M. Heilman", "N. Madnani"], "venue": "Proceedings of the 2nd joint conference on lexical and computational semantics, vol. 2, 2013, pp. 275\u2013279.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "SemEval-2013 task 7: The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge", "author": ["M.O. Dzikovska", "R.D. Nielsen", "C. Brew", "C. Leacock", "D. Giampiccolo", "L. Bentivogli", "P. Clark", "I. Dagan", "H.T. Dang"], "venue": "DTIC Document, Tech. Rep., 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments.", "author": ["M. Mohler", "R.C. Bunescu", "R. Mihalcea"], "venue": "Proceedings of the Conference of the Association for Computational Linguistics (ACL),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "A Perspective on Computer Assisted Assessment Techniques for Short Free-Text Answers", "author": ["S. Roy", "Y. Narahari", "O.D. Deshmukh"], "venue": "Proceedings of the International Conference on Computer Assisted Assessment (CAA). Springer, 2015, pp. 96\u2013109.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Using a MaxEnt Classifier for the Automatic Content Scoring of Free-text Responses", "author": ["J.Z. Sukkarieh", "A. Mohammad-Djafari", "J.-F. o. Bercher", "P. Bessie \u0301 re"], "venue": "Proceedings of the AIP Conference American Institute of Physics, vol. 1305, no. 1, 2011, p. 41.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "c-rater: Automatic Content Scoring for Short Constructed Responses.", "author": ["J.Z. Sukkarieh", "J. Blackmore"], "venue": "Proceedings of the International Florida Artificial Intelligence Research Society Conference (FLAIRS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Diagnosing Meaning Errors in Short Answers to Reading Comprehension Questions", "author": ["S. Bailey", "D. Meurers"], "venue": "Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications (EANL), 2008, pp. 107\u2013115.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Automated Scoring of a Summary Writing Task Designed to Measure Reading Comprehension", "author": ["N. Madnani", "J. Burstein", "J. Sabatini", "T. O\u2019Reilly"], "venue": "Proceedings of the 8th workshop on innovative use of nlp for building educational applications, 2013, pp. 163\u2013168.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "A Taxonomy of Questions for Question Generation", "author": ["R.D. Nielsen", "J. Buckingham", "G. Knoll", "B. Marsh", "L. Palen"], "venue": "Proceedings of the Workshop on the Question Generation Shared Task and Evaluation Challenge, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Using the text to evaluate short answers for reading comprehension exercises", "author": ["A. Horbach", "A. Palmer", "M. Pinkal"], "venue": "Second Joint Conference on Lexical and Computational Semantics (* SEM), vol. 1, 2013, pp. 286\u2013295.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic Short Answer Marking", "author": ["S.G. Pulman", "J.Z. Sukkarieh"], "venue": "Proceedings of the Second Workshop on Building Educational Applications Using NLP (EdAppsNLP), 2005, pp. 9\u201316.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Information Extraction and Machine Learning: Auto-Marking Short Free Text Responses to Science Questions.", "author": ["J.Z. Sukkarieh", "S.G. Pulman"], "venue": "ser. Frontiers in Artificial Intelligence and Applications (AIED),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "SOFTCARDINALITY: Hierarchical Text Overlap for Student Response Analysis", "author": ["S. Jimenez", "C. Becerra", "A. Gelbukh"], "venue": "Proceedings of the 2nd joint conference on lexical and computational semantics, vol. 2, 2013, pp. 280\u2013284.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Celi: EDITS and Generic Text Pair Classification", "author": ["M. Kouylekov", "L. Dini", "A. Bosca", "M. Trevisan"], "venue": "Proceedings of the 2nd joint conference on lexical and computational semantics, vol. 2, 2013, pp. 592\u2013597.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "CoMeT: Integrating Different Levels of Linguistic Modeling for Meaning Assessment", "author": ["N. Ott", "R. Ziai", "M. Hahn", "D. Meurers"], "venue": "Proceedings of the 2nd joint conference on lexical and computational semantics, vol. 2, 2013, pp. 608\u2013616.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "UKP-BIU: Similarity and Entailment Metrics for Student Response Analysis", "author": ["T. Zesch", "O. Levy", "I. Gurevych", "I. Dagan"], "venue": "Proceedings of the joint conference on lexical and computational semantics, vol. 2, 2013, pp. 285\u2013289.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-domain Sentiment Classification via Spectral Feature Alignment", "author": ["S.J. Pan", "X. Ni", "J.-T. Sun", "Q. Yang", "Z. Chen"], "venue": "Proceedings of International Conference on World Wide Web, 2010, pp. 751\u2013760.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Transfer Learning via Dimensionality Reduction.", "author": ["S.J. Pan", "J.T. Kwok", "Q. Yang"], "venue": "Proceedings of AAAI Conference on Artificial Intelligence,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Information-theoretic coclustering", "author": ["I.S. Dhillon", "S. Mallela", "D.S. Modha"], "venue": "Proceedings of International Conference on Knowledge Discovery and Data Mining, 2003, pp. 89\u201398.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Boosting for Transfer Learning", "author": ["W. Dai", "Q. Yang", "G.-R. Xue", "Y. Yu"], "venue": "Proceedings of International Conference on Machine Learning (ICML), 2007, pp. 193\u2013200.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Recognizing Actions across Cameras by Exploring the Correlated Subspace", "author": ["C. Huang", "Y. Yeh", "Y.F. Wang"], "venue": "Proceedings of European  Conference on Computer Vision- Workshops and Demonstrations, 2012, pp. 342\u2013351.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Heterogeneous Domain Adaptation and Classification by Exploiting the Correlation Subspace", "author": ["Y. Yeh", "C. Huang", "Y.F. Wang"], "venue": "IEEE Transactions on Image Processing, vol. 23, no. 5, pp. 2009\u20132018, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Flexible Domain Adaptation for Automated Essay Scoring Using Correlated Linear Regression.", "author": ["P. Phandi", "K.M.A. Chai", "H.T. Ng"], "venue": "Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Bayesian Supervised Domain Adaptation for Short Text Similarity", "author": ["M.A. Sultan", "J. Boyd-Graber", "T. Sumner"], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL- HLT), 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "WordNet: A Lexical Database for English", "author": ["G.A. Miller"], "venue": "Communications of the ACM, vol. 38, no. 11, pp. 39\u201341, 1995.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1995}, {"title": "Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy", "author": ["J.J. Jiang", "D.W. Conrath"], "venue": "Proceedings of the International Conference on Research in Computational Linguistics, 1997, pp. 19\u201333.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1997}, {"title": "An Introduction to Latent Semantic Analysis", "author": ["T.K. Landauer", "P.W. Foltz", "D. Laham"], "venue": "Discourse Processes, vol. 25, pp. 259\u2013284, 1998.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781. [Online]. Available: https://arxiv.org/abs/1301.3781", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1301}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "Proceedings of Conference on Learning Theory, 1998, pp. 92\u2013100.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1998}, {"title": "Beetle II: A System for Tutoring and Computational Linguistics Experimentation.", "author": ["M.O. Dzikovska", "J.D. Moore", "N.B. Steinhauser", "G.E. Campbell", "E. Farrow", "C.B. Callaway"], "venue": "Proceedings of the Conference of the Association for Computational Linguistics (ACL),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "Annotating Students\u2019 Understanding of Science Concepts", "author": ["R.D. Nielsen", "W. Ward", "J.H. Martin", "M. Palmer"], "venue": "Proceedings of the Sixth International Language Resources and Evaluation (LREC), 2008.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Prior work reported in many papers has questioned the effectiveness of such questions to assess knowledge, scholarship, and depth of understanding gathered by students [1], [2].", "startOffset": 168, "endOffset": 171}, {"referenceID": 1, "context": "Prior work reported in many papers has questioned the effectiveness of such questions to assess knowledge, scholarship, and depth of understanding gathered by students [1], [2].", "startOffset": 173, "endOffset": 176}, {"referenceID": 2, "context": "This paper dwells on a computational technique for automatically grading such answers and particularly focuses on short answers which are a few words to a few sentences long (everything in between fill-in-the-gap and essay type answers [3]).", "startOffset": 236, "endOffset": 239}, {"referenceID": 4, "context": "the raw number of overlapping words, F1 score, Lesk score and cosine score between student and model answers as features [5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 5, "context": "The first classifier is a text classifier trained using the classical TFIDF representation [6] of bag-of-words (BoW) features ar X iv :1 60 9.", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "TABLE I EXAMPLES OF QUESTIONS, MODEL ANSWERS, AND STUDENT ANSWERS WITH INSTRUCTOR GIVEN SCORES FROM AN UNDERGRADUATE COMPUTER SCIENCE COURSE [4]", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "While stacking of classifiers has been used in ASAG [7] towards \u201cone-", "startOffset": 52, "endOffset": 55}, {"referenceID": 7, "context": "Two domains, commonly referred to as source (with labeled data; typically aplenty) and target (with less or no labelled data), are said to be different if they have different feature spaces or different marginal probability distributions [8].", "startOffset": 238, "endOffset": 241}, {"referenceID": 8, "context": "In such cases, transfer learning techniques have been shown to be effective (to reduce new labeling efforts) for various tasks such as sentiment classification [9], named entity recognition (NER) [10] and social", "startOffset": 160, "endOffset": 163}, {"referenceID": 9, "context": "In such cases, transfer learning techniques have been shown to be effective (to reduce new labeling efforts) for various tasks such as sentiment classification [9], named entity recognition (NER) [10] and social", "startOffset": 196, "endOffset": 200}, {"referenceID": 10, "context": "media analytics [11].", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "Surprisingly, we found only one prior work [12] where domain adaptation was used for ASAG based on the technique proposed in [10].", "startOffset": 43, "endOffset": 47}, {"referenceID": 9, "context": "Surprisingly, we found only one prior work [12] where domain adaptation was used for ASAG based on the technique proposed in [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "We empirically demonstrate superior performance of the proposed method in comparison to [12] on the dataset released by [13] for the joint task of student response analysis in SemEval 2013 Task 7.", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "We empirically demonstrate superior performance of the proposed method in comparison to [12] on the dataset released by [13] for the joint task of student response analysis in SemEval 2013 Task 7.", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": "quantitative analysis on the dataset collected as a part of an undergraduate computer science course [14] towards bringing out insights on why and when transfer learning in ASAG produces superior performance.", "startOffset": 101, "endOffset": 105}, {"referenceID": 2, "context": "[3]", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15] provide comprehensive views of prior research in ASAG.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16], [17]; CAM (Content Assessment Module) used types of overlap including word unigrams and n-grams,", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[16], [17]; CAM (Content Assessment Module) used types of overlap including word unigrams and n-grams,", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "1Quoting from a recent survey paper [3], \u201c[Finally, concerning the effectiveness scores in Table 7,] the meaningful comparisons that can be performed are limited, as the majority of evaluations have been performed in a bubble.", "startOffset": 36, "endOffset": 39}, {"referenceID": 17, "context": "\u201d noun-phrase chunks, parts of speech [18]; Madnani et.", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "applied BLEU score (commonly used for evaluating machine translation systems), ROUGE (a recall based metric that measures the lexical and phrasal overlap between two pieces of text) for summary assessment [19] and Nielsen et.", "startOffset": 205, "endOffset": 209}, {"referenceID": 19, "context": "used carefully crafted lexical and syntactic features [20].", "startOffset": 54, "endOffset": 58}, {"referenceID": 20, "context": "demonstrated an interesting variation for assessment of reading comprehension questions where they used the original reading text as a feature [21].", "startOffset": 143, "endOffset": 147}, {"referenceID": 21, "context": "and annotated answers [22], [23].", "startOffset": 22, "endOffset": 26}, {"referenceID": 22, "context": "and annotated answers [22], [23].", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "[13] floated a task, \u201cStudent Response Analysis\u201d (SRA) in the Semantic Evaluation (SemEval) workshop in 2013, where participating teams had to categorize student answers into 2-,3- and 5-way categorization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "discussed and compared submissions from 9 participating teams [13].", "startOffset": 62, "endOffset": 66}, {"referenceID": 23, "context": "The trend of feature design continued with most submissions [24],", "startOffset": 60, "endOffset": 64}, {"referenceID": 24, "context": "[25] employing various text similarity based features which were heavily tuned towards the dataset (with more emphasis on winning the task and less on generalizability).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Multiple participants [12], [26], [27] used some form of \u201cone-shot\u201d system combination approach, with several components feeding into a final decision made by a stacked classifier.", "startOffset": 22, "endOffset": 26}, {"referenceID": 25, "context": "Multiple participants [12], [26], [27] used some form of \u201cone-shot\u201d system combination approach, with several components feeding into a final decision made by a stacked classifier.", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "Multiple participants [12], [26], [27] used some form of \u201cone-shot\u201d system combination approach, with several components feeding into a final decision made by a stacked classifier.", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "built on their submission and employed the idea of stacking on a reading comprehension dataset [7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "both the recent survey papers [3], [15] who have independently emphasized the importance of sharing of data and ushered in the era of evolution in ASAG.", "startOffset": 30, "endOffset": 33}, {"referenceID": 14, "context": "both the recent survey papers [3], [15] who have independently emphasized the importance of sharing of data and ushered in the era of evolution in ASAG.", "startOffset": 35, "endOffset": 39}, {"referenceID": 3, "context": "textual representation (along the lines of unsupervised ASAG pioneered by [4]).", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "Transfer learning [8] in text analysis (a.", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "body of domain adaptation literature are around techniques which are based on learning common feature representation [9], [10], [28].", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "body of domain adaptation literature are around techniques which are based on learning common feature representation [9], [10], [28].", "startOffset": 122, "endOffset": 126}, {"referenceID": 27, "context": "body of domain adaptation literature are around techniques which are based on learning common feature representation [9], [10], [28].", "startOffset": 128, "endOffset": 132}, {"referenceID": 8, "context": "Structural Correspondence Learning (SCL) [9], being one of the most widely used techniques, aims to learn the co-occurrence between features expressing similar meaning in different domains.", "startOffset": 41, "endOffset": 44}, {"referenceID": 28, "context": "[29]", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "A similar approach, based on co-clustering [30] was proposed by Dai et al.", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "[31] to leverage common words as bridge between two domains.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Daum\u00e9 [10] proposed a heuristic based non-", "startOffset": 6, "endOffset": 10}, {"referenceID": 31, "context": "In this work, we used a classical feature mapping technique, canonical correlation analysis [32], [33], towards learning a joint subspace where both the source and target domains features are mapped to have maximum correlation.", "startOffset": 92, "endOffset": 96}, {"referenceID": 32, "context": "In this work, we used a classical feature mapping technique, canonical correlation analysis [32], [33], towards learning a joint subspace where both the source and target domains features are mapped to have maximum correlation.", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "Heilman and Madnani discussed about the use of domain adaptation for ASAG [12] by applying the technique from [10] to support generalization across questions and domains.", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "Heilman and Madnani discussed about the use of domain adaptation for ASAG [12] by applying the technique from [10] to support generalization across questions and domains.", "startOffset": 110, "endOffset": 114}, {"referenceID": 33, "context": "for a related task of automated essay scoring [34].", "startOffset": 46, "endOffset": 50}, {"referenceID": 34, "context": "[35] proposed a hierarchical Bayesian model for domain adaptation of short text where they mentioned possible application to short answer grading.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "For numeric features, we used the classical canonical correlation analysis (CCA) [32], [33] which aims to obtain a joint correlation subspace such that the projected features from the source and target domains are maximally correlated, as shown in Eq 1.", "startOffset": 81, "endOffset": 85}, {"referenceID": 32, "context": "For numeric features, we used the classical canonical correlation analysis (CCA) [32], [33] which aims to obtain a joint correlation subspace such that the projected features from the source and target domains are maximally correlated, as shown in Eq 1.", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "Many of such features are discussed in [13] (and the references there in); however, we restricted our proposed technique to general similarity based features rather than using features tailored for specific datasets.", "startOffset": 39, "endOffset": 43}, {"referenceID": 35, "context": "measures based on Wordnet [36].", "startOffset": 26, "endOffset": 30}, {"referenceID": 3, "context": "For each word in student answer, maximum word-to-word similarity scores are obtained with respect to words in model answers which are then summed up and normalized by the length of the two responses as described by Mohler and Mihalcea [4].", "startOffset": 235, "endOffset": 238}, {"referenceID": 36, "context": "the measure proposed by Jiang and Conrath (JC) [37] and Shortest Path (SP).", "startOffset": 47, "endOffset": 51}, {"referenceID": 37, "context": "(LSA) [38] trained on a Wikipedia dump.", "startOffset": 6, "endOffset": 10}, {"referenceID": 38, "context": "We also use the recently popular word2vec tool (W2V) [39] to obtain vector representation of words which are trained on 100 billion words of Google news dataset", "startOffset": 53, "endOffset": 57}, {"referenceID": 39, "context": "proach similar to the classical co-training algorithm proposed by Blum and Mitchell [40].", "startOffset": 84, "endOffset": 88}, {"referenceID": 2, "context": "In fact, the recent survey papers referred to in Section II ([3], [15]) have emphasized the need for sharing of datasets and structured evaluations of", "startOffset": 61, "endOffset": 64}, {"referenceID": 14, "context": "In fact, the recent survey papers referred to in Section II ([3], [15]) have emphasized the need for sharing of datasets and structured evaluations of", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "shop in 2013 [13].", "startOffset": 13, "endOffset": 17}, {"referenceID": 40, "context": "The task released two datasets: BEETLE data, based on transcripts of students interacting with BEETLE II tutorial dialogue system [41], and SCIENTSBANK data based on the corpus of student answers to assessment questions collected by [42].", "startOffset": 130, "endOffset": 134}, {"referenceID": 41, "context": "The task released two datasets: BEETLE data, based on transcripts of students interacting with BEETLE II tutorial dialogue system [41], and SCIENTSBANK data based on the corpus of student answers to assessment questions collected by [42].", "startOffset": 233, "endOffset": 237}, {"referenceID": 3, "context": "CSD:6 This is one of the earliest ASAG datasets comprising of a set of questions, model answers and student answers taken from an undergraduate computer science course [4].", "startOffset": 168, "endOffset": 171}, {"referenceID": 13, "context": "X-CSD:7 This is an extended version of CSD with 87 questions from the same course [14].", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "the macroaverage F1 (= 1/Nc \u2211 c F1(c)) and weighted average F1 (= 1/N \u2211 c |c|\u00d7F1(c)) as described in the end-of-workshop report [13].", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "TECHNIQUE (ETS1 AND ETS2 ) [12] AND THE BEST PERFORMANCE OBTAINED IN THE SRA TASK.", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "EXISTING RESULTS ARE FROM [13].", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "As described in [13], we ignore the \u2018nondomain\u2019 class as it is severely underrepresented and report macro-averaged F1 over 4 classes for consistent comparison.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "1) Aggregate Results: Table III shows performance of the proposed technique on SE2013 dataset against the entry \u201cETS\u201d [12] (the only ASAG technique based on transfer learning as", "startOffset": 118, "endOffset": 122}, {"referenceID": 12, "context": "reviewed in Section II-C) as well as the best performances obtained for the SRA task in SemEval workshop reported in [13].", "startOffset": 117, "endOffset": 121}], "year": 2016, "abstractText": "Automatic short answer grading (ASAG) techniques are designed to automatically assess short answers to questions in natural language, having a length of a few words to a few sentences. Supervised ASAG techniques have been demonstrated to be effective but suffer from a couple of key practical limitations. They are greatly reliant on instructor provided model answers and need labeled training data in the form of graded student answers for every assessment task. To overcome these, in this paper, we introduce an ASAG technique with two novel features. We propose an iterative technique on an ensemble of (a) a text classifier of student answers and (b) a classifier using numeric features derived from various similarity measures with respect to model answers. Second, we employ canonical correlation analysis based transfer learning on a common feature representation to build the classifier ensemble for questions having no labelled data. The proposed technique handsomely beats all winning supervised entries on the SCIENTSBANK dataset from the \u201cStudent Response Analysis\u201d task of SemEval 2013. Additionally, we demonstrate generalizability and benefits of the proposed technique through evaluation on multiple ASAG datasets from different subject topics and standards.", "creator": "LaTeX with hyperref package"}}}