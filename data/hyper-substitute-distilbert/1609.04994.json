{"id": "1609.04994", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "Exploration Potential", "abstract": "we introduce exploration potential, a quantity for that measures in valuable a suitable learning agent already explored diverse environment class. in contrast - information gain, exploration criterion gives the problem's reward structure into view. this contributes to an experimental criterion : exists both necessary and desirable for asymptotic optimality ( learning to search accurately across three core social class ). our experiments offering multi - cornered strategy under predator potential theorists illustrate why different algorithms utilize the tradeoff between exploration regarding exploitation.", "histories": [["v1", "Fri, 16 Sep 2016 10:55:27 GMT  (137kb,D)", "https://arxiv.org/abs/1609.04994v1", "9 pages"], ["v2", "Wed, 28 Sep 2016 14:22:32 GMT  (137kb,D)", "http://arxiv.org/abs/1609.04994v2", "9 pages, including proofs"], ["v3", "Fri, 18 Nov 2016 11:17:56 GMT  (196kb,D)", "http://arxiv.org/abs/1609.04994v3", "10 pages, including proofs"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["jan leike"], "accepted": false, "id": "1609.04994"}, "pdf": {"name": "1609.04994.pdf", "metadata": {"source": "CRF", "title": "Exploration Potential", "authors": ["Jan Leike"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Good exploration strategies are currently a major obstacle for reinforcement learning (RL). The state of the art in deep RL (Mnih et al., 2015, 2016) relies on \u03b5-greedy policies: in every time step, the agent takes a random action with some probability. Yet \u03b5-greedy is a poor exploration strategy and for environments with sparse rewards it is quite ineffective (for example the Atari game \u2018Montezuma\u2019s Revenge\u2019): it just takes too long until the agent randomwalks into the first reward.\nMore sophisticated exploration strategies have been proposed: using information gain about the environment (Sun et al., 2011; Orseau et al., 2013; Houthooft et al., 2016) or pseudo-count (Bellemare et al., 2016). In practice, these exploration strategies are employed by adding an exploration bonus (\u2018intrinsic motivation\u2019) to the reward signal (Schmidhuber, 2010). While the methods above require the agent to have a model of its environment and formalize the strategy \u2018explore by going to where the model has high uncertainty,\u2019 there are also model-free strategies like the automatic discovery of options proposed by Machado and Bowling (2016). However, none of these explicit exploration strategies take the problem\u2019s reward structure into account. Intuitively, we want to explore more in parts of the environment where the\nar X\niv :1\n60 9.\n04 99\n4v 3\n[ cs\n.L G\n] 1\n8 N\nreward is high and less where it is low. This is readily exposed in optimistic policies like UCRL (Jaksch et al., 2010) and stochastic policies like PSRL (Strens, 2000), but these do not make the exploration/exploitation tradeoff explicitly.\nIn this paper, we propose exploration potential, a quantity that measures rewarddirected exploration. We consider model-based reinforcement learning in partially or fully observable domains. Informally, exploration potential is the Bayes-expected absolute deviation of the value of optimal policies. Exploration potential is similar to information gain about the environment, but explicitly takes the problem\u2019s reward structure into account. We show that this leads to a exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across an environment class): a reinforcement learning agent learns to act optimal in the limit if and only if the exploration potential converges to 0. As such, exploration potential captures the essence of what it means to \u2018explore the right amount\u2019.\nAnother exploration quantity that is both necessary and sufficient for asymptotic optimality is information gain about the optimal policy (Russo and Van Roy, 2014; Reddy et al., 2016). In contrast to exploration potential, it is not measured on the scale of rewards, making an explicit value-of-information tradeoff more difficult.\nFor example, consider a 3-armed Gaussian bandit problem with means 0.6, 0.5, and \u22121. The information content is identical in every arm. Hence an exploration strategy based on maximizing information gain about the environment would query the third arm, which is easily identifiable as suboptimal, too frequently (linearly versus logarithmically). This arm provides information, but this information is not very useful for solving the reinforcement learning task. In contrast, an exploration potential based exploration strategy concentrates its exploration on the first two arms."}, {"heading": "2 Preliminaries and Notation", "text": "A reinforcement learning agent interacts with an environment in cycles: at time step t the agent chooses an action at and receives a percept et = (ot, rt) consisting of an observation ot and a reward rt \u2208 [0, 1]; the cycle then repeats for t+ 1. We use \u00e6<t to denote a history of length t\u2212 1. With abuse of notation, we treat histories both as outcomes and as random variables.\nA policy is a function mapping a history \u00e6<t and an action a to the probability \u03c0(a | \u00e6<t) of taking action a after seeing history \u00e6<t. An environment is a function mapping a history \u00e61:t to the probability \u03bd(et | \u00e6<tat) of generating percept et after this history \u00e6<tat. A policy \u03c0 and an environment \u03bd generate a probability measure \u03bd\u03c0 over infinite histories, the expectation over this measure is denoted with\nE\u03c0\u03bd . The value of a policy \u03c0 in an environment \u03bd given history \u00e6<t is defined as\nV \u03c0\u03bd (\u00e6<t) := (1\u2212 \u03b3)E\u03c0\u03bd\n[ \u221e\u2211\nk=t\n\u03b3krk \u2223\u2223\u2223\u2223\u2223\u00e6<t ] ,\nwhere \u03b3 \u2208 (0, 1) is the discount factor. The optimal value is defined as V \u2217\u03bd (\u00e6<t) := sup\u03c0 V \u03c0 \u03bd (\u00e6<t), and the optimal policy is \u03c0 \u2217 \u03bd := arg max\u03c0 V \u03c0 \u03bd . We use \u00b5 to denote the true environment. We assume the nonparametric setting: letM denote a countable class of environments containing the true environment \u00b5. Let w \u2208 \u2206M be a prior probability distribution onM. After observing the history \u00e6<t the prior w is updated to the posterior w(\u03bd | \u00e6<t) := w(\u03bd)\u03bd(\u00e6<t)/( \u2211 \u03c1\u2208Mw(\u03c1)\u03c1(\u00e6<t)). A policy \u03c0 is asymptotically optimal in mean iff for every \u00b5 \u2208 M, E\u03c0\u00b5[V \u2217\u00b5 (\u00e6<t)\u2212 V \u03c0\u00b5 (\u00e6<t)]\u2192 0 as t\u2192\u221e."}, {"heading": "3 Exploration Potential", "text": "We consider model-based reinforcement learning where the agent learns a model of its environment. With this model, we can estimate the value of any candidate policy. Concretely, let V\u0302 \u03c0t denote our estimate of the value of the policy \u03c0 at time step t. We assume that the agent\u2019s learning algorithm satisfies on-policy value convergence (OPVC):\nV \u03c0\u00b5 (\u00e6<t)\u2212 V\u0302 \u03c0t (\u00e6<t)\u2192 0 as t\u2192\u221e \u00b5\u03c0-almost surely. (1) This does not imply that our model of the environment converges to the truth, only that we learn to predict the value of the policy that we are following. On-policy value convergence does not require that we learn to predict off-policy, i.e., the value of other policies. In particular, we might not learn to predict the value of the \u00b5-optimal policy \u03c0\u2217\u00b5.\nFor example, a Bayesian mixture or an MDL-based estimator both satisfy OPVC if the true environment is the environment class; for more details, see Leike (2016, Sec. 4.2.3).\nWe define the V\u0302t-greedy policy as \u03c0\u2217V\u0302 := arg max\u03c0 V\u0302 \u03c0 t ."}, {"heading": "3.1 Definition", "text": "Definition 1 (Exploration Potential). LetM be a class of environments and let \u00e6<t be a history. The exploration potential is defined as\nEPM(\u00e6<t) := \u2211\n\u03bd\u2208M w(\u03bd | \u00e6<t)\n\u2223\u2223\u2223V \u03c0\u2217\u03bd\u03bd (\u00e6<t)\u2212 V\u0302 \u03c0 \u2217 \u03bd t (\u00e6<t) \u2223\u2223\u2223 .\nIntuitively, EP captures the amount of exploration that is still required before having learned the entire environment class. Asymptotically the posterior concentrates around environments that are compatible with the current environment. EP then quantifies how well the model V\u0302t understands the value of the compatible environments\u2019 optimal policies.\nRemark 2 (Properties of EP).\n(i) EPM depends neither on the true environment \u00b5, nor on the agent\u2019s policy \u03c0.\n(ii) EPM depends on the choice of the prior w and on the agent\u2019s model of the world V\u0302t.\n(iii) 0 \u2264 EPM(\u00e6<t) \u2264 1 for all histories \u00e6<t.\nThe last item follows from the fact that the posterior w( \u00b7 | \u00e6<t) and the value function V are bounded between 0 and 1."}, {"heading": "3.2 Sufficiency", "text": "Proposition 3 (Bound on Optimality). For all \u00b5 \u2208M,\nV \u2217\u00b5 (\u00e6<t)\u2212 V \u03c0\u2217 V\u0302 \u00b5 (\u00e6<t) \u2264 V\u0302 \u2217t (\u00e6<t)\u2212 V \u03c0\u2217 V\u0302 \u00b5 (\u00e6<t) + EPM(\u00e6<t) w(\u00b5 | \u00e6<t) .\nProof.\n\u2223\u2223\u2223V \u2217\u00b5 \u2212 V\u0302 \u03c0\u2217\u00b5 t \u2223\u2223\u2223 = w(\u00b5 | \u00e6<t) w(\u00b5 | \u00e6<t) \u2223\u2223\u2223V \u2217\u00b5 \u2212 V\u0302 \u03c0\u2217\u00b5 t \u2223\u2223\u2223 \u2264 \u2211\n\u03bd\u2208M\nw(\u03bd | \u00e6<t) w(\u00b5 | \u00e6<t)\n\u2223\u2223\u2223V \u2217\u03bd \u2212 V\u0302 \u03c0 \u2217 \u03bd t \u2223\u2223\u2223 = EPM w(\u00b5 | \u00e6<t)\nTherefore\nV \u2217\u00b5 \u2212 V \u03c0\u2217 V\u0302 \u00b5 = V \u2217 \u00b5 \u2212 V\u0302 \u03c0\u2217\u00b5 t\ufe38 \ufe37\ufe37 \ufe38\n\u2264EP(\u00e6<t)/w(\u00b5|\u00e6<t)\n+ V\u0302 \u03c0\u2217\u00b5 t \u2212 V\u0302 \u2217t\ufe38 \ufe37\ufe37 \ufe38 \u22640 + V\u0302 \u2217t \u2212 V \u03c0\u2217 V\u0302 \u00b5 .\nThe bound of Proposition 3 is to be understood as follows.\nV \u2217\u00b5 (\u00e6<t)\u2212 V \u03c0\u2217 V\u0302\n\u00b5 (\u00e6<t)\ufe38 \ufe37\ufe37 \ufe38 optimality of the greedy policy\n\u2264 V\u0302 \u2217t (\u00e6<t)\u2212 V \u03c0\u2217 V\u0302\n\u00b5 (\u00e6<t)\ufe38 \ufe37\ufe37 \ufe38 OPVC + EP(\u00e6<t)\ufe38 \ufe37\ufe37 \ufe38 exploration potential /w(\u00b5 | \u00e6<t)\ufe38 \ufe37\ufe37 \ufe38 posterior\nIf we switch to the greedy policy \u03c0\u2217 V\u0302 , then V\u0302 \u2217t \u2212 V \u03c0\u2217 V\u0302\n\u00b5 \u2192 0 due to on-policy value convergence (1). This reflects how well the agent learned the environment\u2019s response to the Bayes-optimal policy. Generally, following the greedy policy does\nnot yield enough exploration for EP to converge to 0. In order to get a policy \u03c0 that is asymptotically optimal, we have to combine an exploration policy which ensures that EP\u2192 0 and then gradually phase out exploration by switching to the \u03c0\u2217 V\u0302\n-greedy policy. Because of property (i), the agent can compute its current EP value and thus check how close it is to 0. The higher the prior belief in the true environment \u00b5, the smaller this value will be (in expectation)."}, {"heading": "3.3 Necessity", "text": "Definition 4 (Policy Convergence). Let \u03c0 and \u03c0\u2032 be two policies. We say the policy \u03c0 converges to \u03c0\u2032 in \u00b5\u03c0-probability iff |V\u0302 \u03c0t (\u00e6<t)\u2212 V\u0302 \u03c0 \u2032 t (\u00e6<t)| \u2192 0 as t\u2192\u221e in V\u0302 .\nWe assume that V\u0302t is continuous in the policy argument. If \u03c0 converges to \u03c0\u2032 in total variation in the sense that \u03c0(a | \u00e6<k)\u2212 \u03c0\u2032(a | \u00e6<k)\u2192 0 for all actions a and k \u2265 t, then \u03c0 converges to \u03c0\u2032 in V\u0302 .\nDefinition 5 (Strongly Unique Optimal Policy). An environment \u00b5 admits a strongly unique optimal policy iff there is a \u00b5-optimal policy \u03c0\u2217\u00b5 such that for all policies \u03c0 if\nV \u2217\u00b5 (\u00e6<t)\u2212 V \u03c0\u00b5 (\u00e6<t)\u2192 0 in \u00b5\u03c0-probability,\nthen \u03c0 converges to \u03c0\u2217\u00b5 in V\u0302 .\nAssuming that V\u0302 \u03c0t is continuous is \u03c0, an environment \u00b5 has a unique optimal policy if there are no ties in arg maxa V \u2217 \u00b5 (\u00e6<ta). Admitting a strongly unique optimal policy is an even stronger requirement because it requires that there exist no other policies that approach the optimal value asymptotically but take different actions (i.e., there is a constant gap in the argmax). For any finite-state (PO)MDP with a unique optimal policy that policy is also strongly unique.\nProposition 6 (Asymptotic Optimality\u21d2 EP\u2192 0). If the policy \u03c0 is asymptotically optimal in mean in the environment classM and each environment \u03bd \u2208 M admits a strongly unique optimal policy, then EPM \u2192 0 in \u00b5\u03c0-probability for all \u00b5 \u2208M.\nProof. Since \u03c0 is asymptotically optimal in mean inM, we have that V \u2217\u00b5 \u2212V \u03c0\u00b5 \u2192 0 and since \u00b5 admits a strongly unique optimal policy, \u03c0 converges to \u03c0\u2217\u00b5 in \u00b5 \u03c0probability, thus V\u0302 \u03c0t \u2212 V\u0302 \u03c0\u2217\u00b5 t \u2192 0. By on-policy value convergence V \u03c0\u00b5 \u2212 V\u0302 \u03c0t \u2192 0. Therefore\nV \u2217\u00b5 \u2212 V\u0302 \u03c0\u2217\u00b5 t = V \u2217 \u00b5 \u2212 V \u03c0\u00b5 + V \u03c0\u00b5 \u2212 V\u0302 \u03c0t + V\u0302 \u03c0t \u2212 V\u0302 \u03c0\u2217\u00b5 t \u2192 0\nand thus E\u03c0\u00b5 \u2223\u2223\u2223V \u03c0 \u2217 \u00b5 \u00b5 (\u00e6<t)\u2212 V\u0302 \u03c0\u2217\u00b5 t (\u00e6<t) \u2223\u2223\u2223\u2192 0 for all \u00b5 \u2208M. (2)\nNow\nE\u03c0\u00b5[EPM(\u00e6<t)] = E\u03c0\u00b5\n[\u2211\n\u03bd\u2208M w(\u03bd | \u00e6<t)\n\u2223\u2223\u2223V \u03c0\u2217\u03bd\u03bd (\u00e6<t)\u2212 V\u0302 \u03c0 \u2217 \u03bd t (\u00e6<t) \u2223\u2223\u2223 ]\n\u2264 1 w(\u00b5) E\u03c0\u03be\n[\u2211\n\u03bd\u2208M w(\u03bd | \u00e6<t)\n\u2223\u2223\u2223V \u03c0\u2217\u03bd\u03bd (\u00e6<t)\u2212 V\u0302 \u03c0 \u2217 \u03bd t (\u00e6<t) \u2223\u2223\u2223 ]\n= 1\nw(\u00b5)\n\u2211\n\u03bd\u2208M w(\u03bd)E\u03c0\u03be [ \u03bd\u03c0(\u00e6<t) \u03be\u03c0(\u00e6<t) \u2223\u2223\u2223V \u03c0\u2217\u03bd\u03bd (\u00e6<t)\u2212 V\u0302 \u03c0 \u2217 \u03bd t (\u00e6<t) \u2223\u2223\u2223 ]\n= 1\nw(\u00b5)\n\u2211\n\u03bd\u2208M w(\u03bd)E\u03c0\u03bd\n\u2223\u2223\u2223V \u03c0\u2217\u03bd\u03bd (\u00e6<t)\u2212 V\u0302 \u03c0 \u2217 \u03bd t (\u00e6<t) \u2223\u2223\u2223\u2192 0\nby (2) and Hutter (2005, Lem. 5.28ii).\nIf we don\u2019t require the condition on strongly unique optimal policies, then the policy \u03c0 could be asymptotically optimal while EP 6\u2192 0: there might be another policy \u03c0\u2032 that is very different from any optimal policy \u03c0\u2217\u00b5, but whose \u00b5-value approaches the optimal value: V \u2217\u00b5 \u2212 V \u03c0 \u2032 \u00b5 \u2192 0 as t \u2192 \u221e. Our policy \u03c0 could converge to \u03c0\u2032 without EP converging to 0."}, {"heading": "4 Exploration Potential in Multi-Armed Bandits", "text": "In this section we use experiments with multi-armed Bernoulli bandits to illustrate the properties of exploration potential. The class of Bernoulli bandits is \u0398 = [0, 1]k (the arms\u2019 means). In each time step, the agent chooses an action (arm) i \u2208 {1, . . . , k} and receives a reward rt \u223c Bernoulli(\u03b8\u2217i ) where \u03b8\u2217 \u2208 \u0398 is the true environment. Since \u0398 is uncountable, exploration potential is defined with an integral instead of a sum:\nEP\u0398(\u00e6<t) := \u222b\n\u0398 p(\u03b8 | \u00e6<t)|\u03b8j(\u03b8) \u2212 \u03b8\u0302j(\u03b8)|d\u03b8\nwhere p(\u03b8 | \u00e6<t) is the posterior distribution given the history \u00e6<t, \u03b8\u0302 := \u222b\n\u0398 \u03b8p(\u03b8 | \u00e6<t)d\u03b8 is the Bayes-mean parameter, and j(\u03b8) := arg maxi \u03b8i is the index of the best arm accoding to \u03b8.\nFigure 1 shows the exploration potential of several bandit algorithms, illustrating how much each algorithm explores. Notably, optimally confident UCB (Lattimore,\n2015) stops exploring around time step 700 and focuses on exploitation (because in contrast to the other algorithms it knows the horizon). Thompson sampling, round robin (alternate between all arms), and \u03b5-greedy explore continuously (but \u03b5-greedy is less effective). The optimal strategy (always pull the first arm) never explores and hence its exploration potential decreases only slightly.\nExploration potential naturally gives rise to an exploration strategy: greedily minimize Bayes-expected exploration potential (MinEP); see Algorithm 1. This strategy unsurprisingly explores more than all the other algorithms when measured on exploration potential, but in bandits it also turns out to be a decent exploitation strategy because it focuses its attention on the most promising arms. For empirical performance see Figure 2. However, MinEP is generally not a good exploitation strategy in more complex environments like MDPs.\nAlgorithm 1 The MinEP Algorithm 1: for t \u2208 N do 2: at := arg mina\u2208A Eet\u223cposterior[EP(\u00e6<taet)] 3: take action at 4: observe percept et"}, {"heading": "5 Discussion", "text": "Several variants on the definition exploration potential given in Definition 1 are conceivable. However, often they do not satisfy at least one of the properties that make our definition appealing. Either they break the necessity (Proposition 3), sufficiency (Proposition 6), our proofs thereof, or they make EP hard to compute. For example, we could replace |V \u2217\u03bd \u2212V\u0302 \u03c0 \u2217 \u03bd\nt | by |V \u2217\u03bd \u2212V \u03c0\u03bd |where \u03c0 is the agent\u2019s future policy. This preserves both necessesity and sufficiency, but relies on computing the agent\u2019s future policy. If the agent uses exploration potential for taking actions (e.g., for targeted exploration), then this definition becomes a self-referential equation and might be very hard to solve. Following Dearden et al. (1999), we could consider |V \u2217\u03bd \u2212 V\u0302 \u2217t | which has the convenient side-effect that it is model-free and therefore applies to more reinforcement learning algorithms. However, in this case the necessity guarantee (Proposition 6) requires the additional condition that the agent\u2019s policy converges to the greedy policy \u03c0\u2217\nV\u0302 . Moreover, this does not remove the\ndependence on a model since we still need a model classM and a posterior. Based on the recent successes in approximating information gain (Houthooft et al., 2016), we are hopeful that exploration potential can also be approximated in practice. Since computing the posterior is too costly for complex reinforcement learning problems, we could (randomly) generate a few environments and estimate the sum in Definition 1 with them.\nIn this paper we only scratch the surface on exploration potential and leave many open questions. Is this the correct definition? What are good rates at which EP should converge to 0? Is minimizing EP the most efficient exploration strategy? Can we compute EP more efficiently than information gain?"}, {"heading": "Acknowledgments", "text": "We thank Tor Lattimore, Marcus Hutter, and our coworkers at the FHI for valuable feedback and discussion."}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Marc G Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "Remi Munos"], "venue": "Technical report,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Model based Bayesian exploration", "author": ["Richard Dearden", "Nir Friedman", "David Andre"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Dearden et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1999}, {"title": "Curiosity-driven exploration in deep reinforcement learning via bayesian neural networks", "author": ["Rein Houthooft", "Xi Chen", "Yan Duan", "John Schulman", "Filip De Turck", "Pieter Abbeel"], "venue": "Technical report,", "citeRegEx": "Houthooft et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Optimally confident UCB: Improved regret for finite-armed bandits", "author": ["Tor Lattimore"], "venue": "Technical report,", "citeRegEx": "Lattimore.,? \\Q2015\\E", "shortCiteRegEx": "Lattimore.", "year": 2015}, {"title": "Nonparametric General Reinforcement Learning", "author": ["Jan Leike"], "venue": "PhD thesis, Australian National University,", "citeRegEx": "Leike.,? \\Q2016\\E", "shortCiteRegEx": "Leike.", "year": 2016}, {"title": "Learning purposeful behaviour in the absence of rewards", "author": ["Marlos C Machado", "Michael Bowling"], "venue": "Technical report,", "citeRegEx": "Machado and Bowling.,? \\Q2016\\E", "shortCiteRegEx": "Machado and Bowling.", "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Universal knowledge-seeking agents for stochastic environments", "author": ["Laurent Orseau", "Tor Lattimore", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Orseau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Orseau et al\\.", "year": 2013}, {"title": "Infomax strategies for an optimal balance between exploration and exploitation", "author": ["Gautam Reddy", "Antonio Celani", "Massimo Vergassola"], "venue": "Journal of Statistical Physics,", "citeRegEx": "Reddy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2016}, {"title": "Learning to optimize via information-directed sampling", "author": ["Dan Russo", "Benjamin Van Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Russo and Roy.,? \\Q2014\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2014}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Schmidhuber.,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2010}, {"title": "A Bayesian framework for reinforcement learning", "author": ["Malcolm Strens"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Strens.,? \\Q2000\\E", "shortCiteRegEx": "Strens.", "year": 2000}, {"title": "Planning to be surprised: Optimal bayesian exploration in dynamic environments", "author": ["Yi Sun", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "More sophisticated exploration strategies have been proposed: using information gain about the environment (Sun et al., 2011; Orseau et al., 2013; Houthooft et al., 2016) or pseudo-count (Bellemare et al.", "startOffset": 107, "endOffset": 170}, {"referenceID": 9, "context": "More sophisticated exploration strategies have been proposed: using information gain about the environment (Sun et al., 2011; Orseau et al., 2013; Houthooft et al., 2016) or pseudo-count (Bellemare et al.", "startOffset": 107, "endOffset": 170}, {"referenceID": 3, "context": "More sophisticated exploration strategies have been proposed: using information gain about the environment (Sun et al., 2011; Orseau et al., 2013; Houthooft et al., 2016) or pseudo-count (Bellemare et al.", "startOffset": 107, "endOffset": 170}, {"referenceID": 1, "context": ", 2016) or pseudo-count (Bellemare et al., 2016).", "startOffset": 24, "endOffset": 48}, {"referenceID": 12, "context": "In practice, these exploration strategies are employed by adding an exploration bonus (\u2018intrinsic motivation\u2019) to the reward signal (Schmidhuber, 2010).", "startOffset": 132, "endOffset": 151}, {"referenceID": 1, "context": ", 2016) or pseudo-count (Bellemare et al., 2016). In practice, these exploration strategies are employed by adding an exploration bonus (\u2018intrinsic motivation\u2019) to the reward signal (Schmidhuber, 2010). While the methods above require the agent to have a model of its environment and formalize the strategy \u2018explore by going to where the model has high uncertainty,\u2019 there are also model-free strategies like the automatic discovery of options proposed by Machado and Bowling (2016). However, none of these explicit exploration strategies take the problem\u2019s reward structure into account.", "startOffset": 25, "endOffset": 483}, {"referenceID": 4, "context": "This is readily exposed in optimistic policies like UCRL (Jaksch et al., 2010) and stochastic policies like PSRL (Strens, 2000), but these do not make the exploration/exploitation tradeoff explicitly.", "startOffset": 57, "endOffset": 78}, {"referenceID": 13, "context": ", 2010) and stochastic policies like PSRL (Strens, 2000), but these do not make the exploration/exploitation tradeoff explicitly.", "startOffset": 42, "endOffset": 56}, {"referenceID": 10, "context": "Another exploration quantity that is both necessary and sufficient for asymptotic optimality is information gain about the optimal policy (Russo and Van Roy, 2014; Reddy et al., 2016).", "startOffset": 138, "endOffset": 183}, {"referenceID": 0, "context": "MinEP outperforms UCB1 (Auer et al., 2002) after 10 000 steps, but neither Thompson sampling nor OCUCB.", "startOffset": 23, "endOffset": 42}, {"referenceID": 2, "context": "Following Dearden et al. (1999), we could consider |V \u2217 \u03bd \u2212 V\u0302 \u2217 t | which has the convenient side-effect that it is model-free and therefore applies to more reinforcement learning algorithms.", "startOffset": 10, "endOffset": 32}, {"referenceID": 3, "context": "Based on the recent successes in approximating information gain (Houthooft et al., 2016), we are hopeful that exploration potential can also be approximated in practice.", "startOffset": 64, "endOffset": 88}], "year": 2016, "abstractText": "We introduce exploration potential, a quantity that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem\u2019s reward structure into account. This leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across the entire environment class). Our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation.", "creator": "LaTeX with hyperref package"}}}