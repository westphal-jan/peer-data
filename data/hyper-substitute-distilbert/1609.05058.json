{"id": "1609.05058", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "A Formal Solution to the Grain of Truth Problem", "abstract": "a bayesian agent acting primal binary multi - agent condition requires to find the marginal agents'policies throughout its tail admits maximal probability to them ( in bare words, its prior contains a \\ emph { grain low truth } ). finding uniquely reasonably ordered class of policies that contains innate cognitive - productive policies with respect to every class is given as simply \\ emph { estimate no truth problem }. only small components contain implied to have a grain likelihood support and the subject has several related quantitative results. towards this version we yield a partial and general solution to theoretical full grain risk freedom problem : we construct their class of policies v contains inherently computable policies rated tall as meta - critical agents for every absolute probability prior utility output target. when goal environment resides unknown, bayes - optimal agents either fail to act perfectly \u03c9 asymptotically. however, agents based independently dynamic agents converge to either { \\ log } - nash equilibria in matching constraint geographic multi - agent environments. meaning these requirements are highly theoretical, we show that observers gradually arrive computationally approximated arbitrarily closely.", "histories": [["v1", "Fri, 16 Sep 2016 14:00:51 GMT  (22kb)", "http://arxiv.org/abs/1609.05058v1", "UAI 2016"]], "COMMENTS": "UAI 2016", "reviews": [], "SUBJECTS": "cs.AI cs.GT cs.LG", "authors": ["jan leike", "jessica taylor", "benya fallenstein"], "accepted": false, "id": "1609.05058"}, "pdf": {"name": "1609.05058.pdf", "metadata": {"source": "CRF", "title": "A Formal Solution to the Grain of Truth Problem", "authors": ["Jan Leike"], "emails": ["jan.leike@anu.edu.au", "jessica@intelligence.org", "benya@intelligence.org"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n05 05\n8v 1\n[ cs\n.A I]\n1 6\nSe p\nA Bayesian agent acting in a multi-agent environment learns to predict the other agents\u2019 policies if its prior assigns positive probability to them (in other words, its prior contains a grain of truth). Finding a reasonably large class of policies that contains the Bayes-optimal policies with respect to this class is known as the grain of truth problem. Only small classes are known to have a grain of truth and the literature contains several related impossibility results. In this paper we present a formal and general solution to the full grain of truth problem: we construct a class of policies that contains all computable policies as well as Bayes-optimal policies for every lower semicomputable prior over the class. When the environment is unknown, Bayes-optimal agents may fail to act optimally even asymptotically. However, agents based on Thompson sampling converge to play \u03b5-Nash equilibria in arbitrary unknown computable multi-agent environments. While these results are purely theoretical, we show that they can be computationally approximated arbitrarily closely.\nKeywords. General reinforcement learning, multi-agent systems, game theory, self-reflection, asymptotic optimality, Nash equilibrium, Thompson sampling, AIXI."}, {"heading": "1 INTRODUCTION", "text": "Consider the general setup of multiple reinforcement learning agents interacting sequentially in a known environment with the goal to maximize discounted reward.1 Each agent knows how the environment behaves, but does not know the other agents\u2019 behavior. The natural (Bayesian) approach would be to define a class of possible policies that the other\n1We mostly use the terminology of reinforcement learning. For readers from game theory we provide a dictionary in Table 1.\nReinforcement learning Game theory\nstochastic policy mixed strategy deterministic policy pure strategy agent player multi-agent environment infinite extensive-form game reward payoff/utility (finite) history history infinite history path of play\nTable 1: Terminology dictionary between reinforcement learning and game theory.\nagents could adopt and take a prior over this class. During the interaction, this prior gets updated to the posterior as our agent learns the others\u2019 behavior. Our agent then acts optimally with respect to this posterior belief.\nA famous result for infinitely repeated games states that as long as each agent assigns positive prior probability to the other agents\u2019 policies (a grain of truth) and each agent acts Bayes-optimal, then the agents converge to playing an \u03b5Nash equilibrium [KL93].\nAs an example, consider an infinitely repeated prisoners dilemma between two agents. In every time step the payoff matrix is as follows, where C means cooperate and D means defect.\nC D C 3/4, 3/4 0, 1 D 1, 0 1/4, 1/4\nDefine the set of policies \u03a0 := {\u03c0\u221e, \u03c00, \u03c01, . . .} where policy \u03c0t cooperates until time step t or the opponent defects (whatever happens first) and defects thereafter. The Bayes-optimal behavior is to cooperate until the posterior belief that the other agent defects in the time step after the next is greater than some constant (depending on the discount function) and then defect afterwards. Therefore Bayes-optimal behavior leads to a policy from the set \u03a0\n(regardless of the prior). If both agents are Bayes-optimal with respect to some prior, they both have a grain of truth and therefore they converge to a Nash equilibrium: either they both cooperate forever or after some finite time they both defect forever. Alternating strategies like TitForTat (cooperate first, then play the opponent\u2019s last action) are not part of the policy class \u03a0, and adding them to the class breaks the grain of truth property: the Bayes-optimal behavior is no longer in the class. This is rather typical; a Bayesian agent usually needs to be more powerful than its environment [LH15b].\nUntil now, classes that admit a grain of truth were known only for small toy examples such as the iterated prisoner\u2019s dilemma above [SLB09, Ch. 7.3]. The quest to find a large class admitting a grain of truth is known as the grain of truth problem [Hut09, Q. 5j]. The literature contains several impossibility results on the grain of truth problem [FY01, Nac97, Nac05] that identify properties that cannot be simultaneously satisfied for classes that allow a grain of truth.\nIn this paper we present a formal solution to multi-agent reinforcement learning and the grain of truth problem in the general setting (Section 3). We assume that our multiagent environment is computable, but it does not need to be stationary/Markov, ergodic, or finite-state [Hut05]. Our class of policies is large enough to contain all computable (stochastic) policies, as well as all relevant Bayes-optimal policies. At the same time, our class is small enough to be limit computable. This is important because it allows our result to be computationally approximated.\nIn Section 4 we consider the setting where the multi-agent environment is unknown to the agents and has to be learned in addition to the other agents\u2019 behavior. A Bayes-optimal agent may not learn to act optimally in unknown multiagent environments even though it has a grain of truth. This effect occurs in non-recoverable environments where taking one wrong action can mean a permanent loss of future value. In this case, a Bayes-optimal agent avoids taking these dangerous actions and therefore will not explore enough to wash out the prior\u2019s bias [LH15a]. Therefore, Bayesian agents are not asymptotically optimal, i.e., they do not always learn to act optimally [Ors13].\nHowever, asymptotic optimality is achieved by Thompson sampling because the inherent randomness of Thompson sampling leads to enough exploration to learn the entire environment class [LLOH16]. This leads to our main result: if all agents use Thompson sampling over our class of multi-agent environments, then for every \u03b5 > 0 they converge to an \u03b5-Nash equilibrium asymptotically.\nThe central idea to our construction is based on reflective oracles [FST15, FTC15b]. Reflective oracles are probabilistic oracles similar to halting oracles that answer whether the probability that a given probabilistic Turing\nmachine T outputs 1 is higher than a given rational number p. The oracles are reflective in the sense that the machine T may itself query the oracle, so the oracle has to answer queries about itself. This invites issues caused by self-referential liar paradoxes of the form \u201cif the oracle says that I return 1 with probability > 1/2, then return 0, else return 1.\u201d Reflective oracles avoid these issues by being allowed to randomize if the machines do not halt or the rational number is exactly the probability to output 1. We introduce reflective oracles formally in Section 2 and prove that there is a limit computable reflective oracle."}, {"heading": "2 REFLECTIVE ORACLES", "text": ""}, {"heading": "2.1 PRELIMINARIES", "text": "Let X denote a finite set called alphabet. The set X \u2217 := \u22c3\u221e\nn=0 X n is the set of all finite strings over the alphabet X , the set X\u221e is the set of all infinite strings over the alphabet X , and the set X \u266f := X \u2217 \u222a X\u221e is their union. The empty string is denoted by \u01eb, not to be confused with the small positive real number \u03b5. Given a string x \u2208 X \u266f, we denote its length by |x|. For a (finite or infinite) string x of length \u2265 k, we denote with x1:k the first k characters of x, and with x<k the first k\u2212 1 characters of x. The notation x1:\u221e stresses that x is an infinite string.\nA function f : X \u2217 \u2192 R is lower semicomputable iff the set {(x, p) \u2208 X \u2217 \u00d7 Q | f(x) > p} is recursively enumerable. The function f is computable iff both f and \u2212f are lower semicomputable. Finally, the function f is limit computable iff there is a computable function \u03c6 such that\nlim k\u2192\u221e \u03c6(x, k) = f(x).\nThe program \u03c6 that limit computes f can be thought of as an anytime algorithm for f : we can stop \u03c6 at any time k and get a preliminary answer. If the program \u03c6 ran long enough (which we do not know), this preliminary answer will be close to the correct one.\nWe use \u2206Y to denote the set of probability distributions over Y . A list of notation can be found in Appendix A."}, {"heading": "2.2 DEFINITION", "text": "A semimeasure over the alphabet X is a function \u03bd : X \u2217 \u2192 [0, 1] such that (i) \u03bd(\u01eb) \u2264 1, and (ii) \u03bd(x) \u2265 \u2211\na\u2208X \u03bd(xa) for all x \u2208 X \u2217. In the terminology of measure theory, semimeasures are probability measures on the probability space X \u266f = X \u2217\u222aX\u221e whose \u03c3-algebra is generated by the cylinder sets \u0393x := {xz | z \u2208 X \u266f} [LV08, Ch. 4.2]. We call a semimeasure (probability) a measure iff equalities hold in (i) and (ii) for all x \u2208 X \u2217.\nNext, we connect semimeasures to Turing machines. The literature uses monotone Turing machines, which naturally\ncorrespond to lower semicomputable semimeasures [LV08, Sec. 4.5.2] that describe the distribution that arises when piping fair coin flips into the monotone machine. Here we take a different route.\nA probabilistic Turing machine is a Turing machine that has access to an unlimited number of uniformly random coin flips. Let T denote the set of all probabilistic Turing machines that take some input in X \u2217 and may query an oracle (formally defined below). We take a Turing machine T \u2208 T to correspond to a semimeasure \u03bbT where \u03bbT (a | x) is the probability that T outputs a \u2208 X when given x \u2208 X \u2217 as input. The value of \u03bbT (x) is then given by the chain rule\n\u03bbT (x) :=\n|x| \u220f\nk=1\n\u03bbT (xk | x<k). (1)\nThus T gives rise to the set of semimeasures M where the conditionals \u03bb(a | x) are lower semicomputable. In contrast, the literature typically considers semimeasures whose joint probability (1) is lower semicomputable. This set M contains all computable measures. However, M is a proper subset of the set of all lower semicomputable semimeasures because the product (1) is lower semicomputale, but there are some lower semicomputable semimeasures whose conditional is not lower semicomputable [LH15c, Thm. 6].\nIn the following we assume that our alphabet is binary, i.e., X := {0, 1}.\nDefinition 1 (Oracle). An oracle is a function O : T \u00d7 {0, 1}\u2217 \u00d7Q \u2192 \u2206{0, 1}.\nOracles are understood to be probabilistic: they randomly return 0 or 1. Let TO denote the machine T \u2208 T when run with the oracle O, and let \u03bbOT denote the semimeasure induced by TO. This means that drawing from \u03bbOT involves two sources of randomness: one from the distribution induced by the probabilistic Turing machine T and one from the oracle\u2019s answers.\nThe intended semantics of an oracle are that it takes a query (T, x, p) and returns 1 if the machine TO outputs 1 on input x with probability greater than p when run with the oracle O, i.e., when \u03bbOT (1 | x) > p. Furthermore, the oracle returns 0 if the machine TO outputs 1 on input x with probability less than p when run with the oracle O, i.e., when \u03bbOT (1 | x) < p. To fulfill this, the oracle O has to make statements about itself, since the machine T from the query may again query O. Therefore we call oracles of this kind reflective oracles. This has to be defined very carefully to avoid the obvious diagonalization issues that are caused by programs that ask the oracle about themselves. We impose the following self-consistency constraint.\nDefinition 2 (Reflective Oracle). An oracle O is reflective iff for all queries (T, x, p) \u2208 T \u00d7 {0, 1}\u2217 \u00d7Q,\n(i) \u03bbOT (1 | x) > p implies O(T, x, p) = 1, and\n(ii) \u03bbOT (0 | x) > 1\u2212 p implies O(T, x, p) = 0.\nIf p under- or overshoots the true probability of \u03bbOT ( \u00b7 | x), then the oracle must reveal this information. However, in the critical case when p = \u03bbOT (1 | x), the oracle is allowed to return anything and may randomize its result. Furthermore, since T might not output any symbol, it is possible that \u03bbOT (0 | x) + \u03bb O T (1 | x) < 1. In this case the oracle can reassign the non-halting probability mass to 0, 1, or randomize; see Figure 1.\nExample 3 (Reflective Oracles and Diagonalization). Let T \u2208 T be a probabilistic Turing machine that outputs 1\u2212O(T, \u01eb, 1/2) (T can know its own source code by quining [Kle52, Thm. 27]). In other words, T queries the oracle about whether it is more likely to output 1 or 0, and then does whichever the oracle says is less likely. In this case we can use an oracle O(T, \u01eb, 1/2) := 1/2 (answer 0 or 1 with equal probability), which implies \u03bbOT (1 | \u01eb) = \u03bb O T (0 | \u01eb) = 1/2, so the conditions of Definition 2 are satisfied. In fact, for this machine T we must have O(T, \u01eb, 1/2) = 1/2 for all reflective oracles O. \u2666\nThe following theorem establishes that reflective oracles exist.\nTheorem 4 ([FTC15a, App. B]). There is a reflective oracle.\nDefinition 5 (Reflective-Oracle-Computable). A semimeasure is called reflective-oracle-computable iff it is computable on a probabilistic Turing machine with access to a reflective oracle.\nFor any probabilistic Turing machine T \u2208 T we can complete the semimeasure \u03bbOT ( \u00b7 | x) into a reflectiveoracle-computable measure \u03bb O\nT ( \u00b7 | x): Using the oracle O and a binary search on the parameter p we search for the crossover point p where O(T, x, p) goes from returning 1 to returning 0. The limit point p\u2217 \u2208 R of the binary search is random since the oracle\u2019s answers may be random. But the main point is that the expectation of p\u2217 exists, so \u03bb O\nT (1 | x) = E[p \u2217] = 1 \u2212 \u03bb\nO T (0 | x) for all\nx \u2208 X \u2217. Hence \u03bb O\nT is a measure. Moreover, if the oracle\nis reflective, then \u03bb O\nT (x) \u2265 \u03bb O T (x) for all x \u2208 X \u2217. In this sense the oracle O can be viewed as a way of \u2018completing\u2019\nall semimeasures \u03bbOT to measures by arbitrarily assigning the non-halting probability mass. If the oracle O is reflective this is consistent in the sense that Turing machines who run other Turing machines will be completed in the same way. This is especially important for a universal machine that runs all other Turing machines to induce a Solomonoffstyle distribution."}, {"heading": "2.3 A LIMIT COMPUTABLE REFLECTIVE ORACLE", "text": "The proof of Theorem 4 given in [FTC15a, App. B] is nonconstructive and uses the axiom of choice. In Section 2.4 we give a constructive proof for the existence of reflective oracles and show that there is one that is limit computable.\nTheorem 6 (A Limit Computable Reflective Oracle). There is a reflective oracle that is limit computable.\nThis theorem has the immediate consequence that reflective oracles cannot be used as halting oracles. At first, this result may seem surprising: according to the definition of reflective oracles, they make concrete statements about the output of probabilistic Turing machines. However, the fact that the oracles may randomize some of the time actually removes enough information such that halting can no longer be decided from the oracle output.\nCorollary 7 (Reflective Oracles are not Halting Oracles). There is no probabilistic Turing machine T such that for every prefix program p and every reflective oracle O, we have that \u03bbOT (1 | p) > 1/2 if p halts and \u03bb O T (1 | p) < 1/2 otherwise.\nProof. Assume there was such a machine T and let O be the limit computable oracle from Theorem 6. Since O is reflective we can turn T into a deterministic halting oracle by calling O(T, p, 1/2) which deterministically returns 1 if p halts and 0 otherwise. Since O is limit computable, we can finitely compute the output of O on any query to arbitrary finite precision using our deterministic halting oracle. We construct a probabilistic Turing machine T \u2032 that uses our halting oracle to compute (rather than query) the oracle O on (T \u2032, \u01eb, 1/2) to a precision of 1/3 in finite time. If O(T \u2032, \u01eb, 1/2)\u00b1 1/3 > 1/2, the machine T \u2032 outputs 0, otherwise T \u2032 outputs 1. Since our halting oracle is entirely deterministic, the output of T \u2032 is entirely deterministic as well (and T \u2032 always halts), so \u03bbOT \u2032(0 | \u01eb) = 1 or \u03bb O T \u2032 (1 | \u01eb) = 1. Therefore O(T \u2032, \u01eb, 1/2) = 1 or O(T \u2032, \u01eb, 1/2) = 0 because O is reflective. A precision of 1/3 is enough to tell them apart, hence T \u2032 returns 0 if O(T \u2032, \u01eb, 1/2) = 1 and T \u2032 returns 1 if O(T \u2032, \u01eb, 1/2) = 0. This is a contradiction.\nA similar argument can also be used to show that reflective oracles are not computable."}, {"heading": "2.4 PROOF OF THEOREM 6", "text": "The idea for the proof of Theorem 6 is to construct an algorithm that outputs an infinite series of partial oracles converging to a reflective oracle in the limit.\nThe set of queries is countable, so we can assume that we have some computable enumeration of it:\nT \u00d7 {0, 1}\u2217 \u00d7Q =: {q1, q2, . . .}\nDefinition 8 (k-Partial Oracle). A k-partial oracle O\u0303 is function from the first k queries to the multiples of 2\u2212k in [0, 1]:\nO\u0303 : {q1, q2, . . . , qk} \u2192 {n2 \u2212k | 0 \u2264 n \u2264 2k}\nDefinition 9 (Approximating an Oracle). A k-partial oracle O\u0303 approximates an oracle O iff |O(qi) \u2212 O\u0303(qi)| \u2264 2\u2212k\u22121 for all i \u2264 k.\nLet k \u2208 N, let O\u0303 be a k-partial oracle, and let T \u2208 T be an oracle machine. The machine T O\u0303 that we get when we run T with the k-partial oracle O\u0303 is defined as follows (this is with slight abuse of notation since k is taken to be understood implicitly).\n1. Run T for at most k steps.\n2. If T calls the oracle on qi for i \u2264 k,\n(a) return 1 with probability O\u0303(qi)\u2212 2\u2212k\u22121, (b) return 0 with probability 1\u2212 O\u0303(qi)\u2212 2\u2212k\u22121, and\n(c) halt otherwise.\n3. If T calls the oracle on qj for j > k, halt.\nFurthermore, we define \u03bbO\u0303T analogously to \u03bb O T as the distribution generated by the machine T O\u0303.\nLemma 10. If a k-partial oracle O\u0303 approximates a reflective oracle O, then \u03bbOT (1 | x) \u2265 \u03bb O\u0303 T (1 | x) and \u03bbOT (0 | x) \u2265 \u03bb O\u0303 T (0 | x) for all x \u2208 {0, 1} \u2217 and all T \u2208 T .\nProof. This follows from the definition of T O\u0303: when running T with O\u0303 instead of O, we can only lose probability mass. If T makes calls whose index is > k or runs for more than k steps, then the execution is aborted and no further output is generated. If T makes calls whose index i \u2264 k, then O\u0303(qi)\u2212 2\u2212k\u22121 \u2264 O(qi) since O\u0303 approximates O. Therefore the return of the call qi is underestimated as well.\nDefinition 11 (k-Partially Reflective). A k-partial oracle O\u0303 is k-partially reflective iff for the first k queries (T, x, p)\n\u2022 \u03bbO\u0303T (1 | x) > p implies O\u0303(T, x, p) = 1, and\n\u2022 \u03bbO\u0303T (0 | x) > 1\u2212 p implies O\u0303(T, x, p) = 0.\nIt is important to note that we can check whether a k-partial oracle is k-partially reflective in finite time by running all machines T from the first k queries for k steps and tallying up the probabilities to compute \u03bbO\u0303T . Lemma 12. If O is a reflective oracle and O\u0303 is a k-partial oracle that approximatesO, then O\u0303 is k-partially reflective.\nLemma 12 only holds because we use semimeasures whose conditionals are lower semicomputable.\nProof. Assuming \u03bbO\u0303T (1 | x) > p we get from Lemma 10 that \u03bbOT (1 | x) \u2265 \u03bb O\u0303 T (1 | x) > p. Thus O(T, x, p) = 1 because O is reflective. Since O\u0303 approximates O, we get 1 = O(T, x, p) \u2264 O\u0303(T, x, p)+2\u2212k\u22121, and since O\u0303 assigns values in a 2\u2212k-grid, it follows that O\u0303(T, x, p) = 1. The second implication is proved analogously.\nDefinition 13 (Extending Partial Oracles). A k + 1-partial oracle O\u0303\u2032 extends a k-partial oracle O\u0303 iff |O\u0303(qi)\u2212O\u0303\u2032(qi)| \u2264 2\u2212k\u22121 for all i \u2264 k.\nLemma 14. There is an infinite sequence of partial oracles (O\u0303k)k\u2208N such that for each k, O\u0303k is a k-partially reflective k-partial oracle and O\u0303k+1 extends O\u0303k.\nProof. By Theorem 4 there is a reflective oracle O. For every k, there is a canonical k-partial oracle O\u0303k that approximates O: restrict O to the first k queries and for any such query q pick the value in the 2\u2212k-grid which is closest to O(q). By construction, O\u0303k+1 extends O\u0303k and by Lemma 12, each O\u0303k is k-partially reflective.\nLemma 15. If the k + 1-partial oracle O\u0303k+1 extends the k-partial oracle O\u0303k, then \u03bb O\u0303k+1 T (1 | x) \u2265 \u03bb O\u0303k T (1 | x) and \u03bb O\u0303k+1 T (0 | x) \u2265 \u03bb O\u0303k T (0 | x) for all x \u2208 {0, 1}\n\u2217 and all T \u2208 T .\nProof. T O\u0303k+1 runs for one more step than T O\u0303k , can answer one more query and has increased oracle precision. Moreover, since O\u0303k+1 extends O\u0303k , we have |O\u0303k+1(qi) \u2212 O\u0303k(qi)| \u2264 2\n\u2212k\u22121, and thus O\u0303k+1(qi)\u22122\u2212k\u22121 \u2265 O\u0303k(qi)\u2212 2\u2212k. Therefore the success to answers to the oracle calls (case 2(a) and 2(b)) will not decrease in probability.\nNow everything is in place to state the algorithm that constructs a reflective oracle in the limit. It recursively traverses a tree of partial oracles. The tree\u2019s nodes are the partial oracles; level k of the tree contains all k-partial oracles. There is an edge in the tree from the k-partial oracle O\u0303k to the i-partial oracle O\u0303i if and only if i = k + 1 and O\u0303i extends O\u0303k.\nFor every k, there are only finitely many k-partial oracles, since they are functions from finite sets to finite sets. In particular, there are exactly two 1-partial oracles (so the search\ntree has two roots). Pick one of them to start with, and proceed recursively as follows. Given a k-partial oracle O\u0303k, there are finitely many (k + 1)-partial oracles that extend O\u0303k (finite branching of the tree). Pick one that is (k + 1)- partially reflective (which can be checked in finite time). If there is no (k+1)-partially reflective extension, backtrack.\nBy Lemma 14 our search tree is infinitely deep and thus the tree search does not terminate. Moreover, it can backtrack to each level only a finite number of times because at each level there is only a finite number of possible extensions. Therefore the algorithm will produce an infinite sequence of partial oracles, each extending the previous. Because of finite backtracking, the output eventually stabilizes on a sequence of partial oracles O\u03031, O\u03032, . . .. By the following lemma, this sequence converges to a reflective oracle, which concludes the proof of Theorem 6.\nLemma 16. Let O\u03031, O\u03032, . . . be a sequence where O\u0303k is a k-partially reflective k-partial oracle and O\u0303k+1 extends O\u0303k for all k \u2208 N. Let O := limk\u2192\u221e O\u0303k be the pointwise limit. Then\n(a) \u03bbO\u0303kT (1 | x) \u2192 \u03bb O T (1 | x) and \u03bb O\u0303k T (0 | x) \u2192 \u03bb O T (0 | x)\nas k \u2192 \u221e for all x \u2208 {0, 1}\u2217 and all T \u2208 T , and\n(b) O is a reflective oracle.\nProof. First note that the pointwise limit must exists because |O\u0303k(qi)\u2212 O\u0303k+1(qi)| \u2264 2\u2212k\u22121 by Definition 13.\n(a) Since O\u0303k+1 extends O\u0303k, each O\u0303k approximates O. Let x \u2208 {0, 1}\u2217 and T \u2208 T and consider the se-\nquence ak := \u03bb O\u0303k T (1 | x) for k \u2208 N. By Lemma 15, ak \u2264 ak+1, so the sequence is monotone increasing. By Lemma 10, ak \u2264 \u03bbOT (1 | x), so the sequence is bounded. Therefore it must converge. But it cannot converge to anything strictly below \u03bbOT (1 | x) by the definition of TO.\n(b) By definition, O is an oracle; it remains to show that O is reflective. Let qi = (T, x, p) be some query. If p < \u03bbOT (1 | x), then by (a) there is a k large\nenough such that p < \u03bbO\u0303tT (1 | x) for all t \u2265 k. For any t \u2265 max{k, i}, we have O\u0303t(T, x, p) = 1 since O\u0303t is t-partially reflective. Therefore 1 = limk\u2192\u221e O\u0303k(T, x, p) = O(T, x, p). The case 1 \u2212 p < \u03bbOT (0 | x) is analogous."}, {"heading": "3 A GRAIN OF TRUTH", "text": ""}, {"heading": "3.1 NOTATION", "text": "In reinforcement learning, an agent interacts with an environment in cycles: at time step t the agent chooses an action at \u2208 A and receives a percept et = (ot, rt) \u2208 E\nconsisting of an observation ot \u2208 O and a real-valued reward rt \u2208 R; the cycle then repeats for t + 1. A history is an element of (A\u00d7E)\u2217. In this section, we use \u00e6 \u2208 A\u00d7E to denote one interaction cycle, and \u00e6<t to denote a history of length t\u2212 1.\nWe fix a discount function \u03b3 : N \u2192 R with \u03b3t \u2265 0 and \u2211\u221e\nt=1 \u03b3t < \u221e. The goal in reinforcement learning is to maximize discounted rewards\n\u2211\u221e t=1 \u03b3trt. The discount\nnormalization factor is defined as \u0393t := \u2211\u221e\nk=t \u03b3k. The effective horizon Ht(\u03b5) is a horizon that is long enough to encompass all but an \u03b5 of the discount function\u2019s mass:\nHt(\u03b5) := min{k | \u0393t+k/\u0393t \u2264 \u03b5} (2)\nA policy is a function \u03c0 : (A \u00d7 E)\u2217 \u2192 \u2206A that maps a history \u00e6<t to a distribution over actions taken after seeing this history. The probability of taking action a after history \u00e6<t is denoted with \u03c0(a | \u00e6<t). An environment is a function \u03bd : (A \u00d7 E)\u2217 \u00d7 A \u2192 \u2206E where \u03bd(e | \u00e6<tat) denotes the probability of receiving the percept e when taking the action at after the history \u00e6<t. Together, a policy \u03c0 and an environment \u03bd give rise to a distribution \u03bd\u03c0 over histories. Throughout this paper, we make the following assumptions.\nAssumption 17. (a) Rewards are bounded between 0 and 1.\n(b) The set of actions A and the set of percepts E are both finite.\n(c) The discount function \u03b3 and the discount normalization factor \u0393 are computable.\nDefinition 18 (Value Function). The value of a policy \u03c0 in an environment \u03bd given history \u00e6<t is defined recursively as V \u03c0\u03bd (\u00e6<t) := \u2211 a\u2208A \u03c0(a | \u00e6<t)V \u03c0 \u03bd (\u00e6<ta) and\nV \u03c0\u03bd (\u00e6<tat) := 1\n\u0393t\n\u2211\net\u2208E\n\u03bd(et | \u00e6<tat) ( \u03b3trt + \u0393t+1V \u03c0 \u03bd (\u00e61:t) )\nif \u0393t > 0 and V \u03c0\u03bd (\u00e6<tat) := 0 if \u0393t = 0. The optimal value is defined as V \u2217\u03bd (\u00e6<t) := sup\u03c0 V \u03c0 \u03bd (\u00e6<t). Definition 19 (Optimal Policy). A policy \u03c0 is optimal in environment \u03bd (\u03bd-optimal) iff for all histories \u00e6<t \u2208 (A\u00d7 E)\u2217 the policy \u03c0 attains the optimal value: V \u03c0\u03bd (\u00e6<t) = V \u2217\u03bd (\u00e6<t).\nWe assumed that the discount function is summable, rewards are bounded (Assumption 17a), and actions and percepts spaces are both finite (Assumption 17b). Therefore an optimal deterministic policy exists for every environment [LH14, Thm. 10]."}, {"heading": "3.2 REFLECTIVE BAYESIAN AGENTS", "text": "Fix O to be a reflective oracle. From now on, we assume that the action space A := {\u03b1, \u03b2} is binary. We can treat\ncomputable measures over binary strings as environments: the environment \u03bd corresponding to a probabilistic Turing machine T \u2208 T is defined by\n\u03bd(et | \u00e6<tat) := \u03bb O T (y | x) = k \u220f\ni=1\n\u03bb O\nT (yi | xy1 . . . yi\u22121)\nwhere y1:k is a binary encoding of et and x is a binary encoding of \u00e6<tat. The actions a1:\u221e are only contextual, and not part of the environment distribution. We define\n\u03bd(e<t | a<t) :=\nt\u22121 \u220f\nk=1\n\u03bd(ek | \u00e6<k).\nLet T1, T2, . . . be an enumeration of all probabilistic Turing machines in T . We define the class of reflective environments\nMOrefl := { \u03bb O T1 , \u03bb O T2 , . . . } .\nThis is the class of all environments computable on a probabilistic Turing machine with reflective oracle O, that have been completed from semimeasures to measures using O.\nAnalogously to AIXI [Hut05], we define a Bayesian mixture over the class MOrefl. Let w \u2208 \u2206M O refl be a lower semicomputable prior probability distribution on MOrefl. Possible choices for the prior include the Solomonoff prior w ( \u03bb O\nT\n)\n:= 2\u2212K(T ), where K(T ) denotes the length of the shortest input to some universal Turing machine that encodes T [Sol78].2 We define the corresponding Bayesian mixture\n\u03be(et | \u00e6<tat) := \u2211\n\u03bd\u2208MOrefl\nw(\u03bd | \u00e6<t)\u03bd(et | \u00e6<tat) (3)\nwhere w(\u03bd | \u00e6<t) is the (renormalized) posterior,\nw(\u03bd | \u00e6<t) := w(\u03bd) \u03bd(e<t | a<t)\n\u03be(e<t | a<t) . (4)\nThe mixture \u03be is lower semicomputable on an oracle Turing machine because the posterior w( \u00b7 | \u00e6<t) is lower semicomputable. Hence there is an oracle machine T such that \u03be = \u03bbOT . We define its completion \u03be := \u03bb O\nT as the completion of \u03bbOT . This is the distribution that is used to compute the posterior. There are no cyclic dependencies since \u03be is called on the shorter history \u00e6<t. We arrive at the following statement.\nProposition 20 (Bayes is in the Class). \u03be \u2208 MOrefl.\nMoreover, since O is reflective, we have that \u03be dominates all environments \u03bd \u2208 MOrefl:\n\u03be(e1:t | a1:t)\n2Technically, the lower semicomputable prior 2\u2212K(T ) is only a semidistribution because it does not sum to 1. This turns out to be unimportant.\n= \u03be(et | \u00e6<tat)\u03be(e<t | a<t)\n\u2265 \u03be(et | \u00e6<tat)\u03be(e<t | a<t) = \u03be(e<t | a<t) \u2211\n\u03bd\u2208MOrefl\nw(\u03bd | \u00e6<t)\u03bd(et | \u00e6<tat)\n= \u03be(e<t | a<t) \u2211\n\u03bd\u2208MOrefl\nw(\u03bd) \u03bd(e<t | a<t)\n\u03be(e<t | a<t) \u03bd(et | \u00e6<tat)\n= \u2211\n\u03bd\u2208MOrefl\nw(\u03bd)\u03bd(e1:t | a1:t)\n\u2265 w(\u03bd)\u03bd(e1:t | a1:t)\nThis property is crucial for on-policy value convergence.\nLemma 21 (On-Policy Value Convergence [Hut05, Thm. 5.36]). For any policy \u03c0 and any environment \u00b5 \u2208 MOrefl with w(\u00b5) > 0,\nV \u03c0\u00b5 (\u00e6<t)\u2212 V \u03c0 \u03be (\u00e6<t) \u2192 0 \u00b5\u03c0-almost surely as t \u2192 \u221e."}, {"heading": "3.3 REFLECTIVE-ORACLE-COMPUTABLE POLICIES", "text": "This subsection is dedicated to the following result that was previously stated but not proved in [FST15, Alg. 6]. It contrasts results on arbitrary semicomputable environments where optimal policies are not limit computable [LH15b, Sec. 4].\nTheorem 22 (Optimal Policies are Oracle Computable). For every \u03bd \u2208 MOrefl, there is a \u03bd-optimal (stochastic) policy \u03c0\u2217\u03bd that is reflective-oracle-computable.\nNote that even though deterministic optimal policies always exist, those policies are typically not reflectiveoracle-computable.\nTo prove Theorem 22 we need the following lemma.\nLemma 23 (Reflective-Oracle-Computable Optimal Value Function). For every environment \u03bd \u2208 MOrefl the optimal value function V \u2217\u03bd is reflective-oracle-computable.\nProof. This proof follows the proof of [LH15b, Cor. 13]. We write the optimal value explicitly as\nV \u2217\u03bd (\u00e6<t) = 1\n\u0393t lim m\u2192\u221e max \u2211\n\u00e6t:m\nm \u2211\nk=t\n\u03b3krk\nk \u220f\ni=t\n\u03bd(ei | \u00e6<i),\n(5) where \u2211 max denotes the expectimax operator:\nmax \u2211\n\u00e6t:m\n:= max at\u2208A\n\u2211\net\u2208E\n. . . max am\u2208A\n\u2211\nem\u2208E\nFor a fixed m, all involved quantities are reflective-oraclecomputable. Moreover, this quantity is monotone increasing in m and the tail sum from m+ 1 to \u221e is bounded by \u0393m+1 which is computable according to Assumption 17c and converges to 0 as m \u2192 \u221e. Therefore we can enumerate all rationals above and below V \u2217\u03bd .\nProof of Theorem 22. According to Lemma 23 the optimal value function V \u2217\u03bd is reflective-oracle-computable. Hence there is a probabilistic Turing machine T such that\n\u03bbOT (1 | \u00e6<t) = ( V \u2217\u03bd (\u00e6<t\u03b1)\u2212 V \u2217 \u03bd (\u00e6<t\u03b2) + 1 ) /2.\nWe define a policy \u03c0 that takes action \u03b1 if O(T,\u00e6<t, 1/2) = 1 and action \u03b2 if O(T,\u00e6<t, 1/2) = 0. (This policy is stochastic because the answer of the oracle O is stochastic.)\nIt remains to show that \u03c0 is a \u03bd-optimal policy. If V \u2217\u03bd (\u00e6<t\u03b1) > V \u2217 \u03bd (\u00e6<t\u03b2), then \u03bb O T (1 | \u00e6<t) > 1/2, thus O(T,\u00e6<t, 1/2) = 1 since O is reflective, and hence \u03c0 takes action \u03b1. Conversely, if V \u2217\u03bd (\u00e6<t\u03b1) < V \u2217 \u03bd (\u00e6<t\u03b2), then \u03bbOT (1 | \u00e6<t) < 1/2, thus O(T,\u00e6<t, 1/2) = 0 since O is reflective, and hence \u03c0 takes action \u03b2. Lastly, if V \u2217\u03bd (\u00e6<t\u03b1) = V \u2217 \u03bd (\u00e6<t\u03b2), then both actions are optimal and thus it does not matter which action is returned by policy \u03c0. (This is the case where the oracle may randomize.)"}, {"heading": "3.4 SOLUTION TO THE GRAIN OF TRUTH PROBLEM", "text": "Together, Proposition 20 and Theorem 22 provide the necessary ingredients to solve the grain of truth problem.\nCorollary 24 (Solution to the Grain of Truth Problem). For every lower semicomputable prior w \u2208 \u2206MOrefl the Bayesoptimal policy \u03c0\u2217\n\u03be is reflective-oracle-computable where \u03be\nis the Bayes-mixture corresponding to w defined in (3).\nProof. From Proposition 20 and Theorem 22.\nHence the environment class MOrefl contains any reflectiveoracle-computable modification of the Bayes-optimal policy \u03c0\u2217\n\u03be . In particular, this includes computable multi-agent\nenvironments that contain other Bayesian agents over the class MOrefl. So any Bayesian agent over the class M O refl has a grain of truth even though the environment may contain other Bayesian agents of equal power. We proceed to sketch the implications for multi-agent environments in the next section."}, {"heading": "4 MULTI-AGENT ENVIRONMENTS", "text": "This section summarizes our results for multi-agent systems. The proofs can be found in [Lei16]."}, {"heading": "4.1 SETUP", "text": "In a multi-agent environment there are n agents each taking sequential actions from the finite action space A. In each time step t = 1, 2, . . ., the environment receives action ait from agent i and outputs n percepts e1t , . . . , e n t \u2208 E , one for\neach agent. Each percept eit = (o i t, r i t) contains an observation oit and a reward r i t \u2208 [0, 1]. Importantly, agent i only sees its own action ait and its own percept e i t (see Figure 2). We use the shorthand notation at := (a1t , . . . , a n t ) and et := (e 1 t , . . . , e n t ) and denote \u00e6 i <t = a i 1e i 1 . . . a i t\u22121e i t\u22121 and \u00e6<t = a1e1 . . . at\u22121et\u22121.\nWe define a multi-agent environment as a function\n\u03c3 : (An \u00d7 En)\u2217 \u00d7An \u2192 \u2206(En).\nThe agents are given by n policies \u03c01, . . . , \u03c0n where \u03c0i : (A\u00d7 E)\u2217 \u2192 \u2206A. Together they specify the history distribution\n\u03c3\u03c01:n(\u01eb) : = 1\n\u03c3\u03c01:n(\u00e61:t) : = \u03c3\u03c01:n(\u00e6<tat)\u03c3(et | \u00e6<tat)\n\u03c3\u03c01:n(\u00e6<tat) : = \u03c3\u03c01:n(\u00e6<t) n \u220f\ni=1\n\u03c0i(a i t | \u00e6 i <t).\nEach agent i acts in a subjective environment \u03c3i given by joining the multi-agent environment \u03c3 with the policies \u03c01, . . . , \u03c0i\u22121, \u03c0i+1, . . . , \u03c0n by marginalizing over the histories that \u03c0i does not see. Together with policy \u03c0i, the environment \u03c3i yields a distribution over the histories of agent i\n\u03c3\u03c0ii (\u00e6 i <t) :=\n\u2211\n\u00e6j<t,j 6=i\n\u03c3\u03c01:n(\u00e6<t).\nWe get the definition of the subjective environment \u03c3i with the identity \u03c3i(eit | \u00e6 i <ta i t) := \u03c3 \u03c0i i (e i t | \u00e6 i <ta i t). It is crucial to note that the subjective environment \u03c3i and the policy \u03c0i are ordinary environments and policies, so we can use the formalism from Section 3.\nOur definition of a multi-agent environment is very general and encompasses most of game theory. It allows for cooperative, competitive, and mixed games; infinitely repeated games or any (infinite-length) extensive form games with finitely many players.\nThe policy \u03c0i is an \u03b5-best response after history \u00e6i<t iff\nV \u2217\u03c3i(\u00e6 i <t)\u2212 V \u03c0i \u03c3i (\u00e6i<t) < \u03b5.\nIf at some time step t, all agents\u2019 policies are \u03b5-best responses, we have an \u03b5-Nash equilibrium. The property of multi-agent systems that is analogous to asymptotic optimality is convergence to an \u03b5-Nash equilibrium."}, {"heading": "4.2 INFORMED REFLECTIVE AGENTS", "text": "Let \u03c3 be a multi-agent environment and let \u03c0\u2217\u03c31 , . . . \u03c0 \u2217 \u03c3n be such that for each i the policy \u03c0\u2217\u03c3i is an optimal policy in agent i\u2019s subjective environment \u03c3i. At first glance this seems ill-defined: The subjective environment \u03c3i depends on each other policy \u03c0\u2217\u03c3j for j 6= i, which depends on the subjective environment \u03c3j , which in turn depends on the policy \u03c0\u2217\u03c3i . However, this circular definition actually has a well-defined solution.\nTheorem 25 (Optimal Multi-Agent Policies). For any reflective-oracle-computable multi-agent environment \u03c3, the optimal policies \u03c0\u2217\u03c31 , . . . , \u03c0 \u2217 \u03c3n\nexist and are reflectiveoracle-computable.\nNote the strength of Theorem 25: each of the policies \u03c0\u2217\u03c3i is acting optimally given the knowledge of everyone else\u2019s policies. Hence optimal policies play 0-best responses by definition, so if every agent is playing an optimal policy, we have a Nash equilibrium. Moreover, this Nash equilibrium is also a subgame perfect Nash equilibrium, because each agent also acts optimally on the counterfactual histories that do not end up being played. In other words, Theorem 25 states the existence and reflectiveoracle-computability of a subgame perfect Nash equilibrium in any reflective-oracle-computable multi-agent environment. From Theorem 6 we then get that these subgame perfect Nash equilibria are limit computable.\nCorollary 26 (Solution to Computable Multi-Agent Environments). For any computable multi-agent environment \u03c3, the optimal policies \u03c0\u2217\u03c31 , . . . , \u03c0 \u2217 \u03c3n\nexist and are limit computable."}, {"heading": "4.3 LEARNING REFLECTIVE AGENTS", "text": "Since our class MOrefl solves the grain of truth problem, the result by Kalai and Lehrer [KL93] immediately implies that for any Bayesian agents \u03c01, . . . , \u03c0n interacting in an infinitely repeated game and for all \u03b5 > 0 and all i \u2208 {1, . . . , n} there is almost surely a t0 \u2208 N such that for all t \u2265 t0 the policy \u03c0i is an \u03b5-best response. However, this hinges on the important fact that every agent has to know the game and also that all other agents are Bayesian agents. Otherwise the convergence to an \u03b5-Nash equilibrium may fail, as illustrated by the following example.\nAt the core of the following construction is a dogmatic prior [LH15a, Sec. 3.2]. A dogmatic prior assigns very\nhigh probability to going to hell (reward 0 forever) if the agent deviates from a given computable policy \u03c0. For a Bayesian agent it is thus only worth deviating from the policy \u03c0 if the agent thinks that the prospects of following \u03c0 are very poor already. This implies that for general multi-agent environments and without additional assumptions on the prior, we cannot prove any meaningful convergence result about Bayesian agents acting in an unknown multi-agent environment.\nExample 27 (Reflective Bayesians Playing Matching Pennies). In the game of matching pennies there are two agents (n = 2), and two actions A = {\u03b1, \u03b2} representing the two sides of a penny. In each time step agent 1 wins if the two actions are identical and agent 2 wins if the two actions are different. The payoff matrix is as follows.\n\u03b1 \u03b2 \u03b1 1,0 0,1 \u03b2 0,1 1,0\nWe use E = {0, 1} to be the set of rewards (observations are vacuous) and define the multi-agent environment \u03c3 to give reward 1 to agent 1 iff a1t = a 2 t (0 otherwise) and reward 1 to agent 2 iff a1t 6= a 2 t (0 otherwise). Note that neither agent knows a priori that they are playing matching pennies, nor that they are playing an infinite repeated game with one other player.\nLet \u03c01 be the policy that takes the action sequence (\u03b1\u03b1\u03b2)\u221e and let \u03c02 := \u03c0\u03b1 be the policy that always takes action \u03b1. The average reward of policy \u03c01 is 2/3 and the average reward of policy \u03c02 is 1/3. Let \u03be be a universal mixture (3). By Lemma 21, V \u03c01\n\u03be \u2192 c1 \u2248 2/3 and V\n\u03c02\n\u03be \u2192 c2 \u2248 1/3\nalmost surely when following policies (\u03c01, \u03c02). Therefore there is an \u03b5 > 0 such that V \u03c01\n\u03be > \u03b5 and V \u03c02 \u03be > \u03b5 for all\ntime steps. Now we can apply [LH15a, Thm. 7] to conclude that there are (dogmatic) mixtures \u03be\u20321 and \u03be \u2032 2 such that \u03c0\n\u2217 \u03be\u2032 1\nalways follows policy \u03c01 and \u03c0\u2217\u03be\u2032 2 always follows policy \u03c02. This does not converge to a (\u03b5-)Nash equilibrium. \u2666\nA policy \u03c0 is asymptotically optimal in mean in an environment class M iff for all \u00b5 \u2208 M\nE\u03c0\u00b5 [ V \u2217\u00b5 (\u00e6<t)\u2212 V \u03c0 \u00b5 (\u00e6<t) ] \u2192 0 as t \u2192 \u221e (6)\nwhere E\u03c0\u00b5 denotes the expectation with respect to the probability distribution \u00b5\u03c0 over histories generated by policy \u03c0 acting in environment \u00b5.\nAsymptotic optimality stands out because it is currently the only known nontrivial objective notion of optimality in general reinforcement learning [LH15a].\nThe following theorem is the main convergence result. It states that for asymptotically optimal agents we get convergence to \u03b5-Nash equilibria in any reflective-oraclecomputable multi-agent environment.\nTheorem 28 (Convergence to Equilibrium). Let \u03c3 be an reflective-oracle-computable multi-agent environment and let \u03c01, . . . , \u03c0n be reflective-oracle-computable policies that are asymptotically optimal in mean in the class MOrefl. Then for all \u03b5 > 0 and all i \u2208 {1, . . . , n} the \u03c3\u03c01:n -probability that the policy \u03c0i is an \u03b5-best response converges to 1 as t \u2192 \u221e.\nIn contrast to Theorem 25 which yields policies that play a subgame perfect equilibrium, this is not the case for Theorem 28: the agents typically do not learn to predict off-policy and thus will generally not play \u03b5-best responses in the counterfactual histories that they never see. This weaker form of equilibrium is unavoidable if the agents do not know the environment because it is impossible to learn the parts that they do not interact with.\nTogether with Theorem 6 and the asymptotic optimality of the Thompson sampling policy [LLOH16, Thm. 4] that is reflective-oracle computable we get the following corollary.\nCorollary 29 (Convergence to Equilibrium). There are limit computable policies \u03c01, . . . , \u03c0n such that for any computable multi-agent environment \u03c3 and for all \u03b5 > 0 and all i \u2208 {1, . . . , n} the \u03c3\u03c01:n -probability that the policy \u03c0i is an \u03b5-best response converges to 1 as t \u2192 \u221e."}, {"heading": "5 DISCUSSION", "text": "This paper introduced the class of all reflective-oraclecomputable environments MOrefl. This class solves the grain of truth problem because it contains (any computable modification of) Bayesian agents defined over MOrefl: the optimal agents and Bayes-optimal agents over the class are all reflective-oracle-computable (Theorem 22 and Corollary 24).\nIf the environment is unknown, then a Bayesian agent may end up playing suboptimally (Example 27). However, if each agent uses a policy that is asymptotically optimal in mean (such as the Thompson sampling policy [LLOH16]) then for every \u03b5 > 0 the agents converge to an \u03b5-Nash equilibrium (Theorem 28 and Corollary 29).\nOur solution to the grain of truth problem is purely theoretical. However, Theorem 6 shows that our class MOrefl allows for computable approximations. This suggests that practical approaches can be derived from this result, and reflective oracles have already seen applications in one-shot games [FTC15b]."}, {"heading": "Acknowledgements", "text": "We thank Marcus Hutter and Tom Everitt for valuable comments."}, {"heading": "A LIST OF NOTATION", "text": ":= defined to be equal N the natural numbers, starting with 0 Q the rational numbers R the real numbers t (current) time step, t \u2208 N k, n, i time steps, natural numbers p a rational number X \u2217 the set of all finite strings over the alphabet X X\u221e the set of all infinite strings over the alphabet X X \u266f the set of all finite and infinite strings over the alphabet X O a reflective oracle O\u0303 a partial oracle q a query to a reflective oracle T the set of all probabilistic Turing machines that can query an oracle T, T \u2032 probabilistic Turing machines that can query an oracle, T, T \u2032 \u2208 T K(x) the Kolmogorov complexity of a string x \u03bbT the semimeasure corresponding to the probabilistic Turing machine T \u03bbOT the semimeasure corresponding to the\nprobabilistic Turing machine T with reflective oracle O\n\u03bb O\nT the completion of \u03bb O T into a measure using\nthe reflective oracle O A the finite set of possible actions O the finite set of possible observations E the finite set of possible percepts, E \u2282 O\u00d7\nR\n\u03b1, \u03b2 two different actions, \u03b1, \u03b2 \u2208 A at the action in time step t ot the observation in time step t rt the reward in time step t, bounded between 0 and 1 et the percept in time step t, we use et = (ot, rt) implicitly \u00e6<t the first t \u2212 1 interactions,\na1e1a2e2 . . . at\u22121et\u22121 (a history of length t\u2212 1)\n\u01eb the empty string/the history of length 0 \u03b5 a small positive real number \u03b3 the discount function \u03b3 : N \u2192 R\u22650\n\u0393t a discount normalization factor, \u0393t := \u2211\u221e k=t \u03b3k \u03bd, \u00b5 environments/semimeasures \u03c3 multi-agent environment \u03c3\u03c01:n history distribution induced by policies\n\u03c01, . . . , \u03c0n acting in the multi-agent environment \u03c3\n\u03c3i subjective environment of agent i \u03c0 a policy, \u03c0 : (A\u00d7 E)\u2217 \u2192 A \u03c0\u2217\u03bd an optimal policy for environment \u03bd V \u03c0\u03bd the \u03bd-expected value of the policy \u03c0 V \u2217\u03bd the optimal value in environment \u03bd M a countable class of environments MOrefl the class of all reflective-oraclecomputable environments w a universal prior, w \u2208 \u2206MOrefl \u03be the universal mixture over all environments MOrefl, a semimeasure \u03be the completion of \u03bbOT into a measure using\nthe reflective oracle O"}], "references": [{"title": "Reflective variants of Solomonoff induction and AIXI", "author": ["Benja Fallenstein", "Nate Soares", "Jessica Taylor"], "venue": "In Artificial General Intelligence. Springer,", "citeRegEx": "Fallenstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fallenstein et al\\.", "year": 2015}, {"title": "Reflective oracles: A foundation for classical game theory", "author": ["Benja Fallenstein", "Jessica Taylor", "Paul F Christiano"], "venue": "Technical report, Machine Intelligence Research Institute,", "citeRegEx": "Fallenstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fallenstein et al\\.", "year": 2015}, {"title": "Reflective oracles: A foundation for game theory in artificial intelligence", "author": ["Benja Fallenstein", "Jessica Taylor", "Paul F Christiano"], "venue": "In Logic, Rationality, and Interaction,", "citeRegEx": "Fallenstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fallenstein et al\\.", "year": 2015}, {"title": "On the impossibility of predicting the behavior of rational agents", "author": ["Dean P Foster", "H Peyton Young"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Foster and Young.,? \\Q2001\\E", "shortCiteRegEx": "Foster and Young.", "year": 2001}, {"title": "Open problems in universal induction & intelligence", "author": ["Marcus Hutter"], "venue": null, "citeRegEx": "Hutter.,? \\Q2009\\E", "shortCiteRegEx": "Hutter.", "year": 2009}, {"title": "Rational learning leads to Nash equilibrium", "author": ["Ehud Kalai", "Ehud Lehrer"], "venue": null, "citeRegEx": "Kalai and Lehrer.,? \\Q1993\\E", "shortCiteRegEx": "Kalai and Lehrer.", "year": 1993}, {"title": "Introduction to Metamathematics", "author": ["Stephen Cole Kleene"], "venue": "Wolters-Noordhoff Publishing,", "citeRegEx": "Kleene.,? \\Q1952\\E", "shortCiteRegEx": "Kleene.", "year": 1952}, {"title": "Nonparametric General Reinforcement Learning", "author": ["Jan Leike"], "venue": "PhD thesis, Australian National University,", "citeRegEx": "Leike.,? \\Q2016\\E", "shortCiteRegEx": "Leike.", "year": 2016}, {"title": "General time consistent discounting", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "Theoretical Computer Science,", "citeRegEx": "Lattimore and Hutter.,? \\Q2014\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2014}, {"title": "Bad universal priors and notions of optimality", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Conference on Learning Theory, pages 1244\u20131259,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "On the computability of AIXI", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "On the computability of Solomonoff induction and knowledge-seeking", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "Thompson sampling is asymptotically optimal in general environments", "author": ["Jan Leike", "Tor Lattimore", "Laurent Orseau", "Marcus Hutter"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Leike et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Leike et al\\.", "year": 2016}, {"title": "An Introduction to Kolmogorov Complexity and Its Applications", "author": ["Ming Li", "Paul M.B. Vit\u00e1nyi"], "venue": "Texts in Computer Science. Springer,", "citeRegEx": "Li and Vit\u00e1nyi.,? \\Q2008\\E", "shortCiteRegEx": "Li and Vit\u00e1nyi.", "year": 2008}, {"title": "Prediction, optimization, and learning", "author": ["John H Nachbar"], "venue": "in repeated games. Econometrica,", "citeRegEx": "Nachbar.,? \\Q1997\\E", "shortCiteRegEx": "Nachbar.", "year": 1997}, {"title": "Asymptotic non-learnability of universal agents with computable horizon functions", "author": ["Laurent Orseau"], "venue": "Theoretical Computer Science,", "citeRegEx": "Orseau.,? \\Q2013\\E", "shortCiteRegEx": "Orseau.", "year": 2013}, {"title": "Multiagent Systems: Algorithmic, GameTheoretic, and Logical Foundations", "author": ["Yoav Shoham", "Kevin Leyton-Brown"], "venue": null, "citeRegEx": "Shoham and Leyton.Brown.,? \\Q2009\\E", "shortCiteRegEx": "Shoham and Leyton.Brown.", "year": 2009}, {"title": "Complexity-based induction systems: Comparisons and convergence theorems", "author": ["Ray Solomonoff"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Solomonoff.,? \\Q1978\\E", "shortCiteRegEx": "Solomonoff.", "year": 1978}], "referenceMentions": [], "year": 2016, "abstractText": "A Bayesian agent acting in a multi-agent environment learns to predict the other agents\u2019 policies if its prior assigns positive probability to them (in other words, its prior contains a grain of truth). Finding a reasonably large class of policies that contains the Bayes-optimal policies with respect to this class is known as the grain of truth problem. Only small classes are known to have a grain of truth and the literature contains several related impossibility results. In this paper we present a formal and general solution to the full grain of truth problem: we construct a class of policies that contains all computable policies as well as Bayes-optimal policies for every lower semicomputable prior over the class. When the environment is unknown, Bayes-optimal agents may fail to act optimally even asymptotically. However, agents based on Thompson sampling converge to play \u03b5-Nash equilibria in arbitrary unknown computable multi-agent environments. While these results are purely theoretical, we show that they can be computationally approximated arbitrarily closely.", "creator": "LaTeX with hyperref package"}}}