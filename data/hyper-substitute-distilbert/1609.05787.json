{"id": "1609.05787", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2016", "title": "Context-aware Sequential Recommendation", "abstract": "when sequential information become an effective role driving accelerating user behaviors, various external recommendation methods have been popularized. methods based on earlier assumption for nowhere - used, research emphasis varies between immensely recent procedures. recently, virtual neural networks ( rnn ) based methods have been usually applied in larger financial modeling markets. however, through real - world analysis, these data have difficulty, evaluating the desired information, itself has been proved more be frequently important for behavior modeling. via this vein, we propose open novel model, named context - aware strategic problem networks ( ca - rnn ). instead of following the constant variable matrix hierarchical transition matrix in various rnn models, ca - rnn employs adaptive host - specific input matrices and delayed value - matched transition matrices. improved adaptive memory - restricted input matrices affect external situations where user behaviors happen, such it time, behavior, geographical conditions so (. and the computational counter - smart transition matrices capture intermediate jumps of time intervals over adjacent behaviors in historical sequences affect persistent communication linking global stationary objects. experimental settings bound below the proposed ca - matrix scheme yields significant improvements ; midnight - or - the - day sequential recommendation schemes and context - guided response methods on two public entities, i. e., the taobao cluster and the movielens - 1m network.", "histories": [["v1", "Mon, 19 Sep 2016 15:33:46 GMT  (629kb,D)", "http://arxiv.org/abs/1609.05787v1", "IEEE International Conference on Data Mining (ICDM) 2016, to apear"]], "COMMENTS": "IEEE International Conference on Data Mining (ICDM) 2016, to apear", "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["qiang liu", "shu wu", "diyi wang", "zhaokang li", "liang wang"], "accepted": false, "id": "1609.05787"}, "pdf": {"name": "1609.05787.pdf", "metadata": {"source": "CRF", "title": "Context-aware Sequential Recommendation", "authors": ["Qiang Liu", "Shu Wu", "Diyi Wang", "Zhaokang Li", "Liang Wang"], "emails": ["shu.wu}@nlpr.ia.ac.cn,", "wang.di@husky.neu.edu,", "zhaokang.li@rice.edu,", "wangliang@nlpr.ia.ac.cn"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nNowadays, people are overwhelmed by a huge amount of information. Recommender systems have been an important tool for users to filter information and locate their preference. Since historical behaviors in different time periods have different effects on users\u2019 behaviors, the importance of sequential information in recommender systems has been gradually recognized by researchers. Methods based on Markov assumption, including Factorizing Personalized Markov Chain (FPMC) [6] and Hierarchical Representation Model (HRM) [11], have been widely used for sequential prediction. However, a major problem of these methods is that these methods independently combine several most recent components. To resolve this deficiency, Recurrent Neural Networks (RNN) have been employed to model global sequential dependency among all possible components. It achieves the state-ofthe-art performance in different applications, e.g., sentence modeling [4], click prediction [14], location prediction [3], and next basket recommendation [13].\nWith enhanced ability in collecting information, a great amount of contextual information, such as location, time, weather and so on, can be collected by systems. Besides, the contextual information has been proved to be useful in determining users\u2019 preferences in recommender systems [1]. In real scenarios, as shown in Figure 1, applications usually contain not only sequential information but also a large amount of contextual information. Though RNN and other sequential recommendation methods have achieved satisfactory performances in sequential prediction, they still have difficulty in modeling rich contextual information in real scenarios. On the other hand, context-aware recommendation has been extensively studied, and several methods have been proposed recently, such as Factorization Machine (FM) [7], Tensor Factorization for MAP maximization (TFMAP) model [10], CARS2 [9] and Contextual Operating Tensor (COT) [2][12]. But these context-aware recommender methods can not take sequential information into consideration.\nTo construct a model to capture the sequential information and contextual information simultaneously, we first investigate the properties of sequential behavioral histories in real scenarios. Here, we conclude two types of contexts, i.e., input contexts and transition contexts. Input contexts denote external situations under which input elements occur in behavioral sequences, that is to say, input contexts are external contexts under which users conduct behaviors. Such contexts usually include location (home or working place), time (weekdays or weekends, morning or evening), weather (sunny or rainy), etc. Transition contexts are contexts of transitions between two adjacent input elements in historical sequences. Specifically, transition contexts denote time intervals between adjacent behaviors. It captures context-adaptive transition effects from past behaviors to future behaviors with different time intervals. Usually, shorter time intervals have more significant effects comparing with longer ones.\nIn this work, we propose a novel model, named ContextAware Recurrent Neural Networks (CA-RNN), to model sequential information and contextual information in one framework. Each layer of conventional RNN contains an input element and recurrent transition from the previous status, which are captured by an input matrix and a transition matrix respectively. Different from conventional RNN using\nar X\niv :1\n60 9.\n05 78\n7v 1\n[ cs\n.I R\n] 1\n9 Se\np 20\n16\na constant input matrix, CA-RNN employs adaptive contextspecific input matrices for input elements under various input contexts. Similarly, instead of using a constant transition matrix, CA-RNN utilizes adaptive context-specific transition matrices for modeling varied transition effects from previous input elements under specific transition contexts. Then, we incorporate Bayesian Personalized Ranking (BPR) [5] and Back Propagation Through Time (BPTT) [8] for the learning of CA-RNN. In summary, the main contributions of this work are listed as follows: \u2022 We address the problem of context-aware sequential\nrecommendation, which includes two types of contexts, i.e., input contexts and transition contexts. \u2022 We employ adaptive context-specific input matrices to model input contexts. These contexts denote external situations under which users conduct behaviors. \u2022 We incorporate adaptive context-specific transition matrices for modeling transition contexts, i.e., time intervals between adjacent behaviors in historical sequences. \u2022 Experiments conducted on two real-world datasets show that CA-RNN is effective and outperforms stateof-the-art methods significantly."}, {"heading": "II. PROPOSED MODEL", "text": "In this section, we introduce the proposed Context-Aware Recurrent Neural Networks (CA-RNN). We first present the problem definition and the conventional RNN model. Then we detail our proposed model. Finally, we introduce the parameter learning process of the CA-RNN model."}, {"heading": "A. Problem Definition", "text": "We have a set of users denoted as U = {u1, u2, ...}, and a set of items denoted as V = {v1, v2, ...}. For each user u, the behavioral history is given as V u = {vu1 , vu2 , ...}, where\nvuk denotes the k-th selected item of user u. Each selected item in the behavior history is associated with corresponding timestamps Tu = {tu1 , tu2 , ...}, where tuk denotes the kth timestamp in the behavioral sequence of user u. For each user u, at specific timestamp tuk , the input context is denoted as cuI,k, such as weather, location and so on. And the corresponding transition context is denoted as cuT,k, which is determined by the time interval between the timestamp tuk of the current behavior and the timestamp t u k\u22121 of the previous behavior. In this paper, given the behavioral history V u = {vu1 , ..., vuk , ...} of user u with input and transition contexts, we would like to predict the next selected item vuk+1 of user u under input context cuI,k+1 and transition context c u T,k+1."}, {"heading": "B. Recurrent Neural Networks", "text": "The architecture of a recurrent neural networks is a recurrent structure with multiple hidden layers. At each time step, we can predict the output unit given the hidden layer, and then feed the new output back into the next hidden status. The formulation of the hidden layer in RNN is:\nhuk = f ( rukM + h u k\u22121W ) , (1)\nwhere huk is the d dimensional hidden status of user u at the timestamp tuk in the behavior sequence, and r u k denotes the d dimensional latent vector of the corresponding item vuk . W is the d\u00d7 d dimensional transition matrix, which works as the recurrent connection to propagate sequential signals from the previous status. M denotes the d\u00d7 d dimensional input matrix, which captures the current behavior of the user on input elements. The activation function f(x) is usually chosen as a sigmoid function f(x) = exp (1/1 + e\u2212x).\nThough RNN has achieved successful performances in sequential modeling, it has difficulty in modeling a variety\nof contextual information. With the increasing of contextual information in practical applications, context-aware sequential recommendation becomes an emerging task. As a consequence, it is appropriate to incorporate significant contextual information in sequences into an adapted RNN architecture."}, {"heading": "C. Modeling Input Contexts", "text": "Input contexts denote the external situations under which users conduct behaviors. This type of contexts in the behavioral history is significant for behavior prediction. For instance, if a man usually takes exercise in the morning, we can predict he will go to the gym in the morning rather than in the evening, even if there are lots of people going to gyms in the evening. Thus, we plan to incorporate input contexts in the conventional RNN model. The constant input matrix in conventional RNN is replaced by adaptive context-specific input matrices according to different input contexts. Then, Equation 1 can be rewritten as:\nhuk = f ( rukMcuI,k + h u k\u22121W ) , (2)\nwhere McuI,k denotes the d\u00d7d dimensional context-specific input matrix of input context cuI,k. It incorporates external contexts into items so as to help appropriately shape interests of users under specific contexts."}, {"heading": "D. Modeling Transition Contexts", "text": "Transition contexts denote lengths of time intervals between adjacent behaviors in historical sequences. Different lengths of time intervals between two behaviors have different impacts on prediction. Generally speaking, longer time intervals usually have weaker effect in predicting next behavior than shorter ones. Therefore, modeling lengths of time interval is essential for predicting future behaviors. Thus, we treat time intervals as transition contexts, and\nincorporate transition contexts into our model. The constant transition matrix in conventional RNN is replaced with adaptive context-specific transition matrices according to different transition contexts. Then Equation 2 can be rewritten as:\nhuk = f ( rukMcuI,k + h u k\u22121WcuT,k ) , (3)\nwhere WcuT,k is a d\u00d7d dimensional adaptive context-specific transition matrix for the transition context cuT,k. It captures how transition contexts between adjacent behaviors affect the transition of global sequential features. The adaptive context-specific transition matrix can be generated according to the length of time interval between the current timestamp and the previous timestamp.\nHowever, it is impossible to learn a context-specific transition matrix for every possible continuous time interval value independently. So we partition the range of all possible time intervals into discrete time bins, and all time intervals are discretized to the floor of the corresponding time bin. Thus, context-specific transition matrices can be generated as:\nWcuT,k = Wbtuk\u2212tuk\u22121c, (4)\nwhere tuk and t u k\u22121 denotes the current and the previous timestamp respectively, tuk \u2212 tuk\u22121 is the length of the corresponding time interval, and \u230a tuk \u2212 tuk\u22121 \u230b denotes the floor of the length of corresponding time interval tuk \u2212 tuk\u22121. Then, as shown in Figure 2, the formulation of each hidden layer in the CA-RNN model becomes:\nhuk = f ( rukMcuI,k + h u k\u22121Wbtuk\u2212tuk\u22121c ) . (5)"}, {"heading": "E. Context-aware Prediction", "text": "When making prediction of user behaviors at timestamp tuk+1, despite historical behavioral contexts modeled above,\ncurrent contexts cuI,k+1 and c u T,k+1 also have significant effects. Thus, our method should incorporate current contexts to make context-aware prediction. The prediction function, i.e., whether user u will select item v at timestamp tuk+1 under contexts cuI,k+1 and c u T,k+1, can be written as:\nyu,k+1,v = h u kW \u2032 cuT,k+1 ( rvM \u2032 cuI,k+1 )T , (6)\nwhere M\u2032cuI,k+1 and W \u2032 cuT,k+1 denote d \u00d7 d dimensional matrix representations of the current input context cuI,k+1 and transition context cuT,k+1 respectively. W \u2032 cuT,k+1\ncan be generated according to the calculation in Equation 4."}, {"heading": "F. Parameter Learning", "text": "In this subsection, we introduce the learning process of CA-RNN with BPR [5] and BPTT [8]. These methods have been successfully used for parameter estimation of RNN based recommendation models [3][13].\nBPR is a widely-used pairwise ranking framework for the implicit feedback data. The basic assumption of BPR is that a user prefers selected items than negative ones. Under the BPR framework, at each sequential position k, the objective of CA-RNN is to maximize the following probability:\np(u, k, v v\u2032) = g(yu,k,v \u2212 yu,k,v\u2032), (7)\nwhere v\u2032 denotes a negative item sample and g(x) is a nonlinear function which can be defined as g(x) = 1/1 + e\u2212x. Incorporating the negative log likelihood, we can solve the following objective function equivalently:\nJ = \u2211 u,k ln(1 + e\u2212(yu,k,v\u2212yu,k,v\u2032 )) + \u03bb 2 \u2016\u0398\u20162 , (8)\nwhere \u0398 = {R,M,W} denotes all the parameters to be estimated, and \u03bb is a parameter to control the power of regularization. According to Equation 6, derivations of J with respect to the parameters can be calculated. Moreover, parameters in CA-RNN can be further learnt by using the BPTT algorithm. According to Equation 3, given the derivation \u2202J/\u2202huk , corresponding gradients of all parameters in the hidden layer can be calculated, which is repeated iteratively in the whole sequence. And stochastic gradient descent can be employed to estimate all the parameters."}, {"heading": "III. EXPERIMENTS", "text": "In this section, we conduct empirical experiments to demonstrate the effectiveness of CA-RNN on context-aware sequential recommendation. First, we introduce our experimental settings. Then we conduct experiments on comparing input contexts and transition contexts. We also compare our proposed CA-RNN model with some state-of-the-art sequential and context-aware recommendation methods. Finally, we study the impact of dimensionality in CA-RNN."}, {"heading": "A. Experimental Settings", "text": "Our experiments are conducted on two real datasets with rich contextual information. The Taobao dataset1 is a public online e-commerce dataset collected from taobao.com2. It contains 13,611,038 shopping records belonging to 1,103,702 users and 462,008 items. The Movielens-1M dataset3 is a dataset for recommender systems. It contains 1,000,209 rating records of 3,900 movies by 6,040 users. Records in both datasets are associated with timestamps.\nMoreover, for each behavioral sequence in the two datasets, we use first 80% elements in the sequence for training, the remaining 20% data for testing. And the regulation parameter in our experiments is set as \u03bb = 0.01. To avoid data sparsity, on both datasets, we remove users with less than 10 records and items with less than 3 records.\nAccording to the contextual information in the two datasets, we extract input contexts and transition contexts to implement our proposed CA-RNN model, as well as other context-aware methods. First, based on timestamps, we can extract different kinds of input contexts on the two datasets. On the Taobao dataset, we extract three kinds of input contexts: seven days in a week, three ten-day time periods in a month, and holiday or not. So, there are totally 42 input context values in the Taobao dataset. On the Movielens-1M dataset, we extract two kinds of input contexts: seven days in a week, and twenty-four hours in a day. So, there are totally 168 input context values in the Movielens-1M dataset. Second, we can extract time intervals between adjacent behaviors in sequences as transition contexts. Time intervals in both datsets are discretized to one-day time bins. To avoid data sparsity, for time intervals larger than 30 days, we treat them as one time bin. So, there are totally 32 transition context values in both datasets.\nMoreover, we evaluate performances of each method through several widely-used evaluation metrics for ranking tasks: Recall@k, F1-score@k, Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG). For Recall@k and F1-score@k, we report results with k = 1, 5 and 10 in our experiments. For all the metrics, the larger the value, the better the performance.\nIn our experiments, to investigate the effectiveness of CA-RNN from various angles, there are several aspects of compared methods: baseline methods, context-aware methods, sequential methods and our proposed methods. Baseline methods contain POP and BPR [5]. Context-aware methods consist of FM [7] and CARS2 [9]. And sequential methods include FPMC [6], HRM [11] and RNN [13]. The length of each transaction in FPMC and HRM is set to be one day and one week on the Taobao and Movielens-1M datasets respectively. Moreover, to compare the effects of input\n1https://tianchi.shuju.aliyun.com/competition/index.htm 2https://www.taobao.com/ 3http://files.grouplens.org/datasets/movielens/ml-1m.zip\ncontexts and transition contexts, we implement not only the CA-RNN model4, but also CA-RNN-input and CA-RNNtransition, with only input or transition contexts.\nB. Input Contexts VS. Transition Contexts\nTo compare input contexts and transition contexts, we illustrate performances of CA-RNN-input, CA-RNNtransition and CA-RNN in Table I. On the Taobao dataset, CA-RNN-input outperforms CA-RNN-transition. While on the Movielens-1M dataset, CA-RNN-transition outperforms CA-RNN-input. These results indicate that the contextual information in input contexts and transition contexts has distinct effects on predicting future behaviors, and their significance differs in different situations. Thus, it is necessary to incorporate both types of contexts to better predict the future. Incorporating both input contexts and transition contexts, CA-RNN achieves the best performance among our proposed methods, and can significantly outperform CARNN-input and CA-RNN-transition."}, {"heading": "C. Performance Comparison", "text": "To evaluate the effectiveness of our proposed CA-RNN, we illustrate the performance comparison of CA-RNN and\n4Code: http://qiangliucasia.github.io/homepage/files/CARNNcode.zip\nother compared methods in Figure II. Comparing with baseline performances of POP and BPR, context-aware methods FM and CARS2 achieve significant improvements. And sequential methods FPMC, HRM and RNN can further outperform FM and CARS2. This indicates that, comparing with contextual information, sequential information has more significant effects. And RNN achieves the best performance among all the compared methods. Moreover, we can observe that, our proposed CA-RNN greatly outperforms compared methods on both datasets in terms of all the metrics. Comparing with RNN, CA-RNN relatively improves Recall@1, @5, @10, MAP and NDCG by 91.8%, 59.7%, 50.8%, 60.4% and 31.5% on the Taobao dataset. And on the Movielens-1M datasets, the relative improvements become 19.1%, 23.9%, 13.9%, 13.8% and 9.5%. These improvements indicate the superiority of our method brought by modeling input and transition contexts.\nD. Impact of Dimensionality\nTo study the impact of dimensionality in our methods, we illustrate performances of RNN-based methods with varying d in Figure 3. On the Taobao dataset, the best performances of CA-RNN, CA-RNN-transition and CA-RNN-input are achieved at d = 20, d = 20 and d = 30 respectively.\nOn Movielens-1M, performances of our methods are stable along with the dimensionality, and best performances are achieved at d = 10. On both dataset, our methods can stably outperform conventional RNN with a significant advantage in all situations. Moreover, even not with the best dimensionality, CA-RNN can still outperform conventional RNN."}, {"heading": "IV. CONCLUSIONS", "text": "In this paper, we address the problem of context-aware sequential recommendation and propose a novel method, i.e., context-aware recurrent neural networks. In CA-RNN, the constant input matrix of conventional RNN is replaced with context-specific input matrices to modeling complex realworld contexts, e.g., time, location and weather. Meanwhile, to model transition contexts, i.e., time intervals between adjacent behaviors in historical sequences, context-specific transition matrices are incorporated, instead of the constant one in conventional RNN. The experimental results on two real datasets show that CA-RNN outperforms competitive sequential and context-aware models."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is jointly supported by National Basic Research Program of China (2012CB316300), and National Natural Science Foundation of China (61403390, U1435221, 61420106015, 61525306)."}], "references": [{"title": "Context-aware recommender systems", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "In Recommender systems handbook,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Cot: Contextual operating tensor for context-aware recommender systems", "author": ["Q. Liu", "S. Wu", "L. Wang"], "venue": "In AAAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Predicting the next location: A recurrent model with spatial and temporal contexts", "author": ["Q. Liu", "S. Wu", "L. Wang", "T. Tan"], "venue": "In AAAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt- Thieme"], "venue": "In UAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Factorizing personalized markov chains for next-basket recommendation", "author": ["S. Rendle", "C. Freudenthaler", "L. Schmidt-Thieme"], "venue": "In WWW,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Fast context-aware recommendations with factorization machines", "author": ["S. Rendle", "Z. Gantner", "C. Freudenthaler", "L. Schmidt- Thieme"], "venue": "In SIGIR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1988}, {"title": "Cars2: Learning context-aware representations for context-aware recommendations", "author": ["Y. Shi", "A. Karatzoglou", "L. Baltrunas", "M. Larson", "A. Hanjalic"], "venue": "In CIKM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Tfmap: Optimizing map for top-n contextaware recommendation", "author": ["Y. Shi", "A. Karatzoglou", "L. Baltrunas", "M. Larson", "A. Hanjalic", "N. Oliver"], "venue": "In SIGIR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Learning hierarchical representation model for next basket recommendation", "author": ["P. Wang", "J. Guo", "Y. Lan", "J. Xu", "S. Wan", "X. Cheng"], "venue": "In SIGIR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Contextual operation for recommender systems", "author": ["S. Wu", "Q. Liu", "L. Wang", "T. Tan"], "venue": "In TKDE,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "A dynamic recurrent model for next basket recommendation", "author": ["F. Yu", "Q. Liu", "S. Wu", "L. Wang", "T. Tan"], "venue": "In SIGIR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Sequential click prediction for sponsored search with recurrent neural networks", "author": ["Y. Zhang", "H. Dai", "C. Xu", "J. Feng", "T. Wang", "J. Bian", "B. Wang", "T.-Y. Liu"], "venue": "In AAAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "Methods based on Markov assumption, including Factorizing Personalized Markov Chain (FPMC) [6] and Hierarchical Representation Model (HRM) [11], have been widely used for sequential prediction.", "startOffset": 91, "endOffset": 94}, {"referenceID": 10, "context": "Methods based on Markov assumption, including Factorizing Personalized Markov Chain (FPMC) [6] and Hierarchical Representation Model (HRM) [11], have been widely used for sequential prediction.", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": ", sentence modeling [4], click prediction [14], location prediction [3], and next basket recommendation [13].", "startOffset": 20, "endOffset": 23}, {"referenceID": 13, "context": ", sentence modeling [4], click prediction [14], location prediction [3], and next basket recommendation [13].", "startOffset": 42, "endOffset": 46}, {"referenceID": 2, "context": ", sentence modeling [4], click prediction [14], location prediction [3], and next basket recommendation [13].", "startOffset": 68, "endOffset": 71}, {"referenceID": 12, "context": ", sentence modeling [4], click prediction [14], location prediction [3], and next basket recommendation [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "Besides, the contextual information has been proved to be useful in determining users\u2019 preferences in recommender systems [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 6, "context": "On the other hand, context-aware recommendation has been extensively studied, and several methods have been proposed recently, such as Factorization Machine (FM) [7], Tensor Factorization for MAP maximization (TFMAP) model [10], CARS2 [9] and Contextual Operating Tensor (COT) [2][12].", "startOffset": 162, "endOffset": 165}, {"referenceID": 9, "context": "On the other hand, context-aware recommendation has been extensively studied, and several methods have been proposed recently, such as Factorization Machine (FM) [7], Tensor Factorization for MAP maximization (TFMAP) model [10], CARS2 [9] and Contextual Operating Tensor (COT) [2][12].", "startOffset": 223, "endOffset": 227}, {"referenceID": 8, "context": "On the other hand, context-aware recommendation has been extensively studied, and several methods have been proposed recently, such as Factorization Machine (FM) [7], Tensor Factorization for MAP maximization (TFMAP) model [10], CARS2 [9] and Contextual Operating Tensor (COT) [2][12].", "startOffset": 235, "endOffset": 238}, {"referenceID": 1, "context": "On the other hand, context-aware recommendation has been extensively studied, and several methods have been proposed recently, such as Factorization Machine (FM) [7], Tensor Factorization for MAP maximization (TFMAP) model [10], CARS2 [9] and Contextual Operating Tensor (COT) [2][12].", "startOffset": 277, "endOffset": 280}, {"referenceID": 11, "context": "On the other hand, context-aware recommendation has been extensively studied, and several methods have been proposed recently, such as Factorization Machine (FM) [7], Tensor Factorization for MAP maximization (TFMAP) model [10], CARS2 [9] and Contextual Operating Tensor (COT) [2][12].", "startOffset": 280, "endOffset": 284}, {"referenceID": 4, "context": "Then, we incorporate Bayesian Personalized Ranking (BPR) [5] and Back Propagation Through Time (BPTT) [8] for the learning of CA-RNN.", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "Then, we incorporate Bayesian Personalized Ranking (BPR) [5] and Back Propagation Through Time (BPTT) [8] for the learning of CA-RNN.", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "In this subsection, we introduce the learning process of CA-RNN with BPR [5] and BPTT [8].", "startOffset": 73, "endOffset": 76}, {"referenceID": 7, "context": "In this subsection, we introduce the learning process of CA-RNN with BPR [5] and BPTT [8].", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "These methods have been successfully used for parameter estimation of RNN based recommendation models [3][13].", "startOffset": 102, "endOffset": 105}, {"referenceID": 12, "context": "These methods have been successfully used for parameter estimation of RNN based recommendation models [3][13].", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "Baseline methods contain POP and BPR [5].", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "Context-aware methods consist of FM [7] and CARS2 [9].", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "Context-aware methods consist of FM [7] and CARS2 [9].", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "And sequential methods include FPMC [6], HRM [11] and RNN [13].", "startOffset": 36, "endOffset": 39}, {"referenceID": 10, "context": "And sequential methods include FPMC [6], HRM [11] and RNN [13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 12, "context": "And sequential methods include FPMC [6], HRM [11] and RNN [13].", "startOffset": 58, "endOffset": 62}], "year": 2016, "abstractText": "Since sequential information plays an important role in modeling user behaviors, various sequential recommendation methods have been proposed. Methods based on Markov assumption are widely-used, but independently combine several most recent components. Recently, Recurrent Neural Networks (RNN) based methods have been successfully applied in several sequential modeling tasks. However, for realworld applications, these methods have difficulty in modeling the contextual information, which has been proved to be very important for behavior modeling. In this paper, we propose a novel model, named Context-Aware Recurrent Neural Networks (CA-RNN). Instead of using the constant input matrix and transition matrix in conventional RNN models, CA-RNN employs adaptive context-specific input matrices and adaptive context-specific transition matrices. The adaptive contextspecific input matrices capture external situations where user behaviors happen, such as time, location, weather and so on. And the adaptive context-specific transition matrices capture how lengths of time intervals between adjacent behaviors in historical sequences affect the transition of global sequential features. Experimental results show that the proposed CARNN model yields significant improvements over state-of-theart sequential recommendation methods and context-aware recommendation methods on two public datasets, i.e., the Taobao dataset and the Movielens-1M dataset.", "creator": "LaTeX with hyperref package"}}}