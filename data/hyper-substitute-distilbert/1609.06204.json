{"id": "1609.06204", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "Italy goes to Stanford: a collection of CoreNLP modules for Italian", "abstract": "in this we paper present tint, an audio - to - accept set generating fast, intelligent and modular natural language processing modules teaching italian. it means used on stanford corenlp and is freely supplied like dedicated standalone software hence a complete interface can occur organized in an existing project.", "histories": [["v1", "Tue, 20 Sep 2016 14:53:05 GMT  (19kb)", "http://arxiv.org/abs/1609.06204v1", null], ["v2", "Thu, 13 Apr 2017 08:33:33 GMT  (19kb)", "http://arxiv.org/abs/1609.06204v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alessio palmero aprosio", "giovanni moretti"], "accepted": false, "id": "1609.06204"}, "pdf": {"name": "1609.06204.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Giovanni Moretti"], "emails": ["aprosio@fbk.eu", "moretti@fbk.eu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n06 20\n4v 1\n[ cs\n.C L\n] 2\n0 Se\np 20\n16\nItaliano. In questo articolo presentiamo Tint, una collezione di moduli semplici, veloci e personalizzabili per l\u2019analisi di testi in Italiano. Tint e\u0300 basato su Stanford CoreNLP e puo\u0300 essere scaricato gratuitamente come software stand-alone o come libreria da integrare in progetti esistenti."}, {"heading": "1 Introduction", "text": "In recent years, the Natural Language Processing (NLP) technologies have become a fundamental basis for complex tasks, such as Question Answering, Event Identification and Topic Classification. While most of the NLP tools freely available on the web (such as Stanford CoreNLP1 and OpenNLP2) are designed for English and sometimes adapted to other languages, there is a lack of this kind of resources for Italian.\nIn this paper, we present Tint, a suite of readyto-use modules for NLP that is:\nNew. Tint is the first completely free and open source tool for NLP in Italian.\nSimple. Tint can be downloaded and used out-ofthe-box (see Section 5). In addition, it relies on Stanford CoreNLP Java interface, therefore it can be included easily into an existing project.\n1http://stanfordnlp.github.io/CoreNLP/ 2 https://opennlp.apache.org/\nModular. Tint can be extended using the CoreNLP Java interfaces. At the same time, existing modules can be replaced with more customized ones.\nEfficient. In its default configuration, Tint is faster than most of its competitors (see Section 4).\nAccurate. Most of Tint modules have a state-ofthe-art accuracy (see Section 4).\nFree. Tint is released as open source software under GNU GPL."}, {"heading": "2 Architecture", "text": "The Tint pipeline is based on Stanford CoreNLP (Manning et al., 2014), an open-source framework written in Java, that provide most of the commons NLP tasks out-of-the-box in various language. The framework provides also an easy interface to extend the annotation to new tasks and/or languages. Differently from some similar tools, such as UIMA (Ferrucci and Lally, 2004) and GATE (Cunningham et al., 2002), CoreNLP is easy to use and does not require it to be learnt: a basic object-oriented programming skill is enough. In Tint, we use this framework to both port the most common NLP tasks to Italian and add some new annotators for external tools, such as entity linking, temporal expression identification, keyword extraction."}, {"heading": "3 Modules", "text": ""}, {"heading": "3.1 Tokenizer", "text": "This module provides text segmentation in tokens and sentences. At first, the text is grossly tokenized; in a second step, tokens that need to be put together are merged using two customizable lists of Italian non-breaking abbreviations (such as \u201cdott.\u201d or \u201cS.p.A.\u201d) and regular expressions (for email addresses, web URIs, numbers, dates)."}, {"heading": "3.2 Morphological Analyzer", "text": "The morphological analyzer module provides the full list of morphological features for each annotated token. The current version of this module has been trained with the Morph-it lexicon (Zanchetta and Baroni, 2005), but it\u2019s possible to extend or retrain it with other Italian datasets. In order to grant fast performance, the model storage has been implemented with the mapDB Java library3 that provides an excellent variation of the Cassandra\u2019s Sorted String Table. To extend the coverage of the results, especially for the complex forms, such as \u201cporta-ce-ne\u201d, \u201cportar-glie-lo\u201d or \u201cbi-direzionale\u201d, the module tries to decompose the token into prefix-root-infix-suffix and attempts to resolve the root form."}, {"heading": "3.3 Part-of-speech tagger", "text": "The part-of-speech annotation is provided through the Maximum Entropy implementation (Toutanova et al., 2003) included in Stanford CoreNLP. The model is trained on the Universal Dependencies4 (UD) dataset for Italian (Bosco et al., 2013), a dataset \u2013 freely available for research purpose \u2013 containing more than 300K tokens annotated with lemma, part-of-speech and syntactic dependencies. As an alternative, a wrapper annotator that uses TreeTagger is also available in Tint."}, {"heading": "3.4 Lemmatizer", "text": "The module for the lemmatization is a rule-based system that works by combining the Part-ofSpeech output and the results of the Morphological Analyzer so to disambiguate the morphological features using the grammatical annotation. In order to increase the accuracy of the results, the module tries to detect the genre of noun lemmas relying to the analysis of their processed articles. For instance, for the correct lemmatization of \u201cil latte/the milk\u201d, the module uses the singular article \u201cil\u201d to identify the correct gender/number of the lemma \u201clatte\u201d and returns \u201clatte/milk\u201d (male, singular) instead of \u201clatta/metal sheet\u201d (female, which plural form is \u201clatte\u201d).\n3http://www.mapdb.org 4 http://universaldependencies.org/"}, {"heading": "3.5 Named Entity Recognition and Classification", "text": "The NER module recognize persons, locations and organizations in the text. It uses a CRF sequence tagger (Finkel et al., 2005) included in Stanford CoreNLP and it is trained on the ICAB (Magnini et al., 2006), a dataset containing 180K words taken from the Italian newspaper \u201cL\u2019Adige\u201d."}, {"heading": "3.6 Dependency Parsing", "text": "This module provides syntactic analysis of the text and uses a transition-based parser (included in Stanford CoreNLP) which produces typed dependency parses of natural language sentences (Chen and Manning, 2014). The parser is powered by a neural network which accepts word embedding inputs: the model is trained on the UD dataset (see Section 3.3) and the word embeddings are built on the Paisa\u0300 corpus (Lyding et al., 2014), that contains 250M tokens of freely available and distributable texts harvested from the web."}, {"heading": "3.7 Entity Linking", "text": "The entity linking task consists in disambiguating a word (or a set of words) and link them to a knowledge base (KB). The biggest (and most used) available KB is Wikipedia, and almost every linking tool relies on it. The Tint pipeline provides a wrapper annotator that can connect to DBpedia Spotlight5 (Daiber et al., 2013) and The Wiki Machine6 (Giuliano et al., 2009). Both tools are distributed as open source software and can be used by the annotator both as external services or through a local installation."}, {"heading": "3.8 Temporal Expression Extraction and Normalization", "text": "The task of temporal expression extraction is included in Tint as a wrapper to HeidelTime (Stro\u0308tgen and Gertz, 2013), a rule-based state-ofthe-art temporal tagger developed at Heidelberg University. HeidelTime also normalizes the expressions according to the TIMEX3 annotation standard. The software is released under the GPL license, therefore it can be used both for educational and commercial purposes.\n5http://bit.ly/dbpspotlight 6 http://bit.ly/thewikimachine"}, {"heading": "3.9 Keyword extraction", "text": "Keyword extraction in Tint is performed by Keyphrase Digger (Moretti et al., 2015), a rulebased system for keyphrase extraction. It combines statistical measures with linguistic information given by part-of-speech patterns to identify and extract weighted keyphrases from texts. The CoreNLP annotator for Keyphrase Digger is included in the Tint pipeline, but the main software must be downloaded and installed from the official website7 as it is not released open source."}, {"heading": "4 Evaluation", "text": "Tint includes a rich set of tools, evaluated separately. In some cases, an evaluation based on the accuracy is not possible, because of the lack of available gold standard or because the tool outcome is not comparable to other tools\u2019 ones.\nWhen possible, Tint is compared with existing pipelines that work with the Italian language: Tanl (Attardi et al., 2010), TextPro (Pianta et al., 2008) and TreeTagger (Schmid, 1994).\nIn calculating speed, we run each experiment 10 times and consider the average execution time. When available, multi-thread capabilities have been disabled. All experiments have been executed on a 2,3 GHz Intel Core i7 with 16 GB of memory.\nThe Tanl API is not available as a downloadable package, but it\u2019s only usable online through a REST API, therefore the speed may be influenced by the network connection. In addition, the Tanl API does not provide offsets for the annotated text, nor it allows a text to be uploaded already tokenized and divided in sentences, therefore an automatic alignment was needed. The tools used for this alignment are distributed as part of the Tint software.\nNo evaluation is performed for the Tint annotators that act as wrappers for an external tools (temporal expression tagging, entity linking, keyword extraction)."}, {"heading": "4.1 Tokenization and sentence splitting", "text": "For the task of tokenization and sentence splitting, Tint outperforms in speed both TextPro and Tanl (see Table 1). The number of tokens per second can be further increased by tuning the features (for example, by deactivating the regular expressions that recognize e-mail or web addresses).\n7 http://dh.fbk.eu/technologies/kd"}, {"heading": "4.2 Part-of-speech tagging", "text": "The evaluation of the part-of-speech tagging is performed against the test set included in the UD dataset, containing 10K tokens. As the tagset used is different for different tools, the accuracy is calculated only on five coarse-grained types: nouns (N), verbs (V), adverbs (B), adjectives (A) and other (O). For each tool, the corresponding tagset is converted to this tagset and accuracy is calculated dividing the number of times the tagger gets the right answer by the total number of tags in the dataset. Table 2 shows the results."}, {"heading": "4.3 Lemmatization", "text": "Like part-of-speech tagging, lemmatization is evaluated, both in terms of accuracy and execution time, on the UD test set. When the lemma is guessed starting form a morphological analysis (such as in Tint and TextPro), the speed is calculated by including both tasks. Table 3 shows the results. All the tools reach the same accuracy of 96% (with minor differences that are not statistically significant)."}, {"heading": "4.4 Named Entities Recognition", "text": "For Named Entity Recognition, we evaluate and compare our system with the test set available on the I-CAB dataset. We consider three classes:\n8The (considerable) speed of TreeTagger includes both lemmatization and part-of-speech tagging.\nPER, ORG, LOC. Both Tanl and TextPro deal also with the GPE class, but we merged it to LOC, as it has been done during the training of Tint. We needed to retrain the EntityPro module of TextPro from scratch (with three classes), as the original model already contains the ICAB test set, therefore it would overfit the results. In training Tint, we add some gazette of names, to help the classifier to recognize entities that are not present in the training set. In particular, we extracted a list of persons, locations and organizations by querying the Airpedia database (Palmero Aprosio et al., 2013) for Wikipedia pages classified as Person, Place and Organisation, respectively. The whole data used for training the NER is available for download from the Tint website. Table 4 shows the results of the named entity recognition task."}, {"heading": "4.5 Dependency parsing", "text": "The evaluation of the dependency parser is performed against Tanl and TextPro w.r.t the usual metrics Labeled Attachment Score (LAS) and Unlabeled Attachment Score (UAS). While Tint is trained on the UD dataset, the parsers included in Tanl (Attardi et al., 2013) and TextPro (Lavelli, 2013) use part of the Turin University Treebank (TUT) (Bosco et al., 2000), as released for the Evalita 2011 parsing task (Magnini et al., 2013). For this reason, the comparison between the two system is not completely fair: on the one hand, the TUT dataset is smaller than the UD; on the other hand, the UD is an automatic combination of two different treebanks, that have been annotated using different guidelines (Bosco et al., 2013). Table 5 shows the results: the Tint evaluation has been performed on the UD test data; LAS and UAS for TextPro and Tanl is taken directly from the Evalita 2011 proceedings."}, {"heading": "5 The tool", "text": "The Tint pipeline is released as an open source software under the GNU General Public License (GPL), version 3. It can be download from the\nTint website9 as a standalone package, or it can be integrated into an existing application with Maven.\nThe tool is written using the Stanford CoreNLP paradigm, therefore a third part software can be integrated easily into the pipeline. Tint accepts plain text or Newsreader Annotation Format (NAF) (Fokkens et al., 2014) as input, and CoNLL, JSON, or NAF as output."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper we presented Tint, a simple, fast and accurate NLP pipeline for Italian, based on Stanford CoreNLP. Currently, we offer out-of-the-box NLP annotation for part-of-speech, lemma, named entities, links to Wikipedia, dependency parsing, time expression identification and keyword extraction; additional custom modules can be added and replaced easily by implementing the CoreNLP Java interfaces.\nIn the future, we plan to better tune the various modules that rely on machine learning (such as dependency parsing, part-of-speech tagging and named entity recognition), that in this preliminary version of Tint have been trained without any linguistic optimization.\nWe are currently working on new modules, in particular Word Sense Disambiguation (WSD) w.r.t. linguistic resources such as MultiWordNet (Pianta et al., 2002) and Semantic Role Labelling, by porting to Italian resources such as Framenet (Baker et al., 1998), now available in English.\nOn the technical side, we are updating some modules to work multi-thread. The Tint pipeline will also be integrated into PIKES (Corcoglioniti et al., 2016), a tool that extracts knowledge from texts using NLP annotation and outputs it in a queryable form (such RDF triples)."}, {"heading": "Acknowledgments", "text": "The research leading to this paper was partially supported by the European Union\u2019s Horizon 2020 Programme via the SIMPATICO Project (H2020EURO-6-2015, n. 692819).\n9 http://tint.fbk.eu/"}], "references": [{"title": "The Tanl Pipeline", "author": ["Attardi et al.2010] G. Attardi", "S. Dei Rossi", "M. Simi"], "venue": "In Proc. of LREC Workshop on WSPP", "citeRegEx": "Attardi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Attardi et al\\.", "year": 2010}, {"title": "Tuning desr for dependency parsing of italian", "author": ["Maria Simi", "Andrea Zanelli"], "venue": "In Evaluation of Natural Language and Speech Tools for Italian,", "citeRegEx": "Attardi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Attardi et al\\.", "year": 2013}, {"title": "The berkeley framenet project", "author": ["Baker et al.1998] Collin F Baker", "Charles J Fillmore", "John B Lowe"], "venue": "In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Compu-", "citeRegEx": "Baker et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Building a treebank for italian: a data-driven annotation", "author": ["Bosco et al.2000] Cristina Bosco", "Vincenzo Lombardo", "Daniela Vassallo", "Leonardo Lesmo"], "venue": null, "citeRegEx": "Bosco et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bosco et al\\.", "year": 2000}, {"title": "Converting italian treebanks: Towards an italian stanford dependency treebank", "author": ["Bosco et al.2013] Cristina Bosco", "Simonetta Montemagni", "Maria Simi"], "venue": null, "citeRegEx": "Bosco et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bosco et al\\.", "year": 2013}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A 2-phase frame-based knowledge extraction framework", "author": ["Marco Rospocher", "Alessio Palmero Aprosio"], "venue": "In Proc. of ACM Symposium on Applied Computing (SAC\u201916)", "citeRegEx": "Corcoglioniti et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Corcoglioniti et al\\.", "year": 2016}, {"title": "Gate: An architecture for development of robust hlt applications", "author": ["Diana Maynard", "Kalina Bontcheva", "Valentin Tablan"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Lin-", "citeRegEx": "Cunningham et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Cunningham et al\\.", "year": 2002}, {"title": "Improving efficiency and accuracy in multilingual entity extraction", "author": ["Max Jakob", "Chris Hokamp", "Pablo N. Mendes"], "venue": "In Proceedings of the 9th International Conference on Semantic Systems (I-Semantics)", "citeRegEx": "Daiber et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Daiber et al\\.", "year": 2013}, {"title": "Uima: An architectural approach to unstructured information processing in the corporate research environment", "author": ["Ferrucci", "Lally2004] David Ferrucci", "Adam Lally"], "venue": null, "citeRegEx": "Ferrucci et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2004}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "author": ["Trond Grenager", "Christopher Manning"], "venue": null, "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Naf and gaf: Linking linguistic annotations", "author": ["Aitor Soroa", "Zuhaitz Beloki", "Niels Ockeloen", "German Rigau", "Willem Robert van Hage", "Piek Vossen"], "venue": "In Proceedings 10th Joint ISO-ACL SIGSEM Workshop", "citeRegEx": "Fokkens et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fokkens et al\\.", "year": 2014}, {"title": "Kernel methods for minimally supervised wsd", "author": ["Alfio Massimiliano Gliozzo", "Carlo Strapparava"], "venue": "Comput. Linguist.,", "citeRegEx": "Giuliano et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Giuliano et al\\.", "year": 2009}, {"title": "An ensemble model for the evalita 2011 dependency parsing task", "author": ["Alberto Lavelli"], "venue": "In Evaluation of Natural Language and Speech Tools for Italian,", "citeRegEx": "Lavelli.,? \\Q2013\\E", "shortCiteRegEx": "Lavelli.", "year": 2013}, {"title": "The paisa corpus of italian web texts", "author": ["Lyding et al.2014] Verena Lyding", "Egon Stemle", "Claudia Borghetti", "Marco Brunello", "Sara Castagnoli", "Felice Dell\u2019Orletta", "Henrik Dittmann", "Alessandro Lenci", "Vito Pirrelli"], "venue": "In Proceedings of the 9th Web", "citeRegEx": "Lyding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lyding et al\\.", "year": 2014}, {"title": "I-cab: the italian content annotation bank", "author": ["Emanuele Pianta", "Christian Girardi", "Matteo Negri", "Lorenza Romano", "Manuela Speranza", "Valentina Bartalesi Lenzi", "Rachele Sprugnoli"], "venue": "Proceedings of LREC,", "citeRegEx": "Magnini et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Magnini et al\\.", "year": 2006}, {"title": "Evaluation of Natural Language and Speech Tool for Italian: International Workshop, EVALITA", "author": ["Francesco Cutugno", "Mauro Falcone", "Emanuele Pianta"], "venue": null, "citeRegEx": "Magnini et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Magnini et al\\.", "year": 2013}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky"], "venue": "In ACL (System Demonstrations),", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Digging in the dirt: Extracting keyphrases from texts with kd", "author": ["Rachele Sprugnoli", "Sara Tonelli"], "venue": "CLiC it,", "citeRegEx": "Moretti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moretti et al\\.", "year": 2015}, {"title": "Automatic expansion of DBpedia exploiting Wikipedia cross-language information", "author": ["Claudio Giuliano", "Alberto Lavelli"], "venue": "In Proceedings of the 10th Extended Semantic Web Conference", "citeRegEx": "Aprosio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Aprosio et al\\.", "year": 2013}, {"title": "Developing an aligned multilingual database", "author": ["Luisa Bentivogli", "Christian Girardi"], "venue": "In Proc. 1st Int\u2019l Conference on Global WordNet. Citeseer", "citeRegEx": "Pianta et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pianta et al\\.", "year": 2002}, {"title": "The textpro tool suite", "author": ["Christian Girardi", "Roberto Zanoli"], "venue": "In LREC. Citeseer", "citeRegEx": "Pianta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pianta et al\\.", "year": 2008}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["Helmut Schmid"], "venue": null, "citeRegEx": "Schmid.,? \\Q1994\\E", "shortCiteRegEx": "Schmid.", "year": 1994}, {"title": "Multilingual and cross-domain temporal tagging", "author": ["Str\u00f6tgen", "Gertz2013] Jannik Str\u00f6tgen", "Michael Gertz"], "venue": "Language Resources and Evaluation,", "citeRegEx": "Str\u00f6tgen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Str\u00f6tgen et al\\.", "year": 2013}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Morph-it! a free corpus-based morphological resource for the italian language", "author": ["Zanchetta", "Baroni2005] Eros Zanchetta", "Marco Baroni"], "venue": "Corpus Linguistics", "citeRegEx": "Zanchetta et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zanchetta et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 17, "context": "The Tint pipeline is based on Stanford CoreNLP (Manning et al., 2014), an open-source framework written in Java, that provide most of the commons NLP tasks out-of-the-box in various language.", "startOffset": 47, "endOffset": 69}, {"referenceID": 7, "context": "Differently from some similar tools, such as UIMA (Ferrucci and Lally, 2004) and GATE (Cunningham et al., 2002), CoreNLP is easy to use and does not require it to be learnt: a basic object-oriented programming skill is enough.", "startOffset": 86, "endOffset": 111}, {"referenceID": 24, "context": "The part-of-speech annotation is provided through the Maximum Entropy implementation (Toutanova et al., 2003) included in Stanford CoreNLP.", "startOffset": 85, "endOffset": 109}, {"referenceID": 4, "context": "The model is trained on the Universal Dependencies4 (UD) dataset for Italian (Bosco et al., 2013), a dataset \u2013 freely available for research purpose \u2013 containing more than 300K tokens annotated with lemma, part-of-speech and syntactic dependencies.", "startOffset": 77, "endOffset": 97}, {"referenceID": 10, "context": "It uses a CRF sequence tagger (Finkel et al., 2005) included in Stanford CoreNLP and it is trained on the ICAB (Magnini et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 15, "context": ", 2005) included in Stanford CoreNLP and it is trained on the ICAB (Magnini et al., 2006), a dataset containing 180K words taken from the Italian newspaper \u201cL\u2019Adige\u201d.", "startOffset": 67, "endOffset": 89}, {"referenceID": 14, "context": "3) and the word embeddings are built on the Pais\u00e0 corpus (Lyding et al., 2014), that contains 250M tokens of freely available and distributable texts harvested from the web.", "startOffset": 57, "endOffset": 78}, {"referenceID": 8, "context": "The Tint pipeline provides a wrapper annotator that can connect to DBpedia Spotlight5 (Daiber et al., 2013) and The Wiki Machine6 (Giuliano et al.", "startOffset": 86, "endOffset": 107}, {"referenceID": 12, "context": ", 2013) and The Wiki Machine6 (Giuliano et al., 2009).", "startOffset": 30, "endOffset": 53}, {"referenceID": 18, "context": "Keyword extraction in Tint is performed by Keyphrase Digger (Moretti et al., 2015), a rulebased system for keyphrase extraction.", "startOffset": 60, "endOffset": 82}, {"referenceID": 0, "context": "When possible, Tint is compared with existing pipelines that work with the Italian language: Tanl (Attardi et al., 2010), TextPro (Pianta et al.", "startOffset": 98, "endOffset": 120}, {"referenceID": 21, "context": ", 2010), TextPro (Pianta et al., 2008) and TreeTagger (Schmid, 1994).", "startOffset": 17, "endOffset": 38}, {"referenceID": 22, "context": ", 2008) and TreeTagger (Schmid, 1994).", "startOffset": 23, "endOffset": 37}, {"referenceID": 1, "context": "While Tint is trained on the UD dataset, the parsers included in Tanl (Attardi et al., 2013) and TextPro (Lavelli, 2013) use part of the Turin University Treebank (TUT) (Bosco et al.", "startOffset": 70, "endOffset": 92}, {"referenceID": 13, "context": ", 2013) and TextPro (Lavelli, 2013) use part of the Turin University Treebank (TUT) (Bosco et al.", "startOffset": 20, "endOffset": 35}, {"referenceID": 3, "context": ", 2013) and TextPro (Lavelli, 2013) use part of the Turin University Treebank (TUT) (Bosco et al., 2000), as released for the Evalita 2011 parsing task (Magnini et al.", "startOffset": 84, "endOffset": 104}, {"referenceID": 16, "context": ", 2000), as released for the Evalita 2011 parsing task (Magnini et al., 2013).", "startOffset": 55, "endOffset": 77}, {"referenceID": 4, "context": "For this reason, the comparison between the two system is not completely fair: on the one hand, the TUT dataset is smaller than the UD; on the other hand, the UD is an automatic combination of two different treebanks, that have been annotated using different guidelines (Bosco et al., 2013).", "startOffset": 270, "endOffset": 290}, {"referenceID": 11, "context": "Tint accepts plain text or Newsreader Annotation Format (NAF) (Fokkens et al., 2014) as input, and CoNLL, JSON, or NAF as output.", "startOffset": 62, "endOffset": 84}, {"referenceID": 20, "context": "linguistic resources such as MultiWordNet (Pianta et al., 2002) and Semantic Role Labelling, by porting to Italian resources such as Framenet (Baker et al.", "startOffset": 42, "endOffset": 63}, {"referenceID": 2, "context": ", 2002) and Semantic Role Labelling, by porting to Italian resources such as Framenet (Baker et al., 1998), now available in English.", "startOffset": 86, "endOffset": 106}, {"referenceID": 6, "context": "The Tint pipeline will also be integrated into PIKES (Corcoglioniti et al., 2016), a tool that extracts knowledge from texts using NLP annotation and outputs it in a queryable form (such RDF triples).", "startOffset": 53, "endOffset": 81}], "year": 2017, "abstractText": "English. In this we paper present Tint, an easy-to-use set of fast, accurate and extendable Natural Language Processing modules for Italian. It is based on Stanford CoreNLP and is freely available as a standalone software or a library that can be integrated in an existing project. Italiano. In questo articolo presentiamo Tint, una collezione di moduli semplici, veloci e personalizzabili per l\u2019analisi di testi in Italiano. Tint \u00e8 basato su Stanford CoreNLP e pu\u00f2 essere scaricato gratuitamente come software stand-alone o come libreria da integrare in progetti esistenti.", "creator": "LaTeX with hyperref package"}}}