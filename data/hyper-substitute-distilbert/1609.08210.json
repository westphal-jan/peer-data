{"id": "1609.08210", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Learning to Translate for Multilingual Question Answering", "abstract": "in multilingual translation answering, thus the term needs to be translated into the initial language, or vice freely. in addition to transcription, there are transmission methods, follow the inquiry, four paths must commonly follow in major regard : word - choice, spoken - best, syllable - linked, and grammar - based. people build inference feature for standardized variant of translation direction and method, and evaluate a model that promises optimal verb generation. on a large forum dataset indicative of posts of hindi, arabic, and chinese, our selective learn - to - translate approach been considered effective than a cognitive baseline ( p & dl ; 14. 05 ) : blending all voices into italics, then training a feature present only on english ( partial yet binary ) text.", "histories": [["v1", "Mon, 26 Sep 2016 22:12:50 GMT  (359kb,D)", "http://arxiv.org/abs/1609.08210v1", "12 pages. To appear in EMNLP'16"]], "COMMENTS": "12 pages. To appear in EMNLP'16", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["ferhan t\u00fcre", "elizabeth boschee"], "accepted": true, "id": "1609.08210"}, "pdf": {"name": "1609.08210.pdf", "metadata": {"source": "CRF", "title": "Learning to Translate for Multilingual Question Answering", "authors": ["Ferhan Ture", "Elizabeth Boschee"], "emails": ["ture@cable.comcast.com", "eboschee@bbn.com"], "sections": [{"heading": "1 Introduction", "text": "Question answering (QA) is a specific form of the information retrieval (IR) task, where the goal is to find relevant well-formed answers to a posed question. Most QA pipelines consist of three main stages: (a) preprocessing the question and collection, (b) retrieval of candidate answers in the collection, and (c) ranking answers with respect to their relevance to the question and return the top N answers. The types of questions can range from factoid (e.g., \u201cWhat is the capital of France?\u201d) to causal (e.g., \u201cWhy are trees green?\u201d), and opinion questions (e.g., \u201cShould USA lower the drinking age?\u201d).\nThe most common approach to multilingual QA (MLQA) has been to translate all content into its\n\u2217This work was completed while author was an employee of Raytheon BBN Technologies.\nmost probable English translation via machine translation (MT) systems. This strong baseline, which we refer to as one-best MT (1MT), has been successful in prior work (Hartrumpf et al., 2009; Lin and Kuo, 2010; Shima and Mitamura, 2010). However, recent advances in cross-lingual IR (CLIR) show that one can do better by representing the translation space as a probability distribution (Ture and Lin, 2014). In addition, MT systems perform substantially worse with user-generated text, such as web forums (Van der Wees et al., 2015), which provide extra motivation to consider alternative translation approaches for higher recall. To our knowledge, it has yet to be shown whether these recent advancements in CLIR transfer to MLQA.\nWe introduce a novel answer ranking approach for MLQA (i.e., Learning to Translate or L2T), a model that learns the optimal translation of question and/or candidate answer, based on how well it discriminates between good and bad answers. We achieve this by introducing a set of features that encapsulate lexical and semantic similarities between a question and a candidate answer through various translation strategies (Section 3.1). The model then learns feature weights for each combination of translation direction and method, through a discriminative training process (Section 3.2). Once a model is trained, it can be used for MLQA, by sorting each candidate answer in the collection by model score. Instead of learning a single model to score candidate answers in any language, it might be meaningful to train a separate model that can learn to discriminate between good and bad answers in each language. This can let each model learn feature weights custom to ar X iv :1\n60 9.\n08 21\n0v 1\n[ cs\n.C L\n] 2\n6 Se\np 20\nthe language, therefore allowing a more fine-grained ranking (Section 3.4). We call this alternative approach Learning to Custom Translate (L2CT).\nExperiments on the DARPA Broad Operational Language Technologies (BOLT) IR task1 confirm that L2T yields statistically significant improvements over a strong baseline (p < 0.05), in three out of four experiments. L2CT outperformed the baseline as well, but was not more effective than L2T."}, {"heading": "2 Related Work", "text": "For the last decade or so, research in QA has mostly been driven by annual evaluation campaigns like TREC,2 CLEF,3 and NTCIR.4 Most earlier work relied on either rule-based approaches where a set of rules were manually crafted for each type of question, or IR-like approaches where each pair of question and candidate answer was scored using retrieval functions (e.g., BM25 (Robertson et al., 2004)). On the other hand, training a classifier for ranking candidate answers allows the exploitation of various features extracted from the question, candidate answer, and surrounding context (Madnani et al., 2007; Zhang et al., 2007). In fact, an explicit comparison at 2007 TREC confirmed the superiority of machine learning-based (ML-based) approaches (F-measure 35.9% vs 38.7%) (Zhang et al., 2007). Learning-torank approaches have also been applied to QA successfully (Agarwal et al., 2012).\nPrevious ML-based approaches have introduced useful features from many aspects of natural language, including lexical (Brill et al., 2001; Attardi et al., 2001), syntactic (Alfonseca et al., 2001; Katz et al., 2005), semantic (Cui et al., 2005; Katz et al., 2005; Alfonseca et al., 2001; Hovy et al., 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al., 2005; Harabagiu and Bejan, 2005), which are especially useful for \u201cwhy\u201d and \u201chow\u201d questions (Kolomiyets and Moens, 2011). Additionally, semantic role labeling and dependency trees are other forms of semantic analysis used widely in NLP applications (Shen and Lapata, 2007; Cui et al., 2005).\n1http://www.darpa.mil/Our_Work/I2O/Programs 2http://trec.nist.gov 3http://www.clef-initiative.eu 4http://research.nii.ac.jp/ntcir/index.html\nWhen dealing with multilingual collections, most prior approaches translate all text into English beforehand, then treat the task as monolingual retrieval (previously referred to as 1MT). At recent evaluation campaigns like CLEF and NTCIR,5 almost all teams simply obtained the one-best question translation, treating some online MT system as a black box (Adafre and van Genabith, 2009; Hartrumpf et al., 2009; Martinez-Gonzalez et al., 2009; Lin and Kuo, 2010; Shima and Mitamura, 2010), with few notable exceptions that took term importance (Ren et al., 2010), or semantics (Munoz-Terol et al., 2009) into account. Even for non-factoid MLQA, most prior work does not focus on the translation component (Luo et al., 2013; Chaturvedi et al., 2014).\nContributions. Ture and Lin described three methods for translating queries into the collection language in a probabilistic manner, improving document retrieval effectiveness over a one-best translation approach (2014). Extending this idea to MLQA appears as a logical next step, yet most prior work relies solely on the one-best translation of questions or answers (Ko et al., 2010b; Garc\u0131\u0301a-Cumbreras et al., 2012; Chaturvedi et al., 2014), or selects the best translation out of few options (Sacaleanu et al., 2008; Mitamura et al., 2006). Mehdad et al. reported improvements by including the top ten translations (instead of the single best) and computing a distance-based entailment score with each (2010). While Espla-Gomis et al. argue that using MT as a black box is more convenient (and modular) (2012), there are potential benefits from a closer integration between statistical MT and multilingual retrieval (Nie, 2010). To the best of our knowledge, there is no prior work in the literature, where the optimal query and/or answer translation is learned via machine learning. This is our main contribution, with which we outperform the state of the art.\nIn addition to learning the optimal translation, we learn the optimal subset of the training data for a given task, where the criteria of whether we include a certain data instance is based on either the source language of the sentence, or the language in which the sentence was annotated. Training data selection strategies have not been studied extensively in the\n5Most recent MLQA tracks were in 2008 (CLEF) and 2010 (NTCIR).\nQA literature, therefore the effectiveness of our simple language-related criteria can provide useful insights to the community.\nWhen there are multiple independent approaches for ranking question-answer pairs, it is required to perform a post-retrieval merge: each approach generates a ranked list of answers, which are then merged into a final ranked list. This type of system combination approach has been applied to various settings in QA research. Merging at the document-level is common in IR systems (e.g., (Tsai et al., 2008)), and has shown to improve multilingual QA performance as well (Garc\u0131\u0301a-Cumbreras et al., 2012). Many QA systems combine answers obtained by different variants of the underlying model (e.g., (Brill et al., 2001) for monolingual, (Ko et al., 2010a; Ko et al., 2010b) for multilingual QA). We are not aware, however, of any prior work that has explored the merging of answers that are generated by language-specific ranking models. Although this does not show increased effectiveness in our experiments, we believe that it brings a new perspective to the problem."}, {"heading": "3 Approach", "text": "Our work is focused on a specific stage of the QA pipeline, namely answer ranking: Given a natural-language question q and k candidate answers s1, . . . , sk, we score each answer in terms of its relevance to q. In our case, candidate answers are sentences extracted from all documents retrieved in the previous stage of the pipeline (using Indri (Metzler and Croft, 2005)). Hereafter, sentence and answer might be used interchangeably.\nWhile our approach is not language-specific, we assume (for simplicity) that questions are in English, whereas sentences are in either English, Arabic, or Chinese. Non-English answers are translated back into English before returning to user.\nOur approach is not limited to any question type, factoid or non-factoid. Our main motivation is to provide good QA quality on any multilingual Web collection. This entails finding answers to questions where there is no single answer, and for which human agreement is low. We aim to build a system that can successfully retrieve relevant information from open-domain and informal-language content.\nIn this scenario, two assumptions made by many of the prior approaches fail:\n1) We can accurately classify questions via template patterns (Chaturvedi et al. argue that this does not hold for non-factoid questions (2014))\n2) We can accurately determine the relevance of an answer, based on its automatic translation into English (Wees et al. show how recall decreases when translating user-generated text (2015))\nTo avoid these assumptions, we opted for a more adaptable approach, in which question-answer relevance is modeled as a function of features, intended to capture the relationship between the question and sentence text. Also, instead of relying solely on a single potentially incorrect English translation, we increase our chances of a hit by translating both the question and the candidate answer, using four different translation methods. Our main features, described throughout this section, are based on lexical similarity computed using these translations. The classifier is trained on a large number of questionanswer pairs, each labeled by a human annotator with a binary relevance label.6"}, {"heading": "3.1 Representation", "text": "In MLQA, since questions and answers are in different languages, most approaches translate both into an intermediary language (usually English). Due to the error-prone nature of MT, valuable information often gets \u201clost in translation\u201d. These errors are especially noticeable when translating informal text or less-studied languages (Van der Wees et al., 2015).\nTranslation Direction. We perform a two-way translation to better retain the original meaning: in addition to translating each non-English sentence into English, we also translate the English questions into Arabic and Chinese (using multiple translation methods, described below). For each question-answer pair, we have two \u201cviews\u201d: comparing translated question to the original sentence (i.e., collection-language (CL) view); and comparing original question to the translated sentence (i.e., question-language (QL) view).\nTranslation Method. When translating text for retrieval tasks like QA, including a variety of alterna-\n6Annotators score each answer from 1 to 5. We label any score of 3 or higher as relevant.\ntive translations is as important as finding the most accurate translation, especially for non-factoid questions, where capturing (potentially multiple) underlying topics is essential. Recent work in crosslanguage IR (CLIR) has shown that incorporating probabilities from the internal representations of an MT system to \u201ctranslate\u201d the question can accomplish this, outperforming standard one-best translation (Ture and Lin, 2014). We hypothesize that these improvements transfer to multilingual QA as well.\nIn addition to translation directions, we explored four translation methods for converting the English question into a probabilistic representation (in Arabic and Chinese). Each method builds a probability distribution for every question word, expressing the translation space in the collection language. More details of first three methods can be found in (Ture and Lin, 2014), while fourth method is a novel query translation method adapted from the neural network translation model described in (Devlin et al., 2014). Word: In MT, a word alignment is a many-tomany mapping between source- and target-language words, learned without supervision, at the beginning of the training pipeline (Och, 2003). These alignments can be converted into word translation probabilities for CLIR (Darwish and Oard, 2003). For example, in an English-Arabic parallel corpus, if an English word appears m times in total and is aligned to a certain Arabic word k times, we assign a probability of km for this translation. This simple idea has performed greatly in IR for generating a probability distribution for query word translations. Grammar: Probabilities are derived from a synchronous context-free grammar, which is a typical translation model found in MT systems (Ture and Lin, 2014). The grammar contains rules r that follow the form \u03b1|\u03b2|A|`(r), stating that sourcelanguage word \u03b1 can be translated into targetlanguage word \u03b2 with an associated likelihood value `(r) (A represents word alignments). For each rule r that applies to the question, we identify each source word sj . From the word alignment information included in the rule, we can find all target words that sj is aligned to. By processing all the rules to accumulate likelihood values, we construct translation probabilities for each word in the question. 10-best: Statistical MT systems retrieve a ranked list of translations, not a single best. Ture and Lin ex-\nploited this to obtain word translation probabilities from the top 10 translations of the question (2014). For each question word w, we can extract which grammar rules were used to produce the translation \u2013 once we have the rules, word alignments allow us to find all target-language words that w translates into. By doing this for each question translation, we construct a probability distribution that defines the translation space of each question word. Context: Neural network-based MT models learn context-dependent word translation probabilities \u2013 the probability of a target word is dependent on the source word it aligns to, as well as a 5-word window of context (Devlin et al., 2014). For example, if the Spanish word \u201cplacer\u201d is aligned to the English word \u201cpleasure\u201d, the model will not only learn from this word-to-word alignment but also consider the source sentence context (e.g., \u201cFue un placer conocerte y tenerte unos meses.\u201d). However, since short questions might lack full sentence context, our model should have the flexibility to translate under partial or no context. Instead of training the NN-base translation model on full, well-formed sentences, we custom-fit it for question translation: words in the context window are randomly masked by replacing it with a special filler token <F>. This teaches the model how to accurately translate with full, partial context, or no context. For the above example, we generate partial contexts such as \u201cfue un placer <F> y\u201d or \u201c<F> <F> placer conocerte y\u201d. Since there are many possibilities, if the context window is large, we randomly sample a few of the possibilities (e.g., 4 out of 9) per training word.\nIn Figure 1, we display the probabilistic structure produced the grammar-based translation method, when implemented as described above. Each English word in the question is translated into a probabilistic structure, consisting of Chinese words and corresponding probabilities that represent how much weight the method decides to put on that specific word. Similar structures are learned with the other three translation methods.\nWe are not aware of any other MLQA approach that represents the question-answer pair based on their probabilistic translation space."}, {"heading": "3.2 Features", "text": "Given two different translation directions (CL and QL), and four different translation methods (Word, Grammar, 10-best, Context), our strategy is to leverage a machine learning process to determine how helpful each signal is with respect to the end task. For this, we introduced separate question-answer similarity features based on each combination of translation direction and method. Collection-language Features. In order to compute a single real-valued vector to represent the question in the collection language (LexCL), we start with the probabilistic structure representing the question translation (e.g., Figure 1 is one such structure when the translation method is grammar-based). For each word in the collection-language vocabulary, we compute a weight by averaging its probability across the terms in the probabilistic structure.\nvqgrammar(w) = avgiPr(w|qi) (1)\nwhere w is a non-Engish word and Pr(w|qi) is the probability of w in the probability distribution corresponding to the ith query term.\nFigure 2 shows the real-valued vector computed based on the probabilistic question translation in Figure 1. The Chinese word translated as \u201cchild labor\u201d has a weight of 0.32, 0.36, and 0 in the probability distributions of the three query terms, respectively. Averaging these three values results in the final weight of 0.23 in vqgrammar in Figure 2. Notice that these weights are normalized by construction.\nSimilarly, a candidate answer s in Chinese is represented by normalized word frequencies:\nvs(w) = freq(w|s)\u2211 w\u2032 freq(w\u2032|s)\n(2)\nGiven the two vectors, we compute the cosine similarity. Same process is repeated for the\nother three translation methods. The four lexical collection-language similarity features are collectively called LexCL.\nQuestion-language Features. As mentioned before, we also obtain a similarity value by translating the sentence (s1best) and computing the cosine similarity with the original question (q). vq and vs1best are computed using Equation 2. Although it is possible to translate the sentence into English using the same four methods, we only used the one-best translation due to the computational cost. Hence, we have only one lexical similarity feature in the QL view (call LexQL).\nThe computation process for the five lexical similarity features is summarized in Table 1. After computation, feature weights are learned via a maximum-entropy model.7 Although not included in the figure or table, we also include the same set of features from the sentence preceding the answer (within the corresponding forum post), in order to represent the larger discourse."}, {"heading": "3.3 Data Selection", "text": "In order to train a machine learning model with our novel features, we need positive and negative examples of question-answer pairs (i.e., (q, s)). For this, for each training question, our approach is to hire\n7Support vector machines yielded worse results.\nhuman annotators to label sentences retrieved from the non-English collections used in our evaluation. It is possible to label the sentences in the source language (i.e., Arabic or Chinese) or in the question language (i.e., translated into English). In this section, we explore the question of whether it is useful to distinguish between these two independently created labels, and whether this redundancy can be used to improve the machine learning process.\nWe hypothesize two reasons why selecting training data based on language might benefit MLQA: i) The translation of non-English candidate answers might lack in quality, so annotators are likely to judge some relevant answers as non-relevant. Hence, training a classifier on this data might lead to a tendency to favor English answers. ii) For the question-answer pairs that were annotated in both languages, we can remove noisy (i.e., labeled inconsistently by annotators) instances from the training set.\nThe question of annotation is an unavoidable part of evaluation of MLQA systems, so finding the optimal subset for training is a relevant problem. In order to explore further, we generated six subsets with respect to (a) the original language of the answer, or (b) the language of annotation (i.e., based on original text or its English translation): en: Sentences from the English corpus. ar/ch: Sentences from the Arabic / Chinese corpus (regardless of how it was judged). consist: All sentences except those that were judged inconsistently. src+: Sentences judged only in original text, or judged in both consistently. en+: Sentences that are either judged only in English, or judged in both original and English translation consistently. all: All sentences.\nThese subsets were determined based on linguistically motivated heuristics, but choosing the most suitable one (for a given task) is done via machine learning (see Section 4)."}, {"heading": "3.4 Language-specific Ranking", "text": "Scoring Arabic sentences with respect to a question is inherently different than scoring English (or Chinese) sentences. The quality of resources, grammar, etc., as well as other internal dynamics might differ\ngreatly across languages. We hypothesize that there is no one-size-fits-all model, so the parameters that work best for English retrieval might not be as useful when scoring sentences in Arabic, and/or Chinese.\nOur proposed solution is to apply a separate classifier, custom-tuned to each collection, and retrieve three single-language ranked lists (i.e., in English, Arabic, and Chinese). In addition to comparing each custom-tuned, language-specific classifier to a single, language-independent one, we also use this idea to propose an approach for MLQA: L2CT(n) Retrieve answers from each language using separate classifiers (call these lists English-only, Arabic-only, and Chinese-only), take the best answers from each language, then merge them into a mixed-language set of n answers.\nWe compare this to the standard approach: L2T(n) Retrieve up to n mixed-language answers using a single classifier.\nFour heuristics were explored for merging lists in the L2CT approach.8 Two common approaches are uniform and alternate merging (Savoy, 2004): Uniform: A straightforward merge can be achieved by using the classifier scores (i.e., probability of answer relevance, given question) to sort all answers, across all languages, and include the top n in the final list of answers. Classifier scores are normalized into the [0,1] range for comparability. Alternate: We alternate between the lists, picking one answer at a time from each, stopping when the limit n has been reached.\nSince answers are expected in English, there is a natural preference for answers that were originally written English, avoiding noisy text due to translation errors. However, it is also important not to restrict answers entirely to English sources, since that would defeat the purpose of searching in a multilingual collection. We implemented the following methods to account for language preferences: English first: We keep all sufficiently-confident (i.e., normalized score above a fixed threshold) answers from the English-only list first, and start including answers from Arabic- and Chinese-only lists only if the limit of n answers has not been reached.\n8In addition to these heuristics, the optimal merge could be learned from training data, as a \u201clearning to rank\u201d problem. This is out of the scope of this paper, but we plan to explore the idea in the future.\nWeighted: Similar to Uniform, but we weight the normalized scores before sorting. The optimal weights can be learned by using a grid-search procedure and a cross-validation split."}, {"heading": "4 Evaluation", "text": "In order to perform controlled experiments and gain more insights, we split our evaluation into four separate tasks: three tasks focus on retrieving answers from posts written in a specified language (English-only, Arabic-only, or Chinese-only) 9, and the last task is not restricted to any language (Mixedlanguage). All experiments were conducted on the DARPA BOLT-IR task. The collection consists of 12.6M Arabic, 7.5M Chinese, and 9.6M English Web forum posts. All runs use a set of 45 nonfactoid (mostly opinion and causal) English questions, from a range of topics. All questions and forum posts were processed with an information extraction (IE) toolkit (Boschee et al., 2005), which performs sentence-splitting, named entity recognition, coreference resolution, parsing, and part-ofspeech tagging.\nAll non-English posts were translated into English (one-best only), and all questions were translated into Arabic and Chinese (probabilistic translation methods from Section 3.1). For all experiments, we used the same state-of-the-art English\u2194Arabic (En-Ar) and English\u2194Chinese (En-Ch) MT systems (Devlin et al., 2014). Models were trained on parallel corpora from NIST OpenMT 2012, in addition to parallel forum data collected as part of the BOLT program (10M En-Ar words; 30M EnCh words). Word alignments were learned with GIZA++ (Och and Ney, 2003) (five iterations of IBM Models 1\u20134 and HMM).\nAfter all preprocessing, features were computed using the original post and question text, and their translations. Training data were created by having annotators label all sentences of the top 200 documents retrieved by Indri from each collection (for each question). Due to the nature of retrieval tasks, training data usually contains an unbalanced portion of negative examples. Hence, we split the data into balanced subsets (each sharing the same set of positively labeled data) and train multiple classifiers,\n9Shortened as Eng, Arz, and Cmn, respectively.\nthen take a majority vote when predicting. For testing, we froze the set of candidate answers and applied the trained classifier to each questionanswer pair, generating a ranked list of answers for each question. This ranked list was evaluated by average precision (AP).10 Due to the size and redundancy of the collections, we sometimes end up with over 1000 known relevant answers for a question. So it is neither reasonable nor meaningful to compute AP until we reach 100% recall (e.g., 11-point AP) for these cases. Instead, we computed AP-k, by accumulating precision values at every relevant answer until we get k relevant answers.11 In order to provide a single metric for the test set, it is common to report the mean average precision (MAP), which in this case is the average of the AP-k values across all questions.\nBaseline. As described earlier, the baseline system computes similarity between question text and the one-best translation of the candidate answer (we run the sentence through our state-of-the-art MT system). After translation, we compute similarity via scoring the match between the parse of the question text and the parse of the candidate answer, using our finely-tuned IE toolkit [reference removed for anonymization]. This results in three different similarity features: matching the tree node similarity, edge similarity, and full tree similarity. Feature weights are then learned by training this classifier discriminatively on the training data described above. This already performs competitively, outperforming the simpler baseline where we compute a single similarity score between question and translated text, and matching the performance of the system by Chaturvedi et al. on the BOLT evaluation (2014). Baseline MAP values are reported on the leftmost column of Table 2.\nData effect. In the baseline approach, we do not perform any data selection, and use all available data for training the classifier. In order to test our hypothesis that selecting a linguistically-motivated subset of the training data might help, we used 10- fold cross-validation to choose the optimal data set\n10Many other metrics (e.g., NDCG, R-precision) were explored during BOLT, and results were very similar.\n11k was fixed to 20 in our evaluation, although we verified that conclusions do not change with varying k.\n(among seven options described in Section 3.3). Results indicate that including English or Arabic sentences when training a classifier for Chinese-only QA is a bad idea, since effectiveness increases when restricted to Chinese sentences (lang=ch). On the other hand, for the remaining three tasks, the most effective training data set is annot=en+consist. These selections are consistent across all ten folds, and the difference is statistically significant for all but Arabic-only. The second column in Table 2 displays the MAP achieved when data selection is applied before training the baseline model.\nFeature effect. To measure the impact of our novel features, we trained classifiers using either LexCL, LexQL, or both feature sets (Section 3.2). In these experiments, the data is fixed to the optimal subset found earlier. Results are summarized on right side of Table 2. Statistically significants improvements over Baseline/Baseline+Data selection are indicated with single/double underlining.\nFor Arabic-only QA, adding LexQL features yields greatest improvements over the baseline, while the same statement holds for LexCL features for the Chinese-only task. For the English-only and mixed-language tasks, the most significant increase in MAP is observed with all of our probabilistic bilingual features. For all but Arabic-only QA, the MAP is statistically significantly better (p < 0.05) than the baseline; for Chinese-only and mixedlanguage tasks, it also outperforms baseline plus data selection (p < 0.05).12 All of this indicates the effectiveness of our probabilistic question translation, as well as our data selection strategy.\nUnderstanding the contribution of each of the four 12Note that bilingual features are not expected to help on the English-only task, and the improvements come solely from data selection.\nLexCL features is also important. To gain insight, we trained a classifier using all LexCL features (using the optimal data subset learned earlier for each task), and then incrementally removed one of the features, and tested on the same task. This controlled experiment revealed that the word translation feature is most useful for Chinese-only QA (i.e., removing it produces largest drop in MAP, 0.6 points), whereas context translation appears to be most useful (by a slighter margin) in Arabic-only QA. In the former case, the diversity provided by word translation might be useful at increasing recall in retrieving Chinese answers. In retrieving Arabic answers, using context to disambiguate the translation might be useful at increasing precision. This result further emphasizes the importance of a customized translation approach for MLQA.\nFurthermore, to test the effectiveness of the probabilistic translation approach (Section 3.1), we replaced all LexCL features with a single lexical similarity feature computed from the one-best question translation. This resulted in lower MAP: 0.427 to 0.423 for Arabic-only, and 0.451 to 0.425 for Chinese-only task (p < 0.01), supporting the hypothesis that probabilistic translation is more effective than the widely-used one-best translation. In fact, almost all gains in Chinese-only QA seems to be coming from the probabilistic translation.\nFor a robustness test, we let cross-validation select the best combination of (data, feature), mimicking a less controlled, real-world setting. In this case, the best MAP for the Arabic-, Chinese-, Englishonly, and Mixed-language tasks are 0.403, 0.448, 0.657, and 0.679, respectively. In all but Arabiconly, these are statistically significantly better (p < 0.05) than not tuning the feature set or training data (i.e., Baseline). This result suggests that our approach can be used for any MLQA task out of the box, and provide improvements.\nLearning to Custom Translate (L2CT). We took the ranked list of answers output by each languagespecific model, and merged all of them into a ranked list of mixed-language answers. For the weighted heuristic, we tried three values for the weight. In Table 3, we see that training separate classifiers for each subtask does not bring overall improvements to the end task. Amongst merging strategies,\nthe most effective were weighted (weights for each query learned by performing a grid-search on other queries) and English first \u2013 however, both are statistically indistinguishable from the single classifier baseline. In the latter case, the percentage of English answers is highest (88%), which might not be desirable. Depending on the application, the ratio of languages can be adjusted with an appropriate merging method. For instance, alternate and norm heuristics tend to represent languages almost equally.\nEven though we get lower MAP in the overall task, Table 2 suggests that it is worthwhile customizing classifiers for each subtask (e.g., the Chinese responses in the ranked list of L2CT are more relevant than Single.). The question of how to effectively combine the results into a mixed-language list, however, remains an open question."}, {"heading": "5 Conclusions", "text": "We introduced L2T, a novel approach for MLQA, inspired from recent success in CLIR research. To our knowledge, this is the first use of probabilistic translation methods for this task, and the first attempt at using machine learning to learn the optimal question translation.\nWe also proposed L2CT, which uses languagespecific classifiers to treat the ranking of English, Arabic, and Chinese answers as three separate subtasks, by applying a separate classifier for each language. While post-retrieval merging has been studied in the past, we have not come across any work that applies this idea specifically to create a language-aware ranking for MLQA.\nOur experimental analysis shows the importance of data selection when dealing with annotations on source and translated text, and the effect of combining translation methods. L2T improved answer\nranking effectiveness significantly for Chinese-only, English-only, and mixed-language QA.\nAlthough results did not support the hypothesis that learning a custom classifier for the retrieval of each language would outperform the single classifier baseline, we think that more research is needed to fully understand how language-specific modeling can benefit MLQA. More sophisticated merging of multiple ranked lists of answers need to be explored. Learning to rank between answers from different languages might be more effective than heuristics. This would allow us to predict the final language ratio, based on many features (e.g., general collection statistics, quality of candidate answers, question category and complexity, MT system confidence levels) to merge question-answer pairs.\nAn even more comprehensive use of machine learning would be to learn word-level translation scores, instead of relying on translation probabilities from the bilingual dictionary, resulting in a fully customized translation. Similar approaches have appeared in learning-to-rank literature for monolingual IR (Bendersky et al., 2010), but not for multilingual retrieval. Another extension of this work would be to apply the same translation for translating answers into the question language (in addition to question translation). By doing this, we would be able to capture the semantics of each answer much better, since we have discussed that one-best translation discards a lot of potentially useful information.\nFinally, since one of the take-away messages of our work is that a deeper understanding of linguistic context can improve QA effectiveness via more sophisticated question translation, we are hoping to see even more improvements by creating features based on word embeddings. One potential next step is to learn bilingual embeddings directly for the task of QA, for which we have started adapting some related work (Bai et al., 2010)."}, {"heading": "Acknowledgements", "text": "Jacob Devlin has provided great help in the design and implementation of the context-based question translation approach. We would also like to thank the anonymous reviewers for their helpful feedback."}], "references": [{"title": "Dublin city university at qaclef 2008", "author": ["SisayFissaha Adafre", "Josef van Genabith."], "venue": "Carol Peters, Thomas Deselaers, Nicola Ferro, Julio Gonzalo, GarethJ.F. Jones, Mikko Kurimo, Thomas Mandl, Anselmo Pe\u00f1as, and Vivien Petras, editors, Evaluating", "citeRegEx": "Adafre and Genabith.,? 2009", "shortCiteRegEx": "Adafre and Genabith.", "year": 2009}, {"title": "Learning to Rank for Robust Question Answering", "author": ["Arvind Agarwal", "Hema Raghavan", "Karthik Subbian", "Prem Melville", "Richard D Lawrence", "David C Gondek", "James Fan."], "venue": "Proceedings of the 21st ACM International Conference on Informa-", "citeRegEx": "Agarwal et al\\.,? 2012", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "A prototype question answering system using syntactic and semantic information for answer retrieval", "author": ["Enrique Alfonseca", "Marco De Boni", "Jos\u00e9-Luis JaraValencia", "Suresh Manandhar."], "venue": "TREC.", "citeRegEx": "Alfonseca et al\\.,? 2001", "shortCiteRegEx": "Alfonseca et al\\.", "year": 2001}, {"title": "Piqasso: Pisa question answering system", "author": ["Giuseppe Attardi", "Antonio Cisternino", "Francesco Formica", "Maria Simi", "Alessandro Tommasi."], "venue": "TREC.", "citeRegEx": "Attardi et al\\.,? 2001", "shortCiteRegEx": "Attardi et al\\.", "year": 2001}, {"title": "Learning to rank with (a lot of) word features", "author": ["Bing Bai", "Jason Weston", "David Grangier", "Ronan Collobert", "Kunihiko Sadamasa", "Yanjun Qi", "Olivier Chapelle", "Kilian Q. Weinberger."], "venue": "Inf. Retr., 13(3):291\u2013314.", "citeRegEx": "Bai et al\\.,? 2010", "shortCiteRegEx": "Bai et al\\.", "year": 2010}, {"title": "Learning concept importance using a weighted dependence model", "author": ["Michael Bendersky", "Donald Metzler", "W. Bruce Croft."], "venue": "Proceedings of the Third ACM International Conference on Web Search and Data Mining, WSDM \u201910, pages 31\u201340, New York, NY,", "citeRegEx": "Bendersky et al\\.,? 2010", "shortCiteRegEx": "Bendersky et al\\.", "year": 2010}, {"title": "Automatic information extraction", "author": ["Elizabeth Boschee", "Ralph Weischedel", "Alex Zamanian."], "venue": "Proceedings of the International Conference on Intelligence Analysis, volume 71.", "citeRegEx": "Boschee et al\\.,? 2005", "shortCiteRegEx": "Boschee et al\\.", "year": 2005}, {"title": "Data-intensive question answering", "author": ["Eric Brill", "Jimmy Lin", "Michele Banko", "Susan Dumais", "Andrew Ng."], "venue": "In Proceedings of the Tenth Text REtrieval Conference (TREC, pages 393\u2013400.", "citeRegEx": "Brill et al\\.,? 2001", "shortCiteRegEx": "Brill et al\\.", "year": 2001}, {"title": "Joint Question Clustering and Relevance Prediction for Open Domain Non-factoid Question Answering", "author": ["Snigdha Chaturvedi", "Vittorio Castelli", "Radu Florian", "Ramesh M Nallapati", "Hema Raghavan."], "venue": "Proceedings of the 23rd International Conference", "citeRegEx": "Chaturvedi et al\\.,? 2014", "shortCiteRegEx": "Chaturvedi et al\\.", "year": 2014}, {"title": "Question answering passage retrieval using dependency relations", "author": ["Hang Cui", "Renxu Sun", "Keya Li", "Min-Yen Kan", "TatSeng Chua."], "venue": "Proceedings", "citeRegEx": "Cui et al\\.,? 2005", "shortCiteRegEx": "Cui et al\\.", "year": 2005}, {"title": "Probabilistic structured query methods", "author": ["Kareem Darwish", "Douglas W. Oard."], "venue": "SIGIR.", "citeRegEx": "Darwish and Oard.,? 2003", "shortCiteRegEx": "Darwish and Oard.", "year": 2003}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M. Schwartz", "John Makhoul."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Com-", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "UAlacant: Using Online Machine Translation for Cross-lingual Textual Entailment", "author": ["Miquel Espl\u00e0-Gomis", "Felipe S\u00e1nchez-Mart\u0131\u0301nez", "Mikel L Forcada"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume", "citeRegEx": "Espl\u00e0.Gomis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Espl\u00e0.Gomis et al\\.", "year": 2012}, {"title": "Architecture and Evaluation of BRUJA, a Multilingual Question Answering System", "author": ["M \u00c1 Garc\u0131\u0301a-Cumbreras", "F Mart\u0131\u0301nez-Santiago", "L A Ure\u00f1a L\u00f3pez"], "venue": "Inf. Retr.,", "citeRegEx": "Garc\u0131\u0301a.Cumbreras et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Cumbreras et al\\.", "year": 2012}, {"title": "Question answering based on temporal inference", "author": ["Sanda Harabagiu", "Cosmin Adrian Bejan."], "venue": "Proceedings of the AAAI-2005 workshop on inference for textual question answering, pages 27\u201334.", "citeRegEx": "Harabagiu and Bejan.,? 2005", "shortCiteRegEx": "Harabagiu and Bejan.", "year": 2005}, {"title": "Efficient question answering with question decomposition and multiple answer streams", "author": ["Sven Hartrumpf", "Ingo Gl\u00c3ckner", "Johannes Leveling."], "venue": "Carol Peters, Thomas Deselaers, Nicola Ferro, Julio Gonzalo, GarethJ.F. Jones, Mikko Kurimo, Thomas", "citeRegEx": "Hartrumpf et al\\.,? 2009", "shortCiteRegEx": "Hartrumpf et al\\.", "year": 2009}, {"title": "Toward semantics-based answer pinpointing", "author": ["Eduard Hovy", "Laurie Gerber", "Ulf Hermjakob", "ChinYew Lin", "Deepak Ravichandran."], "venue": "Proceedings of the first international conference on Human language technology research, pages 1\u20137. Association for", "citeRegEx": "Hovy et al\\.,? 2001", "shortCiteRegEx": "Hovy et al\\.", "year": 2001}, {"title": "Syntactic and semantic decomposition strategies for question answering from multiple resources", "author": ["Boris Katz", "Gary Borchardt", "Sue Felshin."], "venue": "Proceedings of the AAAI 2005 workshop on inference for textual question answering, pages 35\u201341.", "citeRegEx": "Katz et al\\.,? 2005", "shortCiteRegEx": "Katz et al\\.", "year": 2005}, {"title": "Combining Evidence with a Probabilistic Framework", "author": ["Jeongwoo Ko", "Luo Si", "Eric Nyberg"], "venue": null, "citeRegEx": "Ko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ko et al\\.", "year": 2010}, {"title": "Probabilistic Models for Answerranking in Multilingual Question-answering", "author": ["Jeongwoo Ko", "Luo Si", "Eric Nyberg", "Teruko Mitamura."], "venue": "ACM Trans. Inf. Syst., 28(3):16:1\u2014-16:37, July.", "citeRegEx": "Ko et al\\.,? 2010b", "shortCiteRegEx": "Ko et al\\.", "year": 2010}, {"title": "A survey on question answering technology from an information retrieval perspective", "author": ["Oleksandr Kolomiyets", "Marie-Francine Moens."], "venue": "Information Sciences, 181(24):5412\u20135434.", "citeRegEx": "Kolomiyets and Moens.,? 2011", "shortCiteRegEx": "Kolomiyets and Moens.", "year": 2011}, {"title": "Description of the ntou complex qa system", "author": ["Chuan-Jie Lin", "Yu-Min Kuo."], "venue": "Proceedings of NTCIR-8 Workshop.", "citeRegEx": "Lin and Kuo.,? 2010", "shortCiteRegEx": "Lin and Kuo.", "year": 2010}, {"title": "Finding what matters in questions", "author": ["Xiaoqiang Luo", "Hema Raghavan", "Vittorio Castelli", "Sameer Maskey", "Radu Florian."], "venue": "Proceedings of NAACLHLT\u201913.", "citeRegEx": "Luo et al\\.,? 2013", "shortCiteRegEx": "Luo et al\\.", "year": 2013}, {"title": "TREC 2007 ciQA Task: University of Maryland", "author": ["N Madnani", "Jimmy Lin", "Bonnie J Dorr."], "venue": "Proceedings of TREC.", "citeRegEx": "Madnani et al\\.,? 2007", "shortCiteRegEx": "Madnani et al\\.", "year": 2007}, {"title": "The miracle team at the clef 2008 multilingual question answering track", "author": ["\u00c3ngel Martinez-Gonzalez", "Cesar de Pablo-Sanchez", "Concepcion Polo-Bayo", "Mar\u00c3aTeresa Vicente-Diez", "Paloma Martinez-Fernandez", "Jose Luis MartinezFernandez."], "venue": "Carol Pe-", "citeRegEx": "Martinez.Gonzalez et al\\.,? 2009", "shortCiteRegEx": "Martinez.Gonzalez et al\\.", "year": 2009}, {"title": "Towards Cross-lingual Textual Entailment", "author": ["Yashar Mehdad", "Matteo Negri", "Marcello Federico."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT \u201910,", "citeRegEx": "Mehdad et al\\.,? 2010", "shortCiteRegEx": "Mehdad et al\\.", "year": 2010}, {"title": "A Markov random field model for term dependencies", "author": ["Donald Metzler", "W Bruce Croft."], "venue": "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR \u201905, pages 472\u2013479, New York,", "citeRegEx": "Metzler and Croft.,? 2005", "shortCiteRegEx": "Metzler and Croft.", "year": 2005}, {"title": "Keyword translation accuracy and cross-lingual question answering in chinese and japanese", "author": ["Teruko Mitamura", "Mengqiu Wang", "Hideki Shima", "Frank Lin."], "venue": "Proceedings of the Workshop on Multilingual Question Answering, MLQA \u201906, pages 31\u201338,", "citeRegEx": "Mitamura et al\\.,? 2006", "shortCiteRegEx": "Mitamura et al\\.", "year": 2006}, {"title": "Using Coreference for Question Answering", "author": ["Thomas S Morton."], "venue": "Proceedings of the Workshop on Coreference and Its Applications, CorefApp \u201999, pages 85\u201389, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Morton.,? 1999", "shortCiteRegEx": "Morton.", "year": 1999}, {"title": "Integrating logic forms and anaphora resolution in the aliqan system", "author": ["Rafael Munoz-Terol", "Marcel Puchol-Blasco", "Maria Pardino", "Jose Manuel Gomez", "Sandra Roger", "Katia Vila", "Antonio Ferrandez", "Jesus Peral", "Patricio MartinezBarco."], "venue": "Evaluating Systems", "citeRegEx": "Munoz.Terol et al\\.,? 2009", "shortCiteRegEx": "Munoz.Terol et al\\.", "year": 2009}, {"title": "Cross-language information retrieval", "author": ["Jian-Yun Nie."], "venue": "Synthesis Lectures on Human Language Technologies, 3(1):1\u2013125.", "citeRegEx": "Nie.,? 2010", "shortCiteRegEx": "Nie.", "year": 2010}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz J. Och", "Hermann Ney."], "venue": "Computational Linguistics, 29(1):19\u201351.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL \u201903, pages 160\u2013 167, Stroudsburg, PA, USA. Association for Compu-", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Whu question answering system at ntcir-8 aclia task", "author": ["Han Ren", "Donghong Ji", "Jing Wan."], "venue": "Proceedings of NTCIR-8 Workshop.", "citeRegEx": "Ren et al\\.,? 2010", "shortCiteRegEx": "Ren et al\\.", "year": 2010}, {"title": "Simple {BM25} extension to multiple weighted fields", "author": ["Stephen Robertson", "Hugo Zaragoza", "Michael Taylor."], "venue": "Proc. CIKM, pages 42\u201349.", "citeRegEx": "Robertson et al\\.,? 2004", "shortCiteRegEx": "Robertson et al\\.", "year": 2004}, {"title": "Dfki-lt at qaclef 2008", "author": ["Bogdan Sacaleanu", "G\u00fcnter Neumann", "Christian Spurk."], "venue": "Carol Peters and et al., editors, CLEF 2008 Working Notes, Working Notes. Springer Verlag.", "citeRegEx": "Sacaleanu et al\\.,? 2008", "shortCiteRegEx": "Sacaleanu et al\\.", "year": 2008}, {"title": "Evaluation of Complex Temporal Questions in CLEF-QA", "author": ["E Saquete", "J L Vicedo", "P Mart\u0131\u0301nez-Barco", "R Mu\u00f1oz", "F Llopis"], "venue": "In Proceedings of the 5th Conference on Cross-Language Evaluation Forum:", "citeRegEx": "Saquete et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Saquete et al\\.", "year": 2005}, {"title": "Combining multiple strategies for effective monolingual and cross-language retrieval", "author": ["Jacques Savoy."], "venue": "Information Retrieval, 7(1-2):121\u2013148.", "citeRegEx": "Savoy.,? 2004", "shortCiteRegEx": "Savoy.", "year": 2004}, {"title": "Using semantic roles to improve question answering", "author": ["Dan Shen", "Mirella Lapata."], "venue": "EMNLPCoNLL, pages 12\u201321. Citeseer.", "citeRegEx": "Shen and Lapata.,? 2007", "shortCiteRegEx": "Shen and Lapata.", "year": 2007}, {"title": "Bootstrap pattern learning for open-domain clqa", "author": ["Hideki Shima", "Teruko Mitamura."], "venue": "Proceedings of NTCIR-8 Workshop.", "citeRegEx": "Shima and Mitamura.,? 2010", "shortCiteRegEx": "Shima and Mitamura.", "year": 2010}, {"title": "A study of learning a merge model for multilingual information retrieval", "author": ["Ming-Feng Tsai", "Yu-Ting Wang", "Hsin-Hsi Chen."], "venue": "Proceedings of the 31st", "citeRegEx": "Tsai et al\\.,? 2008", "shortCiteRegEx": "Tsai et al\\.", "year": 2008}, {"title": "Exploiting representations from statistical machine translation for crosslanguage information retrieval", "author": ["Ferhan Ture", "Jimmy Lin."], "venue": "ACM Trans. Inf. Syst., 32(4):19:1\u201319:32, October.", "citeRegEx": "Ture and Lin.,? 2014", "shortCiteRegEx": "Ture and Lin.", "year": 2014}, {"title": "What\u2019s in a domain? analyzing genre and topic differences in statistical machine translation", "author": ["Marlies Van der Wees", "Arianna Bisazza", "Wouter Weerkamp", "Christof Monz."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational", "citeRegEx": "Wees et al\\.,? 2015", "shortCiteRegEx": "Wees et al\\.", "year": 2015}, {"title": "Michigan State University at the 2007 TREC ciQA Task", "author": ["Chen Zhang", "Matthew Gerber", "Tyler Baldwin", "Steven Emelander", "Joyce Chai", "Rong Jin."], "venue": "Proceedings of the Sixteenth Text Retrieval Conference, Gaithersburg, Maryland, November.", "citeRegEx": "Zhang et al\\.,? 2007", "shortCiteRegEx": "Zhang et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 41, "context": "However, recent advances in cross-lingual IR (CLIR) show that one can do better by representing the translation space as a probability distribution (Ture and Lin, 2014).", "startOffset": 148, "endOffset": 168}, {"referenceID": 34, "context": ", BM25 (Robertson et al., 2004)).", "startOffset": 7, "endOffset": 31}, {"referenceID": 23, "context": "the other hand, training a classifier for ranking candidate answers allows the exploitation of various features extracted from the question, candidate answer, and surrounding context (Madnani et al., 2007; Zhang et al., 2007).", "startOffset": 183, "endOffset": 225}, {"referenceID": 43, "context": "the other hand, training a classifier for ranking candidate answers allows the exploitation of various features extracted from the question, candidate answer, and surrounding context (Madnani et al., 2007; Zhang et al., 2007).", "startOffset": 183, "endOffset": 225}, {"referenceID": 43, "context": "7%) (Zhang et al., 2007).", "startOffset": 4, "endOffset": 24}, {"referenceID": 1, "context": "Learning-torank approaches have also been applied to QA successfully (Agarwal et al., 2012).", "startOffset": 69, "endOffset": 91}, {"referenceID": 7, "context": "Previous ML-based approaches have introduced useful features from many aspects of natural language, including lexical (Brill et al., 2001; Attardi et al., 2001), syntactic (Alfonseca et al.", "startOffset": 118, "endOffset": 160}, {"referenceID": 3, "context": "Previous ML-based approaches have introduced useful features from many aspects of natural language, including lexical (Brill et al., 2001; Attardi et al., 2001), syntactic (Alfonseca et al.", "startOffset": 118, "endOffset": 160}, {"referenceID": 2, "context": ", 2001), syntactic (Alfonseca et al., 2001; Katz et al., 2005), semantic (Cui et al.", "startOffset": 19, "endOffset": 62}, {"referenceID": 17, "context": ", 2001), syntactic (Alfonseca et al., 2001; Katz et al., 2005), semantic (Cui et al.", "startOffset": 19, "endOffset": 62}, {"referenceID": 9, "context": ", 2005), semantic (Cui et al., 2005; Katz et al., 2005; Alfonseca et al., 2001; Hovy et al., 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al.", "startOffset": 18, "endOffset": 98}, {"referenceID": 17, "context": ", 2005), semantic (Cui et al., 2005; Katz et al., 2005; Alfonseca et al., 2001; Hovy et al., 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al.", "startOffset": 18, "endOffset": 98}, {"referenceID": 2, "context": ", 2005), semantic (Cui et al., 2005; Katz et al., 2005; Alfonseca et al., 2001; Hovy et al., 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al.", "startOffset": 18, "endOffset": 98}, {"referenceID": 16, "context": ", 2005), semantic (Cui et al., 2005; Katz et al., 2005; Alfonseca et al., 2001; Hovy et al., 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al.", "startOffset": 18, "endOffset": 98}, {"referenceID": 28, "context": ", 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al.", "startOffset": 64, "endOffset": 78}, {"referenceID": 36, "context": ", 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al., 2005; Harabagiu and Bejan, 2005), which are especially useful for \u201cwhy\u201d and \u201chow\u201d questions (Kolomiyets and Moens, 2011).", "startOffset": 123, "endOffset": 172}, {"referenceID": 14, "context": ", 2001), and discourse features, such as coreference resolution (Morton, 1999), or identifying temporal/spatial references (Saquete et al., 2005; Harabagiu and Bejan, 2005), which are especially useful for \u201cwhy\u201d and \u201chow\u201d questions (Kolomiyets and Moens, 2011).", "startOffset": 123, "endOffset": 172}, {"referenceID": 20, "context": ", 2005; Harabagiu and Bejan, 2005), which are especially useful for \u201cwhy\u201d and \u201chow\u201d questions (Kolomiyets and Moens, 2011).", "startOffset": 94, "endOffset": 122}, {"referenceID": 38, "context": "Additionally, semantic role labeling and dependency trees are other forms of semantic analysis used widely in NLP applications (Shen and Lapata, 2007; Cui et al., 2005).", "startOffset": 127, "endOffset": 168}, {"referenceID": 9, "context": "Additionally, semantic role labeling and dependency trees are other forms of semantic analysis used widely in NLP applications (Shen and Lapata, 2007; Cui et al., 2005).", "startOffset": 127, "endOffset": 168}, {"referenceID": 33, "context": ", 2009; Lin and Kuo, 2010; Shima and Mitamura, 2010), with few notable exceptions that took term importance (Ren et al., 2010), or semantics (Munoz-Terol et al.", "startOffset": 108, "endOffset": 126}, {"referenceID": 29, "context": ", 2010), or semantics (Munoz-Terol et al., 2009) into account.", "startOffset": 22, "endOffset": 48}, {"referenceID": 22, "context": "ponent (Luo et al., 2013; Chaturvedi et al., 2014).", "startOffset": 7, "endOffset": 50}, {"referenceID": 8, "context": "ponent (Luo et al., 2013; Chaturvedi et al., 2014).", "startOffset": 7, "endOffset": 50}, {"referenceID": 19, "context": "Extending this idea to MLQA appears as a logical next step, yet most prior work relies solely on the one-best translation of questions or answers (Ko et al., 2010b; Garc\u0131\u0301a-Cumbreras et al., 2012; Chaturvedi et al., 2014), or selects the", "startOffset": 146, "endOffset": 221}, {"referenceID": 13, "context": "Extending this idea to MLQA appears as a logical next step, yet most prior work relies solely on the one-best translation of questions or answers (Ko et al., 2010b; Garc\u0131\u0301a-Cumbreras et al., 2012; Chaturvedi et al., 2014), or selects the", "startOffset": 146, "endOffset": 221}, {"referenceID": 8, "context": "Extending this idea to MLQA appears as a logical next step, yet most prior work relies solely on the one-best translation of questions or answers (Ko et al., 2010b; Garc\u0131\u0301a-Cumbreras et al., 2012; Chaturvedi et al., 2014), or selects the", "startOffset": 146, "endOffset": 221}, {"referenceID": 35, "context": "best translation out of few options (Sacaleanu et al., 2008; Mitamura et al., 2006).", "startOffset": 36, "endOffset": 83}, {"referenceID": 27, "context": "best translation out of few options (Sacaleanu et al., 2008; Mitamura et al., 2006).", "startOffset": 36, "endOffset": 83}, {"referenceID": 25, "context": "Mehdad et al. reported improvements by including the top ten translations (instead of the single best) and computing a distance-based entailment score with each (2010).", "startOffset": 0, "endOffset": 168}, {"referenceID": 30, "context": "argue that using MT as a black box is more convenient (and modular) (2012), there are potential benefits from a closer integration between statistical MT and multilingual retrieval (Nie, 2010).", "startOffset": 181, "endOffset": 192}, {"referenceID": 30, "context": "argue that using MT as a black box is more convenient (and modular) (2012), there are potential benefits from a closer integration between statistical MT and multilingual retrieval (Nie, 2010).", "startOffset": 48, "endOffset": 75}, {"referenceID": 40, "context": ", (Tsai et al., 2008)), and has shown to improve multilingual QA performance as well (Garc\u0131\u0301a-Cumbreras et al.", "startOffset": 2, "endOffset": 21}, {"referenceID": 13, "context": ", 2008)), and has shown to improve multilingual QA performance as well (Garc\u0131\u0301a-Cumbreras et al., 2012).", "startOffset": 71, "endOffset": 103}, {"referenceID": 7, "context": ", (Brill et al., 2001) for monolingual, (Ko et al.", "startOffset": 2, "endOffset": 22}, {"referenceID": 19, "context": ", 2001) for monolingual, (Ko et al., 2010a; Ko et al., 2010b) for multilingual QA).", "startOffset": 25, "endOffset": 61}, {"referenceID": 26, "context": "tences extracted from all documents retrieved in the previous stage of the pipeline (using Indri (Metzler and Croft, 2005)).", "startOffset": 97, "endOffset": 122}, {"referenceID": 8, "context": "1) We can accurately classify questions via template patterns (Chaturvedi et al. argue that this does not hold for non-factoid questions (2014)) 2) We can accurately determine the relevance of", "startOffset": 63, "endOffset": 144}, {"referenceID": 42, "context": "an answer, based on its automatic translation into English (Wees et al. show how recall decreases when translating user-generated text (2015))", "startOffset": 60, "endOffset": 142}, {"referenceID": 41, "context": "language IR (CLIR) has shown that incorporating probabilities from the internal representations of an MT system to \u201ctranslate\u201d the question can accomplish this, outperforming standard one-best translation (Ture and Lin, 2014).", "startOffset": 205, "endOffset": 225}, {"referenceID": 11, "context": "and Lin, 2014), while fourth method is a novel query translation method adapted from the neural network translation model described in (Devlin et al., 2014).", "startOffset": 135, "endOffset": 156}, {"referenceID": 32, "context": "Word: In MT, a word alignment is a many-tomany mapping between source- and target-language words, learned without supervision, at the beginning of the training pipeline (Och, 2003).", "startOffset": 169, "endOffset": 180}, {"referenceID": 10, "context": "bilities for CLIR (Darwish and Oard, 2003).", "startOffset": 18, "endOffset": 42}, {"referenceID": 41, "context": "Ture and Lin exploited this to obtain word translation probabilities from the top 10 translations of the question (2014). For each question word w, we can extract which grammar rules were used to produce the translation", "startOffset": 0, "endOffset": 121}, {"referenceID": 11, "context": "Context: Neural network-based MT models learn context-dependent word translation probabilities \u2013 the probability of a target word is dependent on the source word it aligns to, as well as a 5-word window of context (Devlin et al., 2014).", "startOffset": 214, "endOffset": 235}, {"referenceID": 37, "context": "8 Two common approaches are uniform and alternate merging (Savoy, 2004):", "startOffset": 58, "endOffset": 71}, {"referenceID": 6, "context": "All questions and forum posts were processed with an information extraction (IE) toolkit (Boschee et al., 2005), which performs sentence-splitting, named entity recognition, coreference resolution, parsing, and part-ofspeech tagging.", "startOffset": 89, "endOffset": 111}, {"referenceID": 11, "context": "tems (Devlin et al., 2014).", "startOffset": 5, "endOffset": 26}, {"referenceID": 31, "context": "GIZA++ (Och and Ney, 2003) (five iterations of IBM Models 1\u20134 and HMM).", "startOffset": 7, "endOffset": 26}, {"referenceID": 8, "context": "single similarity score between question and translated text, and matching the performance of the system by Chaturvedi et al. on the BOLT evaluation (2014). Baseline MAP values are reported on the leftmost column of Table 2.", "startOffset": 108, "endOffset": 156}, {"referenceID": 5, "context": "peared in learning-to-rank literature for monolingual IR (Bendersky et al., 2010), but not for multilingual retrieval.", "startOffset": 57, "endOffset": 81}, {"referenceID": 4, "context": "One potential next step is to learn bilingual embeddings directly for the task of QA, for which we have started adapting some related work (Bai et al., 2010).", "startOffset": 139, "endOffset": 157}], "year": 2016, "abstractText": "In multilingual question answering, either the question needs to be translated into the document language, or vice versa. In addition to direction, there are multiple methods to perform the translation, four of which we explore in this paper: word-based, 10-best, contextbased, and grammar-based. We build a feature for each combination of translation direction and method, and train a model that learns optimal feature weights. On a large forum dataset consisting of posts in English, Arabic, and Chinese, our novel learn-to-translate approach was more effective than a strong baseline (p < 0.05): translating all text into English, then training a classifier based only on English (original or translated) text.", "creator": "TeX"}}}