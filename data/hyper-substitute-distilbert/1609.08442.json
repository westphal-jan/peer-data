{"id": "1609.08442", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Collaborative Learning for Language and Speaker Recognition", "abstract": "this paper presents a stronger model limiting both argument and sentence recognition further and altogether. the work is based instead a multi - compartment recurrent neural synthesis where the output within one group is captured as the input touches the other, converting to a larger learning scenario whereas models improve implicit language and skill recovery by borrowing signals from on other. our experiments demonstrated that the multi - task model outperforms the task - specific models on both tasks.", "histories": [["v1", "Tue, 27 Sep 2016 13:48:01 GMT  (237kb,D)", "https://arxiv.org/abs/1609.08442v1", "Submitted to ICASSP 2017"], ["v2", "Tue, 23 May 2017 09:56:54 GMT  (622kb,D)", "http://arxiv.org/abs/1609.08442v2", null]], "COMMENTS": "Submitted to ICASSP 2017", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["lantian li", "zhiyuan tang", "dong wang", "rew abel", "yang feng", "shiyue zhang"], "accepted": false, "id": "1609.08442"}, "pdf": {"name": "1609.08442.pdf", "metadata": {"source": "CRF", "title": "Collaborative Learning for Language and Speaker Recognition", "authors": ["Lantian Li", "Zhiyuan Tang", "Dong Wang", "Andrew Abel", "Yang Feng", "Shiyue Zhang"], "emails": ["lilt@cslt.riit.tsinghua.edu.cn;", "tangzy@cslt.riit.tsinghua.edu.cn;", "fengyang@cslt.riit.tsinghua.edu.cn;", "zhangsy@cslt.riit.tsinghua.edu.cn;", "wangdong99@mails.tsinghua.edu.cn;", "andrew.abel@xjtlu.edu.cn"], "sections": [{"heading": null, "text": "and speaker recognition simultaneously and together. This model is based on a multi-task recurrent neural network, where the output of one task is fed in as the input of the other, leading to a collaborative learning framework that can improve both language and speaker recognition by sharing information between the tasks. The preliminary experiments presented in this paper demonstrate that the multi-task model outperforms similar taskspecific models on both language and speaker tasks. The language recognition improvement is especially remarkable, which we believe is due to the speaker normalization effect caused by using the information from the speaker recognition component. Index Terms: language recognition, speaker recognition, deep learning, recurrent neural network"}, {"heading": "1. Introduction", "text": "Language recognition (LRE) [1] and speaker recognition (SRE) [2] are two important tasks in speech processing. Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16]. This lack of overlap can be largely attributed to the intuition that speaker characteristics are language independent in SRE, and dealing with speaker variation is regarded as a basic request in LRE. This independent processing of language identities and speaker traits, however, is not the way we human beings process speech signals: it is easy to imagine that our brain recognizes speaker traits and language identities simultaneously, and that the success of identifying languages helps discriminate between speakers, and vice versa.\nA number of researchers have noticed that language and speaker are two correlated factors. In speaker recognition, it has been confirmed that language mismatch indeed leads to serious performance degradation for speaker recognition [17, 18, 19], and some language-aware models have been demonstrated successfully [20]. In language recognition, speaker variation is seen as a major corruption and is often normalized in the frontend, e.g., by VTLN [21, 22] or CMLLR [23]. These previous studies suggest that speaker and language are inter-correlated factors and should be modelled in an integrated way.\nThis paper presents a novel collaborative learning approach which models speaker and language variations in a single neural model architecture. The key idea is to propagate the output of one task to the input of the other, resulting in a multitask recurrent model. In this way, the two tasks can be learned and inferred simultaneously and collaboratively, as illustrated in Figure 1. It should be noted that collaborative learning is a\ngeneral framework and the component for each task can be implemented using any model, but in this paper, we have chosen to make use of recurrent neural networks (RNN) due to their great potential and good results in various speech processing tasks, including SRE [9, 24] and LRE [15, 22, 25, 26]. Our experiments on the WSJ English database and a Chinese database of a comparable volume demonstrate that the collaborative training method can improve performance on both tasks, and the performance gains on language recognition are especially remarkable.\nIn summary, the contributions of this paper are: firstly, we demonstrate that SRE and LRE can be jointly learned by collaborative learning, and that the collaboration benefits both tasks; secondly, we show that the collaborative learning is especially beneficial for language recognition, which is likely to be due to the normalization effect of using the speaker information provided from the speaker recognition component.\nThe rest of the paper is organized as follows: we first discuss some related work in Section 2, and then present the collaborative learning architecture in Section 3. The experiments are reported in Section 4, and the paper is concluded in Section 5."}, {"heading": "2. Related work", "text": "This collaborative learning approach was proposed by Tang et al. for addressing the close relationship between speech and speaker recognition [27]. The idea of multi-task learning for speech signals has been extensively studied, e.g., [28, 29], and more research on this multi-task learning can be found in [30]. The key difference between collaborative learning and traditional multi-task learning is that the inter-task knowledge share is on-line, i.e., results of one task will impact other tasks, and this impact will be propagated back to itself by the feedback connection, leading to a collaborative and integrated information processing framework.\nThe close correlation between speaker traits and language\nar X\niv :1\n60 9.\n08 44\n2v 2\n[ cs\n.S D\n] 2\n3 M\nay 2\n01 7\nidentities is well known to both SRE and LRE researchers. In language recognition, the conventional phonetic approach [31, 32] relies on the compositional speech recognition system to deal with the speaker variation. In the HMM-GMM era, this often relied on various front-end normalization techniques, such as vocal track length normalization (VTLN) [21, 22] and constrained maximum likelihood linear regression (CMLLR) [23]. In the HMM-DNN era, a DNN model has the natural capability to normalize speaker variation when sufficient training data is available. This capability has been naturally used in i-vector based LRE approaches [33, 34]. However, for pure acousticbased DNN/RNN methods, e.g., [14, 15], there is limited research into speaker-aware learning for LRE.\nFor speaker recognition, language is often not a major concern, perhaps due to the a widely held assumption that speaker traits are language independent. However from the engineering perspective, language mismatch has been found to pose a serious problem due to the different patterns of acoustic space in different languages, according to their own phonetic systems [17, 18, 19]. A simple approach is to train a multilingual speaker model by data pooling [17, 18], but this approach does not model the correlation between language identities and speaker traits. Another potential approach is to treat language and speaker as two random variables and represent them by a linear Gaussian model [35], but this linear Gaussian assumption is perhaps too strong.\nThe collaborative learning approach benefits both tasks. For SRE, the language information provided by LRE helps to identify acoustic units that the recognition should focus on, and for LRE, the speaker information provided by SRE helps to normalize the speaker variation. It is important to note that the models for these two tasks are jointly optimized, and that the information from both tasks during decoding. This means that the collaborative learning is collaborative in both model training and inference."}, {"heading": "3. Multi-task RNN and collaborative learning", "text": "This section first presents the neural model structure for single tasks, and then extends this to the multi-task recurrent model for collaborative learning."}, {"heading": "3.1. Basic single-task model", "text": "For the work in this paper we have chosen a particular RNN, the long short-term memory (LSTM) [36] approach to build the baseline single-task systems for SRE and LRE. LSTM has been shown to deliver good performance for both SRE [9] and LRE [15, 22, 25]. In particular, the recurrent LSTM structure proposed in [37] is used here, as shown in Figure 2, and the associated computation is as follows:\nit = \u03c3(Wixxt +Wirrt\u22121 +Wicct\u22121 + bi)\nft = \u03c3(Wfxxt +Wfrrt\u22121 +Wfcct\u22121 + bf )\nct = ft ct\u22121 + it g(Wcxxt +Wcrrt\u22121 + bc) ot = \u03c3(Woxxt +Worrt\u22121 +Wocct + bo)\nmt = ot h(ct) rt = Wrmmt\npt = Wpmmt\nyt = Wyrrt +Wyppt + by.\nIn the above equations, the W terms denote weight matrices and the b terms denote bias vectors. xt and yt are the input and output vectors; it, ft, ot represent the input, forget and output gates respectively; ct is the cell andmt is the cell output. rt and pt are the two output components derived from mt, in which rt is recurrent and used as an input of the next time step, while pt is not recurrent and contributes to the present output only. \u03c3(\u00b7) is the logistic sigmoid function, and g(\u00b7) and h(\u00b7) are non-linear activation functions, often chosen to be hyperbolic. denotes the element-wise multiplication."}, {"heading": "3.2. Multi-task recurrent model", "text": "The basic idea of the multi-task recurrent model, as shown in Figure 1, is to use the output of one task at the current time step as an auxiliary input into the other task at the next step. In this study, we use the recurrent LSTM model to build the LRE and SRE components, and then combine them with a number of inter-task recurrent connections. This results in a multi-task recurrent model, by which LRE and SRE can be trained and inferred in a collaborative way. The complete model structure is shown in Figure 3, where the superscripts l and s denote the LRE and SRE task respectively, and the dashed lines represent the inter-task recurrent connections.\nA multitude of possible model configurations can be selected. For example, feedback information can be extracted from the cell ct or cell output mt, or from the output component rt or pt; the feedback information can be propagated to the input variable xt, the input gate it, the output gate ot, the forget gate ft, or the non-linear function g(\u00b7).\nGiven the above alternatives, the multi-task recurrent model is rather flexible. The structure shown in Figure 3 is one simple example, where the feedback information is extracted from both the recurrent projection rt and the non-recurrent projection pt, and propagated to the non-linear function g(\u00b7). Using the feedback, the computation for LRE is given as follows:\nilt = \u03c3(W l ixxt +W l irr l t\u22121 +W l icc l t\u22121 + b l i)\nf lt = \u03c3(W l fxxt +W l frr l t\u22121 +W l fcc l t\u22121 + b l f )\nglt = g(W l cxx l t +W l crr l t\u22121 + b l c +W ls crr s t\u22121 +W ls cpp s t\u22121)\nclt = f l t clt\u22121 + ilt glt\nolt = \u03c3(W l oxx l t +W l orr l t\u22121 +W l occ l t + b l o)\nmlt = o l t h(clt)\nrlt = W l rmm l t plt = W l pmm l t\nylt = W l yrr l t +W l ypp l t + b l y\nand the computation for SRE is given as follows:\nist = \u03c3(W s ixxt +W s irr s t\u22121 +W s icc s t\u22121 + b s i )\nfst = \u03c3(W s fxxt +W s frr s t\u22121 +W s fcc s t\u22121 + b s f )\ngst = g(W s cxx s t +W s crr s t\u22121 + b s c +W sl crr l t\u22121 +W sl cpp l t\u22121)\ncst = f s t cst\u22121 + ist gst ost = \u03c3(W s oxx s t +W s orr s t\u22121 +W s occ s t + b s o)\nmst = o s t h(cst )\nrst = W s rmm s t pst = W s pmm s t yst = W s yrr s t +W s ypp s t + b s y"}, {"heading": "3.3. Model training", "text": "The model can be trained \u2018completely\u2019, where each training sample is labelled by both speaker and language, or \u2018incompletely\u2019 where only one task label is available. Our previous research has demonstrated that both cases are suitable [27]. In this preliminary study, we have focused on using \u2018completely\u2019 training. The natural stochastic gradient descent (NSGD) algorithm [38] is employed to train the model."}, {"heading": "4. Experiments", "text": "This section first describes the data profile, and presents the baseline systems. Finally, experimental results of our collaborative learning approach are given."}, {"heading": "4.1. Data", "text": "Two databases were used to perform the experiment: the WSJ database in English and the CSLT-C300 database in Chinese1. All the utterances in both databases were labelled with both language and speaker identities. The development set involves two\n1This database was collected by our institute for commercial usage, so we cannot release the wave data, but the Fbanks and MFCCs in the Kaldi format have been published online. See http://data.cslt. org. The Kaldi recipe to reproduce the results is also available there.\nsubsets: WSJ-E200, which contains 200 speakers (24, 031 utterances) selected from WSJ, and CSLT-C200, which contains 200 speakers (20, 000 utterances) selected from the CSLT-C300 database. The development set was used to train the i-vector, SVM, and multi-task recurrent models.\nThe evaluation set contains an English subset WSJ-E110, which contains 110 speakers selected from WSJ, and a Chinese subset CSLT-C100, which contains 100 speakers selected from the CSLT-C300 database. For each speaker in each subset, 10 utterances were used to enrol its speaker and language identity, and the remaining 13, 236 English utterances and 9, 000 Chinese utterances were used for testing. For SRE, the test is pairwised, leading to 13, 236 target trials and 1, 442, 724 imposter trials in English, plus 9, 000 target trials and 891, 000 Chinese imposter trials. For LRE, the number of test trials is the same as the number of test utterances, which is 13, 236 for English trials and 9, 000 for Chinese trials."}, {"heading": "4.2. LRE and SRE baselines", "text": "Here, we first present the LRE and SRE baselines. For each task, two baseline systems were constructed, one based on ivectors (still state-of-the art), and the other, based on LSTM. All experiments were conducted with the Kaldi toolkit [39]."}, {"heading": "4.2.1. i-vector baseline", "text": "For the i-vector baseline, the acoustic features were 39- dimensional MFCCs. The number of Gaussian components of the universal background model (UBM) was 1, 024, and the dimension of the i-vectors was 200. The resulting i-vectors were used to conduct both SRE and LRE with different scoring methods. For SRE, we consider the simple Cosine distance, as well as the popular discriminative models LDA and PLDA; for LRE, we consider Cosine distance and SVM. All the discriminative models were trained on the development set.\nThe results of the SRE baseline are reported in Table 1, in terms of equal error rate (EER). We tested two scenarios, one is a Full-length test which uses the entire enrolment and test utterance; the other is a Short-length test which involves only 1 second of speech (sampled from the original data after voice activity detection is applied). In both scenarios, the language of each test is assumed to be known in advance, i.e., the tests on English and Chinese datasets are independent.\nLRE is an identification task, with the purpose being to discriminate between two languages (English and Chinese). We therefore use identification error rate (IDR) [40] to measure performance, which is the fraction of the identification mistakes in the total number of identification trials. For a more thorough comparison, the number of identification errors (IDE) is also\nreported. The results of the i-vector/SVM baseline system are reported in Table 2."}, {"heading": "4.2.2. r-vector baseline", "text": "The r-vector baseline is based on the recurrent LSTM structure shown in Figure 2. The SRE and LRE systems use the same configurations: the dimensionality of the cell was set to 1, 024, and the dimensionality of both the recurrent and non-recurrent projections was set to 100. For the SRE system, the output corresponds to the 400 speakers in the training set; For LRE, the output corresponds to the two languages to identify. The output of both projections were concatenated and averaged over all the frames of an utterance, resulting in a 200-dimensional \u2018r-vector\u2019 for that utterance. The r-vector derived from the SRE system represents speaker characters, and the r-vector derived from the LRE system represents the language identity.\nAs in the i-vector baseline, decisions were made based on distance between r-vectors, measured by either the Cosine distance or some discriminative models. The same discriminative models as in the i-vector baseline were used, except that in the LRE system, the softmax outputs of the task-specific LSTMs can be directly used to identify language. The results are shown in Table 1 and Table 2 for SRE and LRE, respectively.\nThe results in Table 1 show that for SRE, the i-vector system with PLDA performs better than the r-vector system in the Full-length test. However, in the Short-length test, the r-vector system is clearly better. This is understandable as the i-vector model is generative and relies on sufficient data to estimate the data distribution; the LSTM model, in contrast, is discriminative and the speaker information can be extracted with even a single frame. Moreover, the PLDA model works very well for the i-vector system, but rather poor for the r-vector system. We estimate that this could be due to the unreliable Gaussian assumption for the residual noise by PLDA. A pair-wised t-test confirms that the performance advantage of the r-vector/LDA system over the i-vector/PLDA system is statistically significant (p < 1e-5).\nThe results in Table 2 show a similar trend, that the i-vector system (with SVM) works well in the full-length test, but in the short-length test, the r-vector system shows much better performance, even with the simple Cosine distance. Again, this can be explained by the fact that the i-vector model is generative, while the r-vector model is discriminative. The advantage of the r-vector model on short utterances has previously been observed, both for LRE [15] and SRE [10]."}, {"heading": "4.3. Collaborative learning", "text": "The multi-task recurrent LSTM system, as shown in Figure 3, was constructed by combining the LRE and SRE r-vector systems, with inter-task recurrent connections augmented. Following research in [27], we selected the output of the recurrent projection layer as the feedback information, and tested several configurations, where the feedback information from one task is propagated into different components of the other task. The results are reported in Tables 3 and 4 for SRE and LRE, where i, f, o denotes the input, forget and output gates, and g denotes the non-linear function.\nThe results show that collaborative learning provides consistent performance improvement on both SRE and LRE, regardless of which component the feedback is applied to. The results show that the output gate is an appropriate component for SRE to receive the feedback, whereas for LRE, the forget gate seems a more suitable choice. However, these observations are based on relatively small databases. More experiments on large data are required to confirm and understand these observations. Finally, it should be highlighted that the collaborative training provides very impressive performance gains for LRE: it significantly improves the single-task r-vector baseline, and beats the i-vector baseline even on the full-length task. This is likely to be because the LRE model trained with the limited training data is largely disturbed by the speaker variation, and the language information provided by the SRE system plays a valuable role of speaker normalization."}, {"heading": "5. Conclusions", "text": "This paper proposed a novel collaborative learning architecture that performs speaker and language recognition as a single and unified model, based on a multi-task recurrent neural network. These preliminary experiments demonstrated that the proposed approach can deliver consistent performance improvement over the single-task baselines for both SRE and LRE. The performance gain on LRE is particularly impressive, which we suggest could be due to the effect of speaker normalization. Future work involves experimenting with large databases and analyzing the properties of the collaborative mechanism, e.g., trainability, stability and extensibility."}, {"heading": "6. References", "text": "[1] J. Navratil, \u201cSpoken language recognition-a step toward multilin-\nguality in speech processing,\u201d IEEE Transactions on Speech and Audio Processing, vol. 9, no. 6, pp. 678\u2013685, 2001.\n[2] F. Bimbot, J.-F. Bonastre, C. Fredouille, G. Gravier, I. MagrinChagnolleau, S. Meignier, T. Merlin, J. Ortega-Garc\u0131\u0301a, D. Petrovska-Delacre\u0301taz, and D. A. Reynolds, \u201cA tutorial on textindependent speaker verification,\u201d EURASIP Journal on Applied Signal Processing, vol. 2004, pp. 430\u2013451, 2004.\n[3] W. M. Campbell, J. P. Campbell, D. A. Reynolds, E. Singer, and P. A. Torres-Carrasquillo, \u201cSupport vector machines for speaker and language recognition,\u201d Computer Speech & Language, vol. 20, no. 2, pp. 210\u2013229, 2006.\n[4] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, \u201cFront-end factor analysis for speaker verification,\u201d IEEE Transactions on Audio, Speech and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.\n[5] Y. Lei, N. Scheffer, L. Ferrer, and M. McLaren, \u201cA novel scheme for speaker recognition using a phonetically-aware deep neural network,\u201d in ICASSP. IEEE, 2014, pp. 1695\u20131699.\n[6] N. Dehak, A.-C. Pedro, D. Reynolds, and R. Dehak, \u201cLanguage recognition via ivectors and dimensionality reduction,\u201d in Interspeech, 2011, pp. 857\u2013860.\n[7] D. Mart\u0131nez, O. Plchot, L. Burget, O. Glembek, and P. Matejka, \u201cLanguage recognition in ivectors space,\u201d in Interspeech, 2011, pp. 861\u2013864.\n[8] V. Ehsan, L. Xin, M. Erik, L. M. Ignacio, and G.-D. Javier, \u201cDeep neural networks for small footprint text-dependent speaker verification,\u201d in ICASSP, 2014, pp. 357\u2013366.\n[9] G. Heigold, I. Moreno, S. Bengio, and N. Shazeer, \u201cEnd-to-end text-dependent speaker verification,\u201d in ICASSP. IEEE, 2016, pp. 5115\u20135119.\n[10] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero, Y. Carmiel, and S. Khudanpur, \u201cDeep neural network-based speaker embeddings for end-to-end speaker verification,\u201d in SLT, 2016.\n[11] I. Lopez-Moreno, J. Gonzalez-Dominguez, O. Plchot, D. Martinez, J. Gonzalez-Rodriguez, and P. Moreno, \u201cAutomatic language identification using deep neural networks,\u201d in ICASSP. IEEE, 2014, pp. 5337\u20135341.\n[12] A. Lozano-Diez, R. Zazo Candil, J. Gonza\u0301lez Dom\u0131\u0301nguez, D. T. Toledano, and J. Gonzalez-Rodriguez, \u201cAn end-to-end approach to language identification in short utterances using convolutional neural networks,\u201d in Interspeech, 2015.\n[13] D. Garcia-Romero and A. McCree, \u201cStacked long-term tdnn for spoken language recognition,\u201d in Interspeech, 2016, pp. 3226\u2013 3230.\n[14] M. Jin, Y. Song, I. Mcloughlin, L.-R. Dai, and Z.-F. Ye, \u201cLIDsenone extraction via deep neural networks for end-to-end language identification,\u201d in Odyssey, 2016.\n[15] R. Zazo, A. Lozano-Diez, J. Gonzalez-Dominguez, D. T. Toledano, and J. Gonzalez-Rodriguez, \u201cLanguage identification in short utterances using long short-term memory (LSTM) recurrent neural networks,\u201d PloS one, vol. 11, no. 1, p. e0146917, 2016.\n[16] M. Kotov and M. Nastasenko, \u201cLanguage identification using time delay neural network d-vector on short utterances,\u201d in Speech and Computer: 18th International Conference, SPECOM 2016, vol. 9811. Springer, 2016, p. 443.\n[17] B. Ma and H. Meng, \u201cEnglish-Chinese bilingual text-independent speaker verification,\u201d in ICASSP. IEEE, 2004, pp. V\u2013293.\n[18] R. Auckenthaler, M. J. Carey, and J. Mason, \u201cLanguage dependency in text-independent speaker verification,\u201d in ICASSP. IEEE, 2001, pp. 441\u2013444.\n[19] A. Misra and J. H. L. Hansen, \u201cSpoken language mismatch in speaker verification: An investigation with nist-sre and crss biling corpora,\u201d in IEEE Spoken Language Technology Workshop (SLT). IEEE, 2014, pp. 372\u2013377.\n[20] A. Rozi, D. Wang, L. Li, and T. F. Zheng, \u201cLanguage-aware plda for multilingual speaker recognition,\u201d in O-COCOSDA 2016, 2016.\n[21] P. Matejka, L. Burget, P. Schwarz, and J. Cernocky, \u201cBrno university of technology system for nist 2005 language recognition evaluation,\u201d in Speaker and Language Recognition Workshop,IEEE Odyssey 2006. IEEE, 2006, pp. 1\u20137.\n[22] G. Gelly, J.-L. Gauvain, V. Le, and A. Messaoudi, \u201cA divide-andconquer approach for language identification based on recurrent neural networks,\u201d in Interspeech, 2016, pp. 3231\u20133235.\n[23] W. Shen and D. Reynolds, \u201cImproved gmm-based language recognition using constrained mllr transforms,\u201d in ICASSP. IEEE, 2008, pp. 4149\u20134152.\n[24] S.-X. Zhang, Z. Chen, Y. Zhao, J. Li, and Y. Gong, \u201cEnd-to-end attention based text-dependent speaker verification,\u201d arXiv preprint arXiv:1701.00562, 2017.\n[25] J. Gonzalez-Dominguez, I. Lopez-Moreno, H. Sak, J. GonzalezRodriguez, and P. J. Moreno, \u201cAutomatic language identification using long short-term memory recurrent neural networks.\u201d in Interspeech, 2014, pp. 2155\u20132159.\n[26] C. Salamea, L. F. D\u2019Haro, R. de Co\u0301rdoba, and R. San-Segundo, \u201cOn the use of phone-gram units in recurrent neural networks for language identification,\u201d in Odyssey, 2016, pp. 117\u2013123.\n[27] Z. Tang, L. Li, D. Wang, and R. C. Vipperla, \u201cCollaborative joint training with multi-task recurrent model for speech and speaker recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.\n[28] X. Li and X. Wu, \u201cModeling speaker variability using long shortterm memory networks for speech recognition,\u201d in Interspeech, 2015, pp. 1086\u20131090.\n[29] Y. Qian, T. Tan, and D. Yu, \u201cNeural network based multi-factor aware joint training for robust speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2231\u20132240, 2016.\n[30] D. Wang and T. F. Zheng, \u201cTransfer learning for speech and language processing,\u201d in APSIPA, 2015, pp. 1225\u20131237.\n[31] L. F. Lamel and J.-L. Gauvain, \u201cLanguage identification using phone-based acoustic likelihoods,\u201d in ICASSP, vol. 1. IEEE, 1994, pp. I\u2013293.\n[32] M. A. Zissman et al., \u201cComparison of four approaches to automatic language identification of telephone speech,\u201d IEEE Transactions on speech and audio processing, vol. 4, no. 1, p. 31, 1996.\n[33] Y. Song, B. Jiang, Y. Bao, S. Wei, and L.-R. Dai, \u201cI-vector representation based on bottleneck features for language identification,\u201d Electronics Letters, vol. 49, no. 24, pp. 1569\u20131570, 2013.\n[34] Y. Tian, L. He, Y. Liu, and J. Liu, \u201cInvestigation of senone-based long-short term memory rnns for spoken language recognition,\u201d Odyssey, pp. 89\u201393, 2016.\n[35] L. Lu, Y. Dong, Z. Xianyu, L. Jiqing, and W. Haila, \u201cThe effect of language factors for robust speaker recognition,\u201d in ICASSP. IEEE, 2009, pp. 4217\u20134220.\n[36] S. Hochreiter and S. Ju\u0308rgen, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[37] H. Sak, A. W. Senior, and F. Beaufays, \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling,\u201d in Interspeech, 2014, pp. 338\u2013342.\n[38] D. Povey, X. Zhang, and S. Khudanpur, \u201cParallel training of deep neural networks with natural gradient and parameter averaging,\u201d arXiv preprint arXiv:1410.7455, 2014.\n[39] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., \u201cThe kaldi speech recognition toolkit,\u201d in IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFLCONF-192584. IEEE Signal Processing Society, 2011.\n[40] B. Yin, A. Eliathamby, and C. Fang, \u201cHierarchical language identification based on automatic language clustering,\u201d in Interspeech, 2007, pp. 178\u2013181."}], "references": [{"title": "Spoken language recognition-a step toward multilinguality in speech processing", "author": ["J. Navratil"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 9, no. 6, pp. 678\u2013685, 2001.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "A tutorial on textindependent speaker verification", "author": ["F. Bimbot", "J.-F. Bonastre", "C. Fredouille", "G. Gravier", "I. Magrin- Chagnolleau", "S. Meignier", "T. Merlin", "J. Ortega-Garc\u0131\u0301a", "D. Petrovska-Delacr\u00e9taz", "D.A. Reynolds"], "venue": "EURASIP Journal on Applied Signal Processing, vol. 2004, pp. 430\u2013451, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Support vector machines for speaker and language recognition", "author": ["W.M. Campbell", "J.P. Campbell", "D.A. Reynolds", "E. Singer", "P.A. Torres-Carrasquillo"], "venue": "Computer Speech & Language, vol. 20, no. 2, pp. 210\u2013229, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P.J. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren"], "venue": "ICASSP. IEEE, 2014, pp. 1695\u20131699.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Language recognition via ivectors and dimensionality reduction", "author": ["N. Dehak", "A.-C. Pedro", "D. Reynolds", "R. Dehak"], "venue": "Interspeech, 2011, pp. 857\u2013860.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Language recognition in ivectors space", "author": ["D. Mart\u0131nez", "O. Plchot", "L. Burget", "O. Glembek", "P. Matejka"], "venue": "Interspeech, 2011, pp. 861\u2013864.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["V. Ehsan", "L. Xin", "M. Erik", "L.M. Ignacio", "G.-D. Javier"], "venue": "ICASSP, 2014, pp. 357\u2013366.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end text-dependent speaker verification", "author": ["G. Heigold", "I. Moreno", "S. Bengio", "N. Shazeer"], "venue": "ICASSP. IEEE, 2016, pp. 5115\u20135119.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep neural network-based speaker embeddings for end-to-end speaker verification", "author": ["D. Snyder", "P. Ghahremani", "D. Povey", "D. Garcia-Romero", "Y. Carmiel", "S. Khudanpur"], "venue": "SLT, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic language identification using deep neural networks", "author": ["I. Lopez-Moreno", "J. Gonzalez-Dominguez", "O. Plchot", "D. Martinez", "J. Gonzalez-Rodriguez", "P. Moreno"], "venue": "ICASSP. IEEE, 2014, pp. 5337\u20135341.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "An end-to-end approach to language identification in short utterances using convolutional neural networks", "author": ["A. Lozano-Diez", "R. Zazo Candil", "J. Gonz\u00e1lez Dom\u0131\u0301nguez", "D.T. Toledano", "J. Gonzalez-Rodriguez"], "venue": "Interspeech, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked long-term tdnn for spoken language recognition", "author": ["D. Garcia-Romero", "A. McCree"], "venue": "Interspeech, 2016, pp. 3226\u2013 3230.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "LIDsenone extraction via deep neural networks for end-to-end language identification", "author": ["M. Jin", "Y. Song", "I. Mcloughlin", "L.-R. Dai", "Z.-F. Ye"], "venue": "Odyssey, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Language identification in short utterances using long short-term memory (LSTM) recurrent neural networks", "author": ["R. Zazo", "A. Lozano-Diez", "J. Gonzalez-Dominguez", "D.T. Toledano", "J. Gonzalez-Rodriguez"], "venue": "PloS one, vol. 11, no. 1, p. e0146917, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Language identification using time delay neural network d-vector on short utterances", "author": ["M. Kotov", "M. Nastasenko"], "venue": "Speech and Computer: 18th International Conference, SPECOM 2016, vol. 9811. Springer, 2016, p. 443.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "English-Chinese bilingual text-independent speaker verification", "author": ["B. Ma", "H. Meng"], "venue": "ICASSP. IEEE, 2004, pp. V\u2013293.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Language dependency in text-independent speaker verification", "author": ["R. Auckenthaler", "M.J. Carey", "J. Mason"], "venue": "ICASSP. IEEE, 2001, pp. 441\u2013444.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Spoken language mismatch in speaker verification: An investigation with nist-sre and crss biling corpora", "author": ["A. Misra", "J.H.L. Hansen"], "venue": "IEEE Spoken Language Technology Workshop (SLT). IEEE, 2014, pp. 372\u2013377.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Language-aware plda for multilingual speaker recognition", "author": ["A. Rozi", "D. Wang", "L. Li", "T.F. Zheng"], "venue": "O-COCOSDA 2016, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Brno university of technology system for nist 2005 language recognition evaluation", "author": ["P. Matejka", "L. Burget", "P. Schwarz", "J. Cernocky"], "venue": "Speaker and Language Recognition Workshop,IEEE Odyssey 2006. IEEE, 2006, pp. 1\u20137.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "A divide-andconquer approach for language identification based on recurrent neural networks", "author": ["G. Gelly", "J.-L. Gauvain", "V. Le", "A. Messaoudi"], "venue": "Interspeech, 2016, pp. 3231\u20133235.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved gmm-based language recognition using constrained mllr transforms", "author": ["W. Shen", "D. Reynolds"], "venue": "ICASSP. IEEE, 2008, pp. 4149\u20134152.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "End-to-end attention based text-dependent speaker verification", "author": ["S.-X. Zhang", "Z. Chen", "Y. Zhao", "J. Li", "Y. Gong"], "venue": "arXiv preprint arXiv:1701.00562, 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Automatic language identification using long short-term memory recurrent neural networks.", "author": ["J. Gonzalez-Dominguez", "I. Lopez-Moreno", "H. Sak", "J. Gonzalez- Rodriguez", "P.J. Moreno"], "venue": "in Interspeech,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "On the use of phone-gram units in recurrent neural networks for language identification", "author": ["C. Salamea", "L.F. D\u2019Haro", "R. de C\u00f3rdoba", "R. San-Segundo"], "venue": "Odyssey, 2016, pp. 117\u2013123.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Collaborative joint training with multi-task recurrent model for speech and speaker recognition", "author": ["Z. Tang", "L. Li", "D. Wang", "R.C. Vipperla"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Modeling speaker variability using long shortterm memory networks for speech recognition", "author": ["X. Li", "X. Wu"], "venue": "Interspeech, 2015, pp. 1086\u20131090.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural network based multi-factor aware joint training for robust speech recognition", "author": ["Y. Qian", "T. Tan", "D. Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2231\u20132240, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Transfer learning for speech and language processing", "author": ["D. Wang", "T.F. Zheng"], "venue": "APSIPA, 2015, pp. 1225\u20131237.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Language identification using phone-based acoustic likelihoods", "author": ["L.F. Lamel", "J.-L. Gauvain"], "venue": "ICASSP, vol. 1. IEEE, 1994, pp. I\u2013293.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1994}, {"title": "Comparison of four approaches to automatic language identification of telephone speech", "author": ["M.A. Zissman"], "venue": "IEEE Transactions on speech and audio processing, vol. 4, no. 1, p. 31, 1996.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "I-vector representation based on bottleneck features for language identification", "author": ["Y. Song", "B. Jiang", "Y. Bao", "S. Wei", "L.-R. Dai"], "venue": "Electronics Letters, vol. 49, no. 24, pp. 1569\u20131570, 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Investigation of senone-based long-short term memory rnns for spoken language recognition", "author": ["Y. Tian", "L. He", "Y. Liu", "J. Liu"], "venue": "Odyssey, pp. 89\u201393, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "The effect of language factors for robust speaker recognition", "author": ["L. Lu", "Y. Dong", "Z. Xianyu", "L. Jiqing", "W. Haila"], "venue": "ICASSP. IEEE, 2009, pp. 4217\u20134220.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "S. J\u00fcrgen"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1997}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A.W. Senior", "F. Beaufays"], "venue": "Interspeech, 2014, pp. 338\u2013342.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Parallel training of deep neural networks with natural gradient and parameter averaging", "author": ["D. Povey", "X. Zhang", "S. Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL- CONF-192584. IEEE Signal Processing Society, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical language identification based on automatic language clustering", "author": ["B. Yin", "A. Eliathamby", "C. Fang"], "venue": "Interspeech, 2007, pp. 178\u2013181.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Language recognition (LRE) [1] and speaker recognition (SRE) [2] are two important tasks in speech processing.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "Language recognition (LRE) [1] and speaker recognition (SRE) [2] are two important tasks in speech processing.", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 153, "endOffset": 156}, {"referenceID": 3, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 177, "endOffset": 189}, {"referenceID": 4, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 177, "endOffset": 189}, {"referenceID": 5, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 177, "endOffset": 189}, {"referenceID": 6, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 177, "endOffset": 189}, {"referenceID": 7, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 8, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 9, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 10, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 11, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 12, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 13, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 14, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 15, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 16, "context": "In speaker recognition, it has been confirmed that language mismatch indeed leads to serious performance degradation for speaker recognition [17, 18, 19], and some language-aware models have been demonstrated successfully [20].", "startOffset": 141, "endOffset": 153}, {"referenceID": 17, "context": "In speaker recognition, it has been confirmed that language mismatch indeed leads to serious performance degradation for speaker recognition [17, 18, 19], and some language-aware models have been demonstrated successfully [20].", "startOffset": 141, "endOffset": 153}, {"referenceID": 18, "context": "In speaker recognition, it has been confirmed that language mismatch indeed leads to serious performance degradation for speaker recognition [17, 18, 19], and some language-aware models have been demonstrated successfully [20].", "startOffset": 141, "endOffset": 153}, {"referenceID": 19, "context": "In speaker recognition, it has been confirmed that language mismatch indeed leads to serious performance degradation for speaker recognition [17, 18, 19], and some language-aware models have been demonstrated successfully [20].", "startOffset": 222, "endOffset": 226}, {"referenceID": 20, "context": ", by VTLN [21, 22] or CMLLR [23].", "startOffset": 10, "endOffset": 18}, {"referenceID": 21, "context": ", by VTLN [21, 22] or CMLLR [23].", "startOffset": 10, "endOffset": 18}, {"referenceID": 22, "context": ", by VTLN [21, 22] or CMLLR [23].", "startOffset": 28, "endOffset": 32}, {"referenceID": 8, "context": "It should be noted that collaborative learning is a general framework and the component for each task can be implemented using any model, but in this paper, we have chosen to make use of recurrent neural networks (RNN) due to their great potential and good results in various speech processing tasks, including SRE [9, 24] and LRE [15, 22, 25, 26].", "startOffset": 315, "endOffset": 322}, {"referenceID": 23, "context": "It should be noted that collaborative learning is a general framework and the component for each task can be implemented using any model, but in this paper, we have chosen to make use of recurrent neural networks (RNN) due to their great potential and good results in various speech processing tasks, including SRE [9, 24] and LRE [15, 22, 25, 26].", "startOffset": 315, "endOffset": 322}, {"referenceID": 14, "context": "It should be noted that collaborative learning is a general framework and the component for each task can be implemented using any model, but in this paper, we have chosen to make use of recurrent neural networks (RNN) due to their great potential and good results in various speech processing tasks, including SRE [9, 24] and LRE [15, 22, 25, 26].", "startOffset": 331, "endOffset": 347}, {"referenceID": 21, "context": "It should be noted that collaborative learning is a general framework and the component for each task can be implemented using any model, but in this paper, we have chosen to make use of recurrent neural networks (RNN) due to their great potential and good results in various speech processing tasks, including SRE [9, 24] and LRE [15, 22, 25, 26].", "startOffset": 331, "endOffset": 347}, {"referenceID": 24, "context": "It should be noted that collaborative learning is a general framework and the component for each task can be implemented using any model, but in this paper, we have chosen to make use of recurrent neural networks (RNN) due to their great potential and good results in various speech processing tasks, including SRE [9, 24] and LRE [15, 22, 25, 26].", "startOffset": 331, "endOffset": 347}, {"referenceID": 25, "context": "It should be noted that collaborative learning is a general framework and the component for each task can be implemented using any model, but in this paper, we have chosen to make use of recurrent neural networks (RNN) due to their great potential and good results in various speech processing tasks, including SRE [9, 24] and LRE [15, 22, 25, 26].", "startOffset": 331, "endOffset": 347}, {"referenceID": 26, "context": "for addressing the close relationship between speech and speaker recognition [27].", "startOffset": 77, "endOffset": 81}, {"referenceID": 27, "context": ", [28, 29], and more research on this multi-task learning can be found in [30].", "startOffset": 2, "endOffset": 10}, {"referenceID": 28, "context": ", [28, 29], and more research on this multi-task learning can be found in [30].", "startOffset": 2, "endOffset": 10}, {"referenceID": 29, "context": ", [28, 29], and more research on this multi-task learning can be found in [30].", "startOffset": 74, "endOffset": 78}, {"referenceID": 30, "context": "In language recognition, the conventional phonetic approach [31, 32] relies on the compositional speech recognition system to deal with the speaker variation.", "startOffset": 60, "endOffset": 68}, {"referenceID": 31, "context": "In language recognition, the conventional phonetic approach [31, 32] relies on the compositional speech recognition system to deal with the speaker variation.", "startOffset": 60, "endOffset": 68}, {"referenceID": 20, "context": "In the HMM-GMM era, this often relied on various front-end normalization techniques, such as vocal track length normalization (VTLN) [21, 22] and constrained maximum likelihood linear regression (CMLLR) [23].", "startOffset": 133, "endOffset": 141}, {"referenceID": 21, "context": "In the HMM-GMM era, this often relied on various front-end normalization techniques, such as vocal track length normalization (VTLN) [21, 22] and constrained maximum likelihood linear regression (CMLLR) [23].", "startOffset": 133, "endOffset": 141}, {"referenceID": 22, "context": "In the HMM-GMM era, this often relied on various front-end normalization techniques, such as vocal track length normalization (VTLN) [21, 22] and constrained maximum likelihood linear regression (CMLLR) [23].", "startOffset": 203, "endOffset": 207}, {"referenceID": 32, "context": "This capability has been naturally used in i-vector based LRE approaches [33, 34].", "startOffset": 73, "endOffset": 81}, {"referenceID": 33, "context": "This capability has been naturally used in i-vector based LRE approaches [33, 34].", "startOffset": 73, "endOffset": 81}, {"referenceID": 13, "context": ", [14, 15], there is limited research into speaker-aware learning for LRE.", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": ", [14, 15], there is limited research into speaker-aware learning for LRE.", "startOffset": 2, "endOffset": 10}, {"referenceID": 16, "context": "However from the engineering perspective, language mismatch has been found to pose a serious problem due to the different patterns of acoustic space in different languages, according to their own phonetic systems [17, 18, 19].", "startOffset": 213, "endOffset": 225}, {"referenceID": 17, "context": "However from the engineering perspective, language mismatch has been found to pose a serious problem due to the different patterns of acoustic space in different languages, according to their own phonetic systems [17, 18, 19].", "startOffset": 213, "endOffset": 225}, {"referenceID": 18, "context": "However from the engineering perspective, language mismatch has been found to pose a serious problem due to the different patterns of acoustic space in different languages, according to their own phonetic systems [17, 18, 19].", "startOffset": 213, "endOffset": 225}, {"referenceID": 16, "context": "A simple approach is to train a multilingual speaker model by data pooling [17, 18], but this approach does not model the correlation between language identities and speaker traits.", "startOffset": 75, "endOffset": 83}, {"referenceID": 17, "context": "A simple approach is to train a multilingual speaker model by data pooling [17, 18], but this approach does not model the correlation between language identities and speaker traits.", "startOffset": 75, "endOffset": 83}, {"referenceID": 34, "context": "Another potential approach is to treat language and speaker as two random variables and represent them by a linear Gaussian model [35], but this linear Gaussian assumption is perhaps too strong.", "startOffset": 130, "endOffset": 134}, {"referenceID": 35, "context": "For the work in this paper we have chosen a particular RNN, the long short-term memory (LSTM) [36] approach to build the baseline single-task systems for SRE and LRE.", "startOffset": 94, "endOffset": 98}, {"referenceID": 8, "context": "LSTM has been shown to deliver good performance for both SRE [9] and LRE [15, 22, 25].", "startOffset": 61, "endOffset": 64}, {"referenceID": 14, "context": "LSTM has been shown to deliver good performance for both SRE [9] and LRE [15, 22, 25].", "startOffset": 73, "endOffset": 85}, {"referenceID": 21, "context": "LSTM has been shown to deliver good performance for both SRE [9] and LRE [15, 22, 25].", "startOffset": 73, "endOffset": 85}, {"referenceID": 24, "context": "LSTM has been shown to deliver good performance for both SRE [9] and LRE [15, 22, 25].", "startOffset": 73, "endOffset": 85}, {"referenceID": 36, "context": "In particular, the recurrent LSTM structure proposed in [37] is used here, as shown in Figure 2, and the associated computation is as follows:", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "Our previous research has demonstrated that both cases are suitable [27].", "startOffset": 68, "endOffset": 72}, {"referenceID": 37, "context": "The natural stochastic gradient descent (NSGD) algorithm [38] is employed to train the model.", "startOffset": 57, "endOffset": 61}, {"referenceID": 38, "context": "All experiments were conducted with the Kaldi toolkit [39].", "startOffset": 54, "endOffset": 58}, {"referenceID": 39, "context": "We therefore use identification error rate (IDR) [40] to measure performance, which is the fraction of the identification mistakes in the total number of identification trials.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "The advantage of the r-vector model on short utterances has previously been observed, both for LRE [15] and SRE [10].", "startOffset": 99, "endOffset": 103}, {"referenceID": 9, "context": "The advantage of the r-vector model on short utterances has previously been observed, both for LRE [15] and SRE [10].", "startOffset": 112, "endOffset": 116}, {"referenceID": 26, "context": "Following research in [27], we selected the output of the recurrent projection layer as the feedback information, and tested several configurations, where the feedback information from one task is propagated into different components of the other task.", "startOffset": 22, "endOffset": 26}], "year": 2017, "abstractText": "This paper presents a unified model to perform language and speaker recognition simultaneously and together. This model is based on a multi-task recurrent neural network, where the output of one task is fed in as the input of the other, leading to a collaborative learning framework that can improve both language and speaker recognition by sharing information between the tasks. The preliminary experiments presented in this paper demonstrate that the multi-task model outperforms similar taskspecific models on both language and speaker tasks. The language recognition improvement is especially remarkable, which we believe is due to the speaker normalization effect caused by using the information from the speaker recognition component.", "creator": "LaTeX with hyperref package"}}}