{"id": "1610.00442", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "A Probability Distribution Strategy with Efficient Clause Selection for Hard Max-SAT Formulas", "abstract": "real real - world conflicts versus solutions can further regarded as consequence of the max - sat problem, which is the optimization graph, each compact satisfiability problem. past this usage, we show a purely probabilistic approach supporting max - con / proms. our algorithm relies initially a stochastic local search strategy comprising a novel probability distribution function incorporating two strategies for the solutions, one based on available values and randomly labeled random one. moreover, while defining convex algorithms based on walksat random unsatisfied clauses randomly, can introduce various novel clause selection mechanism to maintain operational understanding. convex optimization explains why proms outperforms many state - bit - n - design stochastic global search solvers, hard for compact max - po benchmarks.", "histories": [["v1", "Mon, 3 Oct 2016 08:30:47 GMT  (207kb)", "http://arxiv.org/abs/1610.00442v1", "11 pages, 3 tables"]], "COMMENTS": "11 pages, 3 tables", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["sixue liu", "yulong ceng", "gerard de melo"], "accepted": false, "id": "1610.00442"}, "pdf": {"name": "1610.00442.pdf", "metadata": {"source": "CRF", "title": "A Probability Distribution Strategy with Efficient Clause Selection for Hard Max-SAT Formulas", "authors": ["Sixue Liu", "Yulong Ceng", "Gerard de Melo"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n00 44\n2v 1\n[ cs\n.A I]\n3 O\nct 2"}, {"heading": "1 Introduction", "text": "In numerous tasks, we need to jointly make decisions subject to Boolean constraints. Many of these tasks can trivially be reduced to the Max-SAT problem. Examples include learning the structure of a Markov network [9], multi-agent plan recognition [20], and automated planning and scheduling [19]. Given a number of variables and constraints in the form of clauses converted to conjunctive normal form (CNF), the goal in MaxSAT is to find an assignment that maximizes the number of satisfies clauses. Unfortunately, the problem is NP-hard, and for the Max-3-SAT variant (limited to clauses of size 3), it is hard to approximate optimal solutions within a factor of more than 7/8 [11]. Algorithms for Max-SAT are often categorized as either complete or incomplete. The former guarantee the optimality of the output but may fail to deliver any solution at all within a given time.1 Incomplete algorithms, in contrast, can deliver a solution within a given time but do not guarantee its optimality, instead seeking to find near-optimal solutions within a reasonable time.\nFor the incomplete category of both regular SAT and Max-SAT, a number of stochastic local search (SLS) algorithms have been proposed, including GSAT [16], WalkSAT [15], probSAT [5], and configuration checking (CC) [8, 14]. For Max-SAT, Iterated Robust Tabu Search [17] was ranked first in the Max-SAT Evaluation 2012, but, overall, the results were not particularly good. Demonstrating that ideas from incomplete algorithms for SAT can be applied to Max-SAT, a variant of CC named CCLS [13] was ranked highest in the Unweighted Random track of the Max-SAT Evaluations 2013 and 2015, respectively, improving considerably over previous results.\nYet, despite the success of purely probabilistic strategies for SAT, to the best of our knowledge, no one has succeeded in adopting such a strategy for Max-SAT, due to the different nature of the problem.\n1From some complete algorithms such as branch and bound ones, one can extract a partial assignment and an upper bound of the solution at any time, but these are not rigorous complete solutions.\nContributions. Our main contribution is a new probability distribution-based algorithm for Max-SAT, while in recent years there have only been few effective random Max-SAT solvers, most of which adopt a greedy SLS strategy. We present a novel probabilistic strategy for variable selection. Our ProMS (Probability Distribution-based Max-SAT Solving) algorithm is shown to outperform the previous state-of-the-art on hard random problem instances. As additional contributions, we present new optimizations for more efficient calculations, which are also applicable to other SLS solvers."}, {"heading": "2 Variable selection for Max-SAT", "text": "Our input is a formula F = c1\u2227\u00b7 \u00b7 \u00b7\u2227cm in conjunctive normal form (CNF), where ci are disjunctive clauses that consist of literals (Boolean variables or their negations) on a set of variables V = {x1, x2, ..., xn}. A Max-k-SAT formula is a CNF such that each clause contains at most k literals. A complete assignment \u03b1 is a candidate solution such that each variable has a truth value of 0 (false) or 1 (true). The Max-SAT problem consists in finding an \u03b1 that minimizes the number of unsatisfied clauses. We use CU to denote the set of unsatisfied clauses. We also define r = m/n as the ratio of clauses to variables, where n is the total number of Boolean variables and m is the number of clauses.\nDefinition 1 Given a CNF F and a complete assignment \u03b1, the break value b(v) of a variable v is the number of clauses in F that will transition from satisfied to unsatisfied after flipping v under \u03b1, and the make value m(v) of such a v is the number of clauses in F that will transition from unsatisfied to satisfied after flipping v under \u03b1.\nStochastic local search algorithms proceed by repeatedly selecting variables to flip. For probSAT, the variable picking function first randomly selects an unsatisfied clause c (like WalkSAT), and then a variable v in c is chosen with probability f(v)\u2211\nv \u2032\u2208c\nf(v\u2032) , where f(v) is a probability function for variables v. The best\nknown probability function for random 3-SAT, based on empirical studies, has the form f = (0.9+ b(v))\u2212k , with k = 2.06 for the phase transition point. This improves over WalkSAT by mapping each b(v) to a specific probability. Thus, successful SAT methods [5, 15] choose variables v using a function of the form f(v) = g(b(v)), i.e. relying only on the break value.\nOur proposal is that for Max-SAT, the make value, as well, is a crucial piece of information about a variable, and hence our function should take the form f(v) = g(m(v), b(v)). In other words, f(v) will depend on both the make and break values of a variable v. We conjecture that for SAT, when two variables both have high break values (which means they are not worth flipping), it is preferable to pick variables somewhat blindly to escape local optima, whilst for Max-SAT, there are more variables with high break value due to the bigger CU. In this case, the make values are needed to guide the search process towards assignments with fewer unsatisfied clauses. The benefits of the m(v) values thus greatly compensate for the cost of calculating them.\nTheoretical justification. Let F be a uniform random 3-CNF with n variables and m clauses. Further, let Xs be the number of assignments that satisfy at least s clauses in F and let X\u03b1 be the number of clauses that a random assignment \u03b1 satisfies in F . We have E[Xs] = 2nPr[X\u03b1 \u2265 s]. The event that each clause is satisfied by an \u03b1 is independent, so assuming that Xc\u03b1 denotes \u03b1 satisfying a random clause in F , we find that Pr[X\u03b1 \u2265 s] = Pr[Xc\u03b1] s ( m s ) = ( 7 8 )s (m s ) , and thus\nE[Xs] = 2 n\n(\n7\n8\n)s(m\ns\n)\n. (1)\nSuppose s = m\u2212 c with c being a constant. In this case, Eq. 1 becomes (omitting the polynomial)\nE[Xs] = 2 n\n(\n7\n8\n)m\u2212c(m\nc\n) \u223c 2n ( 7\n8\n)m\n.\nIf m > (\u2212 1 log2 7\n8\n+ \u03b4)n \u2248 (5.2 + \u03b4)n for some positive constant \u03b4, then E[Xs] = 2\u2212\u2126(n). By applying\nMarkov\u2019s inequality, we obtain\nPr[Xs > 0] = Pr[Xs \u2265 1] \u2264 E[Xs] = 2 \u2212\u2126(n).\nIn other words, for a large enough random 3-SAT formula with ratio greater than a threshold, it is almost impossible to obtain a solution that only violates a constant number of clauses.\nThe SAT problem is a special case for c = 0. We conjecture that for random k-CNF with ratio lower than the threshold, there is no need to distinguish SAT and Max-SAT algorithms, i.e. for such instances, an ideal algorithm for SAT is also optimal for Max-SAT.2 In Max-SAT evaluation, in order to determine the effectiveness of Max-SAT solvers, all random 3-CNF formulas have ratios greater than 5.2, ranging from 7.5 to 21.5 (while high girth is another category). In fact, one can show that for such formulas, \u03b8(m) clauses\nmust be violated. Now let s = \u03bbm with \u03bb < 1. Using the fact that ( m \u03bbm ) \u223c ( 1 \u03bb\n)\u03bbm (\n1 1\u2212\u03bb\n)(1\u2212\u03bb)m [10], we\nobtain\nE[X\u03bbm] \u223c 2 n\n(\n1\n1\u2212 \u03bb\n)m(7(1 \u2212 \u03bb)\n8\u03bb\n)\u03bbm\n. (2)\nBy setting Eq. 2 to 1, we obtain a mapping function \u03bb = h(r) (recall that m = rn). This implies that for each specific ratio r, at most \u03bbm clauses can be satisfied, i.e. the probability of the existence of a (> \u03bbm)solution vanishes rapidly in an inverse exponential law when n goes to infinity. However, finding a nearthreshold solution (slightly worse than optimal) is much easier. For example, h(21.5) \u2248 0.979, which means that for a large enough random 3-CNF with ratio of 21.5, it is almost impossible to satisfy more than 0.979m clauses. But the number of near-threshold solutions that satisfy 0.972m clauses is E[X0.972m] \u223c 1.913n. This implies that the Hamming distance between a random assignment and a near-threshold solution is n\u2212log2 1.913\u00d7n \u2248 0.064n, and this is extremely easy to be reached by modern solvers [7]. Considering the incremental process of finding a solution for Max-SAT, the convergence time for reaching a near-threshold solution is important.\nComparing to break-only probability functions, assigning a greater probability mass to variables with higher make values encourages the algorithm to decrease the number of unsatisfied clauses more quickly, which reduces the convergence time dramatically. This comes at the price of a smaller coverage of the search space. We will later introduce a special modification to our variable selection, the pure random mode, allowing us to recover such coverage. Our experiments later confirm that our choice leads to highly favorable results."}, {"heading": "3 ProMS algorithm", "text": "As given in Algorithm 1, ProMS first randomly generates a complete assignment, and then repeatedly picks a variable and flips it, for up to a maximal number M of steps. In each step, once a clause has been selected, the incident variables are chosen with probability p according to a distribution function f . We then update the current assignment. If the number of unsatisfied clauses is now lower than for the previous best assignment \u03b1\u2217, we update \u03b1\u2217 to be the current assignment. Ultimately, the best found assignment is returned.\n2A naive way to show this is to enumerate all ( m\nc\n)\ncombinations of violated clauses, fixing the variables in them, and checking the satisfiability of the remaining formulas using a SAT algorithm. For this analysis, we do not consider specific algorithms but only aim to characterize its existence theoretically.\nAlgorithm 1: ProMS Input: CNF-formula F , max. steps M Output: An assignment \u03b1\u2217 of F 1 generate a random assignment \u03b1, \u03b1\u2217 \u2190 \u03b1 2 for step\u2190 1 to M do 3 c\u2190 pickClause(CU(F, \u03b1)) \u22b2 pick unsatisfied clause 4 \u03c4 \u2190 \u2211\nv\u2208c f(v) 5 if \u03c4 > \u03b4 then 6 foreach v \u2208 c do 7 choose v and break the loop with probability f(v)\n\u03c4\n8 else 9 v\u2190 a variable in c chosen at random\n10 \u03b1\u2190 \u03b1 with v flipped 11 if |CU(F, \u03b1)| < |CU(F, \u03b1\u2217)| then 12 \u03b1\u2217 \u2190 \u03b1\n13 return \u03b1\u2217\nAlgorithm 2: pickClause Input: CU, size m, mmax Output: An unsatisfied clause c 1 c\u2190 the second element of CU 2 move the first element of CU to the end 3 m\u2190m + 1 4 if m> mmax then 5 carry out defragmentation 6 m\u2190 |CU|\n7 return c"}, {"heading": "3.1 Variable selection probabilities", "text": "Following our analysis in Section 2, we select variables v for flipping based on both the make values m(v) and the break values b(v), while m(v) is not used in algorithms like WalkSAT and probSAT for the SAT problem. In particular, we define f(v) = m(v)\u03b6(1+b(v))\u03b7 , where \u03b6 , \u03b7 are parameters. Based on the scoring function f , our algorithm iterates over variables v and picks them with probability\np(v) =\n{\nf(v) \u03c4(c) \u03c4(c) \u2265 \u03b4 1 |c| otherwise\n(3)\nwhere \u03c4(c) = \u2211\nv\u2208c f(v) denotes the score of a clause c and \u03b4 is a threshold parameter. When \u03c4(c) is very low, which means that all incident variables have high break values and thus there are no promising variables, every variable within the clause is chosen with equal probability. Such a purely random selection is also used for diversification in dynamic local search [12]. Due to the influence of the make values m(v) on f(v), our algorithm could fall into local optima much faster than with a break-only function. Thus the purely random mode neutralizes excessive greediness and prevents our algorithm from performing poorly.\nWhen \u03c4(c) is above the threshold, every variable is allowed to flip with a probability greater than 0 (note that m(v) is always positive because we choose variables from an unsatisfied clause), while in WalkSAT, some flips are forbidden when 0-break variables exist."}, {"heading": "3.2 Make/break computation", "text": "Instead of computing break values on demand in every iteration (the non-caching scheme), the XOR-caching technique involves maintaining the break values incrementally with XOR scheme optimization, which is 20% faster than the standard caching implementation [4]. Non-caching, however, can be quite successful [18], because for the WalkSAT family, when the current break value exceeds the minimal break value encountered, the computation can be terminated. These two schemes are only for SAT problems with short clauses.\nIn our algorithm, both make values m(v) and break values b(v) need to be calculated. We considered all four combinations in our experiments: \u2022 MCBC: the original implementation, calculating both m(v) and b(v) with XOR-caching \u2022 MCBN: calculate m(v) with caching, but b(v) with non-caching \u2022 MNBC: calculate m(v) with non-caching but b(v) with XOR-caching \u2022 MNBN: calculate both m(v) and b(v) with non-caching\nNote that the XOR scheme is for reducing the complexity of maintaining the break value when the number of true literals transitions to 2 or from 1. The transitions that cause the m(v) value to change are different, so in MCBN, we prefer the faster choice, without XOR scheme."}, {"heading": "3.3 Clause selection", "text": "Studies of WalkSAT-family algorithms focused on the variable picking strategy. Only recently with probSAT was the strategy of selecting unsatisfied clauses investigated further [3]. This work suggested pseudo breadth first search (PBFS) instead of random selection (RS) to select unsatisfied clauses. The unsatisfied clauses CU are usually implemented as an array with dynamic length m to store the actual clauses, and a position array to record the indices of the unsatisfied clauses. In each update step, when a clause turns unsatisfied, one adds the new unsatisfied clause to the end of the array, increasing m and updating the position array (the break step). When a clause turns satisfied, it is swapped with the last element in the array, m is decremented, and the position array updated (the make step). The RS strategy picks a random clause index from {0, . . . ,m\u2212 1}, while PBFS picks it as s mod m, where s is the number of iterations.\nHowever, due to the frequency of make steps, the order of elements in the array is unpredictable. This leads us to propose a stricter alternative called second best breadth first search (SBFS). We implement CU as an array with a dynamic size m to denote the last position. The clauses are maintained in the exact order they were inserted. In each step, we move the first element to the end and choose the second one (simply choosing the first one or the last one would be too strict).\nThe make steps will introduce empty slots at arbitrary positions in the array. When an empty slot is encountered, we simply ignore it and move on until a non-empty slot is reached. A defragmentation procedure is initiated when m exceeds some mmax. This procedure moves all the elements back to the beginning not only to avoid memory limits but also to significantly decrease the time cost for iterating over empty slots. Moreover, the average complexity is very low. Under the optimal mmax we set, only 40 such operations are required per 1,000 search steps.\nComparison with original PBFS and RS: For break steps, necessary operations for SBFS, PBFS, and RS are almost the same: one adds the new unsatisfied clause to the end of the array and increases the size, while updating the position array. However, during make steps, the SBFS simply sets the slot that contains the newly satisfied clause to 0, while PBFS and RS swap it with the last element and update the position array. As a result, the flip option is faster in our case. We evaluated our algorithm with these three clause selection schemes in Experiment 2 of Section 4."}, {"heading": "4 Experiments", "text": "We now describe a series of experiments that assess the make/break calculations and clause selection strategies for ProMS, comparing it with the winners of recent Max-SAT Evaluations."}, {"heading": "4.1 Experimental setup", "text": "Benchmarks. All the benchmarks in our experiment are unweighted instances from the following two particularly hard problem domains. 1. High girth model based benchmarks: This is an important model for generating Max-SAT problem in-\nstances due to the degree of difficulty for satisfication [6]. A high expansion of the incidence graph of a CNF implies high resolution width [2]. In the Max-SAT Evaluation, high girth instances distinguished the performance of different solvers, while the results of other classes are rather close. We use the high girth benchmark of ratio 4 from the Max-SAT Evaluation 2015 as our first class, including 25 instances of 250 variables and 25 of 300 variables. The remaining classes are generated by the high girth generator provided by its author, with 350, 400, 500, 600 variables respectively and the same ratio as the first class. Each of these classes includes 25 instances. 2. Random k-sat benchmark based on fixed clause length random model (also known as uniform random k-SAT): We use 244 random 3-SAT instances from the Max-SAT Evaluation 2015, with particulary large ratios range from 7.5 to 21.5 to evaluate the robustness of our algorithm.\nBaselines. We compared ProMS with five state-of-the-art SLS solvers in Experiment 3, all using the optimal parameters suggested in the referenced literature below.\n\u2022 probSAT: We use probSAT, downloaded from EDACC3, which was the best-performing system in the SAT Competition 2013 \u201cSequential Random SAT\u201d track and the SAT Competition 2014 \u201cParallel Random SAT\u201d track. \u2022 MaxWalkSAT: A version of WalkSAT for Max-SAT, downloaded from its homepage4. \u2022 iraNovelty++: The second place in the Max-SAT Evaluation 2013 \u201cUnweighted Random\u201d track. We use\nthe latest binary, provided by its author [1]. \u2022 CCLS2015: CCLS [13] placed first in the Incomplete Solvers track of the Max-SAT Evaluation 2015\n\u201cUnweighted Random\u201d track. We use the binary submitted to the Max-SAT Evaluation 2015. ProMS is implemented in C, and compiled with gcc using the \u201c-O3\u201d option for optimization. The cutoff time is set to 300 seconds for all instances and all solvers. All experiments are carried out on a machine with Intel Core Xeon E5-2650 2.60GHz CPU and 32GB RAM under Linux.\nParameters. Our approach has 3 parameters: \u03b7, \u03b6 and \u03b4. In order to tune these, we use the benchmark data from the Max-SAT Evaluation 20125, because for all of these instances optimal solutions are available6 . We consider the elapsed time when the algorithm reaches the optimal solution, defined as the best solution ever encountered among all runs for all solvers, or assume No if a cutoff time of 300s is reached. In our result tables, a hyphen indicates that a particular solver never found the optimal solution. Based on a grid search, we fix \u03b7 = \u22122.5 and \u03b6 = r + 17.5, and set \u03b4 = 0.4 \u00b7 r \u2212 1.4, where r is the ratio. The mmax parameter is set as 4.5 times the number of clauses. It is quite possible that better parameters may be found through more careful tuning, and thus the performance of ProMS can be further improved."}, {"heading": "4.2 Results", "text": "Experiment 1. To determine the preferred make/break value computation strategy, we compare the performance of MCBC, MCBN, MNBC, and MNBN. We also trace the transitions of the number of true literals in a clause, which lead to the calculation of the make and break values under MCBC, and determine the percentage of these transitions among all transitions.\nTable 1a shows that using caching to calculate make values is always faster than non-caching, because the make calculations happen so rarely. For calculating break values, MCBN dominates MCBC on high girth benchmarks (11.1% speed-up), while the performance on random k-SAT is very close, since to calculate break value, the flipping speed depends on the number of iterations, the average of which is kr2 for ratio r, and the high girth instances have a lower r. As a result, we use MCBN to calculate make m(v) and break b(v) in our implementation.\nExperiment 2. The second experiment focuses on the influence of different clause selection strategies for ProMS. We use all of the 3-SAT benchmarks from the Max-SAT Evaluation 2014 with optimal solution given for each instance7, so that the algorithm can terminate when the optimal solution is reached. Because their performance is very close, we run each instance 1000 times. All runs discovered the optimal solution within the cutoff time of 300s.\nAs shown in Table 1b, the average steps per instance per run of SBFS is 3.91% less than for RS, and 0.77% less than for PBFS, which means that our new clause selection strategy does not harm the quality of the clause selection. Moreover, SBFS brings a considerable speed-up in the average no. of steps per second, as the computations in each step are reduced. The combination of these two factors leads to the result of SBFS outperforming PBFS by 9.1% and RS by 11.5% in the overall time. Our approach thus relies on SBFS to select clauses.\n3http://satcompetition.org/edacc/sc14/experiment/24/solver-configurations/1559 4http://www.cs.rochester.edu/ kautz/WalkSAT/ 5http://www.maxsat.udl.cat/12/benchmarks/index.html 6http://www.maxsat.udl.cat/12/detailed/ms-random-incomplete-table.html 7http://www.maxsat.udl.cat/14/benchmarks/index.html\nExperiment 3. In the final experiment, we compare our approach with 4 state-of-the-art solvers. Each line in Tables 2 and 3 represent a class of instances, containing many instances of the same size. In Table 2, the hg-v350, hg-400, hg-v500, and hg-600 classes are generated by the high girth generator with 350, 400, 500, 600 variables, respectively, and each of these is evaluated 5 times. The instances in Table 3 are based on the 3-SAT8 benchmarks from the Max-SAT Evaluation 2015, with 294 instances (244 random k-SAT and 50 high girth ones). These instances are evaluated 20 times each. We define the best solution for each instance as the minimal solution found by any of the solvers over all runs. The runs that output the best solution are regarded as successful. We also define the optimal solution for each solver found for each instance as the minimal one among all the runs of the solver on that instance. We report the average time (\u201ctime\u201d) over the successful runs, the average optimal solution (\u201copt.\u201d), and the average solution (\u201cavg.\u201d) over all runs for each class.\nIn Tables 2 and 3, a hyphen in the \u201ctime\u201d columns indicates that a solver failed to deliver a minimal solution in any run, which was the case for iraNovelty++, MaxWalkSAT, and probSAT on some instance classes. CCLS2015 cannot compete with our ProMS, especially for high ratio and high girth instances. Interestingly, probSAT, which only considers the break value in its distribution function and thus can be regarded as a degenerate version of ProMS, turns out to be among the weakest of all approaches. This shows that the make value plays a key role for Max-SAT."}, {"heading": "5 Conclusions and future work", "text": "We have presented a novel algorithm for Max-SAT called ProMS. Unlike most previous Max-SAT approaches, ProMS eschews a greedy strategy in favor of a more probabilistic one. Unlike probabilistic SAT solvers such as probSAT, ProMS relies extensively on make values to guide its search. Our results show that these are crucial for variable selection, especially when paired with an additional pure random mode to constrain their influence. Moreover, selecting the second oldest clause instead of a random one improves the quality of the explored search space even further.\nWe find that ProMS significantly outperforms its competitors on hard problem instances, including solvers with configuration checking heuristics, whose robustness had been shown in several areas of combinatorial search. One downside is that the variance in our running times was also much bigger than for other approaches, due to the strong randomness in our algorithm. Fortunately, this can easily be addressed with a parallelized version of our solver that explores the search space simultaneously in multiple independent threads.\nRegarding future work, it is natural to extend our model by exploring ratio-based parameters, for dynamically handling both high-ratio and more structured instances. While SLS algorithms are not ideal for highly structured instances, our search strategy could be incorporated into algorithms optimized for such instances. Finally, considering the clause weight in our distribution function may allow us extend our model to the Weighted Max-SAT and Weighted Partial Max-SAT problems.\n8Their 2-SAT benchmarks are too easy for the purpose of distinguishing the effectiveness of solvers."}], "references": [{"title": "Inference rules in local search for max-sat", "author": ["A. Abram\u00e9", "D. Habet"], "venue": "IEEE 24th International Conference on Tools with Artificial Intelligence, ICTAI 2012, pages 207\u2013214", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating hard SAT/CSP instances using expander graphs", "author": ["C. Ans\u00f3tegui", "R. B\u00e9jar", "C. Fern\u00e1ndez", "C. Mateu"], "venue": "Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence, pages 1442\u20131443", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Engineering stochastic local search for the satisfiability problem", "author": ["A. Balint"], "venue": "PhD thesis, Universit\u00e4t Ulm. Fakult\u00e4t f\u00fcr Ingenieurwissenschaften und Informatik", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving implementation of SLS solvers for SAT and new heuristics for k-sat with long clauses", "author": ["A. Balint", "A. Biere", "A. Fr\u00f6hlich", "U. Sch\u00f6ning"], "venue": "Theory and Applications of Satisfiability Testing - SAT 2014 - 17th International Conference, pages 302\u2013316", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Choosing probability distributions for stochastic local search and the role of make versus break", "author": ["A. Balint", "U. Sch\u00f6ning"], "venue": "Theory and Applications of Satisfiability Testing\u2013SAT 2012, pages 16\u201329. Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating hard instances for maxsat", "author": ["R. B\u00e9jar", "A. Cabiscol", "F. Many\u00e0", "J. Planes"], "venue": "ISMVL 2009, 39th International Symposium on Multiple-Valued Logic, pages 191\u2013195", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "and H", "author": ["A. Biere", "M. Heule"], "venue": "van Maaren. Handbook of satisfiability, volume 185. ios press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Configuration checking with aspiration in local search for SAT", "author": ["S. Cai", "K. Su"], "venue": "Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning chordal markov networks by constraint satisfaction", "author": ["J. Corander", "T. Janhunen", "J. Rintanen", "H.J. Nyman", "J. Pensar"], "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013, pages 1349\u20131357", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "John Wiley & Sons", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Some optimal inapproximability results", "author": ["J. H\u00e5stad"], "venue": "Journal of the ACM (JACM), 48(4):798\u2013859", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Diversification and determinism in local search for satisfiability", "author": ["C.M. Li", "W.Q. Huang"], "venue": "Theory and Applications of Satisfiability Testing, pages 158\u2013172. Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "CCLS: an efficient local search algorithm for weighted maximum satisfiability", "author": ["C. Luo", "S. Cai", "W. Wu", "Z. Jie", "K. Su"], "venue": "IEEE Trans. Computers, 64(7):1830\u20131843", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Double configuration checking in stochastic local search for satisfiability", "author": ["C. Luo", "S. Cai", "W. Wu", "K. Su"], "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Noise strategies for improving local search", "author": ["B. Selman", "H.A. Kautz", "B. Cohen"], "venue": "AAAI, volume 94, pages 337\u2013343", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1994}, {"title": "A new method for solving hard satisfiability problems", "author": ["B. Selman", "H. Levesque", "D. Mitchell"], "venue": "Proceedings of the Tenth National Conference on Artificial Intelligence, AAAI 1992, pages 440\u2013446. AAAI Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Iterated robust tabu search for MAX-SAT", "author": ["K. Smyth", "H.H. Hoos", "T. St\u00fctzle"], "venue": "Advances in Artificial Intelligence, pages 129\u2013144. Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "UBCSAT: An implementation and experimentation environment for SLS algorithms for SAT and MAX-SAT", "author": ["D.A. Tompkins", "H.H. Hoos"], "venue": "Theory and Applications of Satisfiability Testing, pages 306\u2013320. Springer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning action models from plan examples using weighted MAX-SAT", "author": ["Q. Yang", "K. Wu", "Y. Jiang"], "venue": "Artif. Intell., 171(2-3):107\u2013143", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Action-model based multi-agent plan recognition", "author": ["H.H. Zhuo", "Q. Yang", "S. Kambhampati"], "venue": "Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012, pages 377\u2013385", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "Examples include learning the structure of a Markov network [9], multi-agent plan recognition [20], and automated planning and scheduling [19].", "startOffset": 60, "endOffset": 63}, {"referenceID": 19, "context": "Examples include learning the structure of a Markov network [9], multi-agent plan recognition [20], and automated planning and scheduling [19].", "startOffset": 94, "endOffset": 98}, {"referenceID": 18, "context": "Examples include learning the structure of a Markov network [9], multi-agent plan recognition [20], and automated planning and scheduling [19].", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "Unfortunately, the problem is NP-hard, and for the Max-3-SAT variant (limited to clauses of size 3), it is hard to approximate optimal solutions within a factor of more than 7/8 [11].", "startOffset": 178, "endOffset": 182}, {"referenceID": 15, "context": "For the incomplete category of both regular SAT and Max-SAT, a number of stochastic local search (SLS) algorithms have been proposed, including GSAT [16], WalkSAT [15], probSAT [5], and configuration checking (CC) [8, 14].", "startOffset": 149, "endOffset": 153}, {"referenceID": 14, "context": "For the incomplete category of both regular SAT and Max-SAT, a number of stochastic local search (SLS) algorithms have been proposed, including GSAT [16], WalkSAT [15], probSAT [5], and configuration checking (CC) [8, 14].", "startOffset": 163, "endOffset": 167}, {"referenceID": 4, "context": "For the incomplete category of both regular SAT and Max-SAT, a number of stochastic local search (SLS) algorithms have been proposed, including GSAT [16], WalkSAT [15], probSAT [5], and configuration checking (CC) [8, 14].", "startOffset": 177, "endOffset": 180}, {"referenceID": 7, "context": "For the incomplete category of both regular SAT and Max-SAT, a number of stochastic local search (SLS) algorithms have been proposed, including GSAT [16], WalkSAT [15], probSAT [5], and configuration checking (CC) [8, 14].", "startOffset": 214, "endOffset": 221}, {"referenceID": 13, "context": "For the incomplete category of both regular SAT and Max-SAT, a number of stochastic local search (SLS) algorithms have been proposed, including GSAT [16], WalkSAT [15], probSAT [5], and configuration checking (CC) [8, 14].", "startOffset": 214, "endOffset": 221}, {"referenceID": 16, "context": "For Max-SAT, Iterated Robust Tabu Search [17] was ranked first in the Max-SAT Evaluation 2012, but, overall, the results were not particularly good.", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "Demonstrating that ideas from incomplete algorithms for SAT can be applied to Max-SAT, a variant of CC named CCLS [13] was ranked highest in the Unweighted Random track of the Max-SAT Evaluations 2013 and 2015, respectively, improving considerably over previous results.", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "Thus, successful SAT methods [5, 15] choose variables v using a function of the form f(v) = g(b(v)), i.", "startOffset": 29, "endOffset": 36}, {"referenceID": 14, "context": "Thus, successful SAT methods [5, 15] choose variables v using a function of the form f(v) = g(b(v)), i.", "startOffset": 29, "endOffset": 36}, {"referenceID": 9, "context": "1 1\u2212\u03bb (1\u2212\u03bb)m [10], we obtain", "startOffset": 13, "endOffset": 17}, {"referenceID": 6, "context": "064n, and this is extremely easy to be reached by modern solvers [7].", "startOffset": 65, "endOffset": 68}, {"referenceID": 11, "context": "Such a purely random selection is also used for diversification in dynamic local search [12].", "startOffset": 88, "endOffset": 92}, {"referenceID": 3, "context": "Instead of computing break values on demand in every iteration (the non-caching scheme), the XOR-caching technique involves maintaining the break values incrementally with XOR scheme optimization, which is 20% faster than the standard caching implementation [4].", "startOffset": 258, "endOffset": 261}, {"referenceID": 17, "context": "Non-caching, however, can be quite successful [18], because for the WalkSAT family, when the current break value exceeds the minimal break value encountered, the computation can be terminated.", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "Only recently with probSAT was the strategy of selecting unsatisfied clauses investigated further [3].", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "High girth model based benchmarks: This is an important model for generating Max-SAT problem instances due to the degree of difficulty for satisfication [6].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "A high expansion of the incidence graph of a CNF implies high resolution width [2].", "startOffset": 79, "endOffset": 82}, {"referenceID": 0, "context": "We use the latest binary, provided by its author [1].", "startOffset": 49, "endOffset": 52}, {"referenceID": 12, "context": "\u2022 CCLS2015: CCLS [13] placed first in the Incomplete Solvers track of the Max-SAT Evaluation 2015 \u201cUnweighted Random\u201d track.", "startOffset": 17, "endOffset": 21}], "year": 2016, "abstractText": "Many real-world problems involving constraints can be regarded as instances of the Max-SAT problem, which is the optimization variant of the classic satisfiability problem. In this paper, we propose a novel probabilistic approach for Max-SAT called ProMS. Our algorithm relies on a stochastic local search strategy using a novel probability distribution function with two strategies for picking variables, one based on available information and another purely random one. Moreover, while most previous algorithms based on WalkSAT choose unsatisfied clauses randomly, we introduce a novel clause selection strategy to improve our algorithm. Experimental results illustrate that ProMS outperforms many state-of-the-art stochastic local search solvers on hard unweighted random Max-SAT benchmarks.", "creator": "LaTeX with hyperref package"}}}