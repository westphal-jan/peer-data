{"id": "1610.00465", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Can Evolutionary Sampling Improve Bagged Ensembles?", "abstract": "separation and synthesis ( p & amp ; \u03bb ) group smoothing equations uses multiple versions of the predictor by adapting the additive database / correlation modules then combining them being only single predictor ( allen, 1952 ). the thrust is to improve perceived accuracy in unstable classification and statistical methods. one of the hardest newly devised terms, blending group reduces integration. arcing employing adaptive separation and merge methods against targeting are smarter elements of p & amp ; c methods. in this extended abstract, readers assert strong groundwork for historically diverse usage of methods under the np & amp ; c umbrella, known as fuzzy sampling ( es ). we assess evolutionary algorithms to suggest exponential sampling upon both the feature space ( sub - variable ) but well named training samples. we discuss integrated modelling domains to assess ensembles and empirically compare our judgments against traditional sampling underlying training data / feature x - spaces.", "histories": [["v1", "Mon, 3 Oct 2016 09:53:06 GMT  (38kb)", "http://arxiv.org/abs/1610.00465v1", "3 pages, 1 table, Data Efficient Machine Learning Workshop (DEML'16), ICML"]], "COMMENTS": "3 pages, 1 table, Data Efficient Machine Learning Workshop (DEML'16), ICML", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["harsh nisar", "bhanu pratap singh rawat"], "accepted": false, "id": "1610.00465"}, "pdf": {"name": "1610.00465.pdf", "metadata": {"source": "META", "title": "Can Evolutionary Sampling Improve Bagged Ensembles?", "authors": ["Harsh Nisar", "Bhanu Pratap Singh"], "emails": ["NISAR.HARSH@GMAIL.COM", "BHANUPRATAP.MNIT@GMAIL.COM"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n00 46\n5v 1\n[ cs\n.L G\n] 3\nO ct\n2 01\n6"}, {"heading": "1. Introduction", "text": "Bagging and various variants of it have been widely popular and studied extensively in the last two decades (Breiman, 2001; 1996a; 1999a; Ho, 1998). There has been notable work in understanding the theoretical underpinning of bootstrap aggregating and as to what makes it such a powerful method (Domingos, 1997; Bu\u0308chlmann & Yu, 2002). In traditional bagging, each training example is sampled with replacement and with probability 1\nN . Adap-\ntive Resampling and Combining (Arcing) techniques which modify the probability of each training example being sampled based on heuristics have also been developed and widely used (Freund et al., 1996; Breiman, 1999b).\nRandom subspace methods also known as attribute bagging refer to creating ensembles of predictors trained on randomly selected subsets of total features, that is, predictors\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nconstructed on randomly chosen sub-spaces.\nBoth methods, sub-sampling and sub-spacing reduce the variance of the final ensemble and hence increase the accuracy. Arcing methods are known to reduce the bias of the model as well.\nError based resampling algorithms which try to set the train-set error to zero (Freund et al., 1996), designed bagged ensembles with minimal intersection (Papakonstantinou et al., 2014), diversity and uncorrelated errors (Kuncheva & Whitaker, 2003; Tang et al., 2006), importance sampling (Breiman, 1999a) etc. are some of the areas being studied to improve bagged ensembles. Either there are multiple answers to the question, or the answer changes with each dataset.\nInstead of figuring out precisely as to what sampling and combination of training sets make a bagged ensemble better, we try to fix the definition of better, and allow the bootstrapped training sets to evolve themselves in order to align with the definition. We generate multiple sampled candidate training sets for the final ensemble and let them compete, mutate and mate their way to the optimal sampling and combination. Evolutionary computation has been used for selection of different predictors to be part of an ensemble (Gagne\u0301 et al., 2007) and also for the selection of the most suitable machine learning pipeline for a classification problem (Olson et al., 2016).\nTo our best knowledge, genetic algorithms haven\u2019t been directly used to evolve bootstrapped samples of the training data."}, {"heading": "2. Algorithm", "text": "Evolutionary computation techniques evolve a population of solution variables (bootstrapped training sets in our case) to optimize towards a given criteria. The fittest offspring\nacross all the generations is considered as the most optimal solution. In Evolutionary Sampling (ES), we followed a standard Genetic Algorithm. Initially, a population of multiple ensembles is generated by randomly sampling from the training data multiple times. Each ensemble (henceforth referred to as an individual) in a generation is evaluated based on a fitness function. Fit individuals are selected for the next generation. After this, crossover is applied on a fixed percentage of individuals wherein two individuals swap their predictors. Post this, a fixed percentage of individuals unaffected by crossover undergo a random mutation. Randomly selected member datasets from the selected individual have some of their rows/features deleted, replaced or inserted with equal probability. In feature subspacing the features are subject to perturbation whereas in sub-sampling rows are perturbed.\nWe\u2019ve used the Python package DEAP (Fortin et al., 2012) to implement ES. GA parameters are shown in table 1. As suggested before, instead of understanding as to what makes a bagged ensemble better, we try to rely on the definition of better and try to evolve our ensemble into the same. The fitness function is what guides the sampling and combination of the different sampled datasets. We propose three fitness functions and then try to analyse their performance.\nFEMPO: Fitness Each Model Private Out of Bag. It takes each predictor part of the candidate ensemble and measures their performance on the samples that were left out of it\u2019s training bag (Breiman, 1996c). Final fitness of the ensemble is the mean of each model\u2019s RMSE.\nFEMPT: Fitness Each Model Private Test. It is the average of the performance of member predictors on a private testset which is held out for each sampled dataset during its instantiation.\nFEGT: Fitness Ensemble Global Test. During the start of the algorithm, 20% of the training data is set aside. Each ensemble\u2019s prediction is based on the average prediction of it\u2019s member predictors. RMSE is calculated against the set aside global test."}, {"heading": "3. Experiment", "text": "We conduct experiments on two variants of sampling : Subsampling and Sub-spacing. Sub-sampling works on sampling training examples, whereas sub-spacing works on generating multiple feature sets. We conduct our experiments on 4 benchmark datasets [see table 2]. We compare the mean squared error of the first individual (FI) of the first generation with the hall of fame (best individual) after 30 generations. We assume that the first individual in the first generation is representative of an ensemble which randomly samples its rows or features like in traditional bagging. We try to analyse whether ES is able to evolve better ensembles starting from random specimens. We uniformly use an unpruned Decision Tree Regressor with max depth arbitrarily set as 5."}, {"heading": "4. Results and conclusion", "text": "A 50% win-ratio would suggest that the performance of the ensemble after undergoing ES is better than its random counterpart only half the times. Mean MSE and standard deviation of the same is also a good metric to compare ES with random instantiation. The null hypothesis in the paired t-test suggests that the average mean squared error between the two methods is the same. If the p-value is smaller than a threshold, then we reject the null hypothesis of equal averages.\nIn sub-sampling, FEMPO and FEGT performs equally or\nbetter than their random counterparts. Though the win percentages are almost half in many cases, it could be that the algorithm was initialized with an optimal combination and sampling. FEGT shows the most improvement in Abalone (72% win-ratio).\nIn sub-spacing, both FEMPO and FEGT do significantly better than random sub-spacing. GA has definitely helped in improving accuracy of the model. One should note, GA has a narrow exploration space in case of feature subspacing as compared to sub-sampling and features play a more significant role in deciding the model\u2019s behaviour than a few rows of data.\nResults suggest that ES is possibly useful in cases where maximum accuracy needs to be juiced out and computation is not an issue. It\u2019s evident that better and more robust fitness functions need to be explored, even multi-objective fitness functions, which better represent generalizability and error of the ensemble.\nIt needs to be explored how these methods can be used to generate different models for smaller segments or patches of the dataset (Breiman, 1999a). Can the segments, suggested by ES along with fitness functions that take into account each models fitness (FEPT or FEMPO), be used to find different cohorts in the dataset?\nES guided sub-spacing using linear base estimators can be useful in high dimensional problems like genomic data where selecting features is very important while keeping final models interpretable.\nIt will be interesting to see what happens if the algorithm is allowed to run for generations till the fitness test error reduces approximately to zero. We plan to experiment with different base estimators for each sampled dataset and also explore how sub-spacing and sub-sampling can be combined into one algorithm.\nGoing with the theme of reproduction, we\u2019ve released the basic framework for ES on GitHub (http://github.com/evoml/evoml). We encourage researchers to contribute to the project and test out different fitness functions themselves."}], "references": [{"title": "Bias, variance, and arcing classifiers", "author": ["Breiman", "Leo"], "venue": "1996b. Breiman, Leo. Out-of-bag estimation", "citeRegEx": "Breiman and Leo.,? \\Q1996\\E", "shortCiteRegEx": "Breiman and Leo.", "year": 1996}, {"title": "Pasting small votes for classification in large", "author": ["Citeseer", "1996c. Breiman", "Leo"], "venue": null, "citeRegEx": "Citeseer et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Citeseer et al\\.", "year": 1996}, {"title": "Using adaptive bagging to debias regressions", "author": ["Breiman", "Leo"], "venue": "Technical report, Technical Report 547, Statistics Dept. UCB,", "citeRegEx": "Breiman and Leo.,? \\Q1999\\E", "shortCiteRegEx": "Breiman and Leo.", "year": 1999}, {"title": "Analyzing bagging", "author": ["B\u00fcchlmann", "Peter", "Yu", "Bin"], "venue": "Annals of Statistics, pp", "citeRegEx": "B\u00fcchlmann et al\\.,? \\Q2002\\E", "shortCiteRegEx": "B\u00fcchlmann et al\\.", "year": 2002}, {"title": "Why does bagging work? a bayesian account and its implications", "author": ["Domingos", "Pedro M"], "venue": "In KDD,", "citeRegEx": "Domingos and M.,? \\Q1997\\E", "shortCiteRegEx": "Domingos and M.", "year": 1997}, {"title": "DEAP: Evolutionary algorithms made easy", "author": ["Fortin", "F\u00e9lix-Antoine", "De Rainville", "Fran\u00e7ois-Michel", "Gardner", "Marc-Andr\u00e9", "Parizeau", "Marc", "Gagn\u00e9", "Christian"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fortin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fortin et al\\.", "year": 2012}, {"title": "Experiments with a new boosting algorithm", "author": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "In ICML,", "citeRegEx": "Freund et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1996}, {"title": "Ensemble learning for free with evolutionary algorithms", "author": ["Gagn\u00e9", "Christian", "Sebag", "Michele", "Schoenauer", "Marc", "Tomassini", "Marco"], "venue": "In Proceedings of the 9th annual conference on Genetic and evolutionary computation,", "citeRegEx": "Gagn\u00e9 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gagn\u00e9 et al\\.", "year": 2007}, {"title": "The random subspace method for constructing decision forests", "author": ["Ho", "Tin Kam"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Ho and Kam.,? \\Q1998\\E", "shortCiteRegEx": "Ho and Kam.", "year": 1998}, {"title": "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy", "author": ["Kuncheva", "Ludmila I", "Whitaker", "Christopher J"], "venue": "Machine learning,", "citeRegEx": "Kuncheva et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kuncheva et al\\.", "year": 2003}, {"title": "Bagging by design (on the suboptimality of bagging)", "author": ["Papakonstantinou", "Periklis A", "Xu", "Jia", "Cao", "Zhu"], "venue": "In AAAI, pp. 2041\u20132047,", "citeRegEx": "Papakonstantinou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Papakonstantinou et al\\.", "year": 2014}, {"title": "An analysis of diversity measures", "author": ["Tang", "E Ke", "Suganthan", "Ponnuthurai N", "Yao", "Xin"], "venue": "Machine Learning,", "citeRegEx": "Tang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 6, "context": "Adaptive Resampling and Combining (Arcing) techniques which modify the probability of each training example being sampled based on heuristics have also been developed and widely used (Freund et al., 1996; Breiman, 1999b).", "startOffset": 183, "endOffset": 220}, {"referenceID": 6, "context": "Error based resampling algorithms which try to set the train-set error to zero (Freund et al., 1996), designed bagged ensembles with minimal intersection (Papakonstantinou et al.", "startOffset": 79, "endOffset": 100}, {"referenceID": 10, "context": ", 1996), designed bagged ensembles with minimal intersection (Papakonstantinou et al., 2014), diversity and uncorrelated errors (Kuncheva & Whitaker, 2003; Tang et al.", "startOffset": 61, "endOffset": 92}, {"referenceID": 11, "context": ", 2014), diversity and uncorrelated errors (Kuncheva & Whitaker, 2003; Tang et al., 2006), importance sampling (Breiman, 1999a) etc.", "startOffset": 43, "endOffset": 89}, {"referenceID": 7, "context": "Evolutionary computation has been used for selection of different predictors to be part of an ensemble (Gagn\u00e9 et al., 2007) and also for the selection of the most suitable machine learning pipeline for a classification problem (Olson et al.", "startOffset": 103, "endOffset": 123}, {"referenceID": 5, "context": "We\u2019ve used the Python package DEAP (Fortin et al., 2012) to implement ES.", "startOffset": 35, "endOffset": 56}], "year": 2016, "abstractText": "Perturb and Combine (P&C) group of methods generate multiple versions of the predictor by perturbing the training set or construction and then combining them into a single predictor (Breiman, 1996b). The motive is to improve the accuracy in unstable classification and regression methods. One of the most well known method in this group is Bagging. Arcing or Adaptive Resampling and Combining methods like AdaBoost are smarter variants of P&C methods. In this extended abstract, we lay the groundwork for a new family of methods under the P&C umbrella, known as Evolutionary Sampling (ES). We employ Evolutionary algorithms to suggest smarter sampling in both the feature space (sub-spaces) as well as training samples. We discuss multiple fitness functions to assess ensembles and empirically compare our performance against randomized sampling of training data and feature subspaces.", "creator": "LaTeX with hyperref package"}}}