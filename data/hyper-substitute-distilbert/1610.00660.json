{"id": "1610.00660", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Kernel Selection using Multiple Kernel Learning and Domain Adaptation in Reproducing Kernel Hilbert Space, for Face Recognition under Surveillance Scenario", "abstract": "face recognition ( sham ) capability regained the interest to several researchers starting whole 1990s 22 decades due to its passive nature yielding biometric algorithms. despite high algorithms achieved. advanced recognition algorithms giving minimal conditions, achieving the same performance for error correction obtained in surveillance scanning, face a major hurdle. some attempts have been launched to super - optimal the constant - availability face images and clarify the solution, presenting considerable opportunity of success. the proposed technique in this perspective struggled to cope when the persistent low levels of delayed contrast face detection obtained from surveillance cameras, for implementation under surveillance analysis. for support vector machine classification, generalized variant on accelerated kernel has been a widely discussed issue in special research domain. in this paper, we propose intelligent targeted kernel selection algorithms employing : fast ( sequential - dynamic kernel learning ) players acquire the best forward - kernel algorithms. our advanced technique employs a effective multiple partition structure implement distributed learning ( mkl ) algorithms, carefully sort the optimal kernel then be distinguished while with unsupervised serial adaptation method in automatic reproducing spatial validation space ( rkhs ), for better solution to the problem. rigorous experimentation works been performed on all computer - world rendering face settings : fr \\ _ tab, scface parameter chokepoint. results have been shown using ld - 1 recognition code, roc < cmc alignment. most proposed reconstruction outperforms all other recent science - of - the - art techniques by a perfect sum.", "histories": [["v1", "Mon, 3 Oct 2016 18:22:03 GMT  (8678kb,D)", "http://arxiv.org/abs/1610.00660v1", "13 pages, 15 figures, 4 tables. Kernel Selection, Surveillance, Multiple Kernel Learning, Domain Adaptation, RKHS, Hallucination"]], "COMMENTS": "13 pages, 15 figures, 4 tables. Kernel Selection, Surveillance, Multiple Kernel Learning, Domain Adaptation, RKHS, Hallucination", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["samik banerjee", "sukhendu das"], "accepted": false, "id": "1610.00660"}, "pdf": {"name": "1610.00660.pdf", "metadata": {"source": "CRF", "title": "Kernel Selection using Multiple Kernel Learning and Domain Adaptation in Reproducing Kernel Hilbert Space, for Face Recognition under Surveillance Scenario", "authors": ["Samik Banerjee", "Sukhendu Das"], "emails": [], "sections": [{"heading": null, "text": "1 Kernel Selection using Multiple Kernel Learning and Domain Adaptation in Reproducing Kernel Hilbert Space, for Face\nRecognition under Surveillance Scenario\nSamik Banerjee1 and Sukhendu Das1\n1Department of Computer Science and Engineering, Indian Institute of Technology, Madras, Chennai 600036, INDIA\nFace Recognition (FR) has been the interest to several researchers over the past few decades due to its passive nature of biometric authentication. Despite high accuracy achieved by face recognition algorithms under controlled conditions, achieving the same performance for face images obtained in surveillance scenarios, is a major hurdle. Some attempts have been made to super-resolve the low-resolution face images and improve the contrast, without considerable degree of success. The proposed technique in this paper tries to cope with the very low resolution and low contrast face images obtained from surveillance cameras, for FR under surveillance conditions. For Support Vector Machine classification, the selection of appropriate kernel has been a widely discussed issue in the research community. In this paper, we propose a novel kernel selection technique termed as MFKL (Multi-Feature Kernel Learning) to obtain the best feature-kernel pairing. Our proposed technique employs a effective kernel selection by Multiple Kernel Learning (MKL) method, to choose the optimal kernel to be used along with unsupervised domain adaptation method in the Reproducing Kernel Hilbert Space (RKHS), for a solution to the problem. Rigorous experimentation has been performed on three real-world surveillance face datasets : FR SURV [33], SCface [20] and ChokePoint [44]. Results have been shown using Rank-1 Recognition Accuracy, ROC and CMC measures. Our proposed method outperforms all other recent state-of-the-art techniques by a considerable margin.\nIndex Terms\u2014Kernel Selection, Surveillance, Multiple Kernel Learning, Domain Adaptation, RKHS, Hallucination\nI. INTRODUCTION\nFace Recognition (FR) is a classical problem which is far from being solved. Face Recognition has a clear advantage of being natural and passive over other biometric techniques requiring co-operative subjects. Most face recognition algorithms perform well under a controlled environment. A face recognition system trained at a certain resolution, illumination and pose, recognizes faces under similar conditions with very high accuracy. In contrary, if the face of the same subject is presented with considerable change in environmental conditions, then such a face recognition system fails to achieve a desired level of accuracy. So, we aim to find a solution to the face recognition under unconstrained environment.\nFace images obtained by an outdoor panoramic surveillance camera, are often confronted with severe degradations (e.g., low-resolution, blur, low-contrast, interlacing and noise). This significantly limits the performance of face recognition systems used for binding \u201csecurity with surveillance\u201d applications. Here, images used for training are usually available beforehand which are taken under a well controlled environment in an indoor setup (laboratory, control room), whereas the images used for testing are captured when a subject comes under a surveillance scene. With ever increasing demands to combine \u201csecurity with surveillance\u201d in an integrated and automated framework, it is necessary to analyze samples of face images of subjects acquired by a surveillance camera from a long distance. Hence the subject must be accurately recognized from a low resolution, blurred and degraded (low contrast, aliasing, noise) face image, as obtained from the surveillance camera. These face images are difficult to match\nbecause they are often captured under non-ideal conditions. Thus, face recognition in surveillance scenario is an important and emerging research area which motivates the work presented in this paper.\nPerformance of most classifiers degrade when both the resolution and contrast of face templates used for recognition are low. There have been many advancement in this area during the past decade, where attempts have been made to deal with this problem under an unconstrained environment. For surveillance applications, a face recognition system must recognize a face in an unconstrained environment without the notice of the subject. Degradation of faces is quite evident in the surveillance scenario due to low-resolution and camerablur. Variations in the illuminating conditions of the faces not only reduces the recognition accuracy but occasionally degrades the performance of face detection which is the first step of face recognition. The work presented in this paper deals with such issues involved in FR under surveillance conditions.\nIn the work presented in this paper, the face samples from both gallery and probe are initially passed through a robust face detector, the Chehra face tracker, to find a tightly cropped face image. A domain adaptation (DA) based algorithm, formulated using eigen-domain transformation is designed to bridge the gap between the features obtained from the gallery and the probe samples. A novel Multiple kernel Learning (MKL) based learning method, termed MFKL (Multi-Feature Kernel Learning), is then used to obtain an optimal combination (pairing) of the feature and the kernel for FR. The novelty of the work presented in this paper is the optimal pairing of feature and kernel to provide best performance with DA based learning for FR. Results of performance analysis on\nar X\niv :1\n61 0.\n00 66\n0v 1\n[ cs\n.C V\n] 3\nO ct\n2 01\n6\n2 three real-world surveillance datasets (SCFace [20], FR SURV [33], ChokePoint [44]) exhibit the superiority of our proposed method of kernel selection by MFKL, with DA in Reproducing Kernel Hilbert Space (RKHS) [13]."}, {"heading": "II. DISCUSSION ON RELATED WORK", "text": "The problem of automatic face recognition consists of three key steps/subtasks: (1) detection and rough normalization of faces, (2) feature extraction and accurate normalization of faces, (3) identification and/or verification. These different subtasks are not totally isolated. For example, the discriminating facial features (eyes, nose, mouth) used for face recognition are often used in face detection. Face detection and feature extraction can be achieved simultaneously. Recent advances of face detection, MKL and DA are discussed henceforth.\nThe most widely used algorithm for FD (face detection) was proposed by Viola et al. in [41]. The algorithm proposed in [41] is based on a simple and efficient classifier which is built using the AdaBoost learning algorithm to select a small number of critical visual features from a very large set of potential features. The authors proposed a method for combining classifiers in a cascade which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A very recent state-of-the-art technique proposed by Zhu et al. [48] presents a unified model for face detection, pose estimation, and landmark estimation in real-world, cluttered images. Their proposed model is based on a mixture of trees with a shared pool of parts which model every facial landmark as a part and use global mixtures to capture topological changes due to viewpoint. The tree-structured models are surprisingly effective at capturing global elastic deformation, while being easy to optimize unlike dense graph structures. Yow et al. [47] also proposed a feature-based algorithm for detecting faces that is sufficiently generic and is also easily extensible to cope with more demanding variations of the imaging conditions. The algorithm detects feature points from the image using spatial filters and groups them into face candidates using geometric and gray level constraints. A probabilistic framework is then used to reinforce probabilities and to evaluate the likelihood of the candidate as a face. Our proposed method uses a set of 49 fiducial landmark points detected by the Chehra face detector [4]. The incremental training of discriminative models used by Chehra is not only important for building personspecific models but also to update a generic model in case a new annotated data arrives, since the training procedure is very expensive and time consuming. In particular, incremental training of discriminative models use a cascade of linear regressors to learn the mapping from facial texture to the shape.\nWhile classical kernel-based classifiers are based on a single kernel, in practice it is often desirable to form classifiers based on combinations of multiple kernels. Bach et al. [6] proposed the sequential minimal optimization (SMO) techniques that are essential in large-scale implementations of the SVM for large number of kernels. Shrivastava et al. [36] proposed a multiple kernel learning (MKL) algorithm that is based on the sparse\nrepresentation-based classification (SRC) method efficiently representing the nonlinearities in the high-dimensional feature space, based on the kernel alignment criteria. This method uses a two step training method to learn the kernel weights and sparse codes. At each iteration, the sparse codes are updated first while fixing the kernel mixing coefficients, and then the kernel mixing coefficients are updated while fixing the sparse codes. These two steps are repeated until a stopping criteria is met. Lanckriet et al. [25] considered conic combinations of kernel matrices for classification, leading to a convex quadratically constrained quadratic program. Sonnenburg et al. [37] show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard SVM implementations. Moreover, Sonnenburg et al. [37] generalize the formulation and method to a larger class of problems, including regression and one-class classification. The method proposed in our paper uses Structured MKL [45], which is a modified version of simpleMKL [2], through a primal formulation involving a weighted L2-norm regularization.\nDomain adaptation has gained enormous importance in the recent past. One of the popular solutions of this problem is to weigh each instance in the source domain appropriately such that, when the weighted instances of the source domain are used for training the expected loss of classifiers tested for target domain is minimized. Some of the works where instance weights of source domain have been calculated are [38], [11], [21], [30]. Sugiyama et al. [38] proposed a method to calculate weights for instances in source domain, by using a convex optimization framework which minimizes the KLdivergence between the source and the target domain. There have been some work for building robust classifiers for domain adaptation [5], [46], [12]. Yang et. al. [46] has proposed a method to calculate weights for each instances in source domain, which are used to effectively retrain a pre-learned SVM for target domain. Domain Adaptive Machine (DAM), proposed by Duan et al. [12], learns a robust decision function for labeling the instances in target domain, by leveraging a set of base classifies learned on multiple source domains. Pan et al. [29] proposed transfer component analysis (TCA), which minimizes the disparity of distribution by considering the difference of means between two domains and it also preserves local geometry of underlying manifold. The study of subspaces spanned by the input data have found important applications in computer vision tasks [40]. These subspaces identify important properties of the data which can be used to model the data. Subspace based modeling have also been widely used for the task of visual domain adaptation [15], [7]. Fernando et. al. [15] has calculated a subspace using eigen-vectors of two domains such that the basis vectors of transformed source and target domains are aligned. Manifold alignment has also been used for domain adaptation earlier. Wang et al. [43] has considered the manifold of each domain and estimated a latent space, where the manifolds of both the domains are similar to each other. However, the structures or the distributions of the domains have not been considered in this case. In this paper we transform the source domain data into the target domain based on the eigen vectors of both the domains. Samanta et al. in [35] describe a new method of unsupervised domain adaptation\n3 (DA) using the properties of the sub-spaces spanning the source and target domains, when projected along a path in the Grassmann manifold. A new technique of unsupervised domain adaptation based on eigenanalysis in kernel space, for the purpose of categorisation tasks, had also been proposed in [34]. Here, the authors propose a transformation of data in source domain, such that the eigenvectors and eigenvalues of the transformed source domain become similar to that of the target domain. They extend this idea to the RKHS [13], which enables to deal with non-linear transformation of source domain. To handle non-linearity of the data, we also rely on the DA projected in RKHS.\nRecently, face recognition research in real-life surveillance has become very popular. For high data transmission speed and easy data storage, surveillance cameras generally produce images in low resolution, and face images captured directly by surveillance cameras are usually very small. Besides, images taken by surveillance cameras are generally with noises and corruptions, due to the uncontrolled circumstances and distances. Zou et al. [49] proposed a super-resolution approach to increase the recognition performance for very low-resolution face images. They employ a minimum mean square error estimator to learn the relationship between low and high resolution training pairs. A further discriminative constraint is added to the learning approach using the class label information. Biswas et al. [10] proposed a matching algorithm through using Multidimensional Scaling (MDS). In their approach both the low and high resolution training pairs are projected into a kernel space. Transformation relationship is then learned in the kernel space through iterative majorization algorithm, which is used to match the low-resolution test faces to the highresolution gallery faces. Similarly, Ren et al. [32] proposed the Coupled Kernel Embedding approach, where they map the low and high resolution face images onto different kernel spaces and then transform them to a learned subspace for recognition.\nRudrani et al. in [33] proposed an approach with the combination of partial restoration (using super-resolution) of probe samples and degradation of gallery samples. The authors also proposed a outdoor surveillance dataset, FR SURV [33] for evaluating their approach. In our previous work proposed in [8], we aim to bridge the gap of resolution and contrast using super-resolution and contrast stretching on the probe samples and degrading the gallery samples by downsampling the gallery samples and introducing a Gaussian blur to the downsampled images. In addition to these measures, we had also proposed a DA technique based on an eigen-domain transformation to make the distributions of the features obtained from the gallery and probe samples identical.\nIn the following sections, we first briefly present a few technical background details, followed by our proposed framework and then our experimental results of the proposed technique."}, {"heading": "III. BRIEF TECHNICAL BACKGROUND", "text": "A. Multiple kernel learning problem\nA seemingly unrelated problem in machine learning, the problem of multiple kernel learning is in fact intimately connected with sparsity-inducing norms by duality. It actually\ncorresponds to the most natural extension of sparsity to Reproducing Kernel Hilbert Spaces [13]. It can be shown that for a large class of norms and, among them, many sparsityinducing norms, there exists for each of them a corresponding multiple kernel learning scheme, and, vice-versa, each multiple kernel learning scheme defines a new norm.\nIn the multiple kernel learning problem, n data points (xi, yi) are given, where xi \u2208 X for some input space X \u2208 Rl, and where yi \u2208 {\u22121, 1}. Also, assume that m kernels are given Kj \u2208 Rn\u00d7n, which are assumed to be symmetric positive semidefinite, obtained from evaluating a kernel function on the data xi. Consider the problem of learning the best linear combination \u2211m j=1 \u03b7jKj of kernels Kj with non-negative coefficients \u03b7j > 0 and with a trace constraint tr \u2211m j=1 \u03b7jKj = \u2211m j=1 \u03b7j trKj = c, where c > 0 is fixed. Lanckriet et al. [24] show that this setup yields the following optimization problem:\nmin \u03b6 \u2212 2eT\u03b1 w.r.t. \u03b6 \u2208 R, \u03b1 \u2208 Rn\ns.t. 0 \u2264 \u03b1 \u2264 C,\u03b1T y = 0, and\n\u03b1TD(y)KjD(y)\u03b1 \u2264 trKj c \u03b6, j \u2208 {1, ...,m}\n(1)\nwhere D(y) is a diagonal matrix with all diagonal elements as y, e \u2208 Rn the vector of all ones, and C a positive constant. The coefficients \u03b7j are recovered as Lagrange multipliers for the constraints \u03b1TD(y)KjD(y)\u03b1 \u2264 trKjc \u03b6.\nB. Support Kernel Learning Machine\nConsider a decomposition of Rl into p blocks (subspaces): Rl = Rl1 \u00d7 ...\u00d7Rlp , such that, xfi \u2208 Rlf , (x f i is a vector). The classification algorithm, \u201dsupport kernel machine\u201d, as proposed by Bach et al. [6], is exactly the dual of the problem defined in equation 1.\nThe aim is to obtain a linear classifier of the form y = sign(wTx + b), where w has identical block-level decomposition as w = (w1, ..., wp) \u2208 Rl1+...+lp . To induce sparsity in the vector w at the level of blocks, such that most of its components wi go to zero, the l1-norm of the square of a weighted block of w, ( \u2211p f=1 df\u2016wf\u20162)2, is minimized, where the l2-norm is used within every block. The primal problem is thus:\nmin 1\n2 ( p\u2211 f=1 df\u2016wf\u20162)2 + C n\u2211 i=1 \u03bei\nw.r.t. w = (w1, ..., wp) \u2208 Rl1+...+lp , b \u2208 R, \u03be \u2208 Rn+ s.t. yi( \u2211 f wTf x f i + b) \u2265 1\u2212 \u03bei,\u2200i \u2208 {1, ..., n}. (2)\nThe problem posed in equation 2 is treated as a secondorder cone program (SOCP) [27], which yields the following dual:\n4 min 1\n2 \u03b32 \u2212 \u03b1T e\nw.r.t. \u03b3 \u2208 R, \u03b1 \u2208 Rn\ns.t. 0 \u2264 \u03b1 \u2264 C,\u03b1T y = 0, and \u2016 \u2211 i \u03b1iyix f i \u20162 \u2264 df\u03b3,\u2200f \u2208 {1, ..., p}.\n(3)\nTo induce sparsity in the formulation, the following KKT conditions play an instrumental role to give the complementary slackness equations:\n1) \u03b1i(yi( \u2211 f w T f x f i + b)\u2212 1 + \u03b6i) = 0,\u2200i 2) (C \u2212 \u03b1i)\u03b6i = 0,\u2200i 3) ( wf \u2016wf\u20162 )T (\u2212\u2211i \u03b1iyixfi df\u03b3 ) = 0,\u2200f\n4) \u03b3( \u2211 df tf \u2212 \u03b3) = 0\nIn RKHS, the data points xi are embedded in an Euclidean space via a mapping \u03c6 : X \u2192 Rc and \u03c6(x) has m distinct block components \u03c6(x) = (\u03c61(x), ..., \u03c6m(x)). Following the usual recipe for kernel methods, this embedding is performed implicitly, by specifying the inner product in Rc using a kernel function, which in this case is the sum of individual kernel functions on each block component:\nk(xi, xj) = \u03c6(xi) T\u03c6(xj) = m\u2211 s=1 \u03c6s(xi) T\u03c6s(xj)\n= m\u2211 s=1 ks(xi, xj)\n(4)\nThe minimization task described in equation 2 is kernelized [13] using this kernel function (equation 4). In particular, considering the dual of the problem in equation 2 and substituting the kernel function for the inner products in equation 3, one obtains:\nmin 1\n2 \u03b32 \u2212 \u03b1T e\nw.r.t. \u03b3 \u2208 R, \u03b1 \u2208 Rn\ns.t. 0 \u2264 \u03b1 \u2264 C,\u03b1T y = 0 (\u03b1TD(y)Kj(y)\u03b1) 1 2 \u2264 dj\u03b3,\u2200j.\n(5)\nwhere Kj is the j-th Gram matrix of the points {xi} corresponding to the j-th kernel.\nEquation 5 is rearranged to yield an equivalent formulation in which the quadratic constraints are incorporated into the objective function:\nmin maxj 1\n2d2j \u03b1TD(y)KjD(y)\u03b1\u2212 \u03b1T e\nw.r.t. \u03b1 \u2208 Rn\ns.t. 0 \u2264 \u03b1 \u2264 C,\u03b1T y = 0\n(6)\nLet, Jj(\u03b1) denote 12d2j \u03b1 TD(y)KjD(y)\u03b1\u2212\u03b1T e and J(\u03b1) = maxjJj(\u03b1). Minimization of J(\u03b1) now reduces to an convex optimization problem as described by equation 6, as J(\u03b1) is a non-differentiable convex function subject to linear constraints.\nC. Domain Adaptation (DA) based on Eigen Domain transformation [8]\nGiven a data, its distribution can be estimated using the covariance matrix or Eigen-vectors. Hence, if the Eigenvectors of two datasets are same, then we can say that the distributions of the two datasets are approximately similar to each other. Hence, in this paper we aim to transform the source domain in such a way that the Eigen-vectors of the transformed source domain is same as that of the target domain. We extend this idea of transformation of source domain in Reproducing Kernel Hilbert Space (RKHS) [13] to handle non-linear transformation of data, when necessary. In the following sub-sections, we give the mathematical details necessary for the unsupervised method of domain adaptation (as in our earlier work in [8]).\n1) Learning the Transformation Let S and T be the source and target domains having nS and nT number of instances respectively and S\u0303 be the transformed source domain. Let the ith and jth instance of S and T be represented as Si and Tj respectively. Let, the principal components of S and T be denoted by the matrices US and UT respectively, where the ith column represents the Eigenvector corresponding to the ith largest Eigen-value. Then the principal components of T & S\u0303 = SUSUTT are the same, which is shown in Lemma 1. Hence, a simple transformation of the source domain can be obtained by multiplying it with the Eigen-vectors of the source and target domains.\nLemma 1 If UP and UQ are the principal components of two datasets P and Q respectively, then the principal component of QUQUTP is UP .\nLet \u039bP and \u039bQ be two diagonal matrices whose diagonal elements are the eigen-values of P and Q respectively. The covariance matrices of P and Q can be written as UP\u039bPUTP and UQ\u039bQUTQ respectively. Now, the covariance matrix of Q\u0302 = (QUQU T P ) is given as:\nQ\u0302T Q\u0302 = UPU T QQ TQUQU T P\n= UPU T QUQ\u039bQU T QUQU T P = UP\u039bQU T P\nThis shows that the principle components of QUQUTP is UP . This can be followed by shifting of S\u0303 such that its mean is same as that of the target domain data. 2) Extension to the RKHS\nThe above formulation of DA performs linear transformation of the source domain data. In order to handle non-linear transformation of data, we extend the formulation to RKHS. If \u03a6(.) is a universal kernel function, then in kernel space the source and target domains are \u03a6(S) and \u03a6(T ) respectively. Let KSS and KTT be the Gram matrices of \u03a6(S) and \u03a6(T ) respectively (KSS = \u03a6(S)\u03a6(S)T , KST = \u03a6(S)\u03a6(T )T and KTT = \u03a6(T )\u03a6(T ) T ). Let, U\u03a6S and U \u03a6 T be the principle components of \u03a6(S) and \u03a6(T ) respectively. Also, let V \u03a6S and V \u03a6T be the eigen-vectors of KSS and KTT respectively. Then we can write,\nU\u03a6S = \u03a6(S) TV \u03a6S (7) U\u03a6T = \u03a6(T ) TV \u03a6T (8)\n5 Then the transformed source domain in RKHS is given by:\n\u03a6(S\u0303) = \u03a6(S)U\u03a6S U \u03a6 T T = KSSV \u03a6 S V \u03a6T T \u03a6(T ) (9)\nThe corresponding Gram matrices are given by:\nKS\u0303S\u0303 = \u03a6(S\u0303)\u03a6(S\u0303) T = KSSV \u03a6 S V \u03a6T T KTTV \u03a6 T V \u03a6T S KSS (10)\nKS\u0303T = \u03a6(S\u0303)\u03a6(T ) T = KSSV \u03a6 S V \u03a6T T KTT (11)\nOnce we obtain the Gram matrices, we need to modify them appropriately such that the mean of the transformed source domain is same as that of the target domain. If oS \u2208 RnS and oT \u2208 RnT be two vectors with all elements as 1/nS and 1/nT respectively, then the mean of transformed source domain and target domain in RKHS is given by oTS\u03a6(S\u0303) and oTT\u03a6(T ) respectively. Let the i\nth row of a gram matrix K is denoted by K(i, \u00b7) and let the jth column of K is denoted by K(\u00b7, j). Let K(i, j) denote the value corresponding to the ith row and jth column of K.\nIf K\u0302S\u0303S\u0303 represents the mean shifted Gram matrix, then each element of this matrix can be calculated as:\nK\u0302S\u0303S\u0303(i, j) = (\u03a6(S\u0303i)\u2212 o T S\u03a6(S\u0303) + o T T\u03a6(T ))\u00d7\n(\u03a6(S\u0303i)\u2212 oTS\u03a6(S\u0303) + oTT\u03a6(T ))T\n= KS\u0303S\u0303(i, j)\u2212KS\u0303S\u0303(i, \u00b7)oS +KS\u0303T (i, \u00b7)oT \u2212oTSKS\u0303S\u0303(\u00b7, j) + oTSKS\u0303S\u0303oS \u2212 oTSKS\u0303T oT +oTTKTS\u0303(\u00b7, j)\u2212 oTTKTS\u0303oS + oTTKTT oT (12)\nSimilarly, each element of the mean-shifted Gram matrix K\u0302S\u0303T can be calculated as:\nK\u0302S\u0303T (i, j) = (\u03a6(S\u0303i)\u2212 o T S\u03a6(S\u0303) + o T T\u03a6(T ))\u00d7 \u03a6(Tj)T\n=KS\u0303T (i, j)\u2212 o T SKS\u0303T (\u00b7, j) + oTTKTT (\u00b7, j) (13)\n3) Classification Once we obtain the Gram matrices K\u0302S\u0303S\u0303 and K\u0302S\u0303T , we can\ncalculate the overall Gram matrix\nK\u0302 = [ K\u0302S\u0303S\u0303 K\u0302S\u0303T K\u0302T S\u0303T KTT ] (14)\nWe can now calculate the Euclidean distance between any two instances (i and j) in RKHS, which is given by:\ndist(i, j) = K\u0302(i, i) + K\u0302(j, j)\u2212 2\u00d7 K\u0302(i, j) (15)\nHence, we can now use this distance matrix for classifying test samples using KNN-classifier. The unsupervised method of DA considers the Training set to be the source and the Test set (in a FR dataset) to be the target domains. A subset of a few samples from the target domain are used to estimate the distribution of the target domain (see table I in section IV). Transformation from source to target is estimated using eigen-analysis of the BOW-based features. The unsupervised method of DA, enhances the performance of FR algorithm on surveillance conditions."}, {"heading": "IV. PROPOSED FRAMEWORK", "text": "The proposed framework as shown in figure 1, has been designed for dealing with the problem of FR under low\nresolution and low contrast, using multiple kernel learning [6] and DA. The stages are discussed in the following:\nA. The pre-processing stage\nSince the faces appear with background and noise, preprocessing of the images is always necessary before they are passed into the feature extraction stage. This treatment of images is called the pre-processing stage as shown in figure 2 and described below step-wise:\n1) Robust Face Detection The gallery and the probe images are passed through the Chehra face tracker [4] which uses a cascade linear regression for discriminative face alignment. The incremental update of the cascade of linear regression is a very challenging task, since the results from one level have to propagated to the next. Due to this sequential nature of the training procedure, we refer to this method as Sequential Cascade of Linear Regression which is learned by a Monte-Carlo procedure [4]. This method deals with the problem of updating a discriminative facial deformable model, a problem that has not been thoroughly studied in the literature. In particular, the strategies to update a discriminative model that is trained by a cascade of regressors is handled by this method. Very efficient strategies to update the model is adopted and it is possible to automatically construct robust discriminative person and imaging condition specific models.\n2) Face Hallucination The probe samples are upsampled using a state-of-the-art Face hallucination method proposed by Felix et al. in [22]. This method of single image face hallucination is based on solo dictionary learning. The core idea of the proposed method [22] is to recast the superresolution task as a missing pixel problem, where the low resolution image is considered as its high-resolution counterpart with many pixels missing in a structured manner. A single dictionary is therefore sufficient for recovering the super-resolved image by filling the missing pixels.\n6 3) Degradation of Gallery samples The gallery samples are downsampled by two using simple\nbicubic interpolation method [18]. The gallery samples are then blurred using an estimated Gaussian blur, \u03c3. The \u03c3 is estimated using KL-Divergence [17], between the distributions of the downsampled gallery and the upsampled probe images. The distribution of the target domain is estimated using three samples per class but with no class labels (hence this is unsupervised). The graphs in figure 3 (a), (b) and (c) show the plots of KL-divergence with increasing values of Gaussian blur kernel, \u03c3, for the three different datasets, FR SURV, SCface and ChokePoint, respectively. The optimal \u03c3opt is obtained at 1.75, 1.7 and 1.2 for the three datasets and are recorded in table I. The gallery samples are degraded using this uniform blur \u03c3opt to obtain a degraded gallery.\nFig. 3: Plot of KL-Divergence between degraded gallery and probe images with different values of \u03c3 used for degradation, in case of (a) FR SURV [33], (b) SCface [20] and (c) ChokePoint [44] datasets. The optimal values of \u03c3 are obtained as: (a) 1.75, (b) 1.7 and (c) 1.2, respectively for the three datasets.\n4) Power Law Transformation To cope with the contrast degradation, we perform Power Law transformation [14] for contrast stretching the probe images. The transformation function used is:\nP (i, j) = k.C(i, j) \u03b3 (16)\nwhere, P (i, j) and C(i, j) denotes the gray-level pixel values of the input and output image. We use \u03b3 = 1.25, k = 1. Visual results as shown in figure 4 depicts contrast enhancement of the image for varying values of \u03b3. We set \u03b3 = 1.25 based on visual observation (empirical).\nFig. 4: (a) Probe images of three subjects; (b) Outputs of CHEHRA [4] process; Gamma-corrected (equation 16) images with (c) \u03b3 = 1.25; (d) \u03b3 = 1.5; (e) \u03b3 = 1.75; and (f) \u03b3 = 1.9, from (I) FR SURV [33], (II) SCFace [20] and (III) ChokePoint [44] datasets.\nFor probe samples, we use:\nProbeupsampled = Probecrop \u2191 v (17)\nProbetransformed(i, j) = Probeupsampled(i, j) \u03b3 (18)\nwhere, Probecrop denotes the cropped probe sample based on Chehra [4], \u2191 denotes upsampling of the image by a factor v mentioned to the right of it, Probecrop denotes upsampled image and Probetransformed denotes the image that will be used for feature-extraction. The \u03b3 is the parameter for gammacorrection of the image based on Power Law transformation which is given by equation 16.\nB. Feature Extraction\nThe feature extraction process in the proposed method is based on the set of features extracted from the degraded gallery and the enhanced probe face images. The set of features used in the methods proposed includes Local Binary Pattern (LBP), Eigen Faces, Fisher Faces, Gabor faces, Weber Faces, Bag of Words (BOW), Vector of Linearly Agregated Descriptors encoding based on Scale Invariant Feature Transform (VLADSIFT) [3], Fisher Vector encoding based on SIFT (FV-SIFT) [31].\n1) Eigen Faces This approach [40] of the detection and identification of human faces and describe a working, near-real-time face recognition system which tracks the face of a subject and then recognizes the person by comparing characteristics of the face to those of known individuals. The approach treats face recognition as a two-dimensional recognition problem, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. Face images are projected onto a feature space (face space) that best encodes the variation among known face images. The face space is defined by the eigenfaces, which are the eigenvectors of the set of faces; they do not necessarily correspond to isolated features such as eyes, ears, and noses.\n2) Fisher Faces This face recognition algorithm [9] is insensitive to large variation in lighting direction and facial expression. Taking a\n7 pattern classification approach, each pixel is considered in an image as a coordinate in a high-dimensional space. The images of a particular face, under varying illumination but fixed pose, lie in a 3D linear subspace of the high dimensional image space if the face is a Lambertian surface without shadowing. However, since faces are not truly Lambertian surfaces and do indeed produce self-shadowing, images will deviate from this linear subspace. Rather than explicitly modeling this deviation, the image is projected into a subspace in a manner which discounts those regions of the face with large deviation. The projection method is based on Fishers Linear Discriminant and produces well separated classes in a low-dimensional subspace, even under severe variation in lighting and facial expressions.\n3) Gabor Faces The Gabor Feature Classifier (GFC) method [26] employs an enhanced Fisher discrimination model on an augmented Gabor feature vector; which is derived from the Gabor wavelet transformation office images. For the three datasets used for experimentation, table II gives the values of the parameters used, where v represents the different scales used, n\u00b5 is the number of orientations and \u00b5 is the orientation. Parameter \u03c3, which determines the ratio of the Gaussian window width to wavelength, is set to 2\u03c0, kmax is the wave-vector set to \u03c0 and f the spatial frequency set to \u221a 2. The Gabor wavelets, whose kernels are similar to the 20 receptive field profiles of the mammalian cortical simple cells, exhibit desirable characteristics of spatial locality and orientation selectivity. As a result, the Gabor transformed face images produce salient local and discriminating features that are suitable for face recognition.\n4) Weber Faces Webers law suggests that for a stimulus, the ratio between the smallest perceptual change and the background is a constant, which implies stimuli are perceived not in absolute terms but in relative terms. Inspired from this, a novel illumination insensitive representation of face images is exploited and analyzed under varying illuminations via a ratio image, called Weber-face, [42] where a ratio between local intensity variation and the background is computed.\n5) Local Binary Pattern Local binary patterns (LBP) [1] is a type of feature used for classification in computer vision. LBP is the particular case of the Texture Spectrum model proposed in 1990. The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor. The procedure consists of using the texture descriptor to build several local\ndescriptions of the face and combining them into a global description. The operator assigns a label to every pixel of an image by thresholding the 3X3-neighborhood of each pixel with the center pixel value and considering the result as a binary number. Then, the histogram of the labels can be used as a texture descriptor.\n6) Bag-of-Words (BOW) The feature extraction method proposed here is based on the BOW [16] based on Dense-SIFT features. The dense-SIFT features are calculated with a single-pixel shift of the window over the face. The words used in processing are local image features. They may be constructed around interest points such as scale-space extrema (e.g. SIFT keypoints [28]), or simply on windows extracted from the image at regular positions and various scales. The features can be image patches, histograms of gradient orientations or color histograms. As these features are sensitive to noise and are represented in high dimension spaces, they are not directly used as words, but are categorized using a vector quantization technique such as k-means. The output of this discretization is the dictionary.\n7) Fisher Vector Encoding on dense-SIFT features (FVSIFT)\nThis encoding [31] serves a similar purposes: summarizing in a vectorial statistic a number of local feature descriptors (e.g. SIFT [28]). Similarly to bag of visual words, they assign local descriptor to elements in a visual dictionary, obtained with a Gaussian Mixture Models for Fisher Vectors. However, rather than storing visual word occurrences only, the representation stores a statistics of the difference between dictionary elements and pooled local features. The Fisher encoding uses GMM to construct a visual word dictionary.\n8) VLAD encoding on dense-SIFT features (VLAD-SIFT) The Vector of Linearly Agregated Descriptors [3] is similar to Fisher vectors, but (i) it does not store second-order information about the features and (ii) it typically use KMeans instead of GMMs to generate the feature vocabulary (although the latter is also an option). VLAD is constructed as follows: regions are extracted from an image using an affine invariant detector, and described using the 128 dimensional SIFT descriptor. Each descriptor is then assigned to the closest cluster of a vocabulary of size k (where, k is typically 64 or 256, so that clusters are quite coarse). For each of the k clusters, the residuals (vector differences between descriptors and cluster centers) are accumulated, and the k - 128 dimensional sums of residuals are concatenated into a single k\u00d7128 dimensional descriptor.\nC. Kernel Selection by Multiple Kernel Learning\nIn support vector machine (SVM), selecting the kernel function and its parameters are important issues during training. Generally, to select the best performing kernel among the set of kernel function (like Linear, RBF, etc.), a cross validation procedure is used. In recent years, several MKL techniques have been proposed, where instead of selecting one specific kernel function and its corresponding parameters, multiple kernels are learned. MKL has two main advantages: (a) Different kernels correspond to different notions of similarity\n8\nand instead of finding which works best, a MKL learning method helps to pick the best kernel or a combination of kernels; and (b) Different kernels may use inputs coming from different representations, possibly from different sources. In such cases, combining kernels is one possible way to combine sources of multiple information. The training phase (see figure 5) is described in section IV-D.\nIn this paper, we introduce a novel technique based on MKL, termed MFKL (Multi-feature Kernel Learning), for selecting the optimal feature-kernel combination for classification. The MFKL method determines the optimal weights for the different kernels used for each feature category. All the features are extracted individually from each of the gallery face images and passed into the MFKL module for optimal kernel selection for each feature. The ordered pair of the feature and kernel < F iH ,Ki > is selected and stored for further processing. We assume that we have p features and their corresponding feature space be represented by X f , where f \u2208 {1, .., p}. Data points (xfi , yi) are given, where x f i \u2208 X f represents a feature vector in a particular feature space X f , and yi \u2208 {\u22121, 1}, \u2200i \u2208 1, ..., n, are the class-labels. For each feature space X f , the choice is one out of m kernels Kfj \u2208 Rn\u00d7n.\nWe consider X = \u22c3p f=1 X f and X \u2208 Rl, where l = l1+...+lp, such that x f i \u2208 Rlf , where lf is the dimensionality of the feature vector f . This problem follows a similar formulation as described in the classification algorithm, \u201dsupport kernel machine\u201d, as proposed by Bach et al. [6], and also discussed in section III-B.\nThe primal problem is given by equation 2 and its dual by equation 3, with the same KKT conditions as mentioned in section III-B. In RKHS, we assume the embeddings of the data points xi in each feature space via a mapping \u03c6 : X f \u2192 Rc. We also assume that \u03c6(x) has m distinct block components \u03c6(x) = (\u03c61(x), ..., \u03c6m(x)). Following the usual recipe for kernel methods, we assume that this embedding is performed implicitly, by specifying the inner product in Rc using a kernel function, which in this case is the sum of individual kernel functions on each block (subspace) component:\nkf (x f i , x f j ) = \u03c6(x f i ) T\u03c6(xfj ) = m\u2211 s=1 \u03c6s(x f i ) T\u03c6s(x f j )\n= m\u2211 s=1 ks(x f i , x f j )\n(19)\nNow, in the feature space, X , we have\nk(xi, xj) = p\u2211 f=1 \u03b2fkf (x f i , x f j ) (20)\nWe now kernelize the problem described in equation 2 using this kernel function. In particular, we consider the dual of the problem in equation 2 and substitute the kernel function for the inner products in the equation 3 with the constraint in a particular feature space, rather than over the whole space, as (\u03b1TD(y)Kfj D(y)\u03b1) 1 2 \u2264 dj\u03b3,\u2200j, f , where Kfj is the jth Gram matrix of the points {xfi } corresponding to the jth kernel, for the f -th feature formed using k(xi, xj). These constraints are derived from equations 3 and 5, which lead to the simultaneous selection of feature and its corresponding non-zero kernels, based on the objective function (similar to equation 6), formulated as:\nmin maxj 1\n2d2j \u03b1TD(y)Kfj D(y)\u03b1\u2212 \u03b1 T e\nw.r.t. \u03b1 \u2208 Rn\ns.t. 0 \u2264 \u03b1 \u2264 C,\u03b1T y = 0\n(21)\nSince the sparsity of the weights of the kernels is ensured by KKT conditions, the non-zero kernels are used for classification in the testing phase. Let Jfj (\u03b1) denote\n1 2d2j \u03b1TD(y)Kfj D(y)\u03b1\u2212 \u03b1T e (see equation 21) and Jf (\u03b1) = maxjJ f j (\u03b1). Minimization of Jf (\u03b1) now reduces to an convex optimization problem, as Jf (\u03b1) is also a non-differentiable convex function subject to linear constraints. Our global objective function is J(\u03b1) = \u22c3p f=1 Jf (\u03b1). Union of convex functions is not necessarily convex. Hence, a subgradient method [23] is used to solve each of these convex optimization sub-problems, Jf (\u03b1), and finally the union of these are used to obtain a global solution. Sparse solutions ensure that most of the kernel weights are negligible (go near to zero) and a very few non-zero kernel weights remain for each feature.\nIn the proposed work, we take into account a set of kernel functions for the MKL method. The set of kernels consists of Linear, Polynomial, Gaussian, RBF, Chi-square and RBF + Chi-square. The equations of each of these kernels are tabulated in Table III.\n9 D. The Training Phase\nIn this phase of our proposed framework as shown in figure 5, we have a set of feature F and a set of kernels K pairings.\nF ={LBP,EigenFaces, F isherFaces,Gaborfaces, WeberFaces, V LAD \u2212 SIFT, FV \u2212 SIFT,BOW}\n(22)\nwhere each Fi \u2208 F is the feature extracted from a face image; and\nK ={Linear, Polynomial,Gaussian,RBF, Chi\u2212 square,RBF + Chi\u2212 square}\n(23)\nwhere each Ki \u2208 K is a kernel function for the projection in the RKHS, Hi.\nThe combination of F and K is passed into the MKL module to obtain the set of optimized pair of {Fj ,Kj} using the kernel selection method described in section IV-C. Based on the best feature-kernel pair obtained, the feature vector is projected into a higher dimensional space of RKHS. The training in DA is performed to obtain the final model parameters along with the feature-kernel pairs. The number of probe samples used as targets for DA is mentioned in the table I."}, {"heading": "E. Classification based on K-Nearest Neighbor Classifier", "text": "In the testing phase as shown in figure 6, a query lowresolution face image is first pre-processed based on the preprocessing techniques described in section IV-A. Features are extracted from the pre-processed probe and passed into the DA module (RKHS), as shown in figure 6. The process of kernel selection corresponding to a feature is based on the MFKL technique proposed in the kernel selection stage. The overall gram matrix is created for each of these features. A majority voting is used based on the Nearest neighbor classification for the probe samples. The winning class ID is selected as the best match."}, {"heading": "V. DETAILS OF SURVEILLANCE FACE DATABASES USED", "text": "For the experimentation purpose we have used three realworld surveillance face datasets, which are discussed below. In all three real-world datasets the gallery samples are taken in laboratory conditions, while for probes two out of the three datasets are shot indoor, while one is shot outdoor.\nA. FR SURV [33] FR SURV is a challenging database for FR, because the gallery and the probe images are taken at different resolutions with two different cameras. The gallery samples, taken indoor with high resolution camera, have a resolution of 250 \u00d7 250 pixels, while the probe samples, taken by surveillance camera, have a very low resolution of 45\u00d745 pixels. The probe samples are taken at a distance of 50-100 meters outdoor. Using Chehra [4] on both the gallery and probe samples, we produce cropped face regions at an average of 150X150 pixels and 33 \u00d7 33 pixels respectively. The database consists 51 subjects with 20 samples per class. Figure 7 shows two samples of the gallery images and the their respective probe image (cropped using Chehra [4]).\nB. SCFace [20] SCface is also a challenging database for FR as the images were taken at different surveillance conditions. The database has a huge collection of static images of 130 different people. Images were captured by five different video surveillance cameras (cam1, cam2, cam3, cam4, cam5). Two cameras were also used in the night vision mode (cam6 and cam7). All these images were collected indoor with varying quality and resolution levels at three different distances. The training set consists of nine images: one frontal and four each in left and right rotations. The dataset has an image taken from an infra-red camera(cam8). Figure 8 shows the images for a single person in the dataset. The gallery has images of size 2048\u00d73072 pixels which are cropped by VJFD to an average of 800\u00d7 600 pixels. The probe images at Distance 1,2 and 3 has a resolution of 75\u00d7 100, 108\u00d7 144 and 168\u00d7 224 pixels respectively which are cropped by Chehra [4] to an average of 40\u00d7 40, 60\u00d7 60 and 100\u00d7 100 pixels respectively. We do not use the cam6 and cam7 as they are IR images.\nC. ChokePoint [44] Wong et al. [44] collected a new video dataset, termed ChokePoint, designed for experiments in person identifica-\n10\ntion/verification under real-world surveillance conditions using existing technologies. An array of three cameras was placed above several portals (natural choke points in terms of pedestrian traffic) to capture subjects walking through each portal in a natural way (see figures 9 and 10).\nThe dataset consists of 25 subjects (19 male and 6 female) in portal 1 and 29 subjects (23 male and 6 female) in portal 2. In total, it consists of 48 video sequences and 64,204 face\nimages. Each sequence was named according to the recording conditions (eg. P2E S1 C3) where P, S, and C stand for portal, sequence and camera, respectively. E and L indicate subjects either entering or leaving the portal. The numbers indicate the respective portal, sequence and camera label. For example, P2L S1 C3 indicates that the recording was done in Portal 2, with people leaving the portal, and captured by camera 3 in the first recorded sequence. The ChokePoint dataset does not have variation in resolution. But the difference lies in the different cameras used for capturing the image due to different camera parameters and the illumination variations. This feature makes it suitable to be tackled with DA.\nFor experimentation, we consider the images obtained from camera, C1 as the Gallery set, since it contains maximum frontal images with better lighting conditions. The other cameras are considered as probe images. We do not consider the sequence S5, as it contains crowded scenario. Since the images obtained from C1 are crisp and have better illumination conditions than C2 and C3, the gallery and probe is passed through all the pre-processing stages, except the face hallucination stage, as the resolution of the images obtained from all these cameras are similar.\nD. Intermediate results of face processing\nThe face images go through the several stages of preprocessing, as described in section IV-A. An example to illustrate the pre-processing stages is shown in figure 11, for the SCFace [20] dataset. The top row illustrates the effect of pre-processing on the gallery images, while the bottom row illustrates the effect of pre-processing on the probe images. Figure 11(a) shows the original images available in the dataset, while figure 11(b) shows the landmark points detected by the Chehra [4] on the face images. Based on these landmark points, we obtain a tightly cropped face image as shown in figure 11(c). Figure 11(d) shows the result of downsampling of the gallery images and upsampling of the probe images by Face Hallucination technique, as discussed in section IV-A2. The downsampled gallery images are blurred using a Gaussian kernel to obtain the degraded gallery images while the upsampled probe images are passed through Power law transformation to obtain moderately enhanced probe images as shown in figure 11(e). Face hallucination is not applied on the probe samples in the ChokePoint [44] dataset, as the resolution of the gallery and probe samples are similar in the dataset.\nFigure 12 shows the examples of the degraded gallery (in the top row) and the enhanced probe (in the bottom row) images of a single subject from the three surveillance datasets, used in our experimentation. These degraded gallery and the enhanced probe images are used for feature extraction. The original gallery and probes are also shown with similar spatial resolution to illustrate the efficiency of the pre-processing stage (ignore resolution which is different)."}, {"heading": "VI. EXPERIMENTAL RESULTS", "text": "Rigorous experimentation is carried on three real-world datasets; SCface [20], FR SURV [33], and ChokePoint [44]\n11\nThe proposed methods are compared with several other recent state-of-the-art methods and the results are reported in table IV using Rank-1 Recognition Rate.\nIn case of Naive combination, the source and the target domain samples are used without transformation for training and domain samples are used as probes. In EDA1 method, proposed by Banerjee et al. [8], DA processing based on an eigenvector based transformation, whose extension in the RKHS is termed as KDA1. Rudrani et al. [33] (COMP DEG) tries to bridge the gap between the gallery and the probe samples by projecting them both into a lower dimensional subspace determined by the principal components of the\nfeature vectors obtained from each face. This paper also acts as the source for the dataset, FR SURV [33]. Multi-dimensional scaling (MDS) proposed by Biswas et al. in [10] projects both the gallery and the probe samples into a common subspace for classification. The methods proposed by Gopalan et al. [19] and Kliep [39] are two DA based techniques used for object classification accross domains. We can observe that the method proposed in this paper have outperformed (our results are given in bold, in Table IV) all the other competing methods by a considerable margin. The complexity of the datasets is also observed by the results of FR, which are all moderately low in many cases.\nResults are also reported using ROC (for identification) and CMC (for verification) measures, as shown in figures 13 - 15, for the three datasets respectively. The plot drawn in red show the performance of our proposed method. We can observe that the red curves in all the plots outperform all other competing methods. On an average, the second best performance is given by the naive approach since the MFKL is also incorporated into it, while the method proposed by Gopalan et al. [19] performs generally the worst.\nAs we look closer into the the table IV row-wise, we can easily observe that the FR SURV datasets has the least accuracy. This is an indication that there is still further scope of improvement in this field. Also, it shows that the database is quite tough to handle. As we can see that the gallery samples in FR SURV are all taken in Indoor laboratory conditions and the probe samples are taken in Outdoor conditions which results in the large complexity of the database. Since FR SURV is an outdoor dataset, we can see the accuracy of FR is less than that of the ChokePoint dataset, which is the easiest to handle among the three. The SCface and the ChokePoint datasets are\n12\ntwo indoor surveillance datasets. Experiments are done in both identification and verification mode. There is still scope of improvement to find a more effective effective transformation such that the distribution of the features of the gallery and the probe become similar. The other two datasets have both the gallery and the probe taken in Indoor scenario. The effectiveness of the DA used in this paper is clearly visible in the results of EDA1 and KDA1. The non-linear transformations in KDA1 proves to be more effective which motivates this paper to concentrate mostly on the DA in RKHS. The effectiveness of the MKL based method is evident when we try to focus on the Naive combination results. It is very competitive in all the three datasets. The Naive combination is the complete Framework without the DA module, incorporating also the MKL process. When these two powerful tools are combined, our proposed method outperforms all other methods by a considerable margin."}, {"heading": "VII. CONCLUSION", "text": "An efficient method to tackle the problem of low-contrast and low-resolution in face recognition under surveillance scenario is proposed in this paper. The method proposes a novel kernel selection method using MFKL to obtain an optimal pairing of feature and kernel for eigen-domain transformation based unsupervised DA in the RKHS. The three metrics used to compare the performance of our proposed method with the recent state-of-the-art techniques, show a great deal of superiority of our method than the other techniques, using three real-world surveillance face datasets."}], "references": [{"title": "Face description with local binary patterns: Application to face recognition", "author": ["T. Ahonen", "A. Hadid", "M. Pietikainen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(12):2037\u20132041,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Simple mkl", "author": ["R. Alain", "R. Francis", "C. Phane", "S. Yves"], "venue": "Journal of Machine Learning Research, 9:2491\u20132521,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "All about vlad", "author": ["R. Arandjelovic", "A. Zisserman"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1578\u20131585,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Incremental face alignment in the wild", "author": ["A. Asthana", "S. Zafeiriou", "S. Cheng", "M. Pantic"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Tabula rasa: Model transfer for object category detection", "author": ["Y. Aytar", "A. Zisserman"], "venue": "IEEE International Conference on Computer Vision, pages 2252\u20132259. IEEE,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple kernel learning, conic duality, and the smo algorithm", "author": ["F.R. Bach", "G.R. Lanckriet", "M.I. Jordan"], "venue": "Proceedings of the twenty-first international conference on Machine learning, page 6. ACM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Unsupervised domain adaptation by domain invariant projection", "author": ["M. Baktashmotlagh", "M.T. Harandi", "B.C. Lovell", "M. Salzmann"], "venue": "International Conference on Computer Vision, pages 769\u2013776. IEEE,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Face recognition in surveillance conditions with bag-of-words, using unsupervised domain adaptation", "author": ["S. Banerjee", "S. Samanta", "S. Das"], "venue": "Proceedings of Indian Conference on Computer Vision Graphics and Image Processing, page 50. ACM,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Eigenfaces vs", "author": ["P.N. Belhumeur", "J.P. Hespanha", "D.J. Kriegman"], "venue": "fisherfaces: Recognition using class specific linear projection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(7):711\u2013 720,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Multidimensional scaling for matching low-resolution face images", "author": ["S. Biswas", "K.W. Bowyer", "P.J. Flynn"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(10):2019\u20132030,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Transferring naive bayes classifiers for text classification", "author": ["W. Dai", "G.-R. Xue", "Q. Yang", "Y. Yu"], "venue": "Proceedings of the national conference on artificial intelligence, volume 22, page 540. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Domain adaptation from multiple sources: A domain-dependent regularization approach", "author": ["L. Duan", "D. Xu", "I.W. Tsang"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, 23(3):504\u2013518,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "J contractive matrix functions, reproducing kernel Hilbert spaces and interpolation, volume 71", "author": ["H. Dym"], "venue": "American Mathematical Soc.,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1989}, {"title": "Blind inverse gamma correction", "author": ["H. Farid"], "venue": "IEEE Transactions on Image Processing, 10(10):1428\u20131433,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Unsupervised visual domain adaptation using subspace alignment", "author": ["B. Fernando", "A. Habrard", "M. Sebban", "T. Tuytelaars"], "venue": "International Conference on Computer Vision, pages 2960\u20132967. IEEE,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "A visual bag of words method for interactive qualitative localization and mapping", "author": ["D. Filliat"], "venue": "IEEE International Conference on Robotics and Automation, pages 3921\u20133926. IEEE,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "An efficient image similarity measure based on approximations of kl-divergence between two gaussian mixtures", "author": ["J. Goldberger", "S. Gordon", "H. Greenspan"], "venue": "Internation Conference on Computer Vision, pages 487\u2013493. IEEE,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Digital Image Processing", "author": ["R.C. Gonzalez", "R.E. Woods"], "venue": "Addison- Wesley, Reading, MA,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1992}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "Internation Conference on Computer Vision, pages 999\u20131006,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Scface\u2013surveillance cameras face database", "author": ["M. Grgic", "K. Delac", "S. Grgic"], "venue": "Multimedia tools and applications, 51(3):863\u2013879,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Instance weighting for domain adaptation in nlp", "author": ["J. Jiang", "C. Zhai"], "venue": "Association for Computer Linguistics, volume 7, pages 264\u2013271,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Single face image super-resolution via solo dictionary learning", "author": ["F. Juefei-Xu", "M. Savvides"], "venue": "IEEE International Conference on Image Processing (ICIP), volume 2,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Convergence of a generalized subgradient method for nondifferentiable convex optimization", "author": ["S. Kim", "H. Ahn"], "venue": "Mathematical Programming, 50(1-3):75\u201380,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["G.R. Lanckriet", "N. Cristianini", "P. Bartlett", "L.E. Ghaoui", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research, 5:27\u201372,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "A statistical framework for genomic data fusion", "author": ["G.R. Lanckriet", "T. De Bie", "N. Cristianini", "M.I. Jordan", "W.S. Noble"], "venue": "Bioinformatics, 20(16):2626\u20132635,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition", "author": ["C. Liu", "H. Wechsler"], "venue": "IEEE Transactions on Image processing, 11(4):467\u2013476,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "Applications of second-order cone programming", "author": ["M.S. Lobo", "L. Vandenberghe", "S. Boyd", "H. Lebret"], "venue": "Linear algebra and its applications, 284(1):193\u2013228,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1998}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision, 60(2):91\u2013110,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "IEEE Transactions on Neural Networks, 22(2):199\u2013210,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning algorithms for domain adaptation", "author": ["M.A. Pathak", "E.H. Nyberg"], "venue": "Advances in Machine Learning, pages 293\u2013307. Springer,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Improving the fisher kernel for large-scale image classification", "author": ["F. Perronnin", "J. S\u00e1nchez", "T. Mensink"], "venue": "European Conference on Computer Vision, pages 143\u2013156. Springer,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Coupled kernel embedding for low-resolution face image recognition", "author": ["C.-X. Ren", "D.-Q. Dai", "H. Yan"], "venue": "IEEE Transactions on Image Processing, 21(8):3770\u20133783,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Face recognition on low quality surveillance images, by compensating degradation", "author": ["S. Rudrani", "S. Das"], "venue": "ICIAR, pages 212\u2013221. LNCS, Springer,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised domain adaptation using eigenanalysis in kernel space for categorisation tasks", "author": ["S. Samanta", "S. Das"], "venue": "Image Processing, IET, 9(11):925\u2013930,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling sequential domain shift through estimation of optimal sub-spaces for categorization", "author": ["S. Samanta", "T. Selvan", "S. Das"], "venue": "British Machine Vision Conference,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple kernel learning for sparse representation-based classification", "author": ["A. Shrivastava", "V.M. Patel", "R. Chellappa"], "venue": "IEEE Transactions on Image Processing, 23(7):3013\u20133024,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "The Journal of Machine Learning Research, 7:1531\u20131565,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Direct importance estimation with model selection and its application to covariate shift adaptation", "author": ["M. Sugiyama", "S. Nakajima", "H. Kashima", "P.V. Buenau", "M. Kawanabe"], "venue": "Advances in neural information processing systems, pages 1433\u20131440,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Direct importance estimation with model selection and its application to covariate shift adaptation", "author": ["M. Sugiyama", "S. Nakajima", "H. Kashima", "P. von B\u00fcnau", "M. Kawanabe"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "Eigenfaces for recognition", "author": ["M. Turk", "A. Pentland"], "venue": "Journal of cognitive neuroscience, 3(1):71\u201386,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1991}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M.J. Jones"], "venue": "International Journal of Computer Vision, 57(2):137\u2013154,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2004}, {"title": "Illumination normalization based on weber\u2019s law with application to face recognition", "author": ["B. Wang", "W. Li", "W. Yang", "Q. Liao"], "venue": "Signal Processing Letters, IEEE, 18(8):462\u2013465,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Heterogeneous domain adaptation using manifold alignment", "author": ["C. Wang", "S. Mahadevan"], "venue": "IJCAI Proceedings-International Joint Conference on Artificial Intelligence, volume 22, page 1541,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Patch-based probabilistic image quality assessment for face selection and improved video-based face recognition", "author": ["Y. Wong", "S. Chen", "S. Mau", "C. Sanderson", "B.C. Lovell"], "venue": "IEEE Biometrics Workshop, Computer Vision and Pattern Recognition (CVPR) Workshops, pages 81\u201388. IEEE, June", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2011}, {"title": "Simple and efficient multiple kernel learning by group lasso", "author": ["Z. Xu", "R. Jin", "H. Yang", "I. King", "M.R. Lyu"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), pages 1175\u2013 1182,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Cross-domain video concept detection using adaptive svms", "author": ["J. Yang", "R. Yan", "A.G. Hauptmann"], "venue": "Proceedings of the 15th international conference on Multimedia, pages 188\u2013197. ACM,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2007}, {"title": "Feature-based human face detection", "author": ["K.C. Yow", "R. Cipolla"], "venue": "Image and vision computing, 15(9):713\u2013735,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1997}, {"title": "Face detection, pose estimation, and landmark localization in the wild", "author": ["X. Zhu", "D. Ramanan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2879\u20132886. IEEE,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Very low resolution face recognition problem", "author": ["W.W. Zou", "P.C. Yuen"], "venue": "IEEE Transactions on Image Processing, 21(1):327\u2013340,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 32, "context": "Rigorous experimentation has been performed on three real-world surveillance face datasets : FR SURV [33], SCface [20] and ChokePoint [44].", "startOffset": 101, "endOffset": 105}, {"referenceID": 19, "context": "Rigorous experimentation has been performed on three real-world surveillance face datasets : FR SURV [33], SCface [20] and ChokePoint [44].", "startOffset": 114, "endOffset": 118}, {"referenceID": 43, "context": "Rigorous experimentation has been performed on three real-world surveillance face datasets : FR SURV [33], SCface [20] and ChokePoint [44].", "startOffset": 134, "endOffset": 138}, {"referenceID": 19, "context": "three real-world surveillance datasets (SCFace [20], FR SURV [33], ChokePoint [44]) exhibit the superiority of our proposed method of kernel selection by MFKL, with DA in Reproducing Kernel Hilbert Space (RKHS) [13].", "startOffset": 47, "endOffset": 51}, {"referenceID": 32, "context": "three real-world surveillance datasets (SCFace [20], FR SURV [33], ChokePoint [44]) exhibit the superiority of our proposed method of kernel selection by MFKL, with DA in Reproducing Kernel Hilbert Space (RKHS) [13].", "startOffset": 61, "endOffset": 65}, {"referenceID": 43, "context": "three real-world surveillance datasets (SCFace [20], FR SURV [33], ChokePoint [44]) exhibit the superiority of our proposed method of kernel selection by MFKL, with DA in Reproducing Kernel Hilbert Space (RKHS) [13].", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": "three real-world surveillance datasets (SCFace [20], FR SURV [33], ChokePoint [44]) exhibit the superiority of our proposed method of kernel selection by MFKL, with DA in Reproducing Kernel Hilbert Space (RKHS) [13].", "startOffset": 211, "endOffset": 215}, {"referenceID": 40, "context": "in [41].", "startOffset": 3, "endOffset": 7}, {"referenceID": 40, "context": "The algorithm proposed in [41] is based on a simple and efficient classifier which is built using the AdaBoost learning algorithm to select a small number of critical visual features from a very large set of potential features.", "startOffset": 26, "endOffset": 30}, {"referenceID": 47, "context": "[48] presents a unified model for face detection, pose estimation, and landmark estimation in real-world, cluttered images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[47] also proposed a feature-based algorithm for detecting faces that is sufficiently generic and is also easily extensible to cope with more demanding variations of the imaging conditions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Our proposed method uses a set of 49 fiducial landmark points detected by the Chehra face detector [4].", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "[6] proposed the sequential minimal optimization (SMO) techniques that are essential in large-scale implementations of the SVM for large number of kernels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 35, "context": "[36] proposed a multiple kernel learning (MKL) algorithm that is based on the sparse representation-based classification (SRC) method efficiently representing the nonlinearities in the high-dimensional feature space, based on the kernel alignment criteria.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] considered conic combinations of kernel matrices for classification, leading to a convex quadratically constrained quadratic program.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard SVM implementations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] generalize the formulation and method to a larger class of problems, including regression and one-class classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "The method proposed in our paper uses Structured MKL [45], which is a modified version of simpleMKL [2], through a primal formulation involving a weighted L2-norm regularization.", "startOffset": 53, "endOffset": 57}, {"referenceID": 1, "context": "The method proposed in our paper uses Structured MKL [45], which is a modified version of simpleMKL [2], through a primal formulation involving a weighted L2-norm regularization.", "startOffset": 100, "endOffset": 103}, {"referenceID": 37, "context": "Some of the works where instance weights of source domain have been calculated are [38], [11], [21], [30].", "startOffset": 83, "endOffset": 87}, {"referenceID": 10, "context": "Some of the works where instance weights of source domain have been calculated are [38], [11], [21], [30].", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "Some of the works where instance weights of source domain have been calculated are [38], [11], [21], [30].", "startOffset": 95, "endOffset": 99}, {"referenceID": 29, "context": "Some of the works where instance weights of source domain have been calculated are [38], [11], [21], [30].", "startOffset": 101, "endOffset": 105}, {"referenceID": 37, "context": "[38] proposed a method to calculate weights for instances in source domain, by using a convex optimization framework which minimizes the KLdivergence between the source and the target domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "There have been some work for building robust classifiers for domain adaptation [5], [46], [12].", "startOffset": 80, "endOffset": 83}, {"referenceID": 45, "context": "There have been some work for building robust classifiers for domain adaptation [5], [46], [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 11, "context": "There have been some work for building robust classifiers for domain adaptation [5], [46], [12].", "startOffset": 91, "endOffset": 95}, {"referenceID": 45, "context": "[46] has proposed a method to calculate weights for each instances in source domain, which are used to effectively retrain a pre-learned SVM for target domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12], learns a robust decision function for labeling the instances in target domain, by leveraging a set of base classifies learned on multiple source domains.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] proposed transfer component analysis (TCA), which minimizes the disparity of distribution by considering the difference of means between two domains and it also preserves local geometry of underlying manifold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "The study of subspaces spanned by the input data have found important applications in computer vision tasks [40].", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "Subspace based modeling have also been widely used for the task of visual domain adaptation [15], [7].", "startOffset": 92, "endOffset": 96}, {"referenceID": 6, "context": "Subspace based modeling have also been widely used for the task of visual domain adaptation [15], [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 14, "context": "[15] has calculated a subspace using eigen-vectors of two domains such that the basis vectors of transformed source and target domains are aligned.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] has considered the manifold of each domain and estimated a latent space, where the manifolds of both the domains are similar to each other.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "in [35] describe a new method of unsupervised domain adaptation", "startOffset": 3, "endOffset": 7}, {"referenceID": 33, "context": "A new technique of unsupervised domain adaptation based on eigenanalysis in kernel space, for the purpose of categorisation tasks, had also been proposed in [34].", "startOffset": 157, "endOffset": 161}, {"referenceID": 12, "context": "They extend this idea to the RKHS [13], which enables to deal with non-linear transformation of source domain.", "startOffset": 34, "endOffset": 38}, {"referenceID": 48, "context": "[49] proposed a super-resolution approach to increase the recognition performance for very low-resolution face images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] proposed a matching algorithm through using Multidimensional Scaling (MDS).", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] proposed the Coupled Kernel Embedding approach, where they map the low and high resolution face images onto different kernel spaces and then transform them to a learned subspace for recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "in [33] proposed an approach with the combination of partial restoration (using super-resolution) of probe samples and degradation of gallery samples.", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "The authors also proposed a outdoor surveillance dataset, FR SURV [33] for evaluating their approach.", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "In our previous work proposed in [8], we aim to bridge the gap of resolution and contrast using super-resolution and contrast stretching on the probe samples and degrading the gallery samples by downsampling the gallery samples and introducing a Gaussian blur to the downsampled images.", "startOffset": 33, "endOffset": 36}, {"referenceID": 12, "context": "It actually corresponds to the most natural extension of sparsity to Reproducing Kernel Hilbert Spaces [13].", "startOffset": 103, "endOffset": 107}, {"referenceID": 23, "context": "[24] show that this setup yields the following optimization problem:", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6], is exactly the dual of the problem defined in equation 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "The problem posed in equation 2 is treated as a secondorder cone program (SOCP) [27], which yields the following dual:", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "The minimization task described in equation 2 is kernelized [13] using this kernel function (equation 4).", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "formation [8]", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "We extend this idea of transformation of source domain in Reproducing Kernel Hilbert Space (RKHS) [13] to handle non-linear transformation of data, when necessary.", "startOffset": 98, "endOffset": 102}, {"referenceID": 7, "context": "In the following sub-sections, we give the mathematical details necessary for the unsupervised method of domain adaptation (as in our earlier work in [8]).", "startOffset": 150, "endOffset": 153}, {"referenceID": 5, "context": "The proposed framework as shown in figure 1, has been designed for dealing with the problem of FR under low resolution and low contrast, using multiple kernel learning [6] and DA.", "startOffset": 168, "endOffset": 171}, {"referenceID": 3, "context": "The gallery and the probe images are passed through the Chehra face tracker [4] which uses a cascade linear regression for discriminative face alignment.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "procedure [4].", "startOffset": 10, "endOffset": 13}, {"referenceID": 21, "context": "in [22].", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "The core idea of the proposed method [22] is to recast the superresolution task as a missing pixel problem, where the low resolution image is considered as its high-resolution counterpart with many pixels missing in a structured manner.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "The gallery samples are downsampled by two using simple bicubic interpolation method [18].", "startOffset": 85, "endOffset": 89}, {"referenceID": 16, "context": "The \u03c3 is estimated using KL-Divergence [17], between the distributions of the downsampled gallery and the upsampled probe images.", "startOffset": 39, "endOffset": 43}, {"referenceID": 32, "context": "3: Plot of KL-Divergence between degraded gallery and probe images with different values of \u03c3 used for degradation, in case of (a) FR SURV [33], (b) SCface [20] and (c) ChokePoint [44] datasets.", "startOffset": 139, "endOffset": 143}, {"referenceID": 19, "context": "3: Plot of KL-Divergence between degraded gallery and probe images with different values of \u03c3 used for degradation, in case of (a) FR SURV [33], (b) SCface [20] and (c) ChokePoint [44] datasets.", "startOffset": 156, "endOffset": 160}, {"referenceID": 43, "context": "3: Plot of KL-Divergence between degraded gallery and probe images with different values of \u03c3 used for degradation, in case of (a) FR SURV [33], (b) SCface [20] and (c) ChokePoint [44] datasets.", "startOffset": 180, "endOffset": 184}, {"referenceID": 19, "context": "7 [20] for 30 subjects FR SURV 5 3 1.", "startOffset": 2, "endOffset": 6}, {"referenceID": 32, "context": "75 [33] for 20 subjects ChokePoint 5 6 1.", "startOffset": 3, "endOffset": 7}, {"referenceID": 43, "context": "2 [44] for 20 subjects", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "To cope with the contrast degradation, we perform Power Law transformation [14] for contrast stretching the probe images.", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "4: (a) Probe images of three subjects; (b) Outputs of CHEHRA [4] process; Gamma-corrected (equation 16) images with (c) \u03b3 = 1.", "startOffset": 61, "endOffset": 64}, {"referenceID": 32, "context": "9, from (I) FR SURV [33], (II) SCFace [20] and (III) ChokePoint [44] datasets.", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": "9, from (I) FR SURV [33], (II) SCFace [20] and (III) ChokePoint [44] datasets.", "startOffset": 38, "endOffset": 42}, {"referenceID": 43, "context": "9, from (I) FR SURV [33], (II) SCFace [20] and (III) ChokePoint [44] datasets.", "startOffset": 64, "endOffset": 68}, {"referenceID": 3, "context": "where, Probecrop denotes the cropped probe sample based on Chehra [4], \u2191 denotes upsampling of the image by a factor v mentioned to the right of it, Probecrop denotes upsampled image and Probetransformed denotes the image that will be used for feature-extraction.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "The set of features used in the methods proposed includes Local Binary Pattern (LBP), Eigen Faces, Fisher Faces, Gabor faces, Weber Faces, Bag of Words (BOW), Vector of Linearly Agregated Descriptors encoding based on Scale Invariant Feature Transform (VLADSIFT) [3], Fisher Vector encoding based on SIFT (FV-SIFT) [31].", "startOffset": 263, "endOffset": 266}, {"referenceID": 30, "context": "The set of features used in the methods proposed includes Local Binary Pattern (LBP), Eigen Faces, Fisher Faces, Gabor faces, Weber Faces, Bag of Words (BOW), Vector of Linearly Agregated Descriptors encoding based on Scale Invariant Feature Transform (VLADSIFT) [3], Fisher Vector encoding based on SIFT (FV-SIFT) [31].", "startOffset": 315, "endOffset": 319}, {"referenceID": 39, "context": "This approach [40] of the detection and identification of human faces and describe a working, near-real-time face recognition system which tracks the face of a subject and then recognizes the person by comparing characteristics of the face to those of known individuals.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "This face recognition algorithm [9] is insensitive to large variation in lighting direction and facial expression.", "startOffset": 32, "endOffset": 35}, {"referenceID": 25, "context": "The Gabor Feature Classifier (GFC) method [26] employs an enhanced Fisher discrimination model on an augmented Gabor feature vector; which is derived from the Gabor wavelet transformation office images.", "startOffset": 42, "endOffset": 46}, {"referenceID": 32, "context": "Datasets v n\u03bc \u03bc FR SURV [33] {0, .", "startOffset": 24, "endOffset": 28}, {"referenceID": 19, "context": ", 7} SCFace [20] {0, .", "startOffset": 12, "endOffset": 16}, {"referenceID": 43, "context": ", 15} ChokePoint [44] {0, .", "startOffset": 17, "endOffset": 21}, {"referenceID": 41, "context": "Inspired from this, a novel illumination insensitive representation of face images is exploited and analyzed under varying illuminations via a ratio image, called Weber-face, [42] where a ratio between local intensity variation and the background is computed.", "startOffset": 175, "endOffset": 179}, {"referenceID": 0, "context": "Local binary patterns (LBP) [1] is a type of feature used for classification in computer vision.", "startOffset": 28, "endOffset": 31}, {"referenceID": 15, "context": "The feature extraction method proposed here is based on the BOW [16] based on Dense-SIFT features.", "startOffset": 64, "endOffset": 68}, {"referenceID": 27, "context": "SIFT keypoints [28]), or simply on windows extracted from the image at regular positions and various scales.", "startOffset": 15, "endOffset": 19}, {"referenceID": 30, "context": "This encoding [31] serves a similar purposes: summarizing in a vectorial statistic a number of local feature descriptors (e.", "startOffset": 14, "endOffset": 18}, {"referenceID": 27, "context": "SIFT [28]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "The Vector of Linearly Agregated Descriptors [3] is similar to Fisher vectors, but (i) it does not store second-order information about the features and (ii) it typically use KMeans instead of GMMs to generate the feature vocabulary (although the latter is also an option).", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "[6], and also discussed in section III-B.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "Hence, a subgradient method [23] is used to solve each of these convex optimization sub-problems, Jf (\u03b1), and finally the union of these are used to obtain a global solution.", "startOffset": 28, "endOffset": 32}, {"referenceID": 32, "context": "FR SURV [33]", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "Using Chehra [4] on both the gallery and probe samples, we produce cropped face regions at an average of 150X150 pixels and 33 \u00d7 33 pixels respectively.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "Figure 7 shows two samples of the gallery images and the their respective probe image (cropped using Chehra [4]).", "startOffset": 108, "endOffset": 111}, {"referenceID": 32, "context": "7: Samples of two subjects from FR SURV Database [33]: (I) (a), (c) Gallery images; and (I) (b), (d) corresponding Probe images; (II) (a)-(d) Cropped faces from (I) using Chehra [4].", "startOffset": 49, "endOffset": 53}, {"referenceID": 3, "context": "7: Samples of two subjects from FR SURV Database [33]: (I) (a), (c) Gallery images; and (I) (b), (d) corresponding Probe images; (II) (a)-(d) Cropped faces from (I) using Chehra [4].", "startOffset": 178, "endOffset": 181}, {"referenceID": 19, "context": "SCFace [20]", "startOffset": 7, "endOffset": 11}, {"referenceID": 3, "context": "The probe images at Distance 1,2 and 3 has a resolution of 75\u00d7 100, 108\u00d7 144 and 168\u00d7 224 pixels respectively which are cropped by Chehra [4] to an average of 40\u00d7 40, 60\u00d7 60 and 100\u00d7 100 pixels respectively.", "startOffset": 138, "endOffset": 141}, {"referenceID": 43, "context": "ChokePoint [44]", "startOffset": 11, "endOffset": 15}, {"referenceID": 43, "context": "[44] collected a new video dataset, termed ChokePoint, designed for experiments in person identifica-", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "8: SCface Database [20]: (I) Gallery images; Probe Images at (II) Distance 1; (III) Distance 2; and (IV) Distance 3; Right column shows the Chehra [4] output for any one of the samples in each row.", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": "8: SCface Database [20]: (I) Gallery images; Probe Images at (II) Distance 1; (III) Distance 2; and (IV) Distance 3; Right column shows the Chehra [4] output for any one of the samples in each row.", "startOffset": 147, "endOffset": 150}, {"referenceID": 43, "context": "9: An example of the recording setup used for the ChokePoint dataset [44].", "startOffset": 69, "endOffset": 73}, {"referenceID": 43, "context": "10: Example shots from the ChokePoint dataset [44], showing portals with various backgrounds.", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "An example to illustrate the pre-processing stages is shown in figure 11, for the SCFace [20] dataset.", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "Figure 11(a) shows the original images available in the dataset, while figure 11(b) shows the landmark points detected by the Chehra [4] on the face images.", "startOffset": 133, "endOffset": 136}, {"referenceID": 43, "context": "Face hallucination is not applied on the probe samples in the ChokePoint [44] dataset, as the resolution of the gallery and probe samples are similar in the dataset.", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "Rigorous experimentation is carried on three real-world datasets; SCface [20], FR SURV [33], and ChokePoint [44]", "startOffset": 73, "endOffset": 77}, {"referenceID": 32, "context": "Rigorous experimentation is carried on three real-world datasets; SCface [20], FR SURV [33], and ChokePoint [44]", "startOffset": 87, "endOffset": 91}, {"referenceID": 43, "context": "Rigorous experimentation is carried on three real-world datasets; SCface [20], FR SURV [33], and ChokePoint [44]", "startOffset": 108, "endOffset": 112}, {"referenceID": 19, "context": "11: Pre-processing stages on SCFace [20] for a subject: (a) Original image, (b) Landmarks detected by Chehra [4], (c) the cropped faces obtained using Chehra, (d) Downsampled image of gallery and upsampled image of probe, (e) Final degraded gallery and enhanced probe images.", "startOffset": 36, "endOffset": 40}, {"referenceID": 3, "context": "11: Pre-processing stages on SCFace [20] for a subject: (a) Original image, (b) Landmarks detected by Chehra [4], (c) the cropped faces obtained using Chehra, (d) Downsampled image of gallery and upsampled image of probe, (e) Final degraded gallery and enhanced probe images.", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "12: Example shots from the three datasets (one sample each), showing the degraded gallery and the enhanced probe images on the cropped faces produced by Chehra [4]", "startOffset": 160, "endOffset": 163}, {"referenceID": 19, "context": "Algorithm SCface FR SURV ChokePoint [20] [33] [44]", "startOffset": 36, "endOffset": 40}, {"referenceID": 32, "context": "Algorithm SCface FR SURV ChokePoint [20] [33] [44]", "startOffset": 41, "endOffset": 45}, {"referenceID": 43, "context": "Algorithm SCface FR SURV ChokePoint [20] [33] [44]", "startOffset": 46, "endOffset": 50}, {"referenceID": 7, "context": "1 EDA1 [8] 47.", "startOffset": 7, "endOffset": 10}, {"referenceID": 32, "context": "2 COMP DEG [33] 4.", "startOffset": 11, "endOffset": 15}, {"referenceID": 9, "context": "3 MDS [10] 42.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "4 KDA1 [8] 35.", "startOffset": 7, "endOffset": 10}, {"referenceID": 18, "context": "5 Gopalan [19] 2.", "startOffset": 10, "endOffset": 14}, {"referenceID": 38, "context": "6 Kliep [39] 37.", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "[8], DA processing based on an eigenvector based transformation, whose extension in the RKHS is termed as KDA1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 32, "context": "[33] (COMP DEG) tries to bridge the gap between the gallery and the probe samples by projecting them both into a lower dimensional subspace determined by the principal components of the feature vectors obtained from each face.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "This paper also acts as the source for the dataset, FR SURV [33].", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "in [10] projects both the gallery and the probe samples into a common subspace for classification.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "[19] and Kliep [39] are two DA based techniques used for object classification accross domains.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[19] and Kliep [39] are two DA based techniques used for object classification accross domains.", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "[19] performs generally the worst.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "13: (a) ROC and (b) CMC plots for performance analysis of different methods, using SCFace [20] dataset.", "startOffset": 90, "endOffset": 94}, {"referenceID": 32, "context": "14: (a) ROC and (b) CMC plots for performance analysis of different methods, using FR SURV [33] dataset.", "startOffset": 91, "endOffset": 95}, {"referenceID": 43, "context": "15: (a) ROC and (b) CMC plots for performance analysis of different methods, using ChokePoint [44] dataset.", "startOffset": 94, "endOffset": 98}], "year": 2016, "abstractText": "Face Recognition (FR) has been the interest to several researchers over the past few decades due to its passive nature of biometric authentication. Despite high accuracy achieved by face recognition algorithms under controlled conditions, achieving the same performance for face images obtained in surveillance scenarios, is a major hurdle. Some attempts have been made to super-resolve the low-resolution face images and improve the contrast, without considerable degree of success. The proposed technique in this paper tries to cope with the very low resolution and low contrast face images obtained from surveillance cameras, for FR under surveillance conditions. For Support Vector Machine classification, the selection of appropriate kernel has been a widely discussed issue in the research community. In this paper, we propose a novel kernel selection technique termed as MFKL (Multi-Feature Kernel Learning) to obtain the best feature-kernel pairing. Our proposed technique employs a effective kernel selection by Multiple Kernel Learning (MKL) method, to choose the optimal kernel to be used along with unsupervised domain adaptation method in the Reproducing Kernel Hilbert Space (RKHS), for a solution to the problem. Rigorous experimentation has been performed on three real-world surveillance face datasets : FR SURV [33], SCface [20] and ChokePoint [44]. Results have been shown using Rank-1 Recognition Accuracy, ROC and CMC measures. Our proposed method outperforms all other recent state-of-the-art techniques by a considerable margin.", "creator": "LaTeX with hyperref package"}}}