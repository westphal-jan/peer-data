{"id": "1610.01588", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2016", "title": "Neural Structural Correspondence Learning for Domain Adaptation", "abstract": "domain comparison, transferring values from structures woven within complex binding vector code descriptions valued in class comparisons, is a fundamental nlp phenomenon. simulations introduce deep relevance network model that marries repetitive proteins affecting related earlier strands affecting research on pattern adaptation for representation algorithms : structural correspondence computing ( scl, ( blitzer or al., 2008 ) ) and autoencoder neural networks. again, cozy model is the three - layer coordination network that learns to encode the nonpivot object containing an isolated example into a low - dimensional representation, so if complementary structures of random items ( behaviors that function prominent over both domains and store useful information for appropriate nlp task ) in the example are be decoded from that structure. the low - dimensional representation is invariably employed in a learning mapping for the model. moreover, \u2022 studied how to employ pre - trained spatial embeddings into computational application in to actually obtain generalization across examples covering similar pivot semantics. on the task whereas cross - domain product object classification ( blitzer et al., 1935 ), consisting at 12 domain pairs, our method outperforms both the scl approaches ir functional hierarchical denoising autoencoder ( msda, ( chen et al., 1998 ) ) methods by 78. 12 % till 2. 49 % respectively, only employing 20 domain pairs.", "histories": [["v1", "Wed, 5 Oct 2016 19:57:21 GMT  (204kb)", "https://arxiv.org/abs/1610.01588v1", null], ["v2", "Mon, 5 Jun 2017 19:03:00 GMT  (239kb)", "http://arxiv.org/abs/1610.01588v2", null], ["v3", "Sat, 17 Jun 2017 22:30:57 GMT  (236kb)", "http://arxiv.org/abs/1610.01588v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yftah ziser", "roi reichart"], "accepted": false, "id": "1610.01588"}, "pdf": {"name": "1610.01588.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["syftah@campus.technion.ac.il,", "roiri@ie.technion.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n01 58\n8v 3\n[ cs\n.C L\n] 1\n7 Ju\nn 20\n17\nthat marries together ideas from two prominent strands of research on domain adaptation through representation learning: structural correspondence learning (SCL, (Blitzer et al., 2006)) and autoencoder neural networks (NNs). Our model is a three-layer NN that learns to encode the non-pivot features of an input example into a low-dimensional representation, so that the existence of pivot features (features that are prominent in both domains and convey useful information for the NLP task) in the example can be decoded from that representation. The low-dimensional representation is then employed in a learning algorithm for the task. Moreover, we show how to inject pre-trained word embeddings into our model in order to improve generalization across examples with similar pivot features. We experiment with the task of cross-domain sentiment classification on 16 domain pairs and show substantial improvements over strong baselines.1"}, {"heading": "1 Introduction", "text": "Many state-of-the-art algorithms for Natural Language Processing (NLP) tasks require labeled data. Unfortunately, annotating sufficient amounts of such data is often costly and labor intensive. Consequently, for many NLP applications even resource-rich languages like English have labeled data in only a handful of domains.\nDomain adaptation (Daume\u0301 III, 2007; Ben-David et al., 2010), training an algorithm on\n1Our code is at: https://github.com/yftah89/Neural-SCLDomain-Adaptation.\nlabeled data taken from one domain so that it can perform properly on data from other domains, is therefore recognized as a fundamental challenge in NLP. Indeed, over the last decade domain adaptation methods have been proposed for tasks such as sentiment classification (Bollegala et al., 2011b), POS tagging (Schnabel and Schu\u0308tze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al., 2011a), if to name just a handful of applications and works.\nLeading recent approaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders (Glorot et al., 2011; Chen et al., 2012). These models are believed to extract features that are robust to cross-domain variations. However, while excelling on benchmark domain adaptation tasks such as cross-domain product sentiment classification (Blitzer et al., 2007), the reasons to this success are not entirely understood.\nIn the pre-NN era, a prominent approach to domain adaptation in NLP, and particularly in sentiment classification, has been structural correspondence learning (SCL) (Blitzer et al., 2006, 2007). Following the auxiliary problems approach to semi-supervised learning (Ando and Zhang, 2005), this method identifies correspondences among features from different domains by modeling their correlations with pivot features: features that are frequent in both domains and are important for the NLP task. Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, providing a bridge between the domains. Elegant and well motivated as it may be, SCL does not form the state-of-the-art since the neural approaches took over.\nIn this paper we marry these approaches,\nproposing NNmodels inspired by ideas from both. Particularly, our basic model receives the nonpivot features of an input example, encodes them into a hidden layer and then, instead of decoding the input layer as an autoencoder would do, it aims to decode the pivot features. Our more advanced model is identical to the basic one except that the decoding matrix is not learned but is rather replaced with a fixed matrix consisting of pre-trained embeddings of the pivot features. Under this model the probability of the i-th pivot feature to appear in an example is a (non-linear) function of the dot product of the feature\u2019s embedding vector and the network\u2019s hidden layer vector. As explained in Section 3, this approach encourages the model to learn similar hidden layers for documents that have different pivot features as long as these features have similar meaning. In sentiment classification, for example, although one positive review may use the unigram pivot feature excellent while another positive review uses the pivot great, as long as the embeddings of pivot features with similar meaning are similar (as expected from high quality embeddings) the hidden layers learned for both documents are biased to be similar.\nWe experiment with the task of cross-domain product sentiment classification of (Blitzer et al., 2007), consisting of 4 domains (12 domain pairs) and further add an additional target domain, consisting of sentences extracted from social media blogs (total of 16 domain pairs). For pivot feature embedding in our advanced model, we employ the word2vec algorithm (Mikolov et al., 2013). Our models substantially outperform strong baselines: the SCL algorithm, the marginalized stacked denoising autoencoder (MSDA) model (Chen et al., 2012) and the MSDA-DAN model (Ganin et al., 2016) that combines the power of MSDA with a domain adversarial network (DAN)."}, {"heading": "2 Background and Contribution", "text": "Domain adaptation is a fundamental, long standing problem in NLP (e.g. (Roark and Bacchiani, 2003; Chelba and Acero, 2004; Daume III and Marcu, 2006)). The challenge stems from the fact that data in the source and the target domains are often distributed differently, making it hard for a model trained in the source domain to make valuable predictions in the target domain.\nDomain adaptation has various setups, differing\nwith respect to the amounts of labeled and unlabeled data available in the source and target domains. The setup we address, commonly referred to as unsupervised domain adaptation is where both domains have ample unlabeled data, but only the source domain has labeled training data.\nThere are several approaches to domain adaptation in the machine learning literature, including instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al., 2011) and learning joint target and source feature representations (Blitzer et al., 2006; Daume\u0301 III, 2007; Xue et al., 2008; Glorot et al., 2011; Chen et al., 2012).\nHere, we discuss works that, like us, take the representation learning path. Most works under this approach follow a two steps protocol: First, the representation learning method (be it SCL, an autoencoder network, our proposed network model or any other model) is trained on unlabeled data from both the source and the target domains; Then, a classifier for the supervised task (e.g. sentiment classification) is trained in the source domain and this trained classifier is applied to test examples from the target domain. Each input example of the task classifier, at both training and test, is first run through the representation model of the first step and the induced representation is fed to the classifier. Recently, end-to-end models that jointly learn to represent the data and to perform the classification task have also been proposed. We compare our models to one such method (MSDA-DAN, (Ganin et al., 2016)).\nBelow, we first discuss two prominent ideas in feature representation learning: pivot features and autoencoder neural networks. We then summarize our contribution in light of these approaches.\nPivot and Non-Pivot Features The definitions of this approach are given in Blitzer et al. (2006, 2007), where SCL is presented in the context of POS tagging and sentiment classification, respectively. Fundamentally, the method divides the shared feature space of both the source and the target domains to the set of pivot features that are frequent in both domains and are prominent in the NLP task, and a complementary set of non-pivot features. In this section we abstract away from the actual feature space and its division to pivot and non-pivot subsets. In Section 4 we discuss this issue in the context of sentiment classification.\nFor representation learning, SCL employs the\npivot features in order to learn mappings from the original feature space of both domains to a shared, low-dimensional, real-valued feature space. This is done by training classifiers whose input consists of the non-pivot features of an input example and their binary classification task (the auxiliary task) is predicting, every classifier for one pivot feature, whether the pivot associated with the classifier appears in the input example or not. These classifiers are trained on unlabeled data from both the target and the source domains: the training supervision naturally occurs in the data, no human annotation is required. The matrix consisting of the weight vectors of these classifiers is then post-processed with singular value decomposition (SVD), to facilitate final compact representations. The SVD derived matrix serves as a transformation matrix which maps feature vectors in the original space into a low-dimensional real-valued feature space.\nNumerous works have employed the SCL method in particular and the concept of pivot features for domain adaptation in general. A prominent method is spectral feature alignment (SFA, (Pan et al., 2010)). This method aims to align domain-specific (non-pivot) features from different domains into unified clusters, with the help of domain-independent (pivot) features as a bridge.\nRecently, Gouws et al. (2012) and Bollegala et al. (2015) implemented ideas related to those described here within an NN for cross-domain sentiment classification. For example, the latter work trained a word embedding model so that for every document, regardless of its domain, pivots are good predictors of nonpivots, and the pivots\u2019 embeddings are similar across domains. Yu and Jiang (2016) presented a convolutional NN that learns sentence embeddings using two auxiliary tasks (whether the sentence contains a positive or a negative domain independent sentiment word), purposely avoiding prediction with respect to a large set of pivot features. In contrast to these works our model can learn useful cross-domain representations for any type of input example and in our cross-domain sentiment classification experiments it learns document level embeddings. That is, unlike Bollegala et al. (2015) we do not learn word embeddings and unlike Yu and Jiang (2016) we are not restricted to input sentences.\nAutoencoder NNs An autoencoder is comprised of an encoder function h and a decoder function\ng, typically with the dimension of h smaller than that of its argument. The reconstruction of an input x is given by r(x) = g(h(x)). Autoencoders are typically trained to minimize a reconstruction error loss(x, r(x)). Example loss functions are the squared error, the Kullback-Leibler (KL) divergence and the cross entropy of elements of x and elements of r(x). The last two loss functions are appropriate options when the elements of x or r(x) can be interpreted as probabilities of a discrete event. In Section 3 we get back to this point when defining the cross-entropy loss function of our model. Once an autoencoder has been trained, one can stack another autoencoder on top of it, by training a second model which sees the output of the first as its training data (Bengio et al., 2007). The parameters of the stack of autoencoders describe multiple representation levels for x and can feed a classifier, to facilitate domain adaptation.\nRecent prominent models for domain adaptation for sentiment classification are based on a variant of the autoencoder called Stacked Denoising Autoencoders (SDA, (Vincent et al., 2008)). In a denoising autoencoder (DEA) the input vector x is stochastically corrupted into a vector x\u0303, and the model is trained to minimize a denoising reconstruction error loss(x, r(x\u0303)). SDA for crossdomain sentiment classification was implemented by Glorot et al. (2011). Later, Chen et al. (2012) proposed the marginalized SDA (MSDA) model that is more computationally efficient and scalable to high-dimensional feature spaces than SDA.\nMarginalization of denoising autoencoders has gained interest since MSDA was presented. Yang and Eisenstein (2014) showed how to improve efficiency further by exploiting noising functions designed for structured feature spaces, which are common in NLP. More recently, Clinchant et al. (2016) proposed an unsupervised regularization method for MSDA based on the work of Ganin and Lempitsky (2015) and Ganin et al. (2016).\nThere is a recent interest in models based on variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014), for example the variational fair autoencoder model (Louizos et al., 2016), for domain adaptation. However, these models are still not competitive withMSDA on the tasks we consider here.\nOur Contribution We propose an approach that marries the above lines of work. Our model is sim-\nilar in structure to an autoencoder. However, instead of reconstructing the input x from the hidden layer h(x), its reconstruction function r receives a low dimensional representation of the non-pivot features of the input (h(xnp), where xnp is the non-pivot representation of x (Section 3)) and predicts whether each of the pivot features appears in this example or not. As far as we know, we are the first to exploit the mutual strengths of pivot-based methods and autoencoders for domain adaptation."}, {"heading": "3 Neural SCL Models", "text": "We propose two models: the basic Autoencoder SCL (AE-SCL, 3.2)), that directly integrates ideas from autoencoders and SCL, and the elaborated Autoencoder SCL with Similarity Regularization (AE-SCL-SR, 3.3), where pre-trained word embeddings are integrated into the basic model."}, {"heading": "3.1 Definitions", "text": "We denote the feature set in our problem with f , the subset of pivot features with fp \u2286 {1, . . . , |f |} and the subset of non-pivot features with fnp \u2286 {1, . . . , |f |} such that fp\u222afnp = {1, . . . , |f |} and fp \u2229 fnp = \u2205. We further denote the feature representation of an input example X with x. Following this notation, the vector of pivot features of X is denoted with xp while the vector of non-pivot features is denoted with xnp.\nIn order to learn a robust and compact feature representation for X we will aim to learn a nonlinear prediction function from xnp to xp. As discussed in Section 4 the task we experiment with is cross-domain sentiment classification. Following previous work (e.g. (Blitzer et al., 2006, 2007; Chen et al., 2012) our feature representation consists of binary indicators for the occurrence of word unigrams and bigrams in the represented document. In what follows we hence assume that the feature representation x of an example X is a binary vector, and hence so are xp and xnp."}, {"heading": "3.2 Autoencoder SCL (AE-SCL)", "text": "In order to solve the prediction problem, we present an NN architecture inspired by autoencoders (Figure 1). Given an input example X with a feature representation x, our fundamental idea is to start from a non-pivot feature representation, xnp, encode xnp into an intermediate representation hwh(x\nnp), and, finally, predict with a function rwr(hwh(x np)) the occurrences of pivot features,\nxp, in the example. As is standard in NN modeling, we introduce non-linearity to the model through a non-linear activation function denoted with \u03c3 (the sigmoid function in our models). Consequently we get: hwh(x np) = \u03c3(whxnp) and rwr(hwh(x\nnp)) = \u03c3(wrhwh(x\nnp)). In what follows we denote the output of the model with o = rwr(hwh(x\nnp)). Since the sigmoid function outputs values in the [0, 1] interval, o can be interpreted as a vector of probabilities with the i-th coordinate reflecting the probability of the i-th pivot feature to appear in the input example. Cross-entropy is hence a natural loss function to jointly reason about all pivots:\nL(o, xp) = 1\n|fp|\n|fp|\u2211\ni=1\nxpi\u00b7log(oi)+(1\u2212x p i)\u00b7log(1\u2212oi)\nAs xp is a binary vector, for each pivot feature, xpi, only one of the two members of the sum that take this feature into account gets a non-zero value. The higher the probability of the correct event is (whether or not xpi appears in the input example), the lower is the loss."}, {"heading": "3.3 Autoencoder SCL with Similarity Regularization (AE-SCL-SR)", "text": "An important observation of Blitzer et al. (2007), is that some pivot features are similar to each other to the level that they indicate the same information with respect to the classification task. For example, in sentiment classification with word unigram features, the words (unigrams) great and excellent are likely to serve as pivot features, as the meaning\nof each of them is preserved across domains. At the same time, both features convey very similar (positive) sentiment information to the level that a sentiment classifier should treat them as equals.\nThe AE-SCL-SR model is based on two crucial observations. First, in many NLP tasks the pivot features can be pre-embeded into a vector space where pivots with similar meaning have similar vectors. Second, the set fp Xi of pivot features that appear in an exampleXi is typically much smaller than the set \u02c6fp Xi of pivot features that do not appear in it. Hence, if the pivot features of X1 and X2 convey the same information about the NLP task (e.g. that the sentiment of both X1 and X2 is positive), then even if fp X1 and fp X2 are not identical, the intersection between the larger sets \u02c6fp X1 and \u02c6fp X2 is typically much larger than the symmetric difference between fp X1 and fp X2 .\nFor instance, consider two examples, X1 with the single pivot feature f1 = great, and X2, with the single pivot feature f2 = excellent. Crucially, even though X1 and X2 differ with respect to the existence of f1 and f2, due to the similar meaning of these pivot features, we expect both X1 and X2 not to contain many other pivot features, such as terrible, awful and mediocre, whose meanings conflict with that of f1 and f2.\nTo exploit these observations, in AE-SCL-SR the reconstruction matrix wr is pre-trained with a word embedding model and is kept fixed during the training and prediction phases of the neural network. Particularly, the i-th row of wr is set to be the vector representation of the i-th pivot feature as learned by the word embedding model. Except from this change, the AE-SCL-SR model is identical to the AE-SCL model described above.\nNow, denoting the encoding layer for X1 with h1 and the encoding layer for X2 with h2, we expect both \u03c3(wr~ki \u00b7 h1) and \u03c3(w r ~ki \u00b7 h2) to get low values (i.e. values close to 0), for those ki conflicting pivot features: pivots whose meanings conflict with that of fp X1 and fp X2 . By fixing the representations of similar conflicting features to similar vectors, AE-SCL-SR provides a strong bias for h1 and h2 to be similar, as its only way to bias the predictions with respect to these features to be low is by pushing h1 and h2 to be similar. Consequently, under AE-SCL-SR the vectors that encode the non-pivot features of documents with similar pivot features are biased to be similar to each other. As mentioned in Section 4 the vector\nh\u0303 = \u03c3\u22121(h) forms the feature representation that is fed to the sentiment classifier to facilitate domain adaptation. By definition, when h1 and h2 are similar so are their h\u03031 and h\u03032 counterparts."}, {"heading": "4 Experiments", "text": "In this section we describe our experiments. To facilitate clarity, some details are not given here and instead are provided in the appendices. Cross-domain Sentiment Classification To demonstrate the power of our models for domain adaptation we experiment with the task of crossdomain sentiment classification (Blitzer et al., 2007). The data for this task consist of Amazon product reviews from four product domains: Books (B), DVDs (D), Electronic items (E) and Kitchen appliances (K). For each domain 2000 labeled reviews are provided: 1000 are classified as positive and 1000 as negative, and these are augmented with unlabeled reviews: 6000 (B), 34741 (D), 13153 (E) and 16785 (K).\nWe also consider an additional target domain, denoted with Blog: the University of Michigan sentence level sentiment dataset, consisting of sentences taken from social media blogs.2 The dataset for the original task consists of a labeled training set (3995 positive and 3091 negative) and a 33052 sentences test set for which sentiment labels are not provided. We hence used the original test set as our target domain unlabeled set and the original training set as our target domain test set.\nBaselines Cross-domain sentiment classification has been studied in a large number of papers. However, the difference in preprocessing methods, dataset splits to train/dev/test subsets and the different sentiment classifiers make it hard to directly compare between the numbers reported in past.\nWe hence compare our models to three strong baselines, running all models under the same conditions. We aim to select baselines that represent the state-of-the-art in cross-domain sentiment classification in general, and in the two lines of work we focus at: pivot based and autoencoder based representation learning, in particular.\nThe first baseline is SCL with pivot features selected using the mutual information criterion (SCL-MI, (Blitzer et al., 2007)). This is the SCL method where pivot features are frequent in the unlabeled data of both the source and the target do-\n2https://inclass.kaggle.com/c/si650winter11\nmains, and among those features are the ones with the highest mutual information with the task (sentiment) label in the source domain labeled data. We implemented this method. In our implementation unigrams and bigrams should appear at least 10 times in both domains to be considered frequent. For non-pivot features we consider unigrams and bigrams that appear at least 10 times in their domain. The same pivot and non-pivot selection criteria are employed for our AE-SCL and AE-SCL-SR models.\nAmong autoencoder models, SDA has shown by Glorot et al. (2011) to outperform SFA and SCL on cross-domain sentiment classification and later on Chen et al. (2012) demonstrated superior performance for MSDA over SDA and SCL on the same task. Our second baseline is hence the MSDA method (Chen et al., 2012), with code taken from the authors\u2019 web page.3\nTo consider a regularization scheme on top of MSDA representations we also experiment with the MSDA-DANmodel (Ganin et al., 2016) which employs a domain adversarial network (DAN) with the MSDA vectors as input. In Ganin et al. (2016) MSDA-DAN has shown to substantially outperform the DAN model when DAN is randomly initialized. The DAN code is taken from the authors\u2019 repository. 4\nFor reference we compare to the No-DA case where the sentiment classifier is trained in the source domain and applied to the target domain without adaptation. The sentiment classifier we employ, in this case as well as with our methods and with the SCL-MI and MSDA baselines, is a standard logistic regression classifier.5 6\nExperimental Protocol Following the unsupervised domain adaptation setup (Section 2), we have access to unlabeled data from both the source and the target domains, which we use to train the representation learning models. However, only the source domain has labeled training data for sentiment classification. The original feature set we start from consists of word unigrams and bigrams.\nAll methods (baselines and ours), except from MSDA-DAN, follow a two-step protocol at both\n3 http://www.cse.wustl.edu/\u02dcmchen 4 https://github.com/GRAAL-Research/\ndomain_adversarial_neural_network 5http://scikit-learn.org/stable/ 6We tried to compare to (Bollegala et al., 2015) but failed to replicate their results despite personal communication with the authors.\ntraining and test time. In the first step, the input example is run through the representation model which generates a new feature vector for this example. Then, in the second step, this vector is concatenated with the original feature vector of the example and the resulting vector is fed into the sentiment classifier (this concatenation is a standard convention in the baseline methods).\nFor MSDA-DAN all the above holds, except from one exception. MSDA-DAN gets an input representation that consists of a concatenation of the original and the MSDA-induced feature sets. As this is an end-to-end model that predicts the sentiment class jointly with the new feature representation, we do not employ any additional sentiment classifier. As in the other models, MSDADAN utilizes source domain labeled data as well as unlabeled data from both the source and the target domains at training time.\nWe experiment with a 5-fold cross-validation on the source domain (Blitzer et al., 2007): 1600 reviews for training and 400 reviews for development. The test set for each target domain of Blitzer et al. (2007) consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset. In all five folds half of the training examples and half of the development examples are randomly selected from the positive reviews and the other halves from the negative reviews. We report average results across these five folds, employing the same folds for all models.\nHyper-parameter Tuning The details of the hyper-parameter tuning process for all models (including data splits to training, development and test sets) are described in the appendices. Here we provide a summary. AE-SCL and AE-SCL-SR: For the stochastic gradient descent (SGD) training algorithm we set the learning rate to 0.1, momentum to 0.9 and weightdecay regularization to 10\u22125. The number of pivots was chosen among {100, 200, . . . , 500} and the dimensionality of h among {100, 300, 500}. For the features induced by these models we take their whxnp vector. For AE-SCL-SR, embeddings for the unigram and bigram features were learned with word2vec (Mikolov et al., 2013). Details about the software and the way we learn bigram representations are in the appendices.\nBaselines: For SCL-MI, following (Blitzer et al., 2007) we tuned the number of pivot features\nbetween 500 and 1000 and the SVD dimensions among 50,100 and 150. For MSDA we tuned the number of reconstructed features among {500, 1000, 2000, 5000, 10000}, the number of model layers among {1, 3, 5} and the corruption probability among {0.1, 0.2, . . . , 0.5}. For MSDA-DAN, we followed Ganin et al. (2016): the \u03bb adaptation parameter is chosen among 9 values between 10\u22122 and 1 on a logarithmic scale, the hidden layer size l is chosen among {50, 100, 200} and the learning rate \u00b5 is 10\u22123."}, {"heading": "5 Results", "text": "Table 1 presents our results. In the Blitzer et al. (2007) task (top tables), AE-SCL-SR is the best performing model in 9 of 12 setups and on a unified test set consisting of the test sets of all 12 setups (the Test-All column). AE-SCL, MSDA and MSDA-DAN perform best in one setup each. On the unified test set, AE-SCL-SR improves over SCL-MI by 3.8% (error reduction (ER) of 14.8%) and over MSDA-DAN by 2% (ER of 8.4%), while AE-SCL improves over SCL-MI and MSDA-DAN by 2.7% (ER of 10.5%) and 0.9% (ER of 3.8%), respectively. MSDA-DAN and\nMSDA perform very similarly on the unified test set (0.761 and 0.759, respectively) with generally minor differences in the individual setups.\nWhen adapting from the product review domains to the Blog domain (bottom table), AESCL-SR performs best in 3 of 4 setups, providing particularly large improvements when training is in the Kitchen (K) domain. The average improvement of AE-SCL-SR over MSDA is 5.2% and over a non-adapted classifier is 11.7%. As before, MSDA-DAN performs similarly to MSDA on the unified test set, although the differences in the individual setups are much higher. The differences between AE-SCL-SR and the other models are statistically significant in most cases.7\nClass Based Analysis Table 3 presents a classbased comparison between model pairs. Results are presented for the unified test set of the Blitzer et al. (2007) task. The table reveals that the strength of AE-SCL-SR comes from its improved accuracy on positive examples: in 3.97% of the\n7The difference between two models in a given setup is considered to be statistically significant if and only if it is significant in all five folds of that setup.\ncases over AE-SCL (compared to 2.19% of the positive examples where AE-SCL is better) and in 6.40% of the cases over MSDA (compared to 2.80%). While on negative examples the pattern is reversed and AE-SCL and MSDA outperform AESCL-SR, this is a weaker effect which only moderates the overall superiority of AE-SCL-SR.8\nThe unlabeled documents from all four domains are strongly biased to convey positive opinions (Section 4). This is indicated, for example, by the average score given to these reviews by their authors: 4.29 (B), 4.33 (D), 3.96 (E) and 4.16 (K), on a scale of 1 to 5. This analysis suggests that AESCL-SR better learns from of its unlabeled data.\nSimilar Pivots Recall that AE-SCL-SR aims to learn more similar representations for documents with similar pivot features. Table 2 demonstrates this effect through pairs of test documents from 8\n8The reported numbers are averaged over the 5 folds and rounded to the closest integer, if necessary. The comparison between AE-SCL-SR and MSDA-DAN yields a very similar pattern and is hence excluded from space considerations.\nproduct review setups.9 The documents contain pivot features with very similar meaning and indeed they belong to the same sentiment class. Yet, in all cases AE-SCL-SR correctly classifies both documents, while AE-SCL misclassifies one.\nThe rightmost column of the table presents the difference in the ranking of the cosine similarity between the representation vectors h\u0303 of the documents in the pair, according to each of the models. Results (in numerical values and percentage) are given with respect to all cosine similarity values between the h\u0303 vectors of any document pair in the test set. As the documents with the highest similarity are ranked 1, the positive difference between the ranks of AE-SCL and those of AE-SCLSR indicate that AE-SCL\u2019s rank is lower. That is, AE-SCL-SR learns more similar representations for documents with similar pivot features."}, {"heading": "6 Conclusions and Future Work", "text": "We presented a new model for domain adaptation which combines ideas from pivot based and autoencoder based representation learning. We have demonstrated how to encode information from pre-trained word embeddings to improve the generalization of our model across examples with semantically similar pivot features. We demonstrated strong performance on cross-domain sentiment classification tasks with 16 domain pairs and provided initial qualitative analysis that supports the intuition behind our model. Our approach is general and applicable for a large number of NLP\n9We consider for each setup one example pair from one of the five folds such that the dimensionality of the hidden layers in both models is identical.\ntasks (for AE-SCL-SR this holds as long as the pivot features can be embedded in a vector space).\nIn future we would like to adapt our model to more general domain adaptation setups such as where adaptation is performed between sets of source and target domains and where some labeled data from the target domain(s) is available."}, {"heading": "A Hyperparameter Tuning", "text": "This appendix describes the hyper-parameter tuning process for the models compared in our paper. Some of these details appear in the full paper, but here we provide a detailed description.\nAE-SCL and AE-SCL-SR We tuned the parameters of both our models in two steps. First, we randomly split the unlabeled data from both the source and the target domains in a 80/20 manner and combine the large subsets together and the small subsets together so that to generate unlabeled training and validation sets. On these training/validation sets we tune the hyperparameters of the stochastic gradient descent (SGD) algorithm we employ to train our networks: learning rate (0.1), momentum (0.9) and weight-decay regularization (10\u22125). Note that these values are tuned on the fully unsupervised task of predicting pivot features occurrence from non-pivot input representation, and are then employed in all the source-traget domain combinations, across all folds. 10\nAfter tuning the SGD parameters, in the second step we tuned the model\u2019s hyper-parameters for each fold of each source-target setup. The hyperparameters are the number of pivots (100 to 500 in steps 100) and the dimensionality of h (100 to 500 in steps of 200). We select the values that yield the best performing model when training on the training set and evaluating on the training domain development set of each fold.11\nWe further explored the quality of the various intermediate representations generated by the models as sources of features for the sentiment classifier. The vectors we considered are: whxnp, h = \u03c3(whxnp), wrh and r = \u03c3(wrh). We chose thewhxnp vector, denoted in the paper in the paper with h\u0303.\n10Both AE-SCL and AE-SCL-SR converged to the same values. This is probably because for each parameter we consider only a handful of values: learning rate (0.01,0.1,1), momentum (0.1,0.,5,0.9) and weight-decay regularization (10\u22124,10\u22125, 10\u22126).\n11When tuning the SGD parameters we experimented with 100 and 500 pivots and dimensionality of 100 and 500 for h.\nFor AE-SCL-SR, embeddings for the unigram and bigram features were learned with word2vec (Mikolov et al., 2013). 12 To learn bigram representations, in cases where a bigram pivot (w1,w2) is included in a sentence we generate the triplet w1,w1-w2, w2. For example, the sentence It was a very good book with the bigram pivot very good is re-written as: It was a very very-good good book. The revised corpus is then fed into word2vec. The dimension of the hidden layer h of AE-SCL-SR is the dimension of the induced embeddings.\nIn both parameter tuning steps we use the unlabeled validation data for early stopping: the SGD algorithm stops at the first iteration where the validation data error increases rather then when the training error or the loss function are minimized.\nSCL-MI Following (Blitzer et al., 2007) we used 1000 pivot features .13 The number of SVD dimensions was tuned on the labeled development data to the best value among 50,100 and 150.\nMSDA Using the labeled dev. data we tuned the number of reconstructed features (among 500, 1000, 2000, 5000 and 10000) the number of model layers (among {1, 3, 5}) and the corruption probability (among {0.1, 0.2, . . . , 0.5}). For details on these hyper-parameters see (Chen et al., 2012).\nMSDA-DAN Following Ganin et al. (2016) we tuned the hyperparameters on the labeled development data as follows. The \u03bb adaptation parameter is chosen among 9 values between 10\u22122 and 1 on a logarithmic scale. The hidden layer size l is chosen among {50, 100, 200} and the learning rate \u00b5 is fixed to 10\u22123."}, {"heading": "B Experimental Choices", "text": "Variants of the Product Review Data There are two releases of the datasets of the Blitzer et al. (2007) cross-domain product review task. We use the one from http://www.cs.jhu. edu/\u02dcmdredze/datasets/sentiment/ index2.html where the data is imbalanced, consisting of more positive than negative reviews. We believe that our setup is more realistic as when collecting unlabeled data, it is hard to get a balanced set. Note that Blitzer et al. (2007) used\n12We employed the Gensim package and trained the model on the unlabeled data from both the source and the target domains of each adaptation setup (https:// radimrehurek.com/gensim/).\n13Results with 500 pivots were very similar.\nthe other release where the unlabeled data consists of the same number of positive and negative reviews.\nTest Set Size While Blitzer et al. (2007) used only 400 target domain reviews for test, we use the entire set of 2000 reviews. We believe that this decision yields more robust and statistically significant results."}], "references": [{"title": "A framework for learning predictive structures frommultiple tasks and unlabeled data", "author": ["Rie Kubota Ando", "Tong Zhang."], "venue": "Journal of Machine Learning Research 6(Nov):1817\u20131853.", "citeRegEx": "Ando and Zhang.,? 2005", "shortCiteRegEx": "Ando and Zhang.", "year": 2005}, {"title": "A theory of learning from different domains", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Wortman Vaughan."], "venue": "Machine learning 79(1-2):151\u2013175.", "citeRegEx": "Ben.David et al\\.,? 2010", "shortCiteRegEx": "Ben.David et al\\.", "year": 2010}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle."], "venue": "Proc. of NIPS.", "citeRegEx": "Bengio et al\\.,? 2007", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["John Blitzer", "Mark Dredze", "Fernando Pereira"], "venue": "In Proc. of ACL", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "Proc. of EMNLP.", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Unsupervised cross-domain word representation learning", "author": ["Danushka Bollegala", "Takanori Maehara", "Ken-ichi Kawarabayashi."], "venue": "Proc. of ACL.", "citeRegEx": "Bollegala et al\\.,? 2015", "shortCiteRegEx": "Bollegala et al\\.", "year": 2015}, {"title": "Relation adaptation: learning to extract novel relations with minimum supervision", "author": ["Danushka Bollegala", "Yutaka Matsuo", "Mitsuru Ishizuka."], "venue": "Proc. of IJCAI.", "citeRegEx": "Bollegala et al\\.,? 2011a", "shortCiteRegEx": "Bollegala et al\\.", "year": 2011}, {"title": "Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification", "author": ["Danushka Bollegala", "David Weir", "John Carroll."], "venue": "Proc. of ACL.", "citeRegEx": "Bollegala et al\\.,? 2011b", "shortCiteRegEx": "Bollegala et al\\.", "year": 2011}, {"title": "Adaptation of maximum entropy capitalizer: Little data can help a lot", "author": ["Ciprian Chelba", "Alex Acero."], "venue": "Proc. of EMNLP.", "citeRegEx": "Chelba and Acero.,? 2004", "shortCiteRegEx": "Chelba and Acero.", "year": 2004}, {"title": "Automatic feature decomposition for single view co-training", "author": ["Minmin Chen", "Yixin Chen", "Kilian Q Weinberger."], "venue": "Proc. of ICML.", "citeRegEx": "Chen et al\\.,? 2011", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["Minmin Chen", "Zhixiang Xu", "Kilian Weinberger", "Fei Sha."], "venue": "Proc. of ICML.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "A domain adaptation regularization for denoising autoencoders", "author": ["St\u00e9phane Clinchant", "Gabriela Csurka", "Boris Chidlovskii."], "venue": "Proc. of ACL (short papers).", "citeRegEx": "Clinchant et al\\.,? 2016", "shortCiteRegEx": "Clinchant et al\\.", "year": 2016}, {"title": "Frustratingly easy domain adaptation", "author": ["Hal Daum\u00e9 III."], "venue": "Proc. of ACL.", "citeRegEx": "III.,? 2007", "shortCiteRegEx": "III.", "year": 2007}, {"title": "Domain adaptation for statistical classifiers", "author": ["Hal Daume III", "Daniel Marcu."], "venue": "Journal of Artificial Intelligence Research 26:101\u2013126.", "citeRegEx": "III and Marcu.,? 2006", "shortCiteRegEx": "III and Marcu.", "year": 2006}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Yaroslav Ganin", "Victor Lempitsky."], "venue": "Proc. of ICML.", "citeRegEx": "Ganin and Lempitsky.,? 2015", "shortCiteRegEx": "Ganin and Lempitsky.", "year": 2015}, {"title": "Domain-adversarial training of neural networks", "author": ["Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "Fran\u00e7ois Laviolette", "Mario Marchand", "Victor Lempitsky."], "venue": "Journal of Machine Learning Research", "citeRegEx": "Ganin et al\\.,? 2016", "shortCiteRegEx": "Ganin et al\\.", "year": 2016}, {"title": "Some statistical issues in the comparison of speech recognition algorithms", "author": ["Laurence Gillick", "Stephen J Cox."], "venue": "Proc. of ICASSP. IEEE.", "citeRegEx": "Gillick and Cox.,? 1989", "shortCiteRegEx": "Gillick and Cox.", "year": 1989}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "In proc. of ICML. pages 513\u2013520.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Learning structural correspondences across different linguistic domains with synchronous neural language models", "author": ["Stephan Gouws", "GJ Van Rooyen", "MIH Medialab", "Yoshua Bengio."], "venue": "Proc. of the xLite Workshop on Cross-Lingual Technologies,", "citeRegEx": "Gouws et al\\.,? 2012", "shortCiteRegEx": "Gouws et al\\.", "year": 2012}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["Jiayuan Huang", "Arthur Gretton", "Karsten M Borgwardt", "Bernhard Sch\u00f6lkopf", "Alex J Smola."], "venue": "Proc. of NIPS.", "citeRegEx": "Huang et al\\.,? 2007", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "Instance weighting for domain adaptation in nlp", "author": ["Jing Jiang", "ChengXiang Zhai."], "venue": "Proc. of ACL.", "citeRegEx": "Jiang and Zhai.,? 2007", "shortCiteRegEx": "Jiang and Zhai.", "year": 2007}, {"title": "Autoencoding variational bayes", "author": ["Diederik P Kingma", "Max Welling."], "venue": "Proc. of ICLR.", "citeRegEx": "Kingma and Welling.,? 2014", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "The variational fair autoencoder", "author": ["Christos Louizos", "Kevin Swersky", "Yujia Li", "Max Welling", "Richard Zemel"], "venue": null, "citeRegEx": "Louizos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Louizos et al\\.", "year": 2016}, {"title": "Domain adaptation with multiple sources", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh."], "venue": "Proc. of NIPS.", "citeRegEx": "Mansour et al\\.,? 2009", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "Automatic domain adaptation for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Proc. of NAACL.", "citeRegEx": "McClosky et al\\.,? 2010", "shortCiteRegEx": "McClosky et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proc. of NIPS.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Cross-domain sentiment classification via spectral feature alignment", "author": ["Sinno Jialin Pan", "Xiaochuan Ni", "Jian-Tao Sun", "Qiang Yang", "Zheng Chen."], "venue": "Proceedings of the 19th international conference on World wide web. ACM, pages 751\u2013760.", "citeRegEx": "Pan et al\\.,? 2010", "shortCiteRegEx": "Pan et al\\.", "year": 2010}, {"title": "Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets", "author": ["Roi Reichart", "Ari Rappoport."], "venue": "Proc. of ACL.", "citeRegEx": "Reichart and Rappoport.,? 2007", "shortCiteRegEx": "Reichart and Rappoport.", "year": 2007}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra."], "venue": "Proc. of ICML.", "citeRegEx": "Rezende et al\\.,? 2014", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Supervised and unsupervised pcfg adaptation to novel domains", "author": ["Brian Roark", "Michiel Bacchiani."], "venue": "Proc. of HLT-NAACL.", "citeRegEx": "Roark and Bacchiani.,? 2003", "shortCiteRegEx": "Roark and Bacchiani.", "year": 2003}, {"title": "Improved parsing and pos tagging using inter-sentence consistency constraints", "author": ["Alexander M Rush", "Roi Reichart", "Michael Collins", "Amir Globerson."], "venue": "Proc. of EMNLP-CoNLL.", "citeRegEx": "Rush et al\\.,? 2012", "shortCiteRegEx": "Rush et al\\.", "year": 2012}, {"title": "Towards robust cross-domain domain adaptation for part-ofspeech tagging", "author": ["Tobias Schnabel", "Hinrich Sch\u00fctze."], "venue": "Proc. of IJCNLP.", "citeRegEx": "Schnabel and Sch\u00fctze.,? 2013", "shortCiteRegEx": "Schnabel and Sch\u00fctze.", "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol."], "venue": "Proc. of ICML.", "citeRegEx": "Vincent et al\\.,? 2008", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Topic-bridged plsa for cross-domain text classification", "author": ["Gui-Rong Xue", "Wenyuan Dai", "Qiang Yang", "Yong Yu."], "venue": "Proc. of SIGIR.", "citeRegEx": "Xue et al\\.,? 2008", "shortCiteRegEx": "Xue et al\\.", "year": 2008}, {"title": "Fast easy unsupervised domain adaptationwith marginalized structured dropout", "author": ["Yi Yang", "Jacob Eisenstein."], "venue": "Proc. of ACL (short papers).", "citeRegEx": "Yang and Eisenstein.,? 2014", "shortCiteRegEx": "Yang and Eisenstein.", "year": 2014}, {"title": "Learning sentence embeddings with auxiliary tasks for cross-domain sentiment classification", "author": ["Jianfei Yu", "Jing Jiang."], "venue": "Proc. of EMNLP.", "citeRegEx": "Yu and Jiang.,? 2016", "shortCiteRegEx": "Yu and Jiang.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "We introduce a neural network model that marries together ideas from two prominent strands of research on domain adaptation through representation learning: structural correspondence learning (SCL, (Blitzer et al., 2006)) and autoencoder neural networks (NNs).", "startOffset": 198, "endOffset": 220}, {"referenceID": 1, "context": "Domain adaptation (Daum\u00e9 III, 2007; Ben-David et al., 2010), training an algorithm on", "startOffset": 18, "endOffset": 59}, {"referenceID": 7, "context": "Indeed, over the last decade domain adaptation methods have been proposed for tasks such as sentiment classification (Bollegala et al., 2011b), POS tagging (Schnabel and Sch\u00fctze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al.", "startOffset": 117, "endOffset": 142}, {"referenceID": 31, "context": ", 2011b), POS tagging (Schnabel and Sch\u00fctze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al.", "startOffset": 22, "endOffset": 50}, {"referenceID": 27, "context": ", 2011b), POS tagging (Schnabel and Sch\u00fctze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al.", "startOffset": 70, "endOffset": 142}, {"referenceID": 24, "context": ", 2011b), POS tagging (Schnabel and Sch\u00fctze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al.", "startOffset": 70, "endOffset": 142}, {"referenceID": 30, "context": ", 2011b), POS tagging (Schnabel and Sch\u00fctze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al.", "startOffset": 70, "endOffset": 142}, {"referenceID": 20, "context": ", 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al., 2011a), if to name just a handful of applications and works.", "startOffset": 32, "endOffset": 79}, {"referenceID": 6, "context": ", 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al., 2011a), if to name just a handful of applications and works.", "startOffset": 32, "endOffset": 79}, {"referenceID": 17, "context": "Leading recent approaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders (Glorot et al., 2011; Chen et al., 2012).", "startOffset": 123, "endOffset": 163}, {"referenceID": 10, "context": "Leading recent approaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders (Glorot et al., 2011; Chen et al., 2012).", "startOffset": 123, "endOffset": 163}, {"referenceID": 3, "context": "However, while excelling on benchmark domain adaptation tasks such as cross-domain product sentiment classification (Blitzer et al., 2007), the reasons to this success are not entirely understood.", "startOffset": 116, "endOffset": 138}, {"referenceID": 0, "context": "Following the auxiliary problems approach to semi-supervised learning (Ando and Zhang, 2005), this method identifies correspondences among features from different domains by modeling their correlations with pivot features: features that are frequent in both domains and are important for the NLP task.", "startOffset": 70, "endOffset": 92}, {"referenceID": 3, "context": "We experiment with the task of cross-domain product sentiment classification of (Blitzer et al., 2007), consisting of 4 domains (12 domain pairs) and further add an additional target domain, consisting of sentences extracted from social media blogs (total of 16 domain pairs).", "startOffset": 80, "endOffset": 102}, {"referenceID": 25, "context": "For pivot feature embedding in our advanced model, we employ the word2vec algorithm (Mikolov et al., 2013).", "startOffset": 84, "endOffset": 106}, {"referenceID": 10, "context": "Our models substantially outperform strong baselines: the SCL algorithm, the marginalized stacked denoising autoencoder (MSDA) model (Chen et al., 2012) and the MSDA-DAN model (Ganin et al.", "startOffset": 133, "endOffset": 152}, {"referenceID": 15, "context": ", 2012) and the MSDA-DAN model (Ganin et al., 2016) that combines the power of MSDA with a domain adversarial network (DAN).", "startOffset": 31, "endOffset": 51}, {"referenceID": 29, "context": "(Roark and Bacchiani, 2003; Chelba and Acero, 2004; Daume III and Marcu, 2006)).", "startOffset": 0, "endOffset": 78}, {"referenceID": 8, "context": "(Roark and Bacchiani, 2003; Chelba and Acero, 2004; Daume III and Marcu, 2006)).", "startOffset": 0, "endOffset": 78}, {"referenceID": 19, "context": "There are several approaches to domain adaptation in the machine learning literature, including instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al.", "startOffset": 117, "endOffset": 159}, {"referenceID": 23, "context": "There are several approaches to domain adaptation in the machine learning literature, including instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al.", "startOffset": 117, "endOffset": 159}, {"referenceID": 9, "context": ", 2009), sub-sampling from both domains (Chen et al., 2011) and learning joint target and source feature representations (Blitzer et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 4, "context": ", 2011) and learning joint target and source feature representations (Blitzer et al., 2006; Daum\u00e9 III, 2007; Xue et al., 2008; Glorot et al., 2011; Chen et al., 2012).", "startOffset": 69, "endOffset": 166}, {"referenceID": 33, "context": ", 2011) and learning joint target and source feature representations (Blitzer et al., 2006; Daum\u00e9 III, 2007; Xue et al., 2008; Glorot et al., 2011; Chen et al., 2012).", "startOffset": 69, "endOffset": 166}, {"referenceID": 17, "context": ", 2011) and learning joint target and source feature representations (Blitzer et al., 2006; Daum\u00e9 III, 2007; Xue et al., 2008; Glorot et al., 2011; Chen et al., 2012).", "startOffset": 69, "endOffset": 166}, {"referenceID": 10, "context": ", 2011) and learning joint target and source feature representations (Blitzer et al., 2006; Daum\u00e9 III, 2007; Xue et al., 2008; Glorot et al., 2011; Chen et al., 2012).", "startOffset": 69, "endOffset": 166}, {"referenceID": 15, "context": "We compare our models to one such method (MSDA-DAN, (Ganin et al., 2016)).", "startOffset": 52, "endOffset": 72}, {"referenceID": 26, "context": "A prominent method is spectral feature alignment (SFA, (Pan et al., 2010)).", "startOffset": 55, "endOffset": 73}, {"referenceID": 15, "context": "Recently, Gouws et al. (2012) and Bollegala et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 5, "context": "(2012) and Bollegala et al. (2015) implemented ideas related to those described here within an NN for cross-domain sentiment classification.", "startOffset": 11, "endOffset": 35}, {"referenceID": 5, "context": "(2012) and Bollegala et al. (2015) implemented ideas related to those described here within an NN for cross-domain sentiment classification. For example, the latter work trained a word embedding model so that for every document, regardless of its domain, pivots are good predictors of nonpivots, and the pivots\u2019 embeddings are similar across domains. Yu and Jiang (2016) presented", "startOffset": 11, "endOffset": 371}, {"referenceID": 5, "context": "That is, unlike Bollegala et al. (2015) we do not learn word embeddings and unlike Yu and Jiang (2016) we", "startOffset": 16, "endOffset": 40}, {"referenceID": 5, "context": "That is, unlike Bollegala et al. (2015) we do not learn word embeddings and unlike Yu and Jiang (2016) we", "startOffset": 16, "endOffset": 103}, {"referenceID": 2, "context": "Once an autoencoder has been trained, one can stack another autoencoder on top of it, by training a second model which sees the output of the first as its training data (Bengio et al., 2007).", "startOffset": 169, "endOffset": 190}, {"referenceID": 32, "context": "Recent prominent models for domain adaptation for sentiment classification are based on a variant of the autoencoder called Stacked Denoising Autoencoders (SDA, (Vincent et al., 2008)).", "startOffset": 161, "endOffset": 183}, {"referenceID": 15, "context": "SDA for crossdomain sentiment classification was implemented by Glorot et al. (2011). Later, Chen et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 9, "context": "Later, Chen et al. (2012) proposed the marginalized SDA (MSDA) model that is more computationally efficient and scalable to high-dimensional feature spaces than SDA.", "startOffset": 7, "endOffset": 26}, {"referenceID": 31, "context": "Yang and Eisenstein (2014) showed how to improve efficiency further by exploiting noising functions designed for structured feature spaces, which are common in NLP.", "startOffset": 0, "endOffset": 27}, {"referenceID": 11, "context": "More recently, Clinchant et al. (2016) proposed an unsupervised regularization method for MSDA based on the work of Ganin and Lempitsky (2015) and Ganin et al.", "startOffset": 15, "endOffset": 39}, {"referenceID": 11, "context": "More recently, Clinchant et al. (2016) proposed an unsupervised regularization method for MSDA based on the work of Ganin and Lempitsky (2015) and Ganin et al.", "startOffset": 15, "endOffset": 143}, {"referenceID": 11, "context": "More recently, Clinchant et al. (2016) proposed an unsupervised regularization method for MSDA based on the work of Ganin and Lempitsky (2015) and Ganin et al. (2016).", "startOffset": 15, "endOffset": 167}, {"referenceID": 21, "context": "There is a recent interest in models based on variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014), for example the variational fair autoencoder model (Louizos et al.", "startOffset": 71, "endOffset": 119}, {"referenceID": 28, "context": "There is a recent interest in models based on variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014), for example the variational fair autoencoder model (Louizos et al.", "startOffset": 71, "endOffset": 119}, {"referenceID": 22, "context": ", 2014), for example the variational fair autoencoder model (Louizos et al., 2016), for domain adaptation.", "startOffset": 60, "endOffset": 82}, {"referenceID": 10, "context": "(Blitzer et al., 2006, 2007; Chen et al., 2012) our feature representation consists of binary indicators for the occurrence of word unigrams and bigrams in the represented document.", "startOffset": 0, "endOffset": 47}, {"referenceID": 3, "context": "An important observation of Blitzer et al. (2007), is that some pivot features are similar to each other to the level that they indicate the same information with respect to the classification task.", "startOffset": 28, "endOffset": 50}, {"referenceID": 3, "context": "Cross-domain Sentiment Classification To demonstrate the power of our models for domain adaptation we experiment with the task of crossdomain sentiment classification (Blitzer et al., 2007).", "startOffset": 167, "endOffset": 189}, {"referenceID": 3, "context": "The first baseline is SCL with pivot features selected using the mutual information criterion (SCL-MI, (Blitzer et al., 2007)).", "startOffset": 103, "endOffset": 125}, {"referenceID": 10, "context": "Our second baseline is hence the MSDA method (Chen et al., 2012), with code taken from the authors\u2019 web page.", "startOffset": 45, "endOffset": 64}, {"referenceID": 15, "context": "Among autoencoder models, SDA has shown by Glorot et al. (2011) to outperform SFA and SCL on cross-domain sentiment classification and later on Chen et al.", "startOffset": 43, "endOffset": 64}, {"referenceID": 9, "context": "(2011) to outperform SFA and SCL on cross-domain sentiment classification and later on Chen et al. (2012) demonstrated superior performance for MSDA over SDA and SCL on the same task.", "startOffset": 87, "endOffset": 106}, {"referenceID": 15, "context": "To consider a regularization scheme on top of MSDA representations we also experiment with the MSDA-DANmodel (Ganin et al., 2016) which employs a domain adversarial network (DAN) with the MSDA vectors as input.", "startOffset": 109, "endOffset": 129}, {"referenceID": 15, "context": "To consider a regularization scheme on top of MSDA representations we also experiment with the MSDA-DANmodel (Ganin et al., 2016) which employs a domain adversarial network (DAN) with the MSDA vectors as input. In Ganin et al. (2016) MSDA-DAN has shown to substantially outperform the DAN model when DAN is randomly initialized.", "startOffset": 110, "endOffset": 234}, {"referenceID": 5, "context": "org/stable/ We tried to compare to (Bollegala et al., 2015) but failed to replicate their results despite personal communication with the authors.", "startOffset": 35, "endOffset": 59}, {"referenceID": 3, "context": "We experiment with a 5-fold cross-validation on the source domain (Blitzer et al., 2007): 1600 reviews for training and 400 reviews for development.", "startOffset": 66, "endOffset": 88}, {"referenceID": 3, "context": "We experiment with a 5-fold cross-validation on the source domain (Blitzer et al., 2007): 1600 reviews for training and 400 reviews for development. The test set for each target domain of Blitzer et al. (2007) consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset.", "startOffset": 67, "endOffset": 210}, {"referenceID": 25, "context": "For AE-SCL-SR, embeddings for the unigram and bigram features were learned with word2vec (Mikolov et al., 2013).", "startOffset": 89, "endOffset": 111}, {"referenceID": 3, "context": "Baselines: For SCL-MI, following (Blitzer et al., 2007) we tuned the number of pivot features", "startOffset": 33, "endOffset": 55}, {"referenceID": 16, "context": "Statistical significance (with the McNemar paired test for labeling disagreements (Gillick and Cox, 1989; Blitzer et al., 2006), p < 0.", "startOffset": 82, "endOffset": 127}, {"referenceID": 4, "context": "Statistical significance (with the McNemar paired test for labeling disagreements (Gillick and Cox, 1989; Blitzer et al., 2006), p < 0.", "startOffset": 82, "endOffset": 127}, {"referenceID": 3, "context": "Table 1: Sentiment classification accuracy for the Blitzer et al. (2007) task (top tables), and for adaptation from the Blitzer\u2019s product review domains to the Blog domain (bottom table).", "startOffset": 51, "endOffset": 73}, {"referenceID": 15, "context": "For MSDA-DAN, we followed Ganin et al. (2016): the \u03bb adaptation parameter is chosen among 9 values between 10\u22122 and 1 on a logarithmic scale, the hidden layer size l is chosen among {50, 100, 200} and the learning rate \u03bc is 10\u22123.", "startOffset": 26, "endOffset": 46}, {"referenceID": 3, "context": "In the Blitzer et al. (2007) task (top tables), AE-SCL-SR is the best performing model in 9 of 12 setups and on a uni-", "startOffset": 7, "endOffset": 29}, {"referenceID": 3, "context": "Results are presented for the unified test set of the Blitzer et al. (2007) task.", "startOffset": 54, "endOffset": 76}, {"referenceID": 3, "context": "Table 3: Class based analysis for the unified test set of the Blitzer et al. (2007) task.", "startOffset": 62, "endOffset": 84}, {"referenceID": 25, "context": "For AE-SCL-SR, embeddings for the unigram and bigram features were learned with word2vec (Mikolov et al., 2013).", "startOffset": 89, "endOffset": 111}, {"referenceID": 3, "context": "SCL-MI Following (Blitzer et al., 2007) we used 1000 pivot features .", "startOffset": 17, "endOffset": 39}, {"referenceID": 10, "context": "For details on these hyper-parameters see (Chen et al., 2012).", "startOffset": 42, "endOffset": 61}, {"referenceID": 15, "context": "MSDA-DAN Following Ganin et al. (2016) we tuned the hyperparameters on the labeled development data as follows.", "startOffset": 19, "endOffset": 39}, {"referenceID": 3, "context": "Variants of the Product Review Data There are two releases of the datasets of the Blitzer et al. (2007) cross-domain product review task.", "startOffset": 82, "endOffset": 104}, {"referenceID": 3, "context": "Note that Blitzer et al. (2007) used", "startOffset": 10, "endOffset": 32}, {"referenceID": 3, "context": "Test Set Size While Blitzer et al. (2007) used only 400 target domain reviews for test, we use the entire set of 2000 reviews.", "startOffset": 20, "endOffset": 42}], "year": 2017, "abstractText": "We introduce a neural network model that marries together ideas from two prominent strands of research on domain adaptation through representation learning: structural correspondence learning (SCL, (Blitzer et al., 2006)) and autoencoder neural networks (NNs). Our model is a three-layer NN that learns to encode the non-pivot features of an input example into a low-dimensional representation, so that the existence of pivot features (features that are prominent in both domains and convey useful information for the NLP task) in the example can be decoded from that representation. The low-dimensional representation is then employed in a learning algorithm for the task. Moreover, we show how to inject pre-trained word embeddings into our model in order to improve generalization across examples with similar pivot features. We experiment with the task of cross-domain sentiment classification on 16 domain pairs and show substantial improvements over strong baselines.", "creator": "LaTeX with hyperref package"}}}