{"id": "1610.01986", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Active exploration in parameterized reinforcement learning", "abstract": "typical transition - maker reinforcement learning ( act ) methods with continuous expressions actively playing a virtual domain when dealing under transition - world models such as mobility. however, when extending to non - stationary environments, these methods directly rely here an exploration - exploitation trade - off which is rarely dynamically and which causes future changes in their environment. internally we propose an active exploration and abbreviated rl in structured ( parameterized ) continuous action space. this framework starts under consistent set of dynamic actions, each of which allows parameterized with linear variables. static evaluation is controlled through a defined softmax function with an updated temperature $ \\ n $ parameter. in theory, a gaussian exploration are applied to the standard action cycle. below apply tandem meta - linear algorithm based on statistical partial average variations of short - stationary and long - between - running responses to constant tune $ \\ bar $ and the width of decision parameters distribution around which continuous action parameters last quoted. as applied to a simple hybrid simulation - athlete interaction mechanism, this conclude that this error outperforms by parameterized survival calculations allow overlapping exploration schemes with active exploration based any uncertainty variations measured by a user - q - learning coefficient.", "histories": [["v1", "Thu, 6 Oct 2016 18:34:04 GMT  (217kb,D)", "http://arxiv.org/abs/1610.01986v1", "Submitted to EWRL2016"]], "COMMENTS": "Submitted to EWRL2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mehdi khamassi", "costas tzafestas"], "accepted": false, "id": "1610.01986"}, "pdf": {"name": "1610.01986.pdf", "metadata": {"source": "CRF", "title": "Active exploration in parameterized reinforcement learning", "authors": ["Mehdi Khamassi", "Costas Tzafestas"], "emails": ["mehdi.khamassi@upmc.fr", "ktzaf@cs.ntua.gr"], "sections": [{"heading": null, "text": "Keywords: Reinforcement Learning, Exploration/Exploitation, Multi-Armed Bandits, Meta-Learning, Active Exploration, Parameterized/Structured Reinforcement Learning."}, {"heading": "1. Introduction", "text": "Important progresses have been made in recent years in reinforcement learning (RL) with continuous action spaces, permitting successful real-world applications such as Robotics applications (Kober and Peters, 2011; Stulp and Sigaud, 2013). Nevertheless, a recent review on reinforcement learning applied to Robotics (Kober et al., 2013) highlighted, among other points, that (i) a variety of algorithms have been developed, each being appropriate to specific tasks: model-based versus model-free, function approximation versus policy search, continuous versus discrete action spaces; (ii) important human knowledge is injected concerning the search in the parameter space, either by reducing it through learning from demonstration, or by pre-adjusting parameters such as the exploration rate based on the prior determination of the total number of episodes in the experiment. In particular, the balance between exploration and exploitation is often pre-determined with human prior knowledge and does not extend well to tasks with non-stationary reward functions.\nTo address the first issue, the recent proposal of RL algorithms in structured Parameterized Action Space Markov Decision Processes (PAMDP) (Masson and Konidaris, 2016; Hausknecht and Stone, 2016) seems to open a promising line of research. It combines a\nc\u00a92016 Mehdi Khamassi and Costas Tzafestas.\nar X\niv :1\n61 0.\n01 98\n6v 1\n[ cs\n.L G\n] 6\nO ct\n2 01\nset of discrete actions Ad = {a1, a2, ..., ak}, each action a \u2208 Ad featuring ma continuous parameters {\u03b8a1 , ..., \u03b8ama} \u2208 R ma . Actions are thus represented by tuples (a, \u03b8a1 , ..., \u03b8 a ma) and the overall action space is defined as A = \u222aa\u2208Ad(a, \u03b8a1 , ..., \u03b8ama). This framework has been successfully applied to simulations of a Robocup 2D soccer task where agents have to learn to timely select between discrete actions such as running, turning or kicking the ball, and should learn at the same time with which speed to run, which angle to turn or which strength to kick. To ensure algorithm convergence, Masson and Konidaris (2016) alternate between learning phases: (i) given a fixed policy for parameter selection, they use Q-Learning to optimize the policy discrete action selection; (ii) Next, they fix the policy for discrete action selection and use a policy search method to optimize the parameter selection. In contrast, Hausknecht and Stone (2016) learn both in parallel by employing a parameterized actor that learns both discrete actions and parameters, and a parameterized critic that learns only the action-value function. Instead of relying on an external policy search procedure, they are thus able to directly query the critic for gradients.\nNevertheless, the exploration-exploitation trade-off is fixed in these methods, thus falling into the second issue raised by Kober et al. (2013)\u2019s review. Exploration in continuous action spaces being different from discrete spaces, Hausknecht and Stone (2016) adapt -greedy exploration to parameterized action space by picking a random discrete action a \u2208 Ad with probability and sampling the action\u2019s parameters \u03b8ai from a uniform random distribution. is arbitrarily annealed from 1.0 to 0.1 over the first 10,000 updates, thus requiring human prior knowledge about the duration of the task to appropriately tune exploration.\nHere we use the Gaussian exploration for continuous action parameters proposed by van Hasselt and Wiering (2007), which in the original formulation uses a fixed Gaussian width \u03c3. We then apply a noiseless version of the meta-learning algorithm of Schweighofer and Doya (2003), which tracks online variations of the agent\u2019s performance measured by short-term and long-term reward running averages. At each timestep, we use the difference between the two averages to simultaneously tune the inverse temperature \u03b2t used for selecting between discrete actions aj , and the width \u03c3t of the Gaussian distribution from which each continuous action parameter \u03b8ai is sampled around its current value. We test our algorithm on a simple simulated human-robot interaction, where the algorithm tries to maximise reward computed as the virtual engagement of the human in the task, this engagement representing the attention that the human pays to the robot. We show that the proposed algorithm outperforms both continuous parameterized RL without active exploration and with active exploration based on uncertainty variations measured by a Kalman-RL algorithm (Geist and Pietquin, 2010)."}, {"heading": "2. Active exploration algorithm", "text": "The algorithm is summarized in Algorithm 1. It first employs Q-Learning (Watkins and Dayan, 1992) to learn the value of discrete action at \u2208 Ad selected at timestep t in state st:\nQt+1(st, at)\u2190 Qt(st, at) + \u03b1Q[rt + \u03b3max a (Qt(st+1, a))\u2212Qt(st, at)] (1)\nwhere \u03b1Q is a learning rate and \u03b3 is a discount factor. The probability of executing discrete action aj at timestep t is given by a Boltzmann softmax equation:\nP (aj |st, \u03b2t) = exp (\u03b2tQt(st, aj))\u2211 a exp (\u03b2tQt(st, a))\n(2)\nwhere \u03b2t is a dynamic inverse temperature meta-parameter which will be tuned through meta-learning (see below).\nIn parallel, continuous \u03b8\u0303 aj i,t parameters with which action aj is executed at timestep t are selected from a Gaussian exploration function centered on the current values \u03b8 aj i,t(st) in state st of the parameters of this action (van Hasselt and Wiering, 2007):\nP (\u03b8\u0303 aj i,t|st, aj , \u03c3t) = 1\u221a 2\u03c0\u03c3t\nexp ( \u2212(\u03b8\u0303aji,t \u2212 \u03b8 aj i,t(st)) 2/(2\u03c32t ) )\n(3)\nwhere the width \u03c3t of the Gaussian is a meta-parameter which will be tuned through metalearning (see below) and action parameters \u03b8ai,t(st) are learned with a continuous actor-critic algorithm (van Hasselt and Wiering, 2007). A reward prediction error is computed from the critic: \u03b4t = rt + \u03b3Vt(st+1)\u2212 Vt(st) and is used to update the parameter vectors \u03c9Ct and \u03c9At of the neural network function approximations in the critic and the actor:\n\u03c9Ci,t+1 = \u03c9 C i,t + \u03b1C\u03b4t\n\u03b4Vt(st)\n\u03b4\u03c9Ci,t and \u03c9Ai,t+1 = \u03c9 A i,t + \u03b1A\u03b4t(\u03b8\u0303 a i,t \u2212 \u03b8ai,t(st))\n\u03b4\u03b8ai,t(st)\n\u03b4\u03c9Ai,t (4)\nwhere \u03b1C and \u03b1A are learning rates. In contrast to the original version where \u03c9 A t updates are performed only when \u03b4t > 0 (van Hasselt and Wiering, 2007), here we update them all the time and proportionally to \u03b4t as in (Caluwaerts et al., 2012).\nFinally, in order to perform active exploration on \u03b2t and \u03c3t, we implement a noiseless version of the meta-learning algorithm proposed by (Schweighofer and Doya, 2003). We compute short- and long-term reward running averages:\n\u2206r\u0304(t) = (r(t)\u2212 r\u0304(t))/\u03c41 and \u2206r\u0304(t) = (r\u0304(t)\u2212 r\u0304(t))/\u03c42 (5)\nwhere \u03c41 and \u03c42 are two time constants. We then update \u03b2t and \u03c3t with:\n\u03b2t = F (\u00b5(r\u0304(t)\u2212 r\u0304(t))) and \u03c3t = G(\u00b5(r\u0304(t)\u2212 r\u0304(t))) (6)\nwhere \u00b5 is a learning rate, F (x) > 0 is affine, and 0 < G(x) < 20 is a sigmoid. We also compared this meta-learning algorithm with the Kalman Q-Learning proposed by (Geist and Pietquin, 2010). However, in contrast to the original formulation which proposes a purely exploratory agent by replacing Q-values in Equation 2 by the actionspecific diagonal terms of the covariance matrix \u2013 these terms representing the current variance/uncertainty about an action\u2019s Q-value \u2013, here we multiply these terms by a weight \u03b7 and add them as exploration bonuses bat to Q-values in Equation 2. We also use the covariance terms to tune action-specific \u03c3at with function G(x).\nAlgorithm 1 Active exploration with meta-learning\n1: Initialize \u03c9Ai,0, \u03c9 C i,0, Qi,0, \u03b20 and \u03c30 2: for t = 0, 1, 2, ... do 3: Select discrete action at with softmax(st, \u03b2t) (Eq. 2) 4: Select action parameters \u03b8\u0303ai,t with GaussianExploration(st, at, \u03b8 a i,t, \u03c3t) (Eq. 3) 5: Observe new state and reward {st+1, rt+1} \u2190 Transition(st, at, \u03b8\u0303ai,t) 6: Update Qt+1(st, at) in the discrete Q-Learning (Eq. 1) 7: Update function approx. \u03c9Ci,t+1 and \u03c9 A i,t+1 in continuous actor-critic (Eq. 4) 8: if meta-learning then 9: Update reward running averages r\u0304(t) and r\u0304(t) (Eq. 5)"}, {"heading": "3. Experiments", "text": "We test the algorithm described in Section 2 in a simple simulated human-robot interaction involving a single state, 6 discrete actions, and continuous action parameters between - 100 and 100. The task is similar to a non-stationary stochastic multi-armed bandit task except that rather than associating a fixed probability of reward to each discrete action, an action will yield reward only when its continuous parameters are chosen within a Gaussian distribution around the current optimal action parameter \u00b5? with variance \u03c3? (which are unknown to the robot). Every n timesteps, \u00b5? changes so that the task is non-stationary and requires constant re-exploration and learning by the robot.\nThe reward is given by a dynamical system which is based on the virtual engagement e(t) of the human in the task. This engagement is supposed to represent the attention that the human pays to the robot. It starts at 5, increases up to a maximum emax = 10 when the robot performs the appropriate actions with the appropriate parameters, and decreases down to a minimum emin = 0 otherwise:\ne(t+ 1) =  e(t) + \u03b71(emax \u2212 e(t))H(\u03b8at ), if a(t) = a? & H(\u03b8at ) \u2265 0 e(t)\u2212 \u03b72(emin \u2212 e(t))H(\u03b8at ), if a(t) = a? & H(\u03b8at ) < 0 e(t) + \u03b72(emin \u2212 e(t)), otherwise\nwhere \u03b71 = 0.1 is the increasing rate, \u03b72 = 0.05 is the decreasing rate, and H(x) is the reengagement function given by H(x) = 2 ( exp ( \u2212 (x\u2212\u00b5 ?)2\n2\u03c3?2\n) \u2212 0.5 ) where a?, \u00b5? and \u03c3? are\nrespectively the optimal action, action parameter and variance around a?.\nThe reward function is then computed as r(t+ 1) = (1\u2212 \u03bb)e(t+ 1) + \u03bb\u2206e(t+ 1) where \u03bb = 0.7 is a weight. This reward function ensures that the algorithm gets rewarded in cases where the engagement e(t+ 1) is low but nevertheless has just been increased by the action tuple (a(t), \u03b8a(t)) performed by the robot.\nWe first simulated the algorithm without active exploration (thus with a fixed \u03c3 = 20) in a task where the optimal action tuple (a?, \u00b5?) is (a6,\u221220) during 200 timesteps (\u03c3? = 10 in all the experiments presented here), then switches to (a2,\u221220) until timestep 600. Figure 1A shows that the algorithm first learns the appropriate action tuple (a6,\u221220), then takes some time to learn the second tuple, making the engagement drop between timesteps 200 and 400 and eventually finds the second optimal tuple. Nevertheless, \u03c3 = 20 makes the robot select action parameters \u03b8\u0303at with a large variance (illustrated by the clouds of blue dots around the learned action parameters \u03b82t and \u03b8 6 t plotted as black curves). As a consequence, the engagement is not optimized and always remains below 7.5. In contrast, the same algorithm with a smaller fixed variance \u03c3 = 10 can make the engagement reach the optimum of 10 when the optimal action tuple is learned (Figure 1B before timestep 400), but results in too little exploration which prevents the robot from finding a new action parameter which is too far away from the previously learned one (after timestep 400, the new optimal action tuple is (a6, 20)). These two examples illustrate the need to actively vary the variance \u03c3t as a function of changes in the robot\u2019s performance.\nWe next tested active exploration with the Kalman Q-Learning algorithm in a task alternating between optimal tuples (a2,\u221220) and (a6, 20) every 400 timesteps (Figure 1C). The diagonal terms of the covariance matrix COV in the Kalman filter nearly monotonically\ndecrease, resulting in a large variance \u03c3t when action a6 is executed until about timestep 600, and progressively decreasing the variance until the end of the experiment. Nevertheless, the algorithm quickly finds the appropriate action parameters and rapidly shifts between actions a2 and a6 after each change in the task condition. In the long run, the model progressively averages the statistics of the two conditions and learns to perform both actions with 50/50 probabilities (bottom part of Figure 1C) which decreases the simulated engagement (top).\nWe then tested active exploration with the meta-learning algorithm in a slightly more difficult task where the optimal action tuple alternate between (a2,\u221250) and (a6, 50) every 1000 timesteps (Figure 1D). Transient drops in the engagement result in transient decreases in the exploration parameter \u03b2t as well as transient increases in the variance \u03c3t. This enables the algorithm to go through quick transient but wide exploration phases and to rapidly reconverge to exploitation, thus maximizing the simulated engagement.\nFinally, we performed 10 simulations of each model on the difficult version of the task and plotted the average and standard deviation of the simulated engagement (Figure 2). The blue curve shows the performance of the algorithm without active exploration (i.e. fixed \u03c3 = 19 obtained through parameter optimization), which adapts to each new condition but never exceeds a plateau of about 6. The green curve shows the active exploration with Kalman, which adapts faster at the beginning but progressively decreases its maximal engagement. The red curve shows the active exploration with meta-learning which initially takes more time to adapt but then only performs short transient explorations and reaches the optimum engagement of 10."}, {"heading": "4. Conclusion", "text": "In this work, we have shown that a meta-learning algorithm based on online variations of reward running averages can be used to adaptively tune two exploration parameters simultaneously used to select between both discrete actions and continuous action parameters in a parameterized action space. While we had previously successfully used the Kalman Q-Learning proposed by Geist and Pietquin (2010) to coordinate model-based and modelfree reinforcement learning in a stationary task (Viejo et al., 2015), it was not appropriate for the current non-stationary task. The proposed active exploration scheme could be a promising solution for Robotics applications of parameterized reinforcement learning."}, {"heading": "Acknowledgments", "text": "We would like to thank Kenji Doya, Beno\u0302\u0131t Girard, Olivier Pietquin, Bilal Piot, Inaki Rano, Olivier Sigaud and Guillaume Viejo for useful discussions. This research work has been partially supported by the EU-funded Project BabyRobot (H2020-ICT-24-2015, grant agreement no. 687831) (MK, CT), by the Agence Nationale de la Recherche (ANR-11-IDEX0004-02 Sorbonne-Universite\u0301s SU-15-R-PERSU-14 Robot Parallearning Project) (MK), and by Labex SMART (ANR-11- LABX-65 Online Budgeted Learning Project) (MK)."}], "references": [{"title": "Robot skill learning: From reinforcement learning to evolution", "author": ["F. Stulp", "O. Sigaud"], "venue": null, "citeRegEx": "Stulp and Sigaud.,? \\Q2003\\E", "shortCiteRegEx": "Stulp and Sigaud.", "year": 2003}, {"title": "Modeling choice and reaction time during", "author": ["G. Viejo", "M. Khamassi", "A. Brovelli", "B. Girard"], "venue": null, "citeRegEx": "Viejo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Viejo et al\\.", "year": 2007}], "referenceMentions": [], "year": 2016, "abstractText": "Online model-free reinforcement learning (RL) methods with continuous actions are playing a prominent role when dealing with real-world applications such as Robotics. However, when confronted to non-stationary environments, these methods crucially rely on an exploration-exploitation trade-off which is rarely dynamically and automatically adjusted to changes in the environment. Here we propose an active exploration algorithm for RL in structured (parameterized) continuous action space. This framework deals with a set of discrete actions, each of which is parameterized with continuous variables. Discrete exploration is controlled through a Boltzmann softmax function with an inverse temperature \u03b2 parameter. In parallel, a Gaussian exploration is applied to the continuous action parameters. We apply a meta-learning algorithm based on the comparison between variations of short-term and long-term reward running averages to simultaneously tune \u03b2 and the width of the Gaussian distribution from which continuous action parameters are drawn. When applied to a simple virtual human-robot interaction task, we show that this algorithm outperforms continuous parameterized RL both without active exploration and with active exploration based on uncertainty variations measured by a Kalman-Q-learning algorithm.", "creator": "LaTeX with hyperref package"}}}