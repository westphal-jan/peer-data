{"id": "1610.02493", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Oct-2016", "title": "A Semantic Analyzer for the Comprehension of the Spontaneous Arabic Speech", "abstract": "each work is full of one large research endeavour creating \" a \\'eodule \" pushing at suggesting tools for adaptive speech recognition, translation, and rendering against arabic language. our forum has \" now focused on successful attempt to expand the quantitative predictions on wherein classical computational decoder is created. to exploit this goal, we have decided to test the theory of the pertinent conceptual use, indicative of predicting corresponding data accumulation of different types, assuming the effectiveness of the semantic definition. published findings are quite satisfactory.", "histories": [["v1", "Sat, 8 Oct 2016 06:33:35 GMT  (406kb)", "http://arxiv.org/abs/1610.02493v1", "Advances in Computer Science and Engineering. 12 pages 6 figures"]], "COMMENTS": "Advances in Computer Science and Engineering. 12 pages 6 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mourad mars", "mounir zrigui", "mohamed belgacem", "anis zouaghi"], "accepted": false, "id": "1610.02493"}, "pdf": {"name": "1610.02493.pdf", "metadata": {"source": "CRF", "title": "A Semantic Analyzer for the Comprehension of the Spontaneous Arabic Speech", "authors": ["Mourad Mars", "Mounir Zrigui", "Mohamed Belgacem", "Anis Zouaghi"], "emails": ["Mohamed.belgacem}@e.u-grenoble3.fr", "Mounir.zrigui@fsm.rnu.tn", "Anis.zouaghi@riadi.rnu.tn"], "sections": [{"heading": null, "text": "aimed at developing tools for automatic speech recognition, translation, and synthesis for Arabic language. Our attention has mainly been focused on an attempt to improve the probabilistic model on which our semantic decoder is based. To achieve this goal, we have decided to test the influence of the pertinent context use, and of the contextual data integration of different types, on the effectiveness of the semantic decoder. The findings are quite satisfactory."}, {"heading": "1. Introduction", "text": "Our work fits within the framework of the automatic comprehension of Arabic speech. The use of statistical models for the speech recognition and comprehension [1] [2], have the merit to strongly reduce the resort to human expertise. They can also be applied to other fields such as multilingual applications [3].\nThe automatic association with each word of the recognized utterance of the proper group of FSe (Semantic Feature) [4] based on such models generally requires the analysis of the context. The contextual information plays a major role in the selection of the adequate FSe. These pieces of information spare the trouble of interpretation ambiguities and improve the performance of the comprehension system of [6] [10].\nIn the standard approach, the decoding of the word meaning is generated by analysing the context which precedes it or/and which follows it immediately. However, in the case of the comprehension of spontaneous Arabic, this is not always optimum. Indeed, we have recorded rather high error rates sometimes equal to 57% and 48,6%. The first rate results of analysing the meaning of the word preceding the target word, and the second rate by using the meaning of the two preceding words (see figure 3, paragraph 5). In order to sort out this problem, we have decided not to take into account the meaning of the pertinent words to select the target word sense. We have also taken into account the type of illocutionary act achieved by the utterance to which belongs the word to be interpreted (refusal, request, etc.) and of its nature (for example request for reservation, timing, etc.) for the prediction of FSe to be used."}, {"heading": "2. The Difficulties of the Semantic Decoding of the Spontaneous Arabic Speech", "text": ""}, {"heading": "2.1. Varieties of the Spoken Arabic Language", "text": "Arabic is the sixth most widely spoken language in the world with approximately 250 million speakers. For historical and ideological reasons, this language presents a hierarchy of varieties:\n- The modern standard Arabic: It is the written language of literature and press, witch is usually spoken on the radio, in conferences and official speeches all over the Arab countries. Standard Arabic is learned at school.\n- The intermediate Arabic: it is a simplified alternative of standard Arabic and the well shaped of the dialectical Arabic. It borrows its lexicon from the dialect as well as standard Arabic. Currently this alternative is increasing. It is more and more frequently used in the studies and the medias. It allows approaching the illiterate and the native language of the people.\n- The dialectical Arabic: it is another alternative of the classical Arabic. It is mainly oral; it is the language of daily conversation. Each Arab country has its own dialect. Although there are several dialects, mutual comprehension is possible between the different countries.\nThese various registers make the automatic processing of the Arabic language impossible with its several varieties. That s why the developed systems are conceived to process only one of these alternatives. The semantic decoding model proposed in this article is dedicated to the modern standard Arabic language, since it is the official language of all the Arabic countries."}, {"heading": "2.2. Particularities of Arabic Language", "text": "The automatic comprehension of the natural language is a very hard task. The major difficulties related to the automatic processing of the spontaneous Arabic speech are:\n- The non-vowelization of the majority of the Arabic texts in books and newspapers makes the training task when using a probabilistic model more complicated. At the semantic level, the automatic detection of the sense of a unvowelized word is very ambiguous. For example, the word can have three possible interpretations according to its vowelization. It may mean a school or a teacher (female) or taught (past participle of teach). This problem is similar to the ambiguity resulting from homonyms in other languages.\n- An Arabic word may express a whole French or English expression [8]. For\nexample the word expresses in English Have you seen . Thus the automatic interpretation of such words requires their preliminary segmentation, which is not an easy task.\n- The connection without space of the coordinating conjunction \"and\" to the words. It is rather hard to distinguish between the , as a letter of a word (for example\n\"sheets\") and the having the role of a coordinating conjunction (utterance E). However this type of conjunction plays a significant role in the recognized utterance interpretation, by identifying its proposals.\n. (E)\n(I would like to know the departure time of the train going to Tunis and booking a\nplace.)\n- The order followed to arrange the words in an utterance, is variable. This complicates the construction task of an adapted language model, from which the recognized utterance will be interpreted.\n- The possibility of existence of several graphemic realisations for same phoneme, or several phonetic realisations for same grapheme. Even some graphemes can t be considered during the pronunciation [9]. This aspect makes the recognition harder.\n- Some letters of Arabic language for instance: # # # - - are pronounced by using strong expiration, so the quality of the microphone can affect the recognition of the speech results.\n- The essence of pronunciation of some letters as for example: # ."}, {"heading": "3. The Proposed Approach for the Semantic Decoding of Arabic Speech", "text": ""}, {"heading": "3.1. The Conventional Methods Used", "text": "In literature several methods have been proposed for semantic analysing of the spontaneous speech. Some use the HMM (such as [3], [8], [6]), others the neuronal\nnetworks ([9], [10]), the n-grams language models ([11], [12]), the -calculus ([13]), or also logics ([14]). The table 1 below shows the main formalisms used for the language comprehension, their advantages and disadvantages (The list is not exhaustive owing to the lack of space). Unlike Latin language, the processing of spontaneous Arabic speech remains, without sufficient consideration by scientific research. During the last two decades the efforts were rather concentrated on the realization of the morphological and syntactic analyzers for Arabic ([15], [16], [20]etc). In spite of the importance of the representation and of the semantic analysis for the realization of any comprehension system, there are only some works in this field which are interested in the processing of Arabic language (such as [17], [18], [19], [21]). Al Biruni system [21] for example, is based on a combination of the Fillmore case grammar formalism and of the Mel'cuk sense-text theory, for the semantic analysis, the representation of the Arab text and the handling of its representation. As for [17], he uses the unification grammar HDPSG of [22] which allows the integration of syntactic and semantic knowledge in the same grammar, in order to lead to a deep analysis. All these works are interested in the processing of the written Arabic rather than the spoken one. The method that we are proposing on this paper is inspired of the grammar case. It allows decoding the words\nmeaning of the recognized utterance while being based on the relevant contextual data, i.e. by considering only the context which has a semantic influence on the word to be interpreted. The advantage of our method is that the context is automatically determined and needs no human expert intervention. Moreover, the contextual data which contribute to a word interpretation are of several types: semantic, illocutionary, and linguistic. The consideration of different types of contextual data has enabled us to improve the performance of our decoder. Moreover, our model is adapted to the processing of spoken Arabic, since it is based only on the analysis of the elements carrying meanings in the speaker s request. The insignificant or redundant elements are ignored.\nNeuronal Networks [9], [10] [18] Writing and speech Capacity of generalization and flexibility. Coasty time development and very complex generated\nstructures.\nHDPSG [17] Writing\nAllows an explicit integration in only one structure, the different linguistic analysis levels: phonetic, syntactic, and semantic. Not adapted to be used in an interactive vocal system, (it allows analysing a sentence in term of syntactic component)\nCase frames [3] [21] speech Authorizes the treatment of the\nsentences without respecting grammatical rules and requires less expertise in linguistics.\nReduced the role of syntax.\nFollowing are the proposed approach for the semantic analysis of the spontaneous Arabic speech, and the way of the automatic extraction of the relevant context."}, {"heading": "3.2. The Characteristics of our Approach", "text": "To device our semantic decoder, we have opted for the following choices:\n- A componential representation of the meaning words: each significant word for the application is represented by a group of FSe = {field, semantic class, micro semantic dash} and a group of syntactic dashes Fsy = {gender, number, nature}. The dashes of the FSe group indicate respectively: the application field, the semantic class to which the word to be interpreted belongs, and the last dash is a micro semantic dash which helps to distinguish the meaning of the words belonging to the same semantic class. We note that the synonymic words or those having the same semantic role have the same group of FSe. By applying this representation, the meaning of the\nword going for example is described as follows: FSe = {\"transport\" , \"movement\" , \"destination\" } + Fsy = {\"masculine\" , \"singular\" , \"name\" }.\n- A selective analysis: for the semantic decoding of the recognized utterance, we have relied rather on a semantic analysis and considered only the significant elements for the application. The blank words are eliminated during the pre-processing of the utterance phase, by using a lexical filter. This analysis is more tolerant as for the grammatical errors which characterize the spontaneous speech. Moreover, it doesn t need a high standard language of knowledge.\n- A man/machine co-operation method based on corpus analysis (see figure 1): for building up our structure of sense representation SRS such as it is defined in [4], we developed a method based on a corpus analysis to extract significant words, reference words and semantic classes of the application, and on man-machine co-operation for word s interpretation. According to this method the user role is to indicate and to attribute the group of FSe and Fsy to words. And the machine role is to satisfy the constraint s integrities in order to lead to an unambiguous SRS. Our system is based on about ten constraints. An example of constraint to check is that: two different words can t be described by the same group of FSe only if they are considered as synonymic or having a same semantic role.\nNotice that to extract the referring words (words indicating the type and the illocutionary essence of an utterance) we have adapted the TfxIdf method (Term frequency \u00d7 Inverse document frequency), here after the formula:\npij=[tf (mi, Dj).log (n/df (mi))]/[tf (mi, Dj)+0,5+(1.5.n.l(Dj)/ Dk Dkl )( ).log(n+1)]\nWhere, n and l(Dk) stand respectively for, the number of considered requests types and the length of all the requests relating to the corpus representing the field Dk of the considered finalised application. The term tf (mi, Dj) indicates the number of occurrences of mi in Dj. df (mi) corresponds to the number of requests types including mi. So the referring words associated to the requests of the type Dj = {mi / pij > given threshold}, where pij indicates the weight of the word mi in the requests of Dj type.\nAs for the extraction of semantic classes or the application concepts, we have used\nthe k-means algorithm.\nThis defined method makes the task of words interpretation much easier the same goes for the task of building up the SRS and keeping its coherence. The following figure 2 shows the man/machine interface through which each significant word is interpreted via the groups FSe and Fsy.\n- A probabilistic grammar: this grammar contributes to choose the adequate FSe for the utterance significance description. It enables us to include several contextual data at the same time. Furthermore, it considers only the pertinent informations for the prediction of the word sense. The following equation stands for the interpretation probability of a word Mi by the couple (Ci, TMi) taking into account the utterance type. We notice that in this formula we haven t considered the field application since it has been defined in advance. In our instance, it is about train information field.\nP ((Ci, TMi) / Mi, NTj) = P (NTj / Mrk) x P (Ci / NTj, Mi-1, CPi-1, CPi-2) x\nP (TMi / Ci, FSePi-1)\n(1)\nIt is clear that this probability is expressed by terms of three conditional probabilities product. The first probability P (NTj / Mrk) allows the identification of utterance type, if it is about a booking request, or cancelling a ticket, etc. Thus, taking into account words of references Mrk presents in the speaker s utterance. The words of references are unigram, or bi-grams, or trigrams (which can be separate) whose occurrence probabilities are equal to one. For example the bi-grams 2 1 corresponding to 4-grams I want to book in english is a word of reference indicating that it is about booking request. This first probability is calculated as follows:\nP (NTj / Mrk) = N (NTj(E), Mrk) / N (Mrk)\nWhere, N (NTj(E), Mrk) indicates the number of Mrk words occurrence in the utterances of NTj type. And N(Mrk) is the total number of Mrk occurrences in the same utterance.\nThe second probability P (Ci / NTj, Mi-1, CPi-1, CPi-2) enables us to determine the semantic class Ci to which belongs the word Mi, taking in account the utterance type and the two preceding pertinent semantic classes. This probability is calculated as follows:\nP(Ci / NTj, Mi-1, CPi-1 ,CPi-2) = N(NTj(E), Ci, CPi-1, CPi-2) / N(NTj(E), CPi-1, CPi-2)\nAnd the third probability P (TMi / Ci, FSePi-1) enables us to determine the micro semantic dash TMi to be attributed to Mi, while taking into account the class which has been attributed to this word and of preceding pertinent FSe. This last probability is calculated as follows:\nP(TMi / Ci, FSePi-1) = N(FSei, FSePi-1) / N(Ci, FSePi-1)\nIn paragraph 4 is stated the method that we defined for the extraction of pertinent FSe."}, {"heading": "3.3. The Semantic Analyzing Principle", "text": "We understand by the semantic analyzing of an utterance, the labelling of each one of its significant words through a group of FSe.\nAs shown in figure 3, the semantic decoding of the pre-processing utterance is based on the probabilistic language model of [23] and on semantic lexicon. The probabilistic model contributes to the selection of FSe to be affected to words of the utterance in order to be interpreted, and the semantic lexicon describes the meaning of each word through a group of FSe and a group of Fsy. It is from the decoded utterance that its meaning is deducted. This is by filling the attributes of the identified diagram with the corresponding values.\nDuring the training stage we considered a labelled and pre-processed corpus for the estimation of the probabilistic model parameters. The pre-processing of the representing corpus application enabled us to simplify the complexity and to reduce the size of the probabilistic model. This pre-processing like the pre-processing of the utterance consists in eliminating for example the blank words, by gathering certain words in only one entry, etc."}, {"heading": "4. Extraction of the Pertinent Context for Semantic Decoding", "text": ""}, {"heading": "4.1. The Extraction Principle", "text": "To determine the group of FSe to be assigned to the utterance words, we use in our probabilistic model the group of pertinent FSe only. We understand by pertinent FSe, the FSe used to describe words meaning which have a strong semantic affinity with the word Mi. Thus to identify the semantic class Ci to which the word Mi belongs, we consider in the equation E1 the two semantic classes CP i-1 and CP i-2 of the two FSe assigned to two words having the strongest semantic affinities with Mi. Similarly to determine the micro semantic dash TMi allowing the differentiation in the meaning of Mi of the other words of the same class Ci, we consider in E1 only the group FSePi = {CPi, TMPi } which was assigned to the word having the strongest semantic affinity with Mi. To achieve this goal, we have relied on the concept of average mutual information [24] which helps to calculate the correlation degree of two given words."}, {"heading": "4.2. Calculation of the Semantic Affinity", "text": "Let s consider a recognized utterance E to be interpreted: E = M1 M2...Mn. Let ME = {ME-K ..., ME-1 ME1 \", MEK} the group of words surrounding the word Mi to be interpreted by considering a window of K size. As our model considers only the right context (see remark 1) of Mi for the choice of FSe to be assigned to this word, the group ME is so reduced to MEd = {M1, M2 \" Mi-1} (K is variable), which is the group of words preceding Mi. Now, to find the strongest semantic affinity between Mi and its context, we start by calculating the average mutual information between Mi and each of the words belonging to MEd. Remark 1. The Arabic language is written from right to left. Here below the formula of average mutual information IMm [23]:\nIMm(Mi, MEdj) = P(Mi, MEdj) Log [P(Mi / MEdj) / P(Mi) P(MEdj)] +\nP(Mi, MEdj) Log [P(Mi / MEdj) / P(Mi) P(MEdj)] + P(Mi, MEdj)\nLog [P(Mi / MEdj) / P(Mi) P(MEdj)] + P(Mi, MEdj) Log [P(Mi /\nMEdj) / P(Mi) P(MEdj)]; with 1 j i-1\n(2)\nWe preferred to use IMm (equation 2) rather than traditional mutual information IM (equation 3), because the first measurement is more effective. Indeed IMm allows the calculation of the impact of the word absence on the appearance of the other.\nIM(Mi, MEDj) = Log [P(Mi, MEdj) / P(Mi) P(MEDj)]; avec 1 j i-1 (3)\nThe maximum semantic affinity AffM which the word Mi has with its right context\nis obtained by the following formula:\nAffM(Mi, MEd)= argmaxj I(Mi, MEDj) = argmaxj Log[P(Mi, MEDj) / P(Mi) P(MEDj)] (4)\nWe notice that we consider the FSe of the nearest word Mi, in the case of obtaining\ntwo equal semantic affinities. Here is an illustrating example of words extraction having the biggest semantic affinity with the word to be interpreted (Tunis: the capital of Tunisia) in the following request R:\n. (word by word translation: What is the price going to Tunis)\nFollowing a comparison of semantic affinities that the word Tunis has with each word of its right context (see figure 4), we notice that the words (going) and (to) have the biggest semantic affinity with (Tunis). So, will be used the classes CPi1= (mark) and CPi-2= (movement), which are attributed to the words (going) and (to) for the determination of the semantic class to which belongs the word Tunis, and the group FSePi-1={CPi-1= (mark), TMPi-1 = (destination)} for the prediction of the micro semantic dash of the word Tunis.\nIn figure 4, the words indicated by an arrow are in fact those which have appeared in the right context of the word Tunis in the utterance (R) stated above."}, {"heading": "5. Application of the Model and Results", "text": "We have used a hundred utterances different from those of the training corpus, carrying all on timing requests for the test. The training corpus (constituted of 10000 utterances representing the railway information field) was labelled with 37 FSe. To judge the quality of our decoder, we have calculated the percentage of FSe which are\nincorrectly assigned, using the following formula: Rerror = Ninc / N 100.\nWhere, Ninc is the number of FSe incorrectly assigned, and N is the total number of FSe assigned by an expert to the test corpus. N is equal to 500 in this test. The following figures 5 and 6 show respectively the consideration influence of:\n- The pertinent context on the interpretation result with regard to models taking in account and in advance a fixed and determined historic.\n- And the several types of contextual information and of the context length on the interpretation results. We notice that the use of semantic and illocutionary knowledge for parsing some foreign languages (such as English and French) has already been investigated, but not yet for arabic language.\nThe bi-grams and tri-grams models of the above figure 6 are models of Part-ofspeech type. Those have allowed us to test the consideration influence of different contexts! length, on the semantic analyzer performance. According to the above figure 5, it is obvious that the minimum error rate is reached by using only the pertinent contextual informations. In the figure 6, we notice that\neach time we consider lexical data in a model, the result is improved. The improvement increases by integrating more the utterance type; the error rate reaches 37% (with consideration of the lexical data). By considering later the pertinent FSe (i.e. FSe assigned to the words having strongest affinity with the target word) for the prediction of FSe describing the meaning of the word to be interpreted, we notice that we reach an error rate of about 20%. By analysing our test corpus, we noticed that this error rate is mainly due to the utterances having a very complex syntactic structure. In order to solve this problem, some systems combine a deep syntactic analysis with a selective analysis such as the TINA system of [25]. Other systems use the analyzes strategies of NLP robust [26]. These systems are powerful in open applications."}, {"heading": "6. Conclusion", "text": "In this paper we have presented a semantic analyzer based on a hybrid language model, which helps to integrate simultaneously lexical, semantic and illocutionary contextual data. In addition it not has to take into account only pertinent FSe in the word history. To achieve this goal, we have developed a method based on the average mutual information notion. The results are satisfactory. In the near we have presented future work, we intend to evaluate our model by comparing it to the called distant models or to the models obtained by linear combination of well-known language models like the maximum of entropy. We also hope to define an ungrammaticality gradient to allow evaluating the syntactic complexity of a statement and then choose the adequate model to apply."}], "references": [{"title": "Robust analysis of spoken input combining statistical and knowledge-based information sources", "author": ["R. Cattoni", "M. Federico", "A. Lavie"], "venue": "ASRU, Trento", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Estimation de probabilit\u00e9 non param\u00e9trique pour la reconnaissance markovienne de la parole", "author": ["F. Lef\u00e8vre"], "venue": "Th\u00e8se de l'Universit\u00e9 Pierre et Marie Curie", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Compr\u00e9hension automatique de la parole spontan\u00e9e", "author": ["W. Minker"], "venue": "L!Harmattan, Paris", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Une structure s\u00e9mantique pour l!interpr\u00e9tation des \u00e9nonc\u00e9s", "author": ["A. Zouaghi", "M. Zrigui", "M. Ben Ahmed"], "venue": "JEP-TALN, F\u00e8s", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Un \u00e9tiqueteur s\u00e9mantique des \u00e9nonc\u00e9s en langue arabe", "author": ["A. Zouaghi", "M. Zrigui", "M. Ben Ahmed"], "venue": "RECITAL, Dourdan", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Compr\u00e9hension robuste de la parole spontan\u00e9e dans le dialogue oral homme-machine \" D\u00e9codage conceptuel stochastique", "author": ["C. Bousquet-Vernhettes"], "venue": "Th\u00e8se de l!universit\u00e9 de Toulouse III", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "L!\u00e9tiquetage morpho-syntaxique: comment lever l!ambigu\u00eft\u00e9 dans les textes arabes non voyell\u00e9s ", "author": ["N. Chaabene", "L. Belguith"], "venue": "Journ\u00e9es scientifiques des jeunes chercheurs en g\u00e9nie \u00e9lectrique et informatique, Mahdia, Tunisie", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Natural Language Understanding Using Statistical Machine Translation", "author": ["K. Macherey", "F.J. Och", "H. Ney"], "venue": "European conference on speech communication and technology, Aalborg", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Parsec: a structured connexionist parsing system for spoken language", "author": ["A.N. Jain", "A. Waibel", "D.S. Touretzky"], "venue": "International Conference on Acoustics, Speech and Signal Processing, San Francisco,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Un syst\u00e8me de compr\u00e9hension automatique de la parole pour l!interrogation orale d!une base de donn\u00e9es de bourse", "author": ["S. Jamoussi", "K. Sma\u00efli", "D. Fohr", "J.P. Haton"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Linguistic Analysis in a Slovenian information retrieval system for flight services", "author": ["K. Pepelnjak", "J. Gros", "F. Mihelic", "N. Pave\"ic"], "venue": "Workshop on Spoken Dialogue Systems, Vigso, Danemark", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "Comparing grammar-based and robust approaches to speech understanding: a case study", "author": ["S. Knight", "G. Gorell", "M. Rayner", "D. Milward", "R. Koeling", "I. Lewin"], "venue": "European conference on speech communication and technology, Aalborg, Danemark,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Combining Syntax and Pragmatic Knowledge for the Understanding of Spontaneous Spoken Sentences", "author": ["J. Villaneau", "Antoine J.Y.", "O. Ridoux"], "venue": "Logical Aspects of Computational Linguistic, Croisic, France", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "A Cooperative Spoken Dialogue System Based on a Rational Agent Model: A First Implementation on the AGS Application", "author": ["D. Sadek", "P. Bretier", "V. Cadoret", "A. Cozannet", "P. Dupont", "A. Ferrieux", "F. Panaget"], "venue": "Workshop on Spoken Dialogue Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "Arabic morphological analysis on the Internet", "author": ["K.R. Beesely"], "venue": "International Conference on Multi-Lingual Computing, Cambridge", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "A major offshoot of the Dinar-MBC project: AraParse, a morphosyntactic analyzer for unvowelled Arabic texts", "author": ["R. Ouersighni"], "venue": "ACL/EACL01: Conference of the European Chapter, Workshop: Arabic Language Processing: Status and Prospects", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "A Compositional Approach Towards Semantic Representation and Construction of ARABIC", "author": ["B. Haddad", "M. Yaseen"], "venue": "LACL, Bordeaux, France", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "http://search2.computer.org/advanced/Author_Result.jsp?qtype=3&select=50&qOpt1=DC_ CREATOR&sortOrder=d&queryName=Mohamed%20Tayeb%20Laskri: Generation of the Sense of a Sentence in Arabic Language with a Connectionist Approach, AICCSA'01", "author": ["K. Meftouh", "M.T. Laskri"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "A Logical Meaning Representation for Arabic (LMRA)", "author": ["B. AL-Johar", "J. McGregor"], "venue": "Proceedings of the 15th National Computer Conference, Riyadh, Saudi Arabia", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "analyseur morphologique pour l'arabe", "author": ["Mourad Mars", "Mohamed Belgacem", "Mounir Zrigui", "Georges Antoniadis."], "venue": "CITALA 07. Rabat Maroc", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Compr\u00e9hension automatique de la langue arabe", "author": ["C. Manka\u00ef Naanaa"], "venue": "Application: Le syst\u00e8me Al Biruni. Th\u00e8se de l!universit\u00e9 de Tunis", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Head-Driven Phrase Structure Grammar", "author": ["C. Pollard", "I. Sag"], "venue": "University of Chicago Press, Chicago", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "A statistical model for semantic decoding of Arabic language statements", "author": ["A. Zouaghi", "M. Zrigui", "M. Ben Ahmed"], "venue": "Proceedings of NODALIDA, Joensuu, Finland", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Adaptive statistical language modelling: A maximum entropy approach", "author": ["R. Rosenfeld"], "venue": "Thesis at Carnegie Mellon University,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1994}, {"title": "Robust parsing for spoken language systems", "author": ["S. Seneff"], "venue": "Proceedings of ICASSP", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1992}, {"title": "Quand le TAL robuste s!attaque au langage parl\u00e9: analyze incr\u00e9mentale pour la compr\u00e9hension de la parole spontan\u00e9e", "author": ["Antoine", "J-Y.", "J. Goulian", "J. Villaneau"], "venue": "Proceedings of TALN, Batz-sur-Mer, France", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "The use of statistical models for the speech recognition and comprehension [1] [2], have the merit to strongly reduce the resort to human expertise.", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "The use of statistical models for the speech recognition and comprehension [1] [2], have the merit to strongly reduce the resort to human expertise.", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "They can also be applied to other fields such as multilingual applications [3].", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "The automatic association with each word of the recognized utterance of the proper group of FSe (Semantic Feature) [4] based on such models generally requires the analysis of the context.", "startOffset": 115, "endOffset": 118}, {"referenceID": 5, "context": "These pieces of information spare the trouble of interpretation ambiguities and improve the performance of the comprehension system of [6] [10].", "startOffset": 135, "endOffset": 138}, {"referenceID": 9, "context": "These pieces of information spare the trouble of interpretation ambiguities and improve the performance of the comprehension system of [6] [10].", "startOffset": 139, "endOffset": 143}, {"referenceID": 7, "context": "- An Arabic word may express a whole French or English expression [8].", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "Even some graphemes can\u007ft be considered during the pronunciation [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "Some use the HMM (such as [3], [8], [6]), others the neuronal", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "Some use the HMM (such as [3], [8], [6]), others the neuronal", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "Some use the HMM (such as [3], [8], [6]), others the neuronal", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "networks ([9], [10]), the n-grams language models ([11], [12]), the -calculus ([13]), or also logics ([14]).", "startOffset": 10, "endOffset": 13}, {"referenceID": 9, "context": "networks ([9], [10]), the n-grams language models ([11], [12]), the -calculus ([13]), or also logics ([14]).", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "networks ([9], [10]), the n-grams language models ([11], [12]), the -calculus ([13]), or also logics ([14]).", "startOffset": 51, "endOffset": 55}, {"referenceID": 11, "context": "networks ([9], [10]), the n-grams language models ([11], [12]), the -calculus ([13]), or also logics ([14]).", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "networks ([9], [10]), the n-grams language models ([11], [12]), the -calculus ([13]), or also logics ([14]).", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "networks ([9], [10]), the n-grams language models ([11], [12]), the -calculus ([13]), or also logics ([14]).", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "During the last two decades the efforts were rather concentrated on the realization of the morphological and syntactic analyzers for Arabic ([15], [16], [20]etc).", "startOffset": 141, "endOffset": 145}, {"referenceID": 15, "context": "During the last two decades the efforts were rather concentrated on the realization of the morphological and syntactic analyzers for Arabic ([15], [16], [20]etc).", "startOffset": 147, "endOffset": 151}, {"referenceID": 19, "context": "During the last two decades the efforts were rather concentrated on the realization of the morphological and syntactic analyzers for Arabic ([15], [16], [20]etc).", "startOffset": 153, "endOffset": 157}, {"referenceID": 16, "context": "In spite of the importance of the representation and of the semantic analysis for the realization of any comprehension system, there are only some works in this field which are interested in the processing of Arabic language (such as [17], [18], [19], [21]).", "startOffset": 234, "endOffset": 238}, {"referenceID": 17, "context": "In spite of the importance of the representation and of the semantic analysis for the realization of any comprehension system, there are only some works in this field which are interested in the processing of Arabic language (such as [17], [18], [19], [21]).", "startOffset": 240, "endOffset": 244}, {"referenceID": 18, "context": "In spite of the importance of the representation and of the semantic analysis for the realization of any comprehension system, there are only some works in this field which are interested in the processing of Arabic language (such as [17], [18], [19], [21]).", "startOffset": 246, "endOffset": 250}, {"referenceID": 20, "context": "In spite of the importance of the representation and of the semantic analysis for the realization of any comprehension system, there are only some works in this field which are interested in the processing of Arabic language (such as [17], [18], [19], [21]).", "startOffset": 252, "endOffset": 256}, {"referenceID": 20, "context": "Al Biruni system [21] for example, is based on a combination of the Fillmore case grammar formalism and of the Mel'cuk sense-text theory, for the semantic analysis, the representation of the Arab text and the handling of its representation.", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "As for [17], he uses the unification grammar HDPSG of [22] which allows the integration of syntactic and semantic knowledge in the same grammar, in order to lead to a deep analysis.", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": "As for [17], he uses the unification grammar HDPSG of [22] which allows the integration of syntactic and semantic knowledge in the same grammar, in order to lead to a deep analysis.", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "HMM [3], [6] , [8] speech Existence of powerful algorithms (such as Viterbi and A*) allowing to determine the optimal solution.", "startOffset": 4, "endOffset": 7}, {"referenceID": 5, "context": "HMM [3], [6] , [8] speech Existence of powerful algorithms (such as Viterbi and A*) allowing to determine the optimal solution.", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "HMM [3], [6] , [8] speech Existence of powerful algorithms (such as Viterbi and A*) allowing to determine the optimal solution.", "startOffset": 15, "endOffset": 18}, {"referenceID": 8, "context": "Neuronal Networks [9], [10]", "startOffset": 18, "endOffset": 21}, {"referenceID": 9, "context": "Neuronal Networks [9], [10]", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "[18] Writing and speech Capacity of generalization and flexibility.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "HDPSG [17] Writing Allows an explicit integration in only one structure, the different linguistic analysis levels: phonetic, syntactic, and semantic.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "Case frames [3] [21] speech Authorizes the treatment of the sentences without respecting grammatical rules and requires less expertise in linguistics.", "startOffset": 12, "endOffset": 15}, {"referenceID": 20, "context": "Case frames [3] [21] speech Authorizes the treatment of the sentences without respecting grammatical rules and requires less expertise in linguistics.", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "- A man/machine co-operation method based on corpus analysis (see figure 1): for building up our structure of sense representation SRS such as it is defined in [4], we developed a method based on a corpus analysis to extract significant words, reference words and semantic classes of the application, and on man-machine co-operation for word\u007fs interpretation.", "startOffset": 160, "endOffset": 163}, {"referenceID": 22, "context": "As shown in figure 3, the semantic decoding of the pre-processing utterance is based on the probabilistic language model of [23] and on semantic lexicon.", "startOffset": 124, "endOffset": 128}, {"referenceID": 23, "context": "To achieve this goal, we have relied on the concept of average mutual information [24] which helps to calculate the correlation degree of two given words.", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "Here below the formula of average mutual information IMm [23]:", "startOffset": 57, "endOffset": 61}, {"referenceID": 24, "context": "In order to solve this problem, some systems combine a deep syntactic analysis with a selective analysis such as the TINA system of [25].", "startOffset": 132, "endOffset": 136}, {"referenceID": 25, "context": "Other systems use the analyzes strategies of NLP robust [26].", "startOffset": 56, "endOffset": 60}], "year": 2016, "abstractText": "This work is part of a large research project entitled \"Or\u00e9odule\" aimed at developing tools for automatic speech recognition, translation, and synthesis for Arabic language. Our attention has mainly been focused on an attempt to improve the probabilistic model on which our semantic decoder is based. To achieve this goal, we have decided to test the influence of the pertinent context use, and of the contextual data integration of different types, on the effectiveness of the semantic decoder. The findings are quite satisfactory.", "creator": "PDFCreator Version 1.6.2(Infix Pro)"}}}