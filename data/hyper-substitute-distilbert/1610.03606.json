{"id": "1610.03606", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Oct-2016", "title": "Maximum entropy models for generation of expressive music", "abstract": "in the context of contemporary monophonic music, expression can include seen as graphical difference against a musical template and embodied symbolic representation, i. e. a theater ensemble. scanning interactive paper, designers discover how maximum entropy ( maxent ) and ever be allowed to achieve musical results in order to create performance specific performance. utilizing a training corpus, we had one professional performers play about 150 classics of jazz, pop, like latin jazz. the results show it slight approximate ratio, predicting the choice or our model. additionally, we set up improvised recording device whose results detect bugs on computers, technicians significantly prefer the versions generated by the maxent system than synthesized ones without parental modification, or with other modeled variation. furthermore, in some cases, programmed melodies are almost as popular as the human performed ones.", "histories": [["v1", "Wed, 12 Oct 2016 06:20:22 GMT  (226kb,D)", "http://arxiv.org/abs/1610.03606v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.SD", "authors": ["simon moulieras", "fran\\c{c}ois pachet"], "accepted": false, "id": "1610.03606"}, "pdf": {"name": "1610.03606.pdf", "metadata": {"source": "CRF", "title": "Maximum entropy models for generation of expressive music", "authors": ["Simon Moulieras", "Francois Pachet"], "emails": ["*simon.moulieras@gmail.com"], "sections": [{"heading": null, "text": "In the context of contemporary monophonic music, expression can be seen as the difference between a musical performance and its symbolic representation, i.e. a musical score. In this paper, we show how Maximum Entropy (MaxEnt) models can be used to generate musical expression in order to mimic a human performance. As a training corpus, we had a professional pianist play about 150 melodies of jazz, pop, and latin jazz. The results show a good predictive power, validating the choice of our model. Additionally, we set up a listening test whose results reveal that on average, people significantly prefer the melodies generated by the MaxEnt model than the ones without any expression, or with fully random expression. Furthermore, in some cases, MaxEnt melodies are almost as popular as the human performed ones.\nIntroduction\nHuman performances of written music differ in many aspects from a straight acoustical rendering a computer would make. Besides the fact that we would not be even able to play music like a machine, a human performer would give its own interpretation to a musical score, by changing the timing, the loudness, the duration, and the timbre of each note, or even by removing or adding notes (in jazz for example). In fact, a human listener can easily recognize a human performance from a straight computer rendering, without necessarily being a musician. Nevertheless, there is no straightforward mathematical formulation of what is or is not musically expressive.\nMusical expression is a fundamental component of music. It can translate the intention of the composer via the indications written on the score, in order to help the performer. These indications can be whether associated to a specific note, or a group of notes (dynamical indications, articulations, ...) or a word corresponding to a specific style, or a specific phrasing (e.g. fast swing, ballad, groovy, ...). In the latter case, one can legitimately expect to find non-trivial correlations between some well choosen observables, reflecting the specific musical style, and it is the aim of this paper.\nFor this purpose we will use a MaxEnt model which belongs to the wide class of probabilistic graphical models, built to capture and mimic both unary and pairwise correlations between variables. MaxEnt models have already been used in music1 in order to study melodic patterns. Unlike in Markov chains in which the sampling complexity grows exponentially with the size of the patterns, MaxEnt models consistent with pairwise correlations keep this complexity quadratic, independently from the distance between the correlated variables. Importantly, we will use a translation-invariant model for simplicity, i.e we will consider that the probability distribution of a variable associated to a particular note depends only on the variables associated to the neighbouring notes, independently from its intrinsic position in the melody. This assumption is equivalent to saying that musical expression consists in local texture, rather than long-range correlations.\nLike in other aspects of music such as harmony or rhythm, musical expression has a complex underlying structure, that have been tried to be captured by statistical models, or learning algorithm (see2, 3 on romantic music). Working with expression differs from symbolic music computing, in the fact that it involves continuous variables like loudness, or local tempo modulation that cannot be mathematically treated as categories (like the pitch, for example). Unlike romantic music in which a performer usually continuously modulate the tempo, contemporary western music like jazz, or pop music is often played with a rhythmic section that tend to maintain a constant tempo along the melody. The rhythmic expression is thus contained in the difference between: i: the onset of the performed note and its onset on the score, and ii: the duration of the performed note and its duration on the score. In the following, we will refer to these observables as microtiming deviations.\nar X\niv :1\n61 0.\n03 60\n6v 1\n[ cs\n.A I]\n1 2\nO ct\n2 01\n6"}, {"heading": "1 The Model", "text": ""}, {"heading": "1.1 Construction", "text": "The Principle of Maximum Entropy (PME)4 states that given a set of observations, the probability distribution that best model the data is the one that maximizes the entropy. The observations are expressed in terms of averages, or expected values, of one or more quantities. This principle has applications in many domains, such as information theory, biology, statistics, and many more, but comes from statistical physics in which it aimed at connecting macroscopic properties of physical systems to a microscopic description at the atomic or molecular level.\nGiven a set of observables Fj = \u3008 f j(x)\u3009|P(x), 1\u2264 j \u2264 Nc on a quantity x, in which \u3008\u3009|P(x) represents the expectation value over the probability distribution P(x), the PME allows us to determine the distribution P(x) of maximum entropy, fulfilling the constraints.\nThe variables we need our model to deal with are:\n\u2022 metrical position in the bar (discrete)\n\u2022 onset deviation (continuous)\n\u2022 duration deviation (continuous)\n\u2022 loudness (continuous).\nHence, the PME should be applied for each of these variables, and to carefully treat separately continuous variables that will be denoted by y, from discrete ones denoted by x. z will refer to an arbitrary variable, belonging to one or the other of the previous types of variable..\nDiscrete Variables If x is a discrete variable, taking values in an alphabet {xi}1\u2264i\u2264Q then the maximum entropy probability distribution P(xi) = Pr(x = xi) reads:\nP(xi) = 1\nZ (\u03bb1, ...,\u03bbNc) exp\n( \u2211\nj \u03bb j f j(xi)\n) (1)\nwhere\nZ (\u03bb1, ...,\u03bbNc) = Q\n\u2211 i=1 exp\n( \u2211\nj \u03bb j f j(xi)\n) . (2)\nContinuous Variables On the other hand, if y is a real valued variable, one can show that the probability density function p(y) reads\np(y) = 1\nZ (\u03bb1, ...,\u03bbNc) exp\n( \u2211\nj \u03bb j f j(y)\n) (3)\nwhere\nZ (\u03bb1, ...,\u03bbNc) = \u222b\np(y)>0 exp\n( \u2211\nj \u03bb j f j(y)\n) dy. (4)\nIn both cases, Z (\u03bb1, ...,\u03bbNc) is called partition function and consists in a normalization constant. We consider now a sequence of N notes, each of which carrying Ncont continuous variables(loudness, microtiming deviations) and Ndisc discrete Xvdisc(n)|1\u2264n\u2264N variables (metrical position in the bar). Transversally, we can see the same sequence as Nvoices = Ncont +Ndisc sequences of N elements, all of them being whether discrete or continuous numbers. In this view, we will talk about a superposition of voices (the horizontal blue rectangles on Fig. 1), whereas notes are represented vertically (vertical red rectangles).\nLet us denote Yv and Xv respectively a continuous and a discrete variable belonging to the v-th voice, and Yv(n) and Xv(n) respectively their values at the n-th note. For each voice v, there is a MaxEnt model given by the probability distribution of central variable Zv(n), conditionned on the values of the neighbouring variables. We define the neighbourhood \u2202Zv(n) of a variable Zv of the n-th note by itself, the variables surrounding Zv(n) on the same voice (Zv(n\u00b1 k) with 1\u2264 k \u2264 Khormax),\n2/12\nthe variables on the same vertical line (Zv\u2032(n) with v\u2032 6= v), and the variables diagonally connected to it (Zv\u2032(n\u00b1 k) with 1\u2264 k \u2264 Kdiagmax ). For example, the neighbourhood of a variable is represented on Figure 2, with Khormax = 3 and K diag max = 1. The expression of the MaxEnt probability (or probability density, for continuous variables) distribution can be then written as follows:\nPv(Zv|\u2202Zv) = 1\nZ (\u2202Zv) exp(\u2212Hv(Zv,\u2202Zv)) (5)\nH (Zv,\u2202Zv) = hv(Zv)+ \u2211 Z\u2032\u2208\u2202Zv JZ,Z\u2032 (6)\nwhere z correspond to the value of variable Z. In the previous expressions 5 and 6,, the JZ,Z\u2032\u2019s are called the couplings (or interaction energy) between two variables Z and Z\u2032, and the hv\u2019s the local fields (or bias). Importantly we consider here only symmetric couplings JZ,Z\u2032 = JZ\u2032,Z . This terminology comes from statistical physics in which a hamiltonian H (Zv(n),\u2202Zv(n)) represents an energy associated to a specific configuration of Zv(n) and \u2202Zv(n). Depending on the nature of the concerned variables Z and Z\u2032, JZ,Z\u2032 can whether be a real number, a vector or a matrix.\nPr(Xv(n) = xi|\u2202Xv(n)) = Pi(Xv(n)|\u2202Xv(n)) = 1\nZ (\u2202Xv(n)) exp\n( \u2212hv(xi)\u2212 \u2211\nZ\u2208\u2202Xv(n) JXv,Z(xi,z)\n) (7)\nPr(y < Yv(n)< y+dy|\u2202Yv(n)) = pv(y|\u2202yv(n))dy = 1\nZ (\u2202Yv(n)) exp\n( \u2212hv y\u2212 \u2211\nZ\u2208\u2202Yv(n) JXv,Z(y,z)\n) dy (8)\nImportantly, due to the choice of observables we made, the self-interaction term JY,Y for continuous variables Y is proportional to y2, taking into account the standard deviation 16, while it is zero for integer variables JX ,X = 0. The rest of the couplings to the concerned continuous variable Y contribute to a linear term (proportional to y). Consequently, the conditional probability density pv(y|\u2202yv(n)) is always a gaussian."}, {"heading": "1.2 Learning the parameters", "text": "As mentionned in the introduction, we chose to focus on a translation-invariant model for simplicity, i.e., for each voice, the elementary module carrying the parameters of the model, represented on figure 2, is translated horizontally for every note of the voice. Every possible position of the graph on the sequence, gives rise to a sample to be used in the inference of the parameters.\n3/12\nPseudo log-likelihood We use a Maximum Likelihood Estimation (MLE) of the model parameters, i.e., we want to optimize the parameters of a model Pv such that the probability to generate the dataset Dv associated to the voice v using Pv is maximal. The pseudo log-likelihood approximation,5, 6 consists in substituting the probability of the whole sequence z = {Zv(n)}1\u2264n\u2264N by the product of the conditional probabilities of each sample sv = {sv(m)}1\u2264m\u2264M where M = N\u22122max(Khormax,K diag max ) is the number of samples of the sequence, conditionned on the values of the neighbouring variables. The resulting negative pseudo log-likelihood associated to voice v, Lv reads:\nLv({Jv},hv|Dv) =\u2212 1 M\nM\n\u2211 m=1 logPv(zmv |\u2202 zmv ) (9)\nSince all the couplings are symmetric, the vertical and the diagonal ones are involved in two different models, corresponding to the two different voices. The determination of the Nvoices models are then entangled and cannot be done sequentially, which lead us to minimize the sum of the negative log-likelihoods over all the voices, consistently with the pseudo log-likelihood approximation.\nL ({{Jv},hv}1\u2264v\u2264Nvoices |D) = \u2211 v Lv({Jv},hv|Dv) (10)\nwhere D represents the whole dataset \u22c3 v Dv. The minimization of L in ?? does not generally imply the minimization of the individual Lv, however it gives a good approximation of it while the dataset possesses an overall consistency.6\nThe Corpus The corpus consists in a series of midi melodies recorded twice1 by a professional piano player, with a click at a selected tempo and a very neutral accompaniement (chords played on the downbeats). We chose about 172 tunes from the LeadSheet DataBase7 divided in 5 sub-corpora so to have a balance of expression styles, namely swing, latin, ballad, pop, and groove. The full list of tunes can be found in appendix 3.2. For each tune, the pianist was asked to play the melody freely, without removing any note nor adding any extra note. Of course, being familiar with almost all the tunes, he respected their musical context, and their style conventions: swing phrasing for swing, etc ...\nThe dataset is formed as follows: the first voice corresponds to the rhythmic score: it is described by the metrical position of a note in its bar, which is encoded in an integer. The normalized midi onset deviation \u03b4o, the normalized midi duration deviation \u03b4d, and the reduced midi velocity \u03b4v (loudness) were then measured for each note, and constitute the continuous voices :\n\u03b4o = onsetperf\u2212onsetscore\ndurationscore (11)\n\u03b4d = durationperf\u2212durationscore\ndurationscore (12)\n\u03b4v = velocityperf\n127 (13)\nEach sub-corpus was used to train a style model."}, {"heading": "1.3 Generating sequences", "text": "Once the parameters have been infered, one can generate sequences by sampling from distribution (5) using the Metropolis Algorithm.8 Every variable of the sequence is initialized randomly. Then, the following step is repeated: randomly select a variable, compute its probability conditioned by its neighbours, given by eq. (7) and eq. (8), and draw a new value from this probability distribution. A stationnary regime is reached after a number of Monte Carlo steps of the order of a few times the number of variables nvar (in practice \u2248 10nvar)."}, {"heading": "2 Results", "text": ""}, {"heading": "2.1 Quantitative validation", "text": "In order to test the capacity of our model to capture the structure of correlations in the learning corpus,one can generate a long sequence (10000 notes), compute both unary and binary correlations, and compare them to the corpus\u2019. On figure 3, one can see the a good agreement for high frequencies (more or less \u2265 10\u22122), meaning that patterns sufficiently represented in the\n1one for the training, the other for the validation tests.\n4/12\ncorpus are well captured by the model. In our example we took Khormax = 3 and K diag max = 1, and trained the model on the swing sub-corpus. These values are a good compromise between too many parameters, and a good efficiency in capturing correlations. Two quantities can be computed along the generation process, in order to monitor the level of convergence of the sequence: The negative log-likelihood, and the distance between the generated sequence correlations, and the corpus\u2019 correlations. As we can see on fig. 4, both quantities decrease macroscopically before stabilizing to a stationnary regime, made of typical sequences. We used the same test sequence and the same model (swing) in order to exhibit the convergence properties of the Metropolis procedure.\nFor each musical style, similar features are observed. A more instructive character of our models is its ability to predict the value of a variable in a sequence that has been played by our performer. This is the role of the test corpus.\nmax = 1.\nPredictive power: We can perform a leave-one-out cross-validation and obtain good results. Let us remind that the coefficient of determination R2 measures the predictive power as follows: R2 \u2208 [0,1], 0 representing no improved prediction with respect to an empirical gaussian random variable, 1 representing a perfect prediction.\nGenerally speaking, the onset deviation seems to be better modeled, whereas the loudness gets worse results than the other observables. Note as well that the swing model has, a better predictive power for any observable, than any other model. We will enter into more details in the discussion section 3."}, {"heading": "2.2 Perceptive validation", "text": "In the previous section, we have shown different mathematical criteria which provide the validation of the structure and the implementation of the MaxEnt models for musical expression. To be consistent with our goal, it is necessary to perform a perceptive validation, and this is how we proceeded:\n5/12\nProtocol: We set up a pairwise comparison test of 4 different versions of a melody: our baseline version called \u201cstraight\u201d (with zero microtiming deviations and a constant loudness), a version played by the professional performer (called \u201chuman\u201d), a version generated by our models (called \u201cMaxEnt\u201d) and a control version, called \u201crandom\u201d, where microtiming deviations and loudness were sampled with an empirical gaussian distribution2. Every participant was asked to listen to two different versions of the melody for each of the 6 tunes we picked. The pair of versions were randomly picked so that after an important number of participations, every version had been compared to every other version the same number of times. The participants were then asked to answer two questions:\n\u2022 What version did you prefer ?\n\u2022 What version did you find more expressive ?\nFor equity purposes, all four versions were rendered with the same method, and with the same average loudness value. The random and MaxEnt versions were generated with a constrained Metropolis algorithm,i.e. performing the Metropolis algorithm in which the voice concerning the rhythmic score is fixed to the score\u2019s. Furthermore, the accompaniement was rendered with another soundfont, so it could not be confused with the melody. Finally, a participant could do the test only once.\nThe following table shows the melodies we choose, with the associated MaxEnt model:\nA wide communication was done in order to have as many people as possible do the test. We did not aim at selecting any specific category of people. The website hosting the test has been active for 6 weeks during which it collected 244 participations. The musical samples are available at http://ns2292021.ovh.net:3000/.\nIntepretation of the results: In order to give a general ranking over the four versions, we used a so called Bradley-Terry model9, 10 which infers potential \u03b2i for each version i consistent with a probabilistic model giving the probability that version i \u201cbeats\u201d version j (i > j) :\nP(i > j) = e\u03b2i\ne\u03b2i + e\u03b2 j (14)\nDenoting ni, j the number of votes i > j, we use P(i > j) = ni, j\nni, j+n j,i as input of the inference algorithm, we impose the\npotential associated to the straight version to be zero. The results are plotted on figure 7. Note that any set of values of P(i > j) does not necessarily lead to a fully ordered set of \u03b2 . In the four cases shown on figure 7, the algorithm convergences very quickly, ensuring that the votes are very well fitted by the Bradley-Terry model. The overall trend puts the human performance in the first position question-wise. The MaxEnt melodies are secondlyprefered, with an significant margin with respect to the other versions. This is particularly the case for the swing, in which there are some strongly defined phrasing codes: delaying the onset and stressing every even eighth of a bar (upbeats), and increasing the duration of every odd eighth of a bar (downbeats). This pattern is very typical, easy to capture by our MaxEnt model, and strongly recognizable by a human ear. On the other hand, expressive phrasing in ballads or a bossa nova tunes is much less stereotyped, and have an important versatiliy, observed in the data. In particular for slow tempo melodies, a slightly too stressed, or slightly too delayed or anticipated note can sound very artificial. It is very likely that the latter reason explains the important differences between the swing results, with the overall trend.\nTo the question \u201cwhich melody to you find more expressive?\u201d, we observe a different tendency: There is a clear domination of the human performance and a clear inferiority of the straight version. In between, both generated versions get comparable scores. It can be explained by the fact that the extreme versions are very recognizable, and neither random nor MaxEnt models\n2The same distribution corresponding to zero cross validation coefficient of determination in the previous section\n6/12\nsuccess in generating \u201cas much expressivity\u201d as the one perceived in a human performance. In the title of the question, the notion of \u201cmore expressive\u201d is confusing, and can be understood as \u201dwhich version differs the most from the straight melody?\u201d. In this sense, the results are consistent with the constructions of the versions, but not very instructive from a perceptive point of view."}, {"heading": "3 Conclusion & perspectives", "text": "We proposed a Maximum Entropy model which captures pairwise correlations between rhythmic score, micro timing and dynamics observables in musical sequences. The model is used to generate new expression sequences that mimic the interpretation of a musician, given musical style. The structure of this model (see Fig. 2) leads to the replication of complex patterns, despite the pairwise nature of the information used. The pairwise correlations have Moreover, our model deals with both continuous and categorical variables, remaining in the context of Maximum Entropy models.\nWe performed two different validation processes, quantitative and perceptive, whose conclusions are consistent: MaxEnt models success to capture the local texture of different expression styles. The results are very satisfying in particular for a stereotyped style like swing, whereas they are encouraging for more complex styles, like ballad, or latin jazz.\nOne of the most challenging perspective is to allow a model of expression to add or remove notes. This model would be very meaningful in from a musical point of view, but would also cost us a deep modification of the modelization. One should also take into account harmony and pitches, keeping a tractable sample complexity thanks to the MaxEnt model.\nAppendices"}, {"heading": "3.1 Appendix: Model details", "text": "In this appendix, we expose the target observables that we want to reproduce. First, the statistical mean and standard deviation of a variable Yv associated to a continuous voice v:\nf contmean(Yv) = 1 N\nN\n\u2211 n=1 Yv(n) = Y\u0304v (15)\nf contsd (Yv) = \u221a 1 N N \u2211 n=1 (Yv(n)\u2212 Y\u0304v)2. (16)\n7/12\nSimilarly, for discrete variables, we have the frequency of xi:\nf discmean(Xv)[xi] = 1 N\nN\n\u2211 n=1 \u03b4Xv(n),xi , (17)\nwhere \u03b4.,. is the Kronecker symbol. Now, we add pairwise correlations between the same variable (on the same voice v) separated by k notes are called horizontal correlations, and read :\nf conthor,k(Yv) = 1\nN\u2212 k\nN\u2212k \u2211 n=1 Yv(n)Yv(n+ k) (18)\nf dischor,k(Xv)[xi,x j] = 1\nN\u2212 k\nN\u2212k \u2211 n=1 \u03b4Xv(n),xi \u03b4Xv(n+k),x j (19)\nrespectively for a continuous and a discrete variable. Following the same graphical interpretation, vertical correlations exist between variables belonging to voices va and vb, and are defined by :\nf ccvert(Yva ,Yvb) = 1 N\nN\n\u2211 n=1 Yva(n)Yvb(n) (20)\nf ddvert(Xva ,Xvb)[xi,x j] = 1 N\nN\n\u2211 n=1 \u03b4Xva (n),xi \u03b4Xvb (n),x j (21)\nf cdvert(Yva ,Xvb)[x j] = 1 N\nN\n\u2211 n=1 Yva(n)\u03b4Xvb (n),x j (22)\nf dcvert(Xva ,Yvb)[xi] = 1 N\nN\n\u2211 n=1 \u03b4Xva (n),xi Yvb(n) (23)\nrespectively for continuous-continuous, discrete-discrete, or continuous-discrete variable pairs. Finally, the diagonal correlations can be written very similarly :\nf ccdiag,k(Yva ,Yvb) = 1\nN\u2212 k\nN\u2212k \u2211 n=1 Yva(n)Yvb(n+ k) (24)\nf dddiag,k(Xva ,Xvb)[xi,x j] = 1\nN\u2212 k\nN\u2212k \u2211 n=1 \u03b4Xva (n),xi \u03b4Xvb (n+k),x j (25)\nf cddiag,k(Yva ,Xvb)[x j] = 1\nN\u2212 k\nN\u2212k \u2211 n=1 Yva(n)\u03b4Xvb (n+k),x j (26)\nf dcdiag,k(Xva ,Yvb)[xi] = 1\nN\u2212 k\nN\u2212k \u2211 n=1 \u03b4Xva (n),xi Yvb(n+ k). (27)"}, {"heading": "3.2 Appendix: Corpus", "text": "Yesterday by The Beatles Billie Jean by Michael Jackson Eleanor Rigby by The Beatles Happy by Pharrell Williams All My Loving by The Beatles Eight Days A Week by The Beatles Michelle by The Beatles And I Love Her by The Beatles Folsom Prison Blues by Johnny Cash Fields Of Gold by Sting\n8/12\nSo What by Miles Davis Old Man by Neil Young Feel by Robbie William Moondance by Van Morrison Sir Duke by Stevie Wonder I Shot The Sheriff by Bob Marley Flamenco Sketches by Miles Davis Hello by Lionel Richie Just The Way You Are by Billy Joel In The Mood by Glenn Miller You Are The Sunshine Of My Life by Stevie Wonder On The Road Again by Willie Nelson Lively Up Yourself by Bob Marley Watermelon Man by Herbie Hancock Both Sides Now by Joni Mitchell Celebration by Kool & the Gang Naima by John Coltrane Giant Steps by John Coltrane Blue Train by John Coltrane My Life by Billy Joel Cantaloupe Island by Herbie Hancock Chameleon by Herbie Hancock Goodbye Pork Pie Hat by Charles Mingus Cousin Mary by John Coltrane Y.M.C.A by Village People I Feel Good by James Brown Jeru by Miles Davis Maiden Voyage by Herbie Hancock Every Breath You Take by Sting Sunny by Bobby Hebb Strangers Like me by Phil Collins In A Sentimental Mood by Duke Ellington Countdown by John Coltrane Spiral by John Coltrane Self Portrait In Three Colors by Charles Mingus Wave by Antonio Carlos Jobim Locomotion by John Coltrane Caravan by Duke Ellington & Juan Tizol Boogie Stop Shuffle by Charles Mingus Milestones by Miles Davis Blue Monk by Thelonious Monk Can\u2019t Smile Without You by Barry Manilow Nuages by Django Reinhardt & Jacques Larue Epistrophy by Thelonious Monk Song For My Father by Horace Silver Fables Of Faubus by Charles Mingus Lazybird by John Coltrane Son Of Mr. Green Genes by Frank Zappa Too High by Stevie Wonder Mood Indigo by Duke Ellington Continuum by Jaco Pastorius Jelly Roll by Charles Mingus Pussy Cat Dues by Charles Mingus Part-Time Lover by Stevie Wonder Now\u2019s The Time by Charlie Parker\n9/12\nAll in Love Is Fair by Stevie Wonder Bird Calls by Charles Mingus Acknowledgement (Part 1 of A LOVE SUPREME) by John Coltrane Another Star by Stevie Wonder Strode Rode by Sonny Rollins Chega de Saudade (No More Blues) by Antonio Carlos Jobim Fine And Mellow (My Man Don\u2019t Leave Me) by null When You Got A Good Friend by Robert Johnson Triste by Antonio Carlos Jobim Resolution (Part 2 of A LOVE SUPREME) by John Coltrane Evidence by Thelonious Monk Pannonica by Thelonious Monk Central Park West by John Coltrane Sophisticated Lady by Duke Ellington Bemsha Swing by Thelonious Monk Brilliant Corners by Thelonious Monk Solitude by Duke Ellington Dolphin Dance by Herbie Hancock That Girl by Stevie Wonder Four On Six by Wes Montgomery American Patrol by Glenn Miller Yardbird Suite by Charlie Parker In Walked Bud by Thelonious Monk Long Gone Lonesome Blues by Hank Williams Equinox by John Coltrane Stolen Moments by Oliver Nelson Scrapple From The Apple by Charlie Parker Little Brown Jug by Glenn Miller Off Minor by Thelonious Monk Linus And Lucy by Vince Guaraldi Infant Eyes by Wayne Shorter Four by Miles Davis Impressions by John Coltrane Spain by Chick Corea Irene by Caetano Veloso Love In Vain by Robert Johnson Confirmation by Charlie Parker Speak No Evil by Wayne Shorter Mas Que Nada by Jorge Ben Thelonious by Thelonious Monk Witch Hunt by Wayne Shorter Seven Steps To Heaven by Miles Davis A Night In Tunisia by Dizzy Gillespie Chove chuva by Jorge Benjor Qualquer coisa by Caetano Veloso Summertime by George Gershwin & Ira Gershwin, Du Bose & Dorothy Heyward Un Poco Loco by Bud Powell Friends To Go by Paul Mc Cartney Sgt Peppers Lonely Hearts Club Band by The Beatles Butterfly by Herbie Hancock Donna Lee by Charlie Parker Queixa by Caetano Veloso Look To The Sky by Antonio Carlos Jobim Straight, No Chaser by Thelonious Monk Ran Kan Kan by Tito Puente\n10/12\nMenino do Rio by Caetano Veloso Search For Peace by McCoy Tyner Blues On The Corner by McCoy Tyner Nefertiti by Miles Davis Daphne by Django Reinhardt Crystal Silence by Chick Corea Petite Fleur (Little Flower) by Sidney Bechet Meditation by Antonio Carlos Jobim Blues for Alice by Charlie Parker Antigua by Antonio Carlos Jobim Beleza pura by Caetano Veloso Blue In Green by Bill Evans Tell Me A Bedtime Story by Herbie Hancock Speak Like A Child by Herbie Hancock Luz do Sol by Caetano Veloso Foi um rio que passou em minha vida by Paulinho da viola Diminushing by Django Reinhardt Vivo sonhando by Tom Jobim Aguas de marc\u0327o by Tom Jobim Choro by Tom Jobim Favela by Antonio Carlos Jobim Falando de amor by Tom Jobim One Note Samba by Antonio Carlos Jobim Retrato Em Branco E Preto by Antonio Carlos Jobim Ligia by Antonio Carlos Jobim Pais tropical by Jorge Benjor Aquele abrac\u0327o by Gilberto Gil Samba do avia\u0303o by Tom Jobim Trilhos Urbanos by Caetano Veloso Lullaby Of Birdland by George Gershwin Voce E Linda by Caetano Veloso Back in Bahia by Gilberto Gil Samba Cantina by Paul Desmond I Got You (I Feel Good) by James Brown Minha Saudade by Joao Donato Solar by Miles Davis"}, {"heading": "Acknowledgments", "text": "This research is conducted within the Lrn2Cre8 project which received funding from the European Union\u2019s Seventh Framework Programme (FET grant agreement n. 610859). The authors thank Jason Sakellariou, Gae\u0308tan Hadjeres, and Maarten Grachten for fruitful discussions."}], "references": [{"title": "Maximum entropy model for melodic patterns", "author": ["J. Sakellariou", "F. Tria", "V. Loreto", "F. Pachet"], "venue": "In ICML Workshop on Constructive Machine Learning (Paris (France),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "An evaluation of score descriptors combined with non-linear models of expressive dynamics in music", "author": ["C.E.C. Chac\u00f3n", "M. Grachten"], "venue": "Proceedings of the 18th International Conference on Discovery Science (DS 2015),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Linear basis models for prediction and analysis of musical expression", "author": ["M. Grachten", "G. Widmer"], "venue": "Journal of New Music Research 41,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Information theory and statistical mechanics", "author": ["E.T. Jaynes"], "venue": "Phys. Rev. 106,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1957}, {"title": "High-dimensional ising model selection using l1-regularized logistic regression", "author": ["P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty"], "venue": "Ann. Statist", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Using pseudolikelihoods to infer potts models", "author": ["M. Ekeberg", "C. L\u00f6vkvist", "Y. Lan", "M. Weigt", "Aurell", "E. Improved contact prediction in proteins"], "venue": "Phys. Rev. E 87, 012707", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A comprehensive online database of machine-readable lead sheets for jazz standards", "author": ["J. Pachet", "F. Suzda", "D. Mart\u0131\u0301n"], "venue": "In 14th International Society for Music Information Retrieval Conference (ISMIR 2013),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Equation of state calculations by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "The Journal of Chemical Physics", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1953}, {"title": "I", "author": ["Ralph Allan Bradley", "M.E.T. Rank analysis of incomplete block designs"], "venue": "the method of paired comparisons. Biometrika 39, 324\u2013345", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1952}, {"title": "Die berechnung der turnier-ergebnisse als ein maximumproblem der wahrscheinlichkeitsrechnung", "author": ["E. Zermelo"], "venue": "Mathematische Zeitschrift 29,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1929}], "referenceMentions": [{"referenceID": 0, "context": "Let us remind that the coefficient of determination R2 measures the predictive power as follows: R2 \u2208 [0,1], 0 representing no improved prediction with respect to an empirical gaussian random variable, 1 representing a perfect prediction.", "startOffset": 102, "endOffset": 107}], "year": 2016, "abstractText": "In the context of contemporary monophonic music, expression can be seen as the difference between a musical performance and its symbolic representation, i.e. a musical score. In this paper, we show how Maximum Entropy (MaxEnt) models can be used to generate musical expression in order to mimic a human performance. As a training corpus, we had a professional pianist play about 150 melodies of jazz, pop, and latin jazz. The results show a good predictive power, validating the choice of our model. Additionally, we set up a listening test whose results reveal that on average, people significantly prefer the melodies generated by the MaxEnt model than the ones without any expression, or with fully random expression. Furthermore, in some cases, MaxEnt melodies are almost as popular as the human performed ones.", "creator": "LaTeX with hyperref package"}}}