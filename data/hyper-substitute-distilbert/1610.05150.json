{"id": "1610.05150", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Oct-2016", "title": "Neural Machine Translation Advised by Statistical Machine Translation", "abstract": "interactive functional translation ( sh ) is a new approach behind machine translation that eventually encountered impressive progress in foreign languages. however, meta studies show that linguist generally produces fluent yet inadequate translations ( robinson et al. 1986 ; he et al. 2016 ). rn is in contrast rival earlier verbal machine translates ( rs ), such usually rejects adequate or anti - fluent translations. research remained natural, therefore, to leverage distinct advantages of both models significantly better understand, and in this method we write i incorporate smt model 1 general framework. more notably, at each decoding step, mri offers additional analyses into generated words based below the decoding criteria from nmt ( 1. g., the derived partial translation + attention tensor ). then it add an auxiliary tensor to score the smt recommendations and a gating matrix to combine processed smt recommendations as nmt processes, both categories which may generally trained within the nmt architecture in an end - to - day translation. experimental focus on english - english translation principles without any proposed approach means significant and wider efficacy beyond any - of the - art nmt generation smt systems on multiple semantic test sets.", "histories": [["v1", "Mon, 17 Oct 2016 14:50:58 GMT  (91kb,D)", "http://arxiv.org/abs/1610.05150v1", "submitted to AAAI 2017"], ["v2", "Fri, 30 Dec 2016 02:38:35 GMT  (79kb,D)", "http://arxiv.org/abs/1610.05150v2", "Accepted by AAAI 2017"]], "COMMENTS": "submitted to AAAI 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xing wang", "zhengdong lu", "zhaopeng tu", "hang li", "deyi xiong", "min zhang"], "accepted": true, "id": "1610.05150"}, "pdf": {"name": "1610.05150.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation Advised by Statistical Machine Translation", "authors": ["Xing Wang", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Deyi Xiong", "Min Zhang"], "emails": ["xingwsuda@gmail.com,", "minzhang}@suda.edu.cn", "hangli.hl}@huawei.com"], "sections": [{"heading": "Introduction", "text": "Neural machine translation has been receiving considerable attention in recent years (Kalchbrenner and Blunsom 2013; Cho et al. 2014b; Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2015). Compared with the conventional SMT (Brown et al. 1993; Koehn, Och, and Marcu 2003; Chiang 2005), one great advantage of NMT is that the translation system can be completely constructed by learning from data without human involvement (cf., feature engineering in SMT). Another major advantage of NMT is that the gating (Hochreiter and Schmidhuber 1997) and attention (Bahdanau, Cho, and Bengio 2015) techniques prove to be effective in modeling long-distance dependencies and complicated alignment relations in the translation process, which poses a serious challenge for SMT.\nDespite these benefits, recent studies show that NMT generally produces fluent yet sometimes inaccurate translations, mainly due to:\n1. Coverage problem (Tu et al. 2016; Cohn et al. 2016): NMT lacks of a mechanism to record the source words \u2217Corresponding author\nthat have been translated or need to be translated, resulting in either \u201cover-translation\u201d or \u201cunder-translation\u201d (Tu et al. 2016).\n2. Imprecise translation problem (Arthur, Neubig, and Nakamura 2016): NMT is prone to generate words that seem to be natural in the target sentence, but do not reflect the original meaning of the source sentence.\n3. UNK problem (Jean et al. 2015; Luong et al. 2015): NMT uses a fixed modest-sized vocabulary to represent most frequent words and replaces other words with a UNK word. Experimental results show that translation quality degrades rapidly with the number of UNK words increasing (Cho et al. 2014a). Instead of employing different models which individually\nfocus on the above NMT problems, in this work, we try to address the problems together in a single approach. Our approach is based on the observation that SMT models have desirable properties that can properly deal with the above problems:\n1. SMT has a mechanism to guarantee that every source word is translated.\n2. SMT treats words as discrete symbols, which ensures that a source word will be translated into a target word which has been observed at least once in the training data.\n3. SMT explicitly memorizes all the translations, including translations of rare words that are taken as UNK in NMT. It is natural to leverage the advantages of the two sorts of\nmodels for better translations. Recently, several researchers proposed to improve NMT with SMT features or outputs. For example, He et al. (2016) integrated SMT features with the NMT model under the log-linear framework in the beam search on the development or test set. Stahlberg et al. (2016) extended the beam search decoding by expanding the search space of NMTs with translation hypotheses produced by a syntactic SMT model. In the above work, NMT was treated as a black-box and the combinations were carried out only in the testing phase.\nIn this work, we move a step forward along the same direction by incorporating the SMT model into the training phase of NMT. This enables NMT to effectively learn to incorporate SMT recommendations, rather than to heuristically combine two trained models. Specifically, SMT model\nar X\niv :1\n61 0.\n05 15\n0v 1\n[ cs\n.C L\n] 1\n7 O\nct 2\n01 6\nis firstly trained independently on a bilingual corpus using the conventional phrase-based SMT approach (Koehn, Och, and Marcu 2003). At each decoding step, in both training and testing phases, the trained SMT model provides translation recommendations based on the decoding information from NMT, including the generated partial translation and the attention history.\nWe employ an auxiliary classifier to score the SMT recommendations, and use a gating function to linearly combine the two probabilities between the NMT generations and SMT recommendations. The gating function reads the current decoding information, and thus is able to dynamically assign weights to NMT and SMT probabilities at different decoding steps. Both the SMT classifier and gating function are jointly trained within the NMT architecture in an end-to-end manner. In addition, to better alleviate the UNK problem in the testing phase, we select a proper SMT recommendation to replace a target UNK word by jointly considering the attention probabilities from the NMT model and the coverage information from the SMT model. Experimental results show that the proposed approach can achieve significant improvements of 2.44 BLEU point over the NMT baseline and 3.21 BLEU point over the Moses (SMT) system on five Chinese-English NIST test sets."}, {"heading": "Background", "text": "In this section, we give a brief introduction to NMT and phrase-based SMT, the most popular SMT model."}, {"heading": "Neural Machine Translation", "text": "Given a source sentence x = x1, x2, ..., xTx , attention-based NMT (Bahdanau, Cho, and Bengio 2015) encodes it into a sequence of vectors, then uses the sequence of vectors to generate a target sentence y = y1, y2, ..., yTy .\nAt the encoding step, attention-based NMT uses a bidirectional RNN which consists of forward RNN and backward RNN (Schuster and Paliwal 1997) to encode the source sentence. The forward RNN reads the source sentence x in a forward direction, generating a sequence of forward hidden states \u2212\u2192h = [\u2212\u2192 h1, \u2212\u2192 h2, ..., \u2212\u2192 hTx ] . The backward RNN reads the\nsource sentence x in a backward direction, generating a sequence of backward hidden states \u2190\u2212h = [\u2190\u2212 h1, \u2190\u2212 h2, ..., \u2190\u2212 hTx ] .\nThe pair of hidden states at each position are concatenated to form the annotation of the word at the position, yielding the annotations of the entire source sentence h = [h1, h2, ..., hTx ], where\nh>j = [\u2212\u2192 h>j ; \u2190\u2212 h>j ] (1)\nAt the decoding step t, after outputting target sequence y<t = y1, y2, ..., yt\u22121, the next word yt is generated with probability\np(yt|y<t,x) = softmax(f(st, yt\u22121, ct)) (2)\nwhere f(\u00b7) is a non-linear activation function and st is the decoder\u2019s hidden state at step t:\nst = g(st\u22121, yt\u22121, ct) (3)\nwhere g(\u00b7) is a non-linear activation function. Here we use Gated Recurrent Unit (Cho et al. 2014b) as the activation function for the encoder and decoder. ct is the context vector, computed as a weighted sum of the annotations of the source sentence:\nct = Tx\u2211 j=1 \u03b1t,jhj (4)\nwhere hj is the annotation of source word xj and its weight \u03b1t,j is computed by the attention model."}, {"heading": "Statistical Machine Translation", "text": "Most SMT models are defined with the log-linear framework (Och and Ney 2002).\np(y|x) = exp( \u2211M m=1 \u03bbmhm(y,x))\u2211\ny\u2032 exp( \u2211M m=1 \u03bbmhm(y \u2032 ,x))\n(5)\nwhere hm(y,x) is a feature function and \u03bbm is its weight. Standard SMT features include the bidirectional translation probabilities, the bidirectional lexical translation probabilities, the language model, the reordering model, the word penalty and the phrase penalty. The feature weights can be tuned by the minimum error rate training (MERT) algorithm (Och 2003).\nDuring translation, the SMT decoder expands partial translation (called translation hypothesis in SMT) y<t = y1, y2, ..., yt\u22121 by selecting a proper target word/phrase translation for an untranslated source span from a bilingual phrase table."}, {"heading": "NMT with SMT Recommendations", "text": "Different from attention-based NMT which predicts the next word based on vector representations, the proposed model makes the prediction also based on recommendations from an SMT model. The SMT model can be separately trained on a bilingual corpus using the conventional phrase-based SMT approach (Koehn, Och, and Marcu 2003). Given decoding information from NMT, the SMT model makes word recommendations1 for the next word prediction with SMT features. To integrate the SMT recommendations into the proposed model, we employ a classifier to score the recommendations and a gate to combine SMT recommendations with NMT word prediction probabilities.\nAs shown in Figure 1, the word generation process of the proposed model has three steps:\n1. Inheriting from standard attention-based NMT, the NMT classifier (i.e., \u201cclassifierNMT\u201d) estimates word prediction probabilities on the regular vocabulary V nmt:\npnmt(yt|y<t,x) = softmax(f(st, yt\u22121, ct)) (6)\n2. The SMT classifier (i.e., \u201cclassifierSMT\u201d) computes probabilities of SMT recommendations, which are generated by the auxiliary SMT model.\n1We have not adopted the recently proposed phraseNet which incorporates target phrases into the NMT decoder (Tang et al. 2016). We leave the investigation to future work.\n3. The proposed model updates word prediction probabilities by using a gating mechanism (i.e., \u201cgate\u201d in brown box).\nIn the following subsections, we first elaborate on how the SMT model produces recommendations based on decoding information from the NMT model, which is the main feature of the proposed method. Then we illustrate how to integrate the SMT recommendations into NMT with an SMT classifier and a gating mechanism. Finally, we propose a novel approach to handle UNK in NMT with SMT recommendations."}, {"heading": "Generating SMT Recommendations", "text": "Given previously generated words y<t = y1, y2, ..., yt\u22121 from NMT, SMT generates the next word recommendations, and computes their scores by\nSMTscore(yt|y<t,x) = M\u2211\nm=1\n\u03bbmhm(yt, xt) (7)\nwhere yt is an SMT recommendation and xt is its corresponding source span. hm(yt, xt) is a feature function and \u03bbm is its weight. The SMT model can generate proper word recommendations through expanding generated words (partial translation). However there are two problems with SMT recommendations due to the different word generation mechanisms between SMT and NMT:\nCoverage information SMT lacks of coverage information about the source sentence. In conventional SMT, the decoder maintains a coverage vector to indicate whether a source word/phrase is translated or not, and generates a next target word/phrase from the untranslated part of the source sentence. This coverage information is however missing since our SMT decoder has only the partial translation produced by NMT, which often leads to inappropriate recommendations due to the lack of coverage information.\nTo overcome this problem, we introduce an SMT coverage vector cv = [cv1, cv2, ..., cvTx ] for the SMT model. cvi = 1 means that the source word xi has been translated and SMT will not recommend target words from translated source words. We initialize the SMT coverage vector with zeros. At each decoding step, if the output word which is generated by the proposed model is in the SMT recommendations, SMT coverage vector will be updated. Specifically, as SMT contains the positions of the recommendation words at the source sentence, we can update the SMT coverage vector by setting the corresponding vector element to 1.\nAlignment information The SMT reordering model lacks of alignment information between source and target sentences. Due to the word order difference between languages, the reordering model plays an import role in existing SMT approaches. Typically, the SMT model computes reordering model features based on word alignments. For example, distance-based reordering model computes the reordering cost as follows (Koehn, Och, and Marcu 2003):\nd(yt) = \u2212 \u2223\u2223spyt \u2212 spyt\u22121 \u2212 1\u2223\u2223 (8)\nwhere spy denotes the position of source word which is aligned to target word y. As mentioned in the section of Background, NMT utilizes attention mechanism to align words between the target and source sides. As shown in Equation (4), instead of aligning to a specific source word, each target word is aligned to all the source words with corresponding alignment weights. Therefore the SMT reordering model can not directly work with the attention mechanism of NMT.\nTo address this issue, reordering cost is computed between a target word and the corresponding source words by using the alignment weights. Similar to the estimation of bidirectional word translation probabilities in (He et al. 2016), the reordering cost is computed as follows:\nd(yt) = \u2212 Tx\u2211 j=1 \u03b1t\u22121,j |spyt \u2212 j \u2212 1| (9)\nwhere \u03b1t\u22121,j is alignment weight produced by NMT. The lexical reordering model can work in a similar way.\nThe last question is: what kind of words does SMT recommend? Considering the second limitation of NMT described in the section of Introduction, we keep content words and filter out stop words from SMT recommendations. Now at each decoding step, with the SMT coverage vector and NMT attention mechanism, SMT can use SMT features2 to produce word recommendations which convey the meaning of the untranslated source word/phrase."}, {"heading": "Integrating SMT Recommendations into NMT", "text": "SMT Classifier After SMT generates recommendations, a classifier scores the recommendations and generates probability estimates on them. To ensure the quality of SMT\n2In training, as words out of NMT vocabulary V nmt may also appear in previously generated words, we should train a new language model by replacing the words with UNK and use this language model to score a target sequence.\nrecommendations, we adopt two strategies to filter lowquality recommendations: 1) only top Ntm translations for each source word are retained according to their translation scores, each of which is computed as weighted sum of translation probabilities3. 2) top Nrec recommendations with highest SMT scores are selected, each of which is computed as weighted sum of SMT features.\nAt the decoding step t, the SMT classifier takes current hidden state st, previous word yt\u22121, context vector ct and SMT recommendation yt as input to estimate word probability of recommendation on vocabulary V smtt . We denote the word estimated probability as: psmt(yt|y<t,x) = softmax(scoresmt(yt|y<t,x)) (10)\nwhere scoresmt(yt|y<t,x) is the scoring function for SMT recommendation defined as follows:\nscoresmt(yt|y<t,x) = gsmt(st, yt\u22121, yt, ct) (11) where gsmt(\u00b7) is an activation function which is either an identity or a non-linear function.\nAs we do not want to introduce extra embedding parameters, we let SMT recommendations share the same target word embedding matrix with the NMT model. In this case, SMT word recommendation which is out of regular vocabulary V nmt is replaced by UNK word in the SMT classifier. Since a UNK word does not retain the meaning of the original words, SMT will not record the source coverage information if a UNK word is generated. Gating Mechanism At last, a gate is introduced to update word prediction probabilities for the proposed model. It is computed as follows:\n\u03b1t = ggate(st, yt\u22121, ct) (12) where ggate(\u00b7) is sigmoid function.\nWith the help of the gate, we update word prediction probabilities on regular vocabulary V nmt by combining the two probabilities through linear interpolation between the NMT generations and SMT recommendations:\np(yt|y<t,x) = (1\u2212 \u03b1t)pnmt(yt|y<t,x) +\u03b1tpsmt(yt|y<t,x) (13)\nNote that psmt(yt|y<t,x) = 0 for yt /\u2208 V smtt . In Equation (13), when the gate is close to 0, the proposed model will ignore the SMT recommendations and only focus on the NMT word prediction (a stop word may also be generated). This effectively allows our model to drop SMT recommendations that are irrelevant."}, {"heading": "Handling UNK with SMT recommendations", "text": "To address the UNK word problem, we further directly utilize SMT recommendations to replace UNK words in testing phase. For each UNK word, we choose the recommendation with the highest SMT score as the final replacement.4\n3Bidirectional translation probabilities and bidirectional lexical translation probabilities.\n4There is no UNK word in the previously generated words as our model generates the target sentence from left to right. Here we can use the original language model to score the target sequence. Language model which is trained on large monolingual corpora help make accurate replacements.\nOur method is similar to the method described in (He et al. 2016), both of which can make use of rich contextual information on both the source and target sides to handle UNK. The difference is that our method takes into account the reordering information and the SMT coverage vector to generate more reliable recommendations."}, {"heading": "Model Training", "text": "We train the proposed model by minimizing the negative log-likelihood on a set of training data {(xn,yn)}Nn=1:\nC(\u03b8) = \u2212 1 N N\u2211 n=1 Ty\u2211 t=1 log p(ynt |yn<t,xn) (14)\nThe cost function of our proposed model is the same as a conventional attention-based NMT model, except that we introduce extra parameters for the SMT classifier and the gate. We optimize our model by minimizing the cost function. In practice, we use a simple pre-training strategy to train our model. We first train a regular attention-based NMT model. Then we use this model to initialize the parameters of encoder and decoder of the proposed model and use random initialization to initialize the parameters of the SMT classifier and the gate. At last, we train all parameters of our model to minimize the cost function.\nWe adopt this pre-training strategy for two reasons: 1) The training of the proposed model may take a longer time with computation of SMT recommendation features and ranking of SMT recommendations, both of which are timeconsuming. We treat the pre-trained model as an anchor point, the peak of the prior distribution in model space, which allows us to shorten the training time. 2) The quality of automatically learned attention weights is crucial for computing SMT reordering cost (see Equation (9)). Low quality of attention weights obtained without pre-training may produce unreliable SMT recommendations and consequently negatively affect the training of the proposed model."}, {"heading": "Experiments", "text": "In this section, we evaluate our approach on Chinese-English machine translation."}, {"heading": "Setup", "text": "The training set is a parallel corpus from LDC, containing 1.25M sentence pairs with 27.9M Chinese words and 34.5M English words5. We use NIST 2006 dataset as development set, and NIST 2002, 2003, 2004, 2005 and 2008 datasets as test sets. Case-insensitive BLEU score6 is adopted as evaluation metric. We compare our proposed model with two state-of-the-art systems:\n* Moses: a state-of-the-art phrase-based SMT system with its default setting.\n5The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.\n6ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl\n* RNNSearch: an attention-based NMT system with its default setting. We use the open source system GroundHog7 as the NMT baseline.\nFor Moses, we use the full training data (parallel corpus) to train the model and the target portion of the parallel corpus to train a 4-gram language model using the KenLM8 (Heafield 2011). We use the default system setting for both training and testing.\nFor RNNSearch, we use the parallel corpus to train the attention-based NMT model. We limit the source and target vocabularies to the most frequent 30K words in Chinese and English, covering approximately 97.7% and 99.3% of the data in the two languages respectively. All other words are mapped to a special token UNK. We train the model with the sentences of length up to 50 words in training data and keep the test data at the original length. The word embedding dimension of both sides is 620 and the size of hidden layer is 1000. All the other settings are the same as in (Bahdanau, Cho, and Bengio 2015). We also use our implementation of RNNSearch which adopts feedback attention and dropout as NMT baseline system. Dropout is applied only on the output layer and the dropout rate is set to 0.5.\nFor the proposed model, we put it on the top of encoder same as in (Bahdanau, Cho, and Bengio 2015). As for pretraining, we train the regular attention-based NMT model using our implementation of RNNSearch and use its parameters to initialize the NMT part of our proposed model.\nWe use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler 2012) to train the NMT models and the decay rates \u03c1 and are set as 0.95 and 10\u22126. Each SGD update direction is computed using a minibatch of 80 sentences.\nFor SMT recommendation, we re-implement an SMT decoder which only adopts 6 features. The adopted features are bidirectional translation features, bidirectional lexical translation features, language model feature and distancebased reordering feature. Lexical reordering features are abandoned in order to speed up the SMT recommendation process. As for word-level recommendation, word penalty feature and phrase penalty feature are also abandoned as they cannot provide useful information for calculating recommendation scores. The feature weights are obtained from\n7https://github.com/lisa-groundhog/GroundHog 8https://kheafield.com/code/kenlm/\nMoses9. We add English punctuations into the stop word list and remove numerals from the list. Finally the stop list contains 572 symbols. To ensure the quality of SMT recommendations, we setNtm to 5 andNrec to 25. We adopt a forward neural network with two hidden layers for the SMT classifier (Equation (11)) and gating function (Equation (12)). The numbers of units in the hidden layers are 200 and 50 respectively."}, {"heading": "Results", "text": "Table 1 reports the experiment results measured in terms of BLEU score. We find that our implementation RNNSearch\u2217 using feedback attention and dropout outperforms Groundhog and Moses by 1.51 BLEU point and 0.77 BLEU point. RNNSearch\u2217+SMTrec using SMT recommendations but keeps target UNK words achieves a gain of up to 1.20 BLEU point over RNNSearch\u2217. Surprisingly, RNNSearch\u2217+SMTrec,UNKReplace which further replaces UNK word with SMT recommendation during translation, achieves further improvement over RNNSearch\u2217+SMTrec by 1.24 BLEU point. It outperforms RNNSearch\u2217 and Moses by 2.44 BLEU point and 3.21 BLEU point respectively.\nWe also conduct additional experiments to validate the effectiveness of the key components of our proposed model, namely SMT recommendations and gating mechanism. More specifically, we adopt the following three tests: (1) we set a fixed gate value 0 for RNNSearch\u2217+SMTrec, to block SMT recommendations (+\u03b1 = 0 in Table 2); (2) we set a fixed gate value 0.20 for RNNSearch\u2217+SMTrec, to change the gating mechanism to a fixed mixture (+\u03b1 = 0.20 in Table 2); (3) we randomly generate some high frequency target words and submit the pseudo recommendations to RNNSearch\u2217+SMTrec during translation, to deliberately confuse RNNSearch\u2217+SMTrec (+pseudo recs in Table 2). From Table 2, we can observe that:\n1. Without SMT recommendations, the proposed model suffers from degraded performance (-1.55 BLUE point). This indicates that SMT recommendations are essential for RNNSearch\u2217+SMTrec.\n2. Without a flexible gating mechanism, the proposed model performances on all test sets deteriorate considerably (-\n9Compared with other weights, the weight of distance-based reordering feature is too small, therefore we manually increase it by 10 times.\n2.09 BLEU point). This shows that gating mechanism plays an important role in RNNSearch\u2217+SMTrec.\n3. The experiment results in Table 2 empirically show that the proposed model makes wrong translations and has a significant decrease in performance (- 1.54 BLEU point), which demonstrates that the quality of SMT recommendations is also very important for RNNSearch\u2217+SMTrec."}, {"heading": "Related Work", "text": "In this section we briefly review previous studies that are related to our work. Here we divide previous work into three categories:\nCombination of SMT and NMT: Stahlberg et al. (2016) extended the beam search decoding by expanding the search space of NMT with translation hypotheses produced by a syntactic SMT model. He et al. (2016) enhanced NMT system with effective SMT features. They integrated three SMT features, namely translation model, word reward feature and language model, with the NMT model under the log-linear framework in the beam search. Arthur et al. (2016) proposed to incorporate discrete translation lexicons into NMT model. They calculated lexical predictive probability and integrated the probability with the NMT model probability to predict the next word. Wuebker et al. (2016) applied phrase-based and neural models to complete partial translations in interactive machine translation and find the models can improve next-word suggestion. The significant difference between our work and these studies is that NMT is treated as a black-box in the previous work, while in our work the NMT and SMT models are tightly integrated with the execution of the former being advised by the latter in the training and testing phase.\nCoverage problem in NMT: Tu et al. (2016) proposed a coverage mechanism for NMT to alleviate the \u201cover-translation\u201d and \u201cunder-translation\u201d problems. They introduced a coverage vector for the attention model, to make the NMT model consider more about untranslated source words in the target word generation. Cohn et al. (2016) enhanced the attention model with structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. Feng et al. (2016) proposed to add implicit distortion and fertility models to attention model.\nThese studies tackle the coverage problem by enhancing the encoder of NMT. As we incorporate SMT model into the decoder part of NMT, our work is complementary to the above studies.\nUNK word problem in NMT: Luong et al. (2015) proposed several approaches to track the source word of a UNK word in the target sentence. They first attached aligned information for each target UNK word in training data and trained a model on the data, and then they used the model to generate translation with UNK alignment information. Jean et al. (2015) proposed an efficient approximation based on importance sampling on the softmax function which allows to train NTM with very large vocabulary. On the other hand, instead of modeling word unit, some work focused on smaller unit modeling (Chitnis and DeNero 2015; Sennrich, Haddow, and Birch 2016), especially on character modeling (Ling et al. 2015; Costa-Jussa\u0300 and Fonollosa 2016; Luong and Manning 2016; Chung, Cho, and Bengio 2016). Our work is also motivated by Copynet (Gu et al. 2016), which incorporates copying mechanism in sequence-to-sequence learning."}, {"heading": "Conclusion", "text": "In this paper, we have presented a novel approach that incorporates SMT model into NMT with attention mechanism. Our proposed model remains the power of end-to-end NMT while alleviating its limitations by utilizing recommendation from SMT for better generation in NMT. Different from prior work which usually used a separately trained NMT model as an additional feature, our proposed model containing NMT and SMT is trained in an end-to-end manner. Experiment results on Chinese-English translation have demonstrated the efficacy of the proposed model."}], "references": [{"title": "S", "author": ["P. Arthur", "G. Neubig", "Nakamura"], "venue": "2016. Incorporating discrete translation lexicons into neural machine translation. In EMNLP", "citeRegEx": "Arthur. Neubig. and Nakamura 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Y", "author": ["D. Bahdanau", "K. Cho", "Bengio"], "venue": "2015. Neural machine translation by jointly learning to align and translate. In ICLR", "citeRegEx": "Bahdanau. Cho. and Bengio 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "R", "author": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "Mercer"], "venue": "L.", "citeRegEx": "Brown et al. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "D", "author": ["Chiang"], "venue": "2005. A hierarchical phrase-based model for statistical machine translation. In ACL", "citeRegEx": "Chiang 2005", "shortCiteRegEx": null, "year": 2015}, {"title": "J", "author": ["R. Chitnis", "DeNero"], "venue": "2015. Variable-length word encodings for neural translation models. In EMNLP", "citeRegEx": "Chitnis and DeNero 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho"], "venue": null, "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho"], "venue": "EMNLP", "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "Y", "author": ["J. Chung", "K. Cho", "Bengio"], "venue": "2016. A character-level decoder without explicit segmentation for neural machine translation. In ACL", "citeRegEx": "Chung. Cho. and Bengio 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["T. Cohn", "C.D.V. Hoang", "E. Vymolova", "K. Yao", "C. Dyer", "Haffari"], "venue": "2016. Incorporating structural alignment biases into an attentional neural translation model. In NAACL", "citeRegEx": "Cohn et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "2016", "author": ["M.R. Costa-Juss\u00e0", "Fonollosa", "J. A"], "venue": "Character-based neural machine translation. In ACL", "citeRegEx": "Costa.Juss\u00e0 and Fonollosa 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Implicit distortion and fertility models for attentionbased encoder-decoder nmt model", "author": ["Feng"], "venue": null, "citeRegEx": "Feng,? \\Q2016\\E", "shortCiteRegEx": "Feng", "year": 2016}, {"title": "2016", "author": ["J. Gu", "Z. Lu", "H. Li", "V. O Li"], "venue": "Incorporating copying mechanism in sequence-to-sequence learning. In ACL", "citeRegEx": "Gu et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "H", "author": ["W. He", "Z. He", "H. Wu", "Wang"], "venue": "2016. Improved neural machine translation with smt features. In AAAI", "citeRegEx": "He et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Y", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Bengio"], "venue": "2015. On using very large target vocabulary for neural machine translation. In ACL", "citeRegEx": "Jean et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "P", "author": ["N. Kalchbrenner", "Blunsom"], "venue": "2013. Recurrent continuous translation models. In EMNLP", "citeRegEx": "Kalchbrenner and Blunsom 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "D", "author": ["P. Koehn", "F.J. Och", "Marcu"], "venue": "2003. Statistical phrase-based translation. In NAACL", "citeRegEx": "Koehn. Och. and Marcu 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "A", "author": ["W. Ling", "I. Trancoso", "C. Dyer", "Black"], "venue": "W.", "citeRegEx": "Ling et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "2016", "author": ["Luong", "M.-T.", "Manning", "C. D"], "venue": "Achieving open vocabulary neural machine translation with hybrid word-character models. In ACL", "citeRegEx": "Luong and Manning 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Q", "author": ["M.-T. Luong", "I. Sutskever", "Le"], "venue": "V.; Vinyals, O.; and Zaremba, W.", "citeRegEx": "Luong et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A deep memory-based architecture for sequenceto-sequence learning", "author": ["Meng"], "venue": "arXiv preprint arXiv:1506.06442", "citeRegEx": "Meng,? \\Q2015\\E", "shortCiteRegEx": "Meng", "year": 2015}, {"title": "and Ney", "author": ["F.J. Och"], "venue": "H.", "citeRegEx": "Och and Ney 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "F", "author": ["Och"], "venue": "J.", "citeRegEx": "Och 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "K", "author": ["M. Schuster", "Paliwal"], "venue": "K.", "citeRegEx": "Schuster and Paliwal 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Neural machine translation of rare words with subword units", "author": ["Haddow Sennrich", "R. Birch 2016] Sennrich", "B. Haddow", "A. Birch"], "venue": "In 54th ACL,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Syntactically guided neural machine translation", "author": ["Stahlberg"], "venue": "arXiv preprint arXiv:1605.04569", "citeRegEx": "Stahlberg,? \\Q2016\\E", "shortCiteRegEx": "Stahlberg", "year": 2016}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le"], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "P", "author": ["Y. Tang", "F. Meng", "Z. Lu", "H. Li", "Yu"], "venue": "L.", "citeRegEx": "Tang et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "H", "author": ["Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "Li"], "venue": "2016. Modeling coverage for neural machine translation. In ACL", "citeRegEx": "Tu et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Models and inference for prefix-constrained machine translation", "author": ["Wuebker"], "venue": "In 54th ACL-Volume", "citeRegEx": "Wuebker,? \\Q2016\\E", "shortCiteRegEx": "Wuebker", "year": 2016}, {"title": "M", "author": ["Zeiler"], "venue": "D.", "citeRegEx": "Zeiler 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "Neural Machine Translation (NMT) is a new approach to machine translation that has made great progress in recent years. However, recent studies show that NMT generally produces fluent but inadequate translations (Tu et al. 2016; He et al. 2016). This is in contrast to conventional Statistical Machine Translation (SMT), which usually yields adequate but non-fluent translations. It is natural, therefore, to leverage the advantages of both models for better translations, and in this work we propose to incorporate SMT model into NMT framework. More specifically, at each decoding step, SMT offers additional recommendations of generated words based on the decoding information from NMT (e.g., the generated partial translation and attention history). Then we employ an auxiliary classifier to score the SMT recommendations and a gating function to combine the SMT recommendations with NMT generations, both of which are jointly trained within the NMT architecture in an end-to-end manner. Experimental results on Chinese-English translation show that the proposed approach achieves significant and consistent improvements over state-of-the-art NMT and SMT systems on multiple NIST test sets.", "creator": "LaTeX with hyperref package"}}}