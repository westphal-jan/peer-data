{"id": "1610.05492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Federated Learning: Strategies for Improving Communication Efficiency", "abstract": "federated communication assumes a new learning setting whereby the goal applies to train a thread - quality infrastructure model with training data delivery over random distribution number of variables compliant with unreliable despite relatively slow network connections. we consider learning platforms employing instance situation where on each startup, each client module computes neural assembly to run managed algorithm based on its local traffic, itself communicates another load plus a central server, various subsequently developer - side updates are aggregated to compute a new global model. the typical clients in linux setting are mobile vendors, and communication efficiency is there utmost importance. in both paper, we propose two objectives to decrease memory uplink communication costs. the optimal methods are evaluated on the application of training a deep neural learning simulator perform spatial classification. our collaborative approach reduces some upload task required to train a reasonable model assuming numerous parts per magnitude.", "histories": [["v1", "Tue, 18 Oct 2016 09:11:51 GMT  (82kb,D)", "http://arxiv.org/abs/1610.05492v1", null], ["v2", "Mon, 30 Oct 2017 20:52:14 GMT  (3035kb,D)", "http://arxiv.org/abs/1610.05492v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jakub kone\\v{c}n\\'y", "h brendan mcmahan", "felix x yu", "peter richt\\'arik", "ananda theertha suresh", "dave bacon"], "accepted": false, "id": "1610.05492"}, "pdf": {"name": "1610.05492.pdf", "metadata": {"source": "CRF", "title": "Federated Learning: Strategies for Improving Communication Efficiency", "authors": ["Jakub Kone\u010dn\u00fd", "H. Brendan McMahan", "Felix X. Yu", "Peter Richt\u00e1rik", "Ananda Theertha Suresh", "Dave Bacon"], "emails": ["J.Konecny@sms.ed.ac.uk,", "felixyu}@google.com", "peter.richtarik@ed.ac.uk,", "dabacon}@google.com"], "sections": [{"heading": "1 Introduction", "text": "As datasets grow larger and models more complex, machine learning increasingly requires distributing the optimization of model parameters over multiple machines. Existing machine learning algorithms are designed for highly controlled environments (such as data centers) where the data is distributed among machines in a balanced and i.i.d. fashion, and high-throughput networks are available. Federated learning [10, 14, 9] proposes an alternative setting, where we train a shared global model under the coordination of a central server, from a federation of participating devices. The participating devices (clients) are typically large in number and have slow or unstable internet connections. A motivating example for federated optimization arises when the training data is kept locally on users\u2019 mobile devices, and the devices are used as nodes performing computation on their local data in order to update a global model. The above framework differs from conventional distributed machine learning [18, 12, 20, 23, 5, 4] due to the the large number of clients, highly unbalanced and non-i.i.d. data and unreliable network connections. Federated learning offers distinct advantages compared to performing learning in the data center. The model update is generally less privacy-sensitive than the data itself, and the server never needs to store these updates. Thus, when applicable, federated learning can significantly reduce privacy and security risks by limiting the attack surface to only the device, rather than the device and the cloud. This approach also leverages the data-locality and computational power of the large number of mobile devices.\nFor simplicity, we consider synchronized algorithms for federated learning [14, 3], where a typical round consists of the following steps:\n1. A subset of clients is selected, each of which downloads the current model. \u2217Work performed while at Google, Inc.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 0.\n05 49\n2v 1\n[ cs\n.L G\n] 1\n8 O\nct 2\n2. Each client in the subset computes an updated model based on their local data. 3. The updated models are sent from the selected clients to the sever. 4. The server aggregates these models (typically by averaging) to construct an improved global\nmodel.\nA naive implementation of the above framework requires that each client sends a full model (or a full model gradient) back to the server in each round. For large models, this step is likely to be the bottleneck of federated learning due to the asymmetric property of internet connections: the uplink is typically much slower than downlink. The US average broadband speed was 55.0Mbps download vs. 18.9Mbps upload, with some ISPs being significantly more asymmetric, e.g., Xfinity at 125Mbps down vs. 15Mbps up [1]. It is therefore important to investigate methods which can reduce the uplink communication cost. In this paper, we study two general approaches:\n\u2022 Structured updates, where we learn an update from a restricted lower-dimensional space. \u2022 Sketched updates, where we learn a full model update, but then compress it before sending\nto the server.\nThese approaches can be combined, e.g., first learning a structured update and then sketching it; however, we do not experiment with this combination in the current work.\nIn the following, we formally describe the problem. The goal of federated learning is to learn a model with parameters embodied in a real matrix W \u2208 Rd1\u00d7d2 from data stored across a large number of clients. We first provide a communication-naive version of the federated learning. In round t \u2265 0, the server distributes the current model Wt to a subset St of nt clients (for example, to a selected subset of clients whose devices are plugged into power, have access to broadband, and are idle). These clients independently update the model based on their local data. Let the updated local models be W1t ,W 2 t , . . . ,W nt t , so the updates can be written as H i t := W i t \u2212Wt, for i \u2208 St. Each selected client then sends the update back to the sever, where the global update is computed by aggregating2 all the client-side updates:\nWt+1 = Wt + \u03b7tHt, Ht := 1 nt \u2211 i\u2208St H i t.\nThe sever chooses the learning rate \u03b7t (for simplicity, we choose \u03b7t = 1). Recent works show that a careful choice of the server-side learning rate can lead to faster convergence [13, 12, 10].\nIn this paper, we describe federated learning for neural networks, where we use a separate 2D matrix W to represent the parameters of each layer. We suppose that W gets right-multiplied, i.e., d1 and d2 represent the output and input dimensions respectively. Note that the parameters of a fully connected layer are naturally represented as 2D matrices. However, the kernel of a convolutional layer is a 4D tensor of the shape #input \u00d7 width \u00d7 height \u00d7#output. In such a case, W is reshaped from the kernel to the shape (#input\u00d7 width\u00d7 height)\u00d7#output. The goal of increasing communication efficiency of federated learning is to reduce the cost of sending Hit to the server. In this paper, we propose two general strategies of achieving this, discussed next."}, {"heading": "2 Structured Update", "text": "The first type of communication efficient update restricts the updates Hit to have a pre-specified structure. Two types of structures are considered in the paper:\nLow rank. We enforce Hit \u2208 Rd1\u00d7d2 to be low-rank matrices of rank at most k, where k is a fixed number. We express Hit as the product of two matrices: H i t = A i tB i t, where A i t \u2208 Rd1\u00d7k, Bit \u2208 Rk\u00d7d2 , and Ait is generated randomly and fixed, and only Bit is optimized. Note that Ait can then be compressed in the form of a random seed and the clients only need to send Bit to the server. We also tried fixing Bit and training A i t, as well as training both A i t and B i t; neither performed as well. Our approach seems to perform as well as the best techniques considered in [6],\nRandom mask. We restrict the update Hit to be sparse matrices, following a pre-defined random sparsity pattern (i.e., a random mask). The pattern is generated afresh in each round and for each client. Similar to the low-rank approach, the sparse pattern can be fully specified by a random seed, and therefore it is only required to send the values of the non-zeros entries of Hit. This strategy can be seen as the combination of the master training method and a randomized block coordinate minimization approach [16, 15].\n2A weighted sum might be used to replace the average based on specific implementations."}, {"heading": "3 Sketched Update", "text": "The second type of communication-efficient update, which we call sketched, first computes the full unconstrained Hit, and then encodes the update in a (lossy) compressed form before sending to the server. The server decodes the updates before doing the aggregation. Such sketching methods have application in many domains [21]. We propose two ways of performing the sketching:\nSubsampling. Instead of sending Hit, each client only communicates matrix H\u0302it which is formed from a random subset of the (scaled) values of Hit. The server then averages the sampled updates, producing the global update H\u0302t. This can be done so that the average of the sampled updates is an unbiased estimator of the true average: E[H\u0302t] = Ht. Similar to the random mask structured update, the mask is randomized independently for each client in each round, and the mask itself is stored as a synchronized seed. It was recently shown that, in a certain setting, the expected iterates of SGD converge to the optimal point [19]. Perturbing the iterates by a random matrix of zero mean, which is what our subsampling strategy would do, does not affect this type of convergence.\nProbabilistic quantization. Another way of compressing the updates is by quantizing the weights. We first describe the algorithm of quantizing each scalar to one bit. Consider the update Hit, let h = (h1, . . . , hd1\u00d7d2) = vec(H i t), and let hmax = maxj(hj), hmin = minj(hj). The compressed update of h, denoted by h\u0303, is generated as follows:\nh\u0303j =\n{ hmax, with probability\nhj\u2212hmin hmax\u2212hmin\nhmin, with probability hmax\u2212hj hmax\u2212hmin .\nIt is easy to show that h\u0303 is an unbiased estimator of h. This method provides 32\u00d7 of compression compared to a 4 byte float. One can also generalize the above to more than 1 bit for each scalar. For b-bit quantization, we first equally divide [hmin, hmax] into 2b intervals. Suppose hi falls in the interval bounded by h\u2032 and h\u2032\u2032. The quantization operates by replacing hmin and hmax of the above equation by h\u2032 and h\u2032\u2032, respectively. Incremental, randomized and distributed optimization algorithms can be similarly analyzed in a quantized updates setting [17, 8, 7].\nImproving the quantization by structured random rotations. The above 1-bit and multi-bit quantization approaches work best when the scales are approximately equal across different dimensions. For example, when max = 100 and min = \u2212100 and most of values are 0, the 1-bit quantization will lead to large quantization error. We note that performing a random rotation of h before the quantization (multiplying h by an orthogonal matrix) will resolve this issue. In the decoding phase, the server needs to perform the inverse rotation before aggregating all the updates. Note that in practice, the dimension of h can be as high as d = 1e6, and it is computationally prohibitive to generate (O(d3)) and apply (O(d2)) a rotation matrix. In this work, we use a type of structured rotation matrix which is the product of a Walsh-Hadamard matrix and a binary diagonal matrix, motivated by the recent advance in this topic [22]. This reduces the computational complexity of generating and applying the matrix to O(d) and O(d log d)."}, {"heading": "4 Experiments", "text": "We conducted the experiments using federated learning to train deep neural networks for the CIFAR10 image classification task [11]. There are 50,000 training examples, which we partitioned into 100 clients each containing 500 training examples. The model architecture was taken from the TensorFlow tutorial [2], which consists of two convolutional layers followed by two fully connected layers and then a linear transformation layer to produce logits, for a total of over 1e6 parameters. While this model is not the state-of-the-art, it is sufficient for our needs, as our goal is to evaluate our compression methods, not achieve the best possible accuracy on this task.\nWe employ the Federated Averaging algorithm [14], which significantly decreases the number of rounds of communication required to train a good model. However, we expect our techniques will show a similar reduction in communication costs when applied to synchronized SGD. For Federated Averaging, on each round we select 10 clients at random, each of which performs 10 epochs of SGD with a learning rate of \u03b7 on their local dataset using minibatches of 50 images, for a total of 100 local updates. From this updated model we compute the deltas for each layer Hit.\nWe define medium and high low-rank/sampling parameterizations that result in the same compression rates for both approaches, as given in Table 1. The left and center columns of Figure 1 present nonquantized results for test-set accuracy, both as a function of the number of rounds of the algorithm, and the total number of megabytes uploaded. For all experiments, learning rates were tuned using a multiplicative grid of resolution \u221a 2 centered at 0.15; we plot results for the learning rate with the best median accuracy over rounds 400 \u2013 800. We used a multiplicative learning-rate decay of 0.988, which we selected by tuning only for the baseline algorithm.\nFor medium subsampling, all three approaches provide a dramatic improvement in test set accuracy after a fixed amount of bandwidth usage; the lower row of plots shows little loss in accuracy as a function of the number of update rounds. The exception is that the StructLowRank approach performs poorly for the high subsampling parameters. This may suggest that requiring a low-rank update structure for the convolution layers works poorly. Also, perhaps surprisingly, we see no advantage for StructMask, which optimizes for a random sparse set of coefficients, as compared to SketchMask, which chooses a sparse set of parameters to update after a full update is learned.\nThe right two plots in Figure 1 give results for SketchMask and SketchRotMask, with and without binary quantization; we consider only the medium subsampling regime which is representative. We observe that (as expected) introducing the random rotation without quantization has essentially no effect (solid red and orange lines). However, binary quantization dramatically decreases the total communication cost, and further introducing the random rotation significantly speeds convergence, and also allows us to converge to a higher level of accuracy. We are able to learn a reasonable model (70% accuracy) in only \u223c100MB of communication, two orders of magnitude less than the baseline."}], "references": [{"title": "Revisiting distributed synchronous SGD", "author": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "venue": "In ICLR Workshop Track,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["Trishul Chilimbi", "Yutaka Suzue", "Johnson Apacible", "Karthik Kalyanaraman"], "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "On randomized distributed coordinate descent with quantized updates", "author": ["Mostafa El Gamal", "Lifeng Lai"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Large-scale learning with less ram via randomization", "author": ["Daniel Golovin", "D. Sculley", "H. Brendan McMahan", "Michael Young"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Federated optimization: Distributed optimization beyond the datacenter", "author": ["Jakub Kone\u010dn\u00fd", "H. Brendan McMahan", "Daniel Ramage"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Federated optimization: Distributed machine learning for on-device intelligence", "author": ["Jakub Kone\u010dn\u00fd", "H. Brendan McMahan", "Daniel Ramage", "Peter Richt\u00e1rik"], "venue": "arXiv preprint arXiv:1610.02527,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": "Technical report,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Distributed optimization with arbitrary local solvers", "author": ["Chenxin Ma", "Jakub Kone\u010dn\u00fd", "Martin Jaggi", "Virginia Smith", "Michael I. Jordan", "Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Adding vs. averaging in distributed primal-dual optimization", "author": ["Chenxin Ma", "Virginia Smith", "Martin Jaggi", "Michael Jordan", "Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1973}, {"title": "Federated learning of deep networks using model averaging", "author": ["H. Brendan McMahan", "Eider Moore", "Daniel Ramage", "Blaise Aguera y Arcas"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Coordinate descent with arbitrary sampling I: algorithms and complexity", "author": ["Zheng Qu", "Peter Richt\u00e1rik"], "venue": "Optimization Methods and Software,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Quartz: Randomized dual coordinate ascent with arbitrary sampling", "author": ["Zheng Qu", "Peter Richt\u00e1rik", "Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Quantized incremental algorithms for distributed optimization", "author": ["M.G. Rabbat", "R.D. Nowak"], "venue": "IEEE Journal on Selected Areas in Communications,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "AIDE: Fast and communication efficient distributed optimization", "author": ["Sashank J Reddi", "Jakub Kone\u010dn\u00fd", "Peter Richt\u00e1rik", "Barnab\u00e1s P\u00f3cz\u00f3s", "Alex Smola"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Stochastic reformulation of linear systems and fast stochastic iterative methods", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "Technical report,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Communication-efficient distributed optimization using an approximate Newton-type method", "author": ["Ohad Shamir", "Nathan Srebro", "Tong Zhang"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["David P. Woodruff"], "venue": "Foundations and Trends R  \u00a9 in Theoretical Computer Science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Orthogonal random features", "author": ["Felix X Yu", "Ananda Theertha Suresh", "Krzysztof Choromanski", "Daniel Holtmann-Rice", "Sanjiv Kumar"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "DiSCO: Distributed optimization for self-concordant empirical loss", "author": ["Yuchen Zhang", "Xiao Lin"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Federated learning [10, 14, 9] proposes an alternative setting, where we train a shared global model under the coordination of a central server, from a federation of participating devices.", "startOffset": 19, "endOffset": 30}, {"referenceID": 11, "context": "Federated learning [10, 14, 9] proposes an alternative setting, where we train a shared global model under the coordination of a central server, from a federation of participating devices.", "startOffset": 19, "endOffset": 30}, {"referenceID": 6, "context": "Federated learning [10, 14, 9] proposes an alternative setting, where we train a shared global model under the coordination of a central server, from a federation of participating devices.", "startOffset": 19, "endOffset": 30}, {"referenceID": 15, "context": "The above framework differs from conventional distributed machine learning [18, 12, 20, 23, 5, 4] due to the the large number of clients, highly unbalanced and non-i.", "startOffset": 75, "endOffset": 97}, {"referenceID": 9, "context": "The above framework differs from conventional distributed machine learning [18, 12, 20, 23, 5, 4] due to the the large number of clients, highly unbalanced and non-i.", "startOffset": 75, "endOffset": 97}, {"referenceID": 17, "context": "The above framework differs from conventional distributed machine learning [18, 12, 20, 23, 5, 4] due to the the large number of clients, highly unbalanced and non-i.", "startOffset": 75, "endOffset": 97}, {"referenceID": 20, "context": "The above framework differs from conventional distributed machine learning [18, 12, 20, 23, 5, 4] due to the the large number of clients, highly unbalanced and non-i.", "startOffset": 75, "endOffset": 97}, {"referenceID": 2, "context": "The above framework differs from conventional distributed machine learning [18, 12, 20, 23, 5, 4] due to the the large number of clients, highly unbalanced and non-i.", "startOffset": 75, "endOffset": 97}, {"referenceID": 1, "context": "The above framework differs from conventional distributed machine learning [18, 12, 20, 23, 5, 4] due to the the large number of clients, highly unbalanced and non-i.", "startOffset": 75, "endOffset": 97}, {"referenceID": 11, "context": "For simplicity, we consider synchronized algorithms for federated learning [14, 3], where a typical round consists of the following steps: 1.", "startOffset": 75, "endOffset": 82}, {"referenceID": 0, "context": "For simplicity, we consider synchronized algorithms for federated learning [14, 3], where a typical round consists of the following steps: 1.", "startOffset": 75, "endOffset": 82}, {"referenceID": 10, "context": "Recent works show that a careful choice of the server-side learning rate can lead to faster convergence [13, 12, 10].", "startOffset": 104, "endOffset": 116}, {"referenceID": 9, "context": "Recent works show that a careful choice of the server-side learning rate can lead to faster convergence [13, 12, 10].", "startOffset": 104, "endOffset": 116}, {"referenceID": 7, "context": "Recent works show that a careful choice of the server-side learning rate can lead to faster convergence [13, 12, 10].", "startOffset": 104, "endOffset": 116}, {"referenceID": 3, "context": "Our approach seems to perform as well as the best techniques considered in [6], Random mask.", "startOffset": 75, "endOffset": 78}, {"referenceID": 13, "context": "This strategy can be seen as the combination of the master training method and a randomized block coordinate minimization approach [16, 15].", "startOffset": 131, "endOffset": 139}, {"referenceID": 12, "context": "This strategy can be seen as the combination of the master training method and a randomized block coordinate minimization approach [16, 15].", "startOffset": 131, "endOffset": 139}, {"referenceID": 18, "context": "Such sketching methods have application in many domains [21].", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": "It was recently shown that, in a certain setting, the expected iterates of SGD converge to the optimal point [19].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "Incremental, randomized and distributed optimization algorithms can be similarly analyzed in a quantized updates setting [17, 8, 7].", "startOffset": 121, "endOffset": 131}, {"referenceID": 5, "context": "Incremental, randomized and distributed optimization algorithms can be similarly analyzed in a quantized updates setting [17, 8, 7].", "startOffset": 121, "endOffset": 131}, {"referenceID": 4, "context": "Incremental, randomized and distributed optimization algorithms can be similarly analyzed in a quantized updates setting [17, 8, 7].", "startOffset": 121, "endOffset": 131}, {"referenceID": 19, "context": "In this work, we use a type of structured rotation matrix which is the product of a Walsh-Hadamard matrix and a binary diagonal matrix, motivated by the recent advance in this topic [22].", "startOffset": 182, "endOffset": 186}, {"referenceID": 8, "context": "We conducted the experiments using federated learning to train deep neural networks for the CIFAR10 image classification task [11].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "We employ the Federated Averaging algorithm [14], which significantly decreases the number of rounds of communication required to train a good model.", "startOffset": 44, "endOffset": 48}], "year": 2016, "abstractText": "Federated Learning is a machine learning setting where the goal is to train a highquality centralized model with training data distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of utmost importance. In this paper, we propose two ways to reduce the uplink communication costs. The proposed methods are evaluated on the application of training a deep neural network to perform image classification. Our best approach reduces the upload communication required to train a reasonable model by two orders of magnitude.", "creator": "LaTeX with hyperref package"}}}