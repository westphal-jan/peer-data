{"id": "1610.06484", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "An Evolving Cascade System Based on A Set Of Neo Fuzzy Nodes", "abstract": "net - semantic elements are used as nodes for an unconventional routing system. the proposed system can tune both response efficiency and architecture through an online mode. it components are realized alongside solving a fine range of data related tasks ( such neural series forecasting ). the evolving cascade system called neo - linguistic nodes collectively display rather large data sets with corresponding intelligence and effectiveness.", "histories": [["v1", "Thu, 20 Oct 2016 16:27:11 GMT  (288kb)", "http://arxiv.org/abs/1610.06484v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["zhengbing hu", "yevgeniy v bodyanskiy", "oleksii k tyshchenko", "olena o boiko"], "accepted": false, "id": "1610.06484"}, "pdf": {"name": "1610.06484.pdf", "metadata": {"source": "CRF", "title": "An Evolving Cascade System Based on A Set Of Neo-Fuzzy Nodes", "authors": ["Zhengbing Hu", "Yevgeniy V. Bodyanskiy", "Olena O. Boiko"], "emails": ["hzb@mail.ccnu.edu.cn", "yevgeniy.bodyanskiy@nure.ua", "lehatish@gmail.com,", "olena.boiko@ukr.net"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nThe task of time series forecasting (data sequences forecasting) is well studied nowadays. There are many mathematical methods of different complexity that can be used for solving this task: spectral analysis, exponential smoothing, regression, advanced intellectual systems, etc. In many real-world cases, analyzed time series are nonstationary, nonlinear, and usually contain unknown behavior trends, stochastic or chaotic components. This obstacle complicates time series forecasting and makes the above mentioned systems less effective.\nTo solve this problem, nonlinear models based on mathematical methods of Computational Intelligence [1-3] can be used. It should be especially mentioned that neuro-fuzzy systems [4-6] are widely used for this type of tasks due to their approximating and extrapolating properties, results\u2019 interpretability, and learning abilities. The most appropriate choice for non-stationary data processing is evolving connectionist systems [7-10]. These systems adjust not only their synaptic weights and parameters of membership functions, but also their architectures.\nThere are many evolving systems that are able to process data sets in an online mode. Most of them are based on multilayer neuro-fuzzy systems. The Takagi-Sugeno-Kang (TSK) fuzzy systems [11-12] and adaptive neuro-fuzzy inference systems (ANFIS) are the most popular and effective systems that are used to solve such tasks. But in some cases (e.g. when a size of a data set is not sufficient for training) they cannot rapidly tune their parameters, so their effectiveness can decrease.\nThe first solution for this problem is to decompose an initial task into a set of simpler tasks, so that the obtained system can solve a problem with a data set at hand regardless to its size. One of the most studied approaches based on this principle is the Group Method of Data Handling (GMDH) [13-14]. But in case of online data processing, the GMDH systems are not sufficiently effective. This problem can be solved by an evolving cascade model that tunes both its parameters as well as its architecture in an online mode.\nGenerally speaking, one can use different types of neurons or other more complicated systems as nodes in an evolving cascade system. For example, a compartmental R-neuron was introduced as a node of a cascade system [15, 16]. If a data set to be processed is large enough, it seems reasonable to use neo-fuzzy neurons [17-19]. The neo-fuzzy neuron is capable of finding a global minimum for a learning criterion in an online mode, it also has a high learning speed and good approximating properties. It is also appropriate from the viewpoint of computational simplicity.\nThe remainder of this paper is organized as follows: Section 2 describes an architecture of the evolving cascade system. Section 3 describes an architecture of the neo-fuzzy neuron as a node of the evolving cascade system. Section 4 presents several synthetic and real-world applications to be solved with the help of the proposed evolving cascade system. Conclusions and future work are given in the final section.\nII. AN EVOLVING CASCADE MODEL\nAn architecture of the evolving cascade model is shown in Fig. 1.\nA  1n -dimensional vector of input signals         T1 2, , ..., nx k x k x k x k is fed to the zero layer of the system (here 1, 2, ...k  is an index of the current discrete time). The first hidden layer contains 2nc nodes (each one has two inputs). The outputs of the first hidden layer form signals    1\u02c6sy k ,   21, 2, ..., 0,5 1 ns n n c   . Then these signals are fed to the selection block SB that sorts the first layer nodes by some criteria (e.g. by the mean squared error  [1] 2 sy k  ) so that      [1] [1] [1]1 2 2 2 2 \u02c6\u02c6 \u02c6 ** * ... sy ky k y k      .\nThe outputs    11\u02c6 *y k and    12\u02c6 *y k of the selection block are fed to a unique neuron of the second layer which forms its output signal  [2]y\u0302 k . This signal and the signal    13\u02c6 *y k (an output signal of the selection block SB ) are fed to a node of the next layer. A process of the cascades\u2019 increasing is continued until a required accuracy is obtained.\nIII. THE NEO-FUZZY SYSTEM AS A NODE OF THE EVOLVING CASCADE SYSTEM\nNeo-fuzzy neurons were proposed by T. Yamakawa and co-authors [17-19]. Advantages of this block are good approximating properties, computational simplicity, a high learning speed, and ability of finding a global minimum for a learning criterion in an online mode. An architecture of the neo-fuzzy neuron as a node of the evolving cascade system is shown in Fig.2.\nNonlinear synapses ANS and BNS which are structural elements of the neo-fuzzy neuron fulfill the Takagi\u2013\nSugeno fuzzy inference of the zero order. A two-dimensional vector of input signals       T,A Bx k x k x k is fed to the node\u2019s input. The first layer of each nonlinear synapse contains h membership functions. In [20], it was proposed to use the B-splines as membership functions for the neo-fuzzy neuron. B-splines provide higher approximation quality. A B-spline of the q -th order has the form\n  \n \n    \n    \n1,\n1\n1,\n, 1 1,\n, 1,\n1 if , for 1 0 otherwise\nfor 1\n1, ..., ,\niA A i A\nA iA q iA A\ni q A iAq iA A\ni q A A q i A A\ni q A i A\nc x k c q\nx k c x k\nc c x k\nc x k x k q\nc c\ni h q\n\n \n\n\n \n  \n \n        \n   \n          \n  \n \n    \n    \n1,\n1\n1,\n, 1 1,\n, 1,\n1 if , for 1 0 otherwise\nfor 1\n1, ..., .\niB B i B\nB iB q iB\ni q B iBq iB B\ni q B B q i B B\ni q B i B\nc x k c q\nx k c x k\nc c x k\nc x k x k q\nc c\ni h q\n\n\n\n\n\n \n  \n \n          \n  \n          \nwhere iAc , iBc are parameters that define centers of the membership functions. It should be noticed that when 2q  one can get traditional triangular membership functions, and when 4q  one can get cubic splines, etc. The B-splines meet the unity partitioning conditions\n  \n   1 1\n1,\n1\nh\npA A p\nh\npB B p\nx k\nx k\n\n\n\n         \nthat allows to simplify the node\u2019s architecture excluding a normalization layer. So, the elements of the first layer compute membership levels   pA Ax k ,   pB Bx k , 1, 2, ...,p h . The second layer contains synaptic weights iAw , iBw that are adjusted during a learning process. The third layer is formed by two summation units. It computes sums of the output signals of the second layer for\neach nonlinear synapse ANS and BNS . The outputs of the third layer are values\n          1\n1\n,\n.\nh\nA pA pA A p\nh\nB pB pB B p\nk w x k\nk w x k\n \n \n\n         \nAnother summation unit sums up these two signals in order to produce the output signal  y\u0302 k of the node\n     \n      1 1\n\u02c6 A B h h\npA pA A pB pB B p p\ny k k k\nw x k w x k\n \n   \n  \n   (1)\nThe expression (1) can be written in the form      Ty\u0302 k w k k\nwhere           T1 1, ..., , , ...,A hA B hBw k w k w k w k w k ,\n          1 1, ..., , , ...,A A hA A B Bk x k x k x k       T hB Bx k .\nTo learn the neo-fuzzy neuron, we can use the procedure [21, 22]                        1 T T 1 1 1 , 0 1 w k w k r k y k w k k k r k r k k k                   (2)\nwhich possesses both filtering and tracking properties. It can be noticed that when 1  the procedure (2) coincides with the Kaczmarz\u2013Widrow\u2013Hoff optimal gradient algorithm\n               T T 1 1 y k w k k w k w k k k k         \nthat can be used if a training data set is non-stationary [23].\nIV. EXPERIMENTS\nIn order to prove the effectiveness of the proposed system, several simulation tests were implemented. The system\u2019s effectiveness was analyzed by a value of the root mean square error (RMSE) and data processing time."}, {"heading": "A. A nonlinear system", "text": "A nonlinear system to be identified can be described by the equation\n  1 1 2 1 1\nm\nt i i\nt tm\nt i i\ny y u\ny\n \n\n \n  \n\n\nwhere sin(2 / 20)tu t , 0jy  , 1,...,j m , 10m  . The model is presented in the form\n 1 2 10 1\u02c6 , ,..., ,t t t t ty f y y y u    where \u02c6ty stands for a model\u2019s output. The aim was to predict the next output using previous inputs and outputs.\nThis data set contains 2500 points: 2000 points were selected for a training stage and 500 points were used for testing.\nTo compare results, we used a multilayer perceptron (MLP), a radial-basis function neural network (RBFN), an adaptive neuro-fuzzy inference system (ANFIS), and the proposed evolving cascade system with neo-fuzzy nodes.\nMLP was learnt during five epochs. A number of MLP\u2019s inputs was equal to 5 and a number of hidden nodes was equal to 10. A total number of parameters to be tuned was equal to 51.\nIn the RBFN\u2019s architecture, we used 3 inputs and 11 kernel functions, so a total number of parameters to be tuned in the RBFN\u2019s system was roughly equal to 56.\nOne of the best results was shown by ANFIS, but it was processing data during 5 epochs and a little longer than the proposed system, so it cannot process data in an online mode. ANFIS had 3 inputs and 34 hidden nodes. A total number of parameters to be adjusted was equal to 32.\nThe proposed evolving cascade system had 3 inputs and 4 membership functions in each node. A total number of parameters to be adjusted was 54. The proposed system demonstrated the best prediction results, and its data processing time was the best among the others.\nA comparison of the systems\u2019 results is shown in Table 1.\nANFIS 32 0.0070 0.0085 0.3625\nThe proposed system\n54 0.0413 0.0462 0.3281\nPrediction results for the proposed system are in Fig.3 (a blue line represents signal\u2019s values, a magenta line represents prediction values, and a grey line represents prediction errors).\nB. Internet traffic data (in bits) from an ISP This data set describes hourly traffic in the United Kingdom academic network backbone (taken from datamarket.com). It was collected between November 19th, 2004, at 09:30 and January 27th, 2005, at 11:11. This data set contains 1657 points: 1326 points were selected for a training stage and 331 points were used for testing.\nTo compare results, a similar to the previous experiment set of systems was used but with other characteristics. MLP was learnt during one epoch (this case is similar to learning in an online mode). A number of MLP\u2019s inputs\nwas equal to 3 and a number of hidden nodes was equal to 8. A total number of parameters to be tuned was 31.\nIn the RBFN\u2019s architecture, 7 kernel functions and 3 inputs were used, so a total number of parameters to be adjusted in the RBFN system was roughly equal to that of the proposed system.\nANFIS had 4 inputs and 55 hidden nodes. It was processing data during 5 epochs. A total number of parameters to be tuned was 80. This system showed the best prediction result, but it processed data longer than the proposed system.\nThe proposed evolving cascade system had 4 inputs and 4 membership functions in each node. A total number of parameters to be tuned was 36. The proposed system showed one of the best prediction results according to RMSE (the second result after ANFIS), and its data processing time was the best among the others.\nA comparison of the systems\u2019 results is shown in Table 2."}, {"heading": "C. Darwin Sea Level Pressures", "text": "This data set was taken from research.ics.aalto.fi. This data set describes monthly values of the Darwin sea level pressure. It was collected between 1882 and 1998. This data set contains 1300 points: 1040 points were selected for a training stage and 260 points were used for testing.\nMLP was learnt during five epochs. It had 3 inputs and 10 hidden nodes. A total number of parameters to be adjusted was equal to 41.\nRBFN had 7 kernel functions and 3 inputs, and a total number of parameters to be tuned in the RBFN system was equal to 36, i.e. it was very close to a number of parameters in the proposed system.\nA number of inputs for ANFIS was 4, a number of hidden nodes was equal to 55. ANFIS was processing data during one epoch. A total number of parameters to be tuned was equal to 80. This system showed the best result from the view point of accuracy, but it processed data longer than the proposed system.\nThe proposed evolving cascade system had 3 inputs and 4 membership functions in each node. A total number of parameters to be tuned was 40. The proposed system showed one of the best results, and its data processing time was the best.\nA comparison of the systems\u2019 results is shown in Table 3.\nV. CONCLUSION\nAn evolving cascade model based on the neo-fuzzy nodes is proposed. It can adjust both its architecture in an online mode. The proposed system has a rather simple computational implementation and can process data sets with a high speed. A number of experiments demonstrated that this evolving cascade system can forecast time series with high effectiveness. The results may be successfully used in a wide class of Data Stream Mining and Dynamic Data Mining tasks.\nACKNOWLEDGMENT\nThe authors would like to thank anonymous reviewers for their careful reading of this paper and for their helpful comments.\nThis scientific work was supported by RAMECS and CCNU16A02015.\nREFERENCES [1] Du K-L, Swamy M N S. Neural Networks and Statistical Learning. Springer-Verlag, London, 2014. [2] Kruse R, Borgelt C, Klawonn F, Moewes C, Steinbrecher M, Held P. Computational Intelligence. Springer, Berlin, 2013. [3] Rutkowski L. Computational Intelligence. Methods and Techniques. Springer-Verlag, Berlin-Heidelberg, 2008. [4] Jang J-S, Sun C-T, Mizutani E. Neuro-Fuzzy and Soft Computing: A Computational Approach to Learning and Machine\nIntelligence. Prentice Hall, Upper Saddle River, 1997. [5] Arora N, Saini J R. Estimation and Approximation Using Neuro-Fuzzy Systems. International Journal of Intelligent Systems\nand Applications(IJISA), 2016, 8(6): 9-18. [6] Karthika B S, Deka P C. Modeling of Air Temperature using ANFIS by Wavelet Refined Parameters. International Journal\nof Intelligent Systems and Applications(IJISA), 2016, 8(1): 25-34. [7] Lughofer E. Evolving Fuzzy Systems \u2013 Methodologies, Advanced Concepts and Applications. Springer, Berlin, 2011. [8] Kasabov N. Evolving Connectionist Systems. Springer-Verlag, London, 2003. [9] Kasabov N. Evolving fuzzy neural networks: theory and applications for on-line adaptive prediction, decision making and\ncontrol. Australian J. of Intelligent Information Processing Systems, 1998, 5(3):154-160.\n[10] Kasabov N. Evolving fuzzy neural networks \u2013 algorithms, applications and biological motivation. Proc. \u201cMethodologies for the Conception, Design and Application of Soft Computing\u201d, Singapore, 1998:271-274. [11] Sugeno M, Kang G T. Structure identification of fuzzy model. Fuzzy Sets and Systems, 1998, 28:15-33. [12] Takagi T, Sugeno M. Fuzzy identification of systems and its applications to modeling and control. IEEE Trans. on Systems,\nMan, and Cybernetics, 1985, 15:116-132. [13] Ivakhnenko A G. Heuristic self-organization in problems of engineering cybernetics. Automatica, 1970, 6(2): 207- 219. [14] Ivakhnenko A G. Polynomial theory of complex systems. IEEE Trans. on Systems, Man, and Cybernetics, 1971, 1(4): 364-\n378. [15] Bodyanskiy Ye, Grimm P, Teslenko N. Evolving cascade neural network based on multidimensional Epanechnikov\u2019s\nkernels and its learning algorithm. Int. J. Information Technologies and Knowledge, 2011, 5(1): 25-30. [16] Bodyanskiy Ye, Teslenko N, Grimm P. Hybrid evolving neural network using kernel activation functions. Proc. 17th Zittau\nEast-West Fuzzy Colloquium, Zittau/Goerlitz, Germany, 2010:39-46. [17] Miki T, Yamakawa T. Analog implementation of neo-fuzzy neuron and its on-board learning. In: Computational\nIntelligence and Applications, 1999:144-149. [18] Yamakawa T, Uchino E, Miki T, Kusanagi H. A neo fuzzy neuron and its applications to system identification and\nprediction of the system behavior. Proc. 2nd Int. Conf. on Fuzzy Logic and Neural Networks \u201cIIZUKA-92\u201d, Iizuka, Japan, 1992:477-483. [19] Uchino E, Yamakawa T. Soft computing based signal prediction, restoration, and filtering. In: Intelligent Hybrid Systems: Fuzzy Logic, Neural Networks, and Genetic Algorithms, 1997:331-349. [20] Kolodyazhniy V, Bodyanskiy Ye. Cascaded multiresolution spline-based fuzzy neural network. Proc. Int. Symp. on Evolving Intelligent Systems, Leicester, UK, 2010: 26-29. [21] Otto P, Bodyanskiy Ye, Kolodyazhniy V. A new learning algorithm for a forecasting neuro-fuzzy network. Integrated Computer-Aided Engineering, 2003, 10(4): 399- 409. [22] Bodyanskiy Ye, Kokshenev I, Kolodyazhniy V. An adaptive learning algorithm for a neo-fuzzy neuron. Proc. 3-rd Int. Conf. of European Union Soc. for Fuzzy Logic and Technology (EUSFLAT\u201903), Zittau, Germany, 2003: 375-379. [23] Bodyanskiy Ye, Viktorov Ye. The cascade neo-fuzzy architecture using cubic-spline activation functions. Int. J. Information Theories & Applications, 2009, 16(3): 245-259."}], "references": [{"title": "Neural Networks and Statistical Learning", "author": ["Du K-L", "Swamy M N S"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Neuro-Fuzzy and Soft Computing: A Computational Approach to Learning and Machine Intelligence", "author": ["Jang J-S", "Sun C-T", "Mizutani E"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Estimation and Approximation Using Neuro-Fuzzy Systems", "author": ["N Arora", "R. Saini J"], "venue": "International Journal of Intelligent Systems and Applications(IJISA),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Modeling of Air Temperature using ANFIS by Wavelet Refined Parameters", "author": ["S Karthika B", "C. Deka P"], "venue": "International Journal of Intelligent Systems and Applications(IJISA),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Evolving Fuzzy Systems \u2013 Methodologies, Advanced Concepts and Applications", "author": ["E. Lughofer"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Evolving Connectionist Systems", "author": ["N. Kasabov"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Evolving fuzzy neural networks: theory and applications for on-line adaptive prediction, decision making and control", "author": ["N. Kasabov"], "venue": "Australian J. of Intelligent Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Evolving fuzzy neural networks \u2013 algorithms, applications and biological motivation", "author": ["N. Kasabov"], "venue": "Proc. \u201cMethodologies for the Conception, Design and Application of Soft Computing\u201d,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Structure identification of fuzzy model", "author": ["M Sugeno", "T. Kang G"], "venue": "Fuzzy Sets and Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Fuzzy identification of systems and its applications to modeling and control", "author": ["T Takagi", "M. Sugeno"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1985}, {"title": "Heuristic self-organization in problems of engineering", "author": ["G. Ivakhnenko A"], "venue": "cybernetics. Automatica,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1970}, {"title": "Polynomial theory of complex systems", "author": ["G. Ivakhnenko A"], "venue": "IEEE Trans. on Systems, Man, and Cybernetics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1971}, {"title": "Evolving cascade neural network based on multidimensional Epanechnikov\u2019s kernels and its learning algorithm", "author": ["Ye Bodyanskiy", "P Grimm", "N. Teslenko"], "venue": "Int. J. Information Technologies and Knowledge,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Hybrid evolving neural network using kernel activation functions", "author": ["Ye Bodyanskiy", "N Teslenko", "P. Grimm"], "venue": "Proc. 17th Zittau East-West Fuzzy Colloquium, Zittau/Goerlitz,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Analog implementation of neo-fuzzy neuron and its on-board learning", "author": ["T Miki", "T. Yamakawa"], "venue": "Computational Intelligence and Applications,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "A neo fuzzy neuron and its applications to system identification and prediction of the system behavior", "author": ["T Yamakawa", "E Uchino", "T Miki", "H. Kusanagi"], "venue": "Proc. 2nd Int. Conf. on Fuzzy Logic and Neural Networks \u201cIIZUKA-92\u201d,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1992}, {"title": "Soft computing based signal prediction, restoration, and filtering. In: Intelligent Hybrid Systems: Fuzzy Logic, Neural Networks, and Genetic Algorithms, 1997:331-349", "author": ["E Uchino", "T. Yamakawa"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Cascaded multiresolution spline-based fuzzy neural network", "author": ["Kolodyazhniy V", "Bodyanskiy Ye"], "venue": "Proc. Int. Symp. on Evolving Intelligent Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "A new learning algorithm for a forecasting neuro-fuzzy network", "author": ["P Otto", "Ye Bodyanskiy", "V. Kolodyazhniy"], "venue": "Integrated Computer-Aided Engineering,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "An adaptive learning algorithm for a neo-fuzzy neuron", "author": ["Ye Bodyanskiy", "I Kokshenev", "V. Kolodyazhniy"], "venue": "Proc. 3-rd Int. Conf. of European Union Soc. for Fuzzy Logic and Technology (EUSFLAT\u201903), Zittau,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "The cascade neo-fuzzy architecture using cubic-spline activation functions", "author": ["Bodyanskiy Ye", "Viktorov Ye"], "venue": "Int. J. Information Theories & Applications,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "To solve this problem, nonlinear models based on mathematical methods of Computational Intelligence [1-3] can be used.", "startOffset": 100, "endOffset": 105}, {"referenceID": 1, "context": "It should be especially mentioned that neuro-fuzzy systems [4-6] are widely used for this type of tasks due to their approximating and extrapolating properties, results\u2019 interpretability, and learning abilities.", "startOffset": 59, "endOffset": 64}, {"referenceID": 2, "context": "It should be especially mentioned that neuro-fuzzy systems [4-6] are widely used for this type of tasks due to their approximating and extrapolating properties, results\u2019 interpretability, and learning abilities.", "startOffset": 59, "endOffset": 64}, {"referenceID": 3, "context": "It should be especially mentioned that neuro-fuzzy systems [4-6] are widely used for this type of tasks due to their approximating and extrapolating properties, results\u2019 interpretability, and learning abilities.", "startOffset": 59, "endOffset": 64}, {"referenceID": 4, "context": "The most appropriate choice for non-stationary data processing is evolving connectionist systems [7-10].", "startOffset": 97, "endOffset": 103}, {"referenceID": 5, "context": "The most appropriate choice for non-stationary data processing is evolving connectionist systems [7-10].", "startOffset": 97, "endOffset": 103}, {"referenceID": 6, "context": "The most appropriate choice for non-stationary data processing is evolving connectionist systems [7-10].", "startOffset": 97, "endOffset": 103}, {"referenceID": 7, "context": "The most appropriate choice for non-stationary data processing is evolving connectionist systems [7-10].", "startOffset": 97, "endOffset": 103}, {"referenceID": 8, "context": "The Takagi-Sugeno-Kang (TSK) fuzzy systems [11-12] and adaptive neuro-fuzzy inference systems (ANFIS) are the most popular and effective systems that are used to solve such tasks.", "startOffset": 43, "endOffset": 50}, {"referenceID": 9, "context": "The Takagi-Sugeno-Kang (TSK) fuzzy systems [11-12] and adaptive neuro-fuzzy inference systems (ANFIS) are the most popular and effective systems that are used to solve such tasks.", "startOffset": 43, "endOffset": 50}, {"referenceID": 10, "context": "One of the most studied approaches based on this principle is the Group Method of Data Handling (GMDH) [13-14].", "startOffset": 103, "endOffset": 110}, {"referenceID": 11, "context": "One of the most studied approaches based on this principle is the Group Method of Data Handling (GMDH) [13-14].", "startOffset": 103, "endOffset": 110}, {"referenceID": 12, "context": "For example, a compartmental R-neuron was introduced as a node of a cascade system [15, 16].", "startOffset": 83, "endOffset": 91}, {"referenceID": 13, "context": "For example, a compartmental R-neuron was introduced as a node of a cascade system [15, 16].", "startOffset": 83, "endOffset": 91}, {"referenceID": 14, "context": "If a data set to be processed is large enough, it seems reasonable to use neo-fuzzy neurons [17-19].", "startOffset": 92, "endOffset": 99}, {"referenceID": 15, "context": "If a data set to be processed is large enough, it seems reasonable to use neo-fuzzy neurons [17-19].", "startOffset": 92, "endOffset": 99}, {"referenceID": 16, "context": "If a data set to be processed is large enough, it seems reasonable to use neo-fuzzy neurons [17-19].", "startOffset": 92, "endOffset": 99}, {"referenceID": 0, "context": "by the mean squared error \uf028 \uf029 [1] 2", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "s y k \uf073 ) so that \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 [1] [1] [1] 1 2 2 2 2 \u02c6 \u02c6 \u02c6 * * * .", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "s y k \uf073 ) so that \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 [1] [1] [1] 1 2 2 2 2 \u02c6 \u02c6 \u02c6 * * * .", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "s y k \uf073 ) so that \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 [1] [1] [1] 1 2 2 2 2 \u02c6 \u02c6 \u02c6 * * * .", "startOffset": 38, "endOffset": 41}, {"referenceID": 14, "context": "Yamakawa and co-authors [17-19].", "startOffset": 24, "endOffset": 31}, {"referenceID": 15, "context": "Yamakawa and co-authors [17-19].", "startOffset": 24, "endOffset": 31}, {"referenceID": 16, "context": "Yamakawa and co-authors [17-19].", "startOffset": 24, "endOffset": 31}, {"referenceID": 17, "context": "In [20], it was proposed to use the B-splines as membership functions for the neo-fuzzy neuron.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "To learn the neo-fuzzy neuron, we can use the procedure [21, 22] \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1 T", "startOffset": 56, "endOffset": 64}, {"referenceID": 19, "context": "To learn the neo-fuzzy neuron, we can use the procedure [21, 22] \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 1 T", "startOffset": 56, "endOffset": 64}, {"referenceID": 20, "context": "that can be used if a training data set is non-stationary [23].", "startOffset": 58, "endOffset": 62}], "year": 2016, "abstractText": "Neo-fuzzy elements are used as nodes for an evolving cascade system. The proposed system can tune both its parameters and architecture in an online mode. It can be used for solving a wide range of Data Mining tasks (namely time series forecasting). The evolving cascade system with neo-fuzzy nodes can process rather large data sets with high speed and effectiveness.", "creator": "PScript5.dll Version 5.2.2"}}}