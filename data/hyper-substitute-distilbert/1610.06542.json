{"id": "1610.06542", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Lexicons and Minimum Risk Training for Neural Machine Translation: NAIST-CMU at WAT2016", "abstract": "this year, the first language translated science and press ( naist ) / carnegie mellon conference ( cmu ) presentation to the japanese - english translation experts of the 2016 workshop discussing asian translation adaptation mounted on attentional neural machine translator ( nmt ) simulations. in addition using the standard nmt translations, we make quick couple of improvements, most notably the use of procedural manual modes to administer english evaluation, and additionally encoding the minimum size categories to simulate the mt system default bleu score. \u2014 a result, korean program achieved generally highest translation proficiency scores playing the task.", "histories": [["v1", "Thu, 20 Oct 2016 19:10:09 GMT  (19kb)", "http://arxiv.org/abs/1610.06542v1", "To Appear in the Workshop on Asian Translation (WAT). arXiv admin note: text overlap witharXiv:1606.02006"]], "COMMENTS": "To Appear in the Workshop on Asian Translation (WAT). arXiv admin note: text overlap witharXiv:1606.02006", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["graham neubig"], "accepted": false, "id": "1610.06542"}, "pdf": {"name": "1610.06542.pdf", "metadata": {"source": "CRF", "title": "Lexicons and Minimum Risk Training for Neural Machine Translation: NAIST-CMU at WAT2016", "authors": ["Graham Neubig"], "emails": ["gneubig@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n06 54\n2v 1\n[ cs\n.C L\n] 2\n0 O\nct 2\n01 6"}, {"heading": "1 Introduction", "text": "Neural machine translation (NMT; (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014)), creation of translation models using neural networks, has quickly achieved state-of-the-art results on a number of translation tasks (Luong and Manning, 2015; Sennrich et al., 2016a). In this paper, we describe NMT systems for the Japanese-English scientific paper translation task of the Workshop on Asian Translation (WAT) 2016 (Nakazawa et al., 2016a).\nThe systems are built using attentional neural networks (Bahdanau et al., 2015; Luong et al., 2015), with a number of improvements (\u00a72). In particular we focus on two. First, we follow the recent work of Arthur et al. (2016) in incorporating discrete translation lexicons to improve the probability estimates of the neural translation model (\u00a73). Second, we incorporate minimum-risk training (Shen et al., 2016) to optimize the parameters of the model to improve translation accuracy (\u00a74).\nIn experiments (\u00a75), we examine the effect of each of these improvements, and find that they both contribute to overall translation accuracy, leading to state-of-the-art results on the Japanese-English translation task."}, {"heading": "2 Baseline Neural Machine Translation Model", "text": "Our baseline translation model is the attentional model implemented in the lamtram toolkit (Neubig, 2015), which is a combination of the models of Bahdanau et al. (2015) and Luong et al. (2015) that we found to be effective. We describe the model briefly here for completeness, and refer readers to the previous papers for a more complete description."}, {"heading": "2.1 Model Structure", "text": "Our model creates a model of target sentence E = e|E| 1 given source sentence F = f |F | 1\n. These words belong to the source vocabulary Vf , and the target vocabulary Ve respectively. NMT performs this translation by calculating the conditional probability pm(ei|F, e i\u22121 1\n) of the ith target word ei based on the source F and the preceding target words ei\u22121\n1 . This is done by encoding the context \u3008F, ei\u22121 1 \u3009 as a\nfixed-width vector \u03b7i, and calculating the probability as follows:\npm(ei|F, e i\u22121 1 ) = softmax(Ws\u03b7i + bs), (1)\nwhere Ws and bs are respectively weight matrix and bias vector parameters. The exact variety of the NMT model depends on how we calculate \u03b7i used as input, and as mentioned above, in this case we use an attentional model.\nFirst, an encoder converts the source sentence F into a matrix R where each column represents a single word in the input sentence as a continuous vector. This representation is generated using a bidirectional encoder\n\u2212\u2192r j = enc(embed(fj), \u2212\u2192r j\u22121) \u2190\u2212r j = enc(embed(fj), \u2190\u2212r j+1).\nHere the embed(\u00b7) function maps the words into a representation (Bengio et al., 2006), and enc(\u00b7) is long short term memory (LSTM) neural network (Hochreiter and Schmidhuber, 1997) with forget gates set to one minus the value of the input gate (Greff et al., 2015). For the final word in the sentence, we add a sentence-end symbol to the final state of both of these decoders\n\u2212\u2192r |F |+1 = enc(embed(\u3008s\u3009), \u2212\u2192r |F |)\n\u2190\u2212r |F |+1 = enc(embed(\u3008s\u3009), \u2190\u2212r 1).\nFinally we concatenate the two vectors \u2212\u2192r j and \u2190\u2212r j into a bidirectional representation rj\nrj = [ \u2190\u2212r j ; \u2212\u2192r j ].\nThese vectors are further concatenated into the matrix R where the jth column corresponds to rj . Next, we generate the output one word at a time while referencing this encoded input sentence and tracking progress with a decoder LSTM. The decoder\u2019s hidden state hi is a fixed-length continuous vector representing the previous target words ei\u22121\n1 , initialized as h0 = r|F |+1. This is used to calculate\na context vector ci that is used to summarize the source attentional context used in choosing target word ei, and initialized as c0 = 0.\nFirst, we update the hidden state to hi based on the word representation and context vectors from the previous target time step\nhi = enc([embed(ei\u22121); ci\u22121],hi\u22121). (2)\nBased on this hi, we calculate a similarity vector \u03b1i, with each element equal to\n\u03b1i,j = sim(hi, rj). (3)\nsim(\u00b7) can be an arbitrary similarity function. In our systems, we test two similarity functions, the dot product (Luong et al., 2015)\nsim(hi, rj) := h \u22ba i rj (4)\nand the multi-layered perceptron (Bahdanau et al., 2015)\nsim(hi, rj) := w \u22ba a2tanh(Wa1[hi; rj ]), (5)\nwhere Wa1 and wa2 are the weight matrix and vector of the first and second layers of the MLP respectively.\nWe then normalize this into an attention vector, which weights the amount of focus that we put on each word in the source sentence\nai = softmax(\u03b1i). (6)\nThis attention vector is then used to weight the encoded representation R to create a context vector ci for the current time step\nci = Rai.\nFinally, we create \u03b7i by concatenating the previous hidden state with the context vector, and performing an affine transform\n\u03b7i = W\u03b7[hi; ci] + b\u03b7,\nOnce we have this representation of the current state, we can calculate pm(ei|F, e i\u22121 1\n) according to Equation (1). The next word ei is chosen according to this probability."}, {"heading": "2.2 Parameter Optimization", "text": "If we define all the parameters in this model as \u03b8, we can then train the model by minimizing the negative log-likelihood of the training data\n\u03b8\u0302 = argmin \u03b8\n\u2211\n\u3008F, E\u3009\n\u2211\ni\n\u2212 log(pm(ei|F, e i\u22121 1 ; \u03b8)).\nSpecifically, we use the ADAM optimizer (Kingma and Ba, 2014), with an initial learning rate of 0.001. Minibatches of 2048 words are created by sorting sentences in descending order of length and grouping sentences sequentially, adding sentences to the minibatch until the next sentence would cause the minibatch size to exceed 2048 words.1 Gradients are clipped so their norm does not exceed 5.\nTraining is allowed to run, checking the likelihood of the development set periodically (every 250k sentences processed), and the model that achieves the best likelihood on the development set is saved. Once no improvements on the development set have been observed for 2M training sentences, training is stopped, and re-started using the previously saved model with a halved learning rate 0.0005. Once training converges for a learning rate of 0.0005, the same procedure is performed with a learning rate of 0.00025, resulting in the final model."}, {"heading": "2.3 Search", "text": "At test time, to find the best-scoring translation, we perform beam search with a beam size of 5. At each step of beam search, the best-scoring hypothesis remaining in the beam that ends with the sentence-end symbol is saved. At the point where the highest-scoring hypothesis in the beam has a probability less than or equal to the best sentence-ending hypothesis, search is terminated and the best sentence-ending hypothesis is output as the translation.\nIn addition, because NMT models often tend to be biased towards shorter sentences, we add an optional \u201cword penalty\u201d \u03bb, where each hypothesis\u2019s probability is multiplied by e\u03bb|E\n\u2032| for comparison with other hypotheses of different lengths. This is equivalent to adding an exponential prior probability on the length of output sentences, and if \u03bb > 0, then this will encourage the decoder to find longer hypotheses."}, {"heading": "3 Incorporating Discrete Lexicons", "text": "The first modification that we make to the base model is incorporating discrete lexicons to improve translation probabilities, according to the method of Arthur et al. (2016). The motivation behind this method is twofold:\nHandling low-frequency words: Neural machine translation systems tend to have trouble translating low-frequency words (Sutskever et al., 2014), so incorporating translation lexicons with good coverage of content words could improve translation accuracy of these words.\nTraining speed: Training the alignments needed for discrete lexicons can be done efficiently (Dyer et al., 2013), and by seeding the neural MT system with these efficiently trained alignments it is easier to learn models that achieve good results more quickly.\nThe model starts with lexical translation probabilities pl(e|f) for individual words, which have been obtained through traditional word alignment methods. These probabilities must first be converted to a form that can be used together with pm(ei|e i\u22121 1\n, F ). Given input sentence F , we can construct a matrix in which each column corresponds to a word in the input sentence, each row corresponds to a word in the VE , and the entry corresponds to the appropriate lexical probability:\nLF =\n\n  pl(e = 1|f1) \u00b7 \u00b7 \u00b7 pl(e = 1|f|F |) ... . . . ...\npl(e = |Ve||f1) \u00b7 \u00b7 \u00b7 pl(e = |Ve||f|F |)\n\n  .\n1It should be noted that it is more common to create minibatches with a fixed number of sentences. We use words here because the amount of memory used in processing a minibatch is more closely related to the number of words in the minibatch than the number of sentences, and thus fixing the size of the minibatch based on the number of words leads to more stable memory usage between minibatches.\nThis matrix can be precomputed during the encoding stage because it only requires information about the source sentence F .\nNext we convert this matrix into a predictive probability over the next word: pl(ei|F, e i\u22121 1\n). To do so we use the alignment probability a from Equation (6) to weight each column of the LF matrix:\npl(ei|F, e i\u22121 1 ) = LFai =\n\n  pl(e = 1|f1) \u00b7 \u00b7 \u00b7 plex(e = 1|f|F |) ... . . . ...\npl(e = Ve|f1) \u00b7 \u00b7 \u00b7 plex(e = Ve|f|F |)\n\n \n\n  ai,1 ...\nai,|F |\n\n  .\nThis calculation is similar to the way how attentional models calculate the context vector ci, but over a vector representing the probabilities of the target vocabulary, instead of the distributed representations of the source words.\nAfter calculating the lexicon predictive probability pl(ei|e i\u22121 1\n, F ), next we need to integrate this probability with the NMT model probability pm(ei|e i\u22121 1\n, F ). Specifically, we use pl(\u00b7) to bias the probability distribution calculated by the vanilla NMT model by adding a small constant \u01eb to pl(\u00b7), taking the logarithm, and adding this adjusted log probability to the input of the softmax as follows:\npb(ei|F, e i\u22121 1 ) = softmax(Ws\u03b7i + bs + log(pl(ei|F, e i\u22121 1 ) + \u01eb)).\nWe take the logarithm of pl(\u00b7) so that the values will still be in the probability domain after the softmax is calculated, and add the hyper-parameter \u01eb to prevent zero probabilities from becoming \u2212\u221e after taking the log. We test various values including \u01eb = {10\u22124, 10\u22125, 10\u22126} in experiments."}, {"heading": "4 Minimum Risk Training", "text": "The second improvement that we make to our model is the use of minimum risk training. As mentioned in Section 2.2 our baseline model optimizes the model parameters according to maximize the likelihood of the training data. However, there is a disconnect between the evaluation of our systems using translation accuracy (such as BLEU (Papineni et al., 2002)) and this maximum likelihood objective.\nTo remove this disconnect, we use the method of Shen et al. (2016) to optimize our systems directly using BLEU score. Specifically, we define the following loss function over the model parameters \u03b8 for a single training sentence pair \u3008F,E\u3009\nLF,E(\u03b8) = \u2211\nE\u2032\nerr(E,E\u2032)P (E\u2032|F ; \u03b8),\nwhich is summed over all potential translations E\u2032 in the target language. Here err(\u00b7) can be an arbitrary error function, which we define as 1 \u2212 SBLEU(E,E\u2032), where SBLEU(\u00b7) is the smoothed BLEU score (BLEU+1) proposed by Lin and Och (2004). As the number of target-language translations E\u2032 is infinite, the sum above is intractable, so we approximate the sum by randomly sampling a subset of translations S according to P (E|F ; \u03b8), then enumerating over this sample:2\nLF,E(\u03b8) = \u2211\nE\u2032\u2208S\nerr(E,E\u2032) P (E\u2032|F ; \u03b8)\u2211\nE\u2032\u2032\u2208S P (E \u2032\u2032|F ; \u03b8)\n.\nThis objective function is then modified by introducing a scaling factor \u03b1, which makes it possible to adjust the smoothness of the distribution being optimized, which in turn results in adjusting the strength with which the model will try to push good translations to have high probabilities.\nLF,E(\u03b8) = \u2211\nE\u2032\u2208S\nerr(E,E\u2032) P (E\u2032|F ; \u03b8)\u03b1\u2211\nE\u2032\u2032\u2208S P (E \u2032\u2032|F ; \u03b8)\u03b1\n.\nIn this work, we set \u03b1 = 0.005 following the original paper, and set the number of samples to be 20.\n2The actual procedure for obtaining a sample consists of calculating the probability of the first word P (e1|F ), sampling the first word from this multinomial, and then repeating for each following word until the end of sentence symbol is sampled."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experimental Setup", "text": "To create data to train the model, we use the top 2M sentences of the ASPEC Japanese-English training corpus (Nakazawa et al., 2016b) provided by the task. The Japanese size of the corpus is tokenized using KyTea (Neubig et al., 2011), and the English side is tokenized with the tokenizer provided with the Travatar toolkit (Neubig, 2013). Japanese is further normalized so all full-width roman characters and digits are normalized to half-width. The words are further broken into subword units using joint byte pair encoding (Sennrich et al., 2016b) with 100,000 merge operations."}, {"heading": "5.2 Experimental Results", "text": "In Figure 1 we show results for various settings regarding attention, the use of lexicons, training criterion, and word penalty. In addition, we calculate the ensemble of 6 models, where the average probability assigned by each of the models is used to determine the probability of the next word at test time.\nFrom the results in the table, we can glean a number of observations.\nUse of Lexicons: Comparing (1) with (2-4), we can see that in general, using lexicons tends to provide a benefit, particularly when the \u01eb parameter is set to a small value.\nType of Attention: Comparing (2-4) with (5-7) we can see that on average, multi-layer perceptron attention was more effective than using the dot product.\nUse of Word Penalties: Comparing the first and second columns of results, there is a large increase in accuracy across the board when using a word penalty, demonstrating that this is an easy way to remedy the length of NMT results.\nMinimum Risk Training: Looking at the third column, we can see that there is an additional increase in accuracy from minimum risk training. In addition, we can see that after minimum risk, the model produces hypotheses that are more-or-less appropriate length without using a word penalty, an additional benefit.\nEnsemble: As widely reported in previous work, ensembling together multiple models greatly improved performance."}, {"heading": "5.3 Manual Evaluation Results", "text": "The maximum-likelihood trained ensemble system with a word penalty of 0.8 (the bottom middle system in Table 1) was submitted for manual evaluation. The system was evaluated according to the official\nWAT \u201cHUMAN\u201d metric (Nakazawa et al., 2016a), which consists of pairwise comparisons with a baseline phrase-based system, where the evaluated system receives +1 for every win, -1 for every tie, 0 for every loss, these values are averaged over all evaluated sentences, then the value is multiplied by 100. This system achieved a manual evaluation score of 47.50, which was slightly higher than other systems participating in the task. In addition, while the full results of the minimum-risk-based ensemble were not ready in time for the manual evaluation stage, a preliminary system ensembling the minimum-risktrained versions of the first four systems (1)-(4) in Table 1 was also evaluated (its BLEU/RIBES scores were comparable to the fully ensembled ML-trained system), and received a score of 48.25, the best in the task, albeit by a small margin."}, {"heading": "6 Conclusion", "text": "In this paper, we described the NAIST-CMU system for the Japanese-English task at WAT, which achieved the most accurate results on this language pair. In particular, incorporating discrete translation lexicons and minimum risk training were found to be useful in achieving these results.\nAcknowledgments:\nThis work was supported by JSPS KAKENHI Grant Number 25730136."}], "references": [{"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Arthur et al.2016] Philip Arthur", "Graham Neubig", "Satoshi Nakamura"], "venue": "In Proc. EMNLP", "citeRegEx": "Arthur et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proc. ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Neural probabilistic language models", "author": ["Bengio et al.2006] Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": "In Proc. NAACL,", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "LSTM: A search space odyssey", "author": ["Greff et al.2015] Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R. Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proc. EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Orange: a method for evaluating automatic evaluation metrics for machine translation", "author": ["Lin", "Och2004] Chin-Yew Lin", "Franz Josef Och"], "venue": "In Proc. COLING,", "citeRegEx": "Lin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2004}, {"title": "Stanford neural machine translation systems for spoken language domains", "author": ["Luong", "Manning2015] Minh-Thang Luong", "Christopher D Manning"], "venue": "In Proc. IWSLT", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proc. EMNLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Pointwise prediction for robust, adaptable Japanese morphological analysis", "author": ["Neubig et al.2011] Graham Neubig", "Yosuke Nakata", "Shinsuke Mori"], "venue": "In Proc. ACL,", "citeRegEx": "Neubig et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2011}, {"title": "Travatar: A forest-to-string machine translation engine based on tree transducers", "author": ["Graham Neubig"], "venue": "In Proc. ACL Demo Track,", "citeRegEx": "Neubig.,? \\Q2013\\E", "shortCiteRegEx": "Neubig.", "year": 2013}, {"title": "lamtram: A toolkit for language and translation modeling using neural networks. http://www.github.com/neubig/lamtram", "author": ["Graham Neubig"], "venue": null, "citeRegEx": "Neubig.,? \\Q2015\\E", "shortCiteRegEx": "Neubig.", "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proc. ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "2016a. Edinburgh neural machine translation systems for WMT16", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proc. WMT,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proc. ACL,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["Shen et al.2016] Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proc. ACL,", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Proc. NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "1 Introduction Neural machine translation (NMT; (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014)), creation of translation models using neural networks, has quickly achieved state-of-the-art results on a number of translation tasks (Luong and Manning, 2015; Sennrich et al.", "startOffset": 48, "endOffset": 104}, {"referenceID": 1, "context": "The systems are built using attentional neural networks (Bahdanau et al., 2015; Luong et al., 2015), with a number of improvements (\u00a72).", "startOffset": 56, "endOffset": 99}, {"referenceID": 9, "context": "The systems are built using attentional neural networks (Bahdanau et al., 2015; Luong et al., 2015), with a number of improvements (\u00a72).", "startOffset": 56, "endOffset": 99}, {"referenceID": 17, "context": "Second, we incorporate minimum-risk training (Shen et al., 2016) to optimize the parameters of the model to improve translation accuracy (\u00a74).", "startOffset": 45, "endOffset": 64}, {"referenceID": 0, "context": "First, we follow the recent work of Arthur et al. (2016) in incorporating discrete translation lexicons to improve the probability estimates of the neural translation model (\u00a73).", "startOffset": 36, "endOffset": 57}, {"referenceID": 13, "context": "2 Baseline Neural Machine Translation Model Our baseline translation model is the attentional model implemented in the lamtram toolkit (Neubig, 2015), which is a combination of the models of Bahdanau et al.", "startOffset": 135, "endOffset": 149}, {"referenceID": 1, "context": "2 Baseline Neural Machine Translation Model Our baseline translation model is the attentional model implemented in the lamtram toolkit (Neubig, 2015), which is a combination of the models of Bahdanau et al. (2015) and Luong et al.", "startOffset": 191, "endOffset": 214}, {"referenceID": 1, "context": "2 Baseline Neural Machine Translation Model Our baseline translation model is the attentional model implemented in the lamtram toolkit (Neubig, 2015), which is a combination of the models of Bahdanau et al. (2015) and Luong et al. (2015) that we found to be effective.", "startOffset": 191, "endOffset": 238}, {"referenceID": 2, "context": "Here the embed(\u00b7) function maps the words into a representation (Bengio et al., 2006), and enc(\u00b7) is long short term memory (LSTM) neural network (Hochreiter and Schmidhuber, 1997) with forget gates set to one minus the value of the input gate (Greff et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 4, "context": ", 2006), and enc(\u00b7) is long short term memory (LSTM) neural network (Hochreiter and Schmidhuber, 1997) with forget gates set to one minus the value of the input gate (Greff et al., 2015).", "startOffset": 166, "endOffset": 186}, {"referenceID": 9, "context": "In our systems, we test two similarity functions, the dot product (Luong et al., 2015) sim(hi, rj) := h \u22ba i rj (4) and the multi-layered perceptron (Bahdanau et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 1, "context": ", 2015) sim(hi, rj) := h \u22ba i rj (4) and the multi-layered perceptron (Bahdanau et al., 2015)", "startOffset": 69, "endOffset": 92}, {"referenceID": 18, "context": "The motivation behind this method is twofold: Handling low-frequency words: Neural machine translation systems tend to have trouble translating low-frequency words (Sutskever et al., 2014), so incorporating translation lexicons with good coverage of content words could improve translation accuracy of these words.", "startOffset": 164, "endOffset": 188}, {"referenceID": 3, "context": "Training speed: Training the alignments needed for discrete lexicons can be done efficiently (Dyer et al., 2013), and by seeding the neural MT system with these efficiently trained alignments it is easier to learn models that achieve good results more quickly.", "startOffset": 93, "endOffset": 112}, {"referenceID": 0, "context": "3 Incorporating Discrete Lexicons The first modification that we make to the base model is incorporating discrete lexicons to improve translation probabilities, according to the method of Arthur et al. (2016). The motivation behind this method is twofold: Handling low-frequency words: Neural machine translation systems tend to have trouble translating low-frequency words (Sutskever et al.", "startOffset": 188, "endOffset": 209}, {"referenceID": 14, "context": "However, there is a disconnect between the evaluation of our systems using translation accuracy (such as BLEU (Papineni et al., 2002)) and this maximum likelihood objective.", "startOffset": 110, "endOffset": 133}, {"referenceID": 14, "context": "However, there is a disconnect between the evaluation of our systems using translation accuracy (such as BLEU (Papineni et al., 2002)) and this maximum likelihood objective. To remove this disconnect, we use the method of Shen et al. (2016) to optimize our systems directly using BLEU score.", "startOffset": 111, "endOffset": 241}, {"referenceID": 11, "context": "The Japanese size of the corpus is tokenized using KyTea (Neubig et al., 2011), and the English side is tokenized with the tokenizer provided with the Travatar toolkit (Neubig, 2013).", "startOffset": 57, "endOffset": 78}, {"referenceID": 12, "context": ", 2011), and the English side is tokenized with the tokenizer provided with the Travatar toolkit (Neubig, 2013).", "startOffset": 97, "endOffset": 111}], "year": 2016, "abstractText": "This year, the Nara Institute of Science and Technology (NAIST)/Carnegie Mellon University (CMU) submission to the Japanese-English translation track of the 2016 Workshop on Asian Translation was based on attentional neural machine translation (NMT) models. In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score. As a result, our system achieved the highest translation evaluation scores for the task.", "creator": "LaTeX with hyperref package"}}}