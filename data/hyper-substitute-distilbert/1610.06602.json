{"id": "1610.06602", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Iterative Refinement for Machine Translation", "abstract": "existing systematic algebra analyses that generate translations in a strictly monotonic fashion can never revisit previous decisions. as to result, earlier mistakes best be corrected toward large numerical stage. in this environment, mice produce a translation structure that starts translating an initial language algorithm then makes iterative transformation that substantially revisit previous decisions. we assume programming model as a frequency variation pattern if predicts discrete substitutions to an existing translation based on differing attention mechanism underlying sampling the source sentence as simply as managing outgoing translation output. by making better than seven modification per rotation, managers control the output of a phrase - based translations analysis resulting up to 0. 62 different than obtaining german - english translation.", "histories": [["v1", "Thu, 20 Oct 2016 20:54:07 GMT  (398kb,D)", "https://arxiv.org/abs/1610.06602v1", null], ["v2", "Wed, 26 Oct 2016 16:23:30 GMT  (398kb,D)", "http://arxiv.org/abs/1610.06602v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["roman novak", "michael auli", "david grangier"], "accepted": false, "id": "1610.06602"}, "pdf": {"name": "1610.06602.pdf", "metadata": {"source": "CRF", "title": "Iterative Refinement for Machine Translation", "authors": ["Roman Novak", "Michael Auli", "David Grangier"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Existing decoding schemes for translation generate outputs either left-to-right, such as for phrasebased or neural translation models, or bottom-up as in syntactic models (Koehn et al., 2003; Galley et al., 2004; Bahdanau et al., 2015). All decoding algorithms for those models make decisions which cannot be revisited at a later stage, such as when the model discovers that it made an error earlier on.\nOn the other hand, humans generate all but the simplest translations by conceiving a rough draft of the solution and then iteratively improving it until it is deemed complete. The translator may modify a clause she tackled earlier at any point and make arbitrary modifications to improve the translation.\n\u2217Roman was interning at Facebook for this work.\nIt can be argued that beam search allows to recover from mistakes, simply by providing alternative translations. However, reasonable beam sizes encode only a small number of binary decisions. A beam of size 50 contains fewer than six binary decisions, all of which frequently share the same prefix (Huang, 2008).1\nIn this paper, we present models that tackle translation similar to humans. The model iteratively edits the target sentence until it cannot improve it further. As a preliminary study, we address the problem of finding mistakes in an existing translation via a simple classifier that predicts if a word in a translation is correct (\u00a72). Next, we model word substitutions for an existing translation via a convolutional neural network that attends to the source when suggesting substitutions (\u00a73). Finally, we devise a model that attends both to the source as well as to the existing translation (\u00a74). We repeatedly apply the models to their own output by determining the best substitution for each word in the previous translation and then choosing either one or zero substitutions for each sentence. For the latter we consider various heuristics as well as a classifier-based selection method (\u00a75).\nOur results demonstrate that we can improve the output of a phrase-based translation system on WMT15 German-English data by up to 0.4 BLEU (Papineni et al., 2002) by making on average only 0.6 substitutions per sentence (\u00a76).\nOur approach differs from automatic postediting since it does not require post-edited text which is a scarce resource (Simard et al., 2007; Bojar et al., 2016). For our first model (\u00a73) we merely require parallel text and for our second model (\u00a74) the output of a baseline translation system.\n125 = 32 < 50 < 26 = 64\nar X\niv :1\n61 0.\n06 60\n2v 2\n[ cs\n.C L\n] 2\n6 O\nct 2\n01 6"}, {"heading": "2 Detecting Errors", "text": "Before correcting errors we consider the task of detecting mistakes in the output of an existing translation system.\nIn the following, we use lowercase boldface for vectors (e.g. x), uppercase boldface for matrices (e.g. F) and calligraphy for sets (e.g. X ). We use superscripts for indexing or slicing, e.g., xi, Fi,j , Fi = (Fi,1, . . . ,Fi,|F\ni|). We further denote x as the source sentence, yg as the guess translation from which we start and which was produced by a phrase-based translation system (\u00a76.1), and yref as the reference translation. Sentences are vectors of indices indicating entries in a source vocabulary X or a target vocabulary Y . For example, x = (x1, . . . ,x|x|) \u2208 X |x| with X = {1, . . . , |X |}. We omit biases of linear layers to simplify the notation.\nError detection focuses on word-level accuracy, i.e., we predict for each token in a given translation if it is present in the reference or not. This metric ignores word order, however, we hope that performance on this simple task provides us with a sense of how difficult it will be to modify translations to a positive effect. A token yig in the candidate translation yg is deemed correct iff it is present in the reference translation: yig \u2208 yref. We build a neural network f to predict correctness of each token in yg given the source sentence x:\nf(x,yg) \u2208 [0; 1]|yg| ,\nwhere f(x,yg)i estimates P ( yig \u2208 yref ) .\nArchitecture. We use an architecture similar to the word alignment model of Legrand et al. (2016). The source and the target sequences are embedded via a lookup table that replace each word type with a learned vector. The resulting vector sequences are then processed by alternating convolutions and non-linearities. This results in a vector S (x)i representing each position i in the source x and a vector T ( yg )j representing each position j in the target yg. These vectors are then compared via a dot product. Our prediction estimates the probability of a target word being correct as the largest dot product between any source word and the guess word. We apply the logistic function \u03c3 to this score,\nf(x,yg) i = \u03c3 ( max\n16j6|x|\n[ S(x)T(yg) T ]j,i) .\nTraining. At training time we minimize the cross-entropy loss, with the binary supervision 1 for yig \u2208 yref, 0 otherwise.\nTesting. At test time we threshold the model prediction f(x,yg)i to detect mistakes. We compare the performance of our network to the following baselines:\n1. Predicting that all candidate words are always correct fcor \u2261 1, or always incorrect fwrong \u2261 0.\n2. The prior probability of a word being correct based on the training data fstat(y) =( P [ y \u2208 yref | y \u2208 yg ] > 0.5 ) .\nWe report word-level accuracy metrics in Table 1. While the model significantly improves over the baselines, the probability of correctly labeling a word as a mistake remains low (62.71%). The task of predicting mistakes is not easy as previously shown in confidence estimation (Blatz et al., 2004; Ueffing and Ney, 2007). Also, one should bear in mind that this task cannot be solved with 100% accuracy since a sentence can be correctly in multiple different ways and we only have a single reference translation. In our case, our final refinement objective might be easier than error detection as we do not need to detect all errors. We need to identify some of the locations where a substitution could improve BLEU. At the same time, our strategy should also suggest these substitutions. This is the objective of the model introduced in the next section."}, {"heading": "3 Attention-based Model", "text": "We introduce a model to predict modifications to a translation which can be trained on bilingual text.\nIn \u00a75 we discuss strategies to iteratively apply this model to its own output in order to improve a translation.\nOur model F takes as input a source sentence x and a target sentence y, and outputs a distribution over the vocabulary for each target position,\nF(x,y) \u2208 [0, 1]|y|\u00d7|Y| .\nFor each position i and any word j \u2208 Y , F(x,y)i,j estimates P(yi = j |x,y\u2212i), the probability of word j being at position i given the source and the target context y\u2212i =( y1, . . . ,yi\u22121,yi+1, . . . ,y|y| ) surrounding i. In other words, we learn a non-causal language model (Bengio et al., 2003) which is also conditioned on the source x.\nArchitecture. We rely on a convolutional model with attention. The source sentence is embedded into distributional space via a lookup table, followed by convolutions and non-linearities. The target sentence is also embedded in distributional space via a lookup table, followed by a single convolution and a succession of linear layers and non-linearities. The target convolution weights are zeroed at the center so that the model does not have access to the center word. This means that the model observes a fixed size context of length 2k for any target position i, y\u2212i|k =( yi\u2212k, . . . ,yi\u22121,yi+1, . . . ,yi+k ) where 2k + 1 refers to the convolution kernel width. These operations result in a vector Sj representing each position j in the source sentence x and a vector Ti representing each target context y\u2212i|k. Given a target position i, an attention module then takes as input these representation and outputs a weight for each target position\n\u03b1(i, j) = exp\n( Sj \u00b7Ti )\u2211|x| j\u2032=1 exp (S j\u2032 \u00b7Ti) .\nThese weights correspond to dot-product attention scores (Luong et al., 2015; Rush et al., 2015). The attention weights allow to compute a source summary specific to each target context through a weighted sum,\na ( y\u2212i|k,x ) = |x|\u2211 j=1 \u03b1(i, j) Sj\nFinally, this summary a(y\u2212i|k,x) is concatenated with the embedding of the target context y\u2212i|k ob-\ntained from the target lookup table, L ( y\u2212i|k ) = { Lj , j \u2208 y\u2212i|k } and a multilayer perceptron followed by a softmax computes F(x,y)i from a(y\u2212i|k,x), L(y\u2212i|k). Note that we could alternatively use Ti instead of L(y\u2212i|k) but our preliminary validation experiments showed better result with the lookup table output.\nTraining. The model is trained to maximize the (log) likelihood of the pairs (x,yref) from the training set.\nTesting. At test time the model is given (x,yg), i.e., the source and the guess sentence. Similar to maximum likelihood training for left-to-right translation systems (Bahdanau et al., 2015), the model is therefore not exposed to the same type of context in training (reference contexts from yref) and testing (guess contexts from yg).\nDiscussion. Our model is similar to the attention-based translation approach of Bahdanau et al. (2015). In addition to using convolutions, the main difference is that we have access to both left and right target context y\u2212i|k since we start from an initial guess translation. Right target words are of course good predictors of the previous word. For instance, an early validation experiment with the setup from \u00a76.1 showed a perplexity of 5.4 for this model which compares to 13.9 with the same model trained with the left context only."}, {"heading": "4 Dual Attention Model", "text": "We introduce a dual attention architecture to also make use of the guess at training time. This contrasts with the model introduced in the previous section where the guess is not used during training. Also, we are free to use the entire guess, including the center word, compared to the reference where we have to remove the center word.\nAt training time, the dual attention model takes 3 inputs, that is, the source, the guess and the reference. At test time, the reference input is replaced by the guess. Specifically, the model\nFdual(x,yg,yref) \u2208 [0; 1]|yref|\u00d7|Y|\nestimates P ( yiref |x,yg,y \u2212i ref ) for each position i in the reference sentence. Architecture. The model builds upon the single attention model from the previous section by having two attention functions a with distinct parameters. The first function asource takes\nthe source sentence x and the reference context y\u2212iref to produce the source summary for this context asource ( y\u2212i|k,x ) as in the single attention model. The second function aguess takes the guess sentence yg and the reference context y\u2212iref and produces a guess summary for this context aguess ( y\u2212i|k,yg ) . These two summaries are then concatenated with the lookup representation of the reference context L ( yref \u2212i|k) and input to a final multilayer perceptron followed by a softmax. The reference lookup table contains the only parameters shared by the two attention functions.\nTraining. This model is trained similarly to the single attention model, the only difference being the conditioning on the guess yg.\nTesting. At test time, the reference is unavailable and we replace yref with yg, i.e., the model is given (x,yg,y \u2212i|k g ) to make a prediction at position i. In this case, the distribution shift when going from training to testing is less drastic than in \u00a73 and the model retains access to the whole yg via attention.\nDiscussion. Compared to the single attention model (\u00a73), this model reduces perplexity from 5.4 to 4.1 on our validation set. Since the dual attention model can attend to all guess words, it can copy any guess word if necessary. In our dataset, 68% of guess words are in the reference and can therefore be copied. This also means that for the remaining 32% of reference tokens the model should not copy. Instead, the model should propose a substitution by itself (\u00a76.1). During testing, the fact that the guess is input twice (x,yg,y \u2212i|k g ) means that the guess and the prediction context always match. This makes the model more conservative in its predictions, suggesting tokens from yg more often than the single attention model. However, as we show in \u00a76, this turns out beneficial in our setting."}, {"heading": "5 Iterative Refinement", "text": "The models in \u00a73 and \u00a74 suggest word substitutions for each position in the candidate translation yg given the current surrounding context.\nApplying a single substitution changes the context of the surrounding words and requires updating the model predictions. We therefore perform multiple rounds of substitution. At each round, the model computes its predictions, then our refinement strategy selects a substitution and performs it unless the strategy decides that it can no longer\nimprove the target sentence. This means that the refinement procedure should be able to (i) prioritize the suggested substitutions, and (ii) decide to stop the iterative process.\nWe determine the best edit for each position i in yg by selecting the word with the highest probability estimate:\nyipred = arg max j\u2208Y\nF ( x,yg )i,j .\nThen we compute a confidence score in this prediction s(yg,ypred)i , possibly considering the prediction for the current guess word at the same position.\nThese scores are used to select the next position to edit,\ni? = arg max i s(yg,ypred) i\nand to stop the iterative process, i.e., when the confidence falls below a validated threshold t. We also limit the number of substitutions to a maximum of N . We consider different heuristics for s,\n\u2022 Score positions based on the model confidence in yipred, i.e.,\nsconf(yg,ypred) i = F(x,yg) i,yipred .\n\u2022 Look for high confidence in the suggested substitution yipred and low confidence in the current word yig:\nspr(yg,ypred) i\n= F(x,yg) i,yipred \u00d7 ( 1\u2212 F(x,yg)i,y i g ) .\n\u2022 Train a simple binary classifier taking as input the score of the best predicted word and the current guess word:\nscl(yg,ypred) i = nn ( logF(x,yg) i,yipred , logF(x,yg) i,yig ) ,\nwhere nn is a 2-layer neural network trained to predict whether a substitution leads to an increase in BLEU or not.\nWe compare the above strategies, different score thresholds t, and the maximum number of modifications per sentence allowed N in \u00a76.2."}, {"heading": "6 Experiments & Results", "text": "We first describe our experimental setup and then discuss our results."}, {"heading": "6.1 Experimental Setup", "text": "Data. We perform our experiments on the German-to-English WMT15 task (Bojar et al., 2015) and benchmark our improvements against the output of a phrase-based translation system (PBMT; Koehn et al. 2007) on this language pair. In principle, our approach may start from any initial guess translation. We chose the output of a phrase-based system because it provides a good starting point that can be computed at high speed. This allows us to quickly generate guess translations for the millions of sentences in our training set.\nAll data was lowercased and numbers were mapped to a single special \u201cnumber\u201d token. Infrequent tokens were mapped to an \u201cunknown\u201d token which resulted in dictionaries of 120K and 170K words for English and German respectively.\nFor training we used 3.5M sentence triples (source, reference, and the guess translation output by the PBMT system). A validation set of 180K triples was used for neural network hyperparameter selection and learning rate scheduling. Finally, two 3K subsets of the validation set were used to train the classifier discussed in \u00a75 and to select the best model architecture (single vs dual attention) and refinement heuristic.\nThe initial guess translations were generated with phrase-based systems trained on the same training data as our refinement models. We decoded the training data with ten systems, each trained on 90% of the training data in order to decode the remaining 10%. This procedure avoids the bias of generating guess translation with a system that was trained on the same data.\nImplementation. All models were implemented in Torch (Collobert et al., 2011) and trained with stochastic gradient descent to minimize the cross-entropy loss.\nFor the error detection model in \u00a72 we used two temporal convolutions on top of the lookup table, each followed by a tanh non-linearity to compute S(x) and T(yg). The output dimensions of each convolution was set to 256 and the receptive fields spanned 5 words, resulting in final outputs summarizing a context of 9 words.\nFor the single attention model we set the\nshared context embedding dimension dimSj = dimTi = 512 and use a context of size k = 4 words to the left and to the right, resulting in a window of size 9 for the source and 8 for the target. The final multilayer perceptron has 2 layers with a hidden dimension of 512, see \u00a73).\nFor the dual attention model we used 2-layer context embeddings (a convolution followed by a linear with a tanh in between), each having output dimension 512, context of size k = 4. The final multilayer perceptron has 2 layers with a hidden dimension of 1024, see \u00a74). In this setup, we replaced dot-product attention with MLP attention (Bahdanau et al., 2015) as it performed better on the validation set.\nAll weights were initialized randomly apart from the word embedding layers, which were precomputed with Hellinger Principal Component Analysis (Lebret and Collobert, 2014) applied to the bilingual co-occurrence matrix constructed on the training set. The word embedding dimension was set to 256 for both languages and all models."}, {"heading": "6.2 Results", "text": "Table 2 compares BLEU of the single and dual attention models (F vs Fdual) over the validation set. It reports the performance for the best threshold t \u2208 {0, 0.1, . . . , 1} and the best maximum number of modifications per sentence N \u2208 {0, 1, . . . , 10} for the different refinement heuristics.\nThe best performing configuration is Fdual with the product-based heuristic spr thresholded at t = 0.5 for up to N = 5 substitutions. We report test performance of this configuration in table 3. Tables 4, 5 and 6 show examples of system outputs. Overall the system obtains a small but consistent improvement over all the test sets.\nFigure 1 plots accuracy versus the number of allowed substitutions and Figure 2 shows the percentage of actually modified tokens. The dual attention model (\u00a74) outperforms single attention (\u00a73). Both models achieve most of improvement by making only 1-2 substitutions per sentence. Thereafter only very few substitutions are made with little impact on BLEU. Figure 2 shows that the models saturate quickly, indicating convergence of the refinement output to a state where the models have no more suggestions.\nTo isolate the model contribution from the scoring heuristic, we replace the scoring heuristic with an oracle while keeping the rest of the refinement\nstrategy the same. We consider two types of oracle: The full oracle takes the suggested substitution for each position and then selects which single position should be edited or whether to stop editing altogether. This oracle has the potential to find the largest BLEU improvement. The partial oracle does not select the position, it just takes the heuristic suggestion for the current step and decides whether to edit or stop the process. Notice that both oracles have very limited choice, as they are only able to perform substitutions suggested by our model.\nFigure 3 reports the performance of our best single and dual attention models compared to both oracles on the validation set; Figure 4 shows the corresponding number of substitutions. The full and partial oracles result in an improvement of +1.7 and +1.09 BLEU over the baseline in the dual attention setting (compared to +0.35 with spr).\nIn the single-attention setup the oracles yields a higher improvement (+2.37 and +1.3) and they also perform more substitutions. This supports our earlier conjecture (\u00a74) that Fdual is more conserva-\ntive and prone to copying words from the guess yg compared to the single attention model. While helpful in validation, the cautious nature of the dual model restricts the options of the oracle.\nWe make several observations. First, wordprediction models provide high-quality substitutions ypred that can lead to a significant improvements in BLEU (despite that both oracles are limited in their choice of ypred). This is supported by the simple heuristic sconf performing very close to more sophisticated strategies (Table 2).\nSecond, it is important to have a good confidence estimate on whether a substitution will improve BLEU or not. The full oracle, which yields +1.7 BLEU, acts as an estimate to having a realvalued confidence measure and replaces the scoring heuristic s. The partial oracle, yielding +1.09 BLEU, assesses the benefit of having a binaryvalued confidence measure. The latter oracle can only prevent our model from making a BLEUdamaging substitution. However, confidence estimation is a difficult task as we found in \u00a72.\nFinally, we demonstrate that a significant improvement in BLEU can be achieved through very few substitutions. The full and partial oracle modify only 1.69% and 0.99% of tokens, or 0.4 and 0.24 modifications per sentence, respectively. Of course, oracle substitution assumes access to the reference which is not available at test time. At the same time, our oracle is more likely to generate fluent sentences since it only has access to substitutions deemed likely by the model as opposed to an unrestricted oracle that is more likely to suggest improvements leading to unreasonable sentences. Note that our oracles only allow substitutions (no deletions or insertions), and only those that raise BLEU in a monotonic fashion, with each single refinement improving the score of the previous translation."}, {"heading": "7 Conclusion and Future Work", "text": "We present a simple iterative decoding scheme for machine translation which is motivated by the inability of existing models to revisit incorrect decoding decisions made in the past. Our models improve an initial guess translation via simple word substitutions over several rounds. At each round, the model has access to the source as well as the output of the previous round, which is an entire translation of the source. This is different to existing decoding algorithms which make predictions\nbased on a limited partial translation and are unable to revisit previous erroneous decoding decisions.\nOur results increase translation accuracy by up to 0.4 BLEU on WMT15 German-English translation and modify only 0.6 words per sentence. In our experimental setup we start with the output of a phrase-based translation system but our model is general enough to deal with arbitrary guess translations.\nWe see several future work avenues from here. Experimenting with different initial guess translations such as the output of a neural translation system, or even the result of a simple dictionarybased word-by-word translation scheme. Also one can envision editing a number of guess translations simultaneously by expanding the dual attention mechanism to attend over multiple guesses.\nSo far we only experimented with word substitution, one may add deletion, insertion or even swaps of single or multi-word units. Finally, the dual-attention model in \u00a74 may present a good starting point for neural multi-source translation (Schroeder et al., 2009)."}, {"heading": "Acknowledgments", "text": "We would like to thank Marc\u2019Aurelio Ranzato and Sumit Chopra for helpful discussions related to this work."}, {"heading": "A Examples", "text": ""}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. of ICLR. Association for Computational Linguistics, May.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "J. Mach. Learn. Res., 3:1137\u20131155, March.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Confidence estimation for machine translation", "author": ["John Blatz", "Erin Fitzgerald", "George F. Foster", "Simona Gandrabur", "Cyril Goutte", "Alex Kulesza", "Alberto Sanch\u0131\u0301s", "Nicola Ueffing"], "venue": "In Proc. of COLING", "citeRegEx": "Blatz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Blatz et al\\.", "year": 2004}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin M. Verspoor", "Marcos Zampieri."], "venue": "WMT.", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet."], "venue": "BigLearn, NIPS Workshop.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "What\u2019s in a translation rule", "author": ["Michel Galley", "Mark Hopkins", "Kevin Knight", "Daniel Marcu"], "venue": null, "citeRegEx": "Galley et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2004}, {"title": "Forest-based algorithms in natural language processing", "author": ["Liang Huang."], "venue": "Ph.D. thesis, University of Pennsylvania.", "citeRegEx": "Huang.,? 2008", "shortCiteRegEx": "Huang.", "year": 2008}, {"title": "Statistical Phrase-Based Translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "pages 127\u2013133, Edmonton, Canada, May.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Word embeddings through hellinger pca", "author": ["R\u00e9mi Lebret", "Ronan Collobert."], "venue": "14th Conference of the European Chapter of the Association for Computational Linguistics.", "citeRegEx": "Lebret and Collobert.,? 2014", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "Neural network-based word alignment through score aggregation", "author": ["Joel Legrand", "Michael Auli", "Ronan Collobert."], "venue": "Proceedings of WMT.", "citeRegEx": "Legrand et al\\.,? 2016", "shortCiteRegEx": "Legrand et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Llus Mrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton, editors, EMNLP, pages 1412\u20131421. The", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318,", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A neural attention model for sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proc. of EMNLP.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Word lattices for multi-source translation", "author": ["Josh Schroeder", "Trevor Cohn", "Philipp Koehn."], "venue": "Proc. of EACL.", "citeRegEx": "Schroeder et al\\.,? 2009", "shortCiteRegEx": "Schroeder et al\\.", "year": 2009}, {"title": "Statistical phrase-based post-editing", "author": ["Michel Simard", "Cyril Goutte", "Pierre Isabelle."], "venue": "Proc. of NAACL.", "citeRegEx": "Simard et al\\.,? 2007", "shortCiteRegEx": "Simard et al\\.", "year": 2007}, {"title": "Wordlevel confidence estimation for machine translation", "author": ["Nicola Ueffing", "Hermann Ney."], "venue": "Computational Linguistics, 33:9\u201340.", "citeRegEx": "Ueffing and Ney.,? 2007", "shortCiteRegEx": "Ueffing and Ney.", "year": 2007}], "referenceMentions": [{"referenceID": 7, "context": "Existing decoding schemes for translation generate outputs either left-to-right, such as for phrasebased or neural translation models, or bottom-up as in syntactic models (Koehn et al., 2003; Galley et al., 2004; Bahdanau et al., 2015).", "startOffset": 171, "endOffset": 235}, {"referenceID": 5, "context": "Existing decoding schemes for translation generate outputs either left-to-right, such as for phrasebased or neural translation models, or bottom-up as in syntactic models (Koehn et al., 2003; Galley et al., 2004; Bahdanau et al., 2015).", "startOffset": 171, "endOffset": 235}, {"referenceID": 0, "context": "Existing decoding schemes for translation generate outputs either left-to-right, such as for phrasebased or neural translation models, or bottom-up as in syntactic models (Koehn et al., 2003; Galley et al., 2004; Bahdanau et al., 2015).", "startOffset": 171, "endOffset": 235}, {"referenceID": 6, "context": "A beam of size 50 contains fewer than six binary decisions, all of which frequently share the same prefix (Huang, 2008).", "startOffset": 106, "endOffset": 119}, {"referenceID": 11, "context": "4 BLEU (Papineni et al., 2002) by making on average only 0.", "startOffset": 7, "endOffset": 30}, {"referenceID": 14, "context": "Our approach differs from automatic postediting since it does not require post-edited text which is a scarce resource (Simard et al., 2007; Bojar et al., 2016).", "startOffset": 118, "endOffset": 159}, {"referenceID": 9, "context": "We use an architecture similar to the word alignment model of Legrand et al. (2016). The source and the target sequences are embedded via a lookup table that replace each word type with a learned vector.", "startOffset": 62, "endOffset": 84}, {"referenceID": 2, "context": "The task of predicting mistakes is not easy as previously shown in confidence estimation (Blatz et al., 2004; Ueffing and Ney, 2007).", "startOffset": 89, "endOffset": 132}, {"referenceID": 15, "context": "The task of predicting mistakes is not easy as previously shown in confidence estimation (Blatz et al., 2004; Ueffing and Ney, 2007).", "startOffset": 89, "endOffset": 132}, {"referenceID": 1, "context": "In other words, we learn a non-causal language model (Bengio et al., 2003) which is also conditioned on the source x.", "startOffset": 53, "endOffset": 74}, {"referenceID": 10, "context": "These weights correspond to dot-product attention scores (Luong et al., 2015; Rush et al., 2015).", "startOffset": 57, "endOffset": 96}, {"referenceID": 12, "context": "These weights correspond to dot-product attention scores (Luong et al., 2015; Rush et al., 2015).", "startOffset": 57, "endOffset": 96}, {"referenceID": 0, "context": "Similar to maximum likelihood training for left-to-right translation systems (Bahdanau et al., 2015), the model is therefore not exposed to the same type of context in training (reference contexts from yref) and testing (guess contexts from yg).", "startOffset": 77, "endOffset": 100}, {"referenceID": 0, "context": "Similar to maximum likelihood training for left-to-right translation systems (Bahdanau et al., 2015), the model is therefore not exposed to the same type of context in training (reference contexts from yref) and testing (guess contexts from yg). Discussion. Our model is similar to the attention-based translation approach of Bahdanau et al. (2015). In addition to using convolutions, the main difference is that we have access to both left and right target context y\u2212i|k since we start from an initial guess translation.", "startOffset": 78, "endOffset": 349}, {"referenceID": 4, "context": "All models were implemented in Torch (Collobert et al., 2011) and trained with stochastic gradient descent to minimize the cross-entropy loss.", "startOffset": 37, "endOffset": 61}, {"referenceID": 0, "context": "In this setup, we replaced dot-product attention with MLP attention (Bahdanau et al., 2015) as it performed better on the validation set.", "startOffset": 68, "endOffset": 91}, {"referenceID": 8, "context": "All weights were initialized randomly apart from the word embedding layers, which were precomputed with Hellinger Principal Component Analysis (Lebret and Collobert, 2014) applied to the bilingual co-occurrence matrix constructed on the training set.", "startOffset": 143, "endOffset": 171}, {"referenceID": 13, "context": "Finally, the dual-attention model in \u00a74 may present a good starting point for neural multi-source translation (Schroeder et al., 2009).", "startOffset": 110, "endOffset": 134}], "year": 2016, "abstractText": "Existing machine translation decoding algorithms generate translations in a strictly monotonic fashion and never revisit previous decisions. As a result, earlier mistakes cannot be corrected at a later stage. In this paper, we present a translation scheme that starts from an initial guess and then makes iterative improvements that may revisit previous decisions. We parameterize our model as a convolutional neural network that predicts discrete substitutions to an existing translation based on an attention mechanism over both the source sentence as well as the current translation output. By making less than one modification per sentence, we improve the output of a phrase-based translation system by up to 0.4 BLEU on WMT15 German-English translation.", "creator": "TeX"}}}