{"id": "1610.07419", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "Using Machine Learning to Detect Noisy Neighbors in 5G Networks", "abstract": "fast networks are apt to be more dynamic and chaotic regarding signal structure than efficient communication. unless the initiation of logical function generation ( nfv ), network functions ( nf ) requires h longer handle tightly coupled with the hardware they are receiving it, which poses new challenges in network modelling. noisy deployment is a term thus taken to name situations in wire services where an applicant experiences questionable failure stability due to the fact that vastly extra unnecessary resources customer employs are possessed by other tasks in those same cloud protocol. these situations readily be easily identified citing straightforward approaches, which calls for the use for sophisticated methods for monitoring infrastructure management. user computing capacity data demonstrate multiple magnetic imaging ( ir ) techniques can be used to identify different events. through experiments using data collected beyond real nfv infrastructure, we show that simplified models among efficient classification tools generate the noisy neighbor phenomenon ( an accuracy is more than 90 % in a simple scenario.", "histories": [["v1", "Mon, 24 Oct 2016 14:07:56 GMT  (32kb,D)", "http://arxiv.org/abs/1610.07419v1", null]], "reviews": [], "SUBJECTS": "cs.NI cs.LG", "authors": ["udi margolin", "alberto mozo", "bruno ordozgoiti", "danny raz", "elisha rosensweig", "itai segall"], "accepted": false, "id": "1610.07419"}, "pdf": {"name": "1610.07419.pdf", "metadata": {"source": "CRF", "title": "Using Machine Learning to Detect Noisy Neighbors in 5G Networks", "authors": ["Udi Margolin", "Alberto Mozo", "Bruno Ordozgoiti", "Danny Raz", "Elisha Rosensweig", "Itai Segall"], "emails": ["bruno.ordozgoiti@etsisi.upm.es,", "danny.raz@nokia.com"], "sections": [{"heading": null, "text": "Using Machine Learning to Detect Noisy Neighbors in 5G Networks\nUdi Margolin\u2217, Alberto Mozo\u2020, Bruno Ordozgoiti\u2020, Danny Raz\u2217, Elisha Rosensweig\u2217 and Itai Segall \u2217Bell Labs Israel\n\u2020Universidad Polite\u0301cnica de Madrid Email: bruno.ordozgoiti@etsisi.upm.es, danny.raz@nokia.com\nAbstract\u20145G networks are expected to be more dynamic and chaotic in their structure than current networks. With the advent of Network Function Virtualization (NFV), Network Functions (NF) will no longer be tightly coupled with the hardware they are running on, which poses new challenges in network management. Noisy neighbor is a term commonly used to describe situations in NFV infrastructure where an application experiences degradation in performance due to the fact that some of the resources it needs are occupied by other applications in the same cloud node. These situations cannot be easily identified using straightforward approaches, which calls for the use of sophisticated methods for NFV infrastructure management. In this paper we demonstrate how Machine Learning (ML) techniques can be used to identify such events. Through experiments using data collected at real NFV infrastructure, we show that standard models for automated classification can detect the noisy neighbor phenomenon with an accuracy of more than 90% in a simple scenario.\nI. INTRODUCTION 5G networks are expected to be inherently more dynamic and chaotic in their structure than current networks. With the advent of Network Function Virtualization (NFV), Network Functions (NF) will no longer be tightly coupled with the hardware they are running on. Instead, when considering network design and management a decade into the future, it is believed that many NFs will be virtualized, and thus able to be deployed, scaled and migrated with ease and speed unheard of in todays networks. As a result, while the topology of the physical network infrastructure shall evolve on a slow time scale similar to that of current networks, the dynamics of the virtual/logical topology will undergo a dramatic change and be allowed to evolve at much faster speeds, in service of customer needs and demands.\nWhen considering network management in such an environment, achieving the degree of reliability and stability that is to be expected of 5G becomes a challenge. Specifically, when the network topology is in constant flux, it is more difficult to make decisions regarding where to locate services, ensure the SLA a provider is committed to, and perform Root Cause Analysis (RCA) on system faults. One specific challenge in this domain is the problem of allocating network resources in a dynamically changing environment like NFV [2]. This challenge relates to the problem of service demand prediction and provisioning which allows the network to resize and resource itself, using virtualization, to serve predicted demand according to parameters such as location, time and specific service demand from specific users or user groups.\nThe more specific issues we will address in this paper is identifying the state of a service and the reason for this state. More specifically we want to identify that the service is facing difficulties and identify the cause of these difficulties. This is a critical building block in addressing use cases similar to the Optimized Services in Dynamic Environments use case described in [10]. Noisy neighbor is a term commonly used to describe situations in cloud computing where an application (or a VM) experiences degradation in performance due to the fact that some of the resources it needs are occupied by other applications (VMs) in the same cloud node. Note that in general there should be a clear separation between the different tenants on the same physical machine, but in practice this isolation of the virtual machines is far from perfect and many resources such as internal networking resources and memory access resources are shared at some level. In this paper we demonstrate how Machine Learning (ML) techniques can be used to identify such events. We show that using standard models for automated classification, noisy neighbors can be detected in a simple scenario with more than 90% accuracy. As said this is a critical building block in creating flexible reliable orchestration mechanisms for 5G networks as once identified, the system can resolve the performance issue by migrating one of the tenants to another machine or by allocating more resources. The rest of the paper is structured as folows: section II describes the problem setting in detail. Section III describes the employed machine learning models. Section IV provides experimental results and finally section V outlines future work and offers concluding remarks."}, {"heading": "II. PROBLEM SETTING", "text": "This testbed environment consists of a total of five high performance HP servers (each having 12 cores) with a proprietary management system running on top of an OpenStack cloud [3]. Two of the servers are dedicated to the management processes, while the other three are available as compute machines.\nAn open source voice-over-IP (VoIP) application, Asterisk [1], is deployed on one of the compute machines, in a single virtual machine utilizing one core and 1024 MB of memory. A corresponding traffic generator, SipP [4] is deployed in a second machine, and configured to log in and log out of the server, and initiate calls to a line in which the server is playing music-on-hold. The traffic generator is configured in a manner\nar X\niv :1\n61 0.\n07 41\n9v 1\n[ cs\n.N I]\n2 4\nO ct\n2 01\n6\nthat generates constant reasonable load on the server, e.g. 40% CPU utilization.\nAnother application that was considered is a Fibonacci server a specially-tailored application, designed to mimic Virtual Network Functions in behaviour, but be simple enough for experimentation and analysis. The server computes elements in the Fibonacci series on demand. A client may request the index of the element to be computed. Different implementations of the server are considered, to examine the noisy neighbour effect on them.\nA noisy neighbour is defined as (one or more) virtual machines (VMs) sharing resources with the server under test, thus affecting its performance. Since both of the servers under test are CPU intensive, we focused on CPU noise. Noise was generated using the \u201dstress\u201d Linux application. In its CPU-intensive setting, it continuously computes squareroots of numbers in order to generate load on CPUs. Noise is therefore simulated by further noise generation virtual machines, all launched on the same server as the VoIP server. We experimented with both a single large VM occupying all physical CPUs, as well as multiple small VMs, each occupying only one or two cores. During experimentations, the following metrics were collected: \u2022 CPU utilization of the server VM \u2022 In-bound network traffic of the server VM \u2022 Out-bound network traffic of the server VM \u2022 CPU utilization of the noise VM(s). The purpose of this\nmetric is to act as an indicator of whether noise exists or not, for the ML training period\nMonitoring is performed using Openstack Ceilometer, and the data is collected by the proprietary management system into a DB for further analysis and extraction. Experimentations differ in the following attributes: \u2022 Server under test: Asterisk vs. Synthetic \u2022 Amount of noise: One VM utilizing all 24 cores vs.\nseveral (between 18 and 24) utilizing a single core each. \u2022 Amount of traffic generated (and respectively amount of\nCPU utilization in the server)"}, {"heading": "III. MACHINE LEARNING METHODS", "text": "The problem of determining whether or not the behavior of a virtual machine is being caused by the presence of a noisy neighbor is non-trivial. Based on the metrics described above, a simple thresholding approach or a set of rules would not suffice.\nTherefore, to address this problem we propose the use of machine learning methods. Machine learning has proved successful in a wide variety of domains, such as computer vision [15] [11], bioinformatics [17], natural language processing [12][16] and others. A key advantage of machine learning is that the same models can be applied to completely different domains provided that the data are transformed into an adequate representation.\nIn this paper we address the problem of determining whether or not a noisy neighbor situation is taking place, which can\nbe modelled in machine learning as a classification problem. Hence, we propose to employ two well-known classifiers: support vector machines and random forests."}, {"heading": "A. Support vector machines", "text": "Given a set of data points represented as real-valued vectors corresponding to two different classes, support vector machines (SVM) [9] pose the problem of finding a maximummargin separating hyperplane. It can be shown that this is equivalent to minimizing the norm of the vector orthogonal to said hyperplane, constraining the data points of different classes to be on different sides and at a minimum distance from it. Formally, this can be formulated as follows.\nGiven a set of data points {x1, . . . , xn} and corresponding labels {y1, . . . , yn}, where xi \u2208 Rd and yi \u2208 {\u22121, 1}, i = 1, . . . , n, for some d \u2208 N, find\nmin w\n1 2 \u2016w\u20162 + C \u2211 i \u03bei\ns.t. yi(wxi) \u2265 1\u2212 \u03bei , i = 1, . . . , n \u03bei \u2265 0 i = 1, . . . , n\n(1)\nHere, we introduce slack variables \u03bei, which allow for violations of the separation constraint. This formulation is often referred to as soft-margin SVM. The variable C \u2208 R is a hyperparameter that allows us to control the amount of penalization incurred by these violations.\nIn the case of support vector machines, strong duality holds [6]. Therefore, a Lagrangian can be formulated such that its maximum value provides the solution to our problem. Optimizing with respect to the corresponding variables yields the following formulation.\nmax \u03b1 \u2211 i \u03b1i \u2212 1 2 \u2211 i \u2211 j \u03b1i\u03b1jyiyj\u3008xi, xj\u3009\ns.t. \u2211 i yi\u03b1i = 0\n0 \u2264 \u03b1i \u2264 C i = 1, . . . , n\n(2)\nHere, \u03b1i, i = 1, . . . , n are the Lagrange multipliers corresponding to the inequality constraint of the primal formulation.\nThe sole dependence of this formulation on the inner products allows for the use of a technique often referred to as the kernel trick [5]. Kernel functions implicitly map vectors to a higher dimensional space and compute their inner product. This enables learning separating hyperplanes for non-separable data."}, {"heading": "B. Random forests", "text": "Random forests [8] are a learning model that combines decision trees with the concept of bagging [7] and random feature selection. Random forests are widely popular because of their simplicity, efficiency and effectiveness.\nRandom forests function by combining bagging, or bootstrap aggregation, with decision trees. Specifically a number of random samples with replacement is drawn from the data\nset and a decision tree is trained for each one of them. The final decision for classifying an unseen data instance is taken by majority vote of the trained decision trees. Decision trees, in turn, are built by constructing a hierarchy of binary split nodes. The criterion to choose the splitting rule for each node is the maximization of information gain based of some measure of entropy.\nIn addition to the bagging process, random forests also employ random subsets of features. In our case, though, due to the low number of features we limit the model to bagged decision trees."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "In this section, we describe the experiments carried out in order to assess the validity of machine learning as a method for detecting noisy neighbors, as well as the results obtained."}, {"heading": "A. Data", "text": "We collected the Ceilometer metrics as described in section II, running around 100 experiments of the system described above. The metrics were collected each 10 seconds approximately. These measurements were aggregated into 30 second periods in order to avoid the impact of missing values or irregularities in the sampling frequency. Each of the resulting data points thus represents the average CPU load, inbound and outbound bandwidth of the monitored machine. The corresponding binary label determines whether or not the noisy neighbor was inflicting load during that period. The resulting data set is comprised of 9169 data instances, out of which 3088 correspond to noisy neighbors.\nAs stated above, detecting noisy neighbors from the available data is non-trivial. To illustrate this fact, Table I shows two different measurements of the relationship between the collected features and the noise value. The first row shows the correlation coefficient between each of the different features and the CPU load of the machine that creates noise. The second row shows the maximal information coefficient (MIC) [14]. It is apparent that none of the available features is sufficiently reliable as a predictor on its own, suggesting that the noisy neighbor phenomenon manifests itself in a complex manner."}, {"heading": "B. Experiments", "text": "We train the two models described in section III to automatically classify data instances as corresponding to a noisy neighbor situation or not. The input data is standardized to\nzero mean and unit variance. In both cases we follow a 10- fold cross validation process. We now describe the specific training procedures for each of the models.\n1) Support vector machines: We train `1 soft-margin models with the sequential minimal optimization algorithm [13] using a Gaussian kernel. The Gaussian kernel can be defined as K(u, v) = e\u2212\u03b3\u2016u\u2212v\u2016 2\n, where \u03b3 is a hyperparameter. We train models using different values of C. A quadratic expansion of the features is employed, i.e. each data instance (x1, x2, x3) is transformed into (x1, x2, x3, x21, x 2 2, x 2 3, x1x2, x1x3, x2x3).\n2) Random forests: As stated above, given the small number of features we limit the model to bagged decision trees employing all features. Gini\u2019s diversity index is employed for splitting nodes.\n3) Results: In our experiments, both models attained acceptable levels of classification performance. We measure precision, recall and the F1 score, which are defined as follows\nprecision = tp\ntp+ fp\nrecall = tp\ntp+ fn\nF1 score = 2 \u2217 precision*recall precision+recall\nThe models were trained with varying values of certain parameters. In the case of support vector machines, we employed varying values of the parameter C in equation 1. Figure 1 shows how the model behaves as we increase this value. As C approaches infinity, the model becomes closer to the hard-margin variant and the training process becomes more expensive. It can be seen that no significant gains are obtained beyond a value of C = 22 (note the root scale for C). At a certain point, the model is expected to overfit the training data and thus the errors on test data will increase. In the case of random forests, we train models using different numbers of decision trees. Figure 2 shows how the performance varies as more trees are added to the ensemble. It can be seen that no significant improvement is gained from adding more than 50 trees.\nTable II shows the values of the three performance metrics for the best models in terms of F1 score for both SVMs and random forests. The best SVM model was trained with C = 3.82. However, the edge over more lenient and thus more efficiently trainedd models is very moderate. The best random forest was trained with 300 trees."}, {"heading": "V. CONCLUSIONS & FUTURE WORK", "text": "In this paper we have studied how machine learning can help in managing NFV infrastructure, which is expected to become a prevalent approach in 5G networks. Specifically, we have shown that standard models for classification can detect the noisy neighbor phenomenon with an accuracy of more than 90% in a simple scenario. In the future it would\nbe interesting to evaluate how these models perform in more complex scenarios. It would also be interesting to extract more features from the infrastructure as well as to employ more sophisticated models for improved classification rates."}, {"heading": "ACKNOWLEDGMENT", "text": "The research leading to these results has received funding from the European Union under the H2020 grant agreement n. 671625 (project CogNet).\nThe authors would like to thank Ignacio Rubio Lo\u0301pez for his valuable help running experiments."}], "references": [{"title": "A training algorithm for optimal margin classifiers", "author": ["Bernhard E Boser", "Isabelle M Guyon", "Vladimir N Vapnik"], "venue": "In Proceedings of the fifth annual workshop on Computational learning theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1992}, {"title": "Can machine learning aid in delivering new use cases & scenarios in 5g", "author": ["T.S. Buda"], "venue": "In The First IFIP/IEEE International Workshop on Management of 5G Networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines", "author": ["John Platt"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Detecting novel associations in large data", "author": ["David N Reshef", "Yakir A Reshef", "Hilary K Finucane", "Sharon R Grossman", "Gilean McVean", "Peter J Turnbaugh", "Eric S Lander", "Michael Mitzenmacher", "Pardis C Sabeti"], "venue": "sets. science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In INTERSPEECH,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "A review of ensemble methods in bioinformatics", "author": ["Pengyi Yang", "Yee Hwa Yang", "Bing B Zhou", "Albert Y Zomaya"], "venue": "Current Bioinformatics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "This is a critical building block in addressing use cases similar to the Optimized Services in Dynamic Environments use case described in [10].", "startOffset": 138, "endOffset": 142}, {"referenceID": 6, "context": "vision [15] [11], bioinformatics [17], natural language processing [12][16] and others.", "startOffset": 7, "endOffset": 11}, {"referenceID": 2, "context": "vision [15] [11], bioinformatics [17], natural language processing [12][16] and others.", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "vision [15] [11], bioinformatics [17], natural language processing [12][16] and others.", "startOffset": 33, "endOffset": 37}, {"referenceID": 3, "context": "vision [15] [11], bioinformatics [17], natural language processing [12][16] and others.", "startOffset": 67, "endOffset": 71}, {"referenceID": 7, "context": "vision [15] [11], bioinformatics [17], natural language processing [12][16] and others.", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "The sole dependence of this formulation on the inner products allows for the use of a technique often referred to as the kernel trick [5].", "startOffset": 134, "endOffset": 137}, {"referenceID": 5, "context": "The second row shows the maximal information coefficient (MIC) [14].", "startOffset": 63, "endOffset": 67}, {"referenceID": 4, "context": "1) Support vector machines: We train `1 soft-margin models with the sequential minimal optimization algorithm [13] using a Gaussian kernel.", "startOffset": 110, "endOffset": 114}], "year": 2016, "abstractText": "5G networks are expected to be more dynamic and chaotic in their structure than current networks. With the advent of Network Function Virtualization (NFV), Network Functions (NF) will no longer be tightly coupled with the hardware they are running on, which poses new challenges in network management. Noisy neighbor is a term commonly used to describe situations in NFV infrastructure where an application experiences degradation in performance due to the fact that some of the resources it needs are occupied by other applications in the same cloud node. These situations cannot be easily identified using straightforward approaches, which calls for the use of sophisticated methods for NFV infrastructure management. In this paper we demonstrate how Machine Learning (ML) techniques can be used to identify such events. Through experiments using data collected at real NFV infrastructure, we show that standard models for automated classification can detect the noisy neighbor phenomenon with an accuracy of more than 90% in a simple scenario.", "creator": "LaTeX with hyperref package"}}}