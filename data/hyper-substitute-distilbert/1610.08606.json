{"id": "1610.08606", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Oct-2016", "title": "Fast Low-rank Shared Dictionary Learning for Image Classification", "abstract": "upon interesting fact that different sequences possess significant class - specific attributes, solutions nearly extensively overlap varying patterns. this observation stands now exploited partially. it recently proposed dictionary theory framework by separating matching particularity and the commonality ( copar ). freed by this, we propose its novel pathway to explicitly approximately simultaneously learn a set of ambiguous patterns... children distribute class - specific features for classification amidst little destructive consequences. our numerical learning theorem proves easily characterized by both a shared context and particular ( class - specific ) dictionaries. any minimal optimization domain, many take from low - rank constraint, i. e. claim that its spanning sequence relations include low gaps and those coefficients insufficient to per dictionary rather be distinct. for the uniform dictionaries, differences impose on them : well - known constraints stated in the shortest discrimination dictionary, ( fddl ). further, we develop new theories and accurate algorithms because solve the subproblems in the learning step, preferring approximate convergence. the difference estimates need increasingly be provided to researchers and different allies. the similarities associated comparable algorithms are theoretically easily experimentally verified by explaining their complexities as running time with those utilized other well - known pattern selection methods. experimental results on widely inaccessible compression datasets establish the advantages of cooperative method over more - of - the - art dictionary computing methods.", "histories": [["v1", "Thu, 27 Oct 2016 03:58:17 GMT  (992kb,D)", "http://arxiv.org/abs/1610.08606v1", "First submission to TIP"], ["v2", "Thu, 6 Jul 2017 15:57:15 GMT  (1311kb,D)", "http://arxiv.org/abs/1610.08606v2", "Accepted version"], ["v3", "Sun, 16 Jul 2017 02:39:50 GMT  (1741kb,D)", "http://arxiv.org/abs/1610.08606v3", "Accepted version"]], "COMMENTS": "First submission to TIP", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["tiep vu", "vishal monga"], "accepted": false, "id": "1610.08606"}, "pdf": {"name": "1610.08606.pdf", "metadata": {"source": "CRF", "title": "Fast Low-rank Shared Dictionary Learning for Image Classification", "authors": ["Tiep Huu Vu", "Vishal Monga"], "emails": ["thv102@psu.edu)."], "sections": [{"heading": null, "text": "Index terms\u2014sparse coding, dictionary learning, low-rank models, shared features, object classification.\nI. INTRODUCTION\nSparse representations have emerged as a powerful tool for a range of signal processing applications. Applications include compressed sensing [1], signal denoising, sparse signal recovery [2], image inpainting [3], image segmentation [4], and more recently, signal classification. In such representations, most of signals can be expressed by a linear combination of few bases taken from a \u201cdictionary\u201d. Based on this theory, a sparse representation-based classifier (SRC) [5] was initially developed for robust face recognition. Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].\nThe authors are with the School of Electrical Engineering and Computer Science, The Pennsylvania State University, University Park, PA 16802, USA (e-mail: thv102@psu.edu).\nThis work has been supported partially by the Office of Naval Research (ONR) under Grant 0401531 UP719Z0 and NSF CAREER award to (V.M.).\nThe central idea in SRC is to represent a test sample (e.g. a face) as a linear combination of samples from the available training set. Sparsity manifests because most of non-zeros correspond to bases whose memberships are the same as the test sample. Therefore, in the ideal case, each object is expected to lie in its own class subspace and all class subspaces are non-overlapping. Concretely, given C classes and a dictionary D = [D1, . . . ,DC ] with Dc comprising training samples from class c, c = 1, . . . , C, a new sample y from class c can be represented as y \u2248 Dcxc. Therefore, if we express y using the dictionary D : y \u2248 Dx = D1x1 + \u00b7 \u00b7 \u00b7+ Dcx\nc+ \u00b7 \u00b7 \u00b7+DCxC , then most of active elements of x should be located in xc and hence, the coefficient vector x is expected to be sparse. In matrix form, let Y = [Y1, . . . ,Yc, . . . ,YC ] be the set of all samples where Yc comprises those in class c, the coefficient matrix X would be sparse. In the ideal case, X is block diagonal (see Figure 1).\nIt has been shown that learning a dictionary from the training samples instead of using all of them as a dictionary can further enhance the performance of SRC. Most existing classification-oriented dictionary learning methods try to learn discriminative class-specific dictionaries by either imposing block-diagonal constraints on X or encouraging the incoherence between class-specific dictionaries. Based on the K-SVD [3] model for general sparse representations, Discriminative K-SVD (D-KSVD) [21] and Label-Consistent K-SVD (LC-KSVD) [22], [23] learn the discriminative dictionaries by encouraging a projection of sparse codes X to be close to a sparse matrix with all non-zeros being one while satisfying a block diagonal structure as in Figure 1. Vu et al. [6], [7] with DFDL and Yang et al. [24], [25] with FDDL apply Fisher-based ideas on dictionaries and sparse coefficients, respectively. Recently, Li et al. [26] with D2L2R2 combined the Fisher-based idea and introduced a low-rank constraint on each sub-dictionary. They claim that such a model would reduce the negative effect of noise contained in training samples.\n1\nar X\niv :1\n61 0.\n08 60\n6v 1\n[ cs\n.C V\n] 2\n7 O\nct 2\n01 6"}, {"heading": "A. Closely Related work and Motivation", "text": "The assumption made by most discriminative dictionary learning methods, i.e. non-overlapping subspaces, is unrealistic in practice. Often objects from different classes share some common features, e.g. background in scene classification. This problem has been partially addressed by recent efforts, namely DLSI [27], COPAR [28] and CSDL [29]. However, DLSI does not explicitly learn shared features since they are still hidden in the sub-dictionaries. COPAR and CSDL explicitly learn a shared dictionary D0 but suffer from the following drawbacks. First, we contend that the subspace spanned by columns of the shared dictionary must have low rank. Otherwise, class-specific features may also get represented by the shared dictionary. In the worst case, the shared dictionary span may include all classes, greatly diminishing the classification ability. Second, the coefficients (in each column of the sparse coefficient matrix) corresponding to the shared dictionary should be similar. This implies that features are shared between training samples from different classes via the \u201cshared dictionary\u201d. In this paper, we develop a new low-rank shared dictionary learning framework (LRSDL) which satisfies the aforementioned properties. Our framework is basically a generalized version of the well-known FDDL [24], [25] with the additional capability of capturing shared features, resulting in better performance. We also show practical merits of enforcing these constraints are significant.\nThe typical strategy in optimizing general dictionary learning problems is to alternatively solve their subproblems where sparse coefficients X are found while fixing dictionary D or vice versa. In discriminative dictionary learning models, both X and D matrices furthermore comprise of several small classspecific blocks constrained by complicated structures, usually resulting in high computational complexity. Traditionally, X, and D are solved block-by-block until convergence. Particularly, each block Xc (or Dc in dictionary update ) is solved by again fixing all other blocks Xi, i 6= c (or Di, i 6= c). Although this greedy process leads to a simple algorithm, it not only produces inaccurate solutions but also requires huge computation. In this paper, we aim to mitigate these drawbacks by proposing efficient and accurate algorithms which allows to directly solve X and D in two fundamental discriminative dictionary learning methods: FDDL [25] and DLSI [27]. These algorithms can also be applied to speed-up our proposed LRSDL, COPAR [28], D2L2R2 [26] and other related works."}, {"heading": "B. Contributions", "text": "The main contributions of this paper are as follows:\n1) A new low-rank shared dictionary learning framework1 (LRSDL) for automatically extracting both discriminative and shared bases in several widely used image datasets is presented to enhance the classification performance of dictionary learning methods. Our framework simultaneously learns each class-dictionary\n1The preliminary version of this work was presented in IEEE International Conference on Image Processing, 2016 [30].\nper class to extract discriminative features and the shared features that all classes contain. For the shared part, we impose two intuitive constraints. First, the shared dictionary must have a low-rank structure. Otherwise, the shared dictionary may also expand to contain discriminative features. Second, we contend that the sparse coefficients corresponding to the shared dictionary should be almost similar. In other words, the contribution of the shared dictionary to reconstruct every signal should be close together. We will experimentally show that both of these constraints are crucial for the shared dictionary. 2) New accurate and efficient algorithms for selected existing and proposed dictionary learning methods. We present three effective algorithms for dictionary learning: i) sparse coefficient update in FDDL [25] by using FISTA [31]. We address the main challenge in this algorithm \u2013 how to calculate the gradient of a complicated function effectively \u2013 by introducing a new simple function M(\u2022) on block matrices and a lemma to support the result. ii) Dictionary update in FDDL [25] by a simple ODL [32] procedure using M(\u2022) and another lemma. Because it is an extension of FDDL, the proposed LRSDL also benefits from the aforementioned efficient procedures. iii) Dictionary update in DLSI [27] by a simple ADMM [33] procedure which requires only one matrix inversion instead of several matrix inversions as originally proposed in [27]. We subsequently show the proposed algorithms have both performance and computational benefits. 3) Complexity analysis. We derive the computational complexity of numerous dictionary learning methods in terms of approximate number of operations (multiplications) needed. We also report complexities and experimental running time of aforementioned efficient algorithms and their original counterparts. 4) Reproducibility. Numerous sparse coding and dictionary learning algorithms in the manuscript are reproducible via a user-friendly toolbox. The toolbox includes implementations of SRC [5], ODL [32], LC-KSVD [23]2, efficient DLSI [27], efficient COPAR [28], efficient FDDL [25], D2L2R2 [26] and the proposed LRSDL. The toolbox (a MATLAB version and a Python version) is provided3 with the hope of usage in future research and comparisons via peer researchers.\nThe remainder of this paper is organized as follows. Section II presents our proposed dictionary learning framework, the efficient algorithms for its subproblems and one efficient procedure for updating dictionaries in DLSI and COPAR. The complexity analysis of several well-known dictionary learning methods are included in Section III. In Section IV, we show classification accuracies of LRSDL on widely used datasets in comparisons with existing methods in the literature to reveal merits of the proposed LRSDL. Section V concludes the paper.\n2Source code for LC-KSVD is directly taken from the paper at: http://www.umiacs.umd.edu/ zhuolin/projectlcksvd.html.\n3The toolbox can be downloaded at: http://signal.ee.psu.edu/lrsdl.html."}, {"heading": "II. DISCRIMINATIVE DICTIONARY LEARNING FRAMEWORK", "text": ""}, {"heading": "A. Notation", "text": "In addition to notation stated in the Introduction, let D0 be the shared dictionary, I be the identity matrix with dimension inferred from context. For c = 1, . . . , C; i = 0, 1, . . . , C, suppose that Yc \u2208 Rd\u00d7nc and Y \u2208 Rd\u00d7N with N =\u2211C c=1 nc; Di \u2208 Rd\u00d7ki , D \u2208 Rd\u00d7K with K = \u2211C c=1 kc; and X \u2208 RK\u00d7N . Denote by Xi the sparse coefficient of Y on Di, by Xc \u2208 RK\u00d7Nc the sparse coefficient of Yc on D, by Xic the sparse coefficient of Yc on Di. Let D = [ D D0 ] be the total dictionary, X = [XT , (X0)T ]T and Xc = [(Xc)T , (X0c) T ]T . For every dictionary learning problem, we implicitly constrain each basis to have its Euclidean norm no greater than 1. These variables are visualized in Figure 2a).\nLet m,m0, and mc be the mean of X,X0, and Xc columns, respectively. Let Mc = [mc, . . . ,mc] \u2208 RK\u00d7nc ,M0 = [m0, . . . ,m0] \u2208 Rk0\u00d7N , and M = [m, . . . ,m] be the mean matrices. The number of columns of M depends on context, e.g. by writing Mc\u2212M, we mean that M and Mc have same number of columns nc .The \u2018mean vectors\u2019 are illustrated in Figure 2c).\nGiven a function f(A,B) with A and B being two sets of variables, define fA(B) = f(A,B) as a function of B when the set of variables A is fixed. Greek letters (\u03bb, \u03bb1, \u03bb2, \u03b7) represent positive regularization parameters. Given a block matrix A, define a function M(A) as follows:  \nA11 . . . A1C A21 . . . A2C . . . . . . . . . AC1 . . . ACC\n \n\ufe38 \ufe37\ufe37 \ufe38 A\n7\u2192 A +  \nA11 . . . 0 0 . . . 0 . . . . . . . . . 0 . . . ACC\n \n\ufe38 \ufe37\ufe37 \ufe38 M(A)\n. (1)\nThat is, M(A) doubles diagonal blocks of A. The row and column partitions of A are inferred from context. M(A) is a computationally inexpensive function of A and will be widely used in our LRSDL algorithm and the toolbox.\nWe also recall here the FISTA algorithm [31] for solving the family of problems:\nX = arg min X\nh(X) + \u03bb\u2016X\u20161, (2)\nwhere h(X) is convex, continuously differentiable with Lipschitz continuous gradient. FISTA is an iterative method which requires to calculate gradient of h(X) at each iteration. In this paper, we will focus on calculating the gradient of h."}, {"heading": "B. Closely related work: Fisher discrimination dictionary learning (FDDL)", "text": "FDDL [24] has been used broadly as a technique for exploiting both structured dictionary and learning discriminative coefficient. Specifically, the discriminative dictionary D\nand the sparse coefficient matrix X are learned based on minimizing the following cost function:\nJY(D,X) = 1\n2 fY(D,X) + \u03bb1\u2016X\u20161 + \u03bb2 2 g(X), (3)\nwhere fY(D,X) = C\u2211\nc=1\nrYc(D,Xc) is the discriminative\nfidelity with: rYc(D,Xc) = \u2016Yc\u2212DXc\u20162F+\u2016Yc\u2212DcXcc\u20162F+ \u2211\nj 6=c\n\u2016DjXjc\u20162F ,\ng(X) = \u2211C c=1(\u2016Xc \u2212Mc\u20162F \u2212 \u2016Mc \u2212M\u20162F ) + \u2016X\u20162F is the Fisher-based discriminative coefficient term, and the l1-norm encouraging the sparsity of coefficients."}, {"heading": "C. Proposed Low-rank shared dictionary learning (LRSDL)", "text": "In the presence of the shared dictionary, it is expected that Yc can be well represented by the collaboration of the particular dictionary Dc and the shared dictionary D0. Concretely, the discriminative fidelity term fY(D,X) in (3) can be extended to fY(D,X) = \u2211C c=1 rYc(D,Xc) with rYc(D,Xc) being defined as:\n\u2016Yc\u2212DXc\u20162F+\u2016Yc\u2212DcXcc\u2212D0X0c\u20162F+ C\u2211\nj=1,j 6=c\n\u2016DjXjc\u20162F .\nNote that since r\u0304Yc(D,Xc) = rYc(D,Xc) with Yc = Yc \u2212D0X0c (see Figure 2b)), we have:\nfY(D,X) = fY(D,X), (4)\nwith Y = Y \u2212D0X0.\nThe Fisher-based discriminative coefficient term g(X) is extended to g(X) defined as:\ng(X) = g(X) + \u2016X0 \u2212M0\u20162F , (5) where the term \u2016X0 \u2212M0\u20162F forces the coefficients of all training samples represented via the shared dictionary to be similar (see Figure 2c)).\nFor the shared dictionary, as stated in the Introduction, we constrain rank(D0) to be small by using the nuclear norm \u2016D0\u2016\u2217 which is its convex relaxation [34]. Finally, the cost function JY(D,X) of our proposed LRSDL is:\nJY(D,X) = 1\n2 fY(D,X) + \u03bb1\u2016X\u20161 + \u03bb2 2 g(X) + \u03b7\u2016D0\u2016\u2217.\n(6) By minimizing this objective function, we can jointly find the appropriate dictionaries as we desire. Notice that if there is no shared dictionary D0 (by setting k0 = 0), then D,X become D,X, respectively, JY(D,X) becomes JY(D,X) and our LRSDL reduces to FDDL.\nClassification scheme:\nAfter the learning process, we obtain the total dictionary D and mean vectors mc,m0. For a new test sample y, first we find its coefficient vector x = [xT , (x0)T ]T with the sparsity constraint on x and further encourage x0 to be close to m0:\nx = arg min x\n1 2 \u2016y\u2212Dx\u201622 + \u03bb2 2 \u2016x0 \u2212m0\u201622 + \u03bb1\u2016x\u20161. (7)\nUsing x as calculated above, we extract the contribution of the shared dictionary to obtain y = y \u2212D0x0. The identity of y is determined by:\narg min 1\u2264c\u2264C\n(w\u2016y \u2212Dcxc\u201622 + (1\u2212 w)\u2016x\u2212mc\u201622), (8)\nwhere w \u2208 [0, 1] is a preset weight for balancing the contribution of the two terms."}, {"heading": "D. Efficient solutions for optimization problems", "text": "Before diving into minimizing the LRSDL objective function in (6), we first present efficient algorithms for minimizing the FDDL objective function in (3).\n1) Efficient FDDL dictionary update: Recall that in [24], the dictionary update step is divided into C subproblems, each updates one class-specific dictionary Dc while others fixed. This process is repeated until convergence. This approach is not only highly time consuming but also inaccurate. We will see this in a small example presented in Section IV-B. We refer this original FDDL dictionary update as O-FDDL-D.\nWe propose here an efficient algorithm for updating dictionary called E-FDDL-D where the total dictionary D will be optimized when X is fixed, significantly reducing the computational cost.\nConcretely, when we fix X in equation (3), the problem of solving D becomes:\nD = arg min D fY,X(D) (9)\nTherefore, D can be solved by using the following lemma. Lemma 1: The optimization problem (9) is equivalent to:\nD = arg min D {\u22122trace(EDT ) + trace(FDTD)}, (10)\nwhere E = YM(XT ) and F =M(XXT ).\nProof: See Appendix A.\nThe problem (10) can be solved effectively by Online Dictionary Learning (ODL) method [32]."}, {"heading": "2) Efficient FDDL sparse coefficient update (E-FDDL-X):", "text": "When D is fixed, X will be found by solving:\nX = arg min X\nh(X) + \u03bb1\u2016X\u20161, (11)\nwhere h(X) = 12fY,D(X) + \u03bb2 2 g(X). The problem (11) has the form of equation (2), and can hence be solved by FISTA [31]. We need to calculate gradient of f(\u2022) and g(\u2022) with respect to X.\nLemma 2: Calculating gradient of h(X) in equation (2)\n\u2202 12fY,D(X)\n\u2202X = M(DTD)X\u2212M(DTY), (12) \u2202 12g(X)\n\u2202(X) = 2X + M\u2212 2\n[ M1 M2 . . .Mc ] \ufe38 \ufe37\ufe37 \ufe38\nM\u0302\n. (13)\nThen we obtain: \u2202h(X)\n\u2202X = (M(DTD) + 2\u03bb2I)X\u2212M(DTY) + \u03bb2(M\u2212 2M\u0302).\n(14)\nProof: See Appendix B.\nSince the proposed LRSDL is an extension of FDDL, we can also extend these two above algorithms to optimize LRSDL cost function as follows.\n3) LRSDL dictionary update (LRSDL-D): Returning to our proposed LRSDL problem, we need to find D = [D,D0] when X is fixed. We propose a method to solve D and D0 separately.\nFor updating D, recall the observation that fY(D,X) = fY(D,X), with Y , Y \u2212D0X0 (see equation (4)), and the E-FDDL-D presented in section II-D1, we have:\nD = arg min D {\u22122trace(EDT ) + trace(FDTD)}, (15)\nwith E = YM(XT ) and F =M(XXT ).\nFor updating D0, we use the following lemma:\nLemma 3: When D,X in (6) are fixed,\nJY,D,X(D0,X0) = \u2016V \u2212D0X0\u20162F + \u03bb2 2 \u2016X0 \u2212M0\u20162F + +\u03b7\u2016D0\u2016\u2217 + \u03bb1\u2016X0\u20161 + constant, (16) where V = Y \u2212 12DM(X).\nProof: See Appendix C.\nBased on the Lemma 3, D0 can be updated by solving:\nD0 = arg min D0 trace(FDT0 D0)\u2212 2trace(EDT0 ) + \u03b7\u2016D0\u2016\u2217 where: E = V(X0)T ; F = X0(X0)T (17)\nusing the ADMM [33] method and the singular value thresholding algorithm [35]. The ADMM procedure is as follows. First, we choose a positive \u03c1, initialize Z = U = D0, then alternatively solve each of the following subproblems until convergence:\nD0 = arg min D0 \u22122trace(EDT0 ) + trace\n( FDT0 D0 ) , (18)\nwith E = E + \u03c1 2 (Z\u2212U); F = F + \u03c1 2 I, (19) Z =D\u03b7/\u03c1(Dc + U), (20) U =U + D0 \u2212 Z, (21)\nwhere D is the shrinkage thresholding operator [35]. The optimization problem (18) can be solved by ODL [32]. Note that (19) and (21) are computationally inexpensive.\n4) LRSDL sparse coefficients update (LRSDL-X): In our preliminary work [30], we proposed a method for effectively solving X and X0 alternatively, now we combine both problems into one and find X by solving the following optimization problem:\nX = arg min X\nh(X) + \u03bb1\u2016X\u20161. (22)\nwhere h(X) = 1\n2 fY,D(X) + \u03bb2 2 g(X). We again solve this\nproblem using FISTA [31] with the gradient of h(X):\n\u2202h(X)\n\u2202X =\n  \u2202hX0(X)\n\u2202X \u2202hX(X 0)\n\u2202X0\n  . (23)\nFor the upper term, by combining the observation\nhX0(X) = 1\n2 fY,D,X0(X) + \u03bb2 2 gX0(X),\n= 1\n2 fY,D(X) + \u03bb2 2 g(X) + constant, (24)\nand using equation (14), we obtain:\n\u2202hX0(X)\n\u2202X = (M(DTD)+2\u03bb2I)X\u2212M(DTY)+\u03bb2(M\u22122M\u0302).\n(25)\nFor the lower term, by using Lemma 3, we have:\nhX(X 0) = \u2016V\u2212D0X0\u20162F+ \u03bb2 2 \u2016X0\u2212M0\u20162F+constant. (26)\n\u21d2 \u2202hX(X 0)\n\u2202X0 = 2DT0 D0X 0 \u2212 2DT0 V + \u03bb2(X0 \u2212M0), = (2DT0 D0 + \u03bb2I)X 0 \u2212 2DT0 V \u2212 \u03bb2M0. (27)\nBy combining these two terms, we can calculate (23)."}, {"heading": "E. Efficient solutions for other dictionary learning methods", "text": "We also propose here another efficient algorithm for updating dictionary in two other well-known dictionary learning methods: DLSI [27] and COPAR [28]. The cost function J1(D,X) in DLSI is defined as:\nC\u2211\nc=1\n( ||Yc\u2212DcXc\u20162F+\u03bb\u2016Xc\u20161+ \u03b7\n2\nC\u2211\nj=1,j 6=c\n\u2016DTj Dc\u20162F ) (28)\nEach class-specific dictionary Dc is updated by fixing others and solve:\nDc = arg min Dc \u2016Yc \u2212DcXc\u20162F + \u03b7\u2016ADc\u20162F , (29)\nwith A = [ D1, . . . ,Dc\u22121,Dc+1, . . . ,DC ]T .\nThe original solution for this problem, which will be referred as O-FDDL-D, updates each column dc,j of Dc one by one based on the procedure:\nu = (\u2016xjc\u201622I + \u03b7ATA)\u22121(Yc \u2212 \u2211\ni6=j\ndc,ix i c)x j c, (30)\ndc,j = u/\u2016u\u201622, (31)\nwhere dc,i is the i-th column of Dc and xjc is the j-th row of Xc. This algorithm is highly computational since it requires one matrix inversion for each of kc columns of Dc. We propose one ADMM [33] procedure to update Dc which requires only one matrix inversion, which will be referred as EDLSI-D. First, by letting E = Yc(Xc)T and F = Xc(Xc)T , we rewrite (29) in a more general form:\nDc = arg min Dc trace(FDTc Dc)\u2212 2trace(EDTc ) + \u03b7\u2016ADc\u20162F . (32)\nIn order to solve this problem, first, we choose a \u03c1, let Z = U = Dc, then alternatively solve each of the following sub problems until convergence:\nDc = arg min Dc \u22122trace(EDTc ) + trace\n( FDTc Dc ) , (33)\nwith E = E + \u03c1 2 (Z\u2212U); F = F + \u03c1 2 I. (34) Z =(2\u03b7ATA + \u03c1I)\u22121(Dc + U). (35) U =U + Dc \u2212 Z. (36)\nThis efficient algorithm requires only one matrix inversion. Later in this paper, we will both theoretically and experimentally show that E-DLSI-D is much more efficient than O-DLSI-D [27]. Note that this algorithm can be beneficial for two subproblems of updating the common dictionary and the particular dictionary in COPAR [28] as well."}, {"heading": "III. COMPLEXITY ANALYSIS", "text": "We compare the computational complexity for the efficient algorithms and their corresponding original algorithms. We also evaluate the total complexity of the proposed LRSDL and competing dictionary learning methods: DLSI [27], COPAR [28] and FDDL [24]. The complexity for each algorithm is estimated as the (approximate) number of multiplications required for one iteration (sparse code update and dictionary update). For simplicity, we assume: i) number of training samples, number of dictionary bases in each class (and the shared class) are the same, which means: nc = n, ki = k. ii) The number of bases in each dictionary is comparable to number of training samples per class and much less than the signal dimension, i.e. k \u2248 n d. iii) Each iterative algorithm requires q iterations to convergence. For consistency, we have changed notations in those methods by denoting Y as training sample and X as the sparse code.\nIn the following analysis, we use the fact that: i) if A \u2208 Rm\u00d7n,B \u2208 Rn\u00d7p, then the matrix multiplication AB has complexity mnp. ii) If A \u2208 Rn\u00d7n is nonsingular, then the matrix inversion A\u22121 has complexity n3. iii) The singular value decomposition of a matrix A \u2208 Rp\u00d7q , p > q, is assumed to have complexity O(pq2)."}, {"heading": "A. Online Dictionary Learning (ODL)", "text": "We start with the well-known Online Dictionary Learning [32] whose cost function is:\nJ(D,X) = 1\n2 \u2016Y \u2212DX\u20162F + \u03bb\u2016X\u20161. (37)\nwhere Y \u2208 Rd\u00d7n,D \u2208 Rd\u00d7k,X \u2208 Rk\u00d7n. Most of dictionary learning methods find their solutions by alternatively solving one variable while fixing others. There are two subproblems:\n1) Update X (ODL-X): When the dictionary D is fixed, the sparse coefficient X is updated by solving the problem:\nX = arg min X\n1 2 \u2016Y \u2212DX\u20162F + \u03bb\u2016X\u20161 (38)\nusing FISTA [31]. In each of q iterations, the most computational task is to compute DTDX \u2212 DTY where DTD and DTY are precomputed with complexities k2d and kdn, respectively. The matrix multiplication (DTD)X has complexity k2n. Then, the total complexity of ODL-X is:\nk2d+ kdn+ qk2n = k(kd+ dn+ qkn). (39)\n2) Update D (ODL-D): After finding X, the dictionary D will be updated by:\nD = arg min D \u22122trace(EDT ) + trace(FDTD), (40)\nsubject to: \u2016di\u20162 \u2264 1, with E = YXT , and F = XXT . Each column of D will be updated by fixing all others:\nu\u2190 1 Fii (ei \u2212Dfi)\u2212 di; di \u2190 u max(1, \u2016u\u20162) ,\nwhere di, ei, fi are the i\u2212th columns of D,E,F and Fii is the i\u2212th element in the diagonal of F. The dominant computational task is to compute Dfi which requires dk operators. Since D has k columns and the algorithm requires q iterations, the complexity of ODL-D is qdk2."}, {"heading": "B. Dictionary learning with structured incoherence (DLSI)", "text": "DLSI [27] proposed a method to encourage the independence between bases of different classes by minimizing coherence between cross-class bases. The cost function J1(D,X) of DLSI is defined as (28).\n1) Update X (DLSI-X): In each iteration, the algorithm solves C subproblems:\nXc = arg min Xc \u2016Yc \u2212DcXc\u20162F + \u03bb\u2016Xc\u20161. (41)\nwith Yc \u2208 Rd\u00d7n,Dc \u2208 Rd\u00d7k, and Xc \u2208 Rk\u00d7n. Based on (39), the complexity of updating X (C subproblems) is:\nCk(kd+ dn+ qkn). (42)\n2) Original update D (O-DLSI-D): For updating D, each sub-dictionary Dc is solved via (29). The main step in the algorithm is stated in (30) and (31). The dominant computational part is the matrix inversion which has complexity d3. Matrix-vector multiplication and vector normalization can be ignored here. Since Dc has k columns, and the algorithm requires q iterations, the complexity of the O-DLSI-D algorithm is Cqkd3.\n3) Efficient update D (E-DLSI-D): Main steps of the proposed algorithm are presented in equations (33)\u2013(36) where (34) and (36) require much less computation compared to (33) and (35). The total (estimated) complexity of efficient Dc update is a summation of two terms: i) q times (q iterations) of ODL-D in (33). ii) One matrix inversion (d3) and q matrix multiplications in (35). Finally, the complexity of E-DLSI-D is:\nC(q2dk2 + d3 + qd2k) = Cd3 + Cqdk(qk + d). (43)\nTotal complexities of O-DLSI (the combination of DLSI-X and O-DLSI-D) and E-DLSI (the combination of DLSI-X and E-DLSI-D) are summarized in Table II."}, {"heading": "C. Separating the particularity and the commonality dictionary learning (COPAR)", "text": "1) Cost function: COPAR [28] is another dictionary learning method which also considers the shared dictionary (but without the low-rank constraint). By using the same notation as in LRSDL, we can rewrite the cost function of COPAR in the following form:\n1 2 f1(Y,D,X) + \u03bb \u2225\u2225X \u2225\u2225 1 + \u03b7 C\u2211\nc=0\nC\u2211\ni=0,i6=c\n\u2016DTi Dc\u20162F ,\nwhere f1(Y,D,X) = C\u2211\nc=1\nr1(Yc,D,Xc) and r1(Yc,D,Xc)\nis defined as:\n\u2016Yc \u2212DXc\u20162F + \u2016Yc \u2212D0X0c \u2212DcXcc\u2016+ C\u2211\nj=1,j 6=c\n\u2016Xjc\u20162F .\n2) Update X (COPAR-X): In sparse coefficient update step, COPAR [28] solve Xc one by one via one l1-norm regularization problem:\nX\u0303 = arg min X\u0303 \u2016Y\u0303 \u2212 D\u0303X\u0303\u20162F + \u03bb\u0303\u2016X\u0303\u20161,\nwhere Y\u0303 \u2208 Rd\u0303\u00d7n, D\u0303 \u2208 Rd\u0303\u00d7k\u0303, X\u0303 \u2208 R(k\u0303\u00d7n, d\u0303 = 2d+(C\u22121)k and k\u0303 = (C + 1)k (details can be found in Section 3.1 of [28]). Following results in Section III-A1 and supposing that C 1, q 1, n \u2248 k d, the complexity of COPAR-X is:\nCk\u0303(k\u0303d\u0303+ d\u0303n+ qk\u0303n) \u2248 C3k2(2d+ Ck + qn). 3) Update D (COPAR-D): The COPAR dictionary update algorithm requires to solve (C + 1) problems of form (32). While O-COPAR-D uses the same method as O-DLSI-D (see equations (30-31)), the proposed E-COPAR-D takes advantages of E-DLSI-D presented in Section II-E. Therefore, the total complexity of O-COPAR-D is roughly Cqkd3, while the total complexity of E-COPAR-D is roughly C(q2dk2 + d3 + qd2k). Here we have supposed C + 1 \u2248 C for large C."}, {"heading": "D. Fisher discrimination dictionary learning (FDDL)", "text": "1) Original update X (O-FDDL-X): Based on results reported in DFDL [7], the complexity of O-FDDL-X is roughly C2kn(d+ qCk) + C3dk2 = C2k(dn+ qCkn+ Cdk).\n2) Efficient update X (E-FDDL-X): Based on section II-D2, the complexity of E-FDDL-X mainly comes from equation (14). Recall that function M(\u2022) does not require much computation. The computation of M and Mc can also be neglected since each required calculation of one column, all other columns are the same. Then the total complexity of the algorithm E-FDDL-X is roughly:\n(Ck)d(Ck)\ufe38 \ufe37\ufe37 \ufe38 M(DTD+\u03bb2I) + (Ck)d(Cn)\ufe38 \ufe37\ufe37 \ufe38 M(DTY) +q (Ck)(Ck)(Cn)\ufe38 \ufe37\ufe37 \ufe38 M(DTD+\u03bb2I)X , = C2k(dk + dn+ qCnk). (44)\n3) Original update D (O-FDDL-D): The original dictionary update in FDDL is divided in to C subproblems. In each subproblem, one dictionary Dc will be solved while all others are fixed via:"}, {"heading": "Dc = argmin", "text": "Dc \u2016Y\u0302 \u2212DcXc\u20162F + \u2016Yc \u2212DcXcc\u20162F + \u2211 i 6=c \u2016DcXci\u20162F ,\n= argmin Dc \u22122trace(EDTc ) + trace(FDTc Dc)\ufe38 \ufe37\ufe37 \ufe38\ncomplexity: qdk2\n, (45)\nwhere:\nY\u0302 = Y \u2212 \u2211\ni 6=c\nDiX i complexity: (C \u2212 1)dkCn,\nE = Y\u0302(Xc)T + Yc(X c c) T complexity: d(Cn)k + dnk, F = 2(Xc)(Xc)T complexity k(Cn)k.\nWhen d k,C 1, complexity of updating Dc is: qdk2 + (C2 + 1)dkn+ Ck2n \u2248 qdk2 + C2dkn (46)\nThen, complexity of O-FDDL-D is Cdk(qk + C2n).\n4) Efficient update D (E-FDDL-D): Based on Lemma 1, the complexity of E-FDDL-D is:\nd(Cn)(Ck)\ufe38 \ufe37\ufe37 \ufe38 YM(X)T + (Ck)(Cn)(Ck)\ufe38 \ufe37\ufe37 \ufe38 M(XXT ) + qd(Ck)2\ufe38 \ufe37\ufe37 \ufe38 ODL in (10) ,\n= Cdk(Cn+ Cqk) + C3k2n. (47)\nTotal complexities of O-FDDL and E-FDDL are summarized in Table II."}, {"heading": "E. LRSDL", "text": "1) Update X,X0: From (23), (25) and (27), in each iteration of updating X, we need to compute:\n(M(DTD) + 2\u03bb2I)X\u2212M(DTY) + +\u03bb2(M\u2212 2M\u0302)\u2212M(DTD0X0), and (2DT0 D0 + \u03bb2I)X 0 \u2212 2DT0 Y + DT0 DM(X)\u2212 \u03bb2M0.\nTherefore, the complexity of LRSDL-X is:\n(Ck)d(Ck)\ufe38 \ufe37\ufe37 \ufe38 DTD + (Ck)d(Cn)\ufe38 \ufe37\ufe37 \ufe38 DTY + (Ck)dk\ufe38 \ufe37\ufe37 \ufe38 DTD0 + kdk\ufe38\ufe37\ufe37\ufe38 DT0 D0 + kd(Cn)\ufe38 \ufe37\ufe37 \ufe38 DT0 Y +\n+q   (Ck)2(Cn)\ufe38 \ufe37\ufe37 \ufe38 (M(DTD)+2\u03bb2I)X + (Ck)k(Cn)\ufe38 \ufe37\ufe37 \ufe38 M(DTD0X0) +\n+ k2Cn\ufe38 \ufe37\ufe37 \ufe38 (2DT0 D0+\u03bb2I)X 0 + k(Ck)(Cn)\ufe38 \ufe37\ufe37 \ufe38 DT0 DM(X)\n  ,\n\u2248 C2k(dk + dn) + Cdk2 + qCk2n(C2 + 2C + 1), \u2248 C2k(dk + dn+ qCkn). (48)\nwhich is similar to the complexity of E-FDDL-X. Recall that we have supposed number of classes C 1.\n2) Update D: Compare to E-FDDL-D, LRSDL-D requires one more computation of Y = Y\u2212D0X0 (see section II-D3). Then, the complexity of LRSDL-D is:\nCdk(Cn+ Cqk) + C3k2n\ufe38 \ufe37\ufe37 \ufe38 E-FDDL-D + dk(Cn)\ufe38 \ufe37\ufe37 \ufe38 D0X0 ,\n\u2248 Cdk(Cn+ Cqk) + C3k2n, (49) which is similar to the complexity of E-FDDL-D.\n3) Update D0: The algorithm of LRSDL-D0 is presented in section II-D3 with the main computation comes from (17),\n(18) and (20). The shrinkage thresholding operator in (20) requires one SVD and two matrix multiplications. The total complexity of LRSDL-D0 is:\nd(Ck)(Cn)\ufe38 \ufe37\ufe37 \ufe38 V=Y\u2212 12DM(X) + d(Cn)k\ufe38 \ufe37\ufe37 \ufe38 E in (17) + k(Cn)k\ufe38 \ufe37\ufe37 \ufe38 F in (17) + qdk2\ufe38\ufe37\ufe37\ufe38 (18) +O(dk2) + 2dk2\ufe38 \ufe37\ufe37 \ufe38 (20) , \u2248 C2dkn+ qdk2 +O(dk2), = C2dkn+ (q + q2)dk 2, for some q2. (50)\nBy combing (48), (49) and (50), we obtain the total complexity of LRSDL, which is specified in the last row of Table II."}, {"heading": "F. Summary", "text": "Table I and Table II show final complexity analysis of each proposed efficient algorithm and their original counterparts. Table II compares LRSDL to other state-of-the-art methods. We pick a typical set of parameters with 100 classes, 20 training samples per class, 10 bases per sub-dictionary and shared dictionary, data dimension 500 and 50 iterations for each iterative method. Concretely, C = 100, n = 20, k = 10, q = 50, d = 500. We also assume that in (50), q2 = 50. Table I shows that all three proposed efficient algorithms require less computation than original versions with most significant improvements for speeding up DLSI-D. Table II demonstrates an interesting fact. LRSDL is the least expensive computationally when compared with other original dictionary learning algorithms, and only E-FDDL has lower complexity, which is to be expected since the FDDL cost function is a special case of the LRSDL cost function. COPAR is found to be the most expensive computationally."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "A. Comparing methods and datasets", "text": "We present the experimental results of applying these methods to five diverse datasets: the Extended YaleB face dataset [36], the AR face dataset [37], the AR gender dataset, the Oxford Flower dataset [38], and one multi-class object category dataset \u2013 the Caltech 101 [39]. Example images from these datasets are shown in Figure 3. We compare our results with those using SRC [5] and other state-of-the-art dictionary learning methods: LC-KSVD [23], DLSI [27], FDDL [25], COPAR [28], and D2L2R2 [26]. Regularization parameters in all methods are chosen using cross-validation [40].\nFor two face datasets, feature descriptors are random faces, which are made by projecting a face image onto a random vector using a random projection matrix. As in [21], the dimension of a random-face feature in the Extended YaleB is d = 504, while the dimension in AR face is d = 540. Samples of these two datasets are shown in Figure 3a) and b).\nFor the AR gender dataset, we first choose a non-occluded subset (14 images per person) from the AR face dataset, which consists of 50 males and 50 females, to conduct experiment of gender classification. Training images are taken from the first 25 males and 25 females, while test images comprises all samples from the remaining 25 males and 25 females. PCA was used to reduce the dimension of each image to 300. Samples of this dataset are shown in Figure 3c).\nThe Oxford Flower dataset is a collection of images of flowers drawn from 17 species with 80 images per class, totaling 1360 images. For feature extraction, based on the impressive results presented in [25], we choose the Frequent Local Histogram feature extractor [41] to obtain feature vectors of dimension 10,000. The test set consists of 20 images per class, the remaining 60 images per class are used for training. Samples of this dataset are shown in Figure 3d).\nFor the Caltech 101 dataset, we use a dense SIFT (DSIFT)\ndescriptor. The DSIFT descriptor is extracted from 25 \u00d7 25 patch which is densely sampled on a dense grid with 8 pixels. We then extract the sparse coding spatial pyramid matching (ScSPM) feature [42], which is the concatenation of vectors pooled from words of the extracted DSIFT descriptor. Dimension of words is 1024 and max pooling technique is used with pooling grid of 1\u00d71, 2\u00d72, and 4\u00d74. With this setup, the dimension of ScSPM feature is 21,504; this is followed by dimension reduction to d = 3000 using PCA. Samples of this dataset are shown in Figure 3e).\nB. Validation of efficient algorithms\nTo evaluate the improvement of three efficient algorithms proposed in section II, we apply these efficient algorithms and their original versions on training samples from the AR face dataset to verify the convergence speed of those algorithms. In this example, number of classes C = 100, the randomface feature dimension d = 300, number of training samples per class nc = n = 7, number of atoms in each particular dictionary kc = 7.\n1) E-FDDL-D and E-FDDL-X: Figure 4 shows the cost functions and running time after each of 100 iterations of 4 different versions of FDDL: the original FDDL (O-FDDL), combination of O-FDDL-X and E-FDDL-D, combination of E-FDDL-X and O-FDDL-D, and the efficient FDDL (EFDDL). The first observation is that O-FDDL converges quickly to a suboptimal solution, which is far from the best cost obtained by E-FDDL. In addition, while O-FDDL requires more than 12,000 seconds (around 3 hours and 20 minutes) to run 100 iterations, it takes E-FDDL only half an hour to do\nthe same task.\n2) E-DLSI-D and E-COPAR-D: Figure 5 and 6 compare convergence rates of DLSI and COPAR algorithms. As we can see, while the cost function value improves slightly, the run time of efficient algorithms reduces significantly. Based on benefits in both cost function value and computation, in the rest of this paper, we use efficient optimization algorithms instead of original versions for obtaining classification results.\nC. Visualization of learned shared bases\nTo demonstrate the behavior of dictionary learning methods on a dataset in the presence of shared features, we create a toy example in Figure 7. This is a classification problem with 4 classes whose basic class-specific elements and shared elements are visualized in Figure 7a). Each basis element has dimension 20 pixel \u00d720 pixel. From these elements, we generate 1000 samples per class by linearly combining class-specific elements and shared elements followed by noise added; 200 samples per class are used for training, 800 remaining images are used for testing. Samples of each class are shown in Figure 7b).\nFigure 7c) show sample learned bases using DLSI [27] where shared features are still hidden in class-specific bases. In LCKSVD bases (Figure 7e) and f)), shared features (the squared in the middle of a patch) are found but they are classified as bases of class 1 or class 2, diminishing classification accuracy since most of test samples are classified as class 1 or 2. The same phenomenon happens in FDDL bases (Figure 7g)).\nThe best classification results happen in two shared dictionary learnings (COPAR in Figure 7d) and the proposed LRSDL in Figure 7h)) where the shared bases are extracted and gathered in the shared dictionary. However, in COPAR, shared features still appear in class-specific dictionaries and the shared dictionary also includes features of class 1 and 2. In LRSDL, class-specific elements and shared elements are nearly perfectly decomposed into appropriate sub dictionaries. The reason behind this phenomenon is the low-rank constraint on the shared dictionary of LRSDL. Thanks to this constraint, LRSDL produces perfect results on this simulated data."}, {"heading": "D. Effect of the shared dictionary sizes on overall accuracy", "text": "We perform an experiment to study the effect of the shared dictionary size on the overall classification results of two dictionary methods: COPAR [28] and LRSDL in the AR gender dataset. In this experiment, 40 images of each class are used for training. The number of shared dictionary bases varies from 10 to 80. In LRSDL, because there is a regularization parameter \u03b7 which is attached to the low-rank term (see equation (6)), we further consider three values of \u03b7: \u03b7 = 0, i.e. no low-rank constraint, \u03b7 = 0.01 and \u03b7 = 0.1 for two different degrees of emphasis. Results are shown in Figure 8.\nWe observe that the performance of COPAR heavily depends on the choice of k0 and its results worsen as the size of the shared dictionary increases. The reason is that when k0 is large, COPAR tends to absorb class-specific features into the shared dictionary. This trend is not associated with LRSDL\neven when the low-rank constraint is ignored (\u03b7 = 0), because LRSDL has another constraint (\u2016X0 \u2212M0\u20162F small) which forces the coefficients corresponding to the shared dictionary to be similar. Additionally, when we increase \u03b7, the overall classification of LRSDL also gets better. These observations confirm that our two proposed constraints on the shared dictionary are important, and the LRSDL exhibits robustness to parameter choices."}, {"heading": "E. Overall Classification Accuracy", "text": "Table III shows overall classification results of various methods on all presented datasets. It is evident that in most cases, two dictionary learning methods with shared features (COPAR [28] and our proposed LRSDL) outperform others with all five highest values presenting in our proposed LRSDL."}, {"heading": "F. Performance vs. size of training set", "text": "Real-world classification tasks often have to contend with lack of availability of large training sets. To understand training dependence of the various techniques, we present a comparison of overall classification accuracy as a function of the training set size of the different methods. In Figure 9, overall classification accuracies are reported for all aforementioned datasets corresponding to various scenarios. It is readily apparent that LRSDL exhibits the most graceful decline as training is reduced. In addition, LRSDL also shows high performance even with low training on AR face and AR gender datasets."}, {"heading": "V. DISCUSSION AND CONCLUSION", "text": "In this paper, our primary contribution is the development of a discriminative dictionary learning framework via the introduction of a shared dictionary with two crucial constraints. First, the shared dictionary is constrained to be low-rank. Second, the sparse coefficients corresponding to the shared dictionary obey a similarity constraint. In conjunction with discriminative model as proposed in [24], [25], this leads to a more flexible model where shared features are excluded before doing classification. An important benefit of this model is the robustness of the framework to size (k0) and the regularization parameter (\u03b7) of the shared dictionary. In comparison with state-of-the-art algorithms developed specifically for these tasks, our LRSDL approach offers better classification performance on average.\nIn Section II-D and II-E, we discuss the efficient algorithms for FDDL [25], DLSI [27], then flexibly apply them into more sophisticated models. Thereafter in Section III and IV-B, we both theoretically and practically show that the proposed algorithms indeed significantly improve cost functions and run time speeds of different dictionary learning algorithms. The complexity analysis also shows that the proposed LRSDL requires less computation than competing models.\nAs proposed, the LRSDL model learns a dictionary shared by every class. In some practical problems, a feature may belong to more than one but not all classes. Very recently, researchers have begun to address this issue [43], [44]. In future work, we will investigate the design of hierarchical models for extracting common features among classes.\nAPPENDIX"}, {"heading": "A. Proof of Lemma 1", "text": "Let wc \u2208 {0, 1}K is a binary vector whose j-th element is one if and only if the j-th columns of D belong to Dc, and Wc = diag(wc). We observe that DcXci = DWcXi. We can rewrite fY,X(D) as:\n\u2016Y \u2212DX\u20162F + C\u2211\nc=1\n( \u2016Yc \u2212DcXcc\u20162F + \u2211 j 6=c \u2016DjXjc\u20162F )\n= \u2016Y \u2212DX\u20162F + C\u2211\nc=1\n( \u2016Yc \u2212DWcXc\u20162F + \u2211 j 6=c \u2016DWjXc\u20162F ) ,\n= trace (( XXT +\nC\u2211 c=1 C\u2211 j=1 WjXcX T c W T j ) DTD\n) ,\n\u22122trace (( YXT +\nC\u2211 c=1 YcX T c Wc\n) DT ) + constant,\n= \u22122trace(EDT ) + trace(FDTD) + constant.\nwhere we have defined:\nE = YXT + C\u2211 c=1 YcX T c Wc,\n= YXT + [ Y1(X 1 1) T . . . YC(X C C) T ] ,\n= Y XT + (X 1 1)\nT . . . 0 0 . . . 0 . . . . . . . . . 0 . . . (XCC) T\n  = YM(X)T ,\nF = XXT + C\u2211 c=1 C\u2211 j=1 WjXcX T c W T j ,\n= XXT + C\u2211 j=1 Wj\n( C\u2211\nc=1\nXcX T c ) WTj ,\n= XXT + C\u2211\nj=1\nWjXX TWTj .\nLet:\nXXT = A =  A11 . . . A1j . . . A1C . . . . . . . . . . . . . . . A21 . . . Ajj . . . A2C . . . . . . . . . . . . . . . AC1 . . . ACj . . . ACC  . From definition of Wj , we observe that \u2018left-multiplying\u2019 a matrix by Wj forces that matrix to be zero everywhere except the j-th block\nrow. Similarly, \u2018right-multiplying\u2019 a matrix by WTj = Wj will keep its j-th block column only. Combining these two observations, we can obtain the result:\nWjAW T j =  0 . . . 0 . . . 0 . . . . . . . . . . . . . . . 0 . . . Ajj . . . 0 . . . . . . . . . . . . . . . 0 . . . 0 . . . 0  . Then:\nF = XXT + C\u2211 j=1 WjXX TWTj ,\n= A+ A11 . . . 00 . . . 0. . . . . . . . . 0 . . . ACC  =M(A) =M(XXT ). Lemma 1 has been proved."}, {"heading": "B. Proof of Lemma 2", "text": "We need to prove two parts: For the gradient of f , first we rewrite:\nf(Y,D,X) = C\u2211 c=1\nr(Yc,D,Xc) =\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225  Y1 Y2 . . . YC Y1 0 . . . 0 0 Y2 . . . 0 . . . . . . . . . . . . 0 0 . . . YC  \ufe38 \ufe37\ufe37 \ufe38\nY\u0302\n\u2212  D1 D2 . . . DC D1 0 . . . 0 0 D2 . . . 0 . . . . . . . . . . . . 0 0 . . . DC  \ufe38 \ufe37\ufe37 \ufe38\nD\u0302\nX \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n= \u2016Y\u0302 \u2212 D\u0302X\u20162F .\nThen we obtain:\n\u2202 1 2 fY,D(X)\n\u2202X = D\u0302T D\u0302\u2212 D\u0302T Y\u0302 =M(DTD)X\u2212M(DTY).\nFor the gradient of g, let Eqp be the all-one matrix in Rp\u00d7q . It is easy to verify that:\n(Eqp) T = Epq , Mc = mcE nc 1 =\n1\nnc XcE\nnc nc ,\nEqpE r q = qE r p, (I\u2212\n1 p Epp)(I\u2212 1 p Epp) T = (I\u2212 1 p Epp).\nWe have:\nXc \u2212Mc = Xc \u2212 1\nnc XcE\nnc nc = Xc(I\u2212\n1\nnc Encnc),\n\u21d2 \u2202 \u2202Xc 1 2 \u2016Xc \u2212Mc\u20162F = Xc(I\u2212 1 nc Encnc)(I\u2212 1 nc Encnc) T ,\n= Xc(I\u2212 1\nnc Encnc) = Xc \u2212Mc.\nTherefore we obtain:\n\u2202 1 2 \u2211C c=1 \u2016Xc \u2212Mc\u2016 2 F\n\u2202X = [X1, . . . ,XC ]\u2212 [M1, . . . ,MC ]\ufe38 \ufe37\ufe37 \ufe38\nM\u0302\n= X\u2212 M\u0302. (51)\nFor Mc \u2212M, first we write it in two ways:\nMc \u2212M = 1\nnc XcE\nnc nc \u2212\n1 N XEncN = 1 nc XcE nc nc \u2212 1 N C\u2211 j=1 XjE nc nj ,\n= N \u2212 nc Nnc XcE nc nc \u2212 1 N \u2211 j 6=c XjE nc nj , (52) = 1\nnc XcE\nnc nc \u2212\n1 N XlE nc nl \u2212 1 N \u2211 j 6=l XjE nc nj (l 6= c). (53)\nThen we infer:\n(52)\u21d2 \u2202 \u2202Xc 1 2 \u2016Mc \u2212M\u20162F =\n( 1\nnc \u2212 1 N\n) (Mc \u2212M)Encnc ,\n= (Mc \u2212M) + 1\nN (M\u2212Mc)Encnc .\n(53)\u21d2 \u2202 \u2202Xl 1 2 \u2016Mc \u2212M\u20162F = 1 N (M\u2212Mc)Enlnc(l 6= c).\n\u21d2 \u2202 \u2202Xl 1 2 C\u2211 c=1 \u2016Mc \u2212M\u20162F = Ml \u2212M+ 1 N C\u2211 c=1 (M\u2212Mc)Enlnc .\nNow we prove that C\u2211\nc=1\n(M\u2212Mc)Enlnc = 0. Indeed,\nC\u2211 c=1 (M\u2212Mc)Enlnc = C\u2211 c=1 (mEnc1 \u2212mcE nc 1 )E nl nc ,\n= C\u2211 c=1 (m\u2212mc)Enc1 E nl nc = C\u2211 c=1 nc(m\u2212mc)Enl,1\n= ( C\u2211\nc=1\nncm\u2212 C\u2211\nc=1\nncmc ) E nl 1 = 0 ( since m = \u2211C c=1 ncmc\u2211C\nc=1 nc\n) .\nThen we have:\n\u2202\n\u2202X\n1\n2 C\u2211 c=1 \u2016Mc \u2212M\u20162F = [M1, . . . ,MC ]\u2212M = M\u0302\u2212M. (54)\nCombining (51), (54) and \u2202 1 2 \u2016X\u20162F \u2202X = X , we have:\n\u2202 1 2 g(X)\n\u2202X = 2X+M\u2212 2M\u0302.\nLemma 2 has been proved."}, {"heading": "C. Proof of Lemma 3", "text": "When Y,D,X are fixed, we have:\nJY,D,X(D0,X 0) =\n1 2 \u2016Y \u2212D0X0 \u2212DX\u20162F + \u03b7\u2016D0\u2016\u2217+\nC\u2211 c=1 1 2 \u2016Yc \u2212D0X0c \u2212DcXcc\u20162F + \u03bb1\u2016X0\u20161 + constant. (55)\nLet Y\u0303 = Y \u2212 DX, Y\u0302c = Yc \u2212 DcXcc and Y\u0302 =[ Y\u03021 Y\u03022 . . . Y\u0302C ] , we can rewrite (55) as:\nJY,D,X(D0,X 0) =\n1 2 \u2016Y\u0303 \u2212D0X0\u20162F + 1 2 \u2016Y\u0302 \u2212D0X0\u20162F +\n+\u03bb1\u2016X0\u20161 + \u03b7\u2016D0\u2016\u2217 + constant1,\n= \u2225\u2225\u2225\u2225\u2225Y\u0303 + Y\u03022 \u2212D0X0 \u2225\u2225\u2225\u2225\u2225 2\nF\n+ \u03bb1\u2016X0\u20161 + \u03b7\u2016D0\u2016\u2217 + constant2.\nWe observe that: Y\u0303 + Y\u0302 = 2Y \u2212DX\u2212 [ D1X 1 1 . . . DCX C C ] = 2Y \u2212DM(X).\nNow, by letting V = Y\u0303 + Y\u0302\n2 , Lemma 3 has been proved."}], "references": [{"title": "Compressed sensing,", "author": ["D.L. Donoho"], "venue": "IEEE Transactions on information theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Spratling, \u201cImage segmentation using a sparse coding model of cortical area v1,", "author": ["W. M"], "venue": "IEEE Trans. on Image Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Robust face recognition via sparse representation,", "author": ["J. Wright", "A. Yang", "A. Ganesh", "S. Sastry", "Y. Ma"], "venue": "IEEE Trans. on Pattern Analysis and Machine Int.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "DFDL: Discriminative feature-oriented dictionary learning for histopathological image classification,", "author": ["T.H. Vu", "H.S. Mousavi", "V. Monga", "U. Rao", "G. Rao"], "venue": "Proc. IEEE International Symposium on Biomedical Imaging,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Histopathological image classification using discriminative featureoriented dictionary learning,", "author": ["T.H. Vu", "H.S. Mousavi", "V. Monga", "U. Rao", "G. Rao"], "venue": "IEEE Transactions on Medical Imaging,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Task-driven dictionary learning for hyperspectral image classification with structured sparsity constraints,", "author": ["X. Sun", "N.M. Nasrabadi", "T.D. Tran"], "venue": "IEEE Transactions on Geoscience and Remote Sensing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Structured priors for sparse-representation-based hyperspectral image classification,", "author": ["X. Sun", "Q. Qu", "N.M. Nasrabadi", "T.D. Tran"], "venue": "Geoscience and Remote Sensing Letters, IEEE,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Hyperspectral image classification via kernel sparse representation,", "author": ["Y. Chen", "N.M. Nasrabadi", "T.D. Tran"], "venue": "IEEE Transactions on Geoscience and Remote Sensing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Multi-view automatic target recognition using joint sparse representation,", "author": ["H. Zhang", "N.M. Nasrabadi", "Y. Zhang", "T.S. Huang"], "venue": "IEEE Trans. on Aerospace and Electronic Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "An image recapture detection algorithm based on learning dictionaries of edge profiles,", "author": ["T. Thongkamwitoon", "H. Muammar", "P.-L. Dragotti"], "venue": "IEEE Transactions on Information Forensics and Security,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Adaptive sparse representations for video anomaly detection,", "author": ["X. Mo", "V. Monga", "R. Bala", "Z. Fan"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Multi-task image classification via collaborative, hierarchical spikeand-slab priors,", "author": ["H.S. Mousavi", "U. Srinivas", "V. Monga", "Y. Suo", "M. Dao", "T. Tran"], "venue": "in Proc. IEEE Conf. on Image Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Structured sparse priors for image classification,", "author": ["U. Srinivas", "Y. Suo", "M. Dao", "V. Monga", "T.D. Tran"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Jointstructured-sparsity-based classification for multiple-measurement transient acoustic signals,", "author": ["H. Zhang", "Y. Zhang", "N.M. Nasrabadi", "T.S. Huang"], "venue": "Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Structured sparse representation with low-rank interference,", "author": ["M. Dao", "Y. Suo", "S.P. Chin", "T.D. Tran"], "venue": "in 2014 48th Asilomar Conf. on Signals, Systems and Computers", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Discriminative K-SVD for dictionary learning in face recognition,", "author": ["Q. Zhang", "B. Li"], "venue": "in Proc. IEEE Conf. Computer Vision Pattern Recognition", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Learning a discriminative dictionary for sparse coding via label consistent K-SVD,", "author": ["Z. Jiang", "Z. Lin", "L.S. Davis"], "venue": "in Proc. IEEE Conf. Computer Vision Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Fisher discrimination dictionary learning for sparse representation,", "author": ["M. Yang", "L. Zhang", "X. Feng", "D. Zhang"], "venue": "in Proc. IEEE International Conference on Computer Vision,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Sparse representation based fisher discrimination dictionary learning for image classification,", "author": ["M. Yang", "L. Zhang", "X. Feng", "D. Zhang"], "venue": "Int. Journal of Computer Vision, vol. 109,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Classification and clustering via dictionary learning with structured incoherence and shared features,", "author": ["I. Ramirez", "P. Sprechmann", "G. Sapiro"], "venue": "IEEE Conf. on Comp. Vision and Pattern Recog. IEEE,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "A dictionary learning approach for classification: separating the particularity and the commonality,", "author": ["S. Kong", "D. Wang"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Learning category-specific dictionary and shared dictionary for fine-grained image categorization,", "author": ["S. Gao", "I.W.-H. Tsang", "Y. Ma"], "venue": "IEEE Trans. on Image Processing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Learning a low-rank shared dictionary for object classification,", "author": ["T.H. Vu", "V. Monga"], "venue": "IEEE International Conference on Image Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems,", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Online learning for matrix factorization and sparse coding,", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers,", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization,", "author": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": "SIAM review,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "A singular value thresholding algorithm for matrix completion,", "author": ["J.-F. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose,", "author": ["A. Georghiades", "P. Belhumeur", "D. Kriegman"], "venue": "IEEE Trans. on Pattern Analysis and Machine Int.,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2001}, {"title": "The AR face database,", "author": ["A. Martinez", "R. Benavente"], "venue": "CVC Technical Report,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1998}, {"title": "A visual vocabulary for flower classification,", "author": ["M.-E. Nilsback", "A. Zisserman"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories,", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection.", "author": ["R. Kohavi"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1995}, {"title": "Effective use of frequent itemset mining for image classification,", "author": ["B. Fernando", "E. Fromont", "T. Tuytelaars"], "venue": "Vision\u2013ECCV", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "Linear spatial pyramid matching using sparse coding for image classification,", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "IEEE Conf. on Comp. Vision and Pattern Recog. IEEE,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "Latent dictionary learning for sparse representation based classification,", "author": ["M. Yang", "D. Dai", "L. Shen", "L. Gool"], "venue": "in Proc. IEEE Conf. Computer Vision Pattern Recognition,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "A hierarchical-structured dictionary learning for image classification,", "author": ["J. Yoon", "J. Choi", "C.D. Yoo"], "venue": "in Proc. IEEE Conf. on Image Processing,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Applications include compressed sensing [1], signal denoising, sparse signal recovery [2], image inpainting [3], image segmentation [4], and more recently, signal classification.", "startOffset": 40, "endOffset": 43}, {"referenceID": 1, "context": "Applications include compressed sensing [1], signal denoising, sparse signal recovery [2], image inpainting [3], image segmentation [4], and more recently, signal classification.", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "Based on this theory, a sparse representation-based classifier (SRC) [5] was initially developed for robust face recognition.", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].", "startOffset": 164, "endOffset": 167}, {"referenceID": 7, "context": "Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].", "startOffset": 168, "endOffset": 172}, {"referenceID": 8, "context": "Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].", "startOffset": 226, "endOffset": 230}, {"referenceID": 9, "context": "Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].", "startOffset": 261, "endOffset": 265}, {"referenceID": 10, "context": "Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].", "startOffset": 291, "endOffset": 295}, {"referenceID": 11, "context": "Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].", "startOffset": 316, "endOffset": 320}, {"referenceID": 15, "context": "Based on the K-SVD [3] model for general sparse representations, Discriminative K-SVD (D-KSVD) [21] and Label-Consistent K-SVD (LC-KSVD) [22], [23] learn the discriminative dictionaries by encouraging a projection of sparse codes X to be close to a sparse matrix with all non-zeros being one while satisfying a block diagonal structure as in Figure 1.", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "Based on the K-SVD [3] model for general sparse representations, Discriminative K-SVD (D-KSVD) [21] and Label-Consistent K-SVD (LC-KSVD) [22], [23] learn the discriminative dictionaries by encouraging a projection of sparse codes X to be close to a sparse matrix with all non-zeros being one while satisfying a block diagonal structure as in Figure 1.", "startOffset": 137, "endOffset": 141}, {"referenceID": 3, "context": "[6], [7] with DFDL and Yang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6], [7] with DFDL and Yang et al.", "startOffset": 5, "endOffset": 8}, {"referenceID": 17, "context": "[24], [25] with FDDL apply Fisher-based ideas on dictionaries and sparse coefficients, respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[24], [25] with FDDL apply Fisher-based ideas on dictionaries and sparse coefficients, respectively.", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "This problem has been partially addressed by recent efforts, namely DLSI [27], COPAR [28] and CSDL [29].", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "This problem has been partially addressed by recent efforts, namely DLSI [27], COPAR [28] and CSDL [29].", "startOffset": 85, "endOffset": 89}, {"referenceID": 21, "context": "This problem has been partially addressed by recent efforts, namely DLSI [27], COPAR [28] and CSDL [29].", "startOffset": 99, "endOffset": 103}, {"referenceID": 17, "context": "Our framework is basically a generalized version of the well-known FDDL [24], [25] with the additional capability of capturing shared features, resulting in better performance.", "startOffset": 72, "endOffset": 76}, {"referenceID": 18, "context": "Our framework is basically a generalized version of the well-known FDDL [24], [25] with the additional capability of capturing shared features, resulting in better performance.", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "In this paper, we aim to mitigate these drawbacks by proposing efficient and accurate algorithms which allows to directly solve X and D in two fundamental discriminative dictionary learning methods: FDDL [25] and DLSI [27].", "startOffset": 204, "endOffset": 208}, {"referenceID": 19, "context": "In this paper, we aim to mitigate these drawbacks by proposing efficient and accurate algorithms which allows to directly solve X and D in two fundamental discriminative dictionary learning methods: FDDL [25] and DLSI [27].", "startOffset": 218, "endOffset": 222}, {"referenceID": 20, "context": "These algorithms can also be applied to speed-up our proposed LRSDL, COPAR [28], DLR [26] and other related works.", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "Our framework simultaneously learns each class-dictionary 1The preliminary version of this work was presented in IEEE International Conference on Image Processing, 2016 [30].", "startOffset": 169, "endOffset": 173}, {"referenceID": 18, "context": "We present three effective algorithms for dictionary learning: i) sparse coefficient update in FDDL [25] by using FISTA [31].", "startOffset": 100, "endOffset": 104}, {"referenceID": 23, "context": "We present three effective algorithms for dictionary learning: i) sparse coefficient update in FDDL [25] by using FISTA [31].", "startOffset": 120, "endOffset": 124}, {"referenceID": 18, "context": "ii) Dictionary update in FDDL [25] by a simple ODL [32] procedure using M(\u2022) and another lemma.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "ii) Dictionary update in FDDL [25] by a simple ODL [32] procedure using M(\u2022) and another lemma.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "iii) Dictionary update in DLSI [27] by a simple ADMM [33] procedure which requires only one matrix inversion instead of several matrix inversions as originally proposed in [27].", "startOffset": 31, "endOffset": 35}, {"referenceID": 25, "context": "iii) Dictionary update in DLSI [27] by a simple ADMM [33] procedure which requires only one matrix inversion instead of several matrix inversions as originally proposed in [27].", "startOffset": 53, "endOffset": 57}, {"referenceID": 19, "context": "iii) Dictionary update in DLSI [27] by a simple ADMM [33] procedure which requires only one matrix inversion instead of several matrix inversions as originally proposed in [27].", "startOffset": 172, "endOffset": 176}, {"referenceID": 2, "context": "The toolbox includes implementations of SRC [5], ODL [32], LC-KSVD [23]2, efficient DLSI [27], efficient COPAR [28], efficient FDDL [25], DLR [26] and the proposed LRSDL.", "startOffset": 44, "endOffset": 47}, {"referenceID": 24, "context": "The toolbox includes implementations of SRC [5], ODL [32], LC-KSVD [23]2, efficient DLSI [27], efficient COPAR [28], efficient FDDL [25], DLR [26] and the proposed LRSDL.", "startOffset": 53, "endOffset": 57}, {"referenceID": 19, "context": "The toolbox includes implementations of SRC [5], ODL [32], LC-KSVD [23]2, efficient DLSI [27], efficient COPAR [28], efficient FDDL [25], DLR [26] and the proposed LRSDL.", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "The toolbox includes implementations of SRC [5], ODL [32], LC-KSVD [23]2, efficient DLSI [27], efficient COPAR [28], efficient FDDL [25], DLR [26] and the proposed LRSDL.", "startOffset": 111, "endOffset": 115}, {"referenceID": 18, "context": "The toolbox includes implementations of SRC [5], ODL [32], LC-KSVD [23]2, efficient DLSI [27], efficient COPAR [28], efficient FDDL [25], DLR [26] and the proposed LRSDL.", "startOffset": 132, "endOffset": 136}, {"referenceID": 23, "context": "We also recall here the FISTA algorithm [31] for solving the family of problems:", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "FDDL [24] has been used broadly as a technique for exploiting both structured dictionary and learning discriminative coefficient.", "startOffset": 5, "endOffset": 9}, {"referenceID": 26, "context": "For the shared dictionary, as stated in the Introduction, we constrain rank(D0) to be small by using the nuclear norm \u2016D0\u2016\u2217 which is its convex relaxation [34].", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "arg min 1\u2264c\u2264C (w\u2016y \u2212Dcx\u20162 + (1\u2212 w)\u2016x\u2212mc\u20162), (8) where w \u2208 [0, 1] is a preset weight for balancing the contribution of the two terms.", "startOffset": 58, "endOffset": 64}, {"referenceID": 17, "context": "1) Efficient FDDL dictionary update: Recall that in [24], the dictionary update step is divided into C subproblems, each updates one class-specific dictionary Dc while others fixed.", "startOffset": 52, "endOffset": 56}, {"referenceID": 24, "context": "The problem (10) can be solved effectively by Online Dictionary Learning (ODL) method [32].", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "The problem (11) has the form of equation (2), and can hence be solved by FISTA [31].", "startOffset": 80, "endOffset": 84}, {"referenceID": 25, "context": "Based on the Lemma 3, D0 can be updated by solving: D0 = arg min D0 trace(FD0 D0)\u2212 2trace(ED0 ) + \u03b7\u2016D0\u2016\u2217 where: E = V(X) ; F = X(X) (17) using the ADMM [33] method and the singular value thresholding algorithm [35].", "startOffset": 152, "endOffset": 156}, {"referenceID": 27, "context": "Based on the Lemma 3, D0 can be updated by solving: D0 = arg min D0 trace(FD0 D0)\u2212 2trace(ED0 ) + \u03b7\u2016D0\u2016\u2217 where: E = V(X) ; F = X(X) (17) using the ADMM [33] method and the singular value thresholding algorithm [35].", "startOffset": 210, "endOffset": 214}, {"referenceID": 27, "context": "Z =D\u03b7/\u03c1(Dc + U), (20) U =U + D0 \u2212 Z, (21) where D is the shrinkage thresholding operator [35].", "startOffset": 89, "endOffset": 93}, {"referenceID": 24, "context": "The optimization problem (18) can be solved by ODL [32].", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "4) LRSDL sparse coefficients update (LRSDL-X): In our preliminary work [30], we proposed a method for effectively solving X and X alternatively, now we combine both problems into one and find X by solving the following optimization problem:", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "We again solve this problem using FISTA [31] with the gradient of h(X):", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "We also propose here another efficient algorithm for updating dictionary in two other well-known dictionary learning methods: DLSI [27] and COPAR [28].", "startOffset": 131, "endOffset": 135}, {"referenceID": 20, "context": "We also propose here another efficient algorithm for updating dictionary in two other well-known dictionary learning methods: DLSI [27] and COPAR [28].", "startOffset": 146, "endOffset": 150}, {"referenceID": 25, "context": "We propose one ADMM [33] procedure to update Dc which requires only one matrix inversion, which will be referred as EDLSI-D.", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": "Later in this paper, we will both theoretically and experimentally show that E-DLSI-D is much more efficient than O-DLSI-D [27].", "startOffset": 123, "endOffset": 127}, {"referenceID": 20, "context": "Note that this algorithm can be beneficial for two subproblems of updating the common dictionary and the particular dictionary in COPAR [28] as well.", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "We also evaluate the total complexity of the proposed LRSDL and competing dictionary learning methods: DLSI [27], COPAR [28] and FDDL [24].", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "We also evaluate the total complexity of the proposed LRSDL and competing dictionary learning methods: DLSI [27], COPAR [28] and FDDL [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 17, "context": "We also evaluate the total complexity of the proposed LRSDL and competing dictionary learning methods: DLSI [27], COPAR [28] and FDDL [24].", "startOffset": 134, "endOffset": 138}, {"referenceID": 24, "context": "Online Dictionary Learning (ODL) We start with the well-known Online Dictionary Learning [32] whose cost function is:", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "X = arg min X 1 2 \u2016Y \u2212DX\u2016F + \u03bb\u2016X\u20161 (38) using FISTA [31].", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "DLSI [27] proposed a method to encourage the independence between bases of different classes by minimizing coherence between cross-class bases.", "startOffset": 5, "endOffset": 9}, {"referenceID": 20, "context": "1) Cost function: COPAR [28] is another dictionary learning method which also considers the shared dictionary (but without the low-rank constraint).", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": "2) Update X (COPAR-X): In sparse coefficient update step, COPAR [28] solve Xc one by one via one l1-norm regularization problem:", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "1 of [28]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 4, "context": "1) Original update X (O-FDDL-X): Based on results reported in DFDL [7], the complexity of O-FDDL-X is roughly Ckn(d+ qCk) + Cdk = Ck(dn+ qCkn+ Cdk).", "startOffset": 67, "endOffset": 70}, {"referenceID": 28, "context": "Comparing methods and datasets We present the experimental results of applying these methods to five diverse datasets: the Extended YaleB face dataset [36], the AR face dataset [37], the AR gender dataset, the Oxford Flower dataset [38], and one multi-class object category dataset \u2013 the Caltech 101 [39].", "startOffset": 151, "endOffset": 155}, {"referenceID": 29, "context": "Comparing methods and datasets We present the experimental results of applying these methods to five diverse datasets: the Extended YaleB face dataset [36], the AR face dataset [37], the AR gender dataset, the Oxford Flower dataset [38], and one multi-class object category dataset \u2013 the Caltech 101 [39].", "startOffset": 177, "endOffset": 181}, {"referenceID": 30, "context": "Comparing methods and datasets We present the experimental results of applying these methods to five diverse datasets: the Extended YaleB face dataset [36], the AR face dataset [37], the AR gender dataset, the Oxford Flower dataset [38], and one multi-class object category dataset \u2013 the Caltech 101 [39].", "startOffset": 232, "endOffset": 236}, {"referenceID": 31, "context": "Comparing methods and datasets We present the experimental results of applying these methods to five diverse datasets: the Extended YaleB face dataset [36], the AR face dataset [37], the AR gender dataset, the Oxford Flower dataset [38], and one multi-class object category dataset \u2013 the Caltech 101 [39].", "startOffset": 300, "endOffset": 304}, {"referenceID": 2, "context": "We compare our results with those using SRC [5] and other state-of-the-art dictionary learning methods: LC-KSVD [23], DLSI [27], FDDL [25], COPAR [28], and DLR [26].", "startOffset": 44, "endOffset": 47}, {"referenceID": 19, "context": "We compare our results with those using SRC [5] and other state-of-the-art dictionary learning methods: LC-KSVD [23], DLSI [27], FDDL [25], COPAR [28], and DLR [26].", "startOffset": 123, "endOffset": 127}, {"referenceID": 18, "context": "We compare our results with those using SRC [5] and other state-of-the-art dictionary learning methods: LC-KSVD [23], DLSI [27], FDDL [25], COPAR [28], and DLR [26].", "startOffset": 134, "endOffset": 138}, {"referenceID": 20, "context": "We compare our results with those using SRC [5] and other state-of-the-art dictionary learning methods: LC-KSVD [23], DLSI [27], FDDL [25], COPAR [28], and DLR [26].", "startOffset": 146, "endOffset": 150}, {"referenceID": 32, "context": "Regularization parameters in all methods are chosen using cross-validation [40].", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "As in [21], the dimension of a random-face feature in the Extended YaleB is d = 504, while the dimension in AR face is d = 540.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "For feature extraction, based on the impressive results presented in [25], we choose the Frequent Local Histogram feature extractor [41] to obtain feature vectors of dimension 10,000.", "startOffset": 69, "endOffset": 73}, {"referenceID": 33, "context": "For feature extraction, based on the impressive results presented in [25], we choose the Frequent Local Histogram feature extractor [41] to obtain feature vectors of dimension 10,000.", "startOffset": 132, "endOffset": 136}, {"referenceID": 34, "context": "We then extract the sparse coding spatial pyramid matching (ScSPM) feature [42], which is the concatenation of vectors pooled from words of the extracted DSIFT descriptor.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "Figure 7c) show sample learned bases using DLSI [27] where shared features are still hidden in class-specific bases.", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "Effect of the shared dictionary sizes on overall accuracy We perform an experiment to study the effect of the shared dictionary size on the overall classification results of two dictionary methods: COPAR [28] and LRSDL in the AR gender dataset.", "startOffset": 204, "endOffset": 208}, {"referenceID": 20, "context": "It is evident that in most cases, two dictionary learning methods with shared features (COPAR [28] and our proposed LRSDL) outperform others with all five highest values presenting in our proposed LRSDL.", "startOffset": 94, "endOffset": 98}, {"referenceID": 2, "context": "YaleB AR AR gender Oxford Flower Caltech 101 SRC [5] 97.", "startOffset": 49, "endOffset": 52}, {"referenceID": 19, "context": "60 DLSI [27] 96.", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": "67 FDDL [25] 97.", "startOffset": 8, "endOffset": 12}, {"referenceID": 20, "context": "26 COPAR [28] 98.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "In conjunction with discriminative model as proposed in [24], [25], this leads to a more flexible model where shared features are excluded before doing classification.", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "In conjunction with discriminative model as proposed in [24], [25], this leads to a more flexible model where shared features are excluded before doing classification.", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "In Section II-D and II-E, we discuss the efficient algorithms for FDDL [25], DLSI [27], then flexibly apply them into more sophisticated models.", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "In Section II-D and II-E, we discuss the efficient algorithms for FDDL [25], DLSI [27], then flexibly apply them into more sophisticated models.", "startOffset": 82, "endOffset": 86}, {"referenceID": 35, "context": "Very recently, researchers have begun to address this issue [43], [44].", "startOffset": 60, "endOffset": 64}, {"referenceID": 36, "context": "Very recently, researchers have begun to address this issue [43], [44].", "startOffset": 66, "endOffset": 70}], "year": 2016, "abstractText": "Despite the fact that different objects possess distinct class-specific features, they also usually share common patterns. This observation has been exploited partially in a recently proposed dictionary learning framework by separating the particularity and the commonality (COPAR). Inspired by this, we propose a novel method to explicitly and simultaneously learn a set of common patterns as well as class-specific features for classification with more intuitive constraints. Our dictionary learning framework is hence characterized by both a shared dictionary and particular (class-specific) dictionaries. For the shared dictionary, we enforce a low-rank constraint, i.e. claim that its spanning subspace should have low dimension and the coefficients corresponding to this dictionary should be similar. For the particular dictionaries, we impose on them the well-known constraints stated in the Fisher discrimination dictionary learning (FDDL). Further, we develop new fast and accurate algorithms to solve the subproblems in the learning step, accelerating its convergence. The said algorithms could also be applied to FDDL and its extensions. The efficiencies of these algorithms are theoretically and experimentally verified by comparing their complexities and running time with those of other well-known dictionary learning methods. Experimental results on widely used image datasets establish the advantages of our method over state-of-the-art dictionary learning methods.", "creator": "LaTeX with hyperref package"}}}