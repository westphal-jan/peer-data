{"id": "1610.09491", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2016", "title": "SDP Relaxation with Randomized Rounding for Energy Disaggregation", "abstract": "the develop additional scalable, computationally efficient case : modeling distributed machine energy disaggregation electromagnetic home appliance monitoring. researching this problem overall goal was to estimate the energy consume times each appliance transmitted power and on finite total energy - consumption signal about a plant. the fundamental state of electrical analyst aiming to model the problems describing inference in factorial components, and use exact simulations which find an approximate solution to the resulting quadratic integer program. currently we know a more principled approach, something designed after integer compound implementations, one find corresponding approximate optimum parameter providing cyclic semidefinite relaxations randomized rounding, so well as a scalable lattice filter that exploits the phase structure of discrete resulting semidefinite compound. important results both in synthetic and hardware - world manufacturing demonstrate the superiority complexity our method.", "histories": [["v1", "Sat, 29 Oct 2016 11:48:28 GMT  (313kb)", "http://arxiv.org/abs/1610.09491v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kiarash shaloudegi", "andr\u00e1s gy\u00f6rgy", "csaba szepesv\u00e1ri", "wilsun xu"], "accepted": true, "id": "1610.09491"}, "pdf": {"name": "1610.09491.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Wilsun Xu"], "emails": ["k.shaloudegi16@imperial.ac.uk", "a.gyorgy@imperial.ac.uk", "szepesva@ualberta.ca", "wxu@ualberta.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n09 49\n1v 1\n[ cs\n.L G\n] 2"}, {"heading": "1 Introduction", "text": "Energy efficiency is becoming one of the most important issues in our society. Identifying the energy consumption of individual electrical appliances in homes can raise awareness of power consumption and lead to significant saving in utility bills. Detailed feedback about the power consumption of individual appliances helps energy consumers to identify potential areas for energy savings, and increases their willingness to invest in more efficient products. Notifying home owners of accidentally running stoves, ovens, etc., may not only result in savings but also improves safety. Energy disaggregation or non-intrusive load monitoring (NILM) uses data from utility smart meters to separate individual load consumptions (i.e., a load signal) from the total measured power (i.e., the mixture of the signals) in households.\nThe bulk of the research in NILM has mostly concentrated on applying different data mining and pattern recognition methods to track the footprint of each appliance in total power measurements. Several techniques, such as artificial neural networks (ANN) [Prudenzi, 2002, Chang et al., 2012, Liang et al., 2010], deep neural networks [Kelly and Knottenbelt, 2015], k-nearest neighbor (k-NN) [Figueiredo et al., 2012, Weiss et al., 2012], sparse coding [Kolter et al., 2010], or ad-hoc heuristic methods [Dong et al., 2012] have been employed. Recent works, rather than turning electrical events into features fed into classifiers, consider the temporal structure of the data[Zia et al., 2011, Kolter and Jaakkola, 2012, Kim et al., 2011, Zhong et al., 2014, Egarter et al., 2015, Guo et al., 2015], resulting in state-of-the-art performance [Kolter and Jaakkola, 2012]. These works usually model the individual appliances by independent hidden Markov models (HMMs), which leads to a factorial HMM (FHMM) model describing the total consumption.\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nFHMMs, introduced by Ghahramani and Jordan [1997], are powerful tools for modeling times series generated from multiple independent sources, and are great for modeling speech with multiple people simultaneously talking [Rennie et al., 2009], or energy monitoring which we consider here [Kim et al., 2011]. Doing exact inference in FHMMs is NP hard; therefore, computationally efficient approximate methods have been the subject of study. Classic approaches include sampling methods, such as MCMC or particle filtering [Koller and Friedman, 2009] and variational Bayes methods [Wainwright and Jordan, 2007, Ghahramani and Jordan, 1997]. In practice, both methods are nontrivial to make work and we are not aware of any works that would have demonstrated good results in our application domain with the type of FHMMs we need to work and at practical scales.\nIn this paper we follow the work of Kolter and Jaakkola [2012] to model the NILM problem by FHMMs. The distinguishing features of FHMMs in this setting are that (i) the output is the sum of the output of the underlying HMMs (perhaps with some noise), and (ii) the number of transitions are small in comparison to the signal length. FHMMs with the first property are called additive. In this paper we derive an efficient, convex relaxation based method for FHMMs of the above type, which significantly outperforms the state-of-the-art algorithms. Our approach is based on revisiting relaxations to the integer programming formulation of Kolter and Jaakkola [2012]. In particular, we replace the quadratic programming relaxation of Kolter and Jaakkola, 2012 with a relaxation to an semi-definite program (SDP), which, based on the literature of relaxations is expected to be tighter and thus better. While SDPs are convex and could in theory be solved using interior-point (IP) methods in polynomial time [Malick et al., 2009], IP scales poorly with the size of the problem and is thus unsuitable to our large scale problem which may involve as many a million variables. To address this problem, capitalizing on the structure of our relaxation coming from our FHMM model, we develop a novel variant of ADMM [Boyd et al., 2011] that uses Moreau-Yosida regularization and combine it with a version of randomized rounding that is inspired by the the recent work of Park and Boyd [2015]. Experiments on synthetic and real data confirm that our method significantly outperforms other algorithms from the literature, and we expect that it may find its applications in other FHMM inference problems, too."}, {"heading": "1.1 Notation", "text": "Throughout the paper, we use the following notation: R denotes the set of real numbers, Sn+ denotes the set of n \u00d7 n positive semidefinite matrices, I{E} denotes the indicator function of an event E (that is, it is 1 if the event is true and zero otherwise), 1 denotes a vector of appropriate dimension whose entries are all 1. For an integer K , [K] denotes the set {1, 2, . . . ,K}. N (\u00b5,\u03a3) denotes the Gaussian distribution with mean \u00b5 and covariance matrix \u03a3. For a matrix A, trace(A) denotes its trace and diag(A) denotes the vector formed by the diagonal entries of A."}, {"heading": "2 System Model", "text": "Following Kolter and Jaakkola [2012], the energy usage of the household is modeled using an additive factorial HMM [Ghahramani and Jordan, 1997]. Suppose there areM appliances in a household. Each of them is modeled via an HMM: let Pi \u2208 RKi\u00d7Ki denote the transition-probability matrix of appliance i \u2208 [M ], and assume that for each state s \u2208 [Ki], the energy consumption of the appliance is constant \u00b5i,s (\u00b5i denotes the corresponding Ki-dimensional column vector (\u00b5i,1, . . . , \u00b5i,Ki)\n\u22a4). Denoting by xt,i \u2208 {0, 1}Ki the indicator vector of the state st,i of appliance i at time t (i.e., xt,i,s = I{st,i=s}), the total power consumption at time t is \u2211 i\u2208[M ] \u00b5 \u22a4 i xt,i, which we assume is observed with some additive zero mean Gaussian noise of variance \u03c32: yt \u223c N ( \u2211 i\u2208[M ] \u00b5 \u22a4 i xt,i, \u03c3 2).1\nGiven this model, the maximum likelihood estimate of the appliance state vector sequence can be obtained by minimizing the log-posterior function\nargmin xt,i\nT \u2211\nt=1\n(yt \u2212 \u2211M i=1 x \u22a4 t,i\u00b5i) 2\n2\u03c32 \u2212\nT\u22121 \u2211\nt=1\nM \u2211\ni=1\nx\u22a4t,i(logPi)xt+1,i\nsubject to xt,i \u2208 {0, 1}Ki, 1\u22a4xt,i = 1, i \u2208 [M ] and t \u2208 [T ],\n(1)\n1Alternatively, we can assume that the power consumption yt,iof each appliance is normally distributed with mean \u00b5\u22a4i xt,i and variance \u03c3 2 i , where \u03c3 2 = \u2211 i\u2208[M] \u03c3 2 i , and yt = \u2211 i\u2208[M] yt,i.\nwhere logPi denotes a matrix obtained from Pi by taking the logarithm of each entry.\nIn our particular application, in addition to the signal\u2019s temporal structure, large changes in total power (in comparison to signal noise) contain valuable information that can be used to further improve the inference results (in fact, solely this information was used for energy disaggregation, e.g., by Dong et al., 2012, 2013, Figueiredo et al., 2012). This observation was used by Kolter and Jaakkola [2012] to amend the posterior with a term that tries to match the large signal changes to the possible changes in the power level when only the state of a single appliance changes.\nFormally, let \u2206yt = yt+1 \u2212 yt, \u2206\u00b5 (i) m,k = \u00b5i,k \u2212 \u00b5i,m, and define the matrices Et,i \u2208 R Ki\u00d7Ki by (Et,i)m,k = (\u2206yt \u2212\u2206\u00b5 (i) m,k)\n2/(2\u03c32diff), for some constant \u03c3diff > 0. Intuitively, (Et,i)m,k is the negative log-likelihood (up to a constant) of observing a change \u2206yt in the power level when appliance i transitions from state m to state k under some zero-mean Gaussian noise with variance \u03c32diff. Making the heuristic approximation that the observation noise and this noise are independent (which clearly does not hold under the previous model), Kolter and Jaakkola [2012] added the term (\u2212\n\u2211T\u22121 t=1 \u2211M i=1 x \u22a4 t,iEt,ixt+1,i) to the objective of (1), arriving at\nargmin xt,i f(x1, . . . , xT ) :=\nT \u2211\nt=1\n(yt \u2212 \u2211M i=1 x \u22a4 t,i\u00b5i) 2\n2\u03c32 \u2212\nT\u22121 \u2211\nt=1\nM \u2211\ni=1\nx\u22a4t,i(Et,i + logPi)xt+1,i\nsubject to xt,i \u2208 {0, 1}Ki, 1\u22a4xt,i = 1, i \u2208 [M ] and t \u2208 [T ] .\n(2)\nIn the rest of the paper we derive an efficient approximate solution to (2), and demonstrate that it is superior to the approximate solution derived by Kolter and Jaakkola [2012] with respect to several measures quantifying the accuracy of load disaggregation solutions."}, {"heading": "3 SDP Relaxation and Randomized Rounding", "text": "There are two major challenges to solve the optimization problem (2) exactly: (i) the optimization is over binary vectors xt,i; and (ii) the objective function f , even when considering its extension to a convex domain, is in general non-convex (due to the second term). As a remedy we will relax (2) to make it an integer quadratic programming problem, then apply an SDP relaxation and randomized rounding to solve approximately the relaxed problem. We start with reviewing the latter methods."}, {"heading": "3.1 Approximate Solutions for Integer Quadratic Programming", "text": "In this section we consider approximate solutions to the integer quadratic programming problem\nminimize f(x) = x\u22a4Dx+ 2d\u22a4x subject to x \u2208 {0, 1}n,\n(3)\nwhere D \u2208 Sn+ is positive semidefinite, and d \u2208 R n. While an exact solution of (3) can be found by enumerating all possible combination of binary values within a properly chosen box or ellipsoid, the running time of such exact methods is nearly exponential in the number n of binary variables, making these methods unfit for large scale problems.\nOne way to avoid exponential running times is to replace (3) with a convex problem with the hope that the solutions of the convex problems can serve as a good starting point to find high-quality solutions to (3). The standard approach to this is to linearize (3) by introducing a new variable X \u2208 Sn+ tied to x trough X = xx\n\u22a4, so that x\u22a4Dx = trace(DX), and then relax the nonconvex constraints X = xx\u22a4, x \u2208 {0, 1}n to X xx\u22a4, diag(X) = x, x \u2208 [0, 1]n. This leads to the relaxed SDP problem\nminimize trace(D\u22a4X) + 2d\u22a4x\nsubject to\n[\n1 x\u22a4 x X\n]\n0, diag(X) = x, x \u2208 [0, 1]n (4)\nBy introducing X\u0302 =\n[\n1 x\u22a4\nx X\n]\nthis can be written in the compact SDP form\nminimize trace(D\u0302\u22a4X\u0302) subject to X\u0302 0, AX\u0302 = b . (5)\nwhere D\u0302 =\n[\n0 d\u22a4\nd D\n]\n\u2208 Sn+1+ , b \u2208 R m and A : Sn+ \u2192 R m is an appropriate linear operator. This\ngeneral SDP optimization problem can be solved with arbitrary precision in polynomial time using interior-point methods [Malick et al., 2009, Wen et al., 2010]. As discussed before, this approach becomes impractical in terms of both the running time and the required memory if either the number of variables or the optimization constraints are large [Wen et al., 2010]. We will return to the issue of building scaleable solvers for NILM in Section 5.\nNote that introducing the new variable X , the problem is projected into a higher dimensional space, which is computationally more challenging than just simply relaxing the integrality constraint in (3), but leads to a tighter approximation of the optimum (c.f., Park and Boyd, 2015; see also Lov\u00e1sz and Schrijver, 1991, Burer and Vandenbussche, 2006).\nTo obtain a feasible point of (3) from the solution of (5), we still need to change the solution x to a binary vector. This can be done via randomized rounding [Park and Boyd, 2015, Goemans and Williamson, 1995]: Instead of letting x \u2208 [0, 1]n, the integrality constraint x \u2208 {0, 1}n in (3) can be replaced by the inequalities xi(xi \u2212 1) \u2265 0 for all i \u2208 [n]. Although these constraints are nonconvex, they admit an interesting probabilistic interpretation: the optimization problem\nminimize Ew\u223cN (\u00b5,\u03a3)[w \u22a4Dw + 2d\u22a4w]\nsubject to Ew\u223cN (\u00b5,\u03a3)[wi(wi \u2212 1)] \u2265 0, i \u2208 [n], \u00b5 \u2208 R n, \u03a3 0\nis equivalent to\nminimize trace((\u03a3 + \u00b5\u00b5\u22a4)D) + 2d\u22a4\u00b5\nsubject to \u03a3i,i + \u00b5 2 i \u2212 \u00b5i \u2265 0, i \u2208 [n],\n(6)\nwhich is in the form of (4) with X = \u03a3 + \u00b5\u00b5\u22a4 and x = \u00b5 (above, Ex\u223cP [f(x)] stands for \u222b\nf(x)dP (x)). This leads to the rounding procedure: starting from a solution (x\u2217, X\u2217) of (4), we randomly draw several samples w(j) from N (x\u2217, X\u2217 \u2212 x\u2217x\u2217\u22a4), round w(j)i to 0 or 1 to obtain x(j), and keep the x(j) with the smallest objective value. In a series of experiments, Park and Boyd [2015] found this procedure to be better than just naively rounding the coordinates of x\u2217."}, {"heading": "4 An Efficient Algorithm for Inference in FHMMs", "text": "To arrive at our method we apply the results of the previous subsection to (2). To do so, as mentioned at the beginning of the section, we need to change the problem to a convex one, since the elements of the second term in the objective of (2), \u2212x\u22a4t,i(Et,i + logPi)xt+1,i are not convex. To address this issue, we relax the problem by introducing new variables Zt,i = xt,ix\u22a4t+1,i and replace the constraint Zt,i = xt,ix\u22a4t+1,i with two new ones:\nZt,i1 = xt,i and Z\u22a4t,i1 = xt+1,i.\nTo simplify the presentation, we will assume that Ki = K for all i \u2208 [M ]. Then problem (2) becomes\nargmin xt,i\nT \u2211\nt=1\n{\n1\n2\u03c32 ( yt \u2212 x \u22a4 t \u00b5 )2 \u2212 p\u22a4t zt\n}\nsubject to xt \u2208 {0, 1} MK , t \u2208 [T ],\nz\u0302t \u2208 {0, 1} MKK, t \u2208 [T \u2212 1],\n1 \u22a4xt,i = 1, t \u2208 [T ] and i \u2208 [M ],\nZt,i1 \u22a4 = xt,i, Z \u22a4 t,i1 \u22a4 = xt+1,i , t \u2208 [T \u2212 1] and i \u2208 [M ],\n(7)\nwhere x\u22a4t = [x \u22a4 t,1, . . . , x \u22a4 t,M ], \u00b5 \u22a4 = [\u00b5\u22a41 , . . . , \u00b5 \u22a4 M ], z \u22a4 t = [vec(Zt,1) \u22a4, . . . , vec(Zt,M )\u22a4] and p\u22a4t = [vec(Et,1 + logP1), . . . , vec(logPT )], with vec(A) denoting the column vector obtained\nAlgorithm 1 ADMM-RR: Randomized rounding algorithm for suboptimal solution to (2) Given: number of iterations: itermax, length of input data: T Solve the optimization problem (8): Run Algorithm 2 to get X\u2217t and z \u2217 t\nSet xbestt := z \u2217 t and X best t := X \u2217 t for t = 1, . . . , T for t = 2, . . . , T \u2212 1 do Set x := [xbestt\u22121 \u22a4 , xbestt \u22a4 , xbestt+1 \u22a4 ]\u22a4\nSet X := block(Xbestt\u22121 , X best t , X best t+1 ) where block(\u00b7, \u00b7) constructs block diagonal matrix from input arguments Set fbest := \u221e Form the covariance matrix \u03a3 := X \u2212 xxT and find its Cholesky factorization LL\u22a4 = \u03a3. for k = 1, 2, . . . , itermax do\nRandom sampling: zk := x+ Lw, where w \u223c N (0, I) Round zk to the nearest integer point xk that satisfies the constraints of (7) If fbest > ft(xk) then update xbestt and X best t from the corresponding entries of x k and xkxk \u22a4\n, respectively\nend for end for\nby concatenating the columns of A for a matrix A. Expanding the first term of (7) and following the relaxation method of Section 3.1, we get the following SDP problem:2\narg min Xt,zt\nT \u2211\nt=1\ntrace(D\u22a4t Xt) + d \u22a4 t zt\nsubject to AXt = b, BXt + Czt + EXt+1 = g,\nXt 0, Xt, zt \u2265 0 .\n(8)\nHere A : SMK+1+ \u2192 R m, B, E : SMK+1+ \u2192 R m\u2032 and C \u2208 RMKK\u00d7m \u2032\nare all appropriate linear operators, and the integers m and m\u2032 are determined by the number of equality constraints, while\nDt = 1\n2\u03c32\n[\n0 \u2212yt\u00b5\u22a4\n\u2212yt\u00b5 \u00b5\u00b5\u22a4\n]\nand dt = pt. Notice that (8) is a simple, though huge-dimensional SDP\nproblem in the form of (5) where D\u0302 has a special block structure.\nNext we apply the randomized rounding method from Section 3.1 to provide an approximate solution to our original problem (2). Starting from an optimal solution (z\u2217, X\u2217) of (8) , and utilizing that we have an SDP problem for each time step t, we obtain Algorithm 1 that performs the rounding sequentially for t = 1, 2, . . . , T . However we run the randomized method for three consecutive time steps, since Xt appears at both time steps t \u2212 1 and t + 1 in addition to time t (cf., equation 9). Following Park and Boyd [2015], in the experiments we introduce a simple greedy search within Algorithm 1: after finding the initial point xk , we greedily try to objective the target value by change the status of a single appliance at a single time instant. The search stops when no such improvement is possible, and we use the resulting point as the estimate."}, {"heading": "5 ADMM Solver for Large-Scale, Sparse Block-Structured SDP Problems", "text": "Given the relaxation and randomized rounding presented in the previous subsection all that remains is to find X\u2217t , z \u2217 t to initialize Algorithm 1. Although interior point methods can solve SDP problems efficiently, even for problems with sparse constraints as (4), the running time to obtain an \u01eb optimal solution is of the order of n3.5 log(1/\u01eb) [Nesterov, 2004, Section 4.3.3], which becomes prohibitive in our case since the number of variables scales linearly with the time horizon T .\nAs an alternative solution, first-order methods can be used for large scale problems [Wen et al., 2010]. Since our problem (8) is an SDP problem where the objective function is separable, ADMM is a promising candidate to find a near-optimal solution. To apply ADMM, we use the MoreauYosida quadratic regularization [Malick et al., 2009], which is well suited for the primal formulation we consider. When implementing ADMM over the variables (Xt, zt)t, the sparse structure of our\n2The only modification is that we need to keep the equality constraints in (7) that are missing from (3).\nAlgorithm 2 ADMM for sparse SDPs of the form (8) Given: length of input data: T , number of iterations: itermax. Set the initial values to zero. W 0t , P 0 t , S 0 = 0, \u03bb0t = 0, \u03bd 0 t = 0, and r 0 t , h 0 t = 0\nSet \u00b5 = 0.001 {Default step-size value} for k = 0, 1, . . . , itermax do\nfor t = 1, 2, . . . , T do Update P kt , W k t , \u03bb k, Skt , r k t , h k t , and \u03bd k t , respectively, according to (11) (Appendix A).\nend for end for\nconstraints allows to consider the SDP problems for each time step t sequentially:\narg min Xt,zt\ntrace(D\u22a4t Xt) + d \u22a4 t zt\nsubject to AXt = b,\nBXt + Czt + EXt+1 = g,\nBXt\u22121 + Czt\u22121 + EXt = g,\nXt 0, Xt, zt \u2265 0 .\n(9)\nThe regularized Lagrangian function for (9) is3\nL\u00b5 =trace(D\u22a4X) + d\u22a4z + 1\n2\u00b5 \u2016X \u2212 S\u20162F +\n1\n2\u00b5 \u2016z \u2212 r\u201622 + \u03bb \u22a4(b\u2212AX)\n+ \u03bd\u22a4(g \u2212 BX \u2212 Cz \u2212 EX+) + \u03bd \u22a4 \u2212 (g \u2212 BX\u2212 \u2212 Cz\u2212 \u2212 EX)\n\u2212 trace(W\u22a4X)\u2212 trace(P\u22a4X)\u2212 h\u22a4z,\n(10)\nwhere \u03bb, \u03bd, W \u2265 0, P 0, and h \u2265 0 are dual variables, and \u00b5 > 0 is a constant. By taking the derivatives of L\u00b5 and computing the optimal values of X and z, one can derive the standard ADMM updates, which, due to space constraints, are given in Appendix A. The final algorithm, which updates the variables for each t sequentially, is given by Algorithm 2.\nAlgorithms 1 and 2 together give an efficient algorithm for finding an approximate solution to (2) and thus also to the inference problem of additive FHMMs."}, {"heading": "6 Learning the Model", "text": "The previous section provided an algorithm to solve the inference part of our energy disaggregation problem. However, to be able to run the inference method, we need to set up the model. To learn the HMMs describing each appliance, we use the method of Kontorovich et al. [2013] to learn the transition matrix, and the spectral learning method of Anandkumar et al. [2012] (following Mattfeld, 2014) to determine the emission parameters.\nHowever, when it comes to the specific application of NILM, the problem of unknown, time-varying bias also needs to be addressed, which appears due to the presence of unknown/unmodeled appliances in the measured signal. A simple idea, which is also followed by Kolter and Jaakkola [2012], is to use a \u201cgeneric model\u201d whose contribution to the objective function is downweighted. Surprisingly, incorporating this idea in the FHMM inference creates some unexpected challenges.4\nTherefore, in this work we come up with a practical, heuristic solution tailored to NILM. First we identify all electric events defined by a large change \u2206yt in the power usage (using some adhoc threshold). Then we discard all events that are similar to any possible level change \u2206\u00b5(i)m,k. The remaining large jumps are regarded as coming from a generic HMM model describing the unregistered appliances: they are clustered into K \u2212 1 clusters, and an HMM model is built where each cluster is regarded as power usage coming from a single state of the unregistered appliances. We also allow an \u201coff state\u201d with power usage 0.\n3We drop the subscript t and replace t+ 1 and t\u2212 1 with + and \u2212 signs, respectively. 4For example, the incorporation of this generic model breaks the derivation of the algorithm of\nKolter and Jaakkola [2012]. See Appendix B for a discussion of this."}, {"heading": "7 Experimental Results", "text": "We evaluate the performance of our algorithm in two setups:5 we use a synthetic dataset to test the inference method in a controlled environment, while we used the REDD dataset of Kolter and Johnson [2011] to see how the method performs on non-simulated, \u201creal\u201d data. The performance of our algorithm is compared to the structured variational inference (SVI) method of Ghahramani and Jordan [1997], the method of Kolter and Jaakkola [2012] and that of Zhong et al. [2014]; we shall refer to the last two algorithms as KJ and ZGS, respectively."}, {"heading": "7.1 Experimental Results: Synthetic Data", "text": "The synthetic dataset was generated randomly (the exact procedure is described in Appendix C). To evaluate the performance, we use normalized disaggregation error as suggested by Kolter and Jaakkola [2012] and also adopted by Zhong et al. [2014]. This measures the reconstruction error for each individual appliance. Given the true output yt,i and the estimated output y\u0302t,i (i.e. y\u0302t,i = \u00b5 \u22a4 i x\u0302t,i), the error measure is defined as\nNDE = \u221a \u2211\nt,i(yt,i \u2212 y\u0302t,i) 2/ \u2211 t,i (yt,i) 2 .\nFigures 1 and 2 show the performance of the algorithms as the number HMMs (M ) (resp., number of states, K) is varied. Each plot is a report for T = 1000 steps averaged over 100 random models and realizations, showing the mean and standard deviation of NDE. Our method, shown under the label ADMM-RR, runs ADMM for 2500 iterations, runs the local search at the end of each 250 iterations, and chooses the result that has the maximum likelihood. ADMM is the algorithm which applies naive rounding. It can be observed that the variational inference method is significantly outperformed by all other methods, while our algorithm consistently obtained better results than its competitors, KJ coming second and ZGS third."}, {"heading": "7.2 Experimental Results: Real Data", "text": "In this section, we also compared the 3 best methods on the real dataset REDD [Kolter and Johnson, 2011]. We use the first half of the data for training and the second half for testing. Each HMM (i.e.,\n5Our code is available online at https://github.com/kiarashshaloudegi/FHMM_inference.\nappliance) is trained separately using the associated circuit level data, and the HMM corresponding to unregistered appliances is trained using the main panel data. In this set of experiments we monitor appliances consuming more than 100 watts. ADMM-RR is run for 1000 iterations, and the local search is run at the end of each 250 iterations, and the result with the largest likelihood is chosen. To be able to use the ZGS method on this data, we need to have some prior information about the usage of each appliance; the authors suggestion is to us national energy surveys, but in the lack of this information (also about the number of residents, type of houses, etc.) we used the training data to extract this prior knowledge, which is expected to help this method.\nDetailed results about the precision and recall of estimating which appliances are \u2018on\u2019 at any given time are given in Table 1. In Appendix D we also report the error of the total power usage assigned to different appliances (Table 2), as well as the amount of assigned power to each appliance as a percentage of total power (Figure 3). As a summary, we can see that our method consistently outperformed the others, achieving an average precision and recall of 60.97% and 78.56%, with about 50% better precision than KJ with essentially the same recall (38.68/75.02%), while significantly improving upon ZGS (17.97/36.22%). Considering the error in assigning the power consumption to different appliances, our method achieved about 30\u221235% smaller error (ADMM-RR: 2.87%, KJ: 4.44%, ZGS: 3.94%) than its competitors.\nIn our real-data experiments, there are about 1 million decision variables: M = 7 or 6 appliances (for phase A and B power, respectively) with K = 4 states each and for about T = 30, 000 time steps for one day, 1 sample every 6 seconds. KJ and ZGS solve quadratic programs, increasing their memory usage (14GB vs 6GB in our case). On the other hand, our implementation of their method, using the commercial solver MOSEK inside the Matlab-based YALMIP [L\u00f6fberg, 2004], runs in 5 minutes, while our algorithm, which is purely Matlab-based takes 5 hours to finish. We expect that an optimized C++ version of our method could achieve a significant speed-up compared to our current implementation."}, {"heading": "8 Conclusion", "text": "FHMMs are widely used in energy disaggregation. However, the resulting model has a huge (factored) state space, making standard inference FHMM algorithms infeasible even for only a handful of appliances. In this paper we developed a scalable approximate inference algorithm, based on a semidefinite relaxation combined with randomized rounding, which significantly outperformed the state of the art in our experiments. A crucial component of our solution is a scalable ADMM method that utilizes the special block-diagonal-like structure of the SDP relaxation and provides a good initialization for randomized rounding. We expect that our method may prove useful in solving other FHMM inference problems, as well as in large scale integer quadratic programming."}, {"heading": "Acknowledgements", "text": "This work was supported in part by the Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning and by NSERC. K. is indebted to Pooria Joulani and Mohammad Ajallooeian, whom provided much useful technical advise, while all authors are grateful for Zico Kolter for sharing his code."}, {"heading": "A ADMM updates", "text": "In this section we derive the ADMM updates for the regularized Lagrangian L\u00b5 given by (10). Taking derivatives with respect to X and z and setting them to zeros, we get\n\u2207XL\u00b5 = D + 1\n\u00b5 (X \u2212 S)\u2212A\u22a4\u03bb\u2212 B\u22a4\u03bd \u2212 E\u22a4\u03bd\u2212 \u2212W \u2212 P = 0,\nX\u2217 = S + \u00b5(A\u22a4\u03bb+ B\u22a4\u03bd + E\u22a4\u03bd\u2212 +W + P \u2212D)\nand\n\u2207zL\u00b5 = d+ 1\n\u00b5 (z \u2212 r)\u2212 C\u22a4\u03bd \u2212 h = 0,\nz\u2217 = r + \u00b5(C\u22a4\u03bd + h\u2212 d) .\nSubstituting X = X\u2217 and z = z\u2217 in (10) defines L\u0302\u00b5. Then the standard ADMM iteration yields\nP k+1 = argmin P 0 L\u0302\u00b5(S k, P,W k, \u03bbk, \u03bdk, \u03bdk\u2212),\nW k+1 = arg min W\u22650 L\u0302\u00b5(S k, P k+1,W, \u03bbk, \u03bdk, \u03bdk\u2212),\n\u03bbk+1 = argmin \u03bb L\u0302\u00b5(S k, P k+1,W k+1, \u03bb, \u03bdk, \u03bdk\u2212),\nSk+1 = Sk + \u00b5(A\u22a4\u03bbk+1 + B\u22a4\u03bdk + E\u22a4\u03bdk+1\u2212 +W k+1 + P k+1 \u2212D),\nrk+1 = rk + \u00b5(C\u22a4\u03bdk + hk \u2212 d),\nhk+1 = argmin h\u22650 L\u0302\u00b5(r k+1, \u03bdk),\n\u03bdk+1 = argmin \u03bd L\u0302\u00b5(S k+1, P k+1,W k+1, \u03bbk+1, \u03bd, \u03bdk+1\u2212 , h k+1, rk+1).\nBy rearranging the terms in L\u0302\u00b5, the following update equations can be found:\nW k+1 =max{(D \u2212A\u22a4\u03bbk \u2212 B\u22a4\u03bdk \u2212 E\u22a4\u03bdk\u2212 \u2212 P k \u2212D \u2212 Sk/\u00b5),0},\nP k+1 =(D \u2212A\u22a4\u03bbk \u2212 B\u22a4\u03bdk \u2212 E\u22a4\u03bdk\u2212 \u2212W k \u2212D \u2212 Sk/\u00b5)+,\n\u03bbk+1 = 1\n\u00b5 (AA\u22a4)\u2020\n(\nb\u2212A(B\u22a4\u03bdk + E\u22a4\u03bdk\u2212 +W k+1 + P k+1 \u2212D)\n)\n,\nhk+1 =max{d\u2212 C\u22a4\u03bdk \u2212 rk/\u00b5,0},\n\u03bdk+1 = 1\n\u00b5 (BB\u22a4 + CC\u22a4 + EE\u22a4)\u2020\n( g \u2212 B ( Sk+1 + \u00b5(A\u22a4\u03bbk+1 + E\u22a4\u03bdk+1\u2212 +W k+1 + P k+1 \u2212D) )\n\u2212C ( rk+1 + \u00b5(hk+1 \u2212 d) ) \u2212 E ( Sk+ + \u00b5(A \u22a4\u03bbk+ + B \u22a4\u03bdk+ +W k + + P k + \u2212D) ))\n. (11)\nHere max : X \u00d7 X \u2192 X works elementwise, and for any square matrix A, A\u2020 denotes the MoorePenrose pseudo inverse, and for any real symmetric matrix A, A+ is the projection of A onto the positive semidefinite cone (if the spectral decomposition of A is given by A = \u2211\ni \u03bbiviv \u22a4 i , where \u03bbi\nand vi are the ith eigenvalue and eigenvector of A, respectively, then A+ = \u2211\n\u03bbi>0 \u03bbiviv \u22a4 i ). Note\nthat the projections are done on matrices of small size. Note also that the pseudo-inverses of the matrices involved need only be calculated once."}, {"heading": "B Discussion of the Derivation in Kolter and Jaakkola [2012] in the Presence of the \u201cGeneric Model\u201d", "text": "The \u201cgeneric model\u201d affects the derivation of the algorithm of Kolter and Jaakkola [2012] as follows. The authors of this paper claim to derive the final optimization problem given in equation (15) of their paper from (9) and (10) as follows: equation (9) defines the problem minz\u2208Z,Q\u2208A f1(z,Q), while (10) defines the problem minz\u2032\u2208Z\u2032,Q\u2208A f2(z\u2032, Q) where z\u2032 = g(z). Here, z, z\u2032 are variables that describe the state of the \u201cgeneric model\u201d over time.\nThe claim in the paper is that with some set B (coming from their \u201cone-at-a-time\u201d constraint), minz\u2208Z,z\u2032\u2208Z\u2032,Q\u2208A\u2229B,z\u2032=g(z) f1(z, A) + f2(z\n\u2032, Q) is equivalent to the minimization problem in equation (15). However, careful checking the derivation shows that (15) is equivalent to minz\u2208Z,z\u2032\u2208Z\u2032,Q\u2208A\u2229B f1(z, A) + minz\u2032\u2208Z\u2032 f2(z \u2032, Q), which is smaller in general."}, {"heading": "C Generating the Synthetic Dataset", "text": "The synthetic dataset used in the experiments was generated in the following way: The power levels corresponding to each on state (\u00b5) were generated uniformly at random from [100, 4500] with the additional constraint that the difference of any two non-zero levels must be greater than 100 (to encourage identifiability). The levels for \u201coff states\u201d were set to 0. The transition matrices for each appliance were generated the following way: diagonal elements for \u201coff states\u201d were drawn uniformly at random from [0, 35] and for on-states from [0, 30], while non-diagonal elements were selected from [0, 1] to ensure sparse transitions. Finally, the data matrices were normalized to ensure they are proper transition matrices. The output of each appliance was subject to an additive Gaussian noise with variance \u03c3 \u2208 [0, 6] selected proportionally to the energy consumption level of the given on state, and 1 for off states."}, {"heading": "D Additional Results for the Real-Data Experiment", "text": "In Table 1 we provided prediction and recall values for our experiments on real data. As promised, here we provide some additional results about these experiments: Table 2 presents the total power usage assigned to different appliances, and Figure 3 shows the amount of assigned power to each appliance."}], "references": [{"title": "Non-Intrusive Signature Extraction for Major Residential Loads", "author": ["M. Dong", "Meira", "W. Xu", "C.Y. Chung"], "venue": "Smart Meters. IEEE Transactions on Smart Grid,", "citeRegEx": "Dong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2012}, {"title": "PALDi: Online Load Disaggregation via Particle Filtering", "author": ["D. Egarter", "V.P. Bhuvana", "W. Elmenreich"], "venue": "IEEE Transactions on Smart Grid,", "citeRegEx": "Egarter et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Egarter et al\\.", "year": 2013}, {"title": "Factorial Hidden Markov Models", "author": ["Z. Ghahramani", "M. Jordan"], "venue": "Monitoring (NILM) Systems. Neurocomputing,", "citeRegEx": "Ghahramani and Jordan.,? \\Q2012\\E", "shortCiteRegEx": "Ghahramani and Jordan.", "year": 2012}, {"title": "Implementing spectral methods for hidden Markov models with real-valued emissions", "author": ["C. Mattfeld"], "venue": "Journal on Optimization,", "citeRegEx": "Mattfeld.,? \\Q2009\\E", "shortCiteRegEx": "Mattfeld.", "year": 2009}, {"title": "Leveraging smart meter data to recognize home appliances", "author": ["M. Weiss", "A. Helfenstein", "F. Mattern", "T. Staake"], "venue": null, "citeRegEx": "Weiss et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2007}, {"title": "Note that the projections are done on matrices of small size. Note also that the pseudo-inverses of the matrices involved need only be calculated once", "author": [], "venue": "B Discussion of the Derivation in Kolter and Jaakkola", "citeRegEx": "..,? \\Q2012\\E", "shortCiteRegEx": "..", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": ", 2010], or ad-hoc heuristic methods [Dong et al., 2012] have been employed.", "startOffset": 37, "endOffset": 56}, {"referenceID": 2, "context": "FHMMs, introduced by Ghahramani and Jordan [1997], are powerful tools for modeling times series generated from multiple independent sources, and are great for modeling speech with multiple people simultaneously talking [Rennie et al.", "startOffset": 21, "endOffset": 50}, {"referenceID": 2, "context": "FHMMs, introduced by Ghahramani and Jordan [1997], are powerful tools for modeling times series generated from multiple independent sources, and are great for modeling speech with multiple people simultaneously talking [Rennie et al., 2009], or energy monitoring which we consider here [Kim et al., 2011]. Doing exact inference in FHMMs is NP hard; therefore, computationally efficient approximate methods have been the subject of study. Classic approaches include sampling methods, such as MCMC or particle filtering [Koller and Friedman, 2009] and variational Bayes methods [Wainwright and Jordan, 2007, Ghahramani and Jordan, 1997]. In practice, both methods are nontrivial to make work and we are not aware of any works that would have demonstrated good results in our application domain with the type of FHMMs we need to work and at practical scales. In this paper we follow the work of Kolter and Jaakkola [2012] to model the NILM problem by FHMMs.", "startOffset": 21, "endOffset": 919}, {"referenceID": 2, "context": "FHMMs, introduced by Ghahramani and Jordan [1997], are powerful tools for modeling times series generated from multiple independent sources, and are great for modeling speech with multiple people simultaneously talking [Rennie et al., 2009], or energy monitoring which we consider here [Kim et al., 2011]. Doing exact inference in FHMMs is NP hard; therefore, computationally efficient approximate methods have been the subject of study. Classic approaches include sampling methods, such as MCMC or particle filtering [Koller and Friedman, 2009] and variational Bayes methods [Wainwright and Jordan, 2007, Ghahramani and Jordan, 1997]. In practice, both methods are nontrivial to make work and we are not aware of any works that would have demonstrated good results in our application domain with the type of FHMMs we need to work and at practical scales. In this paper we follow the work of Kolter and Jaakkola [2012] to model the NILM problem by FHMMs. The distinguishing features of FHMMs in this setting are that (i) the output is the sum of the output of the underlying HMMs (perhaps with some noise), and (ii) the number of transitions are small in comparison to the signal length. FHMMs with the first property are called additive. In this paper we derive an efficient, convex relaxation based method for FHMMs of the above type, which significantly outperforms the state-of-the-art algorithms. Our approach is based on revisiting relaxations to the integer programming formulation of Kolter and Jaakkola [2012]. In particular, we replace the quadratic programming relaxation of Kolter and Jaakkola, 2012 with a relaxation to an semi-definite program (SDP), which, based on the literature of relaxations is expected to be tighter and thus better.", "startOffset": 21, "endOffset": 1519}, {"referenceID": 2, "context": "FHMMs, introduced by Ghahramani and Jordan [1997], are powerful tools for modeling times series generated from multiple independent sources, and are great for modeling speech with multiple people simultaneously talking [Rennie et al., 2009], or energy monitoring which we consider here [Kim et al., 2011]. Doing exact inference in FHMMs is NP hard; therefore, computationally efficient approximate methods have been the subject of study. Classic approaches include sampling methods, such as MCMC or particle filtering [Koller and Friedman, 2009] and variational Bayes methods [Wainwright and Jordan, 2007, Ghahramani and Jordan, 1997]. In practice, both methods are nontrivial to make work and we are not aware of any works that would have demonstrated good results in our application domain with the type of FHMMs we need to work and at practical scales. In this paper we follow the work of Kolter and Jaakkola [2012] to model the NILM problem by FHMMs. The distinguishing features of FHMMs in this setting are that (i) the output is the sum of the output of the underlying HMMs (perhaps with some noise), and (ii) the number of transitions are small in comparison to the signal length. FHMMs with the first property are called additive. In this paper we derive an efficient, convex relaxation based method for FHMMs of the above type, which significantly outperforms the state-of-the-art algorithms. Our approach is based on revisiting relaxations to the integer programming formulation of Kolter and Jaakkola [2012]. In particular, we replace the quadratic programming relaxation of Kolter and Jaakkola, 2012 with a relaxation to an semi-definite program (SDP), which, based on the literature of relaxations is expected to be tighter and thus better. While SDPs are convex and could in theory be solved using interior-point (IP) methods in polynomial time [Malick et al., 2009], IP scales poorly with the size of the problem and is thus unsuitable to our large scale problem which may involve as many a million variables. To address this problem, capitalizing on the structure of our relaxation coming from our FHMM model, we develop a novel variant of ADMM [Boyd et al., 2011] that uses Moreau-Yosida regularization and combine it with a version of randomized rounding that is inspired by the the recent work of Park and Boyd [2015]. Experiments on synthetic and real data confirm that our method significantly outperforms other algorithms from the literature, and we expect that it may find its applications in other FHMM inference problems, too.", "startOffset": 21, "endOffset": 2337}, {"referenceID": 0, "context": ", by Dong et al., 2012, 2013, Figueiredo et al., 2012). This observation was used by Kolter and Jaakkola [2012] to amend the posterior with a term that tries to match the large signal changes to the possible changes in the power level when only the state of a single appliance changes.", "startOffset": 5, "endOffset": 112}, {"referenceID": 3, "context": "[2012] (following Mattfeld, 2014) to determine the emission parameters. However, when it comes to the specific application of NILM, the problem of unknown, time-varying bias also needs to be addressed, which appears due to the presence of unknown/unmodeled appliances in the measured signal. A simple idea, which is also followed by Kolter and Jaakkola [2012], is to use a \u201cgeneric model\u201d whose contribution to the objective function is downweighted.", "startOffset": 18, "endOffset": 360}, {"referenceID": 3, "context": "[2012] (following Mattfeld, 2014) to determine the emission parameters. However, when it comes to the specific application of NILM, the problem of unknown, time-varying bias also needs to be addressed, which appears due to the presence of unknown/unmodeled appliances in the measured signal. A simple idea, which is also followed by Kolter and Jaakkola [2012], is to use a \u201cgeneric model\u201d whose contribution to the objective function is downweighted. Surprisingly, incorporating this idea in the FHMM inference creates some unexpected challenges.4 Therefore, in this work we come up with a practical, heuristic solution tailored to NILM. First we identify all electric events defined by a large change \u2206yt in the power usage (using some adhoc threshold). Then we discard all events that are similar to any possible level change \u2206\u03bc m,k. The remaining large jumps are regarded as coming from a generic HMM model describing the unregistered appliances: they are clustered into K \u2212 1 clusters, and an HMM model is built where each cluster is regarded as power usage coming from a single state of the unregistered appliances. We also allow an \u201coff state\u201d with power usage 0. We drop the subscript t and replace t+ 1 and t\u2212 1 with + and \u2212 signs, respectively. For example, the incorporation of this generic model breaks the derivation of the algorithm of Kolter and Jaakkola [2012]. See Appendix B for a discussion of this.", "startOffset": 18, "endOffset": 1376}, {"referenceID": 2, "context": "The performance of our algorithm is compared to the structured variational inference (SVI) method of Ghahramani and Jordan [1997], the method of Kolter and Jaakkola [2012] and that of Zhong et al.", "startOffset": 101, "endOffset": 130}, {"referenceID": 2, "context": "The performance of our algorithm is compared to the structured variational inference (SVI) method of Ghahramani and Jordan [1997], the method of Kolter and Jaakkola [2012] and that of Zhong et al.", "startOffset": 101, "endOffset": 172}, {"referenceID": 2, "context": "The performance of our algorithm is compared to the structured variational inference (SVI) method of Ghahramani and Jordan [1997], the method of Kolter and Jaakkola [2012] and that of Zhong et al. [2014]; we shall refer to the last two algorithms as KJ and ZGS, respectively.", "startOffset": 101, "endOffset": 204}], "year": 2016, "abstractText": "We develop a scalable, computationally efficient method for the task of energy disaggregation for home appliance monitoring. In this problem the goal is to estimate the energy consumption of each appliance over time based on the total energy-consumption signal of a household. The current state of the art is to model the problem as inference in factorial HMMs, and use quadratic programming to find an approximate solution to the resulting quadratic integer program. Here we take a more principled approach, better suited to integer programming problems, and find an approximate optimum by combining convex semidefinite relaxations randomized rounding, as well as a scalable ADMM method that exploits the special structure of the resulting semidefinite program. Simulation results both in synthetic and real-world datasets demonstrate the superiority of our method.", "creator": "LaTeX with hyperref package"}}}