{"id": "1610.09722", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2016", "title": "Represent, Aggregate, and Constrain: A Novel Architecture for Machine Reading from Noisy Sources", "abstract": "in order to create event information replacing text, the machine reading model must work to accurately read then interpret the ways in getting their trend is expressed. recurring problems further emerge, that the human reader must, aggregate numerous individual value hypotheses into a roughly coherent predict analysis, incorporating global simulations which reflect prior knowledge using entire domain.", "histories": [["v1", "Sun, 30 Oct 2016 22:33:47 GMT  (347kb,D)", "http://arxiv.org/abs/1610.09722v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jason naradowsky", "sebastian riedel"], "accepted": false, "id": "1610.09722"}, "pdf": {"name": "1610.09722.pdf", "metadata": {"source": "CRF", "title": "Represent, Aggregate, and Constrain: A Novel Architecture for Machine Reading from Noisy Sources", "authors": ["Jason Naradowsky", "Sebastian Riedel"], "emails": ["s.riedel}@cs.ucl.ac.uk"], "sections": [{"heading": null, "text": "In this work we focus on the task of extracting plane crash event information from clusters of related news articles whose labels are derived via distant supervision. Unlike previous machine reading work, we assume that while most target values will occur frequently in most clusters, they may also be missing or incorrect.\nWe introduce a novel neural architecture to explicitly model the noisy nature of the data and to deal with these aforementioned learning issues. Our models are trained end-to-end and achieve an improvement of more than 12.1 F1 over previous work, despite using far less linguistic annotation. We apply factor graph constraints to promote more coherent event analyses, with belief propagation inference formulated within the transitions of a recurrent neural network. We show this technique additionally improves maximum F1 by up to 2.8 points, resulting in a relative improvement of 50% over the previous state-of-the-art."}, {"heading": "1 Introduction", "text": "Recent work in the area of machine reading has focused on learning in a scenario with perfect in-\nformation. Whether identifying target entities for simple cloze style queries (Hermann et al., 2015; Miao et al., 2015), or reasoning over short passages of artificially generated text (Weston et al., 2015), short stories (Richardson et al., 2013), or children\u2019s stories (Hill et al., 2015), these systems all assume that the corresponding text is the unique source of information necessary for answering the query \u2013 one that not only contains the answer, but does not contain misleading or otherwise contradictory information.\nFor more practical question answering, where an information retrieval (IR) component must first fetch the set of relevant passages, the text sources will be less reliable and this assumption must be discarded. Text sources may vary in terms of their integrity (whether or not they are intentionally misleading or unreliable), their accuracy (as in the case of news events, where a truthful but outdated article may contain incorrect information), or their relevance to the query. These characteristics necessitate not only the creation of high-precision readers, but also the development of effective strategies for aggregating conflicting stories into a single cohesive account of the event.\nAdditionally, while many question answering systems are designed to extract a single answer to a single query, a user may wish to understand many aspects of a single entity or event. In machine reading, this is akin to pairing each text passage with multiple queries. Modeling each query as an independent prediction can lead to analyses that are incoherent, motivating the need to model the dependencies between queries.\nar X\niv :1\n61 0.\n09 72\n2v 1\n[ cs\n.C L\n] 3\n0 O\nct 2\n01 6\nAt least 28 dead as Flight-117 out of Chicago crashed with 63 passengers aboard.\nMarch 10th. ambient temp. 28 C. visibility nominal.\nFlight-117 crash. 22 dead.d1\nd2\nd3\nd4\nd5\nDisaster over Atlanta tonight as Flight-117 burst into flames, 28 confirmed dead.\nFlight-456 crashed outside of Dallas last year killing all 48 passengers on board.\nLabels: Fatalities: 28 Passengers: 63\nMention-level Scores (4 values per document, 5 documents)\nCluster-level Scores via Aggregation (4 values per cluster, 1 cluster)\ndocuments values\nsc or\nes\nsc or\nes\n\u201cFlight 117\u201d\nIR\nGold Data\nDistantly Supervised Data\nTarget Flight:\ncorrect otherkey:\nFigure 1: An example news cluster. While we assume all documents mention the target flight, inaccurate information (d1), incorrect labels (d2), and mentions of non-topical events (d5) are frequent sources of noise the model must deal with. Red tokens indicate mentions of values, i.e. candidate answers.\nWe study these problems through the development of a novel machine reading architecture, which we apply to the task of event extraction from news cluster data. We propose a modular architecture which decomposes the task into three fundamental sub-problems: (1) representation & scoring, (2) aggregation, and (3) global constraints. Each corresponds to an exchangeable component of our model. We explore a number of choices for these components, with our best configuration improving performance by 14.9 F1, a 50% relative improvement, over the previous state-of-the-art."}, {"heading": "1.1 The Case for Aggregation", "text": "Effective aggregation techniques can be crucial for identifying accurate information from noisy sources. Figure 1 depicts an example of our problem scenario. An IR component fetches sev-\neral documents based on the query, and sample sentences are shown for each document. The goal is to extract the correct value, of which there may be many mentions in the text, for each slot. Sentences in d1 express a target slot, the number of fatalities, but the mention corresponds to an incorrect value. This is a common mistake in early news reports. Documents d3 and d4 also express this slot, and with mentions of the correct value, but with less certainty.\nA model which focuses on a single highscoring mention, at the expense of breadth, will make an incorrect prediction. In comparison, a model which learns to correctly accumulate evidence for each value across multiple mentions over the entire cluster can identify the correct information, circumventing this problem. Figure 1 (bottom) shows how this pooling of evidence can produce the correct cluster-level prediction."}, {"heading": "2 Model", "text": "In this section we describe the three modeling components of our proposed architecture:\n1. Representation and Scoring, in which a task-specific encoding is generated for each mention, and scored with respect to each slot.\n2. Aggregation, in which the scores of each value\u2019s mentions are aggregated to produce a single score for each slot-value pair.\n3. Constraint, in which we model additional dependencies between pairs of values, and between pairs of slots, to promote more sensible interpretations of the event as a whole.\nWe begin by defining terminology. A news cluster c is a set of documents, {d1, ..., d|c|} \u2208 c, where each document is described by a sequence of words, d = (w1, ..., w|d|). A mention is an occurrence of a value in its textual context. For each value v \u2208 V , there are potentially many mentions of v in the cluster, defined as m \u2208 M(v). These have been annotated in the data using Stanford CoreNLP (Manning et al., 2014)."}, {"heading": "2.1 Representations and Scoring", "text": "For each mention m we construct a representation r(m) \u2208 Rr of the mention in its context. This representation functions as a general \u201creading\u201d or encoding of the mention, irrespective of the type of slots for which it will later be considered. This differs from some previous machine reading research where the model provides a query-specific reading of the document, or reads the document multiple times when answering a single query (Hermann et al., 2015). As in previous work, an embedding of a mention\u2019s context serves as its representation.\nWe construct an embedding matrix E \u2208 Re\u00d7n, using pre-trained word embeddings, where e is the dimensionality of the embeddings and n the number of words in the cluster. These are held fixed during training. All mentions are masked and receive the same one-hot vector in place of a pretrained embedding. From this matrix we embed the context using a two-layer convolutional\nneural network (CNN), with a detailed discussion of the architecture parameters provided in Section 4. CNNs have been used in a similar manner for a number of information extraction and classification tasks (Kim, 2014; Zeng et al., 2015) and are capable of producing rich sentence representations (Kalchbrenner et al., 2014).1"}, {"heading": "2.1.1 Scoring", "text": "Having produced a representation r(m) for each mention m, a slot-specific attention mechanism produces \u03c6mention(m, s) \u2208 R, representing the compatibility of mention m with slot s. Let R \u2208 Rn\u00d7r be the representation matrix composed of all r(m), and k(m) is the index of m into R. We create a separate embedding, \u03c0s \u2208 Rr, for each slot s, and utilize it to attend over all mentions in the cluster as follows:\nus = R\u03c0s (1) as = softmax(us) (2)\n\u03c6mention(m, s) = a s k(m) (3)\nThe mention-level scores reflect an interpretation of the value\u2019s encoding with respect to the slot. The softmax normalizes the scores over each slot, supplying the attention, and creating competition between mentions. This encourages the model to attend over mentions with the most characteristic contexts for each slot."}, {"heading": "2.2 Aggregating Mention-level Scores", "text": "For values mentioned repeatedly throughout the news cluster, mention scores must be aggregated to produce a single value-level score. In this section we describe (1) how the right aggregation method can better reflect how the gold labels are applied to the data, (2) how domain knowledge can be incorporated into aggregation, and (3) how aggregation can be used as a dynamic approach to identifying missing information."}, {"heading": "2.2.1 Aggregation as a Model of Distant Supervision", "text": "In the traditional view of distant supervision (Mintz et al., 2009), if a mention is found in an\n1We also experimented with sequential context embedding models but observed a negligible effect on performance when pursuing a 1-best decoding strategy.\nexternal knowledge base it is assumed that the mention is an expression of its role in the knowledge base, and it receives the corresponding label. This assumption does not always hold, and the resulting spurious labels are frequently cited as a source of training noise (Riedel et al., 2010; Hoffmann et al., 2011). However, an aggregation over all mention scores provides a more accurate reflection of how distant supervision labels are applied to the data.\nIf one were to assign a label to each mention and construct a loss using the mention-level scores (\u03c6mention) directly, it would recreate the hard labeling of the traditional distant supervision training scenario. Instead, we relax the distant supervision assumption by using a loss on the value-level scores (\u03c6value), with aggregation to pool beliefs from one to the other. This explicitly models the way in which cluster-wide labels are applied to mentions, and allows for spuriously labeled mentions to receive lower scores, \u201cexplaining away\u201d the cluster\u2019s label by assigning a higher score to a mention with a more suitable representation.\nTwo natural choices for this aggregation are max and sum. Formally, under max aggregation the value-level scores for a value v and slot s are computed as:\n\u03c6value(v, s) = max m\u2208M(v) \u03c6mention(m, s) (4)\nAnd under sum aggregation: \u03c6value(v, s) = \u2211\nm\u2208M(v)\n\u03c6mention(m, s) (5)\nIf the most clearly expressed mentions correspond to correct values, max aggregation can be an effective strategy (Riedel et al., 2010). If the data set is noisy with numerous spurious mentions, a sum aggregation favoring values which are expressed both clearly and frequently may be the more appropriate choice."}, {"heading": "2.2.2 Weighted Aggregation", "text": "The aforementioned aggregation methods combine mention-level scores uniformly, but for many domains one may have prior knowledge regarding which mentions should more heavily\ncontribute to the aggregate score. It is straightforward to augment the proposed aggregation methods with a separate weight \u03b1m for each mention m to create, for instance, a weighted sum aggregation:\n\u03c6value(v, s) = \u2211\nm\u2208M(v)\n\u03b1m \u00b7 \u03c6mention(m, s) (6)\nThese weights may be learned from data, or they may be heuristically defined based on a priori beliefs. Here we present two such heuristic methods.\nTopic-based Aggregation News articles naturally deviate from the topical event, often including comparisons to related events, and summaries of past incidents. Any such instance introduces additional noise into the system, as the contexts of topical and nontopical mention are often similar. Weighted aggregation provides a natural foothold for incorporating topicality into the model.\nWe assign aggregation weights heuristically with respect to a simple model of discourse. We assume every document begins on topic, and remains so until a sentence mentions a nontopical flight number. This and all successive sentences are considered nontopical, until a sentence reintroduces the topical flight. Mentions in topical sentences receive aggregation weights of \u03b1m = 1.0, and those in non-topical sentences receive weights of \u03b1m = 0.0, removing them from consideration completely.\nDate-based Aggregation In the aftermath of a momentous event, news outlets scramble to release articles, often at the expense of providing accurate information.\nWe hypothesize that the earliest articles in each cluster are the most likely to contain misinformation, which we explore via a measure of information content. We define the information content of an article as the number of correct values which it mentions. Using this measure, we fit a skewed Gaussian distribution over the ordered news articles, assigning \u03b1m = ic(d), \u2200m \u2208 d, where ic(d) is the smoothed information content of d as drawn from the Gaussian."}, {"heading": "2.2.3 Known Unknowns", "text": "A difficult machine reading problem unique to noisy text sources, where the correct values may not be present in the cluster, is determining whether to predict any value at all. A common solution for handling such missing values is the use of a threshold, below which the model returns null. However, even a separate threshold for each slot would not fully capture the nature of the problem.\nDetermining whether a value is missing is a trade-off between two factors: (1) how strongly the mention-level scores support a non-null answer, and (2) how much general information regarding that event and that slot is given. We incorporate both factors by extending the definition of R and its use in Eq. 1\u2013Eq. 3 to include not only mentions, but all words. Each nonmention word is treated as a mention of the null value:\n\u03c6value(v = null, s) = \u2211 d\u2208c \u2211 w\u2208{d\\M} \u03c6mention(w, s)\n(7)\nwhere M is the set of mentions. The resulting null score varies according to both the cluster size and its content. Smaller clusters with fewer documents require less evidence to predict a non-null value, while larger clusters must accumulate more evidence for a particular candidate or a null value will be proposed instead.\nThe exact words contained in the cluster also have an effect. For example, clusters with numerous mentions of killed, died, dead, will have a higher \u03c6value(v = null, s =Fatalities), requiring the model to be more confident in its answer for that slot during training. Additionally, this provides a mechanism for driving down \u03c6mention(w, s) when w is not strongly associated with s."}, {"heading": "2.3 Global Constraints", "text": "While the combination of learned representations and aggregation produces an effective system in its own right, its predictions may reflect a lack of world knowledge. For instance, we may want to discourage the model from predicting\nthe same value for multiple slots, as this is not a common occurrence.\nFollowing recent work in computer vision which proposes a differentiable interpretation of belief propagation inference (Ross et al., 2011; Zheng et al., 2015), we present a recurrent neural network (RNN) which implements inference under this constraint."}, {"heading": "2.3.1 Belief Propagation as an RNN", "text": "A factor graph is a graphical model which factorizes the model function using a bipartite graph, consisting of variables and factors. Variables maintain beliefs over their values, and factors specify scores over configurations of these values for the variables they neighbor.\nWe constrain model output by applying a factor graph model to the \u03c6value scores it produces. The slot s taking the value v is represented in the factor graph by a Boolean variable Xv,s. Each Xv,s is connected to a local factor uv,s whose initial potential is derived from \u03c6value(v, s). A combinatorial logic factor, Exactly-1(Smith and Eisner, 2008), is (1) created for each slot, connected across all values, and (2) created for each value, connected across all slots. This is illustrated in Figure 2. Each Exactly-1 factor provides a hard constraint over neighboring Boolean variables requiring exactly one variable\u2019s value to be true, therefore diminishing the possibility of duplicate predictions during inference.\nInference The resulting graph contains cycles, preventing the use of exact message passing inference. We instead treat an RNN as implementing loopy belief propagation (LBP), an iterative approximate message passing inference algorithm. The hidden state of the RNN is the set of variable beliefs, and each round of message updates corresponds to one iteration of LBP, or one recurrence in the RNN.\nThere are two types of messages: messages from variables to factors and messages from factors to variables. The message that a variable X sends to a factor f (denoted \u00b5X\u2192f ) is defined recursively w.r.t. to incoming messages from its neighbors n(X) as follows:\n\u00b5X\u2192f = \u220f\nf \u2032\u2208n(X)6=f\n\u00b5f \u2032\u2192X (8)\nand conveys the information \u201cMy other neighbors jointly suggest I have the posterior distribution \u00b5v\u2192u(v) over my values.\u201d In our RNN formulation of message passing the initial outgoing message for a variable Xv,s to its neighboring Exactly-1 factors is:\n\u00b5Xv,s\u2192f = sigmoid(\u03c6value(v, s)) (9)\nwhere the sigmoid moves the scores into probability space.\nA message from an Exactly-1 factor to its neighboring variables is calculated as:\n\u00ac\u00b5X\u2192f = 1.0\u2212 \u00b5X\u2192f (10) Z = \u220f X \u00ac\u00b5X\u2192f (11)\n\u00b5Exactly-1\u2192X = Z\n\u00ac\u00b5X\u2192f (12)\nAll subsequent LBP iterations compute variable messages as in Eq. 8, incorporating the out-going factor beliefs of the previous iteration."}, {"heading": "3 Data", "text": "The Stanford Plane Crash Dataset (Reschke et al., 2014) is a small data set consisting of 80 plane crash events, each paired with a set of related news articles. Of these events, 40 are reserved for training, and 40 for testing, with the\naverage cluster containing more than 2,000 mentions.2 Gold labels for each cluster are derived from Wikipedia infoboxes and cover up to 15 slots, of which 8 are used in evaluation (Figure 3).\nWe follow the same entity normalization procedure as Reschke et al. (2014), limit the cluster size to the first 200 documents, and further reduce the number of duplicate documents to prevent biases in aggregation. We partition out every fifth document from the training set to be used as development data, primarily for use in an early stopping criterion. We also construct additional clusters from the remaining training documents, and use this to increase the size of the development set."}, {"heading": "4 Experiments", "text": "In all experiments we train using adaptive online gradient updates (Adam, see Kingma and Ba (2014)). Model architecture and parameter values were tuned on the development set, and are as follows (chosen values in bold):\n\u2022 CNN layer 1 filter width: [3, 5, 8, 10]\n\u2022 CNN layer 2 filter width: [0, 3, 5, 8, 10]\n\u2022 CNN layer 1 dim: [5, 10, 15, 20]\n\u2022 CNN layer 2 dim: [0, 5, 10, 15, 20]\n2Although it should be noted that only 33 of the training clusters and just 27 of the test clusters contain documents from which to extract information.\n\u2022 max pooling [True, False]\n\u2022 learning rate: [0.001, 0.003, 0.005, 0.01]\n\u2022 L2 regularization: [0.001, 0.003, 0.005, 0.01]\n\u2022 dropout rate: 1-[0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\nThe number of training epochs is determined via early stopping with respect to the model performance on development data. The pre-trained word embeddings are 200-dimensional GLoVe embeddings (Pennington et al., 2014)."}, {"heading": "4.1 Systems", "text": "We evaluate on four categories of architecture:\nExisting baselines Reschke et al. (2014) proposed several methods for event extraction in this scenario. We compare against three notable examples drawn from this work:\n\u2022 Reschke CRF: a conditional random field model. \u2022 Reschke Noisy-OR: a sequence tagger with a \u201dNoisy-OR\u201d form of aggregation that discourages the model from predicting the same value for multiple slots. \u2022 Reschke Best: a sequence tagger using a cost-sensitive classifier, optimized with SEARN (Daume\u0301 III et al., 2009), a learningto-search framework.\nEach of these models uses features drawn from dependency trees, local context (unigram/partof-speech features for up to 5 neighboring words), sentence context (bag-of-word/part-ofspeech), words/part-of-speech of words occurring within the value, as well as the entity type of the mention itself.\nMention-CNN The representation and scoring components of our architecture, with an additional slot for predicting a null value. The \u03c6mention(m, s) scores are used when constructing the loss and during decoding. These scores can also be aggregated in a max/sum manner after decoding, but such aggregation is not incorporated during training.3\n3These models benefit from vastly different training parameters and were trained for 250 iterations with a dropout rate of 0.3.\nRAC-CNN Representation, scoring, and aggregation components, trained end-to-end with a cluster-level loss. Null values are predicted as described in Sec. 2.2.3.\nEE-AS Reader Kadlec et al. (2016) present AS Reader, a state-of-the-art model for clozestyle QA. Like our architecture, AS Reader aggregates mention-level scores, pooling evidence for each answer candidate. However, in clozestyle QA an entity is often mentioned in complementary contexts throughout the text, but are frequently in similar contexts in news cluster extraction.\nWe tailor AS Reader to event extraction to illustrate the importance of choosing an aggregation which reflects how the gold labels are applied to the data. EE-AS Reader is implemented by applying Eq. 1 and Eq. 2 to each document, as opposed to clusters, as documents are analogous to sentences in the cloze-style QA task. We then concatenate the resulting vectors, and apply sum aggregation as before."}, {"heading": "4.2 Evaluation", "text": "We evaluate configurations of our proposed architecture across three measures. The first is a modified version of standard precision, recall, and F1, as proposed by Reschke et al. (2014). It deviates from the standard protocol by (1) awarding full recall for any slot when a single predicted value is contained in the gold slot, (2) only penalizing slots for which there are findable gold values in the text, and (3) limiting candidate values to the set of entities proposed by the Stanford NER system and included in the data set release. Eight of the fifteen slots are used in evaluation. Similarly, the second evaluation measure we present is standard precision, recall, and F1, specifically for null values.\nWe also evaluate the models using mean reciprocal rank (MRR). When calculating the F1based evaluation measure we decode the model by taking the single highest-scoring value for each slot. However, this does not necessarily reflect the quality of the overall value ranking produced by the model. For this reason we in-\nclude MRR, defined as:\nMRR = 1\n|Q| |Q|\u2211 i=1 1 ranki (13)\nwhere ranki is the rank position of the first correct value for a given cluster and slot pair i, and |Q|, the number of such pairs, is |C| \u00b7 |S|, the product of the total number of clusters with the total number of predicted slots."}, {"heading": "4.3 Results", "text": "Results are presented in Table 1. In comparison to previous work, our any configuration of our RAC architecture with sum-based aggregation outperforms the best existing systems by a minimum of 9.8 F1. In comparison to the various Mention-CNN systems, it is clear that this improvement is not a result of different features or the use of pre-trained word embeddings, or even the representational power of the CNNbased embeddings. Rather, it is attributable to training end-to-end with aggregation and a cluster-level loss function.\nAggregation Results With respect to aggregation, sum-based methods consistently outperform their max counterparts, indicating that exploiting the redundancy of information in news clusters is beneficial to the task. The topicbased aggregation is statistically significant im-\nprovement over standard sum aggregation (p \u2264 0.0215), and produces the highest performing unconstrained system.\nDate-based aggregation did not yield a statistically significant improvement over sum aggregation. We hypothesize that the method is sound, but accurate datelines could only be extracted for 31 % documents. We did not modify the aggregation weights (\u03b1(m) = 1.0) for the remaining documents, minimizing the effect of this approach.\nThe EE-AS Reader has the lowest overall performance, which one can attribute to pooling evidence in a manner that is poorly suited to this problem domain. By placing a softmax over each document\u2019s beliefs, what is an advantage in the cloze-style QA setting here becomes a liability: the model is forced to predict a value for every slot, for every each document, even when few are truly mentioned."}, {"heading": "4.4 Effects of Global Constraints", "text": "In Table 2 we show the results of incorporating factor graph constraints into our bestperforming system. Performing one iteration of LBP inference produces our highest performance, an F1 of 44.9. This is 14.9 points higher than Reschke\u2019s best system, and a statistically significant improvement over the unconstrained model (p \u2264 0.0313). The improvements persist\nthroughout training, as shown in Figure 3.\nAdditional iterations reduce performance. This effect is largely due to the constraint assumption not holding absolutely in the data. For instance, multiple slots can have the null value, and zero is common value for a number of slots. Running the constraint inference for a single iteration encourages a 1-to-1 mapping from values to slots, but it does not prohibit it. This result also implies that a hard heuristic decoding constraint time would not be as effective."}, {"heading": "4.5 Error Analysis", "text": "We randomly selected 15 development set instances which our best model predicts incorrectly. Of these, we find three were incorrectly labeled in the gold data as errors from the distance supervision hypothesis (i.e., \u201czero chance\u201d being labeled for 0 survivors, when the number of survivors was not mentioned in the cluster), and should not be predicted.\nSix were clearly expressed and should be predictable, with highly correlated keywords present in the context window, but were assigned low scores by the model. We belief a richer representation which combines the generalization of CNNs with the discrete signal of ngram features (Lei et al., 2015) may solve some of these issues.\nFour of the remaining errors appear to be due to aggregation errors. Specifically, the occurrence of a particular punctuation mark with far greater than average frequency resulted in it being predicted for three slots. While these could be filtered out, a more general solution may be to build a representation based on the actual mention (\u201cRyanair\u201d), in addition to its context. This may reduce the scores of these mentions\nto such an extent that they are removed from consideration.\nTable 3 shows the accuracy of the model on each slot type. The model is struggles with predicting the Injuries and Survivors slots. The nature of news media leads these slots to be discussed less frequently, with their mentions often embedded more deeply in the document, or expressed textually. For instance, it is common to express s=Survivors, v = 0 as \u201cno survivors\u201d, but it is impossible to predict a 0 value in this case, under the current problem definition."}, {"heading": "5 Related Work", "text": "Multi-document and Paraphrase-driven IE Our work is thematically similar to work in multi-document information extraction (Mann and Yarowsky, 2005) and summarization (Barzilay et al., 1999), where the content of many input documents is unified into a cohesive understanding. However, in addition to the many modeling choices we propose, the data sets used in existing work were not linked to specific events. The same denoising nature of these tasks has clear implications for automated factchecking (Vlachos and Riedel, 2014), but no comparable models currently exist.\nIn contrast, the the IDEST system of Krause et al. (2015) is an example of previous work which uses automatically constructed clusters of news articles in order to train an event embedding model. However, the focus of IDEST is to improve event clustering, not information\nextraction, which is reflected in its comparatively simple and heuristically-driven extraction method.\nAttention and Aggregation in Machine Reading In terms of reading methodology, our scoring method is a slot-specific interpretation of the attentive reader (Hermann et al., 2015), and our sum aggregation is closelyrelated to Kadlec et al. (2016), with differences described previously in Sec. 4.1. A similar method is found in the entailment model of Parikh et al. (2016), where alignment scores (between a premise and a hypothesis) are generated via attention and summed. Recent machine reading models have used an iterative attention to refine model predictions (Sordoni et al., 2016). Such methods play a role similar to our factor graph constraint, though they incorporate no prior knowledge.\nExtensions to Distant Supervision Aggregation in our framework is a means to weaken strong distant supervision assumptions, and, unlike Mintz et al. (2009), it does not assume that each mention-level occurrence of a value must express the given relation. In this respect, it exists as a fully-differentiable analog to the work of Hoffmann et al. (2011), and closely related to the \u201cexpressed at least once\u201d constraint of Riedel et al. (2010). Both allow the model to ignore mislabeled instances in certain circumstances. Recently, Lin et al. (2016) have also proposed the use of neural mechanisms to reduce the effect of mislabeled instances, using attention to select the most useful sentences for\nrelation extraction, as we use attention to select the most informative mentions."}, {"heading": "5.1 Connections to Pointer Networks", "text": "A pointer network uses a softmax to normalize a vector the size of the input, to create an output distribution over the dictionary of inputs (Vinyals et al., 2015). This assumes that the input vector is the size of the dictionary, and that each occurrence is scored independently of others. If an element appears repeatedly throughout the input, each occurrence is in competition not only with other elements, but also with its duplicates.\nHere the scoring and aggregation steps of our proposed architecture can together be viewed as a pointer network where there is a redundancy in the input which respects an underlying grouping. Here the softmax normalizes the scores over the input vector, and the aggregation step again yields an output distribution over the dictionary of the input."}, {"heading": "6 Conclusion and Future Work", "text": "In this work we present a machine reading architecture designed to effectively read collections of documents in noisy, less controlled scenarios where information may be missing or inaccurate. Through attention-based mention scoring, cluster-wide aggregation of these scores, and global constraints to discourage unlikely solutions, we improve upon the state-of-the-art on this task by 14.9 F1.\nIn future work, the groundwork laid here may be applied to larger data sets, and may help motivate the development of such data. Larger noisy data sets would enable the differentiable constraints and weighted aggregation to be included during the optimization, and tuned with respect to data. In addition, we find the incorporation of graphical model inference into neural architectures to be a powerful new tool, and potentially an important step towards incorporating higher-level reasoning and prior knowledge into neural models of NLP."}], "references": [{"title": "Information fusion in the context of multi-document summarization", "author": ["Kathleen R. McKeown", "Michael Elhadad"], "venue": "In Proceedings of the 37th Annual Meeting of the Association", "citeRegEx": "Barzilay et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 1999}, {"title": "Search-based structured prediction. Machine Learning Journal (MLJ)", "author": ["John Langford", "Daniel Marcu"], "venue": null, "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Teaching machines to read and comprehend", "author": ["Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": null, "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations. CoRR", "author": ["Hill et al.2015] Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S. Weld"], "venue": "In Proceedings of the 49th Annual Meet-", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Text understanding with the attention sum reader network. In Association for Computational Linguistics (ACL)", "author": ["Kadlec et al.2016] Rudolf Kadlec", "Martin Schmid", "Ond\u0159ej Bajgar", "Jan Kleindienst"], "venue": null, "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. CoRR, abs/1412.6980", "author": ["Kingma", "Ba2014] Diederik P. Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Idest: Learning a distributed representation for event patterns", "author": ["Enrique Alfonseca", "Katja Filippova", "Daniele Pighin"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the As-", "citeRegEx": "Krause et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2015}, {"title": "Molding cnns for text: non-linear, non-consecutive convolutions", "author": ["Lei et al.2015] Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Neural relation extraction with selective attention over instances", "author": ["Lin et al.2016] Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Multi-field information extraction and cross-document fusion", "author": ["Mann", "Yarowsky2005] Gideon S. Mann", "David Yarowsky"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Mann et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2005}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Neural variational inference for text processing. CoRR, abs/1511.06038", "author": ["Miao et al.2015] Yishu Miao", "Lei Yu", "Phil Blunsom"], "venue": null, "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th Interna-", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "A decomposable attention model for natural language inference", "author": ["Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit"], "venue": null, "citeRegEx": "Parikh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Event extraction using distant supervision", "author": ["Martin Jankowiak", "Mihai Surdeanu", "Christopher D. Manning", "Dan Jurafsky"], "venue": "In Proceedings of the 9th edition of the Language Resources", "citeRegEx": "Reschke et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reschke et al\\.", "year": 2014}, {"title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Richardson", "Christopher J.C. Burges", "Erin Renshaw."], "venue": "Proceedings of the 2013 Conference on Empirical", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases:", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Learning message-passing inference machines for structured prediction", "author": ["Ross et al.2011] Stephane Ross", "Daniel Munoz", "Martial Hebert", "J. Andrew Bagnell"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Dependency parsing by belief propagation", "author": ["Smith", "Eisner2008] David Smith", "Jason Eisner"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Smith et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2008}, {"title": "Iterative alternating neural attention for machine", "author": ["Phillip Bachman", "Yoshua Bengio"], "venue": "reading. CoRR,", "citeRegEx": "Sordoni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "Fact checking: Task definition and dataset construction", "author": ["Vlachos", "Riedel2014] Andreas Vlachos", "Sebastian Riedel"], "venue": "In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science,", "citeRegEx": "Vlachos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vlachos et al\\.", "year": 2014}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks. CoRR, abs/1502.05698", "author": ["Weston et al.2015] Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Zeng et al.2015] Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Zeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}, {"title": "Conditional random fields as recurrent neural networks", "author": ["Zheng et al.2015] Shuai Zheng", "Sadeep Jayasumana", "Bernardino Romera-Paredes", "Vibhav Vineet", "Zhizhong Su", "Dalong Du", "Chang Huang", "Philip Torr"], "venue": null, "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Whether identifying target entities for simple cloze style queries (Hermann et al., 2015; Miao et al., 2015), or reasoning over short passages of artificially generated text (Weston et al.", "startOffset": 67, "endOffset": 108}, {"referenceID": 14, "context": "Whether identifying target entities for simple cloze style queries (Hermann et al., 2015; Miao et al., 2015), or reasoning over short passages of artificially generated text (Weston et al.", "startOffset": 67, "endOffset": 108}, {"referenceID": 25, "context": ", 2015), or reasoning over short passages of artificially generated text (Weston et al., 2015), short stories (Richardson et al.", "startOffset": 73, "endOffset": 94}, {"referenceID": 19, "context": ", 2015), short stories (Richardson et al., 2013), or children\u2019s stories (Hill et al.", "startOffset": 23, "endOffset": 48}, {"referenceID": 3, "context": ", 2013), or children\u2019s stories (Hill et al., 2015), these systems all assume that the corresponding text is the unique source of information necessary for answering the query \u2013 one that not only contains the answer, but does not contain misleading or otherwise contradictory information.", "startOffset": 31, "endOffset": 50}, {"referenceID": 13, "context": "These have been annotated in the data using Stanford CoreNLP (Manning et al., 2014).", "startOffset": 61, "endOffset": 83}, {"referenceID": 2, "context": "This differs from some previous machine reading research where the model provides a query-specific reading of the document, or reads the document multiple times when answering a single query (Hermann et al., 2015).", "startOffset": 191, "endOffset": 213}, {"referenceID": 7, "context": "CNNs have been used in a similar manner for a number of information extraction and classification tasks (Kim, 2014; Zeng et al., 2015) and are capable of producing rich sentence representations (Kalchbrenner et al.", "startOffset": 104, "endOffset": 134}, {"referenceID": 26, "context": "CNNs have been used in a similar manner for a number of information extraction and classification tasks (Kim, 2014; Zeng et al., 2015) and are capable of producing rich sentence representations (Kalchbrenner et al.", "startOffset": 104, "endOffset": 134}, {"referenceID": 6, "context": ", 2015) and are capable of producing rich sentence representations (Kalchbrenner et al., 2014).", "startOffset": 67, "endOffset": 94}, {"referenceID": 15, "context": "In the traditional view of distant supervision (Mintz et al., 2009), if a mention is found in an", "startOffset": 47, "endOffset": 67}, {"referenceID": 20, "context": "This assumption does not always hold, and the resulting spurious labels are frequently cited as a source of training noise (Riedel et al., 2010; Hoffmann et al., 2011).", "startOffset": 123, "endOffset": 167}, {"referenceID": 4, "context": "This assumption does not always hold, and the resulting spurious labels are frequently cited as a source of training noise (Riedel et al., 2010; Hoffmann et al., 2011).", "startOffset": 123, "endOffset": 167}, {"referenceID": 20, "context": "If the most clearly expressed mentions correspond to correct values, max aggregation can be an effective strategy (Riedel et al., 2010).", "startOffset": 114, "endOffset": 135}, {"referenceID": 21, "context": "Following recent work in computer vision which proposes a differentiable interpretation of belief propagation inference (Ross et al., 2011; Zheng et al., 2015), we present a recurrent neural network (RNN) which implements inference under this constraint.", "startOffset": 120, "endOffset": 159}, {"referenceID": 27, "context": "Following recent work in computer vision which proposes a differentiable interpretation of belief propagation inference (Ross et al., 2011; Zheng et al., 2015), we present a recurrent neural network (RNN) which implements inference under this constraint.", "startOffset": 120, "endOffset": 159}, {"referenceID": 18, "context": "The Stanford Plane Crash Dataset (Reschke et al., 2014) is a small data set consisting of 80 plane crash events, each paired with a set of related news articles.", "startOffset": 33, "endOffset": 55}, {"referenceID": 18, "context": "We follow the same entity normalization procedure as Reschke et al. (2014), limit the cluster size to the first 200 documents, and further reduce the number of duplicate documents to prevent biases in aggregation.", "startOffset": 53, "endOffset": 75}, {"referenceID": 17, "context": "The pre-trained word embeddings are 200-dimensional GLoVe embeddings (Pennington et al., 2014).", "startOffset": 69, "endOffset": 94}, {"referenceID": 18, "context": "Existing baselines Reschke et al. (2014) proposed several methods for event extraction in this scenario.", "startOffset": 19, "endOffset": 41}, {"referenceID": 5, "context": "EE-AS Reader Kadlec et al. (2016) present AS Reader, a state-of-the-art model for clozestyle QA.", "startOffset": 13, "endOffset": 34}, {"referenceID": 18, "context": "The first is a modified version of standard precision, recall, and F1, as proposed by Reschke et al. (2014). It deviates from the standard protocol by (1) awarding full recall for any slot when a single predicted value is contained in the gold slot, (2) only penalizing slots for which there are findable gold values in the text, and (3) limiting candidate values to the set of entities proposed by the Stanford NER system and included in the data set release.", "startOffset": 86, "endOffset": 108}, {"referenceID": 10, "context": "We belief a richer representation which combines the generalization of CNNs with the discrete signal of ngram features (Lei et al., 2015) may solve some of these issues.", "startOffset": 119, "endOffset": 137}, {"referenceID": 0, "context": "Multi-document and Paraphrase-driven IE Our work is thematically similar to work in multi-document information extraction (Mann and Yarowsky, 2005) and summarization (Barzilay et al., 1999), where the content of many input documents is unified into a cohesive understanding.", "startOffset": 166, "endOffset": 189}, {"referenceID": 9, "context": "In contrast, the the IDEST system of Krause et al. (2015) is an example of previous work which uses automatically constructed clusters of news articles in order to train an event embedding model.", "startOffset": 37, "endOffset": 58}, {"referenceID": 2, "context": "Attention and Aggregation in Machine Reading In terms of reading methodology, our scoring method is a slot-specific interpretation of the attentive reader (Hermann et al., 2015), and our sum aggregation is closelyrelated to Kadlec et al.", "startOffset": 155, "endOffset": 177}, {"referenceID": 23, "context": "Recent machine reading models have used an iterative attention to refine model predictions (Sordoni et al., 2016).", "startOffset": 91, "endOffset": 113}, {"referenceID": 2, "context": "Attention and Aggregation in Machine Reading In terms of reading methodology, our scoring method is a slot-specific interpretation of the attentive reader (Hermann et al., 2015), and our sum aggregation is closelyrelated to Kadlec et al. (2016), with differences described previously in Sec.", "startOffset": 156, "endOffset": 245}, {"referenceID": 2, "context": "Attention and Aggregation in Machine Reading In terms of reading methodology, our scoring method is a slot-specific interpretation of the attentive reader (Hermann et al., 2015), and our sum aggregation is closelyrelated to Kadlec et al. (2016), with differences described previously in Sec. 4.1. A similar method is found in the entailment model of Parikh et al. (2016), where alignment scores (between a premise and a hypothesis) are generated via attention and summed.", "startOffset": 156, "endOffset": 371}, {"referenceID": 13, "context": "Extensions to Distant Supervision Aggregation in our framework is a means to weaken strong distant supervision assumptions, and, unlike Mintz et al. (2009), it does not assume that each mention-level occurrence of a value must express the given relation.", "startOffset": 136, "endOffset": 156}, {"referenceID": 4, "context": "In this respect, it exists as a fully-differentiable analog to the work of Hoffmann et al. (2011), and closely related to the \u201cexpressed at least once\u201d constraint of Riedel et al.", "startOffset": 75, "endOffset": 98}, {"referenceID": 4, "context": "In this respect, it exists as a fully-differentiable analog to the work of Hoffmann et al. (2011), and closely related to the \u201cexpressed at least once\u201d constraint of Riedel et al. (2010). Both allow the model to ignore mislabeled instances in certain circumstances.", "startOffset": 75, "endOffset": 187}, {"referenceID": 4, "context": "In this respect, it exists as a fully-differentiable analog to the work of Hoffmann et al. (2011), and closely related to the \u201cexpressed at least once\u201d constraint of Riedel et al. (2010). Both allow the model to ignore mislabeled instances in certain circumstances. Recently, Lin et al. (2016) have also proposed the use of neural mechanisms to reduce the effect of mislabeled instances, using attention to select the most useful sentences for relation extraction, as we use attention to select the most informative mentions.", "startOffset": 75, "endOffset": 294}], "year": 2016, "abstractText": "In order to extract event information from text, a machine reading model must learn to accurately read and interpret the ways in which that information is expressed. But it must also, as the human reader must, aggregate numerous individual value hypotheses into a single coherent global analysis, applying global constraints which reflect prior knowledge of the domain. In this work we focus on the task of extracting plane crash event information from clusters of related news articles whose labels are derived via distant supervision. Unlike previous machine reading work, we assume that while most target values will occur frequently in most clusters, they may also be missing or incorrect. We introduce a novel neural architecture to explicitly model the noisy nature of the data and to deal with these aforementioned learning issues. Our models are trained end-to-end and achieve an improvement of more than 12.1 F1 over previous work, despite using far less linguistic annotation. We apply factor graph constraints to promote more coherent event analyses, with belief propagation inference formulated within the transitions of a recurrent neural network. We show this technique additionally improves maximum F1 by up to 2.8 points, resulting in a relative improvement of 50% over the previous state-of-the-art.", "creator": "LaTeX with hyperref package"}}}