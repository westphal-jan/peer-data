{"id": "1610.09730", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2016", "title": "Active Learning from Imperfect Labelers", "abstract": "we study active choice algorithm generic database shouldn not create query incorrect labels : also trigger a labeling. we consider different noise and abstention conditions of 1 problem. one construct an analysis commonly utilizes abstention responses, and analyze its statistical consistency limiting query complexity under increasingly natural assumptions varying the noise and reward rate of chosen labeler. this algorithm is adaptive in a task allowing developers ought automatically return less variability elsewhere or more appropriate or less noisy labeler. you couple our optimization with adequate bounds to show that under different technical conditions, it achieves nearly optimal query capability.", "histories": [["v1", "Sun, 30 Oct 2016 23:39:18 GMT  (172kb)", "http://arxiv.org/abs/1610.09730v1", "To appear in NIPS 2016"]], "COMMENTS": "To appear in NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["songbai yan", "kamalika chaudhuri", "tara javidi"], "accepted": true, "id": "1610.09730"}, "pdf": {"name": "1610.09730.pdf", "metadata": {"source": "CRF", "title": "Active Learning from Imperfect Labelers", "authors": ["Songbai Yan"], "emails": ["yansongbai@eng.ucsd.edu", "kamalika@cs.ucsd.edu", "tjavidi@eng.ucsd.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n09 73\n0v 1\n[ cs\n.L G\n] 3\n0 O"}, {"heading": "1 Introduction", "text": "In active learning, the learner is given an input space X , a label space L, and a hypothesis class H such that one of the hypotheses in the class generates ground truth labels. Additionally, the learner has at its disposal a labeler to which it can pose interactive queries about the labels of examples in the input space. Note that the labeler may output a noisy version of the ground truth label (a flipped label). The goal of the learner is to learn a hypothesis in H which is close to the hypothesis that generates the ground truth labels.\nThere has been a significant amount of literature on active learning, both theoretical and practical. Previous theoretical work on active learning has mostly focused on the above basic setting [2, 4, 7, 10, 25] and has developed algorithms under a number of different models of label noise. A handful of exceptions include [3] which allows class conditional queries, [5] which allows requesting counterexamples to current version spaces, and [23, 26] where the learner has access to a strong labeler and one or more weak labelers.\nIn this paper, we consider a more general setting where, in addition to providing a possibly noisy label, the labeler can sometimes abstain from labeling. This scenario arises naturally in difficult labeling tasks and has been considered in computer vision by [11, 15]. Our goal in this paper is to investigate this problem from a foundational perspective, and explore what kind of conditions are needed, and how an abstaining labeler can affect properties such as consistency and query complexity of active learning algorithms.\nThe setting of active learning with an abstaining noisy labeler was first considered by [24], who looked at learning binary threshold classifiers based on queries to an labeler whose abstention rate is higher closer to the decision boundary. They primarily looked at the case when the abstention rate at a distance \u2206 from the decision boundary is less than 1\u2212\u0398(\u2206\u03b1), and the rate of label flips at the same distance is less than 12 \u2212 \u0398(\u2206\u03b2); under these conditions, they provided an active learning algorithm that given parameters \u03b1 and \u03b2, outputs a classifier with error \u01eb using O\u0303(\u01eb\u2212\u03b1\u22122\u03b2) queries\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nto the labeler. However, there are several limitations to this work. The primary limitation is that parameters \u03b1 and \u03b2 need to be known to the algorithm, which is not usually the case in practice. A second major limitation is that even if the labeler has nice properties, such as, the abstention rates increase sharply close to the boundary, their algorithm is unable to exploit these properties to reduce the number of queries. A third and final limitation is that their analysis only applies to one dimensional thresholds, and not to more general decision boundaries.\nIn this work, we provide an algorithm which is able to exploit nice properties of the labeler. Our algorithm is statistically consistent under very mild conditions \u2014 when the abstention rate is nondecreasing as we get closer to the decision boundary. Under slightly stronger conditions as in [24], our algorithm has the same query complexity. However, if the abstention rate of the labeler increases strictly monotonically close to the decision boundary, then our algorithm adapts and does substantially better. It simply exploits the increasing abstention rate close to the decision boundary, and does not even have to rely on the noisy labels! Specifically, when applied to the case where the noise rate is at most 12\u2212\u0398(\u2206\u03b2) and the abstention rate is 1\u2212\u0398(\u2206\u03b1) at distance \u2206 from the decision boundary, our algorithm can output a classifier with error \u01eb based on only O\u0303(\u01eb\u2212\u03b1) queries.\nAn important property of our algorithm is that the improvement of query complexity is achieved in a completely adaptive manner; unlike previous work [24], our algorithm needs no information whatsoever on the abstention rates or rates of label noise. Thus our result also strengthens existing results on active learning from (non-abstaining) noisy labelers by providing an adaptive algorithm that achieves that same performance as [6] without knowledge of noise parameters.\nWe extend our algorithm so that it applies to any smooth d-dimensional decision boundary in a nonparametric setting, not just one-dimensional thresholds, and we complement it with lower bounds on the number of queries that need to be made to any labeler. Our lower bounds generalize the lower bounds in [24], and shows that our upper bounds are nearly optimal. We also present an example that shows that at least a relaxed version of the monotonicity property is necessary to achieve this performance gain; if the abstention rate plateaus around the decision boundary, then our algorithm needs to query and rely on the noisy labels (resulting in higher query complexity) in order to find a hypothesis close to the one generating the ground truth labels."}, {"heading": "1.1 Related work", "text": "There has been a considerable amount of work on active learning, most of which involves labelers that are not allowed to abstain. Theoretical work on this topic largely falls under two categories \u2014 the membership query model [6, 13, 18, 19], where the learner can request label of any example in the instance space, and the PAC model, where the learner is given a large set of unlabeled examples from an underlying unlabeled data distribution, and can request labels of a subset of these examples. Our work and also that of [24] builds on the membership query model.\nThere has also been a lot of work on active learning under different noise models. The problem is relatively easy when the labeler always provides the ground truth labels \u2013 see [8, 9, 12] for work in this setting in the PAC model, and [13] for the membership query model. Perhaps the simplest setting of label noise is random classification noise, where each label is flipped with a probability that is independent of the unlabeled instance. [14] shows how to address this kind of noise in the PAC model by repeatedly querying an example until the learner is confident of its label; [18, 19] provide more sophisticated algorithms with better query complexities in the membership query model. A second setting is when the noise rate increases closer to the decision boundary; this setting has been studied under the membership query model by [6] and in the PAC model by [10, 4, 25]. A final setting is agnostic PAC learning \u2014 when a fixed but arbitrary fraction of labels may disagree with the label assigned by the optimal hypothesis in the hypothesis class. Active learning is known to be particularly difficult in this setting; however, algorithms and associated label complexity bounds have been provided by [1, 2, 4, 10, 12, 25] among others.\nOur work expands on the membership query model, and our abstention and noise models are related to a variant of the Tsybakov noise condition. A setting similar to ours was considered by [6, 24]. [6] considers a non-abstaining labeler, and provides a near-optimal binary search style active learning algorithm; however, their algorithm is non-adaptive. [24] gives a nearly matching lower and upper query complexity bounds for active learning with abstention feedback, but they only give a nonadaptive algorithm for learning one dimensional thresholds, and only study the situation where the\nabstention rate is upper-bounded by a polynomial function. Besides [24] , [11, 15] study active learning with abstention feedback in computer vision applications. However, these works are based on heuristics and do not provide any theoretical guarantees."}, {"heading": "2 Settings", "text": "Notation. 1 [A] is the indicator function: 1 [A] = 1 if A is true, and 0 otherwise. For x = (x1, . . . , xd) \u2208 Rd (d > 1), denote (x1, . . . , xd\u22121) by x\u0303. Define lnx = loge x, log x = log 43 x, [ln ln]+ (x) = ln lnmax{x, ee}. We use O\u0303 and \u0398\u0303 to hide logarithmic factors in 1\u01eb , 1\u03b4 , and d. Definition. Suppose \u03b3 \u2265 1. A function g : [0, 1]d\u22121 \u2192 R is (K, \u03b3)-H\u00f6lder smooth, if it is continuously differentiable up to \u230a\u03b3\u230b-th order, and for any x,y \u2208 [0, 1]d\u22121, \u2223 \u2223\n\u2223g(y)\u2212 \u2211\u230a\u03b3\u230b m=0 \u2202mg(x) m! (y \u2212 x)m \u2223 \u2223 \u2223 \u2264 K \u2016y \u2212 x\u2016\u03b3 . We denote this class of functions by \u03a3(K, \u03b3).\nWe consider active learning for binary classification. We are given an instance space X = [0, 1]d and a label space L = {0, 1}. Each instance x \u2208 X is assigned to a label l \u2208 {0, 1} by an underlying function h\u2217 : X \u2192 {0, 1} unknown to the learning algorithm in a hypothesis space H of interest. The learning algorithm has access to any x \u2208 X , but no access to their labels. Instead, it can only obtain label information through interactions with a labeler, whose relation to h\u2217 is to be specified later. The objective of the algorithm is to sequentially select the instances to query for label information and output a classifier h\u0302 that is close to h\u2217 while making as few queries as possible.\nWe consider a non-parametric setting as in [6, 17] where the hypothesis space is the smooth boundary fragment class H = {hg(x) = 1 [xd > g(x\u0303)] | g : [0, 1]d\u22121 \u2192 [0, 1] is (K, \u03b3)-H\u00f6lder smooth}. In other words, the decision boundaries of classifiers in this class are epigraph of smooth functions (see Figure 1 for example). We assume h\u2217(x) = 1 [xd > g\u2217(x\u0303)] \u2208 H. When d = 1, H reduces to the space of threshold functions {h\u03b8(x) = 1 [x > \u03b8] : \u03b8 \u2208 [0, 1]}. The performance of a classifier h(x) = 1 [xd > g(x\u0303)] is evaluated by the L1 distance between the decision boundaries \u2016g \u2212 g\u2217\u2016 = \u00b4\n[0,1]d\u22121 |g(x\u0303)\u2212 g\u2217(x\u0303)| dx\u0303.\nThe learning algorithm can only obtain label information by querying a labeler who is allowed to abstain from labeling or return an incorrect label (flipping between 0 and 1). For each query x \u2208 [0, 1]d, the labeler L will return y \u2208 Y = {0, 1,\u22a5} (\u22a5 means that the labeler abstains from providing a 0/1 label) according to some distribution PL(Y = y | X = x). When it is clear from the context, we will drop the subscript from PL(Y | X). Note that while the labeler can declare its indecision by outputting \u22a5, we do not allow classifiers in our hypothesis space to output \u22a5. In our active learning setting, our goal is to output a boundary g that is close to g\u2217 while making as few interactive queries to the labeler as possible. In particular, we want to find an algorithm with low query complexity \u039b(\u01eb, \u03b4,A, L, g\u2217), which is defined as the minimum number of queries that Algorithm A, acting on samples with ground truth g\u2217, should make to a labeler L to ensure that the output classifier hg(x) = 1 [xd > g(x\u0303)] has the property \u2016g \u2212 g\u2217\u2016 = \u00b4 [0,1]d\u22121 |g(x\u0303)\u2212 g\u2217(x\u0303)| dx\u0303 \u2264 \u01eb with probability at least 1\u2212 \u03b4 over the responses of L."}, {"heading": "2.1 Conditions", "text": "We now introduce three conditions on the response of the labeler with increasing strictness. Later we will provide an algorithm whose query complexity improves with increasing strictness of conditions. Condition 1. The response distribution of the labeler P (Y | X) satisfies:\n\u2022 (abstention) For any x\u0303 \u2208 [0, 1]d\u22121, xd, x\u2032d \u2208 [0, 1], if |xd \u2212 g\u2217(x\u0303)| \u2265 |x\u2032d \u2212 g\u2217(x\u0303)| then P (\u22a5| (x\u0303, xd)) \u2264 P (\u22a5| (x\u0303, x\u2032d));\n\u2022 (noise) For any x \u2208 [0, 1]d, P (Y 6= 1 [xd > g\u2217(x\u0303)] | x, Y 6=\u22a5) \u2264 12 .\nCondition 1 means that the closer x is to the decision boundary (x\u0303, g\u2217(x\u0303)), the more likely the labeler is to abstain from labeling. This complies with the intuition that instances closer to the decision boundary are harder to classify. We also assume the 0/1 labels can be flipped with probability as large as 12 . In other words, we allow unbounded noise.\nCondition 2. Let C, \u03b2 be non-negative constants, and f : [0, 1] \u2192 [0, 1] be a nondecreasing function. The response distribution P (Y | X) satisfies:\n\u2022 (abstention) P (\u22a5| x) \u2264 1\u2212 f (|xd \u2212 g\u2217(x\u0303)|);\n\u2022 (noise) P (Y 6= 1 [xd > g\u2217(x\u0303)] | x, Y 6=\u22a5) \u2264 12 ( 1\u2212 C |xd \u2212 g\u2217(x\u0303)|\u03b2 ) .\nCondition 2 requires the abstention and noise probabilities to be upper-bounded, and these upper bounds decrease as x moves further away from the decision boundary. The abstention rate can be 1 at the decision boundary, so the labeler may always abstain at the decision boundary. The condition on the noise satisfies the popular Tsybakov noise condition [22].\nCondition 3. Let f : [0, 1] \u2192 [0, 1] be a nondecreasing function such that \u22030 < c < 1, \u22000 < a \u2264 1 \u22000 \u2264 b \u2264 23a, f(b) f(a) \u2264 1\u2212 c. The response distribution satisfies: P (\u22a5| x) = 1\u2212 f (|xd \u2212 g\u2217(x\u0303)|).\nAn example where Condition 3 holds is P (\u22a5| x) = 1\u2212 (x\u2212 0.3)\u03b1 (\u03b1 > 0). Condition 3 requires the abstention rate to increase monotonically close to the decision boundary as in Condition 1. In addition, it requires the abstention probability P (\u22a5 |(x\u0303, xd)) not to be too flat with respect to xd. For example, when d = 1, P (\u22a5| x) = 0.68 for 0.2 \u2264 x \u2264 0.4 (shown as Figure 2) does not satisfy Condition 3, and abstention responses are not informative since this abstention rate alone yields no information on the location of the decision boundary. In contrast, P (\u22a5| x) = 1 \u2212 \u221a\n|x\u2212 0.3| (shown as Figure 3) satisfies Condition 3, and the learner could infer it is getting close to the decision boundary when it starts receiving more abstention responses.\nNote that here c, f, C, \u03b2 are unknown and arbitrary parameters that characterize the complexity of the learning task. We want to design an algorithm that does not require knowledge of these parameters but still achieves nearly optimal query complexity."}, {"heading": "3 Learning one-dimensional thresholds", "text": "In this section, we start with the one dimensional case (d = 1) to demonstrate the main idea. We will generalize these results to multidimensional instance space in the next section.\nWhen d = 1, the decision boundary g\u2217 becomes a point in [0, 1], and the corresponding classifier is a threshold function over [0,1]. In other words the hypothesis space becomes H = {f\u03b8(x) = 1 [x > \u03b8] : \u03b8 \u2208 [0, 1]}). We denote the ground truth decision boundary by \u03b8\u2217 \u2208 [0, 1]. We want to find a \u03b8\u0302 \u2208 [0, 1] such that |\u03b8\u0302 \u2212 \u03b8\u2217| is small while making as few queries as possible."}, {"heading": "3.1 Algorithm", "text": "The proposed algorithm is a binary search style algorithm shown as Algorithm 1. (For the sake of simplicity, we assume log 12\u01eb is an integer.) Algorithm 1 takes a desired precision \u01eb and confidence\nAlgorithm 1 The active learning algorithm for learning thresholds 1: Input: \u03b4, \u01eb 2: [L0, R0] \u2190 [0, 1] 3: for k = 0, 1, 2, . . . , log 12\u01eb \u2212 1 do 4: Define three quartiles: Uk \u2190 3Lk+Rk4 , Mk \u2190 Lk+Rk 2 , Vk \u2190 Lk+3Rk 4\n5: A(u), A(m), A(v), B(u), B(v) \u2190 Empty Array 6: for n = 1, 2, . . . do 7: Query at Uk,Mk, Vk, and receive labels X (u) n , X (m) n , X (v) n 8: for w \u2208 {u,m, v} do 9: \u22b2 We record whether X(w) =\u22a5 in A(w), and the 0/1 label (as -1/1) in B(w) if\nX(w) 6=\u22a5 10: if X(w) 6=\u22a5 then 11: A(w) \u2190 A(w).append(1) , B(w) \u2190 B(w).append(21 [ X(w) = 1 ]\n\u2212 1) 12: else 13: A(w) \u2190 A(w).append(0) 14: end if 15: end for 16: \u22b2 Check if the differences of abstention responses are statistically significant 17: if CHECKSIGNIFICANT-VAR( {\nA (u) i \u2212A (m) i\n}n\ni=1 , \u03b4 4 log 12\u01eb ) then\n18: [Lk+1, Rk+1] \u2190 [Uk, Rk]; break 19: else if CHECKSIGNIFICANT-VAR( {\nA (v) i \u2212A (m) i\n}n\ni=1 , \u03b4 4 log 12\u01eb ) then\n20: [Lk+1, Rk+1] \u2190 [Lk, Vk]; break 21: end if 22: \u22b2 Check if the differences between 0 and 1 labels are statistically significant\n23: if CHECKSIGNIFICANT( { \u2212B(u)i }B(u) .length\ni=1 , \u03b4 4 log 12\u01eb ) then\n24: [Lk+1, Rk+1] \u2190 [Uk, Rk]; break\n25: else if CHECKSIGNIFICANT( {\nB (v) i\n}B(v) .length\ni=1 , \u03b4 4 log 12\u01eb ) then\n26: [Lk+1, Rk+1] \u2190 [Lk, Vk]; break 27: end if 28: end for 29: end for 30: Output: \u03b8\u0302 = (\nLlog 12\u01eb +Rlog 1 2\u01eb\n)\n/2\nlevel \u03b4 as its input, and returns an estimation \u03b8\u0302 of the decision boundary \u03b8\u2217. The algorithm maintains an interval [Lk, Rk] in which \u03b8\u2217 is believed to lie, and shrinks this interval iteratively. To find the subinterval that contains \u03b8\u2217, Algorithm 1 relies on two auxiliary functions (marked in Procedure 2) to conduct adaptive sequential hypothesis tests regarding subintervals of interval [Lk, Rk].\nSuppose \u03b8\u2217 \u2208 [Lk, Rk]. Algorithm 1 tries to shrink this interval to a 34 of its length in each iteration by repetitively querying on quartiles Uk = 3Lk+Rk4 , Mk = Lk+Rk 2 , Vk = Lk+3Rk 4 . To determine which specific subinterval to choose, the algorithm uses 0/1 labels and abstention responses simultaneously. Since the ground truth labels are determined by 1 [x > \u03b8\u2217], one can infer that if the number of queries that return label 0 at Uk (Vk) is statistically significantly more (less) than label 1, then \u03b8\u2217 should be on the right (left) side of Uk (Vk). Similarly, from Condition 1, if the number of nonabstention responses at Uk (Vk) is statistically significantly more than non-abstention responses at Mk, then \u03b8\u2217 should be closer to Mk than Uk (Vk).\nAlgorithm 1 relies on the ability to shrink the search interval via statistically comparing the numbers of obtained labels at locations Uk,Mk, Vk. As a result, a main building block of Algorithm 1 is to test whether i.i.d. bounded random variables Yi are greater in expectation than i.i.d. bounded random variables Zi with statistical significance. In Procedure 2, we have two test functions CheckSignificant and CheckSignificant-Var that take i.i.d. random variables {Xi = Yi \u2212 Zi} (|Xi| \u2264 1)\nProcedure 2 Adaptive sequential testing 1: \u22b2 D0, D1 are absolute constants defined in Proposition 1 and Proposition 2 2: \u22b2 {Xi} are i.i.d. random variables bounded by 1. \u03b4 is the confidence level. Detect if EX > 0 3: function CHECKSIGNIFICANT({Xi}ni=1 , \u03b4) 4: p(n, \u03b4) \u2190 D0 ( 1 + ln 1\u03b4 + \u221a 4n ( [ln ln]+ 4n+ ln 1 \u03b4 ) )\n5: Return \u2211n i=1 Xi \u2265 p(n, \u03b4) 6: end function 7: function CHECKSIGNIFICANT-VAR({Xi}ni=1 , \u03b4) 8: Calculate the empirical variance Var = nn\u22121 ( \u2211n i=1 Xi 2 \u2212 1n ( \u2211n i=1 Xi) 2 ) 9: q(n,Var, \u03b4) \u2190 D1 ( 1 + ln 1\u03b4 + \u221a ( Var + ln 1\u03b4 + 1 ) ( [ln ln]+ ( Var + ln 1\u03b4 + 1 ) + ln 1\u03b4 ) )\n10: Return n \u2265 ln 1\u03b4 AND \u2211n i=1 Xi \u2265 q(n,Var, \u03b4) 11: end function\nand confidence level \u03b4 as their input, and output whether it is statistically significant to conclude EXi > 0.\nCheckSignificant is based on the following uniform concentration result regarding the empirical mean: Proposition 1. Suppose X1, X2, . . . are a sequence of i.i.d. random variables with X1 \u2208 [\u22122, 2], EX1 = 0. Take any 0 < \u03b4 < 1. Then there is an absolute constant D0 such that with probability at least 1\u2212 \u03b4, for all n > 0 simultaneously,\n\u2223 \u2223 \u2223 \u2223 \u2223 n \u2211\ni=1\nXi\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2264 D0 ( 1 + ln 1 \u03b4 +\n\u221a\n4n\n(\n[ln ln]+ 4n+ ln 1\n\u03b4\n)\n)\nIn Algorithm 1, we use CheckSignificant to detect whether the expected number of queries that return label 0 at location Uk (Vk) is more/less than the expected number of label 1 with a statistical significance.\nCheckSignificant-Var is based on the following uniform concentration result which further utilizes\nthe empirical variance Vn = nn\u22121\n(\n\u2211n i=1 X 2 i \u2212 1n ( \u2211n i=1 Xi)\n2 )\n:\nProposition 2. There is an absolute constant D1 such that with probability at least 1 \u2212 \u03b4, for all n \u2265 ln 1\u03b4 simultaneously,\n\u2223 \u2223 \u2223 \u2223 \u2223 n \u2211\ni=1\nXi\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2264 D1 ( 1 + ln 1 \u03b4 +\n\u221a\n(\n1 + ln 1\n\u03b4 + Vn\n)(\n[ln ln]+ (1 + ln 1\n\u03b4 + Vn) + ln\n1\n\u03b4\n)\n)\nThe use of variance results in a tighter bound when Var(Xi) is small.\nIn Algorithm 1, we use CheckSignificant-Var to detect the statistical significance of the relative order of the number of queries that return non-abstention responses at Uk (Vk) compared to the number of non-abstention responses at Mk. This results in a better query complexity than using CheckSignificant under Condition 3, since the variance of the number of abstention responses approaches 0 when the interval [Lk, Rk] zooms in on \u03b8\u2217.1"}, {"heading": "3.2 Analysis", "text": "For Algorithm 1 to be statistically consistent, we only need Condition 1. Theorem 1. Let \u03b8\u2217 be the ground truth. If the labeler L satisfies Condition 1 and Algorithm 1 stops to output \u03b8\u0302, then \u2223 \u2223 \u2223\u03b8\u2217 \u2212 \u03b8\u0302 \u2223 \u2223\n\u2223 \u2264 \u01eb with probability at least 1\u2212 \u03b42 . 1We do not apply CheckSignificant-Var to 0/1 labels, because unlike the difference between the numbers of abstention responses at Uk (Vk) and Mk, the variance of the difference between the numbers of 0 and 1 labels stays above a positive constant.\nUnder additional Conditions 2 and 3, we can derive upper bounds of the query complexity for our algorithm. (Recall f and \u03b2 are defined in Conditions 2 and 3.)\nTheorem 2. Let \u03b8\u2217 be the ground truth, and \u03b8\u0302 be the output of Algorithm 1. Under Conditions 1 and 2, with probability at least 1\u2212 \u03b4, Algorithm 1 makes at most O\u0303 (\n1 f( \u01eb2 )\n\u01eb\u22122\u03b2 ) queries.\nTheorem 3. Let \u03b8\u2217 be the ground truth, and \u03b8\u0302 be the output of Algorithm 1. Under Conditions 1 and 3, with probability at least 1\u2212 \u03b4, Algorithm 1 makes at most O\u0303 (\n1 f( \u01eb2 )\n)\nqueries.\nThe query complexity given by Theorem 3 is independent of \u03b2 that decides the flipping rate, and consequently smaller than the bound in Theorem 2. This improvement is due to the use of abstention responses, which become much more informative under Condition 3."}, {"heading": "3.3 Lower Bounds", "text": "In this subsection, we give lower bounds of query complexity in the one-dimensional case and establish near optimality of Algorithm 1. We will give corresponding lower bounds for the highdimensional case in the next section.\nThe lower bound in [24] can be easily generalized to Condition 2:\nTheorem 4. ([24]) There is a universal constant \u03b40 \u2208 (0, 1) and a labeler L satisfying Conditions 1 and 2, such that for any active learning algorithm A, there is a \u03b8\u2217 \u2208 [0, 1], such that for small enough \u01eb, \u039b(\u01eb, \u03b40,A, L, \u03b8\u2217) \u2265 \u2126 ( 1 f(\u01eb)\u01eb \u22122\u03b2 ) .\nOur query complexity (Theorem 3) for the algorithm is also almost tight under Conditions 1 and 3 with a polynomial abstention rate.\nTheorem 5. There is a universal constant \u03b40 \u2208 (0, 1) and a labeler L satisfying Conditions 1, 2, and 3 with f(x) = C\u2032x\u03b1 (C\u2032 > 0 and 0 < \u03b1 \u2264 2 are constants), such that for any active learning algorithm A, there is a \u03b8\u2217 \u2208 [0, 1], such that for small enough \u01eb, \u039b(\u01eb, \u03b40,A, L, \u03b8\u2217) \u2265 \u2126 (\u01eb\u2212\u03b1)."}, {"heading": "3.4 Remarks", "text": "Our results confirm the intuition that learning with abstention is easier than learning with noisy labels. This is true because a noisy label might mislead the learning algorithm, but an abstention response never does. Our analysis shows, in particular, that if the labeler never abstains, and outputs completely noisy labels with probability bounded by 1 \u2212 |x\u2212 \u03b8\u2217|\u03b3 (i.e., P (Y 6= I [x > \u03b8\u2217] | x) \u2264 1 2 (1\u2212 |x\u2212 \u03b8\u2217| \u03b3 )), then the near optimal query complexity of O\u0303 ( \u01eb\u22122\u03b3 )\nis significantly larger than the near optimal O\u0303 (\u01eb\u2212\u03b3) query complexity associated with a labeler who only abstains with probability P (Y =\u22a5| x) \u2264 1 \u2212 |x\u2212 \u03b8\u2217|\u03b3 and never flips a label. More precisely, while in both cases the labeler outputs the same amount of corrupted labels, the query complexity of the abstention-only case is significantly smaller than the noise-only case.\nNote that the query complexity of Algorithm 1 consists of two kinds of queries: queries which return 0/1 labels and are used by function CheckSignificant, and queries which return abstention and are used by function CheckSignificant-Var. Algorithm 1 will stop querying when the responses of one of the two kinds of queries are statistically significant. Under Condition 2, our proof actually shows that the optimal number of queries is dominated by the number of queries used by CheckSignificant function. In other words, a simplified variant of Algorithm 1 which excludes use of abstention feedback is near optimal. Similarly, under Condition 3, the optimal query complexity is dominated by the number of queries used by CheckSignificant-Var function. Hence the variant of Algorithm 1 which disregards 0/1 labels would be near optimal."}, {"heading": "4 The multidimensional case", "text": "We follow [6] to generalize the results from one-dimensional thresholds to the d-dimensional (d > 1) smooth boundary fragment class \u03a3(K, \u03b3).\nAlgorithm 3 The active learning algorithm for the smooth boundary fragment class 1: Input: \u03b4, \u01eb, \u03b3\n2: M \u2190 \u0398 ( \u01eb\u22121/\u03b3 ) . L \u2190 { 0 M , 1 M , . . . , M\u22121 M\n}d\u22121\n3: For each l \u2208 L, apply Algorithm 1 with parameter (\u01eb, \u03b4/Md\u22121) to learn a threshold gl that approximates g\u2217(l) 4: Partition the instance space into cells {Iq} indexed by q \u2208 { 0, 1, . . . , M\u03b3 \u2212 1 }d\u22121 , where\nIq =\n[\nq1\u03b3 M , (q1 + 1)\u03b3 M\n] \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [ qd\u22121\u03b3 M , (qd\u22121 + 1)\u03b3 M ]\n5: For each cell Iq , perform a polynomial interpolation: gq(x\u0303) = \u2211\nl\u2208Iq\u2229L glQq,l(x\u0303), where\nQq,l(x\u0303) =\nd\u22121 \u220f\ni=1\n\u03b3 \u220f\nj=0,j 6=Mli\u2212\u03b3qi\nx\u0303i \u2212 (\u03b3qi + j)/M li \u2212 (\u03b3qi + j)/M\n6: Output: g(x\u0303) = \u2211 q\u2208{0,1,...,M\u03b3 \u22121}d\u22121 gq(x\u0303)1 [x\u0303 \u2208 q]"}, {"heading": "4.1 Lower bounds", "text": "Theorem 6. There are universal constants \u03b40 \u2208 (0, 1), c0 > 0, and a labeler L satisfying Conditions 1 and 2, such that for any active learning algorithm A, there is a g\u2217 \u2208 \u03a3(K, \u03b3), such that for small enough \u01eb, \u039b(\u01eb, \u03b40,A, L, g\u2217) \u2265 \u2126 (\n1 f(c0\u01eb)\n\u01eb\u22122\u03b2\u2212 d\u22121 \u03b3\n)\n.\nTheorem 7. There is a universal constant \u03b40 \u2208 (0, 1) and a labeler L satisfying Conditions 1, 2, and Condition 3 with f(x) = C\u2032x\u03b1 (C\u2032 > 0 and 0 < \u03b1 \u2264 2 are constants), such that for any active learning algorithm A, there is a g\u2217 \u2208 \u03a3(K, \u03b3), such that for small enough \u01eb, \u039b(\u01eb, \u03b40,A, L, g\u2217) \u2265 \u2126 ( \u01eb\u2212\u03b1\u2212 d\u22121 \u03b3 ) ."}, {"heading": "4.2 Algorithm and Analysis", "text": "Recall the decision boundary of the smooth boundary fragment class can be seen as the epigraph of a smooth function [0, 1]d\u22121 \u2192 [0, 1]. For d > 1, we can reduce the problem to the one-dimensional problem by discretizing the first d\u2212 1 dimensions of the instance space and then perform a polynomial interpolation. The algorithm is shown as Algorithm 3. For the sake of simplicity, we assume \u03b3, M/\u03b3 in Algorithm 3 are integers.\nWe have similar consistency guarantee and upper bounds as in the one-dimensional case. Theorem 8. Let g\u2217 be the ground truth. If the labeler L satisfies Condition 1 and Algorithm 3 stops to output g, then \u2016g\u2217 \u2212 g\u2016 \u2264 \u01eb with probability at least 1\u2212 \u03b42 . Theorem 9. Let g\u2217 be the ground truth, and g be the output of Algorithm 3. Under Conditions 1 and 2, with probability at least 1\u2212 \u03b4, Algorithm 3 makes at most O\u0303 (\nd f(\u01eb/2)\u01eb\n\u22122\u03b2\u2212d\u22121 \u03b3\n)\nqueries.\nTheorem 10. Let g\u2217 be the ground truth, and g be the output of Algorithm 3. Under Conditions 1 and 3, with probability at least 1\u2212 \u03b4, Algorithm 3 makes at most O\u0303 (\nd f(\u01eb/2)\u01eb\n\u2212 d\u22121 \u03b3\n)\nqueries.\nAcknowledgments. We thank NSF under IIS-1162581, CCF-1513883, and CNS-1329819 for research support."}, {"heading": "A Proof of query complexities", "text": "A.1 Properties of adaptive sequential testing in Procedure 2\nLemma 1. Suppose {Xi}\u221ei=1 is a sequence of i.i.d. random variables such that EXi \u2264 0, |Xi| \u2264 1. Let \u03b4 > 0. Then with probability at least 1 \u2212 \u03b4, for all n \u2208 N simultaneously CheckSignificant({Xi}ni=1 , \u03b4) in Procedure 2 returns false.\nProof. This is immediate by applying Proposition 1 to Xi \u2212 EXi.\nLemma 2. Suppose {Xi}\u221ei=1 is a sequence of i.i.d. random variables such that EXi > \u01eb > 0, |Xi| \u2264 1. Let \u03b4 \u2208 [0, 13 ], N \u2265 \u03be \u01eb2 ln 1 \u03b4 [ln ln]+ 1 \u01eb (\u03be is an absolute constant specified in the proof). Then with probability at least 1\u2212 \u03b4, CheckSignificant (\n{Xi}Ni=1 , \u03b4 ) in Procedure 2 returns true.\nProof. Let SN = \u2211N i=1 Xi. CheckSignificant ( {Xi}Ni=1 , \u03b4 ) returns false if and only if SN \u2264 D0 ( 1 + ln 1\u03b4 + \u221a N ( [ln ln]+N + ln 1 \u03b4 ) ) .\nPr\n( SN \u2264 D0 ( 1 + ln 1\n\u03b4 +\n\u221a\nN\n(\n[ln ln]+N + ln 1\n\u03b4\n)\n))\n\u2264Pr ( SN \u2264 D0 ( 1 + ln 1\n\u03b4 + \u221a N [ln ln]+N +\n\u221a\nN ln 1\n\u03b4\n))\n\u2264Pr ( SN \u2212NEXi \u2264 D0 ( 1 + ln 1\n\u03b4 + \u221a N [ln ln]+N +\n\u221a\nN ln 1\n\u03b4\n) \u2212N\u01eb )\nSuppose N = c\u03be\u01eb2 ln 1 \u03b4 [ln ln]+ 1 \u01eb for constant c \u2265 1 and \u03be. \u03be is set to be sufficiently large, such that (1) \u03be \u2265 4D20; (2) 2D0\u221a\u03be +D0 ( 3 + \u221a [ln ln]+\u03be ) +D0 \u2212 \u221a \u03be/2 \u2264 \u2212 \u221a 1 2 ; (3) f(x) = D0 \u221a\n[ln ln]+x \u2212 \u221a x/2 is decreasing when x > \u03be. Here (2) is satisfiable since D0\u221a\n\u03be + D0\n\u221a [ln ln]+\u03be \u2212 \u221a \u03be/2 \u2192\n\u2212\u221e as \u03be \u2192 \u221e, (3) is satisfiable since f \u2032(x) \u2192 \u2212\u221e as x \u2192 \u221e. (2) and (3) together implies 2D0\u221a\n\u03be +D0\n(\n3 + \u221a [ln ln]+c\u03be ) +D0 \u2212 \u221a c\u03be/2 \u2264 \u2212 \u221a 1 2 .\n1\u221a N\n(\nD0\n(\n1 + ln 1 \u03b4 + \u221a N [ln ln]+N +\n\u221a\nN ln 1\n\u03b4\n) \u2212N\u01eb )\n=\n\u221a\nln 1\n\u03b4\n\n\n\nD0\u01eb(1 + ln 1 \u03b4 )\n\u221a\nc\u03be[ln ln]+ 1 \u01eb ln 1 \u03b4\n+D0\n\u221a \u221a \u221a \u221a [ln ln]+ ( c\u03be \u01eb2 ln 1 \u03b4 [ln ln]+ 1 \u01eb )\nln 1\u03b4 +D0 \u2212\n\u221a\nc\u03be[ln ln]+ 1\n\u01eb\n\n\n\nSince [ln ln]+ 1\u01eb , c, ln 1 \u03b4 \u2265 1 and \u01eb < 1, we have\nD0\u01eb(1+ln 1 \u03b4 )\u221a\nc\u03be[ln ln]+ 1 \u01eb ln 1 \u03b4\n\u2264 2D0\u221a \u03be .\nSince [ln ln]+x \u2265 1 if x \u2265 1, we have [ln ln]+ 1\u01eb \u2264 1\u01eb , and thus\n\u221a [ln ln]+ ( c\u03be\n\u01eb2 ln\n1 \u03b4 [ln ln]+ 1 \u01eb\n)\n=\n\u221a\nln\n[\nmax\n{\ne, 2 ln 1\n\u01eb + ln c\u03be + ln ln\n1 \u03b4 + ln[ln ln]+ 1 \u01eb\n}]\n\u2264 \u221a ln [ max { e, 3 ln 1\n\u01eb + ln c\u03be + [ln ln]+\n1\n\u03b4\n}]\n(a) \u2264 \u221a ln [ max { e, 9 ln 1\n\u01eb ln c\u03be[ln ln]+\n1\n\u03b4\n}]\n\u2264 \u221a 3 + [ln ln]+ 1\n\u01eb + [ln ln]+c\u03be + ln[ln ln]+\n1\n\u03b4 (b)\n\u2264 \u221a 3 + \u221a\n[ln ln]+c\u03be +\n\u221a\n[ln ln]+ 1\n\u01eb +\n\u221a\nln[ln ln]+ 1\n\u03b4\nwhere (a) follows by a + b + c \u2264 3abc if a, b, c \u2265 1, and (b) follows by \u221a \u2211 i xi \u2264 \u2211 i \u221a xi if xi \u2265 0. Thus, we have\n1\u221a N\n(\nD0\n(\n1 + ln 1 \u03b4 + \u221a N [ln ln]+N +\n\u221a\nN ln 1\n\u03b4\n) \u2212N\u01eb )\n\u2264 \u221a ln 1\n\u03b4\n\n 2D0\u221a \u03be +D0\n\u221a 3 + \u221a [ln ln]+c\u03be + \u221a [ln ln]+ 1 \u01eb + \u221a ln[ln ln]+ 1 \u03b4\n\u221a\nln 1\u03b4\n+D0 \u2212 \u221a c\u03be[ln ln]+ 1\n\u01eb\n\n\n(c) \u2264 \u221a ln 1\n\u03b4\n(\n2D0\u221a \u03be +D0 ( 3 + \u221a [ln ln]+c\u03be ) +D0 \u2212 \u221a c\u03be/2\n)\n(d) \u2264 \u2212 \u221a ln 1\n\u03b4 /2\n(c) follows by \u221a ln 1\u03b4 \u2265 max { 1, \u221a ln[ln ln]+ 1 \u03b4 } , D0 \u2265 1, and \u221a [ln ln]+ 1 \u01eb\n(\nD0\u221a ln 1\n\u03b4\n\u2212\u221ac\u03be ) \u2264\nD0 \u2212 \u221a c\u03be \u2264 \u2212 \u221a c\u03be/2 if c\u03be \u2265 4D20. (d) follows by our choose of \u03be.\nTherefore,\nPr\n( SN \u2212NEXi \u2264 D0 ( 1 + ln 1\n\u03b4 + \u221a N [ln ln]+N +\n\u221a\nN ln 1\n\u03b4\n) \u2212N\u01eb )\n\u2264Pr ( SN \u2212NEXi \u2264 \u2212 \u221a N ln 1\n\u03b4 /2\n)\nwhich is at most \u03b4 by Hoeffding Bound.\nLemma 3. Suppose {Xi}\u221ei=1 is a sequence of i.i.d. random variables such that EXi \u2264 0, |Xi| \u2264 1. Let \u03b4 > 0. Then with probability at least 1 \u2212 \u03b4, for all n simultaneously CheckSignificantVar({Xi}ni=1 , \u03b4) in Procedure 2 returns false.\nProof. Define Yi = Xi \u2212 EXi. It is easy to check nn\u22121 ( \u2211n i=1 Y 2 i \u2212 1n ( \u2211n i=1 Yi) 2 ) =\nn n\u22121\n(\n\u2211n i=1 X 2 i \u2212 1n ( \u2211n i=1 Xi)\n2 )\n. The result is immediate from Proposition 2.\nLemma 4. Suppose {Xi}\u221ei=1 is a sequence of i.i.d. random variables such that EXi > \u03c4\u01eb, |Xi| \u2264 1, Var (Xi) \u2264 2\u01eb where 0 < \u01eb \u2264 1, \u03c4 > 0. Let \u03b4 < 1, N = \u03be\u03c4\u01eb ln 2\u03b4 (\u03be is a constant specified in the proof). Then with probability at least 1 \u2212 \u03b4, CheckSignificant-Var (\n{Xi}Ni=1 , \u03b4 )\nin Procedure 2 returns true.\nProof. Let Yi = Xi \u2212 EXi, \u03b7 be the constant \u03b7 in Lemma 14. Set \u03be = max(\u03b7, 16\u03c4 + 83 ).\nCheckSignificant-Var ( {Xi}Ni=1 , \u03b4 ) returns false if and only if \u2211N i=1 Xi \u2264 q(N,Var, \u03b4).\nBy applying Lemma 14 to Xi, q(N,Var,\u03b4) N \u2212 EXi \u2264 \u2212\u03c4\u01eb/2 with probability at least 1\u2212 \u03b4/2. Applying Bernstein\u2019s inequality to Yi, we have\nPr\n(\n1\nN\nN \u2211\ni=1\nYi \u2264 \u2212\u03c4\u01eb/2 ) \u2264 exp ( \u2212N (\u2212\u03c4\u01eb) 2 /4\n4\u01eb+ 2\u03c4\u01eb/3\n)\n= exp\n( \u2212 \u03be ln 2 \u03b4\n16/\u03c4 + 8/3\n)\n\u2264 \u03b4/2\nThus, by a union bound,\nPr\n(\nN \u2211\ni=1\nXi \u2264 q(N,Var, \u03b4) )\n\u2264Pr ( q(N,Var, \u03b4) N \u2212 EXi \u2265 \u2212\u03c4\u01eb/2 )\n+ Pr\n(\nq(N,Var, \u03b4) N \u2212 EXi \u2264 \u2212\u03c4\u01eb/2 and 1 N\nN \u2211\ni=1\nXi \u2264 q(N,Var, \u03b4)\nN\n)\n\u2264\u03b4/2 + Pr (\nq(N,Var, \u03b4) N \u2212 EXi \u2264 \u2212\u03c4\u01eb/2 and 1 N\nN \u2211\ni=1\nYi \u2264 q(n,Var, \u03b4)\nN \u2212 EXi\n)\n\u2264\u03b4/2 + Pr ( 1\nN\nN \u2211\ni=1\nYi \u2264 \u2212\u03c4\u01eb/2 )\n\u2264\u03b4\nA.2 The one-dimensional case\nProof of Theorem 1. Since \u03b8\u0302 = (\nLlog 12\u01eb +Rlog 1 2\u01eb\n) /2 and Rlog 12\u01eb \u2212 Llog 12\u01eb = 2\u01eb, \u2223 \u2223 \u2223\u03b8\u0302 \u2212 \u03b8\u2217 \u2223 \u2223 \u2223 > \u01eb is\nequivalent to \u03b8\u2217 /\u2208 [Llog 12\u01eb , Rlog 12\u01eb ]. We have\nPr (\u2223 \u2223 \u2223\u03b8\u0302 \u2212 \u03b8\u2217 \u2223 \u2223 \u2223 > \u01eb ) = Pr ( \u03b8\u2217 /\u2208 [Llog 12\u01eb , Rlog 12\u01eb ] )\n= Pr (\u2203k : \u03b8\u2217 \u2208 [Lk, Rk] and \u03b8\u2217 /\u2208 [Lk+1, Rk+1])\n\u2264 log 12\u01eb\u22121 \u2211\nk=0\nPr (\u03b8\u2217 \u2208 [Lk, Rk] and \u03b8\u2217 /\u2208 [Lk+1, Rk+1])\nFor any k = 0, . . . , log 12\u01eb \u2212 1, define Qk = { (p, q) : p, q \u2208 Q \u2229 [0, 1] and q \u2212 p = ( 3 4 )k } where Q is the set of rational numbers. Note that Lk, Rk \u2208 Qk, and Q is countable. So we have\nPr (\u03b8\u2217 \u2208 [Lk, Rk] and \u03b8\u2217 /\u2208 [Lk+1, Rk+1]) = \u2211\n(p,q)\u2208Qk:p\u2264\u03b8\u2217\u2264q Pr (Lk = p,Rk = q and \u03b8\n\u2217 /\u2208 [Lk+1, Rk+1])\n= \u2211 (p,q)\u2208Qk:p\u2264\u03b8\u2217\u2264q Pr (\u03b8\u2217 /\u2208 [Lk+1, Rk+1]|Lk = p,Rk = q) Pr (Lk = p,Rk = q)\nDefine event Ek,p,q to be the event Lk = p,Rk = q. To show Pr (\u2223 \u2223 \u2223\u03b8\u0302 \u2212 \u03b8\u2217 \u2223 \u2223 \u2223 > \u01eb )\n\u2264 \u03b42 , it suffices to show Pr (\u03b8\u2217 /\u2208 [Lk+1, Rk+1]|Ek,p,q) \u2264 \u03b42 log 12\u01eb for any k = 0, . . . , log 1 2\u01eb \u2212 1, (p, q) \u2208 Qk and p \u2264 \u03b8\u2217 \u2264 q. Conditioning on event Ek,p,q , event \u03b8\u2217 /\u2208 [Lk+1, Rk+1] happens only if some calls of CheckSignificant and CheckSignificant-Var between Line 16 and 27 of Algorithm 1 return true incorrectly. In other words, at least one of following events happens for some n:\n\u2022 O(1)k,p,q: \u03b8\u2217 \u2208 [Lk, Uk] and CheckSignificant-Var( { A (u) i \u2212A (m) i\n}n\ni=1 , \u03b4 4 log 12\u01eb ) returns true;\n\u2022 O(2)k,p,q: \u03b8\u2217 \u2208 [Vk, Rk] and CheckSignificant-Var( { A (v) i \u2212A (m) i\n}n\ni=1 , \u03b4 4 log 12\u01eb ) returns true;\n\u2022 O(3)k,p,q: \u03b8\u2217 \u2208 [Lk, Uk] and CheckSignificant( { \u2212B(u)i }n\ni=1 , \u03b4 4 log 12\u01eb ) returns true;\n\u2022 O(4)k,p,q: \u03b8\u2217 \u2208 [Vk, Rk] and CheckSignificant( { B (v) i\n}n\ni=1 , \u03b4 4 log 12\u01eb ) returns true;\nNote that since [Uk, Vk] \u2282 [Lk+1, Rk+1] for any k by our construction, if \u03b8\u2217 \u2208 [Uk, Vk] then \u03b8\u2217 \u2208 [Lk+1, Rk+1]. Besides, event \u03b8\u2217 \u2208 [Lk, Uk] and event \u03b8\u2217 \u2208 [Vk, Rk] are mutually exclusive. Conditioning on event Ek,p,q , suppose for now \u03b8\u2217 \u2208 [Lk, Uk].\nPr (\nO (1) k,p,q | Ek,p,q\n)\n=Pr\n(\n\u2203n : CheckSignificant-Var( {\nD (u,m) i\n}n\ni=1 ,\n\u03b4\n4 log 12\u01eb ) returns true | \u03b8\u2217 \u2208 [Lk, Uk], Ek,p,q\n)\nOn event \u03b8\u2217 \u2208 [Lk, Uk] and Ek,p,q , the sequences { A (u) i } and { A (m) i } are i.i.d., and E [ A (u) i \u2212\nA (m) i | \u03b8\u2217 \u2208 [Lk, Uk], Ek,p,q\n]\n\u2264 0. By Lemma 3, the probability above is at most \u03b4 4 log 12\u01eb .\nLikewise,\nPr (\nO (3) k,p,q | Ek,p,q\n)\n=Pr\n(\n\u2203n : CheckSignificant( { \u2212B(u)i }n\ni=1 ,\n\u03b4\n4 log 12\u01eb ) returns true | \u03b8\u2217 \u2208 [Lk, Uk], Ek,p,q\n)\nOn event \u03b8\u2217 \u2208 [Lk, Uk] and Ek,p,q , the sequence { B (u) i } is i.i.d., and\nE\n[ \u2212B(u)i | \u03b8\u2217 \u2208 [Lk, Uk], Ek,p,q ]\n\u2264 0. By Lemma 1, the probability above is at most \u03b4 4 log 12\u01eb .\nThus, Pr (\u03b8\u2217 /\u2208 [Lk+1, Rk+1] | Ek,p,q) \u2264 \u03b42 log 12\u01eb when \u03b8 \u2217 \u2208 [Lk, Uk]. Similarly, when \u03b8\u2217 \u2208 [Vk, Rk], we can show Pr (\u03b8\u2217 /\u2208 [Lk+1, Rk+1] | Ek,p,q) \u2264 Pr ( O (2) k,p,q | Ek,p,q ) + Pr (\nO (4) k,p,q | Ek,p,q\n)\n\u2264 \u03b4 2 log 12\u01eb .\nTherefore, Pr (\u03b8\u2217 /\u2208 [Lk+1, Rk+1] | Ek,p,q) \u2264 \u03b42 log 12\u01eb , and thus Pr (\u2223 \u2223 \u2223\u03b8\u0302 \u2212 \u03b8\u2217 \u2223 \u2223 \u2223 > \u01eb ) \u2264 \u03b4/2.\nProof of Theorem 2. Define Tk to be the number of iterations of the loop at Line 6, T = \u2211log 12\u01eb\u22121\nk=0 Tk. For any numbers m1,m2, . . . ,mlog 12\u01eb\u22121, we have:\nPr (T \u2265 m) \u2264 Pr (\u2223 \u2223 \u2223\u03b8\u0302 \u2212 \u03b8\u2217 \u2223 \u2223 \u2223 > \u01eb ) + Pr\n\n\n\u2223 \u2223 \u2223\u03b8\u0302 \u2212 \u03b8\u2217 \u2223 \u2223 \u2223 < \u01eb and T \u2265 log 12\u01eb\u22121 \u2211\nk=0\nmk\n\n\n\u2264 \u03b4 2 + Pr\n T \u2265 log 12\u01eb\u22121 \u2211\nk=0\nmk and \u2223 \u2223 \u2223\u03b8\u0302 \u2212 \u03b8\u2217 \u2223 \u2223 \u2223 < \u01eb\n\n (1)\n\u2264 \u03b4 2 +\nlog 12\u01eb\u22121 \u2211\nk=0\nPr ( Tk \u2265 mk and \u2223 \u2223 \u2223\u03b8\u0302 \u2212 \u03b8\u2217 \u2223 \u2223 \u2223 < \u01eb )\n\u2264 \u03b4 2 +\nlog 12\u01eb\u22121 \u2211\nk=0\nPr (Tk \u2265 mk and \u03b8\u2217 \u2208 [Lk, Rk])\nThe first and the third inequality follows by union bounds. The second follows by Theorem 1. The last follows since \u2223 \u2223 \u2223 \u03b8\u0302 \u2212 \u03b8\u2217 \u2223 \u2223\n\u2223 < \u01eb is equivalent to \u03b8\u2217 \u2208 [Llog 12\u01eb , Rlog 12\u01eb ], which implies \u03b8 \u2217 \u2208 [Lk, Rk] for all k = 0, . . . , log 12\u01eb \u2212 1. We define Qk as in the previous proof. For all k = 0, . . . , log 12\u01eb \u2212 1,\nPr (Tk \u2265 mk and \u03b8\u2217 \u2208 [Lk, Rk]) = \u2211\n(p,q)\u2208Qk:p\u2264\u03b8\u2217\u2264q Pr (Tk \u2265 mk, Lk = p,Rk = q)\n= \u2211\n(p,q)\u2208Qk:p\u2264\u03b8\u2217\u2264q Pr (Tk \u2265 mk|Lk = p,Rk = q) Pr (Lk = p,Rk = q)\nThus, in order to prove the query complexity of Algorithm 1 is O ( \u2211log 12\u01eb\u22121 k=0 mk ) , it suffices to show that Pr (Tk \u2265 mk | Lk = p,Rk = q) \u2264 \u03b42 log 12\u01eb for any k = 0, . . . , log 1 2\u01eb \u2212 1, (p, q) \u2208"}, {"heading": "Qk and p \u2264 \u03b8\u2217 \u2264 q.", "text": "For each k, p, q, define event Ek,p,q to be the event Lk = p,Rk = q. Define lk = q\u2212 p = ( 3 4 )k , Nk to be \u0398\u0303 (\n1 f(lk/4) l\u22122\u03b2k\n)\n. The logarithm factor of Nk is to be specified later. Define S (u) n and S (v) n to\nbe the size of array B(u) and B(v) before Line 16 respectively.\nTo show Pr (Tk \u2265 Nk | Ek,p,q) \u2264 \u03b42 log 12\u01eb , it suffices to show that on event Ek,p,q , with probability at least 1 \u2212 \u03b4\n2 log 12\u01eb , if n = Nk then at least one of the two calls to CheckSignificant between Line\n22 and Line 27 will return true.\nOn event Ek,p,q , if \u03b8\u2217 \u2208 [Lk,Mk] (note that on event Ek,p,q , Lk and Mk are deterministic), then |Vk \u2212 \u03b8\u2217| \u2265 lk4 . We will show\np1 := Pr\n(\nCheckSignificant\n(\n{\nB (v) i\n}S (v) Nk\ni=1 ,\n\u03b4\n4 log 12\u01eb\n) returns false | Ek,p,q )\n\u2264 \u03b4 2 log 12\u01eb\nTo prove this, we will first show that S(v)Nk , the length of the array B (v), is large with high probability, and then apply Lemma 2 to show that CheckSignificant will return true if S(v)Nk is large.\nBy definition, S(v)Nk = \u2211Nk i=1 A (v) i . By Condition 2, E\n[\nA (v) i | Ek,p,q\n]\n=\nPr (Y 6=\u22a5| X = Vk, Ek,p,q) \u2265 f ( lk 4 ) .\nOn event Ek,p,q , { A (v) i } is a sequence of i.i.d. random variables. By the multiplicative Chernoff bound, Pr (\nS (v) Nk \u2264 12Nkf ( lk 4 )\n| Ek,p,q ) \u2264 exp ( \u2212Nkf ( lk 4 ) /8 ) .\nNow,\np1 \u2264Pr ( CheckSignificant ( { B (v) i }S (v) Nk\ni=1 ,\n\u03b4\n4 log 12\u01eb\n)\nreturns false, S(v)Nk \u2265 1\n2 Nkf\n(\nlk 4\n) | Ek,p,q )\n+ Pr\n(\nS (v) Nk\n< 1\n2 Nkf\n(\nlk 4\n) | Ek,p,q )\nBy Condition 2 and |Vk \u2212 \u03b8\u2217| \u2265 lk4 , E [ B (v) i | Ek,p,q ] \u2265 C ( lk 4 )\u03b2 . On event Ek,p,q , { B (v) i } is a sequence of i.i.d. random variables. Thus, On event Ek,p,q , by Lemma 2, with probability at least\n1 \u2212 \u03b4 4 log 12\u01eb , CheckSignificant will return true if 12Nkf ( lk 4 )\n= \u0398 ( 1\nl2\u03b2 k\nln ln 1/\u01eb\u03b4 [ln ln]+ 1\nl2\u03b2 k\n)\n. We\nhave already proved Pr (\nS (v) Nk \u2264 12Nkf ( lk 4 )\n| Ek,p,q ) \u2264 exp ( \u2212Nkf ( lk 4 ) /8 ) . By setting Nk =\n\u0398 (\n1 f(lk/4) l\u22122\u03b2k ln ln 1/\u01eb \u03b4 [ln ln]+ 1\nl2\u03b2 k\n)\n, we can ensure p1 is at most \u03b4/2 log 12\u01eb .\nNow we have proved on event Ek,p,q , if \u03b8\u2217 \u2208 [Lk,Mk], then\nPr\n(\nCheckSignificant\n(\n{\nB (v) i\n}S (v) Nk\ni=1 ,\n\u03b4\n4 log 12\u01eb\n) returns true | Ek,p,q )\n\u2265 1\u2212 \u03b4 2 log 12\u01eb\nLikewise, on event Ek,p,q , if \u03b8\u2217 \u2208 [Mk, Rk], then\nPr\n(\nCheckSignificant\n(\n{ \u2212B(u)i }S\n(u) Nk\ni=1 ,\n\u03b4\n4 log 12\u01eb\n) returns true | Ek,p,q )\n\u2265 1\u2212 \u03b4 2 log 12\u01eb\nTherefore, we have shown Pr (Tk \u2265 Nk | Ek,p,q) \u2264 \u03b42 log 12\u01eb for any k, p, q. By (1), with probability at least 1\u2212 \u03b4, the number of samples queried is at most\nlog 12\u01eb\u22121 \u2211\nk=0\nO\n(\n1\nf( ( 3 4 )k /4)\n(\n3\n4\n)\u22122\u03b2k ln ln 1/\u01eb\n\u03b4 [ln ln]+\n(\n3\n4\n)\u22122k\u03b2)\n=O\n( \u01eb\u22122\u03b2\nf(\u01eb/2) ln\n1\n\u01eb\n(\nln 1\n\u03b4 + ln ln\n1\n\u01eb\n)\n[ln ln]+ 1\n\u01eb\n)\nProof of Theorem 3. For each k in Algorithm 1 at Line 3, Let lk = Rk \u2212 Lk. Let Nk = \u03b7 1f(lk/4) ln 4 log 12\u01eb \u03b4 , where \u03b7 is a constant to be specified later. As with the previous proof, it suffices to show Pr (Tk \u2265 Nk | Ek,p,q) \u2264 \u03b42 log 12\u01eb where event Ek,p,q is defined to be Lk = p,Rk = q, Tk is the number of iterations at the loop at Line 6.\nOn event Ek,p,q , we will show that the loop at Line 6 will terminate after n = Nk with probability at least 1\u2212 \u03b4\n2 log 12\u01eb .\nSuppose for now \u03b8\u2217 \u2208 [Mk, Rk]. Let Zi = A(u)i \u2212 A (m) i , \u03b6 = \u03b8 \u2217 \u2212 Mk. Clearly, |Zi| \u2264 1. On event Ek,p,q , sequence {Zi} is i.i.d.. By Condition 3, E [Zi | Ek,p,q ] = f(\u03b6 + lk4 ) \u2212 f(\u03b6) \u2265\ncf(\u03b6 + lk4 ) since \u03b6 \u2264 23 (\u03b6 + lk 4 ). Var [Zi|Ek,p,q ] = Var\n[\nA (u) i | Ek,p,q\n] + Var [\nA (m) i | Ek,p,q\n] (a)\n\u2264\nE\n[\nA (u) i | Ek,p,q\n] + E [\nA (m) i | Ek,p,q\n] = f(\u03b6 + lk4 ) + f(\u03b6) (b) \u2264 2f(\u03b6 + lk4 ) where (a) follows by\nAi \u2208 {0, 1} and (b) follows by the monotonicity of f . Thus, on event Ek,p,q , by Lemma 4, if we set \u03b7 sufficiently large (independent of lk, \u01eb, \u03b4), then with probability at least 1 \u2212 \u03b44 log 12\u01eb CheckSignificant-Var (\n{Zi}Nki=1 , \u03b44 log 12\u01eb ) in Procedure 2 returns true.\nSimilarly, we can show that on event Ek,p,q , if \u03b8\u2217 \u2208 [Lk,Mk], by Lemma 4, with probability at least 1\u2212 \u03b4\n4 log 12\u01eb , CheckSignificant-Var\n(\n{\nA (v) i \u2212A (m) i\n}Nk\ni=1 , \u03b4 4 log 12\u01eb\n)\nreturns true.\nTherefore, the loop at Line 6 will terminate after n = Nk with probability at least 1 \u2212 \u03b44 log 12\u01eb on event Ek,p,q . Therefore, with probability at least 1 \u2212 \u03b4, the number of samples queried is at most \u2211log 12\u01eb\u22121\nk=0 1\nf(( 34 ) k /4)\nln ln 1/\u01eb\u03b4 = O ( 1 f(\u01eb/2) ln 1 \u01eb ( ln 1\u03b4 + ln ln 1 \u01eb ) ) .\nA.3 The d-dimensional case\nTo prove the d-dimensional case, we only need to use a union bound to show that with high probability all calls of Algorithm 1 succeed, and consequently the output boundary g produced by polynomial interpolation is close to the true underlying boundary due to the smoothness assumption of g\u2217.\nProof of Theorem 8. For q \u2208 { 0, 1, . . . , M\u03b3 \u2212 1 }d\u22121 , define the \u201cpolynomial interpolation\u201d version of g\u2217 as g\u2217q (x\u0303) = \u2211\nl\u2208Iq\u2229L g\u2217(l)Qq,l(x\u0303)\nRecall that we choose M = O ( \u01eb\u22121/\u03b3 ) .\nBy Theorem 1, each run of Algorithm 1 at the line 3 of Algorithm 3 will return a gl such that \u2223 \u2223gl \u2212 g\u2217q (l) \u2223 \u2223 \u2264 \u01eb with probability at least 1\u2212 \u03b4/2Md\u22121.\n\u2016g \u2212 g\u2217\u2016 = \u2211\nq\u2208{0,...,M/\u03b3\u22121}d\u22121 \u2016(gq \u2212 g\u2217)1{x\u0303 \u2208 Iq}\u2016\n\u2264 \u2211\nq\u2208{0,...,M/\u03b3\u22121}d\u22121\n\u2225 \u2225 ( gq \u2212 g\u2217q ) 1{x\u0303 \u2208 Iq} \u2225 \u2225+ \u2225 \u2225 ( g\u2217q \u2212 g\u2217 ) 1{x\u0303 \u2208 Iq} \u2225 \u2225\n\u2225 \u2225 ( g\u2217q \u2212 g\u2217 ) 1{x\u0303 \u2208 Iq} \u2225 \u2225 =\n\u02c6\nIq\n\u2223 \u2223g\u2217q (x\u0303)\u2212 g\u2217(x\u0303) \u2223 \u2223 dx\u0303\n= O\n(\n\u02c6\nIq\nM\u2212\u03b3dx\u0303\n)\n= O ( M\u2212\u03b3\u2212d+1 )\nThe second equality follows from Lemma 3 of [6] that |gq(x\u0303)\u2212 g\u2217(x\u0303)| = O (M\u2212\u03b3) since g\u2217 is \u03b3-H\u00f6lder smooth.\n\u2225 \u2225 ( gq \u2212 g\u2217q ) 1{x\u0303 \u2208 Iq} \u2225 \u2225\n= \u2211\nl\u2208Iq\u2229L\n\u2223 \u2223gl \u2212 g\u2217q (l) \u2223 \u2223 \u2016Qq,l\u2016\n\u2264 \u2211\nl\u2208Iq\u2229L \u01eb \u2016Qq\u2016\n=O(\u01ebM\u2212d+1)\nTherefore, overall we have \u2016g \u2212 g\u2217\u2016 \u2264 O ( M\u2212\u03b3\u2212d+1 + \u01ebM\u2212d+1 )\n(\nM \u03b3\n)d\u22121 = O(\u01eb).\nProof of Theorem 9. By Theorem 2, each run of Algorithm 1 at the line 3 of Algorithm 3 will make O\u0303 (\nd f(\u01eb/2)\u01eb\n\u22122\u03b2 )\nqueries with probability at least 1 \u2212 \u03b4/Md\u22121, thus by a union bound, the total\nnumber of queries made is O\u0303 (\nd f(\u01eb/2)\u01eb\n\u22122\u03b2\u2212d\u22121 \u03b3\n)\nwith probability at least 1\u2212 \u03b4.\nProof of Theorem 10. The proof is similar to the previous proof."}, {"heading": "B Proof of lower bounds", "text": "First, we introduce some notations for this section. Given a labeler L and an active learning algorithm A, denote by PnL,A the distribution of n samples {(Xi, Yi)} n i=1 where Yi is drawn from distribution PL(Y |Xi) and Xi is drawn by the active learning algorithm based solely on the knowledge of {(Xj , Yj)}i\u22121j=1. We will drop the subscripts from PnL,A and PL(Y |X) when it is clear from the context. For a sequence {Xi}\u221ei=1 denote by Xn the subsequence {X1, . . . , Xn}. Definition 1. For any distributions P,Q on a countable support, define KL-divergence as dKL (P,Q) = \u2211\nx P (x) ln P (x)Q(x) . For two random variables X,Y , define the mutual information\nas I(X ;Y ) = dKL (P (X,Y ) \u2016 P (X)P (Y )). We will use Fano\u2019s method shown as below to prove the lower bounds.\nLemma 5. Let \u0398 be a class of parameters, and {P\u03b8 : \u03b8 \u2208 \u0398} be a class of probability distributions indexed by \u0398 over some sample space X . Let d : \u0398 \u00d7 \u0398 \u2192 R be a semi-metric. Let V = {\u03b81, . . . , \u03b8M} \u2286 \u0398 such that \u2200i 6= j, d(\u03b8i, \u03b8j) \u2265 2s > 0. Let P\u0304 = 1M \u2211 \u03b8\u2208V P\u03b8 . If dKL ( P\u03b8 \u2016 P\u0304 )\n\u2264 \u03b4 for any \u03b8 \u2208 V , then for any algorithm \u03b8\u0302 that given a sample X drawn from P\u03b8 outputs \u03b8\u0302(X) \u2208 \u0398, the following inequality holds:\nsup \u03b8\u2208\u0398 P\u03b8\n( d(\u03b8, \u03b8\u0302(X)) \u2265 s ) \u2265 1\u2212 \u03b4 + ln 2 lnM\nProof. For any algorithm \u03b8\u0302, define a test function \u03a8\u0302 : X \u2192 {1, . . . ,M} such that \u03a8\u0302(X) = argmini\u2208{1,...,M} d(\u03b8\u0302(X), \u03b8i). We have\nsup \u03b8\u2208\u0398 P\u03b8\n( d(\u03b8, \u03b8\u0302(X)) \u2265 s )\n\u2265 max \u03b8\u2208V P\u03b8\n( d(\u03b8, \u03b8\u0302(X)) \u2265 s )\n\u2265 max i\u2208{1,...,M} P\u03b8i\n( \u03a8\u0302(X) 6= i )\nLet V be a random variable uniformly taking values from V , and X be drawn from PV . By Fano\u2019s Inequality, for any test function \u03a8 : X \u2192 {1, . . . ,M}\nmax i\u2208{1,...,M}\nP\u03b8i (\u03a8(X) 6= i) \u2265 1\u2212 I(V ;X) + ln 2\nlnM\nThe desired result follows by the fact that I(V ;X) = 1M \u2211 \u03b8\u2208V dKL ( P\u03b8 \u2016 P\u0304 ) .\nB.1 The one dimensional case\nProof of Theorem 5. 2 Without lose of generality, let C = C\u2032 = 1 (C is defined in Condition 2). Let \u01eb \u2264 14 min { ( 1 2 )1/\u03b2 , ( 4 5 )1/\u03b1 , 14 } . We will prove the desired result using Lemma 5.\nFirst, we construct V and P\u03b8 . For any k \u2208 {0, 1, 2, 3}, let PLk(Y | X) be the distribution of the labeler Lk\u2019s response with the ground truth \u03b8k = k\u01eb:\n2Actually we can use Le Cam\u2019s method to prove this one dimensional case (which only needs to construct 2 distributions instead of 4 here), but this proof can be generalized to the multidimensional case more easily.\nPLk (Y =\u22a5 |x) = 1\u2212 \u2223 \u2223 \u2223\n\u2223 x\u2212 1 2 \u2212 k\u01eb\n\u2223 \u2223 \u2223 \u2223 \u03b1\nPLk (Y = 0|x) =\n\n\n\n( x\u2212 12 \u2212 k\u01eb )\u03b1\n(\n1\u2212 ( x\u2212 12 \u2212 k\u01eb )\u03b2 )\n/2 x > 12 + k\u01eb (\n1 2 + k\u01eb\u2212 x\n)\u03b1 (\n1 + ( 1 2 + k\u01eb\u2212 x\n)\u03b2 )\n/2 x \u2264 12 + k\u01eb\nPLk (Y = 1|x) =\n\n\n\n( x\u2212 12 \u2212 k\u01eb )\u03b1\n(\n1 + ( x\u2212 12 \u2212 k\u01eb )\u03b2 )\n/2 x > 12 + k\u01eb (\n1 2 + k\u01eb\u2212 x\n)\u03b1 (\n1\u2212 ( 1 2 + k\u01eb\u2212 x\n)\u03b2 )\n/2 x \u2264 12 + k\u01eb\nClearly, PLk complies with Conditions 1, 2 and 3. Define Pnk to be the distribution of n samples {(Xi, Yi)} n i=1 where Yi is drawn from distribution PLk(Y |Xi) and Xi is drawn by the active learning algorithm based solely on the knowledge of {(Xj , Yj)}i\u22121j=1.\nDefine P\u0304L = 14 \u2211 j PLj and P\u0304 n = 14 \u2211 j P n k . We take \u0398 to be [0, 1], and d(\u03b81, \u03b82) = |\u03b81 \u2212 \u03b82| in Lemma 5. To use Lemma 5, we need to bound dKL ( Pnk \u2016 P\u0304n )\nfor k \u2208 {0, 1, 2, 3}. For any k \u2208 {0, 1, 2, 3} ,\ndKL ( Pnk \u2016 P\u0304n0 )\n=EPn k\n(\nln Pnk ({(Xi, Yi)} n i=1)\nP\u0304n ({(Xi, Yi)}ni=1)\n)\n=EPn k\n(\nln Pnk (X1)P n k (Y1 | X1)Pnk (X2 | X1, Y1) \u00b7 \u00b7 \u00b7Pnk (Yn | X1, Y1, . . . , Xn)\nP\u0304n (X1) P\u0304n (Y1 | X1) P\u0304n (X2 | X1, Y1) \u00b7 \u00b7 \u00b7 P\u0304n (Yn | X1, Y1, . . . , Xn)\n)\n(a) =EPn\nk\n(\nln \u03a0ni=1PLk (Yi|Xi) \u03a0ni=1P\u0304L (Yi|Xi)\n)\n(2)\n=\nn \u2211\ni=1\nEPn k\n(\nEPn k\n(\nln PLk (Yi|Xi) P\u0304L (Yi|Xi) | Xn ))\n\u2264n max x\u2208[0,1] dKL ( PLk(Y | x) \u2016 P\u0304L(Y | x) )\n(a) follows by the fact that Pnk (Xi+1 | X1, Y1, . . . Xi, Yi) = P\u0304n (Xi+1 | X1, Y1, . . . , Xi, Yi) since Xi+1 is drawn by the same active learning algorithm based solely on the knowledge of {(Xj , Yj)}ij=1 regardless of the labeler\u2019s response distribution, and the fact that Pnk (Yi | X1, Y1, . . . , Xi) = PLk (Yi|Xi) and P\u0304n (Yi | X1, Y1, . . . , Xi) = P\u0304L (Yi|Xi) by definition.\nFor any k \u2208 {1, 2, 3}, x \u2208 [0, 1],\nP\u0304L(\u00b7 | x) \u2265 PL0(\u00b7 | x) + PLk(\u00b7 | x)\n4 (3)\nFor any k \u2208 {0, 1, 2, 3}, x \u2208 [0, 1], y \u2208 {1,\u22121,\u22a5}\n( P\u0304L(Y = y | x)\u2212 PLk(Y = y | x) )2\n=\n\n\n\u2211\nj\n1\n4\n( PLj (Y = y | x)\u2212 PL0(Y = y | x) ) + (PL0(Y = y | x)\u2212 PLk(Y = y | x))\n\n\n2\n\u2264\n\n\n5\n16\n\u2211\nj>0\n( PLj (Y = y | x)\u2212 PL0(Y = y | x) )2 + 5 (PL0(Y = y | x)\u2212 PLk(Y = y | x))2  \n\u22646 \u2211\nj>0\n( PLj (Y = y | x)\u2212 PL0(Y = y | x) )2\n(4)\nwhere the first inequality follows by (\n\u22114 i=0 ai\n)2\n\u2264 5 \u22114 i=0 a 2 i by letting aj =\n1 4\n( PLj (Y = y | x) \u2212 PL0(Y = y | x) ) for j = 0, . . . , 3 and a4 = PL0(Y = y | x)\u2212PLk(Y = y | x), and noting that a0 = 0 under this setting.\nThus,\ndKL ( PLk(Y | x) \u2016 P\u0304L(Y | x) )\n\u2264 \u2211\ny\n1\nP\u0304L(Y = y | x) ( PLk(Y = y | x)\u2212 P\u0304L(Y = y | x) )2\n\u226424 \u2211\nj>0\n\u2211\ny\n1\nPLj (y | x) + PL0(y | x) ( PLj (Y = y | x)\u2212 PL0(Y = y | x) )2\n\u2264O(\u01eb\u03b1)\nThe first inequality follows from Lemma 10. The second inequality follows by (3) and (4). The last inequality follows by applying Lemma 11 to PL0(\u00b7 | x) and PLj (\u00b7 | x) and the assumption \u03b1 \u2264 2. Therefore, we have dKL ( Pnk \u2016 P\u0304n0 ) = nO(\u01eb\u03b1). By setting n = \u01eb\u2212\u03b1, we get dKL ( Pnk \u2016 P\u0304n0 )\n\u2264 O (1), and thus by Lemma 5,\nsup \u03b8 P\u03b8\n( d(\u03b8, \u03b8\u0302(X)) \u2265 \u2126 (\u01eb) ) \u2265 1\u2212 O (1) + ln 2 ln 4 = O (1)\nB.2 The d-dimensional case\nAgain, we will use Lemma 5 to prove the lower bounds for d-dimensional cases. We first construct {P\u03b8 : \u03b8 \u2208 \u0398} using a similar idea with [6], and then use Lemma 12 to select a subset \u0398\u0303 \u2282 \u0398 to apply Lemma 5.\nProof of Theorem 6. Again, without lose of generality, let C = 1. Recall that for x = (x1, . . . , xd) \u2208 Rd, we have defined x\u0303 to be (x1, . . . , xd\u22121). Define m = ( 1 \u01eb )1/\u03b3 . L = {\n0, 1m , . . . , m\u22121 m }d\u22121 , h(x\u0303) = \u03a0d\u22121i=1 exp\n(\n\u2212 1 1\u22124x2i\n)\n1 { |xi| < 12 } , \u03c6l(x\u0303) = Km\u2212\u03b3h(m(x\u0303\u2212 l)\u2212 1 2 ) where l \u2208 L. It is easy to check \u03c6l(x\u0303) is (K, \u03b3)-H\u00f6lder smooth and has bounded support [l1, l1 + 1 m ]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [ld\u22121, ld\u22121 + 1m ], which implies that for different l1, l2 \u2208 L, the support of \u03c6l1 and \u03c6l2 do not intersect.\nLet \u2126 = {0, 1}md\u22121. For any \u03c9 \u2208 \u2126, define g\u03c9(x\u0303) = \u2211 l\u2208L \u03c9l\u03c6l(x\u0303). For each \u03c9 \u2208 \u2126, define the conditional distribution of labeler L\u03c9\u2019s response as follows:\nFor xd \u2264 A, PL\u03c9 (y =\u22a5 |x) = 1 \u2212 f(A), PL\u03c9 (y 6= I(xd > g\u03c9(x\u0303))|x, y 6=\u22a5) = 1 2 ( 1\u2212 |xd \u2212 g\u03c9(x\u0303)|\u03b2 ) ;\nFor xd \u2265 A, PL\u03c9 (y =\u22a5 |x) = 1\u2212 f(xd), PL\u03c9 (y 6= I(xd > g\u03c9(x\u0303))|x, y 6=\u22a5) = 12 ( 1\u2212 x\u03b2d ) .\nHere, A = cmax\u03c6(x\u0303) = c\u2032\u01eb for some constants c, c\u2032.\nIt can be easily verified that PL\u03c9 satisfies Conditions 1 and 2. Note that g\u03c9(x\u0303) can be seen as the underlying decision boundary for labeler PL\u03c9 .\nDefine Pn \u03c9 to be the distribution of n samples {(Xi, Yi)}ni=1 where Yi is drawn from distribution PL\u03c9 (Y |Xi) and Xi is drawn by the active learning algorithm based solely on the knowledge of {(Xj , Yj)}i\u22121j=1.\nBy Lemma 12, when \u01eb is small enough so that md\u22121 is large enough, there is a subset {\n\u03c9 (1), . . . ,\u03c9(M)\n} \u2282 \u2126 such that \u2225 \u2225\u03c9 (i) \u2212 \u03c9(j) \u2225 \u2225 0 \u2265 md\u22121/12 for any 0 \u2264 i < j \u2264 M and\nM \u2265 2md\u22121/48. Define Pni = Pn\u03c9(i) , P\u0304n = 1 M \u2211M i=1 P n i .\nNext, we will apply Lemma 5 to { \u03c9 (1), . . . ,\u03c9(M) } with d(\u03c9(i),\u03c9(j)) = \u2016g \u03c9 (i) \u2212 g \u03c9 (j)\u2016. We will lower-bound d(\u03c9(i),\u03c9(j)) and upper-bound dKL ( Pni \u2016 P\u0304n ) .\nFor any 1 \u2264 i < j \u2264 M , \u2016g\n\u03c9 (i) \u2212 g \u03c9 (j)\u2016\n= \u2211\nl\u2208{1,...,m}d\u22121\n\u2223 \u2223 \u2223\u03c9 (i) l \u2212 \u03c9 (j) l \u2223 \u2223 \u2223Km\u2212\u03b3\u2212(d\u22121) \u2016h\u2016\n\u2265md\u22121/12 \u2217Km\u2212\u03b3\u2212(d\u22121) \u2016h\u2016 =Km\u2212\u03b3 \u2016h\u2016 /12 =\u0398 (\u01eb)\nBy the convexity of KL-divergence, dKL ( Pni \u2016 P\u0304n ) \u2264 1M \u2211M j=1 dKL ( Pni \u2016 Pnj )\n, so it suffices to upper-bound dKL ( Pni \u2016 Pnj ) for any i, j.\nFor any 1 < i, j \u2264 M , dKL ( Pni \u2016 Pnj )\n\u2264n max x\u2208[0,1]d dKL\n(\nPnL \u03c9 (i) (Y | x) \u2016 PnL \u03c9 (j)\n(Y | x) )\n=n max x\u2208[0,1]d PnL \u03c9 (i) (Y 6=\u22a5| x)dKL\n(\nPnL \u03c9 (i) (Y | x, Y 6=\u22a5) \u2016 PnL \u03c9 (j)\n(Y | x, Y 6=\u22a5) )\nThe inequality follows as (2) in the proof of Theorem 5. The equality follows since P\u03c9(y =\u22a5 |x) is the same for all \u03c9 \u2208 \u2126. If xd \u2265 A, then PnL\n\u03c9 (i) (Y | x, Y 6=\u22a5) = PnL \u03c9 (j) (Y | x, Y 6=\u22a5), so dKL (\nPnL \u03c9 (i) (Y | x, Y 6=\u22a5) \u2016 PnL \u03c9 (j)\n(Y | x, Y 6=\u22a5) )\n= 0. If xd < A, then PnL \u03c9 (i) (Y 6=\u22a5| x) =\nf(A). Therefore,\ndKL ( Pni \u2016 Pnj ) \u2264 nf(A) max x\u2208[0,1]d dKL\n(\nPnL \u03c9 (i) (Y | x, Y 6=\u22a5) \u2016 PnL \u03c9 (j)\n(Y | x, Y 6=\u22a5) )\n.\nApply Lemma 10 to PnL \u03c9 (i) (Y | x, Y 6=\u22a5) and PnL \u03c9 (i) (Y | x, Y 6=\u22a5), and noting they are bounded\nabove by a constant, we have max x\u2208[0,1]d dKL\n(\nPnL \u03c9 (i) (Y | x, Y 6=\u22a5) \u2016 PnL \u03c9 (j)\n(Y | x, Y 6=\u22a5) ) =\nO ( A2\u03b2 ) . Thus,\ndKL ( Pni \u2016 Pnj ) \u2264 nf(A)O ( A2\u03b2 ) = nf(c\u2032\u01eb)O(\u01eb2\u03b2)\nBy setting n = 1f(c\u2032\u01eb)\u01eb \u22122\u03b2\u2212 d\u22121 \u03b3 , we get dKL ( Pni \u2016 Pnj ) \u2264 O ( \u01eb\u2212 d\u22121 \u03b3 ) . The desired results follows by Lemma 5.\nThe proof of Theorem 7 follows the same structure.\nProof of Theorem 7. As in the proof of Theorem 6, let C = C\u2032 = 1, and define m = ( 1 \u01eb )1/\u03b3 . L = {\n0, 1m , . . . , m\u22121 m }d\u22121 , h(x\u0303) = \u03a0d\u22121i=1 exp\n(\n\u2212 1 1\u22124x2i\n)\n1 { |xi| < 12 } , \u03c6l(x\u0303) = Km\u2212\u03b3h(m(x\u0303\u2212 l) \u2212 12 ) where l \u2208 L. Let \u2126 = {0, 1}m d\u22121 . For any \u03c9 \u2208 \u2126, define g\u03c9(x\u0303) = 12 + \u2211\nl\u2208L \u03c9l\u03c6l(x\u0303), which can be seen as a decision boundary. A = max\u03c6(x\u0303) = c\u2032\u01eb for some constants c\u2032.\nLet g+(x\u0303) = g(1,1,...,1)(x\u0303) = \u2211\nl\u2208L \u03c6l(x\u0303), g\u2212(x\u0303) = g(0,0,...,0)(x\u0303) = 0. In other words, g+ is the \u201chighest\u201d boundary, and g\u2212 is the \u201clowest\u201d boundary.\nFor each \u03c9 \u2208 \u2126, define the conditional distribution of labeler L\u03c9\u2019s response as follows:\nPL\u03c9 (y =\u22a5 |x) = 1\u2212 |xd \u2212 g\u03c9(x\u0303)|\u03b1\nPL\u03c9 (y 6= I(xd > g\u03c9(x\u0303))|x, y 6=\u22a5) = 1\n2\n( 1\u2212 |xd \u2212 g\u03c9(x\u0303)|\u03b2 )\nIt can be easily verified that PL\u03c9 satisfies Conditions 1, 2, and 3.\nLet P+(\u00b7 | x) = PL(1,1,...,1)(\u00b7 | x), P\u2212(\u00b7 | x) = PL(0,0,...,0)(\u00b7 | x). By the construction of g, for any x \u2208 [0, 1]d, any \u03c9 \u2208 \u2126, PL\u03c9 (\u00b7 | x) equals either P+(\u00b7 | x) or P\u2212(\u00b7 | x). Define Pn\n\u03c9 to be the distribution of n samples {(Xi, Yi)}ni=1 where Yi is drawn from distribution\nPL\u03c9 (Y |Xi) and Xi is drawn by the active learning algorithm based solely on the knowledge of {(Xj , Yj)}i\u22121j=1.\nBy Lemma 12, when \u01eb is small enough so that md\u22121 is large enough\u201e there is a subset \u2126\u2032 = {\n\u03c9 (1), . . . ,\u03c9(M)\n} \u2282 \u2126 such that (i) (well-separated) \u2225 \u2225\u03c9 (i) \u2212 \u03c9(j) \u2225 \u2225 0 \u2265 md\u22121/12 for any 0 \u2264 i <\nj \u2264 M , M \u2265 2md\u22121/48; and (ii) (well-balanced) for any j = 1, . . . ,md\u22121, 124 \u2264 1M \u2211M i=1 \u03c9 (i) j \u2264\n3 24 .\nDefine Pni = P n \u03c9 (i) , P\u0304 n = 1M \u2211M i=1 P n i . Define PLi = PL \u03c9 (i) , P\u0304L = 1M \u2211M i=1 PLi . By the well-balanced property, for any x \u2208 [0, 1]d, P\u0304L(\u00b7 | x) is between 124P+(\u00b7 | x) + 2324P\u2212(\u00b7 | x) and 3 24P+(\u00b7 | x) + 2124P\u2212(\u00b7 | x). Therefore\nP\u0304L(\u00b7 | x) \u2265 1\n24 (P+(\u00b7 | x) + P\u2212(\u00b7 | x)) (5)\nMoreover, since PLi(\u00b7 | x) can only take P+(\u00b7 | x) or P\u2212(\u00b7 | x) for any x, \u2223\n\u2223PLi(\u00b7 | x)\u2212 P\u0304L(\u00b7 | x) \u2223 \u2223 \u2264 |P+(\u00b7 | x)\u2212 P\u2212(\u00b7 | x)| (6)\nNext, we will apply Lemma 5 to { \u03c9 (1), . . . ,\u03c9(M) } with d(\u03c9(i),\u03c9(j)) = \u2016g \u03c9 (i) \u2212 g \u03c9 (j)\u2016. We already know from the proof of Theorem 6 \u2016g\n\u03c9 (i) \u2212 g \u03c9 (j)\u2016 = \u2126(\u01eb).\nFor any 0 < i \u2264 M , dKL ( Pni \u2016 P\u0304n0 ) \u2264 nmax x\u2208[0,1]d dKL ( PLi(Y | x) \u2016 P\u0304L(Y | x) )\n. For any x \u2208 [0, 1]d,\ndKL ( PLi(Y | x) \u2016 P\u0304L(Y | x) )\n\u2264 \u2211\ny\n1\nP\u0304L(Y = y | x) ( PLi(Y = y | x)\u2212 P\u0304L(Y = y | x) )2\n\u2264 \u2211\ny\n24\nP+(y | x) + P\u2212(y | x) (P+(Y = y | x)\u2212 P\u2212(Y = y | x))2\n\u2264O(A\u03b1)\nThe first inequality follows from Lemma 10. The second inequality follows by (5) and (6). The last inequality follows by applying Lemma 11 to P+(\u00b7 | x) and P\u2212(\u00b7 | x), setting the \u01eb in Lemma 11 to be g\u03c9(x\u0303), and using g\u03c9(x\u0303) \u2264 A and the assumption \u03b1 \u2264 2.\nTherefore, we have\ndKL (P n i \u2016 Pn0 ) \u2264 nO (A\u03b1) = nO(\u01eb\u03b1)\nBy setting n = \u01eb\u2212\u03b1\u2212 d\u22121 \u03b3 , we get dKL (Pni \u2016 Pn0 ) \u2264 O\n(\n\u01eb\u2212 d\u22121 \u03b3\n)\n. Thus by Lemma 5,\nsup \u03b8 P\u03b8\n( d(\u03b8, \u03b8\u0302(X)) \u2265 \u2126 (\u01eb) ) \u2265 1\u2212 O ( \u01eb\u2212 d\u22121 \u03b3 ) + ln 2\n\u01eb\u2212 d\u22121 \u03b3 /48\n= O (1)\n, from which the desired result follows."}, {"heading": "C Technical lemmas", "text": "C.1 Concentration bounds\nIn this subsection, we define Y1, Y2, . . . to be a sequence of i.i.d. random variables. Assume Y1 \u2208 [\u22122, 2], EY1 = 0, Var(Y1) = \u03c32 \u2264 4. Define Vn = nn\u22121 ( \u2211n i=1 Y 2 i \u2212 1n ( \u2211n i=1 Yi) 2 ) . It is easy to check EVn = n\u03c32.\nWe need following two results from [21]\nLemma 6. ([21], Theorem 2) Take any 0 < \u03b4 < 1. Then there is an absolute constant D0 such that with probability at least 1\u2212 \u03b4, for all n simultaneously,\n\u2223 \u2223 \u2223 \u2223 \u2223 n \u2211\ni=1\nYi\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2264 D0 ( 1 + ln 1 \u03b4 + \u221a n\u03c32 [ln ln]+ (n\u03c3 2) + n\u03c32 ln 1 \u03b4 )\nLemma 7. ([21], Lemma 3) Take any 0 < \u03b4 < 1. Then there is an absolute constant K0 such that with probability at least 1\u2212 \u03b4, for all n simultaneously,\nn\u03c32 \u2264 K0 ( 1 + ln 1\n\u03b4 +\nn \u2211\ni=1\nY 2i\n)\nWe note that Proposition 1 is immediate from Lemma 6 since Var(Yi) \u2264 4. Lemma 8. Take any 0 < \u03b4 < 1. Then there is an absolute constant K3 such that with probability at least 1\u2212 \u03b4, for all n \u2265 ln 1\u03b4 simultaneously,\nn\u03c32 \u2264 K3 ( 1 + ln 1\n\u03b4 + Vn\n)\nProof. By Lemma 7, with probability at least 1\u2212 \u03b4/2, for all n,\nn\u03c32 \u2264 K0 ( n \u2211\ni=1\nY 2i + ln 2\n\u03b4 + 1\n)\n= K0\n\n n\u2212 1 n Vn + 1 n\n(\nn \u2211\ni=1\nYi\n)2\n+ ln 2\n\u03b4 + 1\n\n\nBy Lemma 6, with probability at least 1\u2212 \u03b4/2, for all n,\n1\nn\n(\nn \u2211\ni=1\nYi\n)2\n< 1\nn\n(\nD0\n(\n1 + ln 2\n\u03b4 +\n\u221a\nn\u03c32 [ln ln]+ (n\u03c3 2) + n\u03c32 ln\n2\n\u03b4\n))2\n= D20 n\n(\n1 + ln 2\n\u03b4\n)2\n+D20\u03c3 2 [ln ln]+ (n\u03c3 2) +D20\u03c3 2 ln\n2\n\u03b4\n+2D20\n(\n1 + ln 2\n\u03b4\n)\n\u221a\n\u03c32 [ln ln]+ (n\u03c3 2) + \u03c32 ln 2\u03b4\nn\n\u2264 K1 ( 1 + ln 1\n\u03b4 + [ln ln]+ (n\u03c3\n2)\n)\nfor some absolute constant K1. The last inequality follows by n \u2265 ln 1\u03b4 . Thus, by a union bound, with probability at least 1\u2212 \u03b4, for all n, n\u03c32 \u2264 K0Vn +K0(K1+2) ln 1\u03b4 + K0K1 [ln ln]+ (n\u03c3 2) +K0(K1 + 3).\nLet K2 > 0 be an absolute constant such that \u2200x \u2265 K2, K0K1 [ln ln]+ x \u2264 x2 .\nNow if n\u03c32 \u2265 K2, then n\u03c32 \u2264 K0Vn +K0(K1 + 2) ln 1\u03b4 + n\u03c3 2 2 +K0(K1 + 3), and thus\nn\u03c32 \u2264 2K0Vn + 2K0(K1 + 2) ln 1\n\u03b4 + 2K0(K1 + 3) +K2 (7)\nIf n\u03c32 \u2264 K2, clearly (7) holds. This concludes the proof.\nWe note that Proposition 2 is immediate by applying above lemma to Lemma 6.\nLemma 9. Take any \u03b4, n > 0. Then with probability at least 1\u2212 \u03b4,\nVn \u2264 4n\u03c32 + 8 ln 1\n\u03b4\nProof. Applying Bernstein\u2019s Inequality to Y 2i , and noting that Var(Y 2 i ) \u2264 4\u03c32 since |Yi| \u2264 2, we have with probability at least 1\u2212 \u03b4, n \u2211\ni=1\nY 2i \u2264 4\n3 ln\n1 \u03b4 + n\u03c32 +\n\u221a\n8n\u03c32 ln 1\n\u03b4\n\u2264 4 ln 1 \u03b4 + 2n\u03c32\nThe last inequality follows by the fact that \u221a 4ab \u2264 a+ b.\nThe desired result follows by noting that Vn = nn\u22121\n(\n\u2211n i=1 Y 2 i \u2212 1n ( \u2211n i=1 Yi)\n2 )\n\u2264 2\u2211ni=1 Y 2i .\nC.2 Bounds of distances among probability distributions\nLemma 10. If P,Q are two probability distributions on a countable support X , then\ndKL (P \u2016 Q) \u2264 \u2211\nx\n(P (x)\u2212Q(x))2 Q(x)\nProof.\ndKL (P \u2016 Q) = \u2211\nx\nP (x) ln P (x)\nQ(x)\n\u2264 \u2211\nx\nP (x)\n(\nP (x) Q(x) \u2212 1\n)\n= \u2211\nx\n(P (x) \u2212Q(x))2 Q(x)\nThe first inequality follows by lnx \u2264 x\u22121. The second equality follows by\u2211x P (x) ( P (x) Q(x) \u2212 1 )\n= \u2211\nx\n(\nP 2(x)\u2212P (x)Q(x) Q(x) \u2212 P (x) +Q(x)\n)\n= \u2211 x (P (x)\u2212Q(x))2 Q(x) .\nDefine\nP0 (Y =\u22a5 |x) = 1\u2212 \u2223 \u2223 \u2223\n\u2223 x\u2212 1 2\n\u2223 \u2223 \u2223 \u2223 \u03b1\nP0 (Y = 0|x) =\n\n\n\n( x\u2212 12 )\u03b1\n(\n1\u2212 ( x\u2212 12 )\u03b2 )\n/2 x > 12 (\n1 2 \u2212 x\n)\u03b1 (\n1 + ( 1 2 \u2212 x\n)\u03b2 )\n/2 x \u2264 12\nP0 (Y = 1|x) =\n\n\n\n( x\u2212 12 )\u03b1\n(\n1 + ( x\u2212 12 )\u03b2 )\n/2 x > 12 (\n1 2 \u2212 x\n)\u03b1 (\n1\u2212 ( 1 2 \u2212 x\n)\u03b2 )\n/2 x \u2264 12\nand\nP1 (Y =\u22a5 |x) = 1\u2212 \u2223 \u2223 \u2223\n\u2223 x\u2212 \u01eb \u2212 1 2\n\u2223 \u2223 \u2223 \u2223 \u03b1\nP1 (Y = 0|x) =\n\n\n\n( x\u2212 \u01eb\u2212 12 )\u03b1\n(\n1\u2212 ( x\u2212 \u01eb\u2212 12 )\u03b2 )\n/2 x > \u01eb+ 12 (\n\u01eb+ 12 \u2212 x )\u03b1\n(\n1 + ( \u01eb+ 12 \u2212 x )\u03b2\n)\n/2 x \u2264 \u01eb+ 12\nP1 (Y = 1|x) =\n\n\n\n( x\u2212 \u01eb\u2212 12 )\u03b1\n(\n1 + ( x\u2212 \u01eb\u2212 12 )\u03b2 )\n/2 x > \u01eb+ 12 (\n\u01eb+ 12 \u2212 x )\u03b1\n(\n1\u2212 ( \u01eb+ 12 \u2212 x )\u03b2\n)\n/2 x \u2264 \u01eb+ 12 Lemma 11. Let P0, P1 be the distributions defined above. If x \u2208 [0, 1], \u01eb \u2264 min { (\n1 2\n)1/\u03b2 , (\n4 5 )1/\u03b1 , 14\n}\n, then\n\u2211\ny\n(P0(Y = y|x)\u2212 P1(Y = y|x))2 P0(Y = y|x) + P1(Y = y|x) = O ( \u01eb\u03b1 + \u01eb2 )\n(8)\nProof. By symmetry, it suffices to show for 0 \u2264 x \u2264 1+\u01eb2 . Let t = 12 + \u01eb\u2212 x. We first show (8) holds for \u01eb2 \u2264 t \u2264 \u01eb (i.e. 12 \u2264 x \u2264 1+\u01eb2 ).\nWe claim miny (P0(Y = y|X = t) + P1(Y = y|X = t)) \u2265 12 ( \u01eb 2 )\u03b1 . This is because:\n\u2022 P0(Y =\u22a5 |X = t) + P1(Y =\u22a5 |X = t) = 1 \u2212 (\u01eb \u2212 t)\u03b1 + 1 \u2212 t\u03b1 \u2265 2\u2212 2\u01eb\u03b1 \u2265 12 ( \u01eb 2\n)\u03b1\nwhere the last inequality follows by \u01eb \u2264 ( 4 5 )1/\u03b1 ;\n\u2022 2 (P0(Y = 0|X = t) + P1(Y = 0|X = t)) = (\u01eb\u2212 t)\u03b1 ( 1\u2212 (\u01eb\u2212 t)\u03b2 ) + t\u03b1 ( 1 + t\u03b2 )\n\u2265 t\u03b1 ( 1 + t\u03b2 ) \u2265 (\n\u01eb 2 )\u03b1 . Therefore, P0(Y = 0|X = t) + P1(Y = 0|X = t) \u2265 12 ( \u01eb 2 )\u03b1 .\n\u2022 Similarly, P0(Y = 1|X = t) + P1(Y = 1|X = t) \u2265 12 ( \u01eb 2 )\u03b1 .\nBesides,\n\u2211\ny\n(P0(Y = y|X = t)\u2212 P1(Y = y|X = t))2\n=(t\u03b1 \u2212 (\u01eb\u2212 t)\u03b1)2 + 1 4 ( t\u03b1 ( 1\u2212 t\u03b2 ) \u2212 (\u01eb \u2212 t)\u03b1 ( 1 + (\u01eb\u2212 t)\u03b2 ))2\n+ 1\n4\n(\nt\u03b1 ( 1 + t\u03b2 ) \u2212 (\u01eb\u2212 t)\u03b1 ( 1\u2212 (\u01eb \u2212 t)\u03b2 ))2\n=(t\u03b1 \u2212 (\u01eb\u2212 t)\u03b1)2 + 1 4 ( t\u03b1 \u2212 (\u01eb\u2212 t)\u03b1 \u2212 t\u03b1+\u03b2 \u2212 (\u01eb\u2212 t)\u03b1+\u03b2 )2\n+ 1\n4\n( t\u03b1 \u2212 (\u01eb\u2212 t)\u03b1 + t\u03b1+\u03b2 + (\u01eb\u2212 t)\u03b1+\u03b2 )2\n(a) \u2264 (t\u03b1 \u2212 (\u01eb\u2212 t)\u03b1)2 + 1 2 (t\u03b1 \u2212 (\u01eb\u2212 t)\u03b1)2 + 1 2 ( t\u03b1+\u03b2 + (\u01eb\u2212 t)\u03b1+\u03b2 )2\n+ 1 2 (t\u03b1 \u2212 (\u01eb\u2212 t)\u03b1)2 + 1 2 ( t\u03b1+\u03b2 + (\u01eb\u2212 t)\u03b1+\u03b2 )2\n=2 (t\u03b1 \u2212 (\u01eb\u2212 t)\u03b1)2 + ( t\u03b1+\u03b2 + (\u01eb\u2212 t)\u03b1+\u03b2 )2\n\u22642\u01eb2\u03b1 + 4\u01eb2\u03b1+2\u03b2\n\u22646\u01eb2\u03b1\nwhere (a) follows by the inequality (a+ b)2 \u2264 2a2 + 2b2 for any a, b.\nTherefore, we get \u2211 y (P0(Y =y|x)\u2212P1(Y =y|x))2 P0(Y=y|x)+P1(Y=y|x) \u2264\n\u2211 y(P0(Y =y|x)\u2212P1(Y =y|x))2 miny(P0(Y =y|x)+P1(Y=y|x)) \u2264 12 \u2217 2 \u03b1\u01eb\u03b1 when\n1 2 \u2264 x \u2264 1+\u01eb2 . Next, We show (8) holds for \u01eb \u2264 t \u2264 12 + \u01eb (i.e. 0 \u2264 x \u2264 12 ). We will show (P0(Y=y|x)\u2212P1(Y =y|x))2 P0(Y =y|x)+P1(Y=y|x) = O ( \u01eb\u03b1 + \u01eb2 )\nfor Y =\u22a5, 1, 0. For Y =\u22a5, for the denominator,\nP0(Y =\u22a5 |X = t) + P1(Y =\u22a5 |X = t) = 2\u2212 t\u03b1 \u2212 (t\u2212 \u01eb)\u03b1 \u2265 2\u2212 ( 3\n4\n)\u03b1\n\u2212 ( 1\n2\n)\u03b1\nFor the numerator,\n(P0(Y =\u22a5 |X = t)\u2212 P1(Y =\u22a5 |X = t))2 = (t\u03b1 \u2212 (t\u2212 \u01eb)\u03b1)2 = t2\u03b1 ( 1\u2212 ( 1\u2212 \u01eb t\n)\u03b1)2\nBy Lemma 13, if \u03b1 \u2265 1, t2\u03b1 ( 1\u2212 ( 1\u2212 \u01ebt )\u03b1)2 \u2264 t2\u03b1 ( \u03b1 \u01ebt )2 = t2\u03b1\u22122 (\u03b1\u01eb)2 = O ( \u01eb2 ) . If 0 \u2264 \u03b1 \u2264 1, t2\u03b1 ( 1\u2212 (\n1\u2212 \u01ebt )\u03b1)2 \u2264 t2\u03b1 ( \u01eb t )2 = t2\u03b1\u22122\u01eb2 \u2264 \u01eb2\u03b1.\nThus, we have (P0(Y=\u22a5|x)\u2212P1(Y=\u22a5|x)) 2 P0(Y =\u22a5|x)+P1(Y =\u22a5|x) = O ( \u01eb2\u03b1 + \u01eb2 ) .\nFor Y = 1, for the denominator,\n2 (P0(Y = 1|X = t) + P1(Y = 1|X = t)) = t\u03b1 ( 1\u2212 t\u03b2 ) + (t\u2212 \u01eb)\u03b1 ( 1\u2212 (t\u2212 \u01eb)\u03b2 )\n\u2265 t\u03b1 ( 1\u2212 t\u03b2 )\n\u2265 t\u03b1 ( 1\u2212 ( 3\n4\n)\u03b2 )\nFor the numerator,\n(P0(Y = 1|X = t)\u2212 P1(Y = 1|X = t))2\n= 1\n4\n(\nt\u03b1 ( 1\u2212 t\u03b2 ) \u2212 (t\u2212 \u01eb)\u03b1 ( 1\u2212 (t\u2212 \u01eb)\u03b2 ))2\n\u22641 2 (t\u03b1 \u2212 (t\u2212 \u01eb)\u03b1)2 + 1 2 ( t\u03b1+\u03b2 \u2212 (t\u2212 \u01eb)\u03b1+\u03b2 )2 = 1\n2 t2\u03b1\n( 1\u2212 (1\u2212 \u01eb t )\u03b1 )2 + 1 2 t2\u03b1+2\u03b2 ( 1\u2212 (1\u2212 \u01eb t )\u03b1+\u03b2 )2\n\u22641 2 t2\u03b1 ( 1\u2212 (1\u2212 \u01eb t )\u03b1 )2 + 1 2 t2\u03b1 ( 1\u2212 (1\u2212 \u01eb t )\u03b1+\u03b2 )2\nIf \u03b1 \u2265 1, by Lemma 13, 12 t2\u03b1 ( 1\u2212 (1\u2212 \u01ebt )\u03b1 )2 + 12 t 2\u03b1 ( 1\u2212 (1\u2212 \u01ebt )\u03b1+\u03b2 )2 \u2264 12 t2\u03b1 ( \u03b1 \u01ebt )2\n+ 1 2 t 2\u03b1 ( (\u03b1+ \u03b2) \u01ebt )2 = ( 1 2\u03b1 2 + 12 (\u03b1+ \u03b2) 2 ) t2\u03b1\u22122\u01eb2. Thus, (P0(Y=1|x)\u2212P1(Y=1|x)) 2\nP0(Y =1|x)+P1(Y=1|x) \u2264 (\n1 2\u03b1 2 + 12 (\u03b1+ \u03b2) 2 ) t\u03b1\u22122\u01eb2/ ( 1\u2212 ( 3 4 )\u03b2 ) which is O(\u01eb2) if \u03b1 \u2265 2 and O (\u01eb\u03b1) if \u03b1 \u2264 2.\nIf \u03b1 \u2264 1 and \u03b1 + \u03b2 \u2265 1, by Lemma 13, 12 t2\u03b1 ( 1\u2212 (1 \u2212 \u01ebt )\u03b1 )2 + 12 t 2\u03b1 ( 1\u2212 (1\u2212 \u01ebt )\u03b1+\u03b2 )2 \u2264 1 2 t 2\u03b1 ( \u01eb t )2 + 12 t 2\u03b1 ( (\u03b1+ \u03b2) \u01ebt )2 = ( 1 2 + 1 2 (\u03b1+ \u03b2) 2 ) t2\u03b1\u22122\u01eb2 \u2264 ( 1 2 + 1 2 (\u03b1+ \u03b2) 2 ) t2\u03b1\u22122\u01eb2. Thus, (P0(Y=1|x)\u2212P1(Y=1|x))2 P0(Y =1|x)+P1(Y=1|x) \u2264 ( 1 2 + 1 2 (\u03b1+ \u03b2) 2 ) t\u03b1\u22122\u01eb2/ ( 1\u2212 ( 3 4 )\u03b2 ) = O (\u01eb\u03b1).\nIf \u03b1 \u2264 1, \u03b1+\u03b2 \u2264 1, by Lemma 13, 12 t2\u03b1 ( 1\u2212 (1\u2212 \u01ebt )\u03b1 )2 + 12 t 2\u03b1 ( 1\u2212 (1 \u2212 \u01ebt )\u03b1+\u03b2 )2 \u2264 12 t2\u03b1 ( \u01eb t )2 + 1 2 t 2\u03b1 ( \u01eb t )2 = t2\u03b1\u22122\u01eb2. Thus, (P0(Y=1|x)\u2212P1(Y=1|x)) 2 P0(Y =1|x)+P1(Y=1|x) \u2264 t \u03b1\u22122\u01eb2/ ( 1\u2212 ( 3 4 )\u03b2 ) = O (\u01eb\u03b1).\nTherefore, we have (P0(Y=1|x)\u2212P1(Y=1|x)) 2 P0(Y=1|x)+P1(Y=1|x) = O ( \u01eb\u03b1 + \u01eb2 ) .\nLikewise, we can get (P0(Y=0|x)\u2212P1(Y=0|x)) 2 P0(Y=0|x)+P1(Y=0|x) = O ( \u01eb\u03b1 + \u01eb2 )\n. So we prove \u2211\ny (P0(Y=y|x)\u2212P1(Y=y|x))2 P0(Y=y|x)+P1(Y=y|x) = O ( \u01eb\u03b1 + \u01eb2 ) when x \u2264 12 . This concludes the proof.\nC.3 Other lemmas\nLemma 12. ([20], Lemma 4) For sufficiently large d > 0, there is a subset M \u2282 {0, 1}d with following properties: (i) |M | \u2265 2d/48; (ii) \u2016v \u2212 v\u2032\u20160 > d12 for any two distinct v, v\u2032 \u2208 M ; (iii) for any i = 1, . . . , d, 124 \u2264 1M \u2211\nv\u2208M vi \u2264 324 . Lemma 13. If x \u2264 1,r \u2265 1, then (1\u2212 x)r \u2265 1\u2212 rx and 1\u2212 (1\u2212 x)r \u2264 rx. If 0 \u2264 x \u2264 1,0 \u2264 r \u2264 1, then (1\u2212 x)r \u2265 1\u2212x1\u2212x+rx and 1\u2212 (1 \u2212 x)r \u2264 rx1\u2212(1\u2212r)x \u2264 x.\nInequalities above are know as Bernoulli\u2019s inequalities. One proof can be found in [16]. Lemma 14. Suppose \u01eb, \u03c4 are positive numbers and \u03b4 \u2264 12 . Suppose {Zi} \u221e i=1 is a sequence of i.i.d random variables bounded by 1, EZi \u2265 \u03c4\u01eb, and Var(Zi) = \u03c32 \u2264 2\u01eb. Define Vn = n n\u22121 ( \u2211n i=1 Zi \u2212 1n ( \u2211n i=1 Zi) 2 )\n, qn = q (n, Vn, \u03b4) as Procedure 2. If n \u2265 \u03b7\u03c4\u01eb ln 1\u03b4 for some sufficiently large number \u03b7 (to be specified in the proof), then with probability at least 1 \u2212 \u03b4 , qn n \u2212 EZi \u2264 \u2212\u03c4\u01eb/2.\nProof. By Lemma 9, with probability at least 1\u2212 \u03b4, Vn \u2264 4n\u03c32 + 8 ln 1\u03b4 , which implies\nqn \u2264 D1 ( 1 + ln 1\n\u03b4 +\n\u221a\n(\n4n\u03c32 + 9 ln 1\n\u03b4 + 1\n)(\n[ln ln]+ (4n\u03c3 2 + 9 ln\n1 \u03b4 + 1) + ln 1 \u03b4\n)\n)\nWe denote the RHS by q.\nOn this event, we have\nqn n \u2212 EZi \u2264 q n \u2212 \u03c4\u01eb\n= \u03c4\u01eb ( q\nn\u03c4\u01eb \u2212 1\n)\n(a) \u2264 \u03c4\u01eb (\n2D1 \u03b7 + D1\n\u03b7 ln 1\u03b4\n\u221a\n9\u03b7\n\u03c4 ln\n1\n\u03b4\n(\n[ln ln]+ ( 9\u03b7\n\u03c4 ln\n1 \u03b4 ) + ln 1 \u03b4\n) \u2212 1 )\n= \u03c4\u01eb\n(\n2D1 \u03b7 +D1\n\u221a\n9\n\u03b7\u03c4 ln 1\u03b4 [ln ln]+ (\n9\u03b7\n\u03c4 ln\n1 \u03b4 ) + 9 \u03b7\u03c4 \u2212 1\n)\nwhere (a) follows from qn being monotonically decreasing with respect to n. By choosing \u03b7 sufficiently large, we have 2D1\u03b7 +D1 \u221a 9 \u03b7\u03c4 ln 1\n\u03b4\n[ln ln]+ ( 9\u03b7 \u03c4 ln 1 \u03b4 ) + 9 \u03b7\u03c4 \u2212 1 \u2264 \u2212 12 , and thus qn n \u2212 EZi \u2264\n\u2212\u03c4\u01eb/2."}], "references": [{"title": "Active and passive learning of linear separators under log-concave distributions", "author": ["M.-F. Balcan", "P.M. Long"], "venue": "In COLT,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Agnostic active learning", "author": ["Maria-Florina Balcan", "Alina Beygelzimer", "John Langford"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Robust interactive learning", "author": ["Maria-Florina Balcan", "Steve Hanneke"], "venue": "In Proceedings of The 25th Conference on Learning Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Agnostic active learning without constraints", "author": ["A. Beygelzimer", "D. Hsu", "J. Langford", "T. Zhang"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Search improves label for active learning", "author": ["Alina Beygelzimer", "Daniel Hsu", "John Langford", "Chicheng Zhang"], "venue": "arXiv preprint arXiv:1602.07265,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Minimax bounds for active learning", "author": ["Rui M. Castro", "Robert D. Nowak"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Sequential information maximization: When is greedy near-optimal", "author": ["Yuxin Chen", "S Hamed Hassani", "Amin Karbasi", "Andreas Krause"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Improving generalization with active learning", "author": ["D.A. Cohn", "L.E. Atlas", "R.E. Ladner"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Coarse sample complexity bounds for active learning", "author": ["S. Dasgupta"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "A general agnostic active learning algorithm", "author": ["S. Dasgupta", "D. Hsu", "C. Monteleoni"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "I don\u2019t know the label: Active learning with blind knowledge", "author": ["Meng Fang", "Xingquan Zhu"], "venue": "In Pattern Recognition (ICPR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Teaching dimension and the complexity of active learning", "author": ["Steve Hanneke"], "venue": "In Learning Theory,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Generalized teaching dimensions and the query complexity of learning", "author": ["Tibor Heged\u0171s"], "venue": "In Proceedings of the eighth annual conference on Computational learning theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1995}, {"title": "Active learning in the non-realizable case", "author": ["M. K\u00e4\u00e4ri\u00e4inen"], "venue": "In ALT,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Active learning and discovery of object categories in the presence of unnameable instances", "author": ["Christoph Kading", "Alexander Freytag", "Erik Rodner", "Paul Bodesheim", "Joachim Denzler"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Some equivalent forms of bernoulli\u2019s inequality: A survey", "author": ["Yuan-Chuan Li", "Cheh-Chih Yeh"], "venue": "Applied Mathematics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Plug-in approach to active learning", "author": ["Stanislav Minsker"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Bayesian active learning with nonpersistent noise", "author": ["Mohammad Naghshvar", "Tara Javidi", "Kamalika Chaudhuri"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "The geometry of generalized binary search", "author": ["R.D. Nowak"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Lower bounds for passive and active learning", "author": ["Maxim Raginsky", "Alexander Rakhlin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Sequential nonparametric testing with the law of the iterated logarithm", "author": ["Aaditya Ramdas", "Akshay Balsubramani"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Optimal aggregation of classifiers in statistical learning", "author": ["A.B. Tsybakov"], "venue": "Annals of Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Learning from weak teachers", "author": ["Ruth Urner", "Shai Ben-david", "Ohad Shamir"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Active learning from noisy and abstention feedback", "author": ["Songbai Yan", "Kamalika Chaudhuri", "Tara Javidi"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Beyond disagreement-based agnostic active learning", "author": ["Chicheng Zhang", "Kamalika Chaudhuri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "Previous theoretical work on active learning has mostly focused on the above basic setting [2, 4, 7, 10, 25] and has developed algorithms under a number of different models of label noise.", "startOffset": 91, "endOffset": 108}, {"referenceID": 3, "context": "Previous theoretical work on active learning has mostly focused on the above basic setting [2, 4, 7, 10, 25] and has developed algorithms under a number of different models of label noise.", "startOffset": 91, "endOffset": 108}, {"referenceID": 6, "context": "Previous theoretical work on active learning has mostly focused on the above basic setting [2, 4, 7, 10, 25] and has developed algorithms under a number of different models of label noise.", "startOffset": 91, "endOffset": 108}, {"referenceID": 9, "context": "Previous theoretical work on active learning has mostly focused on the above basic setting [2, 4, 7, 10, 25] and has developed algorithms under a number of different models of label noise.", "startOffset": 91, "endOffset": 108}, {"referenceID": 24, "context": "Previous theoretical work on active learning has mostly focused on the above basic setting [2, 4, 7, 10, 25] and has developed algorithms under a number of different models of label noise.", "startOffset": 91, "endOffset": 108}, {"referenceID": 2, "context": "A handful of exceptions include [3] which allows class conditional queries, [5] which allows requesting counterexamples to current version spaces, and [23, 26] where the learner has access to a strong labeler and one or more weak labelers.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "A handful of exceptions include [3] which allows class conditional queries, [5] which allows requesting counterexamples to current version spaces, and [23, 26] where the learner has access to a strong labeler and one or more weak labelers.", "startOffset": 76, "endOffset": 79}, {"referenceID": 22, "context": "A handful of exceptions include [3] which allows class conditional queries, [5] which allows requesting counterexamples to current version spaces, and [23, 26] where the learner has access to a strong labeler and one or more weak labelers.", "startOffset": 151, "endOffset": 159}, {"referenceID": 10, "context": "This scenario arises naturally in difficult labeling tasks and has been considered in computer vision by [11, 15].", "startOffset": 105, "endOffset": 113}, {"referenceID": 14, "context": "This scenario arises naturally in difficult labeling tasks and has been considered in computer vision by [11, 15].", "startOffset": 105, "endOffset": 113}, {"referenceID": 23, "context": "The setting of active learning with an abstaining noisy labeler was first considered by [24], who looked at learning binary threshold classifiers based on queries to an labeler whose abstention rate is higher closer to the decision boundary.", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": "Under slightly stronger conditions as in [24], our algorithm has the same query complexity.", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "An important property of our algorithm is that the improvement of query complexity is achieved in a completely adaptive manner; unlike previous work [24], our algorithm needs no information whatsoever on the abstention rates or rates of label noise.", "startOffset": 149, "endOffset": 153}, {"referenceID": 5, "context": "Thus our result also strengthens existing results on active learning from (non-abstaining) noisy labelers by providing an adaptive algorithm that achieves that same performance as [6] without knowledge of noise parameters.", "startOffset": 180, "endOffset": 183}, {"referenceID": 23, "context": "Our lower bounds generalize the lower bounds in [24], and shows that our upper bounds are nearly optimal.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "Theoretical work on this topic largely falls under two categories \u2014 the membership query model [6, 13, 18, 19], where the learner can request label of any example in the instance space, and the PAC model, where the learner is given a large set of unlabeled examples from an underlying unlabeled data distribution, and can request labels of a subset of these examples.", "startOffset": 95, "endOffset": 110}, {"referenceID": 12, "context": "Theoretical work on this topic largely falls under two categories \u2014 the membership query model [6, 13, 18, 19], where the learner can request label of any example in the instance space, and the PAC model, where the learner is given a large set of unlabeled examples from an underlying unlabeled data distribution, and can request labels of a subset of these examples.", "startOffset": 95, "endOffset": 110}, {"referenceID": 17, "context": "Theoretical work on this topic largely falls under two categories \u2014 the membership query model [6, 13, 18, 19], where the learner can request label of any example in the instance space, and the PAC model, where the learner is given a large set of unlabeled examples from an underlying unlabeled data distribution, and can request labels of a subset of these examples.", "startOffset": 95, "endOffset": 110}, {"referenceID": 18, "context": "Theoretical work on this topic largely falls under two categories \u2014 the membership query model [6, 13, 18, 19], where the learner can request label of any example in the instance space, and the PAC model, where the learner is given a large set of unlabeled examples from an underlying unlabeled data distribution, and can request labels of a subset of these examples.", "startOffset": 95, "endOffset": 110}, {"referenceID": 23, "context": "Our work and also that of [24] builds on the membership query model.", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "The problem is relatively easy when the labeler always provides the ground truth labels \u2013 see [8, 9, 12] for work in this setting in the PAC model, and [13] for the membership query model.", "startOffset": 94, "endOffset": 104}, {"referenceID": 8, "context": "The problem is relatively easy when the labeler always provides the ground truth labels \u2013 see [8, 9, 12] for work in this setting in the PAC model, and [13] for the membership query model.", "startOffset": 94, "endOffset": 104}, {"referenceID": 11, "context": "The problem is relatively easy when the labeler always provides the ground truth labels \u2013 see [8, 9, 12] for work in this setting in the PAC model, and [13] for the membership query model.", "startOffset": 94, "endOffset": 104}, {"referenceID": 12, "context": "The problem is relatively easy when the labeler always provides the ground truth labels \u2013 see [8, 9, 12] for work in this setting in the PAC model, and [13] for the membership query model.", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": "[14] shows how to address this kind of noise in the PAC model by repeatedly querying an example until the learner is confident of its label; [18, 19] provide more sophisticated algorithms with better query complexities in the membership query model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[14] shows how to address this kind of noise in the PAC model by repeatedly querying an example until the learner is confident of its label; [18, 19] provide more sophisticated algorithms with better query complexities in the membership query model.", "startOffset": 141, "endOffset": 149}, {"referenceID": 18, "context": "[14] shows how to address this kind of noise in the PAC model by repeatedly querying an example until the learner is confident of its label; [18, 19] provide more sophisticated algorithms with better query complexities in the membership query model.", "startOffset": 141, "endOffset": 149}, {"referenceID": 5, "context": "A second setting is when the noise rate increases closer to the decision boundary; this setting has been studied under the membership query model by [6] and in the PAC model by [10, 4, 25].", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "A second setting is when the noise rate increases closer to the decision boundary; this setting has been studied under the membership query model by [6] and in the PAC model by [10, 4, 25].", "startOffset": 177, "endOffset": 188}, {"referenceID": 3, "context": "A second setting is when the noise rate increases closer to the decision boundary; this setting has been studied under the membership query model by [6] and in the PAC model by [10, 4, 25].", "startOffset": 177, "endOffset": 188}, {"referenceID": 24, "context": "A second setting is when the noise rate increases closer to the decision boundary; this setting has been studied under the membership query model by [6] and in the PAC model by [10, 4, 25].", "startOffset": 177, "endOffset": 188}, {"referenceID": 0, "context": "Active learning is known to be particularly difficult in this setting; however, algorithms and associated label complexity bounds have been provided by [1, 2, 4, 10, 12, 25] among others.", "startOffset": 152, "endOffset": 173}, {"referenceID": 1, "context": "Active learning is known to be particularly difficult in this setting; however, algorithms and associated label complexity bounds have been provided by [1, 2, 4, 10, 12, 25] among others.", "startOffset": 152, "endOffset": 173}, {"referenceID": 3, "context": "Active learning is known to be particularly difficult in this setting; however, algorithms and associated label complexity bounds have been provided by [1, 2, 4, 10, 12, 25] among others.", "startOffset": 152, "endOffset": 173}, {"referenceID": 9, "context": "Active learning is known to be particularly difficult in this setting; however, algorithms and associated label complexity bounds have been provided by [1, 2, 4, 10, 12, 25] among others.", "startOffset": 152, "endOffset": 173}, {"referenceID": 11, "context": "Active learning is known to be particularly difficult in this setting; however, algorithms and associated label complexity bounds have been provided by [1, 2, 4, 10, 12, 25] among others.", "startOffset": 152, "endOffset": 173}, {"referenceID": 24, "context": "Active learning is known to be particularly difficult in this setting; however, algorithms and associated label complexity bounds have been provided by [1, 2, 4, 10, 12, 25] among others.", "startOffset": 152, "endOffset": 173}, {"referenceID": 5, "context": "A setting similar to ours was considered by [6, 24].", "startOffset": 44, "endOffset": 51}, {"referenceID": 23, "context": "A setting similar to ours was considered by [6, 24].", "startOffset": 44, "endOffset": 51}, {"referenceID": 5, "context": "[6] considers a non-abstaining labeler, and provides a near-optimal binary search style active learning algorithm; however, their algorithm is non-adaptive.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "[24] gives a nearly matching lower and upper query complexity bounds for active learning with abstention feedback, but they only give a nonadaptive algorithm for learning one dimensional thresholds, and only study the situation where the 2", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Besides [24] , [11, 15] study active learning with abstention feedback in computer vision applications.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "Besides [24] , [11, 15] study active learning with abstention feedback in computer vision applications.", "startOffset": 15, "endOffset": 23}, {"referenceID": 14, "context": "Besides [24] , [11, 15] study active learning with abstention feedback in computer vision applications.", "startOffset": 15, "endOffset": 23}, {"referenceID": 0, "context": "A function g : [0, 1]d\u22121 \u2192 R is (K, \u03b3)-H\u00f6lder smooth, if it is continuously differentiable up to \u230a\u03b3\u230b-th order, and for any x,y \u2208 [0, 1]d\u22121, \u2223", "startOffset": 15, "endOffset": 21}, {"referenceID": 0, "context": "A function g : [0, 1]d\u22121 \u2192 R is (K, \u03b3)-H\u00f6lder smooth, if it is continuously differentiable up to \u230a\u03b3\u230b-th order, and for any x,y \u2208 [0, 1]d\u22121, \u2223", "startOffset": 129, "endOffset": 135}, {"referenceID": 0, "context": "We are given an instance space X = [0, 1] and a label space L = {0, 1}.", "startOffset": 35, "endOffset": 41}, {"referenceID": 5, "context": "We consider a non-parametric setting as in [6, 17] where the hypothesis space is the smooth boundary fragment class H = {hg(x) = 1 [xd > g(x\u0303)] | g : [0, 1]d\u22121 \u2192 [0, 1] is (K, \u03b3)-H\u00f6lder smooth}.", "startOffset": 43, "endOffset": 50}, {"referenceID": 16, "context": "We consider a non-parametric setting as in [6, 17] where the hypothesis space is the smooth boundary fragment class H = {hg(x) = 1 [xd > g(x\u0303)] | g : [0, 1]d\u22121 \u2192 [0, 1] is (K, \u03b3)-H\u00f6lder smooth}.", "startOffset": 43, "endOffset": 50}, {"referenceID": 0, "context": "We consider a non-parametric setting as in [6, 17] where the hypothesis space is the smooth boundary fragment class H = {hg(x) = 1 [xd > g(x\u0303)] | g : [0, 1]d\u22121 \u2192 [0, 1] is (K, \u03b3)-H\u00f6lder smooth}.", "startOffset": 150, "endOffset": 156}, {"referenceID": 0, "context": "We consider a non-parametric setting as in [6, 17] where the hypothesis space is the smooth boundary fragment class H = {hg(x) = 1 [xd > g(x\u0303)] | g : [0, 1]d\u22121 \u2192 [0, 1] is (K, \u03b3)-H\u00f6lder smooth}.", "startOffset": 162, "endOffset": 168}, {"referenceID": 0, "context": "When d = 1, H reduces to the space of threshold functions {h\u03b8(x) = 1 [x > \u03b8] : \u03b8 \u2208 [0, 1]}.", "startOffset": 83, "endOffset": 89}, {"referenceID": 0, "context": "The performance of a classifier h(x) = 1 [xd > g(x\u0303)] is evaluated by the L distance between the decision boundaries \u2016g \u2212 g\u2217\u2016 = \u0301 [0,1]d\u22121 |g(x\u0303)\u2212 g\u2217(x\u0303)| dx\u0303.", "startOffset": 130, "endOffset": 135}, {"referenceID": 0, "context": "For each query x \u2208 [0, 1], the labeler L will return y \u2208 Y = {0, 1,\u22a5} (\u22a5 means that the labeler abstains from providing a 0/1 label) according to some distribution PL(Y = y | X = x).", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "In particular, we want to find an algorithm with low query complexity \u039b(\u01eb, \u03b4,A, L, g\u2217), which is defined as the minimum number of queries that Algorithm A, acting on samples with ground truth g\u2217, should make to a labeler L to ensure that the output classifier hg(x) = 1 [xd > g(x\u0303)] has the property \u2016g \u2212 g\u2217\u2016 = \u0301 [0,1]d\u22121 |g(x\u0303)\u2212 g\u2217(x\u0303)| dx\u0303 \u2264 \u01eb with probability at least 1\u2212 \u03b4 over the responses of L.", "startOffset": 313, "endOffset": 318}, {"referenceID": 0, "context": "The response distribution of the labeler P (Y | X) satisfies: \u2022 (abstention) For any x\u0303 \u2208 [0, 1]d\u22121, xd, xd \u2208 [0, 1], if |xd \u2212 g\u2217(x\u0303)| \u2265 |xd \u2212 g\u2217(x\u0303)| then P (\u22a5| (x\u0303, xd)) \u2264 P (\u22a5| (x\u0303, xd)); \u2022 (noise) For any x \u2208 [0, 1], P (Y 6= 1 [xd > g\u2217(x\u0303)] | x, Y 6=\u22a5) \u2264 1 2 .", "startOffset": 90, "endOffset": 96}, {"referenceID": 0, "context": "The response distribution of the labeler P (Y | X) satisfies: \u2022 (abstention) For any x\u0303 \u2208 [0, 1]d\u22121, xd, xd \u2208 [0, 1], if |xd \u2212 g\u2217(x\u0303)| \u2265 |xd \u2212 g\u2217(x\u0303)| then P (\u22a5| (x\u0303, xd)) \u2264 P (\u22a5| (x\u0303, xd)); \u2022 (noise) For any x \u2208 [0, 1], P (Y 6= 1 [xd > g\u2217(x\u0303)] | x, Y 6=\u22a5) \u2264 1 2 .", "startOffset": 110, "endOffset": 116}, {"referenceID": 0, "context": "The response distribution of the labeler P (Y | X) satisfies: \u2022 (abstention) For any x\u0303 \u2208 [0, 1]d\u22121, xd, xd \u2208 [0, 1], if |xd \u2212 g\u2217(x\u0303)| \u2265 |xd \u2212 g\u2217(x\u0303)| then P (\u22a5| (x\u0303, xd)) \u2264 P (\u22a5| (x\u0303, xd)); \u2022 (noise) For any x \u2208 [0, 1], P (Y 6= 1 [xd > g\u2217(x\u0303)] | x, Y 6=\u22a5) \u2264 1 2 .", "startOffset": 213, "endOffset": 219}, {"referenceID": 0, "context": "Let C, \u03b2 be non-negative constants, and f : [0, 1] \u2192 [0, 1] be a nondecreasing function.", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": "Let C, \u03b2 be non-negative constants, and f : [0, 1] \u2192 [0, 1] be a nondecreasing function.", "startOffset": 53, "endOffset": 59}, {"referenceID": 21, "context": "The condition on the noise satisfies the popular Tsybakov noise condition [22].", "startOffset": 74, "endOffset": 78}, {"referenceID": 0, "context": "Let f : [0, 1] \u2192 [0, 1] be a nondecreasing function such that \u22030 < c < 1, \u22000 < a \u2264 1 \u22000 \u2264 b \u2264 23a, f(b) f(a) \u2264 1\u2212 c.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "Let f : [0, 1] \u2192 [0, 1] be a nondecreasing function such that \u22030 < c < 1, \u22000 < a \u2264 1 \u22000 \u2264 b \u2264 23a, f(b) f(a) \u2264 1\u2212 c.", "startOffset": 17, "endOffset": 23}, {"referenceID": 0, "context": "When d = 1, the decision boundary g\u2217 becomes a point in [0, 1], and the corresponding classifier is a threshold function over [0,1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "When d = 1, the decision boundary g\u2217 becomes a point in [0, 1], and the corresponding classifier is a threshold function over [0,1].", "startOffset": 126, "endOffset": 131}, {"referenceID": 0, "context": "In other words the hypothesis space becomes H = {f\u03b8(x) = 1 [x > \u03b8] : \u03b8 \u2208 [0, 1]}).", "startOffset": 73, "endOffset": 79}, {"referenceID": 0, "context": "We denote the ground truth decision boundary by \u03b8\u2217 \u2208 [0, 1].", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "We want to find a \u03b8\u0302 \u2208 [0, 1] such that |\u03b8\u0302 \u2212 \u03b8\u2217| is small while making as few queries as possible.", "startOffset": 23, "endOffset": 29}, {"referenceID": 0, "context": "Algorithm 1 The active learning algorithm for learning thresholds 1: Input: \u03b4, \u01eb 2: [L0, R0] \u2190 [0, 1] 3: for k = 0, 1, 2, .", "startOffset": 95, "endOffset": 101}, {"referenceID": 23, "context": "The lower bound in [24] can be easily generalized to Condition 2: Theorem 4.", "startOffset": 19, "endOffset": 23}, {"referenceID": 23, "context": "([24]) There is a universal constant \u03b40 \u2208 (0, 1) and a labeler L satisfying Conditions 1 and 2, such that for any active learning algorithm A, there is a \u03b8\u2217 \u2208 [0, 1], such that for small enough \u01eb, \u039b(\u01eb, \u03b40,A, L, \u03b8\u2217) \u2265 \u03a9 ( 1 f(\u01eb)\u01eb \u22122\u03b2 )", "startOffset": 1, "endOffset": 5}, {"referenceID": 0, "context": "([24]) There is a universal constant \u03b40 \u2208 (0, 1) and a labeler L satisfying Conditions 1 and 2, such that for any active learning algorithm A, there is a \u03b8\u2217 \u2208 [0, 1], such that for small enough \u01eb, \u039b(\u01eb, \u03b40,A, L, \u03b8\u2217) \u2265 \u03a9 ( 1 f(\u01eb)\u01eb \u22122\u03b2 )", "startOffset": 159, "endOffset": 165}, {"referenceID": 0, "context": "There is a universal constant \u03b40 \u2208 (0, 1) and a labeler L satisfying Conditions 1, 2, and 3 with f(x) = C\u2032x\u03b1 (C\u2032 > 0 and 0 < \u03b1 \u2264 2 are constants), such that for any active learning algorithm A, there is a \u03b8\u2217 \u2208 [0, 1], such that for small enough \u01eb, \u039b(\u01eb, \u03b40,A, L, \u03b8\u2217) \u2265 \u03a9 (\u01eb\u2212\u03b1).", "startOffset": 210, "endOffset": 216}, {"referenceID": 5, "context": "4 The multidimensional case We follow [6] to generalize the results from one-dimensional thresholds to the d-dimensional (d > 1) smooth boundary fragment class \u03a3(K, \u03b3).", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "2 Algorithm and Analysis Recall the decision boundary of the smooth boundary fragment class can be seen as the epigraph of a smooth function [0, 1]d\u22121 \u2192 [0, 1].", "startOffset": 141, "endOffset": 147}, {"referenceID": 0, "context": "2 Algorithm and Analysis Recall the decision boundary of the smooth boundary fragment class can be seen as the epigraph of a smooth function [0, 1]d\u22121 \u2192 [0, 1].", "startOffset": 153, "endOffset": 159}, {"referenceID": 0, "context": "References [1] M.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Maria-Florina Balcan, Alina Beygelzimer, and John Langford.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Maria-Florina Balcan and Steve Hanneke.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Alina Beygelzimer, Daniel Hsu, John Langford, and Chicheng Zhang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Rui M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Yuxin Chen, S Hamed Hassani, Amin Karbasi, and Andreas Krause.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Meng Fang and Xingquan Zhu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Steve Hanneke.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Tibor Heged\u0171s.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Christoph Kading, Alexander Freytag, Erik Rodner, Paul Bodesheim, and Joachim Denzler.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Yuan-Chuan Li and Cheh-Chih Yeh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Stanislav Minsker.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Mohammad Naghshvar, Tara Javidi, and Kamalika Chaudhuri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Maxim Raginsky and Alexander Rakhlin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Aaditya Ramdas and Akshay Balsubramani.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Ruth Urner, Shai Ben-david, and Ohad Shamir.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Songbai Yan, Kamalika Chaudhuri, and Tara Javidi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Chicheng Zhang and Kamalika Chaudhuri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Let \u03b4 \u2208 [0, 13 ], N \u2265 \u03be \u01eb2 ln 1 \u03b4 [ln ln]+ 1 \u01eb (\u03be is an absolute constant specified in the proof).", "startOffset": 8, "endOffset": 16}, {"referenceID": 0, "context": ", log 1 2\u01eb \u2212 1, define Qk = { (p, q) : p, q \u2208 Q \u2229 [0, 1] and q \u2212 p = ( 3 4 k }", "startOffset": 50, "endOffset": 56}, {"referenceID": 5, "context": "= O ( M\u2212\u03b3\u2212d+1 ) The second equality follows from Lemma 3 of [6] that |gq(x\u0303)\u2212 g\u2217(x\u0303)| = O (M\u2212\u03b3) since g\u2217 is \u03b3-H\u00f6lder smooth.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "We take \u0398 to be [0, 1], and d(\u03b81, \u03b82) = |\u03b81 \u2212 \u03b82| in Lemma 5.", "startOffset": 16, "endOffset": 22}, {"referenceID": 0, "context": "\u2264n max x\u2208[0,1] dKL ( PLk(Y | x) \u2016 P\u0304L(Y | x) )", "startOffset": 9, "endOffset": 14}, {"referenceID": 0, "context": "For any k \u2208 {1, 2, 3}, x \u2208 [0, 1], P\u0304L(\u00b7 | x) \u2265 PL0(\u00b7 | x) + PLk(\u00b7 | x) 4 (3) For any k \u2208 {0, 1, 2, 3}, x \u2208 [0, 1], y \u2208 {1,\u22121,\u22a5} 18", "startOffset": 27, "endOffset": 33}, {"referenceID": 0, "context": "For any k \u2208 {1, 2, 3}, x \u2208 [0, 1], P\u0304L(\u00b7 | x) \u2265 PL0(\u00b7 | x) + PLk(\u00b7 | x) 4 (3) For any k \u2208 {0, 1, 2, 3}, x \u2208 [0, 1], y \u2208 {1,\u22121,\u22a5} 18", "startOffset": 108, "endOffset": 114}, {"referenceID": 5, "context": "We first construct {P\u03b8 : \u03b8 \u2208 \u0398} using a similar idea with [6], and then use Lemma 12 to select a subset \u0398\u0303 \u2282 \u0398 to apply Lemma 5.", "startOffset": 58, "endOffset": 61}, {"referenceID": 0, "context": "\u2264n max x\u2208[0,1]d dKL ( P L \u03c9 (i) (Y | x) \u2016 P L \u03c9 (j) (Y | x) )", "startOffset": 9, "endOffset": 14}, {"referenceID": 0, "context": "=n max x\u2208[0,1]d P L \u03c9 (i) (Y 6=\u22a5| x)dKL ( P L \u03c9 (i) (Y | x, Y 6=\u22a5) \u2016 P L \u03c9 (j) (Y | x, Y 6=\u22a5) )", "startOffset": 9, "endOffset": 14}, {"referenceID": 0, "context": "Therefore, dKL ( P i \u2016 P j ) \u2264 nf(A) max x\u2208[0,1]d dKL ( P L \u03c9 (i) (Y | x, Y 6=\u22a5) \u2016 P L \u03c9 (j) (Y | x, Y 6=\u22a5) )", "startOffset": 43, "endOffset": 48}, {"referenceID": 0, "context": "Apply Lemma 10 to P L \u03c9 (i) (Y | x, Y 6=\u22a5) and P L \u03c9 (i) (Y | x, Y 6=\u22a5), and noting they are bounded above by a constant, we have max x\u2208[0,1]d dKL ( P L \u03c9 (i) (Y | x, Y 6=\u22a5) \u2016 P L \u03c9 (j) (Y | x, Y 6=\u22a5) )", "startOffset": 136, "endOffset": 141}, {"referenceID": 0, "context": "By the construction of g, for any x \u2208 [0, 1], any \u03c9 \u2208 \u03a9, PL\u03c9 (\u00b7 | x) equals either P+(\u00b7 | x) or P\u2212(\u00b7 | x).", "startOffset": 38, "endOffset": 44}, {"referenceID": 0, "context": "By the well-balanced property, for any x \u2208 [0, 1], P\u0304L(\u00b7 | x) is between 1 24P+(\u00b7 | x) + 23 24P\u2212(\u00b7 | x) and 3 24P+(\u00b7 | x) + 21 24P\u2212(\u00b7 | x).", "startOffset": 43, "endOffset": 49}, {"referenceID": 0, "context": "For any 0 < i \u2264 M , dKL ( P i \u2016 P\u0304 0 ) \u2264 nmax x\u2208[0,1]d dKL ( PLi(Y | x) \u2016 P\u0304L(Y | x) ) .", "startOffset": 48, "endOffset": 53}, {"referenceID": 0, "context": "For any x \u2208 [0, 1], dKL ( PLi(Y | x) \u2016 P\u0304L(Y | x) )", "startOffset": 12, "endOffset": 18}, {"referenceID": 20, "context": "We need following two results from [21] Lemma 6.", "startOffset": 35, "endOffset": 39}, {"referenceID": 20, "context": "([21], Theorem 2) Take any 0 < \u03b4 < 1.", "startOffset": 1, "endOffset": 5}, {"referenceID": 20, "context": "([21], Lemma 3) Take any 0 < \u03b4 < 1.", "startOffset": 1, "endOffset": 5}, {"referenceID": 0, "context": "If x \u2208 [0, 1], \u01eb \u2264 min { ( 1 2 1/\u03b2 , ( 4 5 1/\u03b1 , 1 4 }", "startOffset": 7, "endOffset": 13}, {"referenceID": 19, "context": "([20], Lemma 4) For sufficiently large d > 0, there is a subset M \u2282 {0, 1} with following properties: (i) |M | \u2265 2; (ii) \u2016v \u2212 v\u20160 > d 12 for any two distinct v, v\u2032 \u2208 M ; (iii) for any i = 1, .", "startOffset": 1, "endOffset": 5}, {"referenceID": 15, "context": "One proof can be found in [16].", "startOffset": 26, "endOffset": 30}], "year": 2016, "abstractText": "We study active learning where the labeler can not only return incorrect labels but also abstain from labeling. We consider different noise and abstention conditions of the labeler. We propose an algorithm which utilizes abstention responses, and analyze its statistical consistency and query complexity under fairly natural assumptions on the noise and abstention rate of the labeler. This algorithm is adaptive in a sense that it can automatically request less queries with a more informed or less noisy labeler. We couple our algorithm with lower bounds to show that under some technical conditions, it achieves nearly optimal query complexity.", "creator": "gnuplot 4.6 patchlevel 4"}}}