{"id": "1611.00068", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "RNN Approaches to Text Normalization: A Challenge", "abstract": "this paper presents remarkable challenge to tool consortium : given so large corpus lacks written text aligned to its target human form, build project architect to learn the default normalization function. developers present a compact set of general reports where the responses were analyzed above an existing script normalization component resembling any learn - to - spoken system. this data set will be released multiple - source sometime the near future.", "histories": [["v1", "Mon, 31 Oct 2016 22:42:02 GMT  (461kb)", "http://arxiv.org/abs/1611.00068v1", "17 pages, 13 tables, 3 figures"], ["v2", "Tue, 24 Jan 2017 19:51:12 GMT  (461kb)", "http://arxiv.org/abs/1611.00068v2", "17 pages, 13 tables, 3 figures"]], "COMMENTS": "17 pages, 13 tables, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["richard sproat", "navdeep jaitly"], "accepted": false, "id": "1611.00068"}, "pdf": {"name": "1611.00068.pdf", "metadata": {"source": "CRF", "title": "RNN Approaches to Text Normalization: A Challenge", "authors": ["Richard Sproat", "Navdeep Jaitly"], "emails": ["rws@google.com", "ndjaitly@google.com"], "sections": [{"heading": null, "text": "This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future.\nWe also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone.\nThough our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. Andwhenwe open-source our data, we will be providing a novel data set for sequenceto-sequence modeling in the hopes that the the community can find better solutions."}, {"heading": "1 Introduction", "text": "Within the last few years a major shift has taken place in speech and language technology: the field has been taken over by deep learning approaches. For example, at a recent NAACL conference well more than half the papers related in some way to word embeddings or deep or recurrent neural networks. This change is surely justified by the impressive performance gains to be had by deep learning, something that has been demonstrated in a range of areas from image processing, handwriting recognition, acoustic modeling in automatic speech recognition (ASR), parametric speech synthesis for text-tospeech (TTS), machine translation, parsing, and go playing to name but a few. While various approaches have been taken and someNN architectures have surely been carefully designed for the specific task, there is also awidespread feeling that with deep enough architectures, and enough data, one can simply feed the data to one\u2019s NN and have it learn the necessary function. For example:\nNot only do such networks require less human effort than traditional approaches, they generally deliver superior performance. This is particularly true when very large amounts of training data are available, as the benefits of holistic optimisation tend to outweigh those of prior knowledge. (Graves and Jaitly, 2014, page 1)\nIn this paper we present an example of an application that is unlikely to be amenable to such a \u201cturnthe-crank\u201d approach. The example is text normaliza-\ntion, specifically in the sense of a system that converts from a written representation of a text into a representation of how that text is to be read aloud. The target applications are TTS and ASR \u2014 in the latter case mostly for generating language modeling data from rawwritten text. This problem, while often considered mundane, is in fact very important, and a major source of degradation of perceived quality in TTS systems in particular can be traced to problems with text normalization.\nWe start by describing why this application area is a bit different from most other areas of NLP. We then discuss prior work in this area, including related work on applications of RNNs in text normalization more broadly. We then describe a dataset that will be made available open-source as a challenge to the community, and we go on to describe several experiments that we have conducted with this dataset, with various NN architectures. As we show below, some of the RNNs produce very good results when measured in terms of overall accuracy, but they produce errors that would make them risky to use in a real application, since in the errorful cases, the normalization would convey completely the wrong message. As we also demonstrate, these errors can be ameliorated with a simple FSTbased filter used in tandem with the RNN. But with a pure RNN approach, we have not thus far succeeded in avoiding the above-mentioned risky errors, and it is an open question whether such errors can be avoided by such a solution. We present below a hypothesis on why the RNNs tend to make the kinds of errors below. We close the paper by proposing a challenge to the community based on the data that we plan to release."}, {"heading": "2 Why text normalization is different", "text": "To lay the groundwork for discussion let us consider a simple example such as the following:\nIf one were to ask a speaker of English to read this sentence, or if one were to feed it to an English TTS system one would expect that it to be read more or less as follows:\nIn the original written form there are two nonstandard words (Sproat et al., 2001), namely the two measure expressions 6ft and 150lb. In order to read the text, each of these must be normalized into a sequence of ordinary words. In this case both examples are instances of the same semiotic class (Taylor, 2009), namely measure phrases. But in general texts may include non-standard word sequences from a variety of different semiotic classes, including measures, currency amounts, dates, times, telephone numbers, cardinal or ordinal numbers, fractions, among many others. Each of these involves a specific function mapping between the written input form and the spoken output form. If one were to train a deep-learning system for text normalization, one might consider presenting the system with a large number of input-output pairs as in Figure 1. Here we use a special token to indicate that the input is to be left alone. In principle this seems like a reasonable approach, but there are a number of issues that need to be considered. The first is that one desirable application of such an approach, if it can be made to work, is to develop text normalization systems for languages where we do not already have an existing system. If one could do this, one could circumvent the often quite considerable hand labor required to build systems in more traditional approaches to text normalization.1\n1E.g. Ebden and Sproat, 2014.\nBut where would one get the necessary data? In the case of machine translation, the existence of large amounts of parallel texts in multiple languages is motivated by the fact that people want to read texts in their own languages, and therefore someone, somewhere, will often go to the trouble of writing a translation. For speech recognition acoustic model training, one could in theory use closed captioning (Bharadwaj and Medapati, 2015), which again is produced for a reason. In contrast, there is no natural economic reason to produce normalized versions of written texts: no English speaker needs to be told that 6ft is six feet or that 150lb is one hundred fifty pounds, and therefore there is no motivation for anyone to produce such a translation. The situation in text normalization is therefore more akin to the situation with parsing, where one must create treebanks for a language in order to make progress on that language; if one wants to train text normalization systems using NN approaches, one must create the training data to do so. In the case of the present paper, we were able to produce a training corpus since we already had working text normalization systems, which allowed us to produce a normalized form for the raw input. The normalization is obviously errorful (we give an estimate of the percentage of errors below), but it is good enough to serve as a test bed for deep learning approaches \u2014 and by making it public we hope to encourage more serious attention to this problem. But in any event, if one is planning to implement an RNN-based approach to text normalization, one must take into consideration the resources needed to produce the necessary training data. A second issue is that the set of interesting cases in text normalization is usually very sparse. Most tokens, we saw in the small example above, map to themselves, and while it is certainly important to get that right, one generally does not get any credit for doing so either. What is evaluated in text normalization systems is the interesting cases, the numbers, times, dates, measure expressions, currency amounts, and so forth, that require special treatment. Furthermore, the requirements on accuracy are rather stringent: if the text says 381 kg, then an English TTS system had better say three hundred eighty one kilograms, or maybe three hundred eighty one kilogram, but certainly not three hundred forty one kilograms. 920 might be read as nine hundred twenty, or perhaps as nine twenty, but certainly never nine hundred thirty.Oct 4must be read asOctober fourth ormaybe October four, but not November fourth. I mention these cases specifically, since the silly errors are errors that current neural models trained on these sorts of data will make, as we demonstrate below. Again the situation for text normalization is different from that of, say, MT: in the case of MT, one usually must do something for any word or phrase in the source language. The closest equivalent of the map in MT, is probably translating a word or phrase with its most common equivalent. This will of course often be correct: most of the time it would be reasonable to translate the cat as le chat in French, and this translation would count positively towards one\u2019s BLEU score. This is to say that in MT one gets credit for the \u201ceasy\u201d cases as well as the more interesting cases \u2014 e.g. this cat\u2019s in no hurry, where a more appropriate translation of cat might be type or gars. In text normalization one gets credit only for the relatively sparse interesting cases. Indeed, as we shall see below, if one is allowed to count the vast majority of cases where the right answer is to leave the input token alone, some of our RNNs already perform very well. The problem is that they tend to mess up with various semiotic classes in ways that would make them unusable for any real application, since one could never be quite sure for a new example that the system would not read it completely wrongly. As we will see below, the neural models occasionally read things like, \u00a3900 as nine hundred Euros \u2014 something that state-ofthe-art hand-built text normalization systems would never do, brittle though such systems may be. The occasional comparable error in an MT system would be bad, but it would not contribute much to a degradation of the system\u2019s BLEU score. Such a misreading in a TTS system would be something that people would immediately notice (or, worse, not notice if they could not see the text), and would stand out precisely because a TTS system ought to get such examples right. In this paper we try two kinds of neural models on a text normalization problem. The first is a neural equivalent of a source-channel model that uses a sequence-to-sequence LSTM that has been successfully applied to the somewhat similar problem of grapheme-to-phoneme conversion (Rao et al., 2015),\nalong with a standard LSTM language model architecture. The second treats the entire problem as a sequence-to-sequence task, using the same architecture that has been used for a speech-to-text conversion problem (Chan et al., 2016)."}, {"heading": "3 Prior work on text normalization", "text": "Text normalization has a long history in speech technology, dating back to the earliest work on full TTS synthesis (Allen et al., 1987). Sproat (1996) provided a unifying model for most text normalization problems in terms of weighted finite-state transducers (WFSTs). The first work to treat the problem of text normalization as essentially a language modeling problem was (Sproat et al., 2001). More recent machine learning work specifically addressed to TTS text normalization include (Sproat, 2010; Roark and Sproat, 2014; Sproat and Hall, 2014). In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013). This work tends to focus on different problems from those of TTS: on the one hand one, in social media one often has to deal with odd spellings of words such as , , or , which are less of an issue in most applications of TTS; on the other, expansion of digit sequences into words is critical for TTS text normalization, but of no interest to the normalization of social media texts. Some previous work, also on social media normalization, that has made use of neural techniques includes (Chrupa\u0142a, 2014; Min and Mott, 2015). The latter work, for example, achieved second place in the constrained track of the ACL 2015 W-NUT Normalization of Noisy Text (Baldwin et al., 2015), achieving an F1 score of 81.75%. In the work we report below on TTS normalization, we achieve accuracies that are comparable or better than that result (to the extent that it makes sense to compare across such quite different tasks), but we would argue that for the intended application, such results are still not good enough."}, {"heading": "4 Dataset", "text": "Our data consists of 1.1 billion words of English text, and 290 million words of Russian text, from Wikipedia regions that could be decoded as UTF8, divided into sentences, and run through the Google TTS system\u2019s Kestrel text normalization system (Ebden and Sproat, 2014) to produce verbalizations. The format of the annotated data is as in Figure 1 above. As described in (Ebden and Sproat, 2014), Kestrel\u2019s verbalizations are produced by first tokenizing the input and classifying the tokens, and then verbalizing each token according to its semiotic class. The majority of the rules are hand-built using the Thrax finite-state grammar development system (Roark et al., 2012). Statistical components of the system include morphosyntactic taggers for languages like Russian with complex morphology,2 a statistical transliterationmodule (Jansche and Sproat, 2009), and a statistical model to determine if capitalized tokens should be read as words or letter sequences (Sproat and Hall, 2014). Most ordinary words are of course left alone (represented here as ), and punctuation symbols are mostly transduced to (for \u201csilence\u201d). The data were divided into 90 files (roughly 90%) for training, 5 files for online evaluation during training (the \u201cdevelopment\u201d set), and 5 for testing. In the test results reported below, we used the first 100K tokens of the final file (99) of the test data, including the end-of-sentencemarker, working out to about 92K real tokens for English and 93K real tokens for Russian. A manual analysis of about 1,000 examples from the test data suggests an overall error rate of approximately 0.1% for English and 2.1% for Russian. The largest category of errors for Russian involves years being read as cardinal numbers rather than the expected ordinal form. Note that although the test data were of course taken from a different portion of the Wikipedia text than the training and development data, nonetheless a huge percentage of the individual tokens of the test\n2The morphosyntactic tagger is an SVM model using hand-tuned features that classify the morphological bundle for each word independently, similar to SVMTool (Gim\u00e9nez and M\u00e0rquez, 2004) and MateTagger (Bohnet and Nivre, 2012).\ndata \u2014 98.9% in the case of Russian and 99.5% in the case of English \u2014 were found in the training set. This in itself is perhaps not so surprising but it does raise the concern that the RNNmodels may in fact be memorizing their results, without doing much generalization. We discuss this issue further below. Finally some justification of the choice of data is in order. We chose Wikipedia for two reasons. First, it is after all a reasonable application of TTS, and in fact it is used already in systems that give answers to voice queries on the Web. Second, the data are already publicly available, so there are no licensing issues."}, {"heading": "5 Experiment 1: Text normalization using LSTMs", "text": "The first approach depends on the observation that text normalization can be broken down into two subproblems. For any token:\n\u2022 What are the possible normalizations of that token, and\n\u2022 which one is appropriate to the given context?\nThe first of these\u2014 the channel\u2014can be handled in a context-independent way by enumerating the set of possible normalizations: thus 123 might be one hundred twenty three, one two three, or one twenty three. The second requires context: in 123 King Ave., the correct reading in American English would normally be one twenty three. The first component is a string-to-string transduction problem. Furthermore, sinceWFSTs can be used to handle most or all of the needed transductions (Sproat, 1996), the relation between the input and output strings is regular, so that complex network architectures involving, say, stacks should not be needed. For the input, the string must be in terms of characters, since for a string like 123, one needs to see the individual digits in the sequence to know how to read it; similarly it helps to see the individual characters for a possibly OOV word such as snarky to classify it as a token to be left alone ( ). On the other hand since the second component is effectively a language-modeling problem, the appropriate level of representation there is words. Therefore we also want the output of the first component to be in terms of words."}, {"heading": "5.1 LSTM architecture", "text": "We train two LSTMmodels, one for the channel and one for the language model. The data usage of each during training is outlined in Table 1. For the channel model, the LSTM learns to map from a sequence of characters to one or more word tokens of output. For most input tokens this will involve deciding to leave it alone, that is to map it to , or in the case of punctuation to map it to , corresponding to silence. For other tokens it must decide to verbalize it in a variety of different ways. For the language model, the system reads the words either from the input, if mapped to or else from the output if mapped from anything else. For the channel LSTM we used a bidirectional sequence-to-sequence model similar to that reported in (Rao et al., 2015) in two configurations: one with two forward and two backward hidden layers, henceforth the shallow model; and one with three forward and three backward hidden layers, henceforth the deep model. We kept the number of nodes in each hidden layer constant at 256.3. The output layer is a connectionist temporal classification (CTC) (Graves et al., 2006) layer with a softmax error function.4 Input was limited to 250 distinct characters (including the unknown token). For the output, 1,000 distinct\n3A larger shallowmodel with 1024 nodes in each layer ended up severely overfitting the training data.\n4Earlier experiments with non-CTC architectures did not produce results as good as what we obtained with the CTC layer.\nwords (including and the unknown token) were allowed for English, and 2,000 for Russian, the larger number for Russian being required to allow for various inflected forms. The number of words may seem small but it is sufficient to cover the distinct words that are output by the verbalizer for the various semiotic classes where the token does not simply map to . See Figure 2. The language model LSTM follows a standard RNN language model architecture, following on work of Mikolov et al., 2010, with an input and output layer consisting of |V| nodes \u2014 we limited |V| to 100,000, a dimensionality-reduction projection layer, a hidden LSTM layer with a feedback loop, and a hierarchical softmax output layer. During training the LSTM learns to predict the next word given the current word, but the feedback loop allows the model to build up a history of arbitrary length. See Figure 3. The channel and language model LSTMs are trained separately. Table 2 shows the number of training steps and the final (dev) perplexity/label er-\nror rate (LER) for the LM and channel LSTMs."}, {"heading": "5.2 Decoding", "text": "At decoding time we need to combine the outputs of the channel and language model. This is done as follows. First, for each position in the output of the channel model, we prune the predicted output symbols. If one hypothesis has a very high probability (default 0.98), we eliminate all other predictions at that position: in practice this happens in most cases, since the channel model is typically very sure of itself at most of the input positions. We also prune all hypotheses with a low probability (default 0.05). Finally we keep only the n best hypotheses at each output position: in our experiments we kept n = 5. We then use the resulting pruned vectors to populate the corresponding positions in the input to the LM, with the channel probability for each token. This channel probability is multiplied by the LM probability times an LM weighting factor (1 in our experiments). This method of combining the chan-\nnel and LM probabilities can be thought of a as a poor-man\u2019s equivalent of the composition of a channel and LM weighted finite-state transducer (Mohri et al., 2002): the main difference is that there is no straightforward way to represent an arbitrary lattice in an LSTM, so that the representation is more akin to a confusion network (\u201csausage\u201d) at each output position."}, {"heading": "5.3 Results", "text": "The first point to observe is that the overall performance is quite good: the accuracy is about 99% for English and 98% for Russian. But nearly all of this can be attributed to the model predicting for most input tokens, and for punctuation tokens. To be sure these are decisions that a text normalization systemmust make: any such systemmust decide to leave an input token alone, or map it to silence. Still, as we noted in the introduction, one does not usually get credit for getting these decisions right, and when one looks at the interesting cases, the performance starts to break down, with the lowest performance predictably being found for cases such as TIME that are not very common in these data. The deep models also generally tend to be better than the shallow models, though this is not true in all cases\u2014 e.g. MONEY in English. This in itself is a useful sanity check, since we would expect improvement with deeper models. The models are certainly able to get some quite complicated cases right. Thus for example for the Russian input 1 \u043e\u043a\u0442\u044f\u0431\u0440\u044f 1812 \u0433\u043e\u0434\u0430 (\u201c1 October, 1812\u201d), the deep model correctly predicts \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u043e\u043a\u0442\u044f\u0431\u0440\u044f \u0442\u044b\u0441\u044f\u0447\u0430 \u0432\u043e\u0441\u0435\u043c\u044c\u0441\u043e\u0442 \u0434\u0432\u0435\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0433\u043e \u0433\u043e\u0434\u0430 (\u201cfirst of october of the one thousand eight hundred and twelfth year\u201d); or in English 2008-09-30 as the thirtieth of september two thousand eight. But quite often the prediction is off, though in ways that are themselves indicative of a deeper problem. Thus consider the examples in Table 4, all of which are taken from the \u201cdeep\u201d models for English and Russian. In both languages we find examples where the system gets numbers wrong, evidently because it is hard for the system to learn from the training data exactly how to map from digit sequences to number names \u2014 see also (Gorman and Sproat, 2016).5 Other errors include reading the wrong unit\n5Note that in Russian most of the number errors involve the\nas in the last three English examples, the reading of hour rather than gigabyte in the last Russian example, or the tendency of the Russian system to output\nin a lot of examples of measure phrases. These errors are entirely due to the channel model: there is nothing ill-formed about the sequences produced, they just happen to be wrong given the input. One way to see this is to compute the \u201coracle\u201d accuracy, the proportion of the time that the correct answer is in the pseudo-lattice produced by the channel. For the English deep model the oracle accuracy is 0.998. Since the overall accuracy of the model is 0.993, this means that 27 or about 29% of the errors can be attributed to the channel model not giving the LM a choice of selecting the right model. What about the other 71% of the cases? While it is possible that some of these could be because the LM chooses an impossible sequence, in most cases what the channel model offers are perfectly possible verbalizations of something, just not necessarily correct for the given input. If the channel model offers both twenty seconds and twenty kilograms, how is the language model to determine which is correct, since even broader context may not be enough to determine which one is more likely? The wrong-unit readings are particularly interesting in that one thing that is known to be true of RNNs is that they are very good at learning to cluster words into semantic groups based on contextual information. Clearly the system has learned the kinds of expressions that can occur after numbers, and some cases it substitutes one such expression for another. This property is the whole basis of word embeddings (Bengio et al., 2003) and other similar techniques. Interestingly, a recent paper (Arthur et al., 2016) discusses entirely analogous errors in neural machine translation, and attributes them to the same cause.6 It is quite a useful property for many applications, but in the case of text normalization it is a drawback: unless one is trying to model a patient with semantic paraphasia, one generally wants a TTS system\nsystem reading the correct number, but in the wrong case form. These sorts of errors, while not desirable, are not nearly as bad as the system reading the wrong number: if all that is wrong is the inflection, a native speaker could still recover the intended meaning.\n6Arthur et al.\u2019s proposed solution is in some ways similar to the FST-based mechanism we propose below in Section 7.\nto read the written measure expression, not merely something from the same semantic category. Aswe noted above, there was a substantial amount of overlap at the individual token level between the training and test data: could the LSTM simply have been memorizing? In the test data there were 475 unseen cases in English of which the system got 82.9% correct (compared to 99.5% among the seen cases); for Russian there were 1,089 unseen cases of which 83.8% were predicted correctly (compared to 98.5% among the seen cases). Some examples of the correct predictions are given in Table 5. As can be seen, these include some complicated cases, so it is fair to say that the system is not simply memorizing but does have some capability to generalize."}, {"heading": "6 Experiment 2: Attention-based RNN sequence-to-sequence models", "text": "Our second approach involvesmodeling the problem entirely as a sequence-to-sequence problem. That is, rather than have a separate \u201cchannel\u201d and language model phase, we model the whole task as one where we map a sequence of input characters to a sequence of output words. For this we use a Tensor Flow (Abadi et al., 2015) model with an attention mechanism (Mnih et al., 2014). Attention models are particularly good for sequence-to-sequence problems since they are able to continuously update the decoder with information about the state of the encoder and thus attend better to the relation between the input and output sequences. The Tensor Flow implementation used is essentially the same as that reported in (Chan et al., 2016). In principle one might treat this problem in a way similar to how MT has been treated as a sequenceto-sequence problem (Cho et al., 2014), and simply pass the whole sentence to be normalized into a sequence of words. The main problem is that since we need to treat the input as a sequence of characters as we argued above, the input layer would need to be rather large in order to cover sentences of reasonable length. We therefore took a different approach and placed each token in a window of 3 words to the left and 3 to the right, marking the to-be-normalized token with a distinctive begin and end tag . Thus for example the token in the context I live at ... King Ave .\nwould appear as\non the input side, which would map to\non the output side. In this way we were able to limit the number of input and output nodes to something reasonable. The architecture follows closely that of (Chan et al., 2016). Specifically, we used a 4-layer bidirectional LSTM reader (but without the pyramidal structure used Chan et al.\u2019s task) that reads input characters, a layer of 256 attentional units, and a 2-layer decoder that produces word sequences. The reader is referred to (Chan et al., 2016) for more details of the framework.\nIt was noticed in early experiments with this configuration that the overabundance of outputs was swamping the training and causing the system to predict in too many cases. We therefore down-sampled the instances of (and ) in the training so that only roughly one in ten examples were given to the learner; among the various settings we tried, this seemed to give the best results both in terms of performance and reduced training time. The training, development and testing data were the same as described in Section 4 above. The English RNN was trained for about five and a half days (460K steps) on 8 GPUs until the perplexity on the held-out data was 1.003; Russian was trained for five days (400K steps), reaching a perplexity of 1.002."}, {"heading": "6.1 Results", "text": "As the results in Table 6 show, the performance is mostly better than the LSTMmodel described in Section 5. This suggests in turn that modeling the problem as a pure sequence-to-sequence transduction is indeed viable as an alternative to the source-channel approach we had taken previously. Some errors are shown in Table 7. These errors are reminiscent of several of the errors of the LSTM system in Table 4, in that the wrong unit is picked. On the other hand it must be admitted that, in English, the only clear error of that type is the one example shown in Table 7. Again, as with the source-\nchannel approach, there is evidence that while the system may be doing a lot of memorization, it is not merely memorizing. For English, 90.6% of the cases not found in the training data were correctly produced (compared to 99.8% of the seen cases); for Russian 86.7% of the unseen cases were correct (versus 99.4% of the seen cases). Complicated previously unseen cases in Russian, for example include examples like 9 \u0438\u044e\u043d\u044f 1966 \u0433., correctly read as \u0434\u0435\u0432\u044f\u0442\u043e\u0435 \u0438\u044e\u043d\u044f \u0442\u044b\u0441\u044f\u0447\u0430 \u0434\u0435\u0432\u044f\u0442\u044c\u0441\u043e\u0442\u0448\u0435\u0441\u0442\u044c\u0434\u0435\u0441\u044f\u0442 \u0448\u0435\u0441\u0442\u043e\u0433\u043e \u0433\u043e\u0434\u0430 (\u2018ninth of June, of the one thousand nine hundred sixty sixth year\u2019); or 17.04.1750, correctly read as \u0441\u0435\u043c\u043d\u0430\u0434\u0446\u0430\u0442\u043e\u0435 \u0430\u043f\u0440\u0435\u043b\u044f \u0442\u044b\u0441\u044f\u0447\u0430 \u0441\u0435\u043c\u044c\u0441\u043e\u0442 \u043f\u044f\u0442\u0438\u0434\u0435\u0441\u044f\u0442\u043e\u0433\u043e \u0433\u043e\u0434\u0430 (\u2018seventeenth ofApril of the one thousand seven hundred fiftieth year\u2019)."}, {"heading": "6.2 Results on \u201creasonable\u201d-sized data sets.", "text": "The results reported in the previous sections depended on impractically large amounts of training data. To develop a system for a new language one needs a system that could be trained on data sets of a size that one could expect a team of native speakers to hand label. Assuming one is willing to invest a few weeks\u2019 of work with a small team, it is not out of the question that one could label about 10 million words of Wikipedia-style text.7\nWith this point in mind, we retrained the systems on 11.4 million tokens of English from the beginning of the original training set, and 11.9 million tokens of Russian. The system was trained for about 7 days for both languages, until the system had achieved a perplexity on held-out data of 1.002 and for Russian 1.007. Results are presented in Table 8. The overall performance is not greatly different from the system trained on the larger dataset, and in some places is actually better.8 The test data overlapped with the training data in 96.9% of the tokens for English and 95.5% for Russian, with the accuracy of the non-overlapped tokens being 95.0% for English and 93.5% for Russian. The errors made by the system are comparable to errors we have already seen, though in English the errors in this case seem to be more concentrated in the reading of numeric dates. Thus to give just a few examples for English, reading 2008-07-28 as \u201cthe eighteenth of september seven thousand two\u201d, or 2009-\n7The author hand-labeled 1,000 tokens of English in about 7 minutes.\n8One might also anticipate that one could get better results for some underperforming categories by greedy selection of training text that included more examples of those categories, something that was not done in this experiment.\n10-02 as the ninth of october twenty thousand two. Some relatively complicated examples not seen in the training data that the English system got right included 221.049 km\u00b2 as two hundred twenty one point o four nine square kilometers, 24 March 1951 as the twenty fourth of march nineteen fifty one and $42,100 as forty two thousand one hundred dollars. Clearly then, the attention-based models are able to achieve with reasonable-sized data performances that are close to what it achieves with the large training data set. That said, the system of course continues to produce \u201csilly\u201d errors, whichmeans that it will not be sufficient on its own as the text normalization component of a TTS system."}, {"heading": "7 Finite-state filters", "text": "As we saw in the previous section, an approach that uses attention-based sequence-to-sequence models can produce extremely high accuracies, but is still prone to occasionally producing output that is completely misleading given the input. What if we apply some additional knowledge to filter the output so that it removes silly analyses? One way to do this is to construct finitestate filters, and use them to guide the decoding. For example one can construct an FST that maps from expressions of the form to a cardinal or decimal number and the possible verbalizations of the measure abbreviation. Thus 24.2kg might verbalize as twenty four point two kilogram or twenty four point two kilograms. The\nFST thus implements an overgenerating grammar that includes the correct verbalization, but allows other verbalizations as well. We constructed a Thrax grammar (Roark et al., 2012) to cover MEASURE and MONEY expressions, two classes where the RNN is prone to produce silly readings. The grammar consists of about 150 lines, half of which consists purely mechanical language-independent rules to flip the order of currency expressions so that \u00a35 is transformed to its reading order 5\u00a3. The Thrax grammar also incorporates English-specific lists of about 450 money and measure expressions so that, e.g., we know that kg can be kilogram or kilograms, as well as a number FST that is learned from a few hundred number names using the algorithm described in (Gorman and Sproat, 2016). Note that it is minimal effort to produce the lexical lists and the number name training data for a new language, certainly much less effort than producing a complete hand-built normalization grammar.9\nDuring decoding, the FST is composed with the input token being considered by the RNN. If the composition fails \u2014 e.g. because this token is not one of the classes that the FST handles \u2014 then the decoding will proceed as it normally would via the RNN alone. If the composition succeeds, then the FST is projected to the output, and the resulting output lattice is used to restrict the possible outputs from the RNN. Since the input was a specific token\u2014e.g. 2kg \u2014 the output lattice will include only sequences that may be verbalizations of that token. This output lattice is transformed so that all prefixes of the output are also allowed (e.g. two is allowed as well as two kilograms). This can be done simply by walking the states in the lattice, and making all non-final states final with a free exit cost (i.e., 0 in the tropical semiring). However, we wish to give a strong reward for traversing the whole lattice from the initial to an original final state, and so the exit cost for original final states is set to a very low negative value (-1000 in the current implementation).10\nThe RNN decoder queries the lattice with a se-\n9The Thrax grammar and associated data will be released along with the main datasets.\n10This final exit cost is actually set in the Thrax grammar itself, though it could as easily have been done dynamically at runtime.\nquence of labels, the object being to find the possible transitions to the next labels and their cost. The label sequence is first transformed into a trivial acceptor, to which is concatenated an FSA that accepts any single output token, and thus has a branching factor of |V|, the size of the output vocabulary. This FSA is then composed with the lattice. For strings in the FSA that match against the lattice, the cost will be the exit cost at that state in the lattice; for strings that fail the cost will be . Suppose that the input sequence is , and that the output lattice allows or . Then the FST will return a score of for the label , for example. However for or it will return a non-infinite cost, and indeed since exiting on one of these corresponds to an original final state of the grammar, it will accrue the reward discussed above. These costs will then be combined with the RNN\u2019s own scores for the sequence, and the final result computed as with the RNN alone. Note that since all prefixes of the sequences allowed by the grammar are also allowed, the RNN could, in the cited instance, produce two hundred as the output. However, it will get a substantial reward for finishing the sequence (two hundred kilogram or two hundred kilograms). As we shall see below, this is nearly always sufficient to persuade the RNN to take amore reasonable path. We note in passing that this method is more or less the opposite approach to that of (Rastogi et al., 2016). In that work, the FST\u2019s scoring is augmented by an RNN, whereas in the present approach, the RNN\u2019s decoding is guided by the use of an FST. Accuracies in English for the unfiltered and filtered RNN outputs, where the RNN is trained on the smaller training set described in the previous section, are given in Table 9. The MEASURE and MONEY sets show substantial improvement, while none of the other sets are affected, exactly as desired. Indeed, in this and the following tables we retain the scores for the non-MEASURE/MONEY cases in order to demonstrate that the performance on those classes is unaffected\u2014 not a given, since in principle the FSTs could overapply. In order to focus in on the differences between the filtered and unfiltered models, we prepared a different subset of the final training file that was rich in\nMEASURE and MONEY expressions. Specifically, we selected 1,000 sentences, each of which had one expression in that category. We then decoded with the models trained on the smaller training set, with and without the FST filter. Results are presented in Table 10. Once again, the FST filter improves the overall accuracy for MONEY and MEASURE, leaving the other categories unaffected. Some examples of the improvements in both categories are shown in Table 11. Looking more particularly at measures, where the largest differences are found, we find that the only cases where the FST filter does not help is cases where the grammar fails to match against the input and the RNN alone is used to predict the output. These cases are 1/2 cc, 30\u2019 (for thirty feet), 80\u2019, 7000 hg (which uses the unusual unit hectogram), 600 billion kWh (the measure grammar did not allow for a spelled number like billion), and the numberless \u201cmeasures\u201d per km, /m\u00b2. In a couple of other cases, the FST does not constrain the RNN enough: 1 g still comes out as one grams, since the FST allows both it and the correct one gram, but this of course is an \u201cacceptable\u201d error since it is at least not misleading.\nFinally Table 12 shows results for the RNN with and without the FST filter on 1000 MONEY and MEASURE expressions that have not previously been seen in the training data.11 In this case there was no improvement for MONEY, but there was a substantial improvement for MEASURE. In most cases, the MONEY examples that failed to be improved with the FST filter were cases where the filter simply did not match the input, and thus was not used.12\nThe results of a similar experiment on Russian, using the smaller training set on a MEASUREMONEY rich corpus where the MEASURE and MONEY tokens were previously unseen is shown in Table 13. On the face of it it would seem that the FST filter is actually making things worse, until one looks at the differences. Of the 50 cases where the filter made things \u201dworse\u201d, 34 (70%) are cases where there was an error in the data and a perfectly well formed measure was rendered with\n11To remind the reader, all test data are of course held out from the training and development data, but it is common for the same literal expression to recur.\n12Only in three cases involving Indian Rupees, such asRs.149 did the filter match, but still the wrong answer (in this case six), was produced. In that case the RNN probably simply failed to produce any paths including the right answer. In such cases the only solution is probably to override the RNN completely on a case-by-case basis.\nas the \u2018truth\u2019. In nearly all other cases, the input was actually ill formed and both Kestrel and the RNN without the FST filter \u2018corrected\u2019 the input. For example a Wikipedia contributor wrote 47 292 \u0434\u043e\u043b\u043b\u0430\u0440\u043e\u0432 \u201847,292 dollars\u2019, which should correctly be 47 292 \u0434\u043e\u043b\u043b\u0430\u0440\u0430, since the preceding number ends in \u20182\u2019, and thus the word for \u2018dollar\u2019 should be in the genitive singular, not the genitive plural. Now, the Kestrel grammars for Russian have the property that they read measure and money expressions, among other semiotic classes, into an internal format that in some cases abstracts away from the written form. In the case at hand thewritten \u0434\u043e\u043b\u043b\u0430\u0440\u043e\u0432 gets represented internally as dollar. During the verbalization phase the verbalizer grammars translate this into the form of the word required by the grammatical context, in this case \u0434\u043e\u043b\u043b\u0430\u0440\u0430. Thus Kestrel has the (one could argue) undesirable property of\nenforcing grammatical constraints on the input. The result is that the data contains instances of these sorts of corrections where \u0434\u043e\u043b\u043b\u0430\u0440\u043e\u0432 gets rendered as \u0434\u043e\u043b\u043b\u0430\u0440\u0430, and the RNN left to its own devices learns this mapping. Thus the RNN produces \u0441\u043e\u0440\u043e\u043a \u0441\u0435\u043c\u044c \u0442\u044b\u0441\u044f\u0447 \u0434\u0432\u0435\u0441\u0442\u0438 \u0434\u0435\u0432\u044f\u043d\u043e\u0441\u0442\u043e \u0434\u0432\u0430 \u0434\u043e\u043b\u043b\u0430\u0440\u0430. The FST filter, which does not allow \u0434\u043e\u043b\u043b\u0430\u0440\u043e\u0432 to be read as \u0434\u043e\u043b\u043b\u0430\u0440\u0430, verbalizes as written \u2014 arguably the right behavior for a TTS system, which should not be in the business of correcting the grammar of the input text. In addition to these cases, there were 67 cases where the RNN+FST was an unequivocal improvement over the RNN alone, as in 10 \u043a\u041d \u2018ten kiloNewtons\u2019, which was read by the RNN as \u0434\u0435\u0441\u044f\u0442\u0438 \u043a\u0438\u043b\u043e\u043b\u0438\u0442\u0440\u043e\u0432 \u2018ten kiloliters\u2019 but by the RNN+FST as \u0434\u0435\u0441\u044f\u0442\u0438 \u043a\u0438\u043b\u043e\u043d\u044c\u044e\u0442\u043e\u043d\u043e\u0432 \u2018ten kilonewtons\u2019. This is of course an instance of a broad class of category errors sometimes made by the RNN alone, that we have seen many instances of. All in all then, the FST-filtration approach seems to be a viable way to improve the quality of the output for targeted cases where the RNN is prone to make the occasional error."}, {"heading": "8 Discussion and the challenge", "text": "We have presented evidence in this paper that training neural models to learn text normalization is probably not going to be reducible to simply having copious amounts of aligned written- and spoken-form text, and then training a general neural system to compute the mapping. An approach where one combines the RNNwith a more knowledge-based system such as an FST, such as we presented in Section 7, is probably a viable approach, but it has yet to be demonstrated that one can do it with RNNs alone. To be sure, our RNNs were often capable of producing surprisingly good results and learning some complex mappings. Yet they sometimes also produced weird output, making them risky for use in a TTS system. Of course traditional approaches to TTS text normalization make errors, but they are not likely to make an error like reading the wrong number, or substituting hours for gigabytes, something that the RNNs are quite prone to do. The reason the FST filtering approach works, of course, is precisely because it disallows such random mappings. Again, the situation is different from some other\nNLP applications, such as MT or parsing, where deep learning can be used more or less \u201cout-of-the box\u201d. Indeed if one were to evaluate a text normalization system on the basis of how well the system does overall, then the systems reported in this paper are already doing very well, with accuracies over 99%. But when one drills down and looks at the interesting cases \u2014 say dates, which account for about 2% of the tokens in these data \u2014 then the performance is less compelling. An MT system that fails on many instances of a somewhat unusual construction could still be a fairly decent MT system overall. A text normalization system that reads the year 2012 as two twelve is seriously problematic no matter how well it does on text overall. Ultimately the difference comes down to different demands of the domain: the bar for text normalization is simply higher. Given past experience we anticipate three main classes of responses, which we would like to briefly address. The first is that our characterization of what is important for a text normalization is idiosyncratic: what justification do we have for saying that, for example, a text normalization must get dates correct? But the response to that is obvious: the various semiotic classes are precisely where most of the effort has been devoted in developing traditional approaches to text normalization for TTS dating back to the 1970\u2019s (Allen et al., 1987), for the simple reason that a TTS system ought to be able to know how to read something like Sep 12, 2014. The second is that we have set up a straw man: who ever argued that one could expect a deep learning system to learn a text normalization system from these kind of data? It is true that nobody has specifically made that claim for text normalization, but the view is definitely one that is \u201cin the air\u201d: colleagues of one of the authors who work on TTS have been asked why so much hand labor goes into TTS systems. Can one not just get a huge amount of aligned text and speech and learn the mapping? The final and perhaps most anticipated response is: \u201cYou didn\u2019t use the right kind of models; if you had just used an X model with Y objective function, etc., then you would have solved the problems you noted.\u201d Our response to that is that the data described in this paper will be made publicly available, and people are encouraged to try out their clever ideas for themselves.\nThe challenge then can be laid out simply as follows: using the data reported here,13 train a pure deep learning based normalization system for English and Russian that outperforms the results reported in this paper. By \u201coutperform\u201d here we are not primarily focusing on the overall scores, which are already very good, but rather the scores for various of the interesting categories. Rather, can one get a system, for example, that would never read \u00a3 as dollars or euros, or any of the other similar errors where a related but incorrect term has been substituted? (We take it as given that the same training-development-test division of the data is used and the same scoring scripts.) If one could train a pure deep-learning system that failed to make these sorts of silly errors and in general did better than systems reported here on the various semiotic categories, this would represent a true advance over the state of the art reported in this paper."}, {"heading": "Acknowledgements", "text": "We thank Alexander Gutkin for preparing the original code for producing Kestrel text normalization, and to Kyle Gorman for producing the data. Alexander Gutkin also checked a sample of the Russian data to estimate Kestrel\u2019s error rate. We also thank both Alexander and Kyle, as well as Brian Roark and Suyoun Yoon for comments on earlier versions of this paper. Finally Hasim Sak for help with the LSTM models reported in Experiment 1."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org", "author": ["tenberg", "MartinWicke", "YuanYu", "andXiaoqiang Zheng"], "venue": null, "citeRegEx": "tenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "tenberg et al\\.", "year": 2015}, {"title": "Distributed representation and estimation of WFSTbased n-gram models", "author": ["Cyril Allauzen", "Michael Riley", "Brian Roark."], "venue": "ACL SIGFSM Workshop on Statistical NLP and Weighted Automata.", "citeRegEx": "Allauzen et al\\.,? 2016", "shortCiteRegEx": "Allauzen et al\\.", "year": 2016}, {"title": "From Text to Speech: The MITalk System", "author": ["Jonathan Allen", "Sharon M. Hunnicutt", "Dennis Klatt."], "venue": "Cambridge University Press.", "citeRegEx": "Allen et al\\.,? 1987", "shortCiteRegEx": "Allen et al\\.", "year": 1987}, {"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Philip Arthur", "Graham Neubig", "Satoshi Nakamura."], "venue": "EMNLP, Austin, TX.", "citeRegEx": "Arthur et al\\.,? 2016", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "Personalized normalization for a multilingual chat system", "author": ["Ai Ti Aw", "Lian Hau Lee."], "venue": "ACL, pages 31\u201336, Jeju Island, Korea.", "citeRegEx": "Aw and Lee.,? 2012", "shortCiteRegEx": "Aw and Lee.", "year": 2012}, {"title": "Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normalization and named entity recognition", "author": ["Timothy Baldwin", "Young-Bum Kim", "Marie Catherine de Marneffe", "Alan Ritter", "Bo Han", "Wei Xu."], "venue": "InWNUT.", "citeRegEx": "Baldwin et al\\.,? 2015", "shortCiteRegEx": "Baldwin et al\\.", "year": 2015}, {"title": "A hybrid rule/model-based finite-state framework for normalizing SMS messages", "author": ["Richard Beaufort", "Sophie Roekhaut", "Louise-Am\u00e9lie Cougnon", "C\u00e9drick Fairon."], "venue": "ACL, pages 770\u2013779, Uppsala, Sweden.", "citeRegEx": "Beaufort et al\\.,? 2010", "shortCiteRegEx": "Beaufort et al\\.", "year": 2010}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "Journal of Machine Learning Research, 3:1137\u20131155, March.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Training speech recognition using captions, March 26", "author": ["S.S. Bharadwaj", "S.B. Medapati."], "venue": "US Patent App. 14/037,144.", "citeRegEx": "Bharadwaj and Medapati.,? 2015", "shortCiteRegEx": "Bharadwaj and Medapati.", "year": 2015}, {"title": "A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["Bernd Bohnet", "Joakim Nivre."], "venue": "EMNLPCoNLL, pages 1455\u20131465, Jeju Island, Korea.", "citeRegEx": "Bohnet and Nivre.,? 2012", "shortCiteRegEx": "Bohnet and Nivre.", "year": 2012}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals."], "venue": "ICASSP, pages 4960\u20134964.", "citeRegEx": "Chan et al\\.,? 2016", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "CoRR, abs/1406.1078.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Investigation and modeling of the structure of texting language", "author": ["Monojit Choudhury", "Rahul Saraf", "Vijit Jain", "Sudesha Sarkar", "Anupam Basu."], "venue": "International Journal of Document Analysis and Recognition, 10:157\u2013174.", "citeRegEx": "Choudhury et al\\.,? 2007", "shortCiteRegEx": "Choudhury et al\\.", "year": 2007}, {"title": "Normalizing tweets with edit scripts and recurrent neural embeddings", "author": ["Grzegorz Chrupa\u0142a."], "venue": "ACL, Singapore.", "citeRegEx": "Chrupa\u0142a.,? 2014", "shortCiteRegEx": "Chrupa\u0142a.", "year": 2014}, {"title": "The Kestrel TTS text normalization system", "author": ["Peter Ebden", "Richard Sproat."], "venue": "Natural Language Engineering, 21(3):1\u201321.", "citeRegEx": "Ebden and Sproat.,? 2014", "shortCiteRegEx": "Ebden and Sproat.", "year": 2014}, {"title": "Svmtool: A general pos tagger generator based on support vector machines", "author": ["Jes\u00fas Gim\u00e9nez", "Llu\u00eds M\u00e0rquez."], "venue": "Proceedings of the 4th LREC, Lisbon, Portugal.", "citeRegEx": "Gim\u00e9nez and M\u00e0rquez.,? 2004", "shortCiteRegEx": "Gim\u00e9nez and M\u00e0rquez.", "year": 2004}, {"title": "Minimally supervised models for number normalization", "author": ["Kyle Gorman", "Richard Sproat."], "venue": "Transactions of the Association for Computational Linguistics.", "citeRegEx": "Gorman and Sproat.,? 2016", "shortCiteRegEx": "Gorman and Sproat.", "year": 2016}, {"title": "Towards end-toend speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly."], "venue": "ICML, pages 1764\u20131772.", "citeRegEx": "Graves and Jaitly.,? 2014", "shortCiteRegEx": "Graves and Jaitly.", "year": 2014}, {"title": "Connectionist temporal classification: Labeling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Gaustino Gomez", "J\u00fcrgen Schmidhuber."], "venue": "ICML, pages 369\u2013 376.", "citeRegEx": "Graves et al\\.,? 2006", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Social text normalization using contextual graph random walks", "author": ["Hany Hassan", "Arul Menezes."], "venue": "ACL, pages 1577\u20131586.", "citeRegEx": "Hassan and Menezes.,? 2013", "shortCiteRegEx": "Hassan and Menezes.", "year": 2013}, {"title": "Named entity transcription with pair n-gram models", "author": ["Martin Jansche", "Richard Sproat."], "venue": "NEWS \u201909, pages 32\u201335, Singapore.", "citeRegEx": "Jansche and Sproat.,? 2009", "shortCiteRegEx": "Jansche and Sproat.", "year": 2009}, {"title": "Syntactic normalization of Twitter messages", "author": ["Max Kaufmann."], "venue": "International Conference on NLP.", "citeRegEx": "Kaufmann.,? 2010", "shortCiteRegEx": "Kaufmann.", "year": 2010}, {"title": "Normalizing SMS: are two metaphors better than one", "author": ["Catherine Kobus", "Fran\u00e7ois Yvon", "G\u00e9raldine Damnati"], "venue": "In COLING,", "citeRegEx": "Kobus et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kobus et al\\.", "year": 2008}, {"title": "Insertion, deletion, or substitution? Normalizing text messages without pre-categorization nor supervision", "author": ["Fei Liu", "Fuliang Weng", "Bingqing Wang", "Yang Liu."], "venue": "ACL, pages 71\u201376, Portland, Oregon, USA.", "citeRegEx": "Liu et al\\.,? 2011", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "A broadcoverage normalization system for social media language", "author": ["Fei Liu", "Fuliang Weng", "Xiao Jiang."], "venue": "ACL, pages 1035\u20131044, Jeju Island, Korea. Association for Computational Linguistics.", "citeRegEx": "Liu et al\\.,? 2012a", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Joint inference of named entity recognition and normalization for tweets", "author": ["Xiaohua Liu", "Ming Zhou", "Xiangyang Zhou", "Zhongyang Fu", "FuruWei."], "venue": "ACL, pages 526\u2013535, Jeju Island, Korea.", "citeRegEx": "Liu et al\\.,? 2012b", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Interspeech, volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "NCSU SAS WOOKHEE: A deep contextual long-short term memory model for text normalization", "author": ["Wookhee Min", "Bradford Mott."], "venue": "WNUT.", "citeRegEx": "Min and Mott.,? 2015", "shortCiteRegEx": "Min and Mott.", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu."], "venue": "NIPS, pages 2204\u20132212.", "citeRegEx": "Mnih et al\\.,? 2014", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Weighted finite-state transducers in speech recognition", "author": ["Mehryar Mohri", "Fernando Pereira", "Michael Riley."], "venue": "Computer Speech & Language, 16(1):69\u2013", "citeRegEx": "Mohri et al\\.,? 2002", "shortCiteRegEx": "Mohri et al\\.", "year": 2002}, {"title": "A character-level machine translation approach for normalization of SMS abbreviations", "author": ["Deana Pennell", "Yang Liu."], "venue": "IJCNLP.", "citeRegEx": "Pennell and Liu.,? 2011", "shortCiteRegEx": "Pennell and Liu.", "year": 2011}, {"title": "Grapheme-to-phoneme conversion using long short-term memory recurrent neural networks", "author": ["Kanishka Rao", "Fuchun Peng", "Ha\u015fim Sak", "Fran\u00e7oise Beaufays."], "venue": "ICASSP, pages 4225\u20134229.", "citeRegEx": "Rao et al\\.,? 2015", "shortCiteRegEx": "Rao et al\\.", "year": 2015}, {"title": "Weighting finite-state transductions with neural context", "author": ["Pushpendre Rastogi", "Ryan Cotterell", "Jason Eisner."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Rastogi et al\\.,? 2016", "shortCiteRegEx": "Rastogi et al\\.", "year": 2016}, {"title": "Hippocratic abbreviation expansion", "author": ["Brian Roark", "Richard Sproat."], "venue": "ACL, pages 364\u2013369.", "citeRegEx": "Roark and Sproat.,? 2014", "shortCiteRegEx": "Roark and Sproat.", "year": 2014}, {"title": "The OpenGrm open-source finite-state grammar software libraries", "author": ["Brian Roark", "Richard Sproat", "Cyril Allauzen", "Michael Riley", "Jeffrey Sorensen", "Terry Tai."], "venue": "ACL, pages 61\u201366.", "citeRegEx": "Roark et al\\.,? 2012", "shortCiteRegEx": "Roark et al\\.", "year": 2012}, {"title": "Applications of maximum entropy rankers to problems in spoken language processing", "author": ["Richard Sproat", "Keith Hall."], "venue": "Interspeech, pages 761\u2013764.", "citeRegEx": "Sproat and Hall.,? 2014", "shortCiteRegEx": "Sproat and Hall.", "year": 2014}, {"title": "Normalization of non-standard words", "author": ["Richard Sproat", "Alan Black", "Stanley Chen", "Shankar Kumar", "Mari Ostendorf", "Christopher Richards."], "venue": "Computer Speech and Language, 15(3):287\u2013333.", "citeRegEx": "Sproat et al\\.,? 2001", "shortCiteRegEx": "Sproat et al\\.", "year": 2001}, {"title": "Multilingual text analysis for textto-speech synthesis", "author": ["Richard Sproat."], "venue": "Natural Language Engineering, 2(4):369\u2013380.", "citeRegEx": "Sproat.,? 1996", "shortCiteRegEx": "Sproat.", "year": 1996}, {"title": "Lightly supervised learning of text normalization: Russian number names", "author": ["Richard Sproat."], "venue": "IEEE SLT, pages 436\u2013441.", "citeRegEx": "Sproat.,? 2010", "shortCiteRegEx": "Sproat.", "year": 2010}, {"title": "Text-to-Speech Synthesis", "author": ["Paul Taylor."], "venue": "Cambridge University Press, Cambridge.", "citeRegEx": "Taylor.,? 2009", "shortCiteRegEx": "Taylor.", "year": 2009}, {"title": "A phonetic-based approach to Chinese chat text normalization", "author": ["Yunqing Xia", "Kam-Fai Wong", "Wenjie Li."], "venue": "ACL, pages 993\u20131000, Sydney, Australia.", "citeRegEx": "Xia et al\\.,? 2006", "shortCiteRegEx": "Xia et al\\.", "year": 2006}, {"title": "A log-linear model for unsupervised text normalization", "author": ["Yi Yang", "Jacob Eisenstein."], "venue": "EMNLP, pages 61\u201372.", "citeRegEx": "Yang and Eisenstein.,? 2013", "shortCiteRegEx": "Yang and Eisenstein.", "year": 2013}], "referenceMentions": [{"referenceID": 36, "context": "In the original written form there are two nonstandard words (Sproat et al., 2001), namely the two measure expressions 6ft and 150lb.", "startOffset": 61, "endOffset": 82}, {"referenceID": 39, "context": "In this case both examples are instances of the same semiotic class (Taylor, 2009), namely measure phrases.", "startOffset": 68, "endOffset": 82}, {"referenceID": 8, "context": "For speech recognition acoustic model training, one could in theory use closed captioning (Bharadwaj and Medapati, 2015), which again is produced", "startOffset": 90, "endOffset": 120}, {"referenceID": 31, "context": "sequence-to-sequence LSTM that has been successfully applied to the somewhat similar problem of grapheme-to-phoneme conversion (Rao et al., 2015),", "startOffset": 127, "endOffset": 145}, {"referenceID": 10, "context": "sion problem (Chan et al., 2016).", "startOffset": 13, "endOffset": 32}, {"referenceID": 2, "context": "nology, dating back to the earliest work on full TTS synthesis (Allen et al., 1987).", "startOffset": 63, "endOffset": 83}, {"referenceID": 2, "context": "nology, dating back to the earliest work on full TTS synthesis (Allen et al., 1987). Sproat (1996) provided a unifying model for most text normalization problems in terms of weighted finite-state transducers (WFSTs).", "startOffset": 64, "endOffset": 99}, {"referenceID": 36, "context": "normalization as essentially a language modeling problem was (Sproat et al., 2001).", "startOffset": 61, "endOffset": 82}, {"referenceID": 38, "context": "More recent machine learning work specifically addressed to TTS text normalization include (Sproat, 2010; Roark and Sproat, 2014; Sproat and Hall, 2014).", "startOffset": 91, "endOffset": 152}, {"referenceID": 33, "context": "More recent machine learning work specifically addressed to TTS text normalization include (Sproat, 2010; Roark and Sproat, 2014; Sproat and Hall, 2014).", "startOffset": 91, "endOffset": 152}, {"referenceID": 35, "context": "More recent machine learning work specifically addressed to TTS text normalization include (Sproat, 2010; Roark and Sproat, 2014; Sproat and Hall, 2014).", "startOffset": 91, "endOffset": 152}, {"referenceID": 40, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 12, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 22, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 6, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 21, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 23, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 30, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 4, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 24, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 25, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 41, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 13, "context": "cludes (Chrupa\u0142a, 2014; Min and Mott, 2015).", "startOffset": 7, "endOffset": 43}, {"referenceID": 27, "context": "cludes (Chrupa\u0142a, 2014; Min and Mott, 2015).", "startOffset": 7, "endOffset": 43}, {"referenceID": 5, "context": "The latter work, for example, achieved second place in the constrained track of the ACL 2015 W-NUT Normalization of Noisy Text (Baldwin et al., 2015), achieving an F1 score of 81.", "startOffset": 127, "endOffset": 149}, {"referenceID": 14, "context": "Wikipedia regions that could be decoded as UTF8, divided into sentences, and run through the Google TTS system\u2019s Kestrel text normalization system (Ebden and Sproat, 2014) to produce verbalizations.", "startOffset": 147, "endOffset": 171}, {"referenceID": 14, "context": "As described in (Ebden and Sproat, 2014), Kestrel\u2019s verbalizations are produced by first tokenizing the input and classifying the tokens, and then verbalizing each token according to its semi-", "startOffset": 16, "endOffset": 40}, {"referenceID": 34, "context": "The majority of the rules are hand-built using the Thrax finite-state grammar development system (Roark et al., 2012).", "startOffset": 97, "endOffset": 117}, {"referenceID": 20, "context": "Statistical components of the system include morphosyntactic taggers for languages like Russian with complex morphology,2 a statistical transliterationmodule (Jansche and Sproat, 2009), and a statistical model to determine if capitalized tokens should be read as words or letter sequences (Sproat and Hall, 2014).", "startOffset": 158, "endOffset": 184}, {"referenceID": 35, "context": "Statistical components of the system include morphosyntactic taggers for languages like Russian with complex morphology,2 a statistical transliterationmodule (Jansche and Sproat, 2009), and a statistical model to determine if capitalized tokens should be read as words or letter sequences (Sproat and Hall, 2014).", "startOffset": 289, "endOffset": 312}, {"referenceID": 15, "context": "2The morphosyntactic tagger is an SVM model using hand-tuned features that classify the morphological bundle for each word independently, similar to SVMTool (Gim\u00e9nez and M\u00e0rquez, 2004) and MateTagger (Bohnet and Nivre, 2012).", "startOffset": 157, "endOffset": 184}, {"referenceID": 9, "context": "2The morphosyntactic tagger is an SVM model using hand-tuned features that classify the morphological bundle for each word independently, similar to SVMTool (Gim\u00e9nez and M\u00e0rquez, 2004) and MateTagger (Bohnet and Nivre, 2012).", "startOffset": 200, "endOffset": 224}, {"referenceID": 37, "context": "Furthermore, sinceWFSTs can be used to handle most or all of the needed transductions (Sproat, 1996), the relation between the input and output strings is regular, so that complex network architectures involving, say, stacks should not be", "startOffset": 86, "endOffset": 100}, {"referenceID": 31, "context": "in (Rao et al., 2015) in two configurations: one with two forward and two backward hidden layers, henceforth the shallow model; and one with three forward and three backward hidden layers, henceforth the deep model.", "startOffset": 3, "endOffset": 21}, {"referenceID": 18, "context": "The output layer is a connectionist temporal classification (CTC) (Graves et al., 2006) layer with a softmax error function.", "startOffset": 66, "endOffset": 87}, {"referenceID": 1, "context": "For comparison the perplexities for a 5-gram WFST language model with Katz backoff trained using the toolkit reported in (Allauzen et al., 2016) on the same data and evaluated on the same held-out data are given in parentheses.", "startOffset": 121, "endOffset": 144}, {"referenceID": 29, "context": "nel and LM probabilities can be thought of a as a poor-man\u2019s equivalent of the composition of a channel and LM weighted finite-state transducer (Mohri et al., 2002): the main difference is that there is no", "startOffset": 144, "endOffset": 164}, {"referenceID": 16, "context": "In both languages we find examples where the system gets numbers wrong, evidently because it is hard for the system to learn from the training data exactly how to map from digit sequences to number names \u2014 see also (Gorman and Sproat, 2016).", "startOffset": 215, "endOffset": 240}, {"referenceID": 7, "context": "This property is the whole basis of word embeddings (Bengio et al., 2003) and other similar techniques.", "startOffset": 52, "endOffset": 73}, {"referenceID": 3, "context": "Interestingly, a recent paper (Arthur et al., 2016) dis-", "startOffset": 30, "endOffset": 51}, {"referenceID": 28, "context": ", 2015) model with an attention mechanism (Mnih et al., 2014).", "startOffset": 42, "endOffset": 61}, {"referenceID": 10, "context": "The Tensor Flow implementation used is essentially the same as that reported in (Chan et al., 2016).", "startOffset": 80, "endOffset": 99}, {"referenceID": 11, "context": "similar to how MT has been treated as a sequenceto-sequence problem (Cho et al., 2014), and simply", "startOffset": 68, "endOffset": 86}, {"referenceID": 10, "context": "The architecture follows closely that of (Chan et al., 2016).", "startOffset": 41, "endOffset": 60}, {"referenceID": 10, "context": "The reader is referred to (Chan et al., 2016) for more details of the framework.", "startOffset": 26, "endOffset": 45}, {"referenceID": 34, "context": "We constructed a Thrax grammar (Roark et al., 2012) to cover MEASURE and MONEY expres-", "startOffset": 31, "endOffset": 51}, {"referenceID": 16, "context": ", we know that kg can be kilogram or kilograms, as well as a number FST that is learned from a few hundred number names using the algorithm described in (Gorman and Sproat, 2016).", "startOffset": 153, "endOffset": 178}, {"referenceID": 32, "context": "We note in passing that this method is more or less the opposite approach to that of (Rastogi et al., 2016).", "startOffset": 85, "endOffset": 107}, {"referenceID": 2, "context": "The first is that our characterization of what is important for a text normalization is idiosyncratic: what justification do we have for saying that, for example, a text normalization must get dates correct? But the response to that is obvious: the various semiotic classes are precisely where most of the effort has been devoted in developing traditional approaches to text normalization for TTS dating back to the 1970\u2019s (Allen et al., 1987), for the simple reason that a TTS system ought to be able to know how to read something like Sep 12, 2014.", "startOffset": 423, "endOffset": 443}], "year": 2016, "abstractText": "This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. Andwhenwe open-source our data, we will be providing a novel data set for sequenceto-sequence modeling in the hopes that the the community can find better solutions.", "creator": "Preview"}}}