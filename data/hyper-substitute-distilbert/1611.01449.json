{"id": "1611.01449", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Semi-supervised deep learning by metric embedding", "abstract": "deep networks are successfully used among test approaches implementing state - of - the - art results when concentrating on a corresponding amount of labeled samples. these models, however, are usually much less convenient for semi - supervised functions because frequently their naive students alter their compared trained on varying amounts of data. in this theory managers will explore numerous new training objective that consider establishing that semi - uniform domain matching only some small subset assigned labeled pixels. more criterion is based on a deep metric embedding over distance relations within the set of labeled samples, agreeing with polynomials over the locus of similar unlabeled set. the normally learned representations are simply underlying euclidean environments, suggesting hence can not used mentally constructing nearest - neighbor graphs considering the coded samples.", "histories": [["v1", "Fri, 4 Nov 2016 16:39:20 GMT  (102kb,D)", "http://arxiv.org/abs/1611.01449v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["elad hoffer", "nir ailon"], "accepted": false, "id": "1611.01449"}, "pdf": {"name": "1611.01449.pdf", "metadata": {"source": "CRF", "title": "SEMI-SUPERVISED DEEP LEARNING BY METRIC EM- BEDDING", "authors": ["Elad Hoffer"], "emails": ["ehoffer@tx.technion.ac.il", "nailon@cs.technion.ac.il"], "sections": [{"heading": null, "text": "Deep networks are successfully used as classification models yielding state-ofthe-art results when trained on a large number of labeled samples. These models, however, are usually much less suited for semi-supervised problems because of their tendency to overfit easily when trained on small amounts of data. In this work we will explore a new training objective that is targeting a semi-supervised regime with only a small subset of labeled data. This criterion is based on a deep metric embedding over distance relations within the set of labeled samples, together with constraints over the embeddings of the unlabeled set. The final learned representations are discriminative in euclidean space, and hence can be used with subsequent nearest-neighbor classification using the labeled samples."}, {"heading": "1 INTRODUCTION", "text": "Deep neural networks have been shown to perform very well on various classification problems, often yielding state-of-the-art results. Key motivation for the use of these models, is the assumption of hierarchical nature of the underlying problem. This assumption is reflected in the structure of NNs, composed of multiple stacked layers of linear transformations followed by non-linear activation functions. The NN final layer is usually a softmax activated linear transformation indicating the likelihood of each class, which can be trained by cross-entropy using the known target of each sample, and back-propagated to previous layers. The hierarchical property of NNs has been observed to yield high-quality, discriminative representations of the input in intermediate layers. These representative features, although not explicitly part of the training objective, were shown to be useful in subsequent tasks in the same domain as demonstrated by Razavian et al. (2014). One serious problem occurring in neural network is their susceptibility to overfit over the training data. Due to this fact, a considerable part of modern neural network research is devoted to regularization techniques and heuristics such as Srivastava et al. (2014); Ioffe & Szegedy (2015); Wan et al. (2013); Szegedy et al. (2015), to allow the networks to generalize to unseen data samples. The tendency to overfit is most apparent with problems having a very small number of training examples per class, and these are considered ill-suited to solve with neural network models. Because of this property, semisupervised regimes in which most data is unlabeled, are considered hard to learn and generalize with NNs.\nIn this work we will consider a new training criterion designed to be used with deep neural networks in semi-supervised regimes over datasets with a small subset of labeled samples. Instead of a usual cross-entropy between the labeled samples and the ground truth class indicators, we will use the labeled examples as targets for a metric embedding. Under this embedding, which is the mapping of a parameterized deep network, the features of labeled examples will be grouped together in euclidean space. In addition, we will use these learned embeddings to separate the unlabeled examples to belong each to a distinct cluster formed by the labeled samples. We will show this constraint translates to a minimum entropy criterion over the embedded distances. Finally, because of the use of euclidean space interpretation of the learned features, we are able to use a subsequent nearest-neighbor classifier to achieve state-of-the-art results on problems with small number of labeled examples.\nar X\niv :1\n61 1.\n01 44\n9v 1\n[ cs\n.L G\n] 4\nN ov\n2 01\n6"}, {"heading": "2 RELATED WORK", "text": ""}, {"heading": "2.1 LEARNING METRIC EMBEDDING", "text": "Previous works have shown the possible use of neural networks to learn useful metric embedding. One kind of such metric embedding is the \u201cSiamese network\u201d framework introduced by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face. This problem was recently tackled by Schroff et al. (2015) for training a convolutional network model on triplets of examples. Learning features by metric embedding was also shown by Hoffer & Ailon (2015) to provide competitive classification accuracy compare to conventional cross-entropy regression. This work is also related to Rippel et al. (2015), who introduced Magnet loss - a metric embedding approach for fine-grained classification. The Magnet loss is based on learning the distribution of distances for each sample, from K clusters assigned for each classified class. It then uses an intermediate k-means clustering, to reposition the different assigned clusters. This proved to allow better accuracy than both margin-based Triplet loss, and softmax regression. Using metric embedding with neural network was also specifically shown to provide good results in the semi-supervised learning setting as seen in Weston et al. (2012)."}, {"heading": "2.2 SEMI-SUPERVISED LEARNING BY ADVERSARIAL REGULARIZATION", "text": "As stated before, a key approach to generalize from a small training set, is by regularizing the learned model. Regularization techniques can often be interpreted as prior over model parameters or structure, such as Lp regularization over the network weights or activations. More recently, neural network specific regularizations that induce noise within the training process such as Srivastava et al. (2014); Wan et al. (2013); Szegedy et al. (2015) proved to be highly beneficial to avoid overfitting. Another recent observation by Goodfellow et al. (2015) is that training on adversarial examples, inputs that were found to be misclassified under small perturbation, can improve generalization. This fact was explored by Feng et al. (2016) and found to provide notable improvements to the semi supervised regime by Miyato et al. (2015)."}, {"heading": "2.3 SEMI-SUPERVISED LEARNING BY AUXILIARY RECONSTRUCTION LOSS", "text": "Recently, a stacked set of denoising auto-encoders architectures showed promising results in both semi-supervised and unsupervised tasks. A stacked what-where autoencoder by Zhao et al. (2015) computes a set of complementary variables that enable reconstruction whenever a layer implements a many-to-one mapping. Ladder networks by Rasmus et al. (2015) - use lateral connections to allow higher levels of an auto-encoder to focus on invariant abstract features by applying a layer-wise cost function.\nGenerative adversarial network (GAN) is a recently introduced model that can be used in an unsupervised fashion Goodfellow et al. (2014). Adversarial Generative Models use a set of networks, one trained to discriminate between data sampled from the true underlying distribution (e.g., a set of images), and a separate generative network trained to be an adversary trying to confuse the first network. By propagating the gradient through the paired networks, the model learns to generate samples that are distributed similarly to the source data. As shown by Radford et al. (2015), this model can create useful latent representations for subsequent classification tasks. The usage for these models for semi-supervised learning was further developed by Springenberg (2016) and Salimans et al. (2016), by adding a N + 1 way classifier (number of classes + and additional \u201cfake\u201d class) to the discriminator. This proved to allow excellent accuracy with only a small subset of labeled examples."}, {"heading": "2.4 SEMI-SUPERVISED LEARNING BY ENTROPY MINIMIZATION", "text": "Another technique for semi-supervised learning introduced by Grandvalet & Bengio (2004) is concerned with minimizing the entropy over expected class distribution for unlabeled examples. Regularizing for minimum entropy can be seen as a prior which prefers minimum overlap between observed classes. This can also be seen as a generalization of the \u201cself-training\u201d wrapper method\ndescribed by Triguero et al. (2015), in which unlabeled examples are re-introduced after being labeled with the previous classification of the model. This is also related to the \u201cTransductive suport vector machines\u201d (TSVM) Vapnik & Vapnik (1998) which introduces a maximum margin objective over both labeled and unlabeled examples."}, {"heading": "3 OUR CONTRIBUTION: NEIGHBOR EMBEDDING FOR SEMI-SUPERVISED LEARNING", "text": "In this work we are concerned with a semi-supervised setting, in which learning is done on data of which only a small subset is labeled. Given observed sets of labeled data XL = {(x, y)}li=1 and unlabeled data XU = {x}ni=l+1 where x \u2208 X , y \u2208 C, we wish to learn a classifier f : X \u2192 C to have a minimum expected error on some unseen test data Xtest.\nWe will make a couple of assumptions regarding the given data:\n\u2022 The number of labeled examples is small compared to the whole observed set l n. \u2022 Structure assumption - samples within the same structure (such as a cluster or manifold)\nare more likely to share the same label. This assumption is shared with many other semisupervised approaches as discussed in Chapelle et al. (2009),Weston et al. (2012).\nUsing these assumptions, we are motivated to learn a metric embedding that forms clusters such that samples can be classified by their L2 distance to the labeled examples in a nearest-neighbor procedure.\nWe will now define our learning setting on the semi-labeled data, using a neural network model denoted as F (x; \u03b8) where x is the input fed into the network, and \u03b8 are the optimized parameters (dropped henceforward for convenience). The output of the network for each sample is a vector of features of D dimensions F (x) \u2208 RD which will be used to represent the input.\nOur two training objectives which we aim to train our embedding networks by are:\n(i) Create feature representation that form clusters from the labeled examples {(x, y)} \u2208 XL such that two examples x1, x2 sharing the same label y1 = y2 will have a smaller embedded distance than any third example x3 with a different label y1 6= y3\n\u2016F (x1)\u2212 F (x2)\u20162 < \u2016F (x1)\u2212 F (x3)\u20162 (ii) For each unlabeled example, its feature embedding will be close to the embeddings of one\nspecific label occurring in L: For all x \u2208 XU , z \u2208 XL, there exists a specific class l \u2208 C such that\n\u2016F (x)\u2212 F (zl)\u20162 \u2016F (x)\u2212 F (zk)\u20162 where zl is any labeled example of class l and zk is any example from class k \u2208 C \\ {l}.\nAs the defined objectives create embeddings that target a nearest-neighbor classification with regard to the labeled set, we will refer to it as \u201cNeighbor embedding\u201d."}, {"heading": "4 LEARNING BY DISTANCE COMPARISONS", "text": "We will define a discrete distribution for the embedded distance between a sample x \u2208 X , and c labeled examples z1, ..., zc \u2208 XL each belonging to a different class:\nP (x; z1, ..., zc)i = e\u2212\u2016F (x)\u2212F (zi)\u2016 2\u2211c j=1 e \u2212\u2016F (x)\u2212F (zj)\u20162 , i \u2208 {1...c} (1)\nThis definition assigns a probability P (x; z1, ..., zc)i for sample x to be classified into class i, under a 1-nn classification rule, when z1, ..., zc neighbors are given. It is similar to the stochastic-nearestneighbors formulation of Goldberger et al. (2004), and will allow us to state the two underlying objectives as measures over this distribution."}, {"heading": "4.1 DISTANCE RATIO CRITERION", "text": "Addressing objective (i), we will use a sample xl \u2208 XL from the labeled set belonging to class k \u2208 C, and another set of sampled labeled examples z1, ..., zc \u2208 XL. In this work we will sample in uniform over all available samples for each class.\nDefining the class-indicator I(x) as\nI(xl)i = { 1 if i = k 0 otherwise\nwe will minimize the cross-entropy between I(xl) and the distance-distribution of x with respect to z1, ..., zc1:\nL(xl, z1, ..., zc)L = H (I(xl), P (xl; z1, ..., zc)) (2)\nThis is in fact a slightly modified version of distance ratio loss introduced in Hoffer & Ailon (2015).\nL(xl, z1, ..., zc)L = \u2212 log e\u2212\u2016F (xl)\u2212F (zk)\u2016 2\u2211c i=1 e \u2212\u2016F (xl)\u2212F (zi)\u20162 (3)\nThis loss is aimed to ensure that samples belonging to the same class will be mapped to have a small embedded distance compared to samples from different classes."}, {"heading": "4.2 MINIMUM ENTROPY CRITERION", "text": "Another part of the optimized criterion, inspired by Grandvalet & Bengio (2004), is designed to reduce the overlap between the different classes of the unlabeled samples.\nWe will promote this objective by minimizing the entropy of the underlying distance distribution of x, again with respect to labeled samples z1, ..., zc1:\nL(x, z1, ..., zc)U = H(P (x; z1, ..., zc)) (4)\nwhich is defined as\nL(x, z1, ..., zc)U = \u2212 c\u2211\ni=1\ne\u2212\u2016F (x)\u2212F (zi)\u2016 2\u2211c\nj=1 e \u2212\u2016F (x)\u2212F (zj)\u20162\n\u00b7 log e \u2212\u2016F (x)\u2212F (zi)\u20162\u2211c\nj=1 e \u2212\u2016F (x)\u2212F (zj)\u20162\n(5)\nWe note that entropy is lower if the distribution 1 is sparse, and higher if the distribution is dense, and this intuition is compatible with our objectives.\nOur final objective will use a sampled set of labeled examples, where each class is represented {z1, ..., zc} and additional labeled xl and unlabeled xu examples, combining a weighted sum of both 3 and 5 to form:\nL(xl, xu, {z1, ..., zc}) = \u03bbLL(xl, z1, ..., zc)L + \u03bbUL(xu, z1, ..., zc)U (6)\nWhere \u03bbL, \u03bbU \u2208 [0, 1] are used to determine the weight assigned to each criterion. This loss is differentiable and hence can be used for gradient-based training of deep models by existing optimization approaches and back-propagation (Rumelhart et al.) through the embedding neural network. The optimization can further be accelerated computationally by using mini-batches of both labeled and unlabeled examples."}, {"heading": "5 QUALITIES OF NEIGHBOR EMBEDDING", "text": "We will now discuss some observed properties of neighbor embeddings, and their usefulness to semi-supervised regimes using neural network models."}, {"heading": "5.1 REDUCING OVERFIT", "text": "Usually, when using NNs for classification, a cross-entropy loss minimization is employed by using a fixed one-hot indicator (similar to 2) as target for each labeled example, thus maximizing a log-likelihood of the correct label. This form of optimization over a fixed target tend to cause an overfitting of the neural-network, especially on small labeled sets. This was lately discussed and addressed by Szegedy et al. (2015) using added random noise to the targets, effectively smoothing the cross-entropy target distribution. This regularization technique was shown empirically to yield better generalization by reducing the overfitting over the training set. Training on distance ratio comparisons, as shown in our work, provides a natural alternative to this problem. By setting the optimization target to be the embeddings of labeled examples, we create a continuously moving target that is dependent on the current model parameters. We speculate that this reduces the model\u2019s ability to overfit easily on the training data, allowing very small labeled datasets to be exploited."}, {"heading": "5.2 EMBEDDING INTO EUCLIDEAN SPACE", "text": "By training the model to create feature embedding that are discriminative with respect to their distance in euclidean space, we can achieve good classification accuracy using a simple nearestneighbor classifier. This embedding allows an interpretation of semantic relation in euclidean space, which can be useful for various tasks such as information retrieval, or transfer learning."}, {"heading": "5.3 INCORPORATING PRIOR KNOWLEDGE", "text": "We also note that prior knowledge about a problem at hand can be incorporated into the expected measures with respect to the distance distribution 1. E.g, knowledge of relative distance between classes can be used to replace I(x) as target distribution in eq. 3 and knowledge concerning overlap between classes can be used to relax the constraint in eq. 5."}, {"heading": "6 EXPERIMENTS", "text": "All experiments were conducted using the Torch7 framework by Collobert et al. (2011). Code reproducing these results will by available at https://github.com/eladhoffer/ SemiSupContrast. For every experiment we chose a small random subset of examples, with a balanced number from each class and denoted by XL. The remaining training images are used without their labeled to form XU . Finally, we test our final accuracy with a disjoint set of examples Xtest. No data augmentation was applied to the training sets.\nIn each iteration we sampled uniformly a set of labeled examples z1, ...z|C| \u2208 XL. In addition, batches of uniformly sampled examples were also sampled again from the labeled set XL, and the unlabeled set XU .\nA batch-size of b = 32 was used for all experiments, totaling a sampled set of 2 \u00b7 b + |C| examples for each iteration, where |C| = 10 for both datasets. We used 6 as optimization criterion, where \u03bb1 = \u03bb2 = 1. Optimization was done using the Accelerated-gradient method by Nesterov (1983) with an initial learning rate of lr0 = 0.1 which was decreased by a factor of 10 after every 30 epochs. Both datasets were trained on for a total of 90 epochs. Final test accuracy results was achieved by using a k-NN classifier with best results out of k = {1, 3, 5}. These results were average over 10 random subsets of labeled data.\nAs the embedding model was chosen to be a convolutional network, the spatial properties of input space are crucial. We thus omit results on permutation-invariant versions of these problems, noting they usually tend to achieve worse classification accuracies."}, {"heading": "6.1 RESULTS ON MNIST", "text": "The MNIST database of handwritten digits introduced by LeCun et al. (1998) is one of the most studied dataset benchmark for image classification. The dataset contains 60,000 examples of handwritten digits from 0 to 9 for training and 10,000 additional examples for testing, where each sample is a 28 x 28 pixel gray level image.\nWe followed previous works ((Weston et al., 2012),(Zhao et al., 2015),Rasmus et al. (2015)) and used semi-supervised regime in which only 100 samples (10 for each class) were used as XL along with their labels. For the embedding network, we used a convolutional network with 5-convolutional layers, where each layer is followed by a ReLU non-linearity and batch-normalization layer Ioffe & Szegedy (2015). The full network structure is described in Appendix table 3. Results are displayed in table 1 and reflect that our approach yields state-of-the-art results in this regime.\nWe also attempted to visualize the outcome of using this method, by training an additional model with a final 2-dimensional embedding. Figure 1 shows the final embeddings, where labeled examples are marked in color with their respective class, and unlabeled examples are marked in gray. We can see that, in accordance with our objectives, the labeled examples formed clusters in euclidean space separate by their labels, while unlabeled examples were largely grouped to belong each to one of these clusters."}, {"heading": "6.2 RESULTS ON CIFAR-10", "text": "Cifar-10 introduced by Krizhevsky & Hinton (2009) is an image classification benchmark dataset containing 50, 000 training images and 10, 000 test images. The image sizes 32 \u00d7 32 pixels, with color. The classes are airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships and trucks.\nFollowing a commonly used regime, we trained on 4000 randomly picked samples (400 for each class). As the convolutional embedding network, we used a network similar to that of Lin et al. (2014) which is described in table 3. The test error results are brought in table 2.\nAs can be observed, we achieve competitive results with state-of-the-art in this regime. We also note that current best results are from generative models such as Springenberg (2016) and Salimans et al. (2016) that follow an elaborate and computationally heavy training procedure compared with our approach."}, {"heading": "7 CONCLUSIONS", "text": "In this work we have shown how neural networks can be used to learn in a semi-supervised setting using small sets of labeled data, by replacing the classification objective with a metric embedding one. We introduced an objective for semi-supervised learning formulated as minimization of entropy over a distance encoding distribution. This objective is compliant with standard techniques of training deep neural network and requires no modification of the embedding model. Using the method in this work, we were able to achieve state-of-the-art results on MNIST with only 100 labeled examples and competitive results on Cifar10 dataset. We speculate that this form of learning is beneficial to neural network models by decreasing their tendency to overfit over small sets of training data. The objectives formulated here can potentially leverage prior knowledge on the distribution of classes or samples, as well as incorporating this knowledge in the training process. For example, utilizing the learned embedded distance, we speculate that a better sampling can be done instead of a uniform one over the entire set.\nFurther exploration is needed to apply this method to large scale problems, spanning a large number of available classes, which we leave to future work."}, {"heading": "8 APPENDIX", "text": ""}], "references": [{"title": "Signature verification using a siamese time delay neural network", "author": ["Jane Bromley", "James W Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews", "author": ["Olivier Chapelle", "Bernhard Scholkopf", "Alexander Zien"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Chapelle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2009}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Sumit Chopra", "Raia Hadsell", "Yann LeCun"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["Alexey Dosovitskiy", "Jost Tobias Springenberg", "Martin Riedmiller", "Thomas Brox"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2014}, {"title": "Ensemble robustness of deep learning algorithms", "author": ["Jiashi Feng", "Tom Zahavy", "Bingyi Kang", "Huan Xu", "Shie Mannor"], "venue": "arXiv preprint arXiv:1602.02389,", "citeRegEx": "Feng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "Neighbourhood components analysis", "author": ["Jacob Goldberger", "Geoffrey E Hinton", "Sam T Roweis", "Ruslan Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goldberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2004}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Large-scale feature learning with spikeand-slab sparse coding", "author": ["Ian J. Goodfellow", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2012}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Yves Grandvalet", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Grandvalet and Bengio.,? \\Q2004\\E", "shortCiteRegEx": "Grandvalet and Bengio.", "year": 2004}, {"title": "Deep metric learning using triplet network", "author": ["Elad Hoffer", "Nir Ailon"], "venue": "In Similarity-Based Pattern Recognition,", "citeRegEx": "Hoffer and Ailon.,? \\Q2015\\E", "shortCiteRegEx": "Hoffer and Ailon.", "year": 2015}, {"title": "Direct modeling of complex invariances for visual object features", "author": ["Ka Y Hui"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Hui.,? \\Q2013\\E", "shortCiteRegEx": "Hui.", "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Computer Science Department,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Distributional smoothing by virtual adversarial", "author": ["Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Ken Nakae", "Shin Ishii"], "venue": "examples. stat,", "citeRegEx": "Miyato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2015}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "Nesterov.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Semisupervised learning with ladder networks", "author": ["Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Cnn features off-theshelf: an astounding baseline for recognition", "author": ["Ali Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Metric learning with adaptive density discrimination", "author": ["Oren Rippel", "Manohar Paluri", "Piotr Dollar", "Lubomir Bourdev"], "venue": "stat, 1050:18,", "citeRegEx": "Rippel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rippel et al\\.", "year": 2015}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "arXiv preprint arXiv:1606.03498,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Schroff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schroff et al\\.", "year": 2015}, {"title": "Unsupervised and semi-supervised learning with categorical generative adversarial networks", "author": ["Jost Tobias Springenberg"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "Springenberg.,? \\Q2016\\E", "shortCiteRegEx": "Springenberg.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "arXiv preprint arXiv:1512.00567,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study", "author": ["Isaac Triguero", "Salvador Garc\u0131\u0301a", "Francisco Herrera"], "venue": "Knowledge and Information Systems,", "citeRegEx": "Triguero et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Triguero et al\\.", "year": 2015}, {"title": "Statistical learning theory, volume 1", "author": ["Vladimir Naumovich Vapnik", "Vlamimir Vapnik"], "venue": null, "citeRegEx": "Vapnik and Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik and Vapnik.", "year": 1998}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Deep learning via semisupervised embedding", "author": ["Jason Weston", "Fr\u00e9d\u00e9ric Ratle", "Hossein Mobahi", "Ronan Collobert"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Weston et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2012}, {"title": "Stacked what-where auto-encoders", "author": ["Junbo Zhao", "Michael Mathieu", "Ross Goroshin", "Yann Lecun"], "venue": "arXiv preprint arXiv:1506.02351,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "These representative features, although not explicitly part of the training objective, were shown to be useful in subsequent tasks in the same domain as demonstrated by Razavian et al. (2014). One serious problem occurring in neural network is their susceptibility to overfit over the training data.", "startOffset": 169, "endOffset": 192}, {"referenceID": 20, "context": "These representative features, although not explicitly part of the training objective, were shown to be useful in subsequent tasks in the same domain as demonstrated by Razavian et al. (2014). One serious problem occurring in neural network is their susceptibility to overfit over the training data. Due to this fact, a considerable part of modern neural network research is devoted to regularization techniques and heuristics such as Srivastava et al. (2014); Ioffe & Szegedy (2015); Wan et al.", "startOffset": 169, "endOffset": 460}, {"referenceID": 20, "context": "These representative features, although not explicitly part of the training objective, were shown to be useful in subsequent tasks in the same domain as demonstrated by Razavian et al. (2014). One serious problem occurring in neural network is their susceptibility to overfit over the training data. Due to this fact, a considerable part of modern neural network research is devoted to regularization techniques and heuristics such as Srivastava et al. (2014); Ioffe & Szegedy (2015); Wan et al.", "startOffset": 169, "endOffset": 484}, {"referenceID": 20, "context": "These representative features, although not explicitly part of the training objective, were shown to be useful in subsequent tasks in the same domain as demonstrated by Razavian et al. (2014). One serious problem occurring in neural network is their susceptibility to overfit over the training data. Due to this fact, a considerable part of modern neural network research is devoted to regularization techniques and heuristics such as Srivastava et al. (2014); Ioffe & Szegedy (2015); Wan et al. (2013); Szegedy et al.", "startOffset": 169, "endOffset": 503}, {"referenceID": 20, "context": "These representative features, although not explicitly part of the training objective, were shown to be useful in subsequent tasks in the same domain as demonstrated by Razavian et al. (2014). One serious problem occurring in neural network is their susceptibility to overfit over the training data. Due to this fact, a considerable part of modern neural network research is devoted to regularization techniques and heuristics such as Srivastava et al. (2014); Ioffe & Szegedy (2015); Wan et al. (2013); Szegedy et al. (2015), to allow the networks to generalize to unseen data samples.", "startOffset": 169, "endOffset": 526}, {"referenceID": 0, "context": "One kind of such metric embedding is the \u201cSiamese network\u201d framework introduced by Bromley et al. (1993) and later used in the works of Chopra et al.", "startOffset": 83, "endOffset": 105}, {"referenceID": 0, "context": "One kind of such metric embedding is the \u201cSiamese network\u201d framework introduced by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face.", "startOffset": 83, "endOffset": 157}, {"referenceID": 0, "context": "One kind of such metric embedding is the \u201cSiamese network\u201d framework introduced by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face. This problem was recently tackled by Schroff et al. (2015) for training a convolutional network model on triplets of examples.", "startOffset": 83, "endOffset": 437}, {"referenceID": 0, "context": "One kind of such metric embedding is the \u201cSiamese network\u201d framework introduced by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face. This problem was recently tackled by Schroff et al. (2015) for training a convolutional network model on triplets of examples. Learning features by metric embedding was also shown by Hoffer & Ailon (2015) to provide competitive classification accuracy compare to conventional cross-entropy regression.", "startOffset": 83, "endOffset": 583}, {"referenceID": 0, "context": "One kind of such metric embedding is the \u201cSiamese network\u201d framework introduced by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face. This problem was recently tackled by Schroff et al. (2015) for training a convolutional network model on triplets of examples. Learning features by metric embedding was also shown by Hoffer & Ailon (2015) to provide competitive classification accuracy compare to conventional cross-entropy regression. This work is also related to Rippel et al. (2015), who introduced Magnet loss - a metric embedding approach for fine-grained classification.", "startOffset": 83, "endOffset": 730}, {"referenceID": 0, "context": "One kind of such metric embedding is the \u201cSiamese network\u201d framework introduced by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face. This problem was recently tackled by Schroff et al. (2015) for training a convolutional network model on triplets of examples. Learning features by metric embedding was also shown by Hoffer & Ailon (2015) to provide competitive classification accuracy compare to conventional cross-entropy regression. This work is also related to Rippel et al. (2015), who introduced Magnet loss - a metric embedding approach for fine-grained classification. The Magnet loss is based on learning the distribution of distances for each sample, from K clusters assigned for each classified class. It then uses an intermediate k-means clustering, to reposition the different assigned clusters. This proved to allow better accuracy than both margin-based Triplet loss, and softmax regression. Using metric embedding with neural network was also specifically shown to provide good results in the semi-supervised learning setting as seen in Weston et al. (2012).", "startOffset": 83, "endOffset": 1318}, {"referenceID": 20, "context": "More recently, neural network specific regularizations that induce noise within the training process such as Srivastava et al. (2014); Wan et al.", "startOffset": 109, "endOffset": 134}, {"referenceID": 20, "context": "More recently, neural network specific regularizations that induce noise within the training process such as Srivastava et al. (2014); Wan et al. (2013); Szegedy et al.", "startOffset": 109, "endOffset": 153}, {"referenceID": 20, "context": "More recently, neural network specific regularizations that induce noise within the training process such as Srivastava et al. (2014); Wan et al. (2013); Szegedy et al. (2015) proved to be highly beneficial to avoid overfitting.", "startOffset": 109, "endOffset": 176}, {"referenceID": 6, "context": "Another recent observation by Goodfellow et al. (2015) is that training on adversarial examples, inputs that were found to be misclassified under small perturbation, can improve generalization.", "startOffset": 30, "endOffset": 55}, {"referenceID": 5, "context": "This fact was explored by Feng et al. (2016) and found to provide notable improvements to the semi supervised regime by Miyato et al.", "startOffset": 26, "endOffset": 45}, {"referenceID": 5, "context": "This fact was explored by Feng et al. (2016) and found to provide notable improvements to the semi supervised regime by Miyato et al. (2015).", "startOffset": 26, "endOffset": 141}, {"referenceID": 24, "context": "A stacked what-where autoencoder by Zhao et al. (2015) computes a set of complementary variables that enable reconstruction whenever a layer implements a many-to-one mapping.", "startOffset": 36, "endOffset": 55}, {"referenceID": 15, "context": "Ladder networks by Rasmus et al. (2015) - use lateral connections to allow higher levels of an auto-encoder to focus on invariant abstract features by applying a layer-wise cost function.", "startOffset": 19, "endOffset": 40}, {"referenceID": 7, "context": "Generative adversarial network (GAN) is a recently introduced model that can be used in an unsupervised fashion Goodfellow et al. (2014). Adversarial Generative Models use a set of networks, one trained to discriminate between data sampled from the true underlying distribution (e.", "startOffset": 112, "endOffset": 137}, {"referenceID": 7, "context": "Generative adversarial network (GAN) is a recently introduced model that can be used in an unsupervised fashion Goodfellow et al. (2014). Adversarial Generative Models use a set of networks, one trained to discriminate between data sampled from the true underlying distribution (e.g., a set of images), and a separate generative network trained to be an adversary trying to confuse the first network. By propagating the gradient through the paired networks, the model learns to generate samples that are distributed similarly to the source data. As shown by Radford et al. (2015), this model can create useful latent representations for subsequent classification tasks.", "startOffset": 112, "endOffset": 580}, {"referenceID": 7, "context": "Generative adversarial network (GAN) is a recently introduced model that can be used in an unsupervised fashion Goodfellow et al. (2014). Adversarial Generative Models use a set of networks, one trained to discriminate between data sampled from the true underlying distribution (e.g., a set of images), and a separate generative network trained to be an adversary trying to confuse the first network. By propagating the gradient through the paired networks, the model learns to generate samples that are distributed similarly to the source data. As shown by Radford et al. (2015), this model can create useful latent representations for subsequent classification tasks. The usage for these models for semi-supervised learning was further developed by Springenberg (2016) and Salimans et al.", "startOffset": 112, "endOffset": 771}, {"referenceID": 7, "context": "Generative adversarial network (GAN) is a recently introduced model that can be used in an unsupervised fashion Goodfellow et al. (2014). Adversarial Generative Models use a set of networks, one trained to discriminate between data sampled from the true underlying distribution (e.g., a set of images), and a separate generative network trained to be an adversary trying to confuse the first network. By propagating the gradient through the paired networks, the model learns to generate samples that are distributed similarly to the source data. As shown by Radford et al. (2015), this model can create useful latent representations for subsequent classification tasks. The usage for these models for semi-supervised learning was further developed by Springenberg (2016) and Salimans et al. (2016), by adding a N + 1 way classifier (number of classes + and additional \u201cfake\u201d class) to the discriminator.", "startOffset": 112, "endOffset": 798}, {"referenceID": 27, "context": "described by Triguero et al. (2015), in which unlabeled examples are re-introduced after being labeled with the previous classification of the model.", "startOffset": 13, "endOffset": 36}, {"referenceID": 27, "context": "described by Triguero et al. (2015), in which unlabeled examples are re-introduced after being labeled with the previous classification of the model. This is also related to the \u201cTransductive suport vector machines\u201d (TSVM) Vapnik & Vapnik (1998) which introduces a maximum margin objective over both labeled and unlabeled examples.", "startOffset": 13, "endOffset": 246}, {"referenceID": 1, "context": "This assumption is shared with many other semisupervised approaches as discussed in Chapelle et al. (2009),Weston et al.", "startOffset": 84, "endOffset": 107}, {"referenceID": 1, "context": "This assumption is shared with many other semisupervised approaches as discussed in Chapelle et al. (2009),Weston et al. (2012).", "startOffset": 84, "endOffset": 128}, {"referenceID": 6, "context": "It is similar to the stochastic-nearestneighbors formulation of Goldberger et al. (2004), and will allow us to state the two underlying objectives as measures over this distribution.", "startOffset": 64, "endOffset": 89}, {"referenceID": 26, "context": "This was lately discussed and addressed by Szegedy et al. (2015) using added random noise to the targets, effectively smoothing the cross-entropy target distribution.", "startOffset": 43, "endOffset": 65}, {"referenceID": 3, "context": "All experiments were conducted using the Torch7 framework by Collobert et al. (2011). Code reproducing these results will by available at https://github.", "startOffset": 61, "endOffset": 85}, {"referenceID": 3, "context": "All experiments were conducted using the Torch7 framework by Collobert et al. (2011). Code reproducing these results will by available at https://github.com/eladhoffer/ SemiSupContrast. For every experiment we chose a small random subset of examples, with a balanced number from each class and denoted by XL. The remaining training images are used without their labeled to form XU . Finally, we test our final accuracy with a disjoint set of examples Xtest. No data augmentation was applied to the training sets. In each iteration we sampled uniformly a set of labeled examples z1, ...z|C| \u2208 XL. In addition, batches of uniformly sampled examples were also sampled again from the labeled set XL, and the unlabeled set XU . A batch-size of b = 32 was used for all experiments, totaling a sampled set of 2 \u00b7 b + |C| examples for each iteration, where |C| = 10 for both datasets. We used 6 as optimization criterion, where \u03bb1 = \u03bb2 = 1. Optimization was done using the Accelerated-gradient method by Nesterov (1983) with an initial learning rate of lr0 = 0.", "startOffset": 61, "endOffset": 1012}, {"referenceID": 28, "context": "Model Test error % EmbedCNN Weston et al. (2012) 7.", "startOffset": 28, "endOffset": 49}, {"referenceID": 28, "context": "Model Test error % EmbedCNN Weston et al. (2012) 7.75 SWWAE Zhao et al. (2015) 9.", "startOffset": 28, "endOffset": 79}, {"referenceID": 19, "context": "17 Ladder network Rasmus et al. (2015) 0.", "startOffset": 18, "endOffset": 39}, {"referenceID": 19, "context": "17 Ladder network Rasmus et al. (2015) 0.89 (\u00b1 0.50) Conv-CatGAN Springenberg (2016) 1.", "startOffset": 18, "endOffset": 85}, {"referenceID": 30, "context": "We followed previous works ((Weston et al., 2012),(Zhao et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 31, "context": ", 2012),(Zhao et al., 2015),Rasmus et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 15, "context": "The MNIST database of handwritten digits introduced by LeCun et al. (1998) is one of the most studied dataset benchmark for image classification.", "startOffset": 55, "endOffset": 75}, {"referenceID": 15, "context": "The MNIST database of handwritten digits introduced by LeCun et al. (1998) is one of the most studied dataset benchmark for image classification. The dataset contains 60,000 examples of handwritten digits from 0 to 9 for training and 10,000 additional examples for testing, where each sample is a 28 x 28 pixel gray level image. We followed previous works ((Weston et al., 2012),(Zhao et al., 2015),Rasmus et al. (2015)) and used semi-supervised regime in which only 100 samples (10 for each class) were used as XL along with their labels.", "startOffset": 55, "endOffset": 420}, {"referenceID": 15, "context": "The MNIST database of handwritten digits introduced by LeCun et al. (1998) is one of the most studied dataset benchmark for image classification. The dataset contains 60,000 examples of handwritten digits from 0 to 9 for training and 10,000 additional examples for testing, where each sample is a 28 x 28 pixel gray level image. We followed previous works ((Weston et al., 2012),(Zhao et al., 2015),Rasmus et al. (2015)) and used semi-supervised regime in which only 100 samples (10 for each class) were used as XL along with their labels. For the embedding network, we used a convolutional network with 5-convolutional layers, where each layer is followed by a ReLU non-linearity and batch-normalization layer Ioffe & Szegedy (2015). The full network structure is described in Appendix table 3.", "startOffset": 55, "endOffset": 734}, {"referenceID": 6, "context": "Model Test error % Spike-and-Slab Sparse Coding Goodfellow et al. (2012) 31.", "startOffset": 48, "endOffset": 73}, {"referenceID": 6, "context": "Model Test error % Spike-and-Slab Sparse Coding Goodfellow et al. (2012) 31.9 View-Invariant k-means Hui (2013) 27.", "startOffset": 48, "endOffset": 112}, {"referenceID": 4, "context": "7) Exemplar-CNN Dosovitskiy et al. (2014) 23.", "startOffset": 16, "endOffset": 42}, {"referenceID": 4, "context": "7) Exemplar-CNN Dosovitskiy et al. (2014) 23.4 (\u00b1 0.2) Ladder network Rasmus et al. (2015) 20.", "startOffset": 16, "endOffset": 91}, {"referenceID": 4, "context": "7) Exemplar-CNN Dosovitskiy et al. (2014) 23.4 (\u00b1 0.2) Ladder network Rasmus et al. (2015) 20.04 (\u00b1 0.47) Conv-CatGan Springenberg (2016) 19.", "startOffset": 16, "endOffset": 138}, {"referenceID": 4, "context": "7) Exemplar-CNN Dosovitskiy et al. (2014) 23.4 (\u00b1 0.2) Ladder network Rasmus et al. (2015) 20.04 (\u00b1 0.47) Conv-CatGan Springenberg (2016) 19.58 (\u00b1 0.58) ImprovedGan Salimans et al. (2016) 18.", "startOffset": 16, "endOffset": 188}, {"referenceID": 23, "context": "We also note that current best results are from generative models such as Springenberg (2016) and Salimans et al.", "startOffset": 74, "endOffset": 94}, {"referenceID": 22, "context": "We also note that current best results are from generative models such as Springenberg (2016) and Salimans et al. (2016) that follow an elaborate and computationally heavy training procedure compared with our approach.", "startOffset": 98, "endOffset": 121}], "year": 2016, "abstractText": "Deep networks are successfully used as classification models yielding state-ofthe-art results when trained on a large number of labeled samples. These models, however, are usually much less suited for semi-supervised problems because of their tendency to overfit easily when trained on small amounts of data. In this work we will explore a new training objective that is targeting a semi-supervised regime with only a small subset of labeled data. This criterion is based on a deep metric embedding over distance relations within the set of labeled samples, together with constraints over the embeddings of the unlabeled set. The final learned representations are discriminative in euclidean space, and hence can be used with subsequent nearest-neighbor classification using the labeled samples.", "creator": "LaTeX with hyperref package"}}}