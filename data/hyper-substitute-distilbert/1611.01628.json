{"id": "1611.01628", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2016", "title": "Reference-Aware Language Models", "abstract": "we propose a comparative curriculum of simulation models that treat competence as an explicit scientific communication variable. modeling architecture allows experts to create definitions of items and their structures by accessing interactive databases ( required by, e. d., dialogue generation incorporating recipe generation ) and internal memories ( required by, se. g. language structures within model aware data description ). this involves the incorporation of information that can walk structured in predictable patterns in databases or reference context, even when conceptual targets of actual behavior may be inconsistent however. experiments reviewing multimedia perspectives shows our domain variants effects on deterministic attention.", "histories": [["v1", "Sat, 5 Nov 2016 10:55:37 GMT  (314kb,D)", "http://arxiv.org/abs/1611.01628v1", "iclr submission"], ["v2", "Fri, 11 Nov 2016 22:51:28 GMT  (313kb,D)", "http://arxiv.org/abs/1611.01628v2", "iclr submission"], ["v3", "Tue, 7 Feb 2017 20:33:12 GMT  (311kb,D)", "http://arxiv.org/abs/1611.01628v3", "9 pages"], ["v4", "Tue, 8 Aug 2017 17:05:33 GMT  (967kb,D)", "http://arxiv.org/abs/1611.01628v4", "emnlp camera ready"], ["v5", "Wed, 9 Aug 2017 00:39:51 GMT  (967kb,D)", "http://arxiv.org/abs/1611.01628v5", "emnlp camera ready"]], "COMMENTS": "iclr submission", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zichao yang", "phil blunsom", "chris dyer", "wang ling"], "accepted": true, "id": "1611.01628"}, "pdf": {"name": "1611.01628.pdf", "metadata": {"source": "CRF", "title": "REFERENCE-AWARE LANGUAGE MODELS", "authors": ["Zichao Yang", "Phil Blunsom", "Chris Dyer", "Wang Ling"], "emails": ["zichaoy@cs.cmu.edu,", "pblunsom@google.com", "cdyer@google.com", "lingwang@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Referring expressions (REs) in natural language are noun phrases (proper nouns, common nouns, and pronouns) that identify objects, entities, and events in an environment. REs occur frequently and they play a key role in communicating information efficiently. While REs are common, previous works neglect to model REs explicitly, either treating REs as ordinary words in the model or replacing them with special tokens. Here we propose a language modeling framework that explicitly incorporates reference decisions.\nIn Figure 1 we list examples of REs in the context of the three tasks that we consider in this work. Firstly, reference to a database is crucial in many applications. One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016). Here we consider the domain of restaurant recommendation where a system refers to restaurants (name) and their attributes (address, phone number etc) in its responses. When the system says \u201cthe nirala is a nice restaurant\u201d, it refers to the restaurant name the nirala from the database. Secondly, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015). In the task of recipe generation from a list of ingredients (Kiddon et al., 2016), the generation of the recipe will frequently reference these items. As shown in Figure 1, in the recipe \u201cBlend soy milk and . . . \u201d, soy milk refers to the ingredient summaries. Finally, we address references within a document (Mikolov et al., 2010; Ji et al., 2015; Wang & Cho, 2015), as the generation of words will ofter refer to previously generated words. For instance the same entity will often be referred to throughout a document. In Figure 1, the entity you refers to I in a previous utterance.\nIn this work we develop a language model that has a specific module for generating REs. A series of latent decisions (should I generate an RE? If yes, which entity in the context should I refer to? How should the RE be rendered?) augment a traditional recurrent neural network language model and the two components are combined as a mixture model. Selecting an entity in context is similar to familiar models of attention (Bahdanau et al., 2014), but rather than being a deterministic function that reweights representations of elements in the context, it is treated as a distribution over contextual elements which are stochastically selected and then copied or, if the task warrants it, transformed (e.g., a pronoun rather than a proper name is produced as output). Two variants are possible for updating the RNN state: one that only looks at the generated output form; and a second that looks at values of the latent variables. The former admits trivial unsupervised learning, latent decisions are conditionally independent of each other given observed context, whereas the latter enables more\n\u2217Work completed at DeepMind.\nar X\niv :1\n61 1.\n01 62\n8v 1\n[ cs\n.C L\n] 5\nN ov\n2 01\n6\nexpressive models that can extract information from the entity that is being referred to. In each of the three tasks, we demonstrate our reference aware model\u2019s efficacy in evaluations against models that do not explicitly include a reference operation."}, {"heading": "2 REFERENCE-AWARE LANGUAGE MODELS", "text": "Here we propose a general framework for reference-aware language models.\nWe denote each document as a series of tokens x1, . . . , xL, where L is the number of tokens in the document. Our goal is to maximize the probabilities p(xi | ci), for each word in the document based on its previous context ci = x1, . . . , xi\u22121. In contrast to traditional neural language models, we introduce a variable at each position zi, which controls the decision on which source xi is generated from. The token conditional probably is then obtained by marginalizing this variable:\np(xi | ci) = p(xi | zi, ci)p(zi | ci). (1)\nIn dialogue modeling and recipe generation, zi will simply taken on values in {0, 1}. Where zi = 1 denotes that xi is generated as a reference, either to a database entry or an item in a list. However, zi can also be defined as a distribution over previous entities, allowing the model to predict xi conditioned on its a previous mention word. This will be the focus of the coreference language model. When zi is not observed (which it generally will not be), we will train our model to maximize the marginal probability in Eq. 1 directly."}, {"heading": "2.1 DIALOGUE MODEL WITH DATABASE SUPPORT", "text": "We first apply our model on task-oriented dialogue systems in the domain of restaurant recommendations, and work on the data set from the second Dialogue State Tracking Challenge (DSTC2) (Henderson et al., 2014). Table. 1 is one example dialogue from this dataset.\nWe can observe from this example, users get recommendations of restaurants based on queries that specify the area, price and food type of the restaurant. We can support the system\u2019s decisions\nby incorporating a mechanism that allows the model to query the database allowing the model to find restaurants that satisfy the users queries. Thus, we crawled TripAdvisor for restaurants in the Cambridge area, where the dialog dataset was collected. Then, we remove restaurants that do not appear in the data set and create a database with 109 entries with restaurants and their attributes (e.g. food type). A sample of our database is shown in Table. 2. We can observe that each restaurant contains 6 attributes that are generally referred in the dialogue dataset. As such, if the user requests a restaurant that serves \u201cturkish\u201d food, we wish to train a model that can search for entries whose \u201cfood\u201d column contains \u201cturkish\u201d. Now, we describe how we deploy a model that fulfills these requirements."}, {"heading": "2.1.1 DIALOGUE MODEL", "text": "Consider a dialogue with T turns, and the utterance from a user is denoted as X = {xi}Ti=1,\nwhere i is the i-th utterance, whereas the utterance from a machine is denoted as Y = {yi}Ti=1, where i is the i-th utterance. We define xi = {xij}|xi|j=1, yi = {yiv} |yi| v=1, where xij denotes the j-th token in the i-th utterance from the user, whereas yiv denotes the v-th token in the i-th utterance from the machine. Finally, |xi| and |yi| denote the number of tokens in the user and machine utterances, respectively. The dialogue sequence starts with machine utterance {y1, x1, y2, x2, . . . , yT , xT }. We would like to model the utterances from the machine\np(y1, y2, . . . , yT |x1, x2, . . . , xT ) = \u220f i p(yi|y<i, x<i) = \u220f i,v p(yi,v|yi,<v, y<i, x<i).\nwhere y<i denotes all the utterances before i and yi,<v denotes the first v \u2212 1 tokens in the i-th utterance of the machine. A neural model is employed to predict p(yi,v|yi,<v, y<i, x<i), which operates as follows:\nSentence Encoder: We first encode previous utterances y<i and x<i into continuous space by generating employing a LSTM encoder. Thus, for a given utterance xi, and start with the initial LSTM state hxi,0 and apply the recursion h x i,j = LSTME(WExi,j , h x i,j\u22121), where WExi,j denotes a word embedding lookup for the token xi,j , and LSTME denotes the LSTM transition function described in Hochreiter & Schmidhuber (1997). The representation of the user utterance is represented by the final LSTM state hxi = h x i,|xi|. The same process is applied to obtain the machine utterance representation hyi = h y i,|yi|.\nTurn Encoder: Then, combine all the representations of all the utterances with a second LSTM, which encodes the sequence {hy1, hx1 , ..., h y i , h x i } into a continuous vector. Once again, we start with an initial state u0 and feed each of the utterance representation to obtain the following LSTM state, until the final state is obtained. For simplicity, we shall refer to this as ui, which can be seen as the hierarchical encoding of the previous i utterances.\nSeq2Seq Decoder: As for decoding, in order to generate each utterance yi, we can feed ui\u22121 into the a decoder LSTM as the initial state si,0 = ui\u22121 and decode each token in yi. Thus, we can express the decoder as:\nsyi,v = LSTMD(si,v\u22121,WEyi,v\u22121), p y i,v = softmax(Ws y i,v),\nwhere the desired probability p(yi,v|yi,<v, y<i, x<i) is expressed by pyi,v .\nAttention based decoder: We can also encorporate the attention mechanism in our hierarhical model. An attention model builds a representation d by averaging over a set of vectors p. We define the attention function as a = ATTN(p, q), where a is a probability distribution over the set of vectors\np, conditioned on any input representation q. A full description of this operation is described in (Bahdanau et al., 2014). Thus, for each generated token yi,v , we compute the attentions ai,v , conditioned on the current decoder state syi,v , obtaining the attentions over input tokens from previous turn (i\u22121). We denote the vector of all tokens in previous turn as hx,yi\u22121 = [{hxi\u22121,j} |xi\u22121| j=1 , {h y i\u22121,v} |yi\u22121| v=1 ]. Let K = |hx,yi\u22121| be the number of tokens in previous turn. Thus, we obtain the attention probabilities over all previous tokens ai,v as ATTN(s y i,v, h x,y i\u22121). Then, the weighted sum is performed over these\nprobabilities di,v = \u2211 k\u2208K ai,v,kh x,y i\u22121,k, where ai,v,k is the probability of aligning to the k-th token from previous turn. The resulting vector di,v is used to obtain the probability of the following word pyi,v . Thus, we express the decoder as:\nsyi,v = LSTMD(si,v\u22121, di,v\u22121,WEyi,v\u22121), ai,v = ATTN(s y i,v, h x,y i\u22121), di,v = \u2211 k\u2208K ai,v,kh x,y i\u22121,k, p y i,v = softmax(W [s y i,v, di,v])."}, {"heading": "2.2 INCORPORATING TABLE ATTENTION", "text": "We now extend the attention model in order to allow the attention to be computed over a table, allowing the model to condition the generation on a database.\nWe denote a table with R rows and C columns as {fr,c}, r \u2208 [1, R], c \u2208 [1, C], where fr,c is the cell in row r and column c. The attribute of each column is denoted as sc, where c is the c-th attribute.\nTable Encoding: To encode the table, we build a attribute vector gc for each column. For each cell fr,c of the table, we concatenate it with the corresponding attribute gc and then feed it through a one-layer MLP as follows: gc = Wesc and then er,c = tanh(W [Wefr,c, gc]).\nTable Attention: The diagram for table attention is shown in Figure 3a. The attention over cells in the table is conditioned on a given vector q, similarly to the attention model for sequences ATTN(p, q). However, rather than a sequence p, we now operate over a table f . Our attention model computes a attribute attention followed by row attention of the table. We first use the attention mechanism on the attributes to find out which attribute the user asks about. Suppose a user says cheap, then we should focus on the price attribute. After we get the attention probability pa = ATTN({gc}, q), over the attribute, we calculate the weighted representation for each row er = \u2211 c p a cerc conditioned on p\na. Then er has the price information of each row. We further use attention mechanism on er and get the probability pr = ATTN({er}, q) over the rows. Then restaurants with cheap price will be picked. Then, using the probabilities pr, we compute the weighted average over the all rows ec = \u2211 r p r rer,c, which is used in the decoder. The detailed process is:\npa = ATTN({gc}, q), er = \u2211 c pacerc \u2200r, (2)\npr = ATTN({er}, q), ec = \u2211 r prrer,c \u2200c. (3)\nThis is embedded in the decoder by replacing the conditioned state q as the current decoder state syi,0 and then at each step, conditioning the prediction of yi,v on {ec} by using attention mechanism at each step. The detailed diagram of table attention is shown in Figure 3a."}, {"heading": "2.2.1 INCORPORATING TABLE POINTER NETWORKS", "text": "We now describe the mechanism used to refer to specific database entries during decoding. At each timestamp, the model needs to decide whether to generate the next token from an entry of the database or from the word softmax. This is performed as follows.\nPointer Switch: We use zi,v \u2208 [0, 1] to denote the decision of whether to copy one cell from the table. We compute this probability as follows:\np(zi,v|si,v) = sigmoid(W [si,v, di,v]). Thus, if p(zi,v|si,v) = 1, if follows that the next token yi,v will be generated from the database, whereas if p(zi,v|si,v) = 1, then the following token is generated from a softmax. We shall now describe how we generate tokens from the database.\nTable Pointer: If zi,v = 1, the token is generated from the table. The detailed process of calculating the probability distribution over the table is shown in Figure 3b. This is similar to the attention mechanism, except that we perform a column attention to compute the probabilities of copying from each column after Equation. 3. More formally:\npc = ATTN({ec}, q), pcopy = pr \u2297 pc, (4) where pc is a probability distribution over columns, whereas pr is a probability distribution over rows. In order to compute a matrix with the probability of copying each cell, we simply compute the cross product pcopy = pr \u2297 pc. Objective: As we treat zi as a latent variable, we wish to maximize the marginal probability of the sequence yi over all possible values of zi. Thus, our objective function is defined as:\np(yi,v|si,v) = pvocabp(0|si,v) + pcopyp(1|si,v) = pvocab(1\u2212 p(1|si,v)) + pcopyp(1|si,v). (5) The model can also be trained in a fully supervised fashion, if zi is observed. In such cases, we simply maximize the likelihood of p(zi|si,v), based on the observations, rather than using the marginal probability over zi.\n2.3 RECIPE GENERATION\nNext, we consider the task of recipe generation conditioning on the ingredient lists. In this task, we must generate the recipe from a list of ingredients. Table. 3 illustrates the ingredient list and recipe for Spinach and Banana Power Smoothie. We can see that the ingredients soy milk, spinach leaves, and banana occur in the recipe.\nattention based decoder:\nsv = LSTMD(sv\u22121, dv\u22121,WEyv\u22121), pcopyv = ATTN({{hi,j}Ti=1}Lj=1, sv), dv = \u2211 ij pv,i,jhi,j , p(zv|sv) = sigmoid(W [sv, dv]),\npvocabv = softmax(W [s y i+1,v, dv]).\nSimilarly to the previous task, the decision to copy from the ingredient list or generate a new word from the softmax is performed using a switch, denoted as his p(zv|sv). In the attention mechanism, we can obtain a probability distribution of copying each of the words in the ingredients by computing pcopyv = ATTN({{hi,j}Ti=1}Lj=1, sv). For training, we optimize the marginal likelihood function employed in the previous task."}, {"heading": "2.4 COREFERENCE BASED LANGUAGE MODEL", "text": "Finally, we build a language model that uses coreference links to point into previous words. Before generating a word, we first make the decision on whether it is an entity mention. If so, we decide which entity this mention belongs to, then we generate the word based on that entity. Denote the document as X = {xi}Li=1, and the entities are E = {ei}Ni=1, each entity has Mi mentions, ei = {mij}Mij=1, such that {xmij} Mi j=1 refer to the same entity. We use a LSTM to model the document, the hidden state of each token is hi = LSTM(Wexi, hi\u22121). We use a set he = {he0, he1, ..., heM} to keep track of the entity states, where hej is the state of entity j.\num and [I]1 think that is whats - Go ahead [Linda]2. Well and thanks goes to [you]1 and to [the media]3 to help [us]4...So [our]4 hat is off to all of [you]5...\nWord generation: At each time step before generating the next word, we predict whether the word is an entity mention:\npcoref(vi|hi\u22121, he) = ATTN(he, hi\u22121), di = \u2211 vi p(vi)h e vi p(zi|hi\u22121) = sigmoid(W [di, hi\u22121]),\nwhere zi denotes whether the next word is an entity and if yes vi denotes which entity the next word corefers to. If the next word is an entity mention, then p(xi|vi, hi\u22121, he) = softmax(W1 tanh(W2[hevi , hi\u22121])) else p(xi|hi\u22121) = softmax(W1hi\u22121),\np(xi|x<i) = { p(xi|hi\u22121)p(zi|hi\u22121, he) if zi = 0. p(xi|vi, hi\u22121, he)pcoref(vi|hi\u22121, he)p(zi|hi\u22121, he) if zi = 1.\n(6)\nEntity state update: We update the entity state he at each time step. In the beginning, he = {he0}, he0 denotes the state of an virtual empty entity and is a learnable variable. If zi = 1 and vi = 0, then it indicates the next word is a new entity mention, then in the next step, we append hi to he, i.e., he = {he, hi}, if ei > 0, then we update the corresponding entity state with the new hidden state, he[vi] = hi. Another way to update the entity state is to use one LSTM to encode and get the new entity state. Here we use the latest entity mention state as the new entity state. The detailed update process is shown in Figure 5."}, {"heading": "3 EXPERIMENTS", "text": ""}, {"heading": "3.1 DATA SETS", "text": "For the dialogue modeling task, we use the DSTC2 dataset. For recipe generation, we used a crawl of all recipes from www.allrecipes.com, and for the Coref LM, we use the Xinhua News portion of the Gigaword v5 corpus. Details are included in Appendix A."}, {"heading": "3.2 MODEL TRAINING AND EVALUATION", "text": "We train all models with simple stochastic gradient descent with clipping. We use a one-layer LSTM for all RNN components. Hyper-parameters are selected using grid search based on the validation set. We use dropout after the input embedding and LSTM output. The learning rate is selected from [0.1, 0.2, 0.5, 1], maximum gradient norm is selected from [1, 2, 5, 10] and drop ratio is selected from [0.2, 0.3, 0.5]. The batch size and LSTM dimension size is slightly different for different tasks so as to make the model fit into memory. The number of epochs to train are different for each task and we drop the learning rate after reaching a given number of epochs. We report the per-word perplexity for all tasks, specifically, we report the perplexity of all words, words that can be generated from reference and non-reference words. For recipe generation, we also generate the recipe using beam size of 10 and evaluate the generated recipe with BLEU."}, {"heading": "3.3 RESULTS AND ANALYSIS", "text": "The results for dialogue, recipe generation and coref language model are shown in Table 4, 5 and 6 respectively. We can see from Table 4 that models that condition on table performs better in predicting table tokens in general. Table pointer has the lowest perplexity for token in the table. Since the table token appears rarely in the dialogue, the overall perplexity does not differ much and the non-table tokens perplexity are similar. With attention mechanism over the table, the perplexity of table token improves over basic seq2seq model, but not as good as directly pointing to cells in the table. As expected, using sentence attention improves significantly over models without sentence attention. Surprising, table latent performs much worse than table pointer. We also measure the perplexity of table tokens that appear only in test set. For models other than table pointer, because the tokens never appear in training set, the perplexity is quite high, while table pointer can predict these tokens much more accurately. The recipe results in Table 5 in general follows that findings from the dialogue. But the latent model performs better than pointer model since that tokens in ingredients that match with recipe does not necessarily come from the ingredients. Imposing a supervised signal will give wrong information to the model and hence make the result worse. Hence with latent decision, the model learns to when to copy and when to generate it from the vocabulary. The coref LM results are shown in Table 6. We find that coref based LM performs much better on the entities perplexities, but however is a little bit worse than for non-entity words. We found it is an optimization problem and perhaps the model is stuck in local optimum. So we initialize the pointer model with the weights learned from LM, the pointer model performs better than LM both for entity perplexity and non-entity words perplexity."}, {"heading": "4 RELATED WORK", "text": "Recently, there has been great progresses in modeling languages based on neural network, including language modeling (Mikolov et al., 2010; Jozefowicz et al., 2016), machine translation (Sutskever\net al., 2014; Bahdanau et al., 2014), question answering (Hermann et al., 2015) etc. Based on the success of seq2seq models, neural networks are applied in modeling chit-chat dialogue (Li et al., 2016; Vinyals & Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Shang et al., 2015) and task oriented dialogue (Wen et al., 2015; Bordes & Weston, 2016; Williams & Zweig, 2016; Wen et al., 2016). Most of the chit-chat neural dialogue models are simply applying the seq2seq models. For the task oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly. Previous work on recipe generation from ingredients was proposed in (Kiddon et al., 2016). This model extents previous work on attention models (Allamanis et al., 2016) to checklists, where as our proposed model with explicit references to those checklists. Context dependent language models (Mikolov et al., 2010; Ji et al., 2015; Wang & Cho, 2015) are proposed to capture long term dependency of text. There are also lots of works on coreference resolution (Haghighi & Klein, 2010; Wiseman et al., 2016). We are the first to combine coreference with language modeling, to the best of our knowledge. Much effort has been invested in embedding a copying mechanism for neural models (Gu\u0308lc\u0327ehre et al., 2016; Gu et al., 2016; Ling et al., 2016). In general, a gating mechanism is employed to combine the softmax over observed words and a pointer network (Vinyals et al., 2015). These gates can be trained either by marginalizing over both outcomes, or using heuristics (e.g. copy low frequency words). Our models are similar to models proposed in (Ahn et al., 2016; Merity et al., 2016), where the generation of each word can be conditioned on a particular entry in a knowledge lists and previous words. In our work, we describe a model with broader applications, allowing us to condition, on databases, lists and dynamic lists."}, {"heading": "5 CONCLUSION", "text": "We introduce reference-aware language models which explicitly model the decision of from where to generate the token at each step. Our model can also learns the decision by treating it as a latent variable. We demonstrate on three tasks, table based dialogue modeling, recipe generation and coref based LM, that our model performs better than attention based model, which does not incorporate this decision explicitly. There are several directions to explore further based on our framework. The current evaluation method is based on perplexity and BLEU. In task oriented dialogues, we can also try human evaluation to see if the model can reply users\u2019 query accurately. It is also interesting to use reinforcement learning to learn the actions in each step."}, {"heading": "A DATASETS AND PREPROCESSING", "text": "Dialogue: We use the DSTC2 data set. We only extracted the dialogue transcript from data set. There are about 3,200 dialogues in total. Since this is a small data set, we use 5-fold cross validation and report the average result over the 5 partitions. There may be multiple tokens in each table cell, for example in Table.2, the name, address, post code and phone number have multiple tokens, we replace them with one special token. For the name, address, post code and phone number of the j-th row, we replace the tokens in each cell with NAME j, ADDR j, POSTCODE j, PHONE j. If a table cell is empty, we replace it with an empty token EMPTY. We do a string match in the transcript and replace the corresponding tokens in transcripts from the table with the special tokens. Each dialogue on average has 8 turns (16 sentences). We use a vocabulary size of 900, including about 400 table tokens and 500 words.\nRecipes: We crawl all recipes from www.allrecipes.com. There are about 31, 000 recipes in total, and every recipe has a ingredient list and a corresponding recipe. We exclude the recipes that have less than 10 tokens or more than 500 tokens, those recipes take about 0.1% of all data set. On average each recipe has 118 tokens and 9 ingredients. We random shuffle the whole data set and take 80% as training and 10% for validation and test. We use a vocabulary size of 10,000 in the model.\nCoref LM: We use the Xinhua News data set from Gigaword Fifth Edition and sample 100,000 documents from it that has length in range from 100 to 500. Each document has on average 234 tokens, so there are 23 million tokens in total. We use a tool to annotate all the entity mentions and use the annotation in the training. We take 80% as training and 10% as validation and test respectively. We ignore the entities that have only one mention and for the mentions that have multiple tokens, we take the token that is most frequent in the all the mentions for this entity. After the preprocessing, tokens that are entity mentions take about 10% of all tokens. We use a vocabulary size of 50,000 in the model.\nB VISUALIZATION\nM: Hello , welcome to the Cambridge restaurant system ? You can ask for restaurants by area , price range or food type . How may I help you ? U: and expensive restaurant that serves tuscan food M: Sorry there is no tuscan restaurant in the expensive price range U: french food M: cote is a great restaurant serving french food and it is in the expensive price range U: may i have the phone number M: The phone number of cote is 01223 311053 . U: thank you good bye\nM: Hello , welcome to the Cambridge restaurant system ? You can ask for restaurants by area , price range or food type . How may I help you ? U: cheap restaurant M: What kind of food would you like ? U: in the west part of town M: la margherita is a nice restaurant in the west of town in the cheap price range U: address M: Sure , la margherita is on 15 Magdalene Street City Centre U: thank you M: la margherita is a nice restaurant in the west of town in the cheap price range U: good bye"}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning end-to-end goal-oriented dialog", "author": ["Antoine Bordes", "Jason Weston"], "venue": "arXiv preprint arXiv:1605.07683,", "citeRegEx": "Bordes and Weston.,? \\Q2016\\E", "shortCiteRegEx": "Bordes and Weston.", "year": 2016}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "venue": "CoRR, abs/1603.06393,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Coreference resolution in a modular, entity-centered model", "author": ["Aria Haghighi", "Dan Klein"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Haghighi and Klein.,? \\Q2010\\E", "shortCiteRegEx": "Haghighi and Klein.", "year": 2010}, {"title": "Dialog state tracking challenge 2 ", "author": ["Matthew Henderson", "Blaise Thomson", "Jason Williams"], "venue": null, "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Document context language models", "author": ["Yangfeng Ji", "Trevor Cohn", "Lingpeng Kong", "Chris Dyer", "Jacob Eisenstein"], "venue": "arXiv preprint arXiv:1511.03962,", "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Globally coherent text generation with neural checklist models", "author": ["Chlo\u00e9 Kiddon", "Luke Zettlemoyer", "Yejin Choi"], "venue": "In Proc. EMNLP,", "citeRegEx": "Kiddon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kiddon et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Michel Galley", "Jianfeng Gao", "Dan Jurafsky"], "venue": "In Proc. EMNLP,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Latent predictor networks for code generation", "author": ["Wang Ling", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Andrew Senior", "Fumin Wang", "Phil Blunsom"], "venue": "In Proc. ACL,", "citeRegEx": "Ling et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "Pointer sentinel mixture models", "author": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "venue": "arXiv preprint arXiv:1609.07843,", "citeRegEx": "Merity et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Merity et al\\.", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "English gigaword fifth edition, linguistic data consortium", "author": ["Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda"], "venue": "Technical report,", "citeRegEx": "Parker et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parker et al\\.", "year": 2011}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "arXiv preprint arXiv:1503.02364,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Meg Mitchell", "JianYun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": "In Proc. NAACL,", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Proc. ICML Deep Learning Workshop,", "citeRegEx": "Vinyals and Le.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Larger-context language modelling", "author": ["Tian Wang", "Kyunghyun Cho"], "venue": "arXiv preprint arXiv:1511.03729,", "citeRegEx": "Wang and Cho.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Cho.", "year": 2015}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-hao Su", "David Vandyke", "Steve J. Young"], "venue": "In Proc. EMNLP,", "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": "arXiv preprint arXiv:1604.04562,", "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "End-to-end lstm-based dialog control optimized with supervised and reinforcement learning", "author": ["Jason D Williams", "Geoffrey Zweig"], "venue": "arXiv preprint arXiv:1606.01269,", "citeRegEx": "Williams and Zweig.,? \\Q2016\\E", "shortCiteRegEx": "Williams and Zweig.", "year": 2016}, {"title": "Learning global features for coreference resolution", "author": ["Sam Wiseman", "Alexander M Rush", "Stuart M Shieber"], "venue": "arXiv preprint arXiv:1604.03035,", "citeRegEx": "Wiseman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wiseman et al\\.", "year": 2016}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Jason D Williams"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 25, "context": "One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016).", "startOffset": 106, "endOffset": 309}, {"referenceID": 10, "context": "One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016).", "startOffset": 106, "endOffset": 309}, {"referenceID": 21, "context": "One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016).", "startOffset": 106, "endOffset": 309}, {"referenceID": 17, "context": "One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016).", "startOffset": 106, "endOffset": 309}, {"referenceID": 15, "context": "One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016).", "startOffset": 106, "endOffset": 309}, {"referenceID": 16, "context": "One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016).", "startOffset": 106, "endOffset": 309}, {"referenceID": 22, "context": "One example is in task oriented dialogue where access to a database is necessary to answer a user\u2019s query (Young et al., 2013; Li et al., 2016; Vinyals & Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes & Weston, 2016; Williams & Zweig, 2016; Shang et al., 2015; Wen et al., 2016).", "startOffset": 106, "endOffset": 309}, {"referenceID": 9, "context": "Secondly, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015).", "startOffset": 55, "endOffset": 94}, {"referenceID": 21, "context": "Secondly, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015).", "startOffset": 55, "endOffset": 94}, {"referenceID": 9, "context": "In the task of recipe generation from a list of ingredients (Kiddon et al., 2016), the generation of the recipe will frequently reference these items.", "startOffset": 60, "endOffset": 81}, {"referenceID": 13, "context": "Finally, we address references within a document (Mikolov et al., 2010; Ji et al., 2015; Wang & Cho, 2015), as the generation of words will ofter refer to previously generated words.", "startOffset": 49, "endOffset": 106}, {"referenceID": 7, "context": "Finally, we address references within a document (Mikolov et al., 2010; Ji et al., 2015; Wang & Cho, 2015), as the generation of words will ofter refer to previously generated words.", "startOffset": 49, "endOffset": 106}, {"referenceID": 0, "context": "Selecting an entity in context is similar to familiar models of attention (Bahdanau et al., 2014), but rather than being a deterministic function that reweights representations of elements in the context, it is treated as a distribution over contextual elements which are stochastically selected and then copied or, if the task warrants it, transformed (e.", "startOffset": 74, "endOffset": 97}, {"referenceID": 4, "context": "1 DIALOGUE MODEL WITH DATABASE SUPPORT We first apply our model on task-oriented dialogue systems in the domain of restaurant recommendations, and work on the data set from the second Dialogue State Tracking Challenge (DSTC2) (Henderson et al., 2014).", "startOffset": 226, "endOffset": 250}, {"referenceID": 15, "context": "Figure 2: Hierarchical RNN Seq2Seq model We build a model based on the hierarchical RNN model described in (Serban et al., 2016), as in dialogues, the generation of the response is not only dependent on the previous sentence, but on all sentences leading to the response.", "startOffset": 107, "endOffset": 128}, {"referenceID": 0, "context": "A full description of this operation is described in (Bahdanau et al., 2014).", "startOffset": 53, "endOffset": 76}, {"referenceID": 24, "context": "Figure 5: Coreference based language model, example taken from Wiseman et al. (2016). Word generation: At each time step before generating the next word, we predict whether the word is an entity mention: p(vi|hi\u22121, h) = ATTN(h, hi\u22121), di = \u2211", "startOffset": 63, "endOffset": 85}, {"referenceID": 13, "context": "Recently, there has been great progresses in modeling languages based on neural network, including language modeling (Mikolov et al., 2010; Jozefowicz et al., 2016), machine translation (Sutskever", "startOffset": 117, "endOffset": 164}, {"referenceID": 8, "context": "Recently, there has been great progresses in modeling languages based on neural network, including language modeling (Mikolov et al., 2010; Jozefowicz et al., 2016), machine translation (Sutskever", "startOffset": 117, "endOffset": 164}, {"referenceID": 5, "context": ", 2014), question answering (Hermann et al., 2015) etc.", "startOffset": 28, "endOffset": 50}, {"referenceID": 10, "context": "Based on the success of seq2seq models, neural networks are applied in modeling chit-chat dialogue (Li et al., 2016; Vinyals & Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Shang et al., 2015) and task oriented dialogue (Wen et al.", "startOffset": 99, "endOffset": 199}, {"referenceID": 17, "context": "Based on the success of seq2seq models, neural networks are applied in modeling chit-chat dialogue (Li et al., 2016; Vinyals & Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Shang et al., 2015) and task oriented dialogue (Wen et al.", "startOffset": 99, "endOffset": 199}, {"referenceID": 15, "context": "Based on the success of seq2seq models, neural networks are applied in modeling chit-chat dialogue (Li et al., 2016; Vinyals & Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Shang et al., 2015) and task oriented dialogue (Wen et al.", "startOffset": 99, "endOffset": 199}, {"referenceID": 16, "context": "Based on the success of seq2seq models, neural networks are applied in modeling chit-chat dialogue (Li et al., 2016; Vinyals & Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Shang et al., 2015) and task oriented dialogue (Wen et al.", "startOffset": 99, "endOffset": 199}, {"referenceID": 21, "context": ", 2015) and task oriented dialogue (Wen et al., 2015; Bordes & Weston, 2016; Williams & Zweig, 2016; Wen et al., 2016).", "startOffset": 35, "endOffset": 118}, {"referenceID": 22, "context": ", 2015) and task oriented dialogue (Wen et al., 2015; Bordes & Weston, 2016; Williams & Zweig, 2016; Wen et al., 2016).", "startOffset": 35, "endOffset": 118}, {"referenceID": 9, "context": "Previous work on recipe generation from ingredients was proposed in (Kiddon et al., 2016).", "startOffset": 68, "endOffset": 89}, {"referenceID": 13, "context": "Context dependent language models (Mikolov et al., 2010; Ji et al., 2015; Wang & Cho, 2015) are proposed to capture long term dependency of text.", "startOffset": 34, "endOffset": 91}, {"referenceID": 7, "context": "Context dependent language models (Mikolov et al., 2010; Ji et al., 2015; Wang & Cho, 2015) are proposed to capture long term dependency of text.", "startOffset": 34, "endOffset": 91}, {"referenceID": 24, "context": "There are also lots of works on coreference resolution (Haghighi & Klein, 2010; Wiseman et al., 2016).", "startOffset": 55, "endOffset": 101}, {"referenceID": 2, "context": "Much effort has been invested in embedding a copying mechanism for neural models (G\u00fcl\u00e7ehre et al., 2016; Gu et al., 2016; Ling et al., 2016).", "startOffset": 81, "endOffset": 140}, {"referenceID": 11, "context": "Much effort has been invested in embedding a copying mechanism for neural models (G\u00fcl\u00e7ehre et al., 2016; Gu et al., 2016; Ling et al., 2016).", "startOffset": 81, "endOffset": 140}, {"referenceID": 12, "context": "Our models are similar to models proposed in (Ahn et al., 2016; Merity et al., 2016), where the generation of each word can be conditioned on a particular entry in a knowledge lists and previous words.", "startOffset": 45, "endOffset": 84}], "year": 2017, "abstractText": "We propose a general class of language models that treat reference as an explicit stochastic latent variable. This architecture allows models to create mentions of entities and their attributes by accessing external databases (required by, e.g., dialogue generation and recipe generation) and internal state (required by, e.g. language models which are aware of coreference). This facilitates the incorporation of information that can be accessed in predictable locations in databases or discourse context, even when the targets of the reference may be rare words. Experiments on three tasks show our model variants outperform models based on deterministic attention.", "creator": "LaTeX with hyperref package"}}}