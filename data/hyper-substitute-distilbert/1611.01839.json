{"id": "1611.01839", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "Hierarchical Question Answering for Long Documents", "abstract": "reading such article and questioning questions about its content is common fundamental task for natural language understanding. among most intrinsic neural codes addressed this problem relying on recurrent neural codes ( rnns ), learning rnns over long documents can get prohibitively costly. investigations present a novel analogy for question answering that programmers perform scale independently desired answers while avoiding or even exhibiting performance. our approach involves : coarse, inexpensive model for selecting one with 2 relevant sentences surrounding a more expensive rnn that produces the answer from constraint tests. a central challenge poses the lack of intermediate memory for the coarse content, why subjects define using reactive learning. experiments describe out - of - living - art performance on a particular subset of the simple dataset ( carr has al., 2016 ) specifically measuring a newly - tested array, then replacing the spontaneously chosen correct repetitive steps by 12 % against a standard sequence invariant sequence model.", "histories": [["v1", "Sun, 6 Nov 2016 20:24:40 GMT  (1598kb,D)", "http://arxiv.org/abs/1611.01839v1", null], ["v2", "Wed, 8 Feb 2017 07:42:34 GMT  (856kb,D)", "http://arxiv.org/abs/1611.01839v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["eunsol choi", "daniel hewlett", "alexandre lacoste", "illia polosukhin", "jakob uszkoreit", "jonathan berant"], "accepted": false, "id": "1611.01839"}, "pdf": {"name": "1611.01839.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Question Answering for Long Documents", "authors": ["Eunsol Choi", "Daniel Hewlett", "Alexandre Lacoste"], "emails": ["eunsol@cs.washington.edu", "dhewlett@google.com", "allac@google.com", "ipolosukhin@google.com", "usz@google.com", "joberant@cs.tau.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Reading a document and answering questions about its content are among the hallmarks of natural language understanding. Recently, interest in question answering (QA) from unstructured documents has rocketed along with the availability of large scale datasets for reading comprehension (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Onishi et al., 2016).\nCurrent state-of-the-art approaches for QA over documents are based on Recurrent Neural Networks\n(RNNs) that encode the document and the question to determine the answer (Hermann et al., 2015; Chen et al., 2016; Kumar et al., 2016; Kadlec et al., 2016). While this allows the model to access all relevant information, it leads to slow models where the RNN runs sequentially over possibly thousands of tokens. In fact, most neural models for QA over documents truncate the documents and consider only a limited number of tokens (Miller et al., 2016; Hewlett et al., 2016). Inspired by studies (Masson, 1983) on how people answer questions by first skimming the docu-\nar X\niv :1\n61 1.\n01 83\n9v 1\n[ cs\n.C L\n] 6\nN ov\nment, identifying the relevant parts, and then reading more carefully these parts to produce a final answer, we present in this paper a coarse-to-fine model for reading comprehension.\nWe propose a simple neural model that treats QA as a hierarchical process (see Figure 1). First, a fast model is used for sentence selection (Yu et al., 2014; Yang et al., 2016a), that is, to select a few sentences from the document that are relevant for answering the question. Then, a slower RNN is employed to produce the final answer from the selected sentences. Thus, the RNN is run over a fixed number of tokens, regardless of the length of the original document. Reducing the size of the input to the RNN results in a faster model compared to a model that treats the entire document as a single sequence.\nWe do not assume that the answer to a question appears verbatim in the input document (e.g., a question about the genre of a movie can often be answered even if not mentioned explicitly). Therefore, sentence selection is treated as a latent variable that is trained jointly with the answer generation model from the answer signal only. We explore both a hard attention sentence selection model, trained using REINFORCE (Williams, 1992), as well as a fully differentiable soft attention sentence selection model that is trained end-to-end. We find that in datasets where the answer frequently does not appear verbatim in the input document, training jointly the sentence selection and answer generation models improves performance compared to a pipeline approach in which sentence selection is trained separately through distant supervision. We also explore multiple sentence selection models, from a simple bag-of-words (BoW) model to a more expensive but still parallelizable convolutional model.\nWe evaluate our model on a subset of the recently published WIKIREADING dataset (Hewlett et al., 2016), focusing on examples where the input document is lengthy and sentence selection is challenging. We also evaluate our model on a new question answering dataset called WIKISUGGEST that contains more natural questions gathered from a search engine. Our contribution is a new modular framework and learning procedures for addressing question answering over long documents. The hierarchical framework captures document structure such as sentence boundaries and is able to deal with long\ndocuments or potentially multiple documents. Our framework is applicable to any model, be it neural or not. Experiments demonstrate that we improve performance compared to state-of-the-art models on the subset of WIKIREADING, and obtain comparable performance on WIKISUGGEST, while dramatically reducing the number of tokens processed by the RNN at training and test time."}, {"heading": "2 Problem Setting", "text": "Our task is defined as follows. Given a training set of question-document-answer triples {x(i), d(i), y(i)}Ni=1, our goal is to train a question answering model that will produce an answer y for a new question-document pair (x, d). A document d is a list of sentences s1, s2, . . . , s|d|, and we assume there is a latent subset of sentences from which the answer can be produced. Figure 2 illustrates a training example in which sentence s5 is a key sentence for answering the question."}, {"heading": "3 Data", "text": "We evaluate on WIKIREADING LONG, and a newly gathered WIKISUGGEST dataset.\nWIKIREADING (Hewlett et al., 2016) is a recently released dataset that was automatically generated from Wikipedia and Wikidata, where the goal is as follows: given a Wikipedia page about an entity and a Wikidata property, such as PROFESSION, or GENDER, to infer the value based on the document. Due to the structure of Wikipedia, and the short length of most Wikipedia documents (median number of sentences: 9), the answer can usually be generated\nfrom the first few sentences. Thus, this dataset is not optimal for testing a sentence selection model compared to a model that just utilizes the first few sentences. Table 1 quantifies this intuition: We consider sentences containing the answer y\u2217 as a proxy for sentences that are useful for answer generation, and report how often y\u2217 appears in the document. Additionally, we report how frequently this proxy oracle sentence is the first sentence. We observe that in WIKIREADING, the answer appears in the document in 47.1% of the examples, and in 35.4% of the examples the match is on the first sentence, thus headroom for sentence selection models is limited.\nTo remedy that, we filter WIKIREADING and ensure a more even distribution of answers throughout the document. First, we prune documents with less than 10 sentences. Second, we only consider Wikidata properties for which Hewlett et al. (2016)\u2019s best model obtains an accuracy that is lower than 60%. This step prunes out properties such as GENDER, GIVEN NAME and INSTANCE OF. This results in a dataset containing 1.97M examples, including properties such as GENRE, FILM EDITOR.(see Table 2 for further dataset statistics). On this subset, the answer appears in 50.4% of the examples, and it appears in the first sentence only 15.8% of the time (Table 1).\nWhile WIKIREADING offers high-quality, largescale data, the questions (Wikidata properties) are not phrased in natural language and are limited to 858 properties only. To model more realistic natural language queries, we collect the WIKISUGGEST dataset as follows (see Figure 3 for illustration).\nWe use the Google Suggest API to harvest natural language questions and submit them to Google Search. Whenever Google Search returns a box with a short answer whose source is Wikipedia (Figure 3), we create an example from the question, answer, and the Wikipedia document from which the\nanswer was extracted. We evaluated the data quality by manually examining fifty examples. 54% of the examples were well formed question-answer pairs where we can ground answers in the given Wikipedia document, 20% contain answers without textual evidence in the document, and 26% contain incorrect question-answer pairs such as the last two examples in Figure 3.\nTable 1 shows that the exact answer string is often missing from the document in WIKIREADING and WIKIREADING LONG. Extracted from manually curated Wikidata statements, WIKIREADING includes categorical properties such as GENDER and NATIONALITY. When the answer is not explicitly mentioned, it can usually be inferred from the document. On the other hand, in WIKISUGGEST, a missing answer string match often implies a spurious question-answer pair such as (\u2018what time is half time in rugby\u2019, \u201880 minutes, 40 minutes\u2019). Thus, we pruned question-answer pairs without exact answer string match for quality control. In both datasets, multiple sentences may contain the answer string. Most questions, however, are factoid and do not require multiple sentences to answer."}, {"heading": "4 Model", "text": "Our model has two parts (see Figure 1). The first part is a fast sentence selection model (Section 4.1) that defines a probability distribution p(s | x, d) over document sentences given the input question (x) and the document (d). The second part is a more costly\nanswer generation model (Section 4.3) that defines a probability distribution p(y | x, d\u0302) over answers given a question and a document \u201csummary\u201d (d\u0302) that focuses on the relevant parts of the document. The representation d\u0302 is constructed with a soft attention model or a hard attention model (Section 4.2)."}, {"heading": "4.1 Sentence Selection Model", "text": "Following recent work on sentence selection for QA (Yu et al., 2014; Yang et al., 2016b), we build a feedforward neural network model to define a probability distribution (attention) over the list of sentences s1, s2, . . . , s|d|, given the query x and document d. We consider three possible sentence representations for sentence selection: a bag of words (BoW) model, a chunking model, and a (parallelizable) convolutional model. These models are relatively efficient at dealing with larger quantities of text.\nBoW Model A bag-of-words representation of an utterance amounts to averaging the word embeddings in that utterance. We explored two network structures for sentence scoring. The first is concatenation: the bag-of-words representation of the query is concatenated to the bag-of-words representation of the sentence sl, which is then passed through a single layer feed-forward network.\nhl = BoW(x)||BoW(sl), vl = v >ReLU(Whl),\np(s = sl | x, d) = softmax(vl),\nwhere || is the concatenation operator, and the matrix W , the vector v and the word embeddings are learned parameters.\nWe also consider a dot product model without a non-linear interaction between the sentence and query, inspired by its strong performance in MT setting (Luong et al., 2015). The model passes the BoW representation of the sentence and query separately through a single layer feed-forward network and computes the dot product between the two out-\nput vectors.\nhx = ReLU(Wq BoW(x)),\nhl = ReLU(Wd BoW(sl)),\np(s = sl | x, d) = softmax(hx \u00b7 hl),\nwhere the matrices Wq, Wd, the vectors vq, vd and the word embeddings are learned parameters.\nChunked BoW Model To get a more fine-grained granularity, one can split sentences into fixed size smaller chunks (seven tokens per chunk) and score each chunk separately (Miller et al., 2016). This could be beneficial if questions are answered by subsentential units that are lost in a BoW representation of an entire sentence. We split a sentence into a fixed number of chunks (cl,1, cl,2 \u00b7 \u00b7 \u00b7 , cl,J ), generate a BoW representation for each chunk, and score it separately exactly as in the BoW model. We compute a probability distribution over chunks as in the BoW model, and compute sentence probabilities by marginalizing over all chunks belonging to a sentence. Let p(c = cl,j | x, d) be the probability distribution over all chunks from all sentences, then:\np(s = sl | x, d) = J\u2211 j=1 p(c = cl,j | x, d),\nwith the same parameters as in the BoW model.\nConvolutional Neural Network Model While our sentence selection model is designed to be fast, we explore a slightly more complex architecture by adding a convolutional neural network (CNN) that can compose the meaning of words. CNN is still more efficient than an RNN, since the convolutional filters can be computed in parallel. We follow previous work (Kim, 2014; Kalchbrenner et al., 2014) and add a single convolutional layer over the encoding enc(x, sl) with 100 filters of width 5. We use max-pooling to obtain a fixed-length representation for the sentence and query, and pass that representation through a single layer feed-forward network as in the BoW model to score each sentence."}, {"heading": "4.2 Document summary", "text": "After computing an attention distribution over sentences with the sentence selection model, we create a summary that focuses on the document parts related to the question using a deterministic soft attention model or a stochastic hard attention model.\nHard Attention In the hard attention model, we sample a sentence sl \u223c p(s | x, d) and fix the document summary d\u0302 to be the chosen sentence. At test time, we choose the most probable sentence rather than sample.\nTo extend the document summary to contain further information, we also explore selecting K sentences from the document and define the summary to be the concatenation of the sampled sentences d\u0302 = sl1 ||sl2 || . . . ||slK .1 Sampling prevents conventional back propagation and therefore we use the REINFORCE algorithm as discussed in Section 5.\nSoft Attention The soft attention model (Bahdanau et al., 2015) generates the summary by computing a weighted average of sentences word by word according to p(s | x, d). More explicitly, let d\u0302m be the mth token of the document summary. Then, by fixing the length of every sentence toM tokens2, the blended tokens are computed as follows:\nd\u0302m = |d|\u2211 l=1 p(s = sl | x, d) \u00b7 sl,m,\nwhere m \u2208 [1, . . . ,M ], sl,m is the mth word in the lth sentence, and |d| is the number of sentences in a document."}, {"heading": "4.3 Answer Generation Model", "text": "State-of-the-art question answering models (Chen et al., 2016) use sequence-to-sequence models to encode the document and question and generate the answer. We focus on a developing a fast sentence selection model, and do not subscribe to a particular answer generation architecture. We choose to implement the state-of-the-art word-level sequenceto-sequence model with placeholders, described by\n1To prevent re-sampling the same sentences, we mask the previously selected sentences and re-normalize p\u03b8(s | x, d) after sampling each example.\n2Longer sentences are truncated and shorter ones are padded\nHewlett et al. (2016), and review it shortly here for completeness. This recurrent neural network model takes the query tokens, a separating token, and the document (or in out case, document summary) tokens as input and encodes them with a Gated Recurrent Unit (GRU) (Cho et al., 2014). Then, the answer is decoded with another GRU model with shared word embeddings parameters, which defines a distribution over answers p(y | x, d\u0302)."}, {"heading": "5 Learning", "text": "We consider three approaches for learning the parameters of our model. The soft attention model is differentiable and is optimized using end-toend learning. The hard attention model is nondifferentiable and is optimized with the REINFORCE algorithm (Williams, 1992). Last, we consider a pipeline approach, where we use distant supervision to label sentences for selection and train a sentence selection model independently from an answer generation model. We use stochastic gradient descent with Adam (Kingma and Ba, 2015).\nDistant Supervision While we do not have explicit supervision for the sentence selection model, we can define a simple heuristic for labeling sentences. We define the first sentence that has a full match of the answer string as the gold sentence, and if no such sentence exists, we define the first sentence to be the gold sentence.3 By labeling gold sentences we can train the sentence selection and answer generation models independently. In this setup, y and s are given as targets, and s serves as the document summary. Hence, our objective function is:\nJ(\u03b8) = log p\u03b8(y, s | x, d) = log p\u03b8(s | x, d) + log p\u03b8(y | s, x).\nAt training time, we define the probability of the gold sentence p(s = starget | x, d) = 1 and 0 for all other sentences. At test time we do not have access to the gold sentence and use\nargmax sl\u2208d\np(s = sl | d, x)\ndirectly for answer generation. 3We experimented with learning the distribution over sentences that match the answer string, but this did not improve over matching the first sentence.\nReinforcement Learning In this setup, we consider that the target sentence is not provided and we use a reinforcement learning approach with two actions. The first action is sentence selection and the second is answer generation. Our goal is to select sentences that lead to a high reward. We define the reward for selecting a sentence to be the log probability of the correct answer given that sentence, that is, R\u03b8(sl) = log p\u03b8(y= y\u2217 | sl, x). Then the learning objective is to maximize the expected reward:\nJ(\u03b8) = \u2211 sl\u2208d p\u03b8(s=sl | x, d) \u00b7R\u03b8(sl)\n= \u2211 sl\u2208d p\u03b8(s=sl | x, d) \u00b7 log p\u03b8(y=y\u2217 | sl, x)\nFollowing REINFORCE (Williams, 1992), we approximate the gradient of the objective with a sample, s\u0302 \u223c p\u03b8(s | x, d):\n\u2207J(\u03b8) \u2248 \u2207 log p\u03b8(y | s\u0302, x) + log p\u03b8(y | s\u0302, x) \u00b7 \u2207 log p\u03b8(s\u0302 | x, d).\nExtending the derivations above to sampling K sentences is straightforward and we omit it for brevity.\nTraining with REINFORCE is known to be unstable due to the high variance induced by the sampling. To reduce the variance, we use a curriculum learning strategy, gently transitioning from a distant supervision setting to a reinforcement learning setting, similar to the DAGGER algorithm (Ross et al., 2011). We define the probability of using the distant supervision objective at each global step as re, where r is the decay rate and e is the index of the current training epoch. We tuned r \u2208 [0.3, 1] for each sentence selection model and dataset combination on the development set.\nSoft Attention We train the soft attention model by maximizing the conditioned log likelihood of the answer given the question and the document: log p\u03b8(y\n\u2217 | d, x). The model is fully differentiable and is trained end-to-end with back propagation."}, {"heading": "6 Experiments", "text": "Experimental Setup We used 70% of the data for training, 10% for validation, and 20% for testing. We used the first 35 sentences in each document as\ninput to the hierarchical models, where each sentence has a maximum length of 35 tokens. Similar to Miller et al (2016), we add the first five words in the document (typically the title of the article) at the end of each sentence sequence for WIKISUGGEST. We also add the sentence index as a one hot vector to the sentence representation. We fixed most hyperparameters for all models after tuning on the validation set.4 The learning rate and gradient clipping coefficient were tuned separately on the validation set for each model with grid search over the values {0.0005, 0.001, 0.002, 0.004} and {0.5, 1.0}, respectively. We employed dropout (Srivastava et al., 2014) in the smaller WIKIREADING LONG dataset to avoid overfitting.\nEvaluation Metrics Our main evaluation metric is answer accuracy, that is, the proportion of questions answered correctly. We do not perform any normalization on the answer string, and treat only exact string match to be correct.\nFor sentence selection, we report the accuracy of selecting the correct one. Since we do not know which sentence contains the answer, we report approximate accuracy by matching sentences that contain the answer string (y\u2217). For the soft attention model, we treat the sentence with the highest atten-\n4Word embeddings dimension for RNN=256, vocabulary size=100,000 and a GRU state dimension=512. For the sentence selection model, we used concatenation in WIKIREADING LONG and dot product in WIKISUGGEST (see Section 4.1).\ntion probability as the predicted sentence. We report precision@1 (accuracy) and mean reciprocal rank (MRR) for this evaluation.\nModels and Baselines The models PIPELINE, REINFORCE, and SOFTATTEND correspond to the three learning objectives in Section 5. We compare these models against the following baselines:\nFULL is an implementation of the best model by Hewlett et al. (2016). A word-level sequence-to-sequence model, which consumes the first 300 tokens of the document. We experimented with providing additional tokens to match the length of document available to hierarchical models, but the model failed to generalize to longer input sequences and showed degraded performance. FIRST always selects the first sentence of the document. The answer appears in the first sentence in 33% and 15% of documents in WIKISUGGEST and WIKIREADING LONG, respectively. ORACLE selects the first sentence with the answer string if it exists, or otherwise the first sentence in the document.\nFinally, we also compare the accuracy of the sentence selection models (CNN, CHUNKBOW and BOW) described in Section 4.\nResults Table 3 summarizes the answer accuracy for the two datasets. The proposed hierarchical models match or exceed the performance of FULL, while reducing the number of sequential RNN steps by almost an order of magnitude, from 300 to 35 (or 70 forK=2). When the answer appear after the first 300 tokens, FULL cannot access it unlike our models. While our approach incurs an additional cost for the sentence selection models, these models are fast and parallelizable, unlike the RNN model which takes most of the computation time. In WIKIREADINGLONG, our model based on REINFORCE outperforms all other models (excluding ORACLE, which has access to sentence labels at test time).\nJointly learning answer generation and sentence selection, REINFORCE outperforms PIPELINE, which relies on the noisy supervision signal for sentence selection, in WIKIREADING LONG. SOFT ATTEND also did better than PIPELINE, though by a lesser amount. In WIKISUGGEST, all learned models perform similarly, a bit lower than the FULL model. This is potentially explained by the approximate supervision for sentence selection, which exists for 78% of examples vs. 49% in WIKIREADING LONG. Results suggest that modeling the summary as a latent variable is more important when the answer string is absent in the document.\nFor hard attention, adding more information to the summary is achieved by sampling an additional sentence. Allowing REINFORCE to sample an additional sentence increased performance in both datasets, but by a larger amount in WIKIREADING LONG than WIKISUGGEST.5 Additional sampling may allow recovery from mistakes in WIKIREADING LONG, where sentence selection performance was lower.\nIn both datasets, all models outperform the FIRST baseline, which heuristically selects the first sentence for all queries. Using the sentence with answer string match (ORACLE) improves the performance on both datasets, particularly on WIKISUGGEST.6\nComparing hard attention to soft attention, we observe that in general hard attention models were more effective, a pattern similar to results from caption generation (Xu et al., 2015). The attention dis-\n5Sampling more did not help with pipeline methods. 6ORACLE may be able to fit the noise in the data, answering\nmore spurious queries correctly.\ntribution learned by the soft attention model is often less peaked (see Figure 4), which generates noisier summaries with irrelevant sentences. This likely explains the lower performance, since both datasets consist primarily of factoid questions that do not require reasoning over multiple sentences.\nTable 4 reports sentence selection accuracy, according to our approximate evaluation. Precision@1 and MRR show the same trends. As in the answer accuracy, SOFTATTEND performed worse than REINFORCE, while both outperform FIRST. The simpler BOW performed the best in WIKISUGGEST, whereas the CNN and CHUNKBOW model showed a slight improvement over BOW in WIKIREADING LONG, where sentence selection is more challenging. While still representing an upper bound on performance, ORACLE does not achieve a perfect score as some examples do not have an answer string match in the first 35 sentences.\nQualitative Analysis Figure 4 contains a visualization of the attention distribution over sentences, p(sl | d, x), for different learning procedures. The increased frequency of the answer string in WIKISUGGEST vs. WIKIREADING LONG is evident in\nthe leftmost plot. SOFTATTEND and CHUNKBOW clearly distribute attention more evenly across the sentences compared to BOW and CNN.\nWe categorized the primary reasons for the system\u2019s errors in Table 5 and present an example for each error type in Table 6. All examples are from REINFORCE model with BOW sentence selection model. The most frequent source of error for WIKIREADING LONG was the lack of evidence in the input document. While the dataset does not contain false answers, the document does not always provide supporting evidence (examples of properties without clues are ELEVATION ABOVE SEA LEVEL and SISTER). Interestingly, answer string match can still appear in the document as in the first example in Table 6: the answer \u2018Saint Petersburg\u2019 appears multiple times in the document (e.g., 4th\nsentence). In both datasets, answer generation at times failed to generate the exact answer sentence even with the correct sentence selection. This was pronounced especially when the answer was long. In another example, the system generated \u2018Princess UNK of UNK\u20197 instead of \u2018Princess Marie Louise of Bourbon-Parma\u2019. For the automatically collected WIKISUGGEST dataset, noisy question-answer pairs were problematic, as discussed in Section 2. However, the models can frequently guess the answer correctly. Sentence selection was more challenging in WIKIREADING LONG, explaining why sampling two sentences improved performance more.\nWhile the exact answer string appears in many documents, at times it was not mentioned and the model learned to generate the answer string sequence, as in the first correctly predicted example. The second example shows when our model successfully spots the relevant sentence without obvious clues. The third one shows a typical case, where the model correctly paid attention to the first sentence, while in the last example the model correctly spots the sentence far from the head of the document."}, {"heading": "7 Related Work", "text": "In recent years, there has been an increasing interest in datasets that evaluate reading comprehension. The MCTest (Richardson et al., 2013) and BioProcess bank (Berant et al., 2014) are smallerscale datasets focusing on common sense reasoning under a closed world assumption. bAbi (Weston et al., 2015) is a synthetic dataset with simulated questions to capture various aspects of reasoning. WikiQA is a well-curated, smaller-scale answer selection dataset. Jurczyk et al (2016) presented a crowdsourced dataset on Wikipedia which involves sentence selection and the SQuAD (Rajpurkar et al., 2016) dataset proposes answer selection, but covers shorter documents. Cloze-style question answering datasets (CNN (Hermann et al., 2015), Who did What (Onishi et al., 2016), and CBT (Hill et al., 2015)) are designed to assess machine comprehension but do not form valid questions.\nAnswer sentence selection, also referred as answer triggering, has been studied in the TREC QA\n7UNK is the unknown token.\ndataset (Voorhees and Tice, 2000). Recently, neural network architectures (Wang and Nyberg, 2015; Severyn and Moschitti, 2015; dos Santos et al., 2016) achieved improvements over earlier featurebased models (Severyn and Moschitti, 2013). Recent work (Sultan et al., 2016) models answer sentence extraction and answer extraction jointly. Here we study answer sentence selection as a latent variable for a question answering task, and generate answer strings instead of selecting text spans.\nHierarchical attention models have been recently applied to tasks such as text categorization (Yang et al., 2016b), extractive summarization (Cheng and Lapata, 2016), machine translation (Ba et al., 2014), aspect sentiment classification (Lei et al., 2016), and vision (Lu et al., 2016; Xu et al., 2015). To the best of our knowledge, we are the first to directly use the document structure to improve question answering over documents.\nFinally, our work is related to reinforcement learning literature and learning from noisy signals (Mintz et al., 2009; Hoffmann et al., 2011). The variants of hard and soft attention models were examined in the context of caption generation (Xu et al., 2015). Curriculum learning for question answering was investigated in Sachan and Xing (2016), but they focused on the ordering of training examples while we interpolate two supervision signals. Reinforcement learning gained popularity in natural language processing tasks such as coreference resolution (Clark and Manning, 2016), information extraction (Narasimhan et al., 2016), semantic parsing (Andreas et al., 2016) and textual games (Narasimhan et al., 2015; He et al., 2016)."}, {"heading": "8 Conclusion", "text": "We presented a hierarchical framework for QA over long documents that quickly focuses on the relevant portions of a document to answer the question. We demonstrated that our model is more efficient and can match or exceed state-of-the-art performance on two challenging reading comprehension datasets.\nOur framework uses the document structure to handle long documents. In future work we would like to deepen the hierarchy by answering questions over multiple documents and utilize other structural clues such as paragraphs, titles and so on."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Andreas et al\\.,? 2016", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu."], "venue": "The International Conference on Learning Representations.", "citeRegEx": "Ba et al\\.,? 2014", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Modeling biological processes for reading comprehension", "author": ["Jonathan Berant", "Vivek Srikumar", "Pei-Chun Chen", "Abby Vander Linden", "Brittany Harding", "Brad Huang", "Peter Clark", "Christopher D Manning."], "venue": "Proceedings of the Conference of the Empirical Meth-", "citeRegEx": "Berant et al\\.,? 2014", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Neural summarization by extracting sentences and words", "author": ["Jianpeng Cheng", "Mirella Lapata."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Cheng and Lapata.,? 2016", "shortCiteRegEx": "Cheng and Lapata.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of the Confer-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Deep reinforcement learning for mention-ranking coreference models", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Clark and Manning.,? 2016", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Deep reinforcement learning with an unbounded action space", "author": ["Ji He", "Jianshu Chen", "Xiaodong He", "Jianfeng Gao", "Lihong Li", "Li Deng", "Mari Ostendorf."], "venue": "Proceedings of the Conference of the Association for Computational Linguistics.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Wikireading: A novel large-scale language understanding task over wikipedia", "author": ["Daniel Hewlett", "Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot."], "venue": "Proceedings of the Conference of the", "citeRegEx": "Hewlett et al\\.,? 2016", "shortCiteRegEx": "Hewlett et al\\.", "year": 2016}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "The International Conference on Learning Representations.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Knowledgebased weak supervision for information extraction of overlapping relations", "author": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational", "citeRegEx": "Hoffmann et al\\.,? 2011", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "SelQA: A New Benchmark for Selection-based Question Answering", "author": ["Tomasz Jurczyk", "Michael Zhai", "Jinho D. Choi."], "venue": "Proceedings of the 28th International Conference on Tools with Artificial Intelligence, ICTAI\u201916, San Jose, CA.", "citeRegEx": "Jurczyk et al\\.,? 2016", "shortCiteRegEx": "Jurczyk et al\\.", "year": 2016}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ond\u0159ej Bajgar", "Jan Kleindienst."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 908\u2013918,", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "The International Conference on Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher."], "venue": "Proceedings of the International", "citeRegEx": "Kumar et al\\.,? 2016", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Rationalizing neural predictions", "author": ["Tao Lei", "Regina Barzilay", "Tommi S. Jaakkola."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Lei et al\\.,? 2016", "shortCiteRegEx": "Lei et al\\.", "year": 2016}, {"title": "Hierarchical question-image coattention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh."], "venue": "arXiv preprint arXiv:1606.00061.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412\u20131421, Lisbon, Por-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Conceptual processing of text during skimming and rapid sequential reading", "author": ["Michael EJ Masson."], "venue": "Memory & Cognition, 11(3):262\u2013274.", "citeRegEx": "Masson.,? 1983", "shortCiteRegEx": "Masson.", "year": 1983}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Improving information extraction by acquiring external evidence with reinforcement learning", "author": ["Karthik Narasimhan", "Adam Yala", "Regina Barzilay."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Narasimhan et al\\.,? 2016", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2016}, {"title": "Who did what: A largescale person-centered cloze dataset", "author": ["Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester."], "venue": "Proceedings of Empirical Methods in Natural Language Processing.", "citeRegEx": "Onishi et al\\.,? 2016", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["St\u00e9phane Ross", "Geoffrey J Gordon", "Drew Bagnell."], "venue": "International Conference on Artificial Intelligence and Statistics.", "citeRegEx": "Ross et al\\.,? 2011", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Easy questions first? a case study on curriculum learning for question answering", "author": ["Mrinmaya Sachan", "Eric P Xing."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Sachan and Xing.,? 2016", "shortCiteRegEx": "Sachan and Xing.", "year": 2016}, {"title": "Automatic feature engineering for answer selection and extraction", "author": ["Aliaksei Severyn", "Alessandro Moschitti."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Severyn and Moschitti.,? 2013", "shortCiteRegEx": "Severyn and Moschitti.", "year": 2013}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["Aliaksei Severyn", "Alessandro Moschitti."], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 373\u2013", "citeRegEx": "Severyn and Moschitti.,? 2015", "shortCiteRegEx": "Severyn and Moschitti.", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A joint model for answer sentence ranking and answer extraction", "author": ["Md. Arafat Sultan", "Vittorio Castelli", "Radu Florian."], "venue": "Transactions of the Association for Computational Linguistics, 4:113\u2013125.", "citeRegEx": "Sultan et al\\.,? 2016", "shortCiteRegEx": "Sultan et al\\.", "year": 2016}, {"title": "Building a question answering test collection", "author": ["Ellen M Voorhees", "Dawn M Tice."], "venue": "Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 200\u2013207. ACM.", "citeRegEx": "Voorhees and Tice.,? 2000", "shortCiteRegEx": "Voorhees and Tice.", "year": 2000}, {"title": "A long short-term memory model for answer sentence selection in question answering", "author": ["Di Wang", "Eric Nyberg."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Wang and Nyberg.,? 2015", "shortCiteRegEx": "Wang and Nyberg.", "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning, 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "Proceedings of the International Conference", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Yang et al\\.,? 2016a", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Deep Learning for Answer Sentence Selection", "author": ["Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman."], "venue": "NIPS Deep Learning Workshop, December.", "citeRegEx": "Yu et al\\.,? 2014", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "Experiments demonstrate state-of-the-art performance on a challenging subset of the WIKIREADING dataset (Hewlett et al., 2016) and on a newly-gathered dataset, while reducing the number of sequential RNN steps by 88% against a standard sequence to sequence model.", "startOffset": 104, "endOffset": 126}, {"referenceID": 9, "context": "Recently, interest in question answering (QA) from unstructured documents has rocketed along with the availability of large scale datasets for reading comprehension (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Onishi et al., 2016).", "startOffset": 165, "endOffset": 251}, {"referenceID": 11, "context": "Recently, interest in question answering (QA) from unstructured documents has rocketed along with the availability of large scale datasets for reading comprehension (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Onishi et al., 2016).", "startOffset": 165, "endOffset": 251}, {"referenceID": 28, "context": "Recently, interest in question answering (QA) from unstructured documents has rocketed along with the availability of large scale datasets for reading comprehension (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Onishi et al., 2016).", "startOffset": 165, "endOffset": 251}, {"referenceID": 27, "context": "Recently, interest in question answering (QA) from unstructured documents has rocketed along with the availability of large scale datasets for reading comprehension (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Onishi et al., 2016).", "startOffset": 165, "endOffset": 251}, {"referenceID": 9, "context": "(RNNs) that encode the document and the question to determine the answer (Hermann et al., 2015; Chen et al., 2016; Kumar et al., 2016; Kadlec et al., 2016).", "startOffset": 73, "endOffset": 155}, {"referenceID": 4, "context": "(RNNs) that encode the document and the question to determine the answer (Hermann et al., 2015; Chen et al., 2016; Kumar et al., 2016; Kadlec et al., 2016).", "startOffset": 73, "endOffset": 155}, {"referenceID": 18, "context": "(RNNs) that encode the document and the question to determine the answer (Hermann et al., 2015; Chen et al., 2016; Kumar et al., 2016; Kadlec et al., 2016).", "startOffset": 73, "endOffset": 155}, {"referenceID": 14, "context": "(RNNs) that encode the document and the question to determine the answer (Hermann et al., 2015; Chen et al., 2016; Kumar et al., 2016; Kadlec et al., 2016).", "startOffset": 73, "endOffset": 155}, {"referenceID": 23, "context": "number of tokens (Miller et al., 2016; Hewlett et al., 2016).", "startOffset": 17, "endOffset": 60}, {"referenceID": 10, "context": "number of tokens (Miller et al., 2016; Hewlett et al., 2016).", "startOffset": 17, "endOffset": 60}, {"referenceID": 22, "context": "Inspired by studies (Masson, 1983) on how people answer questions by first skimming the docuar X iv :1 61 1.", "startOffset": 20, "endOffset": 34}, {"referenceID": 42, "context": "First, a fast model is used for sentence selection (Yu et al., 2014; Yang et al., 2016a), that is, to select a few sentences from the document that are relevant for answering the question.", "startOffset": 51, "endOffset": 88}, {"referenceID": 41, "context": "First, a fast model is used for sentence selection (Yu et al., 2014; Yang et al., 2016a), that is, to select a few sentences from the document that are relevant for answering the question.", "startOffset": 51, "endOffset": 88}, {"referenceID": 39, "context": "We explore both a hard attention sentence selection model, trained using REINFORCE (Williams, 1992), as well as a fully differentiable soft attention sentence selection model that is trained end-to-end.", "startOffset": 83, "endOffset": 99}, {"referenceID": 10, "context": "We evaluate our model on a subset of the recently published WIKIREADING dataset (Hewlett et al., 2016), focusing on examples where the input document is lengthy and sentence selection is challenging.", "startOffset": 80, "endOffset": 102}, {"referenceID": 10, "context": "WIKIREADING (Hewlett et al., 2016) is a recently released dataset that was automatically generated from Wikipedia and Wikidata, where the goal is as follows: given a Wikipedia page about an entity and a Wikidata property, such as PROFESSION, or GENDER, to infer the value based on the document.", "startOffset": 12, "endOffset": 34}, {"referenceID": 10, "context": "data properties for which Hewlett et al. (2016)\u2019s best model obtains an accuracy that is lower than 60%.", "startOffset": 26, "endOffset": 48}, {"referenceID": 42, "context": "Following recent work on sentence selection for QA (Yu et al., 2014; Yang et al., 2016b), we build a feedforward neural network model to define a probability distribution (attention) over the list of sentences", "startOffset": 51, "endOffset": 88}, {"referenceID": 21, "context": "We also consider a dot product model without a non-linear interaction between the sentence and query, inspired by its strong performance in MT setting (Luong et al., 2015).", "startOffset": 151, "endOffset": 171}, {"referenceID": 23, "context": "Chunked BoW Model To get a more fine-grained granularity, one can split sentences into fixed size smaller chunks (seven tokens per chunk) and score each chunk separately (Miller et al., 2016).", "startOffset": 170, "endOffset": 191}, {"referenceID": 16, "context": "vious work (Kim, 2014; Kalchbrenner et al., 2014) and add a single convolutional layer over the encoding enc(x, sl) with 100 filters of width 5.", "startOffset": 11, "endOffset": 49}, {"referenceID": 15, "context": "vious work (Kim, 2014; Kalchbrenner et al., 2014) and add a single convolutional layer over the encoding enc(x, sl) with 100 filters of width 5.", "startOffset": 11, "endOffset": 49}, {"referenceID": 2, "context": "Soft Attention The soft attention model (Bahdanau et al., 2015) generates the summary by computing a weighted average of sentences word by word according to p(s | x, d).", "startOffset": 40, "endOffset": 63}, {"referenceID": 4, "context": "State-of-the-art question answering models (Chen et al., 2016) use sequence-to-sequence models to encode the document and question and generate the", "startOffset": 43, "endOffset": 62}, {"referenceID": 10, "context": "Longer sentences are truncated and shorter ones are padded Hewlett et al. (2016), and review it shortly here for completeness.", "startOffset": 59, "endOffset": 81}, {"referenceID": 6, "context": "document (or in out case, document summary) tokens as input and encodes them with a Gated Recurrent Unit (GRU) (Cho et al., 2014).", "startOffset": 111, "endOffset": 129}, {"referenceID": 39, "context": "The hard attention model is nondifferentiable and is optimized with the REINFORCE algorithm (Williams, 1992).", "startOffset": 92, "endOffset": 108}, {"referenceID": 17, "context": "We use stochastic gradient descent with Adam (Kingma and Ba, 2015).", "startOffset": 45, "endOffset": 66}, {"referenceID": 39, "context": "Following REINFORCE (Williams, 1992), we approximate the gradient of the objective with a sample, \u015d \u223c p\u03b8(s | x, d):", "startOffset": 20, "endOffset": 36}, {"referenceID": 34, "context": "We employed dropout (Srivastava et al., 2014) in the smaller WIKIREADING LONG dataset to avoid overfitting.", "startOffset": 20, "endOffset": 45}, {"referenceID": 10, "context": "FULL is an implementation of the best model by Hewlett et al. (2016). A word-level sequence-to-sequence model, which consumes the first 300 tokens of the document.", "startOffset": 47, "endOffset": 69}, {"referenceID": 40, "context": "6 Comparing hard attention to soft attention, we observe that in general hard attention models were more effective, a pattern similar to results from caption generation (Xu et al., 2015).", "startOffset": 169, "endOffset": 186}, {"referenceID": 29, "context": "The MCTest (Richardson et al., 2013) and BioProcess bank (Berant et al.", "startOffset": 11, "endOffset": 36}, {"referenceID": 3, "context": ", 2013) and BioProcess bank (Berant et al., 2014) are smallerscale datasets focusing on common sense reasoning", "startOffset": 28, "endOffset": 49}, {"referenceID": 38, "context": "bAbi (Weston et al., 2015) is a synthetic dataset with simulated questions to capture various aspects of reasoning.", "startOffset": 5, "endOffset": 26}, {"referenceID": 38, "context": "bAbi (Weston et al., 2015) is a synthetic dataset with simulated questions to capture various aspects of reasoning. WikiQA is a well-curated, smaller-scale answer selection dataset. Jurczyk et al (2016) presented a crowd-", "startOffset": 6, "endOffset": 203}, {"referenceID": 28, "context": "sourced dataset on Wikipedia which involves sentence selection and the SQuAD (Rajpurkar et al., 2016) dataset proposes answer selection, but covers shorter documents.", "startOffset": 77, "endOffset": 101}, {"referenceID": 9, "context": "Cloze-style question answering datasets (CNN (Hermann et al., 2015), Who did", "startOffset": 45, "endOffset": 67}, {"referenceID": 27, "context": "What (Onishi et al., 2016), and CBT (Hill et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 11, "context": ", 2016), and CBT (Hill et al., 2015)) are designed to assess machine comprehension but do not form valid questions.", "startOffset": 17, "endOffset": 36}, {"referenceID": 36, "context": "dataset (Voorhees and Tice, 2000).", "startOffset": 8, "endOffset": 33}, {"referenceID": 37, "context": "Recently, neural network architectures (Wang and Nyberg, 2015; Severyn and Moschitti, 2015; dos Santos et al., 2016) achieved improvements over earlier feature-", "startOffset": 39, "endOffset": 116}, {"referenceID": 33, "context": "Recently, neural network architectures (Wang and Nyberg, 2015; Severyn and Moschitti, 2015; dos Santos et al., 2016) achieved improvements over earlier feature-", "startOffset": 39, "endOffset": 116}, {"referenceID": 32, "context": "based models (Severyn and Moschitti, 2013).", "startOffset": 13, "endOffset": 42}, {"referenceID": 35, "context": "Recent work (Sultan et al., 2016) models answer sentence extraction and answer extraction jointly.", "startOffset": 12, "endOffset": 33}, {"referenceID": 5, "context": ", 2016b), extractive summarization (Cheng and Lapata, 2016), machine translation (Ba et al.", "startOffset": 35, "endOffset": 59}, {"referenceID": 1, "context": ", 2016b), extractive summarization (Cheng and Lapata, 2016), machine translation (Ba et al., 2014),", "startOffset": 81, "endOffset": 98}, {"referenceID": 19, "context": "aspect sentiment classification (Lei et al., 2016), and vision (Lu et al.", "startOffset": 32, "endOffset": 50}, {"referenceID": 20, "context": ", 2016), and vision (Lu et al., 2016; Xu et al., 2015).", "startOffset": 20, "endOffset": 54}, {"referenceID": 40, "context": ", 2016), and vision (Lu et al., 2016; Xu et al., 2015).", "startOffset": 20, "endOffset": 54}, {"referenceID": 24, "context": "Finally, our work is related to reinforcement learning literature and learning from noisy signals (Mintz et al., 2009; Hoffmann et al., 2011).", "startOffset": 98, "endOffset": 141}, {"referenceID": 12, "context": "Finally, our work is related to reinforcement learning literature and learning from noisy signals (Mintz et al., 2009; Hoffmann et al., 2011).", "startOffset": 98, "endOffset": 141}, {"referenceID": 31, "context": "Curriculum learning for question answering was investigated in Sachan and Xing (2016), but they focused on the ordering of training examples while we interpolate two supervision signals.", "startOffset": 63, "endOffset": 86}, {"referenceID": 7, "context": "in natural language processing tasks such as coreference resolution (Clark and Manning, 2016), information extraction (Narasimhan et al.", "startOffset": 68, "endOffset": 93}, {"referenceID": 26, "context": "in natural language processing tasks such as coreference resolution (Clark and Manning, 2016), information extraction (Narasimhan et al., 2016), semantic parsing (Andreas et al.", "startOffset": 118, "endOffset": 143}, {"referenceID": 0, "context": ", 2016), semantic parsing (Andreas et al., 2016) and textual games (Narasimhan et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 25, "context": ", 2016) and textual games (Narasimhan et al., 2015; He et al., 2016).", "startOffset": 26, "endOffset": 68}, {"referenceID": 8, "context": ", 2016) and textual games (Narasimhan et al., 2015; He et al., 2016).", "startOffset": 26, "endOffset": 68}], "year": 2016, "abstractText": "Reading an article and answering questions about its content is a fundamental task for natural language understanding. While most successful neural approaches to this problem rely on recurrent neural networks (RNNs), training RNNs over long documents can be prohibitively slow. We present a novel framework for question answering that can efficiently scale to longer documents while maintaining or even improving performance. Our approach combines a coarse, inexpensive model for selecting one or more relevant sentences and a more expensive RNN that produces the answer from those sentences. A central challenge is the lack of intermediate supervision for the coarse model, which we address using reinforcement learning. Experiments demonstrate state-of-the-art performance on a challenging subset of the WIKIREADING dataset (Hewlett et al., 2016) and on a newly-gathered dataset, while reducing the number of sequential RNN steps by 88% against a standard sequence to sequence model.", "creator": "LaTeX with hyperref package"}}}