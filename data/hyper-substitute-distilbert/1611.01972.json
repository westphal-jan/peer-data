{"id": "1611.01972", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Fixed-point Factorized Networks", "abstract": "in previous implementation, deep bandwidth screening ( dnns ) sampling methods have achieving extreme efficiency in a growing constellation of tasks frequently automatically positioned among 100 most demanding and broadly used procedures in synthetic vision, speech recognition and natural language processing. however, dnn - testing methods must not computational - intensive and resource - consuming, which hinders the application of these opportunities on embedded screens excluding sim phones. to alleviate this obstacles, we study a novel fixed - point factorized networks ( eps ) on professional - trained models to guarantee extra computational limitation as significant as additional storage artifacts of networks. numerous experiments on large - depth imagenet classification task show apparent vulnerability of our proposed procedure.", "histories": [["v1", "Mon, 7 Nov 2016 10:26:41 GMT  (22kb)", "http://arxiv.org/abs/1611.01972v1", null], ["v2", "Tue, 29 Aug 2017 09:46:41 GMT  (122kb)", "http://arxiv.org/abs/1611.01972v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["peisong wang", "jian cheng"], "accepted": false, "id": "1611.01972"}, "pdf": {"name": "1611.01972.pdf", "metadata": {"source": "CRF", "title": "Fixed-point Factorized Networks", "authors": ["Peisong Wang", "Jian Cheng"], "emails": ["peisong.wang@nlpr.ia.ac.cn", "jcheng@nlpr.ia.ac.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n01 97\n2v 1\n[ cs\n.C V"}, {"heading": "1. Introduction", "text": "Deep neural networks (DNNs) have always been setting new state of the art performance in various fields including computer vision, speech recognition as well as natural language processing. Convolutional neural networks (CNNs), in particular, have outperformed traditional machine learning algorithms on computer vision tasks such as image recognition, object detection, semantic segmentation as well as gesture and action recognition. These breakthroughs are partially due to the added computational complexity and the storage footprint, which makes these models very hard to train as well as to deploy. For example, the Alexnet [10] involves 61M float-point parameters and 729M high precision multiply-accumulate operations\n(MACs). Current DNNs are usually trained offline by utilizing specialized hardwares like NVIDIA GPUs and CPU clusters. But such an amount of computation may be unaffordable for portable devices such as mobile phones, tablets and wearable devices, which usually have limited computing resources. What\u2019s more, the huge storage requirement and large memory accesses may hinder efficient haredware implementation of neural networks, like FPGAs and neural network oriented chips.\nThere have been many studies working on reducing the storage and the computational complexity of DNNs by quantizing the parameters of these models. Some of these works quantize the pretrained weights using several bits (usually 3\u223c12 bits) with a minimal loss of performance. However, in these kind of quantized networks one still needs to employ large numbers of multiply-accumulate operations. Others focuse on training these networks from scratch with binary (+1 and -1) or ternary (+1, 0 and -1) weights. These methods do not rely on pretrained models and may reduce the computations at both training stage as well as testing stage. But on the other hand, these methods could not make use of the pretrained models very efficiently due to the dramatic information loss during the binary or ternary quantization of these weights. Moreover, all these methods are highly constrained by the direct quantization scheme, for example, these methods must have exactely the same number of quantized weights as the full precision weights in each layer of the original networks. However, the redundances of deep neural networks differ from layer to layer, thus it is inefficient to use fixed numbers of weights for the quantized networks.\nIn this paper, we propose a unified framework based on fixed point factorization and full precision recovery of\nweight matrices to simultaneously accelerate and compress DNN models with only minor performance degradation. It is nontrivial to utilize fixed point factorization of weight matrices. One may argue to use full precision matrix decomposition like SVD, followed by fixed point quantization of the decomposed submatrices. However, this kind of method has two obvious shortcomings. Firstly, the matrix approximation is optimized for the full precision submatrices, but not for the fixed point representation, which is our real purpose. Moreover, the weight distributions after decomposition are quite complicate, which makes fixed point quantization more difficult. On the contrary, in this paper we propose to firstly factorize the weight matrix using fixedpoint (+1, 0 and -1) representation followed by recoverying the (pseudo) full precision submatrices. We also propose a effective and practical technique called weight balancing, which makes our fine-tuning much more stable. We demonstrate the effects of the fixed point factorization, full precision weights recovery, weight balancing and whole-model performance on ImageNet classification. The main contributions of this paper can be summarized as follows:\n\u2022 We propose the FFN framework for DNN acceleration and compression, which is more flexible and accurate and to our knowledge, is the first fixed point decomposition based quantization method in this area.\n\u2022 Based on fixed point factorization, we propose a novel full precision weight recovery method, which makes it possible to make full use of the pretrained models even for very deep architectures.\n\u2022 We investigate the weight imbalancing problem generally existing in matrix/tensor decomposition based DNN acceleration methods. Inspired by weights initialization methods, we present an effective weight balancing technique to stabilize the finetuning stage of DNN models."}, {"heading": "2. Related Work", "text": "CNN acceleration and compression is widely studied in recent years. We mainly list works that are closely related with ours, i.e., the low-rank based methods and fixed-point quantization based methods.\nDeep neural networks are usually over-parameterized and the redundancy can be removed as shown in the work of [3], which achieves 1.6\u00d7 speed-up for a single layer using biclustering and low-rank approximation of filter matrix. Since then, many low-rank based methods are proposed. Jaderberg [8] propose to use filter low-rank approximation and data reconstruction to lower the approximation error and achieve 2.5\u00d7 speed-up for scene text character recognition with no loss in accuracy. Zhang et al. [19] propose a novel nonlinear data reconstruction method, which\nallows asymmetric reconstruction to prevent error accumulation across layers. They achieves 3.8\u00d7 speed-up on VGG16 model with only a 0.3% increase of top-5 error in ImageNet [15] classification. Many work also explore the lowrank tensor decomposition method for neural network acceleration and compression. Lebedev et al. [11] propose to use CP-decomposition on 4D convolutional kernels using multiple rank-one tensors, which achieves 8.5\u00d7 speedup for character-classification CNN. Kim et al. [9] propose to use Tucker decomposition to speed up the execution of CNN models. They show great speed-up and energy reduction on smartphone which is equipped with a mobile GPU. Wang et al. [17] propose to accelerate the test-phase computation of CNNs based on low-rank and group sparse tensor decomposition. They achieve 6.6\u00d7 speed-up on VGG-16 model within 1% increased top-5 error.\nFixed-point quantization based method are also investigated by several recent works. Soudry et al. develop the Expectation Backpropagation (EBP) [1] method, which is a variational Bayes method to binarize both weights and neurons and achieves good results for fully connected networks on MNIST dataset. In the work of BinaryConnect (BC) [2], the authors propose to use binary weights for forward and backward computation while keep a full-precision version of weights for gradients accumulation. Great results have been achieved on small datasets like MNIST, CIFA-10 and SVHN. Binary-Weight-Network (BWN) [11] is proposed in a more recent work, which is among the first ones to evaluate the performance of binarization on large-scale datasets like ImageNet [15] and yields good results. These methods train neural networks from scratch and can barely benefit from pretrained networks. Hwang et al. [6] find a way by first quantize pretrained weights using a small number of weights, followed by retraining. However, their method relies on carefully choosing the step size of quantization using exhaustive search. And the scalability on large-scale datasets remains unclear.\nBesides these methods mentioned above, there have been other approaches on DNN acceleration. Han et al. [5] use network pruning to remove low-saliency parametera and small-weight connections to dramatically reduce parameter size. Wu et al. [18] propose to use product quantization for CNN compression and acceleration at the same time. FFT-based methods [12] are also investigated. Many works [14] utilize a smaller network but achieve comparable performance, which is another widely studied stream for DNN acceleration."}, {"heading": "3. Approaches", "text": "Our method exploits the weight matrix approximation method for deep neural network acceleration and compression. Unlike many previous low-rank based matrix decomposition methods which use float point values for the factor-\nized submatrixes, our method aims at fixed-point factorization directly.\nTo more efficiently make use of the pre-trained weights, we also introduce our novel psudo full-precision weight matrix recovery method in addition to the fix-point approximation. Thus the information of the pre-trained models is divided into two parts: the first one is the fixed-point factorized submatrices and the second one resides in the psudo full-precision weight matrices, which on the other hand, will transfor to the fixed-point weight matrices in the finetuning stage.\nMoreover, our weights balancing technique makes the fine-tuning more efficient and has an important role in our whole framework. We will discuss these three parts of our proposed framework thoughly in the following subsections."}, {"heading": "3.1. Fixed-point Factorization of Weight Matrices", "text": "A general deep neural network usually has multiple fully connected layers and / or convolutional layers. For the fully connected layers, the output signal vector so is computed as:\nso = \u03c6(Wsi + b) (1)\nwhere si is the input signal vector and W and b are the weight matrix and the bias term respectively. For a convolutional layer with n filters of size w \u00d7 h \u00d7 c where w, h and c are the kernel width and height and the number of input feature maps, if we reshape the kernel and the input volume at every spatial positions, the feedforward pass of the convolution can also be expressed by equation 1. Thus our decomposition is conducted on the weight matrix W .\nThere is a trivial solution to the fixed-point quantization combined with matrix decomposition, i.e., to firstly conduct full precision matrix decomposition like SVD, followed by fixed point quantization of the decomposed submatrices. However, this kind of method has two obvious shortcomings. Firstly, the matrix approximation is optimized for the full precision submatrices, but not for the fixed point representation, which is our real purpose. Moreover, the weight distributions after decomposition are quite complicate, which makes fixed point quantization more difficult.\nOn the contrary, in this subsection we propose to directly factorize the weight matrices into fixed-point format. More specifically, we want to approximate our weight matrix W \u2208 Rm\u00d7n as a weighted sum of outer products of several (k) vector pairs with only ternary (+1, 0 and -1) entries, which is refered to as the semidiscrete decomposition (SDD) in the following format:\nW \u2248\nk \u2211\ni\ndixiyi = XDY T (2)\nwhere X \u2208 {+1, 0,\u22121}m\u00d7k and Y \u2208 {+1, 0,\u22121}n\u00d7k and D \u2208 Rk\u00d7k is a diagonal matrix. Because of the ternary\nconstraints, the computation of SDD is a NP-hard problem. Kolda and O\u2019Leary propose to obtain an approximate local solution by greedily finding the best next dixiyi. To further reduce the approximation error of the decomposition, we develop an improved version of wheir algorithm as in Algorithm 1 by iteratively minimizing the residual error.\nAlgorithm 1: Improved SDD decomposition.\nInput: Matrix W \u2208 Rm\u00d7n, non-negative integer k; Output: X \u2208 {+1, 0,\u22121}m\u00d7k, Y \u2208 {+1, 0,\u22121}n\u00d7k, diagonal matrix D \u2208 Rk\u00d7k+ Initialization:\ndi \u2190 0 for i = 1, \u00b7 \u00b7 \u00b7 , k;\nSelect Y \u2208 {\u22121, 0, 1}n\u00d7k\nwhile not converge do\nfor i = 1, \u00b7 \u00b7 \u00b7 , k\n1. R \u2190 W \u2212 \u2211 j 6=i djxjy T j\n2. Set yi to the i-th column of Y\n3. while not converge\ni compute xi \u2208 {\u22121, 0, 1}m given yi and R\nii compute yi \u2208 {\u22121, 0, 1}n given xi and R\n4. end while\n5. Set di to the average of R \u25e6 xiyTi over the non-zero locations of xiyTi\n6. Set xi as the i-th column of X , yi the i-th column of Y and di the i-th diagonal value of D\nend for\nend while\nOnce the decomposition is done, we can replace the original weights W with the factorized ones, i.e., the X,Y and D. More formally, for convolutional layers, the original layer is replaced by three layers: the first one is a convolutional layer with k filters of size w \u00d7 h \u00d7 c, which are all with ternary values; The second layer is a \u201dchannel-wise scaling layer\u201d, i.e., each of the k feature maps is multiplied by a scaling factor; The last layer is another convolutional layer with n filters of size 1\u00d71\u00d7k, which also have ternary values."}, {"heading": "3.2. Full-precision Weights Recovery", "text": "Our fixed-point factorization method is much more accurate than direct binarization or ternarization method and many other fixed-point quantization method. But there is still the need of finetuning to restore the precision of DNN\nmodels. Like most of current fixed-point quantization based accelerating method, we want to use the quantized weights (the X,Y in our case) during the forward and backward propagation while use the full-precision weights for gradient accumulation. However, after factorization, we have lost the full-precision weights, i.e., we can not use the original W for gradient accumulation any longer. A simple solution is to use the float-point version of X and Y as fullprecision weights to accumulate gradients. But this is far from satisfactory.\nIn this subsection, we present our novel full-precision weights recovery method to make the fine-tuning stage much easier. Our motivation is very simple, we want to recovery the full-precision version of X and Y , indicated by X\u0302 and Y\u0302 , which can better approximateW , at the constraint that, after fixed-point quantization, X\u0302 and Y\u0302 turn to X and Y respectively. We can treat our full-precision weights recovery method as an inversion of current fixed-point quantization methods. In fixed-point quantization based DNN acceleration and compression methods, we quantize each element of the full-precision weight matrices into the nearest fixed-point format value. While in our method, we have got the fixed-point version of the weights, and we want to determine from which value the fixed-point element is quantized. We turn this problem into an optimization problem as follows:\nminimize X\u0302,Y\u0302\n\u2016 W \u2212 X\u0302DY\u0302 \u2016 2\nF\nsubject to |X\u0302 \u2212X | < 0.5\n|Y\u0302 \u2212 Y | < 0.5\n(3)\nHere the two constraints are introduced to ensure that the X\u0302 and Y\u0302 are quantized to X and Y . The problem can be efficiently solved by alternating method. Note we constraint X\u0302 and Y\u0302 to be always between -1.5 and 1.5 to alleviate overfitting and during fine-tuning, we also clip the weights as well.\nDuring fine-tuning stage, we quantize the full-precision weights of X\u0302 and Y\u0302 according to the following equation:\nq(Aij) =\n\n\n +1 0.5 < Aij < 1.5 0 \u22120.5 \u2264 Aij \u2264 0.5 \u22121 \u22121.5 < Aij < \u22120.5\n(4)\nThe quantized weights are used to conduct forward and backward computation and the full-precision weights X\u0302 and Y\u0302 are used to accumulate gradients during backpropagation. Both X an Y will change during fine-tuning because of the updates of X\u0302 and Y\u0302 , for example, some elements of X and Y will turn from 0 to 1 and so on. We use the recovered full-precision weights instead of the float-point versions of X and Y to accumulate gradients is based on the argument that, for example again, there is more chance for 0.499 to become 1 from 0 than for 0.001 to\nturn to 1 from 0. And these information reside in the fullprecision weight matrices and is transfored to the quantized weights during fine-tuning. Note that when the fine-tuning is done, we will discard the full-precision weighs and we only use the quantized weighs X and Y for prediction."}, {"heading": "3.3. Weight Balancing", "text": "So far, we have presented our fixed-point decomposition and full-precision weights recovery method to improve the test-phase efficiency of deep neural networks. However, there is still a problem to be considered, which we refered to as weight imbalance.\nWeight imbalance is a common problem of decomposition based methods, not just existing in our framework. This problem is caused by the non-uniqueness of the decomposition. Suppose we have a weight matrix W , which is factorized into the product of two matrices P and Q, i.e., W = PQ. Now we consider the partial derivatives of W with respect to P and Q:\n{\n\u2202W \u2202P\n= Q \u2202W \u2202Q = P (5)\nIf we let P \u2032 = 10 \u2217 P and Q\u2032 = 0.1 \u2217 Q, now the decomposition becoms W = P \u2032Q\u2032 and the partial derivatives becom:\n{\n\u2202W \u2202P \u2032 = Q\u2032 = 0.1 \u2217Q = 0.1 \u2217 \u2202W \u2202P \u2202W \u2202Q\u2032 = P \u2032 = 10 \u2217 P = 10 \u2217 \u2202W \u2202Q\n(6)\nFrom equation 6 we can see that, P is enlarged 10 times while the gradient is reduced to one-tenth of the original. And what happend to Q is opposite to P . The consequence is that during back-propagation,Q changes frequently while P almost stays untouched. At this time, one has to use different learning rate for each layers. However, this is quite a hard job especially for very deep neural networks.\nIn our framework, the weight matrix W \u2208 Rm\u00d7n is replaced by X\u0302DY\u0302 T , where the X\u0302 \u2208 Rm\u00d7k and Y\u0302 \u2208 Rn\u00d7k is in the range of [-1.5, 1.5] while D is at the scale of about 0.00001 to 0.01. And for convolutional layers, X\u0302 usually has much more elements than Y\u0302 because of the w \u00d7 h spatial size of filters in X\u0302 . To balance the weights into their appropriate scales, and inspired by the normalized weight initialization methhod proposed in [4], we develop the following weight balancing approaches:\nFirst, we want to find the scale factor \u03bbX and \u03bbY for X\u0302 and Y\u0302 , which are proportional to the square root of the number of their rows and colums.\nSecond, we try to make the balanced D close to identity matrix by setting the mean value of the elements along the diagonal to one. Because for fully-connected layer, D is a element-wise scaling factor and for convolutional layers, D is a channel-wise scaling factor. Thus making D close to one will not affect the calculation of gradients much.\nThis can be expressed by the equation 7 where X\u0303 , Y\u0303 and D\u0303 represent the balanced version of weight matrices.\n\n  \n   \nX\u0303 = \u03bbX \u2217 X\u0302 = \u03bb\u221a m+k \u2217 X\u0302 Y\u0303 = \u03bbY \u2217 Y\u0302 = \u03bb\u221a n+k \u2217 Y\u0302 D\u0303 = D \u03bbX\u2217\u03bbY mean(D\u0303) = 1\n(7)"}, {"heading": "4. Experiments", "text": "In this section, we comprehensively evaluate our method on ILSVRC-12 image classification benchmark, which has 1.2M training examples and 50k validation examples. We report the single-crop evaluation result using top-1 and top5 accuracy. Experiments are conducted on two mostly used CNN models, i.e., AlexNet and VGG-16. All of these models are downloaded from Berkeley\u2019s Caffe model zoo without any change and are used as a baseline for comparison."}, {"heading": "4.1. AlexNet", "text": "Alexnet was the first CNN architecture that showed to be successful on ImageNet classification task. This network has 61M parameters and more than 59% of them reside in the fully-connected layers. Thus we choose relatively smaller k\u2019s for fully-connected layers. Specifically, for the convolutional layers with 4-D weights of size w \u00d7 h \u00d7 c \u00d7 n, we choose k = min(w \u2217 h \u2217 c, n). We set k = 2048, 3072, 1000 for last three fully-connected layers. The resulting architecture has 60M parameters. At finetuning stage, images are resized to 256 \u00d7 256 pixel size as the same with original Alexnet.\nWe also compare our method with the following approaches, whose results on ImageNet dataset are available to us. The BWN method only report their results on AlexNet using batch normalization [7], so in order to compare with their results, we also report our results using batch normalization with the same settings as in BWN.\n\u2022 BWN: [11]: Binary-weight-network, using binary weights and float-point scaling factors;\n\u2022 BC: [2]: BinaryConnec, using binary weights, reproduced by [11];\n\u2022 LDR [13]: Logarithmic Data Representation, 4-bit logarithmic activation and 5-bit logarithmic weights.\nThe results are listed in Table 4.1. The suffix BN indicates that this method use batch normalization [7]. From the results, we can see that without batch normalization, our method only has a 1.4% decrease in top-5 accuracy. Our method can outperform the best results by a large margin of 2.2% top-5 accuracy if batch normalization is incoporated."}, {"heading": "4.2. VGG-16", "text": "VGG-16 [16] use much wider and deeper structure than AlexNet, having 13 convolutional layers and 3 fullyconnected layers. VGG-16 won the 2-nd place in ILSVRC 2014. We use the same rules to choose the k\u2019s and we also set k = 3138, 3072, 1000 for last three fully-connected layers, resulting in the same number of parameters with the original VGG-16 model. During fine-tuning, we resize images to have 256 pixels at their smaller dimension.\nFrom Table 4.2 we can see that after acceleration and compression, our method even outperform the original VGG-16 model by 0.1% top-5 accuracy."}, {"heading": "5. Conclusion", "text": "We introduce a novel fixed-point factorized method for deep neural networks acceleration and compression. To make full use of the pre-trained models, we propose a novel full-precision weight recovery method, which makes the fine-tuning more efficient and effective. More over, we present a weight balancing technique to ease the fine-tuning stage by making training stage more stable. Extensive experiments on AlexNet and VGG-16 show the effectiveness of our method."}], "references": [{"title": "Training binary multilayer neural networks for image classification using expectation backpropagation", "author": ["Z. Cheng", "D. Soudry", "Z. Mao", "Z. Lan"], "venue": "arXiv preprint arXiv:1503.03562", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "Advances in Neural Information Processing Systems, pages 3123\u20133131", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "N", "author": ["M. Denil", "B. Shakibi", "L. Dinh"], "venue": "de Freitas, et al. Predicting parameters in deep learning. In Advances in Neural Information Processing Systems, pages 2148\u20132156", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Journal of Machine Learning Research, 9:249\u2013256", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems, pages 1135\u20131143", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Fixed-point feedforward deep neural network design using weights+ 1", "author": ["K. Hwang", "W. Sung"], "venue": "0, and- 1. In 2014 IEEE Workshop on Signal Processing Systems (SiPS), pages 1\u20136. IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1405.3866", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y.-D. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "arXiv preprint arXiv:1511.06530", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Speeding-up convolutional neural networks using fine-tuned cp-decomposition", "author": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I. Oseledets", "V. Lempitsky"], "venue": "arXiv preprint arXiv:1412.6553", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast training of convolutional networks through ffts", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.5851", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural networks using logarithmic data representation", "author": ["D. Miyashita", "E.H. Lee", "B. Murmann"], "venue": "arXiv preprint arXiv:1603.01025", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.6550", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Accelerating convolutional neural networks for mobile applications", "author": ["P. Wang", "J. Cheng"], "venue": "Proceedings of the 2016 ACM on Multimedia Conference, pages 541\u2013545. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Quantized convolutional neural networks for mobile devices", "author": ["J. Wu", "C. Leng", "Y. Wang", "Q. Hu", "J. Cheng"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Accelerating very deep convolutional networks for classification and detection", "author": ["X. Zhang", "J. Zou", "K. He", "J. Sun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "For example, the Alexnet [10] involves 61M float-point parameters and 729M high precision multiply-accumulate operations (MACs).", "startOffset": 25, "endOffset": 29}, {"referenceID": 2, "context": "Deep neural networks are usually over-parameterized and the redundancy can be removed as shown in the work of [3], which achieves 1.", "startOffset": 110, "endOffset": 113}, {"referenceID": 7, "context": "Jaderberg [8] propose to use filter low-rank approximation and data reconstruction to lower the approximation error and achieve 2.", "startOffset": 10, "endOffset": 13}, {"referenceID": 18, "context": "[19] propose a novel nonlinear data reconstruction method, which allows asymmetric reconstruction to prevent error accumulation across layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "3% increase of top-5 error in ImageNet [15] classification.", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "[11] propose to use CP-decomposition on 4D convolutional kernels using multiple rank-one tensors, which achieves 8.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] propose to use Tucker decomposition to speed up the execution of CNN models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] propose to accelerate the test-phase computation of CNNs based on low-rank and group sparse tensor decomposition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "develop the Expectation Backpropagation (EBP) [1] method, which is a variational Bayes method to binarize both weights and neurons and achieves good results for fully connected networks on MNIST dataset.", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "In the work of BinaryConnect (BC) [2], the authors propose to use binary weights for forward and backward computation while keep a full-precision version of weights for gradients accumulation.", "startOffset": 34, "endOffset": 37}, {"referenceID": 10, "context": "Binary-Weight-Network (BWN) [11] is proposed in a more recent work, which is among the first ones to evaluate the performance of binarization on large-scale datasets like ImageNet [15] and yields good results.", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "Binary-Weight-Network (BWN) [11] is proposed in a more recent work, which is among the first ones to evaluate the performance of binarization on large-scale datasets like ImageNet [15] and yields good results.", "startOffset": 180, "endOffset": 184}, {"referenceID": 5, "context": "[6] find a way by first quantize pretrained weights using a small number of weights, followed by retraining.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] use network pruning to remove low-saliency parametera and small-weight connections to dramatically reduce parameter size.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] propose to use product quantization for CNN compression and acceleration at the same time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "FFT-based methods [12] are also investigated.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "Many works [14] utilize a smaller network but achieve comparable performance, which is another widely studied stream for DNN acceleration.", "startOffset": 11, "endOffset": 15}, {"referenceID": 3, "context": "To balance the weights into their appropriate scales, and inspired by the normalized weight initialization methhod proposed in [4], we develop the following weight balancing approaches: First, we want to find the scale factor \u03bbX and \u03bbY for X\u0302 and \u0176 , which are proportional to the square root of the number of their rows and colums.", "startOffset": 127, "endOffset": 130}, {"referenceID": 6, "context": "The BWN method only report their results on AlexNet using batch normalization [7], so in order to compare with their results, we also report our results using batch normalization with the same settings as in BWN.", "startOffset": 78, "endOffset": 81}, {"referenceID": 10, "context": "\u2022 BWN: [11]: Binary-weight-network, using binary weights and float-point scaling factors;", "startOffset": 7, "endOffset": 11}, {"referenceID": 1, "context": "\u2022 BC: [2]: BinaryConnec, using binary weights, reproduced by [11];", "startOffset": 6, "endOffset": 9}, {"referenceID": 10, "context": "\u2022 BC: [2]: BinaryConnec, using binary weights, reproduced by [11];", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "\u2022 LDR [13]: Logarithmic Data Representation, 4-bit logarithmic activation and 5-bit logarithmic weights.", "startOffset": 6, "endOffset": 10}, {"referenceID": 6, "context": "The suffix BN indicates that this method use batch normalization [7].", "startOffset": 65, "endOffset": 68}, {"referenceID": 9, "context": "AlexNet [10] 57.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "BC-BN [2] 35.", "startOffset": 6, "endOffset": 9}, {"referenceID": 10, "context": "BWN-BN [11] 56.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "4 LDR [13] 75.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "VGG-16 [16] use much wider and deeper structure than AlexNet, having 13 convolutional layers and 3 fullyconnected layers.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "VGG-16 [16] 71.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "LDR [13] 89.", "startOffset": 4, "endOffset": 8}], "year": 2016, "abstractText": "In recent years, Deep Neural Networks (DNN) based methods have achieved remarkable performance in a wide range of tasks and have been among the most powerful and widely used techniques in computer vision, speech recognition and Natural Language Processing. However, DNN-based methods are both computational-intensive and resource-consuming, which hinders the application of these methods on embedded systems like smart phones. To alleviate this problem, we introduce a novel Fixed-point Factorized Networks (FFN) on pre-trained models to reduce the computational complexity as well as the storage requirement of networks. Extensive experiments on large-scale ImageNet classification task show the effectiveness of our proposed method.", "creator": "LaTeX with hyperref package"}}}