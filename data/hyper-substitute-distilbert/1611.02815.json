{"id": "1611.02815", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "An Automated System for Essay Scoring of Online Exams in Arabic based on Stemming Techniques and Levenshtein Edit Operations", "abstract": "in this article, introducing automated system and featured for essay scoring in arabic language weekly online exams advising on stemming techniques and levenshtein distance controls. an sms exam has been developed through the previous methodology, exploiting the capabilities of fast and heavy tones. later implemented online voting system has shown of be an engaging tool enabling automated checking of course assignments.", "histories": [["v1", "Tue, 8 Nov 2016 15:25:02 GMT  (1129kb)", "http://arxiv.org/abs/1611.02815v1", "5 pages, 2 figures"]], "COMMENTS": "5 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["emad fawzi al-shalabi"], "accepted": false, "id": "1611.02815"}, "pdf": {"name": "1611.02815.pdf", "metadata": {"source": "CRF", "title": "An Automated System for Essay Scoring of Online Exams in Arabic based on Stemming Techniques and Levenshtein Edit Operations", "authors": ["Emad Fawzi Al-Shalabi"], "emails": [], "sections": [{"heading": null, "text": "In this article, an automated system is proposed for essay\nstemming techniques and Levenshtein edit operations. An online exam has been developed on the proposed mechanisms, exploiting the capabilities of light and heavy stemming. The implemented online grading system has shown to be an efficient tool for automated scoring of essay questions.\nKeywords: automated essay scoring, online exams, stemming, Levenshtein operations, edit distance, Arabic language, natural language processing"}, {"heading": "1. Introduction", "text": "Recently, automated grading systems have gained increasing attention due to their convenience over traditional grading methods. The rapidly growing reliance on technology in the educational field [1] and the increasing numbers of students; raised the need for an efficient scoring mechanism to fully replace the teacher\u2019s role in the scoring process, while saving time and guaranteeing fairness. The process of Automatic Scoring (AS) addresses the evaluation of a student\u2019s answer by performing a comparison with a model answer.\nSeveral types of tools and web-services have been developed with the purpose of performing automatic scoring of online exams with little or no user intervention. One of the easiest ways to implement automatic scoring is through the adoption of multiple choice (MC) exams [2], due to their nature and ease in scoring by a computer; MC exams have become widely used in online exams. Despite the presented advantages, MC question format has been criticized of unfairness, because they allow students to pick the correct answer based on chance, rendering it difficult to distinguish between a student who chose the correct answer based on exam preparation and the understanding of the presented problem and another who blindly guessed the answer. The case is also similar for true-false and matching question formats.\nOn the other hand, essay questions present far superior advantages over the previously mentioned question formats. Essay questions can reflect the depth of a student\u2019s knowledge and problem solving skills; they can also provide feedback for the instructor by shedding some light on the student\u2019s erroneous conclusions.\nAnother advantage of implementing AES systems is to remove the subjectivity in traditional scoring methods, where the instructor grades essay questions based on their own interpretation of a given answer. This as a result, ensures that a standardized basis for question scoring is being applied for all students alike. However, the implementation of Automatic Essay Scoring (AES) mechanisms is a rather difficult task in comparison with MC-based AS systems, this is mainly because essay answers a complicated process of text analysis.\nSeveral AES models have been developed since the 1960s, and due to the growing use of technology in the educational system in past decade, AES has become a very important area in the research field. However, the majority of available research is more concerned with automated scoring for English language essay questions. However, there is a lack of research when it comes to AES mechanisms for other languages such as Arabic, despite being a widely used language, which raises the important of investigating new mechanisms of automated scoring for essay question.\nIn this research, we propose a stemming-based mechanism for automatic essay scoring in Arabic language. This paper is organized as follows:\nSection II presents related works on automated essay scoring systems for Arabic, Section III introduces the problem statement explaining the challenges in Arabic language processing, in Section IV an automatic essay scoring mechanism is proposed, Section V presents the experimental work produced based on the proposed mechanism, Section VI shows the conclusions of the research and future work.\n2016 International Journal of Computer Science Issues"}, {"heading": "2. Related Works", "text": "One of the most commonly used AES models is presented in [3] other AES models are [2] and [1]\nHowever, it is highly difficult to implement the mentioned mechanisms for Arabic language, due to its complex nature, being highly inflectional and ambiguous in the absence of diacritics. There have been only few attempts in research on this subject, and so far none of them has been able to provide a fully functional auto-grading system.\nIn [4], the authors proposed an Arabic web-based examination system, where students can login using a username and password to take exams online, exam questions are stored along with correct answers on the server, answers are auto-graded by performing a comparison between the correct answer and the student\u2019s answer. However, the proposed system did not provide auto-grading for essay questions, answers to such questions are sent to the instructor for manual grading then passed back to the system, this long and complex process renders the system rather impractical, prone to error and misuse.\nText similarity techniques were used in [3] for the purpose of short answer auto-grading in Arabic language. The article presents an evaluation of the effect of combining corpusbased and string-based similarity measures. Feedback is also provided to students during the exam through comments that describe the answer\u2019s level of correctness. However, the system also requires human intervention and is not fully automated.\nIn [Khalid], automatic grading for online exams is proposed using statistical and computational linguistic techniques, where a variety of statistical distributions are employed to give weights to the words of the instructor\u2019s answer, utilizing human-computer interaction to benefit the grading system."}, {"heading": "3. Problem Statement", "text": "It is clear from the previous section, that there is a lack in the number of research concerned with automated grading of essay questions in Arabic language, the majority of available research available does not provide full implementation of the proposed techniques and often requires manual grading by an instructor at some point\nAn efficient automated grading system for essay questions in Arabic language should prove the possess the following features:\n Fully automated grading capabilities without the requirement of human intervention\n Low in computational complexity to allow for fast grading and lightweight implementation for web-\nservices complying the nature of online exams.\n Efficient handling of the various complex aspects in Arabic language.\nThis article aims to solve the problem in hand by developing an automated essay scoring mechanism that is both efficient and low in complexity for use in online web-based exams, without the requirement of manual grading."}, {"heading": "4. Automatic Essay Scoring Mechanism", "text": "In this article, stemming techniques were exploited for the purpose of auto-grading essay questions in online exams. A stemming algorithm may be defined as the procedure of reducing all words that share the same stem to a common form [5].\nThe proposed scoring system is divided into two algorithms; heavy stemming and light stemming, the general structure of the scoring system is show in figure (1) which explains the general mechanism of the system.\nEach question is loaded from the database and displayed along with a form for the student to fill in the answer, the student\u2019s answer is obtained from the form while the correct answer is retrieved from the database for comparison.\n4.1 Heavy Stemming Approach\nHeavy stemming, also referred to as root-based stemming begins with removing well-known prefixes and suffixes to extract the actual root of a word, the identifies the pattern in correspondence with the remaining word.\nThe auto-grading process is carried as follows:\n2016 International Journal of Computer Science Issues\nStep 1: Get both question and correct answer from the database. Step 2: Get the student\u2019s answer from the form.\nStep 3: Begin heavy stemming on both the student\u2019s answer and the correct answer using, this procedure involves three steps:\n1. Removal of numbers from both answers. 2. Removal of diacritics from both answers. 3. Removal of any letters from other languages.\nStep 4: Split each one of the two answers into an array of words, processing one word at a time as follows:\n1- loop through words of each answer and remove stop words , a list of stop words is\navailable in database ( \u064a\u0641 , \u0648 , \u0646\u0627 , \u0627\u0630\u0627 , \u0648\u0647 , \u0627\u0645\u0647 \u064a\u0647 ) 2- remove the (AL) , and its Derivatives , also available in database ( \u0644\u0627\u0628 , \u0644\u0644 , \u0644\u0627 , \u0644\u0627\u0641 , \u0644\u0627\u0628\u0644 , \u0644\u0627\u0628\u0648\n, \u0644\u0627\u0628\u0641 , \u0644\u0627\u062a , \u0644\u0627\u0648 ,\u0644\u0627\u0643 )\n3- Normalize words by replacing similar letters ( \u0623,\u0622, \u0625 with \u0627, \u0629 with \u0647 ) 4- Remove prefix if word length is greater than 3 , else skip this step 5- Remove suffix , if word length is greater than 3 , else skip this step , note that on the case of\nheavy stemming a different list of suffixes is provided\nStep 5: Find the similarities by giving a weight to each word in both answers.\nThis step requires finding the edit distance, which can be obtained following the two Eq.(1,2): 1- The edit distance, which is the minimum number of operations required to transform\none string into another.\n(1)\n2- The similarity equation:\n(2)\nWhere L is the length of a given string.\nStep 6: Set weight for each word by:\n(3)\nStep 7: For each word in student answer calculate the similarity with words in correct answer:\nA. If similarity between StudentWordi and CorrectWordi= 1 then add weight to the final\nmark.\n(4) B. Else, if the similarity between StudentWordi\nand CorrectWordi < 1 and >= 0.96, add weight to the final mark using Eq. (4). Note that if the similarity is greater than or equal to 0.96, then it is considered a correct word, this percentage can be changed by the instructor. C. Else, if the similarity between StudentWordi and CorrectWordi is < 0.96 and >= 0.80 then\nadd half the weight to the final mark. Note that if the similarity is less than 0.96 then it is considered an incomplete answer/word, this percentage can be decided by the instructor.\nD. Else, if the similarity between StudentWordi\nand CorrectWordi is < 0.80 then no weight is added to the final mark. Note that if the similarity is less than 0.80 then it is considered a wrong answer, this percentage can also be changed by the instructor.\nE. Display the final mark, that is, the sum of\nweights for each word in the student\u2019s answer.\nF. Move to next question, if there\u2019s one.\n4.2 Light Stemming Approach\nLight stemming is rather a less complex process, where the stemming is stopped upon the removal of prefixes and suffixes, without attempting to identify the actual root of the word.\nThe auto-grading process is carried as follows:\nStep 1: Get both question and correct answer from the database.\n2016 International Journal of Computer Science Issues\nStep 2: Get the student\u2019s answer from the form.\nStep 3: Begin the stemming process on both the correct answer and the student\u2019s answer as follows:\n1- Remove numbers from both answers. 2- Remove letters from other languages (i.e.\nEnglish).\nStep 4: Split each of the two answers into an array, processed one word at a time as follows:\n1- loop through words of each answer and remove stop words. A list of stop words is\navailable in the database ( \u064a\u0641 , \u0648 , \u0646\u0627 , \u0627\u0630\u0627 , \u0648\u0647 , \u064a\u0647\n, \u0627\u0645\u0647 )\n2- Remove the (AL), and its Derivatives available in the databse: ( \u0644\u0627\u0628 , \u0644\u0644 , \u0644\u0627 , \u0644\u0627\u0641 , \u0644\u0627\u0628\u0644 , \u0644\u0627\u0628\u0648 , \u0644\u0627\u0628\u0641 ,\n\u0644\u0627\u062a , \u0644\u0627\u0648 ,\u0644\u0627\u0643 )\n3- Normalize words by replacing similar letters.( \u0623,\u0622, \u0625 with \u0627), (\u0629 with \u0647). 4- Remove suffixes if word length is greater than 3. Else, skip this step.\nNote that in the case of light stemming, a different list of suffixes is provided, including 10 suffixes.\nStep 5: Finding similarity, this is done by giving each word in both answers a weight, which requires finding the edit distance between the two words using Eq. (1) and Eq. (2) respectively.\nA. Set the weight of each word using Eq. (3) B. For each word in student answer calculate the\nsimilarity with words in correct answer:\nC. If similarity between StudentWordi and CorrectWordi= 1 then add weight to the final\nmark using Eq. (4).\nD. Else, if the similarity between StudentWordi and CorrectWordi < 1 and >= 0.96, add weight\nto the final mark using Eq. (4).\nNote that if the similarity is greater than or equal to 0.96, then it is considered a correct word, this percentage can be changed by the instructor. E. Else, if the similarity between StudentWordi and CorrectWordi is < 0.96 and >= 0.8 then\nadd half the weight to the final mark. Note that if the similarity is less than 0.96 then it is considered an incomplete answer/word, this percentage can be decided by the instructor.\nF. Else, if the similarity between StudentWordi and CorrectWordi is less than 0.80 then no\nweight is added to the final mark. Note that if the similarity is less than 0.80 then it is considered a wrong answer, this percentage can also be changed by the instructor.\nG. Display the final mark, that is, the sum of\nweights for each word in the student\u2019s answer.\nH. Move to next question, if there\u2019s one."}, {"heading": "5. Experimental Work", "text": "A web-service has been developed based on the proposed scoring mechanisms, an online exam has been conducted to check the efficiency of both mechanisms, and following are two examples demonstrating the automated scoring process.\nExample 1:\nQuestion: \u064a\u0644\u0627\u062a\u0644\u0627 \u062a\u064a\u0628\u0644\u0627 \u0644\u0645\u0643\u0627 ...\u064a\u0646\u0646\u0625\u0641 \u064a\u0646\u0648\u0639\u0641\u0631\u0627 \u064a\u0628\u0627\u062d\u0635\u0644\u0623 \u0644\u0648\u0642\u0623\nCorrect Answer: \u064a\u0647\u0633 \u0646\u0623 \u064a\u0646\u064a\u0639\u0628 \u0631\u0642\u064a\u0627\u064a\u0644 \u0627\u062f\u0628 \u0644\nStudent Answer: \u0627\u062f\u0628 \u0644\u064a\u0647\u0633 \u0646\u0627 \u064a\u0646\u064a\u0639\u0628 \u0631\u0642\u064a\nSolution:\n1. Split each answer into words. 2. Apply normalization, remove stop words,\nprefixes and suffixes from both answers as long as wordi length > 3 3. Find similarity for each word in student answer, demonstrated word by word as\nfollows:\n4. Find weight per word = 1/number of words in correct answer = 1/5 = 0.2, this will be the\nweight for each word.\n5. The word \u0631\u0642\u064a from student answer, For each word in correct answer calculate the similarity, between \u0631\u0642\u064a from student answer and the one in correct answer is 1.\nD (\u0631\u0642\u064a, \u0631\u0642\u064a) = 0\nS (\u0631\u0642\u064a, \u0631\u0642\u064a) = 1 \u2013 0 / Max (Length (\u0631\u0642\u064a) , Length\n(\u0631\u0642\u064a))\nS (\u0631\u0642\u064a, \u0631\u0642\u064a) = 1 \u2013 0 / 3\nS (\u0631\u0642\u064a, \u0631\u0642\u064a) = 1\nSo MarkSum += weightof word\n2016 International Journal of Computer Science Issues\nMarkSum +=0.2\n6. The word \u0646\u064a\u0639 also have a similarity = 1 with \u0646\u064a\u0639 in correct answer. So MarkSum +=0.2, which means MarkSum now = 0.4 7. The word \u0644\u064a\u0647 also have a similar word which means MarkSum +=0.2, which means\nMarkSum now = 0.6\n8. The word \u0627\u062f\u0628 also have a similar word which means MarkSum +=0.2, which means\nMarkSum now = 0.8\n9. The word \u0627\u064a\u0644 in the correct answer have a weight a 0.2 but does not exist in student\nanswer so MarkSum +=0, which means MarkSum still as 0.8 10. Give the result depending on the mark sum that is the sum of all weights MarkSum = 0.8\nNOTE the conditions can be changed: if MarkSum = 1 then it\u2019s a full mark\nElse if MarkSum < 1 & >= 0.96 then it\u2019s also considered a correct answer.\nElse if MarkSum >= 0.75 & < 0.96 then the student answer might be correct must be checked\nElse if MarkSum is less than 0.75 then it\u2019s a wrong answer\nHere Since MarkSum = 0.8 then the student answer might be correct must be checked\nNote that here also both light and heavy scoring systems gave the same answer.\nExample 2 :\nQuestion: \u062f\u062d\u0623\u0631\u062f\u0642\u0644\u0644 \u0629\u064a\u0645\u0644\u0627\u0633\u0644\u0625\u0627 \u0629\u0631\u0638\u0646\u0644\u0627\u0628 \u0642\u0644\u0639\u062a\u064a \u0627\u0645\u064a\u0641 \u062d\u064a\u062d\u0635 \u0631\u064a\u063a \u0629\u064a\u062a\u0644\u0622\u0627\nCorrect answer: \u0644\u0645\u0639\u0644\u0627 \u0628\u062c\u0648\u064a \u0644\u0627 \u0646\u0627\u0645\u064a\u0644\u0625\u0627\nStudent Answer: \u0644\u0645\u0639\u0644\u0627 \u0628\u062c\u0648\u064a \u0646\u0627\u0645\u064a\u0644\u0627\u0627 \u0627\u0645\u064a\u0627\u062f\nSolution:\n1. Split each answer into words 2. Stop words removed, normalized, prefix and suffix\nremoved from both answers as long as word. length > 3\nCorrect answer: \u0644\u0645\u0639 \u0628\u062c\u0648\u064a \u0644\u0627 \u0645\u064a\u0627 Student answer: \u0644\u0645\u0639 \u0628\u062c\u0648\u064a \u0645\u064a\u0627 \u0627\u0645\u064a\u0627\u062f NOTE that any additional words in student answer is dropped, in this example the word \u0627\u0645\u064a\u0627\u062f has no effect\n3. Find similarity for each word in student answer, the process is demonstrated word by word, as follows: 4. find weight per word a 1/number of words in correct answer = 1/4 = 0.25, this will be the weight of each\nword\n5. The word \u0645\u064a\u0627 from student answer , that is \u0646\u0627\u0645\u064a\u0644\u0625\u0627 after removing \u0644\u0627 and suffix \u0646\u0627 6. For each word in correct answer calculate the similarity, the similarity between \u0645\u064a\u0627 from student answer and the\none in correct answer is 1.\nD ( \u0645\u064a\u0627 , \u0645\u064a\u0627 ) = 0 S ( \u0645\u064a\u0627 , \u0645\u064a\u0627 ) = 1 \u2013 0 / Max (Length ( \u0645\u064a\u0627 ) , Length ( \u0645\u064a\u0627 )) S ( \u0645\u064a\u0627 , \u0645\u064a\u0627 ) = 1 \u2013 0 / 3 S ( \u0645\u064a\u0627 , \u0645\u064a\u0627 ) = 1\nSo MarkSum += weightof word MarkSum +=0.25\n7. the word \u0628\u062c\u0648\u064a also have a similarity = 1 with \u0628\u062c\u0648\u064a in correct answer So MarkSum +=0.25 , which means\nMarkSum now = 0.50, the word \u0644\u0645\u0639 also have a similar word which means MarkSum +=0.25 , which means MarkSum now = 0.75.\n8. The word \u0644\u0627 in the correct answer have a weight = 0.25 but does not exist in student answer so MarkSum +=0 ,\nwhich means MarkSum still = 0.75\n9. give the result depending on the mark sum that is the sum of all weights\nMarkSum = 0.75\nNOTE the conditions can be changed :"}, {"heading": "If MarkSum = 1 then it\u2019s a full mark", "text": "Else if MarkSum < 1 & >= .96 then it\u2019s also considered a correct answer\nElse if MarkSum >=.75 & < 0.96 then the student answer might be correct must be checked\nElse if Else if MarkSum is less than 0.75 then it\u2019s a wrong answer.\nHere Since MarkSum = 0.75 then the student answer might be correct must be checked"}, {"heading": "6. Conclusion", "text": "In this article, an automated system for essay scoring of Arabic language was proposed. An online examination webservice has been implemented based of the proposed mechanism. Real-life tests of the implemented system have been conducted, and the proposed mechanisms have shown to be an efficient grading tool for essay questions in Arabic language.\n2016 International Journal of Computer Science Issues\nIn future, the proposed scoring system may be further developed to serve online mathematical exams, by extending the available mechanisms to include numbers and Latin symbols in the scoring process."}], "references": [{"title": "Automated essay scoring: A cross-disciplinary perspective", "author": ["Shermis", "Mark D", "Jill C. Burstein", "eds"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Automated essay scoring using generalized latent semantic analysis.", "author": ["Islam", "Md Monjurul", "ASM Latiful Hoque"], "venue": "Computer and Information Technology (ICCIT),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Automatic scoring for answers to Arabic test questions.", "author": ["Gomaa", "Wael Hassan", "Aly Aly Fahmy"], "venue": "Computer Speech & Language", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "An Arabic web-based exam management system.\"International", "author": ["Rashad", "Magdi Z"], "venue": "Journal of Electrical & Computer Sciences IJECS-IJENS", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Development of a stemming algorithm. Cambridge: MIT Information", "author": ["Lovins", "Julie B"], "venue": "Processing Group, Electronic Systems Laboratory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1968}, {"title": "Short answer grading using string similarity and corpus-based similarity.", "author": ["Gomaa", "Wael H", "Aly A. Fahmy"], "venue": "International Journal of Advanced Computer Science and Applications (IJACSA)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "A survey of text similarity approaches.", "author": ["Gomaa", "Wael H", "Aly A. Fahmy"], "venue": "International Journal of Computer Applications", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "An overview of automated scoring of essays.\" The Journal of Technology, Learning and Assessment", "author": ["Dikli", "Semire"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Methodological Approaches to Online Scoring of Essays.", "author": ["Chung", "Gregory KWK", "Harold F. O'Neil Jr."], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "C-rater: Automated scoring of short-answer questions.", "author": ["Leacock", "Claudia", "Martin Chodorow"], "venue": "Computers and the Humanities", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Automated assessment of students arabic free-text answers.", "author": ["Reafat", "M. M"], "venue": "Int J Cooperative Inform Syst", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "An efficient stemming for arabic text classification.\"Innovations in Information Technology", "author": ["Nehar", "Attia"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "A hybrid automatic scoring system for Arabic essays.", "author": ["Alghamdi", "Mansour"], "venue": "AI Communications", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Rational Kernels for Arabic Stemming and Text Classification.", "author": ["Nehar", "Attia", "Djelloul Ziadi", "Hadda Cherroun"], "venue": "arXiv preprint arXiv:1502.07504", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "An Arabic web-based exam management system.\"International", "author": ["Rashad", "Magdi Z"], "venue": "Journal of Electrical & Computer Sciences IJECS-IJENS", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Arabic Text Summarization Using Latent Semantic Analysis.\"  Author Emad Fawzi Al-shalabi received the B.S. degree in Information Technology, Management Information Systems from Philadelphia University , Jordan in 2004, and the MSc in Computer Information Systems from Yarmouk University (YU), Jordan", "author": ["Ba-Alwi", "Fadl Mutaher", "Ghaleb H. Gaphari", "Fares Nasser Al-Duqaimi"], "venue": "IJCSI International Journal of Computer Science Issues,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "The rapidly growing reliance on technology in the educational field [1] and the increasing numbers of students; raised the need for an efficient scoring mechanism to fully replace the teacher\u2019s role in the scoring process, while saving time and guaranteeing fairness.", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "One of the easiest ways to implement automatic scoring is through the adoption of multiple choice (MC) exams [2], due to their nature and ease in scoring by a computer; MC exams have become widely used in online exams.", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "One of the most commonly used AES models is presented in [3] other AES models are [2] and [1]", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "One of the most commonly used AES models is presented in [3] other AES models are [2] and [1]", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "One of the most commonly used AES models is presented in [3] other AES models are [2] and [1]", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "In [4], the authors proposed an Arabic web-based examination system, where students can login using a username and password to take exams online, exam questions are stored along with correct answers on the server, answers are auto-graded by performing a comparison between the correct answer and the student\u2019s answer.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "Text similarity techniques were used in [3] for the purpose of short answer auto-grading in Arabic language.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "A stemming algorithm may be defined as the procedure of reducing all words that share the same stem to a common form [5].", "startOffset": 117, "endOffset": 120}], "year": 2016, "abstractText": "In this article, an automated system is proposed for essay scoring in Arabic language for online exams based on stemming techniques and Levenshtein edit operations. An online exam has been developed on the proposed mechanisms, exploiting the capabilities of light and heavy stemming. The implemented online grading system has shown to be an efficient tool for automated scoring of essay questions.", "creator": "Adobe Acrobat Pro DC 15.0"}}}