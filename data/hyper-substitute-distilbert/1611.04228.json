{"id": "1611.04228", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Learning Sparse, Distributed Representations using the Hebbian Principle", "abstract": "applying \" error suppression, a reflex \" hebbian model is a severe limitation for engineering in gaming, after surprisingly, it has found limited relevance per modern spatial learning. in this paper, these take a first step to bridging this difficulty, by developing flavors of competitive game games which enhance explicit, secure approximation codes over online adaptation vs minimal constraint. we propose collaborative augmented algorithm, termed adaptive mirror learning ( ahl ). we emphasize the distributed nature of the learned representations via multiple parallel computations to synthetic transformations, and demonstrate superior sensitivity, compared to standard alternatives such as autoencoders, mapping images through deep neural net describing simultaneously simulated rendering.", "histories": [["v1", "Mon, 14 Nov 2016 02:28:13 GMT  (1453kb,D)", "http://arxiv.org/abs/1611.04228v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aseem wadhwa", "upamanyu madhow"], "accepted": false, "id": "1611.04228"}, "pdf": {"name": "1611.04228.pdf", "metadata": {"source": "CRF", "title": "Learning Sparse, Distributed Representations using the Hebbian Principle", "authors": ["Aseem Wadhwa", "Upamanyu Madhow"], "emails": ["aseem@ece.ucsb.edu", "madhow@ece.ucsb.edu"], "sections": [{"heading": "1 Introduction", "text": "Neuroscientific research has provided useful architectural insights for machine learning: for instance, multi-layered processing and convolutional architectures are directly inspired by the arrangement of cells in cortex. However, its contribution in developing practical learning mechanisms (e.g., weight adaptation) has been limited, despite the large body of neuroscientific research exploring development of synaptic weights. The \u201cfire together, wire together\u201d Hebbian principle [19], in which synaptic weights connecting a pair of highly activated pre- and post-synaptic neurons are strengthened, has been the centerpiece in several prominent neuroscientific models of learning developed over the decades, such as self-organizing maps [22] and adaptive resonance theory (ART models) [8]. More recently, it continues to be used in modeling the development of cortical maps [40, 5]. To date, however, these ideas have not been translated into practical machine learning algorithms, although there is renewed interest in this goal; for example, recent work in [13] argues for Hebbian learning as a universal principle for feature learning.\nPrior computational models for Hebbian learning show that, by combining it with components such as competition and inhibition, a variety of well-known representations can be obtained, such as clustering (K-means), Principal Component Analysis (PCA), and sparse coding [18, 35, 15]. These results show that Hebbian learning can yield results consistent with those from optimizing well known cost functions. Indeed, most prior work on unsupervised learning is based on minimization of an explicit cost function (e.g., energy in Boltzmann machines and reconstruction error in autoencoders) along with regularization. However, such loss functions are at best an imperfect proxy for obtaining desirable data representations, and are in any case subject to local minima.\nIn this paper, we take a drastically different approach: instead of optimizing a cost function, we augment Hebbian rules with neuro-plausible mechanisms that directly enforce core properties of the resulting neural code: sparsity, \u201cdistributed-ness\u201d and decorrelation. Several works, spanning both neuroscience [14, 15, 31] and machine learning [1] areas, have identified these core properties as indicators of efficient representations learned in the lower and middle layers of deep architectures.\nar X\niv :1\n61 1.\n04 22\n8v 1\n[ cs\n.L G\n] 1\n4 N\nov 2\nContributions and Organization: After discussing related work (Section 2), we present our algorithm, termed Adaptive Hebbian Learning (AHL), in Section 3. AHL features competition among post-synaptic (i.e., output) neurons as well as, optionally, among synapses associated with presynaptic (i.e., input) neurons. Using a synthetic dataset, we demonstrate that synaptic competition reduces correlation across output neurons, creating a sparse, distributed code, in terms of increasing output entropy while still allowing accurate reconstruction (Section 4). We then use AHL as a basis for unsupervised, bottom-up layer-wise learning in a standard deep convolutional net and demonstrate (Section 5) excellent classification results for standard image datasets (MNIST, NORB, CIFAR), using supervised training of a linear SVM as the final layer. Comparing with layered autoencoders, we find that AHL converges faster, and that the features it provides yield better classification performance for CIFAR, and comparable performance for MNIST and NORB.\nTo the best of our knowledge, the method presented here is the first successful attempt to translate the Hebbian principle into practical machine learning algorithms that give good results on standard datasets. While we use classification performance as an evaluation metric in comparing our scheme against other unsupervised approaches, we note that our goal here is not (yet) to compete with the fully supervised backpropagation-based deep networks that have been developed using many man-years of effort for these databases. Rather, our objective is to motivate further study on Hebbian architectures, by demonstrating their promise for unsupervised and (eventually) semi-supervised learning."}, {"heading": "2 Related Work", "text": "Much of the prior work in neuroscience on Hebbian computation has focused on front-end learning (e.g., of features like those observed in V1 simple cells) aimed at bridging the gap between theoretical and experimental studies of the visual cortex [40, 5], or on very simple recognition tasks [16]. Most of these models are complex, involving detailed spike timing or lateral connections, and hence are not well matched to machine learning applications, unlike the abstractions presented in this paper.\nAHL is similar to online clustering algorithms which have been studied extensively [39, 3, 10], such as fuzzy clustering and soft winner-take-all. However, the focus of this prior literature is on solving the conventional clustering problem, whereas our goal is to use AHL as a building block for feature extraction in a deep architecture. A single layer of K-means with a large number of features (\u223c 4000) was shown to be effective for feature extraction in [12]. Multi-layer K-means followed by SVM has also been used in [11, 2]. In contrast to the heavily correlated K-means centers, due to our introduction of synaptic competition we are able to produce sparse, distributed codes, similar to the empirically observed characteristics of high-performing supervised deep nets [1].\nExisting unsupervised learning approaches such as sparse coding [32] and autoencoders [37, 17] seek to optimize a cost function which combines reconstruction error with some form of regularization. In contrast, AHL does not have an explicit cost function. Rather, its mechanisms for weight update, neuron recruitment and pruning, are designed to directly promote sparsity and decorrelation.\nUnsupervised feature learning was originally employed for initializing deep nets prior to finetuning [6], but was soon observed not to offer substantial advantages over carefully scaled random initializations. Our goal here is to use classification performance to establish Hebbian learning as a competitive alternative to existing unsupervised strategies, rather than to compare against the highly optimized, purely supervised backprop-based deep nets. However, while the latter represent the current state of the art in classification, deep generative modeling continues to be an active area of research [21, 27], with the goal of reducing dependence on labels, and our results motivate further investigation into Hebbian algorithms as building blocks for semi-supervised learning."}, {"heading": "3 AHL: Unsupervised Hebbian Learning", "text": "Consider a single layer of d pre-synaptic neurons connected to K post-synaptic neurons, with activations x \u2208 Rd and y \u2208 RK respectively. Let wj \u2208 Rd denote the weights incident on neuron yj . Our goal is to learn the weight matrix WK\u00d7d. The most commonly used form of Hebbian principle for training the weights is given by:\nw\u2032j(t+ 1) = wj(t) + \u03b7xh(yj), where yj = f(w T j x) (1)\nwj(t+ 1) = w\u2032j(t+ 1)\u2225\u2225w\u2032j(t+ 1)\u2225\u22252 (2)\nwhere \u03b7 is a small constant (learning rate) and h(), f() could be linear or non-linear monotonic functions that causes an increase in the strength of wi,j when xi and yj are high, producing \u201cfire together, wire together\u201d behavior. Weight normalization (2) ensures that the weights do not explode while training, and implicitly creates competition between incoming synapses. In Hebbian literature, it is also a common practice to normalize the inputs, i.e. \u2016x\u20162 = 1. Replacing the plus with a minus in Eq. 1 results in an anti-Hebbian update, which produces inhibitory behavior. When h(z) = f(z) = z, all K weight vectors converge to the largest principal component of the training dataset {xi, i = 1, .., N} [30], while a simple modification produces convergence to the top K eigenvectors [35]. When h(z) = sign(z) and the weights afferent to only the highest activated neuron are updated, a strategy known as the WTA (winner take all), the resulting weights converge to cluster centers, and we get an online version of spherical K-means [18, 39]. Other modifications of Hebbian learning can lead to online solutions for other cost functions such as ICA, sparse coding etc [13], but that typically requires introduction of lateral connections, which slows down inference.\nWeights learnt in feedforward Hebbian architectures are either orthogonal (PCA) or highly correlated (K-means). The representations (i.e. the activation pattern y) they lead to do not display the following fundamental properties of neural codes, that are widely believed to be important for effective learning. \u2022 Sparse, Distributed Code: such a code arises when each post-synaptic neuron has a low probability to fire and the stimulus is forced to be encoded in the activity of a few neurons [14]. This kind of code is a compromise between local (compact code) and totally distributed representations (when there is a single grandmother active neuron) [15]. It has been argued that sparse distributed codes disentangle the causes leading to meaningful representations and also present a pattern that is easier for higher stages of the system to model [31]. Interestingly, such codes appear naturally in supervised deep nets, even though these constraints are not explicitly enforced. The work in [1] presents several empirical studies discovering the sparse and distributed nature of codes in the middle layers of a deep backpropagation trained network. \u2022 Decorrelated neurons: decorrelation between activities of post-synaptic neurons is important to generate naturally sparse representations that are concise and cover the input space efficiently. Decorrelation forces neurons to learn different features and is an important component in most neuroscientific models [15, 40, 5]. However, decorrelation is different from orthogonality: neurons still need to capture the \u201csuspicious coincidences\u201d that define objects [4]. For example, neurons representing concepts such as \u201cwhite\u201d, \u201cfurry\u201d, \u201chas tail\u201d, which would all fire when say a patch of cat is presented as stimuli, will have correlated activity patterns.\nAdaptive Hebbian Learning (AHL) builds on the WTA Hebbian update rule, with h(z) = sign(z) and a max rectified non-linearity for f(). It combines this framework with several ideas, including competition between outgoing weights from a pre-synaptic neuron, and adaptive creation and culling of neurons, to obtain sparse, distributed representations. The key features of AHL are as follows: (1) Bias: The biases are set such that the average activity of each neuron is maintained at a fixed low level Abias (e.g., Abias = 0.2 results in average sparsity of 80%). This approach has been adopted in several papers building biologically plausible models [40, 5] which conform to the observation that neurons tend to have low mean firing rates that span a small range of values. Sparse autoencoders [17] also include a penalty term for the mean firing rate of the neurons. (2) Decorrelation via adaptive adding and pruning of neurons: A new neuron is recruited when an input pattern is \u2018far away\u2019 from the existing neurons (as measured by cosine similarity) and is not well represented thus far (as measured by the summation of total activity generated by it). An existing neuron is pruned when it is too highly correlated with other neurons. This simple scheme gradually grows the number of post-synaptic neurons (starting from 0) , depending on the representational power needed to model the input data. It is worth noting that the idea of recruiting neurons adaptively was proposed in Grossberg\u2019s ART2 model [7] decades ago, but our system architecture and the specifics of neuron recruitment, as well as the inclusion of pruning, is different. (3) Synaptic competition: To further increase the distributed nature of the code, we allow for a soft-WTA strategy in which we update the weights of the top Kw \u2265 2 winners. However, this can increase the correlation between neurons. To counter this, we introduce competition between the outgoing weights from a pre-synaptic neuron: not all weights for the Kw neurons are updated, but only those that are stronger. For instance, for top two winners yj and yk, if wij > wik then only wij (the connection from xi to yj) undergoes the Hebbian update. Such weight competition has been\nAlgorithm 1 Adaptive Hebbian Learning (AHL) INPUT: X(N \u00d7 d) {rows normalized} OUTPUT: K,W(K \u00d7 d),b(K \u00d7 1) PARAMETERS: \u03b7(= 10\u22122): learning rate Abias(= 0.2): average activity level of neurons AT (\u223c 0.5\u2212 5), \u03c1T (= 0.6): control adding of neurons \u03c1U (= 0.8): prune if correlation above this Kw(\u223c 1\u2212 3): control level of competition E(\u223c 15): epochs INITIALIZE: W = X(1, :); b = 0; r = Abias; C = 0; e = 0 for loop = 1, ..., E do\nfor n = 1, 2, ..., N do x = X(n, :) a = max(0,WxT \u2212 b) ADD NEURON: if \u2211K\ni=1 ai < AT & max Wx T < \u03c1T :\nW = [W;x]; b = [b; 0]; r = [r;Abias]; C = [C 0; 0 0]; e = [e; 0] UPDATE WEIGHTS: U \u2190 { set of indices of top Kw values in a } for j = 1, ..., d do wj,max = max\nk\u2208U {wjk}, wj,min = min k\u2208U {wjk}\nif xj > 0 \u2192 \u2200k \u2208 U & wjk \u2265 0.9wj,max : wjk = wjk + \u03b7xj if xj < 0 \u2192 \u2200k \u2208 U & wjk \u2264 0.9wj,min : wjk = wjk + \u03b7xj\nNormalize each row W(k, :) \u2200k \u2208 U UPDATE BIAS: r = 0.99r+ .01sign(a); b = b+ .01(r\u2212Abias) PRUNE: for j, k = 1, ..,K (k > j) do C(j, k) = 0.9999C(j, k) + .0001ajak for j = 1, ..,K do ej = 0.9999ej + .0001a2j if mod(n, 5000) = 0 \u2192 Cn(j, k) = C(j,k)\u221aej\u221aek , \u2200 j, k > j = 1, ..,K while j\u2217, k\u2217=argmaxCn(j, k) & Cn(j\u2217, k\u2217)>\u03c1U \u2192 remove k\u2217 neuron\nobserved in biological studies [34][36]. In addition to producing a more distributed code, it also leads to increased sparsity in the weights, often seen in supervised deep nets, and also recently reported for marginalized denoising autoencoders in [9]. For Kw=1, AHL is similar to spherical K-means, but differs because of the bias, and adaptive adding and pruning.\nThe pseudo code (Matlab based) for the AHL algorithm is provided in Algorithm 1. Vectors are denoted in bold small letters and matrices in bold large letters.\nCode details and Parameters: A moving average filter is used for updating the bias and also for recording the normalized correlation of activities between neurons. In our simulations, normalization of weights is performed using a weight decay term that is added to the Hebbian update (we omitted it from the pseudocode for easier readability). The decay term can be derived in a manner similar to the one discussed in [30], it is useful for improving the computation speed. Note that pruning is done in a greedy fashion, starting with the highest correlated pair. The adding criterion using a total activity threshold is something for which we do not have a theoretical basis. However in practice it seems to be a useful heuristic for capturing how well a given input is explained by the current set of neurons, given that they are not highly correlated to each other. In our simulations we have observed that the weight values converge typically in 10-20 epochs. We set the number E to 15 in all our simulations.\nWhile AHL has a number of parameters, their settings are intuitive and typically represent a direct property (e.g., maximum correlation allowed, mean firing rate etc). Indeed, the values shown in the beginning of the pseudocode are used across datasets and layers, except for AT , which is varied in order to obtain a varying number of neurons across layers. The learning rate is also not changed, since we normalize the input data at each layer. Thus, AHL is easy to tune: we get excellent performance\nfor the classification task, for example, without requiring fine-tuning via cross-validation. However, it would be interesting to follow up this work with detailed ablation studies to capture the effect of each parameter in AHL."}, {"heading": "4 Synthetic data: Activation Code Structure", "text": "Before reporting on AHL within a deep net operating on complex image data, we first derive fundamental insight into the kinds of codes it generates using synthetic data. We use L2 reconstruction error as a measure of information preservation (even though AHL is not optimized for an L2 cost function), and show that the performance is similar to, or better than, that of standard clustering. We use output entropy as a measure of representation power and distributed-ness, and show that AHL is significantly better than clustering.\nSince our inputs are normalized, we draw the synthetic data from a mixture of von Mises-Fisher (vMF) distributions, where vMF is the analog of a Gaussian distribution on the hypersphere (e.g., the spherical K-means algorithm can be derived by assuming a generating density which is a mixture of vMF [3]). The vMF pdf on a (d \u2212 1)-dimensional hypersphere in Rd is given by f(x;\u00b5;\u03ba) = Zd(\u03ba)exp(\u03bax\nT\u00b5), where \u03ba \u2265 0 and \u2016\u00b5\u20162 = 1, and the normalization constant Zd(\u03ba) = (\u03ba d/2\u22121)/((2\u03c0)d/2Id/2\u22121(\u03ba)).\nOur synthetic data {xi\u2208R30, i=1, .., 5000} is generated using a mixture of five vMF distributions, each generating 1000 IID samples in R30 using the sampling procedure described in [20]. The concentration parameter \u03ba plays the role of inverse variance. We generate our data with \u03ba = 50(low), 100, 150(high). We create a few instances of the dataset, and for each, apply the AHL algorithm with Kw=1 (WTA) and Kw=2. We set \u03c1T = 0.8, AT = 1, \u03b7 = 0.1, E = 7. Pruning is switched off, because we wish to observe the correlation between cluster centers. For comparison, we also run standard batch spherical K-means algorithm (SPKM) [39], initialized with centers randomly drawn from the dataset. For each instantiation of the dataset, the number of clusters are set to be the same as the output of AHL (we denote the two cases by SPKM 1 and SPKM 2 for Kw=1, Kw=2 respectively). Once the weights/cluster centers are learnt, the activation code for each datapoint is given by yK\u00d71 = max(0, Cx\u2212 b), where CK\u00d730 is the collection of cluster centers. The bias b is chosen such that each cluster is active on average half the time (i.e. a mean firing rate of Abias = 0.5).\nWe develop quantitative insight into the codes produced by AHL using the following measures: Reconstruction error: For standard clustering, this equals the distortion between the inputs and the nearest cluster centers ( \u2211 i \u2016xi \u2212 cclosest\u20162). This does not work for a distributed code, hence we consider the error (1 \u2212 xT x\u0302) between a datapoint and its best estimate given its activation code (x\u0302i|yi). x\u0302 can be obtained as the solution to the following optimization problem:\nx\u0302 = argmin x\n\u2225\u2225yI1 \u2212 (CI1x\u2212 bI1)\u2225\u22252 2 ; subject to CI0x \u2264 bI0 , \u2016x\u20162 = 1 (3)\nwhere the set of indices where y is zero and non-zero are denoted by I0 and I1 respectively. While this is non-convex due to the L2 constraint, we are able to obtain good solutions using \u201cconvex-concave\u201d sequence convex programming (SCP) [38]. This involves solving a sequence of convex programs by replacing the second constraint by two affine constraints xTx \u2264 1 and x T prevx\n\u2016xprev\u2016 \u2265 0.99. We initialize by solving the problem without the L2 constraint, and then normalizing. We find that 2-4 iterations suffice for convergence. Output Entropy: To evaluate this, we perform binary quantization of the activation codes, which gives a codebook of size 2K for K output neurons. We then compute the empirical pmf of the 2K possible binary codewords activated by the data points. Weight correlation: We compute the KC2 correlations between all pairs of weight vectors and sort them.\nNumerical results are reported in Fig. 1(b) and 1(a). For each value of \u03ba, we average over 5 runs (different random directions for the vMF mixture). We clearly see the higher representational power of AHL: the entropies of both the AHL (Kw = 1, 2) algorithms are higher than their respective counterparts, SPKM 1 and 2, while having similar reconstruction errors. In particular, for Kw=2, we recruit a slightly larger number of neurons, but the codes are far more distributed (higher entropy) while giving slightly better reconstruction error at low \u03ba. AHL also results in more decorrelated weights, as seen in Fig. 1(b), where correlations are plotted for a single run that had 14 centers."}, {"heading": "5 Unsupervised Image Feature Extraction", "text": ""}, {"heading": "5.1 Architecture", "text": "We use a standard CNN architecture as shown in Fig. 2. There are 3 convolutional layers interspersed with max-pooling layers. Hebbian learning is used to train the weights and biases of these layers. Layers are sequentially learnt from 1 to 3 in an unsupervised manner. Training data at each layer comprises of N (typically 100\u2212 200K) normalized activations randomly sampled from the lower layer. The 3 intermediary feature maps (Fig. 2) are used (via average pooling and concatenation) to construct the final features. To study the quality of representations, these features are used to train a linear SVM for classification. At the first layer, raw image patches are processed as described in [12]."}, {"heading": "5.2 Parameters", "text": "Most of the AHL parameters are set to the typical values shown in 1, without optimizing across datasets and layers. The only parameter that is varied is AT , which acts a proxy for K, the number of hidden units: for fixed \u03c1T , increasing AT results in increased K (although K saturates after a point due to the \u03c1U pruning constraint). In our experiments, we choose AT so as to get gradually increasing number of features for higher layers (in the order of a few hundreds), and so that Ki are comparable for the two cases we simulate: WTA (Kw=1) and soft-WTA (Kw=3). The filter and pooling sizes depend on the dataset, and are specified next."}, {"heading": "5.3 Datasets", "text": "MNIST [24] is a 10-digit database with 28x28 binary images. We use filter sizes {f1, f2, f3} = {7, 4, 2} and 2\u00d7 2 max-pooling. The size of the receptive field of a neuron onto the raw image is\ncalculated by back-projecting its receptive field to the layer below, and so on, until we reach layer 0 (the input image). In this case, the sizes are 7\u00d7 7, 14\u00d7 14 and 20\u00d7 20 pixels, respectively, for neurons in hidden layers 1, 2 and 3. One simple way to \u201cvisualize a neuron\u201d is to display the patches that activate it the most. In Fig. 3(a) we show such top-5 activating patches for randomly picked neurons in layers 1, 2 and 3. It is intuitively pleasing to see neurons sensitive to simple edges in layer 1, and to combination of edges in higher layers. CIFAR [23] is a dataset of tiny 32\u00d7 32 color images belonging to 10 classes (e.g., dogs, frogs, ships). The filter sizes used are same as MNIST, also shown in the Fig. 2. In Fig. 3(a) we note that in addition to edges, color sensitive neurons are also learnt. NORB [25], uniform-normalized, is a synthetic dataset of 5 classes of toys photographed under varying lighting and azimuth conditions. The original dataset has 96\u00d796 pixels large grayscale binocular images (hence K0 = 2). However a large part of the margins are plain background, hence for faster processing we prune the images to 80\u00d7 80. We use filter sizes {f1, f2, f3} = {11, 4, 3} and 3\u00d7 3 max-pooling which give receptive fields of sizes 11\u00d7 11, 22\u00d7 22, 46\u00d7 46 pixels of raw images patches."}, {"heading": "5.4 Results", "text": "The distribution of number of active units in the codes, as shown in Fig. 3(b), is what is expected from sparse distributed codes. Classification error rates are reported in Table 1. We note that AHL gives better results than spherical K-means (SPKM). We note that Kw =3 performs consistently better than Kw=1, which shows that more distributed representations indeed help.\nWe replace AHL by an autoencoder in each layer in order to compare against both denoising autoencoders (DAE) [37] (hyperparameters: batch size, noise level, learning rate) and sparse autoencoders (SAE) [29, 17] (hyperparameters: weight decay, sparsity penalty coefficient, target activation). The number of hidden units is set to be comparable to the ones reported by the AHL runs. Autoencoder training generally takes much longer, as two layers are trained concurrently, and it was observed that several iterations (of the order of few hundreds) need to be run to get good performance. For the results reported in Table 1, autoencoder cost function was minimized using 500 iterations of the quasi-newton L-BFGS method (Matlab minFunc package) and 15 epochs were used for AHL. The time taken, for example, in CIFAR layer 2, was 12.3 and 74.8 minutes respectively for AHL and sparse AE (SPKM took about 3 minutes with 5 different randomly initialized runs). This also\nmakes the process of hyperparameter cross-validation for autoencoders very time consuming, which seems to be crucial to get good performance. Although classification performance is not a decisive measure of performance for unsupervised schemes, we found that AHL, while faster and with minimal requirements of tuning, performs consistently better than DAE (which is therefore omitted from Table 1) and comparable or better than SAE, for the CNN architecture considered here. We note that SAE features yield slightly better performance for NORB when concatenating all three layers, but performance with CIFAR is much worse.\nThe error rates we obtain are comparable to some of the best rates reported in literature using unsupervised features for classification without model averaging and data augmentation, such as, MNIST: 0.82% using deep belief networks [26], 0.64% [33] using unsupervised features fed to a supervised two layer NN, NORB: 2.52% [2] using finer pooling (over 5x5 regions instead of quadrants), CIFAR: 80% [12], 82% [11], 80.1% [28] accuracy, using large number of feature maps (> 4k). Our focus in this work is not to finely tune or over build the networks to beat state of the art benchmarks, but to highlight a fast method which requires minimal tuning and performs well with reasonable network size. We note that our performance improves further by incorporating some of the techniques reported in earlier work, giving error rates of 2\u2212 3% for NORB by finer pooling as in [2], and accuracy of up to 80% for CIFAR by increasing K1 as in [12].\nNote that classification accuracies deteriorate considerably when using layer 3 alone. This points out the importance of incorporating labels while training higher layers for classification tasks, so that more discriminative features could be learned. A promising idea that we are currently exploring is to combine Hebbian and anti-Hebbian mechanisms to generate class-specific neurons. Results will be reported in future publications."}, {"heading": "6 Conclusions", "text": "We have demonstrated that the Hebbian principle, with appropriate competitive mechanisms, provides a powerful basis for designing unsupervised learning algorithms. The AHL algorithm presented here is a radical departure from prior approaches: instead of trying to minimize a cost function, AHL is able to directly target desirable properties such as sparsity and decorrelation using neuroplausible mechanisms. The training complexity of AHL is less than that of autoencoders, while the features obtained perform better in the experiments considered here. To the best of our knowledge, this is the first paper in which learning abstractions firmly grounded in neuroscientific models have been demonstrated to be competitive with modern machine learning techniques. We hope that the promising results reported here stimulate further investigation into bridging the gap between neuroscience and machine learning, with the goal of enhancing our understanding in each area.\nAs with any unsupervised approach, as we go up the layers, there is a tendency to \u201cwaste\u201d neurons on modeling features that are not ultimately informative (e.g., if classification is our end goal, modeling different types of backgrounds may not be useful). An exciting area for future work is to explore semi-supervised Hebbian learning techniques which can exploit high volumes of unlabeled data, while employing small amounts of labeled data to prune such task-agnostic neurons."}], "references": [{"title": "Analyzing the performance of multilayer neural networks for object recognition", "author": ["P. Agrawal", "R. Girshick", "J. Malik"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A framework for machine vision based on neuro-mimetic front end processing and clustering", "author": ["E. Akbas", "A. Wadhwa", "M. Eckstein", "U. Madhow"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres", "author": ["A. Banerjee", "J. Ghosh"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Cerebral cortex as model builder", "author": ["H. Barlow"], "venue": "In Matters of Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1987}, {"title": "Hebbian learning of the statistical and geometrical structure of visual input", "author": ["J.A. Bednar"], "venue": "In Neuromathematics of Vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in neural information processing systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Art 2: Self-organization of stable category recognition codes for analog input patterns", "author": ["G.A. Carpenter", "S. Grossberg"], "venue": "Applied optics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1987}, {"title": "The art of adaptive pattern recognition by a self-organizing neural network", "author": ["G.A. Carpenter", "S. Grossberg"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1988}, {"title": "Marginalized denoising auto-encoders for nonlinear representations", "author": ["M. Chen", "K.Q. Weinberger", "F. Sha", "Y. Bengio"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Fuzzy competitive learning", "author": ["F.L. Chung", "T. Lee"], "venue": "Neural Networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Nonlinear hebbian learning as a universal principle in unsupervised feature learning", "author": ["C. De Brito", "W. Gerstner"], "venue": "In Deep Learning Workshop ICML\u201915,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "What is the goal of sensory coding", "author": ["D.J. Field"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Forming sparse representations by local anti-hebbian learning", "author": ["P. F\u00f6ldiak"], "venue": "Biological cybernetics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological cybernetics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1980}, {"title": "Measuring invariances in deep networks. In Advances in neural information processing", "author": ["I. Goodfellow", "H. Lee", "Q.V. Le", "A. Saxe", "A.Y. Ng"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Adaptive pattern classification and universal recoding: I. parallel development and coding of neural feature detectors", "author": ["S. Grossberg"], "venue": "Biological cybernetics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1976}, {"title": "The organisation of behaviour: a neuropsychological theory", "author": ["D.O. Hebb"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1952}, {"title": "movmf: An r package for fitting mixtures of von mises-fisher distributions", "author": ["K. Hornik", "B. Gr\u00fcn"], "venue": "http://CRAN.R-project.org/package=movMF,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "S. Mohamed", "D.J. Rezende", "M. Welling"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Self-organized formation of topologically correct feature maps", "author": ["T. Kohonen"], "venue": "Biological cybernetics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1982}, {"title": "Learning multiple layers of features from tiny", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F.J. Huang", "L. Bottou"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Auxiliary deep generative models", "author": ["L. Maal\u00f8e", "C.K. S\u00f8nderby", "S.K. S\u00f8nderby", "O. Winther"], "venue": "arXiv preprint arXiv:1602.05473,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "A winner-take-all method for training sparse convolutional autoencoders", "author": ["A. Makhzani", "B. Frey"], "venue": "arXiv preprint arXiv:1409.2752,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Sparse autoencoder", "author": ["A. Ng"], "venue": "CS294A Lecture notes,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Simplified neuron model as a principal component analyzer", "author": ["E. Oja"], "venue": "Journal of mathematical biology,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1982}, {"title": "Principles of image representation in visual cortex", "author": ["B.A. Olshausen"], "venue": "The visual neurosciences,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2003}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1996}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M.A. Ranzato", "F.J. Huang", "Y.-L. Boureau", "Y. LeCun"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Development of the vertebrate neuromuscular junction", "author": ["J.R. Sanes", "J.W. Lichtman"], "venue": "Annual review of neuroscience,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1999}, {"title": "Optimal unsupervised learning in a single-layer linear feedforward neural network", "author": ["T.D. Sanger"], "venue": "Neural networks,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1989}, {"title": "Competitive hebbian learning through spike-timingdependent synaptic plasticity", "author": ["S. Song", "K.D. Miller", "L.F. Abbott"], "venue": "Nature neuroscience,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2000}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}, {"title": "The concave-convex procedure", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Neural computation,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "Efficient online spherical k-means clustering", "author": ["S. Zhong"], "venue": "In Neural Networks,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "A sparse coding model with synaptically local plasticity and spiking neurons can account for the diverse shapes of v1 simple cell receptive fields", "author": ["J. Zylberberg", "J.T. Murphy", "M.R. DeWeese"], "venue": "PLoS Comput Biol,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}], "referenceMentions": [{"referenceID": 18, "context": "The \u201cfire together, wire together\u201d Hebbian principle [19], in which synaptic weights connecting a pair of highly activated pre- and post-synaptic neurons are strengthened, has been the centerpiece in several prominent neuroscientific models of learning developed over the decades, such as self-organizing maps [22] and adaptive resonance theory (ART models) [8].", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "The \u201cfire together, wire together\u201d Hebbian principle [19], in which synaptic weights connecting a pair of highly activated pre- and post-synaptic neurons are strengthened, has been the centerpiece in several prominent neuroscientific models of learning developed over the decades, such as self-organizing maps [22] and adaptive resonance theory (ART models) [8].", "startOffset": 310, "endOffset": 314}, {"referenceID": 7, "context": "The \u201cfire together, wire together\u201d Hebbian principle [19], in which synaptic weights connecting a pair of highly activated pre- and post-synaptic neurons are strengthened, has been the centerpiece in several prominent neuroscientific models of learning developed over the decades, such as self-organizing maps [22] and adaptive resonance theory (ART models) [8].", "startOffset": 358, "endOffset": 361}, {"referenceID": 39, "context": "More recently, it continues to be used in modeling the development of cortical maps [40, 5].", "startOffset": 84, "endOffset": 91}, {"referenceID": 4, "context": "More recently, it continues to be used in modeling the development of cortical maps [40, 5].", "startOffset": 84, "endOffset": 91}, {"referenceID": 12, "context": "To date, however, these ideas have not been translated into practical machine learning algorithms, although there is renewed interest in this goal; for example, recent work in [13] argues for Hebbian learning as a universal principle for feature learning.", "startOffset": 176, "endOffset": 180}, {"referenceID": 17, "context": "Prior computational models for Hebbian learning show that, by combining it with components such as competition and inhibition, a variety of well-known representations can be obtained, such as clustering (K-means), Principal Component Analysis (PCA), and sparse coding [18, 35, 15].", "startOffset": 268, "endOffset": 280}, {"referenceID": 34, "context": "Prior computational models for Hebbian learning show that, by combining it with components such as competition and inhibition, a variety of well-known representations can be obtained, such as clustering (K-means), Principal Component Analysis (PCA), and sparse coding [18, 35, 15].", "startOffset": 268, "endOffset": 280}, {"referenceID": 14, "context": "Prior computational models for Hebbian learning show that, by combining it with components such as competition and inhibition, a variety of well-known representations can be obtained, such as clustering (K-means), Principal Component Analysis (PCA), and sparse coding [18, 35, 15].", "startOffset": 268, "endOffset": 280}, {"referenceID": 13, "context": "Several works, spanning both neuroscience [14, 15, 31] and machine learning [1] areas, have identified these core properties as indicators of efficient representations learned in the lower and middle layers of deep architectures.", "startOffset": 42, "endOffset": 54}, {"referenceID": 14, "context": "Several works, spanning both neuroscience [14, 15, 31] and machine learning [1] areas, have identified these core properties as indicators of efficient representations learned in the lower and middle layers of deep architectures.", "startOffset": 42, "endOffset": 54}, {"referenceID": 30, "context": "Several works, spanning both neuroscience [14, 15, 31] and machine learning [1] areas, have identified these core properties as indicators of efficient representations learned in the lower and middle layers of deep architectures.", "startOffset": 42, "endOffset": 54}, {"referenceID": 0, "context": "Several works, spanning both neuroscience [14, 15, 31] and machine learning [1] areas, have identified these core properties as indicators of efficient representations learned in the lower and middle layers of deep architectures.", "startOffset": 76, "endOffset": 79}, {"referenceID": 39, "context": ", of features like those observed in V1 simple cells) aimed at bridging the gap between theoretical and experimental studies of the visual cortex [40, 5], or on very simple recognition tasks [16].", "startOffset": 146, "endOffset": 153}, {"referenceID": 4, "context": ", of features like those observed in V1 simple cells) aimed at bridging the gap between theoretical and experimental studies of the visual cortex [40, 5], or on very simple recognition tasks [16].", "startOffset": 146, "endOffset": 153}, {"referenceID": 15, "context": ", of features like those observed in V1 simple cells) aimed at bridging the gap between theoretical and experimental studies of the visual cortex [40, 5], or on very simple recognition tasks [16].", "startOffset": 191, "endOffset": 195}, {"referenceID": 38, "context": "AHL is similar to online clustering algorithms which have been studied extensively [39, 3, 10], such as fuzzy clustering and soft winner-take-all.", "startOffset": 83, "endOffset": 94}, {"referenceID": 2, "context": "AHL is similar to online clustering algorithms which have been studied extensively [39, 3, 10], such as fuzzy clustering and soft winner-take-all.", "startOffset": 83, "endOffset": 94}, {"referenceID": 9, "context": "AHL is similar to online clustering algorithms which have been studied extensively [39, 3, 10], such as fuzzy clustering and soft winner-take-all.", "startOffset": 83, "endOffset": 94}, {"referenceID": 11, "context": "A single layer of K-means with a large number of features (\u223c 4000) was shown to be effective for feature extraction in [12].", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": "Multi-layer K-means followed by SVM has also been used in [11, 2].", "startOffset": 58, "endOffset": 65}, {"referenceID": 1, "context": "Multi-layer K-means followed by SVM has also been used in [11, 2].", "startOffset": 58, "endOffset": 65}, {"referenceID": 0, "context": "In contrast to the heavily correlated K-means centers, due to our introduction of synaptic competition we are able to produce sparse, distributed codes, similar to the empirically observed characteristics of high-performing supervised deep nets [1].", "startOffset": 245, "endOffset": 248}, {"referenceID": 31, "context": "Existing unsupervised learning approaches such as sparse coding [32] and autoencoders [37, 17] seek to optimize a cost function which combines reconstruction error with some form of regularization.", "startOffset": 64, "endOffset": 68}, {"referenceID": 36, "context": "Existing unsupervised learning approaches such as sparse coding [32] and autoencoders [37, 17] seek to optimize a cost function which combines reconstruction error with some form of regularization.", "startOffset": 86, "endOffset": 94}, {"referenceID": 16, "context": "Existing unsupervised learning approaches such as sparse coding [32] and autoencoders [37, 17] seek to optimize a cost function which combines reconstruction error with some form of regularization.", "startOffset": 86, "endOffset": 94}, {"referenceID": 5, "context": "Unsupervised feature learning was originally employed for initializing deep nets prior to finetuning [6], but was soon observed not to offer substantial advantages over carefully scaled random initializations.", "startOffset": 101, "endOffset": 104}, {"referenceID": 20, "context": "However, while the latter represent the current state of the art in classification, deep generative modeling continues to be an active area of research [21, 27], with the goal of reducing dependence on labels, and our results motivate further investigation into Hebbian algorithms as building blocks for semi-supervised learning.", "startOffset": 152, "endOffset": 160}, {"referenceID": 26, "context": "However, while the latter represent the current state of the art in classification, deep generative modeling continues to be an active area of research [21, 27], with the goal of reducing dependence on labels, and our results motivate further investigation into Hebbian algorithms as building blocks for semi-supervised learning.", "startOffset": 152, "endOffset": 160}, {"referenceID": 29, "context": ", N} [30], while a simple modification produces convergence to the top K eigenvectors [35].", "startOffset": 5, "endOffset": 9}, {"referenceID": 34, "context": ", N} [30], while a simple modification produces convergence to the top K eigenvectors [35].", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "When h(z) = sign(z) and the weights afferent to only the highest activated neuron are updated, a strategy known as the WTA (winner take all), the resulting weights converge to cluster centers, and we get an online version of spherical K-means [18, 39].", "startOffset": 243, "endOffset": 251}, {"referenceID": 38, "context": "When h(z) = sign(z) and the weights afferent to only the highest activated neuron are updated, a strategy known as the WTA (winner take all), the resulting weights converge to cluster centers, and we get an online version of spherical K-means [18, 39].", "startOffset": 243, "endOffset": 251}, {"referenceID": 12, "context": "Other modifications of Hebbian learning can lead to online solutions for other cost functions such as ICA, sparse coding etc [13], but that typically requires introduction of lateral connections, which slows down inference.", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "\u2022 Sparse, Distributed Code: such a code arises when each post-synaptic neuron has a low probability to fire and the stimulus is forced to be encoded in the activity of a few neurons [14].", "startOffset": 182, "endOffset": 186}, {"referenceID": 14, "context": "This kind of code is a compromise between local (compact code) and totally distributed representations (when there is a single grandmother active neuron) [15].", "startOffset": 154, "endOffset": 158}, {"referenceID": 30, "context": "It has been argued that sparse distributed codes disentangle the causes leading to meaningful representations and also present a pattern that is easier for higher stages of the system to model [31].", "startOffset": 193, "endOffset": 197}, {"referenceID": 0, "context": "The work in [1] presents several empirical studies discovering the sparse and distributed nature of codes in the middle layers of a deep backpropagation trained network.", "startOffset": 12, "endOffset": 15}, {"referenceID": 14, "context": "Decorrelation forces neurons to learn different features and is an important component in most neuroscientific models [15, 40, 5].", "startOffset": 118, "endOffset": 129}, {"referenceID": 39, "context": "Decorrelation forces neurons to learn different features and is an important component in most neuroscientific models [15, 40, 5].", "startOffset": 118, "endOffset": 129}, {"referenceID": 4, "context": "Decorrelation forces neurons to learn different features and is an important component in most neuroscientific models [15, 40, 5].", "startOffset": 118, "endOffset": 129}, {"referenceID": 3, "context": "However, decorrelation is different from orthogonality: neurons still need to capture the \u201csuspicious coincidences\u201d that define objects [4].", "startOffset": 136, "endOffset": 139}, {"referenceID": 39, "context": "This approach has been adopted in several papers building biologically plausible models [40, 5] which conform to the observation that neurons tend to have low mean firing rates that span a small range of values.", "startOffset": 88, "endOffset": 95}, {"referenceID": 4, "context": "This approach has been adopted in several papers building biologically plausible models [40, 5] which conform to the observation that neurons tend to have low mean firing rates that span a small range of values.", "startOffset": 88, "endOffset": 95}, {"referenceID": 16, "context": "Sparse autoencoders [17] also include a penalty term for the mean firing rate of the neurons.", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "It is worth noting that the idea of recruiting neurons adaptively was proposed in Grossberg\u2019s ART2 model [7] decades ago, but our system architecture and the specifics of neuron recruitment, as well as the inclusion of pruning, is different.", "startOffset": 105, "endOffset": 108}, {"referenceID": 33, "context": "observed in biological studies [34][36].", "startOffset": 31, "endOffset": 35}, {"referenceID": 35, "context": "observed in biological studies [34][36].", "startOffset": 35, "endOffset": 39}, {"referenceID": 8, "context": "In addition to producing a more distributed code, it also leads to increased sparsity in the weights, often seen in supervised deep nets, and also recently reported for marginalized denoising autoencoders in [9].", "startOffset": 208, "endOffset": 211}, {"referenceID": 29, "context": "The decay term can be derived in a manner similar to the one discussed in [30], it is useful for improving the computation speed.", "startOffset": 74, "endOffset": 78}, {"referenceID": 2, "context": ", the spherical K-means algorithm can be derived by assuming a generating density which is a mixture of vMF [3]).", "startOffset": 108, "endOffset": 111}, {"referenceID": 19, "context": ", 5000} is generated using a mixture of five vMF distributions, each generating 1000 IID samples in R using the sampling procedure described in [20].", "startOffset": 144, "endOffset": 148}, {"referenceID": 38, "context": "For comparison, we also run standard batch spherical K-means algorithm (SPKM) [39], initialized with centers randomly drawn from the dataset.", "startOffset": 78, "endOffset": 82}, {"referenceID": 37, "context": "While this is non-convex due to the L2 constraint, we are able to obtain good solutions using \u201cconvex-concave\u201d sequence convex programming (SCP) [38].", "startOffset": 145, "endOffset": 149}, {"referenceID": 11, "context": "At the first layer, raw image patches are processed as described in [12].", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "MNIST [24] is a 10-digit database with 28x28 binary images.", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "CIFAR [23] is a dataset of tiny 32\u00d7 32 color images belonging to 10 classes (e.", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": "NORB [25], uniform-normalized, is a synthetic dataset of 5 classes of toys photographed under varying lighting and azimuth conditions.", "startOffset": 5, "endOffset": 9}, {"referenceID": 36, "context": "We replace AHL by an autoencoder in each layer in order to compare against both denoising autoencoders (DAE) [37] (hyperparameters: batch size, noise level, learning rate) and sparse autoencoders (SAE) [29, 17] (hyperparameters: weight decay, sparsity penalty coefficient, target activation).", "startOffset": 109, "endOffset": 113}, {"referenceID": 28, "context": "We replace AHL by an autoencoder in each layer in order to compare against both denoising autoencoders (DAE) [37] (hyperparameters: batch size, noise level, learning rate) and sparse autoencoders (SAE) [29, 17] (hyperparameters: weight decay, sparsity penalty coefficient, target activation).", "startOffset": 202, "endOffset": 210}, {"referenceID": 16, "context": "We replace AHL by an autoencoder in each layer in order to compare against both denoising autoencoders (DAE) [37] (hyperparameters: batch size, noise level, learning rate) and sparse autoencoders (SAE) [29, 17] (hyperparameters: weight decay, sparsity penalty coefficient, target activation).", "startOffset": 202, "endOffset": 210}, {"referenceID": 25, "context": "82% using deep belief networks [26], 0.", "startOffset": 31, "endOffset": 35}, {"referenceID": 32, "context": "64% [33] using unsupervised features fed to a supervised two layer NN, NORB: 2.", "startOffset": 4, "endOffset": 8}, {"referenceID": 1, "context": "52% [2] using finer pooling (over 5x5 regions instead of quadrants), CIFAR: 80% [12], 82% [11], 80.", "startOffset": 4, "endOffset": 7}, {"referenceID": 11, "context": "52% [2] using finer pooling (over 5x5 regions instead of quadrants), CIFAR: 80% [12], 82% [11], 80.", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "52% [2] using finer pooling (over 5x5 regions instead of quadrants), CIFAR: 80% [12], 82% [11], 80.", "startOffset": 90, "endOffset": 94}, {"referenceID": 27, "context": "1% [28] accuracy, using large number of feature maps (> 4k).", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "We note that our performance improves further by incorporating some of the techniques reported in earlier work, giving error rates of 2\u2212 3% for NORB by finer pooling as in [2], and accuracy of up to 80% for CIFAR by increasing K1 as in [12].", "startOffset": 172, "endOffset": 175}, {"referenceID": 11, "context": "We note that our performance improves further by incorporating some of the techniques reported in earlier work, giving error rates of 2\u2212 3% for NORB by finer pooling as in [2], and accuracy of up to 80% for CIFAR by increasing K1 as in [12].", "startOffset": 236, "endOffset": 240}], "year": 2016, "abstractText": "The \u201cfire together, wire together\u201d Hebbian model is a central principle for learning in neuroscience, but surprisingly, it has found limited applicability in modern machine learning. In this paper, we take a first step towards bridging this gap, by developing flavors of competitive Hebbian learning which produce sparse, distributed neural codes using online adaptation with minimal tuning. We propose an unsupervised algorithm, termed Adaptive Hebbian Learning (AHL). We illustrate the distributed nature of the learned representations via output entropy computations for synthetic data, and demonstrate superior performance, compared to standard alternatives such as autoencoders, in training a deep convolutional net on standard image datasets.", "creator": "LaTeX with hyperref package"}}}