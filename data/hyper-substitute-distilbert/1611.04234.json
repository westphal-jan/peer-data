{"id": "1611.04234", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media", "abstract": "we add centrally named priority domains ( ip ) unlike chinese social media. with normally unlabeled text and wide limited labelled access, we have a semi - exponential learning model based on b - lstm hierarchical network. integrating take advantage of traditional methods in consideration such how computational, we treat transition values as finite variation in our model. thus bridge the gap reducing sensor accuracy and f - measure of satisfaction, we construct a network which can be directly evaluated on data - signal. when combining the instability of strong - score input programs and meaningful processes provided with label accuracy, we propose an integrated method which improving on both f - score \u2010 label accuracy. our default model records 7. 44 \\ % improvement showing recent state - penetrating - the - environment result.", "histories": [["v1", "Mon, 14 Nov 2016 02:50:33 GMT  (23kb)", "https://arxiv.org/abs/1611.04234v1", null], ["v2", "Tue, 11 Apr 2017 10:57:34 GMT  (24kb)", "http://arxiv.org/abs/1611.04234v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hangfeng he", "xu sun"], "accepted": false, "id": "1611.04234"}, "pdf": {"name": "1611.04234.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["xusun}@pku.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n04 23\n4v 2\n[ cs\n.C L\n] 1\n1 A\npr 2\n01 7\n(NER) for Chinese social media. With massive unlabeled text and quite limited labelled corpus, we propose a semisupervised learning model based on BLSTM neural network. To take advantage of traditional methods in NER such as CRF, we combine transition probability with deep learning in our model. To bridge the gap between label accuracy and F-score of NER, we construct a model which can be directly trained on F-score. When considering the instability of Fscore driven method and meaningful information provided by label accuracy, we propose an integrated method to train on both F-score and label accuracy. Our integrated model yields substantial improvement over previous state-of-the-art result."}, {"heading": "1 Introduction", "text": "With the development of Internet, social media plays an important role in information exchange. The natural language processing tasks on social media are more challenging which draw attention of many researchers (Li and Liu, 2015; Habib and van Keulen, 2015; Radford et al., 2015; Cherry and Guo, 2015). As the foundation of many downstream applications (Weissenborn et al., 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text. NER is a task to identify names in texts and to assign names with particular types (Sun et al., 2009; Sun, 2014; Sun et al., 2014; He and Sun, 2017). It is the informality of social media that discourages accuracy of NER\nsystems. While efforts in English have narrowed the gap between social media and formal domains (Cherry and Guo, 2015), the task in Chinese remains challenging. It is caused by Chinese logographic characters which lack many clues to indicate whether a word is a name, such as capitalization. The scant labelled Chinese social media corpus makes the task more challenging (Neelakantan and Collins, 2015; Skeppstedt, 2014; Liu et al., 2015).\nTo address the problem, one approach is to use the lexical embeddings learnt from massive unlabeled text. To take better advantage of unlabeled text, Peng and Dredze (2015) evaluates three types of embeddings for Chinese text, and shows the effectiveness of positional character embeddings with experiments. Considering the value of word segmentation in Chinese NER, another approach is to construct an integrated model to jointly train learned representations for both predicting word segmentations and NER (Peng and Dredze, 2016).\nHowever, the two above approaches are implemented within CRF model. We construct a semisupervised model based on B-LSTM neural network to learn from the limited labelled corpus by using lexical information provided by massive unlabeled text. To shrink the gap between label accuracy and F-Score, we propose a method to directly train on F-Score rather than label accuracy in our model. In addition, we propose an integrated method to train on both F-Score and label accuracy. Specifically, we make contributions as follows:\n\u2022 We propose a method to directly train on FScore rather than label accuracy. In addition,\nwe propose an integrated method to train on both F-Score and label accuracy.\n\u2022 We combine transition probability into our BLSTM based max margin neural network to\nform structured output in neural network.\n\u2022 We evaluate two methods to use lexical embeddings from unlabeled text in neural net-\nwork."}, {"heading": "2 Model", "text": "We construct a semi-supervised model which is based on B-LSTM neural network and combine transition probability to form structured output. We propose a method to train directly on F-Score in our model. In addition, we propose an integrated method to train on both F-Score and label accuracy."}, {"heading": "2.1 Transition Probability", "text": "B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient (Hammerton, 2003; Hochreiter and Schmidhuber, 1997; Chen et al., 2015; Graves et al., 2006). However, B-LSTM cannot learn sentence level label information. Huang et al. (2015) combine CRF to use sentence level label information. We combine transition probability into our model to gain sentence level label information. To combine transition probability into B-LSTM neural network, we construct a Max Margin Neural Network (MMNN) (Pei et al., 2014) based on B-LSTM. The prediction of label in position t is given as:\nyt = softmax(Why \u2217 ht + by) (1)\nwhere Why are the transformation parameters, ht the hidden vector and by the bias parameter. For a input sentence c[1:n] with a label sequence l[1:n], a sentence-level score is then given as:\ns(c[1:n], l[1:n], \u03b8) =\nn\u2211\nt=1\n(Alt\u22121lt + f\u039b(lt|c[1:n]))\nwheref\u039b(lt|c[1:n]) indicates the probability of label lt at position t by the network with parameters \u039b, A indicates the matrix of transition probability. In our model, f\u039b(lt|c[1:n]) is computed as:\nf\u039b(lt|c[1:n]) = \u2212log(yt[lt]) (2)\nWe define a structured margin loss\u2206(l, l) as Pei et al. (2014):\n\u2206(l, l) =\nn\u2211\nj=1\n\u03ba1{lj 6= lj} (3)\nwhere n is the length of setence x, \u03ba is a discount parameter, l a given correct label sequence and l a predicted label sequence. For a given training instance (xi, yi), our predicted label sequence is the label sequence with highest score:\nl\u2217i = argmax li\u2208Y (xi) s(xi, li, \u03b8)\nThe label sequence with the highest score can be obtained by carrying out viterbi algorithm. The regularized objective function is as follows:\nJ(\u03b8) = 1\nm\nm\u2211\ni=1\nqi(\u03b8) + \u03bb\n2 ||\u03b8||2 (4)\nqi(\u03b8) = max li\u2208Y (xi) (s(xi, li, \u03b8)+\u2206(li, li))\u2212s(xi, li, \u03b8)\nBy minimizing the object, we can increase the score of correct label sequence l and decrease the score of incorrect label sequence l."}, {"heading": "2.2 F-Score Driven Training Method", "text": "Max Margin training method use structured margin loss \u2206(l, l) to describe the difference between the corrected label sequence l and predicted label sequence l. In fact, the structured margin loss \u2206(l, l) reflect the loss in label accuracy. Considering the gap between label accuracy and F-Score in NER, we introduce a new training method to train directly on F-Score. To introduce F-Score driven training method, we need to take a look at the subgradient of equation (4):\n\u2202J \u2202\u03b8 = 1 m\nm\u2211\ni=1\n( \u2202s(x, lmax, \u03b8)\n\u2202\u03b8 \u2212\n\u2202s(x, l, \u03b8)\n\u2202\u03b8 ) + \u03bb\u03b8\nIn the subgradient, we can know that structured margin loss \u2206(l, l) contributes nothing to the subgradient of the regularized objective function J(\u03b8). The margin loss \u2206(l, l) serves as a trigger function to conduct the training process of BLSTM based MMNN. We can introduce a new trigger function to guide the training process of neural network. F-Score Trigger Function The main criterion of NER task is F-score. However, high label accuracy does not mean high F-score. For instance, if every named entity\u2019s last character is labeledas O, the label accuracy can be quite high, but the precision, recall and F-score are 0. We use the FScore between corrected label sequence and predicted label sequence as trigger function, which\nBLSTM BLSTM\u2212MMNN Proposal I Proposal II 45\n50\n55\n60\n65\nModel\nF 1\u2212\nS co\nre NAM NOM Overall\n(a) F-Score of the models.\nBLSTM BLSTM\u2212MMNN Proposal I Proposal II 0\n100\n200\n300\n400\n500\n600\nModel\nT im\ne pe\nr ite\nra tio\nn/ s\n(b) Running time of the models.\n0.1 0.2 0.3 0.4 0.5 45\n50\n55\nO ve\nra ll\nF 1\u2212\nsc or\ne\n(c) Overall F1-Score with different values of beta.\ncan conduct the training process to optimize the F-Score of training examples. Our new structured margin loss can be described as:\n\u2206\u0303(l, l) = \u03ba \u2217 FScore (5)\nwhere FScore is the F-Score between corrected label sequence and predicted label sequence. F-Score and Label Accuracy Trigger Function The F-Score can be quite unstable in some situation. For instance, if there is no named entity in a sentence, F-Score will be always 0 regardless of the predicted label sequence. To take advantage of meaningful information provided by label accuracy, we introduce an integrated trigger function as follows:\n\u2206\u0302(l, l) = \u2206\u0303(l, l) + \u03b2 \u2217\u2206(l, l) (6)\nwhere \u03b2 is a factor to adjust the weight of label accuracy and F-Score.\nBecause F-Score depends on the whole label sequence, we use beam search to find k label sequences with top sentece-level score s(x, l, \u03b8) and then use trigger function to rerank the k label sequences and select the best."}, {"heading": "2.3 Word Segmentation Representation", "text": "Word segmentation takes an important part in Chinese text processing. Both Peng and Dredze (2015) and Peng and Dredze (2016) show the value of word segmentation to Chinese NER in social media. We present two methods to use word segmentation information in neural network model. Character and Position Embeddings To incorporate word segmentation information, we attach every character with its positional tag. This method is to distinguish the same character at different position in the word. We need to word segment the text and learn positional character embeddings from the segmented text.\nCharacter Embeddings and Word Segmentation Features We can treat word segmentation as discrete features in neural network model. The discrete features can be easily incorporated into neural network model (Collobert et al., 2011). We use word embeddings from a LSTM pretrained on MSRA 2006 corpus to initialize the word segmentation features."}, {"heading": "3 Experiments and Analysis", "text": ""}, {"heading": "3.1 Datasets", "text": "We use a modified labelled corpus1 as Peng and Dredze (2016) for NER in Chinese social media. Details of the data are listed in Table 1. We also use the same unlabelled text as Peng and Dredze (2016) from Sina Weibo service in China and the text is word segmented by a Chinese word segmentation system Jieba2 as Peng and Dredze (2016) so that our results are more comparable to theirs."}, {"heading": "3.2 Parameter Estimation", "text": "We pre-trained embeddings using word2vec (Mikolov et al., 2013) with the skip-gram training model, without negative sampling and other default parameter settings. Like Mao et al. (2008), we use bigram features as follow:\nCnCn+1(n = \u22122,\u22121, 0, 1) and C\u22121C1\n1We fix some labeling errors of the data. 2https://github.com/fxsjy/jieba.\nWe use window approach (Collobert et al., 2011) to extract higher level Features from word feature vectors. We treat bigram features as discrete features (Collobert et al., 2011) for our neural network. Our models are trained using stochastic gradient descent with an L2 regularizer. As for parameters in our models, window size for word embedding is 5, word embedding dimension, feature embedding dimension and hidden vector dimension are all 100, discount \u03ba in margin loss is 0.2, and the hyper parameter for the L2 is 0.000001. As for learning rate, initial learning rate is 0.1 with a decay rate 0.95. For integrated model, \u03b2 is 0.2. We train 20 epochs and choose the best prediction for test."}, {"heading": "3.3 Results and Analysis", "text": "We evaluate two methods to incorporate word segmentation information. The results of two methods are shown as Table 2. We can see that positional character embeddings perform better in neural network. This is probably because positional character embeddings method can learn word segmentation information from unlabeled text while word segmentation can only use training corpus.\nWe adopt positional character embeddings in our next four models. Our first model is a BLSTM neural network (baseline). To take advantage of traditional model (Chieu and Ng, 2002; Mccallum et al., 2001) such as CRF, we combine transition probability in our B-LSTM based MMNN. We design a F-Score driven training method in our third model F-Score Driven Model I . We propose an integrated training method in our fourth model F-Score Driven Model II .The re-\nsults of models are depicted as Figure 1(a). From the figure, we can know our models perfrom better with little loss in time.\nTable 3 shows results for NER on test sets. In the Table 3, we also show micro F1-score (Overall) and out-of-vocabulary entities (OOV) recall. Peng and Dredze (2016) is the state-of-the-art NER system in Chinese Social media. By comparing the results of B-LSTM model and B-LSTM + MTNNmodel, we can know transition probability is significant for NER. Compared with B-LSTM + MMNN model, F-Score Driven Model I improves the result of named entity with a loss in nominal mention. The integrated training model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-ofthe-art NER system in Chinese social media. Our integrated model has better performance on named entity and nominal mention.\nTo better understand the impact of the factor \u03b2, we show the results of our integrated model with different values of \u03b2 in Figure 1(c). From Figure 1(c), we can know that \u03b2 is an important factor for us to balance F-score and accuracy. Our integrated model may help alleviate the influence of noise in NER in Chinese social media."}, {"heading": "4 Conclusions and Future Work", "text": "The results of our experiments also suggest directions for future work. We can observe all models in Table 3 achieve a much lower recall than precision (Pink et al., 2014). So we need to design some methods to solve the problem."}, {"heading": "Acknowledgements", "text": "Thanks to Shuming Ma for the help on improving the writing. This work was supported in part by National Natural Science Foundation of China (No. 61673028), and National High Technology Research and Development Program of China (863 Program, No. 2015AA015404). Xu Sun is the corresponding author of this paper. The first author focuses on the design of the method and the experimental results. The corresponding author focuses on the design of the method."}], "references": [{"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["Chen et al.2015] Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "The unreasonable effectiveness of word representations for twitter named entity recognition", "author": ["Cherry", "Guo2015] Colin Cherry", "Hongyu Guo"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Cherry et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cherry et al\\.", "year": 2015}, {"title": "Named entity recognition: a maximum entropy approach using global information", "author": ["Chieu", "Ng2002] Hai Leong Chieu", "Hwee Tou Ng"], "venue": "In Proceedings of the 19th international conference on Computational linguistics-Volume", "citeRegEx": "Chieu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chieu et al\\.", "year": 2002}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "A data driven approach for person name disambiguation in web search results", "author": ["Raquel Mart\u0131\u0301nez", "V\u0131\u0301ctor Fresno", "Soto Montalvo"], "venue": "In Proceedings of COLING", "citeRegEx": "Delgado et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Delgado et al\\.", "year": 2014}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Graves et al.2006] Alex Graves", "Santiago Fernndez", "Faustino Gomez", "Jrgen Schmidhuber"], "venue": "In InternationalConference,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Need4tweet: A twitterbot for tweets named entity extraction and disambiguation", "author": ["Habib", "van Keulen2015] Mena Habib", "Maurice van Keulen"], "venue": "In Proceedings of ACL-IJCNLP 2015 System Demonstrations,", "citeRegEx": "Habib et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Habib et al\\.", "year": 2015}, {"title": "Joint coreference resolution and named-entity linking with multi-pass sieves", "author": ["Leila Zilles", "Daniel S. Weld", "Luke Zettlemoyer"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods", "citeRegEx": "Hajishirzi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hajishirzi et al\\.", "year": 2013}, {"title": "Named entity recognition with long short-term memory", "author": ["James Hammerton"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume", "citeRegEx": "Hammerton.,? \\Q2003\\E", "shortCiteRegEx": "Hammerton.", "year": 2003}, {"title": "A unifiedmodel for cross-domain and semi-supervised named entity recognition in chinese social media", "author": ["He", "Sun2017] Hangfeng He", "Xu Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2017\\E", "shortCiteRegEx": "He et al\\.", "year": 2017}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Improving named entity recognition in tweets via detecting non-standard words", "author": ["Li", "Liu2015] Chen Li", "Yang Liu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Enhancing sumerian lemmatization by unsupervised named-entity recognition", "author": ["Liu et al.2015] Yudong Liu", "Clinton Burkhart", "James Hearne", "Liang Luo"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Chinese word segmentation and named entity recognition based on conditional random fields", "author": ["Mao et al.2008] Xinnian Mao", "Yuan Dong", "Saike He", "Sencheng Bao", "Haila Wang"], "venue": "In IJCNLP,", "citeRegEx": "Mao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2008}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning dictionaries for named entity recognition using minimal supervision", "author": ["Neelakantan", "Collins2015] Arvind Neelakantan", "Michael Collins"], "venue": "Computer Science", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Max-margin tensor neural network for chinese word segmentation", "author": ["Pei et al.2014] Wenzhe Pei", "Tao Ge", "Baobao Chang"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Pei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pei et al\\.", "year": 2014}, {"title": "Named entity recognition for chinese social media with jointly trained embeddings", "author": ["Peng", "Dredze2015] Nanyun Peng", "Mark Dredze"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Improving named entity recognition for chinese social media with word segmentation representation learning", "author": ["Peng", "Dredze2016] Nanyun Peng", "Mark Dredze"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computa-", "citeRegEx": "Peng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2016}, {"title": "Analysing recall loss in named entity slot filling", "author": ["Pink et al.2014] Glen Pink", "Joel Nothman", "James R. Curran"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pink et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pink et al\\.", "year": 2014}, {"title": "Named entity recognition with document-specific kb tag gazetteers", "author": ["Radford et al.2015] Will Radford", "Xavier Carreras", "James Henderson"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Enhancing medical named entity recognition with features derived from unsupervised methods", "author": ["Maria Skeppstedt"], "venue": "In Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association", "citeRegEx": "Skeppstedt.,? \\Q2014\\E", "shortCiteRegEx": "Skeppstedt.", "year": 2014}, {"title": "Latent variable perceptron algorithm for structured classification", "author": ["Sun et al.2009] Xu Sun", "Takuya Matsuzaki", "Daisuke Okanohara", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the 21st International Joint", "citeRegEx": "Sun et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2009}, {"title": "Feature-frequency-adaptive online training for fast and accurate natural language processing", "author": ["Sun et al.2014] Xu Sun", "Wenjie Li", "Houfeng Wang", "Qin Lu"], "venue": "Computational Linguistics,", "citeRegEx": "Sun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Structure regularization for structured prediction", "author": ["Xu Sun"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sun.,? \\Q2014\\E", "shortCiteRegEx": "Sun.", "year": 2014}, {"title": "Multi-objective optimization for the joint disambiguation of nouns and named entities", "author": ["Leonhard Hennig", "Feiyu Xu", "Hans Uszkoreit"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Weissenborn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weissenborn et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "The natural language processing tasks on social media are more challenging which draw attention of many researchers (Li and Liu, 2015; Habib and van Keulen, 2015; Radford et al., 2015; Cherry and Guo, 2015).", "startOffset": 116, "endOffset": 206}, {"referenceID": 26, "context": "As the foundation of many downstream applications (Weissenborn et al., 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text.", "startOffset": 50, "endOffset": 123}, {"referenceID": 4, "context": "As the foundation of many downstream applications (Weissenborn et al., 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text.", "startOffset": 50, "endOffset": 123}, {"referenceID": 7, "context": "As the foundation of many downstream applications (Weissenborn et al., 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text.", "startOffset": 50, "endOffset": 123}, {"referenceID": 23, "context": "NER is a task to identify names in texts and to assign names with particular types (Sun et al., 2009; Sun, 2014; Sun et al., 2014; He and Sun, 2017).", "startOffset": 83, "endOffset": 148}, {"referenceID": 25, "context": "NER is a task to identify names in texts and to assign names with particular types (Sun et al., 2009; Sun, 2014; Sun et al., 2014; He and Sun, 2017).", "startOffset": 83, "endOffset": 148}, {"referenceID": 24, "context": "NER is a task to identify names in texts and to assign names with particular types (Sun et al., 2009; Sun, 2014; Sun et al., 2014; He and Sun, 2017).", "startOffset": 83, "endOffset": 148}, {"referenceID": 22, "context": "The scant labelled Chinese social media corpus makes the task more challenging (Neelakantan and Collins, 2015; Skeppstedt, 2014; Liu et al., 2015).", "startOffset": 79, "endOffset": 146}, {"referenceID": 13, "context": "The scant labelled Chinese social media corpus makes the task more challenging (Neelakantan and Collins, 2015; Skeppstedt, 2014; Liu et al., 2015).", "startOffset": 79, "endOffset": 146}, {"referenceID": 4, "context": ", 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text. NER is a task to identify names in texts and to assign names with particular types (Sun et al., 2009; Sun, 2014; Sun et al., 2014; He and Sun, 2017). It is the informality of social media that discourages accuracy of NER systems. While efforts in English have narrowed the gap between social media and formal domains (Cherry and Guo, 2015), the task in Chinese remains challenging. It is caused by Chinese logographic characters which lack many clues to indicate whether a word is a name, such as capitalization. The scant labelled Chinese social media corpus makes the task more challenging (Neelakantan and Collins, 2015; Skeppstedt, 2014; Liu et al., 2015). To address the problem, one approach is to use the lexical embeddings learnt from massive unlabeled text. To take better advantage of unlabeled text, Peng and Dredze (2015) evaluates three types of embeddings for Chinese text, and shows the effectiveness of positional character embeddings with experiments.", "startOffset": 8, "endOffset": 1024}, {"referenceID": 8, "context": "B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient (Hammerton, 2003; Hochreiter and Schmidhuber, 1997; Chen et al., 2015; Graves et al., 2006).", "startOffset": 96, "endOffset": 187}, {"referenceID": 0, "context": "B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient (Hammerton, 2003; Hochreiter and Schmidhuber, 1997; Chen et al., 2015; Graves et al., 2006).", "startOffset": 96, "endOffset": 187}, {"referenceID": 5, "context": "B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient (Hammerton, 2003; Hochreiter and Schmidhuber, 1997; Chen et al., 2015; Graves et al., 2006).", "startOffset": 96, "endOffset": 187}, {"referenceID": 17, "context": "To combine transition probability into B-LSTM neural network, we construct a Max Margin Neural Network (MMNN) (Pei et al., 2014) based on B-LSTM.", "startOffset": 110, "endOffset": 128}, {"referenceID": 0, "context": "B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient (Hammerton, 2003; Hochreiter and Schmidhuber, 1997; Chen et al., 2015; Graves et al., 2006). However, B-LSTM cannot learn sentence level label information. Huang et al. (2015) combine CRF to use sentence level label information.", "startOffset": 148, "endOffset": 272}, {"referenceID": 17, "context": "We define a structured margin loss\u2206(l, l) as Pei et al. (2014):", "startOffset": 45, "endOffset": 63}, {"referenceID": 3, "context": "The discrete features can be easily incorporated into neural network model (Collobert et al., 2011).", "startOffset": 75, "endOffset": 99}, {"referenceID": 15, "context": "We pre-trained embeddings using word2vec (Mikolov et al., 2013) with the skip-gram training model, without negative sampling and other default parameter settings.", "startOffset": 41, "endOffset": 63}, {"referenceID": 14, "context": "Like Mao et al. (2008), we use bigram features as follow:", "startOffset": 5, "endOffset": 23}, {"referenceID": 3, "context": "We use window approach (Collobert et al., 2011) to extract higher level Features from word feature vectors.", "startOffset": 23, "endOffset": 47}, {"referenceID": 3, "context": "We treat bigram features as discrete features (Collobert et al., 2011) for our neural network.", "startOffset": 46, "endOffset": 70}, {"referenceID": 20, "context": "We can observe all models in Table 3 achieve a much lower recall than precision (Pink et al., 2014).", "startOffset": 80, "endOffset": 99}], "year": 2017, "abstractText": "We focus on named entity recognition (NER) for Chinese social media. With massive unlabeled text and quite limited labelled corpus, we propose a semisupervised learning model based on BLSTM neural network. To take advantage of traditional methods in NER such as CRF, we combine transition probability with deep learning in our model. To bridge the gap between label accuracy and F-score of NER, we construct a model which can be directly trained on F-score. When considering the instability of Fscore driven method and meaningful information provided by label accuracy, we propose an integrated method to train on both F-score and label accuracy. Our integrated model yields substantial improvement over previous state-of-the-art result.", "creator": "LaTeX with hyperref package"}}}