{"id": "1611.04871", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2016", "title": "Audio Event and Scene Recognition: A Unified Approach using Strongly and Weakly Labeled Data", "abstract": "in this proposition we propose these novel learning algorithms called optimal and efficient supervised learning knowing the goal rests to learn conclusions from weakly and strongly labeled data. strongly constrained data ought be automatically understood as fully independent data where particular flavor instances are available. besides weakly described conditions only weak labels are available. our proposed framework is motivated by the conclusion that a small amount behind strongly known data can give considerable improvement given noisy weakly supervised learning. perhaps worst problem domain focus of a paper is acoustic event like fragment detection in vocal recordings. could first test a naive formulation for leveraging labeled conditions in both models. lets then find a more general framework for objective versus weakly linear learning ( swsl ). improved on this intuitive proposition, we choose a decision based idea for synthesis. most newer method follows based on polynomial regularization on conceptual model which we show that the unified learning can directly obtained thus a constraint based problem because can be solved by simple dual - convex procedure ( cccp ). our experiments show why naive strategy framework now address several concerns of inadequate flavor analysis as weakly labeled data.", "histories": [["v1", "Sat, 12 Nov 2016 07:39:50 GMT  (103kb)", "https://arxiv.org/abs/1611.04871v1", null], ["v2", "Wed, 23 Nov 2016 23:17:46 GMT  (103kb)", "http://arxiv.org/abs/1611.04871v2", null], ["v3", "Sat, 18 Feb 2017 07:18:32 GMT  (103kb)", "http://arxiv.org/abs/1611.04871v3", "IJCNN 2017, 8 pages"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.SD", "authors": ["anurag kumar", "bhiksha raj"], "accepted": false, "id": "1611.04871"}, "pdf": {"name": "1611.04871.pdf", "metadata": {"source": "CRF", "title": "Audio Event and Scene Recognition: A Unified Approach using Strongly and Weakly Labeled Data", "authors": ["Anurag Kumar", "Bhiksha Raj"], "emails": ["alnu@andrew.cmu.edu", "bhiksha@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n04 87\n1v 3\n[ cs\n.L G\n] 1\n8 Fe\nb 20\nI. INTRODUCTION\nCapturing an audio or multimedia recording has become extremely easy and hence a huge amount of multimedia data is being generated every day. Audio either on its own or together with other modalities in a multimedia recording carry a significant amount of information. Hence, detection of acoustic events and scenes have become an important research problem [1]. Several applications motivates audio event or scene detection research. These include but are not limited to content based indexing and retrieval of multimedia recordings [2], [3], [4], improving human computer (robot) interaction by making computers (robots) aware of acoustic scene or environment around it [5], [6], surveillance and monitoring application [7], audio based context recognition system [8].\nOver the past few years, several approaches based on different signal processing and machine learning techniques have been proposed for audio event and scene detection, such as [1], [9], [10], [11], [12] to cite a few. Audio event and scene detection challenges [13] (DCASE 2013), [14] (DCASE 2016) have helped in increasing the pace of audio content analysis research.\nHowever, almost all of current literature on audio event detection (AED) rely on fully supervised methods using\nstrongly labeled data. In strongly labeled data either audio clip examples for an acoustic event are directly provided or the time stamps of occurrences of the acoustic event in the recordings are given so that event specific part can be extracted from the whole recordings. Clearly, the purpose of strongly labeled data is supervised training of event detectors. Overall, labeled examples of each event class are available and then some supervised machine learning technique is applied for recognizing and detecting acoustic events. We will alternatively refer to strongly labeled data as supervised data.\nThis reliance on strongly labeled data severely limits the scale and scope of audio event (and scene) detection works and is currently one of the most important challenges faced by the research community. Creating a large amount of strongly labeled data is an extremely time consuming, difficult and expensive process. This can be gauged from the fact that most publicly available datasets have less than an hour of audio data for each event [15], [14], [13], [16]. In fact, in most cases only a few minutes of audio data per acoustic event is available. Moreover, this also limits the vocabulary of audio events in datasets because of difficulties in creating strongly labeled data for a large number of acoustic event.\nRecently, there have been attempts towards weakly supervised learning of audio events [17], [18]. The goal is to learn audio event detectors from weakly labeled data. In weakly labeled data only the presence or absence of an event in the recording is known. Exact time stamps of occurrences of events are not known and hence supervised learning methods cannot be directly applied. The methods proposed in these works are based on Multiple Instance Learning (MIL) framework. The motivation behind this weakly supervised learning is that not only manually annotating audio recordings for weak labels is much easier to do compared to strong labels but also that weakly labeled data can be directly obtained from web. Most of the multimedia or audio recordings on the web have some associated metadata (titles, tags etc) from which weak labels can be inferred to a certain extent. Thus, the time consuming and expensive process of manual annotation is not required any more and large amount of weakly labeled data can be directly obtained from web.\nIn this work we propose a unified approach to learn simultaneously from weakly and strongly labeled data. Three\nimportant factors motivates this unified approach for learning audio event detectors.\n\u2022 The problem with strongly labeled data is that it cannot be\nobtained or created in large amounts which further creates learning challenges. Nonetheless, in a lot of cases strongly labeled data are available or strong labeling can be done, though in a small amount. Weakly labeled data on the other hand can be obtained on a much larger scale and automatically from web. Given labeled data in two forms it is desirable to have a learning framework which can exploit them simultaneously. The labeled data in both forms can together help in learning robust models for audio events. \u2022 The major significance of weakly supervised learning lies\nin the fact that it allows us to scale AED by providing a way to exploit the huge amount of data on web, example from Youtube. Weak labels can be automatically obtained from the metadata associated with a recording. However, such weak labels are always expected to be noisy. Consider, for example, the sound event barking. Searching barking on Youtube also returns recordings such as Hillary Clinton literally barks at Republicans among the top results which clearly has nothing to do with the acoustic event barking. Metadata based weak labels are always expected to be noisy. This label noise can adversely affect the learning process. SWSL can be one way to address these label noises. A small amount of data can be strongly labeled where it is known that the labels are \u201cpure\u201d. It can be added to the pool of noisy weakly labeled data and this supervision can help learn better decision functions. The ill effects of noisy weak labels can be mitigated by the strongly labeled data or in other words presence of supervised data can make weakly supervised learning tolerant to label noises. \u2022 The multimedia data on web contains a large amount of\nwithin-category variations [17]. Different recordings are recorded under different conditions and styles which produces huge intra-class variations. Recording instances of the same event vary by a large amount. This makes the learning process from such data very challenging. Moreover, the audio signal itself might be very noisy. Overall, learning from web data introduces yet another challenge in form of what we can refer to as \u201csignal noise\u201d. It includes the large amount of intra-class variations for a given sound event class. Once again, a small amount of \u201cpure\u201d examples from strongly labeled data can be an answer to these concerns.\nHence, a novel unified learning framework which can leverage labeled data in both forms is proposed in this paper. We call this novel unified learning framework Supervised and Weakly Supervised Learning (SWSL). Under this framework, we propose two leaning methods. The first one called simpleSWSL or naiveSWSL is a naive way to bring supervised data into weakly supervised domain. It adapts the supervised data in MIL framework to learn simultaneously from both forms of labeled data. We then give a more general learning process for SWSL. The central idea behind this second approach is that SWSL can be formulated as semi-supervised learning with\nconstraints imposed by weak labels. Under this formulation, one can adapt a variety of semi-supervised learning for SWSL. In particular, we use graph based semi-supervised learning and refer to this method as graphSWSL.\nIt should be noted that although this paper is driven by audio event detection, our SWSL approach is a completely generic framework to learn using both fully supervised and weakly labeled data. It can be applied to other related problems as well. For example, multiple instance learning based weakly supervised image retrieval and visual object detection have been explored in computer vision community [19], [20], [21]. Our SWSL framework can be applied in these domains as well.\nThe outline of the paper is as follows; in Section 2 we introduce the proposed SWSL framework. In Section 3 we describe graphSWSL formulation. Section 4 describes the audio feature representation used in this work. Section 5 shows and discusses our experiments and results. Finally we conclude in Section 6.\nII. SWSL\n[17] introduced audio event detection (AED) using weakly labeled data. The main idea in weakly supervised AED is that AED using weak labels can be formulated as an multiple instance learning problem [22]. In MIL instances are given in groups called bags and labels are available for each bag. In a negative bag all instances are known to be negative, whereas, in a positive bag it is only known that at least one instance is positive. Thus, in a positive bag both positive and negative instances can be present. The goal is to learn a classifier technique using data in bag-label form.\nFor AED, full audio recordings are segmented into small segments. The full recording is a bag and the segments of the recording are instances of the bag. Since weak label carries information about presence or absence of an event in a recording, it can be used to label bags (recordings). For an event, a bag is positive if that event is marked to be present in the recording otherwise it is labeled as a negative bag. [17] used SVM (miSVM) [22] and neural network (BPMIL) [23] based MIL methods to show that AED with weak labels can be successfully done. Besides learning from weak labels, another major advantage of these methods is that they can be used to estimate temporal location of events as well. The significance of it is due to the fact that no such information was present in the weakly labeled data to begin with. In another work, [18] tries to scale AED with weak labels by proposing scalable MIL methods. These scalable MIL methods convert MIL into supervised learning by representing each bag with a single high dimensional vector. Although, a significant reduction in computational time is achieved, these scalable methods operate at bag-level and cannot be used for temporal localization of events.\nFor SWSL, we retain the basic MIL based framework to work with weakly labeled data. Audio recordings are converted into bags by segmenting into small segments and positive (+1) or negative (-1) labels are assigned to the bags according\nto weak labels. In SWSL, along with weak data in form of labeled bags, we are also given a separate strongly labeled or supervised data in form of labeled instances. The idea is to be able to exploit both supervised and weakly supervised data for AED."}, {"heading": "A. Naive SWSL", "text": "In SWSL, along with weak data in form of labeled bags, we are also given a separate strongly labeled or supervised data in form of labeled instances. The simplest approach for SWSL is to formulate the strongly labeled data as a special case of weakly labeled data. In this special case, each labeled instance is framed as a bag with one instance only. The bag label is same as instance level. Once this is done, any MIL approach can be applied as in case of only weakly supervised learning. We call this method naiveSWSL or simpleSWSL.\nSince each bag contain only one instance, all MIL methods factoring in bag constraints of at least one positive in positive bag and all negatives in negative bag will end up satisfying supervised label constraints. For example, using miSVM as simpleSWSL would imply that for the bags formed from supervised data, label constraints as in classical supervised SVM are satisfied whereas for other bags it would remain same as in miSVM. Hence, data in both strong and weak forms are appropriately used under this formulation."}, {"heading": "B. Generalized SWSL", "text": "The above formulation is a very naive way of unifying supervised and weakly supervised learning. In this section, we present a more general framework for SWSL. In MIL negative bags are known to have only negative instances, meaning labels for all instances in negative bags are essentially known. Thus, instances in negative bags can be considered along with supervised data.\nFor positive bags on the other hand this is not valid. In generalized SWSL formulation, we undertake instances in positive bags as essentially unlabeled but with certain \u201clabel constraints\u201d. The constraint on this unlabeled data is that they are grouped into bags and within each bag of instances at least one instance is positive. Thus, we have labeled data in form of supervised data plus instances in negative bags and a set of unlabeled data with constraints. Hence, this general form of SWSL can be formulated under the framework of semi-supervised learning with additional label constraints on the unlabeled data. A variety of methods for semi-supervised learning including multiple instance learning semi-supervised learning have been proposed [24], [25], [26], [27], [28], [29], [30]. In this work, we adopt one of the most popular method for semi-supervised learning, manifold regularization on graphs [26] for SWSL. We name this variant of SWSL as graphSWSL.\nIII. GRAPHSWSL: GRAPH BASED SWSL\nAll instances in negative bags can be labeled as negative and hence from here on in our mathematical formulation we\nwill simply consider them to be part of strongly labeled or fully supervised set.\nLet us represent the supervised dataset as Ds = {(x1, y1), ...(xn, yn)}. yi \u2208 {\u22121, 1} is label for instance xi and n is total number of instances in Ds. The weakly supervised data Dw, is in form of positive bags. Let Dw = {B1, ..BT } be the set of T bags where Bt = {(xt1, yt1), ..., (xtmt , ytmt)} is a positive bag of instances where labels y are unknown but at least one label is +1. m = \u2211T\nt=1 mt is the total number of instances in all positive bags.\nLet us represent the over all data D as D = {(x1, y1), ...(xn, yn), (xn+1, yn+1), ....(xn+m, yn+m)}. Without loss of generality we have assumed that instances are ordered such that first n are from Ds and n + 1 to n + m are from Dw. Instances n+1 to n+m1 are from bag B1 and so on. We will denote the start and end indices of instances from bag Bt in D as pt and qt. The instance space is denoted by X . The total number of instances in D is N = n+m. Labels for the first n instances in D are known. The labels for the rest of the instances are unknown but constraint by the following relationship\nmax(ypt , ...yqt) = 1 \u2200 t = 1 to T (1)\nIn this supervised and weakly supervised paradigm the goal is to learn the function mapping f : X \u2192 R, which maps the instance space to a decision score. f is assumed to be smooth and let us denote the Reproducing Kernel Hilbert Space (RKHS) of f as H. Since the labels for instances xn+1 to xn+m are essentially unknown and yet constrained by Eq 1, we can formulate this learning process as a constrained form of semi-supervised learning (SSL). A particularly well known method for semisupervised learning is manifold regularization on graphs [26]. This method is inductive which is one of the reasons we adopt it for SWSL."}, {"heading": "A. Manifold Regularization approach for SWSL", "text": "In Graph based semi-supervised learning, all instances are assumed to connected by a graph G = (V,E) where the vertices V are instances in the data. In this paper we assume kNN graph [24] where a vertex xi is connected to another vertex xj by a non-zero weight wij if xi is among the knearest neighbour of xj and vice versa.\nThe edge weight wij is then defined by Gaussian Kernel,\nwij = exp(\u2212 ||xi\u2212xj||\n2\n2\u03c32 ). \u03c3 is the bandwidth parameter for weights. Clearly, when xi and xj are not connected wij = 0. The overall graph is parametrized through a symmetric weight matrix W whose elements are wij . Finally, the unnormalized graph laplacian L is defined by L = D \u2212 W , where D is diagonal matrix with Dii = \u2211\nj wij . Manifold regularization on graphs for SSL solves the fol-\nlowing optimization problem\nmin f\n1\nn\nn \u2211\ni=1\n(yi \u2212 f (xi)) 2 + \u03bb1||f || 2 H + \u03bb2||f || 2 I (2)\nThe first term is simply the squares loss over the labeled instances. The first regularization term ||f ||2H is the RKHS norm which is used to impose smoothness conditions on f . The second penalty term ||f ||2I is a regularization term for intrinsic structure of data distribution. This terms ensures that the solution is smooth with respect to data distribution as well. Together the two regularization terms controls complexity of the solution over both RKHS and intrinsic geometry of data distribution.\nFor SWSL, we need to factor in the weak label information of positive bags in the above optimization problem. To do this, we solve following optimization problem in manifold regularization on graphs for SWSL\nmin f\n1\nn\nn \u2211\ni=1\n(yi \u2212 f (xi)) 2 + \u03bb1||f || 2 H + \u03bb2||f || 2 I\n+ \u03bb3 T\nT \u2211\nt=1\n(1\u2212 max j=pt,...,qt\nf(xj)) 2 (3)\nIn the above formulation, the last term is the squared loss for each positive (+1) bag and factors in the weak label information. To measure loss with respect to each bag, the value of each bag is determined by the maximal output instance.\nUnlike the case of semi-supervised learning, the above SWSL formulation is a non-convex optimization problem. This presents additional challenges and here we describe an approach to solve the above optimization problem.\nFirst, we rewrite the optimization problem in Eq 3 using\nslack variables.\nmin f,\u03be\nn \u2211\ni=1\n(yi \u2212 f (xi)) 2 + \u03bb1||f || 2 H + \u03bb2||f || 2 I + \u03bb3\nT \u2211\nt=1\n\u03be2t\ns.t 1\u2212 max j=pt,...,qt f(xj) \u2264 \u03bet, t = 1, ..., T (4)\n\u03bet \u2265 0, t = 1, ..., T\n\u03bet are the slack variables for loss on positive bags. Also, note that we have factored in the normalization terms (n and T ) in the regularization parameters. To solve the above problem we need a finite dimensional form for f .\nUsing Representer Theorem [31], the solution to the above\nproblem can be expressed as f(x) = N \u2211\ni=1\n\u03b1ik(x,xi), where\nk(\u00b7, \u00b7) is the reproducing kernel of H. Let us denote the N\u00d7N kernel gram matrix over the training data D with K .\nNow, let Y be a N dimensional label vector where Y = [y1, y2, ...yn, 0, 0....0]. Yi is label for the first n instances which are labeled and 0 for the rest. Also, let J be N \u00d7N diagonal matrix where Jii = 1 for i = 1 to n. and Jii = 0 for i = n+1 to (n+m). Using expression for f , the squared loss term for labeled instances can be written as \u2211n\ni=1(yi \u2212 f (xi)) 2 =\n(Y \u2212 JK\u03b1)T (Y \u2212 JK\u03b1).\nThe intrinsic norm ||f ||I is estimated using the graph laplacian matrix L by ||f ||2I = 1 N2\nfTLf [26]. Hence, ||f ||2I = 1 N2 \u03b1TKLK\u03b1. So our, final optimization problem becomes\nmin \u03b1,\u03be\n(Y \u2212 JK\u03b1)T (Y \u2212 JK\u03b1) + \u03bb1\u03b1 TK\u03b1\n+ \u03bb2 1\nN2 \u03b1TKLK\u03b1+ \u03bb3\nT \u2211\nt=1\n\u03be2t (5)\ns.t 1\u2212 max j=pt,...,qt\nK \u2032j\u03b1 \u2264 \u03bet t = 1, ..., T\n\u03bet \u2265 0, t = 1, ..., T\nKj is the j th column of kernel matrix K . The optimization problem in Eq 5 is still not straightforward to solve due the max constraints."}, {"heading": "B. Optimization Solution", "text": "The objective function in the optimization problem in Eq 5 is a convex differentiable function. The first set of constraint (1\u2212 max j=pt,...,qt K \u2032j\u03b1 \u2264 \u03bet) is non-convex but a difference of two convex function. Convex Concave Procedure (CCCP) [32] is a well known method of sequential convex programming to handle problems like this. It is an iterative method in which the non-convex function is converted into a convex function using Taylor series approximation at the current solution.\nFor an objective or constraint in form of g(x)\u2212h(x) where g(x) and h(x) are convex, a convex approximation at x(k) is obtained as g(x) \u2212 h(x(k)) \u2212 \u25bdh(x(k))(x \u2212 x(k)), where \u25bdh(x(k)) is gradient of h(x) at x(k). max() is a non-smooth function and hence we need the subgradient of max() for the Taylor series expansion. The subgradient of max\nj=pt...qt K \u2032j\u03b1 can\nbe defined as [33]\n\u2202( max j=pt,...,qt\nK \u2032j\u03b1) =\nqt \u2211\nj=pt\n\u03b4tjKj (6)\nThe \u03b4tj\u2019s are defined as\n\u03b4tj =\n{\n1 rt , if K \u2032j\u03b1 = max u=pt,...,qt K \u2032u\u03b1 0, otherwise (7)\nrt is the number of instances which maximizes the output K \u2032j\u03b1 in t th bag. Hence, all instances in bag for which maximum is achieved are active in the subgradients. Now, we can rewrite the non-convex constraints in kth iteration of CCCP using the above subgradient. The non-convex constraint 1\u2212 max j=pt,...,qt K \u2032j\u03b1 in k th iteration becomes\n1\u2212 max j=pt...qt K \u2032j\u03b1 \u2248 1\u2212 ( max j=pt...qt K \u2032j\u03b1 (k) +\nqt \u2211\nj=pt\n\u03b4 (k) tj K \u2032 j(\u03b1\u2212 \u03b1 (k))) (8)\nHence, the final optimization problem to solve in kth\niteration of CCCP\nmin \u03b1,\u03be\n(Y \u2212 JK\u03b1)T (Y \u2212 JK\u03b1) + \u03bb1\u03b1 TK\u03b1\n+ \u03bb2 1\nN2 \u03b1TKLK\u03b1+ \u03bb3\nT \u2211\nt=1\n\u03be2t (9)\ns.t\n1\u2212 ( max j=pt...qt\nK \u2032j\u03b1 (k) +\nqt \u2211\nj=pt\n\u03b4 (k) tj K \u2032 j(\u03b1\u2212 \u03b1 (k))) \u2264 \u03bet\nt = 1, ..., T\n\u03bet \u2265 0, t = 1, ..., T\nThe objective function in optimization problem of Eq 9 is convex and the constraints are linear. The overall problem is a convex Quadratic Programming problem. In CCCP the above optimization problem is iteratively solved until convergence.\nOnce \u03b1 has been obtained the output corresponding to any test point xtest can be obtained as f(xtest) = \u2211N\ni=1 \u03b1ik(xtest,xi). It is worth noting that similar to miSVM and neural network based weakly supervised approaches used for AED, graphSWSL can also predict output corresponding to each instance in a bag. Hence, our SWSL approach can also be used for temporal localization of acoustic events in a recording. The bag-level prediction is done by using the max over instance outputs."}, {"heading": "IV. ACOUSTIC FEATURES FOR AUDIO SEGMENTS", "text": "We need a feature representation for each audio segment or instances in bags. Several acoustic feature representations for audio event or scene detection has been proposed [1]. In this work we use the Gaussian Mixture Model (GMM) based histogram characterization of audio segments which has been used in previous weakly supervised audio event detection works [18],[17]. It has been shown to be effective feature representation for audio event detection tasks. Components of GMM represent audio words and the final features are normalized soft-count histograms.\nAll audio recordings are first parameterized through MelCeptra Coefficients (MFCCs). The MFCCs for the training data are then used to train a GMM. Let G = {wc, N(~\u00b5c,\u03a3c)} be this GMM, where wc is the mixture weight for c th component of GMM and ~\u00b5c are \u03a3c are the component mean and covariance matrix. Let the total number of components in GMM be C. We train GMM with diagonal covariance matrices \u03a3c. Now, given an audio segment with a total of M MFCC frames where each frame is represented by ~ft, we compute the following for each GMM component\nPr(c|~ft) = wcN(~ft; ~\u00b5c,\u03a3c) C \u2211\ni=1\nwiN(~ft; ~\u00b5i,\u03a3i)\n(10)\nH(c) = 1\nM\nM \u2211\nt=1\nPr(c|~ft) (11)\nThe final feature for the audio segment is the C dimensional histogram vector ~HC = [H(1), ..., H(C)] T . It is also usually helpful to normalize ~HC to sum to 1."}, {"heading": "V. EXPERIMENTS AND RESULTS", "text": "We evaluate the proposed supervised and weakly supervised learning on both audio event and acoustic scene detection tasks. In our experiments we add a small amount of strongly labeled data to the pool of weakly labeled data to learn event or scene detectors using SWSL and compare it with weakly supervised learning. For weakly supervised learning we use miSVM approach [22]. miSVM has also been used in previous weakly supervised audio event detection work [17]. For simpleSWSL, we again use miSVM approach to integrate strongly and weakly labeled data.\nThe details specific to audio event and scenes are given in corresponding sections. We describe the common experimental set up here. In our experiments all audio recordings are sampled at 44.1KHz sampling frequency. 20 dimensional MFCC features along with delta and acceleration coeffecients are used to parametrize audio recordings. The bag of audio words over the MFCCs as described in Section IV are used as feature representation for audio segments or instances in bags. The GMM component size C is 64 or 128. We show results for both values of C. Exponential Chi-square (\u03c72) kernels in form of exp(\u2212\u03b3d(x, y)) have been known to work remarkably well with histogram features, including for detection of acoustic concepts [34][35]. d(x, y) is \u03c72 distance. Hence, we use exponential \u03c72 kernels for miSVM and simpleSWSL. The parameter \u03b3 is set as the inverse of mean \u03c72 distance between training points. The slack parameter C in SVM training is set through cross validation on the training data.\nFor graphSWSL, the kNN graph is constructed with k as 20 or 40. We evaluate and show results for both cases. The bandwidth \u03c3 for graph weights is set to 1.0 for all experiments. The kernel K is again exponential \u03c72 kernel as used in miSVM and simpleSWSL. The parameter \u03bb3 in graphSWSL is simply set as \u03bb3 = n/T , ratio of the number of supervised instances to the number of positive bags. The other two regularization parameters \u03bb1 and \u03bb2 are selected through cross-validation over a grid of 10\u22123 to 103."}, {"heading": "A. Acoustic Event Detection", "text": "We consider a set of 10 acoustic events namely, Chainsaw (C), Clock Ticking (CT), Crackling Fire (CF), Crying Baby (CR), Dog Barking (DB), Helicopter (H), Rain (RA), Rooster (RO), Seawaves (SE), Sneezing (SN). The events are part of ESC-10 [15] dataset which provides us strongly labeled data for these events. We obtain the weakly labeled training data from Youtube. For each event we use the event name as search query on Youtube 1. To get more sound oriented results the keyword \u201csound\u201d is attached to each event name (e.g Chaisaw sound). We consider the audio from top 60 returned video\n1www.youtube.com\nresults, from which we filter out very long recordings. Finally, we are left with 40 weakly labeled recording for all acoustic events except Crackling Fire (35) and Rain(10). Recordings for Rain are relatively longer and hence total duration of audio for it is in similar range to others. The total duration of all 365 recordings is around 5.1 hours. Audio recording are segmented to form bags and instances and we use the average duration of each event in the strongly labeled data as the segment size.\nWe need a large test dataset for comprehensive evaluation of all methods. For all audio events, we obtain test data from Freesound 2. The number of test recordings for each event are as follows: C (78), CT (69), CF (66), CR (88), DB (100), H (39), RA (76), RO (83), SE (48), SN (75). This total of 722 recordings spanning over 13 hours of audio allows a thorough analysis of all methods. Note that results given in next paragraphs are bag-level detection results. Instance level temporal localization experiments are described in further sections.\nESC-10 dataset contains 40 positive examples for each event, resulting in a total of 400 supervised data instances (positive and negative) for any given event. The dataset comes pre-divided into 5 folds. We include recordings from 4 out of 5 folds for training. This amounts to addition of about 25 minutes of strongly labeled data to the pool of weakly labeled data. Experiments are run all 5 ways (leaving one fold in each case) and the average across all 5 runs are reported. We use Average Precision (AP) as performance metric. The mean average precision (MAP) over all event classes are also shown for all cases.\nTable I shows AP values for different audio events using miSVM and simpleSWSL. Results for acoustic features using both C = 64 and C = 128 are shown. One can observe that adding a small amount of strongly labeled data to the pool of weakly labeled data is extremely helpful. An absolute improvement in MAP of about 7% for C = 64 and 10% for C = 128 can be observed. As far as individual events are concerned, for several events such as Chainsaw, Crackling Fire, Rain an absolute improvement of 15 \u2212 20% in AP can be observed.\n2www.freesound.org\nTable II shows AP values for graph based SWSL approach. We observe that graphSWSL improves over simpleSWSL another 4\u22125% in absolute terms. This amounts to about 12% and 14% improvement over miSVM for C = 64 and 128 and respectively. For graphSWSL the performance remains more or less consistent for the two values of k in kNN graph. From these results one can conclude that a small amount of strongly labeled can play a significant role in reducing the effect of signal-noise and label-noise in weakly labeled data obtained from web."}, {"heading": "B. Acoustic Scene Detection", "text": "The procedure for acoustic scene remains similar to acoustic events. We work with a toal of 15 acoustic scenes from DCASE [14] dataset, which is also source of our strongly labeled data. Weakly labeled training data is again obtained from Youtube in a procedure similar to audio events. In this case we create weakly labeled data of 40 recordings per scene, totaling 600 recordings which spans over 27 hours. Once again we obtain test data from Freesound. A total of 928 test recordings (38 hours) are used. The test data contains an average of 61 recording per scene with a minimum of 53 for Forest Path scene and a maximum of 71 for Residential Area scene. Segment size is average duration of scenes in strongly labeled data. The supervised data from DCASE comes predivided into 4 folds and our experimental approach is same as before. We use 3 out of 4 folds in SWSL and perform experiments all 4 ways. As before, average results across all 4 runs are reported here. The value of k in kNN graph is 40. It is worth noting that acoustic scenes are acoustically much more complex. Intra-class variation is far greater than audio events. Both training and test data contains a significant amount of within class variations and hence, learning and detection of acoustic scenes from weakly labeled data is a much harder problem. This is evident in the low average precision for most classes.\nTable III shows AP values for different methods. For acoustic scenes, we note that the performance of miSVM and simpleSWSL is almost similar in terms of mean average precision. Graph based SWSL on the other hand gives a 20% relative improvement over these methods. graphSWSL improves AP in almost all cases. For several scenes such as\nGrocery Store, Home, Library, Metro Station, Train, Tram AP improves by 50% or more in relative terms. This should be especially noted for Home, which turns out to be the hardest scene to detect. graphSWSL for this scene more than double the average precision when compared to weakly supervised learning. Again, based on these results we can conclude that SWSL is a very effective way of addressing the concerns of weakly supervised learning."}, {"heading": "C. Temporal Localization of Audio Events", "text": "One important advantage of weakly supervised AED using methods such as miSVM is that we can obtain a rough estimate of temporal location of events in an audio recording. In this section we evaluate different methods for temporal localization of events. We consider only audio events since audio events are usually short term acoustic phenomena.\nEvaluating temporal localization is more challenging compared to recording level detection of events. Put more simply temporal localization evaluation implies instance level evaluation of all methods. Hence, we need strongly labeled test data, so that the ground truth labels for instances (segments) are known. Since this is not available for the test data used in previous section, we use the left out fold from the ESC-10 dataset for evaluating instance-level performance. This means the fold which was left out of training process for simpleSWSL\nand graphSWSL is used as test data. Results accumulated over all 5 runs are reported.\nTable IV shows AP for instance-level detection of audio events. Once again we notice that SWSL based approaches gives 15 \u2212 20% absolute improvement over only weakly supervised learning. Graph based SWSL is once again superior to other methods. Overall this shows that our proposed method is suitable for temporal localization of audio events as well."}, {"heading": "VI. CONCLUSIONS", "text": "We presented a novel learning framework named SWSL for learning simultaneously from strongly and weakly labeled data. Labeled data in these two different forms occur naturally for a variety of problems. The unified learning framework proposed in this paper allows one to leverage labeled data in both forms. More importantly, the proposed SWSL framework can help address concerns of only weakly supervised learning of audio events. Weakly supervised learning is a promising approach to scale audio event detection by exploiting weakly labeled web data. However, it comes with its own sets of concerns and we showed that SWSL can successfully address these concerns.\nFrom learning perspective, our primary assertion in this paper is that SWSL can be cast as a constraint form of semisupervised learning. We proposed a method based on manifold regularization on graphs. This is in addition to the naive way of adopting strongly labeled data in weakly supervised learning. For both audio event and scene detection tasks a considerable improvement in performance can be observed by adding a small amount of strongly labeled data. This shows that a small amount of supervised data can mitigate the ill effects of signal and more importantly label noise in weakly labeled data obtained from web.\nFinally, our proposed SWSL is not restricted to the domain of audio content analysis and can be applied for other problems as well. Content based image retrieval and multimedia event detection where weakly supervised learning has been explored in depth are other suitable problem domains where SWSL can be applied."}], "references": [{"title": "Detection and classification of acoustic scenes and events", "author": ["D. Stowell", "D. Giannoulis", "E. Benetos", "M. Lagrange", "M.D. Plumbley"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 10, pp. 1733\u20131746, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimedia information retrieval: content-based information retrieval from large text and audio databases", "author": ["P. Sch\u00e4uble"], "venue": "Springer Science & Business Media,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "A multimedia retrieval framework based on semi-supervised ranking and relevance feedback", "author": ["Y. Yang", "F. Nie", "D. Xu", "J. Luo", "Y. Zhuang", "Y. Pan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, pp. 723\u2013742, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Content-based multimedia information retrieval: State of the art and challenges", "author": ["M.S. Lew", "N. Sebe", "C. Djeraba", "R. Jain"], "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 2, no. 1, pp. 1\u201319, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Soundevent recognition with a companion humanoid", "author": ["M. Janvier", "X. Alameda-Pineda", "L. Girinz", "R. Horaud"], "venue": "2012 12th IEEE- RAS International Conference on Humanoid Robots (Humanoids 2012). IEEE, 2012, pp. 104\u2013111.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Sound representation and classification benchmark for domestic robots", "author": ["J. Maxime", "X. Alameda-Pineda", "L. Girin", "R. Horaud"], "venue": "2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2014, pp. 6285\u20136292.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Scream and gunshot detection and localization for audio-surveillance systems", "author": ["G. Valenzise", "L. Gerosa", "M. Tagliasacchi", "F. Antonacci", "A. Sarti"], "venue": "Advanced Video and Signal Based Surveillance, 2007. AVSS 2007. IEEE Conference on. IEEE, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Adaptive audio-based context recognition", "author": ["W. Dargie"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans, vol. 39, no. 4, pp. 715\u2013725, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Realworld acoustic event detection", "author": ["X. Zhuang", "X. Zhou", "M.A. Hasegawa-Johnson", "T.S. Huang"], "venue": "Pattern Recognition Letters, vol. 31, pp. 1543\u20131551, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Sound event detection in real life recordings using coupled matrix factorization of spectral representations and class activity annotations", "author": ["A. Mesaros", "T. Heittola", "O. Dikmen", "T. Virtanen"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Random regression forests for acoustic event detection and classification", "author": ["H. Phan", "M. Maa\u00df", "R. Mazur", "A. Mertins"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, pp. 20\u201331, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Recognition of acoustic events using deep neural networks", "author": ["O. Gencoglu", "T. Virtanen", "H. Huttunen"], "venue": "2014 22nd European Signal Processing Conference (EUSIPCO). IEEE, 2014, pp. 506\u2013510.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Detection and classification of acoustic scenes and events: an ieee aasp challenge", "author": ["D. Giannoulis", "E. Benetos", "D. Stowell", "M. Rossignol", "M. Lagrange", "M.D. Plumbley"], "venue": "2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics. IEEE, 2013, pp. 1\u20134.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Tut database for acoustic scene classification and sound event detection", "author": ["A. Mesaros", "T. Heittola", "T. Virtanen"], "venue": "24th European Signal Processing Conference, vol. 2016, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Esc: Dataset for environmental sound classification", "author": ["K.J. Piczak"], "venue": "Proceedings of the 23rd ACM International Conference on Multimedia. ACM, 2015, pp. 1015\u20131018.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "A dataset and taxonomy for urban sound research", "author": ["J. Salamon", "C. Jacoby", "J.P. Bello"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 1041\u20131044.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Audio event detection using weakly labeled data", "author": ["A. Kumar", "B. Raj"], "venue": "24th ACM International Conference on Multimedia. ACM Multimedia, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Weakly supervised scalable audio content analysis", "author": ["\u2014\u2014"], "venue": "2016 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Weakly supervised object localization with multi-fold multiple instance learning", "author": ["R.G. Cinbis", "J. Verbeek", "C. Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Keywords to visual categories: Multiple-instance learning forweakly supervised object categorization", "author": ["S. Vijayanarasimhan", "K. Grauman"], "venue": "Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008, pp. 1\u20138.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Content-based image retrieval using multiple-instance learning", "author": ["Q. Zhang", "S.A. Goldman", "W. Yu", "J.E. Fritts"], "venue": "International Conference on Machine Learning, vol. 2. Citeseer, 2002, pp. 682\u2013689.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "Advances in neural information processing systems, 2002, pp. 561\u2013568.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Neural networks for multi-instance learning", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "Proceedings of the International Conference on Intelligent Information Technology, Beijing, China, 2002, pp. 455\u2013459.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Introduction to semi-supervised learning", "author": ["X. Zhu", "A.B. Goldberg"], "venue": "Synthesis lectures on artificial intelligence and machine learning, vol. 3, pp. 1\u2013130, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "International Conference on Machine Learning, 2003.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of machine learning research, vol. 7, no. Nov, pp. 2399\u20132434, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Semi-supervised support vector machines", "author": ["K. Bennett", "A. Demiriz"], "venue": "Advances in Neural Information processing systems, pp. 368\u2013 374, 1999.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Missl: Multiple-instance semisupervised learning", "author": ["R. Rahmani", "S.A. Goldman"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 705\u2013712.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "On the relation between multi-instance learning and semi-supervised learning", "author": ["Z.-H. Zhou", "J.-M. Xu"], "venue": "Proceedings of the 24th International Conference on Machine learning. ACM, 2007, pp. 1167\u2013 1174.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Instance-level semisupervised multiple instance learning.", "author": ["Y. Jia", "C. Zhang"], "venue": "in AAAI,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Kernel methods for missing variables.", "author": ["A.J. Smola", "S. Vishwanathan", "T. Hofmann"], "venue": "in AISTATS. Citeseer,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}, {"title": "A regularization framework for multipleinstance learning", "author": ["P.-M. Cheung", "J.T. Kwok"], "venue": "Proceedings of the 23rd International Conference on Machine learning. ACM, 2006, pp. 193\u2013200.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust audio-codebooks for large-scale event detection in consumer videos", "author": ["S. Rawat", "P.F. Schulam", "S. Burger", "D. Ding", "Y. Wang", "F. Metze"], "venue": "2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Features and kernels for audio event recognition", "author": ["A. Kumar", "B. Raj"], "venue": "arXiv preprint arXiv:1607.05765, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Hence, detection of acoustic events and scenes have become an important research problem [1].", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "These include but are not limited to content based indexing and retrieval of multimedia recordings [2], [3], [4], improving human computer (robot) interaction by making computers (robots) aware of acoustic scene or environment around it [5], [6], surveillance and monitoring application [7], audio based context recognition system [8].", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "These include but are not limited to content based indexing and retrieval of multimedia recordings [2], [3], [4], improving human computer (robot) interaction by making computers (robots) aware of acoustic scene or environment around it [5], [6], surveillance and monitoring application [7], audio based context recognition system [8].", "startOffset": 104, "endOffset": 107}, {"referenceID": 3, "context": "These include but are not limited to content based indexing and retrieval of multimedia recordings [2], [3], [4], improving human computer (robot) interaction by making computers (robots) aware of acoustic scene or environment around it [5], [6], surveillance and monitoring application [7], audio based context recognition system [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "These include but are not limited to content based indexing and retrieval of multimedia recordings [2], [3], [4], improving human computer (robot) interaction by making computers (robots) aware of acoustic scene or environment around it [5], [6], surveillance and monitoring application [7], audio based context recognition system [8].", "startOffset": 237, "endOffset": 240}, {"referenceID": 5, "context": "These include but are not limited to content based indexing and retrieval of multimedia recordings [2], [3], [4], improving human computer (robot) interaction by making computers (robots) aware of acoustic scene or environment around it [5], [6], surveillance and monitoring application [7], audio based context recognition system [8].", "startOffset": 242, "endOffset": 245}, {"referenceID": 6, "context": "These include but are not limited to content based indexing and retrieval of multimedia recordings [2], [3], [4], improving human computer (robot) interaction by making computers (robots) aware of acoustic scene or environment around it [5], [6], surveillance and monitoring application [7], audio based context recognition system [8].", "startOffset": 287, "endOffset": 290}, {"referenceID": 7, "context": "These include but are not limited to content based indexing and retrieval of multimedia recordings [2], [3], [4], improving human computer (robot) interaction by making computers (robots) aware of acoustic scene or environment around it [5], [6], surveillance and monitoring application [7], audio based context recognition system [8].", "startOffset": 331, "endOffset": 334}, {"referenceID": 0, "context": "Over the past few years, several approaches based on different signal processing and machine learning techniques have been proposed for audio event and scene detection, such as [1], [9], [10], [11], [12] to cite a few.", "startOffset": 177, "endOffset": 180}, {"referenceID": 8, "context": "Over the past few years, several approaches based on different signal processing and machine learning techniques have been proposed for audio event and scene detection, such as [1], [9], [10], [11], [12] to cite a few.", "startOffset": 182, "endOffset": 185}, {"referenceID": 9, "context": "Over the past few years, several approaches based on different signal processing and machine learning techniques have been proposed for audio event and scene detection, such as [1], [9], [10], [11], [12] to cite a few.", "startOffset": 187, "endOffset": 191}, {"referenceID": 10, "context": "Over the past few years, several approaches based on different signal processing and machine learning techniques have been proposed for audio event and scene detection, such as [1], [9], [10], [11], [12] to cite a few.", "startOffset": 193, "endOffset": 197}, {"referenceID": 11, "context": "Over the past few years, several approaches based on different signal processing and machine learning techniques have been proposed for audio event and scene detection, such as [1], [9], [10], [11], [12] to cite a few.", "startOffset": 199, "endOffset": 203}, {"referenceID": 12, "context": "Audio event and scene detection challenges [13] (DCASE 2013), [14] (DCASE 2016) have helped in increasing the pace of audio content analysis research.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "Audio event and scene detection challenges [13] (DCASE 2013), [14] (DCASE 2016) have helped in increasing the pace of audio content analysis research.", "startOffset": 62, "endOffset": 66}, {"referenceID": 14, "context": "This can be gauged from the fact that most publicly available datasets have less than an hour of audio data for each event [15], [14], [13], [16].", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "This can be gauged from the fact that most publicly available datasets have less than an hour of audio data for each event [15], [14], [13], [16].", "startOffset": 129, "endOffset": 133}, {"referenceID": 12, "context": "This can be gauged from the fact that most publicly available datasets have less than an hour of audio data for each event [15], [14], [13], [16].", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "This can be gauged from the fact that most publicly available datasets have less than an hour of audio data for each event [15], [14], [13], [16].", "startOffset": 141, "endOffset": 145}, {"referenceID": 16, "context": "Recently, there have been attempts towards weakly supervised learning of audio events [17], [18].", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "Recently, there have been attempts towards weakly supervised learning of audio events [17], [18].", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "\u2022 The multimedia data on web contains a large amount of within-category variations [17].", "startOffset": 83, "endOffset": 87}, {"referenceID": 18, "context": "For example, multiple instance learning based weakly supervised image retrieval and visual object detection have been explored in computer vision community [19], [20], [21].", "startOffset": 156, "endOffset": 160}, {"referenceID": 19, "context": "For example, multiple instance learning based weakly supervised image retrieval and visual object detection have been explored in computer vision community [19], [20], [21].", "startOffset": 162, "endOffset": 166}, {"referenceID": 20, "context": "For example, multiple instance learning based weakly supervised image retrieval and visual object detection have been explored in computer vision community [19], [20], [21].", "startOffset": 168, "endOffset": 172}, {"referenceID": 16, "context": "[17] introduced audio event detection (AED) using weakly labeled data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The main idea in weakly supervised AED is that AED using weak labels can be formulated as an multiple instance learning problem [22].", "startOffset": 128, "endOffset": 132}, {"referenceID": 16, "context": "[17] used SVM (miSVM) [22] and neural network (BPMIL) [23] based MIL methods to show that AED with weak labels can be successfully done.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[17] used SVM (miSVM) [22] and neural network (BPMIL) [23] based MIL methods to show that AED with weak labels can be successfully done.", "startOffset": 22, "endOffset": 26}, {"referenceID": 22, "context": "[17] used SVM (miSVM) [22] and neural network (BPMIL) [23] based MIL methods to show that AED with weak labels can be successfully done.", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "In another work, [18] tries to scale AED with weak labels by proposing scalable MIL methods.", "startOffset": 17, "endOffset": 21}, {"referenceID": 23, "context": "A variety of methods for semi-supervised learning including multiple instance learning semi-supervised learning have been proposed [24], [25], [26], [27], [28], [29], [30].", "startOffset": 131, "endOffset": 135}, {"referenceID": 24, "context": "A variety of methods for semi-supervised learning including multiple instance learning semi-supervised learning have been proposed [24], [25], [26], [27], [28], [29], [30].", "startOffset": 137, "endOffset": 141}, {"referenceID": 25, "context": "A variety of methods for semi-supervised learning including multiple instance learning semi-supervised learning have been proposed [24], [25], [26], [27], [28], [29], [30].", "startOffset": 143, "endOffset": 147}, {"referenceID": 26, "context": "A variety of methods for semi-supervised learning including multiple instance learning semi-supervised learning have been proposed [24], [25], [26], [27], [28], [29], [30].", "startOffset": 149, "endOffset": 153}, {"referenceID": 27, "context": "A variety of methods for semi-supervised learning including multiple instance learning semi-supervised learning have been proposed [24], [25], [26], [27], [28], [29], [30].", "startOffset": 155, "endOffset": 159}, {"referenceID": 28, "context": "A variety of methods for semi-supervised learning including multiple instance learning semi-supervised learning have been proposed [24], [25], [26], [27], [28], [29], [30].", "startOffset": 161, "endOffset": 165}, {"referenceID": 29, "context": "A variety of methods for semi-supervised learning including multiple instance learning semi-supervised learning have been proposed [24], [25], [26], [27], [28], [29], [30].", "startOffset": 167, "endOffset": 171}, {"referenceID": 25, "context": "In this work, we adopt one of the most popular method for semi-supervised learning, manifold regularization on graphs [26] for SWSL.", "startOffset": 118, "endOffset": 122}, {"referenceID": 25, "context": "A particularly well known method for semisupervised learning is manifold regularization on graphs [26].", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "In this paper we assume kNN graph [24] where a vertex xi is connected to another vertex xj by a non-zero weight wij if xi is among the knearest neighbour of xj and vice versa.", "startOffset": 34, "endOffset": 38}, {"referenceID": 25, "context": "The intrinsic norm ||f ||I is estimated using the graph laplacian matrix L by ||f ||I = 1 N fLf [26].", "startOffset": 96, "endOffset": 100}, {"referenceID": 30, "context": "Convex Concave Procedure (CCCP) [32] is a well known method of sequential convex programming to handle problems like this.", "startOffset": 32, "endOffset": 36}, {"referenceID": 31, "context": "be defined as [33]", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "Several acoustic feature representations for audio event or scene detection has been proposed [1].", "startOffset": 94, "endOffset": 97}, {"referenceID": 17, "context": "In this work we use the Gaussian Mixture Model (GMM) based histogram characterization of audio segments which has been used in previous weakly supervised audio event detection works [18],[17].", "startOffset": 182, "endOffset": 186}, {"referenceID": 16, "context": "In this work we use the Gaussian Mixture Model (GMM) based histogram characterization of audio segments which has been used in previous weakly supervised audio event detection works [18],[17].", "startOffset": 187, "endOffset": 191}, {"referenceID": 21, "context": "For weakly supervised learning we use miSVM approach [22].", "startOffset": 53, "endOffset": 57}, {"referenceID": 16, "context": "miSVM has also been used in previous weakly supervised audio event detection work [17].", "startOffset": 82, "endOffset": 86}, {"referenceID": 32, "context": "Exponential Chi-square (\u03c7) kernels in form of exp(\u2212\u03b3d(x, y)) have been known to work remarkably well with histogram features, including for detection of acoustic concepts [34][35].", "startOffset": 171, "endOffset": 175}, {"referenceID": 33, "context": "Exponential Chi-square (\u03c7) kernels in form of exp(\u2212\u03b3d(x, y)) have been known to work remarkably well with histogram features, including for detection of acoustic concepts [34][35].", "startOffset": 175, "endOffset": 179}, {"referenceID": 14, "context": "The events are part of ESC-10 [15] dataset which provides us strongly labeled data for these events.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "We work with a toal of 15 acoustic scenes from DCASE [14] dataset, which is also source of our strongly labeled data.", "startOffset": 53, "endOffset": 57}], "year": 2017, "abstractText": "In this paper we propose a novel learning framework called Supervised and Weakly Supervised Learning where the goal is to learn simultaneously from weakly and strongly labeled data. Strongly labeled data can be simply understood as fully supervised data where all labeled instances are available. In weakly supervised learning only data is weakly labeled which prevents one from directly applying supervised learning methods. Our proposed framework is motivated by the fact that a small amount of strongly labeled data can give considerable improvement over only weakly supervised learning. The primary problem domain focus of this paper is acoustic event and scene detection in audio recordings. We first propose a naive formulation for leveraging labeled data in both forms. We then propose a more general framework for Supervised and Weakly Supervised Learning (SWSL). Based on this general framework, we propose a graph based approach for SWSL. Our main method is based on manifold regularization on graphs in which we show that the unified learning can be formulated as a constraint optimization problem which can be solved by iterative concave-convex procedure (CCCP). Our experiments show that our proposed framework can address several concerns of audio content analysis using weakly labeled data.", "creator": "LaTeX with hyperref package"}}}