{"id": "1611.05774", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "What Do Recurrent Neural Network Grammars Learn About Syntax?", "abstract": "recurrent grammatical network grammars ( rnng ) are a hypothesis proposed ne generative modeling family predicting natural memory. experts show theory - of - the - flow language modeling and parsing performance. observers investigate, actions they bring, from her linguistic perspective, require various ablations to the formulation and the method, and by augmenting the communication constitutes an attention mechanism ( ga - rnng ) they involve closer inspection. approaches find that semantic methods of composition requires crucial as achieving the best performance. through the information mechanism, systems agree that verbal plays superior central capacity toward phrasal awareness ( namely uncertainty model'07 latent coping behavior agreeing between predictions experienced alongside hand - crafted rules, possibly displaying some similar mechanisms ). appropriately handling responses without non - terminal forms, algorithms find that verbal representations depend minimally independent non - terminals, providing support for false default hypothesis.", "histories": [["v1", "Thu, 17 Nov 2016 16:41:41 GMT  (638kb,D)", "http://arxiv.org/abs/1611.05774v1", null], ["v2", "Tue, 10 Jan 2017 19:15:08 GMT  (526kb,D)", "http://arxiv.org/abs/1611.05774v2", "10 pages. To appear in EACL 2017, Valencia, Spain"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["adhiguna kuncoro", "miguel ballesteros", "lingpeng kong", "chris dyer", "graham neubig", "noah a smith"], "accepted": false, "id": "1611.05774"}, "pdf": {"name": "1611.05774.pdf", "metadata": {"source": "CRF", "title": "What Do Recurrent Neural Network Grammars Learn About Syntax?", "authors": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Graham Neubig", "Noah A. Smith"], "emails": ["akuncoro@cs.cmu.edu", "lingpenk@cs.cmu.edu", "gneubig@cs.cmu.edu", "miguel.ballesteros@ibm.com,", "nasmith@cs.washington.edu,", "cdyer@google.com"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we focus on a recently proposed class of probability distributions, recurrent neural network grammars (RNNGs; Dyer et al., 2016b), designed to model syntactic derivations of natural language sentences. We focus on RNNGs as generative probabilistic models over trees, and provide a brief summary in \u00a72.\nFitting such a model to data has often been understood as a way to test or confirm some aspect of a theory. We talk about a model\u2019s assumptions and sometimes explore its parameters or posteriors, to gain understanding of what it \u201cdiscovers\u201d from the\ndata. In some sense, such models can be thought of as mini-scientists.\nNeural networks, including RNNGs, are capable of representing much larger classes hypotheses than traditional probabilistic models, giving them more freedom to explore. Unfortunately, they tend to be bad mini-scientists, because their parameters are difficult for human scientists to interpret.\nRNNGs are striking because they obtain stateof-the-art parsing and language modeling performance. Their relative lack of independence assumptions, while still incorporating a degree of linguistically-motivated prior knowledge, affords the model considerable freedom to derive its own insights about syntax. If they are mini-scientists, the discoveries they make should be of particular interest as propositions about syntax (at least for the particular genre and dialect of the data).\nThis paper manipulates the inductive bias of RNNGs to test linguistic hypotheses.1 We begin with an ablation study to discover the importance of the composition function in \u00a73. Based on the findings, we augment the RNNG composition function with a novel gated attention mechanism (GA-RNNG) to incorporate more interpretability into the model in \u00a74. Using the GA-RNNG, we proceed by investigating the role that individual heads play in phrasal representation (\u00a75) and the role that non-terminal category labels play (\u00a76). Our key findings are that lexical heads play an important role in representing most phrase types (although compositions of multiple salient heads are not infrequent, especially for phrases\n1RNNGs have less inductive bias relative to traditional unlexicalized probabilistic context-free grammars, but more than models that parse by transducing word sequences to linearized parse trees represented as strings (Vinyals et al., 2015). Inductive bias is necessary for learning (Mitchell, 1980); we believe the important question is not \u201chow little can a model get away with?\u201d but rather the benefit of different forms of inductive bias as data vary.\nar X\niv :1\n61 1.\n05 77\n4v 1\n[ cs\n.C L\n] 1\n7 N\nov 2\n01 6\ninvolving conjunctions) and that non-terminal labels only provide little additional information. As a byproduct of our investigation, a variant of the RNNG without ensembling achieved the best reported supervised phrase-structure parsing (93.6 F1; English-PTB) and, through conversion, dependency parsing (95.8 UAS, 94.6 LAS; PTB-SD)."}, {"heading": "2 Recurrent Neural Network Grammars", "text": "An RNNG defines a joint probability distribution over string terminals and phrase-structure nonterminals.2 Formally, the RNNG is defined by a triple \u3008N,\u03a3,\u0398\u3009, where N denotes the set of nonterminal symbols (NP, VP, etc.), \u03a3 the set of all terminal symbols,3 and \u0398 the set of all model parameters. Unlike previous works that rely on hand-crafted rules to compose more fine-grained phrase representations (Collins, 1997; Klein and Manning, 2003), the RNNG implicitly parameterizes the information passed through compositions of phrases (in \u0398 and the neural network architecture), hence weakening the strong independence assumptions in classical probabilistic context-free grammars.\nThe RNNG is based on an abstract state machine like those used in transition-based parsing, with its algorithmic state consisting of a stack of partially completed constituents, a buffer of already-generated terminal symbols, and a list of past actions. To generate a sentence x and its phrase-structure tree y, the RNNG samples a sequence of actions to construct y top-down. Given y, there is one such sequence (easily identified), which we call the oracle, a = \u3008a1, . . . , an\u3009 used during supervised training.\nThe RNNG uses three different actions:\n\u2022 NT(X), where X \u2208 N , introduces an open nonterminal symbol onto the stack, denoted by an open bracket and a non-terminal symbol, e.g., \u201c(NP\u201d, \u2022 GEN(T), where T \u2208 \u03a3, generates a terminal\nsymbol and places it on both the stack and the buffer, and \u2022 REDUCE indicates a closing bracket \u201c)\u201d that in-\ndicates a constituent is now complete. The elements of the stack that comprise this constituent 2Dyer et al. (2016b) also defined a conditional version of the RNNG that can be used only for parsing; here we focus on the generative version since it is more flexible and (rather surprisingly) even learns better estimates of p(y | x).\n3We assume that N \u2229 \u03a3 = \u2205.\n(going back to the last open non-terminal) are popped, a composition function is executed, providing a composed element onto the stack.\nAt each time step, the model encodes the stack, buffer, and past actions, with a separate LSTM (Hochreiter and Schmidhuber, 1997) network for each component as features to define a distribution over the next action to take (conditioned on the full algorithmic state). The overall architecture is illustrated in Figure 1; examples of full action sequences can be found in Dyer et al. (2016b).\nA key element of the RNNG is the composition function, which reduces a completed constituent into a single element on the stack. This function computes a vector representation of the new constituent; it also uses an LSTM (here a bi-directional one). This composition function, which we consider in greater depth in \u00a73, is illustrated in Fig. 2.\nSince the RNNG is a generative model, it at-\ntempts to maximize p(x,y), the joint distribution of strings and trees, defined as\np(x,y) = p(a) =\nn\u220f\nt=1\np(at | a1, . . . , at\u22121).\nIn other words, p(x,y) is defined as a product of local probabilities, conditioned on all past actions. The joint probability estimate p(x,y) can be used for both phrase-structure parsing (finding arg maxy p(y | x)) and language modeling (finding p(x) by marginalizing over the exponential set of possible parses). This is done using an importance sampling procedure for inference.4\nWe report all RNNG performance based on the Corrigendum to Recurrent Neural Network Grammars (Dyer et al., 2016a)."}, {"heading": "3 Composition is Key", "text": "Given the same data and a discrimintive (rather than generative) training objective,5 RNNGs were found to parse with significantly higher accuracy than the model of Vinyals et al. (2015) that represents y as a \u201clinearized\u201d sequence of symbols and parentheses without explicitly capturing the tree structure, or constraining the y to be a wellformed tree (see Table 1). The latter model literally predicts the sequence of non-terminals, terminals, and parentheses from left to right. This suggests that the heart of the RNNG is the composition function (Fig. 2), which Vinyals et al. do not have.\nIndeed, on close inspection, the RNNG\u2019s three data structures\u2014stack, buffer, and action history\u2014are redundant. For example, the action history and buffer contents completely determine\n4Importance sampling works by using a proposal distribution q(y | x) that is easy to sample from. In Dyer et al. (2016b) and this paper, the proposal distribution is the discriminative variant of the RNNG, see Dyer et al. (2016b).\n5As mentioned before, RNNGs can be trained either generatively or discriminatively (conditional on the sentence); to isolate the impact of architectural differences between the RNNG model and Vinyals et al., we compare the discriminative variant.\nthe structure of the stack at every timestep. Every generated word goes onto the stack, too; and some past words will be composed into larger structures, but through the composition function, they are all still \u201cavailable\u201d to the network that predicts the next action. Similarly, the past actions are redundant with the stack. Despite this redundancy, only the stack incorporates the composition function. We propose to empirically validate the importance of composition functions in syntactic neural models by comparing the performance of a stack-only RNNG and several ablated variants."}, {"heading": "3.1 Ablated RNNGs", "text": "Since each of the ablated models is sufficient to encode all necessary partial tree information, the primary difference is that ablations with the stack use explicit composition, to which we can therefore attribute most of the performance difference.\nWe conjecture that the stack\u2014the component that makes use of the composition function\u2014is critical to the RNNG\u2019s performance, and that the buffer and action history are not. In transitionbased parsers built on expert-crafted features, the most recent words and actions are useful if they are salient, although neural representation learners based on LSTMs, work out for themselves what information should be salient.\nTo test this conjecture, we train ablated RNNGs that lack each of the three data structures (action history, buffer, stack), as well as one that lacks both the action history and buffer.6 If our conjecture is correct, performance should degrade most without the stack, and the stack alone should perform competitively.\nExperimental results. We trained each ablation from scratch, and compared these models on three tasks: English phrase-structure parsing (labeled F1), Table 2; dependency parsing, Table 3 by converting parse output to Stanford dependencies (De Marneffe et al., 2006), using the tool by Kong and Smith (2014), and language modeling, Table 4. The last row of each table reports the performance of a novel variant of the RNNG with attention (GA-RNNG, \u00a74); all RNNG variants used no POS tag or any pre-trained word embedding.\n6Note that the ablated RNNG without a stack is similar to Vinyals et al. (2015), who encoded a (partial) phrase-structure tree as a sequence of open and close parentheses, terminals, and non-terminal symbols; our action history is quite close to this, with each NT(X) capturing a left parenthesis and X nonterminal, and each REDUCE capturing a right parenthesis.\nDiscussion. The RNNG with only a stack is the strongest of the ablations, and it even outperforms the \u201cfull\u201d RNNG with all three data structures. Ablating the stack gives the worst among the new results. This strongly supports the importance of the composition function: a proper REDUCE operation that transforms a constituent\u2019s parts and non-terminal label into a single explicit (vector) representation is helpful to performance.\nIt is noteworthy that the stack alone is stronger than the original RNNG, which\u2014in principle\u2014\ncan learn to disregard the buffer and action history. We suspect that the additional parameters these two redundant data structures introduce make it more prone to overfitting.\nA similar performance degradation is seen in language modeling (Table 4): the stack-only RNNG achieves the best performance, and ablating the stack is most harmful. Indeed, modeling syntax without explicit composition (the stackablated RNNG) provides little benefit over a sequential LSTM language model.\nWe remark that the stack-only results are the best ever reported for both phrase-structure and dependency parsing."}, {"heading": "4 Gated Attention RNNG", "text": "Having established that the composition function is key to RNNG performance (\u00a73), we now seek to understand the nature of the composed phrasal representations that are learned. Like most neural networks, interpreting the composition function\u2019s behavior is challenging. Fortunately, linguistic theories offer a number of hypotheses about the nature of representations of phrases that can provide a conceptual scaffolding to understand them."}, {"heading": "4.1 Linguistic Hypotheses", "text": "We consider two theories about phrasal representation. The first is that phrasal representations are strongly determined by a privileged lexical head. Augmenting grammars with lexical head information has a long history in parsing, starting with the models of Collins (1997), and recent theories of syntax such as the \u201cbare phrase structure\u201d hypothesis of the Minimalist Program (Chomsky, 1993) posit that phrases are represented purely by single lexical heads. Proposals for multiple headed phrases (to deal with tricky cases like conjunction)\nlikewise exist (Jackendoff, 1977; Keenan, 1987). Do the phrasal representations learned by RNNGs depend on individual lexical heads or multiple heads? Or do the representations combine information from all children without any salient head?\nRelated to the question about the role of heads in phrasal representation is the question of whether phrase-internal material wholly determines the representation of a phrase (a so-called endocentric representation) or whether non-terminal rules introduce new representations (exocentric representations). To illustrate the contrast between endocentric and exocentric representations, an endocentric representation is representing a noun phrase with a noun category, whereas S\u2192NP VP exocentrically introduces a new syntactic category that is neither NP nor VP (Chomsky, 1970)."}, {"heading": "4.2 Gated Attention Composition", "text": "To investigate what the RNNG learns about headedness (and later endocentricity), we propose a variant of the composition function that makes use of an explicit attention mechanism (Bahdanau et al., 2015) and a sigmoid gate with multiplicative interactions, henceforth called GA-RNNG.\nAt every REDUCE operation, the GA-RNNG assigns an \u201cattention weight\u201d to each of its children (between 0 and 1 such that the total weight off all children sums to 1), and the parent phrase is represented by the combination of a sum of each child\u2019s representation scaled by its attention weight and its non-terminal type. Our weighted sum is more expressive than traditional head rules, however, because it allows attention to be divided among multiple constituents. Head rules, conversely, are analogous to giving all attention to one constituent, the one containing the lexical head.\nWe now formally define the GA-RNNG\u2019s composition function. Recall that ut is the concatenation of the vector representations of the RNNG\u2019s data structures, used to assign probabilities to each of the actions available at timestep t (See Fig. 1, the layer before the softmax at the top). For simplicity, we drop the timestep index here. Let ont denote the vector embedding (learned) of the non-terminal being constructed, for the purpose of computing attention weights.\nNow let c1, c2, . . . denote the sequence of vector embeddings for the constituents of the new phrase. The length of these vectors is defined\nby the dimensionality of the bi-directional LSTM used in the original composition function (Fig. 2).\nThe attention vector is given by:\na = softmax    \nc>1 c>2 ...\n V [u;ont ]   (1)\nNote that the length of a is the same as the number of constituents, and that this vector sums to one due to the softmax. It divides a single unit of attention among the constituents.\nNext, note that the constituent source vector m = [c1; c2; \u00b7 \u00b7 \u00b7 ]a is a convex combination of the child-constituents, weighted by attention. We will combine this with another embedding of the nonterminal denoted as tnt (separate from ont ) using a sigmoid gating mechanism:\ng = \u03c3 (W1tnt + W2m + b) (2)\nNote that the value of the gate is bounded between [0, 1] in each dimension.\nThe new phrase\u2019s final representation uses element-wise multiplication ( ) with respect to both tnt and m, a process reminiscent of the LSTM \u201cforget\u201d gate:\nc = g tnt + (1\u2212 g) m. (3)\nThe intuition is that the composed representation should incorporate both non-terminal information and information about the constituents (through weighted sum and attention mechanism). The gate g modulates the interaction between them to account for varying importance between the two in different contexts.\nThe new parameters added to the model include ont and tnt (for each non-terminal), V, W1, W2, and b; this increases the number of parameters by less than 10%. The rest of the model, other than the composition function, are identical with the baseline RNNG. We train the GA-RNNG using all the model structures (stack, output buffer, and action history). The composition function with attention is learned through backpropagation that attempts to maximize the probability of the correct sequence of actions.\nExperimental results. We include this model\u2019s performance in Tables 2\u20134 (the row \u201cGA-RNNG\u201d, last row in all tables). It is clear that the model matches or outperforms the baseline RNNG with all three structures present."}, {"heading": "5 Headedness in Phrases", "text": "We now exploit the attention mechanism to probe what the RNNG learns about headedness."}, {"heading": "5.1 The Heads GA-RNNG Learns", "text": "The attention weight vectors tell us which constituents are most important to a phrase\u2019s vector representation in the stack. Here, we inspect the attention vectors to investigate whether the model learns to center its attention around a single, or by extension a few, salient elements, which would confirm the presence of headedness in GA-RNNG.\nFirst, we consider several major non-terminal categories, and estimate the average perplexity of the attention vectors.7 The average perplexity can be interpreted as the average number of \u201cchoices\u201d for each non-terminal category; this value is only computed for the cases where the number of components in the composed phrase is at least two (otherwise the attention weight would be trivially 1 around the only component). The minimum possible value for the perplexity is 1, indicating a full attention weight around one component and zero attention everywhere else.\nFigure 3 shows much less than 2 average \u201cchoices\u201d across all non-terminal categories, which also holds true for all other categories not shown, even though the average number of components for each phrase is around 2.53. This implies that phrases\u2019 vectors tend to resemble the vector of one salient constituent, but not exclusively, as the perplexity for most categories are still not close to one.\nNext, we consider the how attention is distributed for the major non-terminal categories in Table 5, where the first five rows of each category represents compositions with highest entropy, and the next five rows are qualitatively analyzed. The high-entropy cases where the attention is most divided represent more complex phrases with conjunctions or more than one plausible head.\nNoun Phrases. In most simple noun phrases (representative samples are the sixth and seventh row of Table 5), the model pays the most attention to the rightmost noun and assigns near-zero attention on determiners and possessive determiners, while also paying nontrivial attention weights to the adjectives. This finding matches existing head rules and our intuition that nouns head noun\n7Recall that these are nonnegative vectors that sum to one by construction.\nphrases, and that adjectives are more important than determiners.\nWe analyze the case where the noun phrase contains a conjunction, in the last three rows of Table 5. We find that how the model represents these highly depends on the context, ranging from the first noun (eighth row) to the last noun (ninth row). This finding overlaps with existing head rules that similarly pick one of the nouns as the head.8 However, in the case of conjunctions of multiple noun phrases (as opposed to multiple noun terminals earlier), the model consistently picks the conjunction as the lexical head. This contradicts previous head rules and indicates that the model propagates the information that the composed phrase contains multiple components (since it is attending to the conjunction terminal that simply indicates multiple elements), while paying less attention to what those components are, although the model still has access to the exocentric fact that the composed phrase will still be another NP (based a gate-modulated interaction).\nVerb Phrases. The attention weights on simple verb phrases (e.g., \u201cVP\u2192 V NP\u201d, ninth row) are peaked around the noun phrase instead of the verb. This implies that the verb phrase would look most similar to the noun under it and contradicts existing head rules that unanimously put the verb as the head of the verb phrase. Another interesting finding is that the model pays attention to polarity information, where negations are almost always assigned non-trivial attention weights.9 Furthermore, we find that the model attends to the conjunction terminal in conjunctions of verb phrases (e.g., \u201cVP\u2192 VP and VP\u201d), reinforcing the similar finding for conjunction of noun phrases.\nPrepositional Phrases. In almost all cases, the model nearly unanimously attends to the preposition terminal instead of the noun phrases or complete clauses under it, regardless of the type of preposition. Even when the prepositional phrase is only used to make a connection between two noun phrases (e.g., PP \u2192 NP after NP, last row), the prepositional connector is still considered the most salient element. This is less consistent with the Collins and Stanford head rule, where preposi-\n8Existing head rules similarly in which noun gets selected as the head.\n9Similar with Li et al. (2016) where sequential LSTMs discover polarity information in sentiment analysis, although perhaps more surprising as polarity information is less intuitively central to syntactic modeling.\ntions are assigned a lower priority when composing PPs, although more consistent with the Johansson head rule (Johansson and Nugues, 2007)."}, {"heading": "5.2 Comparison to Existing Head Rules", "text": "To better measure the overlap between the attention vectors and existing head rules, we converted the trees in PTB \u00a723 into a dependency representation using the attention weights. In this case, the attention weight functions as a \u201cdynamic\u201d head rule, where all other constituents within the same composed phrase are considered to modify the constituent with the highest attention weight, repeated recursively. The head of the composed representation for \u201cS\u201d at the top of the tree is attached to a special root symbol and functions as the head of the sentence.\nWe evaluate the overlap between the resulting dependency tree and conversion results of the same trees using the Collins (Collins, 1997) and Stanford Dependencies (De Marneffe et al., 2006) head rules. Results are evaluated using the standard evaluation script10 in terms of UAS.11\nResults. The model has a higher overlap with the conversion using Collins head rules (49.8 UAS) rather than the Stanford Dependencies head rules (40.4 UAS). We attribute this large gap to the fact that the Stanford Dependencies head rule incorporates more semantic considerations, while\n10Excluding punctuation. 11The resulting conversion using the attention weights are\nunlabeled, hence LAS is inapplicable.\nthe RNNG is a purely syntactic model. In general, the attention-based tree output has a high error rate (\u2248 90%) when the dependent is a verb, since the constituent with the highest attention weight in a verb phrase is often the noun phrase instead of the verb, as discussed above. The conversion accuracy is better for nouns (\u2248 50% error), and much better for determiners (30%) and particles (6%) with respect to the Collins head rule.\nDiscussion. GA-RNNG has the power to infer head rules, and to a large extent, it does. It follows some conventions that are established in one or more previous head rule sets (e.g., prepositions head prepositional phrases, nouns head noun phrases), but attends more to a verb phrase\u2019s nominal constituents than the verb. It is important to note that this is not the byproduct of learning a specific model for parsing; the training objective is joint likelihood, which is not a proxy loss for parsing performance. These decisions were selected because they make the data maximally likely (though admittedly only locally maximally likely). We leave deeper consideration of this noun-centered verb phrase hypothesis to future work."}, {"heading": "6 The Role of Non-terminal Labels", "text": "Emboldened by our finding that GA-RNNGs learn a notion of headedness, we next explore whether heads are sufficient to create representations of phrases (in line with an endocentric theory of phrasal representation) or whether extra nonterminal information is necessary. If the endocentric hypothesis is true (that is, the representation of a phrase is built from within depending on its components but independent of explicit category labels), then the non-terminal types should be easily inferred given the endocentrically-composed representation, and that ablating the non-terminal information would not make much difference in performance. Specifically, we train a GA-RNNG on unlabeled trees (only bracketings without nonterminal types), denoted U-GA-RNNG.\nThis idea has been explored in research on methods for learning syntax with less complete annotation (Pereira and Schabes, 1992). A key finding from Klein and Manning (2002) was that, given bracketing structure, simple dimensionality reduction techniques could reveal conventional non-terminal categories with high accuracy; Petrov et al. (2006) also showed that latent vari-\nables can be used to recover fine-grained nonterminal categories. We therefore expect that the vector embeddings of the constituents that the UGA-RNNG correctly recovers (on test data) will capture categories similar to those in the Penn Treebank.\nExperiments. We first consider unlabeled F1 parsing accuracy. On test data (with the usual split), the GA-RNNG achieves 94.2%, while the U-GA-RNNG achieves 93.5%. This result suggests that non-terminal category labels add a relatively small amount of information compared to purely endocentric representations.\nVisualization. If endocentricity is largely sufficient to account for the behavior of phrases, where do our robust intuitions for syntactic category types come from? We use t-SNE (van der Maaten and Hinton, 2008) to visualize composed phrase vectors from the U-GA-RNNG model applied to the test data, with unlabeled (all nonterminals are replaced with an \u201cX\u201d) gold-standard parses to focus on the representation of phrases (the test set was unseen by the model before).\nFigure 4 shows that the U-GA-RNNG tends to recover non-terminal categories as encoded in the Penn Treebank, even when trained without them.12 These results suggest non-terminal types can be inferred from the purely endocentric compositional process to a certain extent, and that the phrase clusters found by the U-GA-RNNG largely overlap with non-terminal categories."}, {"heading": "7 Related Work", "text": "The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequenceto-sequence machine translation models capture a certain degree of syntactic knowledge as a byproduct of the translation objective. Our experiment on\n12We see a similar clustering for the non-ablated GARNNG model, not shown for brevity.\nthe importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy with sequence-to-sequence models.\nExtensive previous work on phrase-structure parsing typically employs the probabilistic context-free grammar formalism, with lexicalized (Collins, 1997) and non-terminal (Johnson, 1998; Klein and Manning, 2003) augmentations; the RNNG has less inductive bias than these earlier models and hence a weaker independence assumption. The conjecture that fine-grained non-terminal rules and labels can be discovered given weaker bracketing structures were based on several studies (Chiang and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006)."}, {"heading": "8 Conclusion", "text": "We probe what recurrent neural network grammars, a probabilistic generative model of language based on neural networks, learn about syntax, through ablation scenarios and a novel variant with attention mechanism (GA-RNNG) on the compo-\nsition function. The composition function, a key differentiator between the RNNG and other neural models of syntax, is crucial for obtaining good performance. Using the attention weight vectors we discover that the model is learning something similar to heads, although the attention vectors are not completely peaked around a single component. We show some cases where the attention vector is divided and measure the relationship with existing head rules. RNNGs without any non-terminal information during training further support the hypothesis that phrasal representations are largely endocentric, and a clustering analysis of representations shows that traditional non-terminal categories fall out naturally from the clustering of the composed phrases. This confirms previous conjectures that bracketing annotation does most of the work of syntax, with non-terminal categories easily discoverable given bracketing."}, {"heading": "Acknowledgments", "text": "This work was sponsored in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under Contract No. HR0011-15-C-0114; it was also supported in part by Contract No. W911NF-151-0543 with DARPA and the Army Research Office (ARO). Approved for public release, distribution unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proc. of ACL.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Training with exploration improves a greedy stack-LSTM parser", "author": ["Miguel Ballesteros", "Yoav Goldberg", "Chris Dyer", "Noah A. Smith."], "venue": "Proc. of EMNLP.", "citeRegEx": "Ballesteros et al\\.,? 2016", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "Recovering latent information in treebanks", "author": ["David Chiang", "Daniel M. Bikel."], "venue": "Proc. of COLING.", "citeRegEx": "Chiang and Bikel.,? 2002", "shortCiteRegEx": "Chiang and Bikel.", "year": 2002}, {"title": "Parsing as language modeling", "author": ["Do Kook Choe", "Eugene Charniak."], "venue": "Proc. EMNLP.", "citeRegEx": "Choe and Charniak.,? 2016", "shortCiteRegEx": "Choe and Charniak.", "year": 2016}, {"title": "Remarks on nominalization", "author": ["Noam Chomsky."], "venue": "Readings in English Transformational Grammar.", "citeRegEx": "Chomsky.,? 1970", "shortCiteRegEx": "Chomsky.", "year": 1970}, {"title": "Three generative, lexicalised models for statistical parsing", "author": ["Michael Collins."], "venue": "Proc. of EACL.", "citeRegEx": "Collins.,? 1997", "shortCiteRegEx": "Collins.", "year": 1997}, {"title": "Stanford typed dependencies manual", "author": ["Marie-Catherine De Marneffe", "Christopher D. Manning"], "venue": null, "citeRegEx": "Marneffe and Manning.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine De Marneffe", "Bill MacCartney", "Christopher D. Manning."], "venue": "Proc. of LREC.", "citeRegEx": "Marneffe et al\\.,? 2006", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Deep biaffine attention for neural dependency parsing", "author": ["Timothy Dozat", "Christopher D. Manning."], "venue": "ArXiv.", "citeRegEx": "Dozat and Manning.,? 2016", "shortCiteRegEx": "Dozat and Manning.", "year": 2016}, {"title": "Corrigendum to recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "CoRR.", "citeRegEx": "Dyer et al\\.,? 2016a", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "Proc. of NAACL.", "citeRegEx": "Dyer et al\\.,? 2016b", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Extended constituent-to-dependency conversion for English", "author": ["Richard Johansson", "Pierre Nugues."], "venue": "Proc. of NODALIDA.", "citeRegEx": "Johansson and Nugues.,? 2007", "shortCiteRegEx": "Johansson and Nugues.", "year": 2007}, {"title": "PCFG models of linguistic tree representations", "author": ["Mark Johnson."], "venue": "Comput. Linguist.", "citeRegEx": "Johnson.,? 1998", "shortCiteRegEx": "Johnson.", "year": 1998}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li."], "venue": "CoRR.", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Multiply-headed noun phrases", "author": ["Edward L. Keenan."], "venue": "Linguistic Inquiry, 18(3):481\u2013490.", "citeRegEx": "Keenan.,? 1987", "shortCiteRegEx": "Keenan.", "year": 1987}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "TACL.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "A generative constituent-context model for improved grammar induction", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proc. of ACL.", "citeRegEx": "Klein and Manning.,? 2002", "shortCiteRegEx": "Klein and Manning.", "year": 2002}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proc. of ACL.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "An empirical comparison of parsing methods for stanford dependencies", "author": ["Lingpeng Kong", "Noah A. Smith."], "venue": "CoRR.", "citeRegEx": "Kong and Smith.,? 2014", "shortCiteRegEx": "Kong and Smith.", "year": 2014}, {"title": "Distilling an ensemble of greedy dependency parsers into one MST parser", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith."], "venue": "Proc. of EMNLP.", "citeRegEx": "Kuncoro et al\\.,? 2016", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2016}, {"title": "Visualizing and understanding neural models in nlp", "author": ["Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky."], "venue": "Proc. of NAACL.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Effective self-training for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Proc. NAACL.", "citeRegEx": "McClosky et al\\.,? 2006", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "The Need for Biases in Learning Generalizations", "author": ["Tom M. Mitchell."], "venue": "Department of Computer Science, Laboratory for Computer Science Research, Rutgers University.", "citeRegEx": "Mitchell.,? 1980", "shortCiteRegEx": "Mitchell.", "year": 1980}, {"title": "Insideoutside reestimation from partially bracketed corpora", "author": ["Fernando Pereira", "Yves Schabes."], "venue": "Proc. of ACL.", "citeRegEx": "Pereira and Schabes.,? 1992", "shortCiteRegEx": "Pereira and Schabes.", "year": 1992}, {"title": "Improved inference for unlexicalized parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Proc. of NAACL.", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein."], "venue": "Proc. of COLING-ACL.", "citeRegEx": "Petrov et al\\.,? 2006", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Does string-based neural mt learn source syntax? In Proc", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "of EMNLP.", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Bayesian symbol-refined tree substitution grammars for syntactic parsing", "author": ["Hiroyuki Shindo", "Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata."], "venue": "Proc. ACL.", "citeRegEx": "Shindo et al\\.,? 2012", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "Visualizing high-dimensional data using tsne", "author": ["Laurens van der Maaten", "Geoffrey E. Hinton."], "venue": "Journal of Machine Learning Research, 9.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Proc. of NIPS.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Graph-based dependency parsing with bidirectional LSTM", "author": ["Wenhui Wang", "Baobao Chang."], "venue": "Proc. of ACL.", "citeRegEx": "Wang and Chang.,? 2016", "shortCiteRegEx": "Wang and Chang.", "year": 2016}, {"title": "Sequence-to-sequence learning as beam-search optimization", "author": ["Sam Wiseman", "Alexander M. Rush."], "venue": "Proc. of EMNLP.", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}, {"title": "Fast and accurate shiftreduce constituent parsing", "author": ["Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu."], "venue": "Proc. ACL.", "citeRegEx": "Zhu et al\\.,? 2013", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "In this paper, we focus on a recently proposed class of probability distributions, recurrent neural network grammars (RNNGs; Dyer et al., 2016b), designed to model syntactic derivations of natural language sentences.", "startOffset": 117, "endOffset": 144}, {"referenceID": 31, "context": "RNNGs have less inductive bias relative to traditional unlexicalized probabilistic context-free grammars, but more than models that parse by transducing word sequences to linearized parse trees represented as strings (Vinyals et al., 2015).", "startOffset": 217, "endOffset": 239}, {"referenceID": 24, "context": "Inductive bias is necessary for learning (Mitchell, 1980); we believe the important question is not \u201chow little can a model get away with?\u201d but rather the benefit of different forms of inductive bias as data vary.", "startOffset": 41, "endOffset": 57}, {"referenceID": 6, "context": "Unlike previous works that rely on hand-crafted rules to compose more fine-grained phrase representations (Collins, 1997; Klein and Manning, 2003), the RNNG implicitly parameterizes the information passed through compositions of phrases (in \u0398 and the neural network architecture), hence weakening the strong independence assumptions in classical probabilistic context-free grammars.", "startOffset": 106, "endOffset": 146}, {"referenceID": 19, "context": "Unlike previous works that rely on hand-crafted rules to compose more fine-grained phrase representations (Collins, 1997; Klein and Manning, 2003), the RNNG implicitly parameterizes the information passed through compositions of phrases (in \u0398 and the neural network architecture), hence weakening the strong independence assumptions in classical probabilistic context-free grammars.", "startOffset": 106, "endOffset": 146}, {"referenceID": 10, "context": "This figure is due to Dyer et al. (2016b).", "startOffset": 22, "endOffset": 42}, {"referenceID": 12, "context": "At each time step, the model encodes the stack, buffer, and past actions, with a separate LSTM (Hochreiter and Schmidhuber, 1997) network for each component as features to define a distribution over the next action to take (conditioned on the full algorithmic state).", "startOffset": 95, "endOffset": 129}, {"referenceID": 10, "context": "The overall architecture is illustrated in Figure 1; examples of full action sequences can be found in Dyer et al. (2016b). A key element of the RNNG is the composition function, which reduces a completed constituent into a single element on the stack.", "startOffset": 103, "endOffset": 123}, {"referenceID": 11, "context": "Figure 2: RNNG composition function on each REDUCE operation; the network on the right models the structure on the left (Dyer et al., 2016b).", "startOffset": 120, "endOffset": 140}, {"referenceID": 10, "context": "4 We report all RNNG performance based on the Corrigendum to Recurrent Neural Network Grammars (Dyer et al., 2016a).", "startOffset": 95, "endOffset": 115}, {"referenceID": 31, "context": "Given the same data and a discrimintive (rather than generative) training objective,5 RNNGs were found to parse with significantly higher accuracy than the model of Vinyals et al. (2015) that represents y as a \u201clinearized\u201d sequence of symbols and parentheses without explicitly capturing the tree structure, or constraining the y to be a wellformed tree (see Table 1).", "startOffset": 165, "endOffset": 187}, {"referenceID": 31, "context": "Model F1 Vinyals et al. (2015) \u2013 WSJ only 88.", "startOffset": 9, "endOffset": 31}, {"referenceID": 10, "context": "In Dyer et al. (2016b) and this paper, the proposal distribution is the discriminative variant of the RNNG, see Dyer et al.", "startOffset": 3, "endOffset": 23}, {"referenceID": 10, "context": "In Dyer et al. (2016b) and this paper, the proposal distribution is the discriminative variant of the RNNG, see Dyer et al. (2016b). As mentioned before, RNNGs can be trained either generatively or discriminatively (conditional on the sentence); to isolate the impact of architectural differences between the RNNG model and Vinyals et al.", "startOffset": 3, "endOffset": 132}, {"referenceID": 8, "context": "We trained each ablation from scratch, and compared these models on three tasks: English phrase-structure parsing (labeled F1), Table 2; dependency parsing, Table 3 by converting parse output to Stanford dependencies (De Marneffe et al., 2006), using the tool by Kong and Smith (2014), and language modeling, Table 4.", "startOffset": 221, "endOffset": 285}, {"referenceID": 31, "context": "Note that the ablated RNNG without a stack is similar to Vinyals et al. (2015), who encoded a (partial) phrase-structure tree as a sequence of open and close parentheses, terminals, and non-terminal symbols; our action history is quite close to this, with each NT(X) capturing a left parenthesis and X nonterminal, and each REDUCE capturing a right parenthesis.", "startOffset": 57, "endOffset": 79}, {"referenceID": 24, "context": "Model F1 Petrov and Klein (2007) 90.", "startOffset": 9, "endOffset": 33}, {"referenceID": 24, "context": "Model F1 Petrov and Klein (2007) 90.1 Shindo et al. (2012) 91.", "startOffset": 9, "endOffset": 59}, {"referenceID": 24, "context": "Model F1 Petrov and Klein (2007) 90.1 Shindo et al. (2012) 91.1 Zhu et al. (2013)\u2020 91.", "startOffset": 9, "endOffset": 82}, {"referenceID": 22, "context": "3 McClosky et al. (2006)\u2020 92.", "startOffset": 2, "endOffset": 25}, {"referenceID": 22, "context": "3 McClosky et al. (2006)\u2020 92.1 Vinyals et al. (2015)\u2020 92.", "startOffset": 2, "endOffset": 53}, {"referenceID": 4, "context": "1 Choe and Charniak (2016) 92.", "startOffset": 2, "endOffset": 27}, {"referenceID": 4, "context": "1 Choe and Charniak (2016) 92.6 Choe and Charniak (2016)\u2020 93.", "startOffset": 2, "endOffset": 57}, {"referenceID": 1, "context": "9 Ballesteros et al. (2016) 93.", "startOffset": 2, "endOffset": 28}, {"referenceID": 1, "context": "9 Ballesteros et al. (2016) 93.6 91.4 Wang and Chang (2016) 94.", "startOffset": 2, "endOffset": 60}, {"referenceID": 1, "context": "9 Ballesteros et al. (2016) 93.6 91.4 Wang and Chang (2016) 94.1 91.8 Kuncoro et al. (2016) 94.", "startOffset": 2, "endOffset": 92}, {"referenceID": 0, "context": "1 Andor et al. (2016) 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 0, "context": "1 Andor et al. (2016) 94.6 92.8 Dozat and Manning (2016) 95.", "startOffset": 2, "endOffset": 57}, {"referenceID": 0, "context": "1 Andor et al. (2016) 94.6 92.8 Dozat and Manning (2016) 95.4 93.8 Choe and Charniak (2016)\u2020 95.", "startOffset": 2, "endOffset": 92}, {"referenceID": 5, "context": "Augmenting grammars with lexical head information has a long history in parsing, starting with the models of Collins (1997), and recent theories of syntax such as the \u201cbare phrase structure\u201d hypothesis of the Minimalist Program (Chomsky, 1993) posit that phrases are represented purely by single lexical heads.", "startOffset": 109, "endOffset": 124}, {"referenceID": 16, "context": "likewise exist (Jackendoff, 1977; Keenan, 1987).", "startOffset": 15, "endOffset": 47}, {"referenceID": 5, "context": "To illustrate the contrast between endocentric and exocentric representations, an endocentric representation is representing a noun phrase with a noun category, whereas S\u2192NP VP exocentrically introduces a new syntactic category that is neither NP nor VP (Chomsky, 1970).", "startOffset": 254, "endOffset": 269}, {"referenceID": 1, "context": "To investigate what the RNNG learns about headedness (and later endocentricity), we propose a variant of the composition function that makes use of an explicit attention mechanism (Bahdanau et al., 2015) and a sigmoid gate with multiplicative interactions, henceforth called GA-RNNG.", "startOffset": 180, "endOffset": 203}, {"referenceID": 22, "context": "Similar with Li et al. (2016) where sequential LSTMs discover polarity information in sentiment analysis, although perhaps more surprising as polarity information is less intuitively central to syntactic modeling.", "startOffset": 13, "endOffset": 30}, {"referenceID": 13, "context": "tions are assigned a lower priority when composing PPs, although more consistent with the Johansson head rule (Johansson and Nugues, 2007).", "startOffset": 110, "endOffset": 138}, {"referenceID": 6, "context": "We evaluate the overlap between the resulting dependency tree and conversion results of the same trees using the Collins (Collins, 1997) and Stanford Dependencies (De Marneffe et al.", "startOffset": 121, "endOffset": 136}, {"referenceID": 25, "context": "This idea has been explored in research on methods for learning syntax with less complete annotation (Pereira and Schabes, 1992).", "startOffset": 101, "endOffset": 128}, {"referenceID": 18, "context": "A key finding from Klein and Manning (2002) was that, given bracketing structure, simple dimensionality reduction techniques could reveal conventional non-terminal categories with high accuracy; Petrov et al.", "startOffset": 19, "endOffset": 44}, {"referenceID": 18, "context": "A key finding from Klein and Manning (2002) was that, given bracketing structure, simple dimensionality reduction techniques could reveal conventional non-terminal categories with high accuracy; Petrov et al. (2006) also showed that latent vari-", "startOffset": 19, "endOffset": 216}, {"referenceID": 15, "context": "The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016).", "startOffset": 106, "endOffset": 146}, {"referenceID": 22, "context": "The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016).", "startOffset": 106, "endOffset": 146}, {"referenceID": 15, "context": "The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequenceto-sequence machine translation models capture a certain degree of syntactic knowledge as a byproduct of the translation objective.", "startOffset": 107, "endOffset": 166}, {"referenceID": 31, "context": "the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy with sequence-to-sequence models.", "startOffset": 56, "endOffset": 78}, {"referenceID": 31, "context": "the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy with sequence-to-sequence models.", "startOffset": 56, "endOffset": 106}, {"referenceID": 6, "context": "Extensive previous work on phrase-structure parsing typically employs the probabilistic context-free grammar formalism, with lexicalized (Collins, 1997) and non-terminal (Johnson, 1998; Klein and Manning, 2003) augmentations; the RNNG has less inductive bias than these earlier models and hence a weaker independence assumption.", "startOffset": 137, "endOffset": 152}, {"referenceID": 14, "context": "Extensive previous work on phrase-structure parsing typically employs the probabilistic context-free grammar formalism, with lexicalized (Collins, 1997) and non-terminal (Johnson, 1998; Klein and Manning, 2003) augmentations; the RNNG has less inductive bias than these earlier models and hence a weaker independence assumption.", "startOffset": 170, "endOffset": 210}, {"referenceID": 19, "context": "Extensive previous work on phrase-structure parsing typically employs the probabilistic context-free grammar formalism, with lexicalized (Collins, 1997) and non-terminal (Johnson, 1998; Klein and Manning, 2003) augmentations; the RNNG has less inductive bias than these earlier models and hence a weaker independence assumption.", "startOffset": 170, "endOffset": 210}, {"referenceID": 3, "context": "The conjecture that fine-grained non-terminal rules and labels can be discovered given weaker bracketing structures were based on several studies (Chiang and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006).", "startOffset": 146, "endOffset": 216}, {"referenceID": 18, "context": "The conjecture that fine-grained non-terminal rules and labels can be discovered given weaker bracketing structures were based on several studies (Chiang and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006).", "startOffset": 146, "endOffset": 216}, {"referenceID": 27, "context": "The conjecture that fine-grained non-terminal rules and labels can be discovered given weaker bracketing structures were based on several studies (Chiang and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006).", "startOffset": 146, "endOffset": 216}], "year": 2016, "abstractText": "Recurrent neural network grammars (RNNG) are a recently proposed probablistic generative modeling family for natural language. They show state-ofthe-art language modeling and parsing performance. We investigate what information they learn, from a linguistic perspective, through various ablations to the model and the data, and by augmenting the model with an attention mechanism (GA-RNNG) to enable closer inspection. We find that explicit modeling of composition is crucial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model\u2019s latent attention largely agreeing with predictions made by hand-crafted head rules, albeit with some important differences). By training grammars without non-terminal labels, we find that phrasal representations depend minimally on non-terminals, providing support for the endocentricity hypothesis.", "creator": "TeX"}}}