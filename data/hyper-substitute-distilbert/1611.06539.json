{"id": "1611.06539", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2016", "title": "Efficient Stochastic Inference of Bitwise Deep Neural Networks", "abstract": "recently published methods enable training for effective infinite networks which allow generalized representation of down about 60 single object per weight. we produce code formula that exploits tactical decisions based on multiple stochastically sampled computer models to increase performance figures of bitwise complexity networks across scales of classification accuracy at time. our experiments with resistant cifar - 10 - ssr viruses show that the performance of constrained network ensembles limits geometric significance of certain high - limit hash sets. so fast technique we achieve 5. 81 % best classification error on mir - 10 bucket set using bitwise algorithm. concerning attacks on mesh systems we control embedded traffic flows using much hardware efficient ensemble rounding table. our work comes to efficient intelligent bitwise based networks.", "histories": [["v1", "Sun, 20 Nov 2016 16:05:07 GMT  (307kb,D)", "http://arxiv.org/abs/1611.06539v1", "6 pages, 3 figures, Workshop on Efficient Methods for Deep Neural Networks at Neural Information Processing Systems Conference 2016, NIPS 2016, EMDNN 2016"]], "COMMENTS": "6 pages, 3 figures, Workshop on Efficient Methods for Deep Neural Networks at Neural Information Processing Systems Conference 2016, NIPS 2016, EMDNN 2016", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["sebastian vogel", "christoph schorn", "re guntoro", "gerd ascheid"], "accepted": false, "id": "1611.06539"}, "pdf": {"name": "1611.06539.pdf", "metadata": {"source": "CRF", "title": "Efficient Stochastic Inference of Bitwise Deep Neural Networks", "authors": ["Sebastian Vogel", "Robert Bosch", "Christoph Schorn", "Gerd Ascheid"], "emails": ["sebastian.vogel@de.bosch.com", "christoph.schorn@de.bosch.com", "andre.guntoro@de.bosch.com", "ascheid@ice.rwth-aachen.de"], "sections": [{"heading": "1 Introduction", "text": "Research results in recent years have shown tremendous advances in solving complex problems using deep learning approaches. Especially classification tasks based on image data have been a major target for deep neural networks (DNNs) [8, 14]. A challenge for leveraging the strengths of deep learning methods in embedded systems is their massive computational cost. Even relatively small DNNs often require millions of parameters and billions of operations for performing a single classification. Model compression approaches can help to relax memory requirements as well as to reduce the number of required operations of DNNs. While some approaches consider special network topologies [8, 11], another stream of research focuses on precision reduction of the model parameters. Recent publications of bitwise neural networks (BNNs) have shown that network weights and activations can be reduced from a high-precision floating-point down to a binary representation, while maintaining classification accuracy on benchmark datasets [5]. Stochastic projection of the network weights during training is a key component that enables this strong quantization. Studies\n\u2217These authors contributed equally to this work. \u2020Professor Gerd Ascheid is Senior Member IEEE.\nSubmitted to 1st International Workshop on Efficient Methods for Deep Neural Networks at 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. Copyright c\u00a9 2016 Robert Bosch GmbH. Rights reserved.\nar X\niv :1\n61 1.\n06 53\n9v 1\n[ cs\n.N E\nwhich employed this training method have so far only analyzed deterministic projections during test-time [4, 5, 15].\nWith techniques presented in this paper, we contribute to stochastic inference of bitwise neural networks on hardware. We show that stochastic rounding at test-time improves classification accuracy of networks that were trained with stochastic weight projections (Section 3). Furthermore, we present a method which efficiently realizes stochastic rounding of network weights in a dedicated hardware accelerator (Section 4). We start off with a brief review of the literature on weight discretization (Section 2)."}, {"heading": "2 Related Work", "text": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16]. They employ a method which has already been sketched out by [6]. For each iteration of the back-propagation learning algorithm the high-precision weights of the network are projected to discretized values. The discrete weights are used to compute gradient descent based weight updates, which are then applied to the high-precision weights. This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16]. [4] has recently introduced clipping followed by stochastic rounding as a method for projecting high-precision to binary (-1, +1) weights. Before, [7] used a similar method but with a relatively large number of discretization levels and presented a neural network hardware accelerator using multiply-accumulate-units for stochastic rounding. Instead, we present a method avoiding multipliers."}, {"heading": "3 Stochastic Inference", "text": "Our methods are based on neural networks which are trained with stochastic weight projections. In this section, we show that by applying these projections at test-time, a stochastic ensemble of BNNs can be created whose aggregated classification performance surpasses that of the underlying high-precision floating-point model, while maintaining the benefits of bitwise and multiplierless computations."}, {"heading": "3.1 Stochastic Network Ensembles", "text": "We employ the method introduced in [4] during training and inference. Depending on the number of discrete values we speak of binary or ternary network weights. Clipping limits the numerical range of the weights to the interval [\u22121, 1] and the projection W 7\u2192W d is done by stochastic rounding:\nsround(w) = dwe, with probability p = \u2223\u2223\u2223 bwc\u2212wbwc\u2212dwe \u2223\u2223\u2223 bwc, with probability 1\u2212 p = \u2223\u2223\u2223 dwe\u2212wbwc\u2212dwe \u2223\u2223\u2223 . (1)\nBest test-time results in [4] were achieved with the high-precision neural network parameters W . However, discretized values are much better suited for dedicated hardware accelerators, which is why we investigate inference based on W d. One approach is to perform inference at test-time with the same weight discretization projections as in the training procedure. The reasoning behind this is that the network has been optimized for these projections when minimizing the loss function. With Eqn. (1) as projection function, experiments show a high variance in classification accuracy when the projection is performed only once. Ensembles of classifiers can be used to lower the classification variance of the aggregated classification decision. Using multiple stochastic projections W 7\u2192W d we sample different versions of our neural network and combine their outputs as visualized in Figure 1. The ensemble classification decision is then taken based on this accumulated network output."}, {"heading": "3.2 Experimental Results", "text": "For the first evaluation of our method, we train a ConvNet on the CIFAR-10 classification dataset [13], which contains 60 000 images in 32\u00d732 pixel RGB resolution and 10 different classes. We use\nthe setup described in [4] for training, but with sign3 activation function as in [5] and stochastic ternary weights. The network structure is 128C3\u2013128C3\u2013MP2\u2013256C3\u2013256C3\u2013MP2\u2013512C3\u2013512C3\u2013 MP2\u20131024FC\u20131024FC\u201310SVM4. After training the model for 500 epochs with hyperparameters from [4] and without any preprocessing or augmentations on the dataset, we select high-precision model parameters which have the lowest error on the validation set. These weights are used to generate multiple instances of the network by rounding the weights stochastically to ternary values (see Section 3.1). Classification error rates on the CIFAR-10 test set based on the ensemble decision for different accumulation lengths, i. e. numbers of ensemble members, are plotted in Figure 2a. Since classification results are not deterministic in this case, we run the whole experiment 20\u00d7 and provide mean and standard deviation. In our experiment, a stochastic BNN ensemble with at least four members always performs better than the floating-point reference model, which achieves a classification error of 10.74%.\nBetter classification results can be achieved when the same network is trained with ReLU activation function, binary projections, global contrast normalization and ZCA whitening, as well as augmentations on the training data. We apply a commonly used simple data augmentation method [9], consisting of a random translation of up to 4 pixels in the image plane and a random flip around the vertical axis. Classification results for this setup using ternary projections at test-time are shown in Figure 2b. The best result of 5.81% was reached with an ensemble of 29 networks. To the best of our knowledge we are the first to report a classification error of less than 6% on the CIFAR-10 benchmark using bitwise neural networks.\nIn addition, we test our method on the German Traffic Sign Recognition Benchmark dataset [17]. The resulting high-precision network with sign activation leads to 2.19% classification error. For\n3sign(x): 1 for \u2265 0, \u22121 otherwise. 4Preceding numbers indicate the number of channels, C3 denotes a convolution layer with 3\u00d73 kernel, MP2 abbreviates spatial max-pooling with a receptive field of 2\u00d72, FC stands for fully connected layers and SVM for a square hinge loss output layer.\n20 evaluations, a single projected bitwise network results in 2.73% mean error rate (0.092% std.) whereas ensembles of 11 networks reach 1.79% mean error rate (0.042% std.). The best result of 1.63% was achieved with 16 ensemble members.\nInterestingly, the mean performance of discretized ensembles reach better classification results than the high-precision base model. We believe that due to the gradient descent optimization of the loss function which is evaluated for discrete values, best results are achieved with projected versions of the base model."}, {"heading": "4 Efficient Stochastic Rounding in Hardware", "text": "In order to fully exploit the performance of bitwise neural networks in terms of accuracy, the BNN needs to be evaluated more than once and therefore an efficient integration of a stochastic rounding engine is necessary. Based on the publications [2] and [3], a simple multiplexer can be used to perform sround(x) (see Eqn. (1)). Assuming the probability of the select signal sel of an N-to-1 multiplexer to route signal ini \u2208 {0, 1} to the output is equally distributed, the probability of the output signal out being 1 can be written as\nP (out = 1) = N\u2211 i=1 iniP (sel = i) = N\u2211 i=1 ini 1 N . (2)\nHence, the probability P (out = 1) is determined by the number of ones at the input in. However, if the probability function P (sel = i) is chosen to be\nP (sel = i) = 2i\u22121\n2N \u2212 1 , (3)\nthe probability P (out = 1) is directly related to the input in. Additionally, considering in as a binary coded5 fractional number \u2208 [0, 1) then P (out = 1) \u2248 in with a maximum error of 1\n2N . In order to\nuse this technique in hardware, the corresponding signal for sel has to be generated by individual select wires selj . Whereas [2] considers the N equations (3) as an overdetermined problem and proposes a numerical solution, we present an analytic solution to the problem. There are log2(N) individual select bits selj with\nP (selj = 1) = 22\nj\u22121\n22j\u22121 + 1 , P (selj = 0) =\n1\n22j\u22121 + 1\n\u21d2 log2(N)\u220f j=1 P (selj) = P (sel), because log2(M)\u220f k=1 ( 22 k\u22121 + 1 ) = 2M \u2212 1.\n(4)\nBitstreams for selj with the corresponding frequencies can be generated using a linear feedback shift register (LFSR) in combination with Daalen modulators [18].\nIn order to verify the concept of stochastic rounding engines for neural networks using the method presented above, we evaluated the network for road sign recognition with weights stochastically projected in hardware. The results presented in Section 3.2 have been reproduced using this approach. To take a potential hardware parallelization into consideration, we also performed projections in parallel over the dimension of output features. As the generation of random bitstreams using LFSRs is expensive in terms of energy and hardware resources, we evaluated the classification performance when using a single pseudo random bitstream (PRBS) generator to provide the same select signal for all stochastic rounders (i.e. multiplexers) in the network. We found that relying on a single PRBS generator retains mean classification accuracy. Moreover, the mean network performance is preserved when only a single LFSR is used to generate a random base bitstream which is then subject to different modulations [18] to generate PRBS with appropriate frequencies of 1\u2019s (see Eqn. (4)).\n5inN corresponds to the most significant bit (MSB)."}, {"heading": "5 Conclusion and Outlook", "text": "We investigated bitwise neural networks with stochastically projected weights during inference. Results show that an ensemble-based decision of multiple versions of such a BNN enhances performance compared to the inference based on the high-precision shadow weights. Furthermore, we presented a hardware efficient stochastic rounding procedure for the first time used on bitwise DNNs. Our results show that this technique can be used for test-time inference enabling efficient hardware implementation in embedded systems.\nThe methods proposed in [4] and [5] rely on stochastic projections during training. Future research will investigate the integration of our generalized form of stochastic rounding into the training process."}], "references": [{"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["S. Anwar", "K. Hwang", "W. Sung"], "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Lookup table based neural network using fpga", "author": ["S.L. Bade"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Fpga-based stochastic neural networks-implementation", "author": ["S.L. Bade", "B.L. Hutchings"], "venue": "In Proceedings of the IEEE Workshop on FPGAs for Custom Computing Machines", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "ArXiv e-prints,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1", "author": ["M. Courbariaux", "I. Hubara", "D. Soudry", "R. El-Yaniv", "Y. Bengio"], "venue": "ArXiv e-prints,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Weight discretization paradigm for optical neural networks", "author": ["E. Fiesler", "A. Choudry", "H.J. Caulfield"], "venue": "In Proceedings of SPIE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "Deep Learning with Limited Numerical Precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ArXiv e-prints, December", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Deep Networks with Stochastic Depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K. Weinberger"], "venue": "ArXiv e-prints, March 2016", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Fixed-point feed forward deep neural network design using weights +1, 0 and -1", "author": ["K. Hwang", "W. Sung"], "venue": "IEEE Workshop on Signal Processing Systems (SiPS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size", "author": ["F.N. Iandola", "S. Han", "M.W. Moskewicz", "K. Ashraf", "W.J. Dally", "K. Keutzer"], "venue": "ArXiv e-prints,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "X1000 real-time phoneme recognition vlsi using feed-forward deep neural networks", "author": ["J. Kim", "K. Hwang", "W. Sung"], "venue": "In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["A. Krizhevsky"], "venue": "Technical report,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Deep neural networks are robust to weight binarization and other non-linear distortions", "author": ["P. Merolla", "R. Appuswamy", "J. Arthur", "S.K. Esser", "D. Modha"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "ArXiv e-prints, March 2016", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "The German Traffic Sign Recognition Benchmark: A multi-class classification competition", "author": ["J. Stallkamp", "M. Schlipsing", "J. Salmen", "C. Igel"], "venue": "IEEE International Joint Conference on Neural Networks,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Device for generating binary sequences for stochastic computing", "author": ["M. van Daalen", "P. Jeavons", "J. Shawe-Taylor", "D. Cohen"], "venue": "In Electronics Letters,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}], "referenceMentions": [{"referenceID": 7, "context": "Especially classification tasks based on image data have been a major target for deep neural networks (DNNs) [8, 14].", "startOffset": 109, "endOffset": 116}, {"referenceID": 13, "context": "Especially classification tasks based on image data have been a major target for deep neural networks (DNNs) [8, 14].", "startOffset": 109, "endOffset": 116}, {"referenceID": 7, "context": "While some approaches consider special network topologies [8, 11], another stream of research focuses on precision reduction of the model parameters.", "startOffset": 58, "endOffset": 65}, {"referenceID": 10, "context": "While some approaches consider special network topologies [8, 11], another stream of research focuses on precision reduction of the model parameters.", "startOffset": 58, "endOffset": 65}, {"referenceID": 4, "context": "Recent publications of bitwise neural networks (BNNs) have shown that network weights and activations can be reduced from a high-precision floating-point down to a binary representation, while maintaining classification accuracy on benchmark datasets [5].", "startOffset": 251, "endOffset": 254}, {"referenceID": 3, "context": "which employed this training method have so far only analyzed deterministic projections during test-time [4, 5, 15].", "startOffset": 105, "endOffset": 115}, {"referenceID": 4, "context": "which employed this training method have so far only analyzed deterministic projections during test-time [4, 5, 15].", "startOffset": 105, "endOffset": 115}, {"referenceID": 14, "context": "which employed this training method have so far only analyzed deterministic projections during test-time [4, 5, 15].", "startOffset": 105, "endOffset": 115}, {"referenceID": 0, "context": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16].", "startOffset": 183, "endOffset": 208}, {"referenceID": 3, "context": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16].", "startOffset": 183, "endOffset": 208}, {"referenceID": 4, "context": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16].", "startOffset": 183, "endOffset": 208}, {"referenceID": 9, "context": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16].", "startOffset": 183, "endOffset": 208}, {"referenceID": 11, "context": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16].", "startOffset": 183, "endOffset": 208}, {"referenceID": 14, "context": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16].", "startOffset": 183, "endOffset": 208}, {"referenceID": 15, "context": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16].", "startOffset": 183, "endOffset": 208}, {"referenceID": 5, "context": "They employ a method which has already been sketched out by [6].", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16].", "startOffset": 95, "endOffset": 106}, {"referenceID": 9, "context": "This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16].", "startOffset": 95, "endOffset": 106}, {"referenceID": 11, "context": "This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16].", "startOffset": 95, "endOffset": 106}, {"referenceID": 3, "context": "This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16].", "startOffset": 145, "endOffset": 159}, {"referenceID": 4, "context": "This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16].", "startOffset": 145, "endOffset": 159}, {"referenceID": 14, "context": "This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16].", "startOffset": 145, "endOffset": 159}, {"referenceID": 15, "context": "This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16].", "startOffset": 145, "endOffset": 159}, {"referenceID": 3, "context": "[4] has recently introduced clipping followed by stochastic rounding as a method for projecting high-precision to binary (-1, +1) weights.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Before, [7] used a similar method but with a relatively large number of discretization levels and presented a neural network hardware accelerator using multiply-accumulate-units for stochastic rounding.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "We employ the method introduced in [4] during training and inference.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "Best test-time results in [4] were achieved with the high-precision neural network parameters W .", "startOffset": 26, "endOffset": 29}, {"referenceID": 12, "context": "For the first evaluation of our method, we train a ConvNet on the CIFAR-10 classification dataset [13], which contains 60 000 images in 32\u00d732 pixel RGB resolution and 10 different classes.", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "the setup described in [4] for training, but with sign3 activation function as in [5] and stochastic ternary weights.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "the setup described in [4] for training, but with sign3 activation function as in [5] and stochastic ternary weights.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "After training the model for 500 epochs with hyperparameters from [4] and without any preprocessing or augmentations on the dataset, we select high-precision model parameters which have the lowest error on the validation set.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "We apply a commonly used simple data augmentation method [9], consisting of a random translation of up to 4 pixels in the image plane and a random flip around the vertical axis.", "startOffset": 57, "endOffset": 60}, {"referenceID": 16, "context": "In addition, we test our method on the German Traffic Sign Recognition Benchmark dataset [17].", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "Based on the publications [2] and [3], a simple multiplexer can be used to perform sround(x) (see Eqn.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Based on the publications [2] and [3], a simple multiplexer can be used to perform sround(x) (see Eqn.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "Whereas [2] considers the N equations (3) as an overdetermined problem and proposes a numerical solution, we present an analytic solution to the problem.", "startOffset": 8, "endOffset": 11}, {"referenceID": 17, "context": "Bitstreams for selj with the corresponding frequencies can be generated using a linear feedback shift register (LFSR) in combination with Daalen modulators [18].", "startOffset": 156, "endOffset": 160}, {"referenceID": 17, "context": "Moreover, the mean network performance is preserved when only a single LFSR is used to generate a random base bitstream which is then subject to different modulations [18] to generate PRBS with appropriate frequencies of 1\u2019s (see Eqn.", "startOffset": 167, "endOffset": 171}], "year": 2016, "abstractText": "Recently published methods enable training of bitwise neural networks which allow reduced representation of down to a single bit per weight. We present a method that exploits ensemble decisions based on multiple stochastically sampled network models to increase performance figures of bitwise neural networks in terms of classification accuracy at inference. Our experiments with the CIFAR-10 and GTSRB datasets show that the performance of such network ensembles surpasses the performance of the high-precision base model. With this technique we achieve 5.81% best classification error on CIFAR-10 test set using bitwise networks. Concerning inference on embedded systems we evaluate these bitwise networks using a hardware efficient stochastic rounding procedure. Our work contributes to efficient embedded bitwise neural networks.", "creator": "LaTeX with hyperref package"}}}