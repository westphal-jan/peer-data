{"id": "1611.07659", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Improving Efficiency of SVM k-Fold Cross-Validation by Alpha Seeding", "abstract": "numerical km - metric crossed - propagation architecture commonly used while evaluate expected effectiveness of svms with fewer dominant y - pixels. therefore is technically not an simplest k - fold cross - modification is expensive, since analysis requires training k actors. usually, little work has explored proper practical h - th svm for modelling the ( h + 80 ) - oriented svm approach improving the beam versus parallel - slice cross - reactions. in this paper, others specify three algorithms that create the h - th svm consistently improving the efficiency over future generalized ( z + 0 ) - gp procedures. our key idea came to efficiently extract stronger support vectors chosen to accurately maximize their base weights ( also called alpha values ) of the next generation by using different previous svm. our original results explain that our algorithms performs several times faster from the k - h cross - validation system does not make value of the previously trained programs. moreover, our algorithms produce the same algorithms ( hence same objectives ) as the k - spectral cross - measurements which does not produce usage of the previously trained svm.", "histories": [["v1", "Wed, 23 Nov 2016 06:48:25 GMT  (49kb,D)", "https://arxiv.org/abs/1611.07659v1", "9 pages, 2 figures, accepted by AAAI-17"], ["v2", "Sat, 4 Feb 2017 17:28:38 GMT  (38kb)", "http://arxiv.org/abs/1611.07659v2", "9 pages, 2 figures, accepted by AAAI-17"]], "COMMENTS": "9 pages, 2 figures, accepted by AAAI-17", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zeyi wen", "bin li", "kotagiri ramamohanarao", "jian chen", "yawen chen", "rui zhang 0003"], "accepted": true, "id": "1611.07659"}, "pdf": {"name": "1611.07659.pdf", "metadata": {"source": "CRF", "title": "Improving Efficiency of SVM k-fold Cross-validation by Alpha Seeding", "authors": ["Zeyi Wen", "Bin Li", "Kotagiri Ramamohanarao", "Jian Chen", "Yawen Chen", "Rui Zhang"], "emails": ["wenzeyi@gmail.com", "kotagiri}@unimelb.edu.au", "{gitlinux@gmail.com,", "ellachen@scut.edu.cn,", "elfairyhyuk@gmail.com}"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n07 65\n9v 2\n[ cs\n.L G\n] 4\nF eb\n2 01\nSVM. Our key idea is to efficiently identify the support vectors and to accurately estimate their associated weights (also called alpha values) of the next SVM by using the previous SVM. Our experimental results show that our algorithms are several times faster than the k-fold cross-validation which does not make use of the previously trained SVM. Moreover, our algorithms produce the same results (hence same accuracy) as the k-fold cross-validation which does not make use of the previously trained SVM."}, {"heading": "1 Introduction", "text": "In order to train an effective SVM classifier, the hyper-parameters (e.g. the penalty C) need to be selected carefully. The k-fold cross-validation is a commonly used process to evaluate the effectiveness of SVMs with the selected hyper-parameters. It is known that the SVM k-fold cross-validation is expensive, since it requires training k SVMs with different subsets of the whole dataset. To improve the efficiency of k-fold cross-validation, some recent studies (Wen et al. 2014; Athanasopoulos et al. 2011) exploit modern hardware (e.g. Graphic Processing Units). Chu et al. (Chu et al. 2015) proposed to reuse the k linear SVM classifiers trained in the k-fold cross-validation with parameter C for training the k linear SVM classifiers with parameter (C + \u2206). However, little work has explored the possibility of reusing the hth (where h \u2208 {1, 2, ..., (k \u2212 1)}) SVM for improving the efficiency of training the (h+1)th SVM in the k-fold crossvalidation with parameter C.\n\u2217Jian Chen is the corresponding author. Copyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nIn this paper, we propose three algorithms that reuse the hth SVM for training the (h + 1)th SVM in k-fold cross-validation. The intuition behind our algorithms is that the hyperplanes of the two SVMs are similar, since many training instances (e.g. more than 80% of the training instances when k is 10) are the same in training the two SVMs. Note that in this paper we are interested in k > 2, since when k = 2 the two SVMs share no training instance.\nWe present our ideas in the context of training SVMs using Sequential Minimal Optimisation (SMO) (Platt and others 1998), although our ideas are applicable to other solvers (Osuna, Freund, and Girosi 1997; Joachims 1999). In SMO, the hyperplane of the SVM is represented by a subset of training instances together with their weights, namely alpha values. The training instances with alpha values larger than 0 are called support vectors. Finding the optimal hyperplane is effectively finding the alpha values for all the training instances. Without reusing the previous SVM, the alpha values of all the training instances are initialised to 0. Our key idea is to use the alpha values of the hth SVM to initialise the alpha values for the (h + 1)th SVM. Initialising alpha values using the previous SVM is called alpha seeding in the literature of studying leave-one-out cross-validation (DeCoste and Wagstaff 2000). At some risk of confusion to the reader, we will use \u201calpha seeding\u201d and \u201cinitialising alpha values\u201d interchangeably, depending on which interpretation is more natural.\nReusing the hth SVM for training the (h+1)th SVM in k-fold cross-validation has two key challenges. (i) The training dataset for the hth SVM is different from that for the (h+1)th SVM, but the initial alpha values for the (h+ 1)th SVM should be close to their optimal values; improper initialisation of alpha values leads to slower convergence than without reusing the hth SVM. (ii) The alpha value initialisation process should be very efficient; otherwise, the time spent in the initialisation may be larger than that saved in the training. This is perhaps the reason that existing work either (i) reuses the hth SVM trained with parameter C for training the\nhth SVM with parameter (C + \u2206) where both SVMs have the identical training dataset (Chu et al. 2015) or (ii) only studies alpha seeding in leave-oneout cross-validation (DeCoste and Wagstaff 2000; Lee et al. 2004) which is a special case of k-fold cross-validation. Our key contributions in this paper are the proposal of three algorithms (where we progressively refine one algorithm after the other) for reusing the alpha values of the hth SVM for the (h+1)th SVM. (i) Our first algorithm aims to initialise the alpha values to their optimal values for the (h+1)th SVM by exploiting the optimality condition of the SVM training. (ii) To efficiently compute the initial alpha values, our second algorithm only estimates the alpha values for the newly added instances, based on the assumption that all the shared instances between the hth and the (h+1)th SVMs tend to have the same alpha values. (iii) To further improve the efficiency of initialising alpha values, our third algorithm exploits the fact that a training instance in the hth SVM can be potentially replaced by a training instance in the (h+ 1)th SVM. Our experimental results show that when k = 10, our algorithms are several times faster than the k-fold cross-validation in LibSVM; when k = 100, our algorithm dramatically outperforms LibSVM (32 times faster in the Madelon dataset). Moreover, our algorithms produce the same results (hence same accuracy) as LibSVM. The remainder of this paper is organised as follows. We describe preliminaries in Section 2. Then, we elaborate our three algorithms in Section 3, and report our experimental study in Section 4. In Section 5 and 6, we review the related literature, and conclude this paper."}, {"heading": "2 Preliminaries", "text": "Here, we give some details of SVMs, and discuss the relationship of two rounds of k-fold cross-validation.\nSupport Vector Machines\nAn instance xi is attached with an integer yi \u2208 {+1,\u22121} as its label. A positive (negative) instance is an instance with the label of +1 (\u22121). Given a set X of n training instances, the goal of the SVM training is to find a hyperplane that separates the positive and the negative training instances in X with the maximum margin and meanwhile, with the minimum misclassification error on the training instances. To enable handily mapping training instances to other data spaces by kernel functions, finding the hyperplane can be expressed in a dual form (Bennett and Bredensteiner 2000) as the following quadratic programming problem (Nocedal and Wright 2006).\nargmax \u03b1\nn \u2211\ni=1\n\u03b1i \u2212 1\n2 \u03b1\nT Q\u03b1\nsubject to 0 \u2264 \u03b1i \u2264 C,\u2200i \u2208 {1, ..., n};\nn \u2211\ni=1\nyi\u03b1i = 0\n(1)\nwhere \u03b1 \u2208 Rn is also called a weight vector, and \u03b1i denotes the weight of xi; Q denotes an n \u00d7 n matrix [Qi,j] and Qi,j = yiyjK(xi,xj), and K(xi,xj) is a kernel value computed from a kernel function (e.g. Gaussian kernel, K(xi,xj) = exp{\u2212\u03b3||xi \u2212 xj ||\n2}). Then, the goal of the SVM training is to find the optimal \u03b1. If \u03b1i is greater than 0, xi is called a support vector. In this paper, we present our ideas in the context of using SMO to solve Problem (1), although our key ideas are applicable to other solvers (Osuna, Freund, and Girosi 1997; Joachims 1999). The training process and the derivation of the optimality condition are unimportant for understanding our algorithms, and hence are not discussed here. Next, we present the optimality condition for the SVM training which will be exploited in our proposed algorithms in Section 3.\nThe optimality condition for the SVM training In SMO, a training instance xi is associated with an optimality indicator fi which is defined as follows.\nfi = yi\nn \u2211\nj=1\n\u03b1jQi,j \u2212 yi (2)\nThe optimality condition of the SVM training is the Karush-Kuhn-Tucker (KKT) (Kuhn 2014) condition. When the optimality condition is met, we have the optimality indicators satisfying the following constraint.\nmin{fi|i \u2208 Iu \u222a Im} \u2265 max{fi|i \u2208 Il \u222a Im} (3)\nwhere\nIm = {i|xi \u2208 X , 0 < \u03b1i < C}, Iu = {i|xi \u2208 X , yi = +1, \u03b1i = 0} \u222a {i|xi \u2208 X , yi = \u22121, \u03b1i = C}, Il = {i|xi \u2208 X , yi = +1, \u03b1i = C} \u222a {i|xi \u2208 X , yi = \u22121, \u03b1i = 0}. (4)\nAs observed by Keerthi et al. (Keerthi et al. 2001), Constraint (3) is equivalent to the following constraints.\nfi > b for i \u2208 Iu; fi = b for i \u2208 Im; fi < b for i \u2208 Il (5)\nwhere b is the bias of the hyperplane. Our algorithms proposed in Section 3 exploit Constraint (5).\nRelationship between the hth round and the (h+1)th round in k-fold cross-validation\nThe k-fold cross-validation evenly divides the dataset into k subsets. One subset is used as the test set T , while the rest (k\u22121) subsets together form the training set X . Suppose we have trained the hth SVM (in the hth round) using the 1st to (h\u22121)th and (h+1)th to kth\nsubsets as the training set, and the hth subset serves as the testing set (cf. Figure 1b). Now we want to train the (h + 1)th SVM. Then, the 1st to (h \u2212 1)th subsets and the (h + 2)th to kth subsets are shared between the two rounds of the training. To convert the training set used in the hth round to the training set for the (h + 1)th round, we just need to remove the (h + 1)th subset from and add the hth subset to the training set used in the hth round. Hereafter, we call the hth and (h + 1)th SVMs the previous SVM and the next SVM, respectively. For ease of presentation, we denote the shared subsets\u2014(k\u2212 2) subsets in total\u2014by S, denote the unshared subset in the training of the previous round by R, and denote the subset for testing in the previous round by T . Let us continue to use the example shown in Figure 1, S consists of the 1st to (h\u22121)th subsets and the (h + 2)th to kth subsets; R is the (h + 1)th subset; T is the hth subset. To convert the training set X used in the hth round to the training set X \u2032 for the (h+1)th round, we just need to remove R from X and add T to X , i.e. X \u2032 = T \u222a X \\R = T \u222a S. We denote three sets of indices as follows corresponding to R, T and S by IR, IT and IS , respectively.\nIR = {i|xi \u2208 R}, IT = {i|xi \u2208 T }, IS = {i|xi \u2208 S} (6)\nTwo rounds of the k-fold cross-validation often have many training instances in common, i.e. large S. E.g. when k is 10, 8\n9 (or \u223c 90%) of instances in X and X \u2032\nare the instances of S. Next, we study three algorithms for reusing the previous SVM to train the next SVM.\n3 Reusing the previous SVM in k-fold\ncross-validation\nWe present three algorithms that reuse the previous SVM for training the next SVM, where we progressively refine one algorithm after the other. (i) Our first algorithm aims to initialise the alpha values \u03b1\u2032 to their optimal values for the next SVM, based on the alpha values \u03b1 of the previous SVM. We call the first algorithm Adjusting Alpha Towards Optimum (ATO). (ii) To efficiently initialise \u03b1\u2032, our second algorithm keeps the alpha values of the instances in S unchanged (i.e. \u03b1\u2032s = \u03b1s for s \u2208 IS), and estimates \u03b1 \u2032 t for t \u2208 IT . This algorithm effectively performs alpha value initialisation via replacing R by T under constraints of Problem (1), and hence we call the algorithm Multiple Instance Replacement (MIR). (iii) Similar to MIR, our third algorithm also keeps the alpha values of the instances in S unchanged; different from MIR, the algorithm replaces the instances in R by the instances in T one at a time, which dramatically reduces the time for initialising \u03b1\u2032. We call the third algorithm Single Instance Replacement (SIR). Next, we elaborate these three algorithms."}, {"heading": "Adjusting Alpha Towards Optimum (ATO)", "text": "ATO aims to initialise the alpha values to their optimal values. It employs the technique for online SVM training, designed by Karasuyama and\nTakeuchi (Karasuyama and Takeuchi 2009), for the kfold cross-validation. In the online SVM training, a subsetR of outdated training instances is removed from the training set X , i.e. X \u2032 = X \\R; a subset T of newly arrived training instances is added to the training set, i.e. X \u2032 = X \u2032\u222aT . The previous SVM trained using X is adjusted by removing and adding subsets of instances to obtain the next SVM. In the ATO algorithm, we first construct a new training dataset X \u2032 where X \u2032 = S = X \\R. Then, we gradually increase alpha values of the instances in T (i.e. increase \u03b1\u2032t for t \u2208 IT ), denoted by \u03b1 \u2032 T , to (near) their optimal values; meanwhile, we gradually decrease the alpha values of the instances in R (i.e. decrease \u03b1\u2032r for r \u2208 IR), denoted by \u03b1 \u2032 R, to 0. Once the alpha value of an instance in T satisfies the optimal condition (i.e. Constraint (5)), we move the instance from T to the training set X \u2032; similarly once the alpha value of an instance in R equals to 0 (becoming a non-support vector), we remove the instance from R. ATO terminates the alpha value initialisation when R is empty.\nUpdating the alpha values Next, we present details of increasing \u03b1\u2032T and decreasing \u03b1 \u2032 R. We denote the step size for an increment on \u03b1\u2032T and decrement on \u03b1\u2032R by \u03b7. From constraints of Problem (1), all the alpha values must be in [0, C]. Hence, for t \u2208 IT the increment of \u03b1\u2032t, denoted by \u2206\u03b1 \u2032 t, cannot exceed (C\u2212\u03b1 \u2032 t); for r \u2208 IR the decrement of \u03b1 \u2032 r, denoted by \u2206\u03b1 \u2032 r, cannot exceed \u03b1\u2032r. We denote the change of all the alpha values of the instances in T by \u2206\u03b1\u2032T and the change of all the alpha values of the instances in R by \u2206\u03b1\u2032R. Then, we can compute \u2206\u03b1\u2032T and \u2206\u03b1 \u2032 R as follows.\n\u2206\u03b1\u2032T = \u03b7(C1\u2212\u03b1 \u2032 T ), \u2206\u03b1 \u2032 R = \u2212\u03b7\u03b1 \u2032 R (7)\nwhere 1 is a vector with all the dimensions of 1. When we add \u2206\u03b1\u2032T to \u03b1 \u2032 T and \u2206\u03b1 \u2032 R to \u03b1 \u2032 R, constraints of Problem (1) must be satisfied. However, after adjusting \u03b1\u2032T and \u03b1 \u2032 R, the constraint \u2211 i\u2208IT \u222aIS\u222aIR yi\u03b1 \u2032 i = 0 is often violated, so we need to adjust the alpha values of the training instances in X \u2032 (recall that at this stage X \u2032 = S). We propose to adjust the alpha values of the training instances in X \u2032 which are also in M where xi \u2208 M given i \u2208 Im. In summary, after increasing \u03b1 \u2032 T and decreasing \u03b1\u2032R, we adjust \u03b1 \u2032 M. So when adjusting \u03b1 \u2032 T , \u03b1\u2032R and \u03b1 \u2032 M, we have the following equation according to constraints of Problem (1). \u2211\nt\u2208IT\nyt\u2206\u03b1 \u2032 t +\n\u2211\nr\u2208IR\nyr\u2206\u03b1 \u2032 r +\n\u2211\ni\u2208Im\nyi\u2206\u03b1 \u2032 i = 0 (8)\nM often has a large number of instances, and there are many possible ways to adjust \u03b1\u2032M. Here, we propose to use the adjustment on \u03b1\u2032M that ensures all the training instances in M satisfy the optimality condition (i.e. Constraint (5)). According to Constraint (5), we have \u2200i \u2208 Im and fi = b. Combining fi = b and the definition of fi (cf. Equation (2)), we have the following equation for each i \u2208 Im.\nyi( \u2211\nt\u2208IT\nQi,t\u2206\u03b1 \u2032 t +\n\u2211\nr\u2208IR\nQi,r\u2206\u03b1 \u2032 r +\n\u2211\nj\u2208Im\nQi,j\u2206\u03b1 \u2032 j) = 0 (9)\nNote that yi can be omitted in the above equation. We can rewrite Equation (8) and Equation (9) using the matrix notation for all the training instances in M.\n[\nyTT y T R\nQM,T QM,R\n] [\n\u2206\u03b1\u2032T \u2206\u03b1\u2032R\n]\n+\n[\nyTM QM,M\n]\n\u2206\u03b1\u2032M = 0\nWe substitute \u2206\u03b1\u2032T and \u2206\u03b1 \u2032 R using Equation (7); the above equation can be rewritten as follows.\n\u2206\u03b1 \u2032 M = \u2212\u03b7\u03a6 (10)\nwhere \u03a6 =\n[\nyTM QM,M\n]\u22121 [\nyTT y T R\nQM,T QM,R\n] [\nC1\u2212\u03b1\u2032T \u2212\u03b1\u2032R\n]\n. If\nthe inverse of the matrix in Equation (10) does not exist, we find the pseudo inverse (Greville 1960) Computing step size \u03b7: Given an \u03b7, we can use Equations (7) and (10) to adjust \u03b1\u2032M, \u03b1 \u2032 T and \u03b1 \u2032 R. The changes of the alpha values lead to the change of all the optimality indicators f . We denote the change to f by \u2206f which can be computed by the following equation derived from Equation (2).\ny\u2299\u2206f = \u03b7[\u2212QX ,M\u03a6+QX ,T (C1\u2212\u03b1 \u2032 T )\u2212QX ,R\u03b1 \u2032 R] (11)\nwhere \u2299 is the hadamard product (i.e. element-wise product (Schott 2005)). If the step size \u03b7 is too large, more optimality indicators tend to violate Constraint (5). Here, we use Equation (11) to compute the step size \u03b7 by letting the updated fi (where i \u2208 Iu \u222a Il) just violate Constraint (5), i.e. fi +\u2206fi = b for i \u2208 Iu \u222a Il.\nUpdating f After updating \u03b1\u2032, we update f using Equations (2) and (11). Then, we update the sets Im, Iu and Il according to Constraint (5). The process of computing \u03b7 and updating \u03b1\u2032 and f are repeated until R is empty.\nTermination When R is empty, the SVM may not be optimal, because the set T may not be empty. The alpha values obtained from the above process serve as the initial alpha values for the next SVM. To obtain the optimal SVM, we use SMO to adjust the initial alpha values until optimal condition is met. The pseudo-code of the full algorithm is shown in Algorithm 1 in Supplementary Material.\nMultiple Instance Replacement (MIR)\nA limitation of ATO is that it requires adjusting all the alpha values for an unbounded number of times (i.e. until R is empty). Hence, the cost of initialising the alpha values may be very high. In what follows, we propose the Multiple Instance Replacement (MIR) algorithm that only needs to adjust \u03b1\u2032T once. The alpha values of the shared instances between the two rounds stay unchanged (i.e. \u03b1\u2032S = \u03b1S), the intuition is that many support vectors tend to stay unchanged. The key idea of MIR is to replace R by T at once. We obtain the alpha values of the instances in S and R from the previous SVM, and those alpha values satisfy the following constraint.\n\u2211\ns\u2208IS\nys\u03b1s + \u2211\nr\u2208IR\nyr\u03b1r = 0 (12)\nIn the next round of SVM k-fold cross-validation,R is removed and T is added. When reusing alpha values, we should guarantee that the above constraint holds. To improve the efficiency of initialising alpha values, we do not change alpha values in first term of Constraint (12), i.e. \u2211\ns\u2208IS ys\u03b1s.\nTo satisfy the above constraint after replacing R by T , we only need to ensure \u2211\nr\u2208IR yr\u03b1r =\n\u2211\nt\u2208IT yt\u03b1 \u2032 t.\nNext, we present an approach to compute \u03b1\u2032T . According to Equation (2), we can rewrite fi before replacing R by T as follows.\nfi = yi( \u2211\nr\u2208IR\n\u03b1rQi,r + \u2211\ns\u2208IS\n\u03b1sQi,s \u2212 1) (13)\nAfter replacing R by T , fi can be computed as follows.\nfi = yi( \u2211\nt\u2208IT\n\u03b1 \u2032 tQi,t +\n\u2211\ns\u2208IS\n\u03b1 \u2032 sQi,s \u2212 1) (14)\nwhere \u03b1\u2032s = \u03b1s, i.e. the alpha values in S stay unchanged. We can compute the change of fi, denoted by \u2206fi, by subtracting Equation (13) from Equation (14). Then, we have the following equation.\n\u2206fi = yi[ \u2211\nt\u2208IT\n\u03b1 \u2032 tQi,t \u2212\n\u2211\nr\u2208IR\n\u03b1rQi,r] (15)\nTo meet the constraint \u2211\nyi\u03b1i = 0 after replacing R by T , we have the following equation.\n\u2211\ns\u2208IS\nys\u03b1s + \u2211\nr\u2208IR\nyr\u03b1r = \u2211\ns\u2208IS\nys\u03b1 \u2032 s +\n\u2211\nt\u2208IT\nyt\u03b1 \u2032 t\nAs \u03b1\u2032s = \u03b1s, we rewrite the above equation as follows. \u2211\nr\u2208IR\nyr\u03b1r = \u2211\nt\u2208IT\nyt\u03b1 \u2032 t (16)\nWe write Equations (15) and (16) together as follows. [\ny \u2299\u2206f +QX ,R\u03b1R yTR \u00b7\u03b1R\n]\n=\n[\nQX ,T yTT\n]\n\u03b1 \u2032 T (17)\nSimilar to the way we compute \u2206fi in the ATO algorithm, given i in Iu \u222a Il we compute \u2206fi by letting fi + \u2206fi = b (cf. Constraint (5)). Given i in Im, we set \u2206fi = 0 since we try to avoid fi violating Constraint (5). Once we have \u2206f , the only unknown in Equation (17) is \u03b1\u2032T .\nFinding an approximate solution for \u03b1\u2032T The linear system shown in Equation (17) may have no solution. This is because \u03b1\u2032S may also need to be adjusted, but is not considered in Equation (17). Here, we propose to find the approximate solution \u03b1\u2032T for Equation (17) by using linear least squares (Lawson and Hanson 1974) and we have the following equation. [\nQX ,T yTT\n]T [\ny \u2299\u2206f +QX ,R\u03b1R yTR \u00b7\u03b1R\n]\n=\n[\nQX ,T yTT\n]T [\nQX ,T yTT\n]\n\u03b1 \u2032 T\nThen we can compute \u03b1\u2032T using the following equation.\n\u03b1\u2032T = (\n[\nQX ,T yTT ]T [ QX ,T yTT ] )\u22121 [ QX ,T yTT ]T [ y \u2299\u2206f +QX ,R\u03b1R yTR \u00b7 \u03b1R ] (18)\nIf the inverse of the matrix in above equation does not exist, we find the pseudo inverse similar to ATO.\nAdjusting \u03b1\u2032T Due to the approximation, the constraints 0 \u2264 \u03b1\u2032t \u2264 C and \u2211 r\u2208IR yr\u03b1r = \u2211 t\u2208IT yt\u03b1 \u2032 t may not hold. Therefore, we need to adjust \u03b1\u2032T to satisfy the constraints, and we perform the following steps.\n\u2022 If \u03b1\u2032t < 0, we set \u03b1 \u2032 t = 0; if \u03b1 \u2032 t > C, we set \u03b1 \u2032 t = C. \u2022 If \u2211\nt\u2208IT yt\u03b1\n\u2032 t >\n\u2211\nr\u2208IR yr\u03b1r (if\n\u2211\nt\u2208IT yt\u03b1\n\u2032 t <\n\u2211\nr\u2208IR yr\u03b1r), we uniformly decrease (increase) all the\nyt\u03b1 \u2032 t until\n\u2211\nt\u2208IT yt\u03b1 \u2032 t =\n\u2211\nr\u2208IR yr\u03b1r, subjected to\nthe constraint 0 \u2264 \u03b1\u2032t \u2264 C.\nAfter the above adjusting, \u03b1\u2032t satisfies the constraints 0 \u2264 \u03b1\u2032t \u2264 C and \u2211 r\u2208IR yr\u03b1r = \u2211 t\u2208IT yt\u03b1 \u2032 t. Then, we use SMO with \u03b1\u2032 (where \u03b1\u2032 = \u03b1\u2032S \u222a \u03b1 \u2032 T ) as the initial alpha values for training an optimal SVM. The pseudo-code of whole algorithm is shown in Algorithm 2 in Supplementary Material.\nSingle Instance Replacement (SIR)\nBoth ATO and MIR have the following major limitation: the computation for \u03b1\u2032T is expensive (e.g. require computing the inverse of a matrix). The goal of the ATO and MIR is to minimise the number of instances that violate the optimality condition. In the algorithm we propose here, we try to minimise \u2206fi with a hope that the small change to fi will not violate the optimality condition. This slight change of the goal leads to a much cheaper computation cost on computing \u03b1\u2032T . Our key idea is to replace the instance in R one after another with a similar instance in T . Since we replace one instance in R by an instance in T each time, we call this algorithm Single Instance Replacement (SIR). Next, we present the details of the SIR algorithm. According to Equation (2), we can rewrite fi of the previous SVM as follows.\nfi = yi( \u2211\nj\u2208IS\u222aIR\\{p}\n\u03b1jQi,j + \u03b1pQi,p \u2212 1) (19)\nwhere p \u2208 IR. We replace the training instance xp by xq where q \u2208 IT , and then the value of fi after replacing xp by xq is as follows.\nfi = yi( \u2211\nj\u2208IS\u222aIR\\{p}\n\u03b1jQi,j + \u03b1 \u2032 qQi,q \u2212 1) (20)\nwhere \u03b1\u2032q = \u03b1p. By subtracting Equation (19) from Equation (20), the change of fi, denoted by \u2206fi, can be computed by \u2206fi = yi\u03b1p(Qi,q \u2212 Qi,p). Recall that Qi,j = yiyjK(xi,xj). We can write \u2206fi as follows.\n\u2206fi = \u03b1p(yqK(xi,xq)\u2212 ypK(xi,xp)) (21)\nRecall also that in SIR we want to replace xp by an instance, denoted by xq, that minimises \u2206fi. When \u03b1p = 0, \u2206fi has no change after replacing xp by xq. In what follows, we focus on the case that \u03b1p > 0. We propose to replace xp by xq if xq is the \u201cmost similar\u201d instance to xp among all the instances in T . The instance xq is called the most similar to the instance xp among all the instances in T , when the following two conditions are satisfied.\n\u2022 xp and xq have the same label, i.e. yp = yq.\n\u2022 K(xp,xq) \u2265 K(xp,xt) for all xt \u2208 T .\nNote that in the second condition, we use the fact that the kernel function approximates the similarity between two instances (Balcan, Blum, and Srebro 2008). If we can find the most similar instance to each instance in R, the constraint \u2211\ns\u2208IS ys\u03b1 \u2032 s +\n\u2211\nt\u2208IT yt\u03b1 \u2032 t = 0 will\nbe satisfied after the replacing R by T . Whereas, if we cannot find any instance in T that has the same label as xp, we randomly pick an instance from T to replace xp. When the above situation happens, the constraint \u2211\ns\u2208IS ys\u03b1 \u2032 s +\n\u2211\nt\u2208IT yt\u03b1 \u2032 t = 0 is violated.\nHence, we need to adjust \u03b1\u2032T to make the constraint hold. We use the same approach as MIR to adjusting \u03b1\u2032T . The pseudo code for SIR is given in Algorithm 3 in Supplementary Material."}, {"heading": "4 Experimental studies", "text": "We empirically evaluate our proposed algorithms using five datasets from the LibSVM website (Chang and Lin 2011). All our proposed algorithms were implemented in C++. The experiments were conducted on a desktop computer running Linux with a 6-core E5-2620 CPU and 128GB main memory. Following the common settings, we used the Gaussian kernel function and by default k is set to 10. The hyperparameters for each dataset are identical to the existing studies (Catanzaro, Sundaram, and Keutzer 2008; Smirnov, Sprinkhuizen-Kuyper, and Nalbantov 2004; Wu and Li 2006). Table 2 gives more details about the datasets. We study the k-fold cross-validation under the setting of binary classification. Next, we first show the overall efficiency of our proposed algorithms in comparison with LibSVM. Then, we study the effect of varying k from 3 to 100 in the k-fold cross-validation."}, {"heading": "Overall efficiency on different datasets", "text": "We measured the total elapsed time of each algorithm to test their efficiency. The total elapsed time consists of the alpha initialisation time and the time for the rest of the 10-fold cross-validation. The result is shown in Table 1. To make the table to fit in the page, we do not provide the total elapsed time of ATO, MIR and SIR for each dataset. But the total elapsed time can be easily computed by adding the time for alpha initialisation and the time for the rest. Note that the time for \u201cthe rest\u201d (e.g. the fourth column of Table 1) includes the time for partitioning dataset into 10 subsets, training (the most significant part) and classification. As we can see from the table, the total elapsed time of MIR and SIR is much smaller than LibSVM. In the\nMadelon dataset, MIR and SIR are about 2 times and 4 times faster than LibSVM, respectively. In comparison, ATO does not show obvious advantages over MIR and SIR, and is even slower than LibSVM on the Adult dataset due to spending too much time on alpha value initialisation. Another observation from the table is SIR spent the smallest amount of time on the alpha initialisation among our three algorithms, while SIR has the similar \u201ceffectiveness\u201d as MIR on reusing the alpha values. The effectiveness on reusing the alpha values is reflected by the total number of training iterations during the 10-fold cross-validation. More specifically, according to the ninth to twelfth columns of Table 1, LibSVM often requires more training iterations than MIR and SIR; SIR and MIR have similar number of iterations, and in some datasets (e.g. Adult and MNIST) SIR needs fewer iterations, although SIR saves much time in the initialisation. More importantly, the improvement on the efficiency does not sacrifice the accuracy. According to the last two columns of Table 1, we can see that SIR produces the same accuracy as LibSVM. Due to the space limitation, we omit providing the accuracy of ATO and MIR which also produce the same accuracy as LibSVM."}, {"heading": "Effect of varying k", "text": "We varied k from 3 to 100 to study the effect of the value of k. Moreover, because conducting this set of experiments is very time consuming especially when k = 100, we only compare SIR (the best among the our three algorithms according to results in Table 1) with LibSVM. Table 3 shows the results. Note that as LibSVM was very slow when k = 100 on the MNIST dataset, we only ran the first 30 rounds to estimate the total time. As we can see from the table, SIR consistently outperforms LibSVM. When k = 100, SIR is about 32 times faster than LibSVM in the Madelon dataset. The experimental result for the leave-one-out (i.e. k equals to the dataset size) cross-validation is similar to k = 100, and is available in Figure 2 in Supplementary Material."}, {"heading": "5 Related work", "text": "We categorise the related studies into two groups: on alpha seeding, and on online SVM training.\nRelated work on alpha seeding\nDeCoste and Wagstaff (DeCoste and Wagstaff 2000) first introduced the reuse of alpha values in the SVM leave-one-out cross-validation. Their method (i.e. AVG discussed in Supplementary Material) has two main steps: (i) train an SVM with the whole dataset; (ii) remove an instance from the SVM and distribute the associated alpha value uniformly among all the support vectors. Lee et al. (Lee et al. 2004) proposed a technique (i.e. TOP discussed in Supplementary Material) to improve the above method. Instead of uniformly distributing alpha value among all the support vectors, the method distributes the alpha value to the instance with the largest kernel value. Existing studies called \u201cWarm Start\u201d (Kao et al. 2004; Chu et al. 2015) apply alpha seeding in selecting the parameter C for linear SVMs. Concretely, \u03b1 obtained from training the hth linear SVM with C is used for training the hth linear SVM with (C + \u2206) in the two k-fold cross-validation processes by simply setting \u03b1\u2032 = r\u03b1 where r is a ratio computed from C and \u2206. In those studies, no alpha seeding technique is used when training the k SVMs with parameter C. Our work aims to reuse the hth SVM for training the (h + 1)th SVM for the k-fold cross-validation with parameter C.\nRelated work on online SVM training\nGauwenberghs and Poggio (Cauwenberghs and Poggio 2001) introduced an algorithm for training SVM online where the algorithm handles adding or removing one training instance. Karasuyama and Takeuchi (Karasuyama and Takeuchi 2009) extended the above algorithm to the cases where multiple instances need to be added or removed. Their key idea is to gradually reduce the alpha values of the outdated instances to 0, and meanwhile, to gradually increase the alpha values of the new instances. Due to the efficiency concern, the algorithm produces approximate SVMs. Our work aims to train SVMs which meet the optimality condition."}, {"heading": "6 conclusion", "text": "To improve the efficiency of the k-fold cross-validation, we have proposed three algorithms that reuse the previously trained SVM to initialise the next SVM, such that the training process for the next SVM reaches the optimal condition faster. We have conducted extensive experiments to validate the effectiveness and efficiency of our proposed algorithms. Our experimental results\nhave shown that the best algorithm among the three is SIR. When k = 10, SIR is several times faster than the k-fold cross-validation in LibSVM which does not make use of the previously trained SVM; when k = 100, SIR dramatically outperforms LibSVM (32 times faster than LibSVM in the Madelon dataset). Moreover, our algorithms produce same results (hence same accuracy) as the k-fold cross-validation in LibSVM does.\nAcknowledgments This work is supported by Australian Research Council (ARC) Discovery Project DP130104587 and Australian Research Council (ARC) Future Fellowships Project FT120100832. Prof. Jian Chen is supported by the Fundamental Research Funds for the Central Universities (Grant No. 2015ZZ029) and the Opening Project of Guangdong Province Key Laboratory of Big Data Analysis and Processing."}, {"heading": "Supplementary Material", "text": ""}, {"heading": "Pseudo-code of our three algorithm", "text": "Here, we present the pseudo-code of our three algorithms proposed in the paper.\nThe ATO algorithm The full algorithm of ATS is summarised in Algorithm 1. As we can see from Algorithm 1, ATO terminates when R is empty and it might spend a substantial time in the loop especially when the step size \u03b7 is small.\nAlgorithm 1: Adjusting Alpha Towards Optimum (ATO)\nInput: Sets X and R of instances, \u03b1 associated with instances in X , and a set T of new instances.\nOutput: Optimal alpha values for X \\R and T . 1 \u03b1\u2032T \u2190 0 /* Initialise \u03b1 \u2032 T */ /* Initialise index sets Im, Iu and Il */ 2 Init(Im, Iu, Il, \u03b1) 3 repeat 4 \u03b7 \u2190 GetStepSize() /* Eqs (7), (10) and (11) */ /* use Eqs (7) and (10) to update \u03b1 */ 5 \u03b1\u2032, \u03b1\u2032T \u2190 UpdateAlpha(\u03b7, \u03b1, \u03b1 \u2032 T ) 6 f \u2190 UpdateF(\u03b7, f) /* use Eqs (2) and (11) */ 7 foreach r \u2208 IR do 8 if \u03b1\u2032r = 0 then /* safe to remove xr */ 9 IR \u2190 IR \\ {r}, \u03b1 \u2032 \u2190 \u03b1\u2032 \\ {\u03b1\u2032r}\n/* update the sets Im, Ig and Is */ 10 Im, Iu, Il \u2190 Rearrange(Im, Iu, Il, f) 11 until R = \u03c6; 12 \u03b1\u2032 \u2190 \u03b1\u2032T \u222a\u03b1 \u2032 13 X \u2032 \u2190 T \u222a X \\R 14 TrainOptimalSVM(\u03b1\u2032, X \u2032) /* SMO to improve \u03b1\u2032 */\nThe MIR algorithm The full algorithm of MIR is summarised in Algorithm 2.\nAlgorithm 2: Multiple Instance Replacement (MIR)\nInput: Sets X and R of instances, \u03b1 associated with instances in X , and a set T of new instances.\nOutput: Optimal alpha values for X \\R and T . 1 \u03b1\u2032T \u2190 0 /* Initialise \u03b1 \u2032 T for T */ /* Initialise index sets Im, Iu and Il */ 2 Init(Im, Iu, Il, \u03b1) 3 \u2206fi \u2190 ComputeDeltaF() /* Equation (15) */ 4 \u03b1\u2032T \u2190 ComputeAlpha(\u03b1R, yR) /* Equation (18) */\n/* Adjust \u03b1\u2032T to meet constraints of problem (1) */\n5 \u03b1\u2032T \u2190 AdjustAlpha(\u03b1 \u2032 T , yT , \u03b1R, yR) 6 \u03b1\u2032 \u2190 \u03b1\u2032T \u222a\u03b1 \\\u03b1R 7 X \u2032 \u2190 T \u222a X \\R 8 TrainOptimalSVM(\u03b1\u2032, X \u2032) /* SMO to improve \u03b1\u2032 */\nThe SIR algorithm The full algorithm of SIR is summarised in Algorithm 3.\nAlgorithm 3: Single Instance Replacement (SIR)\nInput: Sets X and R of instances, \u03b1 associated with instances in X , and a set T of new instances.\nOutput: Optimal alpha values for X \\ R and T . 1 \u03b1\u2032T \u2190 0 /* Initialise \u03b1 \u2032 T for T */"}, {"heading": "2 foreach r \u2208 IR do", "text": ""}, {"heading": "3 maxV alue \u2190 0, t\u2032 \u2190 \u22121", "text": ""}, {"heading": "4 foreach t \u2208 IT do", "text": "5 if yr = yt \u2227K(xr,xt) > maxV alue then 6 maxV alue \u2190 K(xr,xt), t \u2032 \u2190 t\n7 if t\u2032 6= \u22121 then /* replace xr by xt\u2032 */ 8 IT \u2190 IT \\ {t \u2032}, \u03b1 \u2190 \u03b1 \\ {\u03b1r}, \u03b1t\u2032 \u2190 \u03b1r\n/* Adjust \u03b1\u2032T to meet constraints of Problem (1) */\n9 \u03b1\u2032T \u2190 AdjustAlpha(\u03b1 \u2032 T , yT , \u03b1, yS)\n10 \u03b1\u2032 \u2190 \u03b1\u2032T \u222a\u03b1 11 X \u2032 \u2190 T \u222a X \\ R 12 TrainOptimalSVM(\u03b1\u2032, X \u2032) /* SMO to improve \u03b1\u2032 */\nExisting approaches for leave-one-out cross-validation\nAs our three algorithms (i.e. ATO, MIR and SIR) are proposed to improve the efficiency of k-fold crossvalidation, naturally the three algorithms can accelerate leave-one-out cross-validation. Note that leave-oneout cross-validation is a special case of k-fold crossvalidation, when k equals to the number of instances in the dataset. Here, we present two existing alpha seeding techniques (DeCoste and Wagstaff 2000; Lee et al. 2004) that have been specifically proposed to improve the efficiency of leave-one-out cross-validation. Given a dataset X of n instances, both of the algorithms train the SVM using all the n instances. Recall that the trained SVM meets constraints of Problem (1), and we have the constraint \u2211\nxi\u2208X yi\u03b1i = 0 held. Then,\nin each round of the leave-one-out cross-validation, an instance xt is removed from the trained SVM. To make the constraint \u2211\nxi\u2208X\\{xt} yi\u03b1 \u2032 i = 0 hold, the alpha val-\nues of the instances in X \\{xt}may need to be adjusted. The two existing techniques apply different strategies to adjust the alpha values of the instances in X \\ {xt}.\nUniformly distributing \u03b1tyt to other instances First, the strategy proposed in (DeCoste and Wagstaff 2000) counts the number, denoted by d, of instances with alpha values satisfying 0 < \u03b1i < C where xi \u2208 X \\ {xt}. Then, the average amount of value that the d instances need to be adjusted is yt\u03b1t\nd . For each instance xj in the d\ninstances, adjusting their alpha values is handled in the following two scenarios.\n\u2022 If yt = yj , \u03b1 \u2032 j equals to (\u03b1j + \u03b1t d ).\n\u2022 If yt = \u2212yj, \u03b1 \u2032 j equals to (\u03b1j \u2212 \u03b1t d ). Note that the updated alpha value \u03b1\u2032j subjects to the constraint 0 \u2264 \u03b1\u2032j \u2264 C. Hence, the alpha values of some instances may not allow to be increased/decreased by \u03b1t d . Those alpha values are adjusted to the maximum allowed limit (i.e. increased to C or decreased to 0); similar to the above process, the extra amount of value (of \u03b1t\nd which cannot be added to or removed from \u03b1\u2032j) is uniformly distributed to those alpha values that satisfy 0 < \u03b1i < C. We call this technique AVG, because each alpha value of the d instances is increased/decreased by the average amount (except those near 0 or C) of value from yt\u03b1t. Our ATO algorithm has the similar idea as AVG, where the alpha values of many instances are adjusted by the same (or similar) amount.\nDistributing the \u03b1tyt to similar instances AVG requires changing the alpha values of many instances, which may not be efficient. Lee et al. (Lee et al. 2004) proposed a technique to adjust the alpha values of only a few most similar instances to xt. The technique first finds the instance xj among X \\ {xt} with the largest kernel value, i.e. K(xj ,xt) is the largest. Then, \u03b1\u2032j \u2190 (\u03b1j +\u03b1t) if yt = yj or \u03b1 \u2032 j \u2190 (\u03b1j\u2212\u03b1t) if yt = \u2212yj. Recall that the updated alpha value \u03b1\u2032j needs to satisfy the constraint 0 \u2264 \u03b1\u2032j \u2264 C. Hence, the alpha value of the most similar instance xj may not allow to be increased/decreased by \u03b1t. Then, \u03b1 \u2032 j is increased to C or decreased to 0 depending on yj . The extra amount of value is distributed to the alpha value of the second most similar instance, the third most similar instance, and so on until the constraint \u2211\nxi\u2208X\\{xt} \u03b1\u2032iyi = 0\nholds. We call this technique TOP, since it only adjusts the alpha values of a few most similar (i.e. a top few) instances to xt. Our MIR algorithm and SIR algorithm have the similar idea to TOP, where only the alpha values of a proportion of the instances are adjusted. After the adjusting by either of the two techniques, the constraint \u2211\nxi\u2208X\\{xt} \u03b1\u2032iyi = 0 holds, and \u03b1 \u2032 is\nused as the initial alpha values for training the next SVM. In the next section (more specifically, in Section 12), we empirically evaluate the five techniques for accelerating leave-one-out cross-validation."}, {"heading": "Efficiency comparison on leave-one-out cross-validation", "text": "Here, we study the efficiency of our proposed algorithms, in comparison with LibSVM and the existing alpha seeding techniques, i.e. AVG and TOP (cf. Section 12), for leave-one-out cross-validation. Similar to the other algorithms, we implemented AVG and TOP in C++. Since leave-one-out cross-validation is very expensive for the large datasets, we estimated the total time for leave-one-out cross-validation on the three large datasets (namely Adult, MNIST and Webdata) for each algorithm. For MNIST and Webdata, we ran\nthe first 30 rounds of the leave-one-out cross-validation to estimate the total time for each algorithm; for Adult, we ran the first 100 rounds of the leave-one-out crossvalidation to estimate the total time for each algorithm. As Heart and Madelon are relatively small, we ran the whole leave-one-out cross-validation, and measured their total elapsed time. The experimental results are shown in Figure 2. As we can see from the table, all the five algorithms are faster than LibSVM ranging from a few times to a few hundred times (e.g. SIR is 167 times faster than LibSVM on Webdata). Another observation from the table is AVG and TOP have similar efficiency. It is worth pointing out that our SIR algorithm almost always outperforms all the other algorithms, except Heart and Madelon where MIR is slightly better."}], "references": [{"title": "GPU acceleration for support vector machines", "author": ["Athanasopoulos"], "venue": "In International Workshop on Image Analysis for Multimedia Interactive Services", "citeRegEx": "Athanasopoulos,? \\Q2011\\E", "shortCiteRegEx": "Athanasopoulos", "year": 2011}, {"title": "A theory of learning with similarity functions. Machine Learning 72(1-2):89\u2013112", "author": ["Blum Balcan", "M.-F. Srebro 2008] Balcan", "A. Blum", "N. Srebro"], "venue": null, "citeRegEx": "Balcan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2008}, {"title": "Duality and geometry in svm classifiers", "author": ["Bennett", "E.J. Bredensteiner"], "venue": "In ICML,", "citeRegEx": "Bennett et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bennett et al\\.", "year": 2000}, {"title": "Fast support vector machine training and classification on graphics processors", "author": ["Sundaram Catanzaro", "B. Keutzer 2008] Catanzaro", "N. Sundaram", "K. Keutzer"], "venue": null, "citeRegEx": "Catanzaro et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Catanzaro et al\\.", "year": 2008}, {"title": "Incremental and decremental support vector machine learning. Advances in neural information processing", "author": ["Cauwenberghs", "G. Poggio 2001] Cauwenberghs", "T. Poggio"], "venue": null, "citeRegEx": "Cauwenberghs et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cauwenberghs et al\\.", "year": 2001}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang", "Lin 2011] Chang", "C.-C", "Lin", "C.-J"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Warm start for parameter selection of linear classifiers", "author": ["Chu"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Chu,? \\Q2015\\E", "shortCiteRegEx": "Chu", "year": 2015}, {"title": "Alpha seeding for support vector machines", "author": ["DeCoste", "D. Wagstaff 2000] DeCoste", "K. Wagstaff"], "venue": "In SIGKDD,", "citeRegEx": "DeCoste et al\\.,? \\Q2000\\E", "shortCiteRegEx": "DeCoste et al\\.", "year": 2000}, {"title": "Decomposition methods for", "author": ["Kao"], "venue": null, "citeRegEx": "Kao,? \\Q2004\\E", "shortCiteRegEx": "Kao", "year": 2004}, {"title": "Multiple incremental decremental learning of support vector machines", "author": ["Karasuyama", "M. Takeuchi 2009] Karasuyama", "I. Takeuchi"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Karasuyama et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Karasuyama et al\\.", "year": 2009}, {"title": "Improvements to platt\u2019s smo algorithm for svm classifier design", "author": ["Keerthi"], "venue": "Neural Computation", "citeRegEx": "Keerthi,? \\Q2001\\E", "shortCiteRegEx": "Keerthi", "year": 2001}, {"title": "Solving least squares problems, volume 161", "author": ["Lawson", "C.L. Hanson 1974] Lawson", "R.J. Hanson"], "venue": null, "citeRegEx": "Lawson et al\\.,? \\Q1974\\E", "shortCiteRegEx": "Lawson et al\\.", "year": 1974}, {"title": "An efficient method for computing leave-one-out error in support vector machines with gaussian kernels", "author": ["Lee"], "venue": "Neural Networks, IEEE Transactions on 15(3):750\u2013757", "citeRegEx": "Lee,? \\Q2004\\E", "shortCiteRegEx": "Lee", "year": 2004}, {"title": "An improved training algorithm for support vector machines", "author": ["Freund Osuna", "E. Girosi 1997] Osuna", "R. Freund", "F. Girosi"], "venue": "In IEEE Workshop on Neural Networks for Signal Processing,", "citeRegEx": "Osuna et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Osuna et al\\.", "year": 1997}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines", "author": ["Platt", "J others 1998] Platt"], "venue": null, "citeRegEx": "Platt and Platt,? \\Q1998\\E", "shortCiteRegEx": "Platt and Platt", "year": 1998}, {"title": "Unanimous voting using support vector machines", "author": ["Sprinkhuizen-Kuyper Smirnov", "E. Nalbantov 2004] Smirnov", "I. Sprinkhuizen-Kuyper", "G. Nalbantov"], "venue": "In Belgium-Netherlands Conference on Artificial Intelligence,", "citeRegEx": "Smirnov et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Smirnov et al\\.", "year": 2004}, {"title": "Mascot: fast and highly scalable SVM cross-validation using GPUs and SSDs", "author": ["Wen"], "venue": "In International Conference in Data Mining,", "citeRegEx": "Wen,? \\Q2014\\E", "shortCiteRegEx": "Wen", "year": 2014}, {"title": "Feature selection for classification using transductive support vector machines", "author": ["Wu", "Z. Li 2006] Wu", "C. Li"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2006}], "referenceMentions": [], "year": 2016, "abstractText": "The k-fold cross-validation is commonly used to evaluate the effectiveness of SVMs with the selected hyperparameters. It is known that the SVM k-fold crossvalidation is expensive, since it requires training k SVMs. However, little work has explored reusing the h SVM for training the (h+1) SVM for improving the efficiency of k-fold cross-validation. In this paper, we propose three algorithms that reuse the h SVM for improving the efficiency of training the (h + 1) SVM. Our key idea is to efficiently identify the support vectors and to accurately estimate their associated weights (also called alpha values) of the next SVM by using the previous SVM. Our experimental results show that our algorithms are several times faster than the k-fold cross-validation which does not make use of the previously trained SVM. Moreover, our algorithms produce the same results (hence same accuracy) as the k-fold cross-validation which does not make use of the previously trained SVM.", "creator": "gnuplot 4.6 patchlevel 4"}}}