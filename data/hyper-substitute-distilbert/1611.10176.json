{"id": "1611.10176", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2016", "title": "Effective Quantization Methods for Recurrent Neural Networks", "abstract": "decreased bit - widths bandwidth jumps, activations, inverse gradients reveals a neural network typically shrink data cache size faster memory usage, likely also accelerate infinitely faster decomposition and inference not addressing bitwise shifts. however, currently research that quantization of rnns show considerable hardware doubling capability using low stream - width weights and techniques. in similar paper, we propose modeling to classify the structure of gates such interlinks in lstm and gru cells. in 2015, we propose sophisticated computing methods for complexity to completely deter connectivity degradation. experiments where ptb and imdb continually confirm visibility of symmetric networks while performances of our models ensure probability surpass the previous state - of - the - art independent quantized analysis.", "histories": [["v1", "Wed, 30 Nov 2016 14:33:08 GMT  (32kb)", "http://arxiv.org/abs/1611.10176v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["qinyao he", "he wen", "shuchang zhou", "yuxin wu", "cong yao", "xinyu zhou", "yuheng zou"], "accepted": false, "id": "1611.10176"}, "pdf": {"name": "1611.10176.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Shuchang Zhou", "Yuxin Wu", "Cong Yao", "Xinyu Zhou", "Yuheng Zou"], "emails": ["zouyuheng}@megvii.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 1.\n10 17\n6v 1\n[ cs\n.L G\n] 3\n0 N\nov 2\n01 6"}, {"heading": "1 INTRODUCTION", "text": "Deep Neural Networks have become important tools for modeling nonlinear functions in applications like computer vision (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012a), natural language processing (Bahdanau et al., 2014), and computer games (Silver et al., 2016).\nHowever, inference and training of a DNN may involve up to billions of operations for inputs likes images (Krizhevsky et al., 2012; Szegedy et al., 2014). A DNN may also have large number of parameters, leading to large storage size and runtime memory usage. Such intensive resource requirements impede adoption of DNN in applications requiring real-time responses, especially on resource-limited platforms. To alleviate these requirements, many methods have been proposed, from both hardware and software perspective (Farabet et al., 2011; Pham et al., 2012; Chen et al., 2014a;b). For example, constraints may be imposed on the weights of DNN, like sparsity (Han et al., 2015b;a), circulant matrix (Cheng et al., 2015), low rank (Jaderberg et al., 2014; Zhang et al., 2015), vector quantization (Gong et al., 2014), and hash trick (Chen et al., 2015) etc., to reduce the number of free parameters and computation complexity. However, these methods use high bit-width numbers for computations, which require availability of high precision multiply-and-add instructions.\nAnother line of research tries to reduce bit-width of weights and activations of a DNN by quantization to low bit-width numbers (Rastegari et al., 2016; Hubara et al., 2016b; Zhou et al., 2016; Hubara et al., 2016a). Reducing bit-width of weights of a 32-bit model to k can shrink the storage size of model to k\n32 of the original size. Similarly, reducing bit-widths of activations to k can shrink\nthe runtime memory usage by the same proportion. In addition, when the underlying platform supports efficient bitwise operations and bitcount that counts the number of bits in a bit vector, we can compute the inner product between bit vectors x y by the following formula:\nx \u00b7 y = bitcount(and(x,y)), \u2200i, xi, yi \u2208 {0, 1}. (1) Consequently, convolutions between low bit-width numbers can be considerable accelerated on platforms supporting efficient execution of bitwise operations, including CPU, GPU, FPGA and ASIC. Previous works shows that using only 1-bit weights and 2-bit activation can achieve 51% top-1 accuracy on ImageNet datasets(Hubara et al., 2016a).\nHowever, in contrast to the extensive study in compression and quantization of convolutional neural networks, little attention has been paid to reducing the computational resource requirements of RNN. (Ott et al., 2016) claims that the weight binarization method does not work with RNNs, and introduces weight ternarization and leaves activations as floating point numbers. (Hubara et al., 2016a)\nexperiments with different combinations of bit-widths for weights and activations, and shows 4-bit quantized CNN and RNN can achieve comparable accuracy as their 32-bit counterpart. However, large performance degradation occurs when quantizing weights and activations to 2-bit numbers. Though (Hubara et al., 2016a) has their quantized CNN open-sourced, neither of the two works open-source their quantized RNNs.\nThis paper makes the following contributions:\n1. We outline detailed design for quantizing two popular types of RNN cells: LSTM and GRU. We evaluate our model on different sets of bit-width configurations and two NLP tasks: Penn Treebank and IMDB. We demonstrate that by out design, quantization with 4- bit weights and activations can achieve almost the same performance to 32-bit. In addition, we have significantly better results when quantizing to lower bit-widths.\n2. We propose methods to quantize weights deterministically and adaptively to balanced distributions, especially when weights are 2-bits numbers. The balanced distribution of quantized weights leads to better utilization of the parameter space and consequently increases the prediction accuracy. We explicitly induce the balanced distribution by introducing parameter dependent thresholds into the quantization process during training.\n3. We release code for training our quantized RNNs online 1. The code is implemented in TensorFlow(Abadi et al.) framework."}, {"heading": "2 QUANTIZATION METHODS", "text": "In this section we outline several quantization methods. W.l.o.g., we assume the input to the quantization is a matrix X unless otherwise specified. When all entries of X are in close interval [0, 1], we define the k-bit uniform quantization Qk as follows: .\nQk(X) = 1\n2k \u2212 1\n\u230a (2k \u2212 1)X + 1\n2\n\u230b ,\n0 \u2264 xij \u2264 1\u2200i, j. (2)\nHowever, derivatives of this quantization function equals zero almost everywhere. We adopt the \u201cstraight-through estimator\u201d (STE) method (Hinton et al., 2012b; Bengio et al., 2013) to circumvent this problem.\nFor forward and backward pass of training neural network, using above quantization method together with STE leads to the following update rule during forward and backward propagation of neural networks:\nForward: q \u2190 Qk(p)\nBackward: \u2202c \u2202p \u2190 \u2202c \u2202q ."}, {"heading": "2.1 DETERMINISTIC QUANTIZATION", "text": "When entries in X are not constrained in closed interval [0, 1], an affine transform need to be applied before using function Qk. A straightforward transformation can be done using minimum and maximum of X to get X\u0303, the standardized version of X:\nX\u0303 = X\u2212 \u03b2\n\u03b1\n\u03b1 = max(X)\u2212min(X) \u03b2 = min(X)\nAfter quantization, we can apply a reverse affine transform to approximate the original values. Overall, the quantized result is:\nQdetk (X) = \u03b1Qk(X\u0303) + \u03b2 \u2248 X 1https://github.com/hqythu/bit-rnn"}, {"heading": "2.2 BALANCED DETERMINISTIC QUANTIZATION", "text": "When we quantize values, it may be desirable to make the quantized values have balanced distributions, so as to take full advantage of the available parameter space. Ordinarily, this is not possible as the distribution of the input values has already been fixed. In particular, using Qdetk do not exert any impacts on the distribution of quantized values.\nNext we show that we can induce more uniform distributions of quantized values, by introducing parameter dependent adaptive thresholds \u03b3 median(|X|). We first introduce a different standardization transform that produces X\u0302, and define a balanced quantization method Q\u0302 bal\nk as follows:\nX\u0302 = clip( X \u03b3 median(|X|) ,\u2212 1 2 , 1 2 ) + 1 2 (3)\nQ\u0302 bal\nk (X) = \u03b1Qk(X\u0302) + \u03b2\nThe only difference between Q\u0302 bal\nk and Q det k lies in difference of standardization. In fact, when the\nextremal values of X are symmetric around zero, i.e.\nmin(X) + max(X) = 0,\nwe may rewrite Qdetk equivalently as follows to make the similarity between Q bal k and Q det k more obvious:\nX\u0303 = X\u2212min(X)\nmax(X)\u2212min(X)\n= X\n2 max(X) +\n1\n2\n= clip( X 2 max(X) ,\u22121 2 , 1 2 ) + 1 2\nQdetk (X) = \u03b1Qk(X\u0303) + \u03b2\nHence the only difference between Q\u0302 bal\nk and Q det k lies in difference between properties of 2 max(X)\nand \u03b3 median(|X|). We find that as median is an order statistics, using it as threshold will produce an auto-balancing effect.\n2.2.1 THE AUTO-BALANCING EFFECT OF Q\u0302 bal\nk\nWe consider the case when bit-width is 2 as an example. In this case, under the symmetric distribution assumption, we can prove the auto-balancing effect of Q\u0302 bal\nk .\nTheorem 1. If k = 2, \u03b3 = 3, and suppose X are symmetrically distributed around zero and there are no two entries in X that are equal, then the four bars in the histogram of Q\u0302 bal\nk (X) will all have exactly the same height.\nProof. By Formula 3, entries of Q\u0302 bal\nk (X) will be equal to 1 if corresponding entries in X are above \u03b3 3 median(|X|), equal to 2 3 if between 0 and \u03b3 3 median(|X|), equal to 1 3 if between\u2212\u03b3 3\nmedian(|X|) and 0, and equal to 0 if below \u2212\u03b3\n3 median(|X|). When \u03b3 = 3 and X are symmetrically distributed\naround zero, the values in X will be thresholded by \u2212median(|X|), 0, and median(|X|) into four bins. By the property of median, and the symmetric distribution assumption, the four bins will contain the same number of quantized values.\nIn practice, computing median(|X|) may not be computationally convenient as it requires sorting. We note that when a distribution has bounded variance \u03c3, the mean \u00b5 approximates the median m as there is an inequality bounding the difference(Mallows, 1991):\n|\u00b5\u2212m| \u2264 \u03c3.\nHence we may use mean(|X|) instead of median(|X|) in the quantization. Though with error introduced, empirically we can still observed nearly-balanced distribution.\nIf we further assume the weights follow zero-mean normal distribution N (0, \u03c32), then |X| follows half-normal distribution. By simple calculations we have:\nmean(|X|) median(|X|) =\n\u03c3 \u221a\n2\u221a \u03c0\n\u03c3 \u221a 2 erf\u22121( 1 2 ) \u2248 1 0.4769 \u221a \u03c0 \u2248 1.1830\nand\n3 median(|X|) \u2248 2.5359 mean(|X|)\nPutting all these things together we have the balanced deterministic quantization method:\nX\u0302 = clip( X \u03b3 mean(|X|) ,\u2212 1 2 , 1 2 ) + 1 2 (4)\nQbalk (X) = \u03b1Qk(X\u0302) + \u03b2 \u2248 X, where a natural choice of \u03b3 would be 3 or 2.5 (rounding 2.5359 to a short binary number) under different assumptions. In our following experiments, we adopt 2.5 as the scaling factor.\nAlthough the above argument for balanced quantization applies only to 2-bit quantization, we argue more bit-width also benefit from avoiding extreme value from extending the value range thus increase rounding error. It should be noted that for 1-bit quantization (binarization), the scaling factor should be 2 mean(|X|), which can be proved to be optimal in the sense of reconstruction error measured by Frobenius norm, as in (Rastegari et al., 2016). However, the proof relies on the constant norm property of 1-bit representations, and does not generalize to the cases of other bit-widths."}, {"heading": "2.3 QUANTIZATION OF WEIGHTS", "text": "Weights in neural networks are sometimes known to have a bell-style distribution around zero, similar to normal distribution. Hence we can assume X to have symmetric distribution around 0, and apply the above equation for balanced quantization as\nscale = mean(abs(X)) \u2217 2.5\nQbalk (X) = Qk( X\nscale ) \u2217 scale \u2248 X.\nTo include the quantization into the computation graph of a neural network, we apply STE on entire expression rather than only Qk itself.\nForward: q \u2190 Qbalk (p)\nBackward: \u2202c \u2202p \u2190 \u2202c \u2202q .\nThe specialty about the balanced quantization method Qbalk is that in general, it distort the extremal values due to the clipping in Formula 4, which in general contribute more to the computed sums of inner products. However, in case where the values to be quantized are weights of neural networks and if we introduce the balanced quantization into the training process, we conjecture that the neural networks may gradually adapt to the distortions, so that distributions of weights may be induced to be more balanced. The more balanced distribution will increase the effective bit-width of neural networks, leading to better prediction accuracy. We will empirically validate this conjecture through experiments in Section 4."}, {"heading": "2.4 QUANTIZATION OF ACTIVATIONS", "text": "Quantization of activation follows the method in Zhou et al. (2016), assuming output of the previous layer has passed through a bounded activation function h, and we will apply quantization directly\nto them. In fact, we find that adding a scaling term containing mean or max value to the activations may harm prediction accuracy.\nThere is a design choice on what range of quantized value should be. One choice is symmetric distribution around 0. Under this choice, inputs are bounded by activation function to [\u22120.5, 0.5], and then shifted to the right by 0.5 before feeding into Qk and then shift back.\nXq = Qk(X + 0.5)\u2212 0.5\nAnother choice is having value range of [0, 1], which is closer to the value range of ReLU activation function. Under this choice, we can directly apply Qk. For commonly used tanh activation with domain [\u22121, 1] in RNNs, it seems natural to use symmetry quantization. However, we will point out some considerations for using quantization to range [0, 1] in Section 3."}, {"heading": "3 QUANTIZATION OF RECURRENT NEURAL NETWORK", "text": "In this section, we detail our design considerations for quantization of recurrent neural networks. Different from plain feed forward neural network, recurrent neural networks, especially Long Short Term Memory (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Unit (Chung et al., 2014), have subtle and delicately designed structure, which makes their quantization more complex and need more careful considerations. Nevertheless, the major algorithm is the same as Algorithm 1 in Zhou et al. (2016)."}, {"heading": "3.1 DROPOUT", "text": "It is well known that as fully-connected layers have large number of parameters, they are prone to overfit (Srivastava et al., 2014). There are several FC-like structures in a RNN, for example the input, output and transition matrices in RNN cells (like GRU and LSTM) and the final FC layer for softmax classification. The dropout technique, which randomly dropping a portion of features to 0 at training time, turns out be also an effective way of alleviating overfitting in RNN (Zaremba et al., 2014).\nAs dropped activations are zero, it is necessary to have zero values in the range of quantized values. For symmetric quantization to range [\u22120.5, 0.5], 0 does not exist in range of Qk(X + 0.5) \u2212 0.5. Hence we use [0, 1] as the range of quantized values when dropout is needed."}, {"heading": "3.2 EMBEDDING LAYER", "text": "In tasks related to Natural Language Processing, the input words which are represented by ID\u2019s, are embedded into a low-dimensional space before feeding into RNNs. The word embedding matrix is in R|V |\u00d7N , where |V | is the size of vocabulary and N is length of embedded vectors. Quantization of weights in embedding layers turns out to be different from quantization of weights in FC layers. In fact, the weights of embedding layers actually behave like activations: a certain row is selected and fed to the next layer, so the quantization method should be the same as that of activations rather than that of weights. Similarly, as dropout may also be applied on the outputs of embedding layers, it is necessary to bound the values in embedding matrices to [0, 1].\nTo clip the value range of weights of embedding layers, a natural choice would be using sigmoid function h(x) = 1\n1+e\u2212x such that h(W) will be used as parameters of embedding layers, but we\nobserve severe vanishing gradient problem for gradients \u2202Cost \u2202W in training process. Hence instead, we directly apply a clip function max(min(W, 1), 0), and random initialize the embedding matrices with values drawn from uniform distribution U(0, 1). These two measures are found to improve performance of the model."}, {"heading": "3.3 QUANTIZATION OF GRU", "text": "We first investigate quantization of GRU as it is structurally simpler. The basic structure of GRU cell may be described as follows:\nzt = \u03c3(Wz \u00b7 [ht\u22121, xt]) rt = \u03c3(Wr \u00b7 [ht\u22121, xt]) h\u0303t = tanh(W \u00b7 [rt \u2217 ht\u22121, xt]) ht = (1\u2212 zt) \u2217 ht\u22121 + zt \u2217 h\u0303t,\nwhere \u03c3 stands for the sigmoid function.\nRecall that to benefit from the speed advantage of bit convolution kernels, we need to make the two matrix inputs for multiply in low bit form, so that the dot product can be calculated by bitwise operation. For plain feed forward neural networks, as the convolutions take up most of computation time, we can get decent acceleration by quantization of inputs of convolutions and their weights. But when it comes to more complex structures like GRU, we need to check the bit-width of each interlink.\nExcept for matrix multiplications needed to compute zt,rt and h\u0303t, the gate structure of h\u0303t and ht brings in the need for element-wise multiplication. As the output of the sigmoid function may have large bit-width, the element-wise multiplication may need be done in floating point numbers (or in higher fixed-point format). As h\u0303t and ht are also the inputs to computations at the next timestamp, and noting that a quantized value multiplied by a quantized value will have a larger bit-width, we need to insert additional quantization steps after element-wise multiplications.\nAnother problem with quantization of GRU structure lies in the different value range of gates. The range of tanh is [\u22121, 1], which is different from the value range [0, 1] of zt and rt. If we want to preserve the original activation functions, we will have the following quantization scheme:\nzt = \u03c3(Wz \u00b7 [ht\u22121, xt]) rt = \u03c3(Wr \u00b7 [ht\u22121, xt])\nh\u0303t = tanh(W \u00b7 [2 Qk( 1\n2 (rt \u2217 ht\u22121) +\n1 2 )\u2212 1, xt])\nht = 2 Qk( 1\n2 ((1 \u2212 zt) \u2217 ht\u22121 + zt \u2217 h\u0303t) +\n1 2 )\u2212 1,\nwhere we assume the weights Wz ,Wr,W have already been quantized to [\u22121, 1], and input xt have already been quantized to [\u22121, 1]. However, we note that the quantization function already has an affine transform to shift the value range. To simplify the implementation, we replace the activation functions of h\u0303t to be the sigmoid function, so that (1\u2212 zt) \u2217 ht\u22121 + zt \u2217 h\u0303t \u2208 [0, 1]. Summarizing the above considerations, the quantized version of GRU could be written as\nzt = \u03c3(Wz \u00b7 [ht\u22121, xt]) rt = \u03c3(Wr \u00b7 [ht\u22121, xt]) h\u0303t = \u03c3(W \u00b7 [Qk(rt \u2217 ht\u22121), xt]) ht = Qk((1\u2212 zt) \u2217 ht\u22121 + zt \u2217 h\u0303t),\nwhere we assume the weights Wz ,Wr,W have already been quantized to [\u22121, 1], and input xt have already been quantized to [0, 1]."}, {"heading": "3.4 QUANTIZATION OF LSTM", "text": "The structure of LSTM can be described as follows:\nft = \u03c3(Wf \u00b7 [ht\u22121, xt] + bf ) it = \u03c3(Wi \u00b7 [ht\u22121, xt] + bi) C\u0303t = tanh(WC \u00b7 [ht\u22121, xt] + bi) Ct = ft \u2217 Ct\u22121 + it \u2217 C\u0303t ot = \u03c3(Wo \u00b7 [ht\u22121, xt] + bo) ht = ot \u2217 tanh(Ct)\nDifferent from GRU, Ct can not be easily quantized, since the value is unbounded by not using activation function like tanh and the sigmoid function. This difficulty comes from structure design and can not be alleviated without introducing extra facility to clip value ranges. But it can be noted that the computations involving Ct are all element-wise multiplications and additions, which may take much less time than computing matrix products. For this reason, we leave Ct to be in floating point form.\nTo simplify implementation, tanh activation for output may be changed to the sigmoid function.\nSummarizing above changes, the formula for quantized LSTM can be:\nft = \u03c3(Wf \u00b7 [ht\u22121, xt] + bf ) it = \u03c3(Wi \u00b7 [ht\u22121, xt] + bi) C\u0303t = tanh(WC \u00b7 [ht\u22121, xt] + bi) Ct = ft \u2217 Ct\u22121 + it \u2217 C\u0303t ot = \u03c3(Wo \u00b7 [ht\u22121, xt] + bo) ht = Qk(ot \u2217 \u03c3(Ct)),\nwhere we assume the weights Wf ,Wi,WC ,Wo have already been quantized to [\u22121, 1], and input xt have already been quantized to [0, 1]."}, {"heading": "4 EXPERIMENT RESULTS", "text": "We evaluate the quantized RNN models on two tasks: language modeling and sentence classification."}, {"heading": "4.1 EXPERIMENTS ON PENN TREEBANK DATASET", "text": "For language modeling we use Penn Treebank dataset (Taylor et al., 2003), which contains 10K unique words. We download the data from Tomas Mikolov\u2019s webpage2. For fair comparison, in the following experiments, our model all use one hidden layer with 300 hidden units, which is the same setting as Hubara et al. (2016a). A word embedding layer is used at the input side of the network whose weights are trained from scratch. The performance is measured in perplexity per word (PPW) metric.\nDuring experiments we find the magnitudes of values in dense matrices or full connected layers explode when using small bit-width, and result in overfitting and divergence. This can be alleviated by adding tanh to constrain the value ranges or adding weight decays for regularization.\nOur result is in agreement with (Hubara et al., 2016a) where they claim using 4-bit weights and activations can achieve almost the same performance as 32-bit. However, we report higher accuracy when using less bits, such as 2-bit weight and activations. The 2-bit weights and 3-bit activations LSTM achieve 146 PPW, which outperforms the counterpart in (Hubara et al., 2016a) by a large margin.\nWe also perform experiments in which weights are binarized. The models can converge, though with large performance degradations.\n2http://www.fit.vutbr.cz/ imikolov/rnnlm/simple-examples.tgz"}, {"heading": "4.2 EXPERIMENTS ON PENN IMDB DATASETS", "text": "We do further experiments on sentence classification using IMDB datasets (Maas et al., 2011). We pad or cut each sentence to 500 words, word embedding vectors of length 512, and a single recurrent layer with 512 number of hidden neurons. All models are trained using ADAM(Kingma & Ba, 2014) learning rule with learning rate 10\u22123.\nAs IMDB is a fairly simple dataset, we observe little performance degradation even when quantizing to 1-bit weights and 2-bit activations."}, {"heading": "4.3 EFFECTS OF BALANCED DISTRIBUTION", "text": "All the above experiments show balanced quantization leads to better results compared to unbalanced counterparts, especially when quantizing to 2-bit weights. However, for 4-bit weights, there is no clear gap between scaling by mean and scaling by max (i.e. balanced and unbalanced quantization), indicating that more effective methods for quantizing to 4-bit need to be discovered."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "We have proposed methods for effective quantization of RNNs. By using carefully designed structure and a balanced quantization methods, we have matched or surpassed previous state-of-the-arts in prediction accuracy, especially when quantizing to 2-bit weights.\nThe balanced quantization method for weights we propose can induce balanced distribution of quantized weight value to maximum the utilization of parameter space. The method may also be applied to quantization of CNNs.\nAs future work, first, the method to induce balanced weight quantization when bit-width is more than 2 remains to be found. Second, we have observed some difficulties for quantizing the cell paths in LSTM, which produces unbounded values. One possible way to address this problem is introducing novel scaling schemes to quantize the activations that can deal with unbounded values. Finally, as we have observed GRU and LSTM have different properties in quantization, it remains to be shown whether there exists more efficient recurrent structures designed specifically to facilitate quantization."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Yoshua Bengio", "Nicholas L\u00e9onard", "Aaron Courville"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning", "author": ["Tianshi Chen", "Zidong Du", "Ninghui Sun", "Jia Wang", "Chengyong Wu", "Yunji Chen", "Olivier Temam"], "venue": "In ACM Sigplan Notices,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T Wilson", "Stephen Tyree", "Kilian Q Weinberger", "Yixin Chen"], "venue": "arXiv preprint arXiv:1504.04788,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Yunji Chen", "Tao Luo", "Shaoli Liu", "Shijin Zhang", "Liqiang He", "Jia Wang", "Ling Li", "Tianshi Chen", "Zhiwei Xu", "Ninghui Sun"], "venue": "In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Yu Cheng", "Felix X Yu", "Rogerio S Feris", "Sanjiv Kumar", "Alok Choudhary", "Shi-Fu Chang"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp. 2857\u20132865,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Large-scale fpga-based convolutional networks. Scaling up Machine Learning: Parallel and Distributed Approaches", "author": ["Cl\u00e9ment Farabet", "Yann LeCun", "Koray Kavukcuoglu", "Eugenio Culurciello", "Berin Martini", "Polina Akselrod", "Selcuk Talay"], "venue": null, "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Neural networks for machine learning", "author": ["Geoffrey Hinton", "Nitsh Srivastava", "Kevin Swersky"], "venue": "Coursera, video lectures,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Quantized neural networks: Training neural networks with low precision weights and activations", "author": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.07061,", "citeRegEx": "Hubara et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hubara et al\\.", "year": 2016}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1405.3866,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Another comment on ocinneide", "author": ["Colin Mallows"], "venue": "The American Statistician,", "citeRegEx": "Mallows.,? \\Q1991\\E", "shortCiteRegEx": "Mallows.", "year": 1991}, {"title": "Recurrent neural networks with limited numerical precision", "author": ["Joachim Ott", "Zhouhan Lin", "Ying Zhang", "Shih-Chii Liu", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1608.06902,", "citeRegEx": "Ott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ott et al\\.", "year": 2016}, {"title": "Neuflow: Dataflow vision processing system-on-a-chip", "author": ["Phi-Hung Pham", "Darko Jelaca", "Clement Farabet", "Berin Martini", "Yann LeCun", "Eugenio Culurciello"], "venue": "In Circuits and Systems (MWSCAS),", "citeRegEx": "Pham et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2012}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "The penn treebank: an overview", "author": ["Ann Taylor", "Mitchell Marcus", "Beatrice Santorini"], "venue": "In Treebanks,", "citeRegEx": "Taylor et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2003}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Accelerating very deep convolutional networks for classification and detection", "author": ["Xiangyu Zhang", "Jianhua Zou", "Kaiming He", "Jian Sun"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["Shuchang Zhou", "Yuxin Wu", "Zekun Ni", "Xinyu Zhou", "He Wen", "Yuheng Zou"], "venue": "arXiv preprint arXiv:1606.06160,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Deep Neural Networks have become important tools for modeling nonlinear functions in applications like computer vision (Krizhevsky et al., 2012), speech recognition (Hinton et al.", "startOffset": 119, "endOffset": 144}, {"referenceID": 1, "context": ", 2012a), natural language processing (Bahdanau et al., 2014), and computer games (Silver et al.", "startOffset": 38, "endOffset": 61}, {"referenceID": 24, "context": ", 2014), and computer games (Silver et al., 2016).", "startOffset": 28, "endOffset": 49}, {"referenceID": 18, "context": "However, inference and training of a DNN may involve up to billions of operations for inputs likes images (Krizhevsky et al., 2012; Szegedy et al., 2014).", "startOffset": 106, "endOffset": 153}, {"referenceID": 26, "context": "However, inference and training of a DNN may involve up to billions of operations for inputs likes images (Krizhevsky et al., 2012; Szegedy et al., 2014).", "startOffset": 106, "endOffset": 153}, {"referenceID": 6, "context": ", 2015b;a), circulant matrix (Cheng et al., 2015), low rank (Jaderberg et al.", "startOffset": 29, "endOffset": 49}, {"referenceID": 16, "context": ", 2015), low rank (Jaderberg et al., 2014; Zhang et al., 2015), vector quantization (Gong et al.", "startOffset": 18, "endOffset": 62}, {"referenceID": 29, "context": ", 2015), low rank (Jaderberg et al., 2014; Zhang et al., 2015), vector quantization (Gong et al.", "startOffset": 18, "endOffset": 62}, {"referenceID": 9, "context": ", 2015), vector quantization (Gong et al., 2014), and hash trick (Chen et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 4, "context": ", 2014), and hash trick (Chen et al., 2015) etc.", "startOffset": 24, "endOffset": 43}, {"referenceID": 23, "context": "Another line of research tries to reduce bit-width of weights and activations of a DNN by quantization to low bit-width numbers (Rastegari et al., 2016; Hubara et al., 2016b; Zhou et al., 2016; Hubara et al., 2016a).", "startOffset": 128, "endOffset": 215}, {"referenceID": 30, "context": "Another line of research tries to reduce bit-width of weights and activations of a DNN by quantization to low bit-width numbers (Rastegari et al., 2016; Hubara et al., 2016b; Zhou et al., 2016; Hubara et al., 2016a).", "startOffset": 128, "endOffset": 215}, {"referenceID": 21, "context": "(Ott et al., 2016) claims that the weight binarization method does not work with RNNs, and introduces weight ternarization and leaves activations as floating point numbers.", "startOffset": 0, "endOffset": 18}, {"referenceID": 2, "context": "We adopt the \u201cstraight-through estimator\u201d (STE) method (Hinton et al., 2012b; Bengio et al., 2013) to circumvent this problem.", "startOffset": 55, "endOffset": 98}, {"referenceID": 20, "context": "We note that when a distribution has bounded variance \u03c3, the mean \u03bc approximates the median m as there is an inequality bounding the difference(Mallows, 1991): |\u03bc\u2212m| \u2264 \u03c3.", "startOffset": 143, "endOffset": 158}, {"referenceID": 23, "context": "It should be noted that for 1-bit quantization (binarization), the scaling factor should be 2 mean(|X|), which can be proved to be optimal in the sense of reconstruction error measured by Frobenius norm, as in (Rastegari et al., 2016).", "startOffset": 210, "endOffset": 234}, {"referenceID": 30, "context": "Quantization of activation follows the method in Zhou et al. (2016), assuming output of the previous layer has passed through a bounded activation function h, and we will apply quantization directly", "startOffset": 49, "endOffset": 68}, {"referenceID": 7, "context": "Different from plain feed forward neural network, recurrent neural networks, especially Long Short Term Memory (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Unit (Chung et al., 2014), have subtle and delicately designed structure, which makes their quantization more complex and need more careful considerations.", "startOffset": 169, "endOffset": 189}, {"referenceID": 7, "context": "Different from plain feed forward neural network, recurrent neural networks, especially Long Short Term Memory (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Unit (Chung et al., 2014), have subtle and delicately designed structure, which makes their quantization more complex and need more careful considerations. Nevertheless, the major algorithm is the same as Algorithm 1 in Zhou et al. (2016).", "startOffset": 170, "endOffset": 403}, {"referenceID": 28, "context": "The dropout technique, which randomly dropping a portion of features to 0 at training time, turns out be also an effective way of alleviating overfitting in RNN (Zaremba et al., 2014).", "startOffset": 161, "endOffset": 183}, {"referenceID": 27, "context": "For language modeling we use Penn Treebank dataset (Taylor et al., 2003), which contains 10K unique words.", "startOffset": 51, "endOffset": 72}, {"referenceID": 15, "context": "For fair comparison, in the following experiments, our model all use one hidden layer with 300 hidden units, which is the same setting as Hubara et al. (2016a). A word embedding layer is used at the input side of the network whose weights are trained from scratch.", "startOffset": 138, "endOffset": 160}, {"referenceID": 19, "context": "We do further experiments on sentence classification using IMDB datasets (Maas et al., 2011).", "startOffset": 73, "endOffset": 92}], "year": 2016, "abstractText": "Reducing bit-widths of weights, activations, and gradients of a Neural Network can shrink its storage size and memory usage, and also allow for faster training and inference by exploiting bitwise operations. However, previous attempts for quantization of RNNs show considerable performance degradation when using low bit-width weights and activations. In this paper, we propose methods to quantize the structure of gates and interlinks in LSTM and GRU cells. In addition, we propose balanced quantization methods for weights to further reduce performance degradation. Experiments on PTB and IMDB datasets confirm effectiveness of our methods as performances of our models match or surpass the previous stateof-the-art of quantized RNN.", "creator": "LaTeX with hyperref package"}}}