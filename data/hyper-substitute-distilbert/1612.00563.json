{"id": "1612.00563", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Self-critical Sequence Training for Image Captioning", "abstract": "recently it exists been uncovered that dominance - gradient methodology regarding reinforcement learning lie potentially utilized to train improving end - to - end outcomes dependence on gender - differentiable principles for the task at hand. across long paper we consider the problem pattern optimizing image captioning experiments using reinforcement learning, and maximize proficiency by carefully optimizing our systems using the stable metrics like updated mscoco analytics, immediate gains reward conditioning can be evaluated. newer systems are built using a new optimization scenario \u2014 we derive activity - critical sequence training ( scst ). scst is that requirement about the equilibrium marketing operation that, rather than average squared \" baseline \" and monitor the rate and sample variance, create the solutions of several underlying single - time inference algorithm to capture the rewards conditioning experiences. using reinforce approach, estimating improved reward value ( effective actor - critic actions must do ) \u2014 estimating targets ( as reinforce algorithms typically combine ) is faster, while at times same start harmonizing simulation model with respect to its test - time inference procedure. empirically analyses find whether directly optimizing model cider hierarchy introduces comparisons before prior decoding about measure - time is highly demanding. clinical results using the mscoco evaluation sever reflect a new state - of - the - art on the team, improving the best result modeling terms & benefits from 104. 9 & 141. 3.", "histories": [["v1", "Fri, 2 Dec 2016 04:37:22 GMT  (5428kb,D)", "http://arxiv.org/abs/1612.00563v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["steven j rennie", "etienne marcheret", "youssef mroueh", "jarret ross", "vaibhava goel"], "accepted": false, "id": "1612.00563"}, "pdf": {"name": "1612.00563.pdf", "metadata": {"source": "CRF", "title": "Self-critical Sequence Training for Image Captioning", "authors": ["Steven J. Rennie", "Etienne Marcheret", "Youssef Mroueh", "Jarret Ross", "Vaibhava Goel"], "emails": ["vgoel}@us.ibm.com"], "sections": [{"heading": "1. Introduction", "text": "Image captioning aims at generating a natural language description of an image. Open domain captioning is a very challenging task, as it requires a fine-grained understanding of the global and the local entities in an image, as well as their attributes and relationships. The recently released MSCOCO challenge [1] provides a new, larger scale platform for evaluating image captioning systems, complete with an evaluation server for benchmarking competing methods. Deep learning approaches to sequence modeling have yielded impressive results on the task, dominating the task leaderboard. Inspired by the recently introduced encoder/decoder paradigm for machine translation\nusing recurrent neural networks (RNNs) [2], [3], and [4] use a deep convolutional neural network (CNN) to encode the input image, and a Long Short Term Memory (LSTM) [5] RNN decoder to generate the output caption. These systems are trained end-to-end using back-propagation, and have achieved state-of-the-art results on MSCOCO. More recently in [6], the use of spatial attention mechanisms on CNN layers to incorporate visual context\u2014which implicitly conditions on the text generated so far\u2014was incorporated into the generation process. It has been shown and we have qualitatively observed that captioning systems that utilize attention mechanisms lead to better generalization, as these models can compose novel text descriptions based on the recognition of the global and local entities that comprise images.\nAs discussed in [7], deep generative models for text are typically trained to maximize the likelihood of the next ground-truth word given the previous ground-truth word using back-propagation. This approach has been called \u201cTeacher-Forcing\u201d [8]. However, this approach creates a mismatch between training and testing, since at test-time the model uses the previously generated words from the model distribution to predict the next word. This exposure bias [7], results in error accumulation during generation at test time, since the model has never been exposed to its own predictions.\nSeveral approaches to overcoming the exposure bias problem described above have recently been proposed. In [8] they show that feeding back the model\u2019s own predictions and slowly increasing the feedback probability p during training leads to significantly better test-time performance. Another line of work proposes \u201cProfessor-Forcing\u201d [9], a technique that uses adversarial training to encourage the dynamics of the recurrent network to be the same when training conditioned on ground truth previous words and when sampling freely from the network.\nWhile sequence models are usually trained using the cross entropy loss, they are typically evaluated at test time using discrete and non-differentiable NLP metrics such as BLEU [10], ROUGE [11], METEOR [12] or CIDEr [13].\n1\nar X\niv :1\n61 2.\n00 56\n3v 1\n[ cs\n.L G\n] 2\nD ec\n2 01\nIdeally sequence models for image captioning should be trained to avoid exposure bias and directly optimize metrics for the task at hand.\nRecently it has been shown that both the exposure bias and non-differentiable task metric issues can be addressed by incorporating techniques from Reinforcement Learning (RL) [14]. Specifically in [7], Ranzato et al. use the REINFORCE algorithm [15] to directly optimize nondifferentiable, sequence-based test metrics, and overcome both issues. REINFORCE as we will describe, allows one to optimize the gradient of the expected reward by sampling from the model during training, and treating those samples as ground-truth labels (that are re-weighted by the reward they deliver). The major limitation of the approach is that the expected gradient computed using mini-batches under REINFORCE typically exhibit high variance, and without proper context-dependent normalization, is typically unstable. The recent discovery that REINFORCE with proper bias correction using learned \u201cbaselines\u201d is effective has led to a flurry of work in applying REINFORCE to problems in RL, supervised learning, and variational inference [16, 17, 18]. Actor-critic methods [14] , which instead train a second \u201ccritic\u201d network to provide an estimate of the value of each generated word given the policy of an actor network, have also been investigated for sequence problems recently [19]. These techniques overcome the need to sample from the policy\u2019s (actors) action space, which can be enormous, at the expense of estimating future rewards, and training multiple networks based on one another\u2019s outputs, which as [19] explore, can also be unstable.\nIn this paper we present a new approach to sequence training which we call self-critical sequence training (SCST), and demonstrate that SCST can improve the performance of image captioning systems dramatically. SCST is a REINFORCE algorithm that, rather than estimating the reward signal, or how the reward signal should be normalized, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. As a result, only samples from the model that outperform the current test-time system are given positive weight, and inferior samples are suppressed. Using SCST, attempting to estimate the reward signal, as actor-critic methods must do, and estimating normalization, as REINFORCE algorithms must do, is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 112.3."}, {"heading": "2. Captioning Models", "text": "In this section we describe the recurrent models that we use for caption generation.\nFC models. Similarly to [3, 4], we first encode the input image F using a deep CNN, and then embed it through a linear projection WI . Words are represented with one hot vectors that are embedded with a linear embedding E that has the same output dimension as WI . The beginning of each sentence is marked with a special BOS token, and the end with an EOS token. Under the model, words are generated and then fed back into the LSTM, with the image treated as the first word WICNN(F ). The following updates for the hidden units and cells of an LSTM define the model [5]:\nxt = E1wt\u22121 for t > 1, x1 = WICNN(F ) it = \u03c3 (Wixxt +Wihht\u22121 + bi) (Input Gate) ft = \u03c3 (Wfxxt +Wfhht\u22121 + bf ) (Forget Gate) ot = \u03c3 (Woxxt +Wohht\u22121 + bo) (Output Gate) ct = it \u03c6(W\u2297zxxt +W\u2297zhht\u22121 + b\u2297z ) + ft ct\u22121 ht = ot tanh(ct) st = Wsht,\nwhere \u03c6 is a maxout non-linearity with 2 units (\u2297 denotes the units) and \u03c3 is the sigmoid function. We initialize h0 and c0 to zero. The LSTM outputs a distribution over the next word wt using the softmax function:\nwt \u223c softmax (st) (1)\nIn our architecture, the hidden states and word and image embeddings have dimension 512. Let \u03b8 denote the parameters of the model. Traditionally the parameters \u03b8 are learned by maximizing the likelihood of the observed sequence. Specifically, given a target ground truth sequence {w\u22171 , . . . , w\u2217T }, the objective is to minimize the cross entropy loss (XE):\nL(\u03b8) = \u2212 T\u2211 t=1 log(p\u03b8(w \u2217 t |w\u22171 , . . . w\u2217t\u22121)), (2)\nwhere p\u03b8(wt|w1, . . . wt\u22121) is given by the parametric model in Equation (1).\nAttention Model (Att2in). Rather than utilizing a static, spatially pooled representation of the image, attention models dynamically re-weight the input spatial (CNN) features to focus on specific regions of the image at each time step. In this paper we modify the architecture of the attention model for captioning given in [6], and input the attentionderived image feature only to the cell node of the LSTM. We have found that this architecture outperforms other designs when ADAM [20] is used for optimization.\nxt = E1wt\u22121 for t \u2265 1 w0 = BOS it = \u03c3 (Wixxt +Wihht\u22121 + bi) (Input Gate) ft = \u03c3 (Wfxxt +Wfhht\u22121 + bf ) (Forget Gate) ot = \u03c3 (Woxxt +Wohht\u22121 + bo) (Output Gate) ct = it \u03c6(W\u2297zxxt +W\u2297zIIt +W\u2297zhht\u22121 + b\u2297z ) + ft ct\u22121 ht = ot tanh(ct) st = Wsht,\nwhere It is the attention-derived image feature. This feature is derived as in [6] as follows: given CNN features at N locations {I1, . . . IN}, It = \u2211N i=1 \u03b1 i tIi, where \u03b1t = softmax(at + b\u03b1), and ait = W tanh(WaIIi +Wahht\u22121 + ba). In this work we set the dimension ofW to 1\u00d7512, and set c0 and h0 to zero. Let \u03b8 denote the parameters of the model. Then p\u03b8(wt|w1, . . . wt\u22121) is again defined by (1). The parameters \u03b8 of attention models are also traditionally learned by optimizing the XE loss (2)."}, {"heading": "3. Reinforcement Learning", "text": "Sequence Generation as an RL problem. As described in the previous section, captioning systems are traditionally trained using the cross entropy loss. To directly optimize NLP metrics and address the exposure bias issue, we can cast our generative models in the Reinforcement Learning terminology as in [7]. Our recurrent models (LSTM) introduced above can be viewed as an \u201cagent\u201d that interacts with an external \u201cenvironment\u201d (words and image features). The parameters of the network, \u03b8, define a policy p\u03b8, that results in an \u201caction\u201d that is the prediction of the next word. After each action, the agent (the LSTM) updates its internal \u201cstate\u201d (cells and hidden states of the LSTM, attention weights etc). Upon generating the end-of-sequence (EOS) token, the agent observes a \u201creward\u201d that is, for instance, the CIDEr score of the generated sentence\u2014we denote this reward by r. The reward is computed by an evaluation metric by comparing the generated sequence to corresponding ground-truth sequences. The goal of training is to minimize the negative expected reward:\nL(\u03b8) = \u2212Ews\u223cp\u03b8 [r(ws)] , (3) where ws = (ws1, . . . w s T ) and w s t is the word sampled from the model at the time step t. In practice L(\u03b8) is typically estimated with a single sample from p\u03b8:\nL(\u03b8) \u2248 \u2212r(ws), ws \u223c p\u03b8. Policy Gradient with REINFORCE. In order to compute the gradient \u2207\u03b8L(\u03b8), we use the REINFORCE algorithm [15](See also Chapter 13 in [14]). REINFORCE is based on the observation that the expected gradient of a nondifferentiable reward function can be computed as follows:\n\u2207\u03b8L(\u03b8) = \u2212Ews\u223cp\u03b8 [r(ws)\u2207\u03b8 log p\u03b8(ws)] . (4)\nIn practice the expected gradient can be approximated using a single Monte-Carlo sample ws = (ws1 . . . w s T ) from p\u03b8, for each training example in the minibatch:\n\u2207\u03b8L(\u03b8) \u2248 \u2212r(ws)\u2207\u03b8 log p\u03b8(ws).\nREINFORCE with a Baseline. The policy gradient given by REINFORCE can be generalized to compute the reward associated with an action value relative to a reference reward or baseline b:\n\u2207\u03b8L(\u03b8) = \u2212Ews\u223cp\u03b8 [(r(ws)\u2212 b)\u2207\u03b8 log p\u03b8(ws)] . (5)\nThe baseline can be any arbitrary function, as long as it does not depend on the \u201caction\u201d ws [14] since in this case:\nEws\u223cp\u03b8 [b\u2207\u03b8 log p\u03b8(ws)] = b \u2211 ws \u2207\u03b8p\u03b8(ws)\n= b\u2207\u03b8 \u2211 ws p\u03b8(w s) = b\u2207\u03b81 = 0. (6)\nThis shows that the baseline does not change the expected gradient, but importantly, it can reduce the the variance of the gradient estimate. For each training case, we again approximate the expected gradient with a single sample ws \u223c p\u03b8:\n\u2207\u03b8L(\u03b8) \u2248 \u2212(r(ws)\u2212 b)\u2207\u03b8 log p\u03b8(ws). (7)\nNote that if b is function of \u03b8 or t as in [7], equation (6) still holds and b(\u03b8) is a valid baseline.\nFinal Gradient Expression. Using the chain rule, and the parametric model of p\u03b8 given in Section 2 we have:\n\u2207\u03b8L(\u03b8) = T\u2211 t=1 \u2202L(\u03b8) \u2202st \u2202st \u2202\u03b8 ,\nwhere st is the input to the softmax function. Using REINFORCE with a baseline b the estimate of the gradient of \u2202L(\u03b8) \u2202st is given by [17]:\n\u2202L(\u03b8)\n\u2202st \u2248 (r(ws)\u2212 b)(p\u03b8(wt|ht)\u2212 1wst ). (8)"}, {"heading": "4. Self-critical sequence training (SCST)", "text": "The central idea of the self-critical sequence training (SCST) approach is to baseline the REINFORCE algorithm with the reward obtained by the current model under the inference algorithm used at test time. The gradient of the negative reward of a sample ws from the model w.r.t. to the softmax activations at time-step t then becomes:\n\u2202L(\u03b8)\n\u2202st = (r(ws)\u2212 r(w\u0302))(p\u03b8(wt|ht)\u2212 1wst ). (9)\nwhere r(w\u0302) again is the reward obtained by the current model under the inference algorithm used at test time. Accordingly, samples from the model that return higher reward than w\u0302 will be \u201cpushed up\u201d, or increased in probability, while samples which result in lower reward will be suppressed. Like MIXER [7], SCST has all the advantages of REINFORCE algorithms, as it directly optimizes the true, sequence-level, evaluation metric, but avoids the usual scenario of having to learn a (context-dependent) estimate of expected future rewards as a baseline. In practice we have found that SCST has much lower variance, and can be more effectively trained on mini-batches of samples using SGD. Since the SCST baseline is based on the test-time estimate under the current model, SCST is forced to improve the performance of the model under the inference algorithm used at test time. This encourages training/test time consistency like the maximum likelihood-based approaches \u201cData as Demonstrator\u201d [8], \u201cProfessor Forcing\u201d [9], and E2E [7], but importantly, it can directly optimize sequence metrics. Finally, SCST is self-critical, and so avoids all the inherent training difficulties associated with actor-critic methods, where a second \u201ccritic\u201d network must be trained to estimate value functions, and the actor must be trained on estimated value functions rather than actual rewards.\nIn this paper we focus on scenario of greedy decoding, where:\nw\u0302t = arg max wt\np(wt |ht) (10)\nThis choice, depicted in Figure 1, has several practical advantages. First and foremost, it minimizes the impact of baselining with the test-time inference algorithm on train-\ning time, since it requires only one additional forward pass, and trains the system to be optimized for fast, greedy decoding at test-time. This choice may also be among the best forms of SCST based on a single test-time estimate, as suppressing all samples that underperform relative to the final test-time estimate will tend to favor a very decisive policy. The investigation of forms of SCST that incorporate margin, utilize more than 1 test-time estimate (e.g. an n-best list) to baseline, and/or more elaborate test-time inference procedures (e.g. beam search) are interesting possible directions of future work."}, {"heading": "5. Experiments", "text": "Dataset. We evaluate our proposed method on the MSCOCO dataset [1]. For offline evaluation purposes we used the data splits from [21]. The training set contains 113, 287 images, along with 5 captions each. We use a set of 5K image for validation and report results on a test set of 5K images as well, as given in [21]. We report four widely used automatic evaluation metrics, BLEU-4, ROUGEL, METEOR, and CIDEr. We prune the vocabulary and drop any word that has count less then five, we end up with a vocabulary of size 10096 words.\nImage Features 1) FC Models. We use two type of Features: a) (FC-2k) features, where we encode each image with Resnet-101 (101 layers) [22]. Note that we do not rescale or crop each image. Instead we encode the full image with the final convolutional layer of resnet, and apply average pooling, which results in a vector of dimension\n2048. b) (FC-15K) features where we stack average pooled 13 layers of Resnet-101 (11 \u00d7 1024 and 2 \u00d7 2048). These 13 layers are the odd layers of conv4 and conv5, with the exception of the 23rd layer of conv4, which was omitted. This results in a feature vector of dimension 15360. 2) Spatial CNN features for Attention models: (Att2in) We encode each image using the residual convolutional neural network Resnet-101 [22]. Note that we do not rescale or crop the image. Instead we encode the full image with the final convolutional layer of Resnet-101, and apply spatially adaptive average pooling so that the output has a fixed size of 14 \u00d7 14 \u00d7 2048. At each time step the attention model produces an attention mask over the 96 spatial locations. This mask is applied and then the result is spatially averaged to produce a 2048 dimension representation of the attended portion of the image.\nImplementation Details. The LSTM hidden, image, word and attention embeddings dimension are fixed to 512 for all of the models discussed herein. All of our models are trained according to the following recipe, except where otherwise noted. We initialize all models by training the model under the XE objective using ADAM [20] optimizer with an initial learning rate of 5\u00d710\u22124. We anneal the learning rate by a factor of 0.8 every three epochs, and increase the probability of feeding back a sample of the word posterior by 0.05 every 5 epochs until we reach a feedback probability 0.25 [8]. We evaluate at each epoch the model on the development set and select the model with best CIDEr score as an initialization for SCST training. We then run SCST training initialized with the XE model to optimize the CIDEr metric (specifically, the CIDEr-D metric) using ADAM with a learning rate 5 \u00d7 10\u22125. Initially when experimenting with FC-2k and FC-15k models we utilized curriculum learning (CL) during training, as proposed in [7], by increasing the number of words that are sampled and trained under CIDEr by one each epoch (the prefix of the sentence remains under the XE criterion until eventually being subsumed). We have since realized that for the MSCOCO task CL is not required, and provides little to no boost in performance. The results reported here for the FC-2K and FC-15K models are trained with CL, while the attention models were trained directly on the entire sentence for all epochs after being initialized by the XE seed models."}, {"heading": "5.1. Offline Evaluation", "text": "Evaluating different RL training strategies. Table 1 compares the performance of SCST to that of MIXER [7] on the test portion of the Karpathy splits. In this experiment, we utilize \u201ccurriculum learning\u201d (CL) by optimizing the expected reward of the metric on the last n words of each training sentence, optimizing XE on the remaining sentence prefix, and slowly increasing n\nuntil the entire sentence is being sampled for all training cases. The results reported above were generated with a CL schedule matching the optimized schedule reported in [7]. Interestingly we found that CL was not necessary to successfully train both SCST and REINFORCE with a learned baseline on the MSCOCO dataset\u2014equally good results relative to not applying CL could be obtained by both a learned baseline, as was done in [7], and SCST. The gain of using SCST over using a learned baseline was consistently about 4 CIDEr points, regardless of the CL schedule (or lack thereof), and the initialization seed.\nTraining on different metrics. We experimented with training directly on the evaluation metrics of the MSCOCO challenge. Results for FC-2K models are depicted in table 2. In general we can see that optimizing for a given metric during training leads\nto the best performance on that same metric at test time, an expected result. We experimented with training on multiple test metrics, and found that we were unable to outperform the overall performance of the model trained only on the CIDEr metric, which lifts the performance of all other metrics considerably. For this reason most of our experimentation has since focused on optimizing CIDEr.\nSingle FC-Models Versus Attention Models. We trained FC models (2K and 15 K), as well as attention models using SCST with the CIDEr metric. We trained 4 different models for each FC and attention, starting the optimization from four different random seeds 1. We report in Table 3, the system with best performance for each family of models on the test portion of Karpathy splits [21]. We see that the FC-15K models outperform the FC-2K models. Both FC models are outperformed by the attention model, that establishes a new state of the art for a single model performance on Karpathy splits. Note that this quantitative evaluation favors attention models is inline with our observation that attention models tend to generalize better and compose outside of the context\n1please consult the supplementary material for additional details regarding how the models were trained.\nof the training of MSCOCO, as we will see in Section 6.\nModel Ensembling. In this section we use an ensemble of the four models mentioned above and trained using SCST in the FC and the attention modeling. We see in Table 4 that ensembling improves performance and confirms the supremacy of attention modeling, and establishes yet another state of the art result on Karpathy splits [21]. Note that in our case we ensemble only 4 models and we don\u2019t do any fine-tuning of the Resnet. NIC [23], in contrast, used an ensemble of 15 models with fine-tuned CNNs."}, {"heading": "5.2. Online Evaluation on MS-COCO Server", "text": "Table 5 reports the performance of the 4 ensembled attention models trained with self-critical sequence training (SCST) on the official MSCOCO evaluation server. The previous best result on the leaderboard (as of Nov. 15, 2016) is also depicted. We outperform the previous best system on all evaluation metrics."}, {"heading": "6. Example of Generated Captions", "text": "Here we provide a qualitative example of the captions generated by our systems for the input image in figure 2. This picture is taken from the objects out-of-context (OOOC) dataset of images from [24], and depicts a boat situated in an unusual context, and tests the ability of our models to compose descriptions of images that differ from those seen during training. The top 5 captions returned by the XE and SCST-trained FC-2K, FC-15K, and attention model ensembles when deployed with a decoding \u201cbeam\u201d of 5 are depicted in figure 3 2. On this image the FC models fail completely, and the SCST-trained ensemble of attention models is the only system that is able to correctly describe the image. In general we found that the performance of all captioning systems on MSCOCO data is qualitatively similar, while on images containing objects situated in an uncommon context [24] (i.e. unlike the MSCOCO training set) our attention models perform much better, and SCSTtrained attention models output yet more accurate and descriptive captions. In general we qualitatively found that SCST-trained attention models describe images more accurately, and with higher confidence, as reflected in Figure 2, where the average of the log-likelihoods of the words in each generated caption are also depicted. Additional examples, including an example with the corresponding heatmaps generated by the SCST-trained ensemble of attention models, can be found in the supplementary material (figure 7 of section C)."}, {"heading": "7. Discussion and Future Work", "text": "In this paper we have presented a simple and efficient approach to more effectively baselining the REINFORCE algorithm for policy-gradient based RL, which allows us to more effectively train on non-differentiable metrics, and leads to significant improvements in captioning performance on MSCOCO\u2014our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task. The self-critical approach, which normalizes the reward obtained by sampled sentences with the reward obtained by the model under the test-time inference algorithm is intuitive, and avoids having to estimate any state-dependent or\n2for details regarding our test-time beam search procedure, please consult section A of the supplementary material.\nindependent reward functions. Extensions of SCST that incorporate margin or utilize more than 1 test-time estimate (e.g. an n-best list) to baseline, and/or more elaborate testtime inference procedures (e.g. beam search) are interesting possible directions of future work."}, {"heading": "A. Beam search", "text": "Throughout the paper and in this supplementary material we often refer to caption results and evaluation metric results obtained using \u201cbeam search\u201d. This section briefly summarizes our beam search procedure. While decoding the image to generate captions that describe it, rather than greedily selecting the most probable word (N = 1), we can maintain a list of the N most probable sub-sequences generated so far, generate posterior probabilities for the next word of each of these subsequences, and then again prune down to the N -best sub-sequences. This approach is widely referred to as a beam search, where N is the width of the decoding \u201cbeam\u201d. In our experiments we additionally prune away hypotheses within the N -best list that have a log probability that is below that of the maximally probable partial sentence by more than \u2206log = 20. For all reported results, the value of N is tuned on a per-model basis on the validation set (of the Karpathy splits). On MSCOCO data, N = 2 is typically optimal for cross-entropy (XE) trained models and SCST-trained models, but in the latter case beam search provides only a very small boost in performance. For our captioning demonstrations we set N = 5 for all models for illustrative purposes, and because we have qualitatively observed that for test images that are substantially different from those encountered during training, beam search is important."}, {"heading": "B. Performance of XE versus SCST trained models", "text": "In tables 3 and 4 of the main text we compared the performance of models trained to optimize the CIDEr metric with self-critical sequence training (SCST) with that of their corresponding bootstrap models, which were trained under the crossentropy (XE) criterion using scheduled sampling [8]. We provide some additional details about these experiments here. For all XE models, the probability pf of feeding forward the maximally probable word rather than the ground-truth word was increased by 0.05 every 5 epochs until reaching a maximum value of 0.25. The XE model with the best performance on the validation set of the Karpathy splits was then selected as the bootstrap model for SCST.\nFor all models, the performance of greedily decoding each word at test time is reported, as is the performance of beam search as described in the previous section. As reported in [7], we found that beam search using RL-trained models resulted in very little performance gain. Figure 4 depicts the performance of our best Att2in model, which is trained to directly optimize the CIDEr metric, as a function of training epoch and evaluation metric, on the validation portion of the Karpathy splits. Optimizing CIDEr clearly improves all of the MSCOCO evaluation metrics substantially."}, {"heading": "C. Examples of Generated Captions", "text": "Figures 5-13 depict demonstrations of the captioning performance of all systems. In general we found that the performance of all captioning systems on MSCOCO data is qualitatively similar, while on images containing objects situated in an uncommon context [24] (i.e. unlike the MSCOCO training set) our attention models perform much better, and SCST-trained attention models output yet more accurate and descriptive captions. Attention heat-maps for the image and corresponding captions depicted in 5 and 13 are given in 7. The heatmaps of the attention weights are reasonably inline with the predicted words in both cases, and the SCST attention weights are spatially sharper here, and in general."}, {"heading": "D. Further details and analysis of SCST training", "text": "One detail that was crucial to optimizing CIDEr to produce better models was to include the EOS tag as a word. When the EOS word was omitted, trivial sentence fragments such as \u201cwith a\u201d and \u201cand a\u201d were dominating the metric gains, despite the \u201cgaming\u201d counter-measures (sentence length and precision clipping) that are included in CIDEr-D [13], which is what we optimized. Including the EOS tag substantially lowers the reward allocated to incomplete sentences, and completely resolved this issue. Another more obvious detail that is important is to associate the reward for the sentence with the first EOS encountered. Omitting the reward from the first EOS fails to reward sentence completion which leads to run-on, and rewarding any words that follow the first EOS token is inconsistent with the decoding procedure.\nThis work has focused on optimizing the CIDEr metric, since, as discussed in the paper, optimizing CIDER substantially improves all MSCOCO evaluation metrics, as was shown in tables 3 and 4 and is depicted in figure 4. Nevertheless, directly optimizing another metric does lead to higher evaluation scores on that same metric as shown, and so we have started to experiment with including models trained on Bleu, Rouge-L, and METEOR in our Att2in ensemble to attempt to improve it\nfurther. So far we have not been able to substantially improve performance w.r.t. the other metrics without more substantially degrading CIDEr.\nFigure 5: Picture of a common object in MSCOCO (a giraffe) situated in an uncommon context (out of COCO domain) [24].\n11/15/2016 Image Caption Generation Demo With Visual Attention\nhttp://dccxc027.pok.ibm.com:60000/upload 1/2\nIBM\u00a0Watson Image\u00a0Caption\u00a0Generation\u00a0Demo\u00a0With\u00a0Visual Attention\nImage:\u00a0 \u00a0 \u00a0#captions:\u00a0\n1.\u00a0a\u00a0person\u00a0holding\u00a0a\u00a0giraffe\u00a0in\u00a0a\u00a0field\u00a00.210482 2.\u00a0a\u00a0person\u00a0holding\u00a0a\u00a0giraffe\u00a0in\u00a0a\u00a0hand\u00a00.292092 3.\u00a0a\u00a0person\u00a0holding\u00a0a\u00a0banana\u00a0in\u00a0a\u00a0hand\u00a00.297332 4.\u00a0a\u00a0person\u00a0holding\u00a0a\u00a0banana\u00a0in\u00a0their\u00a0hand\u00a00.304586 5.\u00a0a\u00a0person\u00a0holding\u00a0a\u00a0giraffe\u00a0in\u00a0their\u00a0hand\u00a00.318557\n11/15/2016 Image Caption Generation Demo\nhttp://dccxc031.pok.ibm.com:60000/upload 1/1\nIBM\u00a0Watson\nImage\u00a0Caption\u00a0Generation\u00a0Demo Image:\u00a0 \u00a0 \u00a0#captions:\u00a0\n1.\u00a0a\u00a0small\u00a0child\u00a0is\u00a0holding\u00a0a\u00a0carrot\u00a0to\u00a0a\u00a0giraffe\u00a01.129857 2.\u00a0a\u00a0young\u00a0boy\u00a0is\u00a0holding\u00a0a\u00a0small\u00a0bird\u00a01.192267 3.\u00a0a\u00a0small\u00a0child\u00a0is\u00a0holding\u00a0a\u00a0small\u00a0bird\u00a01.192312 4.\u00a0a\u00a0small\u00a0child\u00a0is\u00a0holding\u00a0a\u00a0carrot\u00a0and\u00a0a\u00a0giraffe\u00a01.263775 5.\u00a0a\u00a0small\u00a0child\u00a0is\u00a0holding\u00a0a\u00a0small\u00a0toy\u00a01.303991\n(a) Ensemble of 4 Attention models (Att2in) trained with XE.\n11/15/2016 Image Caption Generation Demo\nhttp://dccxc230.pok.ibm.com:60000/upload 1/1\nIBM\u00a0Watson\nImage\u00a0Caption\u00a0Generation\u00a0Demo Image:\u00a0 \u00a0 \u00a0#captions:\u00a0\n1.\u00a0a\u00a0young\u00a0boy\u00a0sitting\u00a0on\u00a0a\u00a0table\u00a0with\u00a0a\u00a0bird\u00a00.414382 2.\u00a0a\u00a0person\u00a0holding\u00a0a\u00a0bird\u00a0in\u00a0a\u00a0hand\u00a00.443672 3.\u00a0a\u00a0young\u00a0boy\u00a0sitting\u00a0on\u00a0a\u00a0table\u00a0with\u00a0a\u00a0giraffe\u00a00.489850 4.\u00a0a\u00a0person\u00a0holding\u00a0a\u00a0bird\u00a0on\u00a0a\u00a0giraffe\u00a00.517687 5.\u00a0a\u00a0young\u00a0boy\u00a0holding\u00a0a\u00a0bird\u00a0on\u00a0a\u00a0hand\u00a00.525539\n(c) Ensemble of 4 FC-2K models trained with XE.\n(d) Ensemble of 4 FC-2K models trained with SCST.\n(b) Ensemble of 4 Attention models (Att2in) trained with SCST.\n11/15/2016 Image Caption Generation Demo With Visual Attention\nhttp://dccxc026.pok.ibm.com:60000/upload 1/2\nIBM\u00a0Watson\nImage\u00a0Caption\u00a0Generation\u00a0Demo\u00a0With\u00a0Visual Attention\nImage:\u00a0 \u00a0 \u00a0#captions:\u00a0\n1.\u00a0a\u00a0person\u00a0is\u00a0holding\u00a0a\u00a0small\u00a0animal\u00a0in\u00a0their\u00a0hand\u00a01.000011 2.\u00a0a\u00a0person\u00a0is\u00a0holding\u00a0a\u00a0baby\u00a0giraffe\u00a01.029134 3.\u00a0a\u00a0person\u00a0is\u00a0holding\u00a0a\u00a0small\u00a0giraffe\u00a0in\u00a0their\u00a0hand\u00a01.031801 4.\u00a0a\u00a0person\u00a0is\u00a0holding\u00a0a\u00a0small\u00a0animal\u00a0in\u00a0their\u00a0hands\u00a01.053029 5.\u00a0a\u00a0person\u00a0is\u00a0holding\u00a0a\u00a0small\u00a0giraffe\u00a01.056967\n(f) Ensemble of 4 FC-15K models trained with SCST. (e) Ensemble of 4 FC-15K models trained with XE.\n11/15/2016 Image Caption Generation Demo\nhttp://dccxc041.pok.ibm.com:60000/upload 1/1\nIBM\u00a0Watson\nImage\u00a0Caption\u00a0Generation\u00a0Demo Image:\u00a0 \u00a0 \u00a0#captions:\u00a0\n1.\u00a0a\u00a0person\u00a0is\u00a0holding\u00a0a\u00a0cat\u00a0in\u00a0a\u00a0hand\u00a00.301403 2.\u00a0a\u00a0person\u00a0is\u00a0holding\u00a0a\u00a0bird\u00a0in\u00a0a\u00a0hand\u00a00.302861 3.\u00a0a\u00a0person\u00a0is\u00a0holding\u00a0a\u00a0carrot\u00a0in\u00a0a\u00a0hand\u00a00.350183 4.\u00a0a\u00a0woman\u00a0is\u00a0holding\u00a0a\u00a0bird\u00a0in\u00a0a\u00a0hand\u00a00.354565 5.\u00a0a\u00a0person\u00a0is\u00a0holding\u00a0a\u00a0carrot\u00a0in\u00a0a\u00a0cat\u00a00.390414\n11/15/2016 Image Caption Generation Demo\nhttp://dccxc027.pok.ibm.com:50000/upload 1/1\nIBM\u00a0Watson\nImage\u00a0Caption\u00a0Generation\u00a0Demo Image:\u00a0 \u00a0 \u00a0#captions:\u00a0\n1.\u00a0a\u00a0close\u00a0up\u00a0of\u00a0a\u00a0person\u00a0holding\u00a0a\u00a0small\u00a0bird\u00a00.806333 2.\u00a0a\u00a0close\u00a0up\u00a0of\u00a0a\u00a0person\u00a0holding\u00a0a\u00a0baby\u00a00.854018 3.\u00a0a\u00a0close\u00a0up\u00a0of\u00a0a\u00a0person\u00a0holding\u00a0a\u00a0small\u00a0toy\u00a00.871933 4.\u00a0a\u00a0close\u00a0up\u00a0of\u00a0a\u00a0person\u00a0holding\u00a0a\u00a0remote\u00a00.875986 5.\u00a0a\u00a0close\u00a0up\u00a0of\u00a0a\u00a0person\u00a0holding\u00a0a\u00a0hand\u00a0holding\u00a0a\u00a0carrot\u00a00.932223\nFigure 6: Captions generated by various models discussed in the paper to describe the image depicted in figure 5. Beside each caption we report the average of the log probabilities of each word, normalized by the sentence length. Notice that the attention models trained with SCST give an accurate description of this image with high confidence. Attention models trained with XE are less confident about the correct description. FC models trained with CE or SCST fail at giving an accurate description.\n11/15/2016 Image Caption Generation Demo With Visual Attention\nhttp://dccxc027.pok.ibm.com:60000/upload 2/2"}], "references": [{"title": "Learning phrase representations using RNN encoderdecoder for statistical machine", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Fei-Fei Li"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": "ICLR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Professor forcing: A new algorithm for training recurrent networks", "author": ["Alex Lamb", "Anirudh Goyal", "Ying Zhang", "Saizheng Zhang", "Aaron Courville", "Yoshua Bengio"], "venue": "Neural Information Processing Systems (NIPS) 2016,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In ACL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out", "author": ["Chin-Yew Lin"], "venue": "Proceedings of the ACL-04 workshop,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation", "author": ["Satanjeev Banerjee", "Alon Lavie"], "venue": "and/or summarization,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "In Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "Arxiv, 2015", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor"], "venue": "arXiv preprint arXiv:1402.0030,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "An actor-critic algorithm for sequence prediction", "author": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron C. Courville", "Yoshua Bengio"], "venue": "Arxiv,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "ICLR, 2015", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Fei-Fei Li"], "venue": "In CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR, 2016", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Show and tell: Lessons learned from the 2015 MSCOCO image captioning challenge", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Inspired by the recently introduced encoder/decoder paradigm for machine translation using recurrent neural networks (RNNs) [2], [3], and [4] use a deep convolutional neural network (CNN) to encode the input image, and a Long Short Term Memory (LSTM) [5] RNN decoder to generate the output caption.", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "Inspired by the recently introduced encoder/decoder paradigm for machine translation using recurrent neural networks (RNNs) [2], [3], and [4] use a deep convolutional neural network (CNN) to encode the input image, and a Long Short Term Memory (LSTM) [5] RNN decoder to generate the output caption.", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "Inspired by the recently introduced encoder/decoder paradigm for machine translation using recurrent neural networks (RNNs) [2], [3], and [4] use a deep convolutional neural network (CNN) to encode the input image, and a Long Short Term Memory (LSTM) [5] RNN decoder to generate the output caption.", "startOffset": 138, "endOffset": 141}, {"referenceID": 3, "context": "Inspired by the recently introduced encoder/decoder paradigm for machine translation using recurrent neural networks (RNNs) [2], [3], and [4] use a deep convolutional neural network (CNN) to encode the input image, and a Long Short Term Memory (LSTM) [5] RNN decoder to generate the output caption.", "startOffset": 251, "endOffset": 254}, {"referenceID": 4, "context": "More recently in [6], the use of spatial attention mechanisms on CNN layers to incorporate visual context\u2014which implicitly conditions on the text generated so far\u2014was incorporated into the generation process.", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "As discussed in [7], deep generative models for text are typically trained to maximize the likelihood of the next ground-truth word given the previous ground-truth word using back-propagation.", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "This approach has been called \u201cTeacher-Forcing\u201d [8].", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "This exposure bias [7], results in error accumulation during generation at test time, since the model has never been exposed to its own predictions.", "startOffset": 19, "endOffset": 22}, {"referenceID": 6, "context": "In [8] they show that feeding back the model\u2019s own predictions and slowly increasing the feedback probability p during training leads to significantly better test-time performance.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "Another line of work proposes \u201cProfessor-Forcing\u201d [9], a technique that uses adversarial training to encourage the dynamics of the recurrent network to be the same when training conditioned on ground truth previous words and when sampling freely from the network.", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "While sequence models are usually trained using the cross entropy loss, they are typically evaluated at test time using discrete and non-differentiable NLP metrics such as BLEU [10], ROUGE [11], METEOR [12] or CIDEr [13].", "startOffset": 177, "endOffset": 181}, {"referenceID": 9, "context": "While sequence models are usually trained using the cross entropy loss, they are typically evaluated at test time using discrete and non-differentiable NLP metrics such as BLEU [10], ROUGE [11], METEOR [12] or CIDEr [13].", "startOffset": 189, "endOffset": 193}, {"referenceID": 10, "context": "While sequence models are usually trained using the cross entropy loss, they are typically evaluated at test time using discrete and non-differentiable NLP metrics such as BLEU [10], ROUGE [11], METEOR [12] or CIDEr [13].", "startOffset": 202, "endOffset": 206}, {"referenceID": 11, "context": "Recently it has been shown that both the exposure bias and non-differentiable task metric issues can be addressed by incorporating techniques from Reinforcement Learning (RL) [14].", "startOffset": 175, "endOffset": 179}, {"referenceID": 5, "context": "Specifically in [7], Ranzato et al.", "startOffset": 16, "endOffset": 19}, {"referenceID": 12, "context": "use the REINFORCE algorithm [15] to directly optimize nondifferentiable, sequence-based test metrics, and overcome both issues.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "The recent discovery that REINFORCE with proper bias correction using learned \u201cbaselines\u201d is effective has led to a flurry of work in applying REINFORCE to problems in RL, supervised learning, and variational inference [16, 17, 18].", "startOffset": 219, "endOffset": 231}, {"referenceID": 14, "context": "The recent discovery that REINFORCE with proper bias correction using learned \u201cbaselines\u201d is effective has led to a flurry of work in applying REINFORCE to problems in RL, supervised learning, and variational inference [16, 17, 18].", "startOffset": 219, "endOffset": 231}, {"referenceID": 15, "context": "The recent discovery that REINFORCE with proper bias correction using learned \u201cbaselines\u201d is effective has led to a flurry of work in applying REINFORCE to problems in RL, supervised learning, and variational inference [16, 17, 18].", "startOffset": 219, "endOffset": 231}, {"referenceID": 11, "context": "Actor-critic methods [14] , which instead train a second \u201ccritic\u201d network to provide an estimate of the value of each generated word given the policy of an actor network, have also been investigated for sequence problems recently [19].", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "Actor-critic methods [14] , which instead train a second \u201ccritic\u201d network to provide an estimate of the value of each generated word given the policy of an actor network, have also been investigated for sequence problems recently [19].", "startOffset": 230, "endOffset": 234}, {"referenceID": 16, "context": "These techniques overcome the need to sample from the policy\u2019s (actors) action space, which can be enormous, at the expense of estimating future rewards, and training multiple networks based on one another\u2019s outputs, which as [19] explore, can also be unstable.", "startOffset": 226, "endOffset": 230}, {"referenceID": 1, "context": "Similarly to [3, 4], we first encode the input image F using a deep CNN, and then embed it through a linear projection WI .", "startOffset": 13, "endOffset": 19}, {"referenceID": 2, "context": "Similarly to [3, 4], we first encode the input image F using a deep CNN, and then embed it through a linear projection WI .", "startOffset": 13, "endOffset": 19}, {"referenceID": 3, "context": "The following updates for the hidden units and cells of an LSTM define the model [5]: xt = E1wt\u22121 for t > 1, x1 = WICNN(F ) it = \u03c3 (Wixxt +Wihht\u22121 + bi) (Input Gate) ft = \u03c3 (Wfxxt +Wfhht\u22121 + bf ) (Forget Gate) ot = \u03c3 (Woxxt +Wohht\u22121 + bo) (Output Gate) ct = it \u03c6(W\u2297 zxxt +W\u2297 zhht\u22121 + bz ) + ft ct\u22121 ht = ot tanh(ct) st = Wsht, where \u03c6 is a maxout non-linearity with 2 units (\u2297 denotes the units) and \u03c3 is the sigmoid function.", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "In this paper we modify the architecture of the attention model for captioning given in [6], and input the attentionderived image feature only to the cell node of the LSTM.", "startOffset": 88, "endOffset": 91}, {"referenceID": 17, "context": "We have found that this architecture outperforms other designs when ADAM [20] is used for optimization.", "startOffset": 73, "endOffset": 77}, {"referenceID": 4, "context": "This feature is derived as in [6] as follows: given CNN features at N locations {I1, .", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "To directly optimize NLP metrics and address the exposure bias issue, we can cast our generative models in the Reinforcement Learning terminology as in [7].", "startOffset": 152, "endOffset": 155}, {"referenceID": 12, "context": "In order to compute the gradient \u2207\u03b8L(\u03b8), we use the REINFORCE algorithm [15](See also Chapter 13 in [14]).", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "In order to compute the gradient \u2207\u03b8L(\u03b8), we use the REINFORCE algorithm [15](See also Chapter 13 in [14]).", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "The baseline can be any arbitrary function, as long as it does not depend on the \u201caction\u201d w [14] since in this case:", "startOffset": 92, "endOffset": 96}, {"referenceID": 5, "context": "Note that if b is function of \u03b8 or t as in [7], equation (6) still holds and b(\u03b8) is a valid baseline.", "startOffset": 43, "endOffset": 46}, {"referenceID": 14, "context": "Using REINFORCE with a baseline b the estimate of the gradient of \u2202L(\u03b8) \u2202st is given by [17]:", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "Like MIXER [7], SCST has all the advantages of REINFORCE algorithms, as it directly optimizes the true, sequence-level, evaluation metric, but avoids the usual scenario of having to learn a (context-dependent) estimate of expected future rewards as a baseline.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "This encourages training/test time consistency like the maximum likelihood-based approaches \u201cData as Demonstrator\u201d [8], \u201cProfessor Forcing\u201d [9], and E2E [7], but importantly, it can directly optimize sequence metrics.", "startOffset": 115, "endOffset": 118}, {"referenceID": 7, "context": "This encourages training/test time consistency like the maximum likelihood-based approaches \u201cData as Demonstrator\u201d [8], \u201cProfessor Forcing\u201d [9], and E2E [7], but importantly, it can directly optimize sequence metrics.", "startOffset": 140, "endOffset": 143}, {"referenceID": 5, "context": "This encourages training/test time consistency like the maximum likelihood-based approaches \u201cData as Demonstrator\u201d [8], \u201cProfessor Forcing\u201d [9], and E2E [7], but importantly, it can directly optimize sequence metrics.", "startOffset": 153, "endOffset": 156}, {"referenceID": 18, "context": "For offline evaluation purposes we used the data splits from [21].", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "We use a set of 5K image for validation and report results on a test set of 5K images as well, as given in [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 19, "context": "We use two type of Features: a) (FC-2k) features, where we encode each image with Resnet-101 (101 layers) [22].", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "2) Spatial CNN features for Attention models: (Att2in) We encode each image using the residual convolutional neural network Resnet-101 [22].", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": "We initialize all models by training the model under the XE objective using ADAM [20] optimizer with an initial learning rate of 5\u00d710\u22124.", "startOffset": 81, "endOffset": 85}, {"referenceID": 6, "context": "25 [8].", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "Initially when experimenting with FC-2k and FC-15k models we utilized curriculum learning (CL) during training, as proposed in [7], by increasing the number of words that are sampled and trained under CIDEr by one each epoch (the prefix of the sentence remains under the XE criterion until eventually being subsumed).", "startOffset": 127, "endOffset": 130}, {"referenceID": 5, "context": "Table 1 compares the performance of SCST to that of MIXER [7] on the test portion of the Karpathy splits.", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "Table 1: Performance of self-critical sequence training (SCST) versus MIXER [7] on the test portion of the Karpathy splits when trained to optimize the CIDEr metric (FC2K models).", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "The results reported above were generated with a CL schedule matching the optimized schedule reported in [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 5, "context": "Interestingly we found that CL was not necessary to successfully train both SCST and REINFORCE with a learned baseline on the MSCOCO dataset\u2014equally good results relative to not applying CL could be obtained by both a learned baseline, as was done in [7], and SCST.", "startOffset": 251, "endOffset": 254}, {"referenceID": 18, "context": "Table 2: Performance on the test portion of the Karpathy splits [21] as a function of training metric ( FC-2K models).", "startOffset": 64, "endOffset": 68}, {"referenceID": 18, "context": "We report in Table 3, the system with best performance for each family of models on the test portion of Karpathy splits [21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 18, "context": "We see in Table 4 that ensembling improves performance and confirms the supremacy of attention modeling, and establishes yet another state of the art result on Karpathy splits [21].", "startOffset": 176, "endOffset": 180}, {"referenceID": 20, "context": "NIC [23], in contrast, used an ensemble of 15 models with fine-tuned CNNs.", "startOffset": 4, "endOffset": 8}], "year": 2016, "abstractText": "Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep endto-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a \u201cbaseline\u201d to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 112.3.", "creator": "LaTeX with hyperref package"}}}