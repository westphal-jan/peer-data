{"id": "1612.00567", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Shift-Reduce Constituent Parsing with Neural Lookahead Features", "abstract": "point - bound models can require fast and accurate for constituent parsing. commonly with chart - filled models, they interpret temporal features by extracting history data along a parser stack, smoothing spans over non - local constituents. on ( other hand, namely incremental parsing, constituent information scans the right hand side ( the compound word is briefly evaluated, which is a recommendation best than suffix - mapped parsing. to address entropy limitation, operators leverage a fast neural model to extract consistency errors. in tree, trees yield faster bidirectional lstm approximation, what leverages additional full representation information to predict different pairs of constituents among each attribute starts and receives. candidate results are then accumulated that assume strong transition - based comparison parser based smoothing features. traditional resulting parser reports 1. 53 % absolute improvement in sensitivity and 2. 3 % spatial intrinsic similarity to the latter, given relative highest initial constraints enabling node - supervised parsing.", "histories": [["v1", "Fri, 2 Dec 2016 04:55:24 GMT  (196kb,D)", "http://arxiv.org/abs/1612.00567v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiangming liu", "yue zhang"], "accepted": true, "id": "1612.00567"}, "pdf": {"name": "1612.00567.pdf", "metadata": {"source": "CRF", "title": "Shift-Reduce Constituent Parsing with Neural Lookahead Features", "authors": ["Jiangming Liu", "Yue Zhang"], "emails": ["zhang}@sutd.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "Transition-based constituent parsers are fast and accurate, performing incremental parsing using a sequence of state transitions in linear time. Pioneering models rely on a classifier to make local decisions, searching greedily for local transitions to build a parse tree (Sagae and Lavie, 2005). Zhu et al. (2013) use a beam search framework, which preserves linear time complexity of greedy search, while alleviating the disadvantage of error propagation. The model gives state-of-the-art accuracies at a speed of 89 sentences per second on the standard WSJ benchmark (Marcus et al., 1993).\nZhu et al. (2013) exploit rich features by extracting history information from a parser stack, which spans over non-local constituents. However, due to the incremental nature of shift-reduce parsing, the right-hand side constituents of the current word cannot be used to guide the action at each step. Such lookahead features (Tsuruoka et al., 2011) correspond to the outside scores in chart parsing (Goodman, 1998). It has been shown highly effective for obtaining improved accuracies.\nTo leverage such information for improving shiftreduce parsing, we propose a novel neural model to predict the constituent hierarchy related to each word before parsing. Our idea is inspired by the work of Roark and Hollingshead (2009) and Zhang et al. (2010b), which shows that shallow syntactic information gathered over the word sequence can be utilized for improving chart parsing speed without losing accuracies. For example, Roark and Hollingshead (2009) predict constituent boundary information on words as a pre-processing step, and use such information to prune the chart. Since such information is much lighter-weight compared to full parsing, it can be predicted relatively accurately using sequence labellers.\nDifferent from that of Roark and Hollingshead (2009), we collect lookahead constituent information for shift-reduce parsing, rather than pruning information for chart parsing. In addition, our concern is in the accuracies rather than the speed. Accordingly, our model should predict the constituent hierarchy on each word rather than simple boundary information. For example, in Figure 1(a), the constituent hierarchy that the word \u201cThe\u201d starts is \u201cS \u2192 NP\u201d, and the constituent hierarchy that the word \u201ctable\u201d ends is \u201cS\u2192 VP\u2192 NP\u2192 PP\u2192 NP\u201d. For\nar X\niv :1\n61 2.\n00 56\n7v 1\n[ cs\n.C L\n] 2\nD ec\n2 01\neach word, we predict both the constituent hierarchy it starts and the constituent hierarchy it ends, using them as lookahead features.\nThe task is challenging in three aspects. First, it is significantly more difficult compared to simple sequence labelling, since two sequences of constituent hierarchies must be predicted for each word in the input sequence. Second, for high accuracies, global features from the full sentence are necessary since constituent hierarchies contain rich structural information. Third, to retain high speed for shift-reduce parsing, lookahead feature prediction must be executed efficiently. It is highly difficult to build such a model using manual discrete features and structured search.\nFortunately, sequential recurrent neural networks (RNNs) are remarkably effective models to encode the full input sentence. We leverage RNNs for building our constituent hierarchy predictor. In particular, a LSTM (Hochreiter and Schmidhuber, 1997) is used to learn global features automatically from the input words. For each word, a second LSTM is then used to generate the constituent hierarchies greedily using features from the hidden layer of the first LSTM, in the same way as a neural language model decoder generating output sentences for machine translation (Bahdanau et al., 2014). The resulting model solves all the three challenges raised above. For fully-supervised learning, we learn word embeddings as a part of the model parameters.\nIn the standard WSJ (Marcus et al., 1993) and CTB 5.1 tests (Xue et al., 2005), our parser gives 1.3 F1 and 2.3 F1 improvement, respectively, over the\nbaseline of Zhu et al. (2013), resulting in a accuracy of 91.7 F1 for English and 85.5 F1 for Chinese, which are the best for fully-supervised models in the literature. We release our code, based on ZPar, at https://github.com/SUTDNLP/LookAheadConparser."}, {"heading": "2 Baseline System", "text": "We adopt the parser of Zhu et al. (2013) for a baseline, which is based on the shift-reduce process of Sagae and Lavie (2005) and the beam search strategy of Zhang and Clark (2011) with the global perceptron training."}, {"heading": "2.1 The Shift-Reduce System", "text": "Shift-reduce parsers process an input sentence incrementally from left to right. A stack is used to maintain partial phrase-structures, while the incoming words are ordered in a buffer. At each step, a transition action is applied to consume an input word or construct a new phrase-structure. The set of transition actions are\n\u2022 SHIFT: pop the front word off the buffer, and push it onto the stack.\n\u2022 REDUCE-L/R-X: pop the top two constituents off the stack, combine them into a new constituent with label X, and push the new constituent onto the stack.\n\u2022 UNARY-X: pop the top constituent off the stack, raise it to a new constituent X, and push the new constituent onto the stack.\n\u2022 FINISH: pop the root node off the stack and end parsing.\n\u2022 IDLE: no-effect action on a completed state without changing items on the stack or buffer.\nThe deduction system for the process is shown in Figure 2, where a state is represented as [stack, buffer front index, completion mark, action index], and n is the number of words in the input. For example, given the sentence \u201cThey like apples\u201d, the action sequence \u201cSHIFT, SHIFT, SHIFT, REDUCEL-VP, REDUCE-R-S\u201d gives its syntax \u201c(S They (VP like apples) )\u201d."}, {"heading": "2.2 Search and Training", "text": "Beam-search is used for decoding with the k best state items at each step being kept in the agenda. During initialization, the agenda contains only the initial state [\u03c6, 0, false, 0]. At each step, each state in the agenda is popped and expanded by applying all valid transition actions, and the top k resulting states are put back onto the agenda (Zhu et al., 2013). The process repeats until the agenda is empty, and the best completed state is taken as output.\nThe score of a state is the total score of the transition actions that have been applied to build it:\nC(\u03b1) = N\u2211 i=1 \u03a6(\u03b1i) \u00b7 ~\u03b8 (1)\nHere \u03a6(\u03b1i) represents the feature vector for the ith action \u03b1i in the state item \u03b1. N is the total number of actions in \u03b1.\nThe model parameter set ~\u03b8 is trained online using the averaged perceptron algorithm with the earlyupdate strategy (Collins and Roark, 2004)."}, {"heading": "2.3 Baseline Features", "text": "Our baseline features are taken from Zhu et al. (2013). As shown in Table 1, they include the UNIGRAM, BIGRAM, TRIGRAM features of Zhang and Clark (2009) and the extended features of Zhu et al. (2013)."}, {"heading": "3 Global Lookahead Features", "text": "The baseline features suffer two limitations, as mentioned in the introduction. First, they are relatively local to the state, considering only the neighbouring nodes of s0 (top of stack) and q0 (front of buffer). Second, they do not consider lookahead information beyond s3, or the syntactic structure of the buffer and sequence. We use a LSTM to capture full sentential information in linear time, representing such global information into the baseline parser as a constituent hierarchy for each word. Lookahead features are extracted from the constituent hierarchy to provide top-down guidance for bottom-up parsing."}, {"heading": "3.1 Constituent Hierarchy", "text": "In a constituency tree, each word can start or end a constituent hierarchy. As shown in Figure 1, the word \u201cThe\u201d starts a constituent hierarchy \u201cS\u2192 NP\u201d. In particular, it starts a constituent S in the top level\nand then a following constituent NP. The word \u201ctable\u201d ends a constituent hierarchy \u201cS\u2192 VP\u2192 NP\u2192 PP \u2192 NP\u201d. In particular, it ends a constituent S in the top level, and then a VP (starting from the word \u201clike\u201d), an NP (starting from the noun phrase \u201cthis book\u201d), a PP (starting from the word \u201cin\u201d), and finally an NP (starting from the word \u201cthe\u201d). The extraction of constituent hierarchies for each word is based on unbinarized grammars, reflecting the start and end in unbinarized trees. The constituent hierarchy is empty (denoted as \u03c6) if the corresponding words does not start or end a constituent. The constituent hierarchies are added into the shift-reduce parser as soft features (section 3.2).\nFormally, a constituent hierarchy is defined as\n[type : c1 \u2192 c2 \u2192 ...\u2192 cz],\nwhere c is a constituent label (e.g. NP), \u201c\u2192\u201d represents the top-down hierarchy, and type can be s or e, denoting that the current word starts or ends the constituent hierarchy, respectively, as shown in Figure 1. Compared with full parsing, the constituent hierarchies associated with each word have no forced structural dependencies between each other, and therefore can be modelled more easily, for each word individually. Serving as soft lookahead features rather than hard constraints, their interdependencies are not crucial for the main parser."}, {"heading": "3.2 Lookahead Features", "text": "The lookahead feature templates are defined in Table 2. In order to ensure parsing efficiency, only simple feature templates are taken into consideration. The lookahead features of a state are instantiated for the top two items on the stack (i.e. s0 and s1) and buffer (i.e. q0 and q1). The new function \u03a6\u2032 is defined to output the lookahead features vector. The score of a state in our model is simple extended form the Formula (1):\nC(\u03b1) = N\u2211 i=1 \u03a6(\u03b1i) \u00b7 ~\u03b8 + \u03a6\u2032(\u03b1i) \u00b7 ~\u03b8\u2032\nFor each word, the lookahead feature represents the next level constituent in the top-down hierarchy, which can guide bottom-up parsing.\nFor example, Figure 3 shows two intermediate states during parsing. In Figure 3(a), the s-type and e-type lookahead features of s1 (i.e. the word \u201cThe\u201d) are extracted from the constituent hierarchy in the bottom level, namely NP and NULL, respectively. On the other hand, in Figure 3(b), The s-type lookahead feature of s1 is extracted from the s-type constituent hierarchy of same word \u201cThe\u201d, but is S based on current hierarchical level. The e-type lookahead feature, on the other hand, is extracted from the etype constituent hierarchy of end word \u201cstudents\u201d of the VP constituent, which is NULL in the next level. Lookahead features for items on the buffer are extracted in the same way.\nThe lookahead features are useful for guiding shift-reduce decisions given the current state. For example, given the intermediate state in Figure 3(a), s0 has a s-type lookahead feature ADJP, and q1 in the buffer has e-type lookahead feature ADJP. This indicates that the two items are likely reduced into the same constituent. Further, s0 cannot end a constituent because of the empty e-type constituent hierarchy. As a result, the final shift-reduce parser will assign more possibility to SHIFT decision."}, {"heading": "4 Constituent Hierarchy Prediction", "text": "We propose a novel neural model for constituent hierarchy prediction. Inspired by the encoder-decoder framework for neural machine translation (Bahdanau et al., 2014; Cho et al., 2014), we use a\nLSTM to capture full sentence features, and another LSTM to generate the constituent hierarchies for each word. Compared with a CRF-based sequence labelling model (Roark and Hollingshead, 2009), the proposed model has three advantages. First, the global features can be automatically represented. Second, it can avoid the exponentially large number of labels if constituent hierarchies are treated as unique labels. Third, the model size is relatively small, and does not have a large effect on the final parser model.\nAs shown in Figure 4, the neural network consists of three main layers, namely the input layer, the encoder layer and the decoder layer. The input layer represents each word using its characters and token information; the encoder hidden layer uses a bidirectional recurrent neural network structure to learn global features from the sentence; and the decoder layer predicts constituent hierarchies according to the encoder layer features, by using attention mechanism (Bahdanau et al., 2014) to softly compute the contribution of each hidden unit of the encoder."}, {"heading": "4.1 Input Layer", "text": "The input layer generates a dense vector representation of each input word. We use character embeddings to alleviate OOV problems in word embeddings (Ballesteros et al., 2015; Santos and Zadrozny, 2014; Kim et al., 2016), concatenating a characterembedding representation of a word to its word embedding. Formally, the input representation xi of the word wi is computed by:\nxi = xwi \u2295 ci att ci att = \u2211 j \u03b1ijcij ,\nwhere xwi is a word embedding vector of the word wi according to a embedding lookup table, ci att is a character embedding form of the word wi, cij is the jth character in wi, and \u03b1ij is the contribution of the character cij to ci att, which is computed by:\n\u03b1ij = ef(xwi ,cij)\u2211 k e f(xwi ,cik)\nf is a non-linear transformation function based on tanh function."}, {"heading": "4.2 Encoder Layer", "text": "The encoder first uses a window strategy to represent input nodes with their corresponding local context nodes. Formally, a windowed word representation takes the form\nx\u2032i = [xi\u2212win; ...;xi; ...;xi+win].\nSecond, the encoder scans the input sentence and generates hidden units for each input word using a recurrent neural network (RNN), which represents features of the word from the global sequence. Formally, given the windowed input nodes x\u20321, x \u2032 2, ..., x\u2032n for the sentence w1, w2, ..., wn, the RNN layer calculates a hidden node sequence h1, h2, ..., hn.\nLong Short-Term Memory (LSTM) mitigates the vanishing gradient problem in RNN training, by introducing gates (i.e. input i, forget f and output o)\nand a cell memory vector c. We use the variation of Graves and Schmidhuber (2008). Formally, the values in the LSTM hidden layers are computed as follows:\nii = \u03c3(W1x \u2032 i +W2hi\u22121 +W3 ci\u22121 + b1) fi = 1\u2212 ii c\u0303i = tanh(W4x \u2032 i +W5hi\u22121 + b2)\nci = fi ci\u22121 + ii c\u0303i oi = \u03c3(W6x \u2032 i +W7hi\u22121 +W8 ci + b3)\nhi = oi tanh(ci),\nwhere is pair-wise multiplication. Further, in order to collect features for xi from both x\u20321, .., x \u2032 i\u22121 and x\u2032i+1, ... x \u2032 n, we use a bidirectional variation (Schuster and Paliwal, 1997; Graves et al., 2013). As shown in Figure 4, the hidden units are generated by concatenating the corresponding hidden layers of a left-to-right LSTM \u2212\u2192 hi and a right-to-left LSTM \u2190\u2212 hi , where hi = \u2212\u2192 hi \u2295 \u2190\u2212 hi for each word wi."}, {"heading": "4.3 Decoder Layer", "text": "The decoder hidden layer uses two different LSTMs to generate the s-type and e-type sequences of constituent labels from each encoder hidden output, respectively, as shown in Figure 4. Each constituent hierarchy is generated bottom-up recurrently. In particular, a sequence of state vectors is generated recurrently, with each state yielding a output constituent label. The process starts with a~0 state vector and ends when a NULL constituent is generated. The recurrent state transition process is achieved using LSTM model with the hidden vectors of the encoder layer being used for context features.\nFormally, for the word wi, the value of the jth state unit sij of the LSTM is computed by:\nsij = f(sij\u22121, cij , hi) 1,\nwhere the context cij is computed by: cij = \u2211 k \u03b2ijkhk\n\u03b2ijk = ef(sij\u22121,hk)\u2211 k\u2032 e f(sij\u22121,hk\u2032 )\n1Here, different from typical MT model (Bahdanau et al., 2014), the chain is predicted sequentially in a feed-forward way with no feedback of the prediction made. We found that this fast alternative gives similar results\nHere hk refers to the encoder hidden vector for wk. The weight of contribution \u03b2ijk are computed under attention mechanism (Bahdanau et al., 2014). The start state si,\u22121 = ~0.\nThe constituent labels are generated from each state unit sij , where each constituent label yij is the output of a SOFTMAX function.\np(yij = l) = es > ijWl\u2211\nk e s>ijWk\nyij = l denotes that the jth label of the ith word is l(l \u2208 L).\nAs shown in Figure 4, the SOFTMAX functions are applied to the state units of the decoder, generating hierarchical labels bottom-up, until the default label NULL is predicted."}, {"heading": "4.4 Training", "text": "We use two separate models to assign the s-type and e-type labels, respectively. For training each constituent hierarchy predictor, we minimize the following training objective:\nL(\u03b8) = \u2212 T\u2211 i Zi\u2211 j log pijo + \u03bb 2 ||\u03b8||2,\nwhere T is the the size of the sentence, Zi is the depth of the constituent hierarchy of the word wi, and pijo stands for p(yij = o), which is given by the SOFTMAX function, and o is the golden label.\nWe apply back-propagation, using momentum stochastic gradient descent (Sutskever et al., 2013) with a learning rate of \u03b7 = 0.01 for optimization and regularization parameter \u03bb = 10\u22126."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experiment Settings", "text": "English data come from the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1993). We use sections 2-21 for training, section 24 for system development, and section 23 for final performance evaluation. Chinese data come from the version 5.1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005). We use articles 001- 270 and 440-1151 for training, articles 301-325 for system development, and articles 271-300 were used for final performance evaluation. For both English and\nChinese data, we adopt ZPar2 for POS tagging, and use ten-fold jackknifing to assign auto POS tags to the training data. In addition, we use ten-fold jackknifing to assign auto constituent hierarchies to the training data.\nWe use F1 score to evaluate the constituent hierarchy prediction. For example, the constituent prediction is \u201cS \u2192 S \u2192 VP \u2192 NP\u201d and the golden is \u201cS \u2192 NP \u2192 NP\u201d. The evaluation starts from the bottom to the top, and the precision is 2/4 = 0.5, the recall is 2/3 = 0.66 and the F1 score is 0.57. The metric evaluates the precision and recall of each label in the constituent hierarchy. A label is counted as correct if and only if it occurs in the correct position index. We use EVALB to evaluate parsing performance, including labelled precision (LP ), labelled recall (LR), and bracketing F1.3"}, {"heading": "5.2 Model Settings", "text": "For training the constituent hierarchy prediction model, gold constituent labels are derived from labelled constituency trees in the training data. The hyper-parameters are chosen according to development tests, and the values are shown in Table 3.\nFor the shift-reduce constituency parser, we set the beam size to 16 for both training and decoding, which achieves a good tradeoff between efficiency\n2https://github.com/SUTDNLP/ZPar 3http://nlp.cs.nyu.edu/evalb\nand accuracy (Zhu et al., 2013). The optimal training iteration number is determined on the development sets."}, {"heading": "5.3 Results of Constituent Hierarchy Prediction", "text": "Table 4 shows the results of constituent hierarchy prediction, where word and character embeddings are randomly initialized, and fine-tuned during training. The third column shows the development parsing accuracies when the labels are used for lookahead features. As Table 4 shows, when the number of hidden layer increase, both s-type and e-type constituent hierarchy prediction improve. The accuracies of e-type prediction is relatively lower due to right-branching in the treebank, which makes e-type hierarchies longer than s-type hierarchies. In addition, a 3-layer LSTM does not give significantly improvements compared to 2-layer LSTM. For tradeoff between efficiency and accuracy, we choose the 2-layer LSTM as our constituent hierarchy predictor.\nTable 5 shows ablation results of constituent hierarchy prediction given by different reduced architectures, which include an architecture without character embeddings and an architecture with neither character embeddings nor input windows. We find that the original architecture achieves the highest performance on constituent hierarchy prediction, compared to the two baselines. The baseline only without the character embeddings has relatively small influence on constituent hierarchy prediction. On the other hand, the baseline only without the input word windows has relatively smaller influence on constituent hierarchy prediction. Nevertheless, both of these two ablation architectures lead to much lower parsing accuracies. The baseline removing both the character embeddings and the input word\nwindows has relatively low accuracies."}, {"heading": "5.4 Final Results", "text": "For English, we compare the final results with previous related work on the WSJ test sets. As shown in Table 64, our model achieves 1.3% F1 improvement compared to the baseline parser with fully-supervised learning (Zhu et al., 2013). Our model outperforms the state-of-the-art fullysupervised system (Carreras et al., 2008; Shindo et al., 2012) by 0.6% F1. In addition, our fullysupervised model also catches up with many stateof-the-art semi-supervised models (Zhu et al., 2013; Huang and Harper, 2009; Huang et al., 2010; Durrett and Klein, 2015) by achieving 91.7% F1 on WSJ test set. The size of our model is much smaller than the semi-supervised model of Zhu et al. (2013),\n4We treat the methods as semi-supervised if they use pretrained word embeddings, word clusters (e.g. Brown clusters) or extra resources.\nwhich contains rich features from a large automatically parsed corpus. In contrast, our model is about the same in size compared to the baseline parser.\nWe carry out Chinese experiments with the same models, and compare the final results with previous related work on the CTB test set. As shown in Table 7, our model achieves 2.3% F1 improvement compared to the state-of-the-art baseline system with fully-supervised learning (Zhu et al., 2013), which are by far the best results in the literature. In addition, our fully-supervised model is also comparable to many state-of-the-art semi-supervised models (Zhu et al., 2013; Wang and Xue, 2014; Wang et al., 2015; Dyer et al., 2016) by achieving 85.5% F1 on the CTB test set. Wang and Xue (2014) and Wang et al. (2015) do joint POS tagging and parsing."}, {"heading": "5.5 Comparison of Speed", "text": "Table 8 shows the running times of various parsers on test sets on a Intel 2.2 GHz processor with 16G memory. Our parsers are much faster than the related parser with the same shift-reduce framework (Sagae and Lavie, 2005; Sagae and Lavie, 2006). Compared to the baseline parser, our parser gives significant improvement on accuracies (90.4% to 91.7% F1) at the speed of 79.2 sentences per sec-\nond5, in contrast to 89.5 sentences per second on the standard WSJ benchmark."}, {"heading": "6 Errors Analysis", "text": "We conduct error analysis by measuring: parsing accuracies against different phrase types, constituents of different span lengths, and different sentence lengths."}, {"heading": "6.1 Phrase Type", "text": "Table 9 shows the accuracies of the baseline and the final parsers with lookahead features on 9 common phrase types. As the results show, while the parser with lookahead features achieves improvements on all of the frequent phrase types, there are relatively more improvements on constituent VP, S, SBAR and WHNP.\nThe constituent hierarchy predictor has relatively better performance on s-type labels for the constituents VP and WHNP, which are prone to errors by the baseline system. The constituent hierarchy can give guidance to the constituent parser for tackling the challenges. Compared to the s-type constituent hierarchy, the e-type constituent hierarchy is relatively more difficult to predict, particularly for the constituents with long spans such as VP, S and SBAR. Despite this, the e-type constituent hierarchies with relatively low accuracies also benefit prediction of constituents with long spans.\n5The constituent hierarchy prediction is excluded. The cost of this step is far less than the cost of parsing, and can be eliminated by pipelining the constituent hierarchy prediction and the shift-reduce decoder."}, {"heading": "6.2 Span Length", "text": "Figure 5 shows comparison of the two parsers on constituents with different span lengths. As the results show, lookahead features are helpful on both large spans and small spans, while the performance gap between the two parsers is larger as the size of span increases. This reflects the usefulness of longrange information captured by the constituent hierarchy predictor and lookahead features."}, {"heading": "6.3 Sentence Length", "text": "Figure 6 shows comparison of the two parsers on sentences of different lengths. As the results show, the parser with lookahead features outperforms the baseline system on both short sentences and long sentences. Also, the performance gap between the two parsers is larger as the length of sentence increases.\nThe constituent hierarchy predictors generate hierarchical constituents for each input word using global information. For longer sentences, the predictors yield deeper constituent hierarchies, offering corresponding lookahead features. As a result, compared to the baseline parser, the performance of\nthe parser with lookahead features decreases more slowly as the length of the sentences increases."}, {"heading": "7 Related Work", "text": "Our lookahead features are similar in spirit to the pruners of Roark and Hollingshead (2009) and Zhang et al. (2010b), which infer the maximum length of constituents that a particular word can start or end. However, our method is different in three main perspectives. First, rather than using a CRF with sparse local word window features, a neural network is used for dense global features on the sentence. Second, not only the size of constituents but also the constituent hierarchy is identified for each word. Third, the results are added into a transitionbased parser as soft features, rather then being used as hard constraints to a chart parser.\nOur concepts of constituent hierarchies are similar with the work of supertagging. For lexicalized grammars such as Combinatory Categorial Grammar (CCG), Tree-Adjoining Grammar (TAG) and Head-Driven Phrase Structure Grammar (HPSG), each word in the input sentence is assigned one or more super tags, which are used to identify the syn-\ntactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008). For a lexicalized grammar, supertagging can benefit the parsing in both accuracy and efficiency by offering almost-parsing information. In particular, Carreras et al. (2008) defined the concept spine in TAG, which is similar to our constituent hierarchy. However, there are three differences. First, the spine is defined to describe the main syntactic tree structure with a series of unary projections, while constituent hierarchy is defined to describe how words can start or end hierarchical constituents (it is possible to be empty if the word cannot start or end constituents). Second, spines are extracted from gold trees and used to prune the search space of parsing as hard constraints. In contrast, we use constituent hierarchies as soft features. Third, Carreras et al. (2008) use spines to prune a chart parsing, while we use constituent hierarchies to improve a linear shiftreduce parser.\nUnder the lexicalized grammar, this supertagging can benefit the parsing with more accuracy and efficiency as almost parsing (Bangalore and Joshi, 1999). Recently, the works on obtaining the super tags appear. Zhang et al. (2010a) proposed the efficient methods to obtain super tags for HPSG parsing using dependency information. Xu et al. (2015) and Vaswani et al. (2016) turn to design recursive neural network for supertagging for CCG parsing. In contrast, our models predict the constituent hierarchy instead of single super tag for each word in the input sentence, which are also likely regarded as the member of multiple ordered labels prediction family.\nOur constituent hierarchy predictor is also related to sequence-to-sequence learning (Sutskever et al., 2014), which is successful in neural machine translation (Bahdanau et al., 2014). The neural model encodes the source-side sentence into dense vectors, and then uses them to generate target-side word by word. There has also been work that directly use sequence-to-sequence model for constituent parsing, which generates bracketed constituency tree given raw sentences (Vinyals et al., 2015; Luong et al., 2015). Compared to Vinyals et al. (2015), who predicts a full parser tree from input, our predictors tackle a much simpler task, by predicting the con-\nstituent hierarchies of each word separately. In addition, the outputs of the predictors are used for soft lookahead features in bottom-up parsing, rather than being taken as output structures directly.\nBy integrating the neural constituent hierarchy predictor, our parser is related to neural network models for parsing, which has given competitive accuracies for both constituency parsing (Dyer et al., 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al., 2015; Dyer et al., 2015). In particular, our parser is more closely related to neural models that integrates discrete manual features (Socher et al., 2013; Durrett and Klein, 2015). Soccer et al. use neural features to rerank a sparse baseline parser; Durrett and Klein directly integrate sparse features into neural layers in a chart parser. In contrast, we integrate neural information into sparse features in the form of lookahead features.\nThere has also been work on lookahead features for parsing. Tsuruoka et al. (2011) run a baseline parser for a few future steps, and use the output actions to guide the current action. In contrast to their model, our model leverages full sentential information, yet is significantly faster.\nExisting work on investigated efficiency of parsing without loss of accuracy, which is required by real time applications, such as web parser, processing massive amounts of textual data. Zhang et al. (2010b) introduced a chart pruner to accelerate a CCG parser. Kummerfeld et al. (2010) proposed novel self-training method focusing on increasing the speed of a CCG parser rather than its accuracy."}, {"heading": "8 Conclusion", "text": "We proposed a novel constituent hierarchy predictor based on recurrent neural networks, aiming to capture global sentential information. The resulting constituent hierarchies are fed to a baseline shiftreduce parser as lookahead features, addressing the limitation of shift-reduce parsers in not leveraging right-hand side syntax for local decisions, yet maintaining the same model size and speed. The resulting fully-supervised parser outperforms the state-of-theart baseline parser by achieving 91.7% F1 on standard WSJ evaluation and 85.5% F1 on standard CTB evaluation."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their detailed and constructed comments. This work is supported by T2MOE 201301 of Singapore M-O-E. Yue Zhang is the corresponding author."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A Smith."], "venue": "EMNLP, pages 349\u2013359.", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Supertagging: an approach to almost parsing", "author": ["Srinivas Bangalore", "Aravind K Joshi."], "venue": "Computational Linguistics, 25(2):237\u2013265, June.", "citeRegEx": "Bangalore and Joshi.,? 1999", "shortCiteRegEx": "Bangalore and Joshi.", "year": 1999}, {"title": "On the parameter space of generative lexicalized statistical parsing models", "author": ["Daniel M Bikel."], "venue": "Ph.D Thesis, University of Pennsylvania.", "citeRegEx": "Bikel.,? 2004", "shortCiteRegEx": "Bikel.", "year": 2004}, {"title": "TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing", "author": ["Xavier Carreras", "Michael Collins", "Terry Koo."], "venue": "CoNLL, pages 9\u201316, Morristown, NJ, USA. Association for Computational Linguistics.", "citeRegEx": "Carreras et al\\.,? 2008", "shortCiteRegEx": "Carreras et al\\.", "year": 2008}, {"title": "Coarseto-Fine n-Best Parsing and MaxEnt Discriminative Reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "ACL.", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "A Maximum-Entropy-Inspired Parser", "author": ["Eugene Charniak."], "venue": "ANLP, pages 132\u2013139.", "citeRegEx": "Charniak.,? 2000", "shortCiteRegEx": "Charniak.", "year": 2000}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Danqi Chen", "Christopher Manning."], "venue": "EMNLP, pages 740\u2013750, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP, pages 1724\u20131734.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "The importance of supertagging for wide-coverage CCG parsing", "author": ["Stephen Clark", "James R Curran."], "venue": "COLING, pages 282\u2013288, Morristown, NJ, USA, August. University of Edinburgh, Association for Computational Linguistics.", "citeRegEx": "Clark and Curran.,? 2004", "shortCiteRegEx": "Clark and Curran.", "year": 2004}, {"title": "Supertagging for combinatory categorial grammar", "author": ["Stephen Clark."], "venue": "Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks, pages 101\u2013106, Universita di Venezia.", "citeRegEx": "Clark.,? 2002", "shortCiteRegEx": "Clark.", "year": 2002}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Michael Collins", "Brian Roark."], "venue": "ACL, pages", "citeRegEx": "Collins and Roark.,? 2004", "shortCiteRegEx": "Collins and Roark.", "year": 2004}, {"title": "Head-driven statistical models for natural language parsing", "author": ["Michael Collins."], "venue": "Computational linguistics, 29(4):589\u2013637.", "citeRegEx": "Collins.,? 2003", "shortCiteRegEx": "Collins.", "year": 2003}, {"title": "Enhancing Performance of Lexicalised Grammars", "author": ["R Dridan", "V Kordoni", "J Nicholson."], "venue": "ACL.", "citeRegEx": "Dridan et al\\.,? 2008", "shortCiteRegEx": "Dridan et al\\.", "year": 2008}, {"title": "Neural CRF Parsing", "author": ["Greg Durrett", "Dan Klein."], "venue": "ACL, pages 302\u2013312.", "citeRegEx": "Durrett and Klein.,? 2015", "shortCiteRegEx": "Durrett and Klein.", "year": 2015}, {"title": "TransitionBased Dependency Parsing with Stack Long ShortTerm Memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith."], "venue": "ACL-IJCNLP, pages 334\u2013343.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Recurrent Neural Network Grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A Smith."], "venue": "NAACL, pages 199 \u2013 209.", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Parsing Inside-Out", "author": ["Joshua Goodman."], "venue": "PhD thesis.", "citeRegEx": "Goodman.,? 1998", "shortCiteRegEx": "Goodman.", "year": 1998}, {"title": "Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "NIPS, pages 545\u2013552.", "citeRegEx": "Graves and Schmidhuber.,? 2008", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2008}, {"title": "Hybrid speech recognition with Deep Bidirectional LSTM", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed."], "venue": "IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), pages 273\u2013278. IEEE.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780, November.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "SelfTraining PCFG Grammars with Latent Annotations Across Languages", "author": ["Zhongqiang Huang", "Mary P Harper."], "venue": "EMNLP, pages 832\u2013841.", "citeRegEx": "Huang and Harper.,? 2009", "shortCiteRegEx": "Huang and Harper.", "year": 2009}, {"title": "Self-Training with Products of Latent Variable Grammars", "author": ["Zhongqiang Huang", "Mary P Harper", "Slav Petrov."], "venue": "EMNLP, pages 12\u201322.", "citeRegEx": "Huang et al\\.,? 2010", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Forest Reranking: Discriminative Parsing with Non-Local Features", "author": ["Liang Huang."], "venue": "ACL, pages 586\u2013 594.", "citeRegEx": "Huang.,? 2008", "shortCiteRegEx": "Huang.", "year": 2008}, {"title": "Character-Aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "AAAI.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Faster parsing by supertagger adaptation", "author": ["Jonathan K Kummerfeld", "Jessika Roesner", "Tim Dawborn", "James Haggerty", "James R Curran", "Stephen Clark."], "venue": "ACL, pages 345\u2013355. University of Cambridge, Association for Computational Linguistics, July.", "citeRegEx": "Kummerfeld et al\\.,? 2010", "shortCiteRegEx": "Kummerfeld et al\\.", "year": 2010}, {"title": "Multi-task Sequence to Sequence Learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "ICLR.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["Mitchell P Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Effective self-training for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "HLTNAACL, pages 152\u2013159, Morristown, NJ, USA. Association for Computational Linguistics.", "citeRegEx": "McClosky et al\\.,? 2006", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Extremely lexicalized models for accurate and fast HPSG parsing", "author": ["Takashi Ninomiya", "Takuya Matsuzaki", "Yoshimasa Tsuruoka", "Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "In EMNLP,", "citeRegEx": "Ninomiya et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ninomiya et al\\.", "year": 2006}, {"title": "Improved Inference for Unlexicalized Parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "HLT-NAACL, pages 404\u2013411.", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "A Linear Observed Time Statistical Parser Based on Maximum Entropy Models", "author": ["Adwait Ratnaparkhi."], "venue": "EMNLP.", "citeRegEx": "Ratnaparkhi.,? 1997", "shortCiteRegEx": "Ratnaparkhi.", "year": 1997}, {"title": "Linear Complexity Context-Free Parsing Pipelines via Chart Constraints", "author": ["Brian Roark", "Kristy Hollingshead."], "venue": "HLT-NAACL, pages 647\u2013655.", "citeRegEx": "Roark and Hollingshead.,? 2009", "shortCiteRegEx": "Roark and Hollingshead.", "year": 2009}, {"title": "A classifier-based parser with linear run-time complexity", "author": ["Kenji Sagae", "Alon Lavie."], "venue": "IWPT, pages 125\u2013132, Morristown, NJ, USA. Association for Computational Linguistics.", "citeRegEx": "Sagae and Lavie.,? 2005", "shortCiteRegEx": "Sagae and Lavie.", "year": 2005}, {"title": "Parser combination by reparsing", "author": ["Kenji Sagae", "Alon Lavie."], "venue": "HLT-NAACL, pages 129\u2013132, Morristown, NJ, USA. Association for Computational Linguistics.", "citeRegEx": "Sagae and Lavie.,? 2006", "shortCiteRegEx": "Sagae and Lavie.", "year": 2006}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Cicero D Santos", "Bianca Zadrozny."], "venue": "ICML, pages 1818\u20131826.", "citeRegEx": "Santos and Zadrozny.,? 2014", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "Signal Processing, IEEE transaction, 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Bayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing", "author": ["Hiroyuki Shindo", "Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata."], "venue": "ACL, pages 440\u2013448.", "citeRegEx": "Shindo et al\\.,? 2012", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "Parsing with Compositional Vector Grammars", "author": ["Richard Socher", "John Bauer", "Christopher D Manning", "Andrew Y Ng."], "venue": "ACL, pages 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E Dahl", "Geoffrey E Hinton."], "venue": "ICML, pages 1139\u20131147.", "citeRegEx": "Sutskever et al\\.,? 2013", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "NIPS, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Learning with Lookahead: Can History-Based Models Rival Globally Optimized Models", "author": ["Yoshimasa Tsuruoka", "Yusuke Miyao", "Jun\u2019ichi Kazama"], "venue": "In CoNLL,", "citeRegEx": "Tsuruoka et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tsuruoka et al\\.", "year": 2011}, {"title": "Supertagging with LSTMs", "author": ["A Vaswani", "Y Bisk", "K Sagae."], "venue": "NAACL.", "citeRegEx": "Vaswani et al\\.,? 2016", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "Grammar as a Foreign Language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "NIPS, pages 2773\u2013 2781.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Joint POS Tagging and Transition-based Constituent Parsing in Chinese with Non-local Features", "author": ["Zhiguo Wang", "Nianwen Xue."], "venue": "ACL, pages 733\u2013742, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Wang and Xue.,? 2014", "shortCiteRegEx": "Wang and Xue.", "year": 2014}, {"title": "Feature Optimization for Constituent Parsing via Neural Networks", "author": ["Zhiguo Wang", "Haitao Mi", "Nianwen Xue."], "venue": "ACL-IJCNLP, pages 1138\u20131147, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Transitionbased Neural Constituent Parsing", "author": ["Taro Watanabe", "Eiichiro Sumita."], "venue": "ACL, pages 1169\u20131179.", "citeRegEx": "Watanabe and Sumita.,? 2015", "shortCiteRegEx": "Watanabe and Sumita.", "year": 2015}, {"title": "CCG Supertagging with a Recurrent Neural Network", "author": ["Wenduan Xu", "Michael Auli", "Stephen Clark."], "venue": "ACL-IJCNLP, pages 250\u2013255, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "The Penn Chinese TreeBank: Phrase structure annotation of a large corpus", "author": ["Naiwen Xue", "Fei Xia", "Fudong Chiou", "MarTa Palmer."], "venue": "Natural Language Engineering, 11(2):207\u2013238.", "citeRegEx": "Xue et al\\.,? 2005", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "Transition-based parsing of the Chinese treebank using a global discriminative model", "author": ["Yue Zhang", "Stephen Clark."], "venue": "ICPT, pages 162\u2013171, Morristown, NJ, USA. Association for Computational Linguistics.", "citeRegEx": "Zhang and Clark.,? 2009", "shortCiteRegEx": "Zhang and Clark.", "year": 2009}, {"title": "Syntactic processing using the generalized perceptron and beam search", "author": ["Yue Zhang", "Stephen Clark."], "venue": "Computational linguistics, 37(1):105\u2013151.", "citeRegEx": "Zhang and Clark.,? 2011", "shortCiteRegEx": "Zhang and Clark.", "year": 2011}, {"title": "A simple approach for HPSG supertagging using dependency information", "author": ["Yao-zhong Zhang", "Takuya Matsuzaki", "Jun\u2019ichi Tsujii"], "venue": "In NAACL-HLT,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Chart Pruning for Fast Lexicalised-Grammar Parsing", "author": ["Yue Zhang", "Byung-Gyu Ahn", "Stephen Clark", "Curt Van Wyk", "James R Curran", "Laura Rimell."], "venue": "COLING, pages 1471\u20131479.", "citeRegEx": "Zhang et al\\.,? 2010b", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "A Neural Probabilistic Structured-Prediction Model for Transition-Based Dependency Parsing", "author": ["Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen."], "venue": "ACL, pages 1213\u20131222.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Fast and Accurate Shift-Reduce Constituent Parsing", "author": ["Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu."], "venue": "ACL, pages 434\u2013443.", "citeRegEx": "Zhu et al\\.,? 2013", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 33, "context": "Pioneering models rely on a classifier to make local decisions, searching greedily for local transitions to build a parse tree (Sagae and Lavie, 2005).", "startOffset": 127, "endOffset": 150}, {"referenceID": 27, "context": "The model gives state-of-the-art accuracies at a speed of 89 sentences per second on the standard WSJ benchmark (Marcus et al., 1993).", "startOffset": 112, "endOffset": 133}, {"referenceID": 41, "context": "Such lookahead features (Tsuruoka et al., 2011) correspond to the outside scores in chart parsing (Goodman, 1998).", "startOffset": 24, "endOffset": 47}, {"referenceID": 17, "context": ", 2011) correspond to the outside scores in chart parsing (Goodman, 1998).", "startOffset": 58, "endOffset": 73}, {"referenceID": 31, "context": "Pioneering models rely on a classifier to make local decisions, searching greedily for local transitions to build a parse tree (Sagae and Lavie, 2005). Zhu et al. (2013) use a beam search framework, which preserves linear time complexity of greedy search, while alleviating the disadvantage of error propagation.", "startOffset": 128, "endOffset": 170}, {"referenceID": 26, "context": "The model gives state-of-the-art accuracies at a speed of 89 sentences per second on the standard WSJ benchmark (Marcus et al., 1993). Zhu et al. (2013) exploit rich features by extracting history information from a parser stack, which spans over non-local constituents.", "startOffset": 113, "endOffset": 153}, {"referenceID": 32, "context": "Our idea is inspired by the work of Roark and Hollingshead (2009) and Zhang et al.", "startOffset": 36, "endOffset": 66}, {"referenceID": 32, "context": "Our idea is inspired by the work of Roark and Hollingshead (2009) and Zhang et al. (2010b), which shows that shallow syntactic information gathered over the word sequence can be utilized for improving chart parsing speed without losing accuracies.", "startOffset": 36, "endOffset": 91}, {"referenceID": 32, "context": "Our idea is inspired by the work of Roark and Hollingshead (2009) and Zhang et al. (2010b), which shows that shallow syntactic information gathered over the word sequence can be utilized for improving chart parsing speed without losing accuracies. For example, Roark and Hollingshead (2009) predict constituent boundary information on words as a pre-processing step, and use such information to prune the chart.", "startOffset": 36, "endOffset": 291}, {"referenceID": 32, "context": "Different from that of Roark and Hollingshead (2009), we collect lookahead constituent information for shift-reduce parsing, rather than pruning information for chart parsing.", "startOffset": 23, "endOffset": 53}, {"referenceID": 20, "context": "In particular, a LSTM (Hochreiter and Schmidhuber, 1997) is used to learn global features automatically from the input words.", "startOffset": 22, "endOffset": 56}, {"referenceID": 0, "context": "For each word, a second LSTM is then used to generate the constituent hierarchies greedily using features from the hidden layer of the first LSTM, in the same way as a neural language model decoder generating output sentences for machine translation (Bahdanau et al., 2014).", "startOffset": 250, "endOffset": 273}, {"referenceID": 27, "context": "In the standard WSJ (Marcus et al., 1993) and CTB 5.", "startOffset": 20, "endOffset": 41}, {"referenceID": 48, "context": "1 tests (Xue et al., 2005), our parser gives 1.", "startOffset": 8, "endOffset": 26}, {"referenceID": 54, "context": "baseline of Zhu et al. (2013), resulting in a accuracy of 91.", "startOffset": 12, "endOffset": 30}, {"referenceID": 49, "context": "We adopt the parser of Zhu et al. (2013) for a baseline, which is based on the shift-reduce process of Sagae and Lavie (2005) and the beam search strategy of Zhang and Clark (2011) with the global perceptron training.", "startOffset": 23, "endOffset": 41}, {"referenceID": 32, "context": "(2013) for a baseline, which is based on the shift-reduce process of Sagae and Lavie (2005) and the beam search strategy of Zhang and Clark (2011) with the global perceptron training.", "startOffset": 69, "endOffset": 92}, {"referenceID": 10, "context": "(2013) for a baseline, which is based on the shift-reduce process of Sagae and Lavie (2005) and the beam search strategy of Zhang and Clark (2011) with the global perceptron training.", "startOffset": 134, "endOffset": 147}, {"referenceID": 54, "context": "At each step, each state in the agenda is popped and expanded by applying all valid transition actions, and the top k resulting states are put back onto the agenda (Zhu et al., 2013).", "startOffset": 164, "endOffset": 182}, {"referenceID": 11, "context": "The model parameter set ~ \u03b8 is trained online using the averaged perceptron algorithm with the earlyupdate strategy (Collins and Roark, 2004).", "startOffset": 116, "endOffset": 141}, {"referenceID": 51, "context": "Our baseline features are taken from Zhu et al. (2013). As shown in Table 1, they include the UNIGRAM, BIGRAM, TRIGRAM features of Zhang and Clark (2009) and the extended features of Zhu et al.", "startOffset": 37, "endOffset": 55}, {"referenceID": 10, "context": "As shown in Table 1, they include the UNIGRAM, BIGRAM, TRIGRAM features of Zhang and Clark (2009) and the extended features of Zhu et al.", "startOffset": 85, "endOffset": 98}, {"referenceID": 10, "context": "As shown in Table 1, they include the UNIGRAM, BIGRAM, TRIGRAM features of Zhang and Clark (2009) and the extended features of Zhu et al. (2013).", "startOffset": 85, "endOffset": 145}, {"referenceID": 0, "context": "Inspired by the encoder-decoder framework for neural machine translation (Bahdanau et al., 2014; Cho et al., 2014), we use a LSTM to capture full sentence features, and another LSTM to generate the constituent hierarchies for each word.", "startOffset": 73, "endOffset": 114}, {"referenceID": 8, "context": "Inspired by the encoder-decoder framework for neural machine translation (Bahdanau et al., 2014; Cho et al., 2014), we use a LSTM to capture full sentence features, and another LSTM to generate the constituent hierarchies for each word.", "startOffset": 73, "endOffset": 114}, {"referenceID": 32, "context": "Compared with a CRF-based sequence labelling model (Roark and Hollingshead, 2009), the proposed model has three advantages.", "startOffset": 51, "endOffset": 81}, {"referenceID": 0, "context": "The input layer represents each word using its characters and token information; the encoder hidden layer uses a bidirectional recurrent neural network structure to learn global features from the sentence; and the decoder layer predicts constituent hierarchies according to the encoder layer features, by using attention mechanism (Bahdanau et al., 2014) to softly compute the contribution of each hidden unit of the encoder.", "startOffset": 331, "endOffset": 354}, {"referenceID": 1, "context": "We use character embeddings to alleviate OOV problems in word embeddings (Ballesteros et al., 2015; Santos and Zadrozny, 2014; Kim et al., 2016), concatenating a characterembedding representation of a word to its word embedding.", "startOffset": 73, "endOffset": 144}, {"referenceID": 35, "context": "We use character embeddings to alleviate OOV problems in word embeddings (Ballesteros et al., 2015; Santos and Zadrozny, 2014; Kim et al., 2016), concatenating a characterembedding representation of a word to its word embedding.", "startOffset": 73, "endOffset": 144}, {"referenceID": 24, "context": "We use character embeddings to alleviate OOV problems in word embeddings (Ballesteros et al., 2015; Santos and Zadrozny, 2014; Kim et al., 2016), concatenating a characterembedding representation of a word to its word embedding.", "startOffset": 73, "endOffset": 144}, {"referenceID": 18, "context": "We use the variation of Graves and Schmidhuber (2008). Formally, the values in the LSTM hidden layers are computed as follows:", "startOffset": 24, "endOffset": 54}, {"referenceID": 36, "context": "x \u2032 n, we use a bidirectional variation (Schuster and Paliwal, 1997; Graves et al., 2013).", "startOffset": 40, "endOffset": 89}, {"referenceID": 19, "context": "x \u2032 n, we use a bidirectional variation (Schuster and Paliwal, 1997; Graves et al., 2013).", "startOffset": 40, "endOffset": 89}, {"referenceID": 0, "context": "Here, different from typical MT model (Bahdanau et al., 2014), the chain is predicted sequentially in a feed-forward way with no feedback of the prediction made.", "startOffset": 38, "endOffset": 61}, {"referenceID": 0, "context": "The weight of contribution \u03b2ijk are computed under attention mechanism (Bahdanau et al., 2014).", "startOffset": 71, "endOffset": 94}, {"referenceID": 39, "context": "We apply back-propagation, using momentum stochastic gradient descent (Sutskever et al., 2013) with a learning rate of \u03b7 = 0.", "startOffset": 70, "endOffset": 94}, {"referenceID": 27, "context": "English data come from the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1993).", "startOffset": 81, "endOffset": 102}, {"referenceID": 48, "context": "1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005).", "startOffset": 37, "endOffset": 55}, {"referenceID": 54, "context": "and accuracy (Zhu et al., 2013).", "startOffset": 13, "endOffset": 31}, {"referenceID": 27, "context": "The baseline removing both the character embeddings and the input word Parser LR LP F1 Fully-supervised Ratnaparkhi (1997) 86.", "startOffset": 104, "endOffset": 123}, {"referenceID": 5, "context": "9 Charniak (2000) 89.", "startOffset": 2, "endOffset": 18}, {"referenceID": 5, "context": "9 Charniak (2000) 89.5 89.9 89.5 Collins (2003) 88.", "startOffset": 2, "endOffset": 48}, {"referenceID": 5, "context": "9 Charniak (2000) 89.5 89.9 89.5 Collins (2003) 88.1 88.3 88.2 Sagae and Lavie (2005)\u2020 86.", "startOffset": 2, "endOffset": 86}, {"referenceID": 5, "context": "9 Charniak (2000) 89.5 89.9 89.5 Collins (2003) 88.1 88.3 88.2 Sagae and Lavie (2005)\u2020 86.1 86.0 86.0 Sagae and Lavie (2006)\u2020 87.", "startOffset": 2, "endOffset": 125}, {"referenceID": 5, "context": "9 Charniak (2000) 89.5 89.9 89.5 Collins (2003) 88.1 88.3 88.2 Sagae and Lavie (2005)\u2020 86.1 86.0 86.0 Sagae and Lavie (2006)\u2020 87.8 88.1 87.9 Petrov and Klein (2007) 90.", "startOffset": 2, "endOffset": 165}, {"referenceID": 4, "context": "1 Carreras et al. (2008) 90.", "startOffset": 2, "endOffset": 25}, {"referenceID": 4, "context": "1 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) N/A N/A 91.", "startOffset": 2, "endOffset": 61}, {"referenceID": 4, "context": "1 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) N/A N/A 91.1 Zhu et al. (2013)\u2020 90.", "startOffset": 2, "endOffset": 92}, {"referenceID": 4, "context": "1 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) N/A N/A 91.1 Zhu et al. (2013)\u2020 90.2 90.7 90.4 Socher et al. (2013)* N/A N/A 90.", "startOffset": 2, "endOffset": 129}, {"referenceID": 4, "context": "1 Carreras et al. (2008) 90.7 91.4 91.1 Shindo et al. (2012) N/A N/A 91.1 Zhu et al. (2013)\u2020 90.2 90.7 90.4 Socher et al. (2013)* N/A N/A 90.4 Vinyals et al. (2015)* N/A N/A 88.", "startOffset": 2, "endOffset": 165}, {"referenceID": 37, "context": "Ensemble Shindo et al. (2012) N/A N/A 92.", "startOffset": 9, "endOffset": 30}, {"referenceID": 37, "context": "Ensemble Shindo et al. (2012) N/A N/A 92.4 Vinyals et al. (2015)* N/A N/A 90.", "startOffset": 9, "endOffset": 65}, {"referenceID": 5, "context": "Rerank Charniak and Johnson (2005) 91.", "startOffset": 7, "endOffset": 35}, {"referenceID": 5, "context": "Rerank Charniak and Johnson (2005) 91.2 91.8 91.5 Huang (2008) 92.", "startOffset": 7, "endOffset": 63}, {"referenceID": 22, "context": "Semi-supervised McClosky et al. (2006) 92.", "startOffset": 16, "endOffset": 39}, {"referenceID": 18, "context": "3 Huang and Harper (2009) 91.", "startOffset": 2, "endOffset": 26}, {"referenceID": 18, "context": "3 Huang and Harper (2009) 91.1 91.6 91.3 Huang et al. (2010) 91.", "startOffset": 2, "endOffset": 61}, {"referenceID": 18, "context": "3 Huang and Harper (2009) 91.1 91.6 91.3 Huang et al. (2010) 91.4 91.8 91.6 Zhu et al. (2013)\u2020 91.", "startOffset": 2, "endOffset": 94}, {"referenceID": 14, "context": "3 Durrett and Klein (2015)* N/A N/A 91.", "startOffset": 2, "endOffset": 27}, {"referenceID": 14, "context": "3 Durrett and Klein (2015)* N/A N/A 91.1 Dyer et al. (2016)*\u2020 N/A N/A 92.", "startOffset": 2, "endOffset": 60}, {"referenceID": 54, "context": "3% F1 improvement compared to the baseline parser with fully-supervised learning (Zhu et al., 2013).", "startOffset": 81, "endOffset": 99}, {"referenceID": 4, "context": "Our model outperforms the state-of-the-art fullysupervised system (Carreras et al., 2008; Shindo et al., 2012) by 0.", "startOffset": 66, "endOffset": 110}, {"referenceID": 37, "context": "Our model outperforms the state-of-the-art fullysupervised system (Carreras et al., 2008; Shindo et al., 2012) by 0.", "startOffset": 66, "endOffset": 110}, {"referenceID": 54, "context": "In addition, our fullysupervised model also catches up with many stateof-the-art semi-supervised models (Zhu et al., 2013; Huang and Harper, 2009; Huang et al., 2010; Durrett and Klein, 2015) by achieving 91.", "startOffset": 104, "endOffset": 191}, {"referenceID": 21, "context": "In addition, our fullysupervised model also catches up with many stateof-the-art semi-supervised models (Zhu et al., 2013; Huang and Harper, 2009; Huang et al., 2010; Durrett and Klein, 2015) by achieving 91.", "startOffset": 104, "endOffset": 191}, {"referenceID": 22, "context": "In addition, our fullysupervised model also catches up with many stateof-the-art semi-supervised models (Zhu et al., 2013; Huang and Harper, 2009; Huang et al., 2010; Durrett and Klein, 2015) by achieving 91.", "startOffset": 104, "endOffset": 191}, {"referenceID": 14, "context": "In addition, our fullysupervised model also catches up with many stateof-the-art semi-supervised models (Zhu et al., 2013; Huang and Harper, 2009; Huang et al., 2010; Durrett and Klein, 2015) by achieving 91.", "startOffset": 104, "endOffset": 191}, {"referenceID": 4, "context": "Our model outperforms the state-of-the-art fullysupervised system (Carreras et al., 2008; Shindo et al., 2012) by 0.6% F1. In addition, our fullysupervised model also catches up with many stateof-the-art semi-supervised models (Zhu et al., 2013; Huang and Harper, 2009; Huang et al., 2010; Durrett and Klein, 2015) by achieving 91.7% F1 on WSJ test set. The size of our model is much smaller than the semi-supervised model of Zhu et al. (2013),", "startOffset": 67, "endOffset": 444}, {"referenceID": 5, "context": "Parser LR LP F1 Fully-supervised Charniak (2000) 79.", "startOffset": 33, "endOffset": 49}, {"referenceID": 3, "context": "8 Bikel (2004) 79.", "startOffset": 2, "endOffset": 15}, {"referenceID": 3, "context": "8 Bikel (2004) 79.3 82.0 80.6 Petrov and Klein (2007) 81.", "startOffset": 2, "endOffset": 54}, {"referenceID": 3, "context": "8 Bikel (2004) 79.3 82.0 80.6 Petrov and Klein (2007) 81.9 84.8 83.3 Zhu et al. (2013)\u2020 82.", "startOffset": 2, "endOffset": 87}, {"referenceID": 3, "context": "8 Bikel (2004) 79.3 82.0 80.6 Petrov and Klein (2007) 81.9 84.8 83.3 Zhu et al. (2013)\u2020 82.1 84.3 83.2 Wang et al. (2015)\u2021 N/A N/A 83.", "startOffset": 2, "endOffset": 122}, {"referenceID": 5, "context": "Rerank Charniak and Johnson (2005) 80.", "startOffset": 7, "endOffset": 35}, {"referenceID": 51, "context": "Semi-supervised Zhu et al. (2013)\u2020 84.", "startOffset": 16, "endOffset": 34}, {"referenceID": 51, "context": "Semi-supervised Zhu et al. (2013)\u2020 84.4 86.8 85.6 Wand and Xue (2014)\u2021 N/A N/A 86.", "startOffset": 16, "endOffset": 70}, {"referenceID": 43, "context": "3 Wang et al. (2015)\u2021 N/A N/A 86.", "startOffset": 2, "endOffset": 21}, {"referenceID": 15, "context": "6 Dyer et al. (2016)*\u2020 N/A N/A 82.", "startOffset": 2, "endOffset": 21}, {"referenceID": 54, "context": "3% F1 improvement compared to the state-of-the-art baseline system with fully-supervised learning (Zhu et al., 2013), which are by far the best results in the literature.", "startOffset": 98, "endOffset": 116}, {"referenceID": 54, "context": "In addition, our fully-supervised model is also comparable to many state-of-the-art semi-supervised models (Zhu et al., 2013; Wang and Xue, 2014; Wang et al., 2015; Dyer et al., 2016) by achieving 85.", "startOffset": 107, "endOffset": 183}, {"referenceID": 44, "context": "In addition, our fully-supervised model is also comparable to many state-of-the-art semi-supervised models (Zhu et al., 2013; Wang and Xue, 2014; Wang et al., 2015; Dyer et al., 2016) by achieving 85.", "startOffset": 107, "endOffset": 183}, {"referenceID": 45, "context": "In addition, our fully-supervised model is also comparable to many state-of-the-art semi-supervised models (Zhu et al., 2013; Wang and Xue, 2014; Wang et al., 2015; Dyer et al., 2016) by achieving 85.", "startOffset": 107, "endOffset": 183}, {"referenceID": 16, "context": "In addition, our fully-supervised model is also comparable to many state-of-the-art semi-supervised models (Zhu et al., 2013; Wang and Xue, 2014; Wang et al., 2015; Dyer et al., 2016) by achieving 85.", "startOffset": 107, "endOffset": 183}, {"referenceID": 15, "context": ", 2015; Dyer et al., 2016) by achieving 85.5% F1 on the CTB test set. Wang and Xue (2014) and Wang et al.", "startOffset": 8, "endOffset": 90}, {"referenceID": 15, "context": ", 2015; Dyer et al., 2016) by achieving 85.5% F1 on the CTB test set. Wang and Xue (2014) and Wang et al. (2015) do joint POS tagging and parsing.", "startOffset": 8, "endOffset": 113}, {"referenceID": 33, "context": "Our parsers are much faster than the related parser with the same shift-reduce framework (Sagae and Lavie, 2005; Sagae and Lavie, 2006).", "startOffset": 89, "endOffset": 135}, {"referenceID": 34, "context": "Our parsers are much faster than the related parser with the same shift-reduce framework (Sagae and Lavie, 2005; Sagae and Lavie, 2006).", "startOffset": 89, "endOffset": 135}, {"referenceID": 27, "context": "2 sentences per secParser #Sent/Second Ratnaparkhi (1997) Unk Collins (2003) 3.", "startOffset": 39, "endOffset": 58}, {"referenceID": 10, "context": "2 sentences per secParser #Sent/Second Ratnaparkhi (1997) Unk Collins (2003) 3.", "startOffset": 62, "endOffset": 77}, {"referenceID": 5, "context": "5 Charniak (2000) 5.", "startOffset": 2, "endOffset": 18}, {"referenceID": 5, "context": "5 Charniak (2000) 5.7 Sagae and Lavie (2005) 3.", "startOffset": 2, "endOffset": 45}, {"referenceID": 5, "context": "5 Charniak (2000) 5.7 Sagae and Lavie (2005) 3.7 Sagae and Lavie (2006) 2.", "startOffset": 2, "endOffset": 72}, {"referenceID": 5, "context": "5 Charniak (2000) 5.7 Sagae and Lavie (2005) 3.7 Sagae and Lavie (2006) 2.2 Petrov and Klein (2007) 6.", "startOffset": 2, "endOffset": 100}, {"referenceID": 4, "context": "2 Carreras et al. (2008) Unk Zhu et al.", "startOffset": 2, "endOffset": 25}, {"referenceID": 4, "context": "2 Carreras et al. (2008) Unk Zhu et al. (2013) 89.", "startOffset": 2, "endOffset": 47}, {"referenceID": 54, "context": "The running times of related parsers are taken from Zhu et al. (2013).", "startOffset": 52, "endOffset": 70}, {"referenceID": 32, "context": "Our lookahead features are similar in spirit to the pruners of Roark and Hollingshead (2009) and Zhang et al.", "startOffset": 63, "endOffset": 93}, {"referenceID": 32, "context": "Our lookahead features are similar in spirit to the pruners of Roark and Hollingshead (2009) and Zhang et al. (2010b), which infer the maximum length of constituents that a particular word can start or end.", "startOffset": 63, "endOffset": 118}, {"referenceID": 10, "context": "tactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008).", "startOffset": 47, "endOffset": 151}, {"referenceID": 9, "context": "tactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008).", "startOffset": 47, "endOffset": 151}, {"referenceID": 4, "context": "tactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008).", "startOffset": 47, "endOffset": 151}, {"referenceID": 29, "context": "tactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008).", "startOffset": 47, "endOffset": 151}, {"referenceID": 13, "context": "tactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008).", "startOffset": 47, "endOffset": 151}, {"referenceID": 2, "context": "Under the lexicalized grammar, this supertagging can benefit the parsing with more accuracy and efficiency as almost parsing (Bangalore and Joshi, 1999).", "startOffset": 125, "endOffset": 152}, {"referenceID": 40, "context": "Our constituent hierarchy predictor is also related to sequence-to-sequence learning (Sutskever et al., 2014), which is successful in neural machine translation (Bahdanau et al.", "startOffset": 85, "endOffset": 109}, {"referenceID": 0, "context": ", 2014), which is successful in neural machine translation (Bahdanau et al., 2014).", "startOffset": 59, "endOffset": 82}, {"referenceID": 43, "context": "There has also been work that directly use sequence-to-sequence model for constituent parsing, which generates bracketed constituency tree given raw sentences (Vinyals et al., 2015; Luong et al., 2015).", "startOffset": 159, "endOffset": 201}, {"referenceID": 26, "context": "There has also been work that directly use sequence-to-sequence model for constituent parsing, which generates bracketed constituency tree given raw sentences (Vinyals et al., 2015; Luong et al., 2015).", "startOffset": 159, "endOffset": 201}, {"referenceID": 2, "context": "tactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008). For a lexicalized grammar, supertagging can benefit the parsing in both accuracy and efficiency by offering almost-parsing information. In particular, Carreras et al. (2008) defined the concept spine in TAG, which is similar to our constituent hierarchy.", "startOffset": 85, "endOffset": 327}, {"referenceID": 2, "context": "tactic role of the word for constraint parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008). For a lexicalized grammar, supertagging can benefit the parsing in both accuracy and efficiency by offering almost-parsing information. In particular, Carreras et al. (2008) defined the concept spine in TAG, which is similar to our constituent hierarchy. However, there are three differences. First, the spine is defined to describe the main syntactic tree structure with a series of unary projections, while constituent hierarchy is defined to describe how words can start or end hierarchical constituents (it is possible to be empty if the word cannot start or end constituents). Second, spines are extracted from gold trees and used to prune the search space of parsing as hard constraints. In contrast, we use constituent hierarchies as soft features. Third, Carreras et al. (2008) use spines to prune a chart parsing, while we use constituent hierarchies to improve a linear shiftreduce parser.", "startOffset": 85, "endOffset": 939}, {"referenceID": 1, "context": "Under the lexicalized grammar, this supertagging can benefit the parsing with more accuracy and efficiency as almost parsing (Bangalore and Joshi, 1999). Recently, the works on obtaining the super tags appear. Zhang et al. (2010a) proposed the efficient methods to obtain super tags for HPSG parsing using dependency information.", "startOffset": 126, "endOffset": 231}, {"referenceID": 1, "context": "Under the lexicalized grammar, this supertagging can benefit the parsing with more accuracy and efficiency as almost parsing (Bangalore and Joshi, 1999). Recently, the works on obtaining the super tags appear. Zhang et al. (2010a) proposed the efficient methods to obtain super tags for HPSG parsing using dependency information. Xu et al. (2015) and Vaswani et al.", "startOffset": 126, "endOffset": 347}, {"referenceID": 1, "context": "Under the lexicalized grammar, this supertagging can benefit the parsing with more accuracy and efficiency as almost parsing (Bangalore and Joshi, 1999). Recently, the works on obtaining the super tags appear. Zhang et al. (2010a) proposed the efficient methods to obtain super tags for HPSG parsing using dependency information. Xu et al. (2015) and Vaswani et al. (2016) turn to design recursive neural network for supertagging for CCG parsing.", "startOffset": 126, "endOffset": 373}, {"referenceID": 0, "context": ", 2014), which is successful in neural machine translation (Bahdanau et al., 2014). The neural model encodes the source-side sentence into dense vectors, and then uses them to generate target-side word by word. There has also been work that directly use sequence-to-sequence model for constituent parsing, which generates bracketed constituency tree given raw sentences (Vinyals et al., 2015; Luong et al., 2015). Compared to Vinyals et al. (2015), who predicts a full parser tree from input, our predictors tackle a much simpler task, by predicting the constituent hierarchies of each word separately.", "startOffset": 60, "endOffset": 448}, {"referenceID": 16, "context": "By integrating the neural constituent hierarchy predictor, our parser is related to neural network models for parsing, which has given competitive accuracies for both constituency parsing (Dyer et al., 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al.", "startOffset": 188, "endOffset": 234}, {"referenceID": 46, "context": "By integrating the neural constituent hierarchy predictor, our parser is related to neural network models for parsing, which has given competitive accuracies for both constituency parsing (Dyer et al., 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al.", "startOffset": 188, "endOffset": 234}, {"referenceID": 7, "context": ", 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al., 2015; Dyer et al., 2015).", "startOffset": 58, "endOffset": 120}, {"referenceID": 53, "context": ", 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al., 2015; Dyer et al., 2015).", "startOffset": 58, "endOffset": 120}, {"referenceID": 15, "context": ", 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al., 2015; Dyer et al., 2015).", "startOffset": 58, "endOffset": 120}, {"referenceID": 38, "context": "In particular, our parser is more closely related to neural models that integrates discrete manual features (Socher et al., 2013; Durrett and Klein, 2015).", "startOffset": 108, "endOffset": 154}, {"referenceID": 14, "context": "In particular, our parser is more closely related to neural models that integrates discrete manual features (Socher et al., 2013; Durrett and Klein, 2015).", "startOffset": 108, "endOffset": 154}, {"referenceID": 41, "context": "Tsuruoka et al. (2011) run a baseline parser for a few future steps, and use the output actions to guide the current action.", "startOffset": 0, "endOffset": 23}, {"referenceID": 50, "context": "Zhang et al. (2010b) introduced a chart pruner to accelerate a CCG parser.", "startOffset": 0, "endOffset": 21}, {"referenceID": 25, "context": "Kummerfeld et al. (2010) proposed novel self-training method focusing on increasing the speed of a CCG parser rather than its accuracy.", "startOffset": 0, "endOffset": 25}], "year": 2016, "abstractText": "Transition-based models can be fast and accurate for constituent parsing. Compared with chart-based models, they leverage richer features by extracting history information from a parser stack, which spans over non-local constituents. On the other hand, during incremental parsing, constituent information on the right hand side of the current word is not utilized, which is a relative weakness of shiftreduce parsing. To address this limitation, we leverage a fast neural model to extract lookahead features. In particular, we build a bidirectional LSTM model, which leverages the full sentence information to predict the hierarchy of constituents that each word starts and ends. The results are then passed to a strong transition-based constituent parser as lookahead features. The resulting parser gives 1.3% absolute improvement in WSJ and 2.3% in CTB compared to the baseline, given the highest reported accuracies for fully-supervised parsing.", "creator": "TeX"}}}