{"id": "1612.01859", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2016", "title": "Combinatorial semi-bandit with known covariance", "abstract": "the chain variant type - bandit problem is an inverse of the classical reverse - armed bandit problem satisfying which an algorithm pulls more than infinite rope starting each stage and hidden rewards of all weaker arms are revealed. measuring difference through the stronger arm variant about essentially accurate dependency structure of the arms is crucial. multiple calculations on each mechanism either used generic worst - cases filter or minimize independence per pull arms. experiments introduce some way to quantify the dependency parameter of the problem also design an attribute that adapts to it. the algorithm is comprising both linear regression and the formally developed techniques from the linear reasoning literature. by comparing this performance satisfying certain new exponential bound, we prove that it is optimal, up to one poly - logarithmic factor possessing finite number of available arms.", "histories": [["v1", "Tue, 6 Dec 2016 15:28:22 GMT  (135kb,D)", "http://arxiv.org/abs/1612.01859v1", "in NIPS 2016 (Conference on Neural Information Processing Systems), Dec 2016, Barcelona, Spain"]], "COMMENTS": "in NIPS 2016 (Conference on Neural Information Processing Systems), Dec 2016, Barcelona, Spain", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["r\u00e9my degenne", "vianney perchet"], "accepted": true, "id": "1612.01859"}, "pdf": {"name": "1612.01859.pdf", "metadata": {"source": "CRF", "title": "Combinatorial semi-bandit with known covariance", "authors": ["R\u00e9my Degenne", "Vianney Perchet"], "emails": ["degenne@cmla.ens-cachan.fr", "perchet@normalesup.org"], "sections": [{"heading": "1 Introduction and setting", "text": "The multi-armed bandit problem (MAB) is a sequential learning task in which an algorithm takes at each stage a decision (or, \u201cpulls an arm\u201d). It then gets a reward from this choice, with the goal of maximizing the cumulative reward [15]. We consider here its stochastic combinatorial extension, in which the algorithm chooses at each stage a subset of arms [2, 5, 6, 9]. These arms could form, for example, the path from an origin to a destination in a network. In the combinatorial setting, contrary to the the classical MAB, the inter-dependencies between the arms can play a role (we consider that the distribution of rewards is invariant with time). We investigate here how the covariance structure of the arms affects the difficulty of the learning task and whether it is possible to design a unique algorithm capable of performing optimally in all cases from the\n\u2217V. Perchet is partially funded by the ANR grant ANR-13-JS01-0004-01, benefited from the support of the \"FMJH Program Gaspard Monge in optimization and operation research\" and from EDF, and also from the support of the CNRS.\nar X\niv :1\n61 2.\n01 85\n9v 1\n[ cs\n.L G\nsimple scenario with independent rewards to the more challenging scenario of general correlated rewards.\nFormally, at each stage t \u2208 N, t \u2265 1, an algorithm pulls m \u2265 1 arms among d \u2265 m. Such a set of m arms is called an \u201caction\u201d and will be denoted by At \u2208 {0, 1}d, a vector with exactly m non-zero entries. The possible actions are restricted to an arbitrary fixed subset A \u2282 {0, 1}d. After choosing action At, the algorithm receives the reward A>t Xt, where Xt \u2208 Rd is the vector encapsulating the reward of the d arms at stage t. The successive reward vectors (Xt)t\u22651 are i.i.d with unknown mean \u00b5 \u2208 Rd. We consider a semi-bandit feedback system: after choosing the action At, the algorithm observes the reward of each of the arms in that action, but not the other rewards. Other possible feedbacks previously studied include bandit (only A>t Xt is revealed) and full information (Xt is revealed). The goal of the algorithm is to maximize the cumulated reward up to stage T \u2265 1 or equivalently to minimize the expected regret, which is the difference of the reward that would have been gained by choosing the best action in hindsight A\u2217 and what was actually gained:\nERT = E T\u2211 t=1 (A\u2217>\u00b5\u2212A>t \u00b5) .\nFor an action A \u2208 A, the difference \u2206A = (A\u2217>\u00b5\u2212A>\u00b5) is called gap of A. We denote by \u2206t the gap of At, so that regret rewrites as ERT = E \u2211T t=1 \u2206t. We also define the minimal gap of an arm, \u2206i,min = min{A\u2208A:i\u2208A}\u2206A. This setting was already studied [5], most recently in [7, 12], where two different algorithms are used to tackle on one hand the case where the arms have independent rewards and on the other hand the general bounded case. The regret guaranties of the two algorithms are different and reflect that the independent case is easier. Another algorithm for the independent arms case based on Thompson Sampling was introduced in [11]. One of the main objectives of this paper is to design a unique algorithm that can adapt to the covariance structure of the problem when prior information is available.\nThe following notations will be used throughout the paper: given a matrix M (resp. vector v), its (i, j)th (resp. ith) coefficient is denoted by M (ij) (resp. v(i)). For a matrix M , the diagonal matrix with same diagonal as M is denoted by \u03a3M .\nWe denote by \u03b7t the noise in the reward, i.e. \u03b7t := Xt \u2212 \u00b5. We consider a subgaussian setting, in which we suppose that there is a positive semi-definite matrix C such that for all t \u2265 1,\n\u2200u \u2208 Rd,E[eu >\u03b7t ] \u2264 e 12u >Cu .\nThis is equivalent to the usual setting for bandits where we suppose that the individual arms are subgaussian. Indeed if we have such a matrix C then each \u03b7\n(i) t is\n\u221a C(ii)-subgaussian. And under a subgaussian arms assumption, such a\nmatrix always exists. This setting encompasses the case of bounded rewards.\nWe call C a subgaussian covariance matrix of the noise (see appendix A of the supplementary material). A good knowledge of C can simplify the problem greatly, as we will show. In the case of 1-subgaussian independent rewards, in which C can be chosen diagonal, a known lower bound on the regret appearing in [7] is d\u2206 log T , while [12] proves a dm \u2206 log T lower bound in general. Our goal here is to investigate the spectrum of intermediate cases between these two settings, from the uninformed general case to the independent case in which one has much information on the relations between the arm rewards. We characterize the difficulty of the problem as a function of the subgaussian covariance matrix C. We suppose that we know a positive semi-definite matrix \u0393 such that for all vectors v with positive coordinates, v>Cv \u2264 v>\u0393v, property that we denote by C + \u0393. \u0393 reflects the prior information available about the possible degree of independence of the arms. We will study algorithms that enjoy regret bounds as functions of \u0393.\nThe matrix \u0393 can be chosen such that all its coefficients are non-negative and verify for all i, j, \u0393(ij) \u2264 \u221a \u0393(ii)\u0393(jj). From now on, we suppose that it is the case. In the following, we will use t such that \u03b7t = C 1/2 t and write for the reward: Xt = \u00b5+ C 1/2 t."}, {"heading": "2 Lower bound", "text": "We first prove a lower bound on the regret of any algorithm, demonstrating the link between the subgaussian covariance matrix and the difficulty of the problem. It depends on the maximal off-diagonal correlation coefficient of the covariance matrix. This coefficient is \u03b3 = max{(i,j)\u2208[d],i6=j} C\n(ij) \u221a C(ii)C(jj)\n. The bound is valid for consistent algorithms [13], for which the regret on any problem verifies ERt = o(ta) as t\u2192 +\u221e for all a > 0.\nTheorem 1. Suppose to simplify that d is a multiple of m. Then, for any \u2206 > 0, for any consistent algorithm, there is a problem with gaps \u2206, \u03c3-subgaussian arms and correlation coefficients smaller than \u03b3 \u2208 [0, 1] on which the regret is such that\nlim inf t\u2192+\u221e ERt log t \u2265 (1 + \u03b3(m\u2212 1))2\u03c3 2(d\u2212m) \u2206\nThis bound is a consequence of the classical result of [13] for multi-armed bandits, applied to the problem of choosing one among d/m paths, each of which has m different successive edges (Figure 1). The rewards in the same path are correlated but the paths are independent. A complete proof can be found in appendix B.1 of the supplementary material."}, {"heading": "3 OLS-UCB Algorithm and analysis", "text": "Faced with the combinatorial semi-bandit at stage t \u2265 1, the observations from t \u2212 1 stages form as many linear equations and the goal of an algorithm is to\nchoose the best action. To find the action with the highest mean, we estimate the mean of all arms. This can be viewed as a regression problem. The design of our algorithm stems from this observation and is inspired by linear regression in the fixed design setting, similarly to what was done in the stochastic linear bandit literature [16, 8]. There are many estimators for linear regression and we focus on the one that is simple enough and adaptive: Ordinary Least Squares (OLS)."}, {"heading": "3.1 Fixed design linear regression and OLS-UCB algorithm", "text": "For an action A \u2208 A, let IA be the diagonal matrix with a 1 at line i if A(i) = 1 and 0 otherwise. For a matrix M , we also denote by MA the matrix IAMIA. At stage t, if all actions A1, . . . , At were independent of the rewards, we would have observed a set of linear equations\nIA1X1 = IA1\u00b5+ IA1\u03b71\n... IAt\u22121Xt\u22121 = IAt\u22121\u00b5+ IAt\u22121\u03b7t\u22121\nand we could use the OLS estimator to estimate \u00b5, which is unbiased and has a known subgaussian constant controlling its variance. This is however not true in our online setting since the successive actions are not independent. At stage t, we define\nn (i) t = t\u22121\u2211 s=1 I{i \u2208 As}, n(ij)t = t\u22121\u2211 s=1 I{i \u2208 As}I{j \u2208 As} and Dt = t\u22121\u2211 s=1 IAs ,\nwhere n(i)t is the number of times arm i has been pulled before stage t and Dt is a diagonal matrix of these numbers. The OLS estimator is, for an arm i \u2208 [d],\n\u00b5\u0302 (i) t =\n1\nn (i) t \u2211 s<t:i\u2208As X(i)s = \u00b5 (i) + (D\u22121t t\u22121\u2211 s=1 IAsC 1/2 s) (i) .\nThen for allA \u2208 A, A>(\u00b5\u0302t\u2212\u00b5) in the fixed design setting has a subgaussian matrix equal to D\u22121t ( \u2211t\u22121 s=1 CAs)D \u22121 t . We get confidence intervals for the estimates and can use an upper confidence bound strategy [13, 3]. In the online learning setting the actions are not independent but we will show that using this estimator still leads to estimates that are well concentrated around \u00b5, with confidence intervals given by the same subgaussian matrix. The algorithm OLS-UCB (Algorithm 1) results from an application of an upper confidence bound strategy with this estimator.\nAlgorithm 1 OLS-UCB. Require: Positive semi-definite matrix \u0393, real parameter \u03bb > 0. 1: Choose actions such that each arm is pulled at least one time. 2: loop: at stage t, 3: At = arg maxAA >\u00b5\u0302t + Et(A)\nwith Et(A) = \u221a 2f(t) \u221a A>D\u22121t (\u03bb\u03a3\u0393Dt + \u2211t\u22121 s=1 \u0393As)D \u22121 t A.\n4: Choose action At, observe IAtXt. 5: Update \u00b5\u0302t, Dt. 6: end loop\nWe now turn to an analysis of the regret of OLS-UCB. At any stage t \u2265 1 of the algorithm, let \u03b3t = max{(i,j)\u2208At,i6=j} \u0393(ij)\u221a \u0393(ii)\u0393(jj)\nbe the maximal off-diagonal correlation coefficient of \u0393At and let \u03b3 = max{t\u2208[T ]} \u03b3t be the maximum up to stage T .\nTheorem 2. The OLS-UCB algorithm with parameter \u03bb > 0 and f(t) = log t+ (m+ 2) log log t+ m2 log(1 + e \u03bb ) enjoys for all times T \u2265 1 the regret bound\nE[RT ] \u226416f(T ) \u2211 i\u2208[d] \u0393(ii) \u2206i,min\n( 5(\u03bb+ 1\u2212 \u03b3) \u2308 logm\n1.6\n\u23092 + 45\u03b3m )\n+ 8dm2 maxi{C(ii)}\u2206max\n\u22062min + 4\u2206max ,\nwhere dxe stands for the smallest positive integer bigger than or equal to x. In particular, d0e = 1.\nThis bound shows the transition between a general case with a dm log T\u2206 regime and an independent case with a d log\n2m log T \u2206 upper bound (we recall that the\nlower bound is of the order of d log T\u2206 ). The weight of each case is given by the maximum correlation parameter \u03b3. The parameter \u03bb seems to be an artefact of the analysis and can in practice be taken very small or even equal to 0.\nFigure 1 illustrates the regret of OLS-UCB on the parallel paths problem used to derive the lower bound. It shows a linear dependency in \u03b3 and supports the hypothesis that the true upper bound matches the lower bound with a dependency in m and \u03b3 of the form (1 + \u03b3(m\u2212 1)).\nCorollary 1. The OLS-UCB algorithm with matrix \u0393 and parameter \u03bb > 0 has a regret bounded as\nE[RT ] \u2264 O( \u221a\u221a\u221a\u221adT log T max i\u2208[d] {\u0393(ii)} ( 5(\u03bb+ 1\u2212 \u03b3) \u2308 logm 1.6 \u23092 + 45\u03b3m ) ) .\nProof. We write that the regret up to stage T is bounded by \u2206T for actions with gap smaller than some \u2206 and bounded using theorem 2 for other actions (with \u2206min \u2265 \u2206). Maximizing over \u2206 then gives the result."}, {"heading": "3.2 Comparison with other algorithms", "text": "Previous works supposed that the rewards of the individual arms are in [0, 1], which gives them a 1/2-subgaussian property. Hence we suppose (\u2200i \u2208 [d], C(ii) = 1/2) for our comparison.\nIn the independent case, our algorithm is the same as ESCB-2 from [7], up to the parameter \u03bb. That paper shows that ESCB-2 enjoys an O(d \u221a m log T \u2206 ) upper bound but our analysis tighten it to O(d log 2m log T \u2206 ).\nIn the general (worst) case, [12] prove an O(dm log T\u2206 ) upper bound (which is tight) using CombUCB1, a UCB based algorithm introduced in [6] which at stage t uses the exploration term \u221a 1.5 log t \u2211 i\u2208A 1/ \u221a n (i) t . Our exploration term\nalways verifies Et(A) \u2264 \u221a f(t) \u2211 i\u2208A 1/ \u221a n (i) t with f(t) \u2248 log t (see section 3.6). Their exploration term is a worst-case confidence interval for the means. Their broader confidence intervals however have the desirable property that one can find the action that realizes the maximum index by solving a linear optimization problem, making their algorithm computationally efficient, quality that both ESCB and OLS-UCB are lacking.\nNone of the two former algorithms benefits from guaranties in the other regime. The regret of ESCB in the general possibly correlated case is unknown and the regret bound for CombUCB1 is not improved in the independent case. In contrast, OLS-UCB is adaptive in the sense that its performance gets better when more information is available on the independence of the arms."}, {"heading": "3.3 Regret Decomposition", "text": "Let Hi,t = {|\u00b5\u0302(i)t \u2212 \u00b5(i)| \u2265 \u2206t2m} and Ht = \u222a d i=1Hi,t. Ht is the event that at least one coordinate of \u00b5\u0302t is far from the true mean. Let Gt = {A\u2217>\u00b5 \u2265 A\u2217>\u00b5\u0302t + Et(A\n\u2217)} be the event that the estimate of the optimal action is below its true mean by a big margin. We decompose the regret according to these events:\nRT \u2264 T\u2211 t=1 \u2206tI{Gt,Ht}+ T\u2211 t=1 \u2206tI{Gt}+ T\u2211 t=1 \u2206tI{Ht}\nEvents Gt and Ht are rare and lead to a finite regret (see below). We first simplify the regret due to Gt \u2229Ht and show that it is bounded by the \"variance\" term of the algorithm.\nLemma 1. With the algorithm choosing at stage t the action At = arg maxA(A>\u00b5\u0302t+ Et(A)), we have \u2206tI{Gt,Ht} \u2264 2Et(At)I{\u2206t \u2264 Et(At)}.\nProof in appendix B.2 of the supplementary material. Then the regret is cut into three terms,\nRT \u2264 2 T\u2211 t=1 Et(At)I{\u2206t \u2264 2Et(At)}+ T\u2211 t=1 \u2206tI{Gt}+ T\u2211 t=1 \u2206tI{Ht} .\nThe three terms will be bounded as follows:\n\u2022 The Ht term leads to a finite regret from a simple application of Hoeffding\u2019s inequality.\n\u2022 The Gt term leads to a finite regret for a good choice of f(t). This is where we need to show that the exploration term of the algorithm gives a high probability upper confidence bound of the reward.\n\u2022 The Et(At) term, or variance term, is the main source of the regret and is bounded using ideas similar to the ones used in existing works on semi-bandits."}, {"heading": "3.4 Expected regret from Ht", "text": "Lemma 2. The expected regret due to the event Ht is E[ \u2211T t=1 \u2206tI{Ht}] \u2264 8dm2 maxi{C(ii)}\u2206max \u22062min .\nThe proof uses Hoeffding\u2019s inequality on the arm mean estimates and can be found in appendix B.2 of the supplementary material."}, {"heading": "3.5 Expected regret from Gt", "text": "We want to bound the probability that the estimated reward for the optimal action is far from its mean. We show that it is sufficient to control a selfnormalized sum and do it using arguments from [14], or [1] who applied them to linear bandits. The analysis also involves a peeling argument, as was done in one dimension by [10] to bound a similar quantity.\nLemma 3. Let \u03b4t > 0. With f\u0303(\u03b4t) = log(1/\u03b4t)+m log log t+m2 log(1+ e \u03bb ) and an algorithm given by the exploration term Et(A) = \u221a 2f\u0303(\u03b4t) \u221a A>D\u22121t (\u03bb\u03a3\u0393Dt + \u2211t\u22121 s=1 \u0393As)D \u22121 t A, then the event Gt = {A\u2217>\u00b5 \u2265 A\u2217>\u00b5\u0302t + Et(A\u2217)} verifies P{Gt} \u2264 \u03b4t . With \u03b41 = 1 and \u03b4t = 1t log2 t for t \u2265 2, such that f\u0303(\u03b4t) = f(t), the regret due to Gt is finite in expectation, bounded by 4\u2206max.\nProof. We use a peeling argument: let \u03b7 > 0 and for a = (a1, . . . , am) \u2208 Nm, let Da \u2282 [T ] be a subset of indices defined by (t \u2208 Da \u21d4 \u2200i \u2208 A\u2217, (1 + \u03b7)ai \u2264 n(i)t < (1 + \u03b7)ai+1). For any Bt \u2208 R,\nP { A\u2217>(\u00b5\u2212 \u00b5\u0302t) \u2265 Bt } \u2264 \u2211 a P { A\u2217>(\u00b5\u2212 \u00b5\u0302t) \u2265 Bt|t \u2208 Da } .\nThe number of possible sets Da for t is bounded by (log t/ log(1 + \u03b7))m, since each number of pulls n(i)t for i \u2208 A\u2217 is bounded by t. We now search a bound of the form P { A\u2217>(\u00b5\u2212 \u00b5\u0302t) \u2265 Bt|t \u2208 Da } . Suppose t \u2208 Da and let D be a positive definite diagonal matrix (that depends on a). Let St = \u2211t\u22121 s=1 IAs\u2229A\u2217C 1/2 s, Vt = \u2211t\u22121 s=1 CAs\u2229A\u2217 and IVt+D( ) = 1 2 \u2016St\u2016 2 (Vt+D)\u22121\n. Lemma 4. Let \u03b4t > 0 and let f\u0303(\u03b4t) be a function of \u03b4t. With a choice of D such that IA\u2217D \u03bbIA\u2217\u03a3CDt for all t in Da,\nP { A\u2217>(\u00b5\u2212\u00b5\u0302t)\u2265 \u221a 2f\u0303(\u03b4t)A\u2217>D \u22121 t (\u03bb\u03a3CDt+Vt)D \u22121 t A \u2217 \u2223\u2223\u2223t\u2208Da} \u2264 P{IVt+D( )\u2265f\u0303(\u03b4t)|t\u2208Da} .\nProof in appendix B.2 of the supplementary material. The self-normalized sum IVt( ) is an interesting quantity for the following\nreason: exp( 12IVt( )) = maxu\u2208Rd \u220ft\u22121 s=1 exp(u >IAs\u2229A\u2217C 1/2 s \u2212 u>CAs\u2229A\u2217u). For a given u, the exponential is smaller that 1 in expectation, from the subgaussian hypothesis. The maximum of the expectation is then smaller than 1. To control IVt( ), we are however interested in the expectation of this maximum and cannot interchange max and E. The method of mixtures circumvents this difficulty: it provides an approximation of the maximum by integrating the exponential against a multivariate normal centered at the point V \u22121t St, where the maximum is attained. The integrals over u and can then be swapped by Fubini\u2019s theorem to get an approximation of the expectation of the maximum using an integral of the expectations. Doing so leads to the following lemma, extracted from the proof of Theorem 1 of [1].\nLemma 5. Let D be a positive definite matrix that does not depend on t and Mt(D) =\n\u221a detD\ndet(Vt+D) exp(IVt+D( )). Then E[Mt(D)] \u2264 1. We rewrite P { IVt+D( ) \u2265 f\u0303(\u03b4t) } to introduce Mt(D), P { IVt+D( ) \u2265 f\u0303(\u03b4t)|t\u2208Da } = P { Mt(D) \u2265\n1\u221a det(Id +D\u2212 1/2VtD\u2212 1/2)\nexp(f\u0303(\u03b4t)) \u2223\u2223\u2223t\u2208Da} .\nThe peeling lets us bound Vt. Let Da be the diagonal matrix with entry (i, i) equal to (1 + \u03b7)ai for i \u2208 A\u2217 and 0 elsewhere.\nLemma 6. With D = \u03bb\u03a3CDa + I[d]\\A\u2217 , det(Id +D\u2212 1/2VtD \u22121/2) \u2264 (1 + 1+\u03b7\u03bb ) m .\nThe union bound on the sets Da and Markov\u2019s inequality give P { A\u2217>(\u00b5\u2212 \u00b5\u0302t) \u2265 \u221a 2f\u0303(\u03b4t) \u221a \u03bbA\u2217>\u03a3CD \u22121 t A \u2217 +A\u2217>D\u22121t VtD \u22121 t A \u2217 }\n\u2264 \u2211 Da P { Mt(D) \u2265 (1 + 1 + \u03b7 \u03bb )\u2212m/2 exp(f\u0303(\u03b4t))|t \u2208 Da }\n\u2264 ( log t\nlog(1 + \u03b7)\n)m (1 + 1 + \u03b7\n\u03bb )m/2 exp(\u2212f\u0303(\u03b4t))\nFor \u03b7 = e\u2212 1 and f\u0303(\u03b4t) as in lemma 3, this is bounded by \u03b4t. The result with \u0393 instead of C is a consequence of C + \u0393. With \u03b41 = 1 and \u03b4t = 1/(t log2 t) for t \u2265 2, the regret due to Gt is\nE[ T\u2211 t=1 \u2206tI{Gt}] \u2264 \u2206max(1 + T\u2211 t=2 1 t log2 t ) \u2264 4\u2206max ."}, {"heading": "3.6 Bounding the variance term", "text": "The goal of this section is to bound Et(At) under the event {\u2206t \u2264 Et(At)}. Let \u03b3t \u2208 [0, 1] such that for all i, j \u2208 At with i 6= j, \u0393(ij) \u2264 \u03b3t \u221a \u0393(ii)\u0393(jj). From the\nCauchy-Schwartz inequality, n(ij)t \u2264 \u221a n (i) t n (j) t . Using these two inequalities,\nA>t D \u22121 t ( t\u22121\u2211 s=1 \u0393As)D \u22121 t At = \u2211 i,j\u2208At n (ij) t \u0393 (ij) n (i) t n (j) t \u2264 (1\u2212 \u03b3t) \u2211 i\u2208At \u0393(ii) n (i) t + \u03b3t( \u2211 i\u2208At\n\u221a \u0393(ii)\nn (i) t\n)2 .\nWe recognize here the forms of the indexes used in [7] for independent arms (left term) and [12] for general arms (right term). Using \u2206t \u2264 Et(At) we get\n\u22062t 8f(t) \u2264 (\u03bb+ 1\u2212 \u03b3t) \u2211 i\u2208At \u0393(ii) n (i) t + \u03b3t( \u2211 i\u2208At\n\u221a \u0393(ii)\nn (i) t\n)2 . (1)\nThe strategy from here is to find events that must happen when (1) holds and to show that these events cannot happen very often. For positive integers j and t and for e \u2208 {1, 2}, we define the set of arms in At that were pulled less than a given threshold: Sjt,e = {i \u2208 At, n (i) t \u2264 \u03b1j,e\n8f(t)\u0393(ii)ge(m,\u03b3t) \u22062t\n}, with ge(m, \u03b3t) to be stated later and (\u03b1i,e)i\u22651 a decreasing sequence. Let also S0t,e = At. (S j t,e)j\u22650 is decreasing for the inclusion of sets and we impose limj\u2192+\u221e \u03b1j,e = 0, such that there is an index j\u2205 with S j\u2205 t,e = \u2205. We introduce another positive sequence (\u03b2j,e)j\u22650 and consider the events that at least m\u03b2j,e arms in At are in the set S j t,e and that the same is false for k < j, i.e. for t \u2265 1, Ajt,e = {|S j t,e| \u2265 m\u03b2j,e;\u2200k <\nj, |Skt,e| < m\u03b2k,e}. To avoid having some of these events being impossible we choose (\u03b2j,e)j\u22650 decreasing. We also impose \u03b20,e = 1, such that |S0t,e| = m\u03b20,e.\nLet then At,e = \u222a+\u221ej=1A j t,e and At = At,1 \u222a At,2. We will show that At must happen for (1) to be true. First, remark that under a condition on (\u03b2j,e)j\u22650, At is a finite union of events,\nLemma 7. For e \u2208 {1, 2}, if there exists j0,e such that \u03b2j0,e,e \u2264 1/m, then At,e = \u222aj0j=1A j t,e.\nWe now show that At is impossible by proving a contradiction in (1).\nLemma 8. Under the event At,1, if there exists j0 such that \u03b2j0,1 \u2264 1/m, then\n\u2211 i\u2208At \u0393(ii) n (i) t < m\u22062t 8f(t)g1(m, \u03b3t)  j0\u2211 j=1 \u03b2j\u22121,1 \u2212 \u03b2j,1 \u03b1j,1 + \u03b2j0,1 \u03b1j0,1  . Under the event At,2, if limj\u2192+\u221e \u03b2j,2/ \u221a \u03b1j,2 = 0 and \u2211+\u221e j=1 \u03b2j\u22121,2\u2212\u03b2j,2\u221a \u03b1j,2\nexists, then\n\u2211 i\u2208At\n\u221a \u0393(ii)\nn (i) t \u2264 m\u2206t\u221a 8f(t)g2(m, \u03b3t) +\u221e\u2211 j=1 \u03b2j\u22121,2 \u2212 \u03b2j,2\u221a \u03b1j,2 .\nA proof can be found in appendix B.2 of the supplementary material. To ensure that the conditions of these lemmas are fulfilled, we impose that (\u03b2i,1)i\u22650 and (\u03b2i,2)i\u22650 have limit 0 and that limj\u2192+\u221e \u03b2j,2/ \u221a \u03b1j,2 = 0. Let j0,1 be the smallest integer such that \u03b2j0,1,1 \u2264 1/m. Let l1 = \u03b2j0,1,1 \u03b1j0,1,1 + \u2211j0,1 j=1 \u03b2j\u22121,1\u2212\u03b2j,1 \u03b1j,1 and\nl2 = \u2211+\u221e j=1 \u03b2j\u22121,2\u2212\u03b2j,2\u221a \u03b1j,2\n. Using the two last lemmas with (1), we get that if At is true,\n\u22062t 8f(t) < \u22062t 8f(t)\n( (\u03bb+ 1\u2212 \u03b3t)\nml1 g1(m, \u03b3t) + \u03b3t m2l22 g2(m, \u03b3t)\n) .\nTaking g1(m, \u03b3t) = 2(\u03bb+ 1\u2212 \u03b3t)ml1 and g2(m, \u03b3t) = 2\u03b3tm2l22, we get a contradiction. Hence with these choices At must happen. The regret bound will be obtained by a union bound on the events that form At. First suppose that all gaps are equal to the same \u2206.\nLemma 9. Let \u03b3 = maxt\u22651 \u03b3t. For j \u2208 N\u2217, the event Ajt,e happens at most d\u03b1j,e8f(T ) maxi{\u0393(ii)}ge(m,\u03b3)\nm\u03b2j,e\u22062 times.\nProof. Each time that Ajt,e happens, the counter of plays n (i) t of at least m\u03b2je arms is incremented. After \u03b1j,e8f(T ) maxi{\u0393 (ii)}ge(m,\u03b3)\n\u22062 increments, an arm cannot verify the condition on n(i)t any more. There are d arms, so the event can happen at most d 1m\u03b2je \u03b1j,e8f(T ) maxi{\u0393(ii)}ge(m,\u03b3) \u22062 times.\nIf all gaps are equal to \u2206, an union bound for At gives\nE[ T\u2211 t=1 \u2206I{Ht \u2229Gt}] \u2264 16 max i\u2208[d] {\u0393(ii)}f(T ) \u2206 d (\u03bb+ 1\u2212 \u03b3)l1 j0,1\u2211 j=1 \u03b1j,1 \u03b2j,1 + \u03b3ml22 +\u221e\u2211 j=1 \u03b1j,2 \u03b2j,2  . The general case requires more involved manipulations but the result is similar and no new important idea is used. The following lemma is proved in appendix B.2 of the supplementary material:\nLemma 10. Let \u03b3(i) = max{t,i\u2208At} \u03b3t. The regret from the event Ht \u2229 Gt is such that E[ T\u2211 t=1 \u2206tI{Ht \u2229Gt}] \u2264 16f(T ) \u2211 i\u2208[d] \u0393(ii) \u2206i,min (\u03bb+ 1\u2212 \u03b3)l1 j0\u2211 j=1 \u03b1j,1 \u03b2j,1 + \u03b3ml22 +\u221e\u2211 j=1 \u03b1j,2 \u03b2j,2  . Finally we can find sequences (\u03b1j,1)j\u22651, (\u03b1j,2)j\u22651, (\u03b2j,1)j\u22650 and (\u03b2j,2)j\u22650 such that E[ T\u2211 t=1 \u2206I{Ht \u2229Gt}] \u2264 16f(T ) \u2211 i\u2208[d] \u0393(ii) \u2206i,min ( 5(\u03bb+ 1\u2212 \u03b3(i)) \u2308 logm 1.6 \u23092 + 45\u03b3(i)m )\nSee appendix C of the supplementary material. In [7], \u03b1i,1 and \u03b2i,1 were such that the log2m term was replaced by \u221a m. Our choice is also applicable to their ESCB algorithm. Our use of geometric sequences is only optimal among sequences such that \u03b2i,1 = \u03b1i,1 for all i \u2265 1. It is unknown to us if one can do better. With this control of the variance term, we finally proved Theorem 2."}, {"heading": "4 Conclusion", "text": "We defined a continuum of settings from the general to the independent arms cases which is suitable for the analysis of semi-bandit algorithms. We exhibited a lower bound scaling with a parameter that quantifies the particular setting in this continuum and proposed an algorithm inspired from linear regression with an upper bound that matches the lower bound up to a log2m term. Finally we showed how to use tools from the linear bandits literature to analyse algorithms for the combinatorial bandit case that are based on linear regression.\nIt would be interesting to estimate the subgaussian covariance matrix online to attain good regret bounds without prior knowledge. Also, our algorithm is not computationally efficient since it requires the computation of an argmax over the actions at each stage. It may be possible to compute this argmax less often and still keep the regret guaranty, as was done in [1] and [7].\nOn a broader scope, the inspiration from linear regression could lead to algorithms using different estimators, adapted to the structure of the problem. For example, the weighted least-square estimator is also unbiased and has smaller variance than OLS. Or one could take advantage of a sparse covariance matrix by using sparse estimators, as was done in the linear bandit case in [4]."}, {"heading": "Acknowledgements", "text": "The authors would like to acknowledge funding from the ANR under grant number ANR-13-JS01-0004 as well as EDF through the Program Gaspard Monge for Optimization And the Irsdi project Tecolere."}, {"heading": "A The subgaussian covariance matrix", "text": "Property 0. An \u03b1-subgaussian variable X verifies Var[X] \u2264 \u03b12.\nLet \u03b7 be a noise in Rd with mean 0 and subgaussian covariance matrix C. The following properties are immediate consequences of property 0 and are presented as a justification for the name subgaussian \"covariance\" matrix.\nProperty 1. For all i \u2208 [d], Var[\u03b7(i)] \u2264 C(ii).\nProperty 2. For all (i, j) \u2208 [d], Var[\u03b7(i)] +Var[\u03b7(j)] + 2Cov[\u03b7(i), \u03b7(j)] \u2264 C(ii) + C(jj) + 2C(ij).\nProperty 3. If Var[\u03b7(i)] = C(ii) and Var[\u03b7(j)] = C(jj), Cov[\u03b7(i), \u03b7(j)] \u2264 C(ij)."}, {"heading": "B Missing proofs", "text": "B.1 Lower bound Proof. We consider the problem where A is a set of d/m disjoint actions A1, . . . , Ad/m and suppose that these actions are independent. This is not more than a bandit problem with d/m arms, each with m sub-arms. We choose a noise \u223c N (0, \u03c32Id) (multivariate normal with mean 0 and covariance matrix Id) and a subgaussian matrix C = (1\u2212 \u03b3)Id + \u03b3Jblocs, where Jblocs is a bloc matrix such that each m\u00d7m bloc indexed by an action Aj contains only ones and other coefficients are 0. The actions are independent and the sub-arms are correlated with correlation \u03b3. Then C1/2 = \u221a 1\u2212 \u03b3Id + 1m ( \u221a 1 + \u03b3(m\u2212 1)\u2212 \u221a 1\u2212 \u03b3)Jblocs. The mean of the best action is taken to be 0 and the mean of other actions is \u2212\u2206. The algorithm has access to all these informations, except for the identity of the optimal action.\nSuppose for notational convenience that the first action is optimal. The reward of the best action is \u03bd1 = \u221a 1 + \u03b3(m\u2212 1) \u2211 i\u2208A1 i, while the reward of\na sub-optimal action Aj is \u03bdj = \u221a 1 + \u03b3(m\u2212 1) \u2211 i\u2208Aj i \u2212\u2206.\nWe use a result from [13] which states that in this bandit case,\nlim inf t\u2192+\u221e Rt log t \u2265 d/m\u2211 j=2\n\u2206\nDKL(\u03bdj , \u03bd1) . (2)\nIn our setting, the Kullback-Leibler divergence is DKL(\u03bdj , \u03bd1) = DKL( \u221a 1 + \u03b3(m\u2212 1) \u2211 i\u2208Aj i \u2212\u2206, \u221a 1 + \u03b3(m\u2212 1) \u2211 i\u2208A1 i)\n= DKL\n( N (\u2212\u2206, \u03c32m(1 + \u03b3(m\u2212 1))),N (0, \u03c32m(1 + \u03b3(m\u2212 1))) ) = \u22062\n2\u03c32m(1 + \u03b3(m\u2212 1)) .\nThis together with (2) proves the theorem.\nB.2 Upper bound Proof. Lemma 1.\nI{Gt,Ht} = I{A\u2217>\u00b5 \u2264 A\u2217>\u00b5\u0302t + Et(A\u2217),Ht} \u2264 I{A\u2217>\u00b5 \u2264 A>t \u00b5\u0302t + Et(At),Ht}\n\u2264 I{A\u2217>\u00b5 \u2264 A>t \u00b5+ \u2206t 2 + Et(At)} = I{\u2206t \u2264 2Et(At)} ,\nwhere we used first that the algorithm chooses At = arg maxA(A>\u00b5\u0302t + Et(A)), then that under Ht we have A>t \u00b5\u0302t \u2264 A>t \u00b5+ \u2206t2 . E[ T\u2211 t=1 \u2206tI{Gt,Ht}] \u2264 T\u2211 t=1 E[\u2206tI{\u2206t \u2264 2Et(At)}] \u2264 2 T\u2211 t=1 E[Et(At)I{\u2206t \u2264 2Et(At)}]\nProof. Lemma 2. T\u2211 t=1 \u2206tP{Ht} \u2264 T\u2211 t=1 \u2211 i\u2208At \u2206tP{Hi,t}\n= T\u2211 t=1 \u2211 i\u2208At \u2206tP{|\u00b5\u0302(i)t \u2212 \u00b5(i)| \u2265 \u2206t 2m }\n= d\u2211 i=1 T\u2211 t=1 \u2206tP{i \u2208 At, |\u00b5\u0302(i)t \u2212 \u00b5(i)| \u2265 \u2206t 2m }\n\u2264 \u2206max d\u2211 i=1 T\u2211 t=1 P{i \u2208 At, |\u00b5\u0302(i)t \u2212 \u00b5(i)| \u2265 \u2206min 2m }\n\u2264 \u2206max d\u2211 i=1 T\u2211 t=1 exp(\u2212 \u2206 2 mint 8m2C(ii) )\n\u2264 8dm 2 maxi{C(ii)}\u2206max\n\u22062min .\nProof. Lemma 4.\nA\u2217>(\u00b5\u2212 \u00b5\u0302t) = \u2212A\u2217>D\u22121t t\u22121\u2211 s=1 IAsC 1/2 s\n= \u2212A\u2217>D\u22121t t\u22121\u2211 s=1 IAs\u2229A\u2217C 1/2 s\n= \u2212A\u2217>D\u22121t (D + t\u22121\u2211 s=1 CAs\u2229A\u2217) 1/2(D + t\u22121\u2211 s=1 CAs\u2229A\u2217) \u22121/2 t\u22121\u2211 s=1 IAs\u2229A\u2217C 1/2 s\n\u2264 \u221a\u221a\u221a\u221aA\u2217>D\u22121t (D + t\u22121\u2211 s=1 CAs\u2229A\u2217)D \u22121 t A \u2217 \u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 s=1 IAs\u2229A\u2217C 1/2 s \u2225\u2225\u2225\u2225\u2225 (D+ \u2211t\u22121 s=1 CAs\u2229A\u2217 ) \u22121\nwhere the last step is the Cauchy-Schwarz inequality. Since IA\u2217D \u03bbIA\u2217\u03a3CDt, A\u2217>(\u00b5\u2212 \u00b5\u0302t) \u2264 \u221a \u03bbA\u2217>\u03a3CD \u22121 t A \u2217 +A\u2217>D\u22121t VtD \u22121 t A \u2217 \u221a 2IVt+D( ) .\nWe proved P { A\u2217>(\u00b5\u2212\u00b5\u0302t)\u2265 \u221a 2f\u0303(\u03b4t)A\u2217>D \u22121 t (\u03bb\u03a3CDt+Vt)D \u22121 t A \u2217 \u2223\u2223\u2223t\u2208Da} \u2264 P{IVt+D( )\u2265f\u0303(\u03b4t)|t\u2208Da} .\nProof. Lemma 5. Let Ft be the \u03c3-algebra \u03c3(A1, 1, . . . , At\u22121, t\u22121, At). Let f(u) be the density of a multivariate normal random variable independent of all other variables with mean 0 and covariance D\u22121. Then from the proof of lemma 9 of [1],\nMt(D) =\n\u221a detD\ndet(D + Vt) exp(IVt+D( ))\n= \u222b Rd exp ( u> t\u22121\u2211 s=1 IAs\u2229A\u2217C 1/2 s \u2212 1 2 u> t\u22121\u2211 s=1 CAs\u2229A\u2217u ) f(u)du\nWe define the following quantities, for u \u2208 Rd,\nMut = exp\n( u>\nt\u22121\u2211 s=1 IAs\u2229A\u2217C 1/2 s \u2212 1 2 u> t\u22121\u2211 s=1 CAs\u2229A\u2217u\n) ,\nDus = exp ( u>IAs\u2229A\u2217C 1/2 s \u2212 1\n2 u>CAs\u2229A\u2217u ) such that Mut = \u220ft\u22121 s=1D u s .\nFrom the subgaussian property of s, E[Dus |Fs] \u2264 1.\nE[Mut |Fs\u22121] = E[ t\u22121\u220f s=1 Dus |Fs\u22121]\n= ( t\u22122\u220f s=1 Dus )E[Dut\u22121|Ft\u22121] \u2264Mut\u22121 .\nThus Mut is a supermartingale and E[Mu1 ] = E[Du1 ] \u2264 1. So for all t, E[Mut ] \u2264 1. Finally, E[Mt(D)] = EuE[Mut |u]] \u2264 1.\nProof. Lemma 6. We have 11+\u03b7 IA\u2217Dt Da IA\u2217Dt. We use D = \u03bb\u03a3CDa + I[d]\\A\u2217 . Then\n\u03bb 1+\u03b7 IA\u2217\u03a3CDt D. The I[d]\\A\u2217 part in D is there only to satisfy the positive definiteness and has no consequence.\nThese matrix inequalities show thatD\u22121/2VtD\u2212 1/2 1+\u03b7\u03bb D \u22121/2 t \u03a3 \u22121/2 C Vt\u03a3 \u22121/2 C D \u22121/2 t ,\nwhich is 1+\u03b7\u03bb times a matrix with m ones and d\u2212m zeros on the diagonal. The determinant of a positive definite matrix is smaller than the product of its diagonal terms, so\ndet(Id +D \u22121/2VtD \u22121/2) \u2264 det(Id + 1 + \u03b7 \u03bb D \u22121/2 t \u03a3 \u22121/2 C Vt\u03a3 \u22121/2 C D \u22121/2 t )\n\u2264 (1 + 1 + \u03b7 \u03bb )m .\nProof. Lemma 7. Let j0 such that \u03b2j0,e \u2264 1/m. Then for all j > j0,\nAjt,e = {|S j t,e| \u2265 1;\u2200k < j0, |Skt,e| < m\u03b2k,e;\u2200k \u2208 {j0, . . . , j \u2212 1}, |Skt,e| = 0} .\nBut as the sequence of sets (Sjt,e)j is decreasing, {|S j0 t,e| = 0} and {|S j t,e| \u2265 1} cannot happen simultaneously. Ajt,e cannot happen for j > j0.\nProof. Lemma 8. First we rewrite At,1, following [12],\nAt,1 = \u2229j0j=1A j t,1 = \u2229j0j=1 [ {|Sjt,1| < m\u03b2j,1} \u222a (\u222a j\u22121 k=1{|S k t,1| \u2265 m\u03b2k,1}) ] = \u2229j0j=1{|S j t,1| < m\u03b2j,1}\n= (\u2229j0\u22121j=1 {|S j t,1| < m\u03b2j,1}) \u2229 {|S j0 t,1| = 0} .\nThe complementary set of Sjt,1 in At is S j t,1 = {i \u2208 At, i /\u2208 S j t,1}. If we have At,1, then S j0 t,1 = At and thus\n\u2211 i\u2208At \u0393(ii) n (i) t = j0\u2211 j=1 \u2211 i\u2208Sjt,1\\S j\u22121 t,1 \u0393(ii) n (i) t\n< j0\u2211 j=1 \u2211 i\u2208Sjt,1\\S j\u22121 t,1 \u22062t 8f(t)g1(m, \u03b3t)\u03b1j,1\n= \u22062t\n8f(t)g1(m, \u03b3t) j0\u2211 j=1 |Sjt,1 \\ S j\u22121 t,1 | \u03b1j,1\n< m\u22062t\n8f(t)g1(m, \u03b3t)  j0\u2211 j=1 \u03b2j\u22121,1 \u2212 \u03b2j,1 \u03b1j,1 + \u03b2j0,1 \u03b1j0,1  , (3) where (3) follows the same steps as lemma 4 of [12].\nProof of the second statement of the lemma: First, follow the same steps as before to get\n\u2211 i\u2208At\n\u221a \u0393(ii)\nn (i) t\n< m\u2206t\u221a\n8f(t)g2(m, \u03b3t)  j0\u2211 j=1 \u03b2j\u22121,2 \u2212 \u03b2j,2\u221a \u03b1j,2 + \u03b2j0,2\u221a \u03b1j0,2  . Then take the limit when j0 \u2192 +\u221e.\nProof. Lemma 10. We break the events Ajt,e into sub-events A j,a t,e = A j t,e \u2229 {a \u2208 At, n (a) t \u2264 \u03b1j,e 8f(t)\u0393(aa)ge(m,\u03b3t)\n\u22062t } that at least m\u03b2j,e arms are not pulled often and that the\narm a is one of them. Since Ajt,e implies that at least \u03b2j,em arms are pulled less than the threshold, we have\nI{Ajt,e} \u2264 1\nm\u03b2j,e d\u2211 a=1 I{Aj,at,e} .\nThe regret that we want to bound is\nT\u2211 t=1 \u2206tI{Ht \u2229Gt} \u2264 2\u2211 e=1 T\u2211 t=1 j0\u2211 j=1 \u2206tI{Ajt,e}\n\u2264 2\u2211 e=1 T\u2211 t=1 j0\u2211 j=1 d\u2211 a=1 \u2206t m\u03b2j,e I{Aj,at,e} .\nEach arm a is contained in Na actions. Let \u2206a,1 \u2265 . . . \u2265 \u2206a,Na be the gaps of these actions and let \u2206a,0 = +\u221e. Then T\u2211 t=1 \u2206tI{Ht \u2229Gt} \u2264 2\u2211 e=1 T\u2211 t=1 j0\u2211 j=1 d\u2211 a=1 Na\u2211 n=1 \u2206a,n m\u03b2j,e I{Aj,at,e ,\u2206t = \u2206a,n}\n\u2264 2\u2211 e=1 T\u2211 t=1 j0\u2211 j=1 d\u2211 a=1 Na\u2211 n=1 \u2206a,n m\u03b2j,e I{a \u2208 At, n(a)t \u2264 \u03b1j,e 8f(t)\u0393(aa)ge(m, \u03b3t) \u22062a,n ,\u2206t = \u2206a,n}\nLet \u03b8j,e,a = \u03b1j,e8f(T )\u0393(aa)ge(m, \u03b3). In the following equations, changes between successive lines are highlighted in blue.\nT\u2211 t=1 Na\u2211 n=1 \u2206a,nI{a \u2208 At, n(a)t \u2264 \u03b1j,e 8f(t)\u0393(aa)ge(m, \u03b3t) \u22062a,n ,\u2206t = \u2206a,n}\n\u2264 T\u2211 t=1 Na\u2211 n=1 \u2206a,nI{a \u2208 At, n(a)t \u2264 \u03b8j,e,a \u22062a,n ,\u2206t = \u2206a,n}\n= T\u2211 t=1 Na\u2211 n=1 n\u2211 p=1 \u2206a,nI{a \u2208 At, n(a)t \u2208 ( \u03b8j,e,a \u22062a,p\u22121 , \u03b8j,e,a \u22062a,p ],\u2206t = \u2206a,n} \u2264 T\u2211 t=1 Na\u2211 n=1 n\u2211 p=1 \u2206a,pI{a \u2208 At, n(a)t \u2208 ( \u03b8j,e,a \u22062a,p\u22121 , \u03b8j,e,a \u22062a,p ],\u2206t = \u2206a,n} \u2264 T\u2211 t=1 Na\u2211 n=1 Na\u2211 p=1 \u2206a,pI{a \u2208 At, n(a)t \u2208 ( \u03b8j,e,a \u22062a,p\u22121 , \u03b8j,e,a \u22062a,p ],\u2206t = \u2206a,n} \u2264 T\u2211 t=1 Na\u2211 p=1 \u2206a,pI{a \u2208 At, n(a)t \u2208 ( \u03b8j,e,a \u22062a,p\u22121 , \u03b8j,e,a \u22062a,p ],\u2206t > 0}\n\u2264 \u03b8j,e,a \u2206a,1 + Na\u2211 p=2 \u03b8j,e,a\u2206a,p( 1 \u22062a,p \u2212 1 \u22062a,p\u22121 )\n\u2264 \u03b8j,e,a \u2206a,1 + \u03b8j,e,a \u222b \u2206a,1 \u2206a,Na x\u22122dx = \u03b8j,e,a \u2206a,Na = \u03b8j,e,a \u2206a,min .\nT\u2211 t=1 \u2206tI{Ht \u2229Gt} \u2264 2\u2211 e=1 \u2211 a\u2208[d] j0\u2211 j=1 \u03b8j,e,a m\u03b2j,e\u2206a,min\n= 8f(T ) \u2211 a\u2208[d] \u0393(aa) \u2206a,min 2\u2211 e=1 ge(m, \u03b3) m  j0\u2211 j=1 \u03b1j,e \u03b2j,e  = 16f(T )\n\u2211 a\u2208[d] \u0393(aa) \u2206a,min (\u03bb+ 1\u2212 \u03b3)l1 j0\u2211 j=1 \u03b1j,1 \u03b2j,1 + \u03b3ml22 j0\u2211 j=1 \u03b1j,2 \u03b2j,2  ."}, {"heading": "C Finding the best sequences for the sums indexed by 1.", "text": "The constraints on the four sequences (\u03b1i,1)i\u22651, (\u03b1i,2)i\u22651, (\u03b2i,1)i\u22650 and (\u03b2i,2)i\u22650 are that they must be positive decreasing with limit 0, \u03b20,1 = \u03b20,2 = 1 and limj\u2192+\u221e \u03b2j,2/ \u221a \u03b1j,2 = 0.\nFor i \u2265 1, we take \u03b2i,1 = \u03b1i,1 = \u03b2i with \u03b2 \u2208 (0, 1). Then j0,1 = d logmlog 1/\u03b2 e and l1 \u2211j0,1 j=1 \u03b1j,1 \u03b2j,1 = j20,1(1/\u03b2 \u2212 1) + j0,1 \u2264 j20,1/\u03b2. We take \u03b2i,2 and \u03b1i,2 as in\n[12]: \u03b2i,2 = \u03b2i2 with \u03b22 = 0.236; \u03b1i,2 = ( 1\u2212\u03b22\u221a \u03b1\u2212\u03b22 )2 \u03b1i, with \u03b1 = 0.146. Then\u2211+\u221e\nj=1 \u03b1j,2 \u03b2j,2 \u2264 45 and l2 \u2264 1.\nThe optimal choice for \u03b2 is close to 1/5, value for which we get the wanted regret bound.\nWe now show that the choice of \u03b1j,1 and \u03b2j,1 made previously is close to optimal among the sequences with \u03b1j,1 = \u03b2j,1.\nLemma 11. Suppose that for all j, \u03b1j = \u03b2j. Then the optimal value v of ( \u2211j0 j=1 \u03b1j \u03b2j )( \u03b2j0 \u03b1j0 + \u2211j0 j=1 \u03b2j\u22121\u2212\u03b2j \u03b1j ) is such that\nv \u2265 1.54 log2m .\nProof. We want to minimize j0 \u2211j0 j=1( \u03b2j\u22121 \u03b2j \u2212 1) under the constraint that j0 = min{j : \u03b2j \u2264 1m}. Let hj = \u03b2j\u22121 \u03b2j \u2212 1, then the decreasing constraint on the (\u03b2j) sequence imposes that for all j \u2265 1, hj \u2265 0. Also, \u03b2j0 \u2264 1m implies\u220fj0 j=1 1 hj+1 \u2264 1m .\nWe solve the following minimization problem:\nminimize over j0, (hj): j0(1 + j0\u2211 j=1 hj)\nsuch that: \u2200j \u2265 1, hj \u2265 0 , j0\u220f j=1 (hj + 1) \u2265 m ,\nj0 \u2265 1 .\nFor a fixed j0, the minimum in (hj) is attained for all hj equal to m1/j0 \u2212 1. Then we should minimize j20(m1/j0 \u2212 1) + j0 with respect to j0. We will instead write that j20(m1/j0 \u2212 1) + j0 \u2265 j20(m1/j0 \u2212 1) and find a lower bound with this latter expression.\nLet f(x) = x2(m1/x \u2212 1) for x \u2208 R+. Then with y = xlogm , f(x) = y 2(e1/y \u2212 1) log2m. The function g(y) = y2(e1/y \u2212 1) has a unique minimum and plotting g shows that this minimum x\u2217 is such that f(x\u2217) \u2265 1.54 log2m."}], "references": [{"title": "Improved Algorithms for Linear Stochastic Bandits", "author": ["Yasin Abbasi-Yadkori", "David Pal", "Csaba Szepesvari"], "venue": "Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Regret in online combinatorial optimization", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "G\u00e1bor Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Bandit Theory meets Compressed Sensing for high dimensional Stochastic Linear Bandit", "author": ["Alexandra Carpentier", "R\u00e9mi Munos"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Combinatorial bandits", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Combinatorial multi-armed bandit: General framework and applications", "author": ["Wei Chen", "Yajun Wang", "Yang Yuan"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Combinatorial Bandits Revisited", "author": ["Richard Combes", "M. Sadegh Talebi", "Alexandre Proutiere", "Marc Lelarge"], "venue": "Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Parametric Bandits: The Generalized Linear Case", "author": ["Sarah Filippi", "Olivier Capp\u00e9", "Aur\u00e9lien Garivier", "Csaba Szepesv\u00e1ri"], "venue": "Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations", "author": ["Yi Gai", "Bhaskar Krishnamachari", "Rahul Jain"], "venue": "IEEE/ACM Transactions on Networking,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Informational confidence bounds for self-normalized averages and applications", "author": ["Aur\u00e9lien Garivier"], "venue": "IEEE Information Theory Workshop,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays", "author": ["Junpei Komiyama", "Junya Honda", "Hiroshi Nakagawa"], "venue": "Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Tight regret bounds for stochastic combinatorial semi-bandits", "author": ["Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesvari"], "venue": "Proceedings of the 18th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in applied mathematics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1985}, {"title": "Self-normalized processes: Limit theory and Statistical Applications", "author": ["Victor H Pe\u00f1a", "Tze Leung Lai", "Qi-Man Shao"], "venue": "Springer Science & Business Media,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "In Herbert Robbins Selected Papers,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1985}], "referenceMentions": [{"referenceID": 14, "context": "It then gets a reward from this choice, with the goal of maximizing the cumulative reward [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 1, "context": "We consider here its stochastic combinatorial extension, in which the algorithm chooses at each stage a subset of arms [2, 5, 6, 9].", "startOffset": 119, "endOffset": 131}, {"referenceID": 4, "context": "We consider here its stochastic combinatorial extension, in which the algorithm chooses at each stage a subset of arms [2, 5, 6, 9].", "startOffset": 119, "endOffset": 131}, {"referenceID": 5, "context": "We consider here its stochastic combinatorial extension, in which the algorithm chooses at each stage a subset of arms [2, 5, 6, 9].", "startOffset": 119, "endOffset": 131}, {"referenceID": 8, "context": "We consider here its stochastic combinatorial extension, in which the algorithm chooses at each stage a subset of arms [2, 5, 6, 9].", "startOffset": 119, "endOffset": 131}, {"referenceID": 4, "context": "This setting was already studied [5], most recently in [7, 12], where two different algorithms are used to tackle on one hand the case where the arms have independent rewards and on the other hand the general bounded case.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "This setting was already studied [5], most recently in [7, 12], where two different algorithms are used to tackle on one hand the case where the arms have independent rewards and on the other hand the general bounded case.", "startOffset": 55, "endOffset": 62}, {"referenceID": 11, "context": "This setting was already studied [5], most recently in [7, 12], where two different algorithms are used to tackle on one hand the case where the arms have independent rewards and on the other hand the general bounded case.", "startOffset": 55, "endOffset": 62}, {"referenceID": 10, "context": "Another algorithm for the independent arms case based on Thompson Sampling was introduced in [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 6, "context": "In the case of 1-subgaussian independent rewards, in which C can be chosen diagonal, a known lower bound on the regret appearing in [7] is d \u2206 log T , while [12] proves a dm \u2206 log T lower bound in general.", "startOffset": 132, "endOffset": 135}, {"referenceID": 11, "context": "In the case of 1-subgaussian independent rewards, in which C can be chosen diagonal, a known lower bound on the regret appearing in [7] is d \u2206 log T , while [12] proves a dm \u2206 log T lower bound in general.", "startOffset": 157, "endOffset": 161}, {"referenceID": 12, "context": "The bound is valid for consistent algorithms [13], for which the regret on any problem verifies ERt = o(t) as t\u2192 +\u221e for all a > 0.", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "Then, for any \u2206 > 0, for any consistent algorithm, there is a problem with gaps \u2206, \u03c3-subgaussian arms and correlation coefficients smaller than \u03b3 \u2208 [0, 1] on which the regret is such that", "startOffset": 148, "endOffset": 154}, {"referenceID": 12, "context": "lim inf t\u2192+\u221e ERt log t \u2265 (1 + \u03b3(m\u2212 1)) (d\u2212m) \u2206 This bound is a consequence of the classical result of [13] for multi-armed bandits, applied to the problem of choosing one among d/m paths, each of which has m different successive edges (Figure 1).", "startOffset": 102, "endOffset": 106}, {"referenceID": 7, "context": "The design of our algorithm stems from this observation and is inspired by linear regression in the fixed design setting, similarly to what was done in the stochastic linear bandit literature [16, 8].", "startOffset": 192, "endOffset": 199}, {"referenceID": 12, "context": "We get confidence intervals for the estimates and can use an upper confidence bound strategy [13, 3].", "startOffset": 93, "endOffset": 100}, {"referenceID": 2, "context": "We get confidence intervals for the estimates and can use an upper confidence bound strategy [13, 3].", "startOffset": 93, "endOffset": 100}, {"referenceID": 0, "context": "2 Comparison with other algorithms Previous works supposed that the rewards of the individual arms are in [0, 1], which gives them a 1/2-subgaussian property.", "startOffset": 106, "endOffset": 112}, {"referenceID": 6, "context": "In the independent case, our algorithm is the same as ESCB-2 from [7], up to the parameter \u03bb.", "startOffset": 66, "endOffset": 69}, {"referenceID": 11, "context": "In the general (worst) case, [12] prove an O( log T \u2206 ) upper bound (which is tight) using CombUCB1, a UCB based algorithm introduced in [6] which at stage t uses the exploration term \u221a 1.", "startOffset": 29, "endOffset": 33}, {"referenceID": 5, "context": "In the general (worst) case, [12] prove an O( log T \u2206 ) upper bound (which is tight) using CombUCB1, a UCB based algorithm introduced in [6] which at stage t uses the exploration term \u221a 1.", "startOffset": 137, "endOffset": 140}, {"referenceID": 13, "context": "We show that it is sufficient to control a selfnormalized sum and do it using arguments from [14], or [1] who applied them to linear bandits.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "We show that it is sufficient to control a selfnormalized sum and do it using arguments from [14], or [1] who applied them to linear bandits.", "startOffset": 102, "endOffset": 105}, {"referenceID": 9, "context": "The analysis also involves a peeling argument, as was done in one dimension by [10] to bound a similar quantity.", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "Doing so leads to the following lemma, extracted from the proof of Theorem 1 of [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "Let \u03b3t \u2208 [0, 1] such that for all i, j \u2208 At with i 6= j, \u0393 \u2264 \u03b3t \u221a \u0393(ii)\u0393(jj).", "startOffset": 9, "endOffset": 15}, {"referenceID": 6, "context": "We recognize here the forms of the indexes used in [7] for independent arms (left term) and [12] for general arms (right term).", "startOffset": 51, "endOffset": 54}, {"referenceID": 11, "context": "We recognize here the forms of the indexes used in [7] for independent arms (left term) and [12] for general arms (right term).", "startOffset": 92, "endOffset": 96}, {"referenceID": 6, "context": "In [7], \u03b1i,1 and \u03b2i,1 were such that the logm term was replaced by \u221a m.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "It may be possible to compute this argmax less often and still keep the regret guaranty, as was done in [1] and [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "It may be possible to compute this argmax less often and still keep the regret guaranty, as was done in [1] and [7].", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "Or one could take advantage of a sparse covariance matrix by using sparse estimators, as was done in the linear bandit case in [4].", "startOffset": 127, "endOffset": 130}], "year": 2016, "abstractText": "The combinatorial stochastic semi-bandit problem is an extension of the classical multi-armed bandit problem in which an algorithm pulls more than one arm at each stage and the rewards of all pulled arms are revealed. One difference with the single arm variant is that the dependency structure of the arms is crucial. Previous works on this setting either used a worst-case approach or imposed independence of the arms. We introduce a way to quantify the dependency structure of the problem and design an algorithm that adapts to it. The algorithm is based on linear regression and the analysis develops techniques from the linear bandit literature. By comparing its performance to a new lower bound, we prove that it is optimal, up to a poly-logarithmic factor in the number of pulled arms.", "creator": "LaTeX with hyperref package"}}}