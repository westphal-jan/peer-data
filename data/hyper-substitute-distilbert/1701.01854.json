{"id": "1701.01854", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jan-2017", "title": "Neural Machine Translation on Scarce-Resource Condition: A case-study on Persian-English", "abstract": "neural machine dynamics ( mls ) is us new approach for arabic translation ( mt ), and credited to its success, it has led the attention of professional researchers in the specialty. developing this paper, we demonstrate typing errors on korean - turkish phonetic pairs, to analyze the model and investigate spatial mechanisms of the translations for usage - specific materials, the situation that exists supporting system - centered translation researchers. we adjust the encoding for the persian language and provide the best conclusion and initial parameters through sixth pieces : classification and transliteration. researcher also apply each different task considering whole persian brain which includes to enable for translated third piece the terms versus bleu score. also, we have modified this loss function completely improve the word alignment of the model. this new loss function performs a total of 1. 87 point improvements of terms during bleu score in russian grammatical quality.", "histories": [["v1", "Sat, 7 Jan 2017 16:27:44 GMT  (711kb)", "http://arxiv.org/abs/1701.01854v1", "6 pages, Submitted in ICEE 2017"]], "COMMENTS": "6 pages, Submitted in ICEE 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mohaddeseh bastan", "shahram khadivi", "mohammad mehdi homayounpour"], "accepted": false, "id": "1701.01854"}, "pdf": {"name": "1701.01854.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation on Scarce-Resource Condition: A case-study on Persian-English", "authors": ["Shahram Khadivi", "Mohaddeseh Bastan", "Mohammad Mehdi Homayounpour"], "emails": ["homayoun}@aut.ac.ir"], "sections": [{"heading": null, "text": "* Shahram Khadivi has contributed to this work when he was with Amirkabir University of Technology.\nfor Machine Translation (MT), and due to its success, it has absorbed the attention of many researchers in the field. In this paper, we study NMT model on Persian-English language pairs, to analyze the model and investigate the appropriateness of the model for scarce-resourced scenarios, the situation that exist for Persian-centered translation systems. We adjust the model for the Persian language and find the best parameters and hyper parameters for two tasks: translation and transliteration. We also apply some preprocessing task on the Persian dataset which yields to increase for about one point in terms of BLEU score. Also, we have modified the loss function to enhance the word alignment of the model. This new loss function yields a total of 1.87 point improvements in terms of BLEU score in the translation quality.\nKeywords-component; neural machine translation; cost\nfunction; alignment model; text preprocessing\nI. INTRODUCTION\nNeural Networks are under great consideration. These networks have recently been used in many applications such as speech recognition [1], image processing [2], and natural language processing [3] and achieved remarkable results. Since the introduction of these networks and considerable results in different applications, many researchers in different fields are making use of the neural networks as a solution for their problems. MT which is a subcategory of natural language processing was firstly processed using neural networks by Casta\u00f1o in 1997 [4].\nFor machine translation, these networks have been used for many different language pairs. In this paper, we propose a neural model for Persian translation for the first time. We use Tensorflow MT model [5] which was released by Google in 2015. We improve the base model with a new feature obtained from the statistical model. The new model consists of a new term as a cost function which measures the difference between the alignment obtained from neural model and statistical model. Then this cost is used to improve both accuracy and convergence time for the NMT.\nThe paper is organized as follow. In part II Statistical Machine Translation (SMT) and NMT and the corresponding mathematics are introduced. In part III literature review of NMT is done. In part IV our NMT model is presented. In part V the experiments and the improvements of the new model in\ncomparison with the baselines are discussed. Finally, section VI concludes the paper\nII. STATISTICAL AND NEURAL MACHINE TRANSLATION\nMT is the automation of the translation between human languages [6]. Two of the most successful models for machine translations are SMT and NMT which are discussed in light of the following subsections."}, {"heading": "A. Statistical Machine Translation", "text": "A common SMT model leads to find the target sentence f: y1, y2, \u2026, yT using source side sentence e: x1, x2, \u2026, xS by maximizing the following term [7]:\np(e|f) ~ p(e).p(f|e) \nIn this equation, \ud835\udc5d(\ud835\udc52)is the language model which helps our output to be natural and grammatical, and p(f|e) is the translation model which ensures that e is normally interpreted as f, and not some other thing [8]. Most of the MT systems use log-linear model instead of the pure form, to model more features in the final equation. Then the model will be as follow [8]:\nlog(\ud835\udc5d(\ud835\udc52|\ud835\udc53)) = \u2211 \ud835\udf06\ud835\udc5a\u210e\ud835\udc5a(\ud835\udc52. \ud835\udc53)\n\ud835\udc40\n\ud835\udc5a=1\n+ \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc4d(\ud835\udc52) \nThis equation shows the mth feature of the SMT system with the \u210e\ud835\udc5a symbol and the corresponding weight with the\ud835\udf06\ud835\udc5a. The term Z is a normalization term which is independent from the weights. In Fig. 1 we see an architecture of an SMT. The model searches through different possibilities using its features as shown.\nAlignment is one of the features for MT and the same alignment is used as what described in [10] for estimating parameters of SMT in this paper"}, {"heading": "B. Neural Machine Translation", "text": "Deep neural networks (DNNs) have shown impressive results in machine learning tasks. The success of these networks mostly is the result of the hierarchical aspects of these networks. DNNs are like pipeline processing in which each layer solves part of the issue and the result is fed into the next layer and at the end the last layer generates the output [11]. DNNs are powerful because the ability to perform parallel computations for several steps [12].\nMost of the NMT models consist of two parts including an encoder which encodes the input sequence to a fixed length array, and a decoder which decodes the context vector into the output sequence [13]. Because the task is MT and the source and target sentences may have any lengths, the input and output of the NMT models are variable.\nTo address the following problem, recurrent neural networks (RNNs) are used for machine translation. RNNs are a map from feedforward neural networks into sequences. In each step t, the RNN computes the hidden state from the following equation, where ht is the hidden state at step t and xt is the tth input in a sequence of inputs:\n\u210e\ud835\udc61 = \ud835\udc53(\u210e\ud835\udc61\u22121. \ud835\udc65\ud835\udc61) \nf is an activation function which can be simple as a sigmoid function or complicated as an LSTM [14]. Similarly, the next output symbol is computed using the following equation:\n\ud835\udc66\ud835\udc61 = \ud835\udc54(\u210e\ud835\udc61) \nTherefore, RNNs can easily map a sequence to another sequence. To map a sequence of input words to sequence of output words with different length, the first attempt is done by [13]. In this work, the input sequence is encoded to a fixed length context vector and the output sequence is generated by decoding the context vector. If c is a context vector, the hidden state at the state t is computed using the following equation:\n\u210e\ud835\udc61 = \ud835\udc53(\u210e\ud835\udc61\u22121. \ud835\udc66\ud835\udc61\u22121. \ud835\udc50) \nEach of the encoder and decoder is an RNN and the whole system is trained to maximize the following log-likelihood probability where N is the number of sentences in training set, Yn is the target output corresponding to source input Xn:\nmax( \ud835\udf03\n1 \ud835\udc41 \u2211 log\ud835\udc43\ud835\udf03(\ud835\udc4c \ud835\udc5b|\ud835\udc4b\ud835\udc5b))\n\ud835\udc41\n\ud835\udc5b=1\n \nThe above model works fine for small sentences. But as the length of the sentence increases, the context vector cannot encode all of the source sentences and the performance decreases significantly [15]. So, the context vector is a bottleneck for this model and a vector with fixed length should be revised. Paper [16] proposes a model which does not encode the whole source sentence into a fixed length array. Instead, the input sentence is encoded to a sequence of arrays and a subset of these arrays are selected for decoding. Then, the model can translate the longer sentences easily. In the new model, each conditional probability is defined as follows:\np(\ud835\udc66\ud835\udc56|\ud835\udc661. \ud835\udc662. \u2026 . \ud835\udc66\ud835\udc56\u22121. X) = \ud835\udc54(\ud835\udc66\ud835\udc56\u22121. \ud835\udc60\ud835\udc56 . \ud835\udc50\ud835\udc56) \nWhere yi is the ith word of the output and X is the input sentence, the si is the hidden network state at the ith step and is computed in this way:\n\ud835\udc60\ud835\udc56 = \ud835\udc53(\ud835\udc60\ud835\udc56\u22121. \ud835\udc66\ud835\udc56\u22121. \ud835\udc50\ud835\udc56) \nIn spite of the conventional encoder-decoder method, in this equation for each output yi the probability is conditional on corresponding ci. Each ci is computed as weighted sum of the annotations hi as follows:\n\ud835\udc50\ud835\udc56 =\u2211\ud835\udefc\ud835\udc56\ud835\udc57\u210e\ud835\udc57\n\ud835\udc47\ud835\udc65\n\ud835\udc57=1\n \nIn this equation, Tx is the length of the source sentence and \u03b1ij is the weight for the jth annotation and is computed as follow:\n\ud835\udefc\ud835\udc56\ud835\udc57 = exp(\ud835\udc52\ud835\udc56\ud835\udc57)\n\u2211 exp(\ud835\udc52\ud835\udc56\ud835\udc58) \ud835\udc47\ud835\udc65 \ud835\udc58=1\n \nFinally, eij is the alignment model which shows how the words around the input position j are compliance with the ith output position. The alignment model is a feedforward neural network which is trained simultaneously with the other components of the network. In contrast of the other NMT, the alignment is not a hidden variable here and is computed as a soft alignment [16].\nFor training the model we use Stochastic Gradient Descent [17]. The learning rate is a parameter which controls how large a step should be in the direction of the negative gradient [18]. It is controlled adaptively here. If the improvement in terms of the\nloss function is not seen over last three iterations, the model will decay the learning rate by a specific factor. We take advantage of this soft alignment and use it to train the model faster and also more accurate. We also trained the model for Persian language for the first time and adjusted the parameters and hyper parameters. Finally, we added a feature from statistical model to make the soft alignment more powerful which results in the decrease in convergence time and improves the model. we added another term to the conditional decay which described above. In implementation we are heavily relying on the Tensorflow translation model.\nIII. RELATED WORK\nIn 2003 neural network language models generally introduced in [19]. In machine translation, some researchers used these models for rescoring the translation [20]. For example, [12] used neural networks for rescoring the translation candida sentences and [13] used neural networks for translation scores in phrase table. One of the simplest and most impressive works in NMT is [21] which used neural networks for rescoring the n-best list in an MT system. This improved MT effectively. In 2012, Li proposed an MT model using feedforward neural networks which used an output layer for classification and a short list for rescoring [22].\nFor language model and machine translation, it has been shown that RNNs empirically work better than feedforward neural networks [23]. Most of the RNN translation models are as an encoder-decoder family. encoder-decoder models for MT first used in [24]. They used a convolutional neural network (CNN) to encode the input sentences into an array and then used an RNN for decoding.\nA newer model for encoder-decoder was presented in [25], where the decoder was conditioned on the source sentences. In this work a language model is combined with a topic model and the results show some improvements in rescoring. In [13] another encoder-decoder was introduced which used an LSTM for encoding and decoding. The authors mostly considered on combining their neural network with an SMT model.\nNMT models have two main problems which researchers are trying to solve. First, the ability of the model to translate decreases as the length of sentences increase. Bahdanau used an attention model to address the problem of translating long sentences [16]. Next is the memory issue. As the size of the corpus increases, the accuracy of the translation increases, but the problem of memory usage emerges. In [26] a model was proposed to address this problem. They tried to translate some part of the input sentences like phrase based translation.\nIn [27] a model for scoring phrases was proposed. In this model, a feedforward neural network with input and output of fixed length is used. Devlin [28] proposed an NMT model using feedforward neural network. In his model a language model encoder using neural network and a decoder from MT model combined and used decoder alignment information for the language model to output the most useful words corresponding to the input sentences. This model made a significant improvement in machine translation, but the limitation of the length of the sentences yet remained.\nBidirectional LSTM first proposed by [29] and used for speech recognition task. These networks were used for MT in [30] and created a strong model which used next and previous input words for translation. The idea of guided alignment first proposed in [31]. And our proposed model for using both SMT and NMT alignments is inspired by this paper.\nIV. PERSIAN NEURAL TRANSLATION MODEL\nIn this section we define the issue for Persian translation and the preprocessing needs to be done before feeding the input into the model. Then the proposed model for NMT which uses soft alignment and SMT alignment feature for translation is described."}, {"heading": "C. Data preprocessing", "text": "The Persian language makes MT a difficult task because of its specific characteristics. So the input sentences should be preprocessed and then fed into the NMT model. Here is the list of preprocessing tasks which are done on Persian corpora:\n\u25cf All corpora are changed to have one sentence per line\nending in one of the punctuations: \u2018\u061f\u2019, \u2018.\u2019, or \u2018!\u2019. \u25cf All words are separated with a single space symbol.\n\u25cf All zero-width non-breakings have been removed. For\ninstance, the word \" \u06cc\u0645\u0645\u0633\u06cc\u0648\u0646 \" is changed to \"\u0645\u0633\u06cc\u0648\u0646 \u06cc\u0645\" \u25cf All the adherent words have been tokenized. For\ninstance, the word \"\u0627\u0647\u0646\u0622\" is changed to \"\u0627\u0647 \u0646\u0622\" \u25cf If a word is an adherent with a symbol, punctuation sign\nor other characters, it is disparted.\nAll of these preprocessing tasks, prepare the Persian data to be used for NMT. The first two preprocessing tasks in the above list, are general which should be done for every language pairs and every MT models. The next two are Persian specific. Unlike the SMT, for NMT we use these two preprocessing to distinguish the words. This is a tradeoff between the number of unique words and the length of the sentences. As the problem of the length of the sentence is decreased after using techniques described in [16], we decided to decrease the number of unique words and increase the length of the sentences. This configuration leads to better results. The last one results into disjointing non-related characters. If we do not dispart the word and its adjunct punctuation sign, the system considers them as a whole word, and this is not acceptable for us. Since the NMT system should get them as two distinct words and not one word."}, {"heading": "D. The alignment feature", "text": "One of the properties of the NMT models is that it doesn\u2019t need to define different features and each feature is tuned to maximize the probability function. Indeed, it learns everything via a unique model and translates the source sentence into the target via the trained model. On the other hand, the SMT defines different features and computes the corresponding weights for each of them and tries to maximize the probability function. Because each of them has its own pros. Our model benefits both of these features and tries to increase the accuracy of the alignment model in NMT using alignment model in SMT.\nIn SMT model, we use the GIZA++ [32] tool to align the source and target sentences to each other. This tool uses an EM algorithm to align words in source and target sentences and shows which words of the source sentence is aligned with which word or words of the target sentence. This alignment can be defined as the following matrix:\n[ , ] 1 :\n[ , ] 0\nT S M i j M M i j     \nIf the ith target word is aligned to the jth source word\n\nOtherwise\nHere M is the alignment matrix, S is the size of the source sentence and T is the size of the target sentence. We name this matrix as EM-alignment matrix.\nIn NMT model, we use the soft alignment. As described in part II-B it makes a matrix for each step of the training phase. This matrix is the e matrix in (10). The ith row and jth column of this matrix defines the compliance of the ith target word with the jth source word. So we have another matrix which is the output of each step of the NMT model. We name this matrix as NMTalignment matrix\nWhat we do here is adding a cost function to NMT model consisting of the difference between these two matrices. The fact behind it, is that a fully translated model using EMalignment definitely has a better alignment model than an NMT model which is in the process of training and is not convergent. This helps the NMT model to converge faster and at the same time find the alignment model which fits to the corpora. Here is the term which is added to previous cost function of NMT:\n( ) 2(| | | |) T S T Sf M e T S      \nIn this equation, e is the NMT-alignment matrix and M is the EM-alignment matrix. The | | symbol means the absolute value. Function f, is the summation of all elements of the matrix. The term before the function is for normalizing the summation. And \u03c9 is a weight which defines how important this term is versus default cost function. The higher the \u03c9, the more important the alignment difference is. We arbitrary set this weight to 0.2. This weight seems reasonable, because the cost function itself consists of the difference between the model translation and the target translation and this difference is more important than the alignment difference.\nThe cost function in this model is used for learning rate decay factor. In an NMT at each step we expect the model to decrease the cost function. If after series of iterations, the cost function did not decrease, the learning rate will be changed.\nAdding a new term to cost function helps the model to learn the alignment more accurately. If the NMT-alignment have a wide margin with EM-alignment, the model will be penalized. So it will learn to align the source and target sentences with more attention to EM-alignment. At the end, we expect the model to have an alignment closer to EM-alignment, unless the alignment is in contrast with translation. Since in cost function the translation is weightier, the model will not suffer from\nwrong EM-alignments and it won\u2019t change the correct NMTalignments.\nV. EXPERIMENTS\nIn this section the experiments done for the proposed model with the results and analysis of each experiment are described. First, system configurations, then the datasets and finally, the experiments and results are described."}, {"heading": "E. System configurations", "text": "For our experiment, we use NVIDIA GeForce GTX 780 GPU which increases the speed of processing in comparison with CPU. Also NVIDIA CUDA toolkit v.7 is used specifically for its math libraries and optimization routines. Also we take advantageous of cuDNN library v5 for increasing the training speed. For programming, we used Tensroflow framework v0.10 and made our changes based on MT model proposed by the providers."}, {"heading": "F. Dataset description", "text": "We used two datasets. The first dataset is Verbmobil English-Persian translation dataset which consists of some conversation sentences in English and their translations in Persian. The second dataset is a transliteration dataset. It consists of some separated characters of words in Persian. In this dataset, the sentence means a word with separated characters, and words mean characters. In Table I the information about Training, Development and Test sets of Verbmobil and Transliteration are provided. Sent. Count means the number of sentences for each of the English and Persian datasets. Unique words count is the number of distinct words available in each corpus. This number is for the main dataset without any preprocessing task. In Table II, an example from each dataset is described. One sentence for each dataset is shown in Table II."}, {"heading": "G. Evaluating Measurements", "text": "For evaluating the proposed model, we use different measurements. For translating we use BLEU [33] measurement, which is quick and language independent. This measure is based on precision of n-grams of the translated text in comparison with target reference or references.\nFor transliteration task we use four different measures in addition to BLEU. The first measurement is the accuracy. Accuracy means how many sentences have been transliterated completely without any error in any position of the sentence. The next measurement is WER (Word Error Rate), which counts the number of words transliterated incorrectly. The words should be transliterated exactly in the same order as source words. As we described earlier, words here mean the character. So WER measures the number of characters which has been transliterated into the wrong character.\nThe third measure is PER (Position-independent word Error Rate) [34]. It is the same as WER but ignoring the order of the words. This measure looks at the sentences as bag-of-the-words and does not consider the position of the words. Then it counts the number of words translated incorrectly and are not in the target sentence at all. Finally, the last measure is TER (Translation Error Rate) [35] which counts the number of edits required to change a system output into one of the given translation references."}, {"heading": "H. Experiments and Results", "text": "We evaluate our model by 3 different experiments. First, we find the best configuration for each dataset. The parameters adjusted are number of layers and number of nodes in each layer of RNN. After adjusting the parameters and hyper parameters for each dataset, we evaluate our proposed model. First, using the best adjusted model, the changes to dataset and then the cost function effect is evaluated. The results are shown in Table III through VI. First we describe the results on transliteration task and then for the translation task.\nTable III shows different configurations of the NMT model for transliteration task. As we see by increasing the number of hidden layer nodes, all measurements improve (BLEU and accuracy increase and TER, WER, PER decrease). But stops at a specific configuration, where increasing the number of hidden nodes does not improve the model1. For transliteration task, we do not have any preprocessing step, since all the words are separated by space and there is no punctuation mark or any symbol except the default Persian and English alphabet characters. So the next experiment is adding new cost function to the default cost function. The results are shown in Table IV. Changing cost function improves the model significantly. That is mostly because the previous cost function does not include the EM-alignment and suffers from aligning incorrectly.\nNext experiments are on Verbmobil dataset and translation task. For this dataset first we configure the model for best parameters and hyper parameters. The results are shown in Table V. As we expect, by increasing the number of hidden\n1 Experiments consist more adjustment for the model, the bests are reported.\nnodes the model works better, because the task is translation and the number of unique words are more than the number of unique words in transliteration task, the model responses better by increasing the number of hidden nodes. In Table VI the preprocessing task and cost function are added to the best baseline. As it can be seen due to preprocessing, the BLEU\nmeasure increases for about one unit which shows the importance of preprocessing. The new cost function increases base-line system for about 1.87 units in BLUE measure which shows that the new cost function also works effectively for translation task.\nVI. CONCLUSION\nIn this paper the first NMT system for Persian language\nwhich is trained using scarce data was proposed. The parameters and hyper parameters of the model are adjusted for Persian to English language. Also some preprocessing were tasks introduced which help the Persian model to be translated accurately. Finally, a cost function was added to soft-alignment in neural machine translations. The whole system increased the performance of the base-line system for about 1.87 for translation task and about 0.9 for transliteration task."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "GE. Dahl", "AR. Mohamed", "N. Jaitly"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Multi-column deep neural networks for image classification.", "author": ["D. Ciregan", "U. Meier", "J. Schmidhuber"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Twitter mood predicts the stock market.", "author": ["J. Bollen", "H. Mao", "X. Zeng"], "venue": "Journal of Computational Science", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Machine translation using neural networks and finite-state models", "author": ["MA. Casta\u00f1o", "F. Casacuberta", "E. Vidal"], "venue": "Theoretical and Methodological Issues in Machine Translation, pp. 160-167, Jul. 1997.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C C. Citro"], "venue": "Preliminary White Paper, November 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Readings in machine translation", "author": ["HL S. Nirenburg", "Somers"], "venue": "MIT Press, 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "A statistical approach to machine translation", "author": ["PF. Brown", "J. Cocke", "SA. Pietra", "VJ. Pietra", "F. Jelinek", "JD. Lafferty"], "venue": "Computational linguistics, Vol 16(2), pp. 79-85, Jun. 1990.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1990}, {"title": "Statistical machine translation", "author": ["Y. Al-Onaizan", "J. Curin", "M. Jahr", "K. Knight", "J. Lafferty", "D. Melamed"], "venue": "InFinal Report, JHU Summer Workshop, Vol. 30, 1999.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "The alignment template approach to statistical machine translation", "author": ["FJ. Och", "H. Ney"], "venue": "Computational linguistics, Vol. 30(4), pp. 417-49, Dec. 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["PF. Brown", "VJ. Pietra", "SA. Pietra", "RL. Mercer"], "venue": "Computational linguistics, Vol. 19(2), pp. 263-311, Jun. 1993.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Training and analysing deep recurrent neural networks", "author": ["M. Hermans", "B. Schrauwen"], "venue": "InAdvances in Neural Information Processing Systems, pp. 190-198, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "QV. Le"], "venue": "InAdvances in neural information processing systems, pp. 3104-3112, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing, pp. 1724-34, Jun. 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreite", "J. Schmidhuber"], "venue": "Neural computation, Vol. 9(8), pp. 1735-80, Nov. 1997.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "On the properties of neural machine translation: Encoder\u2013Decoder approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint, Sep 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics, pp. 400-407, Sep. 1951.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1951}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["MD. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, Dec. 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio"], "venue": "The Journal of Machine Learning Research, Vol. 3, pp. 1137-1155, 2003.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Continuous space language models for the IWSLT 2006 task", "author": ["S. Holger", "MR. Costa-Jussa", "J. AR Fonollosa"], "venue": "IWSLT, 2006.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "Presentation at Google, Mountain View, April 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Yvon, \"Continuous space translation models with neural networks.\" Proceedings of the 2012 conference of the north american chapter of the association for computational linguistics: Human language technologies", "author": ["LH. Son", "A. Allauzen", "Fr"], "venue": "Association for Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Comparison of feedforward and recurrent neural network language models.\" Acoustics, Speech and Signal Processing (ICASSP)", "author": ["M. Sundermeyer", "I. Oparin", "JL. Gauvain", "B. Freiberg", "R. Schl\u00fcter", "H. Ney"], "venue": "IEEE International Conference on", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Recurrent Continuous Translation Models", "author": ["N. Kalchbrenner", "B. Phil"], "venue": "EMNLP, Vol. 3, p. 413, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint Language and Translation Modeling with Recurrent Neural Networks", "author": ["M. Auli", "M. Galley", "C. Quirk", "G. Zweig"], "venue": "EMNLP. Vol. 3, pp. 1044-54, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Overcoming the curse of sentence length for neural machine translation using automatic segmentation", "author": ["J. Pouget-Abadie", "D. Bahdanau", "B. van Merri\u00ebnboer", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1257. Sep. 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation", "author": ["H. Schwenk"], "venue": "COLING (Posters), pp. 1071-1080, Dec. 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast and Robust Neural Network Joint Models for Statistical Machine Translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "RM. Schwartz", "J. Makhoul"], "venue": "InACL Vol. 1, pp. 1370-1380, Jun. 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "KK. Paliwal"], "venue": "IEEE Transactions on Signal Processing, Vol. 45, pp. 2673-81, Nov. 1997.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "Translation Modeling with Bidirectional Recurrent Neural Networks", "author": ["M. Sundermeyer", "T. Alkhouli", "J. Wuebker", "H. Ney"], "venue": "InEMNLP 2014, pp. 14-25, Oct. 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Guided Alignment Training for Topic-Aware Neural Machine Translation", "author": ["W. Chen", "E. Matusov", "S. Khadivi", "JT. Peter"], "venue": "arXiv preprint arXiv:1607.01628, Jul 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "WJ. Zhu"], "venue": "InProceedings of the 40th annual meeting on association for computational linguistics, pp. 311-318, Jul. 2002, Association for Computational Linguistics.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Accelerated DP based search for statistical translation", "author": ["C. Tillmann", "S. Vogel", "H. Ney", "A. Zubiaga", "H. Sawaf"], "venue": "InEurospeech, Sep 1997", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["M. Snover", "B. Dorr", "R. Schwartz", "L. Micciulla", "J. Makhoul"], "venue": "InProceedings of association for machine translation in the Americas, Vol. 200, No. 6, pp. 223-31, Aug. 2006.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "These networks have recently been used in many applications such as speech recognition [1], image processing [2], and natural language processing [3] and achieved remarkable results.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "These networks have recently been used in many applications such as speech recognition [1], image processing [2], and natural language processing [3] and achieved remarkable results.", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "These networks have recently been used in many applications such as speech recognition [1], image processing [2], and natural language processing [3] and achieved remarkable results.", "startOffset": 146, "endOffset": 149}, {"referenceID": 3, "context": "MT which is a subcategory of natural language processing was firstly processed using neural networks by Casta\u00f1o in 1997 [4].", "startOffset": 120, "endOffset": 123}, {"referenceID": 4, "context": "We use Tensorflow MT model [5] which was released by Google in 2015.", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "MT is the automation of the translation between human languages [6].", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": ", xS by maximizing the following term [7]:", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": "In this equation, p(e)is the language model which helps our output to be natural and grammatical, and p(f|e) is the translation model which ensures that e is normally interpreted as f, and not some other thing [8].", "startOffset": 210, "endOffset": 213}, {"referenceID": 7, "context": "Then the model will be as follow [8]:", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "Alignment is one of the features for MT and the same alignment is used as what described in [10] for estimating parameters of SMT in this paper", "startOffset": 92, "endOffset": 96}, {"referenceID": 8, "context": "Architecture of Translation approach based on log-linear model [9]", "startOffset": 63, "endOffset": 66}, {"referenceID": 10, "context": "DNNs are like pipeline processing in which each layer solves part of the issue and the result is fed into the next layer and at the end the last layer generates the output [11].", "startOffset": 172, "endOffset": 176}, {"referenceID": 11, "context": "DNNs are powerful because the ability to perform parallel computations for several steps [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "Most of the NMT models consist of two parts including an encoder which encodes the input sequence to a fixed length array, and a decoder which decodes the context vector into the output sequence [13].", "startOffset": 195, "endOffset": 199}, {"referenceID": 13, "context": "f is an activation function which can be simple as a sigmoid function or complicated as an LSTM [14].", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "To map a sequence of input words to sequence of output words with different length, the first attempt is done by [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "But as the length of the sentence increases, the context vector cannot encode all of the source sentences and the performance decreases significantly [15].", "startOffset": 150, "endOffset": 154}, {"referenceID": 15, "context": "Paper [16] proposes a model which does not encode the whole source sentence into a fixed length array.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "In contrast of the other NMT, the alignment is not a hidden variable here and is computed as a soft alignment [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "For training the model we use Stochastic Gradient Descent [17].", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "The learning rate is a parameter which controls how large a step should be in the direction of the negative gradient [18].", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "In 2003 neural network language models generally introduced in [19].", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "In machine translation, some researchers used these models for rescoring the translation [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "For example, [12] used neural networks for rescoring the translation candida sentences and [13] used neural networks for translation scores in phrase table.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "For example, [12] used neural networks for rescoring the translation candida sentences and [13] used neural networks for translation scores in phrase table.", "startOffset": 91, "endOffset": 95}, {"referenceID": 20, "context": "One of the simplest and most impressive works in NMT is [21] which used neural networks for rescoring the n-best list in an MT system.", "startOffset": 56, "endOffset": 60}, {"referenceID": 21, "context": "In 2012, Li proposed an MT model using feedforward neural networks which used an output layer for classification and a short list for rescoring [22].", "startOffset": 144, "endOffset": 148}, {"referenceID": 22, "context": "For language model and machine translation, it has been shown that RNNs empirically work better than feedforward neural networks [23].", "startOffset": 129, "endOffset": 133}, {"referenceID": 23, "context": "encoder-decoder models for MT first used in [24].", "startOffset": 44, "endOffset": 48}, {"referenceID": 24, "context": "A newer model for encoder-decoder was presented in [25], where the decoder was conditioned on the source sentences.", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "In [13] another encoder-decoder was introduced which used an LSTM for encoding and decoding.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Bahdanau used an attention model to address the problem of translating long sentences [16].", "startOffset": 86, "endOffset": 90}, {"referenceID": 25, "context": "In [26] a model was proposed to address this problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "In [27] a model for scoring phrases was proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "Devlin [28] proposed an NMT model using feedforward neural network.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "Bidirectional LSTM first proposed by [29] and used for speech recognition task.", "startOffset": 37, "endOffset": 41}, {"referenceID": 29, "context": "These networks were used for MT in [30] and created a strong model which used next and previous input words for translation.", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "The idea of guided alignment first proposed in [31].", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "As the problem of the length of the sentence is decreased after using techniques described in [16], we decided to decrease the number of unique words and increase the length of the sentences.", "startOffset": 94, "endOffset": 98}, {"referenceID": 31, "context": "For translating we use BLEU [33] measurement, which is quick and language independent.", "startOffset": 28, "endOffset": 32}, {"referenceID": 32, "context": "The third measure is PER (Position-independent word Error Rate) [34].", "startOffset": 64, "endOffset": 68}, {"referenceID": 33, "context": "Finally, the last measure is TER (Translation Error Rate) [35] which counts the number of edits required to change a system output into one of the given translation references.", "startOffset": 58, "endOffset": 62}], "year": 2017, "abstractText": "Neural Machine Translation (NMT) is a new approach for Machine Translation (MT), and due to its success, it has absorbed the attention of many researchers in the field. In this paper, we study NMT model on Persian-English language pairs, to analyze the model and investigate the appropriateness of the model for scarce-resourced scenarios, the situation that exist for Persian-centered translation systems. We adjust the model for the Persian language and find the best parameters and hyper parameters for two tasks: translation and transliteration. We also apply some preprocessing task on the Persian dataset which yields to increase for about one point in terms of BLEU score. Also, we have modified the loss function to enhance the word alignment of the model. This new loss function yields a total of 1.87 point improvements in terms of BLEU score in the translation quality. Keywords-component; neural machine translation; cost function; alignment model; text preprocessing", "creator": "Microsoft\u00ae Word 2016"}}}