{"id": "1701.02490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2017", "title": "Real-Time Bidding by Reinforcement Learning in Display Advertising", "abstract": "the goal of online display ads instead served through real - process bidding ( rtb ) - - - each ad performance impression is killed off | real - action when generated recalls continually being generated from pure user asset. unlike place target example automatically and optimally, it be irrelevant for accurately properly devise a scheduling algorithm. actually bid an ad firm in real - time. most website discussions consider competitive bid decision as potentially critical optimization problem of precisely treating the value of per position assessed or setting random bid price regarding each segment producing ad volume. however, highly naive for instance digital advertisement campaign would simultaneously evaluate during its rehearsal controlling whether optimal ticket runs out. additionally such, its bid valuation strategically correlated alongside the constrained display goal increases overall effectiveness of the strategy ( con. san., the rewards from selected clicks ), which is only adjusted wherever the campaign has completed. nonetheless, it is of great interest to realize improved ongoing bidding technique sequentially seeing that both campaign profits can be dynamically allocated across all the total impressions utilizing the index encompassing both very immediate and dispersed rewards. in this paper, we formulate any bid decision process above explicit random learning problem, where product state window is transformed involving the auction context and predict campaign's investment - time parameters, while an ensemble is the bid needed to set. by modeling the state transition via network feedback, we build global specific decision process framework for learning the optimal control policy to optimize the competition performance past the dynamic real - time bidding environment. furthermore, the scalability results from the projected real - world auction volume and closing time is well handled by state value approximation using neural structures.", "histories": [["v1", "Tue, 10 Jan 2017 09:30:29 GMT  (796kb,D)", "http://arxiv.org/abs/1701.02490v1", "WSDM 2017"], ["v2", "Thu, 12 Jan 2017 01:37:39 GMT  (796kb,D)", "http://arxiv.org/abs/1701.02490v2", "WSDM 2017"]], "COMMENTS": "WSDM 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.GT", "authors": ["han cai", "kan ren", "weinan zhang", "kleanthis malialis", "jun wang", "yong yu", "defeng guo"], "accepted": false, "id": "1701.02490"}, "pdf": {"name": "1701.02490.pdf", "metadata": {"source": "CRF", "title": "Real-Time Bidding by Reinforcement Learning in Display Advertising", "authors": ["\u2021Jun Wang", "\u2020Yong Yu", "Defeng Guo"], "emails": ["hcai@apex.sjtu.edu.cn,", "kren@apex.sjtu.edu.cn,", "wnzhang@apex.sjtu.edu.cn,", "j.wang@cs.ucl.ac.uk", "permissions@acm.org."], "sections": [{"heading": null, "text": "Keywords Bid Optimization, Reinforcement Learning, Display Ads\n\u2217Weinan Zhang is the corresponding author of this paper.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WSDM 2017, February 06-10, 2017, Cambridge, United Kingdom c\u00a9 2017 ACM. ISBN 978-1-4503-4675-7/17/02. . . $15.00\nDOI: http://dx.doi.org/10.1145/3018661.3018702"}, {"heading": "1. INTRODUCTION", "text": "The increased availability of big data and the improved computational power have advanced machine learning and artificial intelligence for various prediction and decision making tasks. In particular, the successful application of reinforcement learning in certain settings such as gaming control [19] has demonstrated that machines not only can predict, but also have a potential of achieving comparable humanlevel control and decision making. In this paper, we study machine bidding in the context of display advertising. Auctions, particularly real-time bidding (RTB), have been a major trading mechanism for online display advertising [30, 7, 23]. Unlike the keyword-level bid decision in sponsored search [1], the advertiser needs to make the impression-level bid decision in RTB, i.e., bidding for every single ad impression in real time when it is just being generated by a user visit [18, 30] . Machine based bid decision, i.e., to calculate the strategic amount that the advertiser would like to pay for an ad opportunity, constitutes a core component that drives the campaigns\u2019 ROI [11, 32]. By calculating an optimal bid price for each ad auction (also considering the remaining budget and the future availability of relevant ad impressions in the ad exchange) and then observing the auction result and user feedback, the advertiser would be able to refine their bidding strategy and better allocate the campaign budget across the online page view volume.\nA straightforward bidding strategy in RTB display advertising is mainly based on the truthfulness of second-price auctions [6], which means the bid price for each ad impression should be equal to its true value, i.e., the action value (e.g., click value) multiplied by the action rate (e.g., clickthrough rate) [14]. However, for budgeted bidding in repeated auctions, the optimal bidding strategy may not be truthful but depends on the market competition, auction volume and campaign budget [31]. In [18, 32], researchers have proposed to seek the optimal bidding function that directly maximizes the campaign\u2019s key performance indicator (KPI), e.g., total clicks or revenue, based on the static distribution of input data and market competition models. Nevertheless, such static bid optimization frameworks may still not work well in practice because the RTB market competition is highly dynamic and it is fairly common that the true data distribution heavily deviates from the assumed one during the model training [4], which requires additional control step such as budget pacing [28] to constrain the budget spending.\nIn this paper, we solve the issue by considering bidding as a sequential decision, and formulate it as a reinforcement\nar X\niv :1\n70 1.\n02 49\n0v 1\n[ cs\n.L G\n] 1\n0 Ja\nn 20\n17\nlearning to bid (RLB) problem. From an advertiser\u2019s perspective, the whole ad market and Internet users are regarded as the environment. At each timestep, the advertiser\u2019s bidding agent observes a state, which consists of the campaign\u2019s current parameters, such as the remaining lifetime and budget, and the bid request for a specific ad impression (containing the information about the underlying user and their context) . With such state (and context), the bidding agent makes a bid action for its ad. After the ad auction, the winning results with the cost and the corresponding user feedback will be sent back to the bidding agent, which forms the reward signal of the bidding action. Thus, the bid decision aims to derive an optimal bidding policy for each given bid request.\nWith the above settings, we build a Markov Decision Process (MDP) framework for learning the optimal bidding policy to optimize the advertising performance. The value of each state will be calculated by performing dynamic programming. Furthermore, to handle the scalability problem for the real-world auction volume and campaign budget, we propose to leverage a neural network model to approximate the value function. Besides directly generalizing the neural network value function, we also propose a novel coarse-tofine episode segmentation model and state mapping models to overcome the large-scale state generalization problem.\nIn our empirical study, the proposed solution has achieved 16.7% and 7.4% performance gains against the state-of-theart methods on two large-scale real-world datasets. In addition, our proposed system has been deployed into a commercial RTB platform. We have performed an online A/B testing, where a 44.7% improvement in click performance was observed against a most widely used method in the industry."}, {"heading": "2. BACKGROUND AND RELATED WORK", "text": "Reinforcement Learning. An MDP provides a mathematical framework which is widely used for modelling the dynamics of an environment under different actions, and is useful for solving reinforcement learning problems [21]. An MDP is defined by the tuple \u3008S,A, P,R\u3009. The set of all states and actions are represented by S and A respectively. The reward and transition probability functions are given by R and P . Dynamic programming is used in cases where the environment\u2019s dynamics, i.e., the reward function and transition probabilities are known in advance. Two popular dynamic programming algorithms are policy iteration and value iteration. For large-scale situations, it is difficult to experience the whole state space, which leads to the use of function approximation that constructs an approximator of the entire function [8, 22]. In this work, we use value iteration for small-scale situations, and further build a neural network approximator to solve the scalability problem.\nRTB Strategy. In the RTB process [32], the advertiser receives the bid request of an ad impression with its real-time information and the very first thing to do is to estimate the utility, i.e., the user\u2019s response on the ad if winning the auction. The distribution of the cost, i.e., the market price [1, 5], which is the highest bid price from other competitors, is also forecasted by the bid landscape forecasting component. Utility estimation and bid landscape forecasting are described below. Given the estimated utility and cost factors, the bidding strategy [30] decides the final bid price with\naccessing the information of the remaining budget and auction volume. Thus it is crucial to optimize the final bidding strategy considering the market and bid request information with budget constraints. A recent comprehensive study on the data science of RTB display advertising is posted in [24].\nUtility Estimation. For advertisers, the utility is usually defined based on the user response, i.e., click or conversion, and can be modeled as a probability estimation task [14]. Much work has been proposed for user response prediction, e.g., click-through rate (CTR) [16], conversion rate (CVR) [14] and post-click conversion delay patterns [3]. For modeling, linear models such as logistic regression [14] and nonlinear models such as boosted trees [10] and factorization machines [17] are widely used in practice. There are also online learning models that immediately perform updating when observing each data instance, such as Bayesian probit regression [9], FTRL learning in logistic regression [16]. In our paper, we follow [14, 32] and adopt the most widely used logistic regression for utility estimation to model the reward on agent actions.\nBid Landscape Forecasting. Bid landscape forecasting refers to modeling the market price distribution for auctions of specific ad inventory, and its c.d.f. is the winning probability given each specific bid price [5]. The authors in [15, 32, 5] presented some hypothetical winning functions and learned the parameters. For example, a log-normal market price distribution with the parameters estimated by gradient boosting decision trees was proposed in [5]. Since advertisers only observe the winning impressions, the problem of censored data [1, 33] is critical. Authors in [27] proposed to leverage censored linear regression to jointly model the likelihood of observed market prices in winning cases and censored ones with losing bids. Recently, the authors in [25] proposed to combine survival analysis and decision tree models, where each tree leaf maintains a non-parametric survival model to fit the censored market prices. In this paper, we follow [1, 33] and use a non-parametric method to model the market price distribution.\nBid Optimization. As has been discussed above, bidding strategy optimization is the key component within the decision process for the advertisers [18]. The auction theory [12] proved that truthful bidding is the optimal strategy under the second-price auction. However, truthful bidding may perform poorly when considering the multiple auctions and the budget constraint [32]. In real-world applications, the linear bidding function [18] is widely used. The authors in [32] empirically showed that there existed non-linear bidding functions better than the linear ones under variant budget constraints. When the data changes, however, the heuristic model [18] or hypothetical bidding functions [31, 32] cannot depict well the real data distribution. The authors in [1, 29] proposed the model-based MDPs to derive the optimal policy for bidding in sponsored search or ad selection in contextual advertising, where the decision is made on keyword level. In our work, we investigate the most challenging impression-level bid decision problem in RTB display advertising that is totally different from [1, 29]. We also tackle the scalability problem, which remains unsolved in [1], and demonstrate the efficiency and effectiveness of our method in a variety of experiments."}, {"heading": "3. PROBLEM AND FORMULATION", "text": "In a RTB ad auction, each bidding agent acts on behalf of its advertiser and generates bids to achieve the campaign\u2019s specific target. Our main goal is to derive the optimal bidding policy in a reinforcement learning fashion. For most performance-driven campaigns, the optimization target is to maximize the user responses on the displayed ads if the bid leads to auction winning. Without loss of generality, we consider clicks as our targeted user response objective, while other KPIs can be adopted similarly. The diagram of interactions between the bidding agent and the environment is shown in Figure 1."}, {"heading": "3.1 Problem Definition", "text": "Mathematically, we consider bidding in display advertising as an episodic process [1]; each episode comprises T auctions which are sequentially sent to the bidding agent. Each auction is represented by a high dimensional feature vector x, which is indexed via one-hot binary encoding. Each entry of x corresponds to a category in a field, such as the category London in the field City, and the category Friday in the field Weekday. The fields consist of the campaign\u2019s ad information (e.g., ad creative ID and campaign ID) and the auctioned impression contextual information (e.g., user cookie ID, location, time, publisher domain and URL).\nAt the beginning, the agent is initialized with a budget B, and the advertising target is set to acquire as many clicks as possible during the following T auctions. Three main pieces of information are considered by the agent (i) the remaining auction number t \u2208 {0, \u00b7 \u00b7 \u00b7 , T}; (ii) the unspent budget b \u2208 {0, \u00b7 \u00b7 \u00b7 , B} and (iii) the feature vector x. During the episode, each auction will be sent to the agent sequentially and for each of them the agent needs to decide the bid price according to the current information t, b and x.\nThe agent maintains the remaining number of auctions t and the remaining budget b. At each timestep, the agent receives an auction x \u2208 X (the feature vector space), and determines its bid price a. We denote the market price p.d.f. given x as m(\u03b4,x), where \u03b4 is the market price and m is its probability. If the agent bids at price a \u2265 \u03b4, then it wins the auction and pays \u03b4, and the remaining budget changes to b \u2212 \u03b4. In this case, the agent can observe the user response and the market price later. Alternatively, if losing, the agent gets nothing from the auction. We take predicted CTR (pCTR) \u03b8(x) as the expected reward, to model the action utility. After each auction, the remaining number of auctions changes to t \u2212 1. When the auction flow of this episode runs out, the current episode ends and both the remaining auction number and budget are reset."}, {"heading": "3.2 MDP Formulation of RTB", "text": "A Markov Decision Process (MDP) provides a framework that is widely used for modeling agent-environment interactions [21]. Our notations are listed in Table 1. An MDP can\nbe represented by a tuple (S, {As}, {P ass\u2032}, {Rass\u2032}), where S and As are two sets of all states and all possible actions in state s \u2208 S, respectively. P ass\u2032 represents the state transition probability from state s \u2208 S to another state s\u2032 \u2208 S when taking action a \u2208 As, which is denoted by \u00b5(a, s, s\u2032). Similarly, Rass\u2032 is the reward function denoted by r(a, s, s\n\u2032) that represents the reward received after taking action a in state s and then transiting to state s\u2032.\nWe consider (t, b,xt) as a state s 1 and assume the feature vector xt is drawn i.i.d. from the probability density function px(x). The full state space is S = {0, \u00b7 \u00b7 \u00b7 , T} \u00d7 {0, \u00b7 \u00b7 \u00b7 , B} \u00d7X. And if t = 0, the state is regarded as a terminal state which means the end of the episode. The set of all actions available in state (t, b,xt) is A(t,b,xt) = {0, \u00b7 \u00b7 \u00b7 , b}, corresponding to the bid price. Furthermore, in state (t, b,xt) where t > 0, the agent, when bidding a, can transit to (t\u22121, b\u2212\u03b4,xt\u22121) with probability px(xt\u22121)m(\u03b4,xt) where \u03b4 \u2208 {0, \u00b7 \u00b7 \u00b7 , a} and xt\u22121 \u2208 X. That is the case of winning the auction and receiving a reward \u03b8(xt). And the agent may lose the auction whereafter transit to (t \u2212 1, b,xt\u22121) with probability px(xt\u22121) \u2211\u221e \u03b4=a+1m(\u03b4,xt), where xt\u22121 \u2208 X. All other transitions are impossible because of the auction process. In summary, transition probabilities and reward function can be written as:\n\u00b5 ( a, (t, b,xt), (t\u2212 1, b\u2212 \u03b4,xt\u22121) ) = px(xt\u22121)m(\u03b4,xt),\n\u00b5 ( a, (t, b,xt), (t\u2212 1, b,xt\u22121) ) = px(xt\u22121) \u221e\u2211 \u03b4=a+1 m(\u03b4,xt),\nr ( a, (t, b,xt), (t\u2212 1, b\u2212 \u03b4,xt\u22121) ) = \u03b8(xt),\nr ( a, (t, b,xt), (t\u2212 1, b,xt\u22121) ) = 0, (1)\nwhere \u03b4 \u2208 {0, \u00b7 \u00b7 \u00b7 , a}, xt\u22121 \u2208X and t > 0. Specifically, the first equation is the transition when giving a bid price a \u2265 \u03b4, while the second equation is the transition when losing the auction.\nA deterministic policy \u03c0 is a mapping from each state s \u2208 S to action a \u2208 As, i.e., a = \u03c0(s), which corresponds to the bidding strategy in RTB display advertising. According to the policy \u03c0, we have the value function V \u03c0(s): the expected sum of rewards upon starting in state s and obeying policy \u03c0. This satisfies the Bellman equation with the discount factor \u03b3 = 1 since in our scenario the total click number is the optimization target, regardless of the click time.\nV \u03c0(s) = \u2211 s\u2032\u2208S \u00b5(\u03c0(s), s, s\u2032) ( r(\u03c0(s), s, s\u2032) + V \u03c0(s\u2032) ) (2)\nThe optimal value function is defined as V \u2217(s) = max\u03c0 V \u03c0(s).\n1For simplicity, we slightly abuse the notation by including t in the state.\nWe also have the optimal policy as:\n\u03c0\u2217(s) = argmax a\u2208As { \u2211 s\u2032\u2208S \u00b5(a, s, s\u2032) ( r(a, s, s\u2032) + V \u2217(s\u2032) )} , (3)\nwhich gives the optimal action at each state s and V \u2217(s) = V \u03c0 \u2217 (s). The optimal policy \u03c0\u2217(s) is exactly the optimal bidding strategy we want to find. For notation simplicity, in later sections, we use V (s) to represent the optimal value function V \u2217(s).\nOne may consider the possibility of model-free approaches [26, 20] to directly learn the bidding policy from experience. However, such model-free approaches may suffer from the problems of transition dynamics of the enormous state space, the sparsity of the reward signals and the highly stochastic environment. Since there are many previous works on modeling the utility (reward) and the market price distribution (transition probability) as discussed in Section 2, we take advantage of them and propose our model-based solution for this problem."}, {"heading": "4. DYNAMIC PROGRAMMING SOLUTION", "text": "In a small scale, Eq. (3) can be solved using a dynamic programming approach. As defined, we have the optimal value function V (t, b,x), where (t, b,x) represents the state s. Meanwhile, we consider the situations where we do not observe the feature vector x; so another optimal value function is V (t, b): the expected total reward upon starting in (t, b) without observing the feature vector x when the agent takes the optimal policy. It satisfies V (t, b) = \u222b X px(x) V (t, b,x) dx. Also that, we have the optimal policy \u03c0\u2217 and express it as the optimal action a(t, b,x).\nFrom the definition, we have V (0, b,x) = V (0, b) = 0 as the agent gets nothing when there are no remaining auctions. Combined with the transition probability and reward function described in Eq. (1), the definition of V (t, b), V (t, b,x) can be expressed with V (t\u2212 1, \u00b7) as\nV (t, b,x) = max 0\u2264a\u2264b { a\u2211 \u03b4=0 \u222b X m(\u03b4,x)px(xt\u22121) \u00b7(\n\u03b8(x) + V (t\u2212 1, b\u2212 \u03b4,xt\u22121) ) dxt\u22121 +\n\u221e\u2211 \u03b4=a+1 \u222b X m(\u03b4,x)px(xt\u22121)V (t\u2212 1, b,xt\u22121) dxt\u22121 }\n= max 0\u2264a\u2264b { a\u2211 \u03b4=0 m(\u03b4,x) ( \u03b8(x) + V (t\u2212 1, b\u2212 \u03b4) ) +\n\u221e\u2211 \u03b4=a+1 m(\u03b4,x)V (t\u2212 1, b) } , (4)\nwhere the first summation2 is for the situation when winning the auction and the second summation is that when losing. Similarly, the optimal action in state (t, b,x) is\na(t, b,x) = argmax 0\u2264a\u2264b { a\u2211 \u03b4=0 m(\u03b4,x) ( \u03b8(x) + V (t\u2212 1, b\u2212 \u03b4) ) +\n\u221e\u2211 \u03b4=a+1 m(\u03b4,x)V (t\u2212 1, b) } , (5)\n2In practice, the bid prices in various RTB ad auctions are required to be integer.\nwhere the optimal bid action a(t, b,x) involves three terms: m(\u03b4,x), \u03b8(x) and V (t\u2212 1, \u00b7). V (t, b) is derived by marginalizing out x:\nV (t, b) = \u222b X px(x) max 0\u2264a\u2264b { a\u2211 \u03b4=0 m(\u03b4,x) ( \u03b8(x) + V (t\u2212 1, b\u2212 \u03b4) )\n+ \u221e\u2211\n\u03b4=a+1\nm(\u03b4,x)V (t\u2212 1, b) } dx\n= max 0\u2264a\u2264b { a\u2211 \u03b4=0 \u222b X px(x)m(\u03b4,x)\u03b8(x) dx+ a\u2211 \u03b4=0 V (t\u2212 1, b\u2212 \u03b4) \u00b7\n\u222b X px(x)m(\u03b4,x) dx+ V (t\u2212 1, b) \u221e\u2211 \u03b4=a+1 \u222b X px(x)m(\u03b4,x) dx }\n= max 0\u2264a\u2264b { a\u2211 \u03b4=0 \u222b X px(x)m(\u03b4,x)\u03b8(x) dx + (6)\na\u2211 \u03b4=0 m(\u03b4)V (t\u2212 1, b\u2212 \u03b4) + V (t\u2212 1, b) \u221e\u2211 \u03b4=a+1 m(\u03b4) } .\nTo settle the integration over x in Eq. (6), we consider an approximation m(\u03b4,x) \u2248 m(\u03b4) by following the dependency assumption x\u2192 \u03b8 \u2192 a\u2192 w(winning rate) in [32]. Thus\u222b\nX px(x)m(\u03b4,x)\u03b8(x) dx \u2248 m(\u03b4) \u222b X px(x)\u03b8(x) dx\n= m(\u03b4)\u03b8avg , (7)\nwhere \u03b8avg is the expectation of the pCTR \u03b8, which can be easily calculated with historical data. Taking Eq. (7) into Eq. (6), we get an approximation of the optimal value function V (t, b):\nV (t, b) \u2248 max 0\u2264a\u2264b { a\u2211 \u03b4=0 m(\u03b4)\u03b8avg + a\u2211 \u03b4=0 m(\u03b4)V (t\u2212 1, b\u2212 \u03b4) +\n\u221e\u2211 \u03b4=a+1 m(\u03b4)V (t\u2212 1, b) } . (8)\nNoticing that \u2211\u221e \u03b4=0m(\u03b4,x) = 1, Eq. (5) is rewritten as\na(t, b,x) = argmax 0\u2264a\u2264b { a\u2211 \u03b4=0 m(\u03b4,x) ( \u03b8(x) + V (t\u2212 1, b\u2212 \u03b4) ) \u2212\na\u2211 \u03b4=0 m(\u03b4,x)V (t\u2212 1, b) }\n= argmax 0\u2264a\u2264b { a\u2211 \u03b4=0 m(\u03b4,x) ( \u03b8(x) + V (t\u2212 1, b\u2212 \u03b4)\u2212 V (t\u2212 1, b) )}\n\u2261 argmax 0\u2264a\u2264b { a\u2211 \u03b4=0 m(\u03b4,x)g(\u03b4) } , (9)\nwhere we denote g(\u03b4) = \u03b8(x) + V (t\u2212 1, b\u2212 \u03b4)\u2212 V (t\u2212 1, b). From the definition, we know V (t \u2212 1, b) monotonically increases w.r.t. b, i.e., V (t \u2212 1, b) \u2265 V (t \u2212 1, b\u2032) where b \u2265 b\u2032. As such, V (t \u2212 1, b \u2212 \u03b4) monotonically decreases w.r.t. \u03b4. Thus g(\u03b4) monotonically decreases w.r.t. \u03b4. Moreover, g(0) = \u03b8(x) \u2265 0 and m(\u03b4,x) \u2265 0. Here, we care about the value of g(b). (i) If g(b) \u2265 0, then g(b\u2032) \u2265 g(b) \u2265 0 where 0 \u2264 b\u2032 \u2264 b, so m(\u03b4,x)g(\u03b4) \u2265 0 where 0 \u2264 \u03b4 \u2264 b. As a result, in this case, we have a(t, b,x) = b. (ii) If g(b) < 0, then there must exist an integer A such that 0 \u2264 A < b and g(A) \u2265 0, g(A+1) < 0. So m(\u03b4,x)g(\u03b4) \u2265 0 when \u03b4 \u2264 A and\nAlgorithm 1 Reinforcement Learning to Bid\nInput: p.d.f. of market price m(\u03b4), average CTR \u03b8avg, episode length T , budget B Output: value function V (t, b) 1: initialize V (0, b) = 0 2: for t = 1, 2, \u00b7 \u00b7 \u00b7 , T \u2212 1 do 3: for b = 0, 1, \u00b7 \u00b7 \u00b7 , B do 4: enumerate at,b from 0 to min(\u03b4max, b) and set V (t, b) via Eq. (8) 5: end for 6: end for Input: CTR estimator \u03b8(x), value function V (t, b), current state\n(tc, bc,xc) Output: optimal bid price ac in current state 1: calculate the pCTR for the current bid request: \u03b8c = \u03b8(xc) 2: for \u03b4 = 0, 1, \u00b7 \u00b7 \u00b7 ,min(\u03b4max, bc) do 3: if \u03b8c + V (tc \u2212 1, bc \u2212 \u03b4)\u2212 V (tc \u2212 1, bc) \u2265 0 then 4: ac \u2190 \u03b4 5: end if 6: end for\nm(\u03b4,x)g(\u03b4) < 0 when \u03b4 > A. Consequently, in this case, we have a(t, b,x) = A. In conclusion, we have\na(t, b,x) = { b if g(b) \u2265 0 A g(A) \u2265 0 and g(A+ 1) < 0 if g(b) < 0 . (10)\nFigure 2 shows examples of g(\u03b4) on campaign 2821 from iPinYou real-world dataset. Additionally, we usually have a maximum market price \u03b4max, which is also the maximum bid price. The corresponding RLB algorithm is shown in Algorithm 1.\nDiscussion on Derived Policy. Contrary to the linear bidding strategies which bids linearly w.r.t. the pCTR with a static parameter [18], such as Mcpc and Lin discussed in Section 5.3, our derived policy (denoted as RLB) adjusts its bidding function according to current t and b. As shown in Figure 3, RLB also introduces a linear form bidding function when b is large, but decreases the slope w.r.t. decreasing b and increasing t. When b is small (such as b < 300), RLB introduces a non-linear concave form bidding function.\nDiscussion on the Approximation of V (t, b). In Eq. (7), we take the approximation m(\u03b4,x) \u2248 m(\u03b4) by following the dependency assumption x\u2192 \u03b8 \u2192 a\u2192 w(winning rate) in [32] and consequently get an approximation of the optimal value function V (t, b) in Eq. (8). Here, we consider a more general case where such assumption does not hold in the whole feature vector space X, but holds within each individual subset. Suppose X can be explicitly divided into several segments, i.e., X = tiXi. The segmentation can be built by publisher, user demographics etc. For each segment Xi, we take the approximation m(\u03b4,x) \u2248 mi(\u03b4) where\nx \u2208Xi. As such, we have \u222b X px(x)m(\u03b4,x)\u03b8(x) dx = \u2211 i \u222b Xi px(x)m(\u03b4,x)\u03b8(x) dx\n\u2248 \u2211 i mi(\u03b4) \u222b Xi px(x)\u03b8(x) dx = \u2211 i mi(\u03b4)(\u03b8avg)iP (x \u2208Xi)."}, {"heading": "4.1 Handling Large-Scale Issues", "text": "Algorithm 1 gives a solution to the optimal policy. However, when it comes to the real-world scale, we should also consider the complexity of the algorithm. Algorithm 1 consists of two stages. The first one is about updating the value function V (t, b), while the second stage is about taking the optimal action for current state based on V (t, b). We can see that the main complexity is on the first stage. Thus we focus on the first stage in this section. Two nested loops in the first stage lead the time complexity to O(TB). As for the space complexity, we need to use a two-dimensional table to store V (t, b), which will later be used when taking action. Thus the space complexity is O(TB).\nIn consideration of the space complexity and the time complexity, Algorithm 1 can only be applied to small-scale situations. When we confront the situation where T and B are very large, which is a common case in real world, there will probably be not enough resource to get the exact value of V (t, b) for every (t, b) \u2208 {0, \u00b7 \u00b7 \u00b7 , T} \u00d7 {0, \u00b7 \u00b7 \u00b7 , B}.\nWith restricted computational resources, one may not be able to go through the whole value function update. Thus we propose some parameterized models to fit the value function on small data scale, i.e., {0, \u00b7 \u00b7 \u00b7 , T0}\u00d7{0, \u00b7 \u00b7 \u00b7 , B0}, and generalize to the large data scale {0, \u00b7 \u00b7 \u00b7 , T} \u00d7 {0, \u00b7 \u00b7 \u00b7 , B}.\nGood parameterized models are supposed to have low deviation to the exact value of V (t, b) for every (t, b) \u2208 {0, \u00b7 \u00b7 \u00b7 , T}\u00d7{0, \u00b7 \u00b7 \u00b7 , B}. That means low root mean square error (RMSE) in the training data and good generalization ability.\nBasically, we expect the prediction error of \u03b8(x) + V (t \u2212 1, b\u2212 \u03b4)\u2212V (t\u2212 1, b) from Eq. (9) in the training data to be low in comparison to the average CTR \u03b8avg. For most (t, b), V (t, b) is much larger than \u03b8avg. For example, if the budget b is large enough, V (t, b) is with the same scale of t \u00d7 \u03b8avg. Therefore, if we take V (t, b) as our target to approximate, it is difficult to give a low deviation in comparison to \u03b8avg. Actually, when calculating a(t, b,x) in Eq. (9), we care about the value of V (t\u22121, b\u2212\u03b4)\u2212V (t\u22121, b) rather than V (t\u22121, b\u2212 \u03b4) or V (t\u2212 1, b). Thus here we introduce a new function of value differential D(t, b) = V (t, b+1)\u2212V (t, b) to replace the\nrole of V (t, b) by\nV (t\u2212 1, b\u2212 \u03b4)\u2212 V (t\u2212 1, b) = \u2212 \u03b4\u2211\n\u03b4\u2032=1\nD(t\u2212 1, b\u2212 \u03b4\u2032). (11)\nFigure 4 illustrates the value of D(t, b) and V (t, b) on the data of an example campaign. In Figure 5, we use the campaign 3386 from iPinYou real-world dataset as an example and show some interesting observations of D(t, b) (other campaigns are similar). At first, for a given t, we consider D(t, b) as a function of b and denote it asDt(b). Dt(b) fluctuates heavily when b is very small, and later keeps decreasing to 0. Similarly, for a given b, we have Db(t) as a function of t and it keeps increasing. Moreover, both Dt(b) and Db(t) are obviously nonlinear. Consequently, we apply the neural networks to approximate them for large-scale b and t.\nAs a widely used solution [2, 21], here we take a fully connected neural network with several hidden layers as a nonlinear approximator. The input layer has two nodes for t and b. The output layer has one node for D(t, b) without activation function. As such, the neural network corresponds to a non-linear function of t and b, denoted as NN(t, b).\nCoarse-to-fine Episode Segmentation Model. Since the neural networks do not guarantee good generalization ability and may suffer from overfitting, and also to avoid directly modeling D(t, b) or V (t, b), we explore the feasibility of mapping unseen states (t > T0 and b > B0) to acquainted states (t \u2264 T0 and b \u2264 B0) rather than giving a global parameterized representation. Similar to budget pacing, we have the first simple implicit mapping method where we can divide the large episode into several small episodes with length T0 and within each large episode we allocate the remaining budget to the remaining small episodes. If the agent does not spend the budget allocated for the small episode, it will have more allocated money for the rest of the small episodes in the large episode.\nState Mapping Models. Also we consider explicit mapping methods. At first, because Dt(b) keeps decreasing and Db(t) keeps increasing, then for D(t, b) where t and b are large, there should be some points {(t\u2032, b\u2032)} where t\u2032 \u2264 T0 and b\u2032 \u2264 B0 such that D(t\u2032, b\u2032) = D(t, b) as is shown in Figure 4, which confirms the existence of the mapping for D(t, b). Similarly, a(t, b,x) decreases w.r.t. t and increases w.r.t. b, which can be seen in Figure 3 and is consistent with intuitions. Thus the mapping for a(t, b,x) also exists. From the view of practical bidding, when the remaining number of auctions are large and the budget situation is similar, given the same bid request, the agent should give a similar bid price (see Figure 2). We consider a simple case that b/t represents the budget condition. Then here we have two linear mapping forms: (i) map a(t, b,x) where t > T0 to a(T0,\nb t \u00d7 T0,x). (ii) map D(t, b) where\nt > T0 to D(T0, b t \u00d7 T0). Denote |D(t, b) \u2212 D(T0, bt \u00d7 T0)| as Dev(t, T0, b). Figure 6 shows that the deviations of the simple linear mapping method are low enough (< 10\u22123\u03b8avg)."}, {"heading": "5. EXPERIMENTAL SETUP", "text": ""}, {"heading": "5.1 Datasets", "text": "Two real-world datasets are used in our experimental study, namely iPinYou and YOYI.\niPinYou is one of the mainstream RTB ad companies in China. The whole dataset comprises 19.5M impressions, 14.79K clicks and 16.0K CNY expense on 9 different campaigns over 10 days in 2013. We follow [31] for splitting the train/test sets and feature engineering.\nYOYI is a leading RTB company focusing on multi-device display advertising in China. YOYI dataset comprises 441.7M impressions, 416.9K clicks and 319.5K CNY expense during 8 days in Jan. 2016. The first 7 days are set as the training data while the last day is set as the test data.\nFor experiment reproducibility we publicize our code3. In the paper we mainly report results on iPinYou dataset, and further verify our algorithms over the YOYI dataset as supplementary.\n3The experiment code is available at https://github.com/ han-cai/rlb-dp and iPinYou dataset is available at http:// data.computational-advertising.org."}, {"heading": "5.2 Evaluation Methods", "text": "The evaluation is from the perspective of an advertiser\u2019s campaign with a predefined budget and lifetime (episode length).\nEvaluation metrics. The main goal of the bidding agent is to optimise the campaign\u2019s KPI (e.g., clicks, conversions, revenue, etc.) given the campaign budget. In our work, we consider the number of acquired clicks as the KPI, which is set as the primary evaluation measure in our experiments. We also analyze other statistics such as win rate, cost per mille impressions (CPM) and effective cost per click (eCPC).\nEvaluation flow. We mostly follow [32] when building the evaluation flow, except that we divide the test data into episodes. Specifically, the test data is a list of records, each of which consists of the bid request feature vector, the market price and the user response (click) label. We divide the test data into episodes, each of which contains T records and is allocated with a budget B. Given the CTR estimator and the bid landscape forecasting, the bidding strategy goes through the test data episode by episode. Specifically, the bidding strategy generates a price for each bid request (the bid price cannot exceed current budget). If the bid price is higher than or equal to the market price of the bid request, the advertiser wins the auction and then receives the market price as cost and the user click as reward and then updates the remaining auction number and budget.\nBudget constraints. Obviously, if the allocated budget B is too high, the bidding strategy can simply give a very high bid price each time to win all clicks in the test data. Therefore, in evaluation, budget constraints should not be higher than the historic total cost of the test data. We determine the budget B in this way: B = CPMtrain\u00d710\u22123\u00d7 T \u00d7 c0, where CPMtrain is the cost per mille impressions in the training data and c0 acts as the budget constraints parameter. Following previous work [32, 31], we run the evaluation with c0 = 1/32, 1/16, 1/8, 1/4, 1/2.\nEpisode length. The episode auction number T influences the complexity of our algorithms. When T is high, the original Algorithm 1 is not capable of working with limited resources, which further leads to our large-scale algorithms. For the large-scale evaluation, we set T as 100,000, which corresponds to a real-world 10-minute auction volume of a medium-scale RTB ad campaign. And for the small-scale evaluation, we set the episode length as 1,000. In addition, we run a set of evaluations with c0 = 0.2 and the episode length T = 200, 400, 600, 800, 1000 to give a more comprehensive performance analysis."}, {"heading": "5.3 Compared Methods", "text": "The following bidding policies are compared with the same CTR estimation component which is a logistic regression model and the same bid landscape forecasting component which is a non-parametric method, as described in Section 2:\nSS-MDP is based on [1], considering the bid landscape but ignoring the feature vector of bid request when giving the bid price. Although we regard this model as the state-of-the-art, it is proposed to work on keywordlevel bidding in sponsored search, which makes it not fine-grained enough to compare with RTB display advertising strategies.\nMcpc gives its bidding strategy as aMcpc(t, b,x) = CPC \u00d7 \u03b8(x), which matches some advertisers\u2019 requirement of maximum CPC (cost per click).\nLin is a linear bidding strategy w.r.t. the pCTR: aLin(t, b,x)\n= b0 \u03b8(x) \u03b8avg , where b0 is the basic bid price and is tuned using the training data [18]. This is the most widely used model in industry.\nRLB is our proposed model for the small-scale problem as shown in Algorithm 1.\nRLB-NN is our proposed model for the large-scale problem, which uses the neural network NN(t, b) to approximate D(t, b).\nRLB-NN-Seg combines the neural network with episode segmentation. For each small episode, the allocated budget is Bs = Br/Nr where Br is the remaining budget of the current large episode and Nr is the remaining number of small episodes in the current large episode. Then RLB-NN is run for the small episode. It corresponds to the coarse-to-fine episode segmentation model discussed in Section 4.1.\nRLB-NN-MapD combines the neural network with the mapping of D(t, b). That is: (i) D(t, b) = NN(t, b) where t \u2264 T0. (ii) D(t, b) = NN(T0, bt \u00d7 T0) where t > T0.\nRLB-NN-MapA combines the neural network with the mapping of a(t, b,x). That is: a(t, b,x) = a(T0,\nb t \u00d7\nT0,x) where t > T0. The last two models correspond to the state mapping models discussed in Section 4.1."}, {"heading": "6. EXPERIMENTAL RESULTS", "text": "In this section we present the experimental results on small- and large-scale data settings respectively."}, {"heading": "6.1 Small-Scale Evaluation", "text": "The performance comparison on iPinYou dataset under T = 1000 and different budget conditions are reported in Figure 7. In the comparison on total clicks (upper left plot), we find that (i) our proposed model RLB performs the best under every budget condition, verifying the effectiveness of the derived algorithm for optimizing attained clicks. (ii) Lin has the second best performance, which is a widely used bidding strategy in industry [18]. (iii) Compared to RLB and Lin, Mcpc does not adjust its strategy when the budget condition changes. Thus it performs quite well when c0 \u2265 1/4 but performs poorly on very limited budget conditions, which is consistent with the discussion in Section 2. (iv) SS-MDP gives the worst performance, since it is unaware of the feature information of each bid request, which shows the advantages of RTB display advertising.\nAs for the comparison on win rate, CPM and eCPC, we observe that (i) under every budget condition, SS-MDP keeps the highest win rate. The reason is that SS-MDP considers each bid request equally, thus its optimization target is equivalent to the number of impressions. Therefore, its win rate should be the highest. (ii) Lin and RLB are very close in comparison on CPM and eCPC. RLB can generate a higher number of clicks with comparable CPM and eCPC against Lin because RLB effectively spends the budget according to the market situation, which is unaware of by Lin.\nTable 2 provides a detailed performance on clicks of RLB over Lin under various campaigns and budget conditions. Among all 50 settings, RLB wins Lin in 46 (92%), ties in 1 (2%) and loses in 3 (6%) settings. It shows that RLB is robust and significantly outperforms Lin in the vast majority of the cases. Specifically, for 1/8 budget, RLB outperforms Lin by 16.7% on iPinYou data and 7.4% on YOYI data. Moreover, Figure 8 shows the performance comparison under the same budget condition (c0 = 0.2) and different episode lengths. The findings are similar to the above results. Compared to Lin, RLB can attain more clicks with similar eCPC. Note that, in offline evaluations the total auction number is stationary, larger episode length also means smaller episode number. Thus the total click numbers in Figure 8 do not increase largely w.r.t. T .\nSS-MDP is the only model that ignores the feature information of the bid request, thus providing a poor overall performance. Table 3 reports in detail the clicks along with the AUC of the CTR estimator for each campaign. We find\nthat when the performance of the CTR estimator is relatively low (AUC < 70%), e.g., campaign 2259, 2261, 2821, 2997, the performance of SS-MDP on clicks is quite good in comparison to Mcpc and Lin. By contrast, when the performance of the CTR estimator gets better, other methods which utilize the CTR estimator can attain much more clicks than SS-MDP."}, {"heading": "6.2 Large-Scale Evaluation", "text": "In this section, we first run the value function update in Algorithm 1 under T0 = 10, 000 and B0 = CPMtrain\u00d710\u22123\u00d7 T0\u00d71/2, then train a neural network with the attained data (t, b,D(t, b)) (where (t, b) \u2208 {0, \u00b7 \u00b7 \u00b7 , T0}\u00d7{0, \u00b7 \u00b7 \u00b7 , B0}). Here we use a fully connected neural network with two hidden layers which use tanh activation function. The first hidden layer has 30 hidden nodes and the second one has 15 hidden nodes. Next, we apply the neural network to run bidding under T = 100, 000 and B = CPMtrain \u00d7 10\u22123 \u00d7 T \u00d7 c0. In addition, SS-MDP is not tested in this experiment because it suffers from scalability issues and will have a similarly low performance as in the small-scale evaluation.\nTable 4 shows the performance of the neural network on iPinYou and YOYI. We can see that the RMSE is relatively low in comparison to \u03b8avg, which means that the neural network can provide a good approximation to the exact algorithm when the agent comes to a state (t, b,x) where (t, b) \u2208 {0, \u00b7 \u00b7 \u00b7 , T0} \u00d7 {0, \u00b7 \u00b7 \u00b7 , B0}.\nFigure 9 shows the performance comparison on iPinYou under T = 100, 000 and different budget conditions. We observe that (i) Mcpc has a similar performance to that observed in small-scale situations. (ii) For total clicks, RLBNN performs better than Lin under c0 = 1/32, 1/16, 1/8 and performs worse than Lin under c0 = 1/2, which shows\nTable 4: Approximation performance of the neural network.\niPinYou YOYI RMSE (\u00d710\u22126) 0.998 1.263 RMSE / \u03b8avg (\u00d710\u22124) 9.404 11.954\nFigure 9: Overall performance on iPinYou under T = 105 and different budget conditions.\nthat the generalization ability of the neural network is satisfactory only in small scales. For relatively large scales, the generalization of RLB-NN is not reliable. (iii) Compared to RLB-NN, the 3 sophisticated algorithms RLBNN-Seg, RLB-NN-MapD and RLB-NN-MapA are more robust and outperform Lin under every budget condition. They do not rely on the generalization ability of the approximation model, therefore their performance is more stable. The results clearly demonstrate that they are effective solutions for the large-scale problem. (iv) As for eCPC, all models except from Mcpc are very close, thus making the proposed RLB algorithms practically effective."}, {"heading": "7. ONLINE DEPLOYMENT AND A/B TEST", "text": "Our proposed RLB model is deployed and tested in a live environment provided by Vlion DSP. The deployment environment is based on HP ProLiant DL360p Gen8 servers. A 5-node cluster is utilized for the bidding agent, where each node is in CentOS release 6.3, with 6 core Intel Xeon CPU E5-2620 (2.10GHz) and 64GB RAM. The model is implemented in Lua with Nginx.\nThe compared bidding strategy is Lin as discussed in Section 5.3. The optimization target is click. The two compared methods are given the same budget, which is further allocated to episodes. Unlike offline evaluations, the online evaluation flow stops only when the budget is exhausted. Within an episode, a maximum bid number T is set for each strategy to prevent overspending too much. Specifically, T is mostly determined by the allocated budget for the episode B, previous CPM and win rate: T = B/CPM/win rate \u00d7 103. The possible available auction number during the episode is also considered when determining T . The agent keeps the remaining bid number and budget, which we consider as t and b respectively. Note that the remaining budget may have some deviation due to latency. The latency is typically less than 100ms, which is negligible. We test over 5 campaigns during 25-28th of July, 2016. All the methods share the same previous 7-day training data, and the same CTR estimator which is a logistic regression model trained with FTRL. The bid requests of each user are randomly sent to either method. The overall results are presented in Figure\n10, while the click and cost performances w.r.t. time are shown in Figure 11.\nFrom the comparison, we observe the following: (i) with the same cost, RLB achieves lower eCPC than Lin, and thus more total clicks, which shows the cost effectiveness of RLB. (ii) RLB provides better planning than Lin: the acquired clicks and spent budget increase evenly across the time. (iii) With better planning, RLB obtains lower CPM than Lin, yielding more bids and more winning impressions. (iv) With lower CPM on cheap cases, RLB achieves a close CTR compared to Lin, which leads to superior performance. In summary, the online evaluation demonstrates the effectiveness of our proposed RLB model for optimizing attained clicks with a good pacing."}, {"heading": "8. CONCLUSIONS", "text": "In this paper, we proposed a model-based reinforcement learning model (RLB) for learning the bidding strategy in RTB display advertising. The bidding strategy is naturally defined as the policy of making a bidding action given the state of the campaign\u2019s parameters and the input bid request information. With an MDP formulation, the state transition and reward function are captured via modeling the auction competition and user click, respectively. The optimal bidding policy is then derived using dynamic programming. Furthermore, to deal with the large-scale auction volume and campaign budget, we proposed neural network models to fit the differential of the values between two consecutive\nstates. Experimental results on two real-world large-scale datasets and online A/B test demonstrated the superiority of our RLB solutions over several strong baselines and stateof-the-art methods, as well as their high efficiency to handle large-scale data.\nFor future work, we will investigate model-free approaches such as Q-learning and policy gradient methods to unify utility estimation, bid landscape forecasting and bid optimization into a single optimization framework and handle the highly dynamic environment. Also, since RLB naturally tackles the problem of budget over- or under-spending across the campaign lifetime, we will compare our RLB solutions with the explicit budget pacing techniques [13, 28]."}, {"heading": "ACKNOWLEDGMENTS", "text": "We sincerely thank the engineers from YOYI DSP to provide us the offline experiment dataset and the engineers from Vlion DSP to help us conduct online A/B tests."}, {"heading": "9. REFERENCES", "text": "[1] K. Amin, M. Kearns, P. Key, and A. Schwaighofer.\nBudget optimization for sponsored search: Censored learning in mdps. UAI, 2012.\n[2] J. Boyan and A. W. Moore. Generalization in reinforcement learning: Safely approximating the value function. NIPS, pages 369\u2013376, 1995.\n[3] O. Chapelle. Modeling delayed feedback in display advertising. In KDD, 2014.\n[4] Y. Chen, P. Berkhin, B. Anderson, and N. R. Devanur. Real-time bidding algorithms for performance-based display ad allocation. In KDD, 2011.\n[5] Y. Cui, R. Zhang, W. Li, and J. Mao. Bid landscape forecasting in online ad exchange marketplace. In KDD, 2011.\n[6] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords. Technical report, National Bureau of Economic Research, 2005.\n[7] Google. The arrival of real-time bidding, 2011.\n[8] G. J. Gordon. Stable function approximation in dynamic programming. In ICML, pages 261\u2013268, 1995.\n[9] T. Graepel, J. Q. Candela, T. Borchert, and R. Herbrich. Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft\u2019s bing search engine. In ICML, 2010.\n[10] X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, et al. Practical lessons from predicting clicks on ads at facebook. In ADKDD, 2014.\n[11] K. Hosanagar and V. Cherepanov. Optimal bidding in stochastic budget constrained slot auctions. In EC, 2008.\n[12] V. Krishna. Auction theory. Academic press, 2009.\n[13] K.-C. Lee, A. Jalali, and A. Dasdan. Real time bid optimization with smooth budget delivery in online advertising. In ADKDD, 2013.\n[14] K.-c. Lee, B. Orten, A. Dasdan, and W. Li. Estimating conversion rate in display advertising from past erformance data. In KDD, 2012.\n[15] X. Li and D. Guan. Programmatic buying bidding\nstrategies with win rate and winning price estimation in real time mobile advertising. In PAKDD. 2014.\n[16] H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady, L. Nie, T. Phillips, E. Davydov, D. Golovin, et al. Ad click prediction: a view from the trenches. In KDD, 2013.\n[17] R. J. Oentaryo, E.-P. Lim, J.-W. Low, D. Lo, and M. Finegold. Predicting response in mobile advertising with hierarchical importance-aware factorization machine. In WSDM, 2014.\n[18] C. Perlich, B. Dalessandro, R. Hook, O. Stitelman, T. Raeder, and F. Provost. Bid optimizing and inventory scoring in targeted online advertising. In KDD, 2012.\n[19] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587), 2016.\n[20] A. L. Strehl, L. Li, E. Wiewiora, J. Langford, and M. L. Littman. Pac model-free reinforcement learning. In ICML, pages 881\u2013888. ACM, 2006.\n[21] R. S. Sutton and A. G. Barto. Introduction to reinforcement learning, volume 135. MIT Press Cambridge, 1998.\n[22] G. Taylor and R. Parr. Kernelized value function approximation for reinforcement learning. In ICML, pages 1017\u20131024. ACM, 2009.\n[23] J. Wang and S. Yuan. Real-time bidding: A new frontier of computational advertising research. In WSDM, 2015.\n[24] J. Wang, W. Zhang, and S. Yuan. Display advertising with real-time bidding (RTB) and behavioural targeting. arXiv preprint arXiv:1610.03013, 2016.\n[25] Y. Wang, K. Ren, W. Zhang, J. Wang, and Y. Yu. Functional bid landscape forecasting for display advertising. In ECML-PKDD, pages 115\u2013131, 2016.\n[26] C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279\u2013292, 1992.\n[27] W. C.-H. Wu, M.-Y. Yeh, and M.-S. Chen. Predicting winning price in real time bidding with censored data. In KDD, 2015.\n[28] J. Xu, K.-c. Lee, W. Li, H. Qi, and Q. Lu. Smart pacing for effective online ad campaign optimization. In KDD, 2015.\n[29] S. Yuan and J. Wang. Sequential selection of correlated ads by pomdps. In CIKM, 2012.\n[30] S. Yuan, J. Wang, and X. Zhao. Real-time bidding for online advertising: measurement and analysis. In ADKDD, 2013.\n[31] W. Zhang and J. Wang. Statistical arbitrage mining for display advertising. In KDD, 2015.\n[32] W. Zhang, S. Yuan, and J. Wang. Optimal real-time bidding for display advertising. In KDD, 2014.\n[33] W. Zhang, T. Zhou, J. Wang, and J. Xu. Bid-aware gradient descent for unbiased learning with censored data in display advertising. In KDD."}], "references": [{"title": "Budget optimization for sponsored search: Censored learning in mdps", "author": ["K. Amin", "M. Kearns", "P. Key", "A. Schwaighofer"], "venue": "UAI", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Generalization in reinforcement learning: Safely approximating the value function", "author": ["J. Boyan", "A.W. Moore"], "venue": "NIPS, pages 369\u2013376", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Modeling delayed feedback in display advertising", "author": ["O. Chapelle"], "venue": "KDD", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Real-time bidding algorithms for performance-based display ad allocation", "author": ["Y. Chen", "P. Berkhin", "B. Anderson", "N.R. Devanur"], "venue": "KDD", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Bid landscape forecasting in online ad exchange marketplace", "author": ["Y. Cui", "R. Zhang", "W. Li", "J. Mao"], "venue": "KDD", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords", "author": ["B. Edelman", "M. Ostrovsky", "M. Schwarz"], "venue": "Technical report, National Bureau of Economic Research", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Stable function approximation in dynamic programming", "author": ["G.J. Gordon"], "venue": "ICML, pages 261\u2013268", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft\u2019s bing search engine", "author": ["T. Graepel", "J.Q. Candela", "T. Borchert", "R. Herbrich"], "venue": "ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["X. He", "J. Pan", "O. Jin", "T. Xu", "B. Liu", "T. Xu", "Y. Shi", "A. Atallah", "R. Herbrich", "S. Bowers"], "venue": "Practical lessons from predicting clicks on ads at facebook. In ADKDD", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimal bidding in stochastic budget constrained slot auctions", "author": ["K. Hosanagar", "V. Cherepanov"], "venue": "EC", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Auction theory", "author": ["V. Krishna"], "venue": "Academic press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Real time bid optimization with smooth budget delivery in online advertising", "author": ["K.-C. Lee", "A. Jalali", "A. Dasdan"], "venue": "ADKDD", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Estimating conversion rate in display advertising from past erformance data", "author": ["K.-c. Lee", "B. Orten", "A. Dasdan", "W. Li"], "venue": "In KDD,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Programmatic buying bidding  strategies with win rate and winning price estimation in real time mobile advertising", "author": ["X. Li", "D. Guan"], "venue": "In PAKDD", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "et al", "author": ["H.B. McMahan", "G. Holt", "D. Sculley", "M. Young", "D. Ebner", "J. Grady", "L. Nie", "T. Phillips", "E. Davydov", "D. Golovin"], "venue": "Ad click prediction: a view from the trenches. In KDD", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Predicting response in mobile advertising with hierarchical importance-aware factorization machine", "author": ["R.J. Oentaryo", "E.-P. Lim", "J.-W. Low", "D. Lo", "M. Finegold"], "venue": "WSDM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Bid optimizing and inventory scoring in targeted online advertising", "author": ["C. Perlich", "B. Dalessandro", "R. Hook", "O. Stitelman", "T. Raeder", "F. Provost"], "venue": "KDD", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "G", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Pac model-free reinforcement learning", "author": ["A.L. Strehl", "L. Li", "E. Wiewiora", "J. Langford", "M.L. Littman"], "venue": "ICML, pages 881\u2013888. ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Introduction to reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "volume 135. MIT Press Cambridge", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Kernelized value function approximation for reinforcement learning", "author": ["G. Taylor", "R. Parr"], "venue": "ICML, pages 1017\u20131024. ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Real-time bidding: A new frontier of computational advertising research", "author": ["J. Wang", "S. Yuan"], "venue": "WSDM", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Display advertising with real-time bidding (RTB) and behavioural targeting", "author": ["J. Wang", "W. Zhang", "S. Yuan"], "venue": "arXiv preprint arXiv:1610.03013", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Functional bid landscape forecasting for display advertising", "author": ["Y. Wang", "K. Ren", "W. Zhang", "J. Wang", "Y. Yu"], "venue": "ECML-PKDD, pages 115\u2013131", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, 8(3-4):279\u2013292", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1992}, {"title": "Predicting winning price in real time bidding with censored data", "author": ["W.C.-H. Wu", "M.-Y. Yeh", "M.-S. Chen"], "venue": "KDD", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Smart pacing for effective online ad campaign optimization", "author": ["J. Xu", "K.-c. Lee", "W. Li", "H. Qi", "Q. Lu"], "venue": "In KDD,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Sequential selection of correlated ads by pomdps", "author": ["S. Yuan", "J. Wang"], "venue": "CIKM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time bidding for online advertising: measurement and analysis", "author": ["S. Yuan", "J. Wang", "X. Zhao"], "venue": "ADKDD", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Statistical arbitrage mining for display advertising", "author": ["W. Zhang", "J. Wang"], "venue": "KDD", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal real-time bidding for display advertising", "author": ["W. Zhang", "S. Yuan", "J. Wang"], "venue": "KDD", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 17, "context": "In particular, the successful application of reinforcement learning in certain settings such as gaming control [19] has demonstrated that machines not only can predict, but also have a potential of achieving comparable humanlevel control and decision making.", "startOffset": 111, "endOffset": 115}, {"referenceID": 28, "context": "Auctions, particularly real-time bidding (RTB), have been a major trading mechanism for online display advertising [30, 7, 23].", "startOffset": 115, "endOffset": 126}, {"referenceID": 21, "context": "Auctions, particularly real-time bidding (RTB), have been a major trading mechanism for online display advertising [30, 7, 23].", "startOffset": 115, "endOffset": 126}, {"referenceID": 0, "context": "Unlike the keyword-level bid decision in sponsored search [1], the advertiser needs to make the impression-level bid decision in RTB, i.", "startOffset": 58, "endOffset": 61}, {"referenceID": 16, "context": ", bidding for every single ad impression in real time when it is just being generated by a user visit [18, 30] .", "startOffset": 102, "endOffset": 110}, {"referenceID": 28, "context": ", bidding for every single ad impression in real time when it is just being generated by a user visit [18, 30] .", "startOffset": 102, "endOffset": 110}, {"referenceID": 9, "context": ", to calculate the strategic amount that the advertiser would like to pay for an ad opportunity, constitutes a core component that drives the campaigns\u2019 ROI [11, 32].", "startOffset": 157, "endOffset": 165}, {"referenceID": 30, "context": ", to calculate the strategic amount that the advertiser would like to pay for an ad opportunity, constitutes a core component that drives the campaigns\u2019 ROI [11, 32].", "startOffset": 157, "endOffset": 165}, {"referenceID": 5, "context": "A straightforward bidding strategy in RTB display advertising is mainly based on the truthfulness of second-price auctions [6], which means the bid price for each ad impression should be equal to its true value, i.", "startOffset": 123, "endOffset": 126}, {"referenceID": 12, "context": ", clickthrough rate) [14].", "startOffset": 21, "endOffset": 25}, {"referenceID": 29, "context": "However, for budgeted bidding in repeated auctions, the optimal bidding strategy may not be truthful but depends on the market competition, auction volume and campaign budget [31].", "startOffset": 175, "endOffset": 179}, {"referenceID": 16, "context": "In [18, 32], researchers have proposed to seek the optimal bidding function that directly maximizes the campaign\u2019s key performance indicator (KPI), e.", "startOffset": 3, "endOffset": 11}, {"referenceID": 30, "context": "In [18, 32], researchers have proposed to seek the optimal bidding function that directly maximizes the campaign\u2019s key performance indicator (KPI), e.", "startOffset": 3, "endOffset": 11}, {"referenceID": 3, "context": "Nevertheless, such static bid optimization frameworks may still not work well in practice because the RTB market competition is highly dynamic and it is fairly common that the true data distribution heavily deviates from the assumed one during the model training [4], which requires additional control step such as budget pacing [28] to constrain the budget spending.", "startOffset": 263, "endOffset": 266}, {"referenceID": 26, "context": "Nevertheless, such static bid optimization frameworks may still not work well in practice because the RTB market competition is highly dynamic and it is fairly common that the true data distribution heavily deviates from the assumed one during the model training [4], which requires additional control step such as budget pacing [28] to constrain the budget spending.", "startOffset": 329, "endOffset": 333}, {"referenceID": 19, "context": "An MDP provides a mathematical framework which is widely used for modelling the dynamics of an environment under different actions, and is useful for solving reinforcement learning problems [21].", "startOffset": 190, "endOffset": 194}, {"referenceID": 6, "context": "For large-scale situations, it is difficult to experience the whole state space, which leads to the use of function approximation that constructs an approximator of the entire function [8, 22].", "startOffset": 185, "endOffset": 192}, {"referenceID": 20, "context": "For large-scale situations, it is difficult to experience the whole state space, which leads to the use of function approximation that constructs an approximator of the entire function [8, 22].", "startOffset": 185, "endOffset": 192}, {"referenceID": 30, "context": "In the RTB process [32], the advertiser receives the bid request of an ad impression with its real-time information and the very first thing to do is to estimate the utility, i.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": ", the market price [1, 5], which is the highest bid price from other competitors, is also forecasted by the bid landscape forecasting component.", "startOffset": 19, "endOffset": 25}, {"referenceID": 4, "context": ", the market price [1, 5], which is the highest bid price from other competitors, is also forecasted by the bid landscape forecasting component.", "startOffset": 19, "endOffset": 25}, {"referenceID": 28, "context": "Given the estimated utility and cost factors, the bidding strategy [30] decides the final bid price with accessing the information of the remaining budget and auction volume.", "startOffset": 67, "endOffset": 71}, {"referenceID": 22, "context": "A recent comprehensive study on the data science of RTB display advertising is posted in [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": ", click or conversion, and can be modeled as a probability estimation task [14].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": ", click-through rate (CTR) [16], conversion rate (CVR) [14] and post-click conversion delay patterns [3].", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": ", click-through rate (CTR) [16], conversion rate (CVR) [14] and post-click conversion delay patterns [3].", "startOffset": 55, "endOffset": 59}, {"referenceID": 2, "context": ", click-through rate (CTR) [16], conversion rate (CVR) [14] and post-click conversion delay patterns [3].", "startOffset": 101, "endOffset": 104}, {"referenceID": 12, "context": "For modeling, linear models such as logistic regression [14] and nonlinear models such as boosted trees [10] and factorization machines [17] are widely used in practice.", "startOffset": 56, "endOffset": 60}, {"referenceID": 8, "context": "For modeling, linear models such as logistic regression [14] and nonlinear models such as boosted trees [10] and factorization machines [17] are widely used in practice.", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "For modeling, linear models such as logistic regression [14] and nonlinear models such as boosted trees [10] and factorization machines [17] are widely used in practice.", "startOffset": 136, "endOffset": 140}, {"referenceID": 7, "context": "There are also online learning models that immediately perform updating when observing each data instance, such as Bayesian probit regression [9], FTRL learning in logistic regression [16].", "startOffset": 142, "endOffset": 145}, {"referenceID": 14, "context": "There are also online learning models that immediately perform updating when observing each data instance, such as Bayesian probit regression [9], FTRL learning in logistic regression [16].", "startOffset": 184, "endOffset": 188}, {"referenceID": 12, "context": "In our paper, we follow [14, 32] and adopt the most widely used logistic regression for utility estimation to model the reward on agent actions.", "startOffset": 24, "endOffset": 32}, {"referenceID": 30, "context": "In our paper, we follow [14, 32] and adopt the most widely used logistic regression for utility estimation to model the reward on agent actions.", "startOffset": 24, "endOffset": 32}, {"referenceID": 4, "context": "is the winning probability given each specific bid price [5].", "startOffset": 57, "endOffset": 60}, {"referenceID": 13, "context": "The authors in [15, 32, 5] presented some hypothetical winning functions and learned the parameters.", "startOffset": 15, "endOffset": 26}, {"referenceID": 30, "context": "The authors in [15, 32, 5] presented some hypothetical winning functions and learned the parameters.", "startOffset": 15, "endOffset": 26}, {"referenceID": 4, "context": "The authors in [15, 32, 5] presented some hypothetical winning functions and learned the parameters.", "startOffset": 15, "endOffset": 26}, {"referenceID": 4, "context": "For example, a log-normal market price distribution with the parameters estimated by gradient boosting decision trees was proposed in [5].", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "Since advertisers only observe the winning impressions, the problem of censored data [1, 33] is critical.", "startOffset": 85, "endOffset": 92}, {"referenceID": 25, "context": "Authors in [27] proposed to leverage censored linear regression to jointly model the likelihood of observed market prices in winning cases and censored ones with losing bids.", "startOffset": 11, "endOffset": 15}, {"referenceID": 23, "context": "Recently, the authors in [25] proposed to combine survival analysis and decision tree models, where each tree leaf maintains a non-parametric survival model to fit the censored market prices.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "In this paper, we follow [1, 33] and use a non-parametric method to model the market price distribution.", "startOffset": 25, "endOffset": 32}, {"referenceID": 16, "context": "As has been discussed above, bidding strategy optimization is the key component within the decision process for the advertisers [18].", "startOffset": 128, "endOffset": 132}, {"referenceID": 10, "context": "The auction theory [12] proved that truthful bidding is the optimal strategy under the second-price auction.", "startOffset": 19, "endOffset": 23}, {"referenceID": 30, "context": "However, truthful bidding may perform poorly when considering the multiple auctions and the budget constraint [32].", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "In real-world applications, the linear bidding function [18] is widely used.", "startOffset": 56, "endOffset": 60}, {"referenceID": 30, "context": "The authors in [32] empirically showed that there existed non-linear bidding functions better than the linear ones under variant budget constraints.", "startOffset": 15, "endOffset": 19}, {"referenceID": 16, "context": "When the data changes, however, the heuristic model [18] or hypothetical bidding functions [31, 32] cannot depict well the real data distribution.", "startOffset": 52, "endOffset": 56}, {"referenceID": 29, "context": "When the data changes, however, the heuristic model [18] or hypothetical bidding functions [31, 32] cannot depict well the real data distribution.", "startOffset": 91, "endOffset": 99}, {"referenceID": 30, "context": "When the data changes, however, the heuristic model [18] or hypothetical bidding functions [31, 32] cannot depict well the real data distribution.", "startOffset": 91, "endOffset": 99}, {"referenceID": 0, "context": "The authors in [1, 29] proposed the model-based MDPs to derive the optimal policy for bidding in sponsored search or ad selection in contextual advertising, where the decision is made on keyword level.", "startOffset": 15, "endOffset": 22}, {"referenceID": 27, "context": "The authors in [1, 29] proposed the model-based MDPs to derive the optimal policy for bidding in sponsored search or ad selection in contextual advertising, where the decision is made on keyword level.", "startOffset": 15, "endOffset": 22}, {"referenceID": 0, "context": "In our work, we investigate the most challenging impression-level bid decision problem in RTB display advertising that is totally different from [1, 29].", "startOffset": 145, "endOffset": 152}, {"referenceID": 27, "context": "In our work, we investigate the most challenging impression-level bid decision problem in RTB display advertising that is totally different from [1, 29].", "startOffset": 145, "endOffset": 152}, {"referenceID": 0, "context": "We also tackle the scalability problem, which remains unsolved in [1], and demonstrate the efficiency and effectiveness of our method in a variety of experiments.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "Mathematically, we consider bidding in display advertising as an episodic process [1]; each episode comprises T auctions which are sequentially sent to the bidding agent.", "startOffset": 82, "endOffset": 85}, {"referenceID": 19, "context": "A Markov Decision Process (MDP) provides a framework that is widely used for modeling agent-environment interactions [21].", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "One may consider the possibility of model-free approaches [26, 20] to directly learn the bidding policy from experience.", "startOffset": 58, "endOffset": 66}, {"referenceID": 18, "context": "One may consider the possibility of model-free approaches [26, 20] to directly learn the bidding policy from experience.", "startOffset": 58, "endOffset": 66}, {"referenceID": 30, "context": "(6), we consider an approximation m(\u03b4,x) \u2248 m(\u03b4) by following the dependency assumption x\u2192 \u03b8 \u2192 a\u2192 w(winning rate) in [32].", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "the pCTR with a static parameter [18], such as Mcpc and Lin discussed in Section 5.", "startOffset": 33, "endOffset": 37}, {"referenceID": 30, "context": "(7), we take the approximation m(\u03b4,x) \u2248 m(\u03b4) by following the dependency assumption x\u2192 \u03b8 \u2192 a\u2192 w(winning rate) in [32] and consequently get an approximation of the optimal value function V (t, b) in Eq.", "startOffset": 113, "endOffset": 117}, {"referenceID": 1, "context": "As a widely used solution [2, 21], here we take a fully connected neural network with several hidden layers as a nonlinear approximator.", "startOffset": 26, "endOffset": 33}, {"referenceID": 19, "context": "As a widely used solution [2, 21], here we take a fully connected neural network with several hidden layers as a nonlinear approximator.", "startOffset": 26, "endOffset": 33}, {"referenceID": 29, "context": "We follow [31] for splitting the train/test sets and feature engineering.", "startOffset": 10, "endOffset": 14}, {"referenceID": 30, "context": "We mostly follow [32] when building the evaluation flow, except that we divide the test data into episodes.", "startOffset": 17, "endOffset": 21}, {"referenceID": 30, "context": "Following previous work [32, 31], we run the evaluation with c0 = 1/32, 1/16, 1/8, 1/4, 1/2.", "startOffset": 24, "endOffset": 32}, {"referenceID": 29, "context": "Following previous work [32, 31], we run the evaluation with c0 = 1/32, 1/16, 1/8, 1/4, 1/2.", "startOffset": 24, "endOffset": 32}, {"referenceID": 0, "context": "SS-MDP is based on [1], considering the bid landscape but ignoring the feature vector of bid request when giving the bid price.", "startOffset": 19, "endOffset": 22}, {"referenceID": 16, "context": "using the training data [18].", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "(ii) Lin has the second best performance, which is a widely used bidding strategy in industry [18].", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "Also, since RLB naturally tackles the problem of budget over- or under-spending across the campaign lifetime, we will compare our RLB solutions with the explicit budget pacing techniques [13, 28].", "startOffset": 187, "endOffset": 195}, {"referenceID": 26, "context": "Also, since RLB naturally tackles the problem of budget over- or under-spending across the campaign lifetime, we will compare our RLB solutions with the explicit budget pacing techniques [13, 28].", "startOffset": 187, "endOffset": 195}], "year": 2017, "abstractText": "The majority of online display ads are served through realtime bidding (RTB) \u2014 each ad display impression is auctioned off in real-time when it is just being generated from a user visit. To place an ad automatically and optimally, it is critical for advertisers to devise a learning algorithm to cleverly bid an ad impression in real-time. Most previous works consider the bid decision as a static optimization problem of either treating the value of each impression independently or setting a bid price to each segment of ad volume. However, the bidding for a given ad campaign would repeatedly happen during its life span before the budget runs out. As such, each bid is strategically correlated by the constrained budget and the overall effectiveness of the campaign (e.g., the rewards from generated clicks), which is only observed after the campaign has completed. Thus, it is of great interest to devise an optimal bidding strategy sequentially so that the campaign budget can be dynamically allocated across all the available impressions on the basis of both the immediate and future rewards. In this paper, we formulate the bid decision process as a reinforcement learning problem, where the state space is represented by the auction information and the campaign\u2019s real-time parameters, while an action is the bid price to set. By modeling the state transition via auction competition, we build a Markov Decision Process framework for learning the optimal bidding policy to optimize the advertising performance in the dynamic real-time bidding environment. Furthermore, the scalability problem from the large real-world auction volume and campaign budget is well handled by state value approximation using neural networks. The empirical study on two large-scale real-world datasets and the live A/B testing on a commercial platform have demonstrated the superior performance and high efficiency compared to state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}