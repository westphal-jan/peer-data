{"id": "1701.03849", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2017", "title": "Deep Neural Networks for Czech Multi-label Document Classification", "abstract": "this paper successfully focused with automatic point - label algebraic classification producing particular textual documents. the basic approaches can use naive pre - processing which naturally have negative applications ( loss of information, additional additional work, etc ). therefore, researchers would worry because omit it only assumes deep neural processes generally learn from simple ones. this theory grows conditioned by periodically varying usage in different other machine genetic regions. two different networks are compared : the first one is possibly natural multi - interacting perceptron, while the second network proposes a simplified convolutional network. the experiments on a quantum newspaper scan demonstrate likely both versions consistently outperform baseline databases which uses a sparse corpus of features with maximum accessibility classifier. we then also shown that sentinel network gives the best results.", "histories": [["v1", "Fri, 13 Jan 2017 23:23:12 GMT  (135kb)", "http://arxiv.org/abs/1701.03849v1", "Dears, this paper has been already accepted for CICLing 2016 and presented at the conference in 04 2016. Unfortunately, the proceedings is not ready yet. Therefore, we have decided to publish this paper at ArXiv. -acceptance letter begin- Dear Professor Kral, We are happy to notify you that your paper 167: \"Deep Neural Networks for Czech Multi-label Document Classification\" has been ACCEPTED"], ["v2", "Wed, 18 Jan 2017 23:17:30 GMT  (135kb)", "http://arxiv.org/abs/1701.03849v2", "Accepted for CICLing 2016 and presented at the conference in 04 2016"]], "COMMENTS": "Dears, this paper has been already accepted for CICLing 2016 and presented at the conference in 04 2016. Unfortunately, the proceedings is not ready yet. Therefore, we have decided to publish this paper at ArXiv. -acceptance letter begin- Dear Professor Kral, We are happy to notify you that your paper 167: \"Deep Neural Networks for Czech Multi-label Document Classification\" has been ACCEPTED", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ladislav lenc", "pavel kr\\'al"], "accepted": false, "id": "1701.03849"}, "pdf": {"name": "1701.03849.pdf", "metadata": {"source": "CRF", "title": "Deep Neural Networks for Czech Multi-label Document Classification", "authors": ["Ladislav Lenc", "Pavel Kr\u00e1l"], "emails": ["llenc@kiv.zcu.cz", "pkral@kiv.zcu.cz"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n03 84\n9v 1\n[ cs\n.C L\n] 1\n3 Ja\nKeywords: Czech, Deep Neural Networks, Document Classification, Multi-label"}, {"heading": "1 Introduction", "text": "The amount of electronic text documents is growing extremely rapidly and therefore automatic document classification (or categorization) becomes very important for information organization, storage and retrieval. Multi-label classification is considerably more important than the single-label classification because it usually corresponds better to the needs of the current applications.\nThe modern approaches usually use several pre-processing tasks: feature selection/reduction [1]; precise document representation (e.g. POS-filtering, particular lexical and syntactic features, lemmatization, etc.) [2] to reduce the feature space with minimal negative impact on classification accuracy. However, this pre-processing has several drawbacks as for instance loss of information, significant additional implementation work, dependency on the task/application, etc.\nNeural networks with deep learning are today very popular in machine learning field and it was proved that they outperform many state-of-the-art approaches without\nany parametrization. This fact is particularly evident in image processing [3], however it was further showed that they are also superior in Natural Language Processing (NLP) including Part-Of-Speech (POS) tagging, chunking, named entity recognition or semantic role labelling [4]. However, to the best of our knowledge, the current published work does not include their application for multi-label document classification.\nTherefore, the main goal of this paper consists in using neural networks for multilabel document classification of Czech text documents. We will compare several topologies with different number of parameters to show that they can have better accuracy than the state-of-the-art methods.\nWe use and compare standard feed-forward networks (i.e. multi-layer perceptron) and popular Convolutional Networks (CNNs). To the best of our knowledge, this comparison was never been done on this task before. Therefore, it is another contribution of this paper. Note that we expect better performance of the CNNs as shown for instance in the OCR task [5].\nThe results of this work should be integrated into an experimental multi-label document classification system. The system should be used to replace manual annotation of the newspaper documents which is very expensive and time consuming task and thus save the human resources in the Czech News Agency (C\u030cTK)1.\nThe rest of the paper is organized as follows. Section 2 is a short review of document classification methods with a particular focus on neural networks. Section 3 describes our document classification approaches. Section 4 deals with experiments realized on the C\u030cTK corpus and then discusses the obtained results. In the last section, we conclude the experimental results and propose some future research directions."}, {"heading": "2 Related Work", "text": "Document classification is usually based on supervised machine learning methods that exploit an annotated corpus to train a classifier which then assigns the classes to unlabelled documents. The most of works use Vector Space Model (VSM), which usually represents each document with a vector of all word occurrences weighted by their Term Frequency-Inverse Document Frequency (TF-IDF).\nSeveral classification methods have been successfully used [6], for instance Bayesian classifiers, Maximum Entropy (ME), Support Vector Machines (SVMs), etc. However, the main issue of this task is that the feature space in the VSM is highly dimensional which decreases the accuracy of the classifier.\nNumerous feature selection/reduction approaches have been introduced [1,7] to solve this problem. Furthermore, a better document representation should help to decrease the feature vector dimension, e.g. using lexical and syntactic features as shown in [2]. Chandrasekar et al. further show in [8] that it is beneficial to use POS-tag filtration in order to represent a document more accurately.\nMore recently, some interesting approaches based on Latent Dirichlet Allocation (LLDA) [9] have been introduced. Another method exploits partial labels to discover latent topics [10]. Principal Component Analysis [11] incorporating semantic concepts [12] has also been used for the document classification.\n1 http://www.ctk.eu\nRecently, \u201cdeep\u201d Neural Nets (NN) have shown their superior performance in many natural language processing tasks including POS tagging, chunking, named entity recognition and semantic role labelling [4] without any parametrization. Several different topologies and learning algorithms were proposed.\nFor instance, the authors of [13] propose two Convolutional Neural Nets (CNN) for ontology classification, sentiment analysis and single-label document classification. Their networks are composed of 9 layers out of which 6 are convolutional layers and 3 fully-connected layers with different numbers of hidden units and frame sizes. They show that the proposed method significantly outperforms the baseline approaches (bag of words) on English and Chinese corpora. Another interesting work [14] uses in the first layer (i.e. lookup table) pre-trained vectors from word2vec [15]. The authors show that the proposed models outperform the state-of-the-art on 4 out of 7 tasks, which include sentiment analysis and question classification.\nFor additional information about architectures, algorithms, and applications of deep learning, please refer the survey [16].\nOn the other hand, classical feed-forward neural nets architectures represented particularly by multi-layer perceptrons are used rather rarely. However, these models were very popular before and some approaches for document classification exist. Manevitz et al. show in [17] that their simple feed-forward neural network with three layers (20 inputs, 6 neurons in hidden layer and 10 neurons in the output layer, i.e. number of classes) gives F-measure about 78% on the standard Reuters dataset.\nTraditional multi-layer neural networks were also used for multi-label document classification in [18]. The authors have modified standard backpropagation algorithm for multi-label learning which employs a novel error function. This approach is evaluated on functional genomics and text categorization.\nThe most of the proposed approaches is focused on English and only few works deal with Czech language. Hrala et al. use in [19] lemmatization and Part-Of-Speech (POS) filtering for a precise representation of Czech documents. In [20], three different multi-label classification approaches are compared and evaluated. Another recent work proposes novel features based on the unsupervised machine learning [21]. To the best of our knowledge, no document classification approach using neural nets deals with Czech language."}, {"heading": "3 Neural Nets for Multi-label Document Classification", "text": ""}, {"heading": "3.1 Baseline Classification", "text": "The feature set is created according to Brychc\u0131\u0301n et al. [21] and is composed of words, stems and features created by S-LDA and COALS. They are used because the authors experimentally proved that the additional unsupervised features significantly improve classification results.\nFor multi-label classification, we use an efficient approach presented by Tsoumakas et al. in [22]. This method employs n binary classifiers Cni=1 : d \u2192 l,\u00acl (i.e. each binary classifier assigns the document d to the label l iff the label is included in the document, \u00acl otherwise). The classification result is given by the following equation:\nC(d) = \u222ani=1: Ci(d) (1)\nThe Maximum Entropy (ME) model is used for classification."}, {"heading": "3.2 Standard Feed-forward Deep Neural Network (FDNN)", "text": "Feed-forward neural networks are probably the most commonly used type of NNs. We propose to use an MLP with two hidden layers which can be seen as a deep network2. As an input of our network we use the simple Bag of Words (BoW) which is a binary vector where value 1 means that the word with a given index is present in the document. The size of this vector depends on the size of the dictionary which is limited by N most frequent words. The only preprocessing is the conversion of all characters to lower case and also replacing of all numbers by one common token.\nThe size of the input layer thus depends on the size of the dictionary that is used for the feature vector creation. The first hidden layer has 1024 while the second one has 512 nodes3. The output layer has size equal to the number of categories which is 37 in our case. To handle the multi-label classification, we threshold the values of nodes in the output layer. Only the values larger than a given threshold are assigned to the labels."}, {"heading": "3.3 Convolutional Neural Network (CNN)", "text": "The input feature of the CNN is a sequence of words in the document. We use similar document preprocessing and also similar dictionary as in the previous approach. The words are then represented by the indexes into the dictionary.\nThe first important issue of this network for document classification is variable length of documents. It is usually solved by setting a fixed value and longer documents are shortened while shorter ones must be padded to ensure exactly the same length. The words that are not in the dictionary are assigned to a reserved index and the padding has also a reserved index.\nThe architecture of our network is motivated by Kim in [14]. However, we use just one size of the convolutional kernel and not the combination of several sizes. Our kernels have only 1 dimension (1D) while Kim have used larger 2 dimensional kernels. This is mainly due to our preliminary experiments where the simple 1 dimensional kernels gave better results than the larger ones.\nThe input of our network is a vector of word indexes of the length L where L is the number of words used for document representation. The second layer is an embedding layer which represents each input word as a vector of a given length. The document is thus represented as a matrix with L rows and EMB columns where EMB is the length of embedding vectors. The third layer is the convolutional one. We use NC convolution kernels of the size K \u00d7 1 which means we do 1D convolution over one position in the embedding vector over K input words. The following layer performs max pooling over the length L \u2212K + 1 resulting in NC 1 \u00d7 EMB vectors. The output of this layer is then flattened and connected with the output layer containing 37 nodes.\n2 We have also experimented with an MLP with one hidden layer with lower accuracy. 3 This configuration was set experimentally.\nThe output of the network is then thresholded to get the final results. The values greater than a given threshold indicate the labels that are assigned to the classified document. The architecture of the network is depicted in Figure 1."}, {"heading": "4 Experiments", "text": "In this section we first describe the Czech document corpus that we used for evaluation of our methods. After that we describe the performed experiments and the final results. The results are compared with previously published results on the Czech document corpus."}, {"heading": "4.1 Tools and Corpus", "text": "For implementation of all neural-nets we used Keras tool-kit [23] which is based on the Theano deep learning library [24]. It has been chosen mainly because of good performance and our previous experience with this tool. All experiments were computed on GPU to achieve reasonable computation times.\nAs already stated, the results of this work shall be used by the C\u030cTK. Therefore, for the following experiments we used the Czech text documents provided by the C\u030cTK. This corpus contains 2,974,040 words belonging to 11,955 documents. The documents are annotated from a set of 60 categories out of which we used 37 most frequent ones. The category reduction was done to allow comparison with previously reported results on this corpus where the same set of 37 categories was used. Figure 2 illustrates the distribution of the documents depending on the number of labels. Figure 3 shows the\ndistribution of the document lengths (in word tokens). This corpus is freely available for research purposes at http://home.zcu.cz/\u02dcpkral/sw/.\nWe use the five-folds cross validation procedure for all following experiments, where 20% of the corpus is reserved for testing and the remaining part for training of our models. For evaluation of the document classification accuracy, we use the standard F-measure (F1) metric [25]. The confidence interval of the experimental results is 0.6% at a confidence level of 0.95 [26]."}, {"heading": "4.2 Experimental Results", "text": "FDNN As a first experiment, we would like to validate the proposition of thresholding applied to the output layer of the FDNN. For this task we use the Receiver Operating Characteristic (ROC) curve which clearly shows the relationship between the true positive and the false positive rate for different values of the acceptance threshold. We use 20,000 most common words to create the dictionary. The ROC curve is depicted in Figure 4. According to the shape of this curve we can conclude that the proposed approach is suitable for multi-label document classification.\nIn the second experiment we would like to identify the optimal activation function of the nodes in the output layer. Two functions (sigmoid and softmax) are compared and evaluated. We have evaluated the threshold values in interval [0; 1], however only the best classification scores are depicted (see Table 1, best threshold values in brackets). This table shows that the softmax gives better results. Based on these results, we will further use this activation function and the threshold is set to 0.11.\nThe third experiment studies the influence of the dictionary size on the performance of the FDNN. Table 2 shows the dependency of F-measure on the word number in the dictionary. This table shows that the previously chosen 20,000 words is a reasonable choice and further increasing the number does not bring any significant improvement.\nCNN In all experiments performed with the CNN we use the same dictionary size (20,000 words) as in the case of FDNN to allow a straightforward comparison of the results. According to the analysis of our corpus we estimate that a suitable vector size for document representation is 400 words. As well as for the FDNN we first compute the ROC curve to validate the proposition of thresholding in the output. Figure 5 clearly shows that this approach is suitable for our task.\nAs a second experiment we identify an optimal activation function of neurons in the output layer. We compare the softmax and sigmoid functions. The achieved F-measures are depicted in Table 3. It is clearly visible that in this case the sigmoid function performs better. We will thus use the sigmoid activation function and the threshold will be set to 0.1 for all further experiments with CNN.\nIn this experiment, we will show the impact of the number of convolutional kernels in our network on the classification score. 400 words are used for document representation (L = 400) and the embedding vector size is 200. This experiment shows (see Table 4) that this parameter influences the classification score only very slightly (\u2206F1 \u223c +1%). All values from interval [20; 64] are suitable for our goal and therefore we chose the value of 40 for further experimentation.\nThe following experiment shows the dependency of F-measure on the size of convolutional kernels. We use 40 kernels and the size of the kernel varies from 2 to 40. The size of the kernels can be interpreted as the length of word sequences that the CNN works with. Figure 6 shows that the results are comparable and as a good compromise we chose the size of 16 for the following experiments.\nFinally, we tested our network with different numbers of input words and with varying size of the embedding vectors. Table 5 shows the achieved results with several combinations of these parameters. We can conclude that the 400 words that we chose at the beginning was a reasonable choice. However, it is beneficial to use longer embedding vectors. It must be noted that the further increasing of the embedding size has a strong impact on the computation time and might be not practical for real-world applications.\nSummary of the Results Table 6 compares the results of our approaches with another efficient method [21]. The results show that both proposed approaches significantly outperform this baseline approach that uses several features with ME classifier."}, {"heading": "5 Conclusions and Future Work", "text": "In this paper, we have used two different neural nets for multi-label document classification of Czech text documents. Several experiments were realized to set optimal network topologies and parameters. An important contribution is the evaluation of the\nperformance of neural networks using simple features. Therefore we have used the BoW representation for the FDNN and sequence of word indexes for the CNN as the inputs. Based on these experiments we can conclude:\n\u2013 the two proposed network topologies together with thresholding of the output are efficient for multi-label classification task \u2013 softmax activation function is better for FDNN, while sigmoid activation function gives better results for CNN \u2013 CNN outperforms FDNN only very slightly (\u2206 F1 \u223c +0.6%)\n\u2013 the most important is the fact that both neural nets with only basic pre-processing and without any parametrization significantly improve the baseline maximum entropy method with a rich set of parameters (\u2206 F1 \u223c +4%)\nBased on these results, we want to integrate CNN into our experimental document classification system.\nIn this paper, we have used relatively simple convolution neural network. Therefore, our first perspective consists in designing a more complicated CNN architecture. According to the literature, we assume that more layers in this network will have a positive impact on the classification score. Our embedding layer was also not initialized by some pre-trained semantic vectors (e.g. word2vec or GloVe). Another perspective thus consists in initializing of the embedding CNN layer with pre-trained vectors."}, {"heading": "Acknowledgements", "text": "This work has been supported by the project LO1506 of the Czech Ministry of Education, Youth and Sports. We also would like to thank Czech New Agency (C\u030cTK) for support and for providing the data."}], "references": [{"title": "A comparative study on feature selection in text categorization", "author": ["Y. Yang", "J.O. Pedersen"], "venue": "Proceedings of the Fourteenth International Conference on Machine Learning. ICML \u201997, San Francisco, CA, USA, Morgan Kaufmann Publishers Inc.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Multiple sets of features for automatic genre classification of web documents", "author": ["C.S. Lim", "K.J. Lee", "G.C. Kim"], "venue": "Information Processing and Management 41", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research 12", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A comparison between multi-layer perceptrons and convolutional neural networks for text image super-resolution", "author": ["C. Peyrard", "F. Mamalet", "C. Garcia"], "venue": "International Conference on Computer Vision Theory and Applications.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Inducing features of random fields", "author": ["S. Della Pietra", "V. Della Pietra", "J. Lafferty"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 19", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Optimizing text classification through efficient feature selection based on quality metric", "author": ["J.C. Lamirel", "P. Cuxac", "A.S. Chivukula", "K. Hajlaoui"], "venue": "Journal of Intelligent Information Systems", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Using syntactic information in document filtering: A comparative study of part-of-speech tagging and supertagging", "author": ["R. Chandrasekar", "B. Srinivas"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora", "author": ["D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1. EMNLP \u201909, Stroudsburg, PA, USA, Association for Computational Linguistics", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Partially labeled topic models for interpretable text mining", "author": ["D. Ramage", "C.D. Manning", "S. Dumais"], "venue": "Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. KDD \u201911, New York, NY, USA, ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Pca document reconstruction for email classification", "author": ["J.C. Gomez", "M.F. Moens"], "venue": "Computer Statistics and Data Analysis 56", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "A multi-layer text classification framework based on two-level representation model", "author": ["J. Yun", "L. Jing", "Y.J.", "H. Huang"], "venue": "Expert Systems with Applications 39", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Text understanding from scratch", "author": ["X. Zhang", "Y. LeCun"], "venue": "arXiv preprint arXiv:1502.01710", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Proceedings of Workshop at ICLR.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "A tutorial survey of architectures, algorithms, and applications for deep learning", "author": ["L. Deng"], "venue": "APSIPA Transactions on Signal and Information Processing 3", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "One-class document classification via neural networks", "author": ["L. Manevitz", "M. Yousef"], "venue": "Neurocomputing 70", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Multilabel neural networks with applications to functional genomics and text categorization", "author": ["M.L. Zhang", "Z.H. Zhou"], "venue": "Knowledge and Data Engineering, IEEE Transactions on 18", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Evaluation of the Document Classification Approaches", "author": ["M. Hrala", "P. Kr\u00e1l"], "venue": "8th International Conference on Computer Recognition Systems (CORES 2013), Milkow, Poland, Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-label document classification in Czech", "author": ["M. Hrala", "P. Kral"], "venue": "16th International conference on Text, Speech and Dialogue (TSD 2013), Pilsen, Czech Republic, Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Novel unsupervised features for Czech multi-label document classification", "author": ["T. Brychc\u0131\u0301n", "P. Kr\u00e1l"], "venue": "13th Mexican International Conference on Artificial Intelligence (MICAI 2014), Tuxtla Gutierrez, Chiapas, Mexic, Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-label classification: An overview", "author": ["G. Tsoumakas", "I. Katakis"], "venue": "International Journal of Data Warehousing and Mining (IJDWM) 3", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for scientific computing conference (SciPy). Volume 4., Austin, TX", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluation: From precision, recall and f-measure to roc., informedness, markedness & correlation", "author": ["D. Powers"], "venue": "Journal of Machine Learning Technologies", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Numerical recipes in C", "author": ["W.H. Press", "S.A. Teukolsky", "W.T. Vetterling", "B.P. Flannery"], "venue": "Volume 2. Citeseer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "The modern approaches usually use several pre-processing tasks: feature selection/reduction [1]; precise document representation (e.", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": ") [2] to reduce the feature space with minimal negative impact on classification accuracy.", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": "This fact is particularly evident in image processing [3], however it was further showed that they are also superior in Natural Language Processing (NLP) including Part-Of-Speech (POS) tagging, chunking, named entity recognition or semantic role labelling [4].", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "This fact is particularly evident in image processing [3], however it was further showed that they are also superior in Natural Language Processing (NLP) including Part-Of-Speech (POS) tagging, chunking, named entity recognition or semantic role labelling [4].", "startOffset": 256, "endOffset": 259}, {"referenceID": 4, "context": "Note that we expect better performance of the CNNs as shown for instance in the OCR task [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Several classification methods have been successfully used [6], for instance Bayesian classifiers, Maximum Entropy (ME), Support Vector Machines (SVMs), etc.", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "Numerous feature selection/reduction approaches have been introduced [1,7] to solve this problem.", "startOffset": 69, "endOffset": 74}, {"referenceID": 6, "context": "Numerous feature selection/reduction approaches have been introduced [1,7] to solve this problem.", "startOffset": 69, "endOffset": 74}, {"referenceID": 1, "context": "using lexical and syntactic features as shown in [2].", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "further show in [8] that it is beneficial to use POS-tag filtration in order to represent a document more accurately.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "More recently, some interesting approaches based on Latent Dirichlet Allocation (LLDA) [9] have been introduced.", "startOffset": 87, "endOffset": 90}, {"referenceID": 9, "context": "Another method exploits partial labels to discover latent topics [10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "Principal Component Analysis [11] incorporating semantic concepts [12] has also been used for the document classification.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "Principal Component Analysis [11] incorporating semantic concepts [12] has also been used for the document classification.", "startOffset": 66, "endOffset": 70}, {"referenceID": 3, "context": "Recently, \u201cdeep\u201d Neural Nets (NN) have shown their superior performance in many natural language processing tasks including POS tagging, chunking, named entity recognition and semantic role labelling [4] without any parametrization.", "startOffset": 200, "endOffset": 203}, {"referenceID": 12, "context": "For instance, the authors of [13] propose two Convolutional Neural Nets (CNN) for ontology classification, sentiment analysis and single-label document classification.", "startOffset": 29, "endOffset": 33}, {"referenceID": 13, "context": "Another interesting work [14] uses in the first layer (i.", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "lookup table) pre-trained vectors from word2vec [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "For additional information about architectures, algorithms, and applications of deep learning, please refer the survey [16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 16, "context": "show in [17] that their simple feed-forward neural network with three layers (20 inputs, 6 neurons in hidden layer and 10 neurons in the output layer, i.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "Traditional multi-layer neural networks were also used for multi-label document classification in [18].", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "use in [19] lemmatization and Part-Of-Speech (POS) filtering for a precise representation of Czech documents.", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": "In [20], three different multi-label classification approaches are compared and evaluated.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Another recent work proposes novel features based on the unsupervised machine learning [21].", "startOffset": 87, "endOffset": 91}, {"referenceID": 20, "context": "[21] and is composed of words, stems and features created by S-LDA and COALS.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "in [22].", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "The architecture of our network is motivated by Kim in [14].", "startOffset": 55, "endOffset": 59}, {"referenceID": 22, "context": "For implementation of all neural-nets we used Keras tool-kit [23] which is based on the Theano deep learning library [24].", "startOffset": 61, "endOffset": 65}, {"referenceID": 23, "context": "For implementation of all neural-nets we used Keras tool-kit [23] which is based on the Theano deep learning library [24].", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "For evaluation of the document classification accuracy, we use the standard F-measure (F1) metric [25].", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "95 [26].", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Summary of the Results Table 6 compares the results of our approaches with another efficient method [21].", "startOffset": 100, "endOffset": 104}, {"referenceID": 20, "context": "[21] 89.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "This paper is focused on automatic multi-label document classification of Czech text documents. The current approaches usually use some preprocessing which can have negative impact (loss of information, additional implementation work, etc). Therefore, we would like to omit it and use deep neural networks that learn from simple features. This choice was motivated by their successful usage in many other machine learning fields. Two different networks are compared: the first one is a standard multi-layer perceptron, while the second one is a popular convolutional network. The experiments on a Czech newspaper corpus show that both networks significantly outperform baseline method which uses a rich set of features with maximum entropy classifier. We have also shown that convolutional network gives the best results.", "creator": "gnuplot 4.6 patchlevel 6"}}}