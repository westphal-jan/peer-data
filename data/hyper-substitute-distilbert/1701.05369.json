{"id": "1701.05369", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jan-2017", "title": "Variational Dropout Sparsifies Deep Neural Networks", "abstract": "we has recently proposed variational correlation technique which represent an elegant comparative challenge to implementation. alternatives compare variational probability to the case when statistical result reaches unknown : show algorithms it also approximate found by optimizing evidence variational lower bound. proponents found that as is possible will discover and find individual dropout scores versus each data concerning dnn. interestingly such complexity leads to extremely sparse comparisons both in mathematical - formulated numerical experimental models. reliability effect is similar however automatic relevance determination ( ard ) effect applying empirical analyses when has a number of advantages. practitioners imagine up to 128 fold compression of popular algorithm satisfying a large issue of accuracy providing additional evidence linking the fact that modern deep projects are very brittle.", "histories": [["v1", "Thu, 19 Jan 2017 10:44:55 GMT  (171kb,D)", "https://arxiv.org/abs/1701.05369v1", null], ["v2", "Mon, 27 Feb 2017 20:43:27 GMT  (90kb,D)", "http://arxiv.org/abs/1701.05369v2", null], ["v3", "Tue, 13 Jun 2017 11:01:55 GMT  (93kb,D)", "http://arxiv.org/abs/1701.05369v3", "Published in ICML 2017"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["dmitry molchanov", "arsenii ashukha", "dmitry vetrov"], "accepted": true, "id": "1701.05369"}, "pdf": {"name": "1701.05369.pdf", "metadata": {"source": "META", "title": "Variational Dropout Sparsifies Deep Neural Networks", "authors": ["Dmitry Molchanov", "Arsenii Ashukha", "Dmitry Vetrov"], "emails": ["<dmitry.molchanov@skolkovotech.ru>,", "<ars.ashuha@gmail.com>,", "<vetrovd@yandex.ru>."], "sections": [{"heading": "1. Introduction", "text": "Deep neural networks (DNNs) are a widely popular family of models which is currently state-of-the-art in many important problems (Szegedy et al., 2016; Silver et al., 2016). However, DNNs often have many more parameters than the number of the training instances. This makes them prone to overfitting (Hinton et al., 2012; Zhang et al., 2016) and necessitates using regularization. A commonly used regularizer is Binary Dropout (Hinton et al., 2012) that prevents co-adaptation of neurons by randomly dropping them during training. An equally effective alternative is Gaussian Dropout (Srivastava et al., 2014) that multiplies the outputs of the neurons by Gaussian random noise.\nDropout requires specifying the dropout rates which are the\n*Equal contribution 1Yandex, Russia 2Skolkovo Institute of Science and Technology, Skolkovo Innovation Center, Moscow, Russia 3National Research University Higher School of Economics, Moscow, Russia 4Moscow Institute of Physics and Technology, Moscow, Russia. Correspondence to: Dmitry Molchanov <dmitry.molchanov@skolkovotech.ru>, Arsenii Ashukha <ars.ashuha@gmail.com>, Dmitry Vetrov <vetrovd@yandex.ru>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nprobabilities of dropping a neuron. The dropout rates are typically optimized using grid search. To avoid the exponential complexity of optimizing multiple hyperparameters, the dropout rates are usually shared for all layers. Recently it was shown that dropout can be seen as a special case of Bayesian regularization (Gal & Ghahramani, 2015; Kingma et al., 2015). It is an important theoretical result that justifies dropout and at the same time allows us to tune individual dropout rates for each weight, neuron or layer in a Bayesian way.\nInstead of injecting noise we can regularize a model by reducing the number of its parameters. This technique is especially attractive in the case of deep neural networks. Modern neural networks contain hundreds of millions of parameters (Szegedy et al., 2015; He et al., 2015) and require a lot of computational and memory resources. It restricts us from using deep neural networks when those resources are limited. Inducing sparsity during training of DNNs leads to regularization, compression, and acceleration of the resulting model (Han et al., 2015a; Scardapane et al., 2016).\nSparse Bayesian Learning (Tipping, 2001) provides a principled framework for training of sparse models without the manual tuning of hyperparameters. Unfortunately, this approach does not extend straightforwardly to DNNs. During past several years, a number of papers (Hoffman et al., 2013; Kingma & Welling, 2013; Rezende et al., 2014) on scalable variational inference have appeared. These techniques make it possible to train Bayesian Deep Neural Networks using stochastic optimization and provide us an opportunity to transfer Bayesian regularization techniques from simple models to DNNs.\nIn this paper, we study Variational Dropout (Kingma et al., 2015) in the case when each weight of a model has its individual dropout rate. We propose Sparse Variational Dropout that extends Variational Dropout to all possible values of dropout rates and leads to a sparse solution. To achieve this goal, we provide a new approximation of the KL-divergence term in Variational Dropout objective that is tight on the full domain. We also propose a way to greatly reduce the variance of the stochastic gradient estimator and show that it leads to a much faster convergence and a better value of the objective function. We show theoretically that\nar X\niv :1\n70 1.\n05 36\n9v 3\n[ st\nat .M\nL ]\n1 3\nJu n\n20 17\nSparse Variational Dropout applied to linear models can lead to a sparse solution. Like classical Sparse Bayesian models, our method provides the Automatic Relevance Determination effect, but overcomes certain disadvantages of empirical Bayes.\nOur experiments show that Sparse Variational Dropout leads to a high level of sparsity in fully-connected and convolutional layers of Deep Neural Networks. Our method achieves a state-of-the-art sparsity level on LeNet architectures and scales on larger networks like VGG with negligible performance drop. Also we show that our method fails to overfit on randomly labeled data unlike Binary Dropout networks."}, {"heading": "2. Related Work", "text": "Deep Neural Nets are prone to overfitting and regularization is used to address this problem. Several successful techniques have been proposed for DNN regularization, among them are Dropout (Srivastava et al., 2014), DropConnect (Wan et al., 2013), Max Norm Constraint (Srivastava et al., 2014), Batch Normalization (Ioffe & Szegedy, 2015), etc.\nAnother way to regularize deep model is to reduce the number of parameters. One possible approach is to use tensor decompositions (Novikov et al., 2015; Garipov et al., 2016). Another approach is to induce sparsity into weight matrices. Most recent works on sparse neural networks use pruning (Han et al., 2015b), elastic net regularization (Lebedev & Lempitsky, 2015; Liu et al., 2015; Scardapane et al., 2016; Wen et al., 2016) or composite techniques (Han et al., 2015a; Guo et al., 2016; Ullrich et al., 2017).\nSparsity can also be obtained by using the Sparse Bayesian Learning framework (Tipping, 2001). Automatic Relevance Determination was introduced in (Neal, 1996; MacKay et al., 1994), where small neural networks were trained with ARD regularization on the input layer. This approach was later studied on linear models like the Relevance Vector Machine (Tipping, 2001) and other kernel methods (Van Gestel et al., 2001). In the Relevance Tagging Machine model (Molchanov et al., 2015) Beta prior distribution is used to obtain the ARD effect in a similar setting.\nRecent works on Bayesian DNNs (Kingma & Welling, 2013; Rezende et al., 2014; Scardapane et al., 2016) provide different ways to train deep models with a huge number of parameters in a Bayesian way. These techniques can be applied to improve latent variables models (Kingma & Welling, 2013), to prevent overfitting and to obtain model uncertainty (Gal & Ghahramani, 2015). Recently several works on efficient training of Sparse Bayesian Models have appeared (Challis & Barber, 2013; Titsias & La\u0301zaro-\nGredilla, 2014). Soft Weights Sharing (Ullrich et al., 2017) uses an approach, similar to Sparse Bayesian Learning framework, to obtain a sparse and quantized Bayesian Deep Neural Network, but utilizes a more flexible family of prior distributions.\nVariational Dropout (Kingma et al., 2015) is an elegant interpretation of Gaussian Dropout as a special case of Bayesian regularization. This technique allows us to tune dropout rate and can, in theory, be used to set individual dropout rates for each layer, neuron or even weight. However, that paper uses a limited family for posterior approximation that does not allow for ARD effect. Other Bayesian interpretations of dropout training have also appeared during past several years (Maeda, 2014; Gal & Ghahramani, 2015; Srinivas & Babu, 2016). Generalized Dropout (Srinivas & Babu, 2016) provides a way to tune individual dropout rates for neurons, but uses a biased gradient estimator. Also, the posterior distribution is modelled by a delta function, so the resulting neural network is effectively not Bayesian. Variational Spike-and-Slab Neural Networks (Louizos, 2015) is yet another Bayesian interpretation of Binary Dropout that allows for tuning of individual dropout rates and also leads to a sparse solution. Unfortunately, this procedure does not scale well with model width and depth."}, {"heading": "3. Preliminaries", "text": "We begin by describing the Bayesian Inference and Stochastic Variational Inference frameworks. Then we describe Variational Dropout, a recently proposed Bayesian regularization technique (Kingma et al., 2015)."}, {"heading": "3.1. Bayesian Inference", "text": "Consider a dataset D which is constructed from N pairs of objects (xn, yn)Nn=1. Our goal is to tune the parameters w of a model p(y |x,w) that predicts y given x and w. In Bayesian Learning we usually have some prior knowledge about weights w, which is expressed in terms of a prior distribution p(w). After data D arrives, this prior distribution is transformed into a posterior distribution p(w | D) = p(D |w)p(w)/p(D). This process is called Bayesian Inference. Computing posterior distribution using the Bayes rule usually involves computation of intractable multidimensional integrals, so we need to use approximation techniques.\nOne of such techniques is Variational Inference. In this approach the posterior distribution p(w | D) is approximated by a parametric distribution q\u03c6(w). The quality of this approximation is measured in terms of the Kullback-Leibler divergence DKL(q\u03c6(w) \u2016 p(w | D)). The optimal value of variational parameters \u03c6 can be found by maximization of\nthe variational lower bound:\nL(\u03c6) = LD(\u03c6)\u2212DKL(q\u03c6(w) \u2016 p(w))\u2192 max \u03c6\u2208\u03a6\n(1)\nLD(\u03c6) = N\u2211 n=1 Eq\u03c6(w)[log p(yn |xn, w)] (2)\nIt consists of two parts, the expected log-likelihood LD(\u03c6) and the KL-divergence DKL(q\u03c6(w) \u2016 p(w)), which acts as a regularization term."}, {"heading": "3.2. Stochastic Variational Inference", "text": "In the case of complex models expectations in (1) and (2) are intractable. Therefore the variational lower bound (1) and its gradients can not be computed exactly. However, it is still possible to estimate them using sampling and optimize the variational lower bound using stochastic optimization.\nWe follow (Kingma & Welling, 2013) and use the Reparameterization Trick to obtain an unbiased differentiable minibatch-based Monte Carlo estimator of the expected log-likelihood (3). The main idea is to represent the parametric noise q\u03c6(w) as a deterministic differentiable function w = f(\u03c6, ) of a non-parametric noise \u223c p( ). This trick allows us to obtain an unbiased estimate of \u2207\u03c6LD(q\u03c6). Here we denote objects from a mini-batch as (x\u0303m, y\u0303m) M m=1.\nL(\u03c6)'LSGVB(\u03c6)=LSGVBD (\u03c6)\u2212DKL(q\u03c6(w)\u2016p(w)) (3)\nLD(\u03c6)'LSGVBD (\u03c6)= N\nM M\u2211 m=1 log p(y\u0303m|x\u0303m, f(\u03c6, m)) (4)\n\u2207\u03c6LD(\u03c6)' N\nM M\u2211 m=1 \u2207\u03c6 log p(y\u0303m|x\u0303m, f(\u03c6, m)) (5)\nThe Local Reparameterization Trick is another technique that reduces the variance of this gradient estimator even further (Kingma et al., 2015). The idea is to sample separate weight matrices for each data-point inside mini-batch. It is computationally hard to do it straight-forwardly, but it can be done efficiently by moving the noise from weights to activations (Wang & Manning, 2013; Kingma et al., 2015)."}, {"heading": "3.3. Variational Dropout", "text": "In this section we consider a single fully-connected layer with I input neurons and O output neurons before a nonlinearity. We denote an output matrix as BM\u00d7O, input matrix as AM\u00d7I and a weight matrix as W I\u00d7O. We index the elements of these matrices as bmj , ami and wij respectively. Then B = AW .\nDropout is one of the most popular regularization methods for deep neural networks. It injects a multiplicative random\nnoise \u039e to the layer input A at each iteration of training procedure (Hinton et al., 2012).\nB = (A \u039e)W, with \u03bemi \u223c p(\u03be) (6)\nThe original version of dropout, so-called Bernoulli or Binary Dropout, was presented with \u03bemi \u223c Bernoulli(1\u2212 p) (Hinton et al., 2012). It means that each element of the input matrix is put to zero with probability p, also known as a dropout rate. Later the same authors reported that Gaussian Dropout with continuous noise \u03bemi \u223c N (1, \u03b1 = p 1\u2212p ) works as well and is similar to Binary Dropout with dropout rate p (Srivastava et al., 2014). It is beneficial to use continuous noise instead of discrete one because multiplying the inputs by a Gaussian noise is equivalent to putting Gaussian noise on the weights. This procedure can be used to obtain a posterior distribution over the model\u2019s weights (Wang & Manning, 2013; Kingma et al., 2015). That is, putting multiplicative Gaussian noise \u03beij \u223c N (1, \u03b1) on a weight wij is equivalent to sampling of wij from q(wij | \u03b8ij , \u03b1) = N (wij | \u03b8ij , \u03b1\u03b82ij). Now wij becomes a random variable parametrized by \u03b8ij .\nwij = \u03b8ij\u03beij = \u03b8ij(1 + \u221a \u03b1 ij) \u223c N (wij | \u03b8ij , \u03b1\u03b82ij)\nij \u223c N (0, 1) (7)\nGaussian Dropout training is equivalent to stochastic optimization of the expected log likelihood (2) in the case when we use the reparameterization trick and draw a single sample W \u223c q(W | \u03b8, \u03b1) per minibatch to estimate the expectation. Variational Dropout extends this technique and explicitly uses q(W | \u03b8, \u03b1) as an approximate posterior distribution for a model with a special prior on the weights. The parameters \u03b8 and \u03b1 of the distribution q(W | \u03b8, \u03b1) are tuned via stochastic variational inference, i.e. \u03c6 = (\u03b8, \u03b1) are the variational parameters, as denoted in Section 3.2. The prior distribution p(W ) is chosen to be improper logscale uniform to make the Variational Dropout with fixed \u03b1 equivalent to Gaussian Dropout (Kingma et al., 2015).\np(log |wij |) = const \u21d4 p(|wij |) \u221d 1\n|wij | (8)\nIn this model, it is the only prior distribution that makes variational inference consistent with Gaussian Dropout (Kingma et al., 2015). When parameter \u03b1 is fixed, the DKL(q(W | \u03b8, \u03b1) \u2016 p(W )) term in the variational lower bound (1) does not depend on \u03b8 (Kingma et al., 2015). Maximization of the variational lower bound (1) then becomes equivalent to maximization of the expected loglikelihood (2) with fixed parameter \u03b1. It means that Gaussian Dropout training is exactly equivalent to Variational Dropout with fixed \u03b1. However, Variational Dropout provides a way to train dropout rate \u03b1 by optimizing the variational lower bound (1). Interestingly, dropout rate \u03b1 now\nbecomes a variational parameter and not a hyperparameter. In theory, it allows us to train individual dropout rates \u03b1ij for each layer, neuron or even weight (Kingma et al., 2015). However, no experimental results concerning the training of individual dropout rates were reported in the original paper. Also, the approximate posterior family was manually restricted to the case \u03b1 \u2264 1."}, {"heading": "4. Sparse Variational Dropout", "text": "In the original paper, authors reported difficulties in training the model with large values of dropout rates \u03b1 (Kingma et al., 2015) and only considered the case of \u03b1 \u2264 1, which corresponds to a binary dropout rate p \u2264 0.5. However, the case of large \u03b1ij is very exciting (here we mean separate \u03b1ij per weight). High dropout rate \u03b1ij \u2192 +\u221e corresponds to a binary dropout rate that approaches p = 1. It effectively means that the corresponding weight or neuron is always ignored and can be removed from the model. In this work, we consider the case of individual \u03b1ij for each weight of the model."}, {"heading": "4.1. Additive Noise Reparameterization", "text": "Training Neural Networks with Variational Dropout is difficult when dropout rates \u03b1ij are large because of a huge variance of stochastic gradients (Kingma et al., 2015). The cause of large gradient variance arises from multiplicative noise. To see it clearly, we can rewrite the gradient of LSGVB w.r.t. \u03b8ij as follows.\n\u2202LSGVB \u2202\u03b8ij = \u2202LSGVB \u2202wij \u00b7 \u2202wij \u2202\u03b8ij\n(9)\nIn the case of original parametrization (\u03b8, \u03b1) the second multiplier in (9) is very noisy if \u03b1ij is large.\nwij = \u03b8ij(1 + \u221a \u03b1ij \u00b7 ij),\n\u2202wij \u2202\u03b8ij = 1 + \u221a \u03b1ij \u00b7 ij ,\nij \u223c N (0, 1)\n(10)\nWe propose a trick that allows us to drastically reduce the variance of this term in the case when \u03b1ij is large. The idea is to replace the multiplicative noise term 1+\u221a\u03b1ij \u00b7 ij with an exactly equivalent additive noise term \u03c3ij \u00b7 ij , where \u03c32ij = \u03b1ij\u03b8 2 ij is treated as a new independent variable. After this trick we will optimize the variational lower bound w.r.t. (\u03b8, \u03c3). However, we will still use \u03b1 throughout the paper, as it has a nice interpretation as a dropout rate.\nwij = \u03b8ij(1 + \u221a \u03b1ij \u00b7 ij) = \u03b8ij + \u03c3ij \u00b7 ij\n\u2202wij \u2202\u03b8ij = 1, ij \u223c N (0, 1) (11)\nFrom (11) we can see that \u2202wij\u2202\u03b8ij now has no injected noise, but the distribution over wij \u223c q(wij | \u03b8ij , \u03c32ij) remains exactly the same. The objective function and the posterior approximating family are unaltered. The only thing that changed is the parametrization of the approximate posterior. However, the variance of a stochastic gradient is greatly reduced. Using this trick, we avoid the problem of large gradient variance and can train the model within the full range of \u03b1ij \u2208 (0,+\u221e).\nIt should be noted that the Local Reparametrization Trick does not depend on parametrization, so it can also be applied here to reduce the variance even further. In our experiments, we use both Additive Noise Reparameterization and the Local Reparameterization Trick. We provide the final expressions for the outputs of fully-connected and convolutional layers for our model in Section 4.4."}, {"heading": "4.2. Approximation of the KL Divergence", "text": "As the prior and the approximate posterior are fully factorized, the full KL-divergence term in the lower bound (1) can be decomposed into a sum:\nDKL(q(W | \u03b8, \u03b1)\u2016 p(W )) = = \u2211 ij DKL(q(wij | \u03b8ij , \u03b1ij) \u2016 p(wij)) (12)\nThe log-scale uniform prior distribution is an improper prior, so the KL divergence can only be calculated up to an additive constant C (Kingma et al., 2015).\n\u2212DKL(q(wij | \u03b8ij , \u03b1ij) \u2016 p(wij)) =\n= 1\n2 log\u03b1ij \u2212 E \u223cN (1,\u03b1ij) log | |+ C\n(13)\nIn the Variational Dropout model this term is intractable, as the expectation E \u223cN (1,\u03b1ij) log | | in (13) cannot be computed analytically (Kingma et al., 2015). However, this\nterm can be sampled and then approximated. Two different approximations were provided in the original paper, however they are accurate only for small values of the dropout rate \u03b1 (\u03b1 \u2264 1). We propose another approximation (14) that is tight for all values of alpha. Here \u03c3(\u00b7) denotes the sigmoid function. Different approximations and the true value of \u2212DKL are presented in Fig. 1. Original \u2212DKL was obtained by averaging over 107 samples of with less than 2\u00d7 10\u22123 variance of the estimation.\n\u2212DKL(q(wij | \u03b8ij , \u03b1ij) \u2016 p(wij)) \u2248 \u2248 k1\u03c3(k2 + k3 log\u03b1ij))\u2212 0.5 log(1 + \u03b1\u22121ij ) + C k1 = 0.63576 k2 = 1.87320 k3 = 1.48695 (14)\nWe used the following intuition to obtain this formula. The negative KL-divergence goes to a constant as log\u03b1ij goes to infinity, and tends to 0.5 log\u03b1ij as log\u03b1ij goes to minus infinity. We model this behaviour with \u22120.5 log(1 +\u03b1\u22121ij ). We found that the remainder \u2212DKL + 0.5 log(1 + \u03b1\u22121ij ) looks very similar to a sigmoid function of log\u03b1ij , so we fit its linear transformation k1\u03c3(k2 +k3 log\u03b1ij) to this curve. We observe that this approximation is extremely accurate (less than 0.009 maximum absolute deviation on the full range of log\u03b1ij \u2208 (\u2212\u221e,+\u221e); the original approximation (Kingma et al., 2015) has 0.04 maximum absolute deviation with log\u03b1ij \u2208 (\u2212\u221e, 0]).\nOne should notice that as \u03b1 approaches infinity, the KLdivergence approaches a constant. As in this model the KL-divergence is defined up to an additive constant, it is convenient to choose C = \u2212k1 so that the KL-divergence goes to zero when \u03b1 goes to infinity. It allows us to compare values of LSGVB for neural networks of different sizes."}, {"heading": "4.3. Sparsity", "text": "From the Fig. 1 one can see that \u2212DKL term increases with the growth of \u03b1. It means that this regularization term favors large values of \u03b1.\nThe case of \u03b1ij \u2192 \u221e corresponds to a Binary Dropout rate pij \u2192 1 (recall \u03b1 = p1\u2212p ). Intuitively it means that the corresponding weight is almost always dropped from the model. Therefore its value does not influence the model during the training phase and is put to zero during the testing phase.\nWe can also look at this situation from another angle. Infinitely large \u03b1ij corresponds to infinitely large multiplicative noise in wij . It means that the value of this weight will be completely random and its magnitude will be unbounded. It will corrupt the model prediction and decrease the expected log likelihood. Therefore it is beneficial to put the corresponding weight \u03b8ij to zero in such a way that \u03b1ij\u03b8 2 ij goes to zero as well. It means that q(wij | \u03b8ij , \u03b1ij)\nis effectively a delta function, centered at zero \u03b4(wij).\n\u03b8ij \u2192 0, \u03b1ij\u03b82ij \u2192 0 \u21d3\nq(wij | \u03b8ij , \u03b1ij)\u2192 N (wij | 0, 0) = \u03b4(wij) (15)\nIn the case of linear regression this fact can be shown analytically. We denote a data matrix as XN\u00d7D and \u03b1, \u03b8 \u2208 RD. If \u03b1 is fixed, the optimal value of \u03b8 can also be obtained in a closed form.\n\u03b8 = (X>X + diag(X>X)diag(\u03b1))\u22121X>y (16)\nAssume that (X>X)ii 6= 0, so that i-th feature is not a constant zero. Then from (16) it follows that \u03b8i = \u0398(\u03b1\u22121i ) when \u03b1i \u2192 +\u221e, so both \u03b8i and \u03b1i\u03b82i tend to 0."}, {"heading": "4.4. Sparse Variational Dropout for Fully-Connected and Convolutional Layers", "text": "Finally we optimize the stochastic gradient variational lower bound (3) with our approximation of KL-divergence (14). We apply Sparse Variational Dropout to both convolutional and fully-connected layers. To reduce the variance of LSGVB we use a combination of the Local Reparameterization Trick and Additive Noise Reparameterization. In order to improve convergence, optimization is performed w.r.t. (\u03b8, log \u03c32).\nFor a fully connected layer we use the same notation as in Section 3.3. In this case, Sparse Variational Dropout with the Local Reparameterization Trick and Additive Noise Reparameterization can be computed as follows:\nbmj \u223c N (\u03b3mj , \u03b4mj)\n\u03b3mj = I\u2211 i=1 ami\u03b8ij , \u03b4mj = I\u2211 i=1 a2mi\u03c3 2 ij\n(17)\nNow consider a convolutional layer. Take a single input tensor AH\u00d7W\u00d7Cm , a single filter w h\u00d7w\u00d7C k and corresponding output matrix bH \u2032\u00d7W \u2032\nmk . This filter has corresponding variational parameters \u03b8h\u00d7w\u00d7Ck and \u03c3 h\u00d7w\u00d7C k . Note that in this case Am, \u03b8k and \u03c3k are tensors. Because of linearity of convolutional layers, it is possible to apply the Local Reparameterization Trick. Sparse Variational Dropout for convolutional layers then can be expressed in a way, similar to (17). Here we use (\u00b7)2 as an element-wise operation, \u2217 denotes the convolution operation, vec(\u00b7) denotes reshaping of a matrix/tensor into a vector.\nvec(bmk) \u223c N (\u03b3mk, \u03b4mk) \u03b3mk = vec(Am\u2217\u03b8k), \u03b4mk = diag(vec(A2m\u2217\u03c32k)) (18)\nThese formulae can be used for the implementation of Sparse Variational Dropout layers. Lasagne and PyTorch\nsource code of Sparse Variational Dropout layers is available at https://goo.gl/2D4tFW. Both forward and backward passes through Sparse VD layers take twice as much time as passes through original layers."}, {"heading": "4.5. Relation to RVM", "text": "The Relevance Vector Machine (RVM, (Tipping, 2001)) is a classical example of a Sparse Bayesian model. The RVM is essentially a Bayesian treatment of L2-regularized linear or logistic regression, where each weight has a separate regularization parameter \u03b1i. These parameters are tuned by empirical Bayes. During training, a large portion of parameters \u03b1i goes to infinity, and corresponding features are excluded from the model since those weights become zero. This effect is known as Automatic Relevance Determination (ARD) and is a popular way to construct sparse Bayesian models.\nEmpirical Bayes is a somewhat counter-intuitive procedure since we optimize prior distribution w.r.t. the observed data. Such trick has a risk of overfitting, and indeed it was reported in (Cawley, 2010). However, in our work the ARD-effect is achieved by straightforward variational inference rather than by empirical Bayes. Similarly to the RVM, in Sparse VD dropout rates \u03b1i are responsible for the ARD-effect. However, in Sparse VD \u03b1i are parameters of the approximate posterior distribution rather than parameters of the prior distribution. In our work, the prior distribution is fixed and does not have any parameters, and we tune \u03b1i to obtain a more accurate approximation of the posterior distribution p(w | D). Therefore there is no risk of additional overfitting from model selection unlike the case of empirical Bayes.\nThat said, despite this difference, the analytical solution for maximum a posteriori estimation is very similar for the RVM-regression\nwMAP = (X>X + diag(\u03b1))\u22121X>y (19)\nand Sparse Variational Dropout regression\n\u03b8 = (X>X + diag(X>X)diag(\u03b1))\u22121X>y (20)\nInterestingly, the expression for Binary Dropout-regularized linear regression is exactly the same as (20) if we substitute \u03b1i with pi1\u2212pi (Srivastava et al., 2014)."}, {"heading": "5. Experiments", "text": "We perform experiments on classification tasks and use different neural network architectures including architectures with a combination of batch normalization and dropout layers. We explore the relevance determination performance of our algorithm as well as the classification accuracy of the\nresulting sparse model. Our experiments show that Sparse Variational Dropout leads to extremely sparse models.\nIn order to make a Sparse Variational Dropout analog to an existing architecture, we only need to remove existing dropout layers and replace all dense and convolutional layers with their Sparse Variational Dropout counterparts as described in Section 4.4 and use LSGVB as the objective function. The value of the variational lower bound can be used to choose among several local optima."}, {"heading": "5.1. General Empirical Observations", "text": "We provide a general intuition about training of Sparse Bayesian DNNs using Sparse Variational Dropout.\nAs it is impossible for the weights to converge exactly to zero in a stochastic setting, we explicitly put weights with high corresponding dropout rates to 0 during testing. In our experiments with neural networks, we use the value log\u03b1 = 3 as a threshold. This value corresponds to a Binary Dropout rate p > 0.95. Unlike most other methods (Han et al., 2015b; Wen et al., 2016), this trick usually does not hurt the performance of our model. It means that Sparse VD does not require finetuning after thresholding.\nTraining our model from a random initialization is troublesome, as a lot of weights become pruned away early during training, before they could possibly learn a useful representation of the data. In this case we obtain a higher sparsity level, but also a high accuracy drop. The same problem is reported by (S\u00f8nderby et al., 2016) and is a common problem for Bayesian DNNs. One way to resolve this problem is to start from a pre-trained network. This trick provides a fair sparsity level with almost no drop of accuracy. Here by pre-training we mean training of the original architecture without Sparse Variational Dropout until full convergence.\nAnother way to approach this problem is to use warm-up, as described by (S\u00f8nderby et al., 2016). The idea is to rescale the KL-divergence term during training by a scalar term \u03b2t, individual for each training epoch. During the first epochs we used \u03b2t = 0, then increased \u03b2t linearly from 0 to 1 and after that used \u03b2t = 1. The final objective function remains the same, but the optimization trajectory becomes different. In some sense it is equivalent to choosing a better initial guess for the parameters.\nWe use the final value of the variational lower bound to choose the initialization strategy. We observe that the initialization does not matter much on simple models like LeNets, but in the case of more complex models like VGG, the difference is significant.\nOn most architectures we observe that the number of epochs required for convergence from a random initialization is roughly the same as for the original network. How-\never, we only need to make a several epochs (10-30) in order for our method to converge from a pre-trained network.\nWe train all networks using Adam (Kingma & Ba, 2014). When we start from a random initialization, we train for 200 epochs and linearly decay the learning rate from 10\u22124 to zero. When we start from a pre-trained model, we finetune for 10-30 epochs with learning rate 10\u22125."}, {"heading": "5.2. Variance Reduction", "text": "To see how Additive Noise Reparameterization reduces the variance, we compare it with the original parameterization. We used a fully-connected architecture with two layers with 1000 neurons each. Both models were trained with identical random initializations and with the same learning rate, equal to 10\u22124. We did not rescale the KL term during training. It is interesting that the original version of Variational Dropout with our approximation of KL-divergence and with no restriction on alphas also provides a sparse solution. However, our method has much better convergence rate and provides higher sparsity and a better value of the variational lower bound."}, {"heading": "5.3. LeNet-300-100 and LeNet5 on MNIST", "text": "We compare our method with other methods of training sparse neural networks on the MNIST dataset using a fullyconnected architecture LeNet-300-100 and a convolutional architecture LeNet-5-Caffe1. These networks were trained from a random initialization and without data augmentation. We consider pruning (Han et al., 2015b;a), Dynamic Network Surgery (Guo et al., 2016) and Soft Weight Sharing (Ullrich et al., 2017). In these architectures, our method achieves a state-of-the-art level of sparsity, while its accuracy is comparable to other methods. It should be noted\n1A modified version of LeNet5 from (LeCun et al., 1998). Caffe Model specification: https://goo.gl/4yI3dL\nthat we only consider the level of sparsity and not the final compression ratio."}, {"heading": "5.4. VGG-like on CIFAR-10 and CIFAR-100", "text": "To demonstrate that our method scales to large modern architectures, we apply it to a VGG-like network (Zagoruyko, 2015) adapted for the CIFAR-10 (Krizhevsky & Hinton, 2009) dataset. The network consists of 13 convolutional and two fully-connected layers, each layer followed by pre-activation batch normalization and Binary Dropout. We experiment with different sizes of this architecture by scaling the number of units in each network by k \u2208 {0.25, 0.5, 1.0, 1.5}. We use CIFAR-10 and CIFAR-100 for evaluation. The reported error of this architecture on the CIFAR-10 dataset with k = 1 is 7.55%. As no pretrained weights are available, we train our own network and achieve 7.3% error. Sparse VD also achieves 7.3% error for k = 1, but retains 48\u00d7 less weights.\nWe observe underfitting while training our model from a random initialization, so we pre-train the network with Binary Dropout and L2 regularization. It should be noted that most modern DNN compression techniques also can be applied only to pre-trained networks and work best with networks, trained with L2 regularization (Han et al., 2015b).\nOur method achieves over 65x sparsification on the CIFAR10 dataset with no accuracy drop and up to 41x sparsification on CIFAR-100 with a moderate accuracy drop."}, {"heading": "5.5. Random Labels", "text": "Recently is was shown that the CNNs are capable of memorizing the data even with random labeling (Zhang et al., 2016). The standard dropout as well as other regularization techniques did not prevent this behaviour. Following that work, we also experiment with the random labeling of data. We use a fully-connected network on the MNIST dataset and VGG-like networks on CIFAR-10. We put Bi-\nnary Dropout (BD) with dropout rate p = 0.5 on all fullyconnected layers of these networks. We observe that these architectures can fit a random labeling even with Binary Dropout. However, our model decides to drop every single weight and provide a constant prediction. It is still possible to make our model learn random labeling by initializing it with a network, pre-trained on this random labeling, and then finetuning it. However, the variational lower bound L(\u03b8, \u03b1) in this case is lower than in the case of 100% sparsity. These observations may mean that Sparse VD implicitly penalizes memorization and favors generalization. However, this still requires a more thorough investigation."}, {"heading": "6. Discussion", "text": "The \u201cOccam\u2019s razor\u201d principle states that unnecessarily complex should not be preferred to simpler ones (MacKay, 1992). Automatic Relevance Determination is effectively a Bayesian implementation of this principle that occurs in different cases. Previously, it was mostly studied in the case of factorized Gaussian prior in linear models, Gaussian Processes, etc. In the Relevance Tagging Machine model (Molchanov et al., 2015) the same effect was achieved using Beta distributions as a prior. Finally, in this work, the ARD-effect is reproduced in an entirely different setting. We consider a fixed prior and train the model using variational inference. In this case, the ARD effect is caused by the particular combination of the approximate posterior distribution family and prior distribution, and not by model selection. This way we can abandon the empirical Bayes approach that is known to overfit (Cawley, 2010).\nWe observed that if we allow Variational Dropout to drop irrelevant weights automatically, it ends up cutting most of the model weights. This result correlates with results of other works on training of sparse neural networks (Han et al., 2015a; Wen et al., 2016; Ullrich et al., 2017; Soravit Changpinyo, 2017). All these works can be viewed\nas a kind of regularization of neural networks, as they restrict the model complexity. Further investigation of such redundancy may lead to an understanding of generalization properties of DNNs and explain the phenomenon, observed by (Zhang et al., 2016). According to that paper, although modern DNNs generalize well in practice, they can also easily learn a random labeling of data. Interestingly, it is not the case for our model, as a network with zero weights has a higher value of objective than a trained network.\nIn this paper we study only the level of sparsity and do not report the actual network compression. However, our approach can be combined with other modern techniques of network compression, e.g. quantization and Huffman coding (Han et al., 2015a; Ullrich et al., 2017), as they use sparsification as an intermediate step. As our method provides a higher level of sparsity, we believe that it can improve these techniques even further. Another possible direction for future research is to find a way to obtain structured sparsity using our framework. As reported by (Wen et al., 2016), structured sparsity is crucial to acceleration of DNNs."}, {"heading": "Acknowledgements", "text": "We would like to thank Michael Figurnov, Ekaterina Lobacheva and Max Welling for valuable feedback. Dmitry Molchanov was supported by the Ministry of Education and Science of the Russian Federation (grant 14.756.31.0001), Arsenii Ashukha was supported by HSE International lab of Deep Learning and Bayesian Methods which is funded by the Russian Academic Excellence Project \u20195-100\u2019, Dmitry Vetrov was supported by the Russian Science Foundation grant 17-11-01027. We would also like to thank the Department of Algorithms and Theory of Programming, Faculty of Innovation and High Technology in Moscow Institute of Physics and Technology for the provided computational resources."}], "references": [{"title": "On over-fitting in model selection and subsequent selection bias in performance evaluation", "author": ["Cawley", "Nicola L.C. Talbot"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cawley and Talbot.,? \\Q2010\\E", "shortCiteRegEx": "Cawley and Talbot.", "year": 2010}, {"title": "Gaussian kullback-leibler approximate inference", "author": ["E Challis", "D. Barber"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Challis and Barber,? \\Q2013\\E", "shortCiteRegEx": "Challis and Barber", "year": 2013}, {"title": "Dropout as a bayesian approximation: Insights and applications", "author": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "In Deep Learning Workshop,", "citeRegEx": "Gal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gal et al\\.", "year": 2015}, {"title": "Ultimate tensorization: compressing convolutional and fc layers alike", "author": ["Garipov", "Timur", "Podoprikhin", "Dmitry", "Novikov", "Alexander", "Vetrov"], "venue": "arXiv preprint arXiv:1611.03214,", "citeRegEx": "Garipov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garipov et al\\.", "year": 2016}, {"title": "Dynamic network surgery for efficient dnns", "author": ["Guo", "Yiwen", "Yao", "Anbang", "Chen", "Yurong"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Guo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Stochastic variational inference", "author": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John William"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Learning multiple layers of features from tiny", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Fast convnets using group-wise brain damage", "author": ["Lebedev", "Vadim", "Lempitsky", "Victor"], "venue": "arXiv preprint arXiv:1506.02515,", "citeRegEx": "Lebedev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebedev et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Sparse convolutional neural networks", "author": ["Liu", "Baoyuan", "Wang", "Min", "Foroosh", "Hassan", "Tappen", "Marshall", "Pensky", "Marianna"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Bayesian interpolation", "author": ["MacKay", "David JC"], "venue": "Neural computation,", "citeRegEx": "MacKay and JC.,? \\Q1992\\E", "shortCiteRegEx": "MacKay and JC.", "year": 1992}, {"title": "Bayesian nonlinear modeling for the prediction competition", "author": ["MacKay", "David JC"], "venue": "ASHRAE transactions,", "citeRegEx": "MacKay and JC,? \\Q1994\\E", "shortCiteRegEx": "MacKay and JC", "year": 1994}, {"title": "A bayesian encourages dropout", "author": ["Maeda", "Shin-ichi"], "venue": "arXiv preprint arXiv:1412.7003,", "citeRegEx": "Maeda and Shin.ichi.,? \\Q2014\\E", "shortCiteRegEx": "Maeda and Shin.ichi.", "year": 2014}, {"title": "Relevance tagging machine", "author": ["Molchanov", "Dmitry", "Kondrashkin", "Vetrov"], "venue": "Machine Learning and Data Analysis,", "citeRegEx": "Molchanov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Molchanov et al\\.", "year": 2015}, {"title": "Bayesian learning for neural networks, volume 118", "author": ["Neal", "Radford M"], "venue": "Springer Science & Business Media,", "citeRegEx": "Neal and M.,? \\Q1996\\E", "shortCiteRegEx": "Neal and M.", "year": 1996}, {"title": "Tensorizing neural networks", "author": ["Novikov", "Alexander", "Podoprikhin", "Dmitrii", "Osokin", "Anton", "Vetrov", "Dmitry P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Novikov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Novikov et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1401.4082,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Group sparse regularization for deep neural networks", "author": ["Scardapane", "Simone", "Comminiello", "Danilo", "Hussain", "Amir", "Uncini", "Aurelio"], "venue": "arXiv preprint arXiv:1607.00485,", "citeRegEx": "Scardapane et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Scardapane et al\\.", "year": 2016}, {"title": "How to train deep variational autoencoders and probabilistic ladder networks", "author": ["S\u00f8nderby", "Casper Kaae", "Raiko", "Tapani", "Maal\u00f8e", "Lars", "S\u00f8ren Kaae", "Winther", "Ole"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning", "citeRegEx": "S\u00f8nderby et al\\.,? \\Q2016\\E", "shortCiteRegEx": "S\u00f8nderby et al\\.", "year": 2016}, {"title": "The power of sparsity in convolutional neural networks", "author": ["Soravit Changpinyo", "Mark Sandler", "Andrey Zhmoginov"], "venue": "In Under review on ICLR", "citeRegEx": "Changpinyo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Changpinyo et al\\.", "year": 2017}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["Szegedy", "Christian", "Ioffe", "Sergey", "Vanhoucke", "Vincent", "Alemi", "Alex"], "venue": "arXiv preprint arXiv:1602.07261,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Sparse bayesian learning and the relevance vector machine", "author": ["Tipping", "Michael E"], "venue": "Journal of machine learning research,", "citeRegEx": "Tipping and E.,? \\Q2001\\E", "shortCiteRegEx": "Tipping and E.", "year": 2001}, {"title": "Doubly stochastic variational bayes for non-conjugate inference", "author": ["Titsias", "Michalis", "L\u00e1zaro-Gredilla", "Miguel"], "venue": "Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Titsias et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Titsias et al\\.", "year": 2014}, {"title": "Soft weight-sharing for neural network compression", "author": ["Ullrich", "Karen", "Meeds", "Edward", "Welling", "Max"], "venue": "arXiv preprint arXiv:1702.04008,", "citeRegEx": "Ullrich et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ullrich et al\\.", "year": 2017}, {"title": "Automatic relevance determination for least squares support vector machine regression", "author": ["Van Gestel", "Tony", "JAK Suykens", "De Moor", "Bart", "Vandewalle", "Joos"], "venue": "In Neural Networks,", "citeRegEx": "Gestel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Gestel et al\\.", "year": 2001}, {"title": "Regularization of neural networks using dropconnect", "author": ["Wan", "Li", "Zeiler", "Matthew", "Zhang", "Sixin", "Cun", "Yann L", "Fergus", "Rob"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Learning structured sparsity in deep neural networks", "author": ["Wen", "Wei", "Wu", "Chunpeng", "Wang", "Yandan", "Chen", "Yiran", "Li", "Hai"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Wen et al\\.,? \\Q2082\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2082}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Zhang", "Chiyuan", "Bengio", "Samy", "Hardt", "Moritz", "Recht", "Benjamin", "Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1611.03530,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 28, "context": "Deep neural networks (DNNs) are a widely popular family of models which is currently state-of-the-art in many important problems (Szegedy et al., 2016; Silver et al., 2016).", "startOffset": 129, "endOffset": 172}, {"referenceID": 8, "context": "This makes them prone to overfitting (Hinton et al., 2012; Zhang et al., 2016) and necessitates using regularization.", "startOffset": 37, "endOffset": 78}, {"referenceID": 35, "context": "This makes them prone to overfitting (Hinton et al., 2012; Zhang et al., 2016) and necessitates using regularization.", "startOffset": 37, "endOffset": 78}, {"referenceID": 8, "context": "A commonly used regularizer is Binary Dropout (Hinton et al., 2012) that prevents co-adaptation of neurons by randomly dropping them during training.", "startOffset": 46, "endOffset": 67}, {"referenceID": 7, "context": "Modern neural networks contain hundreds of millions of parameters (Szegedy et al., 2015; He et al., 2015) and require a lot of computational and memory resources.", "startOffset": 66, "endOffset": 105}, {"referenceID": 24, "context": "Inducing sparsity during training of DNNs leads to regularization, compression, and acceleration of the resulting model (Han et al., 2015a; Scardapane et al., 2016).", "startOffset": 120, "endOffset": 164}, {"referenceID": 9, "context": "During past several years, a number of papers (Hoffman et al., 2013; Kingma & Welling, 2013; Rezende et al., 2014) on scalable variational inference have appeared.", "startOffset": 46, "endOffset": 114}, {"referenceID": 23, "context": "During past several years, a number of papers (Hoffman et al., 2013; Kingma & Welling, 2013; Rezende et al., 2014) on scalable variational inference have appeared.", "startOffset": 46, "endOffset": 114}, {"referenceID": 33, "context": ", 2014), DropConnect (Wan et al., 2013), Max Norm Constraint (Srivastava et al.", "startOffset": 21, "endOffset": 39}, {"referenceID": 22, "context": "One possible approach is to use tensor decompositions (Novikov et al., 2015; Garipov et al., 2016).", "startOffset": 54, "endOffset": 98}, {"referenceID": 3, "context": "One possible approach is to use tensor decompositions (Novikov et al., 2015; Garipov et al., 2016).", "startOffset": 54, "endOffset": 98}, {"referenceID": 16, "context": ", 2015b), elastic net regularization (Lebedev & Lempitsky, 2015; Liu et al., 2015; Scardapane et al., 2016; Wen et al., 2016) or composite techniques (Han et al.", "startOffset": 37, "endOffset": 125}, {"referenceID": 24, "context": ", 2015b), elastic net regularization (Lebedev & Lempitsky, 2015; Liu et al., 2015; Scardapane et al., 2016; Wen et al., 2016) or composite techniques (Han et al.", "startOffset": 37, "endOffset": 125}, {"referenceID": 4, "context": ", 2016) or composite techniques (Han et al., 2015a; Guo et al., 2016; Ullrich et al., 2017).", "startOffset": 32, "endOffset": 91}, {"referenceID": 31, "context": ", 2016) or composite techniques (Han et al., 2015a; Guo et al., 2016; Ullrich et al., 2017).", "startOffset": 32, "endOffset": 91}, {"referenceID": 20, "context": "In the Relevance Tagging Machine model (Molchanov et al., 2015) Beta prior distribution is used to obtain the ARD effect in a similar setting.", "startOffset": 39, "endOffset": 63}, {"referenceID": 23, "context": "Recent works on Bayesian DNNs (Kingma & Welling, 2013; Rezende et al., 2014; Scardapane et al., 2016) provide different ways to train deep models with a huge number of parameters in a Bayesian way.", "startOffset": 30, "endOffset": 101}, {"referenceID": 24, "context": "Recent works on Bayesian DNNs (Kingma & Welling, 2013; Rezende et al., 2014; Scardapane et al., 2016) provide different ways to train deep models with a huge number of parameters in a Bayesian way.", "startOffset": 30, "endOffset": 101}, {"referenceID": 31, "context": "Soft Weights Sharing (Ullrich et al., 2017) uses an approach, similar to Sparse Bayesian Learning framework, to obtain a sparse and quantized Bayesian Deep Neural Network, but utilizes a more flexible family of prior distributions.", "startOffset": 21, "endOffset": 43}, {"referenceID": 8, "context": "It injects a multiplicative random noise \u039e to the layer input A at each iteration of training procedure (Hinton et al., 2012).", "startOffset": 104, "endOffset": 125}, {"referenceID": 8, "context": "The original version of dropout, so-called Bernoulli or Binary Dropout, was presented with \u03bemi \u223c Bernoulli(1\u2212 p) (Hinton et al., 2012).", "startOffset": 113, "endOffset": 134}, {"referenceID": 25, "context": "The same problem is reported by (S\u00f8nderby et al., 2016) and is a common problem for Bayesian DNNs.", "startOffset": 32, "endOffset": 55}, {"referenceID": 25, "context": "Another way to approach this problem is to use warm-up, as described by (S\u00f8nderby et al., 2016).", "startOffset": 72, "endOffset": 95}, {"referenceID": 4, "context": ", 2015b;a), Dynamic Network Surgery (Guo et al., 2016) and Soft Weight Sharing (Ullrich et al.", "startOffset": 36, "endOffset": 54}, {"referenceID": 31, "context": ", 2016) and Soft Weight Sharing (Ullrich et al., 2017).", "startOffset": 32, "endOffset": 54}, {"referenceID": 15, "context": "A modified version of LeNet5 from (LeCun et al., 1998).", "startOffset": 34, "endOffset": 54}, {"referenceID": 4, "context": ", 2015b;a), DNS (Guo et al., 2016), SWS (Ullrich et al.", "startOffset": 16, "endOffset": 34}, {"referenceID": 31, "context": ", 2016), SWS (Ullrich et al., 2017)) on LeNet architectures.", "startOffset": 13, "endOffset": 35}, {"referenceID": 35, "context": "Recently is was shown that the CNNs are capable of memorizing the data even with random labeling (Zhang et al., 2016).", "startOffset": 97, "endOffset": 117}, {"referenceID": 20, "context": "In the Relevance Tagging Machine model (Molchanov et al., 2015) the same effect was achieved using Beta distributions as a prior.", "startOffset": 39, "endOffset": 63}, {"referenceID": 31, "context": "This result correlates with results of other works on training of sparse neural networks (Han et al., 2015a; Wen et al., 2016; Ullrich et al., 2017; Soravit Changpinyo, 2017).", "startOffset": 89, "endOffset": 174}, {"referenceID": 35, "context": "Further investigation of such redundancy may lead to an understanding of generalization properties of DNNs and explain the phenomenon, observed by (Zhang et al., 2016).", "startOffset": 147, "endOffset": 167}, {"referenceID": 31, "context": "quantization and Huffman coding (Han et al., 2015a; Ullrich et al., 2017), as they use sparsification as an intermediate step.", "startOffset": 32, "endOffset": 73}], "year": 2017, "abstractText": "We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fullyconnected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.", "creator": "LaTeX with hyperref package"}}}