{"id": "1701.05779", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jan-2017", "title": "Empirical Study of Drone Sound Detection in Real-Life Environment with Deep Neural Networks", "abstract": "original effort aims to investigate sophisticated use of deep neural interaction operators detect commercial hobby drones in geologic - times contexts concurrently analyzing their autonomous ecology. 1 purpose of sequencing is to contribute effectively a method allowing artificial behaviour ideal for malicious communication, disguised as for construction. specifically, we defined a diagram reminiscent of detecting the presence of commercial manned drones as a generic classification step based on sound object detection. critics supplement the examples produced above a few sophisticated commercial hobby builders, and even incorporating this data with diverse environmental interference data subsequently remedy the scarcity of drone electronic data upon normal environments. further investigated tangible benefits of state - of - the - art scanning sound classification methods, i. t., a numerical mixture transform ( gmm ), horizontal linear network ( cnn ), and recurrent neural stimulation ( rnn ), for drone sound characterization. our empirical results, which are combined with sonar testing document collected nearby an urban street, confirmed expected effectiveness of these models for operating within far safer person. in summary, our rnn models tested the actual detection performance with an aggregate - score scoring 5. 8009 with 240 ms of input occurring with a short processing procedure, indicating inconsistent responses to u - war detection systems.", "histories": [["v1", "Fri, 20 Jan 2017 12:48:02 GMT  (542kb,D)", "http://arxiv.org/abs/1701.05779v1", "IEEE 5 Pages, Submitted"]], "COMMENTS": "IEEE 5 Pages, Submitted", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["sungho jeon", "jong-woo shin", "young-jun lee", "woong-hee kim", "younghyoun kwon", "hae-yong yang"], "accepted": false, "id": "1701.05779"}, "pdf": {"name": "1701.05779.pdf", "metadata": {"source": "CRF", "title": "Empirical Study of Drone Sound Detection in Real-Life Environment with Deep Neural Networks", "authors": ["Sungho Jeon", "Jong-Woo Shin", "Young-Jun Lee", "Woong-Hee Kim", "YoungHyoun Kwon", "Hae-Yong Yang"], "emails": ["formant}@nsr.re.kr", "sdeva14@gmail.com"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nMotivation. Popularization of commercial hobby drones brings unexpected threats to the environment in which we live, such as terror to people or important facilities. A common four-propeller drone is suitable to enjoy as a hobby and for broadcasting, however, at the same time, it surprisingly makes existing defense systems appear to be outdated legacy systems. Some accidents already proved that these drones can easily penetrate the highest level of security systems, such as landing in front of the prime minister of Germany, on the rooftop of the official residence of the prime minister of Japan, and at the White House in the United States. Thus, the ability to detect the appearance of a drone is a matter of the highest priority to prevent any threats.\nExisting work. Even though few studies have been concerned with the problem of drone sound detection, previous work was conducted in isolated or calm places rather than in a real-life environment without the polyphonic sound environment typical of outside areas, such as on the rooftop of a building in a calm place or isolated environment [1], [2], [3]. However, considering our target problem, which is to detect drones used for malicious purposes, the system inevitably needs to be utilized in a real-life environment, and this requires us to consider polyphonic sound data. Other work differs by using an impressive approach based on radar information or the RF frequency [4], [5], but we need to consider a combined\n1Contact email: sdeva14@gmail.com\ndetection system with a multiple approach to complement the drawback of each method.\nEvent Sound Classification (ESC) in a real environment has been highlighted for diverse purposes. Many researchers have focused on finding useful features and classifiers based on the machine-learning approach. The most popular combination of feature and classification is Mel-frequency Cepstrum Coefficients (MFCC) [6] with the Gaussian Mixture Model (GMM) [7], [8]. More recently, the impressive success achieved with Deep Neural Networks (DNNs) has motivated researchers to introduce these networks to environmental sound recognition. Two popular DNN models, the Convolutional Neural Network (CNN) [9], [10] and Recurrent Neural Network (RNN) [11], have also been highlighted for audio-related tasks. Even though these previous studies cover the ESC problem, considering the importance and urgency of our problem in terms of terrorism, it is worth exploring how ESC work can be applied and to assess its effectiveness for drone sound detection. Here it should be noted that rather than intended to propose novel features or models for drone sound detection, our work aims to investigate the practical effectiveness of popular classification models for our problem in real environments used in previous ESC studies.\nContribution. Our contributions are summarized as follows:\n\u2022 To the best of our knowledge, we are the first to investigate drone sound detection in highly noisy real environments with the aim of constructing a detection method for practical usage with real-time systems based on three popular ESC models: GMM, CNN, and RNN.\n\u2022 We show that the shortage of training data for a drone sound classification model can be remedied with our audio augmentation that synthesizes raw drone sound with diverse background sounds.\n\u2022 We investigate the effectiveness of these models for a testing dataset collected from real-life environments in terms of the F-Score and by taking consideration of the processing time for application to real-time systems."}, {"heading": "II. METHOD", "text": ""}, {"heading": "A. Data Augmentation", "text": "Especially in real environments, unseen event sound has a detrimental effect in terms of deterioration of the detection rate.\nar X\niv :1\n70 1.\n05 77\n9v 1\n[ cs\n.S D\n] 2\n0 Ja\nn 20\n17\nThe most challenging difficulty for this work is the absence of public drone sound data for training. Even though supplying for commercial hobby drone is available, collecting drone sound in diverse environments is only possible to a limited extent, because flying a drone in most public or residential areas is restricted. We therefore remedied the shortage of training data, by augmenting the drone sound with diverse real-life environmental sounds from a public dataset [12], [13] and our collection. The drone sounds were collected in a quiet place outside. The purpose of this augmentation is to produce drone sound combined with realistic noise data, while preserving the characteristics of drone sound. Data augmentation involved amplifying the power of the drone sound such that it exceeded that of the background sound data by 5% in terms of max peak to emphasize the characteristics of the drone sound. Our augmented audio clip consists of concatenated raw background sound and overlapped background sound with repeated drone sounds equal to the length of the background."}, {"heading": "B. Feature: MFCC and Mel-spectrogram", "text": "In many previous ESC studies, MFCC is known to possess outstanding features for classifiers. MFCC also has useful features to capture periodicity from the fundamental frequencies caused by the rotor blades of a drone. Our recorded drone sound indicated a noticeable harmonic shape below a frequency of 1500 Hz. In addition, we also observed a noticeable influence area on the spectrogram between 5000 Hz and 7000 Hz (Figure 1) as was previously pointed out [1], [2]. However, these characteristics were not exhibited by all the drone models used in our experiments. Furthermore, low-frequency data is carried farther than high-frequency data in terms of their energy. Therefore, we only focus on lowfrequency data below 1500 Hz.\nThe other important consideration for feature engineering is the length of instantaneous input data to the model. The minimum length of audio data converted to an MFCC vector and that shows the best performance with our GMM configuration is 40 ms with 50% of overlapping. The other models, CNN and RNN, deliver the best performance when they process data of at least 240 ms in length, converted to mel-spectrogram with mel-bin as 40."}, {"heading": "C. Classifier1: Gaussian Mixture Model", "text": "The GMM detector we construct consists two GMMs trained by positive and negative respectively. For a given length of audio data, it is clipped as fixed windows, which is described as the sample X = x1, x2, ..., xl, where l is the frame length. Then we compare the log-likelihood (L) of both models with a decision threshold to decide drone appearance, Labelpredicted = L1 \u2212 L2 > \u03b8decision. In our experiment, GMM, with the number of Gaussian as 13, the number of MFCC as 20, and the number of mel-bin as 40, shows the best detection performance. Higher values for these parameters, as proposed in previous work, lead to the overfitting problem that shows higher detection performance in training, but produces dissatisfactory results on the testing dataset. The type of covariance shape affected by the detection performance is nearly 0.1 in our training, although we apply a diagonal shape instead of a full shape to alleviate the overfitting effect."}, {"heading": "D. Classifier2: Convolutional Neural Network", "text": "CNN for audio-related tasks showed outstanding results with spectral features instead of focusing on feature engineering [9], [10]. The main idea of a CNN is the use of a convolutional layer that performs localized filtering for local connectivity. This local connectivity is known to be effective to capture invariance useful patterns and highly correlated values with time-frequency representation of sound signal data. Our observation that drone sound has noticeable invariance characteristics below 1500 Hz with harmonics (Figure 1).\nOur proposed simple architecture consists of nine stages contrary to previous approaches proposed in audio-related tasks (Table I), because rather than improving the performance, a more complex model easily leads to the overfitting problem. During training, we periodically checked the accuracy and loss with the testing dataset, then stopped training if the accuracy did not improve for three epochs of training. Eventually, we selected the model that showed the best accuracy. We shuffled the training dataset every epoch with a learning rate of 0.001 and a batch size of 128."}, {"heading": "E. Classifier3: Recurrent Neural Network", "text": "The other popular DNN model, RNN, is designed to make use of past information to feedforward the network. They perform the same task repeatedly with memory, which represents the context of the information accumulated up to that moment. This memory component has the role of preventing the vanishing gradient problem that decays the influence of past data. Based on this idea, the long short-term memory (LSTM) design is commonly used for standard RNN through replacing simple neurons to LSTM memory blocks, which consist of several gates, such as a tanh input gate, a forget gate to decide whether to remain, and an output gate to control which value is used to compute the output activation. Finally, the output of the LSTM memory block is computed as a multiplication of these gates.\nIn this work, bi-directional LSTM-RNN with three layers and 300 LSTM blocks shows the best detection performance. More complicated network configuration shows worse performance or easily results in overfitting. Likewise, when training the CNN model, an early stopping strategy is used in RNN model training by periodically checking the accuracy and loss from the testing dataset. We stop training if it is not improved over 3 epochs of training, after which we retain the model that shows the best accuracy. We shuffle the training dataset every epoch and use a learning rate of 0.0005 and batch size of 64."}, {"heading": "III. EXPERIMENT", "text": "We evaluated our methodology by answering the following questions: (Q1) comparing the detection performance of three models, GMM, CNN, and RNN. (Q2) determining the detection performance for unseen types of data such as detecting different drone models or in different environments. (Q3) considering the required computing cost for application to realtime detection systems. All reported performance values are averaged across 10 evaluation results. We implemented the model in Python 2.7 with Scikit-learn 0.18, Librosa 0.4.3, and Tensorflow 0.12 on the following system: 4-core 2.6-GHz CPU, SSD, and GTX 1070."}, {"heading": "A. Data description", "text": "Our training dataset is augmented by raw background sound and drone sound. The background sound consists of data from our own recording and from a public dataset [12], [13] and the drone sound was collected manually with two popular commercial drones \u2013 Phantom3 and Phantom4 from DJI. Our background sound data contains sounds from ordinary real-life situations with common noise such as chatting, car passing,\nand airplane noise with a total time of 677 seconds. Our drone sound data was recorded in a quiet outdoor place at a distance of 30m, 70m, and 150m for two types of behavior, hovering and approaching, with a total time of 64 seconds. Exact labeling of the drone sound was achieved by starting to record after the drone is activated, and stopping before deactivation. As a result of augmentation, the total audio time used for training is 9556 seconds.\nNote that we separate the training and testing datasets to enable us to strictly measure the performance, instead of the k-fold cross validation technique, which is commonly used to remedy a data shortage. Although an augmented dataset is useful for training, it has limited scope for completely\nmimicking a real dataset. We observe that the real dataset is not completely reproduced by augmentation, due to the complexity of audio characteristics and influence from the environment. Our testing dataset was collected on a real urban street, half of the data relating to a normal situation and the other half in proximity of a building construction site for 151 seconds with an equal amount of positive and negative data. Additionally, we built another testing dataset to measure detection performance for unseen types of data in training, such as unseen types of drones and background. This dataset includes other drone types, DJI Inspire and 3DR Solo, and other types of background such as near a highway or a very noisy road."}, {"heading": "B. Testing: detection performance", "text": "We evaluated the detection performance of the drone using the proposed three models with the actual predicted period (Figure 1), precision, recall, F-Score, and accuracy (Figure 2). In our experiment, RNN achieves the best performance on the training datasets in terms of F-Score (RNN > CNN > GMM: 0.8009 > 0.6415 > 0.5232). Our RNN also shows the most balanced detection performance between precision and recall (0.7953, 0.8066). It is evident that our data augmentation is meaningful to remedy the shortage of the drone training dataset through this high-detection performance. Our CNN model is reported as the second best model in terms of FScore. We note that it remains difficult to decide whether our CNN model outperforms GMM. Our CNN and GMM show a distinctly different tendency according in terms of precision (CNN, GMM: 0.5346 < 0.9031) and recall (CNN, GMM: 0.8019 > 0.3683). Our CNN shows the tendency to predict data as positive rather than negative. On the contrary, GMM treats most of the data as negative, thus it shows lower recall but higher precision. However, considering our detection label result (Figure 1), GMM shows more accurate detection performance than statistics, but discontinuity in the positive prediction degrades the detection performance. This unstable consistency of positive prediction can be remedied by smoothing techniques. Therefore, in view of the operator, GMM can be regarded a more appropriate detection model to operate in practice. We also report the accuracy of these models, but do not consider it as important as the other measures.\nDespite our diverse attempts we were unable to find CNN model architecture for previously proposed models. This could be attributed to the variation in audio data of the audio part unrelated to drone sound. In a real environment, we observe that the noticeable area affected by drone sound is small compared with the entire spectrogram image. Because of the fundamental mechanism of CNN, it is easily influenced by the other different areas of the spectrogram consisting of diverse environmental sound rather than focusing on drone sound only."}, {"heading": "C. Testing: unseen types of data", "text": "The drawback of the machine-learning approach is the possibility of significant deterioration of detection performance when processing unlearned data. In this experiment, we aim to report degradation of detection performance for unseen types of data and improved understanding of the tendency of the proposed model. Our RNN still achieves the best performance\nin terms of F-Score (0.6984) with balanced precision and recall (0.5477, 0.9635). Interestingly, our report shows that the CNN model failed to classify the data, instead treating all data as positive. This misclassification could be caused by unseen highly noisy background sound that could not be distinguished from drone sound by the CNN model. According to this result, the CNN model is vulnerable to unseen noisy background data. Our GMM exhibits more accurate detection performance than CNN, but has a significantly decreased measure such that it would not be appropriate to operate in practice (0.3910 of FScore).\nOur experiment with unseen data provides additional insight on the tendency of the proposed models for GMM to predict data as negative and the other models based on deep neural networks to predict data as positive. In our experiment, even introducing additional training data does not improve the GMM model significantly; however, RNN can improve their precision performance through the diverse background training dataset. Above all, this experiment confirmed that it is essential to collect diverse types of data for the target environment.\nGMM CNN RNN Models\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nS co re s\nPrecision Recall F-Score Accuracy\nGMM CNN RNN Models with testing data\nFig. 3: Performance comparison for unseen type data. The RNN still achieves the best performance with F-Score of 0.6984."}, {"heading": "D. Required cost versus Detection performance", "text": "In practice, the other factor that should be considered to operate the detection system is the cost required by the\ndetection model. The two main factors determining the required cost for operating this system in practice are the processing time and the amount of input data for prediction. Many previous studies overlooked the practical constraint for improved detection performance, but it indeed may dominate the form of the system in a real environment, such as the difference in processing time between the GMM and DNN models [14]. Statistically, we only measured the execution time for each process except for the additional time required to execute the Python program. According to the results, all three proposed models seem appropriate for application to real-time systems. The most time-consuming stage is feature engineering to create the MFCC vector, but it only takes 145 ms for a 1- minute audio clip. The execution time for data loading varies according to the target platform and the classification time does not adversely affect real-time system operation. For a fair comparison, we report the processing time for CNN and RNN without GPU usage.\nHowever, we should note that the reported processing time is the minimal partial execution time only for each stage. In practice, the execution time of the detection algorithm varies according to the platform and is influenced to a greater extent by other costs related to program execution. Especially, if we plan to operate with a different programming language, then importing the Python program into a system programmed in another language would seriously deteriorate the execution time. Second, we note that models based on a deep neural network require a larger amount of data than GMM for optimal performance (240 ms > 40 ms). This indicates that our actual initial detected time would increase as a function of the amount of input data. If we avoid importing the program or operating a low-performance embedded platform, the amount of input data can affect the initial detection time substantially."}, {"heading": "IV. CONCLUSION", "text": "This paper presents our binary classification model that uses audio data to detect the existence of a drone. We configured the parameters for GMM and the network for CNN and RNN for our model. Then, we evaluated their detection performance in terms of the F-Score and the required cost for application to real-time systems in practice. In our experiment, the RNN model showed the best F-Score (0.8009) with 240 ms of audio input data. Our experiment also confirmed that the use of data augmentation to synthesize raw drone sound with diverse background sounds can alleviate the shortage of drone training data.\nThe other main concern of our work was the influence on the detection performance of increasing drone distance. Because of practical constraints, we could not evaluate distances exceeding 150 m. In our experience, audio data recorded at distances further than 150 m do not display noticeable characteristics on the spectrogram with a na\u2019\u0301ive recording with\na single microphone. This was attributed to the drone sound exhibiting weakened characteristics in the spectrogram because it is covered by background data. The usage of multiple microphones with Beamforming, a signal processing technique used for filtering to achieve directional signal transmission, is expected to increase the maximum detection distance of our model. The other interesting future work would be utilization of the Generative Adversarial Network (GAN) to remedy the shortage of drone sound training data."}], "references": [{"title": "Drone sound detection", "author": ["J. Mezei", "V. Fiaska", "A. Moln\u00e1r"], "venue": "Computational Intelligence and Informatics (CINTI), 2015 16th IEEE International Symposium on. IEEE, 2015, pp. 333\u2013338.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Detection and tracking of drones using advanced acoustic cameras", "author": ["J. Busset", "F. Perrodin", "P. Wellig", "B. Ott", "K. Heutschi", "T. R\u00fchl", "T. Nussbaumer"], "venue": "SPIE Security+ Defence. International Society for Optics and Photonics, 2015, pp. 96 470F\u201396 470F.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Drone sound detection by correlation", "author": ["J. Mezei", "A. Moln\u00e1r"], "venue": "Applied Computational Intelligence and Informatics (SACI), 2016 IEEE 11th International Symposium on. IEEE, 2016, pp. 509\u2013518.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning based doppler radar for micro uas detection and classification", "author": ["G.J. Mendis", "T. Randeny", "J. Wei", "A. Madanayake"], "venue": "Military Communications Conference, MILCOM 2016-2016 IEEE. IEEE, 2016, pp. 924\u2013929.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Investigating cost-effective rf-based detection of drones", "author": ["P. Nguyen", "M. Ravindranatha", "A. Nguyen", "R. Han", "T. Vu"], "venue": "Proceedings of the 2nd Workshop on Micro Aerial Vehicle Networks, Systems, and Applications for Civilian Use. ACM, 2016, pp. 17\u201322.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Acoustic scene classification: Classifying environments from the sounds they produce", "author": ["D. Barchiesi", "D. Giannoulis", "D. Stowell", "M.D. Plumbley"], "venue": "IEEE Signal Processing Magazine, vol. 32, no. 3, pp. 16\u2013 34, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Detection of shouted speech in the presence of ambient noise.", "author": ["J. Pohjalainen", "T. Raitio", "P. Alku"], "venue": "in INTERSPEECH,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Tut database for acoustic scene classification and sound event detection", "author": ["A. Mesaros", "T. Heittola", "T. Virtanen"], "venue": "Signal Processing Conference (EUSIPCO), 2016 24th European. IEEE, 2016, pp. 1128\u2013 1132.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust sound event recognition using convolutional neural networks", "author": ["H. Zhang", "I. McLoughlin", "Y. Song"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 559\u2013563.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Filterbank learning for deep neural network based polyphonic sound event detection", "author": ["E. Cakir", "E.C. Ozan", "T. Virtanen"], "venue": "Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016, pp. 3399\u20133406.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural networks for polyphonic sound event detection in real life recordings", "author": ["G. Parascandolo", "H. Huttunen", "T. Virtanen"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 6440\u20136444.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "DARES-G1: Database of Annotated Real-world Everyday Sounds", "author": ["J.K.M.W.W. Grootel", "T.C. Andringa"], "venue": "Proceedings of the NAG/DAGA Meeting 2009, 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Histogram of gradients of timefrequency representations for audio scene classification", "author": ["A. Rakotomamonjy", "G. Gasso"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), vol. 23, no. 1, pp. 142\u2013153, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic environmental sound recognition: Performance versus computational cost", "author": ["S. Sigtia", "A.M. Stark", "S. Krstulovi\u0107", "M.D. Plumbley"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 11, pp. 2096\u20132107, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Even though few studies have been concerned with the problem of drone sound detection, previous work was conducted in isolated or calm places rather than in a real-life environment without the polyphonic sound environment typical of outside areas, such as on the rooftop of a building in a calm place or isolated environment [1], [2], [3].", "startOffset": 325, "endOffset": 328}, {"referenceID": 1, "context": "Even though few studies have been concerned with the problem of drone sound detection, previous work was conducted in isolated or calm places rather than in a real-life environment without the polyphonic sound environment typical of outside areas, such as on the rooftop of a building in a calm place or isolated environment [1], [2], [3].", "startOffset": 330, "endOffset": 333}, {"referenceID": 2, "context": "Even though few studies have been concerned with the problem of drone sound detection, previous work was conducted in isolated or calm places rather than in a real-life environment without the polyphonic sound environment typical of outside areas, such as on the rooftop of a building in a calm place or isolated environment [1], [2], [3].", "startOffset": 335, "endOffset": 338}, {"referenceID": 3, "context": "Other work differs by using an impressive approach based on radar information or the RF frequency [4], [5], but we need to consider a combined", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "Other work differs by using an impressive approach based on radar information or the RF frequency [4], [5], but we need to consider a combined", "startOffset": 103, "endOffset": 106}, {"referenceID": 5, "context": "The most popular combination of feature and classification is Mel-frequency Cepstrum Coefficients (MFCC) [6] with the Gaussian Mixture Model (GMM) [7], [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "The most popular combination of feature and classification is Mel-frequency Cepstrum Coefficients (MFCC) [6] with the Gaussian Mixture Model (GMM) [7], [8].", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "The most popular combination of feature and classification is Mel-frequency Cepstrum Coefficients (MFCC) [6] with the Gaussian Mixture Model (GMM) [7], [8].", "startOffset": 152, "endOffset": 155}, {"referenceID": 8, "context": "Two popular DNN models, the Convolutional Neural Network (CNN) [9], [10] and Recurrent Neural Network (RNN) [11], have also been highlighted for audio-related tasks.", "startOffset": 63, "endOffset": 66}, {"referenceID": 9, "context": "Two popular DNN models, the Convolutional Neural Network (CNN) [9], [10] and Recurrent Neural Network (RNN) [11], have also been highlighted for audio-related tasks.", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "Two popular DNN models, the Convolutional Neural Network (CNN) [9], [10] and Recurrent Neural Network (RNN) [11], have also been highlighted for audio-related tasks.", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "We therefore remedied the shortage of training data, by augmenting the drone sound with diverse real-life environmental sounds from a public dataset [12], [13] and our collection.", "startOffset": 149, "endOffset": 153}, {"referenceID": 12, "context": "We therefore remedied the shortage of training data, by augmenting the drone sound with diverse real-life environmental sounds from a public dataset [12], [13] and our collection.", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "In addition, we also observed a noticeable influence area on the spectrogram between 5000 Hz and 7000 Hz (Figure 1) as was previously pointed out [1], [2].", "startOffset": 146, "endOffset": 149}, {"referenceID": 1, "context": "In addition, we also observed a noticeable influence area on the spectrogram between 5000 Hz and 7000 Hz (Figure 1) as was previously pointed out [1], [2].", "startOffset": 151, "endOffset": 154}, {"referenceID": 8, "context": "CNN for audio-related tasks showed outstanding results with spectral features instead of focusing on feature engineering [9], [10].", "startOffset": 121, "endOffset": 124}, {"referenceID": 9, "context": "CNN for audio-related tasks showed outstanding results with spectral features instead of focusing on feature engineering [9], [10].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "The background sound consists of data from our own recording and from a public dataset [12], [13] and the drone sound was collected manually with two popular commercial drones \u2013 Phantom3 and Phantom4 from DJI.", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "The background sound consists of data from our own recording and from a public dataset [12], [13] and the drone sound was collected manually with two popular commercial drones \u2013 Phantom3 and Phantom4 from DJI.", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Many previous studies overlooked the practical constraint for improved detection performance, but it indeed may dominate the form of the system in a real environment, such as the difference in processing time between the GMM and DNN models [14].", "startOffset": 240, "endOffset": 244}], "year": 2017, "abstractText": "This work aims to investigate the use of deep neural network to detect commercial hobby drones in real-life environments by analyzing their sound data. The purpose of work is to contribute to a system for detecting drones used for malicious purposes, such as for terrorism. Specifically, we present a method capable of detecting the presence of commercial hobby drones as a binary classification problem based on sound event detection. We recorded the sound produced by a few popular commercial hobby drones, and then augmented this data with diverse environmental sound data to remedy the scarcity of drone sound data in diverse environments. We investigated the effectiveness of state-of-the-art event sound classification methods, i.e., a Gaussian Mixture Model (GMM), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), for drone sound detection. Our empirical results, which were obtained with a testing dataset collected on an urban street, confirmed the effectiveness of these models for operating in a real environment. In summary, our RNN models showed the best detection performance with an F-Score of 0.8009 with 240 ms of input audio with a short processing time, indicating their applicability to real-time detection systems.", "creator": "LaTeX with hyperref package"}}}