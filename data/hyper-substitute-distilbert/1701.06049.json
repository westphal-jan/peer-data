{"id": "1701.06049", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2017", "title": "Interactive Learning from Policy-Dependent Human Feedback", "abstract": "encouraging agents and robots to become uniformly useful, they deserve be able to effortlessly learn involving non - knowing users. this paper investigates recent problem of interactively analyzing behaviors communicated by a female teacher integrating positive and negative feedback. much renewed attention after this debate ultimately made the assumption that actions given feedback for strategies positively vary dependent but the behavior employers are teaching yet is remote from student learner's current policy. we discovered empirical results that show an argument even be false - - - whether human trainers give a constructive or incorrect feedback via a decision implies inconsistent on the host's rational decision. we argue that policy - specific feedback, in addition to being commonplace, enables useful constructive strategies from letting agents should benefit. based on this paradigm, we introduce convergent theory - training by humans ( coach ), an algorithm about constructing socially policy - dependent arguments that flows over a local optimum. finally, teacher realized that coach can successfully learn other behaviors onto a learning occasion, even avoids noisy image editing.", "histories": [["v1", "Sat, 21 Jan 2017 16:37:41 GMT  (107kb,D)", "http://arxiv.org/abs/1701.06049v1", "7 pages, 2 figures"]], "COMMENTS": "7 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["james macglashan", "mark k ho", "robert tyler loftin", "bei peng", "guan wang", "david l roberts", "matthew e taylor", "michael l littman"], "accepted": true, "id": "1701.06049"}, "pdf": {"name": "1701.06049.pdf", "metadata": {"source": "CRF", "title": "Interactive Learning from Policy-Dependent Human Feedback", "authors": ["James MacGlashan", "Mark K Ho", "Robert Loftin", "Bei Peng", "David Roberts", "Matthew E. Taylor", "Michael L. Littman", "\u2217Cogitai", "\u2020Brown"], "emails": [], "sections": [{"heading": null, "text": "I. INTRODUCTION\nProgramming robots is very difficult, in part because the real world is inherently rich and\u2014to some degree\u2014unpredictable. In addition, our expectations for physical agents are quite high and often difficult to articulate. Nevertheless, for robots to have a significant impact on the lives of individuals, even non-programmers need to be able to specify and customize behavior. Because of these complexities, relying on endusers to provide instructions to robots programmatically seems destined to fail.\nReinforcement learning (RL) from human trainer feedback provides a compelling alternative to programming because agents can learn complex behavior from very simple positive and negative signals. Furthermore, real-world animal training is an existence proof that people can train complex behavior using these simple signals. Indeed, animals have been successfully trained to guide the blind, locate mines in the ocean, detect cancer or explosives, and even solve complex, multistage puzzles.\nHowever, traditional reinforcement-learning algorithms have yielded limited success when the reward signal is provided by humans and have largely failed to benefit from the sophisticated training strategies that expert animal trainers use with animals. This failure has led to the development of new reinforcement-learning algorithms that are designed to learn from human-generated rewards and investigations into how people give interactive feedback [1, 2, 3, 4, 5, 6]. In general, many of these human-centered learning algorithms are built on the insight that people tend to give feedback that reflects the\npolicy the agent should be following, rather than as a numeric value that is meant to be maximized by the agent. While this insight seems accurate, existing approaches assume models of feedback that are independent of the policy the agent is currently following. We present empirical results that demonstrate that this assumption is incorrect and further argue that policydependent feedback enables effective training strategies, such as differential feedback and policy shaping, from which we would like a learning agent to benefit. Following this result, we present Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent human feedback that is capable of benefiting from these strategies. COACH is based on the insight that the TD-error used by actor-critic algorithms is an unbiased estimate of the advantage function, which is a policy-dependent value roughly corresponding to how much better or worse an action is compared to the current policy and which captures these previously mentioned training strategies. To validate that COACH scales to complex problems, we train five different behaviors on a TurtleBot robot that makes decisions every 33ms from noisy image features that are not visible to the trainer."}, {"heading": "II. BACKGROUND", "text": "For modeling the underlying decision-making problem of an agent being taught by a human, we adopt the Markov Decision Process (MDP) formalism. An MDP is a 5-tuple: \u3008S,A, T,R, \u03b3\u3009, where S is the set of possible states of the environment; A is the set of actions available to the agent; T (s\u2032|s, a) is the transition function, which defines the probability of the environment transitioning to state s\u2032 when the agent takes action a in environment state s; R(s, a, s\u2032) is the reward function specifying the numeric reward the agent receives for taking action a in state s and transitioning to state s\u2032; and \u03b3 \u2208 [0, 1] is a discount factor specifying how much immediate rewards are preferred to more distant rewards.\nA stochastic policy \u03c0 for an MDP is a per-state action probability distribution that defines a mode of behavior; \u03c0 : S \u00d7 A \u2192 [0, 1], where \u2211 a\u2208A \u03c0(s, a) = 1,\u2200s \u2208 S. In the MDP setting, the goal is to find the optimal policy \u03c0\u2217, which maximizes the expected future discounted reward when the agent selects actions in each state according to \u03c0\u2217; \u03c0\u2217 = argmax\u03c0 E[ \u2211\u221e t=0 \u03b3\ntrt|\u03c0], where rt is the reward received at time t. Two important concepts in MDPs are the value function (V \u03c0) and action\u2013value function (Q\u03c0). The value function defines the expected future discounted reward from each state\nar X\niv :1\n70 1.\n06 04\n9v 1\n[ cs\n.A I]\n2 1\nJa n\n20 17\nwhen following some policy and the action\u2013value function defines the expected future discounted reward when an agent takes some action in some state and then follows some policy \u03c0 thereafter. These equations can be recursively defined via the Bellman equation: V \u03c0(s) = \u2211 a \u03c0(s, a)Q\n\u03c0(s, a) and Q\u03c0(s, a) = \u2211 s\u2032 T (s\n\u2032|s, a) [R(s, a, s\u2032) + \u03b3V \u03c0(s\u2032)]. For shorthand, the value functions for the optimal policies are usually denoted V \u2217 and Q\u2217.\nIn reinforcement learning (RL), an agent interacts with an environment modeled as an MDP, but does not have direct access to the transition function or reward function and instead must learn a policy from environment observations. A common class of RL algorithms are actor-critic algorithms. Bhatnagar et al. [7] provides a general template for these algorithms. Actor-critic algorithms are named for the two main components of the algorithms: The actor is a parameterized policy that dictates how the agent selects actions; the critic estimates the value function for the actor and provides critiques at each time step that are used to update the policy parameters. Typically, the critique is the temporal difference (TD) error: \u03b4t = rt+\u03b3V (st)\u2212V (st\u22121), which describes how much better or worse a transition went than expected."}, {"heading": "III. HUMAN-CENTERED REINFORCEMENT LEARNING", "text": "In this work, a human-centered reinforcement-learning (HCRL) problem is a learning problem in which an agent is situated in an environment described by an MDP but in which rewards are generated by a human trainer instead of from a stationary MDP reward function that the agent is meant to maximize. The trainer has a target policy \u03c0\u2217 they are trying to teach the agent. The trainer communicates this policy by giving numeric feedback as the agent acts in the environment. The goal of the agent is to learn the target policy \u03c0\u2217 from the feedback.\nTo define a learning algorithm for this problem, we first characterize how human trainers typically use numeric feedback to teach target policies. If feedback is stationary and intended to be maximized, it can be treated as a reward function and standard RL algorithms used. Although this approach has had some success [8, 9], there are complications that limit its applicability. In particular, a trainer must take care that the feedback they give contains no unanticipated exploits, constraining the feedback strategies they can use. Indeed, prior research has shown that interpreting human feedback like a reward function often induces positive reward cycles that leads to unintended behaviors [10, 11].\nThe issues with interpreting feedback as reward have led to the insight that human feedback is better interpreted as a comment on the agent\u2019s behavior; for example, positive feedback roughly corresponds to \u201cthat was good\u201d and negative feedback roughly corresponds to \u201cthat was bad.\u201d Existing HCRL work adopting this perspective includes TAMER [1], SABL [6], and Policy Shaping [5], discussed in more detail in the Related Work section. We note, though, that all assume that human feedback is independent of the agent\u2019s current policy.\nWe provide empirical results that show this assumption to be incorrect."}, {"heading": "IV. POLICY-DEPENDENT FEEDBACK", "text": "Evidence that human feedback is influenced by the agent\u2019s current policy can be seen in previous work. For example, it was observed that trainers taper their feedback over the course of learning [11, 12, 9]. One explanation for decreasing feedback rates is policy-dependent feedback, but trainer fatigue is another. We provide a stronger result showing that trainers\u2014 for the same state\u2013action pair\u2014choose positive or negative feedback depending on their perception of the learner\u2019s behavior. This finding serves as a warning for algorithms that rely on an assumption policy-independent feedback."}, {"heading": "A. Empirical Results", "text": "We had Amazon Mechanical Turk (AMT) participants teach an agent in a simple sequential task, illustrated in Figure 1. Participants were instructed to train a virtual dog to walk to the yellow goal location in a grid world as fast as possible but without going through the green cells. They were additionally told that, as a result of prior training, their dog was already either \u201cbad\u201d, \u201calright\u201d, or \u201cgood\u201d at the task and were shown examples of each behavior before training. In all cases, the dog would start in the location shown in Figure 1. \u201cBad\u201d dogs walked straight through the green cells to the yellow cell. \u201cAlright\u201d dogs first moved left, then up, and then to the goal, avoiding green but not taking the shortest route. \u201cGood\u201d dogs took the shortest path to yellow without going through green.\nDuring training, participants saw the dog take an action from one tile to another and then gave feedback after every action using a continuous labeled slider as shown. The slider always started in the middle of the scale on each trial, and several points were labeled with different levels of reward (praise and treats) and punishment (scolding and a mild electric shock). Participants went through a brief tutorial using this interface. Responses were coded as a numeric value from \u221250 to 50, with \u201cDo Nothing\u201d as the zero-point.\nDuring the training phase, participants trained a dog for three episodes that all started in the same position and ended\nat the goal. The dog\u2019s behavior was pre-programmed for all episodes in such a way that the first step of the final episode would reveal if feedback was policy-dependent. The dog always performed the same behavior in the first two episodes, and then performed the \u201calright\u201d behavior in the third episode. Each user was placed into one of three different conditions that determined how the dog would behave in the first two episodes: either \u201cbad,\u201d \u201calright,\u201d or \u201cgood.\u201d If feedback is policy dependent, we expect more positive feedback in the \u201cbad\u201d condition than in the \u201calright\u201d or \u201cgood\u201d condition; \u201calright\u201d behavior is an improvement over the previous \u201cbad\u201d behavior whereas it is either no improvement or a deterioration compared to \u201calright\u201d or \u201cgood\u201d behavior.\nFigure 2 shows boxplots and individual responses for the first step of the final episode under each of the three conditions. These results indicate that the sign of feedback is sensitive to the learner\u2019s policy, as predicted. The mean and median feedback under the \u201cbad\u201d condition is slightly positive (Mean = 9.8, Median = 24, S.D. = 22.2; planned Wilcoxon one-sided signed-rank test: Z = 1.71, p < 0.05), whereas it is negative for the \u201calright\u201d condition (Mean = \u221218.3, Median = \u221223.5, S.D. = 24.6; planned Wilcoxon two-sided signed-rank test: Z = \u22123.15, p < 0.01) and \u201cgood\u201d condition (Mean = \u221210.8, Median = \u221218.0, S.D. = 20.7; planned Wilcoxon one-sided signed-rank test: Z = \u22122.33, p < 0.05). There was a main effect across the three conditions (p < 0.01, Kruskal-Wallace Test), and pairwise comparisons indicated that only the \u201cbad\u201d condition differed from \u201calright\u201d and \u201cgood\u201d conditions (p < 0.01 for both, Bonferroni-corrected, Mann-Whitney Pairwise test)."}, {"heading": "B. Training Strategies", "text": "Beyond the fact that our previous evidence suggests that people give policy-dependent feedback, we argue that policydependent feedback affords desirable training strategies. Specifically, we consider three different feedback schemes that can be viewed as operationalizations of well-studied behavior analysis reinforcement schedules [13]: Diminishing Returns:\ngradually decrease positive feedback for good actions as the agent adopts those actions. Differential Feedback: vary the magnitude of feedbacks w.r.t. the degree of improvement or deterioration in behavior. Policy Shaping: provide positive feedback for suboptimal actions that improve behavior and then negative feedback after the improvement has been made.\nDiminishing returns is a useful strategy because it decreases the burden of how actively a trainer must supply feedback and removes the need for explicit training and execution phases. Differential feedback is useful because it can serve to highlight the most important behaviors in the state space and communicate a kind of urgency for learning them. Finally, policy shaping concerns feedback that signals an improvement relative to the current baseline\u2014as in the AMT study above. It is useful for providing a direction for the learner to follow at all times, even when the space of policies is continuous or otherwise impractical to search."}, {"heading": "V. CONVERGENT ACTOR-CRITIC BY HUMANS", "text": "In this section, we introduce Convergent Actor-Critic by Humans (COACH), an actor-critic-based algorithm capable of learning from policy-dependent feedback. COACH is based on the insight that the advantage function is a good model of human feedback and that actor-critic algorithms update a policy using the critic\u2019s TD error, which is an unbiased estimate of the advantage function. Consequently, an agent\u2019s policy can be directly modified by human feedback without a critic component. We first define the advantage function and describe how it relates to the three previously mentioned training strategies. Then, we present the general update rule for COACH and its convergence. Finally, we present Real-time COACH, which includes mechanisms for providing variable magnitude feedback and learning in problems with a highfrequency decision cycle."}, {"heading": "A. The Advantage Function and Training Strategies", "text": "The advantage function [14] A\u03c0 is defined as\nA\u03c0(s, a) = Q\u03c0(s, a)\u2212 V \u03c0(s). (1)\nRoughly speaking, the advantage function describes how much better or worse an action selection is compared to the agent\u2019s performance following policy \u03c0. We now show that feedback assigned according to the advantage function follows the patterns of all three training strategies. Showing that the advantage function captures the differential feedback strategy is trivial, because the advantage function is defined by how much better taking an action over its current policy is expected to be.\nTo show that the advantage function induces diminishing returns, consider what happens when the learner improves its behavior by shifting to a higher scoring action a in some state s. As its probability of selecting a goes to 1, A\u03c0(s, a) = Q\u03c0(s, a) \u2212 Q\u03c0(s, a) = 0, because the value function V \u03c0(s) = \u2211 a \u03c0(s, a)Q\n\u03c0(s, a). Since the expected value is a smooth linear combination of the Q-values, as the\nagent adopts action a, A\u03c0(s, a) \u2192 0, resulting in gradually decreasing feedback.\nTo show that the advantage function induces policy shaping, let us first assume w.l.o.g. that there are three possible actions, where action a3 is exclusively optimal, and action a2 is better than action a1. That is, Q\u2217(s, a3) > Q\u2217(s, a2) > Q\u2217(s, a1). For illustrative purposes, let us also assume that the agent has learned the optimal policy in all states except state s, wherein the agent with near-certain probability selects the worst action a1 (\u03c0(s, a1) \u2248 1) and that all actions lead to some other state. In this scenario, A\u03c0(s, a) = Q\u2217(s, a) \u2212 \u2211 a\u2032 \u03c0(s, a\n\u2032)Q\u2217(s, a\u2032) \u2248 Q\u2217(s, a) \u2212 Q\u2217(s, a1). Since Q\u2217(s, a2) > Q\u2217(s, a1), it follows that A\u03c0(s, a2) is positive. Consequently, in this condition, suboptimal action a2 would receive positive feedback.\nNow consider the case when the agent with near-certainty selects optimal action a3. Under this condition, A\u03c0(s, a) \u2248 Q\u2217(s, a) \u2212 Q\u2217(s, a3) and since Q\u2217(s, a3) > Q\u2217(s, a2), A\u03c0(s, a2) is negative. Once again, since V \u03c0 is a smooth linear function of Q\u03c0 , as the agent adopts optimal action a3, A\u03c0(s, a2) becomes negative. Therefore, suboptimal actions can be rewarded and then punished as the agent improves at the task, producing policy shaping."}, {"heading": "B. Convergence and Update Rule", "text": "Given a performance metric \u03c1, Sutton et al. [15] derive a policy gradient algorithm of the form: \u2206\u03b8 = \u03b1\u2207\u03b8\u03c1. Here, \u03b8 represents the parameters that control the agent\u2019s behavior and \u03b1 is a learning rate. Under the assumption that \u03c1 is the discounted expected reward from a fixed start state distribution, they show that\n\u2207\u03b8\u03c1 = \u2211 s d\u03c0(s) \u2211 a \u2207\u03b8\u03c0(s, a)Q\u03c0(s, a),\nwhere d\u03c0(s) is the component of the (discounted) stationary distribution at s. A benefit of this form of the gradient is that, given that states are visited according to d\u03c0(s) and actions are taken according to \u03c0(s, a), the update at time t can be made as:\n\u2206\u03b8t = \u03b1t\u2207\u03b8\u03c0(st, at) ft+1\n\u03c0(st, at) , (2)\nwhere E[ft+1] = Q\u03c0(st, at)\u2212v(s) for any action-independent function v(s).\nIn the context of the present paper, ft+1 represents the feedback provided by the trainer. It follows trivially that if the trainer chooses the policy-dependent feedback ft = Q\u03c0(st, at), we obtain a convergent learning algorithm that (locally) maximizes discounted expected reward. In addition, feedback of the form ft = Q\u03c0(st, at) \u2212 V \u03c0(st) = A\u03c0(s, a) also results in convergence. Note that for the trainer to provide feedback in the form of Q\u03c0 or A\u03c0 , they would need to \u201cpeer inside\u201d the learner and observe its policy. In practice, the trainer estimates \u03c0 by observing the agent\u2019s actions.\nAlgorithm 1 Real-time COACH Input: policy \u03c0\u03b80 , trace set \u03bb, delay d, learning rate \u03b1\nInitialize traces e\u03bb \u2190 0 \u2200\u03bb \u2208 \u03bb observe initial state s0 for t = 0 to \u221e do\nselect and execute action at \u223c \u03c0\u03b8t(st, \u00b7) observe next state st+1, sum feedback ft+1, and \u03bb for \u03bb\u2032 \u2208 \u03bb do e\u03bb\u2032 \u2190 \u03bb\u2032e\u03bb\u2032 + 1\u03c0\u03b8t (st\u2212d,at\u2212d)\u2207\u03b8t\u03c0\u03b8t(st\u2212d, at\u2212d) end for \u03b8t+1 \u2190 \u03b8t + \u03b1ft+1e\u03bb\nend for"}, {"heading": "C. Real-time COACH", "text": "There are challenges in implementing Equation 2 for realtime use in practice. Specifically, the interface for providing variable magnitude feedback needs to be addressed, and the question of how to handle sparseness and the timing of feedback needs to be answered. Here, we introduce Real-time COACH, shown in Algorithm 1, to address these issues.\nFor providing variable magnitude reward, we use reward aggregation [1]. In reward aggregation, a trainer selects from a discrete set of feedback values and further raises or lowers the numeric value by giving multiple feedbacks in succession that are summed together.\nWhile sparse feedback is not especially problematic (because no feedback results in no change in policy), it may slow down learning unless the trainer is provided with a mechanism to allow feedback to affect a history of actions. We use eligibility traces [16] to help apply feedback to the relevant transitions. An eligibility trace is a vector that keeps track of the policy gradient and decays exponentially with a parameter \u03bb. Policy parameters are then updated in the direction of the trace, allowing feedback to affect earlier decisions. However, a trainer may not always want to influence a long history of actions. Consequently, real-time COACH maintains multiple eligibility traces with different temporal decay rates and the trainer chooses which eligibility trace to use. This trace choice may be handled implicitly with the feedback value selection or explicitly.\nDue to reaction time, human feedback is typically delayed by about 0.2 to 0.8 seconds from the event to which they meant to give feedback [10]. To handle this delay, feedback in Realtime COACH is associated with events from d steps ago to cover the gap. Eligibility traces further smooth the feedback to older events.\nFinally, we note that just as there are numerous variants of actor-critic update rules, similar variations can be used in the context of COACH."}, {"heading": "VI. RELATED WORK", "text": "An inspiration for our work is the TAMER framework [10]. In TAMER, trainers provide interactive numeric feedback as the learner takes actions. The feedback is interpreted as an exemplar of the reward function for the previous state\u2013action\npair and is used to learn the reward function. When the agent makes rapid decisions, TAMER divides the feedback among the recent state\u2013action pairs according to a probability distribution. TAMER makes decisions by myopically choosing the action with the highest reward estimate. Because the agent myopically maximizes reward, the feedback may be interpreted as exemplars of Q\u2217. Later work investigated nonmyopically maximizing the learned reward function with a planning algorithm [17], but this approach requires a model of the environment and special treatment of termination conditions. Because TAMER expects feedback to be policy independent, it does not support the diminishing returns or policy-shaping strategies, and handles differential feedback only in so far as it uses numeric feedback. In our robotics case study, we provide an explicit example where TAMER\u2019s failure to support diminishing returns can result in unlearning.\nTwo other closely related approaches are SABL [6] and Policy Shaping [5] (unrelated to the policy shaping feedback strategy defined above). Both of these approaches treat feedback as discrete probabilistic evidence of a parametrized policy. SABL\u2019s probabilistic model additionally includes (learnable) parameters for describing how often a trainer is expected to give explicit positive or negative feedback. Both of these approach assume policy-independent feedback and do not support the three training strategies described.\nThere have also been some domains in which treating human feedback as reward signals to maximize has had some success, such as in shaping the control for a prosthetic arm [8] and learning how to interact in an online chat room from multiple users\u2019 feedback [9]. An interesting area of future work is to test whether performance in these domains can be improved with COACH given our insights into the nature of human feedback\u2014work on the chat-room learning domain did report challenges due to diminishing feedback.\nSome research has examined combining human feedback with more traditional environmental rewards [18, 19, 20, 21]. A challenge in this context in practice is that rewards do not naturally come from the environment and must be programmatically defined. However, it is appealing because the agent can learn in the absence of an active trainer. We believe that COACH could be straightforwardly modified to learn in this setting as well.\nFinally, a related research area is learning from demonstration (LfD), in which a human provides examples of the desired behavior. There are a number of different approaches to solving this problem surveyed by Argall et al. [22]. We see these approaches as complementary to HCRL because it is not always possible, or convenient, to provide demonstrations. LfD approaches that learn a parametrized policy could also operate with COACH, allowing the agent to have their policy seeded by demonstrations, and then fine tuned with interactive feedback."}, {"heading": "VII. ROBOTICS CASE STUDY", "text": "In this section, we present qualitative results applying Realtime COACH to a TurtleBot robot. The goal of this study\nwas to test that COACH can scale to a complex domain involving multiple challenges, including training an agent that operates on a fast decision cycle (33ms), noisy nonMarkov observations from a camera, and agent perception that is hidden from the trainer. To demonstrate the flexibility of COACH, we trained it to perform five different behaviors involving a pink ball and cylinder with an orange top using the same parameter selections. We discuss these behaviors below. We also contrast the results to training with TAMER. We chose TAMER as a comparison because, to our knowledge, it is the only HCRL algorithm with success on a similar platform [23].\nThe TurtleBot is a mobile base with two degrees of freedom that senses the world from a Kinect camera. We discretized the action space to five actions: forward, backward, rotate clockwise, rotate counterclockwise, and do nothing. The agent selects one of these actions every 33ms. To deliver feedback, we used a Nintendo Wii controller to give +1, +4, or \u22121 numeric feedback, and pause and continue training. For perception, we used only the RGB image channels from the Kinect. Because our behaviors were based around a relocatable pink ball and a fixed cylinder with an orange top, we hand constructed relevant image features to be used by the learning algorithms. These features were generated using techniques similar to those used in neural network architectures. In the future, we will investigate learning these features along with the policy. The features were constructed by first transforming the image into two color channels associated with the color of the ball and cylinder. Sum pooling to form a lowerdimensional 8 \u00d7 8 grid was applied to each color channel. Each sum-pooling unit was then passed through three different normalized threshold units defined by Ti(x) = min( x\u03c6i , 1), where \u03c6i specifies how quickly the ith threshold unit saturates. Using multiple saturation parameters differentiates the distance of objects, resulting in three \u201cdepth\u201d scales per color channel. Finally, we passed these results through a 2\u00d7 8 max-pooling layer with stride 1.\nThe five behaviors we trained were push\u2013pull, hide, ball following, alternate, and cylinder navigation. In push\u2013pull, the TurtleBot is trained to navigate toward the ball when it is far, and back away from it when it is near. The hide behavior has the TurtleBot back away from the ball when it is near and turn away from it when it is far. In ball following, the TurtleBot is trained to navigate to the ball. In the alternate task, the TurtleBot is trained to go back and forth between the cylinder and ball. Finally, cylinder navigation involves the agent navigating to the cylinder. We further classify training methods for each of these behaviors as flat, involving the push\u2013 pull, hide, and ball following behaviors; and compositional, involving the alternate and cylinder navigation behaviors.\nIn all cases, our human trainer (one of the co-authors) used differential feedback and diminishing returns to quickly reinforce behaviors and restrict focus to the areas needing tuning. However, in alternate and cylinder navigation, they attempted more advanced compositional training methods. For alternate, the agent was first trained to navigate to the ball when it sees it, and then turn away when it is near. Then,\nthe same was independently done for the cylinder. After training, introducing both objects would cause the agent to move back and forth between them. For cylinder navigation, they attempted to make use of an animal-training method called lure training in which an animal is first conditioned to follow a lure object that is then used to guide it through more complex behaviors. In cylinder navigation, they first trained the ball to be a lure, used it to guide the TurtleBot to the cylinder, and finally gave a +4 reward to reinforce the behaviors it took when following the ball (turning to face the cylinder, moving toward it, and stopping upon reaching it). The agent would then navigate to the cylinder without requiring the ball to be present.\nFor COACH parameters, we used a softmax parameterized policy, where each action preference value was a linear function of the image features, plus tanh(\u03b8a), where \u03b8a is a learnable parameter for action a, providing a preference in the absence of any stimulus. We used two eligibility traces with \u03bb = 0.95 for feedback +1 and \u22121, and \u03bb = 0.9999 for feedback +4. The feedback-action delay d was set to 6, which is 0.198 seconds. Additionally, we used an actorcritic parameter-update rule variant in which action preference values are directly modified (along its gradient), rather than by the gradient of the policy [24]. This variant more rapidly communicates stimulus\u2013response preferences. For TAMER, we used typical parameter values for fast decision cycle problems: delay-weighted aggregate TAMER with uniform distribution credit assignment over 0.2 to 0.8 seconds, p = 0, and cmin = 1 [10]. (See prior work for parameter meaning.) For TAMER\u2019s reward function approximation, we used the same parameters as the action preferences in COACH."}, {"heading": "A. Results and Discussion", "text": "COACH was able to successfully learn all five behaviors and a video showing its learning is available online at https: //vid.me/3h2s. Each of these behaviors were trained in less than two minutes, including the time spent verifying that a behavior worked. Differential feedback and diminishing returns allowed only the behaviors in need of tuning to be quickly reinforced or extinguished without any explicit division between training and testing. Moreover, the agent successfully benefited from the compositional training methods, correctly combining subbehaviors for alternate, and quickly learning cylinder navigation with the lure.\nTAMER only successfully learned the behaviors using the flat training methodology and failed to learn the compositionally trained behaviors. In all cases, TAMER tended to forget behavior, requiring feedback for previous decisions it learned to be resupplied after it learned a new decision. For the alternate behavior, this forgetting led to failure: after training the behavior for the cylinder, the agent forgot some of the ballrelated behavior and ended up drifting off course when it was time to go to the ball. TAMER also failed to learn from lure training, which was expected since TAMER does not allow reinforcing a long history of behaviors.\nWe believe TAMER\u2019s forgetting is a result of interpreting feedback as reward-function exemplars in which new feedback in similar contexts can change the target. To illustrate this problem, we constructed a well-defined scenario in which TAMER consistently unlearns behavior. In this scenario, the goal was for the TurtleBot to always stay whenever the ball was present, and move forward if just the cylinder was present. We first trained TAMER to stay when the ball alone was present using many rapid rewards (yielding a large aggregated signal). Next, we trained it to move forward when the cylinder alone was present. We then introduced both objects, and the TurtleBot correctly stayed. After rewarding it for staying with a single reward (weaker than the previously-used many rapid rewards), the TurtleBot moved forward. This counter-intuitive unlearning is a consequence of the small reward decreasing its reward-function target for the stay action to a point lower than the value for moving forward. COACH does not exhibit this problem\u2014any reward for staying will strengthen the behavior."}, {"heading": "VIII. CONCLUSION", "text": "In this work, we presented empirical results that show that the numeric feedback people give agents in an interactive training paradigm is influenced by the agent\u2019s current policy and argued why such policy-dependent feedback enables useful training strategies. We then introduced COACH, an algorithm that, unlike existing human-centered reinforcementlearning algorithms, converges to a local optimum when trained with policy-dependent feedback. We finally we showed that COACH scales up in the context of a robotics case study in which a TurtleBot was successfully taught multiple behaviors with advanced training methods.\nThere are a number of exciting future directions to extend this work. In particular, because COACH is built on the actorcritic paradigm, it should be possible to combine it straightforwardly with learning from demonstration and environmental rewards, allowing an agent to be trained in a variety of ways. Second, because people give policy-dependent feedback, greater gains may be possible by investigating how people model the current policy of the agent and how their model differs from the agent\u2019s actual policy."}], "references": [{"title": "Interactively shaping agents via human reinforcement: The tamer framework", "author": ["W.B. Knox", "P. Stone"], "venue": "Proceedings of the fifth international conference on Knowledge capture. ACM, 2009, pp. 9\u201316.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Reinforcement learning with human teachers: Evidence of feedback and guidance with implications for learning performance", "author": ["A.L. Thomaz", "C. Breazeal"], "venue": "AAAI, vol. 6, 2006, pp. 1000\u20131005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Robot learning via socially guided exploration", "author": ["A. Thomaz", "C. Breazeal"], "venue": "ICDL Development on Learning, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners", "author": ["A.L. Thomaz", "C. Breazeal"], "venue": "2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Policy shaping: Integrating human feedback with reinforcement learning", "author": ["S. Griffith", "K. Subramanian", "J. Scholz", "C. Isbell", "A.L. Thomaz"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2625\u20132633.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning", "author": ["R. Loftin", "B. Peng", "J. MacGlashan", "M.L. Littman", "M.E. Taylor", "J. Huang", "D.L. Roberts"], "venue": "Autonomous Agents and Multi-Agent Systems, vol. 30, no. 1, pp. 30\u2013 59, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural actor\u2013critic algorithms", "author": ["S. Bhatnagar", "R.S. Sutton", "M. Ghavamzadeh", "M. Lee"], "venue": "Automatica, vol. 45, no. 11, pp. 2471\u20132482, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning", "author": ["P.M. Pilarski", "M.R. Dawson", "T. Degris", "F. Fahimi", "J.P. Carey", "R.S. Sutton"], "venue": "2011 IEEE International Conference on Rehabilitation Robotics. IEEE, 2011, pp. 1\u20137.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "A social reinforcement learning agent", "author": ["C. Isbell", "C.R. Shelton", "M. Kearns", "S. Singh", "P. Stone"], "venue": "Proceedings of the fifth international conference on Autonomous agents. ACM, 2001, pp. 377\u2013384.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning from human-generated reward", "author": ["W.B. Knox"], "venue": "Ph.D. dissertation, University of Texas at Austin, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "How humans teach agents", "author": ["W.B. Knox", "B.D. Glass", "B.C. Love", "W.T. Maddox", "P. Stone"], "venue": "International Journal of Social Robotics, vol. 4, no. 4, pp. 409\u2013421, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Miltenberger, Behavior modification: Principles and procedures", "author": ["G. R"], "venue": "Cengage Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "Proceedings of the twelfth international conference on machine learning, 1995, pp. 30\u201337.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}, {"title": "Policy gradient methods for reinforcement learning with function approximation.", "author": ["R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Y. Mansour"], "venue": "in NIPS, vol", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A. Barto", "R. Sutton", "C. Anderson"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on, vol. SMC-13, no. 5, pp. 834 \u2013846, sept.-oct. 1983.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1983}, {"title": "Learning non-myopically from human-generated reward", "author": ["W.B. Knox", "P. Stone"], "venue": "Proceedings of the 2013 international conference on Intelligent user interfaces. ACM, 2013, pp. 191\u2013202.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining manual feedback with subsequent MDP reward signals for reinforcement learning", "author": ["B. Knox", "P. Stone"], "venue": "Proc. of 9th Int. Conf. on Autonomous  Agents and Multiagent Systems, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic reward shaping: training a robot by voice", "author": ["A.C. Tenorio-Gonzalez", "E.F. Morales", "L. Villase\u00f1or- Pineda"], "venue": "Advances in Artificial Intelligence\u2013IBERAMIA 2010. Springer, 2010, pp. 483\u2013492.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "A teaching method for reinforcement learning", "author": ["J.A. Clouse", "P.E. Utgoff"], "venue": "Proceedings of the Ninth International Conference on Machine Learning (ICML92), 1992, pp. 92\u2013101.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1992}, {"title": "Giving advice about preferred actions to reinforcement learners via knowledge-based kernel regression", "author": ["R. Maclin", "J. Shavlik", "L. Torrey", "T. Walker", "E. Wild"], "venue": "Proceedings of the National Conference on Artificial intelligence, vol. 20, no. 2, 2005, p. 819.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and autonomous systems, vol. 57, no. 5, pp. 469\u2013483, 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Training a robot via human feedback: A case study", "author": ["W.B. Knox", "P. Stone", "C. Breazeal"], "venue": "Social Robotics. Springer, 2013, pp. 460\u2013470.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "This failure has led to the development of new reinforcement-learning algorithms that are designed to learn from human-generated rewards and investigations into how people give interactive feedback [1, 2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 216}, {"referenceID": 1, "context": "This failure has led to the development of new reinforcement-learning algorithms that are designed to learn from human-generated rewards and investigations into how people give interactive feedback [1, 2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 216}, {"referenceID": 2, "context": "This failure has led to the development of new reinforcement-learning algorithms that are designed to learn from human-generated rewards and investigations into how people give interactive feedback [1, 2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 216}, {"referenceID": 3, "context": "This failure has led to the development of new reinforcement-learning algorithms that are designed to learn from human-generated rewards and investigations into how people give interactive feedback [1, 2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 216}, {"referenceID": 4, "context": "This failure has led to the development of new reinforcement-learning algorithms that are designed to learn from human-generated rewards and investigations into how people give interactive feedback [1, 2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 216}, {"referenceID": 5, "context": "This failure has led to the development of new reinforcement-learning algorithms that are designed to learn from human-generated rewards and investigations into how people give interactive feedback [1, 2, 3, 4, 5, 6].", "startOffset": 198, "endOffset": 216}, {"referenceID": 0, "context": "the reward function specifying the numeric reward the agent receives for taking action a in state s and transitioning to state s\u2032; and \u03b3 \u2208 [0, 1] is a discount factor specifying how much immediate rewards are preferred to more distant rewards.", "startOffset": 139, "endOffset": 145}, {"referenceID": 0, "context": "A stochastic policy \u03c0 for an MDP is a per-state action probability distribution that defines a mode of behavior; \u03c0 : S \u00d7 A \u2192 [0, 1], where \u2211 a\u2208A \u03c0(s, a) = 1,\u2200s \u2208 S.", "startOffset": 125, "endOffset": 131}, {"referenceID": 6, "context": "[7] provides a general template for these algorithms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Although this approach has had some success [8, 9], there are complications", "startOffset": 44, "endOffset": 50}, {"referenceID": 8, "context": "Although this approach has had some success [8, 9], there are complications", "startOffset": 44, "endOffset": 50}, {"referenceID": 9, "context": "Indeed, prior research has shown that interpreting human feedback like a reward function often induces positive reward cycles that leads to unintended behaviors [10, 11].", "startOffset": 161, "endOffset": 169}, {"referenceID": 0, "context": "\u201d Existing HCRL work adopting this perspective includes TAMER [1], SABL [6], and Policy Shaping [5], discussed in more detail in", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "\u201d Existing HCRL work adopting this perspective includes TAMER [1], SABL [6], and Policy Shaping [5], discussed in more detail in", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "\u201d Existing HCRL work adopting this perspective includes TAMER [1], SABL [6], and Policy Shaping [5], discussed in more detail in", "startOffset": 96, "endOffset": 99}, {"referenceID": 10, "context": "For example, it was observed that trainers taper their feedback over the course of learning [11, 12, 9].", "startOffset": 92, "endOffset": 103}, {"referenceID": 8, "context": "For example, it was observed that trainers taper their feedback over the course of learning [11, 12, 9].", "startOffset": 92, "endOffset": 103}, {"referenceID": 11, "context": "analysis reinforcement schedules [13]: Diminishing Returns: gradually decrease positive feedback for good actions as the", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "The advantage function [14] A is defined as", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "[15] derive a policy gradient algorithm of the form: \u2206\u03b8 = \u03b1\u2207\u03b8\u03c1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For providing variable magnitude reward, we use reward aggregation [1].", "startOffset": 67, "endOffset": 70}, {"referenceID": 14, "context": "We use eligibility traces [16] to help apply feedback to the relevant transitions.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "8 seconds from the event to which they meant to give feedback [10].", "startOffset": 62, "endOffset": 66}, {"referenceID": 9, "context": "An inspiration for our work is the TAMER framework [10].", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "Later work investigated nonmyopically maximizing the learned reward function with a planning algorithm [17], but this approach requires a model of the environment and special treatment of termination conditions.", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "Two other closely related approaches are SABL [6] and Policy Shaping [5] (unrelated to the policy shaping feedback strategy defined above).", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "Two other closely related approaches are SABL [6] and Policy Shaping [5] (unrelated to the policy shaping feedback strategy defined above).", "startOffset": 69, "endOffset": 72}, {"referenceID": 7, "context": "There have also been some domains in which treating human feedback as reward signals to maximize has had some success, such as in shaping the control for a prosthetic arm [8] and learning how to interact in an online chat room from multiple users\u2019 feedback [9].", "startOffset": 171, "endOffset": 174}, {"referenceID": 8, "context": "There have also been some domains in which treating human feedback as reward signals to maximize has had some success, such as in shaping the control for a prosthetic arm [8] and learning how to interact in an online chat room from multiple users\u2019 feedback [9].", "startOffset": 257, "endOffset": 260}, {"referenceID": 16, "context": "Some research has examined combining human feedback with more traditional environmental rewards [18, 19, 20, 21].", "startOffset": 96, "endOffset": 112}, {"referenceID": 17, "context": "Some research has examined combining human feedback with more traditional environmental rewards [18, 19, 20, 21].", "startOffset": 96, "endOffset": 112}, {"referenceID": 18, "context": "Some research has examined combining human feedback with more traditional environmental rewards [18, 19, 20, 21].", "startOffset": 96, "endOffset": 112}, {"referenceID": 19, "context": "Some research has examined combining human feedback with more traditional environmental rewards [18, 19, 20, 21].", "startOffset": 96, "endOffset": 112}, {"referenceID": 20, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "We chose TAMER as a comparison because, to our knowledge, it is the only HCRL algorithm with success on a similar platform [23].", "startOffset": 123, "endOffset": 127}, {"referenceID": 22, "context": "Additionally, we used an actorcritic parameter-update rule variant in which action preference values are directly modified (along its gradient), rather than by the gradient of the policy [24].", "startOffset": 187, "endOffset": 191}, {"referenceID": 9, "context": "8 seconds, p = 0, and cmin = 1 [10].", "startOffset": 31, "endOffset": 35}], "year": 2017, "abstractText": "For agents and robots to become more useful, they must be able to quickly learn from non-technical users. This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner\u2019s current policy. We present empirical results that show this assumption to be false\u2014whether human trainers give a positive or negative feedback for a decision is influenced by the learner\u2019s current policy. We argue that policy-dependent feedback, in addition to being commonplace, enables useful training strategies from which agents should benefit. Based on this insight, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that COACH can successfully learn multiple behaviors on a physical robot, even with noisy image features.", "creator": "LaTeX with hyperref package"}}}