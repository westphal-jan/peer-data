{"id": "1701.06796", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jan-2017", "title": "Discriminative Neural Topic Models", "abstract": "we propose a neural network based approach integrate learning topics against open and composite datasets. the model makes no assumptions like mutual conditional distribution of the observed features with the latent topics. this required simulations to perform topic modelling efficiently using aggregation of documents and patches but already labelled feature tracks, rather than accepting ourselves as detection. importantly, the bigger ones be online, probably more can be used for streaming data. presently, since the infrastructure utilizes private networks, it can be implemented like memory through ease, and as it is very scalable.", "histories": [["v1", "Tue, 24 Jan 2017 10:29:31 GMT  (500kb,D)", "https://arxiv.org/abs/1701.06796v1", "6 pages, 9 figures"], ["v2", "Tue, 28 Feb 2017 14:17:16 GMT  (512kb,D)", "http://arxiv.org/abs/1701.06796v2", "6 pages, 9 figures"]], "COMMENTS": "6 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gaurav pandey", "ambedkar dukkipati"], "accepted": false, "id": "1701.06796"}, "pdf": {"name": "1701.06796.pdf", "metadata": {"source": "CRF", "title": "Discriminative Neural Topic Models", "authors": ["Gaurav Pandey"], "emails": ["ad}@csa.iisc.ernet.in"], "sections": [{"heading": "1 Introduction", "text": "Mixed membership modeling is the task of assigning the observed features of an object to latent classes. Each object is assumed to be a mixture over the latent classes, and these classes are shared among the objects. This is distinct from mixture modeling, where all the features of the object are assumed to be sampled from a single latent class.\nTraditional approaches to mixed membership modeling assume a parametric form for the distribution of a latent class over the features, and the distribution of an object over these classes. For instance, in case of documents, the observed features are the words, and the latent classes are the topics. A topic is assumed to have a multinomial distribution over the words, while a document is assumed to have a multinomial distribution over the topics [6]. This statistical model is also referred to as probabilistic latent semantic indexing (pLSI). One can further define Dirichlet priors on the parameters of the multinomial distributions, to obtain a full Bayesian treatment of topic modeling [1, 9]. This model, commonly referred to as latent Dirichlet allocation (LDA), is widely used for finding topics in document collections, and inferring population structures from genotype data.\nBoth LDA and pLSI are generative models, whereby the conditional distribution of the observed features given the latent classes is explicitly modeled. This requires explicit knowledge about the parametric form of these distributions. The number of unique words in a language are finite, and hence, it is possible to make these distributions as general as possible by choosing them to be multinomial. However, as the set of all unique observed features becomes comparable to the size of the collection, this approach becomes less and less meaningful.\nHence, while generative topic models are useful for modeling words in a document collection, they can\u2019t be directly used for modeling sentences and paragraphs in documents, or pixels of an image. An intermediate preprocessing step is required that transforms all the observed features in the collection to a relatively small set of unique \u2018codewords\u2019 [3]. Since this intermediate step is independent of topic modeling, it can result in suboptimal features. Finding good intermediate representations suitable for topic modeling can be as challenging as finding the topics themselves.\nWhile the observed features can be extremely complicated to model (for instance, the pixels in an image), the conditional distribution of the latent classes given the observed features is always multinomial. The parameters of the multinomial distribution are functions of the observed features. In\nar X\niv :1\n70 1.\n06 79\n6v 2\n[ cs\n.L G\n] 2\n8 Fe\nb 20\nthis paper, we propose a model for learning the latent classes of an object without explicitly modeling the observed features. This allows us to make minimal assumptions about the distributions of the observed features. In particular, we propose discriminative neural topic models (DNTM), whereby the conditional distribution of the latent classes or topics given the observed features is directly modeled using neural networks. The model can be trained end-to-end using backpropagation. To our knowledge, this is the first approach to topic modeling that doesn\u2019t explicitly model the distribution of observed features given the latent classes.\nIn order to establish that DNTM indeed learns meaningful topics, we use it for assigning words to topics for well-known text corpuses. The learnt topics are used in secondary tasks such as clustering. Despite the fact that LDA makes minimal assumptions for modeling words in document collections, our model is able to outperform LDA even for these datasets. Note that these experiments only serve as a sanity check confirming that the proposed model indeed learns meaningful topics, and is competitive with LDA even for tasks where LDA makes minimal assumptions.\nWe also use DNTM for learning topics on CIFAR-10 dataset in an unsupervised manner. We use a convolutional neural network to convert the image into spatial features, which correspond to the words of the document. The CNN is trained by the backpropagating the gradient from the topic model, that is, we train the CNN to learn features, that correspond to meaningful topics. In order to prevent the model from overfitting, we use adversarial training by coupling it with a generator [4]. The model is trained to perform poorly on generated images, while the generator is trained to generate images that perform well on the topic modelling task."}, {"heading": "2 Discriminative Neural Topic Models", "text": "Let X(1), . . .X(n) be the observed features for n objects, while Z(1), . . . ,Z(n) be the corresponding latent classes. Let K be the number of latent classes."}, {"heading": "2.1 Modeling the words in a document", "text": "In case of documents, X(i)j represents the embedding vector of the j th word in the ith document X(i) with an arbitrary order, and Z(i)j represents the corresponding topic or latent class. In particular, there exists an embedding vector for every word in the vocabulary. The embeddings are initialized randomly and are trained by backpropagating the gradient of the topic model.\nThe length of the ith document is given by mi. We assume that given the words of the document, the topics are independently distributed, that is,\nP\u03b8(Z (i)|X(i)) = mi\u220f j=1 P\u03b8(Z (i) j |X (i)) , (1)\nZ (i) j , 1 \u2264 j \u2264 mi are multinomial random variables whose parameters are functions of X(i). An explicit parametric form for this distribution is specified in the experiments section. In the rest of the paper, the dependence of the distributions on \u03b8 is assumed without being explicitly stated.\nSince, we wish to associate each word X(i)j , with a single topic Z (i) j with high confidence, we minimize the entropy of P (Z(i)j |X(i)), 1 \u2264 j \u2264 mi, that is\nH(Z(i)j |X (i)) = \u2212 K\u2211 k=1 P (Z (i) j = k|X (i)) logP (Z (i) j = k|X (i)) (2)\nMinimizing entropy ensures that the conditional distribution over the topics for a word is highly peaked for a very few topics. This is also referred to as cluster assumption, and has been used for clustering [7] and semi supervised learning [5, 2], where it enjoys considerable success.\nNext, we need to ensure that the words in a single document have similar distribution over the topics. This corresponds to minimizing the variance between the probability distributions over topics for the words of the document. We achieve this by minimizing the KL-divergence between the conditional distribution of topics given the words P (Z(i)j |X(i)), and the corresponding average over the words,\nthat is\nKL(P Z (i) j ||P\u0304Z(i) ;X(i)) = K\u2211 k=1 P (Z (i) j = k|X (i)) log P (Z (i) j = k|X(i)) Pd(k|X(i))\n(3)\nwhere Pd(k|X(i)) is the distribution over the topics for document d. If we assume that each observed word in the document occurs with equal probability, then the document distribution over the topics is given by Pd(k|X(i)) = 1mi \u2211mi j=1 P (Z (i) j = k|X(i)).\nFinally, in order to ensure that all the documents don\u2019t get assigned the same topic, we add a term for encouraging balance among the topics. Towards this end, we define\nP\u0304 (k) = 1\nn n\u2211 i=1 Pd(k|X(i)) (4)\nWe maximize the entropy of the above distribution to enforce a uniform prior on the topics. Ways to relax this assumption are discussed later in the paper.\nCombining the above three criteria, the objective function for the jth word of ith document is given by\nFij(\u03b8) = H(Z(i)j |X (i)) +KL(P Z (i) j ||PdZ(i) ;X(i))\u2212H(P\u0304 ) , (5)\nwhere \u03b8 is the parameter of the distribution P (Zi|X(i)). The above term is minimized for all the words of all the documents in the corpus, that is, \u2211n i=1 \u2211mi j=1 Fij(\u03b8)."}, {"heading": "2.2 Regularizing the model", "text": "In order to ensure that the model doesn\u2019t overfit the training data, we use negative sampling. In particular, for text documments, we randomly sample the words in the vocabulary, to create a fake document. Next, we force the model to perform poorly on the fake document as follows:\n1. The distributions over topics for the words in a fake document should be highly uncertain. This corresponds to maximizing the entropy of the corresponding topic distributions, that is, H(Z(i)j |X(i)) is maximized for 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 mi.\n2. The distribution over topics for the words in a fake document, should have high variance. This corresponds to maximizing the KL-divergence of the topic distribution from the average for the words in the fake document, that is KL(P\nZ (i) j ||PdZ(i) ;X(i)) is maximized."}, {"heading": "2.3 Document Clustering", "text": "Firstly, we verify that the model indeed performs topic modelling. Towards that end, we apply our model to learn topics on 20 newsgroup, one of the most widely used datasets for text categorization. In order to ensure reproducibility, we use a pre-processed version of the dataset 1. The preprocessed 20 newsgroup dataset consists of 11,293 documents for training and 7,528 documents for testing, with a vocabulary size of 8,165 stemmed words. The data is almost evenly divided between the 20 classes. We discard documents with less than 2 words.\nFor clustering tasks, we combine the training and test sets. We use two metrics to evaluate the effectiveness of the proposed topic model for clustering - purity and normalized mutual information (NMI) 2.\nWe compare the proposed model against several standard algorithms for clustering and topicmodelling. These include K-means, Normalized cuts [10], probabilistic Latent Semantic indexing (pLSI) and Latent Dirichlet Allocation (LDA). For LDA, pLSI and the proposed model DNTM, the topics correspond to clusters and a document is assigned to the cluster/topic with the highest posterior probability for the given document. This approach has been shown to be more effective than clustering the topic representations of documents [8].\n1The pre-processed dataset is available at https://sites.google.com/site/renatocorrea02/textcategorizationdatasets 2Details about the two metrics can be obtained at http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-\nof-clustering-1.html\nThe clustering results for 20 newsgroup are given in Table 1(a). As can be seen from the results, there is a distinct improvement in performance as one moves from standard clustering algorithms to topic-modelling algorithms such as pLSI, LDA and DNTM. Moreover, the performance obtained using DNTM is at par with the performance using LDA and pLSI."}, {"heading": "2.4 The topics", "text": "Although the proposed model DNTM only learns the distribution of the topics given the words, we can obtain the distribution of the words given the topics as follows: First, we compute the joint probability of the topic t and the word w occurring together as follows:\nP\u0304 (t, w) = 1\nn n\u2211 i=1 1 mi mi\u2211 j=1 P (Z (i) j = t|X (i) j = w)P (X (i) j = w)\nHere, P (X(i)j = w) = 1, if the i th word of the jth document is w and 0 otherwise. Informally, the above equation simply computes the probability of observing the topic t, every time the word w occurs, and takes the average of all these probabilities. Finally, to compute the posterior, we normalize the above distribution.\nP\u0304 (w|t) = P\u0304 (w, t)\u2211 w\u2032\u2208vocabulary P\u0304 (w \u2032, t) (6)\nUsing the above method, we obtain the distribution of the topics over the words for DNTM. The 20 most probable words for each topic obtained using DNTM are listed in Table 2.4. One can observe that the topics learnt by the model are coherent, that is, the words corresponding to a single topic occur commonly in a single class."}, {"heading": "3 Topic modelling in images", "text": "In order to extend the proposed model for images, one needs to define \u2018words\u2019 and \u2018documents\u2019 for images. The most common approach for obtaining words from images involves extraction of features (SIFT, HOG etc.,) from images. These features are then clustered using K-means. The\ncorresponding quantized features are used as words [? ? ? ] for topic modelling. Learning of features happens independently of topic modelling, thereby resulting in suboptimal features for topic modelling. Another approach that has been considered in [? ], involves training a deep Boltzmann machine on the pixels of an image. The samples from the deepest latent layer of DBM are then used as words for training a hierarchical Bayesian model.\nIn this work, however, we learn the words from the pixels of an image by employing a convolutional network. In particular, let X(i) represents the ith image, Y (i)j represents the j th word of the ith image and Z(i)j represents the corresponding topic. In order to convert the the X (i) into words Y (i)j , we feed X(i) to a convolutional neural network. The output of the CNN is treated as words of the image. For instance, if the output of CNN consists of 100 features maps of size 8x8, we treat them as 64 words, where each word is represented using 100 dimensions. The topic distribution of a word is obtained by applying 1\u00d7 1 convolution to the word representation, and then applying softmax to the output. Note that the words Y (i)j are deterministic functions of X (i), and hence, allow the backpropagation of gradient from Y (i)j to the parameters of the CNN.\nNow that we have defined the words in an image, we need to define the concept of a document. The definition of a document depends on the underlying task. In this paper, we consider two possible definitions for documents. In the first scenario, we are interested in clustering similar images together. For this task, we consider that all the words in an image belong to the same document. This is the usual assumption in topic modelling. The words and the document are then fed to the proposed model, exactly as for documents."}, {"heading": "3.1 Treating images as documents", "text": "We use the proposed model to learn topics in CIFAR-10 dataset. The dataset consists of 50,000 training images and 10,000 test images divided into 10 classes. This dataset is quite challenging,\nsince there is high variability within each class, even though the individual images are only 32x32 pixels.\nWe use convolutional neural networks for obtaining the features from the images. In particular, for the 32x32 CIFAR-10 images, we apply 4 layers of strided convolution and ReLU nonlinearity to obtain 64 (8x8) features per image. These features function as words, and are fed as input to the DNTM. Note that CNN is trained only by the DNTM, thereby coupling feature extraction and topic modelling.\nWe train the DNTM to extract 100 topics from the CIFAR-10 dataset. 9 of those topics are shown in\nFigure 3. In particular, for each topic, we have listed the 24 most probable images. One can observe that the topics are qualitatively coherent.\nWe evaluate the purity of the learned topics by retrieving the 100 most probable CIFAR-10 images for each topic. The most common label among the 100 images associated with the topic, is then assigned to the topic. The purity of a given topic is the fraction of the retrieved images that have the same label as the one assigned to the topic. Using the proposed model, we obtain a mean purity of 49.3% for the learned topics."}], "references": [{"title": "Latent Dirichlet Allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Semi-supervised classification by low density separation", "author": ["Olivier Chapelle", "Alexander Zien"], "venue": "In AISTATS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["Li Fei-Fei", "Pietro Perona"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Yves Grandvalet", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Probabilistic Latent Semantic Indexing", "author": ["Thomas Hofmann"], "venue": "Proceedings of the 22nd annual International ACM SIGIR conference on Research and Development in Information Retrieval,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Discriminative clustering by regularized information maximization", "author": ["Andreas Krause", "Pietro Perona", "Ryan G Gomes"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Investigating task performance of probabilistic topic models: an empirical study of plsa and lda", "author": ["Yue Lu", "Qiaozhu Mei", "ChengXiang Zhai"], "venue": "Information Retrieval,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Inference of population structure using multilocus genotype", "author": ["Jonathan K Pritchard", "Matthew Stephens", "Peter Donnelly"], "venue": "data. Genetics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Normalized cuts and image segmentation", "author": ["Jianbo Shi", "Jitendra Malik"], "venue": "IEEE Transactions on pattern analysis and machine intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}], "referenceMentions": [{"referenceID": 5, "context": "A topic is assumed to have a multinomial distribution over the words, while a document is assumed to have a multinomial distribution over the topics [6].", "startOffset": 149, "endOffset": 152}, {"referenceID": 0, "context": "One can further define Dirichlet priors on the parameters of the multinomial distributions, to obtain a full Bayesian treatment of topic modeling [1, 9].", "startOffset": 146, "endOffset": 152}, {"referenceID": 8, "context": "One can further define Dirichlet priors on the parameters of the multinomial distributions, to obtain a full Bayesian treatment of topic modeling [1, 9].", "startOffset": 146, "endOffset": 152}, {"referenceID": 2, "context": "An intermediate preprocessing step is required that transforms all the observed features in the collection to a relatively small set of unique \u2018codewords\u2019 [3].", "startOffset": 155, "endOffset": 158}, {"referenceID": 3, "context": "In order to prevent the model from overfitting, we use adversarial training by coupling it with a generator [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "This is also referred to as cluster assumption, and has been used for clustering [7] and semi supervised learning [5, 2], where it enjoys considerable success.", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "This is also referred to as cluster assumption, and has been used for clustering [7] and semi supervised learning [5, 2], where it enjoys considerable success.", "startOffset": 114, "endOffset": 120}, {"referenceID": 1, "context": "This is also referred to as cluster assumption, and has been used for clustering [7] and semi supervised learning [5, 2], where it enjoys considerable success.", "startOffset": 114, "endOffset": 120}, {"referenceID": 9, "context": "These include K-means, Normalized cuts [10], probabilistic Latent Semantic indexing (pLSI) and Latent Dirichlet Allocation (LDA).", "startOffset": 39, "endOffset": 43}, {"referenceID": 7, "context": "This approach has been shown to be more effective than clustering the topic representations of documents [8].", "startOffset": 105, "endOffset": 108}], "year": 2017, "abstractText": "We propose a neural network based approach for learning topics from text and image datasets. The model makes no assumptions about the conditional distribution of the observed features given the latent topics. This allows us to perform topic modelling efficiently using sentences of documents and patches of images as observed features, rather than limiting ourselves to words. Moreover, the proposed approach is online, and hence can be used for streaming data. Furthermore, since the approach utilizes neural networks, it can be implemented on GPU with ease, and hence it is very scalable.", "creator": "LaTeX with hyperref package"}}}