{"id": "1701.07148", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2017", "title": "CP-decomposition with Tensor Power Method for Convolutional Neural Networks Compression", "abstract": "generalized encoded networks ( paws ) has delivered a great success in many software using complex image classification method. however, they change a lot within memory limited computational functionality, which hinders them from running primarily relatively low - end binary cartridges such as smart notebook. we propose a cnn compression method based on cp - flex and tensor power coding. we so propose having increased notch tuning, with which nodes q - tune randomly distributed network approximately completing each layer, periodically before playing the next pixel. significant shifts in memory and computation cost is good compared to piece - of - the - art filter work with no external pixel loss.", "histories": [["v1", "Wed, 25 Jan 2017 02:58:06 GMT  (3185kb,D)", "http://arxiv.org/abs/1701.07148v1", "Accepted as a conference paper at BigComp 2017"]], "COMMENTS": "Accepted as a conference paper at BigComp 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marcella astrid", "seung-ik lee"], "accepted": false, "id": "1701.07148"}, "pdf": {"name": "1701.07148.pdf", "metadata": {"source": "CRF", "title": "CP-decomposition with Tensor Power Method for Convolutional Neural Networks Compression", "authors": ["Marcella Astrid", "Seung-Ik Lee"], "emails": ["marcella.astrid@ust.ac.kr", "silee@etri.re.kr"], "sections": [{"heading": null, "text": "I. INTRODUCTION Convolutional neural networks (CNNs) have shown notable results in image recognition: VGG [10] and GoogleNet [11] achieved around 90% accuracy for top-5 classification in ImageNet2012 dataset; AlexNet [7] also achieved around 80% top-5 accuracy with the same dataset.\nOn the other hand, there is an emerging need for embedding or executing CNNs on smart devices such as mobile phones, robots, and embedded devices, in order to give them more intelligence. But the barrier here is that CNNs will require high amounts of memory and computational resources, which are unfortunately hard to be met by small-sized smart devices.\nIn order to tackle this problem, several approaches recently have been proposed based on tensor decomposition, including Tucker decomposition [5] and Canonical Polyadic (CP) decomposition [2], [8]. Kim et al. [5] successfully decompose all the layers by using Tucker decomposition. However, Tucker decomposition does not seem to compress as much as CP-decomposition due to the core tensors. Moreover, CPdecomposition [2], [8] has not been successful in compressing the whole convolution layers of a CNN because of the CP instability issue [8].\nIn this work, we further investigate low-rank CPdecomposition to compress the whole convolution layers of a CNN. Our method, called CP-TPM, is based on lowrank CP-decomposition with Tensor Power Method (TPM) for efficient optimization. We also propose iterative fine-tuning to overcome the CP-decomposition instability. We expect the followings with our method: \u2022 The whole convolution layer decomposition with CP:\nTo the best of our knowledge, the whole convolution layer decomposition has not been successful because of CPdecomposition\u2019s instability [8]. We expect that CP-TPM\ncan decompose the whole convolution layers in contrast to previous CP-decomposition approaches [2], [8]. \u2022 Overcoming CP-decomposition instability by iterative fine-tuning: The instability of CP-decomposition, in our view, is the cause of ill-training, such that the loss does not decrease or even amplified when all of the layers are decomposed by CP and fine-tuned once at the final stage. We believe that CP-decomposition on the whole convolution layers without fine-tuning in between causes the loss to be accumulated and become unrecoverable by the once-and-for-all fine-tuning after decomposition. We empirically prove in the experiment that this instability can be overcome by iterative fine-tuning.\nOur CP-TPM achieves much reduction in memory and computational cost without a very small accuracy drop."}, {"heading": "II. METHOD", "text": "Our approach has two main steps: decomposition and finetuning. We apply the two steps layer-by-layer until all the layers of a CNN are decomposed and fine-tuned. All the layers except the fully connected layers are decomposed by CP decomposition, while the fully connected layers are decomposed by SVD. After each decomposition, the whole network is fine-tuned by back propagation.\nBefore starting the details, these are notations that we use in this paper. Tensors will be notated in calligraphy font capital letters, e.g. X . Matrices will be notated in bold capital letters, e.g. X. Vectors will be notated in bold small letters, e.g. x. Scalars will be notated in regular font small letters, e.g. x. Regular font capital letters, e.g. X , will be used for dimension size."}, {"heading": "A. Kernel Tensor Decomposition", "text": "CP decomposes a tensor as a linear combination of rank one tensors (1). The number of components is the tensor rank R. Each component is an outer product of n vectors, where n corresponds to the number of ways of the target tensor. Rank R determines the amount of weight reduction, i.e., the smaller the R is, the more reduced the weights in a convolution layer.\nX = R\u2211\nr=1\nar \u2297 br \u2297 cr (1)\nar X\niv :1\n70 1.\n07 14\n8v 1\n[ cs\n.L G\n] 2\n5 Ja\nn 20\n17\nConvolution kernel tensor: In general, convolution layers in CNNs map a 3-way input tensor X of size S\u00d7W \u00d7H into a 3-way output tensor Y of size S \u00d7W \u2032 \u00d7H \u2032 using a 4-way kernel tensor K of size T\u00d7S\u00d7D\u00d7D with T corresponding to different output channels, S corresponding to different input channels, and the last two dimensions corresponding to the spatial dimension (for simplicity, we assume square shaped kernels and odd D)\nYt,w\u2032,h\u2032 = S\u2211\ns=1 D\u2211 j=1 D\u2211 i=1 Kt,s,j,iXs,wj ,hi (2)\nwj = (w \u2032 \u2212 1)4+j \u2212 p and hi = (h\u2032 \u2212 1)4+i\u2212 p,\nwhere 4 is stride and p is zero-padding size. CP decomposition: Now the problem is to approximate the kernel tensor K with rank-R CP-decomposition. This can be represented as in (3). Spatial dimensions are not decomposed as they are relatively small (e.g., 3\u00d7 3 or 5\u00d7 5).\nKt,s,j,i = R\u2211\nr=1\nU(1)r,sU (2) r,j,iU (3) t,r (3)\nwhere U(1)r,s , U (2)r,j,i, and U (3) t,r are the three components of sizes R\u00d7 S, R\u00d7D \u00d7D, and T \u00d7R, respectively. Substituting (3) into (2) and performing simple manipulations gives (4) for the approximate evaluation of the convolution (2) from the input tensor X into the output tensor Y .\nYt,w\u2032,h\u2032 = R\u2211\nr=1\nU (3) t,r ( D\u2211 j=1 D\u2211 i=1 U (2)r,j,i( S\u2211 s=1 U(1)r,sXs,wj ,hi)) (4)\nEquation (4) tells us that the output tensor Y is computed by a sequence of three separate convolution operations from the input tensor X with smaller kernels (Fig. 1(b)):\nZr,w,h = S\u2211\ns=1\nU(1)r,sXs,w,h (5)\nZ \u2032 r,w\u2032,h\u2032 = D\u2211\nj=1 D\u2211 i=1 U (2)r,j,iZt,wj ,hi (6)\nYt,w\u2032,h\u2032 = R\u2211\nr=1\nU (3) t,rZ\n\u2032 r,w\u2032,h\u2032 (7)\nwhere Zr,w,h and Z \u2032\nr,w\u2032,h\u2032 are intermediate tensors of sizes R\u00d7W \u00d7H and R\u00d7W \u2032 \u00d7H \u2032, respectively.\nFully Connected Layers (FC): Fully connected layer calculation has the form in (8):\nyT = xTW (8)\nwhere yT is the transpose vector of the output of size M , xT is the transpose vector of the input of size N , and W is a weight matrix of size M \u00d7N .\nDecomposition of FC: As the weights are in a matrix form, we apply Singular Value Decomposition (SVD) to decompose the weight matrix as in (9):\nW = UDVT = (UD)VT (9)\nwhere U and VT are left and right singular matrices of sizes M \u00d7R and R\u00d7N , respectively, and D is a R\u00d7R diagonal singular-value matrix.\nSubstituting (9) into (8) and performing grouping gives (10) for approximate evaluation of a fully connected layer with smaller weight matrices.\nyT = (xT (UD))VT (10)\nTherefore, one fully connected layer can be represented as two fully connected layers as in (11) and (12):\nzT = xT (UD) (11)\nyT = zTVT (12)\nwhere zT is an intermediate layer of size R."}, {"heading": "B. Complexity Analysis", "text": "The initial convolution operation in (2) requires TSD2 parameters and TSD2W \u2032H \u2032 multiplication operation. With CP decomposition, the compression ratio E and speed-up ratio C are given by:\nE = TSD2\nRS +RD2 + TR (13)\nC = TSD2W \u2032H \u2032\nRSWH +RD2W \u2032H \u2032 + TRW \u2032H \u2032 (14)\nThe initial operations in FC of (8) is defined by MN parameters and requires the same number of multiplicationaddition operations. Therefore, the compression ratio E and speed-up ration C are the same and given by:\nE = C = MN\nMR+RN (15)"}, {"heading": "C. Rank Selection", "text": "Ranks play a key role in CP decomposition. If the rank is too high, compression would not be maximized, and if it is too low, the accuracy would drop too much to be recovered by fine-tuning. However, there is no straight algorithm to find the optimal tensor rank [6]. In fact, determining the rank is NP-hard [3].\nThus, we apply a primitive principle in determining the rank: the higher the accuracy loss caused by a layer, the higher rank the layer needs. Rank proportion is the proportion of rank of a layer among other layers. In order to figure out how sensitive a layer is to decomposition, we perform kind of prior decomposition of each layer with a very low, but constant rank (e.g., 5), and then fine-tune the whole network (one epoch)."}, {"heading": "D. Computation of Tensor Decomposition", "text": "In general, tensor decomposition is an optimization problem, i.e., minimizing the difference between the decomposed tensor and the target tensor. We employ Tensor Power Method (TPM) [1]. TPM is known to explain the same variance with less rank compared to ALS [1] because the rank-1 tensors found in the early steps of the process explains most of the variances in the target tensor.\nTPM approximates a target tensorW by adding rank-1 tensors iteratively. First, TPM finds a rank-1 tensor,Wdecomposed, to approximate W by minimizing ||W \u2212 Wdecomposed||2 in a coordinate descent manner. The main idea in the decomposition is that it utilizes the residual Wresidual = W \u2212 Wdecomposed, so that the next iteration approximates the residual tensor Wresidual by minimizing ||Wresidual \u2212 Wdecomposed||2. This continues until the number of rank-1 tensors found is equal to R. More details can be found in [1]."}, {"heading": "E. Fine-Tuning", "text": "As the accuracy will usually drop after decomposition because of the error in decomposition, fine-tuning is needed to recover the accuracy drop. However, as Lebedev et al. [8] pointed out, CP decomposition has not yet successfully applied to the whole convolution layers of a CNN with one-time finetuning because of its instability [5], [8].\nTo overcome the instability, we iteratively fine-tune the whole network after decomposing each layer in order to prevent the errors from getting too big to recover. In the iterative fine-tuning, no layer is frozen because freezing some layers makes the approach greedy, which usually tends to stuck in local minima. As experimented in [12], letting the layers unfrozen shows better results compared to freezing. In this way, all the layers including the already decomposed can adjust to the newly decomposed layer."}, {"heading": "III. EXPERIMENTS", "text": "In this section, we test our approach on AlexNet [7], one of the representative CNNs using Caffe framework [4]. Before describing the main experiments to all of layers, we first briefly introduce AlexNet."}, {"heading": "A. AlexNet Overview", "text": "AlexNet is one of famous object recognition architectures and its pre-trained model is available online in Caffe model zoo [4]. As a baseline, we evaluated the accuracy of the pre-trained model using 50,000 validation images from the ImageNet2012 [9] dataset for 1,000 class classification. Top-1 accuracy is 56.83% and top-5 accuracy is 79.95%. AlexNet has eight layers in total consisting of five convolution layers and three fully connected layers."}, {"heading": "B. Whole Network Decomposition", "text": "In this section, we explain the results of decomposing the whole network with CP-TPM and iterative fine tuning. As mentioned before, to the best of our knowledge, CP-based decomposition has not yet been successful to the whole convolution layer decomposition because of its instability causing the network to be ill-trained. To overcome the problem, we apply fine tuning iteratively so that the errors from each decomposition are not amplified, which usually is with normal one-shot fine tuning performed after the whole convolution layer decomposition.\nFig. 2 shows the steps of our method in AlexNet. Decomposition and fine tuning are performed for each layer. This process iterates from Conv1 to FC8 in sequence.\nIn order to decompose a layer, we first need to figure out the rank. It is desirable for a layer to have as low rank as possible assuming the same level of accuracy. That means each layer should have ranks proportional to its sensitivity, which is defined as the ratio of loss/total loss.\nIn order to calculate sensitivity, we first give the same but very small rank (e.g., 5) to all layers and decompose the layers with that rank, which allows us to get the top-5 accuracy loss as in TABLE II. For example, the total loss for Conv layers is 57.99, and each convolutional layers is assigned with a rank proportional to its sensitivity value from a total of 750 ranks. The same method is also applied to the fully connected layers and in this case we use a total of 900 ranks, which is higher than for the convolutional layers because the FC layers will certainly have much more effects on accuracy.\nAs seen in Table I, We achieved a better compression results in both the number of weights and computation cost compared with [5] while maintaining roughly the same level of accuracy. Specifically, we achieved 1.42% accuracy loss which is less than Tucker accuracy loss of 1.62%, with better compression rate. Our method achieved \u00d76.98 parameter reduction and \u00d73.53 speeding-up, which is better than Tucker-based method that shows \u00d75.46 and \u00d72.67 respectively."}, {"heading": "IV. CONCLUSION", "text": "We have demonstrated that a TPM-based low-rank CPdecomposition combined with the iterative fine-tuning can achieve a whole network decomposition, which has not been tried before with CP. This approach outperforms the previous Tucker-based decomposition method of Kim et al. [5] for the whole network decomposition. Specifically, our method achieves \u00d76.98 parameter reduction and \u00d73.53 speeding-up in\nAlexnet, while Tucker-based method shows \u00d75.46 and \u00d72.67 respectively.\nThere remains much work to be further researched. Firstly, there can be many variations in the iterative fine-tuning, for example, freezing layers after fine-tuning and unfreezing in the final fine-tuning; or starting fine-tuning from the last layer. Secondly, this work has focused more on convolution layers than fully connected layers, where we just have applied SVD. Thus, finding the best algorithms for fully connected layers still needs to be tried. Finally and the most importantly, figuring out the ranks in a systematic manner rather than arbitrary will be another key issue in the further research."}, {"heading": "ACKNOWLEDGEMENT", "text": "This work was supported by the ICT R&D program of MSIP/IITP. [B0101-15-0551, Technology Development of Virtual Creatures with Digital Emotional DNA of Users]."}], "references": [{"title": "Sparse higher-order principal components analysis", "author": ["G. Allen"], "venue": "AISTATS, pages 27\u201336,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems, pages 1269\u20131277,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Most tensor problems are np-hard", "author": ["C.J. Hillar", "L.-H. Lim"], "venue": "Journal of the ACM (JACM), 60(6):45,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y.-D. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "arXiv preprint arXiv:1511.06530,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM review, 51(3):455\u2013500,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Speeding-up convolutional neural networks using fine-tuned cpdecomposition", "author": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I. Oseledets", "V. Lempitsky"], "venue": "arXiv preprint arXiv:1412.6553,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei- Fei"], "venue": "International Journal of Computer Vision (IJCV), 115(3):211\u2013252,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "INTRODUCTION Convolutional neural networks (CNNs) have shown notable results in image recognition: VGG [10] and GoogleNet [11] achieved around 90% accuracy for top-5 classification in ImageNet2012 dataset; AlexNet [7] also achieved around 80% top-5 accuracy with the same dataset.", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": "INTRODUCTION Convolutional neural networks (CNNs) have shown notable results in image recognition: VGG [10] and GoogleNet [11] achieved around 90% accuracy for top-5 classification in ImageNet2012 dataset; AlexNet [7] also achieved around 80% top-5 accuracy with the same dataset.", "startOffset": 122, "endOffset": 126}, {"referenceID": 6, "context": "INTRODUCTION Convolutional neural networks (CNNs) have shown notable results in image recognition: VGG [10] and GoogleNet [11] achieved around 90% accuracy for top-5 classification in ImageNet2012 dataset; AlexNet [7] also achieved around 80% top-5 accuracy with the same dataset.", "startOffset": 214, "endOffset": 217}, {"referenceID": 4, "context": "In order to tackle this problem, several approaches recently have been proposed based on tensor decomposition, including Tucker decomposition [5] and Canonical Polyadic (CP) decomposition [2], [8].", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "In order to tackle this problem, several approaches recently have been proposed based on tensor decomposition, including Tucker decomposition [5] and Canonical Polyadic (CP) decomposition [2], [8].", "startOffset": 188, "endOffset": 191}, {"referenceID": 7, "context": "In order to tackle this problem, several approaches recently have been proposed based on tensor decomposition, including Tucker decomposition [5] and Canonical Polyadic (CP) decomposition [2], [8].", "startOffset": 193, "endOffset": 196}, {"referenceID": 4, "context": "[5] successfully decompose all the layers by using Tucker decomposition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Moreover, CPdecomposition [2], [8] has not been successful in compressing the whole convolution layers of a CNN because of the CP instability issue [8].", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "Moreover, CPdecomposition [2], [8] has not been successful in compressing the whole convolution layers of a CNN because of the CP instability issue [8].", "startOffset": 31, "endOffset": 34}, {"referenceID": 7, "context": "Moreover, CPdecomposition [2], [8] has not been successful in compressing the whole convolution layers of a CNN because of the CP instability issue [8].", "startOffset": 148, "endOffset": 151}, {"referenceID": 7, "context": "We expect the followings with our method: \u2022 The whole convolution layer decomposition with CP: To the best of our knowledge, the whole convolution layer decomposition has not been successful because of CPdecomposition\u2019s instability [8].", "startOffset": 232, "endOffset": 235}, {"referenceID": 1, "context": "We expect that CP-TPM can decompose the whole convolution layers in contrast to previous CP-decomposition approaches [2], [8].", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "We expect that CP-TPM can decompose the whole convolution layers in contrast to previous CP-decomposition approaches [2], [8].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "However, there is no straight algorithm to find the optimal tensor rank [6].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "In fact, determining the rank is NP-hard [3].", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "We employ Tensor Power Method (TPM) [1].", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "TPM is known to explain the same variance with less rank compared to ALS [1] because the rank-1 tensors found in the early steps of the process explains most of the variances in the target tensor.", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "More details can be found in [1].", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "[8] pointed out, CP decomposition has not yet successfully applied to the whole convolution layers of a CNN with one-time finetuning because of its instability [5], [8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[8] pointed out, CP decomposition has not yet successfully applied to the whole convolution layers of a CNN with one-time finetuning because of its instability [5], [8].", "startOffset": 160, "endOffset": 163}, {"referenceID": 7, "context": "[8] pointed out, CP decomposition has not yet successfully applied to the whole convolution layers of a CNN with one-time finetuning because of its instability [5], [8].", "startOffset": 165, "endOffset": 168}, {"referenceID": 11, "context": "As experimented in [12], letting the layers unfrozen shows better results compared to freezing.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "EXPERIMENTS In this section, we test our approach on AlexNet [7], one of the representative CNNs using Caffe framework [4].", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "EXPERIMENTS In this section, we test our approach on AlexNet [7], one of the representative CNNs using Caffe framework [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "AlexNet Overview AlexNet is one of famous object recognition architectures and its pre-trained model is available online in Caffe model zoo [4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 8, "context": "As a baseline, we evaluated the accuracy of the pre-trained model using 50,000 validation images from the ImageNet2012 [9] dataset for 1,000 class classification.", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "0M 724M Tucker[5] VBMF 15 128 0.", "startOffset": 14, "endOffset": 17}], "year": 2017, "abstractText": "Convolutional Neural Networks (CNNs) has shown a great success in many areas including complex image classification tasks. However, they need a lot of memory and computational cost, which hinders them from running in relatively low-end smart devices such as smart phones. We propose a CNN compression method based on CP-decomposition and Tensor Power Method. We also propose an iterative fine tuning, with which we fine-tune the whole network after decomposing each layer, but before decomposing the next layer. Significant reduction in memory and computation cost is achieved compared to state-ofthe-art previous work with no more accuracy loss.", "creator": "LaTeX with hyperref package"}}}