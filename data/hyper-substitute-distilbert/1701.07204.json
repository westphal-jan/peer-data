{"id": "1701.07204", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2017", "title": "Fast Exact k-Means, k-Medians and Bregman Divergence Clustering in 1D", "abstract": "now $ k $ - means coding problem on $ n $ starts as np - hard satisfying any dimension $ \u03c7 \\ ge > $, p, for binary 1d case there exist four polynomial time algorithms. currently balanced state check transition constraint encoding algorithm $ p ( a ^ c ) $ hk programming algorithm that uses $ o ( nk ) $ space. we wrote a new algorithm improving this to $ r ( * \\ log n ) $ \u03c9 _ optimal $ o ( b ) $ space. functions like our algorithm to ask for vector distance distance instead of squared space versus to work for your bregman divergence value optimal.", "histories": [["v1", "Wed, 25 Jan 2017 08:44:04 GMT  (385kb,D)", "https://arxiv.org/abs/1701.07204v1", null], ["v2", "Thu, 16 Feb 2017 20:40:50 GMT  (24kb)", "http://arxiv.org/abs/1701.07204v2", null], ["v3", "Fri, 30 Jun 2017 10:37:16 GMT  (22kb)", "http://arxiv.org/abs/1701.07204v3", null]], "reviews": [], "SUBJECTS": "cs.DS cs.AI cs.LG", "authors": ["allan gr{\\o}nlund", "kasper green larsen", "alexander mathiasen", "jesper sindahl nielsen", "stefan schneider", "mingzhou song"], "accepted": false, "id": "1701.07204"}, "pdf": {"name": "1701.07204.pdf", "metadata": {"source": "CRF", "title": "Fast Exact k-Means, k-Medians and Bregman Divergence Clustering in 1D", "authors": ["Allan Gr\u00f8nlund", "Kasper Green Larsen", "Alexander Mathiasen", "Jesper Sindahl Nielsen", "Stefan Schneider", "Mingzhou Song"], "emails": ["jallan@cs.au.dk.", "larsen@cs.au.dk.", "alexander.mathiasen@gmail.com.", "jasn@cs.au.dk.", "stschnei@cs.ucsd.edu.", "joemsong@cs.nmsu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n07 20\n4v 3\n[ cs\n.D S]\n3 0\nJu n\nto n2O( \u221a lg lgn lg k) time. We generalize the new algorithm(s) to work for the absolute distance instead of squared distance and to work for any Bregman Divergence as well.\n\u2217Aarhus University. Email: jallan@cs.au.dk. Supported by MADALGO - Center for Massive Data Algorithmics, a Center of the Danish National Research Foundation.\n\u2020Aarhus University. Email: larsen@cs.au.dk. Supported by MADALGO, a Villum Young Investigator Grant and an AUFF Starting Grant.\n\u2021Aarhus University. Email: alexander.mathiasen@gmail.com. Supported by MADALGO and an AUFF Starting Grant. \u00a7Aarhus University. Email: jasn@cs.au.dk. Supported by MADALGO. \u00b6University of California, San Diego. Email: stschnei@cs.ucsd.edu. Supported by NSF grant CCF-1213151 from the Division of Computing and Communication Foundations. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.\n\u2016New Mexico State University. Email: joemsong@cs.nmsu.edu"}, {"heading": "1 Introduction", "text": "Clustering is the problem of grouping elements into clusters such that each element is similar to the elements in the cluster assigned to it and not similar to elements in any other cluster. It is one of, if not the, primary problem in the area of machine learning known as Unsupervised Learning and no clustering problem is as famous and widely considered as the k-Means problem: Given a multiset X = {x1, ..., xn} \u2282 R\nd find k centroids M = {\u00b51, ..., \u00b5k} \u2282 R d minimizing \u2211 x\u2208X min\u00b5\u2208M ||x \u2212 \u00b5|| 2. Several NP-Hardness results exist for finding the optimal k-Means clustering in general, forcing one to turn towards heuristics. k-Means is NP-hard even for k = 2 and general dimension [4] and it is also NP-hard for d = 2 and general k [16]. Even hardness of approximation results exist [15, 8]. In [8] the authors show there exists a \u03b5 > 0 such that it is NP-hard to approximate k-Means to within a factor 1+\u03b5 of optimal, and in [15] it is proved that \u03b5 \u2265 0.0013. On the upper bound side the best known polynomial time approximation algorithm for k-Means has an approximation factor of 6.357 [3]. In practice, Lloyd\u2019s algorithm is a popular iterative local search heuristic that starts from some random or arbitrary clustering. The running time of Lloyd\u2019s algorithm is O(tknd) where t is the number of rounds of the local search procedure. In theory, if Lloyd\u2019s algorithm is run to convergence to a local minimum, t could be exponential and there is no guarantee on how well the solution found approximates the optimal solution [6, 20]. Lloyd\u2019s algorithm is often combined with the effective seeding technique for selecting initial centroids due to [7] that gives an expected O(lg k) approximation ratio for the initial clustering, which can then improved further by Lloyd\u2019s algorithm.\nFor the one-dimensional case, the k-Means problem is not NP-hard. In particular, there is an O(kn2) time and O(kn) space dynamic programming solution for the 1D case, due to work by [22]. The 1D kMeans problem is encountered surprisingly often in practice, some examples being in data analysis in social networks, bioinformatics and retail market [5, 13, 18].\nIt is only natural to try other reasonable distance measures for the data considered and define different clustering problems. There are many other choices than the sum of squares of the Euclidian distances that define k-Means. For instance, one could use any Lp norm instead. The special case of p = 1 is known as k-Medians clustering and has also received considerable attention. The k-Medians problems is also NP-hard and the best polynomial time approximation algorithms has an approximation factor of 2.633 [8].\nIn [9] the authors consider and define clustering with Bregman Divergences. Bregman Divergence generalize squared Euclidian distance and thus Bregman Clusterings include the k-Means problem, as well as a wide range of other clustering problems that can be defined from Bregman Divergences like e.g. clustering with KullbackLeibler divergence as the cost. Interestingly, the heuristic local search algorithm for Bregman Clustering [9] is basically the same approach as Lloyd\u2019s algorithm for k-Means. Clustering with Bregman Divergences is clearly NP-Hard as well since it includes k-Means clustering. We refer the reader to [9] for more about the general problem. For the 1D version of the problem, [17] generalized the algorithm from [22] to the k-Medians problems and Bregman Divergences achieving the same O(kn2) time and O(kn) space bounds."}, {"heading": "1.1 Our Results", "text": "In this paper we give theoretically and practically efficient algorithm for 1D clustering problems, in particular k-Means.\nThe k-Means clustering problem in 1D is defined as follows. Given X = {x1, ..., xn} \u2282 R and k, find centroids M = {\u00b51, ..., \u00b5k} \u2282 R minimizing the cost\n\u2211 x\u2208X min \u00b5\u2208M (x \u2212 \u00b5)2\nThe main results of this paper are new fast algorithms for 1D k-Means. First we give an algorithm that computes the optimal k-Means clustering that runs in O(n lg n + kn) time using optimal O(n) space, or O(kn) time if the input is already sorted. The algorithm also computes the cost of the optimal clustering using k\u2032 clusters for all k\u2032 \u2264 k. This is relevant for instance for model selection of the right k. This is an\nimprovement by a factor of n in time and k in space compared to the existing solution (which also supports computing the cost for all k\u2032 \u2264 k. The constant factors hidden by the O-notation are small and we expect this algorithm to be very efficient in practice. Second, we show how to compute an optimal k-Means clustering in n2O( \u221a lg lgn lg k) time using O(n) space. This algorithm is mainly of theoretical interest as we expect constants to be rather large. As opposed to the O(kn) time algorithm, this algorithm does not compute the optimal costs for using k\u2032 clusters for all k\u2032 \u2264 k.\nThe n2O( \u221a lg lgn lg k) time algorithm relates to a natural regularized version of k-Means clustering where instead of specifying the number of clusters beforehand, we instead specify a cost of using an extra cluster and then minimize the cost of the clustering plus the cost of the number of clusters used. Formally, the problem is as follows: Given X = {x1, ..., xn} \u2282 R and \u03bb, compute the optimal regularized clustering:\nargmin k,M={\u00b51,...,\u00b5k}\n\u2211 x\u2208X min \u00b5\u2208M (x\u2212 \u00b5)2 + \u03bbk\nWe show that this problem is solvable in O(n) time if the input is sorted. In 1D, Lloyd\u2019s algorithm can be implemented to run in O(n lg n+ tk lgn) time where t is the number of rounds, and we expect that our algorithm can compute the optimal clustering for reasonable k in essentially the same time as Lloyd\u2019s algorithm can approximate it.\nThe k-Medians problem is to compute a clustering that minimize the sum of absolute distances to the centroid, i.e. compute M = {\u00b51, ..., \u00b5k} \u2282 R that minimize\n\u2211 x\u2208X min \u00b5\u2208M |x\u2212 \u00b5|\nOur algorithms generalize naturally to solve this problem in the same time bounds as for the k-means problem.\nLet f be a differentiable real-valued strictly convex function. The Bregman Divergence Df induced by f is defined as\nDf (x, y) = f(x)\u2212 f(y)\u2212\u2207f (y)(x \u2212 y)\nNotice that the Bregman Divergence induced from f(x) = x2, gives squared Euclidian Distance (k-Means). Bregman divergences are not metrics since they are not symmetric in general and the triangle inequality is not necessarily satisfied. They do however have many redeeming qualities, for instance Bregman Divergences are convex in the first argument, albeit not the second, see [9, 10] for a more comprenhensive treatment.\nThe Bregman Clustering problem as defined in [9] is to find k centroids M = {\u00b51, ..., \u00b5k} that minimize\n\u2211 x\u2208X min \u00b5\u2208M Df (x, \u00b5)\nwhere Df is a Bregman Divergence. For our case, where the inputs x, y \u2208 R, we assume that computing a Bregman Divergence, i.e. evaluating f and its derivative, takes constant time. We show that our algorithms naturally generalize to 1D clustering using any Bregman Divergence to define the cluster cost while still maintaing the same running time as for k-Means.\nImplementation. An independent implementation of the O(n lg n+kn) time algorithm is available in the R package Ckmeans.1d.dp [21]. The implementation is for k-Means clustering, and uses O(kn) space."}, {"heading": "1.2 Outline", "text": "In Section 2 we describe the existing O(kn2) time algorithm for 1D k-Means clustering that uses O(kn) space. In Section 3 we show how to compute the same output as the old algorithm using only O(kn) time and O(n) space. Then we show how to improve the running time to O(n2O( \u221a lg lgn lg k)). Finally, in Section 4 we show how our new algorithms generalizes to different cluster costs than squared Euclidian distance.\n2 The O(kn2) Dynamic Programming Algorithm\nIn this section, we describe the previous O(kn2) time and O(kn) space algorithm presented in [22]. We also introduce the definitions and notation we use in our new algorithm. We will always assume sorted input x1 \u2264 ... \u2264 xn \u2208 R. If the input is not sorted, we start by sorting it in O(n lg n) time. We also remark that there could be many ways of partitioning the point set and computing centroids that achieve the same cost. This is for instance the case if the input is n identical points. The task at hand is to find any optimal solution.\nLet CC(i, j) = \u2211j \u2113=i(x\u2113\u2212\u00b5i,j) 2 be the cost of grouping xi, ..., xj into one cluster with the optimal choice\nof centroid, \u00b5i,j = 1 j\u2212i+1 \u2211j \u2113=i x\u2113, the arithmetic mean of the points.\nLemma 1. There is an O(n) space data structure that can compute CC(i, j) in O(1) time for any i \u2264 j using O(n) time preprocessing.\nProof. This is a standard application of prefix sums and works as follows. By definition,\nCC(i, j) =\nj \u2211\n\u2113=i\n(x\u2113 \u2212 \u00b5i,j) 2 =\nj \u2211\n\u2113=i\nx2\u2113 + \u00b5 2 i,j \u2212 2x\u2113\u00b5i,j = (j \u2212 i+ 1)\u00b5 2 i,j + \u00b5i,j\nj \u2211\n\u2113=i\nx\u2113 +\nj \u2211\n\u2113=i\nx2\u2113 .\nWith access to prefix sum arrays of x1, . . . , xn and x 2 1, . . . , x 2 n both the centroid \u00b5i,j and the cost is easily computed in constant time ."}, {"heading": "2.1 Algorithm Sketch", "text": "The algorithm computes the optimal clustering using i clusters for all prefixes of input points x1, . . . , xm, for m = 1, . . . , n, and for all i = 1, . . . , k using Dynamic Programming as follows.\nLet D[i][m] be the cost of optimally clustering x1, ..., xm into i clusters. For i = 1 the cost of optimally clustering x1, ..., xm into one cluster is the cluster cost CC(1,m). That is, D[1][m] = CC(1,m) for all m. This can be computed in O(n) time by Lemma 1.\nFor i > 1\nD[i][m] = m\nmin j=1 D[i\u2212 1][j \u2212 1] + CC(j,m) (1)\nNotice that D[i \u2212 1][j \u2212 1] is the cost of optimally clustering x1, ..., xj\u22121 into i \u2212 1 clusters and CC(j,m) is the cost of clustering xj , ..., xm into one cluster. This makes xj the first point in the last and rightmost cluster. Let T [i][m] be the argument that minimizes (1)\nT [i][m] := arg m\nmin j=1 D[i \u2212 1][j \u2212 1] + CC(j,m) (2)\nIt is possible there exists multiple j obtaining same minimal value for (1). To make the optimal clustering C\u0303(m) unique, such ties are broken in favour of smaller j.\nNotice xT [i][m] is the first point in the rightmost cluster of the optimal clustering. Thus, given T one can find the optimal solution by standard backtracking:\nX\u0303k = {xT [k][n], ..., xn},\nX\u0303k\u22121 = {xT [k\u22121][T [k][n]\u22121], ..., xT [k][n]\u22121}\n...\nHere X\u0303i is the i\u2019th cluster in the optimal clustering. One can naively compute each entry of D and T using (1) and (2). This takes O(n) time for each cell, thus D and T can be computed in O(kn2) time using O(kn) space. This is exactly what is described in [22]."}, {"heading": "3 New Algorithms", "text": "The idea of the first new algorithm is simply to compute the tables D and T faster, by reducing the time to compute each row of D and T to O(n) time instead of O(n2) time. This improvement exploits a monotonicity property of the values stored in a row of T . This is explained in Section 3.1, resulting in an O(kn) time and O(kn) space solution, assuming sorted inputs. Section 3.2 then shows how to reduce the space usage to just O(n) while retaining O(kn) running time. In Section 3.3 we show that the same property allows us to solve 1D k-Means for k = \u2126(lgn) in, n2O( \u221a lg lg n lg k) time and linear space, and solve the regularized version of 1D k-Means in O(n) time."}, {"heading": "3.1 Faster Algorithm From Monotone Matrices", "text": "In this section we reduce the problem of computing a row of D and T to searching an implicitly defined n\u00d7 n matrix of a special form, which allows us to compute each row of D and T in linear time.\nDefine Ci[m][j] as the cost of the optimal clustering of x1, . . . , xm using i clusters, restricted to having the rightmost cluster (largest cluster center) contain the elements xj , . . . , xm. For convenience, we define Ci[m][j] for j > m as the cost of clustering x1, . . . , xm into i\u2212 1 clusters, i.e. the last cluster is empty. This means that Ci satisfies:\nCi[m][j] = D[i\u2212 1][min{j \u2212 1,m}] + CC(j,m)\nwhere by definition CC(j,m) = 0 when j > m (which is consistent with the definition in Section 2). We have that D[i][m] relates to Ci as follows:\nD[i][m] = min j Ci[m][j]\nwhere ties are broken in favor of smaller j (as defined in Section 2.1). This means that when we compute a row of D and T , we are actually computing minj Ci[m][j] for all m = 1, . . . , n. We think of Ci as an n\u00d7 n matrix with rows indexed by m and columns indexed by j. With this interpretation, computing the i\u2019th row of D and T corresponds to computing for each row r in Ci, the column index c that corresponds to the smallest value in row r. In particular, the entries D[i][m] and T [i][m] correpond to the value and the index of the minimum entry in the m\u2019th row of Ci respectively. The problem of finding the minimum value in every row of a matrix has been studied before [1]. First we need the definition of a monotone matrix.\nDefinition 1. [1] Let A be a matrix with real entries and let argmin(i) be the index of the leftmost column containing the minimum value in row i of A. A is said to be monotone if a < b implies that argmin(a) \u2264 argmin(b). A is totally monotone if all of its submatrices are monotone.1\nIn [1], the authors showed the following:\nTheorem 1. [1] Finding argmin(i) for each row i of an arbitrary n\u00d7m monotone matrix requires \u0398(m lgn) time, whereas if the matrix is totally monotone, the time is O(m) when m > n and is O(m(1 + lg(n/m))) when m < n.\nThe fast algorithm for totally monotone matrices is known as the SMAWK algorithm and we will refer to it by that (cool) name.\nLet\u2019s relate this to the 1D k-Means clustering problem. That Ci is monotone means that if we consider the optimal clustering of the points x1, . . . , xa with i clusters, then if we start adding more points xa+1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 xb after xa, then the first (smallest) point in the last of the i clusters can only increase (move right) in the new optimal clustering of x1, . . . , xb. This sounds like it should be true for 1D k-Means and it turns out it is. Thus, applying the algorithm for monotone matrices, we can fill a row of D and T in O(n lg n) time leading to an O(kn lg n) time algorithm for 1D k-Means, which is already a great improvement.\n1In [1] the authors use the maximum instead of the minimum\nHowever, as we show below, the matrix Ci induced by the 1D k-Means problem is in fact totally monotone:\nLemma 2. The matrix Ci is totally monotone.\nProof. As [1] remarks, a matrix A is totally monotone if all its 2 \u00d7 2 submatrices are monotone. To prove that Ci is totally monotone, we thus need to prove that for any two row indices a, b with a < b and two column indices u, v with u < v, it holds that if Ci[a][v] < Ci[a][u] then Ci[b][v] < Ci[b][u].\nNotice that these values correspond to the costs of clustering elements x1, . . . , xa and x1, . . . , xb, starting the rightmost cluster with element xv and xu respectively. Since Ci[m][j] = D[i\u22121][min{j\u22121,m}]+CC(j,m), this is the same as proving that\nD[i \u2212 1][min{v \u2212 1,m}] + CC(v, a) < D[i\u2212 1][min{u\u2212 1,m}] + CC(u, a) \u21d2\nD[i\u2212 1][min{v \u2212 1,m}] + CC(v, b) < D[i\u2212 1][min{u\u2212 1,m}] + CC(u, b)\nwhich is true if we can prove that CC(v, b)\u2212CC(v, a) \u2264 CC(u, b)\u2212CC(u, a). Rearranging terms, what we need to prove is that for any a < b and u < v, it holds that:\nCC(v, b) + CC(u, a) \u2264 CC(u, b) + CC(v, a). (3)\nThis is the property known as the concave (concave for short) property [24, 12, 23] and has been used to significantly speed up algorithms, including Dynamic Programming algorithms, for for other problems. We start by handling the special case where v > a. In this case, we have by definition that CC(v, a) = 0, thus we need to show that CC(v, b) + CC(u, a) \u2264 CC(u, b). This is the case since any point amongst xu, . . . , xb is included in at most one of xv, . . . , xb and xu, . . . , xa (since a < v). Thus CC(v, b) + CC(u, a) is the cost of taking two disjoint and consecutive subsets of the points xu, . . . , xb and clustering the two sets using the optimal choice of centroid in each. Clearly this cost is less than clustering all the points using one centroid.\nWe now turn to the general case where u < v \u2264 a < b. Let \u00b5v,a be the mean of xv, . . . , xa and \u00b5u,b be the mean of xu, . . . , xb and assume that \u00b5v,a \u2264 \u00b5u,b (the other case is symmetric). Finally, let CC(w, z)\u00b5 = \u2211z \u2113=w(x\u2113 \u2212 \u00b5)\n2 denote the cost of grouping the elements xw, . . . , xz in a cluster with centroid \u00b5. Split the cost CC(u, b) into the cost of the elements xu, . . . , xv\u22121 and the cost of the elements xv, . . . , xb as\nCC(u, b) =\nb \u2211\n\u2113=u\n(x\u2113 \u2212 \u00b5u,b) 2 =\nv\u22121 \u2211\n\u2113=u\n(x\u2113 \u2212 \u00b5u,b) 2 +\nb \u2211\n\u2113=v\n(x\u2113 \u2212 \u00b5u,b) 2 = CC(u, v \u2212 1)\u00b5u,b + CC(v, b)\u00b5u,b .\nWe trivially get CC(v, b)\u00b5u,b \u2265 CC(v, b) since CC(v, b) is the cost using the optimal centroid. Secondly,\nCC(u, v \u2212 1)\u00b5u,b + CC(v, a) \u2265 CC(u, v \u2212 1)\u00b5v,a + CC(v, a) = CC(u, a)\u00b5v,a \u2265 CC(u, a)\nsince \u00b5v,a \u2264 \u00b5u,b and all elements xu, . . . , xv\u22121 are less than or equal to \u00b5v,a (since \u00b5v,a is the mean of points xv, . . . , xa that all are greater than xu, . . . , xv\u22121). Combining the results, we see that:\nCC(v, b) + CC(u, a) \u2264 CC(v, b)\u00b5u,b + CC(u, v \u2212 1)\u00b5u,b + CC(v, a) = CC(u, b) + CC(v, a).\nThis completes the proof.\nTheorem 2. Computing an optimal k-Means clustering of a sorted input of size n for takes O(kn) time.\nBy construction the cost of the optimal clustering is computed for all k\u2032 \u2264 k. If we store the T table the cluster centers for any k\u2032 \u2264 k can be extracted in O(k\u2032) time."}, {"heading": "3.2 Reducing Space Usage", "text": "In the following, we show how to reduce the space usage to just O(n) while maintaining O(kn) running time using a space reduction technique of Hirschberg [11]. First observe that each row of T and D only refers to the previous row. Thus one can clearly \u201cforget\u201drow i\u2212 1 when we are done computing row i. The problem is that if we don\u2019t store all of T , we cannot backtrack and find the optimal solution. In the following, we present an algorithm that avoids the table T entirely.\nOur key observation is the following: Assume k > 1 and that for every prefix x1, . . . , xm, we have computed the optimal cost of clustering x1, . . . , xm into \u230ak/2\u230b clusters. Note that this is precisely the set of values stored in the \u230ak/2\u230b\u2019th row of D. Assume furthermore that we have computed the optimal cost of clustering every suffix xm, . . . , xn into k \u2212 \u230ak/2\u230b clusters. Let us denote these costs by D\u0303[k \u2212 \u230ak/2\u230b][m] for m = 1, . . . , n. Then clearly the optimal cost of clustering x1, . . . , xn into k clusters is given by:\nD[k][n] = n\nmin j=1 D[\u230ak/2\u230b][j] + D\u0303[k \u2212 \u230ak/2\u230b][j + 1]. (4)\nOur main idea is to first compute row \u230ak/2\u230b of D and row k\u2212\u230ak/2\u230b of D\u0303 using linear space. From these two, we can compute the argument j minimizing (4). We can then split the reporting of the optimal clustering into two recursive calls, one reporting the optimal clustering of points x1, . . . , xj into \u230ak/2\u230b clusters, and one call reporting the optimal clustering of xj+1, . . . , xn into k \u2212 \u230ak/2\u230b clusters. When the recursion bottoms out with k = 1, we can clearly report the optimal clustering using linear space and time as this is just the full set of points.\nFrom Section 3.1 we already know how to compute row \u230ak/2\u230b of D using linear space: Simply call SMAWK to compute row i of D for i = 1, . . . , \u230ak/2\u230b, where we throw away row i \u2212 1 of D (and don\u2019t even store T ) when we are done computing row i. Now observe that table D\u0303 can be computed by taking our points x1, . . . , xn and reversing their order by negating the values. This way we obtain a new ordered sequence of points X\u0303 = x\u03031 \u2264 x\u03032 \u2264 \u00b7 \u00b7 \u00b7 \u2264 x\u0303n where x\u0303i = \u2212xn\u2212i+1. Running SMAWK repeatedly for i = 1, . . . , k\u2212\u230ak/2\u230b on the point set X\u0303 produces a table D\u0302 such that D\u0302[i][m] is the optimal cost of clustering x\u03031, . . . , x\u0303m = \u2212xn, . . . ,\u2212xn\u2212m+1 into i clusters. Since this cost is the same as clustering xn\u2212m+1, . . . , xn into i clusters, we get that the (k \u2212 \u230ak/2\u230b)\u2019th row of D\u0302 is identical to the i\u2019th row of D\u0303 if we reverse the order of the entries.\nTo summarize our algorithm for reporting the optimal clustering, do as follows: Let L be an initially empty output list of clusters. If k = 1, append to L a cluster containing all points. Otherwise (k > 1), use SMAWK on x1, . . . , xn and \u2212xn, . . . ,\u2212x1 to compute row \u230ak/2\u230b of D and row k \u2212 \u230ak/2\u230b of D\u0303 using linear space (by evicting row i \u2212 1 from memory when we have finished computing row i) and O(kn) time. Compute the argument j minimizing (4) in O(n) time. Evict row \u230ak/2\u230b of D and row k \u2212 \u230ak/2\u230b of D\u0303 from memory. Recursively report the optimal clustering of points x1, . . . , xj into \u230ak/2\u230b clusters (which appends the output to L). When this terminates, recursively report the optimal clustering of points xj+1, . . . , xn into k \u2212 \u230ak/2\u230b clusters. When the algorithm terminates, L contains the optimal clustering of x1, . . . , xn into k clusters.\nAt any given time, our algorithm uses only O(n) space. To see this, first note that we evict all memory used to compute the value j minimizing (4) before recursing. Furthermore, we complete the first recursive call (and evict all memory used) before starting the second. Finally, for the recursion, we don\u2019t need to make a copy of points x1, . . . , xj . It suffices to remember that we are only working on the subset of inputs x1, . . . , xj .\nNow let F (n, k) denote the time used by the above algorithm to compute the optimal clustering of n sorted points into k clusters. Then there is some constant C > 0 such that F (n, k) satisfies the recurrence:\nF (n, 1) \u2264 Cn,\nand for k > 1: F (n, k) \u2264\nn max j=1 F (j, \u230ak/2\u230b) + F (n\u2212 j, k \u2212 \u230ak/2\u230b) + Cnk.\nWe claim that F (n, k) satisfies F (n, k) \u2264 3Ckn. We prove the claim by induction in k. The base case k = 1 follows trivially by inspection of the formula for F (n, 1). For the inductive step k > 1, we use the induction hypothesis to conclude:\nF (n, k) \u2264 n\nmax j=1 3Cj\u230ak/2\u230b+ 3C(n\u2212 j)(k \u2212 \u230ak/2\u230b) + Cnk\n\u2264 n\nmax j=1 3Cj\u2308k/2\u2309+ 3C(n\u2212 j)\u2308k/2\u2309+ Cnk\n= 3Cn\u2308k/2\u2309+ Ckn.\nFor k > 1, we have that \u2308k/2\u2309 \u2264 (2/3)k, therefore:\nF (n, k) \u2264 3Cn(2/3)k + Ckn\n= 3Ckn.\nWhich is what we needed to prove.\nTheorem 3. Computing an optimal k-Means clustering of a sorted input of size n takes O(kn) time and uses O(n) space.\nNote to compute the cost of the optimal clustering for all k\u2032 \u2264 k we ensure that we never delete the last column of the cost matrix D which requires an additional O(k) = O(n) space."}, {"heading": "3.3 Even Faster Algorithm", "text": "In this section we show that the concave property we proved for the cluster costs yields and algorithm for computing the optimal k-Means clustering for one given k = \u2126(lgn) in n2O( \u221a lg lg n lg k) time. The result follows almost directly from [19]. In [19] Schieber gives an algorithm with the aforementioned running time for the problem of finding the shortest path of fixed length k in a directed acyclic graph with nodes 1, . . . , n where the weights, w(i, j), satisfy the concave property and are represented as a function that returns the weight of a given edge in constant time.\nTheorem 4 ([19]). Computing a minimum weight path of length k between any two nodes in a directed acyclic graph of size n where the weights satisfy the concave property takes n2O( \u221a lg lgn lg k) time using O(n) space.\nWe reduce the 1D k-Means problem to a directed graph problem as follows. Sort the input in O(n lg n) time and let x1 \u2264 x2 \u2264 . . . xn denote the sorted input sequence. For each input xi we associate a node vi and add an extra node vn+1. Now define the weight of the edge from vi to vj as the cost of clustering xi, . . . , xj\u22121 in one cluster, which is CC(i, j\u2212 1). Each edge weight is computed in constant time and by the proof of Lemma 2, particularly Equation 3, the edge weights satisfy the monge concave property. Finally, to compute the optimal clustering we use Schiebers algorithm to compute the lowest weight path with k edges from v1 to vn+1.\nTheorem 5. Computing an optimal k-Means clustering of an input of size n for given k = \u2126(lg n) takes n2O( \u221a lg lgn lg k) time using O(n) space.\nIt is relevant to briefly consider parts of Schiebers algorithm and how it relates to k-Means clustering, in particular a regularized version of the problem. Schiebers algorithm relies crucially on algorithms that given a directed acyclic graph where the weights satisfy the concave property computes a minimum weight path in O(n) time [23, 14]. Note the only difference in this problem compared to above, is that the search is not restricted to paths of k edges only."}, {"heading": "3.3.1 Regularized Clustering", "text": "Consider a regularized version of the k-Means clustering problem where we instad of providing the number of clusters k specify the cost of a cluster and ask to minimize the cost of the clustering plus the penalty \u03bb for each cluster used. For simplicity, assume all the input points are distinct.\nIf we set \u03bb = 0 the optimal clustering has cost zero and use a cluster for each input point. If we let \u03bb increase towards infinity, the optimal number of clusters used in the optimal solution monotonically decrease towards one (zero clusters is not well defined). Let dmin be the smallest distance between points in the input. The optimal cost of using n \u2212 1 clusters is then d2min/2. When \u03bb > \u03bbn = d 2 min/2 it is less costly to use only n\u2212 1 clusters since the added clustering of using one less cluster is smaller than the cost of a cluster. Letting \u03bb increase again will inevitably lead to a miminum value \u03bbn\u22121 < \u03bbn such that for \u03bb > \u03bbn\u22121 only n \u2212 2 clusters is used in the optimal solution. Following the same pattern \u03bbn\u22121 is the difference between the optimal cost using n \u2212 2 clusters and n \u2212 1 clusters. Continuing this yields the very interesting event sequence 0 < \u03bbn < \u00b7 \u00b7 \u00b7 < \u03bb1 that encodes the only relevant choices for the regularization parameter. Note that the O(nk) algorithm actually yields \u03bb1, . . . , \u03bbk since it computes the optimal cost for all k\n\u2032 \u2264 k. In the reduction to the directed graph problem, adding a cost of \u03bb for each cluster used corresponds to adding \u03bb to the weight of each edge. Note that the edge weights clearly still satisfy the concave property. Thus, solving the regularized version of k-Means clustering correpoonds to finding the shortest path (of any length) in a directed acyclic graph where the weights satisfy the concave property. By the algorithms in [23, 14] this takes O(n) time.\nTheorem 6. Computing an optimal regularized 1D k-Means clustering of a sorted input of size n takes O(n) time.\nNow notice if we actually use \u03bbk\u22121 as the cost per cluster, or any \u03bb \u2208 [\u03bbk\u22121, \u03bbk[ there is an optimal solution using k clusters which is an optimal k-Means clustering. This means that if the inputs are integers, we can solve the 1D k-Means problem by a simple application of binary search in O(n lgU) time where U is the universe size [2]."}, {"heading": "4 Extending to More Distance Measures", "text": "In the following we show how to generalize our algorithm to Bregman Divergences and sum of absolute distances while retaining the same running time and space usage."}, {"heading": "4.1 Bregman Divergence and Bregman Clustering", "text": "In this section we show how our algorithm generalizes to any Bregman Divergence. First, let us remind ourselves what a Bregman Divergence and a Bregman Clustering is. Let f be a differentiable real-valued strictly convex function. The Bregman Divergence Df defined by f is defined as\nDf (x, y) = f(x)\u2212 f(y)\u2212\u2207f (y)(x \u2212 y)\nBregman Clustering. The Bregman Clustering problem as defined in [9], is to find a clustering, M = {\u00b51, ..., \u00b5k}, that minimize\n\u2211 x\u2208X min \u00b5\u2208M Df (x, \u00b5)\nNotice that the cluster center is the second argument of the Bregman Divergence. This is important since Bregman Divergences are not in general symmetric.\nFor the purpose of 1D clustering, we mention two important properties of Bregman Divergences. For any Bregman Divergence, the unique element that minimizes the summed distance to a multiset of elements is\nthe mean of the elements, exactly as it was for squared Euclidian distance. This is in one sense the defining property of Bregman Divergences [9].\nThe second important is the linear separator property, which is very important for clustering with Bregman Divergences but also very relevavant to Bregman Voronoi Diagrams [9, 10].\nLinear Separators For Bregman Divergences. For all Bregman divergences, the locus of points that are equidistant to two fixed points \u00b51, \u00b52 in terms of a Bregman divergence is given by\n{x \u2208 X | Df (x, p) = Df (x, q)} = {x \u2208 X | x(\u2207f (\u00b51)\u2212\u2207f (\u00b52)) = f(\u00b51)\u2212 \u00b51\u2207f (\u00b51)\u2212 f(\u00b52) + \u00b52\u2207f (\u00b52)}\nwhich corresponds to a hyperplane. Also, the points \u00b51, \u00b52 sits on either side of the hyperplane and the Voronoi cells defined using Bregman divergences are connected.\nThis means, in particular, that between any two points in 1D, \u00b51 < \u00b52, there is a hyperplane (point) h with \u00b51 < h < \u00b52 and all points smaller than h are closer to \u00b51 and all points larger than h are closer to \u00b52. We capture what we need from this observation in a simple \u201cdistance\u201d lemma:\nLemma 3. Given two fixed real numbers \u00b51 < \u00b52, then for any point xr \u2265 \u00b52, we have Df (xr , \u00b51) > Df (xr, \u00b52), and for any point xl \u2264 \u00b51 we have Df(xl, \u00b51) < Df(xl, \u00b52)\nComputing Cluster Costs for Bregman Divergences. Since the mean minizes Bregman Divergences, the centroids used in optimal clusterings are unchanged compared to the k-Means case. The prefix sums idea used to implement the data structure used for Lemma 1 generalizes to Bregman Divergences as observed in [17] (under the name Summed Area Tables). The formula for computing the cost of grouping the points xi, . . . , xj in one cluster is as follows. Let \u00b5i,j = 1 j\u2212i+1 \u2211j\n\u2113=i x\u2113 be the arithmetic mean of the points xi, . . . , xj , then\nCC(i, j) =\nj \u2211\n\u2113=i\nDf (x\u2113, \u00b5i,j) =\nj \u2211\n\u2113=i\nf(x\u2113)\u2212 f(\u00b5i,j)\u2212\u2207f (\u00b5i,j)(x\u2113 \u2212 \u00b5i,j)\n=\n(\nj \u2211\n\u2113=i\nf(x\u2113)\n)\n\u2212 (j \u2212 i+ 1)f(\u00b5i,j)\u2212\u2207f (\u00b5i,j)\n((\nj \u2211\n\u2113=i\nx\u2113\n)\n\u2212 (j \u2212 i+ 1)\u00b5i,j\n)\nIt follows that the Bregman Divergence cost of a consecutive subset of input points and the centroid can be computed in in constant time with stored prefix sums for x1, . . . , xn and f(x1), . . . , f(xn).\nMonge Concave - Totally Monotone Matrix. The only properties we used in Section 3.1 to prove the monge concave property and that the matrix Ci is totally monotone, is that the mean is the minimizer of the sum of distances to a multiset of points, and that\nCC(u, v \u2212 1)\u00b5u,b + CC(v, a) \u2265 CC(u, v \u2212 1)\u00b5v,a + CC(v, a) = CC(u, a)\u00b5v,a\nwhen \u00b5v,a \u2264 \u00b5u,b and all elements in xu, . . . , xv\u22121 \u2264 \u00b5v,a. This is clearly still true by Lemma 3. It follows that the algorithms we specified for 1D k-Means generalize to any Bregman Divergence.\n4.2 k-Medians - Clustering with sum of absolute values\nFor the k-Medians problem we replace the the sum of squared Euclidian distances with the sum of absolute distances. Formally, the k-Medians problem is to compute a clustering, M = {\u00b51, ..., \u00b5k}, minimizing\n\u2211 x\u2208X min \u00b5\u2208M |x\u2212 \u00b5|\nNote that in 1D, all Lp norms are the same and reduce to this case. Also note that the minimizing centroid for a cluster is no longer the mean of the points in that cluster, but the median. To solve this problem, we\nchange the centroid to be the median, and if there an even number of points, we fix the median to be the exact middle point between the two middle elements, making the choice of centroid unique.\nAs for Bregman Divergences, we need to show that we can compute the cost CC(i, j) with this new cost in constant time. Also, we need to compute the centroid in constant time and argue that the cost is monge moncave which implies the implicit matrix Ci is totally monotone. The arguments are essentially the same, but for completeness we briefly cover them below.\nComputing Cluster Costs for Absolute Distances. Not surprisingly, using prefix sums still allow constant time computation of CC(i, j). Let mi,j = j+i 2 , and compute the centroid as \u00b5i,j = x\u230ami,j\u230b+x\u2308mi,j\u2309 2\nCC(i, j) =\nj \u2211\n\u2113=i\n|x\u2113 \u2212 \u00b5i,j | =\n\u230ami,j\u230b \u2211\n\u2113=i\n\u00b5i,j \u2212 x\u2113 +\nj \u2211\n\u2113=1+\u230ami,j\u230b x\u2113 \u2212 \u00b5i,j\nwhich can be computed in constant time with access to a prefix sum table of x1, . . . , xn. This was also observed in [17].\nMonge Concave - Totally Monotone Matrix. The monge concave and totally monotone matrix argument above for Bregman Divergences (and for squared Euclidian distance) remain valid since first of all, we still have xu, . . . , xv\u22121 \u2264 \u00b5v,a as \u00b5v,a is the median of points all greater than xu, . . . , xv\u22121. Furthermore, it still holds that when \u00b5v,a \u2264 \u00b5u,b and all elements xu, . . . , xv\u22121 are less than or equal to \u00b5v,a, then:\nCC(u, v \u2212 1)\u00b5u,b + CC(v, a) \u2265 CC(u, v \u2212 1)\u00b5v,a + CC(v, a) = CC(u, a)\u00b5v,a\nIt follows that the algorithms we specified for 1D k-Means generalize to the 1D k-Median problem."}, {"heading": "Acknowledgements", "text": "We wish to thank Pawel Gawrychowski for pointing out important earlier work on concave property."}], "references": [{"title": "Geometric applications of a matrixsearching algorithm", "author": ["A. Aggarwal", "M.M. Klawe", "S. Moran", "P. Shor", "R. Wilber"], "venue": "Algorithmica, 2(1):195\u2013208", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1987}, {"title": "Finding a minimum-weightk-link path in graphs with the concave monge property and applications", "author": ["A. Aggarwal", "B. Schieber", "T. Tokuyama"], "venue": "Discrete & Computational Geometry, 12(3):263\u2013280", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Better guarantees for k-means and euclidean k-median by primal-dual algorithms", "author": ["S. Ahmadian", "A. Norouzi-Fard", "O. Svensson", "J. Ward"], "venue": "CoRR, abs/1612.07925", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Np-hardness of euclidean sum-of-squares clustering", "author": ["D. Aloise", "A. Deshpande", "P. Hansen", "P. Popat"], "venue": "Machine Learning, 75(2):245\u2013248", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Analysis of ego network structure in online social networks", "author": ["V. Arnaboldi", "M. Conti", "A. Passarella", "F. Pezzoni"], "venue": "Privacy, security, risk and trust ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "How slow is the k-means method? In Proceedings of the Twenty-second Annual Symposium on Computational Geometry", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SCG \u201906, pages 144\u2013153, New York, NY, USA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027\u20131035. Society for Industrial and Applied Mathematics", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "The hardness of approximation of euclidean k-means", "author": ["P. Awasthi", "M. Charikar", "R. Krishnaswamy", "A.K. Sinop"], "venue": "L. Arge and J. Pach, editors, 31st International Symposium on Computational Geometry, SoCG 2015, June 22-25, 2015, Eindhoven, The Netherlands, volume 34 of LIPIcs, pages 754\u2013767. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Clustering with bregman divergences", "author": ["A. Banerjee", "S. Merugu", "I.S. Dhillon", "J. Ghosh"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Bregman voronoi diagrams", "author": ["J.-D. Boissonnat", "F. Nielsen", "R. Nock"], "venue": "Discrete & Computational Geometry, 44(2):281\u2013307", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "A linear space algorithm for computing maximal common subsequences", "author": ["D.S. Hirschberg"], "venue": "Commun. ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1975}, {"title": "The least weight subsequence problem", "author": ["D.S. Hirschberg", "L.L. Larmore"], "venue": "SIAM Journal on Computing, 16(4):628\u2013638", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1987}, {"title": "From genome mining to phenotypic microarrays: Planctomycetes as source for novel bioactive molecules", "author": ["O. Jeske", "M. Jogler", "J. Petersen", "J. Sikorski", "C. Jogler"], "venue": "Antonie Van Leeuwenhoek, 104(4):551\u2013567", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "A simple linear time algorithm for concave one-dimensional dynamic programming", "author": ["M.M. Klawe"], "venue": "Technical report, Vancouver, BC, Canada, Canada", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "Improved and simplified inapproximability for k-means", "author": ["E. Lee", "M. Schmidt", "J. Wright"], "venue": "Information Processing Letters, 120:40\u201343", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "The Planar k-Means Problem is NP-Hard", "author": ["M. Mahajan", "P. Nimbhorkar", "K. Varadarajan"], "venue": "pages 274\u2013285. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimal interval clustering: Application to bregman clustering and statistical mixture learning", "author": ["F. Nielsen", "R. Nock"], "venue": "IEEE Signal Process. Lett., 21:1289\u20131292", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "The retail market as a complex system", "author": ["D. Pennacchioli", "M. Coscia", "S. Rinzivillo", "F. Giannotti", "D. Pedreschi"], "venue": "EPJ Data Science, 3(1):1", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Computing a minimum weightk-link path in graphs with the concave monge property", "author": ["B. Schieber"], "venue": "Journal of Algorithms, 29(2):204 \u2013 222", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "k-means requires exponentially many iterations even in the plane", "author": ["A. Vattani"], "venue": "Discrete & Computational Geometry, 45(4):596\u2013616", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Ckmeans.1d.dp: Optimal and fast univariate clustering; R package version", "author": ["H. Wang", "J. Song"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "Ckmeans", "author": ["H. Wang", "M. Song"], "venue": "1d. dp: optimal k-means clustering in one dimension by dynamic programming. The R Journal, 3(2):29\u201333", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "The concave least-weight subsequence problem revisited", "author": ["R. Wilber"], "venue": "Journal of Algorithms, 9(3):418 \u2013 425", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1988}, {"title": "Efficient dynamic programming using quadrangle inequalities", "author": ["F.F. Yao"], "venue": "Proceedings of the Twelfth Annual ACM Symposium on Theory of Computing, STOC \u201980, pages 429\u2013435, New York, NY, USA", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1980}], "referenceMentions": [{"referenceID": 3, "context": "k-Means is NP-hard even for k = 2 and general dimension [4] and it is also NP-hard for d = 2 and general k [16].", "startOffset": 56, "endOffset": 59}, {"referenceID": 15, "context": "k-Means is NP-hard even for k = 2 and general dimension [4] and it is also NP-hard for d = 2 and general k [16].", "startOffset": 107, "endOffset": 111}, {"referenceID": 14, "context": "Even hardness of approximation results exist [15, 8].", "startOffset": 45, "endOffset": 52}, {"referenceID": 7, "context": "Even hardness of approximation results exist [15, 8].", "startOffset": 45, "endOffset": 52}, {"referenceID": 7, "context": "In [8] the authors show there exists a \u03b5 > 0 such that it is NP-hard to approximate k-Means to within a factor 1+\u03b5 of optimal, and in [15] it is proved that \u03b5 \u2265 0.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "In [8] the authors show there exists a \u03b5 > 0 such that it is NP-hard to approximate k-Means to within a factor 1+\u03b5 of optimal, and in [15] it is proved that \u03b5 \u2265 0.", "startOffset": 134, "endOffset": 138}, {"referenceID": 2, "context": "357 [3].", "startOffset": 4, "endOffset": 7}, {"referenceID": 5, "context": "In theory, if Lloyd\u2019s algorithm is run to convergence to a local minimum, t could be exponential and there is no guarantee on how well the solution found approximates the optimal solution [6, 20].", "startOffset": 188, "endOffset": 195}, {"referenceID": 19, "context": "In theory, if Lloyd\u2019s algorithm is run to convergence to a local minimum, t could be exponential and there is no guarantee on how well the solution found approximates the optimal solution [6, 20].", "startOffset": 188, "endOffset": 195}, {"referenceID": 6, "context": "Lloyd\u2019s algorithm is often combined with the effective seeding technique for selecting initial centroids due to [7] that gives an expected O(lg k) approximation ratio for the initial clustering, which can then improved further by Lloyd\u2019s algorithm.", "startOffset": 112, "endOffset": 115}, {"referenceID": 21, "context": "In particular, there is an O(kn) time and O(kn) space dynamic programming solution for the 1D case, due to work by [22].", "startOffset": 115, "endOffset": 119}, {"referenceID": 4, "context": "The 1D kMeans problem is encountered surprisingly often in practice, some examples being in data analysis in social networks, bioinformatics and retail market [5, 13, 18].", "startOffset": 159, "endOffset": 170}, {"referenceID": 12, "context": "The 1D kMeans problem is encountered surprisingly often in practice, some examples being in data analysis in social networks, bioinformatics and retail market [5, 13, 18].", "startOffset": 159, "endOffset": 170}, {"referenceID": 17, "context": "The 1D kMeans problem is encountered surprisingly often in practice, some examples being in data analysis in social networks, bioinformatics and retail market [5, 13, 18].", "startOffset": 159, "endOffset": 170}, {"referenceID": 7, "context": "633 [8].", "startOffset": 4, "endOffset": 7}, {"referenceID": 8, "context": "In [9] the authors consider and define clustering with Bregman Divergences.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "Interestingly, the heuristic local search algorithm for Bregman Clustering [9] is basically the same approach as Lloyd\u2019s algorithm for k-Means.", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "We refer the reader to [9] for more about the general problem.", "startOffset": 23, "endOffset": 26}, {"referenceID": 16, "context": "For the 1D version of the problem, [17] generalized the algorithm from [22] to the k-Medians problems and Bregman Divergences achieving the same O(kn) time and O(kn) space bounds.", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "For the 1D version of the problem, [17] generalized the algorithm from [22] to the k-Medians problems and Bregman Divergences achieving the same O(kn) time and O(kn) space bounds.", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "They do however have many redeeming qualities, for instance Bregman Divergences are convex in the first argument, albeit not the second, see [9, 10] for a more comprenhensive treatment.", "startOffset": 141, "endOffset": 148}, {"referenceID": 9, "context": "They do however have many redeeming qualities, for instance Bregman Divergences are convex in the first argument, albeit not the second, see [9, 10] for a more comprenhensive treatment.", "startOffset": 141, "endOffset": 148}, {"referenceID": 8, "context": "The Bregman Clustering problem as defined in [9] is to find k centroids M = {\u03bc1, .", "startOffset": 45, "endOffset": 48}, {"referenceID": 20, "context": "dp [21].", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "2 The O(kn) Dynamic Programming Algorithm In this section, we describe the previous O(kn) time and O(kn) space algorithm presented in [22].", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "That is, D[1][m] = CC(1,m) for all m.", "startOffset": 10, "endOffset": 13}, {"referenceID": 21, "context": "This is exactly what is described in [22].", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "The problem of finding the minimum value in every row of a matrix has been studied before [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "[1] Let A be a matrix with real entries and let argmin(i) be the index of the leftmost column containing the minimum value in row i of A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "In [1], the authors showed the following: Theorem 1.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "[1] Finding argmin(i) for each row i of an arbitrary n\u00d7m monotone matrix requires \u0398(m lgn) time, whereas if the matrix is totally monotone, the time is O(m) when m > n and is O(m(1 + lg(n/m))) when m < n.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "1In [1] the authors use the maximum instead of the minimum", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "As [1] remarks, a matrix A is totally monotone if all its 2 \u00d7 2 submatrices are monotone.", "startOffset": 3, "endOffset": 6}, {"referenceID": 23, "context": "This is the property known as the concave (concave for short) property [24, 12, 23] and has been used to significantly speed up algorithms, including Dynamic Programming algorithms, for for other problems.", "startOffset": 71, "endOffset": 83}, {"referenceID": 11, "context": "This is the property known as the concave (concave for short) property [24, 12, 23] and has been used to significantly speed up algorithms, including Dynamic Programming algorithms, for for other problems.", "startOffset": 71, "endOffset": 83}, {"referenceID": 22, "context": "This is the property known as the concave (concave for short) property [24, 12, 23] and has been used to significantly speed up algorithms, including Dynamic Programming algorithms, for for other problems.", "startOffset": 71, "endOffset": 83}, {"referenceID": 10, "context": "2 Reducing Space Usage In the following, we show how to reduce the space usage to just O(n) while maintaining O(kn) running time using a space reduction technique of Hirschberg [11].", "startOffset": 177, "endOffset": 181}, {"referenceID": 18, "context": "The result follows almost directly from [19].", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "In [19] Schieber gives an algorithm with the aforementioned running time for the problem of finding the shortest path of fixed length k in a directed acyclic graph with nodes 1, .", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "Theorem 4 ([19]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 22, "context": "Schiebers algorithm relies crucially on algorithms that given a directed acyclic graph where the weights satisfy the concave property computes a minimum weight path in O(n) time [23, 14].", "startOffset": 178, "endOffset": 186}, {"referenceID": 13, "context": "Schiebers algorithm relies crucially on algorithms that given a directed acyclic graph where the weights satisfy the concave property computes a minimum weight path in O(n) time [23, 14].", "startOffset": 178, "endOffset": 186}, {"referenceID": 22, "context": "By the algorithms in [23, 14] this takes O(n) time.", "startOffset": 21, "endOffset": 29}, {"referenceID": 13, "context": "By the algorithms in [23, 14] this takes O(n) time.", "startOffset": 21, "endOffset": 29}, {"referenceID": 1, "context": "This means that if the inputs are integers, we can solve the 1D k-Means problem by a simple application of binary search in O(n lgU) time where U is the universe size [2].", "startOffset": 167, "endOffset": 170}, {"referenceID": 8, "context": "The Bregman Clustering problem as defined in [9], is to find a clustering, M = {\u03bc1, .", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "This is in one sense the defining property of Bregman Divergences [9].", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "The second important is the linear separator property, which is very important for clustering with Bregman Divergences but also very relevavant to Bregman Voronoi Diagrams [9, 10].", "startOffset": 172, "endOffset": 179}, {"referenceID": 9, "context": "The second important is the linear separator property, which is very important for clustering with Bregman Divergences but also very relevavant to Bregman Voronoi Diagrams [9, 10].", "startOffset": 172, "endOffset": 179}, {"referenceID": 16, "context": "The prefix sums idea used to implement the data structure used for Lemma 1 generalizes to Bregman Divergences as observed in [17] (under the name Summed Area Tables).", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "This was also observed in [17].", "startOffset": 26, "endOffset": 30}], "year": 2017, "abstractText": "The k-Means clustering problem on n points is NP-Hard for any dimension d \u2265 2, however, for the 1D case there exist exact polynomial time algorithms. Previous literature reported an O(kn) time dynamic programming algorithm that uses O(kn) space. We present a new algorithm computing the optimal clustering in only O(kn) time using linear space. For k = \u03a9(lg n), we improve this even further to n2 \u221a lg lgn lg k) time. We generalize the new algorithm(s) to work for the absolute distance instead of squared distance and to work for any Bregman Divergence as well. \u2217Aarhus University. Email: jallan@cs.au.dk. Supported by MADALGO Center for Massive Data Algorithmics, a Center of the Danish National Research Foundation. \u2020Aarhus University. Email: larsen@cs.au.dk. Supported by MADALGO, a Villum Young Investigator Grant and an AUFF Starting Grant. \u2021Aarhus University. Email: alexander.mathiasen@gmail.com. Supported by MADALGO and an AUFF Starting Grant. \u00a7Aarhus University. Email: jasn@cs.au.dk. Supported by MADALGO. \u00b6University of California, San Diego. Email: stschnei@cs.ucsd.edu. Supported by NSF grant CCF-1213151 from the Division of Computing and Communication Foundations. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. \u2016New Mexico State University. Email: joemsong@cs.nmsu.edu", "creator": "LaTeX with hyperref package"}}}