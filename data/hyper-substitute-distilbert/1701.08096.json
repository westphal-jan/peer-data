{"id": "1701.08096", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jan-2017", "title": "Efficiently Summarising Event Sequences with Rich Interleaving Patterns", "abstract": "explains the complete functionality of certain category easiest because of the main goals preventing database classification. in pattern prediction databases we prefer so of discovering a simple set of characteristics if together describe the data well. the richer the class structure patterns we consider, and actually more powerful our description language, the better guys best see capable to navigate these data. in published paper we propose \\ ourmethod, a novel greedy mdl - greedy method called summarising sequential paths combining key patterns that are ready to interleave. experiments through \\ ourmethod is orders of being faster achieving the state : the art, results in better models, but well even discovers meaningful defects in the relevant patterns directly identify other choices than values.", "histories": [["v1", "Fri, 27 Jan 2017 16:02:54 GMT  (226kb,D)", "http://arxiv.org/abs/1701.08096v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DB", "authors": ["apratim bhattacharyya", "jilles vreeken"], "accepted": false, "id": "1701.08096"}, "pdf": {"name": "1701.08096.pdf", "metadata": {"source": "CRF", "title": "Efficiently Summarising Event Sequences with Rich Interleaving Patterns", "authors": ["Apratim Bhattacharyya", "Jilles Vreeken"], "emails": ["abhattac@mpi-inf.mpg.de", "jilles@mpi-inf.mpg.de"], "sections": [{"heading": "1 Introduction", "text": "Discovering the key patterns from a database is one of the main goals of data mining. Modern approaches do not to ask for all patterns that satisfy a local interestingness constraint, such as frequency [2, 10], but instead ask for that set of patterns that is optimal for the data at hand. There are different ways to define this optimum. The Minimum Description Length (MDL) principle [14, 5] has proven to be particularly successful [22, 16]. Loosely speaking, by MDL we say that the best set of patterns is the set that compresses the data best. How well we can compress, or better, describe the data depends on the description language we use. The richer this language, the more relevant structure we can identify. At the same time, a richer language means a larger search space, and hence requires more efficient search.\nIn this paper we consider databases of event sequences, and are after that set of sequential patterns that together describe the data best\u2014as we did previously with SQS [20]. Like SQS we describe a database with occurrences of patterns. Whereas SQS requires these occurrences to be disjoint, however, we allow patterns to interleave. This leads to more succinct descriptions as well as better pattern recall. Moreover, we use a richer class of patterns. That is, we do not only allow for gaps in occurrences, but also allow patterns to emit one out of multiple events at a certain location. For example,\n\u25e6Max-Planck Institute for Informatics and Saarland University, Saarland Informatics Campus, Saarbru\u0308cken, Germany. {abhattac,jilles}@mpi-inf.mpg.de\nthe pattern \u2018paper [proposes | presents] new\u2019 discovered in the JMLR abstract database matches two common forms of expressing that a paper presents or proposes something new.\nWith this richer language, we can obtain much better compression rates with much fewer patterns. To discover good models we propose SQUISH, a highly efficient and versatile search algorithm. Its efficiency stems from reuse of information, partitioning the data, and in particular from considering only the currently relevant occurrences of patterns in the data. It is a natural any-time algorithm, and can be ran for any time budget that is opportune.\nExtensive experimental evaluation shows that SQUISH performs very well in practice. It is much better at retrieving interleaving patterns than the very recent proposal by Fowkes and Sutton [4], and obtains much better compression rates than SQS [20], while being orders of magnitude faster than both. The choice-patterns it discovers give insight in the data beyond the state of the art, identifying semantically coherent patterns. Moreover, SQUISH is highly extendable, allowing for richer pattern classes to be considered in the future."}, {"heading": "2 Preliminaries", "text": "Here we introduce basic notation, and give short introductions to the MDL principle.\n2.1 Notation We consider databases of event sequences. Such a database D is composed of |D| sequences. A sequence S \u2208 D consists of |S| events drawn from an alphabet \u2126. The total number of events occurring in the database, denoted by ||D||, is simply the sum of lengths of all sequences \u2211 S\u2208D |S|. We write S [j] to refer to the jth event in sequence S. The support of an event e \u2208 \u2126 in a sequence S is simply the number of occurrences of e in S, i.e. supp(e | S) = |{j | S[j] = e}|. The support of e in a database D is defined as supp(e|D) = \u2211|D| i=1 supp(e | Si).\nWe consider two types of sequential patterns. A serial episode X \u2208 \u2126|X| is a sequence of |X| events, and we say that a sequence S contains X if there is a subsequence in S equal to X . We allow noise, or gap events, within an occurrence of X . We also consider choice episodes, or choicisodes. These are serial episodes with positions matching one out of multiple events. For example, serial\nar X\niv :1\n70 1.\n08 09\n6v 1\n[ cs\n.A I]\n2 7\nJa n\n20 17\nepisode ac matches an a followed by c, whereas choicisode [a, b] c matches occurrences of a or b followed by c.\n2.2 Brief introduction to MDL The Minimum Description Length principle (MDL) [14, 5] is a practical version of Kolmogorov Complexity [9]. Both embrace the slogan Induction by Compression. We use the MDL principle for model selection.\nBy MDL, the best model is the model that gives the best lossless compression. More specifically, given a set of modelsM, the best modelM \u2208M is the one that minimizes L(M) + L(D | M), in which L(M) is the length in bits of the description of M , and L(D | M) is the length of the data when encoded with model M . Simply put, we are interested in that model that best compresses the data without loss. MDL as describe above is known as two-part MDL, or crude MDL; as opposed to refined MDL. In refined MDL model and data are encoded together [5]. We use two-part MDL because we are specifically interested in the model: the patterns that give the best description. In MDL we are only concerned with code lengths, not actual code words.\nNext, we formalise our problem in terms of MDL."}, {"heading": "3 MDL for Event Sequences", "text": "To use MDL we need to define a model classM, and how to encode a model M \u2208M and data D in bits.\nAs models we will consider code tables [22, 20]. A code table CT is a dictionary between patterns and associated codes. A code table consists of the singleton patterns e \u2208 \u2126, as well as a set P of non-singleton patterns. We write codep(X) to denote the pattern code that identifies a pattern X \u2208 CT . Similarly, we write codef (X) and codeg(X) for the codes resp. identifying a fill resp. a gap in the occurrence of a pattern X .\nWe can encode a sequence databaseD using the patterns in a code table CT , which generates a cover C of the database. A cover C uniquely defines a pattern code stream Cp and a meta code streamCm. The pattern stream is simply the concatenation of the codes corresponding to the patterns in the cover, in the order of their appearance. Likewise, the meta stream Cm is the concatenation of the gap and fill codes corresponding to the cover. In Fig. 1, we illustrate two example covers and corresponding code tables, the first using only singletons and the second cover with interleaving using patterns from a richer code table with choicisodes.\nBefore formalising our score, it is helpful to know how to decode a database given a code table and the code streams.\n3.1 Decoding a database To decode a database, we start by reading a codep(X) from the pattern stream Cp. If the corresponding pattern X is a singleton, we append it to our reconstruction of the database D. If it is a non-singleton, we append its first event, X[1], D. To allow for interleaving, we\nhave to add a new context to context list \u039b. A context is a tuple (X, i) consisting of a pattern X , and a pointer i to the next event to be read from the pattern. For an example, let us consider Cover 2 in Fig. 1. We read codep(p) from codep, append p[1] = a to D, and add (p, 2) to the context list.\nNext, if the context list is non-empty, we read as many meta codes from Cm as there are contexts in \u039b. If we read a fill code codef (X) corresponding to one of the contexts (X, i) \u2208 \u039b, we append the next event from X , X[i] to the data D, and increment the pointer. If after this step we have finished reading the pattern, we remove its context from the list. If we only read gap codes codeg(X) for every pattern X in the context list, we read again from the pattern stream. We do this until we reach the end of the pattern stream Cp.\nContinuing our example, we read codeg(p) from Cm, which corresponds to a gap in the occurrence of pattern p. We read codep(q) from Cp, write q[1] = b to D, and insert context (q, 2) to \u039b. Next, \u039b contains two contexts, and we read two meta codes from Cm, viz. codeg(p) and codef (q). As for context (q, 2) we read a fill code, we write q[2] = d to D, and increment its pointer to 3. Etc.\n3.2 Calculating Encoded Lengths Given the above scheme we know which codes to expect when, and can now formalise our score. We build upon and extend the encoding on Tatti & Vreeken [20] for richer covers and patterns.\nEncoded Length of the Database We encode the pattern stream Cp using Shannon optimal prefix codes. The length of the pattern code L(codep(X)) for a pattern X depends on how often it is used in the pattern stream. We write usage(X) to denote the number of times codep(X) occurs in Cp. The length of the optimal pattern code for X then is\nL(codep(X)) = \u2212 log ( usage(X)\u2211\nY \u2208CT usage(Y )\n) .\nThe encoded length of the whole pattern stream Cp is then simply L(Cp) = \u2211 X\u2208CT usage(X)L(codep(X)).\nTo avoid arbitrary choices in the model encoding, we use prequential codes [5] to encode the meta stream. Prequential codes are asymptotically optimal without knowing the distribution beforehand. The idea is that we start with an uniform distribution over the events in the stream and update the counts after every received event. This means we have a valid probability distribution at every point in time, and can hence send optimal prefix codes. The total encoded length of the meta stream pattern is\nL(Cm) = \u2211\nX\u2208CT\n( \u2212 fills(X)\u2211 i=1 log ( + i 2 + i )\n\u2212 gaps(X)\u2211\ni=1\nlog\n( + i\n2 + fills(X) + i\n)) .\nwhere = 0.5 is a constant by which we initialize the distribution [5], fills(X) and gaps(X) are the number of times codef (X) resp. codeg(X) occurs in Cm.\nFor lossless decoding of database D, the number of sequences |D| and the length of each sequence S \u2208 D should also be encoded. We do this using LN, the MDL optimal code for integers n \u2265 1 [15].\nCombining the above, for the total encoded length of a database, given a code table CT and cover C, we have L(D | CT ) = LN(|D|) + \u2211 S\u2208D LN(|S|) +L(Cp) +L(Cm) .\nNext we discuss how to encode a model.\nEncoded Length of the Code Table Note that the simplest valid code table consists of only the singletons \u2126. We refer to this code table as ST , or, the standard code table. We use ST to encode the non-singleton patterns P of a code table CT . The usage of a singleton e \u2208 ST is simply its support in D, and hence the code length codep(e) =\n\u2212 log (\nsupp(e|D) ||D||\n) . To use these codes the recipient needs to\nknow the supports of the singletons. We encode these using a data to model code\u2014an index over a canonically ordered enumeration of all possibilities [21]; here it is the number of possible supports of |\u2126| alphabets over a database length of ||D||, (||D|| |\u2126| ) . The length of the code is now simply the logarithm over the number of possibilities. Given the standard code table ST , we can now encode the patterns in the code table. We first encode the length |X| of the pattern, and then number of choice spots in the pattern, ||X|| \u2212 |X|. We encode how many choices we have per location using a data to model code. We finally encode\nthe events X[i] using the standard code table, ST . That is,\nL(X | ST ) = LN(|X|) + LN(||X|| \u2212 |X|+ 1)\n+ log ( ||X|| \u2212 1 |X| \u2212 1 ) + ||X||\u2211 i=1 L(X[i] | ST ) .\nNote that if we do not consider choicisodes, we can simplify the above as we only need to transmit the first and last part of this code. That is, the length and the events in the pattern.\nRecall that, pattern codes in the pattern stream Cp are optimal prefix codes. The occurrences of the non-singleton patterns need to be transmitted with the model. We do this again using a data to model code. We encode the sum of pattern usages, usage(P) = \u2211 X\u2208CT\\\u2126 usage(X), by the MDL optimal code for integers. It is equivalent to use a pattern code per choicisode and then identify the choiceevents, or to use a separate pattern code for each instantiation of the choicisode. For simplicity we make the latter choice.\nThe total encoded size of code table CT given a cover C of database D is then given by\nL(CT | D,C) =LN(|\u2126|) + log ( ||D|| |\u2126| ) + LN(|P|+ 1) + LN(usage(P) + 1)\n+ L(usage(P), |P|) + \u2211\nX\u2208CT L(X,CT ) .\nWe are interested in the set of patterns and a corresponding cover C which minimizes the total encoded length of the code table and the database, which is,\nL(CT , D) = L(CT | C) + L(D | CT ) .\nWe can now formally define our problem as follows.\nMinimal Code Table Problem Let \u2126 be a set of events and let D be a sequence database over \u2126, find the minimal set of serial (choice) episodes P such that for the optimal cover C of D using P and \u2126, the total encoded cost L(CT , D) is minimal, where CT is the code-optimal code table for C.\nFor a given database D, we would like to find its optimal pattern set in polynomial time. However, there are exponentially many possible pattern sets, and given a pattern set, there are exponentially many possible covers. For neither problem there exists trivial structure such as monotonicity or sub-modularity that would allow for an optimal polynomial time solution.\nHence, we resort to heuristics. In particular, we split our problem into two parts. We first explain our greedy algorithm to find a good cover given a set of patterns. We describe how to find a set of good patterns in Sec. 5."}, {"heading": "4 Covering a Database", "text": "Given a pattern set P and database D, we are after a cover C with interleaving and nesting, that minimises L(CT , D).\nEach occurrence of a pattern X in database D, possibly with gaps, defines a window. We denote by S [a, b] a window in sequence S that extends from the position a to b. Two windows are non-overlapping if they do not have any events in common which belong to their respective patterns. Two interleaving or nesting windows might have common events, which, as we do not allow overlap, leads to gap events for one of the two windows. Two windows are disjoint if they do not have any events in common. For every event in the database D, there can be many windows with which we can choose to cover it. The optimal cover depends upon the pattern, fill, gap codes of the patterns. The choices grow exponentially with sequence length, with no trivial sub-structure.\nTo find good disjoint covers, Tatti & Vreeken [20] use an EM-style approach. At each step until convergence, given the pattern, gap and fill codes, the authors use the dynamic programming based algorithm ALIGN to find a cover. ALIGN takes a set of possibly overlapping minimal windows and returns a subset of disjoint minimal windows (i.e. a cover) which maximizes the sum of gain (a heuristic measure) of each window. Then, the lengths of the codes are reset based upon the found cover. It is unclear if this scheme can be extended to return a cover with interleaved or nested windows efficiently. Moreover if we extend our model with a new pattern, we have to rerun ALIGN from scratch.\nWe propose an efficient and easily extendible heuristic for good covers with interleaved and nested windows.\n4.1 Window Lengths For a given pattern, as we consider windows with gaps, the length of an window in the database can be arbitrarily long. Tatti & Vreeken therefore consider only minimal windows. A window w = S [i, j] is a minimal window of a pattern X if w contains X but no other proper sub-windows of w contain X . If no interleaving or nesting is allowed, it is optimal to consider only minimal windows. Otherwise, it is easy to construct examples where the optimal cover consists of non-minimal windows.\nConsider the sequence abdccdc and a code table with the pattern abc, dc and the singletons a, b, c and d. Two possible covers are: (ab d c) c dc using only minimal windows and (ab(dc)c) dc where a non-minimal window of abc is used and is nested with a window of dc. It is easy to see that the second cover leads to lower encoded length L(abdccdc, {abc, dc, a, b, c, d}) (see Fig 2) of about 2.9 bits.\nIdeally, we should consider all possible windows. The number of possible windows of a pattern, however, is quadratic in the length of the database. This means that even a search for all windows is computationally inefficient. Therefore, we first search for only the shortest window from each starting position in the database. We consider longer windows when necessary. We do so as follows.\n4.2 Window Search Given a pattern X , we use the pseudo-code FINDWIN presented as Algorithm 1 to search for its windows in the a sequence or sub-sequence S of database D. It returns us Wcand(X) which is a set of candidate windows of the pattern X . It considers only the first window from each starting position in the sequence S. We later choose a subset of these windows (along with those of patterns other than X in CT ) to create a cover of the database L(D | CT ). To control the ratio of gaps and fills, we maintain a budget variable. This is the number of extra allowed overall gaps. Ideally we would like to have more fills than gaps as it leads to better compression.\nTo search for windows efficiently in FINDWIN, we use an inverted index: index\u22121(x) which gives us a list of positions of the event x \u2208 \u2126 in the database. We use a priority queueQ to store potential windows sorted by length. Shorter windows means more fills than gaps. We initialize FINDWIN (line 3) by creating potential windows at all the positions where the first alphabet of pattern X occurs in the sequence Si and pushing these potential windows toQ. Each windoww inQ contains the starting position in Si, its length, and a pointer wi. This pointer points to a certain event in pattern X which we are search for in Si. At every step of FINDWIN (line 7) we look at the potential window at the top of the queue Q. We check if the next event in the database equals the character of the pattern X pointed to by wi and increment the length of the window w. There are now two possibilities i) (line 10) The next database event is the same as the event in X pointed to by wi. If we have found the full patternX in the database, we add this window toWcand(X). We can now update our budget if we used more fills than\nAlgorithm 1: FINDWIN(Si, X, budget) input : sequence S and a pattern X output : set of windows for X 1 Wcand \u2190 \u2205; 2 for p in index\u22121(x) do 3 w \u2190 (start , length, wi = 1); 4 push(w,Q);\n5 while T is not empty do 6 w \u2190 top(Q); 7 (start , length, wi)\u2190 w; 8 length = length + 1; 9 if Si [start+ length] equals X [wi] then\n10 if wi points to the end of X then 11 append w to Wcand ; 12 b\u2190 b+ 2\u00d7 length(X)\u2212 length \u2212 1;\n13 else 14 if b+ 2\u00d7 length(X)\u2212 length \u2212 1 < 0 then 15 delete w from Q;\ngaps. Using less gaps in one window allows us to use more gaps in another. ii) (line 15) The next database event does not equal the event of X pointed to by we. This means that the potential window w has one extra gap. We check if this extra gap is allowed by our budget (line 16). Otherwise, we drop the window.\nNow that we can search for windows of patterns, we describe how to choose a subset which generates a good cover C of the database D.\n4.3 Candidate Order In the first step of our greedy strategy, we sort the set of patterns in a fixed order, similar to [3]. We call this order the Candidate Order. We cover the database using windows of patterns in this order. This order is designed to minimize the code length. This is achieved by putting longer and more frequently occurring patterns higher up in the candidate order. This means we can cover more events while minimizing the code length.\nWe consider the patterns X \u2208 CT in the order,\n1. Decreasing \u2193 in length | X |\n2. Decreasing \u2193 in support support(X | D)\n3. Decreasing \u2193 in length of encoding it with the standard table.\n4. Increasing \u2191 lexicographically.\n4.4 Greedy Cover We now describe our greedy algorithm GREEDYCOVER which we use incrementally build a good cover as pseudo-code in Algorithm 2. We consider patterns in the candidate order. We maintain a set of selected windows Wsel . GREEDYCOVER takes this set of selected\nAlgorithm 2: GREEDYCOVER(Wsel ,Wcand) input : set of selected windows Wsel and a set of\ncandidate windows Wcand(X) for pattern X . output : set of selected windows Wsel combined with\nthose in Wcand(X) not overlapping with Wsel . 1 Wadd \u2190 \u2205; 2 Wext \u2190 create window extends from Wsel and Wcand(X); 3 last \u2190 \u2205; 4 for window w \u2208Wext do 5 Wtmp \u2190 all windows of X between last and v; 6 for v in Wtmp , in order of decreasing length do 7 if v does not overlap with Wtmp then 8 append v to Wadd ;\n9 Wtmp \u2190Wtmp \u222a { windows of X inside w in D}; 10 last \u2190 w ; 11 return Merge Wsel and Wadd ;\nwindows Wsel and extends it with a subset of candidate windows of patternX ,Wcand(X), found with FINDWIN and possibly with (longer, interleaved) windows found on the fly. We assume that both Wsel and Wcand are sorted.\nWe refer to a block of windows which are interleaved or nested with each other as an window extend. For ease of notation, we refer to windows which are not interleaved or nested also as window extends (containing a single window). We begin GREEDYCOVER by dividing the set Wsel into a set of window extends Wext by a linear sweep (if Wsel is sorted). For patterns at the top of the candidate order Wsel is empty, so we can select all candidate windows Wcand(X). For any other pattern, we iterate though the list of window extends Wext (line 4). All the windows of the pattern occurring between any two extends in Wext can be potentially chosen. These are put in Wtmp (line 5), a temporary list. It is possible that some windows of X in Wtmp overlap. We consider these windows in order of decreasing length (line 6) and discard any window that overlaps with a previously chosen window. We additionally search (on the fly) for interleaved windows occurring within the window extends Wext (line 9).\nFor example consider the sequence abcdacbd which we want to cover with the patterns ac and bd. Using FINDWIN we get two windows each for the two patterns. If ac is higher up in the candidate order, we first select the two windows of ac; abcdacbd. We now have two window extends in Wext . We search for windows of bd within the first window extend of ac to find one interleaved window: abcdacbd and we select the second window of bd as it is between the two window extends of ac.\nNote that, GREEDYCOVER now takes time O(|Wsel | + ||D|| + |Wcand | log(|Wcand |)), in the worst case. Where, |Wsel | is the number of windows in Wsel and |Wcand | is the number of candidate windows. Let, Wmax(P)\nbe the maximum number of candidate windows of any pattern in P . Then GREEDYCOVER takes time O(|P| (Wmax(P) log(Wmax(P)) + ||D||)) to construct a cover C of the database D using the patterns P in the code table CT in the worst case. The maximum number of candidate windows of any pattern Wmax(P) is bounded by the size of the database O(||D||). However, GREEDYCOVER makes it computationally more efficient to extend the code table with a new pattern X . We can discard windows of patterns in P below X in the candidate order from the cover and run GREEDYCOVER for Wcand(X) and the patterns in the code table below X in candidate order. This means that we do not have to recompute the cover from scratch. This is very efficient if the pattern X is near the bottom of the candidate order. As we shall see GREEDYCOVER is very competitive in its execution time compared to SQS [20].\nHaving presented our greedy approach of covering a database given a set of patterns, we now turn our attention to the task of mining good set patterns."}, {"heading": "5 Mining Good Code Tables", "text": "Given a pattern setP we have a greedy algorithm to cover the database D and obtain the encoded length of the model and dataL(CT , D). To solve the Minimal Code Table Problem we want to find that set P of patterns which minimizes the total encoded length L(CT , D) of the database. As discussed before, there does not seem to be any trivial substructure in the problem which we can exploit to obtain an optimal set of patterns P in polynomial time. So, we resort to heuristics. We build upon and extend SQS-SEARCH [20].\n5.1 Generating Candidates We build a pattern set P incrementally. Given a set of patterns P and a cover C, we aim to find a patternX and an extension Y , such thatX,Y \u2208 P \u222a\u2126, whose combination XY would decrease the encoded length L(P \u222a XY,D). We do this until we cannot find any XY that when added to P reduces the total encode size. Doing is exactly, however, is computationally prohibitive. At every iteration, there would be O((|P| + |\u2126|)2) possible candidates. Thus, we again resort to heuristics. We use the heuristic algorithm ESTIMATE from [20] that can find good candidates, with likely decrease in code length if added, in O(|P|+ |\u2126|+ ||D||) time. For readability and succinctness, we describe algorithm ESTIMATE in Appendix A.\nCandidates are accepted or rejected based on the compression gain. As we can now find richer covers with interleaving and nesting, candidates are potentially more likely to be accepted. However, we want to find a succinct set of patterns which describe the data well. Choicisodes can help in this search for a succinct summary of the data.\n5.2 Choicisodes Recall from Sec. 3.2 that we can encode patterns as choicisodes. We have the possibility of combin-\ning a newly discovered non-singleton pattern with a previously discovered non-singleton pattern or choicisode to create or expand a choicisode. Combining non-singleton patterns into a single choisisode may hence lead to savings in the encoded length of the code table L(CT | C) while providing a more succinct representation of the pattern set.\nWe use a greedy strategy based on MDL for discovering choicisodes. For each newly discovered non-singleton pattern, we consider all previously discovered non-singleton patterns or choicisodes which differ with it at one position. Then, we calculate the increase in code length (of the model) if we encode it as a choicisode with each of these nonsingleton patterns or choicisodes. We also consider the increase in code length if we encode it as independently. We choose whichever option with leads to the minimum increase in code length.\nNext we present our algorithm SQUISH for mining a succinct and representative pattern set.\n5.3 The SQUISH algorithm The present the complete algorithm SQUISH as pseudo-code in Algorithm 3. At each iteration, it considers each pattern X \u2208 CT . It creates potential extensions XY , with Y \u2208 CT , based on estimated change in the encoded length using ESTIMATE (line 6). SQUISH then considers each of these patterns in the order of the estimated decrease in gain if added to CT (line 7). FINDWIN is used to find the candidate windows of each of these extensions (line 9). GREEDYCOVER is used to cover the data with this candidate pattern XY added to CT . We simultaneously consider the possibility of encoding XY as a choicisode. If XY leads to a decrease in the encoded length of the database D then, we add XY to CT . If XY is to be added, we PRUNE (see Appendix A) the code table to remove redundant patterns. Consider, for example if we decide to add abcd, the pattern ab and cd may not be required to construct an effective cover of the database. We also consider the singletons occurring in the gaps of XY , by constructing new extended patterns by using these gap alphabets as intermediate alphabets."}, {"heading": "6 Related Work", "text": "Discovering sequential patterns is an active research topic. Traditionally there was a focus on mining frequent sequential patterns, with different definitions of how to count occurrences [10, 23, 8]. Mining general patterns, patterns where the order of events are specified by a DAG is surprisingly hard. Even testing whether a sequence contains a pattern is NP-complete [18]. Consequently, research has focused on mining subclasses of episodes, such as, episodes with unique labels [1, 12], strict episodes [19], and injective episodes [1].\nTraditional pattern mining typically results in overly many and highly redundant results. Once approach to counter this is mining statistically significant patterns. Com-\nAlgorithm 3: SQUISH(D) input : database D output : pattern set P with low L(CT , D) 1 P \u2190 \u03c6; 2 C \u2190 GREEDYCOVER(P, D); 3 while changes do 4 F \u2190 \u03c6; 5 for X \u2208 CT do 6 add ESTIMATE(X,A,D) to F ;\n7 for Z \u2208 F ordered by estimated gain do 8 Sort P \u222a Z in candidate order; 9 Wcand(Z)\u2190 FINDWIN(D,Z, budget);\n10 C \u2190 GREEDYCOVER(P \u222a Z,D); 11 if L(D,P \u222a Z) < L(D,P) then 12 P \u2190 PRUNE(P \u222a Z,D);\nputing the expected frequency of a sequential pattern under a null hypothesis is very complex, however [17, 13].\nSQUISH builds upon and extends SQS [20]. Both draw inspiration from the KRIMP [22] and SLIM [16] algorithms. KRIMP pioneered the use of MDL for mining good patterns from transaction databases. Encoding sequential data with serial episodes is much more complicated, and hence SQS uses a much more elaborate encoding scheme. Here, we extend it to discover richer structure in the data. The SLIM algorithm [16] mines KRIMP code table directly from data. SLIM iteratively seeks to improve the current model by considering as candidates joins XY of patterns X,Y \u2208 CT . Whereas SLIM considers the full Cartesian product and ranks on the basis of estimated gain, SQS and SQUISH take a batch based approach.\nLam et al. introduced GOKRIMP [7] for mining sets of serial episodes. As opposed to the MDL principle, they use fixed length codes, and do not punish gaps within patterns. This means, their goal is essentially to cover the sequence with as few patterns as possible, which is different from our goal of finding patterns that succinctly summarize the data.\nRecently, Fowkes and Sutton proposed the ISM algorithm [4]. ISM is based on a generative probabilistic model of the sequence database, and uses EM to search for that set of patterns that is most likely to generate the database. ISM does not explicitly consider model complexity. Like SQUISH, ISM can handle interleaving and nesting of sequences. We will empirically compare to ISM in the experiments."}, {"heading": "7 Experiments", "text": "Next we empirically evaluate SQUISH on synthetic and real world data. We compare against SQS [20] and ISM [4]. All algorithms were implemented in C++. We provide the code\nfor research purposes.1\nWe evaluate quantitatively on the basis of achieved compression, pattern recall, and execution times. Specifically, we consider the compression gain \u2206L = L(D,ST ) \u2212 L(D,CT ). That is, the gain in compression using discovered patterns versus using the singleton-only code table. Higher scores are better. All experiments were executed single threaded on quad-core Intel Xeon machines with 32GB of memory, running Linux.\nDatabases We consider four synthetic, and five real databases. We give their base statistics in Table 1.\nIndep, Plant-10, and Plant-50 are synthetic data consisting of a single sequence of 10 000 events, over an alphabet of 1000 events. For Indep, all events are independent. For Plant-10, and Plant-50 we plant resp. 10 and 50 patterns of 5 events long 10 times each over an otherwise independent sequence, with a 10% probability of having a gap between consecutive events. To evaluate the ability of SQUISH to discover interleaved and nested patterns, we consider the Parallel database [4]. Each event in this database is generated by five independent parallel processes chosen at random. Each process i generates the events {ai, bi, ci, di, ei} in sequence.\nWe further consider five real data sets. Gazelle is clickstream data from an e-commerce website [6]. The Sign database is a list of American sign language utterances [11]. To allow for interpretability we also consider text data. Here the events are the (stemmed) words in the text, with stop words removed. Addresses contains speeches of American presidents. JMLR contains abstracts from the Journal of Machine Learning research, and Moby is the famous novel Moby Dick by Herman Melville.\nSynthethic Data As a sanity check we first compare to SQS considering only serial episodes and not allowing interleaving or nesting. We find that in this setting SQUISH performs\n1http://eda.mmci.uni-saarland/squish/\nTable 2: Comparing SQS and SQUISH. Results for SQUISH using resp. disjoint serial episodes, interleaving serial episodes, and interleaving serial episodes and choicisodes. Given are the number of non-singleton patterns (|P|), time to finish (t), time reach the same score as SQS (SQS-t), and the gain in compression \u2206L (higher is better).\nSQUISH\nSQS Disjoint Interleaving Choisisodes\nDataset |P| t \u2206L |P| SQS-t t \u2206L |P| SQS-t t \u2206L |P| SQS-t t \u2206L\nSign 127 81s 15.5k 157 3.0s 59s 22.5k 156 4.3s 132s 22.7k 93 4.3s 103s 23.5k Gazelle 934 26m 14.7k 880 1.5s 76m 160.4k 901 0.6s 96m 161.6k 605 1.4s 159m 165.7k Addresses 155 5m 5.4k 181 3.9s 4m 6.5k 182 3.9s 7m 6.5k 126 3.9s 12m 7.3k JMLR 580 8m 29.2k 583 5.4s 67m 37.2k 593 6.5s 87m 37.7k 334 5.6s 420m 40.9k Moby 231 46m 9.6k 231 3m 23m 10.9k 328 270s 39m 10.9k 224 20.3s 66m 12.5k\n0 5 10 15 20 25 30\n0\n0.2\n0.4\n0.6\n0.8\n1\nnumber of patterns\nPa tte\nrn re\nca ll\nSQUISH ISM\n0 200 400 600\n6.78\n6.8\n6.82\n6.84\n6.86 \u00b710 5\ntime (s)\nL (C\nT , D )\nSQUISH SQS\nFigure 3: (left) Recall of interleaving patterns (higher is better) for SQUISH and ISM on Parallel. (right) Runtime of SQUISH and SQS in seconds vs. encoded length of databases for Addresses (lower is better).\non par with SQS in terms of recovering non-interleaving patterns from synthetic data; like SQS it correctly discovers no patterns from Indep, it recovers all patterns from Plant-10, and recovers 45 patterns exactly from Plant-50 and fragments of the remaining 5, but does so approximately ten times faster than SQS.\nTo investigate how well SQUISH retrieves interleaving patterns, we consider the Parallel dataset, and compare to ISM. (We also considered SQS but found it did not finish within a day.) To make the comparison fair, we restrict ourselves again to serial episodes, but now do allow for interleaving and nesting. We measure success in terms of pattern recall. That is, given a set of patterns P and a set of target patterns T , we consider the set T as the data and cover it with P (not allowing for gaps). The pattern recall is the ratio of the total number covered events in T to the maximum of the total number of events in T or P .\nWe give the results in Fig. 3. We find that SQUISH obtains much higher recall scores than ISM. Inspecting the results, we see that SQUISH discovers large fragments of each pattern, whereas ISM retrieves only eight small patterns, most of length 2, and hence does not reconstruct the generating set of patterns well.\nReal data Next we evaluate SQUISH on real data. We compare to SQS in terms of number of patterns, achieved compression, and runtime. We consider three different configurations, 1) disjoint covers of only serial episodes, 2) allowing interleaving and nesting of serial episodes, and 3) allowing interleaving and nesting of serial episodes and choicisodes. We give the results in Table 2.\nFirst of all, the SQS-t columns show that in all setups SQUISH needs only a fraction of the time\u2014up to three orders of magnitude less\u2014to discover a model that is at least good as what SQS returns. To fully converge, SQUISH and SQS take roughly the same amount of time for the disjoint setting, as well as when we do allow interleaving. However, when converged SQUISH discovers models with much better compression rates, i.e. with much higher \u2206L, than SQS does.\nSQUISH is also significantly faster than ISM, taking only 87 minutes instead of 259 on the JMLR database, and on Gazelle SQUISH requires only 96 instead of 680 minutes.\nSQUISH performs best when we consider our richest description language, allowing both interleaving and choicisodes, discovering much more succinct models that obtain much better scores than if we restrict ourselves. For example, for Gazelle, with choicisodes enabled SQUISH needs only 605 instead of 901 patterns to achieve a \u2206L of 165.7k instead of 161.6k. Overall, we observe that many choicisodes form semantically coherent groups. We present a number of exemplar choisisode patterns in Table 3. Interesting examples include: data-set and training-set from JMLR, god-bless and god-help from Address, cape-horn and cape-cod from Moby.\nLast, but not least, we report on the convergence of L(CT , D), the encoded length of the database, over time for both SQUISH and SQS in Fig. 3. Both algorithms estimate batches of candidates, and test them one by one tests. We see that the initial candidates are highly effective on increasing compression gain. Candidates generated in the latter iterations lead to only little increase in compression gain. This leads to the possibility of executing SQUISH based upon a time budget, as an any-time algorithm."}, {"heading": "8 Conclusion", "text": "We considered summarising event sequences. Specifically, we aimed at discovering sets of patterns that capture rich structure in the data. We considered interleaved, nested, and partial pattern occurrences. We proposed the algorithm FINDWIN to efficiently search for pattern occurrences and the greedy algorithm GREEDYCOVER for efficiently covering the data. Experiments show that SQUISH works well in practice, outperforming the state of the art by a wide margin in terms of scores and speed, while discovering pattern sets that are both more succinct and easier to interpret.\nAs future work we are considering parallel episodes, patterns where certain events are un-ordered e.g. a {b, c} d [10]. Discovering such structure presents a significant computational challenges and requires novel scores and algorithms."}, {"heading": "Acknowledgements", "text": "Apratim Bhattacharyya and Jilles Vreeken are supported by the Cluster of Excellence \u201cMultimodal Computing and Interaction\u201d within the Excellence Initiative of the German Federal Government."}, {"heading": "A Appendix", "text": "A.1 Estimating Candidates Here we describe our heuristic strategy for finding new candidates of the form XY as in Sec. 5.1. First, we need two crucial observations.\nConstant Time Difference Estimation Given a database D and an cover C. Let P and Q be two patterns. Let V = {v1, ..., vN} and W = {w1, ..., wN} be two set of windows for P and Q, respectively. Both V and W occur in C. Each of these windows vi and wi occur in the same sequence. Given the start positions and end positions of the pattern in sequence ki, we can write them as vi = (ai, bi, P, ki) and wi = (ci, di, Q, ki). Let U be the set of windows produced by combining them, U = (a1, d1, R, k1), ..., (aN , dN , R, kN ). Let the windows in U be disjoint and the windows in U be disjoint with the windows in C \\ (V \u222aW ). Then the difference L(D,C \u222aU \\ (V \u222aW ))\u2212L(D,C) depends only N , gaps(V ), gaps(W ), and gaps(U) and can be computed in constant time from these values.\nShorter Windows in Optimal Cover Given a database D and an cover C. Let v = (i, j,X, k) \u2208 C. Assume that there exists a window S [a, b] containing X such that w = (a, b,X, l) does not overlap with any window in C and b\u2212 a < j \u2212 i. Then C is not an optimal cover.\nWe refer the reader to [20] for detailed proofs. We present our heuristic procedure ESTIMATE as pseudo-code in Algorithm 4. In this algorithm, given pattern X and a cover C, for a possible extension Y , we enumerate the windows of XY from the shortest to the longest. These windows are constructed by combining two windows in the cover C. We maintain the sets VY , WY and UY (line 1), containing windows of X , windows of Y (to be combined together), and new windows of XY (resulting from the combination) respectively. We do this for every possible extension Y in the code table. At each step we compute the difference in code length of using these windows instead. We maintain dY to store this difference. By the observation Constant Time Difference Estimation, this can be done in constant time. We prefer patterns XY which are frequently occurring, with more fills than other meta stream characters. Thus, we want to find shorter windows of XY first. Such a set of windows U could potentially lead to a estimated decrease in code length. Therefore, to ensure that we find shorter windows first and efficiency, we search for all windows (all possible Y ) simultaneously using a priority queue T and look only at windows in the cover C. For each window of X in the cover C, we look at windows after it to construct windows XY (Y is the pattern of the window following the window ofX). We initialize the priority queue T with these windows (line 4-9), sorted based on length. At each step of the candidate generation algorithm, we retrieve\nAlgorithm 4: ESTIMATE(X,C,D). Heuristic for finding pattern XY with low L(D,CT \u222aXY )\ninput : database D, cover C, and pattern X output : pattern XY with low L(D,CT \u222aXY )\n1 foreach Y \u2208 CT do 2 VY \u2190 \u2205; WY \u2190 \u2205; UY \u2190 \u2205; dY \u2190 0; 3 T \u2190 \u2205; 4 foreach window v of X in cover C do 5 (a, b,X, k)\u2190 v; 6 d\u2190 end index of window following v in C; 7 t \u2190 (v, d, 0); l(t)\u2190 d\u2212 a; 8 add t into T ;\n9 while T is not empty do 10 t\u2190 argminu\u2208T l(u); 11 (v, d, s)\u2190 t; a\u2190 first index of v; 12 w = (c, d, Y, k)\u2190 active w of Y ending at d; 13 if Y = X and (event at a or d is marked) then 14 delete t from T ; 15 continue;\n16 if Sk[a, d] is a minimal window of XY then 17 add v into VY ; 18 add w into WY ; 19 add (a, d,XY , k) into UY ; 20 dY \u2190 min(div(V,W,U ;A) + s, dY ); 21 if |Y | > 1 then 22 s\u2190 s+ gain(w); 23 if Y = X then 24 mark the events at a and d; 25 delete t from T ; 26 continue;\n27 if w is the last window in the sequence then 28 delete t from T ; 29 else 30 d\u2190 end index of the active w\u2032 following w; 31 update t to (v, d, s) and l(t) to d\u2212 a;\nonce such window of XY from the priority queue T (line 11) add it to our list UY of windows of XY and estimate the change in code length (line 22). As we do not allow overlaps, we need to ensure that windows in UY are not overlapping. If a window ofXY overlaps with any other window inC, we cannot use both of these windows at the same time. We take this into account by subtracting the gain(w) of this window w overlapping with the window of XY (line 23) [20]. The gain(w) of a window w if a upper bound on the bits gained by encoding the events in the database with this window vs. encoding them as singletons. We define the gain as in [20] for a window w of the pattern Y (Sk[i, j]),\ngain(w) =\u2212 L(codep(X))\u2212 (j \u2212 i\u2212 |X|)L(codeg(X)) \u2212 (|X| \u2212 1)L(codef (X)) + \u2211 x\u2208X L(codep(x)) .\nOverlapping could also happen if Y = X . So we simply check if the adjacent scans have already used these two instances of X for creating a window for pattern XX (line 25). We now extend our search by looking at the window following the currently considered window of Y in the cover C (line 34). As we allow interleaving and nesting in our covers, we also look at possible windows Y occurring inside or interleaved with windows of other patterns. That is, we look at singletons inside gaps of windows. For each window X in the cover C, we look at all windows following it, until we reach the window of X or the end of the cover.\nA.2 Pruning the Code Table Here, we present the algorithm we use to prune the code table CT , used at line 12 of SQUISH as pseudo-code in Algorithm 5.\nAlgorithm 5: PRUNE(P, D) input : pattern set P , database D output : pruned pattern set P; 1 foreach X \u2208 P do 2 CT \u2190 code table corresponding to GREEDYCOVER(D,P); 3 CT \u2032 \u2190 code table obtained from CT by deleting X; 4 g \u2190 \u2211 w=(i,j,X,k)\u2208C gain(w); 5 if g < L(CT )\u2212 L(CT \u2032) then 6 if L(D,P \\X) < L(D,P) then 7 P \u2190 P \\X;"}], "references": [{"title": "Discovering injective episodes with general partial orders", "author": ["A. Achar", "S. Laxman", "R. Viswanathan", "P. Sastry"], "venue": "Data Min. Knowl. Disc., 25(1):67\u2013108,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast algorithms for mining association rules", "author": ["R. Agrawal", "R. Srikant"], "venue": "VLDB, pages 487\u2013499,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Keeping it short and simple: Summarising complex event sequences with multivariate patterns", "author": ["R. Bertens", "J. Vreeken", "A. Siebes"], "venue": "KDD, pages 735\u2013744,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "A subsequence interleaving model for sequential pattern mining", "author": ["J. Fowkes", "C. Sutton"], "venue": "KDD,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "The Minimum Description Length Principle", "author": ["P. Gr\u00fcnwald"], "venue": "MIT Press,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "KDD-Cup 2000 organizers\u2019 report: Peeling the onion", "author": ["R. Kohavi", "C. Brodley", "B. Frasca", "L. Mason", "Z. Zheng"], "venue": "SIGKDD Explor., 2(2):86\u201398,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Mining compressing sequential patterns", "author": ["H.T. Lam", "F. M\u00f6rchen", "D. Fradkin", "T. Calders"], "venue": "SDM,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast algorithm for finding frequent episodes in event streams", "author": ["S. Laxman", "P. Sastry", "K. Unnikrishnan"], "venue": "KDD, pages 410\u2013419. ACM,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "An Introduction to Kolmogorov Complexity and its Applications", "author": ["M. Li", "P. Vit\u00e1nyi"], "venue": "Springer,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "Discovery of frequent episodes in event sequences", "author": ["H. Mannila", "H. Toivonen", "A.I. Verkamo"], "venue": "Data Min. Knowl. Disc., 1(3):259\u2013289,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Discovering frequent arrangements of temporal intervals", "author": ["P. Papapetrou", "G. Kollios", "S. Sclaroff", "D. Gunopulos"], "venue": "ICDM, pages 354\u2013361. IEEE,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Discovering frequent closed partial orders from strings", "author": ["J. Pei", "H. Wang", "J. Liu", "K. Wang", "J. Wang", "P.S. Yu"], "venue": "IEEE TKDE, 18(11):1467\u20131481,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Skopus: Mining top-k sequential patterns under leverage", "author": ["F. Petitjean", "T. Li", "N. Tatti", "G.I. Webb"], "venue": "Data Min. Knowl. Disc., 30(5):1086\u20131111,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Modeling by shortest data description", "author": ["J. Rissanen"], "venue": "Automatica, 14(1):465\u2013471,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1978}, {"title": "A universal prior for integers and estimation by minimum description length", "author": ["J. Rissanen"], "venue": "Annals Stat., 11(2):416\u2013431,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1983}, {"title": "SLIM: Directly mining descriptive patterns", "author": ["K. Smets", "J. Vreeken"], "venue": "SDM, pages 236\u2013247. SIAM,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Ranking episodes using a partition model", "author": ["N. Tatti"], "venue": "Data Min. Knowl. Disc., 29(5):1312\u20131342,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Mining closed episodes with simultaneous events", "author": ["N. Tatti", "B. Cule"], "venue": "KDD, pages 1172\u20131180,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Mining closed strict episodes", "author": ["N. Tatti", "B. Cule"], "venue": "Data Min. Knowl. Disc., 25(1):34\u201366,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "The long and the short of it: Summarizing event sequences with serial episodes", "author": ["N. Tatti", "J. Vreeken"], "venue": "KDD, pages 462\u2013470. ACM,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Kolmogorov\u2019s structure functions and model selection", "author": ["N. Vereshchagin", "P. Vitanyi"], "venue": "IEEE TIT, 50(12):3265\u2013 3290,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "KRIMP: Mining itemsets that compress", "author": ["J. Vreeken", "M. van Leeuwen", "A. Siebes"], "venue": "Data Min. Knowl. Disc.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Bide: Efficient mining of frequent closed sequences", "author": ["J. Wang", "J. Han"], "venue": "ICDE, 0:79,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 1, "context": "Modern approaches do not to ask for all patterns that satisfy a local interestingness constraint, such as frequency [2, 10], but instead ask for that set of patterns that is optimal for the data at hand.", "startOffset": 116, "endOffset": 123}, {"referenceID": 9, "context": "Modern approaches do not to ask for all patterns that satisfy a local interestingness constraint, such as frequency [2, 10], but instead ask for that set of patterns that is optimal for the data at hand.", "startOffset": 116, "endOffset": 123}, {"referenceID": 13, "context": "The Minimum Description Length (MDL) principle [14, 5] has proven to be particularly successful [22, 16].", "startOffset": 47, "endOffset": 54}, {"referenceID": 4, "context": "The Minimum Description Length (MDL) principle [14, 5] has proven to be particularly successful [22, 16].", "startOffset": 47, "endOffset": 54}, {"referenceID": 21, "context": "The Minimum Description Length (MDL) principle [14, 5] has proven to be particularly successful [22, 16].", "startOffset": 96, "endOffset": 104}, {"referenceID": 15, "context": "The Minimum Description Length (MDL) principle [14, 5] has proven to be particularly successful [22, 16].", "startOffset": 96, "endOffset": 104}, {"referenceID": 19, "context": "In this paper we consider databases of event sequences, and are after that set of sequential patterns that together describe the data best\u2014as we did previously with SQS [20].", "startOffset": 169, "endOffset": 173}, {"referenceID": 3, "context": "It is much better at retrieving interleaving patterns than the very recent proposal by Fowkes and Sutton [4], and obtains much better compression rates than SQS [20], while being orders of magnitude faster than both.", "startOffset": 105, "endOffset": 108}, {"referenceID": 19, "context": "It is much better at retrieving interleaving patterns than the very recent proposal by Fowkes and Sutton [4], and obtains much better compression rates than SQS [20], while being orders of magnitude faster than both.", "startOffset": 161, "endOffset": 165}, {"referenceID": 13, "context": "2 Brief introduction to MDL The Minimum Description Length principle (MDL) [14, 5] is a practical version of Kolmogorov Complexity [9].", "startOffset": 75, "endOffset": 82}, {"referenceID": 4, "context": "2 Brief introduction to MDL The Minimum Description Length principle (MDL) [14, 5] is a practical version of Kolmogorov Complexity [9].", "startOffset": 75, "endOffset": 82}, {"referenceID": 8, "context": "2 Brief introduction to MDL The Minimum Description Length principle (MDL) [14, 5] is a practical version of Kolmogorov Complexity [9].", "startOffset": 131, "endOffset": 134}, {"referenceID": 4, "context": "In refined MDL model and data are encoded together [5].", "startOffset": 51, "endOffset": 54}, {"referenceID": 21, "context": "As models we will consider code tables [22, 20].", "startOffset": 39, "endOffset": 47}, {"referenceID": 19, "context": "As models we will consider code tables [22, 20].", "startOffset": 39, "endOffset": 47}, {"referenceID": 0, "context": "If it is a non-singleton, we append its first event, X[1], D.", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "We read codep(p) from codep, append p[1] = a to D, and add (p, 2) to the context list.", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "We read codep(q) from Cp, write q[1] = b to D, and insert context (q, 2) to \u039b.", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "As for context (q, 2) we read a fill code, we write q[2] = d to D, and increment its pointer to 3.", "startOffset": 53, "endOffset": 56}, {"referenceID": 19, "context": "We build upon and extend the encoding on Tatti & Vreeken [20] for richer covers and patterns.", "startOffset": 57, "endOffset": 61}, {"referenceID": 4, "context": "To avoid arbitrary choices in the model encoding, we use prequential codes [5] to encode the meta stream.", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "5 is a constant by which we initialize the distribution [5], fills(X) and gaps(X) are the number of times codef (X) resp.", "startOffset": 56, "endOffset": 59}, {"referenceID": 14, "context": "We do this using LN, the MDL optimal code for integers n \u2265 1 [15].", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "We encode these using a data to model code\u2014an index over a canonically ordered enumeration of all possibilities [21]; here it is the number of possible supports of |\u03a9| alphabets over a database length of ||D||, (||D|| |\u03a9| ) .", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "To find good disjoint covers, Tatti & Vreeken [20] use an EM-style approach.", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "3 Candidate Order In the first step of our greedy strategy, we sort the set of patterns in a fixed order, similar to [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 19, "context": "As we shall see GREEDYCOVER is very competitive in its execution time compared to SQS [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "We build upon and extend SQS-SEARCH [20].", "startOffset": 36, "endOffset": 40}, {"referenceID": 19, "context": "We use the heuristic algorithm ESTIMATE from [20] that can find good candidates, with likely decrease in code length if added, in O(|P|+ |\u03a9|+ ||D||) time.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "Traditionally there was a focus on mining frequent sequential patterns, with different definitions of how to count occurrences [10, 23, 8].", "startOffset": 127, "endOffset": 138}, {"referenceID": 22, "context": "Traditionally there was a focus on mining frequent sequential patterns, with different definitions of how to count occurrences [10, 23, 8].", "startOffset": 127, "endOffset": 138}, {"referenceID": 7, "context": "Traditionally there was a focus on mining frequent sequential patterns, with different definitions of how to count occurrences [10, 23, 8].", "startOffset": 127, "endOffset": 138}, {"referenceID": 17, "context": "Even testing whether a sequence contains a pattern is NP-complete [18].", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "Consequently, research has focused on mining subclasses of episodes, such as, episodes with unique labels [1, 12], strict episodes [19], and injective episodes [1].", "startOffset": 106, "endOffset": 113}, {"referenceID": 11, "context": "Consequently, research has focused on mining subclasses of episodes, such as, episodes with unique labels [1, 12], strict episodes [19], and injective episodes [1].", "startOffset": 106, "endOffset": 113}, {"referenceID": 18, "context": "Consequently, research has focused on mining subclasses of episodes, such as, episodes with unique labels [1, 12], strict episodes [19], and injective episodes [1].", "startOffset": 131, "endOffset": 135}, {"referenceID": 0, "context": "Consequently, research has focused on mining subclasses of episodes, such as, episodes with unique labels [1, 12], strict episodes [19], and injective episodes [1].", "startOffset": 160, "endOffset": 163}, {"referenceID": 16, "context": "puting the expected frequency of a sequential pattern under a null hypothesis is very complex, however [17, 13].", "startOffset": 103, "endOffset": 111}, {"referenceID": 12, "context": "puting the expected frequency of a sequential pattern under a null hypothesis is very complex, however [17, 13].", "startOffset": 103, "endOffset": 111}, {"referenceID": 19, "context": "SQUISH builds upon and extends SQS [20].", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "Both draw inspiration from the KRIMP [22] and SLIM [16] algorithms.", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "Both draw inspiration from the KRIMP [22] and SLIM [16] algorithms.", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "The SLIM algorithm [16] mines KRIMP code table directly from data.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "introduced GOKRIMP [7] for mining sets of serial episodes.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "Recently, Fowkes and Sutton proposed the ISM algorithm [4].", "startOffset": 55, "endOffset": 58}, {"referenceID": 19, "context": "We compare against SQS [20] and ISM [4].", "startOffset": 23, "endOffset": 27}, {"referenceID": 3, "context": "We compare against SQS [20] and ISM [4].", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "To evaluate the ability of SQUISH to discover interleaved and nested patterns, we consider the Parallel database [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "Gazelle is clickstream data from an e-commerce website [6].", "startOffset": 55, "endOffset": 58}, {"referenceID": 10, "context": "The Sign database is a list of American sign language utterances [11].", "startOffset": 65, "endOffset": 69}, {"referenceID": 9, "context": "a {b, c} d [10].", "startOffset": 11, "endOffset": 15}], "year": 2017, "abstractText": "Discovering the key structure of a database is one of the main goals of data mining. In pattern set mining we do so by discovering a small set of patterns that together describe the data well. The richer the class of patterns we consider, and the more powerful our description language, the better we will be able to summarise the data. In this paper we propose SQUISH, a novel greedy MDL-based method for summarising sequential data using rich patterns that are allowed to interleave. Experiments show SQUISHis orders of magnitude faster than the state of the art, results in better models, as well as discovers meaningful semantics in the form patterns that identify multiple choices of values.", "creator": "LaTeX with hyperref package"}}}