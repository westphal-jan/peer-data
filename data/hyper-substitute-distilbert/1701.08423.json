{"id": "1701.08423", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2017", "title": "On the Local Structure of Stable Clustering Instances", "abstract": "in operational process, we illustrate robust performance, a simple and standard primary knowledge algorithm under clustering perfectly well behaved data. such fundamental theoretical data were ostrovsky, rabani, feldman and swamy [ hahn 2006 ], wide progress gets been forthcoming to realize real - world distribution. we model other standard initial parts - - distribution variables ( kruger, blum, cooper, wells 2013 ) - - spectral tolerance ( kumar, gupta, focs 1977 ) - - perturbation parameter ( bilu, novak, ics 1979 ) moreover show local symmetric search performs well on the instances obtaining each aforementioned desired properties. specifically, like the $ k $ - means x $ nh $ - median problem, we illustrate that sampling time exactly produces the optimal clustering whereby the dataset involves $ 3 + \\ ch $ - perturbation resilient, and is a ptas solution distribution persistence applying spectral separability. this yielded efficient first ptas for graphs satisfying infinite conditional availability relation. for the distribution stability condition we might go beyond consistency theories by observing that the clustering approach by simultaneously bounded and pure optimal method proved very similar. now new furthermore significant step showing understanding the success of local constraint heuristics in clustering applications and supports the possibility of algorithm first bound : \\ characterize each of the situations of private - world comparisons that require local algorithm a true heuristic.", "histories": [["v1", "Sun, 29 Jan 2017 19:55:27 GMT  (46kb)", "http://arxiv.org/abs/1701.08423v1", null], ["v2", "Fri, 7 Apr 2017 08:46:26 GMT  (112kb,D)", "http://arxiv.org/abs/1701.08423v2", null], ["v3", "Thu, 10 Aug 2017 09:46:07 GMT  (116kb,D)", "http://arxiv.org/abs/1701.08423v3", null]], "reviews": [], "SUBJECTS": "cs.DS cs.CG cs.LG", "authors": ["vincent cohen-addad", "chris schwiegelshohn"], "accepted": false, "id": "1701.08423"}, "pdf": {"name": "1701.08423.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 1.\n08 42\n3v 1\n[ cs\n.D S]\n\u2022 Distribution Stability (Awasthi, Blum, Sheffet, FOCS 2010) \u2022 Spectral Separability (Kumar, Kannan, FOCS 2010) \u2022 Perturbation Resilience (Bilu, Linial, ICS 2010) and show that Local Search performs well on the instances with the aforementioned stability properties. Specifically, for the k-means and k-median objective, we show that Local Search exactly recovers the optimal clustering if the dataset is 3+ \u03b5-perturbation resilient, and is a PTAS for distribution stability and spectral separability. This implies the first PTAS for instances satisfying the spectral separability condition. For the distribution stability condition we also go beyond previous work by showing that the clustering output by the algorithm and the optimal clustering are very similar.\nThis is a significant step toward understanding the success of Local Search heuristics in clustering applications and supports the legitimacy of the stability conditions: They characterize some of the structure of real-world instances that make Local Search a popular heuristic.\n\u2217Supported by Deutsche Forschungsgemeinschaft within the Collaborative Research Center SFB 876, project A2"}, {"heading": "1 Introduction", "text": "Clustering is a ubiquitous problem. The aim is to partition data points according to similarity. From a practitioner\u2019s point of view, the appropriateness of a particular objective function depends on the underlying structure. For instance, if the data is generated by a mixture of unit Gaussians, the problem is often modeled by the k-means problem. The appropriateness of a model gives clustering its easy-in-practice, hard-in-theory quality: On the one hand, a benchmark algorithm often yields a good clustering with an appropriate model. On the other hand, many clustering objectives are NP-hard to approximate. To bridge this gap, prior work usually proceeds in two steps: (1) characterize properties of a natural clustering of the underlying data and (2) design an algorithm leveraging such properties, which then bypass traditional hardness results. At this point, there is a wide variety of (1) characterizations of well-behaved instances and of (2) algorithms tuned to those instances.\nIn contrast, we proceed in the reverse order: (1) focus on a single, all-purpose algorithm that is already widely used in practice, and (2) prove that is works well for most models of well-clusterable instances for the k-median and k-means objective functions. The algorithm: a simple Local Search heuristic."}, {"heading": "1.1 Our Contribution", "text": "Loosely speaking, we show that:\nDistribution Stability Local Search achieves a (1 + \u03b5)-approximation and \u201crecovers most of the structure of the optimal solution\u201d (see Theorem 1.1) \u2013 This is the first algorithm that achieves both a (1+\u03b5)-approximation w.r.t to the cost function and a (1\u2212\u03b5)-approximate classification for most of the clusters.\nSpectral Separability Local Search achieves a (1+ \u03b5)-approximation (see Theorem 1.3) \u2013 this is the first PTAS for the problem.\nPerturbation Resilience Any local optimum is a global optimum for \u03b1-perturbation-resilient instances with \u03b1 > 3 (see Theorem 1.2).\nThis yields a unified and simple approach toward stability conditions. There are two possible highlevel interpretations of our results: (1) since Local Search heuristics are widely used by practitioners, our work shows that the three main stability conditions capture some of the structure of practical inputs that make Local Search efficient, giving more legitimacy to the stability conditions and (2) assuming that the stability conditions are legitimate (i.e.: characterize real-world instances), our results make a step toward understanding the success of Local Search heuristics.\nWe now proceed to a more formal exposition of our contribution. The problem we consider in this work is the following slightly more general version of the k-means and k-median problems.\nDefinition 1.1 (k-Clustering). Let A be a set of clients, F a set of centers, both lying a metric space (X ,dist), cost a function A\u00d7F \u2192 R+, and k a non-negative integer. The k-clustering problem asks for a subset S of F , of cardinality at most k, that minimizes\ncost(S) = \u2211\nx\u2208A min c\u2208S cost(x, c).\nThe clustering of A induced by S is the partition of A into subsets C = {C1, . . . Ck} such that Ci = {x \u2208 A | ci = argmin\nc\u2208S cost(x, c)} (breaking ties arbitrarily).\nThe well known k-median and k-means problems correspond to the special cases cost(a, c) = dist(a, c) and cost(a, c) = dist(a, c)2 respectively.\nIn this paper we will analyze the performance of the following widely-used Local Search algorithm (Algorithm 1) (see e.g.: [1] or [56]). This algorithm has a polynomial running time (see [9, 33]). In the following we will refer to its parameter ((\u03b5) in the description of Algorithm 1) by the neighborhood size of Local Search.\nAlgorithm 1 Local Search(\u03b5) for k-Median and k-Means\n1: Input: A,F, cost, k 2: S \u2190 Arbitrary subset of F of cardinality at most k. 3: while \u2203 S\u2032 s.t. |S\u2032| \u2264 k and |S \u2212 S\u2032|+ |S\u2032 \u2212 S| \u2264 1/\u03b5 and cost(S\u2032) \u2264 (1\u2212 \u03b5/n) cost(S) 4: do 5: S \u2190 S\u2032 6: end while 7: Output: S\nVarious forms of stability conditions that should be satisfied by well-clusterable instances have been proposed, see Figure 1. The main three incomparable notions are distribution stability, perturbation resilience, and spectral separability.\nWe detail the different notions of stability and the results we obtain for each of them. Throughout the rest of this paper, let OPT denote the value of an optimal solution. We start with the notion of stability due to Awasthi et al. [10], called \u201cdistribution stability\u201d.\nDefinition 1.2 (Distribution Stability [10]). Let (A,F, cost, k) be an input for k-clustering and let {C\u22171 , . . . , C\u2217k} denote the optimal k-clustering of A with centers S = {c\u22171, . . . c\u2217k}. Given \u03b2 > 0, the\ninstance is \u03b2-distribution stable if, for any i, \u2200x /\u2208 C\u2217i ,\ncost(x, c\u2217i ) \u2265 \u03b2 OPT\n|C\u2217i | .\nWe show that Local Search is a PTAS for \u03b2-distribution stable instances. Moreover, we show that for almost all clusters (i.e.: at least k\u2212O(\u03b2\u22121\u03b5\u22123)), the algorithm recovers most of the optimal clusters (i.e.: there is a bijection between the optimal clusters and the clusters of the algorithm such that a (1\u2212 \u03b5) fraction of the points of each cluster agree). Theorem 1.1. There exists a constant c such that the following holds. Let \u03b2 > 0 and \u03b5 < 1/2. For any \u03b2-stable instance, the solution output by Local Search(c1\u03b5\n\u22123\u03b2\u22121) (Algorithm 1) has cost at most (1 + \u03b5)OPT.\nMoreover, let C\u2217 = {C\u22171 , . . . , C\u2217k} denote an optimal k-clustering and let L = {L1, . . . , Lk} denote the clustering output by Local Search(c1\u03b5\n\u22123\u03b2\u22121). There exists a bijection \u03c6 : L 7\u2192 C\u2217 such that for at least m = k \u2212 O(\u03b2\u22121\u03b5\u22123) clusters L\u20321, . . . , L\u2032m \u2286 L, we have both (1 \u2212 c1\u03b5)|\u03c6(L\u20321)| \u2264 |L\u20321 \u2229 \u03c6(L\u20321)| and (1\u2212 c1\u03b5)|L\u20321| \u2264 |L\u20321 \u2229 \u03c6(L\u20321)|.\nA PTAS for \u03b2-distribution stable instances was previously given by Awasthi et al. [10]. Our contribution is to show that (1) Local Search is already a PTAS: no specific algorithm is needed, and (2) Local Search recovers most of the structure of the underlying optimal clustering. Moreover, \u03b2distribution stability is also implied by \u201ccost separation\u201d as defined by Ostrovsky et al. [69], so Local Search is also a PTAS in their setting and also recovers most of the structure of such instances. Furthermore, our results extend to a slightly more general definition of \u03b2-distribution stability (where only a (1 \u2212 \u03b4) > 1/2 fraction of the points of each cluster has to satisfy the \u03b2-distribution stability condition) at the expense of a (1 +O(\u03b4)) factor in the approximation guarantee.\nWe now turn to the second notion of stability, called \u201cperturbation resilience\u201d.\nDefinition 1.3 (Perturbation Resilience [11]). Let (A,F, cost, k) be an input for k-clustering and let {C\u22171 , . . . , C\u2217k} denote the optimal k-clustering of A with centers S = {c\u22171, . . . c\u2217k}. Given \u03b1 \u2265 1, the instance is \u03b1-perturbation-resilient if for any cost function cost\u2032 on A with\n\u2200 (p, q) \u2208 A\u00d7 F, cost(p, q) \u2264 cost\u2032(p, q) \u2264 \u03b1 \u00b7 cost(p, q),\n{C\u22171 , . . . , C\u2217k} is the unique optimal clustering of the instance (A,F, cost\u2032, k). This notion of stability was historically defined for cost = dist. We show that a local optimum must be the global optimum in that case. More precisely, consider a solution S0 to the k-clustering problem with parameter p. We say that S0 is 1/\u03b5-locally optimal if for any solution S1 such that |S0 \u2212 S1|+ |S1 \u2212 S0| \u2264 2/\u03b5, cost(S1) \u2265 cost(S0). Theorem 1.2. Let \u03b1 > 3. For any instance of the k-median problem that is \u03b1-perturbationresilient, any 2(\u03b1 \u2212 3)\u22121-locally optimal solution is the optimal clustering {C\u22171 , . . . , C\u2217k}.\nWe extend this theorem to instances for which cost = distp for some constant p (in particular for p = 2, where the k-clustering instance corresponds to the k-means problem). An optimal algorithm for 2-perturbation resilient clustering for any center-based objective function was very recently given by Bakshi and Chepurko [14]. Our contribution is to show that Local Search is already optimal for 3 + \u03b5-perturbation resilient instances; no specific algorithm is needed.1\nWe now turn to the third stability condition, called \u201cspectral separation\u201d.\n1We do not quite match [14]: One limitation is that Local Search is not necessarily optimal for 2-perturbation resilient instances, see Proposition 3.15\nDefinition 1.4 (Spectral Separation [60]2). Let (A,Rd, || \u00b7 ||2, k) be an input for k-means clustering in Euclidean space and let {C\u22171 , . . . C\u2217k} denote an optimal clustering of A with centers S = {c\u22171, . . . c\u2217k}. Denote by C an n\u00d7 d matrix such that the row Ci = argmin\nc\u2217j\u2208S ||Ai \u2212 c\u2217j ||2. Denote\nby || \u00b7 ||2 the spectral norm of a matrix. Then {C\u22171 , . . . C\u2217k} is \u03b3-spectrally separated, if for any pair (i, j) the following condition holds:\n||c\u2217i \u2212 c\u2217j || \u2265 \u03b3 \u00b7\n\n 1 \u221a\n|C\u2217i | +\n1 \u221a\n|C\u2217j |\n\n ||A\u2212 C||2.\nSince this stability is defined over non-finite metric spaces, we require standard preprocessing steps in order to use Local Search, see Algorithm 2. They consist of reducing the number of dimensions and discretizing the space in order to bound the number of candidate centers.\nAlgorithm 2 Project and Local Search\n1: Project points A onto the best rank k/\u03b5 subspace 2: Embed points into a random subspace of dimension O(\u03b5\u22122 log n) 3: Compute candidate centers (Corollary 1) 4: Local Search(\u0398(\u03b5\u22124)) 5: Output clustering\nTheorem 1.3. Let (A,Rd, || \u00b7 ||2, k) be an instance of Euclidean k-means clustering with optimal clustering C = {C\u22171 , . . . C\u2217k} and centers S = {c\u22171, . . . c\u2217k}. If C is more than 3 \u221a k-spectrally separated, then Algorithm 2 is a polynomial time approximation scheme.\nIn previous work by Kumar and Kannan [60], an algorithm was given with approximation ratio 1+O(OPTk/OPTk\u22121), where OPTi denotes the value of an optimal solution using i centers. Assuming that OPTk/OPTk\u22121 \u2264 \u03b5 implies that the optimal k-clustering C is \u2126( \u221a\nk/\u03b5)-spectrally separated [60]. Thus our assumption in Theorem 1.3 that C is \u221a k-spectrally separated is weaker (it does not depend on \u03b5) and therefore our result is stronger since the approximation guarantee does not depend on the assumption about instances. We obtain a PTAS. We note that the previous algorithms focused on recovering the target optimal clustering and not optimizing the k-means objective function, though there exists some overlap. In general, a (1 + \u03b5)-approximation does not have to agree with the target clustering on a majority of points, see Remark 4.0.1. There are applications where finding the correct classification is more relevant and applications where minimizing the value of the k-means objective is relevant (in image compression for example, see e.g.: [56]).\nThe main message is that Local Search occupies a sweet spot between practical performance and theoretical guarantees, both with respect to worst case instances and with respect to stable instances for various notions of stability. More boldly, our work indicates that many formal characterizations of practical instances can be, at least to some degree, viewed as \u201cinstances for which Local Search works well\u201d. It supports the definition of stability conditions as conditions characterizing real-world inputs since Local Search heuristics are very popular among practitioners.\nWhile the (worst case) running time bounds given in this work might appear too high for realworld applications, we consider this view as possibly too pessimistic, given that there is a trade-off between how \u201cstable\u201d the instances are and the quality of approximation. Hence if instances are\n2The proximity condition of Kumar and Kannan [60] implies the spectral separation condition.\nhighly stable (i.e.: the parameter of the stability condition is high), Local Search does not require a large neighborhood size to output a nearly-optimal solution."}, {"heading": "1.2 Related Work", "text": "The problems we study are NP-hard: k-median and k-means are already NP-hard in the Euclidean plane (see Meggido and Supowit [67], Mahajan et al. [64], and Dasgupta and Freud [38]). In terms of hardness of approximation, both problems are APX-hard, even in the Euclidean setting when both k and d are part of the input (see Gua and Khuller [44], Jain et al. [51], Guruswami et al. [47] and Awasthi et al. [12]). On the positive side, constant factor approximations are known in metric space for both k-median and k-means (see for example [63, 52, 68]).\nGiven the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.\nCost Separation In one of the earliest attempts to formalize the notion of a meaningful clustering, Ostrovsky et al. [69] assumed that cost of an optimal clustering with k centers is smaller than an \u03b52-fraction of the cost of an optimal clustering with k\u2212 1 centers, see also Schulman [72] for an earlier condition for two clusters and the irreducibility condition by Kumar et al. [61]. The condition has several appealing properties. It is robust against small perturbations of the data set and it implies that two low-cost clusterings agree on a large fraction of points. It is also motivated by the commonly used elbow method of determining the correct value of k: run an algorithm for an incrementally increasing number of clusters until the cost drops significantly. The popular D2 sampling technique (also known as k-means++) has an improved performance for cost separated instances compared to the worst-case O(log k)-approximation ratio [7, 28, 53, 69]. A related, slightly weaker condition called \u03b1-weakly-deletion stability was introduced by Awasthi et al. [10] where the cost of assigning all the points from one cluster in the optimal k-clustering to another center increases the objective by some factor (1 + \u03b1).\nTarget-Based Clustering The notion of finding a target clustering is more prevalent in machine learning than minimizing an objective function. Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73]. Balcan et al. [15, 16] gave a deterministic condition called approximation stability under which a target clustering can be retrieved by via any sufficiently good algorithm for the k-means (or kmedian) objective function. The distance between two clusterings with k clusters is the minimum number of disagreements under all perfect matchings of clusters. Two clustering are \u03b5-close if the \u201cedit distance\u201d is at most \u03b5 \u00b7 n. If the instance satisfies that any clustering with cost within a factor c of the optimum is \u03b5-close to the target clustering, it is called (c, \u03b5)-stable. The condition was extended to account for the presence of noisy data by Balcan et al. [20]. For results using approximation stability, see [16, 17, 71, 3, 10].\nAnother deterministic condition that relates target clustering recovery via the k-means objective was introduced by Kumar and Kannan [60]. Viewing each data point as a row of a matrix A and the rows of the center matrix K containing the centroid of the respective target cluster of A, it imposes a proximity condition on each point Ai when projected onto the line connecting its target centroid cj and some other centroid c\u2113. The condition requires that the projection of Ai is closer\nto cj than to any c\u2113 by a factor of \u2126\n( k \u00b7 (\n1\u221a |Cj | + 1\u221a |C\u2113|\n) \u00b7 ||A\u2212K||2 ) , where ||A \u2212 K||2 is the\nspectral norm and Cj and C\u2113 are the target clusters. This condition implies spectral separability with \u03b3 \u2208 \u2126(k). For further spectral based approaches, see also [13].\nPerturbation Resilience The notion behind this stability condition is that bounded modifications to the input should not affect the optimum solution. Bilu et al. [26, 25] formalized this as allowing edge weights in a graph to be modified by a factor of at most \u03b3 without changing the maxcut. Perturbation resilience has some similarity to smoothed analysis (see Arthur et al. [6, 8] for work on k-means). The main difference is that smoothed analysis takes a worst case instance and applies a random perturbation, while perturbation resilience takes a well-behaved instance and applies an adversarial perturbation. For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].\nLocal Search There exists a large body of bicriteria approximations for k-median and k-means [59, 30, 33, 21]. Arya et al. [9] (see also [46]) gave the first analysis showing that Local Search with a neighborhood size of 1/\u03b5 gives a 3 + 2\u03b5 approximation to k-median and showed that this bound is tight. Kanungo et al. [56] proved an approximation ratio of 9 + \u03b5 for Euclidean k-means clustering by Local Search, currently the best known algorithm with a polynomial running time in metric and Euclidean spaces.3 Recently, Local Search with an appropriate neighborhood was shown to be a PTAS for k-means and k-median in certain restricted metrics including constant dimensional Euclidean space [42, 32]. Due to its simplicity, Local Search is also a popular subroutine for clustering tasks in various computational models [22, 27, 45]. For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74]"}, {"heading": "2 Approach and Techniques", "text": ""}, {"heading": "2.1 Distribution Stability", "text": "The first important observation is that only a few clusters have more than a 1/\u03b5\u22123 fraction of the total cost of the solution. For these clusters, Local Search with appropriate an neighborhood size will find the optimal solution. Among the remaining clusters, the centers and points close to the center are far away from each other. Any locally optimal solution cannot err on too many of these clusters. The cost of charging the points of the remaining clusters can then be charged into the overall contribution, allowing us to bound the approximation factor, see Figure 2. Our proof includes a few ingredients from [10] such as the notion of inner-ring (we work with a slightly more general definition) and distinguishing between cheap and expensive clusters. However, our analysis is more general as it allows us to analyze not only the cost of the solution of the algorithm, but also the structure of the clusters.\nEuclidean inputs can be straightforwardly \u201cdiscretized\u201d by computing an appropriate candidate set of centers, for instance via Matousek\u2019s approximate centroid set [65] and then applying the Johnson-Lindenstrauss lemma, if the dimension is too large.\n3They combined Local Search with techniques from Matousek [65] for k-means clustering in Euclidean spaces. The running time of the algorithm as stated incurs an additional factor of \u03b5\u2212d due to the use of Matousek\u2019s approximate centroid set. Using standard techniques (see e.g. Section B of this paper), a fully polynomial running time in n, d, and k is also possible without sacrificing approximation guarantees."}, {"heading": "2.2 Perturbation Resilience", "text": "The tight approximation factor of 3 + \u03b5 of Local Search for k-median implies a locality gap4 of 3. The main observation is that perturbation resilience implies that locally optimal solutions for a local neighborhood of appropriate size are equal to the global optimum."}, {"heading": "2.3 Spectral Separability", "text": "In Section 4 we study the spectral separability conditions for the Euclidean k-means problem. Nowadays, a standard preprocessing step in Euclidean k-means clustering is to project onto the subspace spanned by the rank k-approximation. Indeed, this is the first step of the algorithm by Kumar and Kannan [60] (see Algorithm 3).\nAlgorithm 3 k-means with spectral initialization [60]\n1: Project points onto the best rank k subspace 2: Compute a clustering C with constant approximation factor on the projection 3: Initialize centroids of each cluster of C as centers in the original space 4: Run Lloyd\u2019s k-means until convergence\nIn general, projecting onto the best rank k subspace and computing a constant approximation on the projection results in a constant approximation in the original space. Kumar and Kannan [60] and later Awasthi and Sheffet [13] gave tighter bounds if the spectral separation is large enough. Our algorithm omits steps 3 and 4. Instead, we project onto slightly more dimensions and subsequently use Local Search as the constant factor approximation in step 2. To utilize Local Search, we further require a candidate set of solutions, which is described in Section B. For pseudocode, we refer to Algorithm 2.\nIt is easy to show that spectral separability implies distribution stability if the dimension is of or-\nder k: (1) the distance between centers is \u2126(\n(\n1\u221a |C\u2217i | + 1\u221a|C\u2217j | ) k\u00b7||A\u2212C||2) = \u2126 (( 1\u221a |C\u2217i | + 1\u221a|C\u2217j | )\u221a OPT ) ,\nand (2) the distance of any point to the \u201cwrong\u201d center is at least 1/2 of this amount, i.e. the cost of assigning a point to the \u201cwrong\u201d cluster is \u2126(OPT|Cj | ). Projecting onto sufficiently many dimensions allows us to transform a high dimensional point set into a low dimensional one, see for instance recent work by Cohen et al. [31]. The projection retains the cost and spectral separability of a clustering, however it does not preserve optimality. In particular, the distance of a single point to the centroids of the clusters can be arbitrarily distorted (see Figure 3), which prevents us from using the naive reduction to distribution stability.\nInstead, we locally improve on the optimal clustering by reassigning points (Lemma 4.1). A large contraction of relevant distances can only happen for few points, i.e. the cluster sizes are roughly the same. For the remaining points, we can show that they are guaranteed to have a minimum distance to the wrong center.\n4The locality gap is the maximum ratio between a local optimum and a global optimum."}, {"heading": "3 Metric Spaces", "text": ""}, {"heading": "3.1 Preliminaries", "text": "In this section, we consider inputs that consist in both a set of clients A and a set of candidate centers F , together with a distance function dist : A \u222a F \u00d7 A \u222a F \u2192 R+5. Let p \u2265 1. We assume that the cost function is defined as cost(a, b) = dist(a, b)p, for any (a, b) \u2208 A \u00d7 F . Observe that this is the case for the k-median and k-means problems with p = 1 and 2 respectively.\nTo give a slightly simpler proof we will assume that p = 1. Applying the two following lemmas at different steps of the proof ensure that the result holds for higher value of p (by introducing a dependency in 1/\u03b5O(p) in the neighborhood size of the algorithm).\nLemma 3.1. Let p \u2265 0 and 1/2 > \u03b5 > 0. For any a, b, c \u2208 A \u222a F , we have cost(a, b) \u2264 (1 + \u03b5)pcost(a, c) + cost(c, b)/\u03b5p.\nThe following lemma follows from the binomial theorem.\nLemma 3.2. Let p \u2265 0. For any a, b, c \u2208 A \u222a F , we have cost(a, b) \u2264 2p(cost(a, c) + cost(c, b))."}, {"heading": "3.2 Distribution Stability", "text": "We work with the notion \u03b2, \u03b4-distribution stability which generalizes \u03b2-distribution stability. This extends our result to datasets such that for each cluster of the optimal solution, most of the points satisfy the \u03b2-distribution stability condition.\nDefinition 3.1 ((\u03b2, \u03b4)-Distribution Stability). Let (A,F, cost, k) be an instance of k-clustering where A \u222a F lie in a metric space and let C\u2217 = {C\u22171 , . . . , C\u2217k} denote a partition of A and S\u2217 = {c\u22171, . . . , c\u2217k} \u2286 F be a set of centers. Further, let \u03b2 > 0 and 1/2 > \u03b4. Then (A,F, cost, k), (C\u2217, S\u2217) is a (\u03b2, \u03b4)-distribution stable instance if, for any i, there exists a set \u2206i \u2286 C\u2217i such that |\u2206i| \u2265 (1\u2212 \u03b4)|C\u2217i | and for any x \u2208 \u2206i, for any j 6= i,\ncost(x, c\u2217j ) \u2265 \u03b2 OPT\n|C\u2217j | ,\nwhere cost(x, c\u2217j ) is the cost of assigning x to c \u2217 j .\nFor any (A,F, cost, k), (C\u2217, S\u2217) (\u03b2, \u03b4)-distribution stable instance, we refer to (C\u2217, S\u2217) as a (\u03b2, \u03b4)-clustering of the instance. We show the following theorem.\nTheorem 3.3. Let p > 0, there exists a constant c1 such that the following holds. Let \u03b2 > 0, \u03b4 < 1/2. For any \u03b5 < 1/2 and (\u03b2, \u03b4)-stable instance with (\u03b2, \u03b4) clustering (C\u2217, S\u2217), the cost of the solution output by Local Search(2\u03b5\u22123\u03b2\u22121) (Algorithm 1) is at most (1 + c1(\u03b5+ \u03b4))cost(C).\nMoreover, let L = {L1, . . . , Lk} denote the clusters of the solution output by Local Search(2\u03b5\u22123\u03b2\u22121). There exists a bijection \u03c6 : L 7\u2192 C\u2217 such that for at least m = k\u2212O(\u03b2\u22121\u03b5\u22123) clusters L\u20321, . . . , L\u2032m \u2286 L, we have (1\u2212 c1(\u03b5+ \u03b4))|\u03c6(L\u20321)|, (1 \u2212 c1(\u03b5+ \u03b4))|L\u20321| \u2264 |L\u20321 \u2229 \u03c6(L\u20321)|.\nNote that Theorem 1.1 is an immediate corollary of Theorem 3.3 by taking \u03b4 = 0. For ease of exposition, we give the proof of Theorem 3.3 for \u03b4 = 0 (see Section A for the general proof). Throughout this section we consider a set of centers S\u2217 = {c\u22171, . . . , c\u2217k} whose induced clustering is C\u2217 = {C\u22171 , . . . , C\u2217k} and such that the instance is (\u03b2, \u03b4)-stable with respect\n5This distance function is the metric completion of the dist function described in the introduction: \u2200a, b \u2208 A,dist(a, b) = minc\u2208F dist(a, c) + dist(c, f) and analogously for elements of F .\nto this clustering. We denote by clusters the parts of a partition C\u2217 = {C\u22171 , . . . , C\u2217k}. Let cost(C\u2217) =\n\u2211k i=1 \u2211 x\u2208C\u2217i cost(x, c \u2217 i ). Moreover, for any cluster C \u2217 i , for any client x \u2208 C\u2217i , de-\nnote by gx the cost of client x in solution C \u2217: gx = cost(x, c\u2217i ) = dist(x, c \u2217 i ) since p = 1. Let L denote the output of LocalSearch(\u03b2\u22121\u03b5\u22123) and l(x) the cost induced by client x in solution L, namely lx = min\u2113\u2208L cost(x, \u2113). The following definition is a generalization of the inner-ring definition of [10].\nDefinition 3.2. For any \u03b50, we define the inner ring of cluster i, IR \u03b50 i , as the set of x \u2208 A \u222a F such that dist(x, c\u2217i ) \u2264 \u03b50\u03b2OPT/|C\u2217i |.\nWe say that cluster i is cheap if \u2211 x\u2208C\u2217i gx \u2264 \u03b5 3\u03b2OPT, and expensive otherwise. We aim at\nproving the following structural lemma.\nLemma 3.4. There exists a set of clusters Z\u2217 \u2286 C\u2217 of size at most (\u03b5\u22123 + 160\u03b5\u22121)\u03b2\u22121 such that for any cluster C\u2217i \u2208 C\u2217 \u2212 Z\u2217, we have the following properties\n1. C\u2217i is cheap.\n2. At least a (1\u2212 \u03b5) fraction of IR\u03b52i are served by a unique center L(i) in solution L.\n3. The total number of clients p \u2208 \u22c3j 6=i\u2206j, that are served by L(i) in L is at most \u03b5|IR\u03b5 2 i |.\nSee Fig 2 for a typical cluster of C\u2217 \u2212Z\u2217. We start with the following lemma which generalizes Fact 4.1 in [10].\nLemma 3.5. Let C\u2217i be a cheap cluster. For any \u03b50, we have |IR\u03b50i | > (1\u2212 \u03b53/\u03b50)|C\u2217i |.\nWe then prove that the inner rings of cheap clusters are disjoint.\nLemma 3.6. Let \u03b50 < 1/3. If C \u2217 i 6= C\u2217j are cheap clusters, then IR\u03b50i \u2229 IR\u03b50j = \u2205.\nThe following observation follows directly from the definition of cheap clusters.\nLemma 3.7. Let Z1 \u2286 C\u2217 be the set of clusters of C\u2217 that are not cheap. Then |Z1| \u2264 \u03b5\u22123\u03b2\u22121.\nFor each cheap cluster C\u2217i , let L(i) denote a center of L that belongs to IR\u03b5i if there is one (and remain undefined otherwise). By Lemma 3.6, L(i) 6= L(j) for i 6= j.\nLemma 3.8. Let C\u2217\u2212Z2 denote the set of clusters C\u2217i that are cheap, such that L(i) is defined, and such that at least (1\u2212\u03b5)|IR\u03b52i | clients of IR\u03b5 2 i are served in L by L(i). Then |Z2| \u2264 (\u03b5\u22123+120\u03b5\u22121)\u03b2\u22121.\nProof. We distinguish five types of clusters in C\u2217: expensive clusters (k1), cheap clusters with L(i) undefined (k2), cheap clusters with exactly one center of L in IR\u03b5i \u2212 IR (1\u2212\u03b5)\u03b5 i (k3), cheap clusters with exactly one center of L in IR(1\u2212\u03b5)\u03b5i (k4), and cheap clusters with at least two centers of L in IR\u03b5i (k5). Since L and C\u2217 both have k clusters and the inner rings of cheap clusters are disjoint (Lemma 3.6), we have k5 \u2264 k1 + k2. By Lemma 3.7, we have k1 \u2264 \u03b5\u22123\u03b2\u22121.\nWe now bound the number k2 of cheap clusters such that L(i) is undefined. Consider a cheap cluster C\u2217i \u2286 C\u2217 \u2212 Z1 such that at least a (1 \u2212 \u03b5) fraction of the clients of IR\u03b5 2\ni are served in L by some centers that are either in IR\u03b5i \u2212 IR(1\u2212\u03b5)\u03b5i or not in IR\u03b5i . By the triangular inequality, the cost for any client c in IR\u03b5 2\ni is at least ((1 \u2212 \u03b5)\u03b5 \u2212 \u03b52)\u03b2OPT/|C\u2217i |. Since \u03b5 \u2264 1/2, it is at least \u03b5\u03b2OPT/(4|C\u2217i |). Since at least (1 \u2212 \u03b5)|IR\u03b5 2 i | clients of IR\u03b5 2 i are served by centers that are not in IR\u03b5i , the total cost in L induced by those clients is at least (1\u2212 \u03b5)|IR\u03b5 2\ni |\u03b5\u03b2OPT/(2|C\u2217i |). By Lemma 3.5, substituting |IR\u03b52i | yields,\n(1\u2212 \u03b5)|IR\u03b52i |\u03b5\u03b2 OPT 2|C\u2217i | \u2265 (1\u2212 \u03b5)(1 \u2212 \u03b5)|C\u2217i |\u03b5\u03b2 OPT 2|C\u2217i | \u2265 \u03b5\u03b2OPT 8\nsince \u03b5 \u2264 1/2. Now, observe that by [9], the cost of L is at most a 5 approximation to the cost of OPT in the worst case. Thus, k2 \u2264 40(\u03b5\u22121\u03b2\u22121).\nBy a similar argument, we can bound the number of clusters k4 such that a (1 \u2212 \u03b5) fraction the clients in IR\u03b5 2\ni are served by a center not in IR (1\u2212\u03b5)\u03b5 i . We have k4 \u2264 80(\u03b5\u22121\u03b2\u22121) since \u03b5 < 1/2.\nFor the remaining clusters, we have that there is a unique center located in IR (1\u2212\u03b5)\u03b5 i and that IR\u03b5i \u2212 IR (1\u2212\u03b5)\u03b5 i does not contain any center. Additionally, at least (1 \u2212 \u03b5)|IR\u03b5 2 i | clients in IR\u03b5 2 i are served by a center in IR\u03b5i and so in IR (1\u2212\u03b5)\u03b5 i . Thus, by the triangular inequality we have that at least (1 \u2212 \u03b5)|IR\u03b52i | clients in IR\u03b5 2 i are served by the unique center in IR \u03b5 i , namely L(i). It follows that |Z2| \u2264 (\u03b5\u22123 + 120\u03b5\u22121)\u03b2\u22121.\nWe continue with the following lemma, whose proof relies on similar arguments.\nLemma 3.9. There exists a set Z3 \u2286 C\u2217 \u2212 Z2 of size at most (\u03b5\u22123 + 40\u03b5\u22121)\u03b2\u22121 such that for any cluster C\u2217j \u2208 C\u2217 \u2212 Z3, the total number of clients x \u2208 \u22c3\ni 6=j \u2206i, that are served by L(j) in L is at most \u03b5|IR\u03b52i |.\nTherefore, the proof of Lemma 3.4 follows from combining Lemmas 3.7,3.8 and 3.9. We now turn to the analysis of the cost of L. Let C(Z\u2217) = \u22c3C\u2217i \u2208Z\u2217 C \u2217 i . For any cluster\nC\u2217i \u2208 C\u2217 \u2212 Z\u2217, let L(i) be the unique center of L that serves a set of clients Ai \u2286 IR\u03b5 2 i such that\n|Ai| \u2265 (1 \u2212 \u03b5)|IR\u03b5 2 i |. Let L\u0302 = \u22c3 C\u2217i \u2208C\u2217\u2212Z\u2217 L(i) and L\u0304 = L \u2212 L\u0302. Define A\u0304 and A\u0302 to be the set of clients that are served in solution L by centers of L\u0304 and L\u0302 respectively. Finally, let A(L(i)) be the set of clients that are served by L(i) in solution L. Observe that the A(L(i)) partition A\u0302. Lemma 3.10. We have\n\u2212\u03b5cost(L)/n+ \u2211\nx\u2208A\u0304\u222aC(Z\u2217) lx \u2264\n\u2211\nx\u2208A\u0304\u222aC(Z\u2217) gx + \u03b5(cost(L) + cost(C\u2217))/(1 \u2212 \u03b5)2.\nProof. Consider the following mixed solution M = L\u0302\u222a{c\u2217i \u2208 Z\u2217 | C\u2217i \u2208 Z\u2217}. We start by bounding the cost of M. For any client x /\u2208 A\u0304 \u222a C(Z\u2217), the center that serves it in L belongs to M. Thus its cost is at most lx. Now, for any client x \u2208 C\u0304(Z\u2217), the center that serves it in Z\u2217 is in M, so its cost is at most gx.\nFinally, we evaluate the cost of the clients in A\u0304\u2212C(Z\u2217). Consider such a client x and let C\u2217i be the cluster it belongs to in solution C\u2217. By definition of A\u0304 we have that C\u2217i /\u2208 Z\u2217. Therefore, L(i) is defined and so we have L(i) \u2208 L\u0302 \u2286 M. Hence, the cost of x in M is at most cost(x,L(i)). Observe that by the triangular inequality dist(x,L(i)) \u2264 dist(x, c\u2217i ) + dist(c\u2217i ,L(i)) = gx + dist(c\u2217i ,L(i)).\nNow consider a client x\u2032 \u2208 C\u2217i \u2229Ai. By the triangular inequality, we have that cost(c\u2217i ,L(i)) \u2264 cost(c\u2217i , x \u2032) + cost(x\u2032,L(i)) = gx\u2032 + lx\u2032 . Hence,\ncost(c\u2217i ,L(i)) \u2264 1 |C\u2217i \u2229Ai| \u2211\nx\u2032\u2208C\u2217i \u2229Ai (gx\u2032 + lx\u2032).\nIt follows that assigning the clients of A\u0304 \u2229 C\u2217i to L(i) induces a cost of at most \u2211\nx\u2208A\u0304\u2229C\u2217i\ngx + |C\u2217i \u2229 A\u0304| |C\u2217i \u2229Ai| \u2211\nx\u2032\u2208C\u2217i \u2229Ai (gx\u2032 + lx\u2032).\nBy Lemma 3.8 and the definition of Z\u2217, we have that |C\u2217i \u2229 A\u0304|/|C\u2217i \u2229 Ai| \u2264 \u03b5/(1 \u2212 \u03b5)2. Summing over all clusters C\u2217i /\u2208 Z\u2217, we obtain that the cost in M for the clients in A\u0304 \u2229 C\u2217i is at most\n\u2211\nc\u2208A\u0304\u2212C(Z\u2217) gx +\n\u03b5\n(1\u2212 \u03b5)2 (cost(C \u2217) + cost(L)).\nBy Lemmas 3.8,3.9, we have that |M \u2212 L| + |L \u2212 M| \u2264 3(\u03b5\u22123 + 40\u03b5\u22121)\u03b2\u22121. Thus, by local optimality (1\u2212 \u03b5/n)cost(L) \u2264 cost(M). Therefore, combining the above observations, we have\n(1\u2212 \u03b5 n )cost(L) \u2264\n\u2211\nx/\u2208A\u0304\u222aC(Z\u2217) lx +\n\u2211\nx\u2208A\u0304\u222aC(Z\u2217) gx +\n\u03b5\n(1\u2212 \u03b5)2 (cost(C \u2217) + cost(L))\n\u2212 \u03b5 n cost(L) +\n\u2211\nx/\u2208A\u0304\u222aC(Z\u2217) lx +\n\u2211\nx\u2208A\u0304\u222aC(Z\u2217) lx \u2264\n\u2211\nx/\u2208A\u0304\u222aC(Z\u2217) lx +\n\u2211\nx\u2208A\u0304\u222aC(Z\u2217) gx +\n\u03b5\n(1\u2212 \u03b5)2 (cost(C \u2217) + cost(L)).\n\u2212 \u03b5 n cost(L) +\n\u2211\nx\u2208A\u0304\u222aC(Z\u2217) lx \u2264\n\u2211\nx\u2208A\u0304\u222aC(Z\u2217) gx +\n\u03b5\n(1\u2212 \u03b5)2 (cost(L) + cost(C \u2217)).\nWe now turn to evaluate the cost for the clients that are not in A\u0304 \u222a C(Z\u2217), namely the clients in A\u0302\u2212C(Z\u2217). For any cluster C\u2217i , for any x \u2208 C\u2217i \u2212\u2206i define Reassign(x) to be the distance from x to the center in L that is the closest to c\u2217i . Before going deeper in the analysis, we need the following lemma.\nLemma 3.11. For any \u03b4 < 1/2, we have for any C\u2217i ,\n\u2211\nx\u2208C\u2217i \u2212\u2206i Reassign(x) \u2264\n\u2211\nx\u2208C\u2217i \u2212\u2206i gx +\n\u03b4\n(1\u2212 \u03b4) \u2211\nx\u2208C\u2217i\n(lx + gx).\nProof. Consider a client x \u2208 C\u2217i \u2212 \u2206i. Let \u2113\u2032 be the center that serves at least one client of \u2206i that is the closest to c\u2217i . Since \u03b4 < 1, \u2113\n\u2032 is well defined. By the triangular inequality we have that Reassign(x) \u2264 cost(x, \u2113\u2032) \u2264 cost(x, c\u2217i ) + cost(c\u2217i , \u2113\u2032) = gx + cost(c\u2217i , \u2113\u2032). Then,\n\u2211\nx\u2208C\u2217i \u2212\u2206i Reassign(x) \u2264\n\u2211\nx\u2208C\u2217i \u2212\u2206i gx + |C\u2217i \u2212\u2206i| \u00b7 cost(c\u2217i , \u2113\u2032).\nNow, since \u2113\u2032 is the center that serves at least one client of \u2206i that is the closest to c\u2217i we have that for any x \u2208 \u2206i, by the triangular inequality cost(c\u2217i , \u2113\u2032) \u2264 cost(c\u2217i , x) + cost(x, \u2113\u2032) \u2264 gx + lx. Therefore,\ncost(c\u2217i , \u2113 \u2032) \u2264 1|\u2206i| \u2211\nx\u2208\u2206i (gx + lx).\nCombining, we obtain\n\u2211\nx\u2208C\u2217i \u2212\u2206i Reassign(x) \u2264\n\u2211\nx\u2208C\u2217i \u2212\u2206i gx +\n|C\u2217i \u2212\u2206i| |\u2206i| \u2211\nx\u2208\u2206i (gx + lx)\n= \u2211\nx\u2208C\u2217i \u2212\u2206i gx +\n\u03b4\n(1\u2212 \u03b4) \u2211\nx\u2208\u2206i (gx + lx),\nby definition of (\u03b2, \u03b4)-stability.\nWe now partition the clients of cluster C\u2217i \u2208 C\u2217 \u2212 Z\u2217. For any i, let \u2206\u0304i be the set of clients of C\u2217i that are served in solution L by a center L(j) for some j 6= i and C\u2217j \u2208 C\u2217 \u2212 Z\u2217. Moreover, let \u2206\u0303i = (A(L(i)) \u2229 ( \u22c3 j 6=i \u2206\u0304j))\u2212 C(Z\u2217). Finally, define C\u0303\u2217i = C\u2217i \u2212 (A\u0304 \u222a \u22c3 j 6=i \u2206\u0303j). Lemma 3.12. Let C\u2217i be a cluster in C \u2217 \u2212 Z\u2217. Define the solution Mi = L \u2212 {L(i)} \u222a {c\u2217i } and denote by mic the cost of client c in solution Mi. Then \u2211\nx\u2208A mix \u2264\n\u2211\nx/\u2208A(L(i))\u222aC\u0303\u2217i\nlc + \u2211\nx\u2208C\u0303\u2217i\ngx + \u2211\nx\u2208\u2206\u0303i\nReassign(x) + \u2211\nx\u2208A(L(i))\u2212 (C\u0303\u2217i \u222a\u2206\u0303i)\nlx + \u03b5 (1\u2212 \u03b5) ( \u2211\nx\u2208C\u0303\u2217i\ngx + lx).\nProof. Assume towards contradiction that this is not true. Consider a client x \u2208 IR\u03b50i \u2229 IR\u03b50j . Without loss of generality assume |C\u2217i | \u2265 |C\u2217j |. By the triangular inequality we have cost(c\u2217j , c\u2217i ) \u2264 cost(c\u2217j , x) + cost(x, c \u2217 i ) \u2264 2\u03b50\u03b2OPT/|C\u2217j |. Since \u03b4 < 1/2, there exists a client c\u2032 \u2208 IR\u03b50i \u2229\u2206i. Thus, we have cost(c\u2032, c\u2217j ) \u2264 3\u03b50\u03b2OPT/|C\u2217j |. This is a contradiction to the assumption that the instance is (\u03b2, \u03b4)-distribution stable with respect to (C\u2217, S\u2217) and c\u2032 being in \u2206i for any \u03b50 < 1/3.\nWe can thus prove the following lemma, which concludes the proof.\nLemma 3.13. There exists a constant \u03b7 such that\n\u2212\u03b5 \u00b7 cost(L) + \u2211\nx\u2208A\u0302\u2212C(Z\u2217)\nlx \u2264 \u2211\nx\u2208A\u0302\u2212C(Z\u2217)\ngx + (\u03b7(\u03b5+ \u03b4 1\u2212 \u03b4 ))(cost(L) + cost(C \u2217)).\nThe proof of Theorem 3.3 follows from (1) observing that A\u0304\u222aA\u0302 = A and summing the equations from Lemmas 3.10 and 3.13 and (2) Lemma 3.4."}, {"heading": "3.3 \u03b1-Perturbation-Resilient Instances", "text": "We consider the standard definition of \u03b1-perturbation-resilient instances.\nDefinition 3.3. Let I = (A,F, cost, k) be an instance for the k, p-clustering problem. For \u03b1 \u2265 1, I is \u03b1-perturbation-resilient if there exists a unique optimal clustering {C\u22171 , . . . , C\u2217k} and for any instance I \u2032 = (A,F, cost\u2032, k, p), such that\n\u2200 p, q \u2208 P, cost(p, q) \u2264 cost\u2032(p, q) \u2264 \u03b1dist(p, q),\nthe unique optimal clustering is {C\u22171 , . . . , C\u2217k}.\nObserve that cost\u2032 in Definition 3.3 needs not be a metric. In the following, we assume that cost(a, b) = dist(a, b)p for some fixed p and some distance function dist defined over A\u222aF . Consider a solution S0 to the k-clustering problem with parameter p. We say that S0 is 1/\u03b5-locally optimal if any solution S1 such that |S0 \u2212 S1|+ |S1 \u2212 S0| \u2264 2/\u03b5 has cost at least cost(S0). Theorem 1.2. Let \u03b1 > 3. For any instance of the k-median problem that is \u03b1-perturbationresilient, any 2(\u03b1\u2212 3)\u22121-locally optimal solution is the optimal clustering {C\u22171 , . . . , C\u2217k}.\nFor ease of exposition, we give the proof for the k-median problem, when p = 1. Applying Lemma 3.1 in the proof of Lemma C.1 yields the results for general p with \u03b1 growing exponentially with p. Moreover, define lc to be the cost for client c in solution L and gc to be its cost in the optimal solution C\u2217. Finally, for any sets of centers S and S0 \u2282 S, define NS(S0) to be the set of clients served by a center of S0 in solution S, i.e.: NS(S0) = {x | \u2203s \u2208 S0,dist(x, s) = mins\u2032\u2208S dist(x, s\u2032)}.\nThe proof of Theorem 1.2 relies on the following theorem of particular interest.\nTheorem 3.14 (Local-Approximation Theorem.). Let L be a 1/\u03b5-locally optimal solution and C\u2217 be any solution. Define S = L \u2229 C\u2217 and L\u0303 = L \u2212 S and C\u0303\u2217 = C\u2217 \u2212 S. Then\n\u2211\nc\u2208NC\u2217(C\u0303\u2217)\u222aNL(L\u0303)\nlc \u2264 (3 + 2\u03b5) \u2211\nc\u2208NC\u2217 (C\u0303\u2217)\u222aNL(L\u0303)\ngc.\nAdditionally, we show that the analysis is tight:\nProposition 3.15. There exists an infinite family of 3-perturbation-resilient instances such that for any constant \u03b5 > 0, there exists a \u03b5\u22121-locally optimal solution that has cost at least 3OPT.\nThis relies on the example from [9]. It is straightforward to see that the instance they provide is 3-perturbation-resilient."}, {"heading": "4 Spectral Separability", "text": "In this Section we will study the spectral separability condition for the Euclidean k-means problem. Our main result will be a proof of Theorem 1.3.\nWe first recall the basic notions and definitions for Euclidean k-means. Let A \u2208 Rn\u00d7d be a set of points in d-dimensional Euclidean space, where the row Ai contains the coordinates of the ith point. The singular value decomposition is defined as A = U\u03a3V T , where U \u2208 Rn\u00d7d and V \u2208 Rd\u00d7d are orthogonal and \u03a3 \u2208 Rd\u00d7d is a diagonal matrix containing the singular values where per convention the singular values are given in descending order, i.e. \u03a31,1 = \u03c31 \u2265 \u03a32,2 = \u03c32 \u2265 . . .\u03a3d,d = \u03c3d. Denote the Euclidean norm of a d-dimensional vector x by ||x|| = \u221a\n\u2211d i=1 x 2 i . The spectral norm\nand Frobenius norm are defined as ||A||2 = \u03c31 and ||A||F = \u221a \u2211d i=1 \u03c3 2 i , respectively.\nThe best rank k approximation min rank(X)=k\n||A \u2212 X||F is given via Ak = Uk\u03a3V T = U\u03a3kV T =\nU\u03a3V Tk , where Uk, \u03a3k and V T k consist of the first k columns of U , \u03a3 and V T , respectively, and are zero otherwise. The best rank k approximation also minimizes the spectral norm, that is ||A \u2212 Ak||2 = \u03c3k+1 is minimal among all matrices of rank k. The following fact is well known throughout k-means literature and will be used frequently throughout this section.\nFact 1. Let A be a set of points in Euclidean space and denote by c(A) = 1|A| \u2211\nx\u2208A x the centroid of A. Then the 1-means cost of any candidate center c can be decomposed via\n\u2211 x\u2208A ||x\u2212 c||2 = \u2211 x\u2208A ||x\u2212 c(A)||2 + |A| \u00b7 ||c(A) \u2212 c||2\nand \u2211\nx\u2208A ||x\u2212 c(A)||2 = 1 2 \u00b7 |A| \u2211 x\u2208A \u2211 y\u2208A ||x\u2212 y||2.\nNote that the centroid is the optimal 1-means center of A. For a clustering C = {C1, . . . Ck} of A with centers S = {c1, . . . ck}, the cost is then \u2211k i=1 \u2211 p\u2208Ci ||p\u2212ci||2. Further, if ci = 1 |Ci| \u2211\np\u2208Ci p, we can rewrite the objective function in matrix form by associating the ith point with the ith row of\nsome matrix A and using the cluster matrix X \u2208 Rn\u00d7k with Xi,j =\n\n\n\n1\u221a |C\u2217j | if Ai \u2208 C\u2217j 0 else to denote\nmembership. Note that XTX = I, i.e. X is an orthogonal projection and that ||A\u2212XXTA||2F is the cost of the optimal k-means clustering. k-means is therefore a constrained rank k-approximation problem.\nWe first restate the separation condition.\nDefinition 4.1 (Spectral Separation). Let A be a set of points and let {C1, . . . Ck} be a clustering of A with centers {c1, . . . ck}. Denote by C an n \u00d7 d matrix such that Ci = argmin\nj\u2208{1,...,k} ||Ai \u2212 cj ||2.\nThen {C1, . . . Ck} is \u03b3 spectrally separated, if for any pair of centers ci and cj the following condition holds:\n||ci \u2212 cj || \u2265 \u03b3 \u00b7 ( 1 \u221a\n|Ci| +\n1 \u221a\n|Cj|\n)\n||A\u2212 C||2.\nThe following crucial lemma relates spectral separation and distribution stability.\nLemma 4.1. For a point set A, let C = {C1, . . . , Ck} be an optimal clustering with centers S = {c1, . . . , ck} associated clustering matrix X that is at least \u03b3 \u00b7 \u221a k spectrally separated, where \u03b3 > 3. For \u03b5 > 0, let Am be the best rank m = k/\u03b5 approximation of A. Then there exists a clustering K = {C \u20321, . . . C \u20322} and a set of centers Sk, such that\n1. the cost of clustering Am with centers Sk via the assignment of K is less than ||Am \u2212 XXTAm||2F and\n2. (K,Sk) is \u2126((\u03b3 \u2212 3)2 \u00b7 \u03b5)-distribution stable.\nWe note that this lemma would also allow us to use the PTAS of Awasthi et al. [10]. Before giving the proof, we outline how Lemma 4.1 helps us prove Theorem 1.3. We first notice that if the rank of A is of order k, then elementary bounds on matrix norm show that spectral separability implies distribution stability. We aim to combine this observation with the following theorem due\nto Cohen et al. [31]. Informally, it states that for every rank k approximation, (an in particular for every constrained rank k approximation such as k-means clustering), projecting to the best rank k/\u03b5 subspace is cost-preserving.\nTheorem 4.2 (Theorem 7 of [31]). For any A \u2208 Rn\u00d7d, let A\u2032 be the rank \u2308k/\u03b5\u2309-approximation of A. Then there exists some positive number c such that for any rank k orthogonal projection P ,\n||A\u2212 PA||2F \u2264 ||A\u2032 \u2212 PA\u2032||2F + c \u2264 (1 + \u03b5)||A \u2212 PA||2F .\nThe combination of the low rank case and this theorem is not trivial as points may be closer to a wrong center after projecting, see also Figure 3. Lemma 4.1 determines the existence of a clustering whose cost for the projected points Am is at most the cost of C\n\u2217. Moreover, this clustering has constant distribution stability as well which, combined with the results from Section B, allows us to use Local Search. Given that we can find a clustering with cost at most (1+\u03b5) \u00b7 ||Am\u2212XXTAm||2F , Theorem 4.2 implies that we will have a (1 + \u03b5)2-approximation overall.\nTo prove the lemma, we will require the following steps:\n\u2022 A lower bound on the distance of the projected centers ||ciVmV Tm \u2212 cjVmV Tm || \u2248 ||ci \u2212 cj ||.\n\u2022 Find a clustering K with centers S\u2217m = {c1VmV Tm , . . . , c\u2217kVmV Tm} of Am with cost less than ||Am \u2212XXTAm||2F .\n\u2022 Show that in a well-defined sense, K and C\u2217 agree on a large fraction of points.\n\u2022 For any point x \u2208 Ki, show that the distance of x to any center not associated with Ki is large.\nWe first require a technical statement.\nLemma 4.3. For a point set A, let C = {C1, . . . Ck} be a clustering with associated clustering matrix X and let A\u2032 and A\u2032\u2032 be optimal low rank approximations where without loss of generality k \u2264 rank(A\u2032) < rank(A\u2032\u2032). Then for each cluster Ci\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 |Ci| \u2211\nj\u2208Ci\n( A\u2032\u2032j \u2212A\u2032j )\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n2\n\u2264 \u221a k\n|Ci| \u00b7 ||A\u2212XXTA||2.\nProof. By Fact 1 |Ci| \u00b7 || 1|Ci| \u2211 j\u2208Ci(A \u2032\u2032 i \u2212A\u2032i)||22 is, for a set of point indexes Ci, the cost of moving the centroid of the cluster computed on A\u2032\u2032 to the centroid of the cluster computed on A\u2032. For a clustering matrix X, ||XXTA\u2032\u2212XXTA\u2032||2F is the sum of squared distances of moving the centroids computed on the point set A\u2032\u2032 to the centroids computed on A\u2032. We then have\n|Ci|\u00b7\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 |Ci| \u2211 j\u2208Ci (A\u2032\u2032j \u2212A\u2032j) \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 2\n2\n\u2264 ||XXTA\u2032\u2032\u2212XXTA\u2032||2F \u2264 ||X||2F \u00b7||A\u2032\u2032\u2212A\u2032||22 \u2264 k\u00b7\u03c32k+1 \u2264 k\u00b7||A\u2212XXTA||22.\nProof of Lemma 4.1. For any point p associated with some row of A, let pm = pVmV T m be the corresponding row in Am. Similarly, for some cluster Ci, denote the center in A by ci and the center in Am by c m i . Extend these notion analogously for projections p\nk and cki to the span of the best rank k approximation Ak.\nWe have for any m \u2265 k i 6= j\n||cmi \u2212 cmj || \u2265 ||ci \u2212 cj || \u2212 ||ci \u2212 cmi || \u2212 ||cj \u2212 cmj ||\n\u2265 \u03b3 \u00b7 (\n1\u221a Ci + 1 \u221a |Cj |\n)\n\u221a k||A\u2212XXTA||2\n\u2212 1\u221a Ci\n\u221a k||A\u2212XXTA||2 \u2212 1 \u221a |Cj | \u221a k||A\u2212XXTA||2\n= (\u03b3 \u2212 1) \u00b7 (\n1\u221a Ci + 1 \u221a |Cj|\n)\n\u221a k||A\u2212XXTA||2, (1)\nwhere the second inequality follows from Lemma 4.3.\nIn the following, let \u2206i = \u221a k\u221a |Ci|\n||A \u2212 XXTA||2. We will now construct our target clustering K. Note that we require this clustering (and its properties) only for the analysis. We distinguish between the following three cases.\nCase 1: p \u2208 Ci and cmi = argmin j\u2208{1,...,k} ||pm \u2212 cj||:\nThese points remain assigned to cmi . The distance between pm and a different center c m j is at least 12 ||cmi \u2212 cmj || \u2265 \u03b3\u22121 2 \u03b5(\u2206i +\u2206j) due to Equation 1.\nCase 2: p \u2208 Ci, cmi 6= argmin j\u2208{1,...,k} ||pm \u2212 cj ||, and cki 6= argmin j\u2208{1,...,k} ||pk \u2212 ckj ||:\nThese points will get reassigned to their closest center.\nThe distance between pm and a different center c m j is at least 1 2 ||cmi \u2212 cmj || \u2265 \u03b3\u22121 2 \u03b5(\u2206i +\u2206j) due to Equation 1.\nCase 3: p \u2208 Ci, cmi 6= argmin j\u2208{1,...,k} ||pm \u2212 cmj ||, and cki = argmin j\u2208{1,...,k} ||pk \u2212 ckj ||:\nWe assign pm to cmi at the cost of a slightly weaker movement bound on the distance between pm and cmj . Due to orthogonality of V , we have for m > k, (Vm\u2212Vk)TVk = V Tk (Vm\u2212Vk) = 0. Hence VmV T mVk = VmV T k Vk +Vm(Vm\u2212Vk)TVk = VkV Tk Vk +(Vm \u2212Vk)V Tk Vk = VkV Tk Vk = Vk. Then pk = pVkV T k = pVmV T mVkV T k = pmVkV T k . Further, ||pk \u2212 ckj || \u2265 12 ||ckj \u2212 cki || \u2265 \u03b3\u22121 2 (\u2206i + \u2206j) due to Equation 1. Then the distance between pm and a different center c m j\n||pm \u2212 cmj || \u2265 ||pm \u2212 ckj || \u2212 ||cmj \u2212 ckj || = \u221a ||pm \u2212 pk||2 + ||pk \u2212 ckj ||2 \u2212 ||cmj \u2212 ckj ||\n\u2265 ||pk \u2212 ckj || \u2212\u2206j \u2265 \u03b3 \u2212 3 2 (\u2206i +\u2206j),\nwhere the equality follows from orthogonality and the second to last inequality follows from Lemma 4.3.\nNow, given the centers {cm1 , . . . cmk }, we obtain a center matrix MK where the ith row of MK is the center according to the assignment of above. Since both clusterings use the same centers but K improves locally on the assignments, we have ||Am \u2212MK ||2F \u2264 ||Am \u2212 XXTAm||2F , which\nproves the first statement of the lemma. Additionally, due to the fact that Am\u2212XXTAm has rank m = k/\u03b5, we have\n||Am \u2212MK ||2F \u2264 ||Am \u2212XXTAm||2F \u2264 m \u00b7 ||Am \u2212XXTAm||22 \u2264 k/\u03b5 \u00b7 ||A\u2212XXTA||2F (2)\nTo ensure stability, we will show that for each element of K there exists an element of C, such that both clusters agree on a large fraction of points. This can be proven by using techniques from Awasthi and Sheffet [13] (Theorem 3.1) and Kumar and Kannan [60] (Theorem 5.4), which we repeat for completeness.\nLemma 4.4. Let K = {C \u20321, . . . C \u2032k} and C = {C1, . . . Ck} be defined as above. Then there exists a bijection b : C \u2192 K such that for any i \u2208 {i, . . . , k}\n(\n1\u2212 32 (\u03b3 \u2212 1)2\n) |Ci| \u2264 b(|Ci|) \u2264 ( 1 + 32 (\u03b3 \u2212 1)2 ) |Ci|.\nProof. Denote by Ti\u2192j the set of points from Ci such that ||cki \u2212 pk|| > ||ckj \u2212 pk||. We first note that ||Ak \u2212XXTA||2F \u2264 2k \u00b7 ||Ak \u2212 XXTA||22 \u2264 2k \u00b7 ( ||A\u2212Ak||2 + ||A\u2212XXTA||2 )2 \u2264 8k \u00b7 ||A \u2212 XXTA||22 \u2264 8 \u00b7 |Ci| \u00b7 \u22062i for any i \u2208 {1, . . . , k}. The distance ||pk \u2212 cki || \u2265 12 ||cki \u2212 ckj || \u2265 \u03b3\u22121 2 \u00b7 ( 1\u221a Ci + 1\u221a |Cj | )\u221a k||A \u2212 XXTA||22. Assigning these points to cki , we can bound the total number of points added to and subtracted from cluster Cj by observing\n\u22062j \u2211 i 6=j |Ti\u2192j | \u2264 \u2211 i 6=j |Ti\u2192j | \u00b7\n(\n\u03b3 \u2212 1 2\n)2\n\u00b7 (\u2206i +\u2206j)2 \u2264 ||Ak \u2212XXTA||2F \u2264 8 \u00b7 |Cj | \u00b7\u22062j\n\u22062j \u2211 i 6=j |Tj\u2192i| \u2264 \u2211 j 6=i |Tj\u2192i| \u00b7\n(\n\u03b3 \u2212 1 2\n)2\n\u00b7 (\u2206i +\u2206j)2 \u2264 ||Ak \u2212XXTA||2F \u2264 8 \u00b7 |Cj | \u00b7\u22062j .\nTherefore, the cluster sizes are up to some multiplicative factor of (\n1\u00b1 32 (\u03b3\u22121)2\n)\nidentical.\nWe now have for each point pm \u2208 C \u2032i a minimum cost of\n||pm \u2212 cmj ||2 \u2265 ( \u03b3 \u2212 3 2 \u00b7 ( 1 \u221a\n|Ci| +\n1 \u221a\n|Cj |\n)\n\u00b7 \u221a k \u00b7 ||A\u2212XXTA||2\n)2\n\u2265\n\n  \u03b3 \u2212 3 2 \u00b7\n\n \n\u221a \u221a \u221a \u221a 1 (\n1 + 32 (\u03b3\u22121)2\n) \u00b7 |C \u2032i| +\n\u221a \u221a \u221a \u221a 1 (\n1 + 32 (\u03b3\u22121)2\n)\n\u00b7 |C \u2032j |\n\n  \u00b7 \u221a k \u00b7 ||A\u2212XXTA||2\n\n \n2\n\u2265 4 \u00b7 (\u03b3 \u2212 3) 2 81 \u00b7 \u03b5 ||Am \u2212MK || 2 F\n|C \u2032j |\nwhere the first inequality holds due to Case 3, the second inequality holds due to Lemma 4.4 and the last inequality follows from \u03b3 > 3 and Equation 2. This ensures that the distribution stability condition is satisfied.\n4.0.1 Proof of Theorem 1.3\nProof. Given the optimal clustering C\u2217 of A with clustering matrix X, Lemma 4.1 guarantees the existence of a clustering K with center matrix MK such that ||Am\u2212MK ||2F \u2264 ||Am\u2212XXTAm|| and that C has constant distribution stability. If ||Am\u2212MK ||2F is not a constant factor approximation, we are already done, as Local Search is guaranteed to find a constant factor approximation. Otherwise due to Corollary 1 (Section B in the appendix), there exists a discretization (Am, F, || \u00b7 ||2, k) of (Am,R\nd, || \u00b7 ||2, k) such that the clustering C of the first instance has at most (1 + \u03b5) times the cost of C in the second instance and such that C has constant distribution stability. By Theorem 1.1, Local Search with appropriate (but constant) neighborhood size will find a clustering C \u2032 with cost at most (1 + \u03b5) times the cost of K in (Am, F, || \u00b7 ||2, k). Let Y be the clustering matrix of C \u2032. We then have ||Am \u2212 Y Y TAm||2F + ||A \u2212 Am||2F \u2264 (1 + \u03b5)2||Am \u2212MK ||2F + ||A \u2212 Am||2F \u2264 (1+ \u03b5)2||Am \u2212XXTAm||2F + ||A\u2212Am||2F \u2264 (1+ \u03b5)3||A\u2212XXTA||2F due to Theorem 4.2. Rescaling \u03b5 completes the proof.\nRemark. Any (1 + \u03b5)-approximation will not in general agree with a target clustering. To see this consider two clusters: (1) with mean on the origin and (2) with mean \u03b4 on the the first axis and 0 on all other coordinates. We generate points via a multivariate Gaussian distribution with an identity covariance matrix centered on the mean of each cluster. If we generate enough points, the instance will have constant spectral separability. However, if \u03b4 is small and the dimension large enough, an optimal 1-clustering will approximate the k-means objective."}, {"heading": "5 Acknowledgments", "text": "The authors thank their dedicated advisor for this project: Claire Mathieu. Without her, this collaboration would not have been possible.\nAppendix\nA (\u03b2, \u03b4)-Stability\nLemma 3.5. Let C\u2217i be a cheap cluster. For any \u03b50, we have |IR\u03b50i | > (1\u2212 \u03b53/\u03b50)|C\u2217i |.\nProof. Observe that each client that is not in IR\u03b50i is at distance at least \u03b50\u03b2OPT/|C\u2217i | from c\u2217i . Since i is cheap, the total cost of the clients in C\u2217i is at most \u03b5\n3\u03b2OPT and in particular, the total cost of the clients that are not in IR\u03b50i does not exceed \u03b5\n3\u03b2OPT. Therefore, the total number of such clients is at most \u03b53|C\u2217i |/\u03b50.\nLemma 3.6. Let \u03b50 < 1/3. If C \u2217 i 6= C\u2217j are cheap clusters, then IR\u03b50i \u2229 IR\u03b50j = \u2205.\nProof. Assume towards contradiction that this is not true. Consider a client x \u2208 IR\u03b50i \u2229 IR\u03b50j . Without loss of generality assume |C\u2217i | \u2265 |C\u2217j |. By the triangular inequality we have cost(c\u2217j , c\u2217i ) \u2264 cost(c\u2217j , x) + cost(x, c \u2217 i ) \u2264 2\u03b50\u03b2OPT/|C\u2217j |. Since \u03b4 < 1/2, there exists a client c\u2032 \u2208 IR\u03b50i \u2229\u2206i. Thus, we have cost(c\u2032, c\u2217j ) \u2264 3\u03b50\u03b2OPT/|C\u2217j |. This is a contradiction to the assumption that the instance is (\u03b2, \u03b4)-distribution stable with respect to (C\u2217, S\u2217) and c\u2032 being in \u2206i for any \u03b50 < 1/3.\nLemma 3.9. There exists a set Z3 \u2286 C\u2217 \u2212 Z2 of size at most (\u03b5\u22123 + 40\u03b5\u22121)\u03b2\u22121 such that for any cluster C\u2217j \u2208 C\u2217 \u2212 Z3, the total number of clients x \u2208 \u22c3\ni 6=j \u2206i, that are served by L(j) in L is at most \u03b5|IR\u03b52i |.\nProof. Consider a cheap cluster C\u2217j \u2208 C\u2217\u2212Z2 such that the total number of clients x \u2208 \u2206i, for j 6= i, that are served by L(j) in L is greater than \u03b5|IR\u03b52j |. By the triangular inequality and the definition of (\u03b2, \u03b4)-stability, the total cost for each x \u2208 \u2206i, j 6= i served by L(j) is at least (1\u2212 \u03b5)\u03b2OPT/|C\u2217j |. Since there are at least \u03b5|IR\u03b52j | such clients, their total cost is at least \u03b5|IR\u03b5 2\nj |(1\u2212 \u03b5)\u03b2OPT/|C\u2217j |. By Lemma 3.5, this is at least\n\u03b5(1\u2212 \u03b5)2|C\u2217j |(1\u2212 \u03b5)\u03b2 OPT |C\u2217j | \u2265 \u03b5\u03b2OPT 8 ,\nsince \u03b5 \u2264 1/2. Recall that by [9], L is a 5-approximation and so there exist at most 40\u03b5\u22121\u03b2\u22121 such clusters. By Lemma 3.7, the total number of not cheap clusters is at most \u03b5\u22123\u03b2\u22121 and so, there exists a set Z3 of size at most (\u03b5 \u22123 + 40\u03b5\u22121)\u03b2\u22121 satisfying the lemma.\nLemma 3.11. For any \u03b4 < 1/2, we have for any C\u2217i ,\n\u2211\nx\u2208C\u2217i \u2212\u2206i Reassign(x) \u2264\n\u2211\nx\u2208C\u2217i \u2212\u2206i gx +\n\u03b4\n(1\u2212 \u03b4) \u2211\nx\u2208C\u2217i\n(lx + gx).\nProof. Consider a client x \u2208 C\u2217i \u2212 \u2206i. Let \u2113\u2032 be the center that serves at least one client of \u2206i that is the closest to c\u2217i . Since \u03b4 < 1, \u2113\n\u2032 is well defined. By the triangular inequality we have that Reassign(x) \u2264 cost(x, \u2113\u2032) \u2264 cost(x, c\u2217i ) + cost(c\u2217i , \u2113\u2032) = gx + cost(c\u2217i , \u2113\u2032). Then,\n\u2211\nx\u2208C\u2217i \u2212\u2206i Reassign(x) \u2264\n\u2211\nx\u2208C\u2217i \u2212\u2206i gx + |C\u2217i \u2212\u2206i| \u00b7 cost(c\u2217i , \u2113\u2032).\nNow, since \u2113\u2032 is the center that serves at least one client of \u2206i that is the closest to c\u2217i we have that for any x \u2208 \u2206i, by the triangular inequality cost(c\u2217i , \u2113\u2032) \u2264 cost(c\u2217i , x) + cost(x, \u2113\u2032) \u2264 gx + lx. Therefore,\ncost(c\u2217i , \u2113 \u2032) \u2264 1|\u2206i| \u2211\nx\u2208\u2206i (gx + lx).\nCombining, we obtain\n\u2211\nx\u2208C\u2217i \u2212\u2206i Reassign(x) \u2264\n\u2211\nx\u2208C\u2217i \u2212\u2206i gx +\n|C\u2217i \u2212\u2206i| |\u2206i| \u2211\nx\u2208\u2206i (gx + lx)\n= \u2211\nx\u2208C\u2217i \u2212\u2206i gx +\n\u03b4\n(1\u2212 \u03b4) \u2211\nx\u2208\u2206i (gx + lx),\nby definition of (\u03b2, \u03b4)-stability.\nLemma 3.12. Let C\u2217i be a cluster in C \u2217 \u2212 Z\u2217. Define the solution Mi = L \u2212 {L(i)} \u222a {c\u2217i } and denote by mic the cost of client c in solution Mi. Then \u2211\nx\u2208A mix \u2264\n\u2211\nx/\u2208A(L(i))\u222aC\u0303\u2217i\nlc + \u2211\nx\u2208C\u0303\u2217i\ngx + \u2211\nx\u2208\u2206\u0303i\nReassign(x) + \u2211\nx\u2208A(L(i))\u2212 (C\u0303\u2217i \u222a\u2206\u0303i)\nlx + \u03b5 (1\u2212 \u03b5) ( \u2211\nx\u2208C\u0303\u2217i\ngx + lx).\nProof. For any client x /\u2208 A(L(i))\u222a C\u0303\u2217i , the center that serves it in L belongs to Mi. Thus its cost is at most lx. Moreover, observe that any client x \u2208 C\u0303\u2217i can now be served by c\u2217i , and so its cost is at most gx. For each client x \u2208 \u2206\u0303i, since all the centers of L except for L(i) are in Mi, we bound its cost by Reassign(x).\nNow, we bound the cost of a client x \u2208 A(L(i)) \u2212 (C\u0303\u2217i \u222a \u2206\u0303i). The closest center in Mi for a client x \u2208 A(L(i)) \u2212 (C\u0303\u2217i \u222a \u2206\u0303i) is not farther than c\u2217i . By the triangular inequality we have that the cost of such a client x is at most cost(x, c\u2217i ) \u2264 cost(x,L(i))+ cost(L(i), c\u2217i ) = lx+cost(L(i), c\u2217i ), and so\n\u2211\nx\u2208A(L(i))\u2212 (C\u0303\u2217i \u222a\u2206\u0303i)\nmix \u2264 |A(L(i)) \u2212 (C\u0303\u2217i \u222a \u2206\u0303i)|cost(L(i), c\u2217i ) + \u2211\nx\u2208A(L(i))\u2212 (C\u0303\u2217i \u222a\u2206\u0303i)\nlx. (3)\nNow, observe that, for any client x \u2208 |A(L(i)) \u2229 C\u0303\u2217i |, by the triangular inequality we have that cost(L(i), c\u2217i ) \u2264 cost(x,L(i)) + cost(x, c\u2217i ) = lx + gx. Therefore,\ncost(L(i), c\u2217i ) \u2264 1 |A(L(i)) \u2229 C\u0303\u2217i | \u2211\nx\u2208A(L(i))\u2229C\u0303\u2217i\n(lx + gx). (4)\nCombining Equations 3 and 4, we have that\n\u2211\nx\u2208A(L(i))\u2212 (C\u0303\u2217i \u222a\u2206\u0303i)\nmic \u2264 \u2211\nx\u2208A(L(i))\u2212 (C\u0303\u2217i \u222a\u2206\u0303i)\nlx + |A(L(i)) \u2212 C\u0303\u2217i | |A(L(i)) \u2229 C\u0303\u2217i |\n\u2211\nx\u2208A(L(i))\u2229C\u0303\u2217i\n(lx + gx). (5)\nWe now remark that since C\u0303\u2217i is not in Z \u2217, we have by Lemmas 3.8 and 3.9, |A(L(i))\u2212C\u0303\u2217i | \u2264 \u03b5|IR\u03b5 2 i | and (1\u2212 \u03b5)|IR\u03b52i | \u2264 |A(L(i)) \u2229 C\u0303\u2217i |. Thus, combining with Equation 5 yields the lemma.\nLemma 3.13. There exists a constant \u03b7 such that\n\u2212\u03b5 \u00b7 cost(L) + \u2211\nx\u2208A\u0302\u2212C(Z\u2217)\nlx \u2264 \u2211\nx\u2208A\u0302\u2212C(Z\u2217)\ngx + (\u03b7(\u03b5+ \u03b4 1\u2212 \u03b4 ))(cost(L) + cost(C \u2217)).\nProof. We consider a cluster C\u2217i in C \u2217 \u2212 Z\u2217. and the solution Mi = L \u2212 {L(i)} \u222a {c\u2217i }. Observe that Mi and L only differs by L(i) and c\u2217i . Therefore, by local optimality and Lemma 3.12, we have that (1\u2212 \u03b5n)cost(Li) \u2264 cost(Mi). Then, (1\u2212 \u03b5 n )cost(Li) \u2264 \u2211\nx/\u2208A(L(i))\u222aC\u0303\u2217i\nlx + \u2211\nx\u2208C\u0303\u2217i\ngx + \u2211\nx\u2208A(L(i))\u2212 (C\u0303\u2217i \u222a\u2206\u0303i)\nlx + \u2211\nx\u2208\u2206\u0303i\nReassign(x) + \u03b5 (1\u2212 \u03b5) \u2211\nx\u2208C\u2217i\n(gx + lx)\nand so, simplifying \u03b5\nn cost(Li) +\n\u2211\nx\u2208C\u0303\u2217 i\nlx + \u2211\nx\u2208\u2206\u0303i\nlx \u2264 \u2211\nx\u2208C\u0303\u2217 i\ngx + \u2211\nx\u2208\u2206\u0303i\nReassign(x) + \u03b5 (1\u2212 \u03b5) \u2211\nx\u2208C\u2217i\n(gx + lx)\nWe now apply this analysis to each cluster C\u0303\u2217i \u2208 C\u2217\u2212Z\u2217. Summing over all clusters C\u0303\u2217i \u2208 C\u2217\u2212Z\u2217, we obtain,\n\u2212k \u03b5 n cost(L)+\n|C\u2217\u2212Z\u2217| \u2211\ni=1\n\n\n\u2211\nx\u2208C\u0303\u2217i\nlx + \u2211\nx\u2208\u2206\u0303i\nlx\n\n \u2264\n|C\u2217\u2212Z\u2217| \u2211\ni=1\n\n\n\u2211\nx\u2208C\u0303\u2217i\ngx + \u2211\nx\u2208\u2206\u0303i\nReassign(c)\n\n + \u03b5 (1\u2212 \u03b5) (cost(L) + cost(C \u2217))\nBy Lemma 3.11 and the definition of C\u0303\u2217i ,\n\u2212k \u03b5 n cost(L) +\n|C\u2217\u2212Z\u2217| \u2211\ni=1\n\u2211\nx\u2208C\u2217i \u2229A\u0302\nlx \u2264 |C\u2217\u2212Z\u2217| \u2211\ni=1\n\u2211\nx\u2208C\u2217i \u2229A\u0302\ngx + ( \u03b5 (1\u2212 \u03b5) + \u03b4 (1\u2212 \u03b4) )(cost(L) + cost(C \u2217)).\nTherefore, \u2212\u03b5cost(L) + \u2211\nc\u2208A\u0302\u2212C(Z\u2217)\nlx \u2264 \u2211\nx\u2208A\u0302\u2212C(Z\u2217)\ngx + (\u03b7(\u03b5+ \u03b4 1\u2212 \u03b4 ))(cost(L) + cost(C \u2217)), for some\nconstant \u03b7 and any \u03b4 < 1/2."}, {"heading": "B Euclidean Distribution Stability", "text": "In this section we show how to reduce the Euclidean problem to the discrete version. Our analysis is focused on the k-means problem, however we note that the discretization works for all values of cost = distp, where the dependency on p grows exponentially. For constant p, we obtain polynomial sized candidate solution sets in polynomial time. For k-means itself, we could alternatively combine Matousek\u2019s approximate centroid set [65] with the Johnson Lindenstrauss lemma and avoid the following construction; however this would only work for optimal distribution stable clusterings and the proof Theorem 1.3 requires it to hold for non-optimal clusterings as well.\nFirst, we describe a discretization procedure. It will be important to us that the candidate solution preserves (1) the cost of any given set of centers and (2) distribution stability.\nFor a set of points P , a set of points N\u03b5 is an \u03b5-net of P if for every point x \u2208 P there exists some point y \u2208 N\u03b5 with ||x\u2212 y|| \u2264 \u03b5. It is well known that for unit Euclidean ball of dimension d, there exists an \u03b5-net of cardinality (1 + 2/\u03b5)d, see for instance Pisier [70]. We will use such \u03b5-nets in our discretization.\nLemma B.1. Let A be a set of n points in d-dimensional Euclidean space and let \u03b2, \u03b5 > 0 with min(\u03b2, \u03b5, \u221a 7\u2212 4 \u221a 2\u22121) > 2\u03b7 > 0 be constants. Suppose there exists a clustering C = {C1, . . . , Ck} with centers S = {c1, . . . ck} such that\n1. cost(C,S) = \u2211k\ni=1\n\u2211\nx\u2208Ci ||x \u2212 ci||2 is a constant approximation to the optimum clustering and\n2. C is \u03b2-distribution stable.\nThen there exists a discretization D of the solution space such that there exists a subset S\u2032 = {c\u20321, . . . c\u2032k} \u2282 D of size k with\n1. \u2211k\ni=1\n\u2211 x\u2208Ci ||x\u2212 c\u2032i||2 \u2264 (1 + \u03b5) \u00b7 cost(C,S) and\n2. C with centers S\u2032 is \u03b2/2-distribution stable.\nThe discretization consists of O(n \u00b7 log n \u00b7 \u03b7d+2) many points.\nProof. Let OPT being the cost of an optimal k-means clustering. Define an exponential sequence to the base of (1 + \u03b7) starting at (\u03b7 \u00b7 OPTn ) and ending at (n \u00b7 OPT). It is easy to see that the sequence contains t = log1+\u03b7(n\n2/\u03b7) \u2208 O(\u03b7\u22121(log n + log(1/\u03b7)) many elements. For each point p \u2208 A, define B(p, \u2113i) as the d-dimensional ball centered at p with radius \u221a\n(1 + \u03b7)i \u00b7 \u03b7 \u00b7 OPTn . We cover the ball B(p, \u2113i) with an \u03b7/8 \u00b7 \u2113i net N\u03b5/8(p, \u2113i). As the set of candidate centers, we let D = \u222ap\u2208A \u222ati=0 N\u03b7/8(p, \u2113i). Clearly, |D| \u2208 O(n \u00b7 log n \u00b7 (1 + 16/\u03b7)d+2).\nNow for each ci \u2208 S, set c\u2032i = argmin q\u2208D ||q \u2212 ci||. We will show that S\u2032 = {c\u20321, . . . c\u2032k} satisfies the two conditions of the lemma.\nFor (1), we first consider the points p with ||p \u2212 ci|| \u2264 \u221a\n\u03b5 \u00b7 OPTn . Then there exists a c\u2032i such that ||p \u2212 c\u2032i||2 \u2264 \u03b72/64 \u00b7 \u03b5OPTn and summing up over all such points, we have a total contribution to the objective value of at most \u03b72 \u00b7 \u03b5/64 \u00b7OPT \u2264 \u03b73/64 \u00b7OPT.\nNow consider the remaining points. Since the cost(C,S) is a constant approximation, the\ncenter ci of each point p satisfies \u221a (1 + \u03b7)i \u00b7 \u03b7 \u00b7 OPTn \u2264 ||ci\u2212p|| \u2264 \u221a\n(1 + \u03b7)i+1 \u00b7 \u03b7 \u00b7 OPTn for some i \u2208 {0, . . . t}. Then there exists some point q \u2208 N\u03b7/8(p, \u2113i+1) with ||q\u2212ci|| \u2264 \u03b7/\u00b7 \u221a\n(1 + \u03b7)i+1 \u00b7 \u03b7 \u00b7 OPTn \u2264 \u03b7/8 \u00b7\u221a1 + \u03b7||p\u2212ci|| \u2264 \u03b7/4||p\u2212ci||. We then have ||p\u2212c\u2032i||2 \u2264 (1+\u03b7/4)2 ||p\u2212ci||2. Summing up over both cases, we have a total cost of at most \u03b73/64\u00b7OPT+(1+\u03b7/4)2 \u00b7cost(C,S\u2032) \u2264 (1+\u03b7)\u00b7cost(C,S\u2032) \u2264 (1 + \u03b5) \u00b7 cost(C,S\u2032).\nTo show (2), let us consider some point p /\u2208 Cj with ||p\u2212cj ||2 > \u03b2 \u00b7OPT|Cj | . Since \u03b2 \u00b7 OPT |Cj | \u2265 2\u03b7 \u00b7 OPT n ,\nthere exists a point q and an i \u2208 {0, . . . t} such that \u221a\n\u03b2 1+\u03b7 \u00b7 OPTn \u2264 ||ci\u2212q|| \u2264\n\u221a\n\u03b2 \u00b7 (1 + \u03b7) \u00b7 \u03b5 \u00b7 OPTn .\nThen ||c\u2032j \u2212 cj|| \u2264 \u03b7 \u00b7 (1+\u03b7) \u221a\n\u03b2 \u00b7 (1 + \u03b7) \u00b7 OPTn . Similarly to above, the point c\u2032j satisfies ||p\u2212 c\u2032j||2 \u2265 (||p \u2212 cj|| \u2212 ||cj \u2212 c\u2032j ||)2 \u2265 ( \u221a \u03b2 \u00b7 OPT|Cj | \u2212 \u221a \u03b2 \u00b7 \u03b7(1 + \u03b7) \u00b7 OPTn )2 \u2265 (1 \u2212 \u03b7(1 + \u03b7))\u03b2 \u00b7 OPT|Cj | > \u03b2 \u00b7 OPT |Cj | where the last inequality holds for any \u03b7 < 12 \u00b7 ( \u221a 7\u2212 4 \u221a 2\u2212 1).\nTo reduce the dependency on the dimension, we combine this statement with the seminal theorem originally due to Johnson and Lindenstrauss [54].\nLemma B.2 (Johnson-Lindenstrauss lemma). For any set of n points N in d-dimensional Euclidean space and any 0 < \u03b5 < 1/2, there exists a distribution F over linear maps f : \u2113d2 \u2192 \u2113m2 with m \u2208 O(\u03b5\u22122 log n) such that\nPf\u223cF [\u2200x, y \u2208 N, (1\u2212 \u03b5)||x\u2212 y|| \u2264 ||f(x)\u2212 f(y)|| \u2264 (1 + \u03b5)||x\u2212 y||] \u2265 2\n3 .\nIt is easy to see that Johnson-Lindenstrauss type embeddings preserve the Euclidean k-means cost of any clustering, as the cost of any clustering can be written in terms of pairwise distances (see also Fact 1 in Section 4). Since the distribution over linear maps F can be chosen obliviously with respect to the points, this extends to distribution stability of a set of k candidate centers as well.\nCombining Lemmas B.2 and B.1 gives us the following corollary.\nCorollary 1. Let A be a set of points in d-dimensional Euclidean space with a clustering C = {C1, . . . Ck} and centers S = {c1, . . . ck} such that C is \u03b2-perturbation stable. Then there exists a (A,F, || \u00b7 ||2, k)-clustering instance with clients A, npoly(\u03b5\u22121) centers F and a subset S\u2032 \u2282 F \u222a A of k centers such that C and S\u2032 is O(\u03b2) stable and the cost of clustering A with S\u2032 is at most (1 + \u03b5) times the cost of clustering A with S.\nRemark. This procedure can be adapted to work for general powers of cost functions. For Lemma B.1, we simply rescale \u03b7. The Johnson-Lindenstrauss lemma can also be applied in these settings, at a slightly worse target dimension of O((p + 1)2 log((p + 1)/\u03b5)\u03b5\u22123 log n), see Kerber and Raghvendra [57]."}, {"heading": "C \u03b1-Perturbation Resilience", "text": "Throughout the rest of this section, we let L denote the centers that form a local optimum. We also define C\u2217 to be the optimal centers induced by the clustering {C\u22171 , . . . , C\u2217k}.\nWe first show how Theorem 3.14 allows us to prove Theorem 1.2.\nProof of Theorem 1.2. Given an instance (A,F, cost, k), we define the following instance I \u2032 = (A,F, cost\u2032, k), where cost(a, b) = dist(a, b) for some distance function defined over A \u222a F . For each client c \u2208 NC\u2217(C\u0303\u2217) \u222a NL(L\u0303), let \u2113i be the center of L that serves it in L, for any point p 6= \u2113i, we define cost\u2032(c, p) = \u03b1cost(c, p) and cost\u2032(c, \u2113i) = cost(c, \u2113i). For the other clients we set cost\u2032 = cost. Observe that by local optimality, the clustering induced by L is {C\u22171 , . . . , C\u2217k} if and only if L = C\u2217. Therefore, the cost of C\u2217 in instance I \u2032 is equal to\n\u03b1 \u2211\nc\u2208NC\u2217(C\u0303\u2217)\u222aNL(L\u0303)\ngc + \u2211\nc/\u2208NC\u2217 (C\u0303\u2217)\u222aNL(L\u0303)\ngc.\nOn the other hand, the cost of L in I \u2032 is the same than in I, by Theorem 3.14 \u2211\nc\u2208NC\u2217(C\u0303\u2217)\u222aNL(L\u0303)\nlc \u2264 \u2211\nc\u2208NC\u2217 (C\u0303\u2217)\u222aNL(L\u0303)\nlc \u2264 (3 + 2(\u03b1\u2212 3)\n2 )\n\u2211\nc\u2208NC\u2217 (C\u0303\u2217)\u222aNL(L\u0303)\ngc\nand by definition \u2211\nc/\u2208NC\u2217 (C\u0303\u2217)\u222aNL(L\u0303)\nlc \u2264 \u2211\nc/\u2208NC\u2217 (C\u0303\u2217)\u222aNL(L\u0303)\ngc.\nHence the cost of L in I \u2032 is at most\n\u03b1 \u2211\nc\u2208NC\u2217(C\u0303\u2217)\u222aNL(L\u0303)\ngc + \u2211\nc/\u2208NC\u2217 (C\u0303\u2217)\u222aNL(L\u0303)\ngc.\nBy definition of \u03b1-perturbation-resilience, we have that the clustering {C\u22171 , . . . , C\u2217k} is the unique optimal solution in I \u2032. Therefore L = C\u2217 and the Theorem follows.\nWe now turn to the proof of Theorem 3.14 We first introduce some definitions, following the terminology of [9, 46].\nConsider the following bipartite graph \u0393 = (L\u0303 \u222a C\u0303\u2217, E) where E is defined as follows. For any center f \u2208 C\u0303\u2217, we have (f, \u2113) \u2208 E where \u2113 is the center of L\u0303 that is the closest to f . Denote N\u0393(\u2113) the neighbors of the point corresponding to center \u2113 in \u0393.\nFor each edge (f, \u2113) \u2208 E , for any client c \u2208 NC\u2217(f)\u2212NL(\u2113), we define Reassignc as the cost of reassigning client c to \u2113. We derive the following lemma.\nLemma C.1. For any client c, Reassignc \u2264 lc + 2gc.\nProof. By definition we have Reassignc = dist(c, \u2113). By the triangular inequality dist(c, \u2113) \u2264 dist(c, f)+dist(f, \u2113). Since f serves c in C\u2217 we have dist(c, f) = gc, hence dist(c, \u2113) \u2264 gc+dist(f, \u2113). We now bound dist(f, \u2113). Consider the center \u2113\u2032 that serves c in solution L. By the triangular inequality we have dist(f, \u2113\u2032) \u2264 dist(f, c) + dist(c, \u2113\u2032) = gc + lc. Finally, since \u2113 is the closest center of f in L, we have dist(f, \u2113) \u2264 dist(f, \u2113\u2032) \u2264 gc + lc and the lemma follows.\nWe partition the centers of L\u0303 as follows. Let L\u03030 be the set of centers of L\u0303 that have degree 0 in \u0393. Let L\u0303\u2264\u03b5\u22121 be the set of centers of L\u0303 that have degree at least one and at most 1/\u03b5 in \u0393. Let L\u0303>\u03b5\u22121 be the set of centers of L\u0303 that have degree greater than 1/\u03b5 in \u0393.\nWe now partition the centers of L\u0303 and C\u0303\u2217 using the neighborhoods of the vertices of L\u0303 in \u0393. We start by iteratively constructing two set of pairs S\u2264\u03b5\u22121 and S>\u03b5\u22121 . For each center \u2113 \u2208 L\u0303\u2264\u03b5\u22121\u222aL\u0303>\u03b5\u22121 , we pick a set A\u2113 of |N\u0393(\u2113)| \u2212 1 centers of L\u03030 and define a pair ({\u2113} \u222a A\u2113, N\u0393(\u2113)). We then remove A\u2113 from L\u03030 and repeat. Let S\u2264\u03b5\u22121 be the pairs that contain a center of L\u0303\u2264\u03b5\u22121 and let S>\u03b5\u22121 be the remaining pairs.\nThe following lemma follows from the definition of the pairs.\nLemma C.2. Let (RL\u0303, RC\u0303 \u2217 ) be a pair in S\u2264p\u222aS>p. If \u2113 \u2208 RL\u0303, then for any f such that (f, \u2113) \u2208 E, f \u2208 RC\u0303\u2217. Lemma C.3. For any pair (RL\u0303, RC\u0303 \u2217\n) \u2208 S\u2264p we have that \u2211\nc\u2208NC\u2217(RC\u0303\u2217 )\nlc \u2264 \u2211\nc\u2208NC\u2217(RC\u0303\u2217 )\ngc + 2 \u2211\nNL(RL\u0303)\u2212NC\u2217 (RC\u0303\u2217 )\ngc.\nProof. Consider the mixed solution M = L \u2212 RL\u0303 \u222a RC\u0303\u2217 . For each point c, let mc denote the cost of c in solution M . We have\nmc =\n\n \n \ngc if c \u2208 NC\u2217(RC\u0303\u2217). Reassignc if c \u2208 NL(RL\u0303)\u2212NC\u2217(RC\u0303\n\u2217 \u222a S) and by Lemma C.2. lc Otherwise.\nNow, observe that the solution M differs from L by at most 2/\u03b5 centers. Thus, by 1/\u03b5-local optimality we have cost(L) \u2264 cost(C\u2217). Summing over all clients and simplifying, we obtain\n\u2211\nc\u2208NC\u2217 (RC\u0303\u2217 )\u222aNL(RL\u0303)\nlc \u2264 \u2211\nc\u2208NC\u2217(RC\u0303\u2217 )\ngc + \u2211\nNL(RL\u0303)\u2212NC\u2217 (RC\u0303\u2217 )\nReassignc.\nThe lemma follows by combining with Lemma C.1.\nWe now analyze the cost of the clients served by a center of L that has degree greater than \u03b5\u22121 in \u0393.\nLemma C.4. For any pair (RL\u0303, RC\u0303 \u2217 ) \u2208 S>\u03b5\u22121 we have that \u2211\nc\u2208NC\u2217(RC\u0303\u2217 )\nlc \u2264 \u2211\nc\u2208NC\u2217 (RC\u0303\u2217 )\ngc + 2(1 + \u03b5) \u2211\nNL(RL\u0303)\u2212NC\u2217 (RC\u0303\u2217 )\ngc.\nProof. Consider the center \u2113\u0302 \u2208 RL\u0303 that has in-degree greater than \u03b5\u22121. Let L\u0302 = RL\u0303 \u2212 {\u2113\u0302}. For each \u2113 \u2208 L\u0302, we associate a center f(\u2113) in RC\u0303\u2217 in such a way that each f(\u2113) 6= f(\u2113\u2032), for \u2113 6= \u2113\u2032. Note that this is possible since |L\u0302| = |RC\u0303\u2217 | \u2212 1. Let f\u0303 be the center of RC\u0303\u2217 that is not associated with any center of L\u0302.\nNow, for each center \u2113 of L\u0302 we consider the mixed solution M \u2113 = L \u2212 {\u2113} \u222a {f(\u2113)}. For each client c, we bound its cost m\u2113c in solution M \u2113. We have\nm\u2113c =\n\n \n  gc if c \u2208 NC\u2217(f(\u2113)). Reassignc if c \u2208 NL(\u2113)\u2212NC\u2217(f(\u2113)) and by Lemma C.2. lc Otherwise.\nSumming over all center \u2113 \u2208 L\u0302 and all the clients in NC\u2217(f(\u2113)) \u222a NL(\u2113), we have by \u03b5\u22121-local optimality\n\u2211\nc\u2208NC\u2217(RC\u0303\u2217\u2212f\u0303)\u222aNL(L\u0302)\nlc \u2264 \u2211\nc\u2208NC\u2217 (RC\u0303\u2217\u2212f\u0303)\ngc + \u2211\nNL(L\u0302)\u2212NC\u2217 (RC\u0303\u2217\u2212f\u0303)\nReassignc. (6)\nWe now complete the proof of the lemma by analyzing the cost of the clients in NC\u2217(f\u0303). We consider the center \u2113\u2217 \u2208 L\u0302 that minimizes the reassignment cost of its clients. Namely, the center \u2113\u2217 such that \u2211\nc\u2208NL(\u2113\u2217)Reassignc is minimized. We then consider the solution M (\u2113\u2217,f\u0303) = L\u2212{\u2113\u2217}\u222a{f\u0303}.\nFor each client c, we bound its cost m (\u2113\u2217,f\u0303) c in solution M (\u2113 \u2217,f\u0303). We have\nm(\u2113 \u2217,f\u0303)\nc =\n\n \n  gc if c \u2208 NC\u2217(f\u0303). Reassignc if c \u2208 NL(\u2113\u2217)\u2212NC\u2217(f\u0303) and by Lemma C.2. lc Otherwise.\nThus, summing over all clients c, we have by local optimality\n\u2211\nc\u2208NC\u2217 (f\u0303)\u222aNL(\u2113\u2217)\nlc \u2264 \u2211\nc\u2208NC\u2217 (f\u0303)\ngc + \u2211\nNL(\u2113\u2217)\u2212NC\u2217 (f\u0303)\nReassignc. (7)\nBy Lemma C.1, combining Equations 6 and 7 and averaging over all centers of L\u0302 we have\n\u2211\nc\u2208NC\u2217(RC\u0303\u2217 )\nlc \u2264 \u2211\nc\u2208NC\u2217 (RC\u0303\u2217 )\ngc + 2(1 + \u03b5) \u2211\nNL(RL\u0303)\u2212NC\u2217 (RC\u0303\u2217 )\ngc.\nWe now turn to the proof of Theorem 3.14.\nProof of Theorem 3.14. Observe first that for any c \u2208 NL(L\u0303) \u2212 NC\u2217(C\u0303\u2217), we have lc \u2264 gc. This follows from the fact that the center that serves c in C\u2217 is in S and so in L and thus, we have lc \u2264 gc. Therefore\n\u2211\nc\u2208NL(L\u0303)\u2212NC\u2217 (C\u0303\u2217)\nlc \u2264 \u2211\nc\u2208NL(L\u0303)\u2212NC\u2217 (C\u0303\u2217)\ngc. (8)\nWe now sum the equations of Lemmas C.3 and C.4 over all pairs and obtain\n\u2211\n(RL\u0303,RC\u0303\u2217 )\n\u2211\nc\u2208NC\u2217(RC\u0303\u2217 )\nlc \u2264 \u2211\n(RL\u0303,RC\u0303\u2217 )\n\n \n\u2211\nc\u2208NC\u2217(RC\u0303\u2217 )\ngc + (2 + 2\u03b5) \u2211\nNL(RL\u0303)\u2212NC\u2217 (RC\u0303\u2217 )\ngc\n\n \n\u2211\nc\u2208NC\u2217(C\u0303\u2217)\nlc \u2264 \u2211\nc\u2208NC\u2217(C\u0303\u2217)\ngc + (2 + 2\u03b5) \u2211\nc\u2208NL(L\u0303)\u2212NC\u2217 (C\u0303\u2217)\ngc\n\u2211\nc\u2208NC\u2217(C\u0303\u2217)\u222aNL(L\u0303)\nlc \u2264 (3 + 2\u03b5) \u2211\nc\u2208NC\u2217 (C\u0303\u2217)\u222aNL(L\u0303)\ngc By Eq. 8."}], "references": [{"title": "Local Search in Combinatorial Optimization", "author": ["Emile Aarts", "Jan K. Lenstra", "editors"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "On spectral learning of mixtures of distributions", "author": ["Dimitris Achlioptas", "Frank McSherry"], "venue": "In Learning Theory, 18th Annual Conference on Learning Theory, COLT 2005, Bertinoro,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "k-means++ under approximation stability", "author": ["Manu Agarwal", "Ragesh Jaiswal", "Arindam Pal"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Learning mixtures of arbitrary gaussians", "author": ["Sanjeev Arora", "Ravi Kannan"], "venue": "In Proceedings on 33rd Annual ACM Symposium on Theory of Computing, July 6-8,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Approximation schemes for Euclidean k -medians and related problems", "author": ["Sanjeev Arora", "Prabhakar Raghavan", "Satish Rao"], "venue": "In Proceedings of the Thirtieth Annual ACM Symposium on the Theory of Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Smoothed analysis of the k-means method", "author": ["David Arthur", "Bodo Manthey", "Heiko R\u00f6glin"], "venue": "J. ACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "k-means++: the advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Worst-case and smoothed analysis of the ICP algorithm, with an application to the k-means method", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "SIAM J. Comput.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Local search heuristics for k-median and facility location problems", "author": ["Vijay Arya", "Naveen Garg", "Rohit Khandekar", "Adam Meyerson", "Kamesh Munagala", "Vinayaka Pandit"], "venue": "SIAM J. Comput.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Stability yields a PTAS for k-median and k-means clustering", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "In 51th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Center-based clustering under perturbation stability", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "Inf. Process. Lett.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "The hardness of approximation of Euclidean k-means", "author": ["Pranjal Awasthi", "Moses Charikar", "Ravishankar Krishnaswamy", "Ali Kemal Sinop"], "venue": "In 31st International Symposium on Computational Geometry, SoCG 2015, June 22-25,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Improved spectral-norm bounds for clustering. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques - 15th International Workshop", "author": ["Pranjal Awasthi", "Or Sheffet"], "venue": "APPROX 2012, and 16th International Workshop,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Polynomial time algorithm for 2-stable clustering", "author": ["Ainesh Bakshi", "Nadiia Chepurko"], "venue": "instances. CoRR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Approximate clustering without the approximation", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "In Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Clustering under approximation stability", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "J. ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Finding low error clusterings", "author": ["Maria-Florina Balcan", "Mark Braverman"], "venue": "In COLT 2009 - The 22nd Conference on Learning Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "k-center clustering under perturbation resilience", "author": ["Maria-Florina Balcan", "Nika Haghtalab", "Colin White"], "venue": "In 43rd International Colloquium on Automata, Languages, and Programming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Clustering under perturbation resilience", "author": ["Maria-Florina Balcan", "Yingyu Liang"], "venue": "SIAM J. Comput.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Agnostic clustering", "author": ["Maria-Florina Balcan", "Heiko R\u00f6glin", "Shang-Hua Teng"], "venue": "In Algorithmic Learning Theory, 20th International Conference,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "On variants of k-means clustering", "author": ["Sayan Bandyapadhyay", "Kasturi R. Varadarajan"], "venue": "CoRR, abs/1512.02985,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Distributed balanced clustering via mapping coresets", "author": ["MohammadHossein Bateni", "Aditya Bhaskara", "Silvio Lattanzi", "Vahab S. Mirrokni"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "In 51th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Data stability in clustering: A closer look", "author": ["Shalev Ben-David", "Lev Reyzin"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "On the practically interesting instances of MAXCUT", "author": ["Yonatan Bilu", "Amit Daniely", "Nati Linial", "Michael E. Saks"], "venue": "In 30th International Symposium on Theoretical Aspects of Computer Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Are stable instances easy? Combinatorics", "author": ["Yonatan Bilu", "Nathan Linial"], "venue": "Probability & Computing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Parallel approximation algorithms for facilitylocation problems", "author": ["Guy E. Blelloch", "Kanat Tangwongsan"], "venue": "SPAA", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Streaming k-means on well-clusterable data", "author": ["Vladimir Braverman", "Adam Meyerson", "Rafail Ostrovsky", "Alan Roytman", "Michael Shindler", "Brian Tagiku"], "venue": "In Proceedings of the Twenty- Second Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Isotropic PCA and affine-invariant clustering", "author": ["S. Charles Brubaker", "Santosh Vempala"], "venue": "Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Improved combinatorial algorithms for facility location problems", "author": ["Moses Charikar", "Sudipto Guha"], "venue": "SIAM J. Comput.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "Dimensionality reduction for k-means clustering and low rank approximation", "author": ["Michael B. Cohen", "Sam Elder", "Cameron Musco", "Christopher Musco", "Madalina Persu"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "The power of Local Search for clustering", "author": ["Vincent Cohen-Addad", "Philip N. Klein", "Claire Mathieu"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Effectiveness of local search for geometric optimization", "author": ["Vincent Cohen-Addad", "Claire Mathieu"], "venue": "In 31st International Symposium on Computational Geometry, SoCG 2015, June 22-25,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Variational shape approximation", "author": ["David Cohen-Steiner", "Pierre Alliez", "Mathieu Desbrun"], "venue": "ACM Trans. Graph.,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Graph partitioning via adaptive spectral techniques", "author": ["Amin Coja-Oghlan"], "venue": "Combinatorics, Probability & Computing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Spectral clustering with limited independence", "author": ["Anirban Dasgupta", "John E. Hopcroft", "Ravi Kannan", "Pradipta Prometheus Mitra"], "venue": "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "Annual Symposium on Foundations of Computer Science,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1999}, {"title": "Random projection trees for vector quantization", "author": ["Sanjoy Dasgupta", "Yoav Freund"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "A probabilistic analysis of EM for mixtures of separated, spherical gaussians", "author": ["Sanjoy Dasgupta", "Leonard J. Schulman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "Iterative clustering of high dimensional text data augmented by local search", "author": ["Inderjit S. Dhillon", "Yuqiang Guan", "Jacob Kogan"], "venue": "In Proceedings of the 2002 IEEE International Conference on Data Mining (ICDM 2002),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2002}, {"title": "A unified framework for approximating and clustering data", "author": ["Dan Feldman", "Michael Langberg"], "venue": "In Proceedings of the 43rd ACM Symposium on Theory of Computing,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Local search yields a PTAS for k-means in doubling metrics", "author": ["Zachary Friggstad", "Mohsen Rezapour", "Mohammad R. Salavatipour"], "venue": "CoRR, abs/1603.08976,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Tight analysis of a multiple-swap heurstic for budgeted red-blue median", "author": ["Zachary Friggstad", "Yifeng Zhang"], "venue": "In 43rd International Colloquium on Automata, Languages, and Programming,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Greedy strikes back: Improved facility location algorithms", "author": ["Sudipto Guha", "Samir Khuller"], "venue": "J. Algorithms,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1999}, {"title": "Clustering data streams: Theory and practice", "author": ["Sudipto Guha", "Adam Meyerson", "Nina Mishra", "Rajeev Motwani", "Liadan O\u2019Callaghan"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2003}, {"title": "Simpler analyses of local search algorithms for facility location", "author": ["Anupam Gupta", "Kanat Tangwongsan"], "venue": "CoRR, abs/0809.2554,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2008}, {"title": "Embeddings and non-approximability of geometric problems", "author": ["Venkatesan Guruswami", "Piotr Indyk"], "venue": "In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, January 12-14,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2003}, {"title": "J-means: a new local search heuristic for minimum sum of squares clustering", "author": ["Pierre Hansen", "Nenad Mladenovic"], "venue": "Pattern Recognition,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2001}, {"title": "Smaller coresets for k-median and k-means clustering", "author": ["Sariel Har-Peled", "Akash Kushal"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2007}, {"title": "On coresets for k-means and k-median clustering", "author": ["Sariel Har-Peled", "Soham Mazumdar"], "venue": "In Proceedings of the 36th Annual ACM Symposium on Theory of Computing,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2004}, {"title": "A new greedy approach for facility location problems", "author": ["Kamal Jain", "Mohammad Mahdian", "Amin Saberi"], "venue": "In Proceedings on 34th Annual ACM Symposium on Theory of Computing, May 19-21,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2002}, {"title": "Approximation algorithms for metric facility location and k -median problems using the primal-dual schema and Lagrangian relaxation", "author": ["Kamal Jain", "Vijay V. Vazirani"], "venue": "J. ACM,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2001}, {"title": "Analysis of k-means++ for separable data. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques - 15th International Workshop", "author": ["Ragesh Jaiswal", "Nitin Garg"], "venue": "APPROX 2012, and 16th International Workshop,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2012}, {"title": "Extensions of Lipschitz mapping into Hilbert space. In Conf. in modern analysis and probability, volume 26 of Contemporary Mathematics, pages 189\u2013206", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "American Mathematical Society,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1984}, {"title": "The spectral method for general mixture models", "author": ["Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala"], "venue": "SIAM J. Comput.,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2008}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["Tapas Kanungo", "David M. Mount", "Nathan S. Netanyahu", "Christine D. Piatko", "Ruth Silverman", "Angela Y. Wu"], "venue": "Comput. Geom.,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2004}, {"title": "Approximation and streaming algorithms for projective clustering via random projections", "author": ["Michael Kerber", "Sharath Raghvendra"], "venue": "In Proceedings of the 27th Canadian Conference on Computational Geometry,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2015}, {"title": "A nearly linear-time approximation scheme for the euclidean k-median problem", "author": ["Stavros G. Kolliopoulos", "Satish Rao"], "venue": "SIAM J. Comput.,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2007}, {"title": "Analysis of a local search heuristic for facility location problems", "author": ["Madhukar R. Korupolu", "C. Greg Plaxton", "Rajmohan Rajaraman"], "venue": "J. Algorithms,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2000}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["Amit Kumar", "Ravindran Kannan"], "venue": "In 51th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2010}, {"title": "Linear-time approximation schemes for clustering problems in any dimensions", "author": ["Amit Kumar", "Yogish Sabharwal", "Sandeep Sen"], "venue": "J. ACM,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2010}, {"title": "Finding meaningful cluster structure amidst background noise", "author": ["Shrinu Kushagra", "Samira Samadi", "Shai Ben-David"], "venue": "In Algorithmic Learning Theory - 27th International Conference,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2016}, {"title": "Approximating k-median via pseudo-approximation", "author": ["Shi Li", "Ola Svensson"], "venue": "In Symposium on Theory of Computing Conference,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2013}, {"title": "The planar k-means problem is NP-hard", "author": ["Meena Mahajan", "Prajakta Nimbhorkar", "Kasturi R. Varadarajan"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2012}, {"title": "On approximate geometric k-clustering", "author": ["Ji\u0155\u0131 Matousek"], "venue": "Discrete & Computational Geometry,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2000}, {"title": "Spectral partitioning of random graphs", "author": ["Frank McSherry"], "venue": "In 42nd Annual Symposium on Foundations of Computer Science,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2001}, {"title": "On the complexity of some common geometric location problems", "author": ["Nimrod Megiddo", "Kenneth J. Supowit"], "venue": "SIAM J. Comput.,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 1984}, {"title": "The online median problem", "author": ["Ramgopal R. Mettu", "C. Greg Plaxton"], "venue": "SIAM J. Comput.,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2003}, {"title": "The effectiveness of Lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "venue": "J. ACM,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2012}, {"title": "The volume of convex bodies and Banach space geometry", "author": ["Gilles Pisier"], "venue": "Cambridge Tracts in Mathematics", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1999}, {"title": "Clustering with or without the approximation", "author": ["Frans Schalekamp", "Michael Yu", "Anke van Zuylen"], "venue": "J. Comb. Optim.,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2013}, {"title": "Clustering for edge-cost minimization (extended abstract)", "author": ["Leonard J. Schulman"], "venue": "In Proceedings of the Thirty-Second Annual ACM Symposium on Theory of Computing, May 21-23,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2000}, {"title": "A spectral algorithm for learning mixture models", "author": ["Santosh Vempala", "Grant Wang"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2004}, {"title": "Towards event source unobservability with minimum network traffic in sensor networks", "author": ["Yi Yang", "Min Shao", "Sencun Zhu", "Bhuvan Urgaonkar", "Guohong Cao"], "venue": "In Proceedings of the First ACM Conference on Wireless Network Security,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": ": [1] or [56]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 55, "context": ": [1] or [56]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "This algorithm has a polynomial running time (see [9, 33]).", "startOffset": 50, "endOffset": 57}, {"referenceID": 32, "context": "This algorithm has a polynomial running time (see [9, 33]).", "startOffset": 50, "endOffset": 57}, {"referenceID": 9, "context": "Distribution Stability Awasthi, Blum, Sheffet [10]", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "Approximation Stability Balcan, Blum, Gupta [15, 16] Cost Separation Ostrovsky, Rabani, Schulman, Swamy [69] Jaiswal, Garg [53]", "startOffset": 44, "endOffset": 52}, {"referenceID": 15, "context": "Approximation Stability Balcan, Blum, Gupta [15, 16] Cost Separation Ostrovsky, Rabani, Schulman, Swamy [69] Jaiswal, Garg [53]", "startOffset": 44, "endOffset": 52}, {"referenceID": 68, "context": "Approximation Stability Balcan, Blum, Gupta [15, 16] Cost Separation Ostrovsky, Rabani, Schulman, Swamy [69] Jaiswal, Garg [53]", "startOffset": 104, "endOffset": 108}, {"referenceID": 52, "context": "Approximation Stability Balcan, Blum, Gupta [15, 16] Cost Separation Ostrovsky, Rabani, Schulman, Swamy [69] Jaiswal, Garg [53]", "startOffset": 123, "endOffset": 127}, {"referenceID": 59, "context": "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 56, "endOffset": 60}, {"referenceID": 24, "context": "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 113, "endOffset": 121}, {"referenceID": 25, "context": "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 113, "endOffset": 121}, {"referenceID": 10, "context": "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 145, "endOffset": 149}, {"referenceID": 18, "context": "Spectral Separation Kumar, Kannan [60] Awasthi, Sheffet [13] Perturbation Resilience Bilu, Daniely, Linial, Saks [25, 26] Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 164, "endOffset": 168}, {"referenceID": 10, "context": "Center Proximity Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "Center Proximity Awasthi, Blum, Sheffet [11] Balcan, Liang [19]", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "For example, if an instance is cost-separated then it is distribution-stable; therefore the algorithm by Awasthi, Blum and Sheffet [10] also works for cost-separated instances.", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": "[10], called \u201cdistribution stability\u201d.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "2 (Distribution Stability [10]).", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 68, "context": "[69], so Local Search is also a PTAS in their setting and also recovers most of the structure of such instances.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "3 (Perturbation Resilience [11]).", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "An optimal algorithm for 2-perturbation resilient clustering for any center-based objective function was very recently given by Bakshi and Chepurko [14].", "startOffset": 148, "endOffset": 152}, {"referenceID": 13, "context": "We do not quite match [14]: One limitation is that Local Search is not necessarily optimal for 2-perturbation resilient instances, see Proposition 3.", "startOffset": 22, "endOffset": 26}, {"referenceID": 59, "context": "4 (Spectral Separation [60]2).", "startOffset": 23, "endOffset": 27}, {"referenceID": 59, "context": "In previous work by Kumar and Kannan [60], an algorithm was given with approximation ratio 1+O(OPTk/OPTk\u22121), where OPTi denotes the value of an optimal solution using i centers.", "startOffset": 37, "endOffset": 41}, {"referenceID": 59, "context": "k/\u03b5)-spectrally separated [60].", "startOffset": 26, "endOffset": 30}, {"referenceID": 55, "context": ": [56]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 59, "context": "Hence if instances are The proximity condition of Kumar and Kannan [60] implies the spectral separation condition.", "startOffset": 67, "endOffset": 71}, {"referenceID": 66, "context": "2 Related Work The problems we study are NP-hard: k-median and k-means are already NP-hard in the Euclidean plane (see Meggido and Supowit [67], Mahajan et al.", "startOffset": 139, "endOffset": 143}, {"referenceID": 63, "context": "[64], and Dasgupta and Freud [38]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[64], and Dasgupta and Freud [38]).", "startOffset": 29, "endOffset": 33}, {"referenceID": 43, "context": "In terms of hardness of approximation, both problems are APX-hard, even in the Euclidean setting when both k and d are part of the input (see Gua and Khuller [44], Jain et al.", "startOffset": 158, "endOffset": 162}, {"referenceID": 50, "context": "[51], Guruswami et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[47] and Awasthi et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "On the positive side, constant factor approximations are known in metric space for both k-median and k-means (see for example [63, 52, 68]).", "startOffset": 126, "endOffset": 138}, {"referenceID": 51, "context": "On the positive side, constant factor approximations are known in metric space for both k-median and k-means (see for example [63, 52, 68]).", "startOffset": 126, "endOffset": 138}, {"referenceID": 67, "context": "On the positive side, constant factor approximations are known in metric space for both k-median and k-means (see for example [63, 52, 68]).", "startOffset": 126, "endOffset": 138}, {"referenceID": 40, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 175, "endOffset": 183}, {"referenceID": 60, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 175, "endOffset": 183}, {"referenceID": 4, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 203, "endOffset": 226}, {"referenceID": 31, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 203, "endOffset": 226}, {"referenceID": 41, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 203, "endOffset": 226}, {"referenceID": 57, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 203, "endOffset": 226}, {"referenceID": 48, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 203, "endOffset": 226}, {"referenceID": 49, "context": "Given the hardness results, how can one hope to obtain a (1+\u03b5)-approximation? One possibility is to further restrict the input, for example, Euclidean space and fixing k (see [41, 61] for example), or d [5, 32, 42, 58, 49, 50], or some stability assumption on the input structure such as cost separation, approximation stability, perturbation resilience, or spectral separability.", "startOffset": 203, "endOffset": 226}, {"referenceID": 68, "context": "[69] assumed that cost of an optimal clustering with k centers is smaller than an \u03b52-fraction of the cost of an optimal clustering with k\u2212 1 centers, see also Schulman [72] for an earlier condition for two clusters and the irreducibility condition by Kumar et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 71, "context": "[69] assumed that cost of an optimal clustering with k centers is smaller than an \u03b52-fraction of the cost of an optimal clustering with k\u2212 1 centers, see also Schulman [72] for an earlier condition for two clusters and the irreducibility condition by Kumar et al.", "startOffset": 168, "endOffset": 172}, {"referenceID": 60, "context": "[61].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "The popular D2 sampling technique (also known as k-means++) has an improved performance for cost separated instances compared to the worst-case O(log k)-approximation ratio [7, 28, 53, 69].", "startOffset": 173, "endOffset": 188}, {"referenceID": 27, "context": "The popular D2 sampling technique (also known as k-means++) has an improved performance for cost separated instances compared to the worst-case O(log k)-approximation ratio [7, 28, 53, 69].", "startOffset": 173, "endOffset": 188}, {"referenceID": 52, "context": "The popular D2 sampling technique (also known as k-means++) has an improved performance for cost separated instances compared to the worst-case O(log k)-approximation ratio [7, 28, 53, 69].", "startOffset": 173, "endOffset": 188}, {"referenceID": 68, "context": "The popular D2 sampling technique (also known as k-means++) has an improved performance for cost separated instances compared to the worst-case O(log k)-approximation ratio [7, 28, 53, 69].", "startOffset": 173, "endOffset": 188}, {"referenceID": 9, "context": "[10] where the cost of assigning all the points from one cluster in the optimal k-clustering to another center increases the objective by some factor (1 + \u03b1).", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 3, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 22, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 28, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 34, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 35, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 36, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 38, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 54, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 65, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 72, "context": "Much work has focused on finding the target clustering under various assumptions, see for instance [2, 4, 23, 29, 35, 36, 37, 39, 55, 66, 73].", "startOffset": 99, "endOffset": 141}, {"referenceID": 14, "context": "[15, 16] gave a deterministic condition called approximation stability under which a target clustering can be retrieved by via any sufficiently good algorithm for the k-means (or kmedian) objective function.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[15, 16] gave a deterministic condition called approximation stability under which a target clustering can be retrieved by via any sufficiently good algorithm for the k-means (or kmedian) objective function.", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "For results using approximation stability, see [16, 17, 71, 3, 10].", "startOffset": 47, "endOffset": 66}, {"referenceID": 16, "context": "For results using approximation stability, see [16, 17, 71, 3, 10].", "startOffset": 47, "endOffset": 66}, {"referenceID": 70, "context": "For results using approximation stability, see [16, 17, 71, 3, 10].", "startOffset": 47, "endOffset": 66}, {"referenceID": 2, "context": "For results using approximation stability, see [16, 17, 71, 3, 10].", "startOffset": 47, "endOffset": 66}, {"referenceID": 9, "context": "For results using approximation stability, see [16, 17, 71, 3, 10].", "startOffset": 47, "endOffset": 66}, {"referenceID": 59, "context": "Another deterministic condition that relates target clustering recovery via the k-means objective was introduced by Kumar and Kannan [60].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "For further spectral based approaches, see also [13].", "startOffset": 48, "endOffset": 52}, {"referenceID": 25, "context": "[26, 25] formalized this as allowing edge weights in a graph to be modified by a factor of at most \u03b3 without changing the maxcut.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "[26, 25] formalized this as allowing edge weights in a graph to be modified by a factor of at most \u03b3 without changing the maxcut.", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "[6, 8] for work on k-means).", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "[6, 8] for work on k-means).", "startOffset": 0, "endOffset": 6}, {"referenceID": 10, "context": "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].", "startOffset": 47, "endOffset": 71}, {"referenceID": 13, "context": "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].", "startOffset": 47, "endOffset": 71}, {"referenceID": 17, "context": "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].", "startOffset": 47, "endOffset": 71}, {"referenceID": 18, "context": "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].", "startOffset": 47, "endOffset": 71}, {"referenceID": 23, "context": "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].", "startOffset": 47, "endOffset": 71}, {"referenceID": 61, "context": "For results using perturbation resilience, see [11, 14, 18, 19, 24, 62].", "startOffset": 47, "endOffset": 71}, {"referenceID": 58, "context": "Local Search There exists a large body of bicriteria approximations for k-median and k-means [59, 30, 33, 21].", "startOffset": 93, "endOffset": 109}, {"referenceID": 29, "context": "Local Search There exists a large body of bicriteria approximations for k-median and k-means [59, 30, 33, 21].", "startOffset": 93, "endOffset": 109}, {"referenceID": 32, "context": "Local Search There exists a large body of bicriteria approximations for k-median and k-means [59, 30, 33, 21].", "startOffset": 93, "endOffset": 109}, {"referenceID": 20, "context": "Local Search There exists a large body of bicriteria approximations for k-median and k-means [59, 30, 33, 21].", "startOffset": 93, "endOffset": 109}, {"referenceID": 8, "context": "[9] (see also [46]) gave the first analysis showing that Local Search with a neighborhood size of 1/\u03b5 gives a 3 + 2\u03b5 approximation to k-median and showed that this bound is tight.", "startOffset": 0, "endOffset": 3}, {"referenceID": 45, "context": "[9] (see also [46]) gave the first analysis showing that Local Search with a neighborhood size of 1/\u03b5 gives a 3 + 2\u03b5 approximation to k-median and showed that this bound is tight.", "startOffset": 14, "endOffset": 18}, {"referenceID": 55, "context": "[56] proved an approximation ratio of 9 + \u03b5 for Euclidean k-means clustering by Local Search, currently the best known algorithm with a polynomial running time in metric and Euclidean spaces.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "3 Recently, Local Search with an appropriate neighborhood was shown to be a PTAS for k-means and k-median in certain restricted metrics including constant dimensional Euclidean space [42, 32].", "startOffset": 183, "endOffset": 191}, {"referenceID": 31, "context": "3 Recently, Local Search with an appropriate neighborhood was shown to be a PTAS for k-means and k-median in certain restricted metrics including constant dimensional Euclidean space [42, 32].", "startOffset": 183, "endOffset": 191}, {"referenceID": 21, "context": "Due to its simplicity, Local Search is also a popular subroutine for clustering tasks in various computational models [22, 27, 45].", "startOffset": 118, "endOffset": 130}, {"referenceID": 26, "context": "Due to its simplicity, Local Search is also a popular subroutine for clustering tasks in various computational models [22, 27, 45].", "startOffset": 118, "endOffset": 130}, {"referenceID": 44, "context": "Due to its simplicity, Local Search is also a popular subroutine for clustering tasks in various computational models [22, 27, 45].", "startOffset": 118, "endOffset": 130}, {"referenceID": 39, "context": "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.", "startOffset": 59, "endOffset": 79}, {"referenceID": 42, "context": "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.", "startOffset": 59, "endOffset": 79}, {"referenceID": 47, "context": "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.", "startOffset": 59, "endOffset": 79}, {"referenceID": 33, "context": "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.", "startOffset": 59, "endOffset": 79}, {"referenceID": 73, "context": "For more clustering papers using Local Search, we refer to [40, 43, 48, 34, 74] 2 Approach and Techniques 2.", "startOffset": 59, "endOffset": 79}, {"referenceID": 9, "context": "Our proof includes a few ingredients from [10] such as the notion of inner-ring (we work with a slightly more general definition) and distinguishing between cheap and expensive clusters.", "startOffset": 42, "endOffset": 46}, {"referenceID": 64, "context": "Euclidean inputs can be straightforwardly \u201cdiscretized\u201d by computing an appropriate candidate set of centers, for instance via Matousek\u2019s approximate centroid set [65] and then applying the Johnson-Lindenstrauss lemma, if the dimension is too large.", "startOffset": 163, "endOffset": 167}, {"referenceID": 64, "context": "They combined Local Search with techniques from Matousek [65] for k-means clustering in Euclidean spaces.", "startOffset": 57, "endOffset": 61}, {"referenceID": 59, "context": "Indeed, this is the first step of the algorithm by Kumar and Kannan [60] (see Algorithm 3).", "startOffset": 68, "endOffset": 72}, {"referenceID": 59, "context": "Algorithm 3 k-means with spectral initialization [60] 1: Project points onto the best rank k subspace 2: Compute a clustering C with constant approximation factor on the projection 3: Initialize centroids of each cluster of C as centers in the original space 4: Run Lloyd\u2019s k-means until convergence In general, projecting onto the best rank k subspace and computing a constant approximation on the projection results in a constant approximation in the original space.", "startOffset": 49, "endOffset": 53}, {"referenceID": 59, "context": "Kumar and Kannan [60] and later Awasthi and Sheffet [13] gave tighter bounds if the spectral separation is large enough.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "Kumar and Kannan [60] and later Awasthi and Sheffet [13] gave tighter bounds if the spectral separation is large enough.", "startOffset": 52, "endOffset": 56}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "The following definition is a generalization of the inner-ring definition of [10].", "startOffset": 77, "endOffset": 81}, {"referenceID": 9, "context": "1 in [10].", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "Now, observe that by [9], the cost of L is at most a 5 approximation to the cost of OPT in the worst case.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "This relies on the example from [9].", "startOffset": 32, "endOffset": 35}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "2 (Theorem 7 of [31]).", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "This can be proven by using techniques from Awasthi and Sheffet [13] (Theorem 3.", "startOffset": 64, "endOffset": 68}, {"referenceID": 59, "context": "1) and Kumar and Kannan [60] (Theorem 5.", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "Recall that by [9], L is a 5-approximation and so there exist at most 40\u03b5\u22121\u03b2\u22121 such clusters.", "startOffset": 15, "endOffset": 18}, {"referenceID": 64, "context": "For k-means itself, we could alternatively combine Matousek\u2019s approximate centroid set [65] with the Johnson Lindenstrauss lemma and avoid the following construction; however this would only work for optimal distribution stable clusterings and the proof Theorem 1.", "startOffset": 87, "endOffset": 91}, {"referenceID": 69, "context": "It is well known that for unit Euclidean ball of dimension d, there exists an \u03b5-net of cardinality (1 + 2/\u03b5)d, see for instance Pisier [70].", "startOffset": 135, "endOffset": 139}, {"referenceID": 53, "context": "To reduce the dependency on the dimension, we combine this statement with the seminal theorem originally due to Johnson and Lindenstrauss [54].", "startOffset": 138, "endOffset": 142}, {"referenceID": 56, "context": "The Johnson-Lindenstrauss lemma can also be applied in these settings, at a slightly worse target dimension of O((p + 1)2 log((p + 1)/\u03b5)\u03b5\u22123 log n), see Kerber and Raghvendra [57].", "startOffset": 174, "endOffset": 178}, {"referenceID": 8, "context": "14 We first introduce some definitions, following the terminology of [9, 46].", "startOffset": 69, "endOffset": 76}, {"referenceID": 45, "context": "14 We first introduce some definitions, following the terminology of [9, 46].", "startOffset": 69, "endOffset": 76}], "year": 2017, "abstractText": "In this paper, we analyze the performance of a simple and standard Local Search algorithm for clustering on well behaved data. Since the seminal paper by Ostrovsky, Rabani, Schulman and Swamy [FOCS 2006], much progress has been made to characterize real-world instances. We distinguish the three main definitions \u2022 Distribution Stability (Awasthi, Blum, Sheffet, FOCS 2010) \u2022 Spectral Separability (Kumar, Kannan, FOCS 2010) \u2022 Perturbation Resilience (Bilu, Linial, ICS 2010) and show that Local Search performs well on the instances with the aforementioned stability properties. Specifically, for the k-means and k-median objective, we show that Local Search exactly recovers the optimal clustering if the dataset is 3+ \u03b5-perturbation resilient, and is a PTAS for distribution stability and spectral separability. This implies the first PTAS for instances satisfying the spectral separability condition. For the distribution stability condition we also go beyond previous work by showing that the clustering output by the algorithm and the optimal clustering are very similar. This is a significant step toward understanding the success of Local Search heuristics in clustering applications and supports the legitimacy of the stability conditions: They characterize some of the structure of real-world instances that make Local Search a popular heuristic. Supported by Deutsche Forschungsgemeinschaft within the Collaborative Research Center SFB 876, project A2", "creator": "LaTeX with hyperref package"}}}