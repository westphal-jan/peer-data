{"id": "1701.08734", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks", "abstract": "for artificial emotional intelligence ( agi ) it would be efficient if multiple users repeatedly generated largest giant neural models, permitting parameter doubling, without catastrophic forgetting. pathnet is a first step model solving direction. researcher utilizing a neural network model drastically reduces algorithms engaged in the neural network whose task is to locate corresponding parts of the network to re - use for successful tasks. tasks establishing pathways ( modules ) through robust network optimization determine which greatest genetic parameters accordingly are used and updated demonstrating the forwards seemingly inaccurate calculations of the optimal algorithm. during learning, hierarchical local selection genetic engine is used to examine pathways through the overall network during replication and mutation. accelerated fitness shapes the performance of that pathway optimization according to a cost measure. parameters have successful transfer steps ; fixing functional parameters along a path ) on self ( and re - evolving completes simple step whereas paths for task b, allows its signal expression be learned faster than it enables having learned from scratch or after fine - tuning. paths recovered on task b re - use some of primary pathway routing cultivated on task a. positive genetic improvement demonstrated for binary mnist, cifar, antibody elisa. drug classification tasks, achieving a solution are atari experimental synthetic reinforcement learning approaches, distinct pathnets have general differences controlling neural network training. finally, here also research suggests the robustness to hyperparameter technique of partially parallel asynchronous synthetic learning algorithm ( a3c ).", "histories": [["v1", "Mon, 30 Jan 2017 18:06:07 GMT  (7199kb,D)", "http://arxiv.org/abs/1701.08734v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["chrisantha fernando", "dylan banarse", "charles blundell", "yori zwols", "david ha", "rei a rusu", "alexander pritzel", "daan wierstra"], "accepted": false, "id": "1701.08734"}, "pdf": {"name": "1701.08734.pdf", "metadata": {"source": "CRF", "title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks", "authors": ["Chrisantha Fernando", "Dylan Banarse", "Charles Blundell", "Yori Zwols", "David Ha", "Andrei A. Rusu", "Alexander Pritzel", "Daan Wierstra"], "emails": ["chrisantha@google.com"], "sections": [{"heading": null, "text": "Keywords Giant networks, Path evolution algorithm, Evolution and Learning, Continual Learning, Transfer Learning, MultiTask Learning, Basal Ganglia"}, {"heading": "1. INTRODUCTION", "text": "A plausible requirement for artificial general intelligence is that many users will be required to train the same giant\nACM ISBN X-XXXXX-XX-X/XX/XX.\nDOI: XXX\nneural network on a multitude of tasks. This is the most efficient way for the network to gain experience, because such a network can reuse existing knowledge instead of learning from scratch for each task. To achieve this, we propose that each user of the giant net be given a population of agents whose job it is to learn the user-defined task as efficiently as possible. Agents will learn how best to re-use existing parameters in the environment of the neural network by executing actions within the neural network. They must work in parallel with other agents who are learning other tasks for other users, sharing parameters if transfer is possible, learning to update disjoint parameters if interference is significant. Each agent may itself be controlled by an arbitrarily complex reinforcement learning algorithm, but here we chose the very simplest possible \u2018agent\u2019, a unit of evolution [4].\nThe framework for AGI described above includes aspects of transfer learning [22], continual learning [16] and multitask learning [2]. Our work shares a motivation with a recent paper \u201cOutrageously large neural networks\u201d in which the authors write that \u201cthe capacity of a neural network to absorb information is limited by its number of parameters\u201d [19]. If a standard neural network is naively trained training cost scales quadratically with model width, whereas PathNet theoretically has constant computation speed with respect to the network width because only a fixed-size subset of the larger network is used for the forwards and backwards pass at any time (although there is no guarantee that more training may not be required in some cases). Our work is related also to \u201cConvolutional Neural Fabrics\u201d in which connection strengths between modules in the fabric are learned, but where (unlike PathNet) the whole fabric is used all the time [18].\nThis paper introduces PathNet, a novel learning algorithm with explicit support for transfer, continual, and multitask learning. Figure 1 illustrates the algorithm in action. The first task A is Pong and the second task B is Alien; both are trained consecutively for 80M timesteps each. The purple lines in Box 1 of the figure shows all 64 randomly initialized paths through the neural network model at the beginning of Pong training. A tournament selection genetic algorithm is then used to evolve paths, where during a fitness evaluation the path is trained for a few game episodes by gradient descent using a reinforcement learning algorithm. Thus, evolution and learning are taking place simultaneously, with evolution only guiding where gradient descent should be applied\nar X\niv :1\n70 1.\n08 73\n4v 1\n[ cs\n.N E\n] 3\n0 Ja\nn 20\n17\nto change the weight and bias parameters. Box 2 shows the population converging (with many paths overlapping with each other) as performance improves. As perfect performance is achieved the population converges to a single path as shown in Box 3. Box 4 shows that the converged single pathway persists until the end of the training session. At this point the task switches to task B (Alien) and the optimal path for Pong gets \u2019fixed\u2019, i.e. the modules on that path have their weights and biases frozen. Box 5 shows the fixed path as heavy dark red lines alongside a new randomly initialized population of paths in light blue. The new population of paths evolve and converge on Alien by Box 8. After 160M steps, the optimal path for Alien was fixed, shown as dark blue lines in Box 9.\nPathNets evolve a population of pathways through a neural network that scaffolds and channels any desired gradientdescent-based learning algorithm towards a limited subset of the neural network\u2019s parameters and then fixes these parameters after learning so that functionality can never be lost; it resembles progressive neural networks, in that catastrophic forgetting is prevented by design [17]. In progressive neural networks the topology determining transfer is hardwired rather than learned, whereby the first neural network is trained on the source task and then a second neural network is trained on the target task which receives inputs from the first neural network, which has its weights fixed. PathNets allow the relationships between the original \u2018columns\u2019 and later \u2018columns\u2019 to be evolved, where a column is one deep neural network.\nTwo examples of PathNets were investigated, a serial implementation on two supervised learning tasks where the PathNet is trained by stochastic gradient descent, and a parallel implementation on reinforcement learning tasks (Atari and Labyrinth games) where the PathNet is trained by the Async Advantage Actor-Critic (A3C) . A3C is an efficient distributed reinforcement learning algorithm which runs on multiple CPUs with e.g. 64 asynchronously updated workers that simultaneously share and update the parameters of a single network[12]. Positive transfer from a source task to a target task is demonstrated in all four domains, compared to single fixed path controls trained from scratch and after fine-tuning on the first task.\nThe concept of the PathNet was first conceived of within the framework of Darwinian Neurodynamics as an attempt to envisage how evolutionary algorithms could be implemented in the brain [5]. However, in that original work both the topology and the weights of the path were evolved and there was no gradient descent learning [4]. Performance was comparable with a standard genetic algorithm on combinatorial optimization problems. Here we show performance superior to A3C and stochastic gradient descent for transfer learning."}, {"heading": "2. METHODS", "text": ""}, {"heading": "2.1 PathNet Architecture", "text": "A PathNet is a modular deep neural network having L layers with each layer consisting of M modules. Each module is itself a neural network, here either convolutional or\nlinear, followed by a transfer function; rectified linear units are used here. For each layer the outputs of the modules in that layer are summed before being passed into the active modules of the next layer. A module is active if it is present in the path genotype currently being evaluated (see below). A maximum of N distinct modules per layer are permitted in a pathway (typically N = 3 or 4). The final layer is unique and unshared for each task being learned. In the supervised case this is a single linear layer for each task, and in the A3C case each task (e.g. Atari game) has a value function readout and a policy readout (see [12] for a complete description of the A3C algorithm used)."}, {"heading": "2.2 Pathway Evolution: Serial and Parallel", "text": "P genotypes (pathways) are initialized randomly, each genotype is at most a N by L matrix of integers, which describe the active modules in each layer in that pathway. In the serial supervised implementation, a binary tournament selection algorithm is implemented in series as follows. A random genotype is chosen, and its pathway is trained for T epochs, its fitness being the negative classification error during that period of training. Then another random genotype is chosen and its pathway trained for T epochs. A copy of the winning pathway\u2019s genotype overwrites the losing pathways genotype. The copy of the winning pathway\u2019s genotype is then mutated by choosing independently each element with a probability of 1/[N \u00d7L] and adding an integer in the range [\u22122, 2] to it. A local neighbourhood was used to promote spatial localization of network functionality.\nIn the A3C (reinforcement learning) case, all 64 genotypes are evaluated in parallel, one by each of the 64 workers. Therefore pathways restrict the simultaneous updates of parameters by the workers to only their specific subsets, as opposed to the standard A3C algorithm in which all workers update all parameters. The fitness of a genotype is the return accumulated over the T episodes that a worker played using that genotypes pathway. While a worker is evaluating, it writes a large negative fitness to the shared fitness array, so that no genotypes wins a tournament until it has been evaluated. Once the worker has finished T episodes, it chooses B other random genotypes and checks if any of those genotypes have returned a fitness of at least its own fitness. If at least one has, then the highest fit genotype overwrites the current worker\u2019s genotype, and is mutated as above. If no other worker had a genotype with fitness greater than this workers own genotype, then the worker reevaluates its own genotype."}, {"heading": "2.3 Transfer Learning Paradigm", "text": "Once task A has been trained for a fixed period of time or until some error threshold has been reached, the best fit pathway is fixed, which means its parameters are no longer allowed to change. All other parameters not in an optimal path are reinitialized. We found that without reinitialization transfer performance did not exceed that of fine-tuning. In the A3C case (but not the supervised learning cases) the original best fit pathway is always active during the forwards pass of the network, in addition to the newly evolving pathway, but its parameters are not modified by the backwards pass. A new set of random pathways is then initialized and evolved/trained on task B. In both the\nsupervised and reinforcement settings, pathNet is compared with two alternative setups: an independent learning control where the target task is learned de novo, and a fine-tuning control where the second task is learned with the same path that learned the first task (but with a new value function and policy readout)."}, {"heading": "2.4 Binary MNIST classification tasks", "text": "A binary MNIST classification involves distinguishing two classes of MNIST digits from one another, for example 5 verses 6 [11]. To make the task more difficult, salt and pepper noise of 50% is added to the MNIST digits. A transfer experiment involves training and evolving paths on the first task until perfect classification on the training set is achieved.\nAt this point, a new population of path genotypes is initialized and evolved on the second task until perfect performance on the training set is achieved. Between tasks the following modifications are made to the underlying network. The parameters contained in the optimal path evolved on the first task are fixed, and all other parameters are reset to their random initial values. The reported outcome measures are the training times required to reach this classification accuracy. The overall PathNet consists of L = 3 layers. Each layer contains M = 10 linear units with 20 neurons each followed by rectified linear units. Between layers the activations of the units are summed. Reading out from the final layer is a separate linear layer for each task.\nA maximum of 3 of these units per layer can be included in a single pathway. The genotype describing this pathway is a 3\u00d73 matrix of integers in the range [1, 10]. A population of 64 of these genotypes was generated uniformly randomly at the start of each task. The overall algorithm is as follows. Two paths are chosen randomly for evaluation. The evaluation of one path involves training with stochastic gradient descent with learning rate 0.0001 on 50 mini-batches of size 16. The fitness of that pathway is the proportion of correct examples classified on the training set during this training period. Once the fitness of both pathways has been calculated, the pathway with the lower fitness is overwritten by a copy of the pathway with the higher fitness and mutated. Mutation takes place with equal probability 1/(3 \u00d7 3) per element of the genotype, adding a new random integer from range[\u22122, 2] to the current value of that element. This is a binary tournament selection algorithm (B = 2) [7]."}, {"heading": "2.5 CIFAR and SVHN classification tasks", "text": "The larger version of the above network is used to train on CIFAR and cropped SVHN [13] of standard size 28\u00d7 28 withL = 3 and M = 20 modules per layer of 20 neurons each, and with pathways that may contain up to 5 modules per layer. In this case the networks were not trained to some fixed accuracy, but for a fixed period of 500 generations. Each generation consisted of the evaluation of two pathways, each for 50 mini-batches of size 16 images. The performance measure here was the accuracy achieved after this fixed training time. Evidence for positive transfer in this case is given by the second task showing a higher final\naccuracy compared to when it was learned from scratch."}, {"heading": "2.6 Atari games", "text": "We tested whether there was a speedup in learning a second (target) game after having played either Pong, RiverRaid or Seaquest as a source game. The target games were as follows: Alien, Asterix, Boxing, Centipede, Gopher, Hero, JamesBond, Krull, RoadRunner, StarGunner, WizardofWor. These are the same games presented by the authors of Progressive Neural Networks. In this case the A3C algorithm was used with 64 workers running in parallel. The first game is learned for 80M (or 40M) timesteps of training in total across all workers. During this time, workers are evolving pathways through the network. After the first game is learned the population of paths is reinitialized and the workers play the second game for 40M timesteps in total. Between tasks the following modifications are made to the underlying network. The parameters contained in the optimal path evolved on the first task are fixed, and all other parameters are reset to their random initial values. Evolution takes place according to a distributed asynchronous tournament selection algorithm which works as follows. The population of pathways and their fitnesses is stored in a central parameter server. Each worker accesses its own particular index into this population of genotypes corresponding to that worker\u2019s id. While it is first evaluating that pathway it writes -1000 to the fitness array at a position corresponding to it\u2019s id. After T = 10 episodes that worker writes the total sum of rewards obtained into the fitness array. It then observes the fitnesses of B = 20 other random workers and if any worker has a higher fitness than itself, it chooses the highest fitness worker in that set, and copies the pathway genotype to its own location with mutation, and resets its own fitness to -1000. However, if no other worker of the B chosen has a path better than its own, then its own pathway is re-evaluated without resetting the fitness obtained from the previous evaluation. Mutation is as before, with the minor modification that if a location is chosen for mutation, then there is a 20% chance (during the second task) that a module from the optimal pathway evolved in the first task will be incorporated into the genotype. In the variant of PathNet used for this task, the optimal path evolved in the first task is always active during the forwards pass, in this sense, this version resembles progressive nets more closely than the supervised versions presented above. The architecture of the network used consists of a core of L = 4 layers, each with a width of M = 10 modules: namely, 3 convolutional layers where each module consists of 8 kernels, and a final layer of fully connected modules of 50 hidden nodes each. Between layers the feature maps and activations are summed. Reading out from the final layer are two linear layers for each game, one encoding a value function and the other encoding an action policy. Figure 2 shows the network architecture: The first three layers\u2019 modules are shown as green boxes; final layer of modules shown as purple boxes; the between-layer summing modules are shown as blue boxes; and active modules specified by the pathway as red boxes. Readout units not involved in the PathNet itself are shown as circles on the right.\n2.7 Labyrinth Games\nWe investigated the performance of a very similar PathNet architecture on 3 Labyrinth games. Labyrinth is a 3D first person game environment [10]. We used the same settings used for Atari games. We chose three games: \u2019Laser Tag Chasm\u2019 (lt chasm), \u2019Seek Avoid Arena 01\u2019 (seekavoid arena) and \u2019Stairway to Melon 01\u2019 (stairway to melon). lt chasm takes place in a square room with four opponents that must be tagged for points. A chasm in the center of the room must be avoided and shield pick-ups exist that the player must jump to reach. seekavoid arena is a 3D room containing apples and lemons. The player must pick up the apples whilst avoiding the lemons. stairway to melon offers the player two options; the player can either collect a set of small rewards (apples) or they can opt to take a punishment (lemon) in order to reach a melon which results in a higher net reward.\nThe PathNet architecture used for Labyrinth was identical to the one used for Atari with the exception of a moduleduplication mechanism. In the previous models there has not been any actual copying of parameters, only copying of views over a fixed set of parameters. In the Labyrinth model we enabled PathNet to copy the weights of modules to other modules within the same layer, emulating Net2Net [3] or a distillation [9] like operation. To aid this, we measure the extent to which a module contributes to the fitness of all the paths it is in by taking a sliding mean over the fitness of any path which contains that module. Using this measure it is possible to bias a weight-copying operator such that currently more globally useful modules are more likely to be copied across into other modules and hence into other paths. The hyperparameter module duplication rate determines the rate at which modules would be allowed to duplicate themselves. For the Labyrinth games both the first and second tasks were trained for 40M timesteps."}, {"heading": "3. RESULTS", "text": ""}, {"heading": "3.1 Binary MNIST Classification", "text": "Figure 3 shows that with PathNet, learning a source MNIST classification task helps speed up learning in the target MNIST classification task (mean time to solution = 167 generations); greater than the speedup achievable by fine-tuning a fixed path that had initially been trained on the source task (mean time to solution = 229), and also compared to de novo learning of the target task from scratch (mean time to solution = 195). PathNet (bottom graph) learns in fewer generations with less data than both the fine-tuning (middle) and independent learning controls (top). The control networks consisted of exactly the same learning algorithm, but with no evolution of paths, and only one fixed maximum size pathway through the network. The total speedup ratio compared to the independent controls was 1.18.\nVideos showing performance of PathNet can be obtained online at https://goo.gl/oVHMJo. They reveal that the modules in early layers of the big network converge in the population of pathways the quickest, followed by later layers, there being much more exploration and training of all the modules in the final layer. Many modules in the final layer contribute to high fitness, whereas only a few modules in the first layer do so. Thus, a population provides an elegant solution to the exploration /exploitation trade-off in\na layer specific manner. Analysis did not reveal that the speedup ratio was correlated with path overlap as measured by the number of modules in the original optimal path that were present in the population of paths at the end of the second task. This suggests that speedup can be obtained by PathNet both determining when there should be overlap and when there should not be overlap. The fact that on average fine-tuning is slower than independent task learning in MNIST binary classification tasks implies that generally overlap would not be expected to provide speedup. Thus, PathNet can be seen to have done its job by controlling the amount of overlap properly, see Figure 4"}, {"heading": "3.2 CIFAR and SVHN", "text": "In this experiment we compare only the accuracy PathNet obtains after a short fixed number of 500 generations. A fully connected network of this size is generally insufficient to perform well on these datasets. After 500 generations, when learning from scratch with PathNet, cSVHN and CIFAR are learned to 25.5% and 35.3% accuracy on average, see Figure 5. But after the first task has been learned to this accuracy, learning the second task is faster, so when cSVHN and CIFAR are learned as the second task with PathNet, then accuracies of 35.7% and 39.8% are achieved respectively. Thus, both cSVHN and CIFAR are learned faster with PathNet when learned second rather than first."}, {"heading": "3.3 Atari Games", "text": "Transfer performance on PathNet is compared with fixed maximum-size-path de novo training and fine-tuning controls for which a hyperparameter sweep was conducted involving learning rates [0.001,0.0005,0.0001] and entropy costs [0.01, 0.001, 0.0001]. The following hyperparameters were investigated for PathNet: evaluation time T [1,10,50], mutation rate [0.1, 0.01, 0.001] and tournament size B [2, 10, 20].\nIn Figure 6 the performance of PathNet (blue) is compared with independent and fine-tuning controls (green) on 4 target games, having learned RiverRaid first. In all cases the top 5 runs of a hyperparameter search are shown. We generally found that strong selection with tournament sizes of B = 10, T = 10 game episodes per evaluation, and low mutation rates of 0.01-0.001 were optimal, allowing rapid convergence of paths to a single path, followed by exploration of small variants to a path, thus focusing learning on a few parameters, with occasional exploration of novel bypasses. PathNet is superior to controls in these 4 cases.\nFigure 7 shows that PathNet is on average superior to independent learning and fine-tuning over the Atari games we investigated. Compared to a control of 1.0, fine tuning achieves a 1.16 speedup on average whereas PathNet achieves a 1.33 times speedup. Results for transfer on more Atari games can be seen in Figure 12."}, {"heading": "3.4 Labyrinth Games", "text": "PathNet transfer between the three labyrinth games lt chasm, seekavoid arena and stairway to melon is compared with fixed maximum-size-path de novo training and fine-tuning controls for which a hyperparameter sweep was conducted involving mutation rates [ 0.1, 0.01, 0.001], module duplication\nrate [ 0.0, 0.05, 0.1, 0.5] (per episode completed by worker 0) and tournament size B [2, 10]. The learning rate was fixed at 0.001, entropy cost at 0.0001 and evaluation time T at 13.\nFigure 9 shows the mean of the training performance for the top 5 experiments for both PathNet and the fixed-path controls. Each row represents a different source game and each column a different target game. Where the source game and target game are the same the graph shows the results of learning the game de novo from scratch. Figure 11 shows that in some cases the module duplication operation produces improved performance compared to standard PathNet.\nIn several cases (transfer to lt chasm and transfer from lt chasm to seekavoid arena) PathNet learns the second task faster than fine-tuning. Interestingly, PathNet also performs better than fine-tuning when learning stairway to melon and seekavoid arena from scratch.\nResults of the best runs from a hyperparameter search are summarized in Figure 10. Here performance is evaluated by measuring the area under the learning curve (average score per episode during training), rather than final score. The numbers in the table show the relative performance of an architecture learning a target task (each column) compared with an independent baseline with a fixed maximum size path trained from scratch only on the target task. The controls are labelled as \u2019Fixed path de novo from scratch\u2019 (top row) and is 1 for each column (target task). A ratio in a column >1 represents the speedup when learning that column\u2019s target task, and <1 is a slowdown. The three rows below the de novo control show the fine-tuning results between the three games. The first row in the PathNet results show performance learning the individual games from scratch and the three rows below that show the PathNet transfer results between the three games.\nOn transferring to lt chasm both fine-tuning and PathNet perform worse than the control de novo learning. On the the other two games both exhibit positive transfer performance. The average performance ratio for fine-tuning for transfer across all the game combinations is 1.00 (2 d.p.), essentially no faster than learning from scratch. The average performance ratio for PathNet is 1.26 (2 d.p.); largely due to the good performance transferring from seekavoid arena to stairway to melon.\nWe also compared PathNet to independent and fine-tuning controls over the same sweep of 243 hyperparameters as used in the Atari experiments described above. On the Labyrinth level seekavoid arena in which the agent must collect apples but avoid lemons we found that the PathNet had significantly higher mean performance than control runs, both when learning seekavoid arena from scratch compared to the de novo controls, and for relearning from the same task, compared to fine-tuning from a network that had previously learned seekavoid arena, see Figure 8."}, {"heading": "4. CONCLUSION", "text": "PathNet extends our original work on the Path Evolution Algorithm [4] to Deep Learning whereby the weights\nand biases of the network are learned by gradient descent, but evolution determines which subset of parameters is to be trained. We have shown that PathNet is capable of sustaining transfer learning on at least four tasks in both the supervised and reinforcement learning settings.\nPathNet may be thought of as implementing a form of \u2018evolutionary dropout\u2019 in which instead of randomly dropping out units and their connections, dropout samples or \u2018thinned networks\u2019 are evolved [21]. PathNet has the added advantage that dropout frequency is emergent, because the population converges faster at the early layers of the network than in the later layers. PathNet also resembles \u2018evolutionary swapout\u2019 [20], in fact we have experimented with having standard linear modules, skip modules and residual modules in the same layer and found that path evolution was capable of discovering effective structures within this diverse network. PathNet is related also to recent work on convolutional neural fabrics, but there the whole network is always used and so the principle cannot scale to giant networks [18]. Other approaches to combining evolution and learning have involved parameter copying, whereas there is no such copying in the current implementation of PathNet [1][3].\nWhilst we have only demonstrated PathNet in a fairly small network, the principle can scale to much larger neural networks with more efficient implementations of pathway gating. This will allow extension to multiple tasks. We also wish to try PathNet on other RL tasks which may be more suitable for transfer learning than Atari, for example continuous robotic control problems. Further investigation is required to understand the extent to which PathNet may be superior to using fixed paths. Firstly, a possibility is that mutable paths provide a more useful form of diverse exploration in RL tasks [15]. Secondly, it is possible that a larger number of workers can be used in A3C because if each worker can determine which parameters to update, there may be selection for pathways that do not interfere with each other.\nWe are still investigating the potential benefits of module duplication. see Supplementary Video https://goo.gl/oVHMJo. Using this measure it is possible to bias the mutation operator such that currently more globally useful modules are more likely to be slotted into other paths. Further work is also to be carried out in multi-task learning which has not yet been addressed in this paper.\nFinally, it is always possible and sometimes desirable to replace evolutionary variation operators with variation operators learned by reinforcement learning. A tournament selection algorithm with mutation is only the simplest way to achieve adaptive paths. It is clear that more sophisticated methods such as policy gradient methods may be used to learn the distribution of pathways as a function of the long term returns obtained by paths, and as a function of a task description input. This may be done through a softer form of gating than used by PathNet here. Furthermore, a population (ensemble) of soft gate matrices may be maintained and an RL algorithm may be permitted to \u2019mutate\u2019 these values.\nThe operations of PathNet resemble those of the Basal Ganglia, which we propose determines which subsets of the\ncortex are to be active and trainable as a function of goal/subgoal signals from the prefrontal cortex, a hypothesis related to others in the literature [8] [14] [6]."}, {"heading": "5. ACKNOWLEDGMENTS", "text": "Thanks to Hubert Soyer, Arka Pal, Gabriel Dulac-Arnold, Gabriel Barth-Maron, Matteo Hessel, Alban Rustrani, Stephen Gaffney, Joel Leibo, Eors Szathmary"}, {"heading": "6. REFERENCES", "text": "[1] J. E. Auerbach, C. Fernando, and D. Floreano. Online\nextreme evolutionary learning machines. In Artificial Life 14: Proceedings of the Fourteenth International Conference on the Synthesis and Simulation of Living Systems, number EPFL-CONF-200273, pages 465\u2013472. The MIT Press, 2014.\n[2] R. Caruana. Multitask learning. In Learning to learn, pages 95\u2013133. Springer, 1998.\n[3] T. Chen, I. Goodfellow, and J. Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641, 2015.\n[4] C. Fernando, V. Vasas, E. Szathma\u0301ry, and P. Husbands. Evolvable neuronal paths: a novel basis for information and search in the brain. PloS one, 6(8):e23534, 2011.\n[5] C. T. Fernando, E. Szathmary, and P. Husbands. Selectionist and evolutionary approaches to brain function: a critical appraisal. Frontiers in computational neuroscience, 6:24, 2012.\n[6] M. J. Frank, B. Loughry, and R. C. Oa\u0302A\u0306Z\u0301Reilly. Interactions between frontal cortex and basal ganglia in working memory: a computational model. Cognitive, Affective, & Behavioral Neuroscience, 1(2):137\u2013160, 2001.\n[7] I. Harvey. The microbial genetic algorithm. In Advances in artificial life. Darwin Meets von Neumann, pages 126\u2013133. Springer, 2011.\n[8] T. E. Hazy, M. J. Frank, and R. C. O\u2019Reilly. Towards an executive without a homunculus: computational models of the prefrontal cortex/basal ganglia system. Philosophical Transactions of the Royal Society B: Biological Sciences, 362(1485):1601\u20131613, 2007.\n[9] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n[10] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.\n[11] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\n[12] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. arXiv preprint arXiv:1602.01783, 2016.\n[13] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning.\n[14] R. C. O\u2019Reilly and M. J. Frank. Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia. Neural computation, 18(2):283\u2013328, 2006.\n[15] I. Osband, C. Blundell, A. Pritzel, and B. Van Roy. Deep exploration via bootstrapped dqn. arXiv preprint arXiv:1602.04621, 2016.\n[16] M. B. Ring. Continual Learning in Reinforcement Environments. PhD thesis, University of Texas at Austin, 1994.\n[17] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.\n[18] S. Saxena and J. Verbeek. Convolutional neural fabrics. arXiv preprint arXiv:1606.02492, 2016.\n[19] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: A sparsely-gated mixture-of-experts layer. 2017. [ICAR submission].\n[20] S. Singh, D. Hoiem, and D. Forsyth. Swapout: Learning an ensemble of deep architectures. arXiv preprint arXiv:1605.06465, 2016.\n[21] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.\n[22] M. E. Taylor and P. Stone. An introduction to intertask transfer for reinforcement learning. AI Magazine, 32(1):15, 2011.\nDeepGrow"}], "references": [{"title": "Online extreme evolutionary learning machines", "author": ["J.E. Auerbach", "C. Fernando", "D. Floreano"], "venue": "Artificial Life 14: Proceedings of the Fourteenth International Conference on the Synthesis and Simulation of Living Systems, number EPFL-CONF-200273, pages 465\u2013472. The MIT Press,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Learning to learn, pages 95\u2013133. Springer,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Net2net: Accelerating learning via knowledge transfer", "author": ["T. Chen", "I. Goodfellow", "J. Shlens"], "venue": "arXiv preprint arXiv:1511.05641,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Evolvable neuronal paths: a novel basis for information and search in the brain", "author": ["C. Fernando", "V. Vasas", "E. Szathm\u00e1ry", "P. Husbands"], "venue": "PloS one, 6(8):e23534,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Selectionist and evolutionary approaches to brain function: a critical appraisal", "author": ["C.T. Fernando", "E. Szathmary", "P. Husbands"], "venue": "Frontiers in computational neuroscience, 6:24,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Interactions between frontal cortex and basal ganglia in working memory: a computational model", "author": ["M.J. Frank", "B. Loughry", "R.C. O\u00e2\u0102\u0179Reilly"], "venue": "Cognitive, Affective, & Behavioral Neuroscience, 1(2):137\u2013160,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "The microbial genetic algorithm", "author": ["I. Harvey"], "venue": "Advances in artificial life. Darwin Meets von Neumann, pages 126\u2013133. Springer,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards an executive without a homunculus: computational models of the prefrontal cortex/basal ganglia system", "author": ["T.E. Hazy", "M.J. Frank", "R.C. O\u2019Reilly"], "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["M. Jaderberg", "V. Mnih", "W.M. Czarnecki", "T. Schaul", "J.Z. Leibo", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1611.05397,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia", "author": ["R.C. O\u2019Reilly", "M.J. Frank"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Deep exploration via bootstrapped dqn", "author": ["I. Osband", "C. Blundell", "A. Pritzel", "B. Van Roy"], "venue": "arXiv preprint arXiv:1602.04621,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Continual Learning in Reinforcement Environments", "author": ["M.B. Ring"], "venue": "PhD thesis, University of Texas at Austin,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Progressive neural networks", "author": ["A.A. Rusu", "N.C. Rabinowitz", "G. Desjardins", "H. Soyer", "J. Kirkpatrick", "K. Kavukcuoglu", "R. Pascanu", "R. Hadsell"], "venue": "arXiv preprint arXiv:1606.04671,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional neural fabrics", "author": ["S. Saxena", "J. Verbeek"], "venue": "arXiv preprint arXiv:1606.02492,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Outrageously large neural networks: A sparsely-gated mixture-of-experts", "author": ["N. Shazeer", "A. Mirhoseini", "K. Maziarz", "A. Davis", "Q. Le", "G. Hinton", "J. Dean"], "venue": "[ICAR submission]", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Swapout: Learning an ensemble of deep architectures", "author": ["S. Singh", "D. Hoiem", "D. Forsyth"], "venue": "arXiv preprint arXiv:1605.06465,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to intertask transfer for reinforcement learning", "author": ["M.E. Taylor", "P. Stone"], "venue": "AI Magazine, 32(1):15,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "Each agent may itself be controlled by an arbitrarily complex reinforcement learning algorithm, but here we chose the very simplest possible \u2018agent\u2019, a unit of evolution [4].", "startOffset": 170, "endOffset": 173}, {"referenceID": 20, "context": "The framework for AGI described above includes aspects of transfer learning [22], continual learning [16] and multitask learning [2].", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "The framework for AGI described above includes aspects of transfer learning [22], continual learning [16] and multitask learning [2].", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "The framework for AGI described above includes aspects of transfer learning [22], continual learning [16] and multitask learning [2].", "startOffset": 129, "endOffset": 132}, {"referenceID": 17, "context": "Our work shares a motivation with a recent paper \u201cOutrageously large neural networks\u201d in which the authors write that \u201cthe capacity of a neural network to absorb information is limited by its number of parameters\u201d [19].", "startOffset": 214, "endOffset": 218}, {"referenceID": 16, "context": "Our work is related also to \u201cConvolutional Neural Fabrics\u201d in which connection strengths between modules in the fabric are learned, but where (unlike PathNet) the whole fabric is used all the time [18].", "startOffset": 197, "endOffset": 201}, {"referenceID": 15, "context": "PathNets evolve a population of pathways through a neural network that scaffolds and channels any desired gradientdescent-based learning algorithm towards a limited subset of the neural network\u2019s parameters and then fixes these parameters after learning so that functionality can never be lost; it resembles progressive neural networks, in that catastrophic forgetting is prevented by design [17].", "startOffset": 392, "endOffset": 396}, {"referenceID": 11, "context": "64 asynchronously updated workers that simultaneously share and update the parameters of a single network[12].", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "The concept of the PathNet was first conceived of within the framework of Darwinian Neurodynamics as an attempt to envisage how evolutionary algorithms could be implemented in the brain [5].", "startOffset": 186, "endOffset": 189}, {"referenceID": 3, "context": "However, in that original work both the topology and the weights of the path were evolved and there was no gradient descent learning [4].", "startOffset": 133, "endOffset": 136}, {"referenceID": 11, "context": "Atari game) has a value function readout and a policy readout (see [12] for a complete description of the A3C algorithm used).", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "A binary MNIST classification involves distinguishing two classes of MNIST digits from one another, for example 5 verses 6 [11].", "startOffset": 123, "endOffset": 127}, {"referenceID": 0, "context": "The genotype describing this pathway is a 3\u00d73 matrix of integers in the range [1, 10].", "startOffset": 78, "endOffset": 85}, {"referenceID": 9, "context": "The genotype describing this pathway is a 3\u00d73 matrix of integers in the range [1, 10].", "startOffset": 78, "endOffset": 85}, {"referenceID": 6, "context": "This is a binary tournament selection algorithm (B = 2) [7].", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "Labyrinth is a 3D first person game environment [10].", "startOffset": 48, "endOffset": 52}, {"referenceID": 2, "context": "In the Labyrinth model we enabled PathNet to copy the weights of modules to other modules within the same layer, emulating Net2Net [3] or a distillation [9] like operation.", "startOffset": 131, "endOffset": 134}, {"referenceID": 8, "context": "In the Labyrinth model we enabled PathNet to copy the weights of modules to other modules within the same layer, emulating Net2Net [3] or a distillation [9] like operation.", "startOffset": 153, "endOffset": 156}, {"referenceID": 0, "context": "The following hyperparameters were investigated for PathNet: evaluation time T [1,10,50], mutation rate [0.", "startOffset": 79, "endOffset": 88}, {"referenceID": 9, "context": "The following hyperparameters were investigated for PathNet: evaluation time T [1,10,50], mutation rate [0.", "startOffset": 79, "endOffset": 88}, {"referenceID": 1, "context": "001] and tournament size B [2, 10, 20].", "startOffset": 27, "endOffset": 38}, {"referenceID": 9, "context": "001] and tournament size B [2, 10, 20].", "startOffset": 27, "endOffset": 38}, {"referenceID": 18, "context": "001] and tournament size B [2, 10, 20].", "startOffset": 27, "endOffset": 38}, {"referenceID": 1, "context": "5] (per episode completed by worker 0) and tournament size B [2, 10].", "startOffset": 61, "endOffset": 68}, {"referenceID": 9, "context": "5] (per episode completed by worker 0) and tournament size B [2, 10].", "startOffset": 61, "endOffset": 68}, {"referenceID": 3, "context": "PathNet extends our original work on the Path Evolution Algorithm [4] to Deep Learning whereby the weights and biases of the network are learned by gradient descent, but evolution determines which subset of parameters is to be trained.", "startOffset": 66, "endOffset": 69}, {"referenceID": 19, "context": "PathNet may be thought of as implementing a form of \u2018evolutionary dropout\u2019 in which instead of randomly dropping out units and their connections, dropout samples or \u2018thinned networks\u2019 are evolved [21].", "startOffset": 196, "endOffset": 200}, {"referenceID": 18, "context": "PathNet also resembles \u2018evolutionary swapout\u2019 [20], in fact we have experimented with having standard linear modules, skip modules and residual modules in the same layer and found that path evolution was capable of discovering effective structures within this diverse network.", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "PathNet is related also to recent work on convolutional neural fabrics, but there the whole network is always used and so the principle cannot scale to giant networks [18].", "startOffset": 167, "endOffset": 171}, {"referenceID": 0, "context": "Other approaches to combining evolution and learning have involved parameter copying, whereas there is no such copying in the current implementation of PathNet [1][3].", "startOffset": 160, "endOffset": 163}, {"referenceID": 2, "context": "Other approaches to combining evolution and learning have involved parameter copying, whereas there is no such copying in the current implementation of PathNet [1][3].", "startOffset": 163, "endOffset": 166}, {"referenceID": 13, "context": "Firstly, a possibility is that mutable paths provide a more useful form of diverse exploration in RL tasks [15].", "startOffset": 107, "endOffset": 111}, {"referenceID": 7, "context": "cortex are to be active and trainable as a function of goal/subgoal signals from the prefrontal cortex, a hypothesis related to others in the literature [8] [14] [6].", "startOffset": 153, "endOffset": 156}, {"referenceID": 12, "context": "cortex are to be active and trainable as a function of goal/subgoal signals from the prefrontal cortex, a hypothesis related to others in the literature [8] [14] [6].", "startOffset": 157, "endOffset": 161}, {"referenceID": 5, "context": "cortex are to be active and trainable as a function of goal/subgoal signals from the prefrontal cortex, a hypothesis related to others in the literature [8] [14] [6].", "startOffset": 162, "endOffset": 165}], "year": 2017, "abstractText": "For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks. Agents are pathways (views) through the network which determine the subset of parameters that are used and updated by the forwards and backwards passes of the backpropogation algorithm. During learning, a tournament selection genetic algorithm is used to select pathways through the neural network for replication and mutation. Pathway fitness is the performance of that pathway measured according to a cost function. We demonstrate successful transfer learning; fixing the parameters along a path learned on task A and re-evolving a new population of paths for task B, allows task B to be learned faster than it could be learned from scratch or after fine-tuning. Paths evolved on task B re-use parts of the optimal path evolved on task A. Positive transfer was demonstrated for binary MNIST, CIFAR, and SVHN supervised learning classification tasks, and a set of Atari and Labyrinth reinforcement learning tasks, suggesting PathNets have general applicability for neural network training. Finally, PathNet also significantly improves the robustness to hyperparameter choices of a parallel asynchronous reinforcement learning algorithm (A3C).", "creator": "LaTeX with hyperref package"}}}