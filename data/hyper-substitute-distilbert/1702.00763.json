{"id": "1702.00763", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2017", "title": "Natasha: Faster Non-Convex Stochastic Optimization via Strongly Non-Convex Parameter", "abstract": "given if non - convex map $ f ( v ) $ infinity is labeled irregular z $ n $ smooth \u0192, we examine stochastic sub - century methods to find finite approximate stationary points. geometric performance of completely new methods limits on exactly smallest ( exact ) vector $ - \\ \u03b1 $ not the hessian. this fact $ \\ sigma $ captures how strongly np - convex $ f ( 7 ) $ is, and gets equal to ordinary expected curve parameter for general optimization.", "histories": [["v1", "Thu, 2 Feb 2017 17:45:09 GMT  (1325kb,D)", "http://arxiv.org/abs/1702.00763v1", null], ["v2", "Mon, 27 Feb 2017 03:50:25 GMT  (1040kb,D)", "http://arxiv.org/abs/1702.00763v2", "typos"], ["v3", "Tue, 5 Sep 2017 09:37:06 GMT  (1047kb,D)", "http://arxiv.org/abs/1702.00763v3", "Corrected several typos and polished writing"]], "reviews": [], "SUBJECTS": "math.OC cs.DS cs.LG stat.ML", "authors": ["zeyuan allen-zhu"], "accepted": true, "id": "1702.00763"}, "pdf": {"name": "1702.00763.pdf", "metadata": {"source": "CRF", "title": "Natasha: Faster Stochastic Non-Convex Optimization Via Strongly Non-Convex Parameter", "authors": ["Zeyuan Allen-Zhu"], "emails": ["zeyuan@csail.mit.edu"], "sections": [{"heading": null, "text": "Our methods outperform the best known results for a wide range of \u03c3, and can also be used to find approximate local minima.\nIn particular, we find an interesting dichotomy: there exists a threshold \u03c30 so that the fastest methods for \u03c3 > \u03c30 and for \u03c3 < \u03c30 have drastically different behaviors: the former scales with n2/3 and the latter scales with n3/4."}, {"heading": "1 Introduction", "text": "We study the fundamental problem of composite non-convex minimization:\nmin x\u2208Rd\n{ F (x) def = \u03c8(x) + f(x) def = \u03c8(x) + 1\nn\nn\u2211\ni=1\nfi(x) }\n(1.1)\nwhere each fi(x) is nonconvex but smooth, and \u03c8(\u00b7) is proper convex and relatively simple. We are interested in finding a point x that is an approximate local minimum of F (x).\n\u2022 The finite-sum structure f(x) = 1n \u2211n\ni=1 fi(x) arises prominently in large-scale problems and especially in machine learning. In particular, when minimizing loss over a training set, each example i in the set can correspond to one loss function fi(\u00b7) in the summation. This finite-sum structure allows one to perform stochastic gradient descent with respect to a random \u2207fi(x). \u2022 The additional term \u03c8(x), usually called the proximal term, adds more flexibility to the model.\nFor instance, if \u03c8(x) is the indicator function of a convex set, then problem (1.1) becomes constraint minimization; if \u03c8(x) = \u2016x\u20161, then we can allow problem (1.1) to perform feature selection. In general, \u03c8(x) has to be a simple function where the projection operation arg minx{\u03c8(x) + 12\u03b7\u2016x\u2212x0\u20162} is efficiently computable. At a first reading of this paper, one can assume \u03c8(x) \u2261 0 for simplicity.\nIn many interesting practical problems \u2014such as training neural nets and classifications with sigmoid loss, see [3] for details\u2014 neither fi(x) or the overall f(x) is convex. However, there are very limited research for this challenging non-convex family of problems.\nar X\niv :1\n70 2.\n00 76\n3v 1\n[ m\nat h.\nO C\n] 2\nF eb"}, {"heading": "1.1 Strongly Non-Convex Optimization", "text": "We mainly focus on the case when each fi(x) is L-smooth, meaning all the eigenvalues of \u22072fi(x) lie in [\u2212L,L], and their average f(x) = 1n \u2211n i=1 fi(x) is \u03c3-strongly nonconvex, meaning\nall the eigenvalues of \u22072f(x) lie in [\u2212\u03c3, L] for some \u03c3 \u2208 [0, L].1\nWe emphasize here that this parameter \u03c3 is analogous to the strong-convexity parameter \u00b5 for convex optimization, where all the eigenvalues of \u22072f(x) lie in [\u00b5,L] for some \u00b5 > 0.\nWe wish to find an \u03b5-approximate stationary point (a.k.a. critical point) of F (x), that is any point x satisfying \u2016G\u03b7(x)\u2016 \u2264 \u03b5 where G(x) is the so-called gradient mapping of F (x). Note that in the special case of \u03c8(\u00b7) \u2261 0, gradient mapping G(x) is the same as gradient \u2207f(x), so x satisfies \u2016\u2207f(x)\u2016 \u2264 \u03b5.\nSince f(\u00b7) is \u03c3-strongly nonconvex, any \u03b5-approximate stationary point is automatically also an (\u03b5, \u03c3)-approximate local minimum \u2014 meaning that the Hessian of the output point \u22072f(x) \u2212\u03c3I is approximately positive semidefinite (PSD).\nMotivations and Remarks\n\u2022 We focus on strongly non-convex optimization because introducing this parameter \u03c3 allows us to perform a more refined study of non-convex optimization. If \u03c3 equals L then L-strongly nonconvex optimization is equivalent to the general non-convex optimization.\n\u2022 We focus only on finding stationary points as opposed to local minima, because in a recent study \u2014see Appendix A\u2014 researchers have shown that finding (\u03b5, \u03b4)-approximate local minima reduces to finding \u03b5-approximate stationary points in an O(\u03b4)-strongly nonconvex function.\n\u2022 Parameter \u03c3 is usually not constant and can be much smaller than L. In particular, second-order literatures usually find (\u03b5, \u221a \u03b5)-approximate local minima [18], and this corresponds to \u03c3 = \u221a \u03b5."}, {"heading": "1.2 Known Results", "text": "Despite the widespread use of nonconvex models in machine learning and related fields, our understanding to non-convex optimization is still very limited. Until recently, nearly all research papers have been mostly focusing on either \u03c3 = 0 so f(x) is convex, or \u03c3 = L so f(x) is simply L-smooth:\n\u2022 If \u03c3 = 0, the accelerated SVRG method [8, 21] find a point x satisfying F (x) \u2212 F (x\u2217) \u2264 \u03b5, in gradient complexity O\u0303 ( n+ n3/4 \u221a L/\u03b5 ) .2\n\u2022 If \u03c3 = L, the SVRG method [3] finds an \u03b5-approximate stationary point of F (x) with gradient complexity O(n+ n2/3L/\u03b52).\n\u2022 If \u03c3 = L, gradient descent finds an \u03b5-approximate stationary point of F (x) with gradient complexity O(nL/\u03b52).\nThroughout this paper, we refer to gradient complexity as the total number of stochastic gradient computations \u2207fi(x) and proximal computations y \u2190 Prox\u03c8,\u03b7(x) def= arg miny{\u03c8(y) + 12\u03b1\u2016y\u2212x\u20162}.3\nVery recently, it was observed by two independent groups [1, 9] \u2014although implicitly, see Section 2.1\u2014 that for solving the \u03c3-strongly nonconvex problem, one can repeatedly regularize F (x) to make it \u03c3-strongly convex, and then apply the accelerated SVRG method to minimize this new regularized function. Under mild assumption \u03c3 \u2265 \u03b52, this simple approach\n1This definition also applies to functions f(x) that are not twice differentiable, see Section 2 for details. 2We use the O\u0303 notation to hide poly-logarithmic factors in n,L, 1/\u03b5. 3Some authors also refer to them as incremental first-order oracle (IFO) and proximal oracle (PO) calls. In most machine learning applications, each IFO and PO call can be implemented to run in time O(d) where d is the dimension of the model, or even in time O(s) if s is the average sparsity of the data vectors.\n\u2022 finds an \u03b5-approximate stationary point in gradient complexity O\u0303 ( n\u03c3+n3/4 \u221a L\u03c3\n\u03b52\n) .\nWe call this method repeatSVRG in this paper. Unfortunately, repeatSVRG is even slower than the vanilla SVRG for \u03c3 = L by a factor n1/3.\n1.3 Our New Results\nIn this paper, we identify an interesting dichotomy with respect to the spectrum of the nonconvexity parameter \u03c3 \u2208 [0, L]. In particular, we showed that if \u03c3 \u2264 L/\u221an, then our new method Natasha finds an \u03b5-approximate stationary point of F (x) in gradient complexity\nO ( n log 1\n\u03b5 + n2/3(L2\u03c3)1/3 \u03b52\n) .\nIn other words, together with repeatSVRG, we have improved the best known gradient complexity for \u03c3-stringly nonconvex optimization to4\nO\u0303 ( min {n3/4\n\u221a L\u03c3\n\u03b52 , n2/3(L2\u03c3)1/3 \u03b52\n})\nand the first term in the min is smaller if \u03c3 > L/ \u221a n and the second term is smaller if \u03c3 < L/ \u221a n. We illustrate our performance improvement in Figure 1. Note that our result matches that of SVRG [3] for \u03c3 = L, and has a much simpler analysis.\nAdditional Results. One can take a step further and ask what if each function fi(x) is (`1, `2)- smooth for parameters `1, `2 \u2265 \u03c3. This means all the eigenvalues of \u22072fi(x) lie in [\u2212`2, `1].\nWe show that a variant of our method, which we call Natashafull, solves this more refined\nproblem of (1.1) with total gradient complexity O ( n log 1\u03b5 + n2/3(`1`2\u03c3)1/3 \u03b52 ) as long as `1`2 \u03c32 \u2264 n2. Remark 1.1. In many applications, `1 and `2 can be of different magnitudes. Perhaps the most influential example is finding the leading eigenvector of a symmetric matrix. Using the so-called shift-and-invert reduction [12], computing leading eigenvector reduces to a convex version of problem (1.1) where each fi(x) is (\u03bb, 1)-smooth for some \u03bb 1. Other examples include all the applications that are built on shift-and-invert, including high rank SVD/PCA [5], canonical component analysis [4], online matrix learning [6], and approximate local minima algorithms [1, 9]."}, {"heading": "1.4 Our Techniques", "text": "We first recall the main ideas of stochastic variance-reduced gradient methods, such as SVRG [14]. The SVRG method keeps a snapshot point x\u0303 for every epoch of n iterations, and compute the full gradient \u2207f(x\u0303) only for snapshots. Then, for each iteration at point xt, SVRG defines gradient estimator \u2207\u0303 = \u2207fi(xt) \u2212\u2207fi(x\u0303) +\u2207f(x\u0303) which satisfies Ei[\u2207\u0303] = \u2207f(xt), and performs proximal update xt+1 \u2190 Prox\u03c8,\u03b1 ( xt \u2212 \u03b1\u2207\u0303 ) for some learning rate \u03b1.5\nNote that the epoch length of SVRG is always n (or a constant multiple of n in practice), because this ensures the computation of \u2207\u0303 is of amortized gradient complexity O(1). The per-iteration complexity of SVRG is thus the same as the traditional stochastic gradient descent (SGD).\n4We remark here that this is under mild assumptions for \u03b5 being sufficiently small. For instance, the result of [1, 9] requires \u03b52 \u2264 \u03c3. In our result, the term n log 1\n\u03b5 disappears when \u03b56 \u2264 L2\u03c3/n.\n5Recall that if \u03c8(\u00b7) \u2261 0 then we would have xt+1 \u2190 xt \u2212 \u03b1\u2207\u0303.\nIn nearly all the aforementioned results for strongly nonconvex optimization, researchers have either directly applied SVRG [3, 19] (for the case \u03c3 = L), or repeatedly applied SVRG [1, 9] (for general \u03c3 \u2208 [0, L]). This puts some limitation in the algorithmic design, because SVRG requires each epoch to be of length exactly n.\nIn this paper, we propose Natasha and Natashafull, two methods that are no longer blackbox reductions to SVRG. Both of them still divide iterations into epochs of length n, and compute gradient estimators \u2207\u0303 the same way as SVRG. However, we do not apply compute xt\u2212\u03b1\u2207\u0303 directly.\n\u2022 In our base algorithm Natasha, we further divide each epoch into p sub-epochs, each with a starting vector x\u0302. Then, we replace the use of \u2207\u0303 with \u2207\u0303 + \u03c3(xt \u2212 x\u0302). This is equivalent to saying that we replace f(x) with its regularized version f(x) + \u03c3\u2016x\u2212 x\u0302\u20162, with x\u0302 varying across sub-epochs. We provide pseudocode in Algorithm 1 and illustrate it in Figure 2.\nWe view this additional term \u03c3(xt \u2212 x\u0302) as a type of retraction, which stabilizes the algorithm by moving the vector a bit in the backward direction towards x\u0302.\n\u2022 In our full algorithm Natashafull, we add one more ingredient on top of Natasha. That is, we perform updates zt+1 \u2190 Prox\u03c8,\u03b1(zt \u2212 \u03b1\u2207\u0303) with respect to a different sequence {zt}, and then define xt = 1 2zt + 1 2 x\u0303 and compute gradient estimators \u2207\u0303 at points xt. We provide pseudocode\nin Algorithm 2 and illustrate it in Figure 3.\nWe view this averaging xt = 1 2zt + 1 2 x\u0303 as another type of retraction, which stabilizes the algorithm by moving the vector a bit in the backward direction towards x\u0303. The technique of having the gradients computed at a point xt but moving with respect to a different sequence zt is related to the Katyusha momentum recently developed for convex optimization [2]."}, {"heading": "1.5 Other Related Work", "text": "Methods based on variance-reduced stochastic gradients were first introduced for convex optimization. The first such method is SAG by Schmidt et al [20]. The two most popular choices for gradient estimators are the SVRG-like one we adopted in this paper (independently introduced by [14, 22], and the SAGA-like one introduced by [10]. In nearly all applications, the results proven for SVRG-like estimators and SAGA-like estimators are simply exchangeable (therefore, the results of this paper naturally generalize to SAGA-like estimators as well).\nThe first \u201cnon-convex use\u201d of variance reduction is by Shalev-Shwartz [21] who assumes that each fi(x) is non-convex but their average f(x) is still convex. This result has been slightly improved to several more refined settings [8]. The first truly non-convex use of variance reduction (i.e., for f(x) being also non-convex) is independently by [3] and [19]. First-order algorithms only find stationary points (unless there is sufficient assumption on the randomness of the data), and converge no faster than 1/\u03b52.\nWhen the second-order Hessian information is used, one can (1) find local minima instead of stationary points, and (2) improve the 1/\u03b52 rate to 1/\u03b51.5. The first such result is by cubic regularized Newton\u2019s method [18]; however, its per-iteration complexity is very slow. Very recently, two independent groups of authors tackled this problem from a somewhat similar viewpoint [1, 9]: if the computation of Hessian-vector multiplications (i.e., ( \u22072fi(x) ) v) is on the same order of the\ncomputation of gradients \u2207fi(x),6 then one can obtain a (\u03b5, \u221a \u03b5)-approximate local minimum in gradient complexity O\u0303 ( n \u03b51.5 + n 3/4 \u03b51.75 ) , if we use big-O to also hide dependencies on the smoothness\n6A lot of interesting problems satisfy this property, including training neural nets.\nparameters.7 Although Carmon et al. [9] only stated a complexity of O\u0303 (\nn \u03b51.75\n) in the non-stochastic\nsetting, their result generalizes to our stated complexity in the stochastic setting. As we have argued in Appendix A, both these methods reduce the problem of finding (\u03b5, \u221a \u03b5)-approximate local minima to that of finding \u03b5-approximate stationary points in \u221a \u03b5-strongly nonconvex functions.\nOther related papers include Ge et al. [13] where the authors showed that a noise-injected version of SGD converges to local minima instead of critical points, as long as the underlying function is \u201cstrict-saddle.\u201d Their theoretical running time is a large polynomial in the dimension. Lee et al. [15] showed that gradient descent, starting from a random point, almost surely converges to a local minimum of a \u201cstrict-saddle\u201d function. The rate of convergence required is somewhat unknown."}, {"heading": "2 Preliminaries", "text": "Throughout this paper, we denote by \u2016 \u00b7 \u2016 the Euclidean norm. We use i \u2208R [n] to denote that i is generated from [n] = {1, 2, . . . , n} uniformly at random. We denote by \u2207f(x) the full gradient of function f if it is differentiable, and \u2202f(x) any subgradient if f is only Lipschitz continuous at point x. We let x\u2217 be any minimizer of F (x).\nRecall some definitions on strong convexity (SC), strongly nonconvexity, and smoothness.\nDefinition 2.1. For a function f : Rd \u2192 R, \u2022 f is \u03c3-strongly convex if \u2200x, y \u2208 Rd, it satisfies f(y) \u2265 f(x) + \u3008\u2202f(x), y \u2212 x\u3009+ \u03c32 \u2016x\u2212 y\u20162. \u2022 f is \u03c3-strongly nonconvex if \u2200x, y \u2208 Rd, it satisfies f(y) \u2265 f(x) + \u3008\u2202f(x), y \u2212 x\u3009 \u2212 \u03c32 \u2016x\u2212 y\u20162. \u2022 f is (`1, `2)-smooth if \u2200x, y \u2208 Rd, it satisfies\nf(x) + \u3008\u2207f(x), y \u2212 x\u3009+ `12 \u2016x\u2212 y\u20162 \u2265 f(y) \u2265 f(x) + \u3008\u2207f(x), y \u2212 x\u3009 \u2212 `22 \u2016x\u2212 y\u20162 . \u2022 f is L-smooth if it is (L,L)-smooth.\nThe (`1, `2)-smoothness parameters were introduced in [8] to tackle the convex setting of problem (1.1). The notion of strong nonconvexity is also known as \u201clower smoothness [8]\u201d or \u201calmost convexity [9]\u201d. We refrain from using the name \u201calmost convexity\u201d because it coincides with several other definitions in optimization literatures.\nDefinition 2.2. Given a parameter \u03b7 > 0, the gradient mapping of F (\u00b7) in (1.1) at point x is\nG\u03b7(x) def= 1\n\u03b7\n( x\u2212 x\u2032 ) where x\u2032 = arg min\ny\n{ \u03c8(y) + \u3008\u2207f(x), y\u3009+ 1\n2\u03b7 \u2016y \u2212 x\u20162\n}"}, {"heading": "In particular, if \u03c8(\u00b7) \u2261 0, then G\u03b7(x) \u2261 \u2207f(x).", "text": "The following theorem for the SVRG method can be found for instance in [8], which is built on top of the results [11, 16, 21]:\nTheorem 2.3 (SVRG). Let G(y) def = \u03c8(y) + 1n \u2211n i=1 gi(y) be \u03c3-strongly convex, then the SVRG method finds a point y satisfying G(y)\u2212G(y\u2217) \u2264 \u03b5 \u2022 with gradient complexity is O ( (n+ L 2\n\u03c32 ) log 1\u03b5 ) , if each gi(\u00b7) is L-smooth (for L \u2265 \u03c3); or\n\u2022 with gradient complexity is O ( (n+ `1`2\n\u03c32 ) log 1\u03b5 ) , if each gi(\u00b7) is (`1, `2)-smooth (for `1, `2 \u2265 \u03c3).\nIf one performs acceleration the running times become O\u0303 ( n+n3/4 \u221a L/\u03c3 ) and O\u0303 ( n+n3/4(`1`2\u03c3 2)1/4 ) .\n7More precisely, they obtain an (\u03b5, \u221a L2\u03b5)-approximate local minimum using gradient complexity O\u0303 ( n \u221a L2\n\u03b51.5 +\nn3/4L 1/4 2 L 1/2\n\u03b51.75\n) where L2 is the second-order smoothness of f(\u00b7)."}, {"heading": "2.1 RepeatSVRG", "text": "We recall the idea behind a simple algorithm \u2014that we call repeatSVRG\u2014 which finds the \u03b5approximate stationary points for problem (1.1) when f(x) is \u03c3-strongly nonconvex. The algorithm is divided into iterations. In each iteration t, consider a modified function Ft(x) def = F (x)+\u03c3\u2016x\u2212xt\u20162. It is easy to see that Ft(x) is now \u03c3-strongly convex, so one can apply the accelerated SVRG method to minimize Ft(x). Let xt+1 be any sufficiently accurate approximate minimizer for Ft(x). 8\nNow, one can prove (c.f. Section 4) that that xt+1 is an O(\u03c3\u2016xt\u2212xt+1\u2016)-approximate stationary point for F (x). Therefore, if \u03c3\u2016xt\u2212xt+1\u2016 \u2264 \u03b5 we can stop the iterative process because we already have an O(\u03b5)-approximate stationary point. If \u03c3\u2016xt\u2212xt+1\u2016 > \u03b5 , then it must satisfy that F (xt)\u2212 F (xt+1) \u2265 \u03c3\u2016xt\u2212 xt+1\u20162 \u2265 \u2126(\u03b52/\u03c3), but this cannot happen for more than T = O ( \u03c3 \u03b52\n(F (x0)\u2212F \u2217) times. Therefore, the total gradient complexity is T multiplied with the complexity of accelerated SVRG in each iteration (which is O\u0303(n+ n3/4 \u221a L/\u03c3) according to Theorem 2.3).\nWe remark here that the above complexity of repeatSVRG can be inferred from papers [1, 9], but is not explicitly stated. For instance, the paper [9] does not allow F (x) to have a non-smooth proximal term \u03c8(x), and applies accelerated gradient descent instead of accelerated SVRG."}, {"heading": "3 Our Algorithms", "text": "We introduce two variants of our algorithms: (1) base method Natasha targets on the simple regime when f(x) and each fi(x) are both L-smooth, and (2) full method Natasha\nfull targets on the more refined regime when f(x) is L-smooth but each fi(x) is (`1, `2)-smooth.\nBoth methods follow the general idea of variance-reduced stochastic gradient descent: in each inner-most iteration, they compute a gradient estimator \u2207\u0303 that is of the form \u2207\u0303 = \u2207f(x\u0303)\u2212\u2207fi(x\u0303)+ \u2207fi(x) and satisfies Ei\u2208R [\u2207\u0303] = \u2207f(x). Here, x\u0303 is a snapshot point that is changed once every n iterations (i.e., for each different k = 1, 2, . . . , T \u2032), and we call it a full epoch for every distinct k. Notice that the amortized gradient complexity for computing \u2207\u0303 is O(1) per-iteration.\nIn Natasha, as illustrated by Figure 2, we divide each full epoch into p sub-epochs s = 0, 1, . . . , p \u2212 1, each of length m = n/p. In each sub-epoch s, we start with a point x0 = x\u0302, and replace f(x) with its regularized version fs(x) def = f(x) + \u03c3\u2016x \u2212 x\u0302\u20162. Then, in each iteration t of the sub-epoch s, we\n\u2022 compute gradient estimator \u2207\u0303 with respect to fs(xt), and \u2022 perform update xt+1 = arg miny { \u03c8(y) + \u3008\u2207\u0303, y\u3009+ 12\u03b1\u2016y \u2212 xt\u20162 } with learning rate \u03b1.\n8Since the accelerated SVRG method has a linear convergence rate for strongly convex functions, the complexity to find such xt+1 only depends (poly-)logarithmically on its accuracy.\nEffectively, the introduction of the regularizer \u03c3\u2016x\u2212 x\u0302\u20162 makes sure that when performing update xt \u2190 xt+1, we also move a bit towards point x\u0302 (i.e., retraction by regularization). Finally, when the sub-epoch is done, we define x\u0302 to be a random one from {x0, . . . , xm\u22121}.\nIn Natashafull, as illustrated by Figure 3, we also divide each full epoch into p sub-epochs. In each sub-epoch s, we start with a point x0 = z0 = x\u0302 and define f s(x) def = f(x)+\u03c3\u2016x\u2212 x\u0302\u20162. However, this time in each iteration t, we\n\u2022 compute gradient estimator \u2207\u0303 with respect to fs(xt), \u2022 perform update zt+1 = arg miny { \u03c8(y) + \u3008\u2207\u0303, y\u3009+ 12\u03b1\u2016y \u2212 zt\u20162 } with learning rate \u03b1, and\n\u2022 choose xt+1 = 12zt+1 + 12 x\u0303. Effectively, the regularizer \u03c3\u2016x \u2212 x\u0302\u20162 makes sure that when performing updates, we move a bit towards point x\u0302 (i.e., retraction by regularization); at the same time, the choice xt+1 = 1 2zt+1 + 1 2 x\u0303 makes sure we also move a bit towards point x\u0303 (i.e., retraction by the so-called \u201cKatyusha momentum\u201d9). Finally, when the sub-epoch is over, we define x\u0302 to be a random one from the set {x0, . . . , xm\u22121}, and move to the next sub-epoch."}, {"heading": "4 A Sufficient Stopping Criterion", "text": "In this section, we present a sufficient condition for finding approximate stationary points in a \u03c3-strongly nonconvex function. Lemma 4.1 below states that, if we regularize the original function with G(x) def = F (x)+\u03c3\u2016x\u2212x\u0302\u20162 for some arbitrary point x\u0302, then any sufficiently accurate approximate minimizer of G(x) is an approximate saddle-point for F (x).\nLemma 4.1. Suppose G(y) = F (y)+\u03c3\u2016y\u2212 x\u0302\u20162 for some given point x\u0302, and let x\u2217 be the minimizer of G(y). If we minimize G(y) and obtain a point x satisfying\nG(x)\u2212G(x\u2217) \u2264 \u03b42\u03c3 , (4.1) then for every \u03b7 \u2208 (0, 1max{L,4\u03c3}) we have the gradient mapping\n\u2016G\u03b7(x)\u20162 \u2264 12\u03c32\u2016x\u2217 \u2212 x\u0302\u20162 +O ( \u03b42 ) .\n(Notice that when \u03c8(x) \u2261 0 this lemma is trivial, and can be found for instance in [9]). 9The idea for this second kind of retraction, and the idea of having the updates on a sequence zt but computing gradients at points xt, is largely motivated by our recent work on the Katyusha momentum and the Katyusha acceleration [2].\nAlgorithm 1 Natasha(x\u2205, p, T \u2032, \u03b1) Input: starting vector x\u2205, sub-epoch count p \u2208 [n], epoch count T \u2032, learning rate \u03b1 > 0. Output: vector xout.\n1: x\u0302\u2190 x\u2205; m\u2190 n/p; X \u2190 []; 2: for k \u2190 1 to T \u2032 do T \u2032 full epochs 3: x\u0303\u2190 x\u0302; \u00b5\u2190 \u2207f(x\u0303); 4: for s\u2190 0 to p\u2212 1 do p sub-epochs in each epoch 5: x0 \u2190 x\u0302; X \u2190 [X, x\u0302]; 6: for t\u2190 0 to m\u2212 1 do m iterations in each sub-epoch 7: i\u2190 a random choice from {1, \u00b7 \u00b7 \u00b7 , n}. 8: \u2207\u0303 \u2190 \u2207fi(xt)\u2212\u2207fi(x\u0303) + \u00b5+ 2\u03c3(xt \u2212 x\u0302) Ei[\u2207\u0303] = \u2207 ( f(x) + \u03c3\u2016x\u2212 x\u0302\u20162 )\u2223\u2223 xt\n9: xt+1 = arg miny\u2208Rd { \u03c8(y) + 12\u03b1\u2016y \u2212 xt\u20162 + \u3008\u2207\u0303, y\u3009 }\n10: end for 11: x\u0302\u2190 a random choice from {x0, x1, . . . , xm\u22121}; for practitioners, choose the average 12: end for 13: end for 14: x\u0302\u2190 a random vector in X; for practitioners, choose the last 15: xout \u2190 an approximate minimizer of G(y) def= F (y) + \u03c3\u2016y \u2212 x\u0302\u20162 using SVRG. 16: return xout. it suffices to run SVRG for O(n log 1\n\u03b5 ) iterations.\nAlgorithm 2 Natashafull(x\u2205, p, T \u2032, \u03b1) Input: starting vector x\u2205, sub-epoch count p \u2208 [n], epoch count T \u2032, learning rate \u03b1 > 0. Output: vector xout.\n1: x\u0302\u2190 x\u2205; m\u2190 n/p; X \u2190 []; 2: for k \u2190 1 to T \u2032 do T \u2032 full epochs 3: x\u0303\u2190 x\u0302; \u00b5\u2190 \u2207f(x\u0303); 4: for s\u2190 0 to p\u2212 1 do p sub-epochs in each epoch 5: z0 \u2190 x\u0302; x0 \u2190 x\u0302; X \u2190 [X, x\u0302]; 6: for t\u2190 0 to m\u2212 1 do m iterations in each sub-epoch 7: i\u2190 a random choice from {1, \u00b7 \u00b7 \u00b7 , n}; 8: \u2207\u0303 \u2190 \u2207fi(xt)\u2212\u2207fi(x\u0303) + \u00b5+ 2\u03c3(xt \u2212 x\u0302); Ei[\u2207\u0303] = \u2207 ( f(x) + \u03c3\u2016x\u2212 x\u0302\u20162 )\u2223\u2223 xt\n9: zt+1 = arg miny\u2208Rd { \u03c8(y) + 12\u03b1\u2016y \u2212 zt\u20162 + \u3008\u2207\u0303, y\u3009 } ;\n10: xt+1 = 1 2zt+1 + 1 2 x\u0303; Katyusha momentum xt+1 = (1\u2212 \u03b2)zt+1 + \u03b2x\u0303\ntheory predicts \u03b2 = \u0398(\u03c3(`1+`2) `1`2 ) gives the best performance\n\u03b2 = 1/2 however leads to the simplest proof 11: end for 12: x\u0302\u2190 a random choice from {x0, x1, . . . , xm\u22121}; for practitioners, choose the average 13: end for 14: end for 15: x\u0302\u2190 a random vector in X; for practitioners, choose the last 16: xout \u2190 an approximate minimizer of G(y) def= F (y) + \u03c3\u2016y \u2212 x\u0302\u20162 using SVRG. 17: return xout. it suffices to run SVRG for O(n log 1\n\u03b5 ) iterations.\nProof. Let x\u2217 be the (unique) minimizer of G(y). Define auxiliary functions:\n\u03a6(y) def = \u03c8(y) +\n1\n2\u03b7 \u2016y \u2212 x\u20162 + \u3008\u2207f(x), y \u2212 x\u3009 \u2212 \u03c8(x) and \u03a6(y) def= \u03a6(y) + \u03c3\u2016y \u2212 x\u0302\u20162 \u2212 \u03c3\u2016x\u2212 x\u0302\u20162\nand letting z = arg miny \u03a6(y) and z = arg miny \u03a6(y). Observe that\n\u2022 \u03a6(\u00b7) is 1\u03b7 -strongly convex so \u2212\u03a6(z) = \u03a6(x)\u2212 \u03a6(z) \u2265 12\u03b7\u2016z \u2212 x\u20162; \u2022 \u03a6(\u00b7) is 1\u03b7 -strongly convex so \u03a6(z) \u2265 \u03a6(z) + 12\u03b7\u2016z \u2212 z\u20162; \u2022 \u03a6(z) \u2265 G(z)\u2212G(x) \u2265 G(x\u2217)\u2212G(x) \u2265 \u2212\u03b42\u03c3 (because \u03b7 \u2264 1/L and f(\u00b7) is L-smooth).\nSumming the three inequalities up we have\n\u03c3\u2016z \u2212 x\u0302\u20162 \u2212 \u03c3\u2016x\u2212 x\u0302\u20162 \u2265 \u2212\u03b42\u03c3 + 1 2\u03b7 \u2016z \u2212 x\u20162 + 1 2\u03b7 \u2016z \u2212 z\u20162 .\nSince we have inequality \u2016z \u2212 x\u0302\u20162 = \u2016(z \u2212 z) + (z \u2212 x\u0302)\u20162 \u2264 (1 + 1/\u03b2)\u2016(z \u2212 z)\u20162 + (1 + \u03b2)\u2016(z \u2212 x\u0302)\u20162 for any \u03b2 > 0, we can choose \u03b2 = 4\u03b7\u03c3 and obtain\n(\u03c3 + 1 4\u03b7 )\u2016(z \u2212 z)\u20162 + (\u03c3 + 4\u03b7\u03c32)\u2016(z \u2212 x\u0302)\u20162 \u2212 \u03c3\u2016x\u2212 x\u0302\u20162 \u2265 \u2212\u03b42\u03c3 + 1 2\u03b7 \u2016z \u2212 x\u20162 + 1 2\u03b7 \u2016z \u2212 z\u20162\n=\u21d2 (\u03c3 + 4\u03b7\u03c32)\u2016z \u2212 x\u0302\u20162 \u2212 \u03c3\u2016x\u2212 x\u0302\u20162 \u2265 \u2212\u03b42\u03c3 + 1 2\u03b7 \u2016z \u2212 x\u20162\n(4.2)\nwhere the implication uses the fact that 14\u03b7 \u2265 \u03c3. At this point, notice that:\n\u2022 We have \u2016x\u2212 x\u2217\u20162 \u2264 2\u03c3 (G(x)\u2212G(x\u2217)) \u2264 2\u03b42 by the strong convexity of G(\u00b7), and thus \u2212\u03c3\u2016x\u2212 x\u0302\u20162 \u2264 \u2212(\u03c3 \u2212 \u03b7\u03c32)\u2016x\u2217 \u2212 x\u0302\u20162 +O(\u03b42/\u03b7) .\n\u2022 We have \u2016z \u2212 x\u2217\u20162 \u2264 2\u03c3 (G(z)\u2212G(x\u2217)) \u2264 2\u03b42 because G(z) \u2264 G(x), and thus (\u03c3 + 4\u03b7\u03c32)\u2016z \u2212 x\u0302\u20162 \u2264 (\u03c3 + 5\u03b7\u03c32)\u2016x\u2217 \u2212 x\u0302\u20162 +O(\u03b42/\u03b7) .\nPlugging them into (4.2), we have\n(\u03c3 + 5\u03b7\u03c32)\u2016x\u2217 \u2212 x\u0302\u20162 \u2212 (\u03c3 \u2212 \u03b7\u03c32)\u2016x\u2217 \u2212 x\u0302\u20162 \u2265 1 2\u03b7 \u2016z \u2212 x\u20162 \u2212O\n( \u03b42/\u03b7 )\nand rearranging it we have\n\u2016G\u03b7(x)\u20162 = 1 \u03b72 \u2016x\u2212 z\u20162 \u2264 12\u03c32\u2016x\u2217 \u2212 x\u0302\u20162 +O\n( \u03b42 ) ."}, {"heading": "5 Base Method: Analysis for One Full Epoch", "text": "In this section, we consider problem (1.1) where each fi(x) is L-smooth and F (x) is \u03c3-approximateconvex. We use our base method Natasha to minimize F (x), and analyze its behavior for one full epoch in this section. We assume \u03c3 \u2264 L without loss of generality, because any L-smooth function is also L-strongly nonconvex.\nNotations. We introduce some notations for analysis purpose only.\n\u2022 Let x\u0302s be the value of x\u0302 at the beginning of sub-epoch s. \u2022 Let xst be the value of xt in sub-epoch s. \u2022 Let ist be the value of i \u2208 [n] in sub-epoch s at iteration t.\n\u2022 Let fs(x) def= f(x) + \u03c3\u2016x\u2212 x\u0302s\u20162, F s(x) def= F (x) + \u03c3\u2016x\u2212 x\u0302s\u20162, and xs\u2217 def = arg minx{F s(x)}. \u2022 Let \u2207\u0303f s(xst ) def = \u2207fi(xst )\u2212\u2207fi(x\u0303) +\u2207f(x\u0303) + 2\u03c3(xt \u2212 x\u0302) where i = ist . \u2022 Let \u2207\u0303f(xst ) def = \u2207fi(xst )\u2212\u2207fi(x\u0303) +\u2207f(x\u0303) where i = ist .\nWe obviously have that fs(x) and F s(x) are \u03c3-strongly convex, and fs(x) is (L+ 2\u03c3)-smooth."}, {"heading": "5.1 Variance Upper Bound", "text": "The following lemma gives an upper bound on the variance of the gradient estimator \u2207\u0303fs(xst ): Lemma 5.1. We have Eist [ \u2016\u2207\u0303f s(xst )\u2212\u2207fs(xst )\u20162 ] \u2264 pL2\u2016xst \u2212 x\u0302s\u20162 + pL2 \u2211s\u22121 k=0 \u2016x\u0302k \u2212 x\u0302k+1\u20162 .\nProof. We have\nEist [ \u2016\u2207\u0303fs(xst )\u2212\u2207fs(xst )\u20162 ] = Eist [ \u2016\u2207\u0303f(xst )\u2212\u2207f(xst )\u20162 ]\n= Ei\u2208R[n] [\u2225\u2225(\u2207fi(xst )\u2212\u2207fi(x\u0303) ) \u2212 ( \u2207f(xst )\u2212\u2207f(x\u0303)) )\u2225\u22252] \u00ac \u2264 Ei\u2208R[n] [\u2225\u2225\u2207fi(xst )\u2212\u2207fi(x\u0303) \u2225\u22252]  \u2264 pEi\u2208R[n] [\u2225\u2225\u2207fi(xst )\u2212\u2207fi(x\u0302s) \u2225\u22252]+ p\u2211s\u22121k=0 Ei\u2208R[n] [\u2225\u2225\u2207fi(x\u0302k)\u2212\u2207fi(x\u0302k+1) \u2225\u22252] \u00ae \u2264 pL2\u2016xst \u2212 x\u0302s\u20162 + pL2 \u2211s\u22121 k=0 \u2016x\u0302k \u2212 x\u0302k+1\u20162 .\nAbove, inequality \u00ac is because for any random vector \u03b6 \u2208 Rd, it holds that E\u2016\u03b6 \u2212E\u03b6\u20162 = E\u2016\u03b6\u20162 \u2212 \u2016E\u03b6\u20162; inequality  is because x\u03020 = x\u0303 and for any p vectors a1, a2, . . . , ap \u2208 Rd, it holds that \u2016a1 + \u00b7 \u00b7 \u00b7+ ap\u20162 \u2264 p\u2016a1\u20162 + \u00b7 \u00b7 \u00b7+ p\u2016ap\u20162; and inequality \u00ae is because each fi(\u00b7) is L-smooth."}, {"heading": "5.2 Analysis for One Sub-Epoch", "text": "The following inequality is classically known as the \u201cregret inequality\u201d for mirror descent [7].\nFact 5.2. \u3008\u2207\u0303fs(xst ), xst+1\u2212u\u3009+\u03c8(xst+1)\u2212\u03c8(u) \u2264 \u2016xst\u2212u\u20162 2\u03b1 \u2212 \u2016xst+1\u2212u\u20162 2\u03b1 \u2212 \u2016xst+1\u2212xst\u20162 2\u03b1 for every u \u2208 Rd.\nProof. Recall that the minimality of xst+1 = arg miny\u2208Rd{ 12\u03b1\u2016y\u2212xst\u20162+\u03c8(y)+\u3008\u2207\u0303fs(xst ), y\u3009} implies the existence of some subgradient g \u2208 \u2202\u03c8(xst+1) which satisfies 1\u03b1(xst+1 \u2212 xst ) + \u2207\u0303fs(xst ) + g = 0. Combining this with \u03c8(u) \u2212 \u03c8(xst+1) \u2265 \u3008g, u \u2212 xst+1\u3009, which is due to the convexity of \u03c8(\u00b7), we immediately have \u03c8(u)\u2212\u03c8(xst+1)+ \u3008 1\u03b1(xst+1\u2212xst )+ \u2207\u0303f s(xst ), u\u2212xst+1\u3009 \u2265 \u3008 1\u03b1(xst+1\u2212xst )+ \u2207\u0303fs(xst )+ g, u\u2212 xst+1\u3009 = 0. Rearranging this inequality we have\n\u3008\u2207\u0303fs(xst ), xst+1 \u2212 u\u3009+ \u03c8(xst+1)\u2212 \u03c8(u) \u2264 \u3008\u2212 1\n\u03b1 (xst+1 \u2212 xst ), xst+1 \u2212 u\u3009\n= \u2016xst \u2212 u\u20162 2\u03b1 \u2212 \u2016x s t+1 \u2212 u\u20162 2\u03b1 \u2212 \u2016x s t+1 \u2212 xst\u20162 2\u03b1 .\nThe following lemma is our main contribution for the base method Natasha.\nLemma 5.3. As long as \u03b1 \u2264 12L+4\u03c3 , we have\nE [( F s(x\u0302s+1)\u2212 F s(xs\u2217) )] \u2264 E [F s(x\u0302s)\u2212 F s(xs\u2217) \u03c3\u03b1m/2 + \u03b1pL2 ( s\u2211\nk=0\n\u2016x\u0302k \u2212 x\u0302k+1\u20162 )] .\nProof. We first compute that\nF s(xst+1)\u2212 F s(u) = fs(xst+1)\u2212 f(u) + \u03c8(xst+1)\u2212 \u03c8(u) \u00ac \u2264 f s(xst ) + \u3008\u2207fs(xst ), xst+1 \u2212 xst \u3009+ L+ 2\u03c3\n2 \u2016xst \u2212 xst+1\u20162 \u2212 f(u) + \u03c8(xst+1)\u2212 \u03c8(u)\n \u2264 \u3008\u2207fs(xst ), xst+1 \u2212 xst \u3009+ L+ 2\u03c3\n2 \u2016xst \u2212 xst+1\u20162 + \u3008\u2207fs(xst ), xst \u2212 u\u3009+ \u03c8(xst+1)\u2212 \u03c8(u) . (5.1)\nAbove, inequality \u00ac uses the fact that fs(\u00b7) is (L+2\u03c3)-smooth; and inequality  uses the convexity of fs(\u00b7). Now, we take expectation with respect to ist on both sides of (5.1), and derive that:\nEist [ F s(xst+1) ] \u2212 F s(u)\n\u00ac \u2264 Eist [ \u3008\u2207\u0303fs(xst )\u2212\u2207fs(xst ), xst \u2212 xst+1\u3009+ \u3008\u2207\u0303fs(xst ), xst+1 \u2212 u\u3009+ L+ 2\u03c3\n2 \u2016xst \u2212 xst+1\u20162 + \u03c8(xst+1)\u2212 \u03c8(u)\n]\n \u2264 Eist [ \u3008\u2207\u0303fs(xst )\u2212\u2207f s(xst ), xst \u2212 xst+1\u3009+ \u2016xst \u2212 u\u20162 2\u03b1 \u2212 \u2016x s t+1 \u2212 u\u20162 2\u03b1 \u2212 ( 1 2\u03b1 \u2212 L+ 2\u03c3 2 ) \u2016xst+1 \u2212 xst\u20162 ] \u00ae \u2264 Eist [ \u03b1 \u2225\u2225\u2207\u0303fs(xst )\u2212\u2207fs(xst ) \u2225\u22252 + \u2016x s t \u2212 u\u20162 2\u03b1 \u2212 \u2016x s t+1 \u2212 u\u20162 2\u03b1 ]\n\u00af \u2264 Eist [ \u03b1pL2\u2016xst \u2212 x\u0302s\u20162 + \u03b1pL2 s\u22121\u2211\nk=0\n\u2016x\u0302k \u2212 x\u0302k+1\u20162 + \u2016x s t \u2212 u\u20162 2\u03b1 \u2212 \u2016x s t+1 \u2212 u\u20162 2\u03b1 ] . (5.2)\nAbove, inequality \u00ac is follows from (5.1) together with the fact that Eist [\u2207\u0303fs(xst )] = \u2207fs(xst ) implies\nEist [ \u3008\u2207f s(xst ), xst+1 \u2212 xst \u3009+ \u3008\u2207fs(xst ), xst \u2212 u\u3009 ]\n= Eist [ \u3008\u2207\u0303fs(xst )\u2212\u2207fs(xst ), xst \u2212 xst+1\u3009+ \u3008\u2207\u0303fs(xst ), xst+1 \u2212 u\u3009 ] ;\ninequality  uses Fact 5.2; inequality \u00ae uses \u03b1 \u2264 12L+4\u03c3 together with Young\u2019s inequality \u3008a, b\u3009 \u2264 1 2\u2016a\u20162 + 12\u2016b\u20162; and inequality \u00af uses Lemma 5.1.\nFinally, choosing u = xs\u2217 to be the (unique) minimizer of F s(\u00b7) = fs(\u00b7) + \u03c8(\u00b7), and telescoping\ninequality (5.2) for t = 0, 1, . . . ,m\u2212 1, we have\nE [m\u22121\u2211\nt=1\n( F s(xst )\u2212 F s(xs\u2217)\n)]\n\u2264 E [\u2016xs0 \u2212 xs\u2217\u20162 2\u03b1 + m\u22121\u2211\nt=0\n( \u03b1pL2\u2016xst \u2212 x\u0302s\u20162 + \u03b1pL2 s\u22121\u2211\nk=0\n\u2016x\u0302k \u2212 x\u0302k+1\u20162 )]\n\u2264 E [F s(x\u0302s)\u2212 F s(xs\u2217)\n\u03c3\u03b1 + \u03b1pmL2\n( s\u2211\nk=0\n\u2016x\u0302k \u2212 x\u0302k+1\u20162 )] .\nAbove, the second inequality uses the fact that x\u0302s+1 is chosen from {xs0, . . . , xsm\u22121} uniformly at random, as well as the \u03c3-strong convexity of F s(\u00b7).\nDividing both sides by m and rearranging the terms (using 12\u03c3\u03b1 \u2265 1), we have\nE [( F s(x\u0302s+1)\u2212 F s(xs\u2217) )] \u2264 E [F s(x\u0302s)\u2212 F s(xs\u2217) \u03c3\u03b1m/2 + \u03b1pL2 ( s\u2211\nk=0\n\u2016x\u0302k \u2212 x\u0302k+1\u20162 )] ."}, {"heading": "5.3 Analysis for One Full Epoch", "text": "We telescope Lemma 5.3 for an entire epoch and arrive at the following lemma:\nLemma 5.4. If \u03b1 \u2264 12L+4\u03c3 , \u03b1 \u2265 4\u03c3m and \u03b1 \u2264 \u03c3p2L2 , we have p\u22121\u2211\ns=0\nE [( F s(x\u0302s)\u2212 F s(xs\u2217) )] \u2264 2E [ F (x\u03020)\u2212 F (x\u0302p) ] .\nProof. Telescoping Lemma 5.3 for all the subepochs s = 0, 1, . . . , p\u2212 1, we have p\u22121\u2211\ns=0\nE [( F s(x\u0302s+1)\u2212 F s(xs\u2217) )] \u2264 p\u22121\u2211\ns=0\nE [F s(x\u0302s)\u2212 F s(xs\u2217)\n\u03c3\u03b1m/2 + \u03b1p2L2\u2016x\u0302s \u2212 x\u0302s+1\u20162\n]\n\u00ac \u2264\np\u22121\u2211\ns=0\nE [F s(x\u0302s)\u2212 F s(xs\u2217)\n\u03c3\u03b1m/2 + \u03c3 \u00b7 \u2016x\u0302s+1 \u2212 x\u0302s\u20162\n]\n =\np\u22121\u2211\ns=0\nE [F s(x\u0302s)\u2212 F s(xs\u2217) \u03c3\u03b1m/2 + ( F s(x\u0302s+1)\u2212 F s(x\u0302s) ) \u2212 ( F (x\u0302s+1)\u2212 F (x\u0302s) )]\nAbove, \u00ac uses \u03b1p2L2 \u2264 \u03c3, and  uses the definition F s(y) = F (y)+\u03c3\u2016y\u2212x\u0302s\u20162. Finally, rearranging both sides, and using the fact that 1\u03c3\u03b1m \u2264 14 , we have the desired inequality."}, {"heading": "6 Base Method: Final Theorem", "text": "We are now ready to state and prove our main convergence theorem for Natasha:\nTheorem 1. Suppose in (1.1), each fi(x) is L-smooth and F (x) is \u03c3-approximate-convex for \u03c3 \u2264 L. Then, if L2 \u03c32 \u2264 n, p = \u0398 ( ( \u03c3 2 L2 n)1/3 ) and \u03b1 = \u0398( \u03c3 p2L2 ), our base method Natasha outputs\na point xout satisfying E[\u2016G\u03b7(xout)\u20162] \u2264 \u03b52 for every \u03b7 \u2208 ( 0, 1max{L,4\u03c3} )\nwith total gradient complexity O ( n log 1\u03b5 + (L2\u03c3)1/3n2/3 \u03b52 \u00b7 (F (x\u2205)\u2212 F \u2217) ) .\nIn the above theorem, we have assumed \u03c3 \u2264 L without loss of generality because any L-smooth function is also L-strongly nonconvex. Also, we have assumed L 2\n\u03c32 \u2264 n and if this inequality does\nnot hold, then one should apply repeatSVRG for a faster running time (see Figure 1). Proof of Theorem 6. We choose p = ( \u03c32 24L2 n )1/3 , m = n/p, and \u03b1 = 4\u03c3m = \u03c3 6p2L2 \u2264 12L+4\u03c3 , so we can apply Lemma 5.4. If we telescope Lemma 5.4 for the entire algorithm (which has T \u2032 full epochs), and use the fact that x\u0302p of the previous epoch equals x\u03020 of the next epoch, we conclude that if we choose a random epoch and a random subepoch s, we will have\nE[F s(x\u0302s)\u2212 F s(xs\u2217)] \u2264 2 pT \u2032 (F (x\u2205)\u2212 F \u2217) .\nBy the \u03c3-strong convexity of F s(\u00b7), we have E[\u03c3\u2016x\u0302s \u2212 xs\u2217\u20162] \u2264 4pT \u2032 (F (x\u2205)\u2212 F \u2217). Now, F s(x) = F (x) + \u03c3\u2016x\u2212 x\u0302s\u20162 satisfies the assumption of G(x) in Lemma 4.1. If we use the SVRG method (see Theorem 2.3) to minimize the convex function F s(x), we get an output xout satisfying F s(xout)\u2212 F s(xs\u2217) \u2264 \u03b52\u03c3 in gradient complexity O ( (n+ L 2 \u03c32 ) log 1\u03b5 ) \u2264 O(n log 1\u03b5 ).\nWe can therefore apply Lemma 4.1 and conclude that this output xout satisfies\nE[\u2016G\u03b7(xout)\u20162] \u2264 O ( \u03c3 pT \u2032 ) \u00b7 (F (x\u2205)\u2212 F \u2217) = O ((L2\u03c3)1/3n2/3 T \u2032n ) \u00b7 (F (x\u2205)\u2212 F \u2217) .\nIn other words, we obtain E[\u2016G\u03b7(xout)\u20162] \u2264 \u03b52 with gradient complexity\nT \u2032n = O ( n+ (L2\u03c3)1/3n2/3\n\u03b52 \u00b7 (F (x\u2205)\u2212 F \u2217)\n) .\nHere, the additive term n is because the gradient complexity is T \u2032n but T \u2032 is at least 1."}, {"heading": "7 Full Method: Analysis for One Full Epoch", "text": "In this section, we study a more refined version of problem (1.1), where f(x) is L-smooth, each fi(x) is (`1, `2)-smooth, and F (x) is \u03c3-approximate-convex. As later argued in Remark 8.1, we can assume \u03c3 \u2264 min{`1, `2, L} almost without loss of generality.\nWe use our full method Natashafull to minimize F (x), and analyze its behavior for one full epoch in this section. Note that parameter L is not needed in the specification of Natashafull, but used only for analysis purpose.\nNotations. We use the same notations as in Section 5, with an additional one highlighted here:\n\u2022 Let x\u0302s be the value of x\u0302 at the beginning of sub-epoch s. \u2022 Let xst be the value of xt in sub-epoch s. \u2022 Let ist be the value of i \u2208 [n] in sub-epoch s at iteration t. \u2022 Let F s(x) def= F (x) + \u03c3\u2016x\u2212 x\u0302s\u20162 and xs\u2217 def = arg minx{F s(x)}.\n\u2022 Let f s(x) def= f(x) + \u03c3\u2016x\u2212 x\u0302s\u20162 and fsi (x) def = fi(x) + \u03c3\u2016x\u2212 x\u0302s\u20162 .\n\u2022 Let \u2207\u0303fs(xst ) def = \u2207fi(xst )\u2212\u2207fi(x\u0303) +\u2207f(x\u0303) + 2\u03c3(xt \u2212 x\u0302) where i = ist . \u2022 Let \u2207\u0303f(xst ) def = \u2207fi(xst )\u2212\u2207fi(x\u0303) +\u2207f(x\u0303) where i = ist .\nWe obviously have that fs(x) and F s(x) are \u03c3-strongly convex, and fs(x) is (L+ 2\u03c3)-smooth."}, {"heading": "7.1 Variance Upper Bound", "text": "In this subsection we derive a new upper bound on the variance of the gradient estimator \u2207\u0303. This bound will be tighter than Lemma 5.1, and will make use of the asymmetry between parameters `1 and `2. To achieve so, we first need to introduce the following lemma: Lemma 7.1. If g(y) = 1n \u2211n i=1 gi(y) is convex, and if each gi is (`1, `2)-smooth, then we have\nEi\u2208R[n] [ \u2016\u2207gi(y1)\u2212\u2207gi(y2)\u20162 ]\n\u2264 2(`1 + `2)(g(y2)\u2212 g(y1)\u2212 \u3008\u2207g(y1), y2 \u2212 y1\u3009) ] + 6`1`2\u2016y2 \u2212 y1\u20162 .\nProof. We consider two cases: `2 \u2264 `1 and `2 \u2265 `1.\n\u2022 In the first case, we define \u03c6i(z) def= gi(z) \u2212 \u3008\u2207gi(y1), z\u3009 + `22 \u2016z \u2212 y1\u20162 for each i \u2208 [n]. This function \u03c6i(z) is a convex, (`1 + `2)-smooth function that has a minimizer z = y1 (which can be seen by taking the derivative). For this reason, we claim that\n\u2200z : \u03c6i(y1) \u2264 \u03c6i(z)\u2212 1\n`1 + `2 \u2016\u2207\u03c6i(z)\u20162 , (7.1)\nand this inequality is classical for smooth functions (see for instance Theorem 2.1.5 in textbook [17]). By expanding out the definition of \u03c6i(\u00b7) in (7.1), we immediately have\ngi(y1)\u2212 \u3008\u2207gi(y1), y1\u3009 \u2264 gi(z)\u2212 \u3008\u2207gi(y1), z\u3009+ `2 2 \u2016z \u2212 y1\u20162\n\u2212 1 2(`1 + `2) \u2016\u2207gi(z)\u2212\u2207gi(y1) + `2(z \u2212 y1)\u20162\nwhich then implies\n\u2016\u2207gi(z)\u2212\u2207gi(y1)\u20162 \u2264 2\u2016\u2207gi(z)\u2212\u2207gi(y1) + `2(z \u2212 y1)\u20162 + 2\u2016`2(z \u2212 y1)\u20162\n\u2264 2(`1 + `2)(gi(z)\u2212 gi(y1)\u2212 \u3008\u2207gi(y1), z \u2212 y1\u3009) + (4`22 + 2`1`2)\u2016z \u2212 y1\u20162 . (7.2)\nNow, by choosing z = y2 and taking expectation with i in (7.2), we have\nEi [\u2225\u2225\u2207gi(y2)\u2212\u2207gi(y1) \u2225\u22252]\n\u2264 2(`1 + `2) ( g(y2)\u2212 g(y1)\u2212 \u3008\u2207g(y1), y2 \u2212 y1\u3009) ) + (4`22 + 2`1`2)\u2016y2 \u2212 y1\u20162 (7.3)\n\u2022 In the second case, we define \u03c6i(z) def= \u2212gi(z) + \u3008\u2207gi(y2), z\u3009+ `12 \u2016z \u2212 y2\u20162 for each i \u2208 [n]. It is clear that \u03c6i(z) is a convex, (`1 + `2)-smooth function that has a minimizer z = y2 (which can be seen by taking the derivative). For this reason, we have\n\u2200z : \u03c6i(y2) \u2264 \u03c6i(z)\u2212 1\n`1 + `2 \u2016\u2207\u03c6i(z)\u20162 . (7.4)\nBy expanding out the definition of \u03c6i(\u00b7) in (7.4), we immediately have\n\u2212 gi(y2) + \u3008\u2207gi(y2), y2\u3009 \u2264 \u2212gi(z) + \u3008\u2207gi(y2), z\u3009+ `1 2 \u2016z \u2212 y2\u20162\n\u2212 1 2(`1 + `2) \u2016\u2207gi(z)\u2212\u2207gi(y2)\u2212 `1(z \u2212 y2)\u20162\nwhich then implies that\n\u2016\u2207gi(z)\u2212\u2207gi(y2)\u20162 \u2264 2\u2016\u2207gi(z)\u2212\u2207gi(y2)\u2212 `1(z \u2212 y2)\u20162 + 2\u2016`2(z \u2212 y2)\u20162\n\u2264 2(`1 + `2)(gi(y2)\u2212 gi(z) + \u3008\u2207gi(y2), z \u2212 y2\u3009) + (4`21 + 2`1`2)\u2016z \u2212 y2\u20162 . (7.5)\nNow by choosing z = y1 and taking expectation over i in (7.5), we have\nEi [\u2225\u2225\u2207gi(y1)\u2212\u2207gi(y2) \u2225\u22252]\n\u2264 2(`1 + `2) ( g(y2)\u2212 g(y1) + \u3008\u2207g(y2), y1 \u2212 y2\u3009 ) + (4`21 + 2`1`2)\u2016y1 \u2212 y2\u20162 \u2264 (4`21 + 2`1`2)\u2016y1 \u2212 y2\u20162 \u2264 2(`1 + `2) ( g(y2)\u2212 g(y1)\u2212 \u3008\u2207g(y1), y2 \u2212 y1\u3009) ) + (4`22 + 2`1`2)\u2016y2 \u2212 y1\u20162 . (7.6)\nAbove, the second and third inequalities use the convexity of g(\u00b7).\nCombining (7.3) and (7.6) we finish the proof of the lemma. We are now ready to state our final variance upper bound:\nLemma 7.2 (variance bound). There exists constant C \u2265 1 such that, if we define \u2022 \u03a6s(y) def= C(`1 + `2) \u00b7 (fs(x\u0302s)\u2212 fs(y)\u2212 \u3008\u2207f s(y), x\u0302s \u2212 y\u3009) ] + C(`1`2) \u00b7 \u2016y \u2212 x\u0302s\u20162 \u2265 0;\n\u2022 \u03a6st = \u03a6s(xst ) and \u03a6s = \u03a6s(x\u0302s+1), then, we have Ei [ \u2016\u2207\u0303fs(xst )\u2212\u2207fs(xst )\u20162 ] \u2264 p\u03a6st + p \u2211s\u22121 k=0 \u03a6 k where i = ist .\nBefore proceeding to the proof, we point out that if `1 = `2 = L like in the base setting, then we shall have \u03a6s(y) \u2264 O(L2)\u2016y \u2212 x\u0302s\u20162 and Lemma 7.2 becomes identical to Lemma 5.1.\nProof. If we plug in g = f s and gi = f s i in Lemma 7.1, we have gi is (`1 + 2\u03c3, `2 \u2212 2\u03c3)-smooth and thus each gi is also (3`1, `2)-smooth. Therefore, Lemma 7.1 implies there exists constant C \u2265 1 such that\nEi [\u2225\u2225\u2207fi(y)\u2212\u2207fi(x\u0302s) \u2225\u22252] \u2264 2Ei [\u2225\u2225\u2207fsi (y)\u2212\u2207fsi (x\u0302s) \u2225\u22252]+ 2 \u2225\u22252\u03c3(y \u2212 x\u0302s) \u2225\u22252\n\u2264 C(`1 + `2) \u00b7 (fs(x\u0302s)\u2212 fs(y)\u2212 \u3008\u2207f s(y), x\u0302s \u2212 y\u3009) ]\n+ C(`1`2) \u00b7 \u2016y \u2212 x\u0302s\u20162 = \u03a6s(y) . (7.7)\nTherefore, the variance term:\nEi [ \u2016\u2207\u0303f s(xst )\u2212\u2207fs(xst )\u20162 ] = Ei [ \u2016\u2207\u0303f(xst )\u2212\u2207f(xst )\u20162 ]\n= Ei [\u2225\u2225(\u2207fi(xst )\u2212\u2207fi(x\u0303) ) \u2212 ( \u2207f(xst )\u2212\u2207f(x\u0303)) )\u2225\u22252] \u00ac \u2264 Ei [\u2225\u2225\u2207fi(xst )\u2212\u2207fi(x\u0303) \u2225\u22252]  \u2264 pEi [\u2225\u2225\u2207fi(xst )\u2212\u2207fi(x\u0302s) \u2225\u22252]+ p\u2211s\u22121k=0 Ei [\u2225\u2225\u2207fi(x\u0302k)\u2212\u2207fi(x\u0302k+1) \u2225\u22252] \u00ae \u2264 p\u03a6st + p \u2211s\u22121 k=0 \u03a6 k . (7.8)\nAbove, inequality \u00ac is because for any random vector \u03b6 \u2208 Rd, it holds that E\u2016\u03b6 \u2212E\u03b6\u20162 = E\u2016\u03b6\u20162 \u2212 \u2016E\u03b6\u20162; inequality  is because x\u03020 = x\u0303 and for any p vectors a1, a2, . . . , ap \u2208 Rd, it holds that \u2016a1 + \u00b7 \u00b7 \u00b7+ ap\u20162 \u2264 p\u2016a1\u20162 + \u00b7 \u00b7 \u00b7+ p\u2016ap\u20162; and inequality \u00ae is from repeatedly applying (7.7)."}, {"heading": "7.2 Analysis for One Sub-Epoch", "text": "The following fact is analogous to Fact 5.2, and the only difference is that in Natashafull we are applying proximal updates on the {zst }t sequence.\nFact 7.3. \u3008\u2207\u0303f s(xst ), zst+1\u2212u\u3009+\u03c8(zst+1)\u2212\u03c8(u) \u2264 \u2016zst\u2212u\u20162 2\u03b1 \u2212 \u2016zst+1\u2212u\u20162 2\u03b1 \u2212 \u2016zst+1\u2212zst \u20162 2\u03b1 for every u \u2208 Rd.\nProof. Recall that the minimality of zst+1 = arg miny\u2208Rd{ 12\u03b1\u2016y\u2212zst \u20162 +\u03c8(y)+\u3008\u2207\u0303fs(xst ), y\u3009} implies the existence of some subgradient g \u2208 \u2202\u03c8(zst+1) which satisfies 1\u03b1(zst+1 \u2212 zst ) + \u2207\u0303fs(xst ) + g = 0. Combining this with \u03c8(u) \u2212 \u03c8(zst+1) \u2265 \u3008g, u \u2212 zst+1\u3009, which is due to the convexity of \u03c8(\u00b7), we immediately have \u03c8(u)\u2212\u03c8(zst+1) + \u3008 1\u03b1(zst+1\u2212 zst ) + \u2207\u0303fs(xst ), u\u2212 zst+1\u3009 \u2265 \u3008 1\u03b1(zst+1\u2212 zst ) + \u2207\u0303fs(xst ) + g, u\u2212 zst+1\u3009 = 0. Rearranging this inequality we have\n\u3008\u2207\u0303fs(xst ), zst+1 \u2212 u\u3009+ \u03c8(zst+1)\u2212 \u03c8(u) \u2264 \u3008\u2212 1\n\u03b1 (zst+1 \u2212 zst ), zst+1 \u2212 u\u3009\n= \u2016zst \u2212 u\u20162 2\u03b1 \u2212 \u2016z s t+1 \u2212 u\u20162 2\u03b1 \u2212 \u2016z s t+1 \u2212 zst \u20162 2\u03b1 .\nThe following lemma is our technical main contribution for the full method Natashafull.\nLemma 7.4. If \u03b1 \u2264 1L+2\u03c3 , then we have the following inequality for sub-epoch s:\nE [( F s(x\u0302s+1)\u2212 F s(xs\u2217) )]\n\u2264 E [F s(x\u0302s)\u2212 F s(xs\u2217)\n\u03c3\u03b1m/2 + \u03b1p\n( s\u2211\nk=0\n\u03a6k ) + \u3008\u2207fs(x\u0302s+1), x\u0302s \u2212 x\u0302s+1\u3009+ ( \u03c8(x\u0302s)\u2212 \u03c8(x\u0302s+1) )] .\nProof. We first compute that\n2F s(xst+1)\u2212 F s(xst )\u2212 F s(u) = 2fs(xst+1)\u2212 fs(xst )\u2212 f(u) + 2\u03c8(xst+1)\u2212 \u03c8(xst )\u2212 \u03c8(u) \u00ac \u2264 fs(xst ) + 2\u3008\u2207fs(xst ), xst+1 \u2212 xst \u3009+ (L+ 2\u03c3)\u2016xst \u2212 xst+1\u20162 \u2212 f(u) + 2\u03c8(xst+1)\u2212 \u03c8(xst )\u2212 \u03c8(u)  = fs(xst ) + \u3008\u2207fs(xst ), zst+1 \u2212 zst \u3009+ L+ 2\u03c3\n4 \u2016zst \u2212 zst+1\u20162 \u2212 f(u) + 2\u03c8(xst+1)\u2212 \u03c8(xst )\u2212 \u03c8(u)\n\u00ae \u2264 \u3008\u2207fs(xst ), zst+1 \u2212 zst \u3009+ L+ 2\u03c3\n4 \u2016zst \u2212 zst+1\u20162 + \u3008\u2207fs(xst ), xst \u2212 u\u3009+ \u03c8(zst+1) + \u03c8(x\u0302s)\u2212 \u03c8(xst )\u2212 \u03c8(u)\n(7.9)\nAbove, inequality \u00ac uses the fact that fs(\u00b7) is (L + 2\u03c3)-smooth; equality  uses the fact that zst+1 \u2212 zst = 2(xst+1 \u2212 xst ); inequality \u00ae uses the convexity of fs(\u00b7), the convexity of \u03c8(\u00b7), and the fact xst+1 = 1 2(z s t+1 + x\u0302\ns). Now, we take expectation with respect to ist on both sides of (7.9), and derive that:\n2Eist [ F s(xst+1) ] \u2212 F s(xst )\u2212 F s(u)\n\u00ac \u2264 Eist [ \u3008\u2207\u0303fs(xst )\u2212\u2207fs(xst ), zst \u2212 zst+1\u3009+ \u3008\u2207\u0303fs(xst ), zst+1 \u2212 u\u3009+ L+ 2\u03c3\n4 \u2016zst \u2212 zst+1\u20162 + \u03c8(zst+1)\u2212 \u03c8(u)\n]\n+ \u3008\u2207fs(xst ), xst \u2212 zst \u3009+ \u03c8(x\u0302s)\u2212 \u03c8(xst )  \u2264 Eist [ \u3008\u2207\u0303fs(xst )\u2212\u2207fs(xst ), zst \u2212 zst+1\u3009+ \u2016zst \u2212 u\u20162 2\u03b1 \u2212 \u2016z s t+1 \u2212 u\u20162 2\u03b1 \u2212 ( 1 2\u03b1 \u2212 L+ 2\u03c3 4 ) \u2016zst+1 \u2212 zst \u20162 ] + \u3008\u2207fs(xst ), xst \u2212 zst \u3009+ \u03c8(x\u0302s)\u2212 \u03c8(xst ) \u00ae \u2264 Eist [ \u03b1 \u2225\u2225\u2207\u0303fs(xst )\u2212\u2207f s(xst ) \u2225\u22252 + \u2016z s t \u2212 u\u20162 2\u03b1 \u2212 \u2016z s t+1 \u2212 u\u20162 2\u03b1 ] + \u3008\u2207fs(xst ), xst \u2212 zst \u3009+ \u03c8(x\u0302s)\u2212 \u03c8(xst )\n\u00af \u2264 Eist [ \u03b1p\u03a6st + \u03b1p s\u22121\u2211\nk=0\n\u03a6k + \u2016zst \u2212 u\u20162 2\u03b1 \u2212 \u2016z s t+1 \u2212 u\u20162 2\u03b1 ] + \u3008\u2207fs(xst ), x\u0302s \u2212 xst \u3009+ \u03c8(x\u0302s)\u2212 \u03c8(xst ) .\nAbove, inequality \u00ac is from (7.9) together with the fact that Eist [\u2207\u0303fs(xst )] = \u2207fs(xst ) implies\nEist [ \u3008\u2207f s(xst ), zst+1 \u2212 zst \u3009+ \u3008\u2207f s(xst ), xst \u2212 u\u3009 ]\n= Eist [ \u3008\u2207fs(xst ), xst \u2212 zst \u3009+ \u3008\u2207\u0303fs(xst )\u2212\u2207fs(xst ), zst \u2212 zst+1\u3009+ \u3008\u2207\u0303fs(xst ), zst+1 \u2212 u\u3009 ] ;\ninequality  uses Fact 7.3; inequality \u00ae uses \u03b1 \u2264 1L+2\u03c3 together with Young\u2019s inequality \u3008a, b\u3009 \u2264 1 2\u2016a\u20162 + 12\u2016b\u20162; and inequality \u00af uses Lemma 7.2.\nFinally, choosing u = xs\u2217 to be the (unique) minimizer of F s(\u00b7) = fs(\u00b7) + \u03c8(\u00b7), and telescoping\nthe above inequality for t = 0, 1, . . . ,m\u2212 1, we have\nE [m\u22121\u2211\nt=1\n( F s(xst )\u2212 F s(xs\u2217)\n)]\n\u2264 E [\u2016zs0 \u2212 xs\u2217\u20162 2\u03b1 + m\u22121\u2211\nt=0\n( \u03b1p\u03a6st + \u03b1p s\u22121\u2211\nk=0\n\u03a6k + \u3008\u2207fs(xst ), x\u0302s \u2212 xst \u3009+ \u03c8(x\u0302s)\u2212 \u03c8(xst ) )] .\nUsing the fact x\u0302s+1 is chosen uniformly at random from {xs0, . . . , xsm\u22121}, the above inequality implies\nE [ m ( F s(x\u0302s+1)\u2212 F s(xs\u2217) ) \u2212 ( F s(x\u0302s)\u2212 F s(xs\u2217) )]\n\u2264 E [F s(x\u0302s)\u2212 F s(xs\u2217)\n\u03c3\u03b1 + \u03b1pm\n( s\u2211\nk=0\n\u03a6k ) +m\u3008\u2207f s(x\u0302s+1), x\u0302s \u2212 x\u0302s+1\u3009+m ( \u03c8(x\u0302s)\u2212 \u03c8(x\u0302s+1) )] .\nDividing both sides by m and rearranging the terms (using 1\u03c3\u03b1 \u2265 1), we have\nE [( F s(x\u0302s+1)\u2212 F s(xs\u2217) )]\n\u2264 E [F s(x\u0302s)\u2212 F s(xs\u2217)\n\u03c3\u03b1m/2 + \u03b1p\n( s\u2211\nk=0\n\u03a6k ) + \u3008\u2207f s(x\u0302s+1), x\u0302s \u2212 x\u0302s+1\u3009+ ( \u03c8(x\u0302s)\u2212 \u03c8(x\u0302s+1) )] ."}, {"heading": "7.3 Analysis for One Full Epoch", "text": "We telescope Lemma 7.4 for an entire epoch and arrive at the following lemma:\nLemma 7.5. If \u03b1 \u2264 O( \u03c3 p2`1`2 ) and \u03b1 \u2265 \u2126( 1\u03c3m), we have p\u22121\u2211\ns=0\nE [( F s(x\u0302s)\u2212 F s(xs\u2217) )] \u2264 3E [ F (x\u03020)\u2212 F (x\u0302p) ] .\nProof. Telescoping Lemma 7.4 for all the subepochs s = 0, 1, . . . , p\u2212 1, we have p\u22121\u2211\ns=0\nE [( F s(x\u0302s+1)\u2212 F s(xs\u2217) )]\n\u00ac \u2264\np\u22121\u2211\ns=0\nE [F s(x\u0302s)\u2212 F s(xs\u2217)\n\u03c3\u03b1m/2 + \u03b1p2\u03a6s + \u3008\u2207f s(x\u0302s+1), x\u0302s \u2212 x\u0302s+1\u3009+\n( \u03c8(x\u0302s)\u2212 \u03c8(x\u0302s+1)\n)]\n \u2264\np\u22121\u2211\ns=0\nE [F s(x\u0302s)\u2212 F s(xs\u2217)\n\u03c3\u03b1m/2 + \u3008\u2207f s(x\u0302s+1), x\u0302s \u2212 x\u0302s+1\u3009+\n( \u03c8(x\u0302s)\u2212 \u03c8(x\u0302s+1) )\n+ \u03b1p2C(`1 + `2) \u00b7 (fs(x\u0302s)\u2212 f s(x\u0302s+1)\u2212 \u3008\u2207fs(x\u0302s+1), x\u0302s \u2212 x\u0302s+1\u3009) ] + \u03b1p2C(`1`2) \u00b7 \u2016x\u0302s+1 \u2212 x\u0302s\u20162 ]\n\u00ae \u2264\np\u22121\u2211\ns=0\nE [F s(x\u0302s)\u2212 F s(xs\u2217)\n\u03c3\u03b1m/2 + \u3008\u2207f s(x\u0302s+1), x\u0302s \u2212 x\u0302s+1\u3009+\n( \u03c8(x\u0302s)\u2212 \u03c8(x\u0302s+1) )\n+ (fs(x\u0302s)\u2212 fs(x\u0302s+1)\u2212 \u3008\u2207fs(x\u0302s+1), x\u0302s \u2212 x\u0302s+1\u3009) ] + 2\u03c3 \u00b7 \u2016x\u0302s+1 \u2212 x\u0302s\u20162 ]\n=\np\u22121\u2211\ns=0\nE [F s(x\u0302s)\u2212 F s(xs\u2217) \u03c3\u03b1m/2 + ( F s(x\u0302s)\u2212 F s(x\u0302s+1) ) + 2\u03c3 \u00b7 \u2016x\u0302s+1 \u2212 x\u0302s\u20162 ]\n\u00af =\np\u22121\u2211\ns=0\nE [F s(x\u0302s)\u2212 F s(xs\u2217) \u03c3\u03b1m/2 + ( F s(x\u0302s+1)\u2212 F s(x\u0302s) ) \u2212 2 ( F (x\u0302s+1)\u2212 F (x\u0302s) )]\nAbove, inequality \u00ac uses Lemma 7.4 and \u03a6s \u2265 0; inequality  uses the definition of \u03a6s from Lemma 7.2; inequality \u00ae uses \u03b1p2C(`1 + `2) \u2264 1 and \u03b1p2C(`1`2) \u2264 2\u03c3; and equality \u00af uses the definition F s(y) = F (y) + \u03c3\u2016y \u2212 x\u0302s\u20162.\nFinally, rearranging both sides, and using the fact that 1\u03c3\u03b1m \u2264 16 , we have p\u22121\u2211\ns=0\nE [( F s(x\u0302s)\u2212 F s(xs\u2217) )] \u2264 3E [ F (x\u03020)\u2212 F (x\u0302p) ] ."}, {"heading": "8 Full Method: Final Theorem", "text": "We are now ready to state and prove our main convergence theorem for Natashafull:\nTheorem 2. Suppose f(x) is L-smooth, each fi(x) is (`1, `2)-smooth, and F (x) is \u03c3-strongly\nnonconvex, for \u03c3 \u2264 min{`1, `2, L}. If `1`2\u03c32 \u2264 n, p = \u0398 ( ( \u03c3 2 `1`2 n)1/3 ) and \u0398( \u03c3 p2`1`2 ), Natashafull outputs\na point xout satisfying E[\u2016G\u03b7(xout)\u20162] \u2264 \u03b52 for every \u03b7 \u2208 ( 0, 1max{L,4\u03c3} )\nwith total gradient complexity O ( n log 1\u03b5 + (`1`2\u03c3)1/3n2/3 \u03b52 \u00b7 (F (x\u2205)\u2212 F \u2217) ) .\nRemark 8.1. One can assume \u03c3 \u2264 L without loss of generality because any L-smooth function is also L-strongly nonconvex. One can assume \u03c3 \u2264 `2 without loss of generality because f(x) is `2-strongly nonconvex if each fi(x) is (`1, `2)-smooth. Only \u03c3 \u2264 `1 is a minor requirement for Theorem 2, but if this is not true, one can replace `1 with \u03c3 before applying Theorem 2. Remark 8.2. In Theorem 2 we have assumed `1`2 \u03c32 \u2264 n2. If this inequality does not hold, one should apply repeatSVRG instead and it gives faster running time (see Figure 1). More specifically, repeatSVRG gives a refined complexity of O\u0303 (n\u03c3+n3/4(`1`2\u03c32)1/4\n\u03b52\n) under a mild assumption of \u03c3 \u2265 \u03b52.\nProof of Theorem 2. One can verify that our choices of p and \u03b1 satisfy p \u2208 [n], \u03b1 \u2264 O( \u03c3 p2`1`2 ) and \u03b1 \u2265 \u2126( 1\u03c3m), so we can apply Lemma 7.5 and telescope it for the entire algorithm (which has T \u2032 full epochs). Use the fact that x\u0302p of the previous epoch equals x\u03020 of the next epoch, we conclude that if we choose a random epoch and a random subepoch s, we will have\nE[F s(x\u0302s)\u2212 F s(xs\u2217)] \u2264 3 pT \u2032 (F (x\u2205)\u2212 F \u2217) .\nBy the \u03c3-strong convexity of F s(\u00b7), we have E[\u03c3\u2016x\u0302s \u2212 xs\u2217\u20162] \u2264 6pT \u2032 (F (x\u2205)\u2212 F \u2217). Now, F s(x) = F (x) + \u03c3\u2016x\u2212 x\u0302s\u20162 satisfies the assumption of G(x) in Lemma 4.1. If we use the SVRG method (see Theorem 2.3) to minimize the convex function F s(x), we get an output xout satisfying F s(xout)\u2212 F s(xs\u2217) \u2264 \u03b52\u03c3 in gradient complexity O ( (n+ `1`2 \u03c32 ) log 1\u03b5 ) \u2264 O(n log 1\u03b5 ).\nWe can therefore apply Lemma 4.1 and conclude that this output xout satisfies\nE[\u2016G\u03b7(xout)\u20162] \u2264 O ( \u03c3 pT \u2032 ) \u00b7 (F (x\u2205)\u2212 F \u2217) = O ((`1`2\u03c3)1/3n2/3 T \u2032n ) \u00b7 (F (x\u2205)\u2212 F \u2217) .\nIn other words, we obtain E[\u2016G\u03b7(xout)\u20162] \u2264 \u03b52 with gradient complexity\nT \u2032n = O ( n+ (`1`2\u03c3) 1/3n2/3\n\u03b52 \u00b7 (F (x\u2205)\u2212 F \u2217)\n) .\nHere, the additive term n is because the gradient complexity is T \u2032n but T \u2032 is at least 1."}, {"heading": "Acknowledgements", "text": "This paper is partially supported by a Microsoft Research Award, no. 0518584, and an NSF grant, no. CCF-1412958. We thank Yuanzhi Li for enlightening conversations.\nAppendix"}, {"heading": "A From Stationary Points to Local Minima", "text": "Recently, researchers have shown that the general problem of finding (\u03b5, \u03c1)-approximate local minima, under mild conditions, reduces to (repeatedly) finding \u03b5-approximate stationary points for an O(\u03c1)-strongly nonconvex function [1, 9]. We sketch the details here for the sake of completeness, in the special case of \u03c8(x) \u2261 0.10\nWe say that a point x is (\u03b5, \u03b4)-approximate local minimum, if \u2016\u2207f(x)\u2016 \u2264 \u03b5 and \u22072f(x) \u2212\u03b4I. Carmon et al. [9] showed that an (\u03b5, \u03b4)-approximate minimum for the general problem (1.1) can be solved via the following iterative procedure. In every iteration at point xt, detect whether the smallest eigenvalue of \u22072f(xt) is below \u2212\u03b4: \u2022 if yes, find the smallest eigenvector of \u22072f(xt) approximately and move in this direction. (One\ncan use for instance the shift-and-invert method [12].)\n\u2022 if no, define ft(x) = f(x)+L ( max { 0, \u2016x\u2212xt\u2016\u2212 \u03b4L2 })2\nwhere L2 is the second-order smoothness of f(x) and ft(x) can be proven as 5L-smooth and 3\u03b4-strongly nonconvex; we then find an \u03b5-approximate stationary point of f \u2032(x) and move there.\nThe Trade-Off on \u03b4. The final running time of the above algorithm depends on the maximum between (1) the eigenvector computation and (2) the stationary-point computation. The larger \u03b4 is, the faster (1) becomes and the slower (2) becomes; the smaller \u03b4 is, the faster (2) becomes and the slower (1) becomes.\nAs argued in [1, 9], if the Hessian-vector multiplication ( \u22072fi(x) ) v for an arbitrary vector runs in the same time as computing \u2207fi(x) \u2014which is the case for training neural nets\u2014 the optimum trade-off is \u03b4 = \u221a L2\u03b5. This again confirms that in strongly non-convex optimization, the parameter \u03b4 can usually be much smaller than L."}], "references": [{"title": "Finding Approximate Local Minima for Nonconvex Optimization in Linear Time", "author": ["Naman Agarwal", "Zeyuan Allen-Zhu", "Brian Bullins", "Elad Hazan", "Tengyu Ma"], "venue": "ArXiv e-prints,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods", "author": ["Zeyuan Allen-Zhu"], "venue": "ArXiv e-prints,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Variance Reduction for Faster Non-Convex Optimization", "author": ["Zeyuan Allen-Zhu", "Elad Hazan"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "ArXiv e-prints,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Follow the Compressed Leader: Faster Algorithm for Matrix Multiplicative Weight Updates", "author": ["Zeyuan Allen-Zhu", "Yuanzhi Li"], "venue": "ArXiv e-prints,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent", "author": ["Zeyuan Allen-Zhu", "Lorenzo Orecchia"], "venue": "In Proceedings of the 8th Innovations in Theoretical Computer Science, ITCS \u201917,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non- Convex Objectives", "author": ["Zeyuan Allen-Zhu", "Yang Yuan"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Accelerated Methods for Non-Convex Optimization", "author": ["Yair Carmon", "John C. Duchi", "Oliver Hinder", "Aaron Sidford"], "venue": "ArXiv e-prints,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["Roy Frostig", "Rong Ge", "Sham M. Kakade", "Aaron Sidford"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation", "author": ["Dan Garber", "Elad Hazan", "Chi Jin", "Sham M. Kakade", "Cameron Musco", "Praneeth Netrapalli", "Aaron Sidford"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": "In Proceedings of the 28th Annual Conference on Learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Gradient descent only converges to minimizers", "author": ["Jason D. Lee", "Max Simchowitz", "Michael I. Jordan", "Benjamin Recht"], "venue": "In Proceedings of the 29th Conference on Learning Theory, COLT 2016,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "A Universal Catalyst for First-Order Optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Introductory Lectures on Convex Programming Volume: A Basic course, volume I", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Accelerating the cubic regularization of newton\u2019s method on convex problems", "author": ["Yurii Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Stochastic variance reduction for nonconvex optimization", "author": ["Sashank J. Reddi", "Ahmed Hefny", "Suvrit Sra", "Barnabas Poczos", "Alex Smola"], "venue": "ArXiv e-prints,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "SDCA without Duality, Regularization, and Individual Convexity", "author": ["Shai Shalev-Shwartz"], "venue": "In ICML,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Linear convergence with condition number independent access of full gradients", "author": ["Lijun Zhang", "Mehrdad Mahdavi", "Rong Jin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "In many interesting practical problems \u2014such as training neural nets and classifications with sigmoid loss, see [3] for details\u2014 neither fi(x) or the overall f(x) is convex.", "startOffset": 112, "endOffset": 115}, {"referenceID": 17, "context": "In particular, second-order literatures usually find (\u03b5, \u221a \u03b5)-approximate local minima [18], and this corresponds to \u03c3 = \u221a \u03b5.", "startOffset": 87, "endOffset": 91}, {"referenceID": 7, "context": "Until recently, nearly all research papers have been mostly focusing on either \u03c3 = 0 so f(x) is convex, or \u03c3 = L so f(x) is simply L-smooth: \u2022 If \u03c3 = 0, the accelerated SVRG method [8, 21] find a point x satisfying F (x) \u2212 F (x\u2217) \u2264 \u03b5, in gradient complexity \u00d5 ( n+ n3/4 \u221a L/\u03b5 ) .", "startOffset": 181, "endOffset": 188}, {"referenceID": 20, "context": "Until recently, nearly all research papers have been mostly focusing on either \u03c3 = 0 so f(x) is convex, or \u03c3 = L so f(x) is simply L-smooth: \u2022 If \u03c3 = 0, the accelerated SVRG method [8, 21] find a point x satisfying F (x) \u2212 F (x\u2217) \u2264 \u03b5, in gradient complexity \u00d5 ( n+ n3/4 \u221a L/\u03b5 ) .", "startOffset": 181, "endOffset": 188}, {"referenceID": 2, "context": "2 \u2022 If \u03c3 = L, the SVRG method [3] finds an \u03b5-approximate stationary point of F (x) with gradient complexity O(n+ n2/3L/\u03b52).", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "Very recently, it was observed by two independent groups [1, 9] \u2014although implicitly, see Section 2.", "startOffset": 57, "endOffset": 63}, {"referenceID": 8, "context": "Very recently, it was observed by two independent groups [1, 9] \u2014although implicitly, see Section 2.", "startOffset": 57, "endOffset": 63}, {"referenceID": 2, "context": "Note that our result matches that of SVRG [3] for \u03c3 = L, and has a much simpler analysis.", "startOffset": 42, "endOffset": 45}, {"referenceID": 11, "context": "Using the so-called shift-and-invert reduction [12], computing leading eigenvector reduces to a convex version of problem (1.", "startOffset": 47, "endOffset": 51}, {"referenceID": 4, "context": "Other examples include all the applications that are built on shift-and-invert, including high rank SVD/PCA [5], canonical component analysis [4], online matrix learning [6], and approximate local minima algorithms [1, 9].", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "Other examples include all the applications that are built on shift-and-invert, including high rank SVD/PCA [5], canonical component analysis [4], online matrix learning [6], and approximate local minima algorithms [1, 9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 5, "context": "Other examples include all the applications that are built on shift-and-invert, including high rank SVD/PCA [5], canonical component analysis [4], online matrix learning [6], and approximate local minima algorithms [1, 9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 0, "context": "Other examples include all the applications that are built on shift-and-invert, including high rank SVD/PCA [5], canonical component analysis [4], online matrix learning [6], and approximate local minima algorithms [1, 9].", "startOffset": 215, "endOffset": 221}, {"referenceID": 8, "context": "Other examples include all the applications that are built on shift-and-invert, including high rank SVD/PCA [5], canonical component analysis [4], online matrix learning [6], and approximate local minima algorithms [1, 9].", "startOffset": 215, "endOffset": 221}, {"referenceID": 13, "context": "We first recall the main ideas of stochastic variance-reduced gradient methods, such as SVRG [14].", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "For instance, the result of [1, 9] requires \u03b5 \u2264 \u03c3.", "startOffset": 28, "endOffset": 34}, {"referenceID": 8, "context": "For instance, the result of [1, 9] requires \u03b5 \u2264 \u03c3.", "startOffset": 28, "endOffset": 34}, {"referenceID": 2, "context": "In nearly all the aforementioned results for strongly nonconvex optimization, researchers have either directly applied SVRG [3, 19] (for the case \u03c3 = L), or repeatedly applied SVRG [1, 9] (for general \u03c3 \u2208 [0, L]).", "startOffset": 124, "endOffset": 131}, {"referenceID": 18, "context": "In nearly all the aforementioned results for strongly nonconvex optimization, researchers have either directly applied SVRG [3, 19] (for the case \u03c3 = L), or repeatedly applied SVRG [1, 9] (for general \u03c3 \u2208 [0, L]).", "startOffset": 124, "endOffset": 131}, {"referenceID": 0, "context": "In nearly all the aforementioned results for strongly nonconvex optimization, researchers have either directly applied SVRG [3, 19] (for the case \u03c3 = L), or repeatedly applied SVRG [1, 9] (for general \u03c3 \u2208 [0, L]).", "startOffset": 181, "endOffset": 187}, {"referenceID": 8, "context": "In nearly all the aforementioned results for strongly nonconvex optimization, researchers have either directly applied SVRG [3, 19] (for the case \u03c3 = L), or repeatedly applied SVRG [1, 9] (for general \u03c3 \u2208 [0, L]).", "startOffset": 181, "endOffset": 187}, {"referenceID": 1, "context": "The technique of having the gradients computed at a point xt but moving with respect to a different sequence zt is related to the Katyusha momentum recently developed for convex optimization [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 19, "context": "The first such method is SAG by Schmidt et al [20].", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "The two most popular choices for gradient estimators are the SVRG-like one we adopted in this paper (independently introduced by [14, 22], and the SAGA-like one introduced by [10].", "startOffset": 129, "endOffset": 137}, {"referenceID": 21, "context": "The two most popular choices for gradient estimators are the SVRG-like one we adopted in this paper (independently introduced by [14, 22], and the SAGA-like one introduced by [10].", "startOffset": 129, "endOffset": 137}, {"referenceID": 9, "context": "The two most popular choices for gradient estimators are the SVRG-like one we adopted in this paper (independently introduced by [14, 22], and the SAGA-like one introduced by [10].", "startOffset": 175, "endOffset": 179}, {"referenceID": 20, "context": "The first \u201cnon-convex use\u201d of variance reduction is by Shalev-Shwartz [21] who assumes that each fi(x) is non-convex but their average f(x) is still convex.", "startOffset": 70, "endOffset": 74}, {"referenceID": 7, "context": "This result has been slightly improved to several more refined settings [8].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": ", for f(x) being also non-convex) is independently by [3] and [19].", "startOffset": 54, "endOffset": 57}, {"referenceID": 18, "context": ", for f(x) being also non-convex) is independently by [3] and [19].", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "The first such result is by cubic regularized Newton\u2019s method [18]; however, its per-iteration complexity is very slow.", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "Very recently, two independent groups of authors tackled this problem from a somewhat similar viewpoint [1, 9]: if the computation of Hessian-vector multiplications (i.", "startOffset": 104, "endOffset": 110}, {"referenceID": 8, "context": "Very recently, two independent groups of authors tackled this problem from a somewhat similar viewpoint [1, 9]: if the computation of Hessian-vector multiplications (i.", "startOffset": 104, "endOffset": 110}, {"referenceID": 8, "context": "[9] only stated a complexity of \u00d5 ( n \u03b51.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] where the authors showed that a noise-injected version of SGD converges to local minima instead of critical points, as long as the underlying function is \u201cstrict-saddle.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] showed that gradient descent, starting from a random point, almost surely converges to a local minimum of a \u201cstrict-saddle\u201d function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The (`1, `2)-smoothness parameters were introduced in [8] to tackle the convex setting of problem (1.", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "The notion of strong nonconvexity is also known as \u201clower smoothness [8]\u201d or \u201calmost convexity [9]\u201d.", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "The notion of strong nonconvexity is also known as \u201clower smoothness [8]\u201d or \u201calmost convexity [9]\u201d.", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "The following theorem for the SVRG method can be found for instance in [8], which is built on top of the results [11, 16, 21]:", "startOffset": 71, "endOffset": 74}, {"referenceID": 10, "context": "The following theorem for the SVRG method can be found for instance in [8], which is built on top of the results [11, 16, 21]:", "startOffset": 113, "endOffset": 125}, {"referenceID": 15, "context": "The following theorem for the SVRG method can be found for instance in [8], which is built on top of the results [11, 16, 21]:", "startOffset": 113, "endOffset": 125}, {"referenceID": 20, "context": "The following theorem for the SVRG method can be found for instance in [8], which is built on top of the results [11, 16, 21]:", "startOffset": 113, "endOffset": 125}, {"referenceID": 0, "context": "We remark here that the above complexity of repeatSVRG can be inferred from papers [1, 9], but is not explicitly stated.", "startOffset": 83, "endOffset": 89}, {"referenceID": 8, "context": "We remark here that the above complexity of repeatSVRG can be inferred from papers [1, 9], but is not explicitly stated.", "startOffset": 83, "endOffset": 89}, {"referenceID": 8, "context": "For instance, the paper [9] does not allow F (x) to have a non-smooth proximal term \u03c8(x), and applies accelerated gradient descent instead of accelerated SVRG.", "startOffset": 24, "endOffset": 27}, {"referenceID": 8, "context": "(Notice that when \u03c8(x) \u2261 0 this lemma is trivial, and can be found for instance in [9]).", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "The idea for this second kind of retraction, and the idea of having the updates on a sequence zt but computing gradients at points xt, is largely motivated by our recent work on the Katyusha momentum and the Katyusha acceleration [2].", "startOffset": 230, "endOffset": 233}, {"referenceID": 6, "context": "The following inequality is classically known as the \u201cregret inequality\u201d for mirror descent [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 16, "context": "5 in textbook [17]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "Recently, researchers have shown that the general problem of finding (\u03b5, \u03c1)-approximate local minima, under mild conditions, reduces to (repeatedly) finding \u03b5-approximate stationary points for an O(\u03c1)-strongly nonconvex function [1, 9].", "startOffset": 229, "endOffset": 235}, {"referenceID": 8, "context": "Recently, researchers have shown that the general problem of finding (\u03b5, \u03c1)-approximate local minima, under mild conditions, reduces to (repeatedly) finding \u03b5-approximate stationary points for an O(\u03c1)-strongly nonconvex function [1, 9].", "startOffset": 229, "endOffset": 235}, {"referenceID": 8, "context": "[9] showed that an (\u03b5, \u03b4)-approximate minimum for the general problem (1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "(One can use for instance the shift-and-invert method [12].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "As argued in [1, 9], if the Hessian-vector multiplication ( \u2207fi(x) ) v for an arbitrary vector runs in the same time as computing \u2207fi(x) \u2014which is the case for training neural nets\u2014 the optimum trade-off is \u03b4 = \u221a L2\u03b5.", "startOffset": 13, "endOffset": 19}, {"referenceID": 8, "context": "As argued in [1, 9], if the Hessian-vector multiplication ( \u2207fi(x) ) v for an arbitrary vector runs in the same time as computing \u2207fi(x) \u2014which is the case for training neural nets\u2014 the optimum trade-off is \u03b4 = \u221a L2\u03b5.", "startOffset": 13, "endOffset": 19}], "year": 2017, "abstractText": "Given a non-convex function f(x) that is an average of n smooth functions, we design stochastic first-order methods to find its approximate stationary points. The performance of our new methods depend on the smallest (negative) eigenvalue \u2212\u03c3 of the Hessian. This parameter \u03c3 captures how strongly non-convex f(x) is, and is analogous to the strong convexity parameter for convex optimization. Our methods outperform the best known results for a wide range of \u03c3, and can also be used to find approximate local minima. In particular, we find an interesting dichotomy: there exists a threshold \u03c30 so that the fastest methods for \u03c3 > \u03c30 and for \u03c3 < \u03c30 have drastically different behaviors: the former scales with n and the latter scales with n.", "creator": "LaTeX with hyperref package"}}}