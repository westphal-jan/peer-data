{"id": "1702.00860", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2017", "title": "Topic Modeling the H\\`an di\\u{a}n Ancient Classics", "abstract": "tibet archives places a emerging archive of enormous challenge and opportunity for humanities studies interested in powerful computational methods to assist in the formulation of new insights meaning interpretations, particularly significant materials. 2002 exchange paper liang organized a collaborative effort when korea university and dun'an history university together support exploration as interpretation of which large corpus of among 650, 000 medieval classical documents, which we refer to \" the \" dong \" ancient classics corpus ( da \\ `, di \\ bao { a } n g \\ u { u } \u03b5 \\'i, i. mean, literally \" han renaissance \" or \" wang classics \" ). it contains classics documenting ancient beijing texts, documents of research and biographical significance, and literary works. and commence loosely describing the earlier humanities context of this joint project, suggesting the advances in humanities computing that marked this project significant. we describe the corpus these introduce our vocabulary of probabilistic topic modeling from us corpus, with contrast to the particular challenges posed by generating ancient chinese documents. to include a specific idea of how chinese software labs have developed thus move organized within aid consolidation and comprehension of themes describing the corpus. we contain pioneering advanced forms of matrix - aided imagery that are subsequently made possible by graphical programming platforms provided by our system, and the general implications of these databases allows showcasing his nature of computation modeling these texts.", "histories": [["v1", "Thu, 2 Feb 2017 22:51:04 GMT  (2635kb)", "http://arxiv.org/abs/1702.00860v1", "24 pages; 14 pages supplemental"]], "COMMENTS": "24 pages; 14 pages supplemental", "reviews": [], "SUBJECTS": "cs.CL cs.CY cs.DL cs.HC cs.IR", "authors": ["colin allen", "hongliang luo", "jaimie murdock", "jianghuai pu", "xiaohong wang", "yanjie zhai", "kun zhao"], "accepted": false, "id": "1702.00860"}, "pdf": {"name": "1702.00860.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Colin ALLEN", "Hongliang LUO", "Jaimie MURDOCK", "Jianghuai PU", "Xiaohong WANG", "Yanjie ZHAI", "Kun ZHAO"], "emails": [], "sections": [{"heading": null, "text": "1 of 24\nTopic Modeling the H\u00e0n di\u0103n Ancient Classics (\u6c49\u5178\u53e4\u7c4d) Colin ALLEN1,2, Hongliang LUO3, Jaimie MURDOCK2, Jianghuai PU1, Xiaohong WANG1, Yanjie ZHAI1, Kun ZHAO3 1 Department of Philosophy, School of Humanities and Social Sciences, Xi\u2019an Jiaotong\nUniversity, Shaanxi, China 2 Cognitive Science Program, Indiana University, Bloomington, Indiana, USA 3 Institute of Computer Software and Theory, School of Electronic and Information Engineering,\nXi\u2019an Jiaotong University, Shaanxi, China Authors listed alphabetically Abstract: Ancient Chinese texts present an area of enormous challenge and opportunity for humanities scholars interested in exploiting computational methods to assist in the development of new insights and interpretations of culturally significant materials. In this paper we describe a collaborative effort between Indiana University and Xi\u2019an Jiaotong University to support exploration and interpretation of a digital corpus of over 18,000 ancient Chinese documents, which we refer to as the \u201cHandian\u201d ancient classics corpus (\u6c49\u5178\u53e4\u7c4d or H\u00e0n di\u0103n g\u016d j\u00ed, i.e, the \u201cHan canon\u201d or \u201cChinese classics\u201d). It contains classics of ancient Chinese philosophy, documents of historical and biographical significance, and literary works. We begin by describing the Digital Humanities context of this joint project, and the advances in humanities computing that made this project feasible. We describe the corpus and introduce our application of probabilistic topic modeling to this corpus, with attention to the particular challenges posed by modeling ancient Chinese documents. We give a specific example of how the software we have developed can be used to aid discovery and interpretation of themes in the corpus. We outline more advanced forms of computer-aided interpretation that are also made possible by the programming interface provided by our system, and the general implications of these methods for understanding the nature of meaning in these texts.\n2 of 24\nIntroduction and Context The use of computers to support scholarship in the humanities reaches back over 50 years.1 The first decades of the twenty-first Century have seen the acceleration of humanities computing, particularly in North America and Europe, with the field coalescing around the label \u201cDigital Humanities\u201d (DH). The recent growth of DH is the product of a feedback loop caused by several factors including: (1) the increasing availability of digitized materials, especially on the World Wide Web; (2) increased computer storage capacity and processing capacity; (3) advances in text modeling and visualization algorithms; (4) deepening understanding by scholars of the interpretive possibilities provided by computational methods; (5) funding commitments from government and private foundations; and (6) last, but not least, the growing perception among many young scholars and doctoral students that DH is an exciting area of inquiry and an important enhancement to their career prospects.2 DH projects may concern themselves with many different media: text, images, audio, video, etc. However, our focus here is the analysis of written texts. Textual analysis constitutes the largest component of DH. This is largely because written language has been central to the construction and transmission of intellectual culture, and because of the relative ease with which text can be encoded and shared. These factors have resulted in enormous amounts of textual material recently becoming available. As an example from category (1), increased availability of digitized materials, we highlight the HathiTrust (HT) digital library.3 It started as a collaboration among major university research libraries in the United States, to digitally scan the books in their collections.4 The page images from these books have been converted to text using optical character recognition (OCR) software. The HT collection now comprises over 14 million scanned volumes, equivalent to around five billion (5,000,000,000) pages based on the HathiTrust estimated average of 350 pages per book.5 (Perhaps as many as half a million of these books are Chinese language volumes, as determined by a search at babel.hathitrust.org.) By any standard, this is a vast amount of text: more than could be read in multiple human lifetimes. Because of its enormous scale, the digitized pages in the HT are relatively uncurated. Despite the care with which editors prepared the original physically printed editions, the images and OCR representations of the pages contain scanning errors that have not been corrected. Nevertheless, the HT digital library is a treasure trove for DH that offers multiple possibilities for analysis.6 At the same time, traditional scholarly editions have become increasingly digital, making available highly curated editions of historically and culturally significant text corpora. These projects use a laborintensive process of inserting markup according to widely adopted standards for the semantic web, such as TEI (Text Encoding Initiative) and OWL (Web Ontology Language).7 Only some of these editions are fully accessible to all users, however, because the costs of producing and curating them must often be recovered by subscriptions paid by libraries or individual users. For some scholarly purposes, the standard of care with which such editions are produced is essential, but access to imperfectly digitized texts, such as provided by the HathiTrust and by Project Gutenberg,8 is adequate for many projects. However, even full access to relatively open archives such as the HathiTrust faces some restrictions because of complicated copyright issues which vary from country to country.\n3 of 24\nNone of the large-scale repositories would have been possible without the exponential growth in computer capacity that has occurred since the advent of computing. Known as \u201cMoore\u2019s Law\u201d, the doubling of computer speed and memory every 18 months for the same cost has produced supercomputers capable of processing terabytes of data, and personal laptops capable of storing and processing far more material than any single person could hope to read in several lifetimes. At the same time, the Internet and its hypertext offshoot, the World Wide Web, have made distributed repositories and cluster computing possible. This growth in speed, storage, and networking, has been accompanied by increasingly sophisticated algorithms for processing text and visualizing the results. The earliest efforts in humanities computing focused mainly on counting and localizing key words in texts. Other DH approaches applied network analysis to names, dates, places and other metadata such as citations, extracted from text. More recently, techniques for modeling full text contents have been introduced by computer scientists. Originally developed for the purposes of information retrieval, techniques such as latent semantic analysis (LSA), probabilistic topic modeling using latent Dirichlet allocation (LDA), and neural-network models of word embeddings, have been adopted within DH.9 Although differing in their details, what these methods have in common is their representation of documents and words as vectors within a multidimensional space. In some representations, the dimensions of the space correspond to words. In other representations, the dimensions may correspond to concepts, topics, or other abstractions from the data. Algorithms based on these vector representations are capable of identifying hidden (latent) factors in text. Such representations allow for interesting and meaningful measures of similarity among terms and documents, for example the cosine of the angle between vectors, or well-defined informationtheoretic measures on the probability spaces. With such methods, DH scholars are beginning to move beyond counting words, to detecting and analyzing patterns of historical significance at cultural scale10 and at the scale of an individual.11 There is a small but growing literature on large-scale statistical modeling of Chinese language texts. Ouyang analyzed a corpus of over 40,000 ancient documents downloaded from multiple sources. This was used to plot the temporal distributions of word frequencies and geographic distributions of authors.12 Huang and Yu modeled the SongCi poetry corpus, first converting it to tonally marked pinyin to conserve poetically important pronunciation information.13 Nichols and colleagues reported initial modeling of the Chinese Text Project corpus14 in a conference paper. (Further below, we describe differences between this corpus and the Handian.) With additional collaborators, this group has now conducted two studies that are currently unpublished but under review. In the first, they apply topic models to address scholarly questions about the relationships among important texts of Ancient Chinese philosophy. In the second, they use topic modeling to investigate the concepts of mind and body in ancient Chinese philosophy.15 Although we share similar scholarly objectives with these researchers, our approach in this paper is unique in that for the first time anywhere we bring the benefits of computational modeling of ancient Chinese texts to a robust public platform that is mirrored on both sides of the Pacific. Besides being just a useful portal to the texts, our approach foregrounds the\n4 of 24\ninterpretive issues surrounding topic models,16 and makes more sophisticated exploration and analysis of interpretive questions possible for experts and novices alike. The Chinese language presents interesting challenges for humanities computing. Both modern and ancient Chinese, but especially the latter, rely heavily on context for the interpretation of individual characters and words17 and some researchers have argued that differences in Chinese morphology make some of the techniques that work well for DH work in Western languages less applicable to Chinese.18 Words in Chinese are highly polysemous, requiring considerable amounts of context for their proper interpretation. The study of ancient Chinese philosophy is especially challenging because this ambiguity and openness to multiple interpretation seems to be deliberately exploited by the ancient masters.19 Take, for example the character \u2018\u9053\u2019 which could refer to Taoism, but has up to 10 meanings in ancient Chinese texts, such as \u2018way\u2019 or \u2018road\u2019, and is also used as a verb to mean \u2018say\u2019. At the same time, the long and relatively continuous history of the Chinese nation has enabled the transmission of a rich corpus of ancient texts to the present day. Computational modeling of these texts does not, as we see it, aim to remove the human from the humanities. Rather, by enabling the discovery and quantitative analysis of connections, computational methods promise at least these two benefits: (i) enhanced means of access to large sets of documents, and (ii) new sources of evidence about texts that can support the ongoing discussion of their interpretation relative to the past and the present. We are also interested in a more general theme (iii), concerning the potential broader significance for theoretical discussions of the nature of meaning and the role of language in conceptual schemes. Our primary contribution in this paper is of type (i), to provide enhanced access to a corpus of ancient Chinese documents. Specifically, we introduce an application of the InPhO Topic Explorer20 developed at Indiana University, Bloomington, USA, to a large, public corpus of ancient Chinese texts, resulting from collaboration with philosophers and computer scientists at Xi\u2019an Jiaotong University, Shaanxi, China. We also discuss potential projects and future research of type (ii) concerning the analysis of the themes in ancient Chinese philosophy and other literary sources. We present a very brief discussion of the broader significance (iii) before the conclusions section of this paper. Select ing and Preparing the Corpus A good understanding of Chinese intellectual culture during the classical period is important in itself, and essential for understanding the reception of Western ideas during various stages of China\u2019s history, and vice versa. As philosophers, we are particularly interested in philosophical texts, but we recognize that the boundaries between philosophy and other areas such as religion and political theory are fuzzy at best, and practically non-existent in some cultures or during certain periods of history. Thus, rather than try to demarcate \u201cphilosophy\u201d from the rest, we decided to pursue our computational inquiry with as broad a corpus as we could locate. A secondary consideration is that we want our work to provide a public benefit by being accessible to scholars and the public. It is less than optimal to analyze sources that only a few\n5 of 24\npeople -- not even all scholars -- have access to. For example, although the Wenyuange Edition of the Siku Quanshu archive21 is of high quality for scholars, it is accessible only to those with subscriptions that are locked to specific IP addresses. Thus we conducted a scan of repositories of ancient Chinese documents, and found that the crowd-sourced website at zdic.net provided the best combination of quantity and access to a large number of classic texts, thanks to its permissive re-use policy under a Creative Commons 1.0 Public Domain Dedication.22 The full website at www.zdic.net contains a dictionary of Chinese characters, a dictionary of words, dictionary of idioms and several other resources. Among them is the collection of classics identified as \u6c49\u5178\u53e4\u7c4d (H\u00e0n di\u0103n g\u016d j\u00ed) or Chinese classics\u2014 the portion we refer to as the \u201cHandian\u201d corpus \u2014 directly accessible at http://gj.zdic.net/, and it is this portion of the website that we chose to model. This section of the website is not without problems, however. It contains a diverse collection of different file formats, containing both traditional and simplified characters, and of varying quality because they have been crowd-sourced from many different users using many different sources, with varying degrees of scholarly care. A better-curated corpus is the Chinese Text Project used by Nichols, Slingerland and colleagues.23 Although this site can be downloaded for private and academic use, its re-use policy is not as permissive as the Handian, and the online analysis tools require a subscription. Furthermore, because ctext.org is registered in Panama and hosted in the USA as well as directed towards Englishspeaking users, access by users in mainland China is generally slower and more difficult than zdic.net, which is registered and hosted in China. For our initial goals, the benefits of accessibility, especially to Chinese users, outweighed the concerns about corpus curation quality. Such concerns are also partly mitigated by the topic modeling methods (described in more detail below). Because topic models treat documents as unordered \u201cbags of words\u201d, they are relatively robust in the face of the \u201cnoise\u201d provided by the variable quality of the texts. The techniques we describe here can be applied to more scholarly editions of the same texts. By demonstrating the power of the approach with the Handian corpus, we hope to encourage curators of scholarly editions to incorporate similar methods and make their efforts publicly available. We have made the products of our research available for all at our Indiana University website in the USA, mirrored at the Xi\u2019an Jiaotong University website in China.24 In November of 2016 we crawled and downloaded the four sections of the Han classics from the gj.zdic.net site. These sections, which are derived from the Siku Quanshu (the library of the Qianlong Emperor in Four Sections) are the \u7ecf\u90e8 (J\u012bng B\u00f9), containing Confucian classics, \u53f2\u90e8 (Sh\u01d0 B\u00f9), historiographic works, \u5b50\u90e8 (Z\u01d0 B\u00f9), containing writings of the philosophical schools, and \u96c6\u90e8 (J\u00ed B\u00f9), a section of miscellaneous anthologies, including poetry, drama, and other works of literature. Each of the sections contains a multi-level tree of further subsections terminating in text files. For example, within the \u7ecf (Jing) section are three subsections, labeled \u5341\u4e09\u7ecf (thirteen classics), \u5341\u4e09\u7ecf\u6ce8\u758f (thirteen classics annotations), and \u7ecf\u5b66\u53f2\u53ca\u5c0f\u5b66\u7c7b (\u201chistory of classical studies and traditional Chinese philology\u201d), and these are further subdivided.25 We found that some of the files were index files listing the contents of the directories, so we discarded these.\n6 of 24\nWe developed some custom mixture of automated and semi-automated methods to extract the original texts from the downloaded HTML pages. Next we cleaned the corpus by regularizing the characters and their encoding method. Because of the mixture of traditional and simplified characters in the corpus, we decided to map all characters to simplified characters. This entails a loss of visual, aural and etymological information, important for interpretation by knowledgeable readers, but of no direct use to the algorithms beneath the topic modeling process. (In the future we will provide additional support for both traditional and simplified characters within the Topic Explorer.) After this preliminary processing, we found that quite a few files were empty \u2014 some representing documents lost to history, others not present for other reasons. So, we removed these files leaving 18,818 files for analysis.26 These files contain approximately 100 million individual characters. Chinese does not use spaces to separate words, but some words comprise multiple characters. Hence, text modelers face a choice of whether to model the corpus character-by-character or whether to segment the text into words. Because the vast majority of ancient Chinese words are written as single characters, the character-by-character option may have been a reasonable choice for this corpus. It was our judgment, however, that segmentation of the texts into words rather than characters would improve interpretability of the models.27 Software to address the word segmentation problem in modern Chinese exists, but these solutions are dictionary-based. Thus it was necessary for us to find and deploy a dictionary of ancient Chinese that we constructed from different sources.28 After applying the dictionary to our corpus, we identified nearly 84 million word tokens comprising nearly 85,000 unique word types. The most common word in the Handian corpus is \u4e4b (zh\u012b, it/this/for) at just over 1.25 million occurrences and the most common two-character word was \u5929\u4e0b (ti\u0101nxi\u00e0, the World) at 83,805 occurrences, 93rd most frequent in the overall list. Very high frequency words are relatively uninformative and they tend to overwhelm the available methods for corpus analysis, both because of the additional time to process so many characters in a corpus of this scale, and because the highly frequent terms tend to dominate more meaningful terms in the trained models. Therefore, it is normal to develop a \u201cstop list\u201d of such words to remove them from the corpus.29 Our stop list of 187 words is larger than the 132 words listed by Slingerland et al.,30 and the two lists overlap in 50 words. The relative disjointness of the two can be explained by the differences in size and scope of the two corpora and the different objectives of the two projects. For example, we found it useful to filter out more of the frequently occurring number words.\n7 of 24\nLDA Topic Modeling Based on our previous experience working with large text collections within the InPhO team at IU, we chose to apply LDA (Latent Dirichlet Allocation) Topic Modeling to the Handian corpus. (LDA is named for the 19th C. mathematician Gustav Dirichlet who laid the foundation in probability theory for the technique.) LDA Topic modeling has become popular within DH in recent years, although the interpretation of this kind of model remains a matter of considerable discussion.31 It treats documents initially as \u201cbags of words\u201d \u2014 that is, all grammatical structure and information about word order within sentences or documents is ignored, and the document\u2019s initial profile is simply the frequency with which of all the words appear in it. Topic modeling aims to find latent (hidden) structure among these \u201cbags of words\u201d, by re-representing each document as a mixture of topics. A topic may also be thought of as a writing context, as we now explain. We understand topic models to provide a theory about writing. Authors of documents combine different subjects of discussion. Different authors working within similar cultural contexts have overlapping interests in various subjects, but they combine the available topics differently. When writing about good behavior, for example, one may be concerned with the good behavior in the public sphere of business or politics or religion, or in the family or social community, or as a topic within moral philosophy. An author is more or less likely to use a given word when writing about each of these subjects. For example, the words \u2018sister\u2019 or \u2018father\u2019 are more likely to be used when the author\u2019s subject is family than when writing about business. Other words may have very similar likelihoods of being used in these contexts. For example, the word \u201cvirtue\u201d might be equally likely to be used by authors discussing family or business matters. Discussion of good behavior may span the contexts of nature, family history, legal cases, theology, mythology, etc. Across a large corpus of documents we may expect to see these themes arising in different combinations \u2014 both when different authors are writing within similar cultures, and when one author writes at different times in his or her career. Furthermore, writers write for different contexts and audiences: letters to friends or family or superiors, philosophical dialogues, public speeches, etc. Each of these contexts also changes the likelihood of the author selecting certain words, and the same word in different contexts may produce slight or major variations in meaning. LDA topic modeling provides a method for automatically identifying topics within a set of documents. At the end of a training process: (a) each topic is represented as a total probability distribution over all the words in the corpus \u2014\nthat is, every word is assigned a probability in every topic, and the sum of all the word probabilities within one topic is equal to one; and\n(b) each document is represented as a total probability distribution over the topics \u2014 that is, every topic is assigned a probability in every document, and the sum of the topic probabilities within one document is likewise equal to one.\n8 of 24\nThe model starts with random probabilities assigned to the word-topic and topic-document distributions. It is trained by a process of adjusting the word-topic and topic-document probability distributions. The word-topic and topic-document distributions are controlled by two parameters (technically \u201chyperparameters\u201d or \u201cpriors\u201d) that are set to ensure that there is sufficient variation in the probabilities assigned to the topics in the documents and to the words in the topics. The number of topics is chosen by the modeler. Our group typically trains multiple models with different numbers of topics, and we compare the different models to each other. For the present study we trained models with 20, 40, 60, 80, and 100 topics. In general, with too few topics, each topic becomes very general and hard to interpret. With too many topics, some of the topics are specialized on just a few documents, making them less useful for finding common themes. While there exist methods within computer science for estimating an optimally efficient number of topics for a given corpus, users of the models may prefer a coarse-grained scheme (fewer topics) for some purposes while other users may prefer a more fine-grained scheme (more topics) for other purposes.32 Furthermore, working with multiple models simultaneously, fosters the kind of \u201cinterpretive pluralism\u201d that characterizes humanities computing.33 The process by which we built these models using the InPhO topic-explorer package consists of four steps: initialization of the corpus object, preparation of the corpus by filtering words according to their frequency, training the corpus models, and launching the Topic Explorer\u2019s Hypershelf interface.34 Using the Topic Explorer & Notebooks The Topic Explorer provides both a map-like visualization of the topic space (described further below) and a \u201cHypershelf\u201d that allows users to experiment with the trained model to explore the corpus in any standard web browser. We call the latter interface a Hypershelf because although the browser initially presents documents from the corpus in a single linear order, it can be rearranged by the users to reflect their interests, and any document can be opened to view the full text. Thus, the Hypershelf initially provides a top level \u201cdistant reading\u201d35 view of the corpus, but allows the user to zoom down to the original text. This supports a two way interaction in which interpretation of the texts helps the user to interpret the topics in the model, while interpretation of the topics in the model can help the user to interpret the texts. (We provide an example of this interplay below.)\n9 of 24\nThe Hypershelf has two main modes: a document-centered view and a topic-centered view. Beginning with the document-centered view, the user may either select a document at random or use the search box to enter a few characters. These characters are automatically matched to the document labels, and the user can select a document from the drop-down list (Figure 1 shows initial options for \u8bba\u8bed \u2014 L\u00fany\u01d4, the Analects). Once a document is selected, and a number of topics for the model is chosen, the browser window is filled with a row of multicolored bars (Figure 2), each block of color corresponding to a topic. The top row represents the topics assigned to the document by the computer during the final training cycle, according to the key at the right. Hovering over any of the colored sections displays a list of the highest probability words for that topic (see Figure 3). It is important to remember, however, that every word is assigned a probability in every topic, so these first few words do not exhaust the context provided by the topic. Each subsequent row represents the topic distribution of another document from the corpus, scaled such that the length of the bar indicates similarity to the top document.36\nFigure 1. Autocompletion of document names.\n10 of 24\nInitially, similarity between documents is shown with respect to the entire topic mixture associated with the focal document, but clicking the mouse on any of the topics re-sorts the list according to overall proportion of each document that the model assigned to the selected topic. This capacity of the HyperShelf allows users to rearrange the documents according to their interest in a particular topic (Figure 3). From this point the user may click the \u201cTop documents for Topic\u2026\u201d button below the key on the right (not shown in the Figure) to select the documents from the entire corpus that have been assigned the highest proportion of the selected topic. Alternatively, the user may choose to refocus on any of the other documents in the display by clicking on the arrow icon to the left of a\nadditional details about the Topic Explorer display.\n11 of 24\nrow. (This icon appears when the mouse hovers nearby.) The user may read the full document by clicking on the \u201cpage\u201d icon, which appears to the left of the arrow icon. Results We successfully trained topic models on the corpus of over 18,000 classical Chinese documents and made them available to explore interactively online.37 We believe our choice to do word segmentation rather than single character modeling is justified by the contribution that the two-character words make to the interpretability of the topics, as well as by our investigation of \u9634\u9633 (yinyang) within the corpus, as described below. Topic models for humanities computing cannot be evaluated against a \u201cgold standard\u201d of correct performance, because no such standard exists. Neither could such a standard exist if one takes seriously the idea that the process of interpretation at the core of the humanities applies to the models as much as the texts (see Discussion section below), and is as variable as the interests\nFigure 3. Highest probability words for each topic appear in the topic key at right when the cursor is positioned over the key, or over the corresponding topic in any of the document rows. Clicking in either location causes the Hypershelf to re-sort the list of results according to\nproportion of that topic assigned to each document. Here we show the reordering of results after selecting Topic 51 as the comparison dimension for documents most similar to Book 1 Chapter 1 (\u5377\u4e00 \u5b66\u800c\u7b2c\u4e00) of the Analects (\u8bba\u8bed) in the 60-topic model.\n12 of 24\nof the users themselves. Ultimately, a topic-modeling approach succeeds or fails according to the ability of users to use the models for their own purposes, be it self-education, pedagogy, exploratory research, or systematic analysis of the texts. In future work we intend to assess how users respond to the topic models, and to conduct more complete analyses of relationships among the texts using the models. Here we present an example of how a particular individual used the Topic Explorer modeling and visualization results for self-guided investigation and serendipitous discovery \u2014 a process we refer to as \u201cguided serendipity\u201d.38 Our subject, one of the Chinese coauthors of this paper, began this project with only a basic familiarity with ancient Chinese philosophy acquired from an undergraduate course. He decided initially to investigate the concept of yinyang (\u9634\u9633). Using the capacity of the Topic Explorer for topic-mediated term search, this term was queried in the 60 topic model.39 Documents are retrieved according to their overall similarity to the topics selected by the term. The practical import is that because searches are topic-mediated, the documents retrieved need not contain the actual query term. The first document identified in this way is from the \u5b50\u90e8 (z\u01d0 b\u00f9) section of the corpus, which contains writings of the philosophical schools. It is from the \u672f\u6570 (shu shu, or divination) section of the corpus, specifically the \u4e09\u547d\u901a\u4f1a (S\u0101n M\u00ecng T\u014dng Hui), an important book from the Ming dynasty. The specific chapter located in this way is \u5377\u4e00\u00b7\u8bba\u652f\u5e72\u6e90\u6d41 (On the Origin of the Chinese Sexagenary Cycle), describing the \u201cten Heavenly Stems\u201d (yang) used in combination with \u201ctwelve Earthly Branches\u201d (yin) as a calendar dating system. A little further down the list of documents, in the 7th row is a chapter from the Confucianism subsection \u5112\u5bb6 (r\u00faji\u0101 ). The chapter labeled \u2018\u53c3\u5169\u8439\u7b2c\u4e8c\u2019 from the volume labeled \u2018\u5f20\u5b50\u6b63\u8499\u2019 in the corpus is part of the work Zheng Meng (\u6b63\u8499), which is very significant within the Confucian tradition. It was written by Zhang Zai (1020-1077), an important thinker of the Song Dynasty from Shaanxi province. The chapter relates yinyang theory to the astronomical calendar and the classical theory of Five Phases: Wood, Fire, Earth, Metal, Water (also referring to Jupiter, Mars, Saturn, Venus and Mercury respectively) used to explain the laws governing speed and direction of planetary motion. Pursuing the idea that the Five Phases Theory provides the backbone of a broad system of thought encompassing many areas, our subject re-sorted the Hypershelf by clicking the arrow to the left of the top row, to refocus on this document.40 He then inspected the topics and identified topic 15 as seemingly most relevant to his interests. Clicking on that topic reorders the results according to the proportion of the topic allocated to each document in the list. The top document identified in this way is also from the Confucian section of the Handian corpus, but in this case volume 12 from the book \u6625\u79cb\u7e41\u9732 (Ch\u016bn Qi\u016b F\u00e1n L\u00f9, sometimes abbreviated as \u201cFan Lu\u201d, and also known in English as the \u201cLuxuriant Dew of the Spring and Autumn Annals) which relates the changing of the seasons to yinyang. By inspecting the titles of the documents near the top of the list, our subject noticed that many of them are from the Ch\u016bn\n13 of 24\nQi\u016b F\u00e1n L\u00f9, from the Zheng Meng, and from a third important work titled \u2018\u4e09\u547d\u901a\u4f1a\u2019 (S\u0101n M\u00ecng T\u014dng Hu\u00ec), which is a text about fortune telling and divination. This helped our subject to understand that the topic explorer could help him identify in which parts of Chinese culture the yinyang theory was prominent, namely Confucianism, Daoism, and traditional Chinese medicine. For example, the document \u5377\u4e8c\u00b7\u8bba\u4e94\u884c\u65fa\u76f8\u4f11\u56da\u6b7b\u5e76\u5bc4\u751f\u5341\u4e8c\u5bab is part of the S\u0101n M\u00ecng T\u014dng Hu\u00ec about the Five Phases theory, explaining the positive and negative relationships existing among Wood, Fire, Earth, Metal, and Water, and various ways in which those relationships may be in which these elements are related to human life, health, and death. Also present are numerous documents from the \u533b\u5bb6 (Y\u012bji\u0101, or traditional medicine) section. Refocusing the topic explorer by clicking on the arrow icon to the left of the document HandianCorpus/\u300e\u5b50\u90e8\u300f/\u533b\u5bb6 /\u533b\u5b66\u6e90\u6d41\u8bba/\u5377\u4e0a\u00b7\u75c5\u540c\u4eba\u5f02\u8bba.txt retrieves a large number of related medical texts. In particular, the document from the \u7d20\u95ee (S\u00f9 w\u00e8n, or \u2018basic questions\u2019) section labeled \u516b\u6b63\u795e\u660e\u8bba\u7bc7\u7b2c\u4e8c\u5341 \u516d (Part 26 of the book B\u0101 Zh\u00e8ng Sh\u00e9nm\u00edng L\u00f9n) which is a very famous dialog between the mythologized \u201cYellow Emperor\u201d Hu\u00e1ngd\u00ec (\u9ec4\u5e1d) and his minister, Q\u00ed B\u00f3 (\u5c90\u4f2f), supposed to have lived in the third millennium BCE. They discuss acupuncture in the context of qi (\u6c14), a very important concept concerning life force or vital energy in traditional Chinese medicine, and they connect qi to yinyang. Our coauthor reports that before using the Topic Explorer his concept of the Five Phases Theory was ambiguous, but in the interplay between topics and documents he learned many\nFigure 4. Topics from all five models arranged and clustered by the isomap algorithm. Circle size is inversely proportional to the number of topics in the model: largest circles\nrepresenting topics from the 20-topic model, smallest from the 100-topic model. Overlapping circles of different sizes indicates congruence between topics from models at different levels. See main text for further details, and the next two figures for applications of the map to topics and terms.\n14 of 24\nnew details about the Five Phases Theory and its relationship to yinyang. For an expert, these interrelations may be well-known, but for a learner, the capacity to rapidly relate the concepts in this way serves a very valuable function. His understanding of the complexity of the concept of qi was also broadened, leading to a plan to work further on this concept in future work with the topic models. This example shows how one individual\u2019s understanding of the connectedness of concepts from traditional Chinese medicine, astronomy, and religion was deepened by interaction with both the high-level overviews provided by the topic model and the close reading of specific texts directed by following the models. Although just one case, we believe that this case is not unique: the Hypershelf interface of the Topic Explorer supports spontaneous exploration and guided serendipity, customized to the user\u2019s particular interests. We turn now from the Hypershelf to an interactive visualization of the entire topic space which is also provided by the Topic Explorer software package. This visualization can be explored interactively at InPhO websites. Figure 4 shows a map and cluster analysis of the topics across all five models. The map is generated using the isomap procedure applied to the JSD measure.41 Isomap is a technique for reducing a high dimensional space (in this case the probability space of the word distributions in the models) to fewer dimensions, in this case two. Such dimensionality reductions are useful for identifying principal components of the model structure. Whereas the standard MDS (multi-dimensional reduction) algorithms are linear, isomap detects non-linear structure in the data. The map allows one to assess the overall similarity of topics in the different models (20, 40, 60, 80, 100). The relationships among topics revealed by these figures are not strictly hierarchical; nevertheless, topics from the models with higher numbers of topics tend to cluster around topics from the models with smaller number of topics. Groups of topics are clustered and colored automatically according to an arbitrary choice of ten clusters. Although these clusters are very broad, some general themes emerge \u2014 for example, the dark green and dark purple clusters in the lower right are related to literature and poetry, the light blue region contains topics related to Confucianism, while the dark blue region below it spans topics related to traditional religions and traditional Chinese medicine. The light orange and dark orange regions cover different aspects of history; for example, topics related to military history are more prominent in the darker orange region. The dark red area corresponds to political and diplomatic topics while the pink cluster at the bottom left covers topics related to administration. The four topics colored light purple at the bottom center are heavily loaded with geographical terms \u2014 terms which are of course quite generally used in everything from poetry to military history and administration.\nThe interactive Topic Isomap supports exploration of the models in a variety of ways. Hovering the mouse over the elements of the map shows the highest probability words for the topics, and allows the user to click through to the Hypershelf, showing documents most similar to that topic. Alternatively, entering a word or words in the search box above the map adjusts the colors in the map to show the relative weight of the term across all the topics. More saturated colors indicate a higher probability of the term being generated by that topic. Figure 5 shows a comparison of the terms \u5b54\u5b50, Confucius (Figure 5a), and \u4f5b, the Buddha (Figure 5b). Both are\n15 of 24\nimplicated in many of the topics, as indicated by the high number of topics receiving some color. However, the color map for \u4f5b shows a more concentrated set of topics belonging to the dark blue cluster. These topics, listed here, are all clearly related to Buddhism (X:Y indicates k=X, topic number Y): 20:1 \u5e08, \u7ecf, \u50e7, \u4f5b, \u65f6, \u9053, \u5bfa, \u65e0, \u751f, \u6cd5, \u738b, \u5982\u4f55, \u771f, \u540d, \u5904, \u2026 40:19 \u5e08, \u7ecf, \u50e7, \u4f5b, \u65f6, \u9053, \u65e0, \u5bfa, \u751f, \u6cd5, \u5982\u4f55, \u8bd1, \u738b, \u7985\u5e08, \u5904, \u2026 60:5 \u7ecf, \u4f5b, \u5bfa, \u50e7, \u738b, \u6cd5, \u8bd1, \u65f6, \u83e9\u8428, \u5df2, \u6c99\u95e8, \u540d, \u8eab, \u65e0, \u5584, \u2026 80:37 \u4f5b, \u5bfa, \u7ecf, \u50e7, \u65f6, \u738b, \u6cd5, \u65e0, \u5df2, \u8eab, \u5584, \u83e9\u8428, \u540d, \u751f, \u53d7, \u2026 100:96 \u7ecf, \u4f5b, \u5bfa, \u50e7, \u6cd5, \u8bd1, \u83e9\u8428, \u672c, \u6c99\u95e8, \u738b, \u91ca, \u540d, \u5854, \u5f55, \u90e8, \u2026 Clicking on any of the topics identified in this way takes the user to the Hypershelf with the top documents for that topic already loaded. Figure 6 shows a similar comparison for the terms \u6c14 (qi) and \u9634\u9633 (yinyang). Here the distributions are quite similar, although the topics related to qi are more concentrated on the right side of the diagram whereas topics related to yinyang are distributed a bit more across central parts of the map. The relative confinement of topics related to qi corresponds to the fact that the Isomap algorithm has placed health and traditional medicine topics on the right side of the map in the dark blue cluster.\n16 of 24\norigin of the map. See main text for further discussion.\n17 of 24\nAmong the most central topics in the map (i.e., those closest to the 0,0 origin in the map) are these from right side of the dark orange cluster: 20:11 \u547d, \u5b98, \u8d3c, \u6388, \u5175, \u5c14, \u5de1\u629a, \u8425, \u963f, \u6c11, \u5927\u81e3, \u90e8, \u660e, \u9980, \u603b\u7763, \u2026 40:23 \u547d, \u5c14, \u5927\u81e3, \u6388, \u9980, \u989d, \u963f, \u8425, \u514b, \u603b\u7763, \u90e8, \u660e, \u5de1\u629a, \u5e03, \u4e7e\u9686, \u2026 60:18 \u547d, \u5927\u81e3, \u9980, \u603b\u7763, \u6388, \u5de1\u629a, \u8425, \u8bae, \u4e7e\u9686, \u989d, \u7f72, \u594f, \u532a, \u5eb7\u7199, \u7531, \u2026 80:40 \u9980, \u547d, \u5de1\u629a, \u603b\u7763, \u5927\u81e3, \u8bae, \u7f72, \u532a, \u7531, \u8c03, \u4e7e\u9686, \u594f, \u514d, \u5dde\u53bf, \u8bbe, \u2026 100:63 \u547d, \u5927\u81e3, \u5de1\u629a, \u603b\u7763, \u594f, \u8bae, \u5b66\u58eb, \u8c15, \u4e7e\u9686, \u7f72, \u7531, \u6388, \u5dde\u53bf, \u76f4, \u8c03, \u2026 These topics are highly loaded with terms related to government officials, but also contain some words related to criminality. The centrality of these topics may be seen as reflecting both the large number of government documents in the Handian corpus, and the central importance of the civil service in China for the preservation and transmission of classical Chinese culture and values. It is also worth noting that the 20:2 model (\u9619, \u5fb7, \u81e3, \u65e0, \u5143, \u5723, \u8868, \u53ef, \u547d, \u5b9e, \u5929, \u5949, \u9053, \u6587, \u795e, ...) is actually slightly closer to the center than 20:11, but it is grouped with the light blue cluster of topics. Visual inspection of the highest probability terms suggests that 20:11than 20:2 is more aligned with the other topics listed above, and helps give some confidence in the clustering technique. It is important to keep in mind, however, that the isomap plot is generated using the full term distribution, not just the first 15 terms shown here. A complete assessment of the topics and their related documents would go beyond simple inspection of the top terms.\n18 of 24\n19 of 24\nDiscussion LDA topic models are not themselves interpretations of the documents \u2014 indeed they stand in need of interpretation themselves42 \u2014 but they may assist scholars in exploring and interpreting large collections of materials. Ultimately there is no substitute for reading the documents, but the Topic Explorer interface, through its Hypershelf and Topic Isomap components, can guide scholars and learners alike to documents that they might not have otherwise encountered or thought to look for, resulting in a particularly productive form of guided serendipity. Topic models are interesting to think about from the perspective of theories of meaning. While they do not capture exact meanings \u2014 \u201cJohn loves Mary\u201d and \u201cMary loves John\u201d are viewed as identical statements under the \u201cbag of words\u201d assumption \u2014 they are quite successful at capturing something like the general gist or context of the words being used. Scholars of Chinese literature have emphasized the high degree of context sensitivity for the meanings of words in the Chinese language,43 but a strength of the topic modeling approach is that the same word is placed in multiple contexts, helping with the process of disambiguation. At the same time, because the models have a solid grounding in information theory, the use of metric measures such as the Jensen-Shannon distance is feasible for many applications. This provides new forms of evidence for humanistic discussions. Although the corpus we used may be missing some potentially important documents, it is large enough that the topic models we derived from this corpus prove to be adequate for various purposes. Improved curation of the corpus nevertheless remains an important goal of our group for the future, and will be reflected in future iterations of the Handian Topic Explorer mirror site. Future work will allow us to address questions about topical relationships among the documents in the Handian corpus and about historical and geographical shifts in the topic distributions as represented in the corpus model, and ultimately to analyze the behavior of individual authors. Finally, and more speculatively, philosophers of mind and cognitive science have sometimes been tempted by the idea that meaning or semantic content assignment is a kind of measurement process rather than the assertion of a relationship to a determinate abstract proposition.44 Computer scientists have started to provide the means to convert this idea into quantitative models45 to which measures such as Jensen-Shannon distance can be applied. Thus, the Digital Humanities are poised to have a significant impact on philosophical and practical discussions of the nature of meaning. Conclusions Topic models present a powerful new tool for computer-assisted interpretation in the humanities. We have described some particular issues faced for using topic models with ancient Chinese texts, and we have detailed the process of training LDA topic models on the Handian corpus of over 18,000 classical Chinese texts using the InPhO Topic Explorer. The results of these efforts and the software we have developed have been made publicly available via the Hypershelf interface at mirror sites at Xi\u2019an Jiaotong University and Indiana University. This\n20 of 24\ninterface allows users to visualize the results of the modeling process. We have provided some preliminary description and analysis of the topics discovered by the algorithms using the more advanced notebook features of the Topic Explorer. These preliminary investigations have revealed some interrelationships among Confucian, Taoist and Buddhist themes, and the penetration of these themes in many aspects of traditional Chinese culture, from medicine to government. By following the threads among specific texts, guided by these topic models, scholars may exploit these new tools to enrich their understanding and interpretation of China\u2019s rich cultural heritage. Acknowledgements The software described in this paper was originally developed at Indiana University with generous support from the National Endowment for the Humanities and IU\u2019s Office of the Vice President for Research. Its extension to ancient Chinese owes much to IU\u2019s support and to the research funding provided by the College of Humanities and Social Sciences, the office of Dean Yanjie Bian and the Philosophy Department at Xi\u2019an Jiaotong University. The authors of this paper, we would like to thank Xiaoliang Wang and Wenjing Yuan at XJTU for their initial guidance concerning ancient Chinese philosophy. We also acknowledge the prior programming efforts of Robert Rose, Doori Lee, Jessie Pusateri, and Adithya Nagaraj-Tirumale at Indiana University. We are grateful to Henry Rosemont, Jr., for comments on the manuscript. All errors of interpretation remain our own.\n21 of 24\nNotes 1 Paul Tasman, \u201cLiterary data processing,\u201d IBM Journal of Research and Development 1, no. 3 (1957), 249-256. Stephen M. Parrish, A Concordance to the Poems of Matthew Arnold. *Ithaca, NY: Cornell University Press, 1959). Robert J. Glickman and Gerrit J. Staalman, Manual for the Printing of Literary Texts and Concordances by Computer (University of Toronto Press, 1966). John B. Smith, \u201cImage and imagery in Joyce\u2019s Portrait: A computer-assisted analysis,\u201d in Stanley Weintraub (Ed.) Directions in Literary Criticism: Contemporary Approaches to Literature. (University Park, PA: Pennsylvania State University Press, 1973). John B. Smith, \u201cComputer criticism,\u201d Style 12, no. 4 (1978), 326-356. For a historical overview, see Geoffrey Rockwell and St\u00e9fan Sinclair, Hermeneutica: Computer-Assisted Interpretation in the Humanities (Cambridge, MA: The MIT Press, 2016), chapter 3. 2 Rockwell & Sinclair, ibid. 3 http://hathitrust.org/. 4 Heather Christenson, \u201cHathiTrust: A Research Library at Web Scale,\u201d Library Resources and Technical Services 55 (2011), 93-102. 5 https://www.hathitrust.org/statistics_info 6 Stacy T. Kowalczyk, Yiming Sun, Zong Peng, Beth Plale, Aaron Todd, Loretta Auvil, Craig Willis, Jiaan Zeng, Milinda Pathirage, Samitha Liyanage, Guangchen Ruan and Stephen J. Downie, \u201cBig Data at Scale for Digital Humanities: An Architecture for the HathiTrust Research Center,\u201d in Wen-Chen Hu and Naima Kaabouch (Eds.) Big Data Management, Technologies, and Applications (Hershey, PA: IGI Global, 2014), 345-369. 7 Such projects include: Barbara B. Oberg and J. Jefferson Looney (Eds.) The Papers of Thomas Jefferson Digital Edition (Charlottesville, VA: University of Virginia Press, 2009\u2014); the correspondence of Charles Darwin (www.darwinproject.ac.uk); the Folger Shakespeare Library (http://www.folger.edu/folger-digital-texts); a corpus of Chinese historical records by Hui Dong, Lei Xu, Fei Wang and Siwei Yu, \u201cStudy on Semantic Analysis System (III) \u2014Implementation of Chinese Historical Records Semantic Analysis System,\u201d Journal of the China Society for Scientific and Technical Information 33, no. 2 (2014), 204-214; and too many other projects to mention. 8 http://gutenberg.org/ 9 Susan T. Dumais, \u201cLatent Semantic Analysis,\u201d Annual Review of Information Science and Technology Volume 38 (2004), chapter 4, 189-230. David M. Blei, Andrew Y. Ng, and Michael I. Jordan, \u201cLatent Dirichlet Allocation,\u201d Journal of Machine Learning Research, 3 (2003), 993\u2013 1022. Tomas Mikolov, Kai Chen, Greg S. Corrado and Jeffrey Dean, \u201cEfficient Estimation of Word Representations in Vector Space,\u201d ArXiv e-prints (2013), https://arxiv.org/abs/1301.3781 (retrieved Dec 21, 2016). 10 Sarah Klingenstein, Tim Hitchcock and Simon DeDeo, \u201cThe civilizing process in London\u2019s Old Bailey,\u201d Proceedings of the National Academy of Sciences 111, no. 26 (2014), 9419-9424. Andrew Goldstone and Ted Underwood, \u201cThe Quiet Transformations of Literary Studies: What Thirteen Thousand Scholars Could Tell Us,\u201d New Literary History 45 (2014), 359-384. Ted Underwood and Jordan Sellers, \u201cThe Longue Dur\u00e9e of Literary Prestige,\u201d Modern Language Quarterly 77, no. 3 (2016), 321-344.\n22 of 24\n11 Lauren Klein and Jacob Eisenstein, \u201cReading Thomas Jefferson with TopicViz : Towards a Thematic Method for Exploring Large Cultural Archives,\u201d Scholarly and Research Communication 4 (2013), 1\u201312. Jaimie Murdock, Colin Allen and Simon DeDeo, \u201cExploration and Exploitation of Victorian Science in Darwin\u2019s Reading Notebooks,\u201d Cognition 159 (2017), 117\u2013126. 12 Jian Ouyang, \u201cVisual Analysis and Exploration of Ancient Texts for Digital Humanities Research,\u201d Journal of Library Science in China 42, no. 222 (2016), 66-80. 13Zixuan Huang and Jinsong Yu, \u201cTopic Model based SongCi Corpus Construction and Research on Computer aided SongCi Writing,\u201d International Journal of Knowledge and Language Processing 3, no. 2 (2012), 1-19. 14 http://ctext.org/. 15 Ryan Nichols, Kristoffer L. Nielbo and Uffe Bergeton, \u201cTopic Modeling the Ancient Chinese Corpus: The Textual Contexts of High and Low Gods in Chinese Thought.\u201d Conference presentation at Cultural Evolution of Religion Research Consortium, Montreal, Canada, May 2015. Ryan Nichols, Edward Slingerland, Kristoffer Nielbo, Uffe Bergeton, Carson Logan and Scott Kleinbahn, \u201cModeling the contested relationship between Analects, Mencius, and Xunzi: Preliminary evidence from a Machine-Learning Approach,\u201d submitted. Slingerland, Nichols, Nielbo and Logan, \u201cThe Distant Reading of Religious Texts: A \u201cBig Data\u201d Approach to MindBody Concepts in Early China,\u201d submitted. 16 Rockwell & Sinclair op cit. David M. Blei, \u201cProbabilistic Topic Models,\u201d Communications of the ACM 55 (2012), 77\u201384. Ted Underwood, \u201cTheorizing Research Practices We Forgot to Theorize Twenty Years Ago,\u201d Representations 127 no. 1 (2014), 64-72. 17 Henry Rosemont, Jr. \u201cTranslating and Interpreting Chinese Philosophy,\u201d The Stanford Encyclopedia of Philosophy (Summer 2016 Edition), Edward N. Zalta (ed.), URL http://plato.stanford.edu/archives/sum2016/entries/chinese-translate-interpret. 18 Qi Zhao Q, Zengchang Qin and Tao Wan, \u201cTopic Modeling of Chinese Language Using Character-Word Relations,\u201d in: Bao-Liang Lu, Liqing Zhang and James Kwok (Eds.), Proceedings of the 18th International Conference on Neural Information Processing, ICONIP 2011. Lecture Notes in Computer Science, vol 7064. (Berlin, Heidelberg: Springer, 2011). 19 Rosemont op. cit. 20 Jaimie Murdock and Colin Allen, \u201cVisualization Techniques for Topic Model Checking,\u201d Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI). (Austin, TX: AAAI Press, 2015), http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10007. 21 http://online.eastview.com/projects/skqs/ 22 http://www.zdic.net/aboutus/ (retrieved November 1, 2016). 23 Nichols et al. 2015 and submitted, Slingerland et al. submitted. 24 http://inphodata.cogs.indiaa.edu/handian/ and http://inpho.xjtu.edu.cn/handian/. 25 The 2nd-level sections of the Handian corpus are listed in appendix 1 of the supplemental materials available at <http:URL>. 26 After training the topic models, we discovered, with the help of the models, that two folders of English translations of the Analects (20 files) and Lao Tze (84 files) had slipped through our net.\n23 of 24\nWe decided not to remove these and retrain the models because the presence of these files has a minimal effect on the overall results. 27 Zhao, Qin & Wang op. cit. Ouyang op. cit. 28 Corpus cleaning methods are detailed in appendix 2 of supplemental materials. The ancient words dictionary is available at https://github.com/inpho/topicexplorer/blob/master/topicexplorer/lib/ancient%20words.dic and embedded within the Topic Explorer using the \u201coch\u201d (old Chinese) extensions. 29 Gerard Salton, Andrew Wong and Chungshu Yang, \u201cA Vector Space Model for Automatic Indexing,\u201d Information Retrieval and Language Processing 18 no. 11 (1975), 613-620. Christopher Fox, \u201cA stop list for general text,\u201d SIGIR Forum 24 nos. 1-2 (1989), 19-21. Steven Bird, Ewan Loper and Edward Klein, Natural Language Processing with Python, (Sebastopol, CA, O'Reilly Media Inc., 2009). 30 Supplemental materials appendix 3 shows the 20 most common words, and their frequencies, and describes the process by which we developed our list of 187 words for ancient Chinese in Appendix 3. For the stop word list used by Slingerland et al. see their \u201cModeling the contested relationship\u201d, submitted. 31 Blei, \u201cProbablistic Topic Models.\u201d 32 Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov and David Mimno, \u201cEvaluation Methods for Topic Models,\u201d in Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909 (New York, NY, USA: ACM, 2009), pp. 1105\u201312. Margaret Roberts, Brandon Stewart and Dustin Tingley, \u201cNavigating the Local Modes of Big Data: The Case of Topic Models,\u201d in Data Analytics in Social Science, Government, and Industry (New York: Cambridge University Press, 2015). Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L Boyd-Graber and David M Blei, \u201cReading Tea Leaves: How Humans Interpret Topic Models,\u201d in Advances in Neural Information Processing Systems, 2009, pp. 288\u201396. 33 Rockwell and Sinclair Hermeneutica. 34 Details of the modeling process provided in supplemental appendix 4. The topics discovered by training on the Handian corpus are represented in appendix 5, which shows the 15 highest probability words from each topic in the 20, 40, 60, 80, and 100 topic models. 35 Franco Moretti, Distant Reading (New York: Verso, 2013). 36 Jensen-Shannon Distance is due to Dominik M. Endres and Johannes E. Schindelin, \u201cA New Metric for Probability Distributions,\u201d IEEE Transactions on Information Theory 49, no. 7 (2003), 1859-1861. It is based on Kullback-Leibler divergence introduced by Solomon Kullback and Richard A. Leibler, \u201cOn information and sufficiency,\u201d Annals of Mathematical Statistics 22, no. 1 (1951), 79\u201386. See supplemental appendix 6 for additional information. 37 The models may be explored interactively at http://inphodata.cogs.indiana.edu/handian/ or at http://inpho.xjtu.edu.cn/handian/. See also supplemental appendix 5. 38 Cameron Buckner, Matthias Niepert and Colin Allen, \u201cFrom encyclopedia to ontology: toward dynamic representation of the discipline of philosophy,\u201d Synthese 182 (2011), 205-233. 39 http://inphodata.cogs.indiana.edu/handian/60/?q=\u9634\u9633.\n24 of 24\n40 Equivalent to http://inphodata.cogs.indiana.edu/handian/60/?doc=HandianCorpus%2F\u300e\u5b50\u90e8 \u300f%2F \u5112\u5bb6%2F \u5f20\u5b50\u6b63\u8499%2F \u53c3\u5169\u8439\u7b2c\u4e8c.txt. 41 Joshua B. Tennenbaum, Vin de Silva and John C. Langford, \u201cA Global Geometric Framework for Nonlinear Dimensionality Reduction,\u201d Science 290, no. 5500 (2000), 2319-2323. We applied the isomap algorithmhm to the JSD measure. 42 Blei, \u201cProbabilistic Topic Models\u201d; Rockwell and Sinclair Hermeneutica; Underwood, \u201cTheorizing Research Practices.\u201d 43 Rosemont, \u201cTranslating and Interpreting Chinese Philosophy.\u201d 44 For example, Paul M. Churchland, Scientific Realism and the Plasticity of Mind (Cambridge: Cambridge University Press, 1979), and Robert Matthews, Measure of Mind (New York: Oxford University Press, 2007). 45 For a review see Michael N. Jones, Thomas M Gruenenfelder and Gabriel Recchia, \u201cIn Defense of Spatial Models of Lexical Semantics,\u201d in Laura Carlson, Christoph H\u00f6lscher and Thomas Shipley (Eds.), Proceedings of the 33rd Annual Conference of the Cognitive Science Society (Austin, TX: Cognitive Science Society, 2011), 3444\u20133449.\nTopic Modeling the H\u00e0n di\u0103n Ancient Classics (\u6c49\u5178\u53e4\u7c4d) Colin ALLEN1,2, Hongliang LUO3, Jaimie MURDOCK2, Jianghuai PU1, Xiaohong WANG1, Yanjie ZHAI1, Kun ZHAO3\n1 Department of Philosophy, School of Humanities and Social Sciences, Xi\u2019an Jiaotong University, Shaanxi, China 2 Cognitive Science Program, Indiana University, Bloomington, Indiana, USA 3 Institute of Computer Software and Theory, School of Electronic and Information Engineering,\nXi\u2019an Jiaotong University, Shaanxi, China\nAuthors listed alphabetically\nSUPPLEMENTAL MATERIAL\nAPPENDIX 1: LEVEL 2 SUBSECTIONS OF THE HANDIAN CLASSICS CORPUS APPENDIX 2: CORPUS CLEANUP APPENDIX 3: HIGHEST FREQUENCY WORDS APPENDIX 4: THE TOPIC EXPLORER MODELING PROCESS APPENDIX 5: THE HANDIAN TOPICS APPENDIX 6: DISTANCE MEASURES\nAbstract: Humanities scholars have become increasingly interested in exploiting computational methods to assist in the development of new insights and interpretations of culturally significant materials. Ancient Chinese texts present an area of enormous challenge and opportunity. In this paper we describe a collaborative effort between Indiana University and Xi\u2019an Jiaotong University to support exploration and interpretation of a digital corpus of over 18,000 ancient Chinese documents, which we refer to as the \u201cHandian\u201d ancient classics corpus (\u6c49\u5178\u53e4\u7c4d or H\u00e0n di\u0103n g\u016d j\u00ed, i.e, the \u201cHan canon\u201d or \u201cChinese classics\u201d). It contains classics of ancient Chinese philosophy, documents of historical and biographical significance, and literary works. We begin by describing the Digital Humanities context of this joint project, and the advances in humanities computing that made this project feasible. We describe the corpus and introduce our application of probabilistic topic modeling to this corpus, with attention to the particular challenges posed by modeling ancient Chinese documents. We give a specific example of how the software we have developed can be used to aid discovery and interpretation of themes in the corpus. We outline more advanced forms of computer-aided interpretation that are also made possible by the programming interface provided by our system, and the general implications of these methods for understanding the nature of meaning in these texts.\nAppendix 1: Level 2 subsections of the Handian classics corpus\n\u7ecf\u90e8 (J\u012bng B\u00f9) : \u2f17\u5341\u4e09\u7ecf the thirteen classics Classics \u2f17\u5341\u4e09\u7ecf\u6ce8\u758f thirteen classics annotations\n\u7ecf\u5b66\u53f2\u53ca\u2f29\u5c0f\u5b66\u7c7b history of classical studies and traditional philology\n\u53f2\u90e8 (Sh\u01d0 B\u00f9) : \u6b63\u53f2 official dynastic histories Histories \u7f16\u5e74\uf98e annals and chronicles\n\u7eaa\u4e8b\u672c\u672b complete accounts of major historical events \u522b\u6742\u53f2\u7b49 alternative and miscellaneous histories \u53f2\u8bc4 historical commentaries \u4f20\u8bb0 biographies \u8f7d\u8bb0 regional histories \u5730\u7406\uf9e4 geography \u804c\u5b98 state functions and powers \u653f\u4e66 Book of Zheng (politics and policy)\n\u2f26\u5b50\u90e8 (Z\u01d0 B\u00f9) : \u5112\u5bb6 Confucianism Masters \u91ca\u5bb6 Buddhism\n\u9053\u5bb6 Taoism \u6cd5\u5bb6 legalism \u5175\u5bb6 School of the Military \u519c\u5bb6 School of Agronomy \u6742\u5bb6 miscellaneous schools \u672f\u6570 numerology \u533b\u5bb6 traditional medicine \u79d1\u6280 science and technology \u827a\u672f art \u4e66\u753b calligraphy and painting \u8c31\u5f55 miscellaneous treatises \u7c7b\u4e66 reference works \u8499\u5b66 basic education\n\u96c6\u90e8 (J\u00ed B\u00f9) : \u603b\u96c6 major anthologies Belle-lettres \u522b\u96c6 secondary anthologies \u695a\u8f9e Songs of Chu / Songs of the South \u8bcd lyrics \u8bd7\u2f42\u6587\u8bc4 reviews of poetry and prose \u66f2 Yuan dynasty opera\n\u2f29\u5c0f\u8bf4 fiction\nAppendix 2: Corpus cleanup\n1. 38,182 HTML files were downloaded from gj.zdic.net. 2. These files were processed to extract the primary text within an html container marked by\nzdic.net with their div=\u201csnr2\u201d container. 3. All HTML tags within this text were removed. 4. Files were converted, if necessary, to use the unicode \u201cutf-8\u201d specification and saved with\nthe name of the document and a .txt extension. 5. All traditional Chinese characters were converted to simplified Chinese using the Python\nhanziconv package (https://pypi.python.org/pypi/hanziconv, version 0.3.2). Fortunately, the mapping between the two character sets is 1-1.\n6. We removed a subfolder of the \u96c6\u90e8 section of the corpus labeled \u2f29\u5c0f\u8bf4 that contained 18,049 files corresponding to modern fiction in order to focus on the classical corpus. This left 20,133 files. 7. We removed empty files leaving 19,865 files. 8. Each file was tested for the presence of modern Chinese words. 1,459 files thus identified\nwere manually inspected to see whether they should be discarded, or edited by hand to extract ancient text only. 412 of these were determined to be important and kept in the corpus after being manually edited to remove the modern Chinese sections. Thus we eliminated 1,047 files, leaving 18,818 files.\nAppendix 3: Highest frequency words\nTop 20 words in the unfiltered corpus, with frequencies \u4e4b 1275107 \u5176 566797 \u4e0d\uf967 326500 \u4e0e 230072 \u4ee5 759017 \u4e3a 565127 \u662f 318320 \u5728 225898 \u4e5f 648940 \u8005 560030 \u5219 278791 \u5dde 218932 \u6709 608156 \u2f48\u66f0 531502 \u2f83\u81ea 237334 \u2f00\u4e00 217999 \u2f7d\u800c 587775 \u4e8e 502754 \u4e91 232693 \u2f84\u81f3 216245\nWe developed a stopword list that includes 187 words, including 19 of these 20 most frequent terms (all but \u5dde, zh\u014du, state or prefecture). The list was developed using a mixed strategy of (a) checking word frequencies; (b) checking inverse document frequencies (i.e., the logarithm of the ratio of the total number of documents to the number of documents containing the word \u2014 a low score means the word is highly distributed throughout the corpus with a score of zero meaning that the word is present in every file); and (c) a cycle of training and retraining topic models, inspecting the topics, and adding terms to the stop list if they occurred in a high proportion of the topics and were judged uninformative about the topic.\nThe criteria we considered in deciding whether to place a term on the stop list were: 1. Does the term sometimes have a culturally/philosophically significant meaning? 2. Where it appears among the highest probability words for a topic, is it most likely to be interpreted with that significant meaning? 3. If it were removed from those topics, would the topics be less interpretable? The resulting list of 187 stop words is available at http:// inphodata.cogs.indiana.edu/handian/cn_stop.txt. The top 20 words with their frequencies after the stopword list was applied are shown below.\nTop 20 words in the filtered corpus with frequencies \u5dde 218932 \u5b98 160995 \u4e66 107519 \u5377 102601 \u2f46\u65e0 200605 \u4ee4 146193 \u53bf 107071 \u5df2 102028 \u738b 172465 \u2f8f\u884c\ufa08 132642 \u2f42\u6587 104966 \u547d 101657 \u2f82\u81e3 164252 \u5c06 116283 \u5175 104701 \u9053 101096 \u65f6 163895 \u5e1d 109950 \u8bcf 103798 \u521d 101023\nAppendix 4: The Topic Explorer modeling process\nThe InPhO Topic Explorer software can be freely downloaded at https://github.com/inpho/topicexplorer. User and developer versions of the software are installed in a few easy steps as explained in the site\u2019s Readme file which is also displayed at the URL above. There are four main phases to generating a functioning Topic Explorer instance. All commands are run in a shell or terminal window, depending on the operating system used (linux, mac os x, and Windows are supported).\n1. \u2018topicexplorer init <corpus_location>\u2019 requires the user to specify the location of the corpus. The notation <\u2026> indicates an item to be supplied by the user, in this case corpus_location should identify a directory or folder containing all the files to be modeled. By default, the software will suggest the name of the directory corresponding to the corpus_location as the corpus_name. The init step prompts the user to choose a project name (by default the same as corpus_location, and it creates a corpus file which records the count of every word in each document. The corpus file is placed in a folder called \u2018models\u2019. Also, it creates a configuration \u201c.ini\u201d file using the project name, i.e. corpus_name.ini for whatever corpus_name the user supplied. For ancient Chinese word segmentation we run the init step with the special tokenizer flag \u2018ltc\u2019, which uses pymmseg-cpp\u2019 \u2014 a Python interface to a high performance implementation of mmseg in the Ruby language. We supplemented mmseg with a list of 3,636 multi-character words in ancient Chinese that we extracted from https://zh.wiktionary.org/zh/User:Wihwang/\u53e4\u6f22\ufa47\u8a9e\u5e38\u2f64\u7528\u5b57\u5b57\u5178. The resulting list is available at https://github.com/inpho/topic-explorer/blob/master/topicexplorer/lib/ancient%20words.dic. By default, the software filters out all words occurring 5 or fewer times in the corpus, although this behavior can be controlled with the --freq flag. Summing up:\ntopicexplorer init <corpus_location> --tokenizer ltc\n2. \u2018topicexplorer prep <config_file>\u2019 allows the user to apply a stopword list and interactively set low and high thresholds for excluding the most common and the most rare words. After the high and low thresholds are set, an updated corpus file is placed in the models directory. Summary:\ntopicexplorer prep <config_file> --stopword_file <file_name>\n3. \u2018vsm train <config_file>\u2019 allows the user to select interactively the number of topics in each model, and the number of training cycles. Training in the InPhO Topic Explorer proceeds via a version of the widely-used Gibbs sampling method. For this present study we trained models with 20, 40, 60, 80 and 100 topics for 1000 training cycles. Summary:\ntopicexplorer train <config_file> -k 20 40 60 80 100 --iter 1000\n4a. \u2018launch\u2019 starts an instance of the interactive Topic Explorer Hypershelf in the web browser: topicexplorer launch <config_file> --fulltext The optional argument flag \u2018--fulltext\u2019 makes the full texts accessible from within the Hypershelf.\n4b. \u2018notebook\u2019 starts up an interactive Python notebook server in Jupyter: topicexplorer notebook <config_file> We do not provide a detailed explanation of the use of notebooks in this paper. However, running the topicexplorer notebook command produces a tutorial notebook for the corpus which provides a basis for further exploration.\nAppendix 5: The Handian topics\nThe models described in this work were trained in December 2016 by the first author following the steps outlined in Appendix 4 on a MacBook Pro. The resulting model files are archived at <<URI>>. Here we show the 15 highest probability words for each topic in the five models trained from the Handian corpus archived at <<URI>>. Topic labels (index numbers) are arbitrarily assigned by the computer, and have no intrinsic significance. Future iterations of the inpho/Handian web site may not correspond exactly to the topics shown here. This is because repetition of the modeling process with the same corpus and parameters typically produces slightly different results due to stochastic aspects of the modeling process. (See Murdock & Allen 2015 for discussion.) Additional variation will result as we upgrade the corpus and make other adjustments to model parameters."}, {"heading": "20 topic model, showing 15 highest probability words in each topic", "text": "Topic 0 \u70ba, \u65bc, \u7121, \u570b, \u8ecd, \u5f8c, \u5247, \u66f8, \u8207, \u2ed1\u2fa7\u9577, \u8b02, \u6642, \u79ae, \u8af8, \u6771 Topic 1 \u5e08, \u7ecf, \u50e7, \u4f5b, \u65f6, \u9053, \u5bfa, \u2f46\u65e0, \u2f63\u751f, \u6cd5, \u738b, \u5982\u4f55, \u771f, \u540d, \u5904 Topic 2 \u9619, \u5fb7, \u2f82\u81e3, \u2f46\u65e0, \u5143, \u5723, \u8868, \u53ef, \u547d, \u5b9e, \u5929, \u5949, \u9053, \u2f42\u6587, \u795e Topic 3 \u670d, \u2f53\u6c14, \u6cbb, \u75c5, \u70ed, \u6c64, \u8109, \u2f8e\u8840, \u2f63\u751f, \u5bd2, \u5347, \u4e38, \u9634, \u94b1, \u6563 Topic 4 \u661f, \u72af, \u2f49\u6708, \u2f47\u65e5, \u6714, \u5929, \u5360, \u5c81, \u2f55\u706b, \u592a\u2f69\u767d, \u2fac\u96e8, \u2f4a\u6728, \u2f9a\u8d64, \u8367\u60d1, \u2fa0\u8fb0\uf971 Topic 5 \u5c06\u519b, \u523a\uf9ff\u53f2, \u738b, \u5e1d, \u5dde, \u4ee4, \u5c1a\u4e66, \u90e1, \u592a\u5b88, \u5c01, \u9b4f, \u8fc1, \u65f6, \u521d, \u8bcf Topic 6 \u2f82\u81e3, \u965b\u4e0b, \u2f46\u65e0, \u8bae, \u65f6, \u594f, \u5df2, \u5b98, \u53ef, \u4e66, \u8bba, \u5929\u4e0b, \u4e0d\uf967\u53ef, \u4ee5\u4e3a, \u7f6a Topic 7 \u738b, \u4faf, \u6c49, \u79e6, \u5e1d, \u56fd, \u2f74\u7acb, \u4ee4, \u9b4f, \u5c01, \u8d75, \u2f82\u81e3, \u695a, \u65f6, \u5c06 Topic 8 \u5377, \u82b1, \u6625, \u5f52, \u2f46\u65e0, \u65f6, \u5ba2, \u2f2d\u5c71, \u7a7a, \u79cb, \u2f47\u65e5, \u2edb\u98ce, \u5c3d, \u9152, \u9001 Topic 9 \u8bd7, \u8d4b, \u6b4c, \u7eaa, \u2f5f\u7389, \u2f2d\u5c71, \u6e38, \u2edc\u98de, \u96c6, \u2edb\u98ce, \u65f6, \u2f46\u65e0, \u7c7b\u805a, \u672c, \u5fa1 Topic 10 \u5377, \u4e66, \u672c, \u4f20, \u96c6, \u2f47\u65e5, \u8bd7, \u2f42\u6587, \u4f59, \u5ea6, \u5fd7, \u64b0, \u7bc7, \u7ecf, \u8bb0 Topic 11 \u547d, \u5b98, \u8d3c, \u6388, \u5175, \u5c14, \u5de1\u629a, \u8425, \u963f, \u2ea0\u6c11, \u2f24\u5927\u2f82\u81e3, \u90e8, \u660e, \u9980, \u603b\u7763 Topic 12 \u5dde, \u5b98, \u8bcf, \u5e1d, \u5e9c, \u5c1a\u4e66, \u5b97, \u53f8, \u5fa1\u53f2, \u738b, \u90ce, \u4ee4, \u53f3, \u4e1e, \u8282\u5ea6 Topic 13 \u5b98, \u53f8, \u8bcf, \u5dde, \u672c, \u94b1, \u4ee4, \u8def\uf937, \u636e, \u2ea0\u6c11, \u594f, \u4e5e, \u2f82\u81e3, \u519b, \u5dee Topic 14 \u2f46\u65e0, \u5904, \u5982\u6b64, \u2f3c\u5fc3, \u5148\u2f63\u751f, \u987b, \u5982\u4f55, \u7406\uf9e4, \u7b87, \u7269, \u5b66, \u4ec1, \u4ed6, \u5e95, \u5374 Topic 15 \u5175, \u519b, \u5dde, \u8d3c, \u9063, \u5c06, \u57ce, \u653b, \u6218, \u5b88, \u964d, \u738b, \u7834, \u8fdb, \u6b7b Topic 16 \u53bf, \u5dde, \u2f54\u6c34, \u7f6e, \u4e1c, \u2f2d\u5c71, \u5357, \u2ec4\u897f, \u90e1, \u5e9c, \u5317\uf963, \u5c5e, \u6cb3, \u57ce, \u6cbb Topic 17 \u2f46\u65e0, \u5929\u4e0b, \u2ea0\u6c11, \u80fd, \u4e0d\uf967\u80fd, \u4ee5\u4e3a, \u5584, \u4e0d\uf967\u53ef, \u2f8f\u884c\ufa08, \u2f63\u751f, \u8005\u4e5f, \u9053, \u53ef, \u7269, \u6cbb Topic 18 \u90d1, \u2f24\u5927\u592b, \u738b, \u2eec\u9f50, \u8bf8\u4faf, \u2fb3\u97f3, \u4f20, \u4f2f, \u793c, \u664b, \u91ca, \u2f46\u65e0, \u670d, \u4e66, \u4faf Topic 19 \u796d, \u793c, \u7940, \u5e99, \u5b98, \u4e50, \u732e, \u670d, \u5236, \u795e, \u5bab, \u6b21, \u2ec4\u897f, \u594f, \u8bbe"}, {"heading": "40 topic model, showing 15 highest probability words in each topic", "text": "Topic 0 \u90d1, \u8bf8\u4faf, \u793c, \u2fb3\u97f3, \u91ca, \u738b, \u6b63\u4e49, \u5929\u2f26\u5b50, \u2f24\u5927\u592b, \u796d, \u2f20\u58eb, \u547d, \u65f6, \u4f20, \u6b63 Topic 1 \u6b4c, \u2f5f\u7389, \u2f8f\u884c\ufa08, \u7a7a, \u2f46\u65e0, \u79cb, \u5f52, \u2edc\u98de, \u2f47\u65e5, \u6e05, \u60b2, \u671b, \u2f63\u751f, \u5df2, \u66f2 Topic 2 \u5b98, \u738b, \u8d3c, \u2f82\u81e3, \u90fd, \u5ef7, \u5fa1\u53f2, \u547d, \u674e\uf9e1, \u6b7b, \u5e1d, \u5fe0, \u65f6, \u5f20, \u5de1\u629a Topic 3 \u2f82\u81e3, \u965b\u4e0b, \u2f46\u65e0, \u5929\u4e0b, \u5df2, \u8bae, \u53ef, \u4f0f, \u594f, \u4ee4, \u4efb, \u5723, \u56fd, \u6715, \u5b98 Topic 4 \u4e66, \u4ee5\u4e3a, \u65f6, \u2f46\u65e0, \u4e0d\uf967\u80fd, \u4e0d\uf967\u53ef, \u59cb, \u2f42\u6587, \u53d6, \u5b66, \u53ef, \u591a, \u4e16, \u4e0d\uf967\u77e5, \u5510 Topic 5 \u5dde, \u8282\u5ea6, \u738b, \u523a\uf9ff\u53f2, \u674e\uf9e1, \u5b97, \u519b, \u8bcf, \u90fd, \u9547, \u5143, \u5e1d, \u5510, \u662d, \u5fe0 Topic 6 \u2f42\u6587, \u2f46\u65e0, \u795e, \u7075, \u5fb7, \u2edb\u98ce, \u6d41, \u8fdc, \u6e05, \u7269, \u9053, \u5468, \u5149, \u2fbc\u9ad8, \u2f3c\u5fc3\nTopic 7 \u70ba, \u65bc, \u7121, \u570b, \u8ecd, \u5247, \u5f8c, \u66f8, \u8207, \u2ed1\u2fa7\u9577, \u8b02, \u6642, \u79ae, \u8af8, \u6771 Topic 8 \u2f47\u65e5, \u5ea6, \u5206, \u4f59, \u5386, \u6cd5, \u5dee, \u2f49\u6708, \u5b9a, \u6714, \u51cf, \u2f8f\u884c\ufa08, \u52a0, \u534a, \u6570 Topic 9 \u94b1, \u5b98, \u2ea0\u6c11, \u4ee4, \u2f65\u7530, \u53f8, \u6237, \u5c81, \u76d0, \u7ed9, \u8bcf, \u6cd5, \u5dde, \u7a0e, \u5f79 Topic 10 \u5377, \u9001, \u5f52, \u5bc4, \u2f69\u767d, \u95f2, \u5e94, \u2f2d\u5c71, \u9898, \u5ba2, \u5c45\u6613\uf9e0, \u65e7, \u79cb, \u6625, \u2f47\u65e5 Topic 11 \u7940, \u7687, \u5e1d, \u5e99, \u793c, \u796d, \u7687\u5e1d, \u795e, \u5bab, \u594f, \u5b97, \u732e, \u592a, \u6bbf, \u90ca Topic 12 \u72af, \u661f, \u2f49\u6708, \u6714, \u5c81, \u5929, \u5360, \u79cb, \u5175, \u2f47\u65e5, \u590f, \u2fac\u96e8, \u51ac, \u592a\u2f69\u767d, \u6625 Topic 13 \u2eec\u9f50, \u664b, \u4f10, \u695a, \u4f2f, \u4e66, \u738b, \u4faf, \u4f20, \u5e08, \u90d1, \u53d4, \u536b, \u9c81, \u8bf8\u4faf Topic 14 \u56fd, \u9063, \u738b, \u9063\u4f7f, \u90e8, \u5dde, \u2ec4\u897f, \u2ee2\u9a6c, \u8d21, \u5730, \u590f, \u57ce, \u5317\uf963, \u2f74\u7acb, \u7f57 Topic 15 \u5377, \u672c, \u4f20, \u4e66, \u96c6, \u7bc7, \u64b0, \u5fd7, \u5f55, \u2f42\u6587, \u7ecf, \u6309, \u5f15, \u8bb0, \u5b8b Topic 16 \u5dde, \u523a\uf9ff\u53f2, \u5c06\u519b, \u738b, \u5e1d, \u5c01, \u4ee4, \u5c1a\u4e66, \u8bcf, \u5b5d, \u62dc, \u521d, \u65f6, \u5e9c, \u9b4f Topic 17 \u2f2d\u5c71, \u2f6f\u77f3, \u2f54\u6c34, \u2fa5\u91cc\uf9e9, \u2ec4\u897f, \u2ed4\u95e8, \u4e1c, \u5357, \u5bfa, \u8bb0, \u4f59, \u5317\uf963, \u540d, \u5ca9, \u2f4a\u6728 Topic 18 \u2fce\u9f13, \u2fa6\u91d1\uf90a, \u670d, \u4e50, \u2ecb\u8f66, \u65d7, \u58f0, \u5236, \u2f90\u8863, \u5f8b\uf9d8, \u2ee9\u9ec4, \u6b21, \u821e, \u5e26, \u51a0 Topic 19 \u5e08, \u7ecf, \u50e7, \u4f5b, \u65f6, \u9053, \u2f46\u65e0, \u5bfa, \u2f63\u751f, \u6cd5, \u5982\u4f55, \u8bd1, \u738b, \u7985\u5e08, \u5904 Topic 20 \u670d, \u2e9f\u6bcd, \u2f57\u7236, \u2f25\u5973\uf981, \u4e27, \u846c, \u59bb, \u592b\u2f08\u4eba, \u6b7b, \u5987, \u793c, \u4eb2, \u7956, \u54ed, \u2f46\u65e0 Topic 21 \u82b1, \u6625, \u2fb9\u9999, \u2f5f\u7389, \u2edb\u98ce, \u7ea2, \u6101, \u4f3c, \u66f4\uf901, \u2f49\u6708, \u70df, \u9189, \u2fac\u96e8, \u9152, \u5c3d Topic 22 \u8bd7, \u2f46\u65e0, \u4e66, \u2f7c\u8001\uf934, \u65f6, \u4f59, \u9152, \u2f47\u65e5, \u5ba2, \u8bed, \u5f52, \u5df2, \u2f69\u767d, \u5c11, \u753b Topic 23 \u547d, \u5c14, \u2f24\u5927\u2f82\u81e3, \u6388, \u9980, \u989d, \u963f, \u8425, \u514b, \u603b\u7763, \u90e8, \u660e, \u5de1\u629a, \u5e03, \u4e7e\u9686\uf9dc Topic 24 \u5dde, \u2fa6\u91d1\uf90a, \u8bcf, \u519b, \u547d, \u5e9c, \u6539\u4f5c, \u738b, \u90fd, \u8def\uf937, \u53f8, \u8d50, \u2fa6\u91d1\uf90a\u2f08\u4eba, \u9063, \u5b8b Topic 25 \u65bc, \u5bbe, \u2ec4\u897f, \u62dc, \u7235, \u4e3b\u2f08\u4eba, \u5f15, \u5347, \u4f4d, \u5b98, \u524d, \u8bbe, \u518d\u62dc, \u4e1c, \u795d Topic 26 \u8bd7, \u7eaa, \u8d4b, \u7c7b\u805a, \u5fa1, \u89c8, \u5f15, \u96c6, \u5207, \u6c49\u4e66, \u8bd7\u2f48\u66f0, \u2f42\u6587\u9009, \u521d, \u4e50\u5e9c, \u6b4c Topic 27 \u5175, \u8d3c, \u519b, \u5c06, \u653b, \u57ce, \u9063, \u6218, \u5b88, \u7834, \u51fb, \u8d25, \u5e08, \u8fdb, \u654c Topic 28 \u9619, \u5143, \u5fb7, \u547d, \u53ef, \u5949, \u2f46\u65e0, \u8d50, \u5c14, \u671d, \u4f0f, \u8868, \u9053, \u5b98, \u8282 Topic 29 \u670d, \u6cbb, \u75c5, \u70ed, \u8109, \u6c64, \u2f53\u6c14, \u2f8e\u8840, \u4e38, \u5bd2, \u5347, \u94b1, \u6563, \u672b, \u52a0 Topic 30 \u5b98, \u53f8, \u4ee4, \u7f6e, \u6b63, \u90ce, \u638c, \u5e9c, \u4e1e, \u76d1, \u7701, \u5c1a\u4e66, \u5458, \u5de6, \u5236 Topic 31 \u5e1d, \u8fc1, \u5b98, \u65f6, \u5fa1\u53f2, \u5352, \u5c1a\u4e66, \u521d, \u8bae, \u53ec, \u594f, \u8bcf, \u5b66\u2f20\u58eb, \u90ce, \u540f\uf9de Topic 32 \u5929\u4e0b, \u2f46\u65e0, \u2ea0\u6c11, \u80fd, \u4e0d\uf967\u80fd, \u2f26\u5b50\u2f48\u66f0, \u5584, \u6cbb, \u2f8f\u884c\ufa08, \u8005\u4e5f, \u9053, \u4ee5\u4e3a, \u5fb7, \u6076, \u2f7d\u800c\u4e0d\uf967 Topic 33 \u5dde, \u8bcf, \u672c, \u53f8, \u5b98, \u2f82\u81e3, \u636e, \u4e5e, \u8def\uf937, \u4ee4, \u6539, \u594f, \u9601, \u5b8b, \u671d\u5ef7 Topic 34 \u5904, \u2f3c\u5fc3, \u2f46\u65e0, \u5982\u6b64, \u5982\u4f55, \u5148\u2f63\u751f, \u7b87, \u987b, \u7406\uf9e4, \u5e95, \u4ed6, \u4ec1, \u5374, \u6027, \u5b66 Topic 35 \u738b, \u4faf, \u6c49, \u79e6, \u5e1d, \u5c01, \u2f74\u7acb, \u4ee4, \u5c06\u519b, \u90e1, \u9b4f, \u65f6, \u8d75, \u5929\u4e0b, \u540f\uf9de Topic 36 \u2f53\u6c14, \u2f46\u65e0, \u795e, \u2f63\u751f, \u9633, \u8c61, \u9634, \u2f47\u65e5, \u5929, \u2f55\u706b, \u65f6, \u5409, \u5929\u5730, \u5f62, \u9634\u9633 Topic 37 \u5c06\u519b, \u738b, \u592a\u5b88, \u523a\uf9ff\u53f2, \u9063, \u90e1, \u4ee4, \u664b, \u5c1a\u4e66, \u5f81, \u5357, \u9b4f, \u666f, \u5dde, \u4faf Topic 38 \u2f54\u6c34, \u53bf, \u6cb3, \u4e1c, \u5317\uf963, \u57ce, \u5357, \u2f2d\u5c71, \u2ec4\u897f, \u5f84, \u4e1c\u5357, \u4e1c\u5317\uf963, \u90e1, \u9633, \u6c49 Topic 39 \u5dde, \u53bf, \u7f6e, \u5e9c, \u90e1, \u5c5e, \u2f2d\u5c71, \u2ec4\u897f, \u5357, \u4e1c, \u5e9f, \u521d, \u6cbb, \u6539, \u53f8"}, {"heading": "60 topic model, showing 15 highest probability words in each topic", "text": "Topic 0 \u5dde, \u8bcf, \u672c, \u5b8b, \u5b98, \u6539, \u636e, \u8d50, \u6bbf, \u9601, \u9662, \u90fd, \u547d, \u519b, \u8f6c\u8fd0 Topic 1 \u963f, \u547d, \u5c14, \u54c8, \u9c81, \u8def\uf937, \u8d50, \u53f0, \u8499\u53e4, \u2f8f\u884c\ufa08\u7701, \u90fd, \u660e, \u524c, \u4e2d\u4e66, \u8fbe Topic 2 \u4e50, \u821e, \u2fce\u9f13, \u670d, \u2fa6\u91d1\uf90a, \u58f0, \u5f8b\uf9d8, \u5236, \u5bab, \u949f, \u2ee9\u9ec4, \u2ecb\u8f66, \u594f, \u2f90\u8863, \u66f2 Topic 3 \u5b98, \u53f8, \u6b63, \u7f6e, \u638c, \u5e9c, \u4ee4, \u90ce, \u76d1, \u4e1e, \u7701, \u5458, \u5de6, \u53f3, \u536b Topic 4 \u5b98, \u5fa1\u53f2, \u738b, \u2f82\u81e3, \u90fd, \u5ef7, \u5fe0, \u5f20, \u65f6, \u674e\uf9e1, \u547d, \u671d, \u6b7b, \u629a, \u5e9c Topic 5 \u7ecf, \u4f5b, \u5bfa, \u50e7, \u738b, \u6cd5, \u8bd1, \u65f6, \u83e9\u8428, \u5df2, \u6c99\u2ed4\u95e8, \u540d, \u8eab, \u2f46\u65e0, \u5584 Topic 6 \u2f82\u81e3, \u965b\u4e0b, \u4f0f, \u2f46\u65e0, \u5723, \u4efb, \u5df2, \u5949, \u7a83, \u594f, \u671b, \u8c28, \u6df1, \u6069, \u613f Topic 7 \u6714, \u8bcf, \u590f, \u2f49\u6708, \u6625, \u5c1a\u4e66, \u79cb, \u51ac, \u7532\u2f26\u5b50, \u2f47\u65e5, \u2f04\u4e59\u536f, \u4e01\u536f, \u2f9f\u8f9b\u536f, \u7532\u7533, \u5e9a\u7533\nTopic 8 \u8ecd, \u7532, \u4e19, \u570b, \u90e8, \u6703, \u653f\u5e9c, \u7f8e, \u54e1, \u2f47\u65e5, \u2ed1\u2fa7\u9577, \u5230, \u620a, \u2f00\u4e00\u2f06\u4e8c, \u59d4 Topic 9 \u4e66, \u8bd7, \u4f59, \u2f42\u6587, \u65f6, \u4e88, \u4f20, \u5510, \u5143, \u5b66, \u591a, \u540d, \u4e16, \u53e4, \u5c11 Topic 10 \u56fd, \u9063, \u738b, \u9063\u4f7f, \u5dde, \u2ec4\u897f, \u90e8, \u2ee2\u9a6c, \u5317\uf963, \u5730, \u5175, \u7a81\u53a5, \u57ce, \u2f74\u7acb, \u590f Topic 11 \u5dde, \u53bf, \u90e1, \u7f6e, \u5c5e, \u5e9c, \u5e9f, \u5510, \u57ce, \u521d, \u6cbb, \u6c49, \u6539, \u9886, \u664b Topic 12 \u5377, \u96c6, \u64b0, \u5f55, \u7ecf, \u5fd7, \u672c, \u8bb0, \u9648\u2f52\u6c0f, \u4e66, \u5510, \u7f16, \u8bba, \u25b3, \u2f00\u4e00\u767e Topic 13 \u5e1d, \u8fc1, \u65f6, \u8bcf, \u5b98, \u594f, \u53ec, \u7f62, \u5b66\u2f20\u58eb, \u8bae, \u8fdb, \u5352, \u521d, \u5b97, \u8bba Topic 14 \u2edb\u98ce, \u2f46\u65e0, \u2fbc\u9ad8, \u6d41, \u2f5f\u7389, \u6e05, \u8fdc, \u2fa6\u91d1\uf90a, \u2f47\u65e5, \u8d4b, \u671b, \u9980, \u5149, \u6e38, \u2f2d\u5c71 Topic 15 \u661f, \u72af, \u5929, \u5360, \u2f49\u6708, \u592a\u2f69\u767d, \u5175, \u2fac\u96e8, \u5c81, \u8367\u60d1, \u2f9a\u8d64, \u2f47\u65e5, \u5730, \u2f55\u706b, \u2ec4\u897f Topic 16 \u2eec\u9f50, \u664b, \u4f2f, \u4f10, \u4e66, \u695a, \u90d1, \u4faf, \u4f20, \u5e08, \u536b, \u9c81, \u53d4, \u8bf8\u4faf, \u738b Topic 17 \u5c06\u519b, \u523a\uf9ff\u53f2, \u5dde, \u738b, \u9b4f, \u666f, \u2eec\u9f50, \u9063, \u5357, \u2fbc\u9ad8\u7956, \u519b, \u592a\u5b88, \u4faf, \u90fd\u7763, \u5e1d Topic 18 \u547d, \u2f24\u5927\u2f82\u81e3, \u9980, \u603b\u7763, \u6388, \u5de1\u629a, \u8425, \u8bae, \u4e7e\u9686\uf9dc, \u989d, \u7f72, \u594f, \u532a, \u5eb7\u7199, \u7531 Topic 19 \u70ba, \u7121, \u65bc, \u5247, \u570b, \u6642, \u8207, \u5f8c, \u66f8, \u2f92\u898b\ufa0a, \u2ed1\u2fa7\u9577, \u4f86\uf92d, \u5c07, \u6f22\ufa47, \u842c Topic 20 \u70ba, \u79ae, \u5b98, \u65bc, \u8af8, \u8b02, \u8b70, \u594f, \u5f9e, \u4ee4, \u4e26, \u8acb, \u5f8c, \u66f8, \u672c Topic 21 \u738b, \u4faf, \u79e6, \u6c49, \u8d75, \u2f82\u81e3, \u695a, \u9b4f, \u5c06\u519b, \u2eec\u9f50, \u2f74\u7acb, \u5c01, \u5929\u4e0b, \u56fd, \u5175 Topic 22 \u5e08, \u50e7, \u5982\u4f55, \u9053, \u7985\u5e08, \u5904, \u2f46\u65e0, \u548c\u5c1a, \u65f6, \u4e0a\u5802, \u2f63\u751f, \u2f2d\u5c71, \u4e3e, \u5374, \u4f5b Topic 23 \u670d, \u6c64, \u6cbb, \u4e38, \u5347, \u94b1, \u672b, \u52a0, \u53f3, \u5473, \u6563, \u53d6, \u714e, \u70ed, \u9152 Topic 24 \u8fc1, \u90ce, \u5c1a\u4e66, \u65f6, \u62dc, \u5352, \u4ee4, \u5e74\uf98e, \u90e1, \u2f57\u7236, \u521d, \u8d60, \u5e9c, \u5c11, \u2f24\u5927\u592b Topic 25 \u2fa6\u91d1\uf90a, \u5dde, \u519b, \u6539\u4f5c, \u5e9c, \u2fa6\u91d1\uf90a\u2f08\u4eba, \u9063, \u5b98, \u5b97, \u5ba3, \u53f8, \u8bcf, \u738b, \u90fd, \u5b8b Topic 26 \u2fb3\u97f3, \u90d1, \u738b, \u6b63\u4e49, \u8bf8\u4faf, \u91ca, \u5929\u2f26\u5b50, \u793c, \u796d, \u638c, \u2f24\u5927\u592b, \u547d, \u5730, \u5b98, \u671d Topic 27 \u5bbe, \u65bc, \u4e3b\u2f08\u4eba, \u7235, \u2f24\u5927\u592b, \u2f2b\u5c38, \u62dc, \u793c, \u2ec4\u897f, \u91ca, \u5c04, \u5347, \u9636, \u2f20\u58eb, \u53d7 Topic 28 \u5dde, \u8282\u5ea6, \u519b, \u90fd, \u738b, \u9547, \u664b, \u523a\uf9ff\u53f2, \u9063, \u5951\u4e39\uf95e, \u5f66, \u5e1d, \u592a\u7956, \u5168, \u2f8f\u884c\ufa08 Topic 29 \u5fb7, \u547d, \u5143, \u5929, \u9619, \u5c14, \u7687, \u8f7d, \u2f46\u65e0, \u795e, \u5c06, \u793c, \u6c38, \u514b, \u5723 Topic 30 \u2f46\u65e0, \u7269, \u9053, \u2f63\u751f, \u2f3c\u5fc3, \u5f62, \u6210, \u80fd, \u7406\uf9e4, \u5929\u5730, \u5929, \u660e, \u4e0d\uf967\u80fd, \u6027, \u53ef Topic 31 \u5377, \u9001, \u5f52, \u5bc4, \u5e94, \u2f69\u767d, \u6625, \u6811, \u2f2d\u5c71, \u79cb, \u8fdc, \u2f47\u65e5, \u522b, \u9898, \u5c45\u6613\uf9e0 Topic 32 \u4ed6, \u5c31, \u548c, \u4ecb, The, \u5f1f, said, \u4f60, \u5bf9, \u5230, \u2f83\u81ea\u2f30\u5df1, \u65e6, Master, \u5c06, \u8981 Topic 33 \u2f54\u6c34, \u53bf, \u4e1c, \u2f2d\u5c71, \u2ec4\u897f, \u5357, \u6cb3, \u5317\uf963, \u5e9c, \u4e1c\u5357, \u4e1c\u5317\uf963, \u2ec4\u897f\u5357, \u57ce, \u2ec4\u897f\u5317\uf963, \u5fd7 Topic 34 \u2f47\u65e5, \u5ea6, \u5206, \u4f59, \u5386, \u5dee, \u6cd5, \u2f49\u6708, \u6714, \u5b9a, \u51cf, \u52a0, \u2f8f\u884c\ufa08, \u534a, \u6c42 Topic 35 \u9619, \u795e, \u771f, \u4ed9, \u2f5e\u7384, \u7075, \u2f5f\u7389, \u7cbe, \u7ecf, \u9053, \u4e39\uf95e, \u5143, \u5bab, \u2f53\u6c14, \u2fa6\u91d1\uf90a Topic 36 \u672c, \u5f15, \u7bc7, \u6309, \u4f20, \u6849, \u6c49\u4e66, \u672c\u4f5c, \u5fa1, \u8bef, \u2f42\u6587, \u2fb3\u97f3, \u4e66, \u53f2\u8bb0, \u738b Topic 37 \u2f46\u65e0, \u5f52, \u5df2, \u5ba2, \u2f7c\u8001\uf934, \u2f47\u65e5, \u2f8f\u884c\ufa08, \u7a7a, \u79cb, \u9152, \u2f2d\u5c71, \u8eab, \u65f6, \u8c01, \u6e38 Topic 38 \u5dde, \u8282\u5ea6, \u523a\uf9ff\u53f2, \u674e\uf9e1, \u5fa1\u53f2, \u5143, \u738b, \u5c1a\u4e66, \u5b97, \u4f8d\u90ce, \u4e2d\u4e66, \u2f24\u5927\u592b, \u5de6, \u53f3, \u517c Topic 39 \u6b7b, \u65f6, \u4ee4, \u2f46\u65e0, \u56e0, \u5bb6, \u5c06, \u6570, \u6740, \u544a, \u5f52, \u5750, \u2e9f\u6bcd, \u4e0d\uf967\u80fd, \u5df2 Topic 40 \u5175, \u8d3c, \u519b, \u5c06, \u653b, \u57ce, \u6218, \u7834, \u9063, \u8fdb, \u51fb, \u5e08, \u5b88, \u8d25, \u5dde Topic 41 \u2f63\u751f, \u2f69\u767d, \u2edd\u2fb7\u98df, \u2f4a\u6728, \u2ee5\u9c7c, \u2f54\u6c34, \u65f6, \u2f6f\u77f3, \u2f46\u65e0, \u2ee2\u9a6c, \u2f55\u706b, \u5934, \u2f5c\u725b, \u8349, \u591a Topic 42 \u7940, \u796d, \u5e99, \u793c, \u5b98, \u7687\u5e1d, \u795e, \u732e, \u4eea, \u90ca, \u5949, \u594f, \u5f15, \u6bbf, \u4f4d Topic 43 \u670d, \u4e27, \u2f57\u7236, \u846c, \u793c, \u2e9f\u6bcd, \u54ed, \u796d, \u592b\u2f08\u4eba, \u7956, \u4eb2, \u5987, \u2f24\u5927\u592b, \u2f57\u7236\u2e9f\u6bcd, \u2f20\u58eb Topic 44 \u5929\u4e0b, \u2f46\u65e0, \u53ef, \u4e0d\uf967\u53ef, \u4ee5\u4e3a, \u2ea0\u6c11, \u4e0d\uf967\u80fd, \u5175, \u56fa, \u56fd, \u80fd, \u5236, \u5c06, \u5df2, \u6613\uf9e0 Topic 45 \u2f53\u6c14, \u75c5, \u8109, \u9634, \u9633, \u70ed, \u5bd2, \u2f55\u706b, \u6cbb, \u2f47\u65e5, \u2f8e\u8840, \u865a, \u4f24, \u2f63\u751f, \u6b7b Topic 46 \u8d4b, \u5207, \u8bd7\u2f48\u66f0, \u2fb3\u97f3, \u6c49\u4e66, \u2f51\u6bdb, \u5584, \u2f26\u5b50\u2f48\u66f0, \u2f2d\u5c71, \u4f20, \u8c8c, \u5de6, \u5fd7, \u4f59, \u8bd7 Topic 47 \u5b98, \u4ee4, \u6555, \u6715, \u594f, \u8bcf, \u5df2, \u5236, \u4e3e, \u53f8, \u53ef, \u4efb, \u5211, \u7f6a, \u2f46\u65e0 Topic 48 \u65d7, \u8425, \u2ee2\u9a6c, \u2ecb\u8f66, \u4ee4, \u5c3a, \u5175, \u603b, \u519b, \u524d, \u2f28\u5bf8, \u4e08, \u6b65, \u2f34\u5e7f, \u2f38\u5f13 Topic 49 \u8bd7, \u7eaa, \u7c7b\u805a, \u6b4c, \u672c, \u4e50\u5e9c, \u96c6, \u2f42\u6587\u9009, \u2f42\u6587\u82d1, \u89c8, \u5fa1, \u521d, \u5f15, \u66f2, \u2f5f\u7389 Topic 50 \u53f8, \u4e5e, \u5b98, \u8bcf, \u8def\uf937, \u672c, \u5dde, \u594f, \u671d\u5ef7, \u4ee4, \u636e, \u2f82\u81e3, \u5143, \u5df2, \u5b89\u2f6f\u77f3 Topic 51 \u5904, \u5982\u6b64, \u7b87, \u5148\u2f63\u751f, \u5982\u4f55, \u2f3c\u5fc3, \u987b, \u4ec1, \u5e95, \u5374, \u7406\uf9e4, \u5b66, \u2f46\u65e0, \u65f6, \u4ed6 Topic 52 \u2f2d\u5c71, \u2f6f\u77f3, \u5bfa, \u8bb0, \u2fa5\u91cc\uf9e9, \u2ec4\u897f, \u5357, \u2ed4\u95e8, \u4e1c, \u5ca9, \u5cf0, \u9662, \u4f59, \u65e7, \u2f42\u6587 Topic 53 \u82b1, \u6625, \u2fb9\u9999, \u2f5f\u7389, \u2edb\u98ce, \u7ea2, \u2f49\u6708, \u70df, \u6101, \u4f3c, \u66f4\uf901, \u9189, \u9152, \u68a6, \u5c3d\nTopic 54 \u94b1, \u2ea0\u6c11, \u5b98, \u2f65\u7530, \u76d0, \u5c81, \u6237, \u7ed9, \u4ee4, \u7a0e, \u2f6f\u77f3, \u5dde, \u53f8, \u8bcf, \u5f79 Topic 55 \u4f20, \u4e66, \u6c49, \u5468, \u5e1d, \u2f42\u6587, \u8bae, \u65f6, \u793c, \u59cb, \u660e, \u53e4, \u4e16, \u4ee5\u4e3a, \u738b Topic 56 \u8c61, \u5409, \u5366, \u2f46\u65e0, \u4e7e, \u6613\uf9e0, \u9633, \u9634, \u8d1e, \u5764, \u548e, \u6b63, \u4f4d, \u521a, \u5229\uf9dd Topic 57 \u5e1d, \u738b, \u5c01, \u7687, \u592a\u540e, \u592a\u2f26\u5b50, \u5b5d, \u2f74\u7acb, \u5b97, \u7687\u540e, \u5bab, \u5983, \u7687\u5e1d, \u85a8, \u55e3 Topic 58 \u2ea0\u6c11, \u5929\u4e0b, \u2f26\u5b50\u2f48\u66f0, \u6cbb, \u2f8f\u884c\ufa08, \u2f46\u65e0, \u738b, \u5b54\u2f26\u5b50, \u5584, \u80fd, \u2f82\u81e3, \u5fb7, \u56fd, \u8d24, \u4e0d\uf967\u80fd Topic 59 \u5c06\u519b, \u5e1d, \u592a\u5b88, \u5c06, \u9063, \u90e1, \u738b, \u4faf, \u9b4f, \u5434, \u664b, \u5f81, \u5175, \u523a\uf9ff\u53f2, \u519b"}, {"heading": "80 topic model, showing 15 highest probability words in each topic", "text": "Topic 0 \u70ba, \u65bc, \u79ae, \u5b98, \u8af8, \u8b02, \u5f8c, \u5f9e, \u7121, \u66f8, \u594f, \u4ee4, \u5247, \u8b70, \u8acb Topic 1 \u670d, \u2fa6\u91d1\uf90a, \u2f90\u8863, \u2ecb\u8f66, \u6b21, \u51a0, \u5236, \u65d7, \u5e26, \u2ed8\u9752, \u9a7e, \u8f82, \u9970, \u2ee9\u9ec4, \u2f5f\u7389 Topic 2 \u2f46\u65e0, \u8d4b, \u2f42\u6587, \u8fdc, \u795e, \u7269, \u89c2, \u7406\uf9e4, \u660e, \u6d41, \u2edb\u98ce, \u65f6, \u53ef, \u901a, \u8c61 Topic 3 \u5dde, \u8bcf, \u5b8b, \u8d50, \u6bbf, \u9662, \u8fbd, \u90fd, \u547d, \u67a2, \u5b66\u2f20\u58eb, \u519b, \u5951\u4e39\uf95e, \u5b97, \u9601 Topic 4 \u5dde, \u53bf, \u7f6e, \u5e9c, \u5c5e, \u90e1, \u53f8, \u6539, \u5e9f, \u5143, \u9886, \u6cbb, \u6c5f, \u2faa\u96b6, \u5b81 Topic 5 \u5982\u6b64, \u7b87, \u5982\u4f55, \u5904, \u5e95, \u4ed6, \u987b, \u5374, \u770b, \u505a, \u9053\u7406\uf9e4, \u65f6, \u4ec1, \u90fd, \u5148\u2f63\u751f Topic 6 \u7532, \u4e19, \u2f47\u65e5, \u5b98, \u90e8, \u8d35, \u8d22, \u620a, \u65fa, \u65f6, \u653f\u5e9c, \u5230, \u7f8e, \u5e9a, \u8fd0 Topic 7 \u5e08, \u50e7, \u5982\u4f55, \u9053, \u7985\u5e08, \u5904, \u2f46\u65e0, \u548c\u5c1a, \u4e0a\u5802, \u2f2d\u5c71, \u2f63\u751f, \u65f6, \u4e3e, \u5374, \u4f60 Topic 8 \u72af, \u661f, \u6714, \u2f49\u6708, \u5c81, \u5929, \u5360, \u5175, \u592a\u2f69\u767d, \u2f47\u65e5, \u7532\u2f26\u5b50, \u2f9f\u8f9b\u536f, \u4e01\u536f, \u4e19\u5348, \u7532\u5bc5 Topic 9 \u8d3c, \u5175, \u653b, \u519b, \u57ce, \u7834, \u6218, \u5bc7, \u8d25, \u5dde, \u8fdb, \u5c06, \u9677, \u5b88, \u7387 Topic 10 \u547d, \u8def\uf937, \u90fd, \u519b, \u5b8b, \u8d50, \u5dde, \u4e2d\u4e66, \u8bcf, \u2f8f\u884c\ufa08\u7701, \u9c81, \u53f8, \u524c, \u7701, \u963f Topic 11 \u5dde, \u5e1d, \u968b, \u5c01, \u5468, \u5c06\u519b, \u603b\u7ba1, \u523a\uf9ff\u53f2, \u738b, \u592a\u5b97, \u4ee4, \u2fbc\u9ad8\u7956, \u7a81\u53a5, \u5e9c, \u90fd\u7763 Topic 12 \u4e66, \u2eec\u9f50, \u4f20, \u4faf, \u664b, \u4f2f, \u90d1, \u89e3\u4e91, \u4f10, \u5e08, \u79f0, \u536b, \u8bf8\u4faf, \u5352, \u76df Topic 13 \u6715, \u53ef, \u6555, \u8bcf, \u5b98, \u537f, \u5236, \u5c14, \u8d50, \u2f46\u65e0, \u5b88, \u5177, \u524d, \u4efb, \u5df2 Topic 14 \u4faf, \u5e1d, \u6c49, \u90e1, \u5c06\u519b, \u540f\uf9de, \u5c01, \u65f6, \u4ee4, \u592a\u5b88, \u4f20, \u738b, \u8bcf, \u56fd, \u2f24\u5927\u592b Topic 15 \u2f26\u5b50\u2f48\u66f0, \u5b54\u2f26\u5b50, \u793c, \u5b5f\u2f26\u5b50, \u4ec1, \u2f8f\u884c\ufa08, \u4e49, \u5b54, \u5723\u2f08\u4eba, \u2f46\u65e0, \u7ae0, \u5929\u4e0b, \u8005\u4e5f, \u5584, \u9053 Topic 16 \u8fc1, \u65f6, \u5c1a\u4e66, \u5352, \u62dc, \u90ce, \u521d, \u4ee4, \u5c11, \u537f, \u2f57\u7236, \u592a\u2f26\u5b50, \u8f6c, \u7d2f, \u5584 Topic 17 \u2f46\u65e0, \u2f82\u81e3, \u4e49, \u4f20, \u6614, \u547d, \u6000, \u5b9e, \u5fd7, \u5fb7, \u56fd, \u53ef, \u4ee4, \u5174, \u8fdc Topic 18 \u8bd7, \u5510, \u753b, \u96c6, \u4f59, \u53e5\uf906, \u5143, \u674e\uf9e1, \u8bed, \u65f6, \u8bcd, \u738b, \u540d, \u2f2f\u5de5, \u2f42\u6587 Topic 19 \u672c, \u5dde, \u8bcf, \u636e, \u53f8, \u5b98, \u8def\uf937, \u4ee4, \u6539, \u9980, \u2f82\u81e3, \u519b, \u90fd, \u4e5e, \u5377 Topic 20 \u8d4b, \u6c49\u4e66, \u5207, \u8bd7\u2f48\u66f0, \u2fb3\u97f3, \u4f20, \u2f51\u6bdb, \u2f26\u5b50\u2f48\u66f0, \u5584, \u8bd7, \u8bb0, \u2f2d\u5c71, \u6c49, \u6881\uf97a, \u5de6 Topic 21 \u2eec\u9f50, \u738b, \u695a, \u664b, \u2f82\u81e3, \u4f10, \u5c06, \u53d4, \u56fd, \u5434, \u5e08, \u4f2f, \u9c81, \u2f46\u65e0, \u5bf9\u2f48\u66f0 Topic 22 \u9619, \u547d, \u2f8f\u884c\ufa08, \u5143, \u5c06, \u9980, \u9053, \u671d, \u6388, \u5b8f, \u5c01, \u5fb7, \u5e9c, \u5dde, \u529f Topic 23 \u2f82\u81e3, \u4f0f, \u5949, \u8868, \u8c28, \u8d50, \u5723, \u2f46\u65e0, \u6069, \u4efb, \u8c22, \u965b\u4e0b, \u542f, \u4e4b\u2f84\u81f3, \u4f0f\u60df Topic 24 \u5dde, \u8282\u5ea6, \u523a\uf9ff\u53f2, \u674e\uf9e1, \u5143, \u5b97, \u5c1a\u4e66, \u4f8d\u90ce, \u5fa1\u53f2, \u738b, \u4e2d\u4e66, \u8bcf, \u53f3, \u671d, \u5de6 Topic 25 \u2f8f\u884c\ufa08, \u601d, \u2f46\u65e0, \u6000, \u2f3c\u5fc3, \u60b2, \u2f63\u751f, \u54c0, \u8fdc, \u4f59, \u5fd7, \u6e38, \u4e16, \u5c06, \u5c14 Topic 26 \u2f2d\u5c71, \u6e38, \u2edb\u98ce, \u6e05, \u2f47\u65e5, \u5df2, \u671b, \u6797\uf9f4, \u5f52, \u6811, \u2f49\u6708, \u7a7a, \u521d, \u79cb, \u5f00 Topic 27 \u2ec4\u897f, \u5b98, \u62dc, \u4f4d, \u5f15, \u7235, \u524d, \u5347, \u518d\u62dc, \u8bbe, \u4e1c, \u4eea, \u9636, \u4e3b\u2f08\u4eba, \u2f74\u7acb Topic 28 \u670d, \u4e27, \u793c, \u2f57\u7236, \u54ed, \u2f24\u5927\u592b, \u2f20\u58eb, \u846c, \u51a0, \u2e9f\u6bcd, \u796d, \u2f29\u5c0f, \u2f46\u65e0, \u7956, \u8870 Topic 29 \u2ea0\u6c11, \u5929\u4e0b, \u6cbb, \u56fd, \u5211, \u2f46\u65e0, \u653f, \u6cd5, \u2f8f\u884c\ufa08, \u5584, \u738b, \u660e, \u4e71, \u4ee4, \u2f82\u81e3 Topic 30 \u672c, \u6309, \u5f15, \u7bc7, \u672c\u4f5c, \u6849, \u4f20, \u2f42\u6587, \u8bef, \u636e, \u6821, \u8865, \u6c49\u4e66, \u738b, \u5fa1 Topic 31 \u70ba, \u8ecd, \u570b, \u7121, \u65bc, \u2ed1\u2fa7\u9577, \u5247, \u8207, \u6642, \u6771, \u2f92\u898b\ufa0a, \u5f8c, \u4f86\uf92d, \u5c07, \u6703 Topic 32 \u2f82\u81e3, \u965b\u4e0b, \u5929\u4e0b, \u8bae, \u594f, \u2f46\u65e0, \u671d\u5ef7, \u8c0f, \u4ee5\u4e3a, \u6050, \u4e0d\uf967\u53ef, \u5723, \u5df2, \u7a83, \u4f0f Topic 33 \u5377, \u9001, \u5bc4, \u2f69\u767d, \u5f52, \u5c45\u6613\uf9e0, \u95f2, \u5e94, \u8fdc, \u9898, \u79cb, \u5ba2, \u2f46\u65e0, \u6625, \u5c3d Topic 34 \u796d, \u7940, \u5e99, \u793c, \u90ca, \u795e, \u914d, \u7960, \u575b, \u7956, \u4eab, \u5e1d, \u8bae, \u732e, \u5b97\nTopic 35 \u5e1d, \u5b98, \u5fa1\u53f2, \u5352, \u8bcf, \u8fc1, \u2ea0\u6c11, \u65f6, \u540f\uf9de, \u6cbb, \u53ec, \u8bae, \u594f, \u7f62, \u64e2 Topic 36 \u2f54\u6c34, \u4e1c, \u53bf, \u2f2d\u5c71, \u2ec4\u897f, \u5357, \u5317\uf963, \u6cb3, \u4e1c\u5357, \u4e1c\u5317\uf963, \u2ec4\u897f\u5357, \u2ec4\u897f\u5317\uf963, \u5f84, \u5e9c, \u5fd7 Topic 37 \u4f5b, \u5bfa, \u7ecf, \u50e7, \u65f6, \u738b, \u6cd5, \u2f46\u65e0, \u5df2, \u8eab, \u5584, \u83e9\u8428, \u540d, \u2f63\u751f, \u53d7 Topic 38 \u5c14, \u963f, \u660e, \u5e03, \u989d, \u54c8, \u547d, \u56fe, \u5587\uf90b, \u514b, \u7279, \u52d2\uf952, \u6388, \u9980, \u5580 Topic 39 \u5b98, \u2f82\u81e3, \u738b, \u90fd, \u5fa1\u53f2, \u5ef7, \u5fe0, \u5f20, \u65f6, \u674e\uf9e1, \u547d, \u5de1\u629a, \u5df2, \u5de6, \u671d Topic 40 \u9980, \u547d, \u5de1\u629a, \u603b\u7763, \u2f24\u5927\u2f82\u81e3, \u8bae, \u7f72, \u532a, \u7531, \u8c03, \u4e7e\u9686\uf9dc, \u594f, \u514d, \u5dde\u53bf, \u8bbe Topic 41 \u4e66, \u4f20, \u53f2, \u2f42\u6587, \u5fd7, \u6c49, \u8bba, \u53e4, \u6625\u79cb, \u7bc7, \u8bb0, \u7ecf, \u5e8f, \u7740, \u64b0 Topic 42 \u2f46\u65e0, \u5f52, \u2f8f\u884c\ufa08, \u5ba2, \u2f63\u751f, \u6b4c, \u2f69\u767d, \u7a7a, \u9001, \u65f6, \u5df2, \u8eab, \u5929, \u2f7c\u8001\uf934, \u8c01 Topic 43 \u5fb7, \u7687, \u795e, \u5723, \u5929, \u7075, \u732e, \u964d, \u4e50, \u8083, \u548c, \u5e1d, \u793c, \u662d, \u8f7d Topic 44 \u5175, \u5c06, \u9063, \u519b, \u57ce, \u653b, \u9a91, \u6218, \u51fb, \u9980, \u2f46\u65e0, \u6570, \u964d, \u65a9, \u56e0 Topic 45 \u94b1, \u2ea0\u6c11, \u2f65\u7530, \u5b98, \u76d0, \u5c81, \u6237, \u7a0e, \u2f6f\u77f3, \u7ed9, \u5dde, \u4ee4, \u8bcf, \u53f8, \u5f79 Topic 46 \u8c61, \u5409, \u2f46\u65e0, \u5366, \u9633, \u4e7e, \u6613\uf9e0, \u9634, \u5764, \u8d1e, \u6b63, \u548e, \u4f4d, \u7269, \u51f6 Topic 47 \u2f53\u6c14, \u75c5, \u8109, \u70ed, \u6cbb, \u5bd2, \u9634, \u2f8e\u8840, \u9633, \u865a, \u75db, \u2f55\u706b, \u90aa, \u8865, \u8bc1 Topic 48 \u5c06\u519b, \u664b, \u9b4f, \u5e1d, \u738b, \u5434, \u592a\u5b88, \u5c06, \u8700, \u523a\uf9ff\u53f2, \u90e1, \u4eae, \u53f8\u2ee2\u9a6c, \u8d25, \u5f81 Topic 49 \u4e5e, \u5143, \u53f8, \u5b89\u2f6f\u77f3, \u7199, \u671d\u5ef7, \u7f62, \u9664, \u7950, \u5b97, \u5b98, \u8fdb, \u5723, \u8bcf, \u5fa1\u53f2 Topic 50 \u2fa6\u91d1\uf90a, \u5dde, \u6539\u4f5c, \u2fa6\u91d1\uf90a\u2f08\u4eba, \u519b, \u5e9c, \u8bcf, \u5ba3, \u9063, \u5175, \u53f8, \u6dee, \u517c, \u5b97, \u738b Topic 51 \u5e1d, \u738b, \u7687, \u592a\u540e, \u5c01, \u592a\u2f26\u5b50, \u5b97, \u7687\u540e, \u2f74\u7acb, \u5b5d, \u5bab, \u7687\u5e1d, \u6bbf, \u8bcf, \u5983 Topic 52 \u2f46\u65e0, \u7269, \u9053, \u2f63\u751f, \u5f62, \u6210, \u80fd, \u5929\u4e0b, \u4e0d\uf967\u80fd, \u4e0d\uf967\u77e5, \u5fb7, \u5929\u5730, \u2fb3\u97f3, \u2f7d\u800c\u4e0d\uf967, \u540d Topic 53 \u6b7b, \u65f6, \u2e9f\u6bcd, \u2f25\u5973\uf981, \u5bb6, \u59bb, \u2f46\u65e0, \u2f57\u7236, \u56e0, \u5987, \u2f63\u751f, \u5f52, \u6740, \u2edd\u2fb7\u98df, \u8bed Topic 54 \u5377, \u7ecf, \u96c6, \u5f55, \u64b0, \u8bd1, \u672c, \u7eb8, \u53f3, \u25b3, \u9648\u2f52\u6c0f, \u2f00\u4e00\u767e, \u7b2c, \u7f16, \u8bba Topic 55 \u91ca, \u90d1, \u2f24\u5927\u592b, \u65bc, \u793c, \u5bbe, \u5c04, \u8bf8\u4faf, \u2f20\u58eb, \u738b, \u2fb3\u97f3, \u5929\u2f26\u5b50, \u796d, \u547d, \u53f8 Topic 56 \u2f3c\u5fc3, \u2f46\u65e0, \u6027, \u7406\uf9e4, \u7269, \u2f53\u6c14, \u5148\u2f63\u751f, \u5b66, \u6240\u8c13, \u5584, \u4f53, \u5929\u5730, \u53d1, \u52a8, \u9759 Topic 57 \u6b63\u4e49, \u738b, \u5468, \u2fb3\u97f3, \u4f20, \u547d, \u6bb7, \u4f2f, \u5fb7, \u59d3, \u2f42\u6587, \u8bd7, \u590f, \u821c, \u2f63\u751f Topic 58 \u5929\u4e0b, \u2f46\u65e0, \u53ef, \u4e0d\uf967\u80fd, \u56fa, \u4e0d\uf967\u53ef, \u4ee5\u4e3a, \u2ea0\u6c11, \u5175, \u80fd, \u8005\u4e5f, \u56fd, \u5df2, \u2f84\u81f3\u4e8e, \u2f7d\u800c\u4e0d\uf967 Topic 59 \u4e50, \u821e, \u58f0, \u2fce\u9f13, \u5f8b\uf9d8, \u6b4c, \u594f, \u5bab, \u949f, \u66f2, \u953a, \u5e94, \u2fb3\u97f3, \u5f26, \u8c03 Topic 60 \u5b98, \u53f8, \u6b63, \u7f6e, \u638c, \u5e9c, \u4ee4, \u4e1e, \u90ce, \u76d1, \u5458, \u7701, \u5de6, \u53f3, \u53f2 Topic 61 \u2f6f\u77f3, \u2f2d\u5c71, \u2fa5\u91cc\uf9e9, \u2ed4\u95e8, \u4f59, \u2ec4\u897f, \u5bfa, \u9662, \u8bb0, \u5357, \u6d1e\ufa05, \u4e1c, \u5ca9, \u5cf0, \u6865 Topic 62 \u5b98, \u4e3e, \u5b66\u2f20\u58eb, \u8fdb\u2f20\u58eb, \u5236, \u90ce, \u8003, \u53f8, \u8bcf, \u9009, \u79d1, \u5b66, \u2f20\u58eb, \u8865, \u2f42\u6587 Topic 63 \u56fd, \u738b, \u9063\u4f7f, \u2ec4\u897f, \u9063, \u90e8, \u8d21, \u5730, \u5317\uf963, \u2ee2\u9a6c, \u57ce, \u590f, \u2f74\u7acb, \u5357, \u5c45 Topic 64 \u4ee4, \u5b98, \u594f, \u5df2, \u53f8, \u6555, \u51c6, \u7f6a, \u5e94, \u672c, \u987b, \u4e0d\uf967\u5f97, \u5dee, \u5408, \u6cd5 Topic 65 \u670d, \u5347, \u94b1, \u6c64, \u4e38, \u6cbb, \u672b, \u5473, \u52a0, \u53f3, \u53d6, \u9152, \u2f62\u7518\u8349, \u714e, \u6bcf Topic 66 \u4e66, \u4f59, \u5df2, \u53ef, \u4e88, \u2f46\u65e0, \u5c11, \u65f6, \u4e0d\uf967\u80fd, \u591a, \u2f47\u65e5, \u610f, \u2f8f\u884c\ufa08, \u5c14, \u80fd Topic 67 \u5c06\u519b, \u738b, \u523a\uf9ff\u53f2, \u592a\u5b88, \u5dde, \u666f, \u4faf, \u5357, \u90e1, \u519b, \u9063, \u4e49, \u6881\uf97a, \u53c2\u519b, \u9886 Topic 68 \u82b1, \u6625, \u9189, \u2fb9\u9999, \u9152, \u2fac\u96e8, \u68a6, \u66f4\uf901, \u4f3c, \u2edb\u98ce, \u5c3d, \u95f2, \u2f7c\u8001\uf934, \u7ea2, \u2f29\u5c0f Topic 69 \u53bf, \u90e1, \u5dde, \u7f6e, \u57ce, \u5c5e, \u6c49, \u664b, \u521d, \u2f54\u6c34, \u6cbb, \u9633, \u5e9f, \u5510, \u6cb3 Topic 70 \u738b, \u79e6, \u4faf, \u8d75, \u695a, \u9b4f, \u2eec\u9f50, \u6c49, \u2f82\u81e3, \u5929\u4e0b, \u71d5, \u5175, \u2f74\u7acb, \u8bf8\u4faf, \u51fb Topic 71 \u5dde, \u8282\u5ea6, \u519b, \u90fd, \u738b, \u9547, \u523a\uf9ff\u53f2, \u664b, \u5f66, \u5951\u4e39\uf95e, \u5168, \u592a\u7956, \u5510, \u6307\u6325, \u5ef6 Topic 72 \u795e, \u771f, \u4ed9, \u2f5e\u7384, \u7cbe, \u7ecf, \u2f53\u6c14, \u9053, \u7075, \u2f5f\u7389, \u4e39\uf95e, \u2fa6\u91d1\uf90a, \u5929, \u2f63\u751f, \u5bab Topic 73 \u5c06\u519b, \u523a\uf9ff\u53f2, \u5dde, \u9b4f, \u738b, \u5c1a\u4e66, \u8bcf, \u7235, \u9664, \u90fd\u7763, \u6b66, \u5143, \u4faf, \u4eea, \u2eec\u9f50 Topic 74 \u2f47\u65e5, \u5ea6, \u5206, \u4f59, \u5386, \u2f49\u6708, \u5dee, \u6cd5, \u6714, \u51cf, \u5b9a, \u2f8f\u884c\ufa08, \u52a0, \u534a, \u6c42 Topic 75 \u8bd7, \u7eaa, \u7c7b\u805a, \u89c8, \u5fa1, \u5f15, \u2f42\u6587\u9009, \u2f42\u6587\u82d1, \u4e50\u5e9c, \u96c6, \u672c, \u5b66\u8bb0, \u521d, \u827a\u2f42\u6587, \u2f17\u5341\u2f0b\u516b Topic 76 \u2f5f\u7389, \u82b1, \u6b4c, \u66f2, \u2fa6\u91d1\uf90a, \u6625, \u2fb9\u9999, \u2edc\u98de, \u2edb\u98ce, \u7f57, \u2f49\u6708, \u591c, \u8f7b, \u6811, \u6101 Topic 77 \u94ed, \u592b\u2f08\u4eba, \u8bb3, \u5e74\uf98e, \u5dde, \u7f3a, \u7891\ufa4b, \u5fd7, \u2f57\u7236, \u5fb7, \u2f42\u6587, \u2f63\u751f, \u846c, \u5e9c, \u4e16 Topic 78 \u2f63\u751f, \u2f4a\u6728, \u2fac\u96e8, \u2f54\u6c34, \u2f55\u706b, \u65f6, \u2edd\u2fb7\u98df, \u2f69\u767d, \u2f53\u6c14, \u2f1f\u571f, \u2ea0\u6c11, \u65f1, \u2f47\u65e5, \u2f9a\u8d64, \u9634 Topic 79 \u5175, \u519b, \u8425, \u654c, \u65d7, \u2ee2\u9a6c, \u5c06, \u9635, \u603b, \u6218, \u4ee4, \u2ecb\u8f66, \u5907, \u8d3c, \u961f"}, {"heading": "100 topic model, showing 15 highest probability words in each topic", "text": "Topic 0 \u672c, \u6309, \u5f15, \u7bc7, \u4f20, \u6849, \u672c\u4f5c, \u6c49\u4e66, \u8bef, \u53f2\u8bb0, \u2f42\u6587, \u5fa1, \u636e, \u6821, \u4e66 Topic 1 \u6625, \u82b1, \u2fb9\u9999, \u9189, \u6101, \u7ea2, \u68a6, \u2edb\u98ce, \u6e05, \u66f4\uf901, \u70df, \u2f5f\u7389, \u8c01, \u5c3d, \u524d Topic 2 \u664b, \u5e1d, \u4f20, \u9b4f, \u6c49, \u5fd7, \u4ee4, \u6b66\u5e1d, \u738b, \u8bcf, \u59cb, \u4e66, \u5434, \u5143, \u2f42\u6587\u5e1d Topic 3 \u5dde, \u5e9c, \u86ee, \u53f8, \u6c5f, \u536b, \u7f6e, \u2f1f\u571f, \u5c5e, \u5730, \u5143, \u5be8, \u6539, \u5ddd, \u5b98\u53f8 Topic 4 \u5929\u4e0b, \u2f26\u5b50\u2f48\u66f0, \u2ea0\u6c11, \u5b54\u2f26\u5b50, \u5b5f\u2f26\u5b50, \u6cbb, \u8005\u4e5f, \u5584, \u4ec1, \u5b54, \u4e0d\uf967\u80fd, \u738b, \u2f7d\u800c\u4e0d\uf967, \u7ae0, \u4e49 Topic 5 \u70ba, \u8b02, \u672c, \u7f6a, \u6e1b, \u5b98, \u8b70, \u5f9e, \u5f92, \u52a0, \u9322, \u6bba, \u7121, \u8af8, \u61c9 Topic 6 \u4e50, \u821e, \u58f0, \u2fce\u9f13, \u5f8b\uf9d8, \u594f, \u6b4c, \u5bab, \u949f, \u66f2, \u953a, \u5f26, \u2fb3\u97f3, \u8c03, \u2f7b\u7fbd Topic 7 \u56fd, \u738b, \u9063\u4f7f, \u9063, \u2ec4\u897f, \u90e8, \u8d21, \u5317\uf963, \u5730, \u5357, \u2f74\u7acb, \u57ce, \u4e1c, \u53ef\u6c57, \u5c45 Topic 8 \u5dde, \u53bf, \u90e1, \u7f6e, \u5c5e, \u9886, \u5e9f, \u6cbb, \u6c49, \u6237, \u6539, \u5e9c, \u57ce, \u521d, \u9633 Topic 9 \u2fb3\u97f3, \u5207, \u2f4a\u6728, \u2ee5\u9c7c, \u2edd\u2fb7\u98df, \u2f63\u751f, \u2f46\u65e0, \u5b9e, \u2f54\u6c34, \u2f69\u767d, , \u591a, \u540d, \u4f3c, \u8c13\u4e4b Topic 10 \u65f6, \u8eab, \u2f63\u751f, \u2f46\u65e0, \u5df2, \u4ee4, \u2f3c\u5fc3, \u987b, \u2edd\u2fb7\u98df, \u5e38, \u591a, \u4e0d\uf967\u5f97, \u8bed, \u80fd, \u5ff5 Topic 11 \u5175, \u519b, \u5c06, \u654c, \u6218, \u80dc, \u8d3c, \u5907, \u53ef, \u8ba1, \u5352, \u5229\uf9dd, \u653b, \u5b88, \u864f Topic 12 \u8d4b, \u8bd7\u2f48\u66f0, \u6c49\u4e66, \u2f51\u6bdb, \u2f26\u5b50\u2f48\u66f0, \u5584, \u4f20, \u5207, \u8bd7, \u5de6, \u8c8c, \u5c1a\u4e66, \u695a\u8f9e, \u8bf4\u2f42\u6587, \u793c\u8bb0 Topic 13 \u6b4c, \u2f8f\u884c\ufa08, \u2f46\u65e0, \u5f52, \u7a7a, \u80e1, \u9001, \u60b2, \u5929, \u8f9e, \u2f69\u767d, \u2f63\u751f, \u8c01, \u8d60, \u6101 Topic 14 \u5e08, \u50e7, \u5982\u4f55, \u9053, \u7985\u5e08, \u5904, \u2f46\u65e0, \u548c\u5c1a, \u2f2d\u5c71, \u4e0a\u5802, \u2f63\u751f, \u65f6, \u4e3e, \u5374, \u4f5b Topic 15 \u5c06\u519b, \u523a\uf9ff\u53f2, \u738b, \u592a\u5b88, \u5dde, \u5357, \u666f, \u519b, \u9063, \u4e49, \u90e1, \u2fbc\u9ad8\u7956, \u9886, \u53c2\u519b, \u4faf Topic 16 \u5b8b, \u547d, \u519b, \u8def\uf937, \u90fd, \u8d50, \u2f8f\u884c\ufa08\u7701, \u524c, \u4e2d\u4e66, \u9c81, \u963f, \u653f\u4e8b, \u2fa5\u91cc\uf9e9, \u5e9c, \u2fa6\u91d1\uf90a Topic 17 \u8fc1, \u5352, \u5b98, \u8fdb\u2f20\u58eb, \u8bcf, \u65f6, \u6388, \u521d, \u6539, \u4e3e, \u64e2, \u5c1a\u4e66, \u53ec, \u5e74\uf98e, \u8fdb Topic 18 \u664b, \u4f2f, \u695a, \u2eec\u9f50, \u90d1, \u53d4, \u4f10, \u5e08, \u4faf, \u738b, \u9c81, \u536b, \u5b63, \u6b63\u4e49, \u56fd Topic 19 \u795e, \u7687, \u5fb7, \u5723, \u4e50, \u732e, \u6b4c, \u5929, \u7075, \u594f, \u5bab, \u793c, \u548c, \u8083, \u662d Topic 20 \u670d, \u5347, \u94b1, \u6c64, \u4e38, \u672b, \u6cbb, \u5473, \u52a0, \u53f3, \u9152, \u53d6, \u2f63\u751f, \u2f62\u7518\u8349, \u6bcf Topic 21 \u2ea0\u6c11, \u2f65\u7530, \u4ee4, \u540f\uf9de, \u2f46\u65e0, \u6cbb, \u5b98, \u591a, \u56fd, \u5c81, \u5f79, \u8d4b, \u519c, \u5730, \u5929\u4e0b Topic 22 \u5929\u4e0b, \u2f46\u65e0, \u56fa, \u53ef, \u4e0d\uf967\u80fd, \u2ea0\u6c11, \u4e0d\uf967\u53ef, \u56fd, \u5df2, \u8005\u4e5f, \u4ea1, \u6076, \u4e71, \u6743, \u5929\u2f26\u5b50 Topic 23 \u82b1, \u2f5f\u7389, \u6625, \u591c, \u66f2, \u79cb, \u2edb\u98ce, \u2fa6\u91d1\uf90a, \u2f49\u6708, \u5f00, \u6811, \u2f54\u6c34, \u5bf9, \u53f6, \u2edc\u98de Topic 24 \u5bbe, \u65bc, \u4e3b\u2f08\u4eba, \u7235, \u2f2b\u5c38, \u2f24\u5927\u592b, \u2ec4\u897f, \u62dc, \u793c, \u91ca, \u5c04, \u9636, \u5347, \u796d, \u4e1c Topic 25 \u6715, \u53ef, \u6555, \u8d50, \u537f, \u5c14, \u5b98, \u5236, \u5b88, \u5177, \u8282, \u6388, \u5143, \u8bcf, \u4ffe Topic 26 \u5211, \u7f6a, \u72f1, \u6cd5, \u76d7, \u8d66, \u4ee4, \u540f\uf9de, \u5f8b\uf9d8, \u6756, \u5b98, \u6740, \u72af, \u65ad, \u6b7b Topic 27 \u5982\u6b64, \u7b87, \u5982\u4f55, \u5e95, \u5904, \u987b, \u4ed6, \u5374, \u770b, \u505a, \u4ec1, \u9053\u7406\uf9e4, \u90fd, \u65f6, \u7406\uf9e4\u4f1a Topic 28 \u8282\u5ea6, \u5dde, \u519b, \u5175, \u8d3c, \u8bcf, \u5c06, \u5b97, \u5149, \u5fe0, \u674e\uf9e1, \u5168, \u738b, \u6000, \u6b66 Topic 29 \u6b7b, \u2e9f\u6bcd, \u5bb6, \u65f6, \u2f57\u7236, \u2f25\u5973\uf981, \u59bb, \u56e0, \u5f52, \u5e74\uf98e, \u5352, \u2f46\u65e0, \u2ed4\u95e8, \u6570, \u2f47\u65e5 Topic 30 \u5c14, \u660e, \u5e03, \u963f, \u54c8, \u989d, \u56fe, \u514b, \u5587\uf90b, \u5175, \u7279, \u52d2\uf952, \u547d, \u6388, \u5e08 Topic 31 \u2f46\u65e0, \u2f8f\u884c\ufa08, \u5c06, \u9053, \u793c, \u53ef, \u8f9e, \u80fd, \u4e0d\uf967\u80fd, \u5584, \u5fb7, \u2f42\u6587, \u4fe1, \u4ee5\u4e3a, \u4e0d\uf967\u53ef Topic 32 \u2f46\u65e0, \u5fb7, \u547d, \u5b9e, \u5eb6, \u5929, \u5df2, \u738b, \u7687, \u793c, \u660e, \u6000, \u5178, \u620e, \u56fe Topic 33 \u2f82\u81e3, \u5e1d, \u965b\u4e0b, \u594f, \u8bae, \u7f62, \u8bba, \u8c0f, \u2f46\u65e0, \u671d\u5ef7, \u5929\u4e0b, \u65f6, \u53ec, \u2f24\u5927\u2f82\u81e3, \u56e0 Topic 34 \u5377, \u96c6, \u64b0, \u5f55, \u9648\u2f52\u6c0f, \u5fd7, \u672c, \u8bb0, \u2f00\u4e00\u767e, \u25b3, \u7f16, \u5510, \u7ecf, \u56fe, \u53f3 Topic 35 \u2f82\u81e3, \u4e5e, \u53f8, \u5b98, \u8bcf, \u594f, \u671d\u5ef7, \u5b89\u2f6f\u77f3, \u8def\uf937, \u5143, \u5df2, \u4ee4, \u5dee, \u2f8f\u884c\ufa08, \u9664 Topic 36 \u5dde, \u523a\uf9ff\u53f2, \u5fa1\u53f2, \u4f8d\u90ce, \u5143, \u8282\u5ea6, \u674e\uf9e1, \u4e2d\u4e66, \u5c1a\u4e66, \u2f24\u5927\u592b, \u5b97, \u5e73\u7ae0, \u5236, \u5d14, \u53f3 Topic 37 \u2f6f\u77f3, \u2f2d\u5c71, \u4f59, \u2ec4\u897f, \u5bfa, \u2ed4\u95e8, \u5ca9, \u5cf0, \u6d1e\ufa05, \u6865, \u4ead, \u5357, \u2fa5\u91cc\uf9e9, \u2f88\u821f, \u2f54\u6c34 Topic 38 \u738b, \u5e1d, \u5c01, \u7687, \u592a\u540e, \u5b97, \u592a\u2f26\u5b50, \u2f74\u7acb, \u5b5d, \u7687\u540e, \u5983, \u85a8, \u55e3, \u5bab, \u8c25 Topic 39 \u2fac\u96e8, \u5dde, \u65f1, \u5730\u9707, \u2ea0\u6c11, \u9965, \u2f24\u5927\u2f54\u6c34, \u96f9, \u2f55\u706b, \u2f4a\u6728, \u574f, \u79cb, \u2f63\u751f, \u8757, \u4e03\u5e74\uf98e Topic 40 \u2f82\u81e3, \u4f0f, \u5949, \u965b\u4e0b, \u8c28, \u5723, \u2f46\u65e0, \u8868, \u6069, \u4efb, \u8c22, \u8d50, \u542f, \u8499, \u72b6 Topic 41 \u4e66, \u4f20, \u2f42\u6587, \u5b66, \u53f2, \u7ecf, \u6625\u79cb, \u7bc7, \u4e49, \u7740, \u5112, \u6613\uf9e0, \u5e8f, \u6c49, \u8bba Topic 42 \u2f54\u6c34, \u4e1c, \u53bf, \u5317\uf963, \u5f84, \u5357, \u2f2d\u5c71, \u2ec4\u897f, \u57ce, \u9633, \u6c49, \u6cb3, \u4e1c\u5317\uf963, \u8c13\u4e4b, \u4e1c\u5357 Topic 43 \u5c06\u519b, \u523a\uf9ff\u53f2, \u5dde, \u9b4f, \u738b, \u8bcf, \u9664, \u7235, \u5c1a\u4e66, \u5143, \u90fd\u7763, \u6563, \u8d60, \u9a91, \u8282\nTopic 44 \u771f, \u795e, \u4ed9, \u2f5e\u7384, \u7ecf, \u4e39\uf95e, \u7075, \u2f5f\u7389, \u7cbe, \u9053, \u2f53\u6c14, \u5143, \u5bab, \u2fa6\u91d1\uf90a, \u771f\u2f08\u4eba Topic 45 \u8bcf, \u5dde, \u6bbf, \u5b66\u2f20\u58eb, \u9662, \u8d50, \u547d, \u5236, \u671d, \u5b98, \u67a2, \u8fbd, \u5b97, \u9601, \u2f82\u81e3 Topic 46 \u670d, \u4e27, \u2f57\u7236, \u793c, \u54ed, \u846c, \u2e9f\u6bcd, \u2f24\u5927\u592b, \u51a0, \u2f20\u58eb, \u7956, \u2f57\u7236\u2e9f\u6bcd, \u2f29\u5c0f, \u8870, \u796d Topic 47 \u4faf, \u6c49, \u5c06\u519b, \u738b, \u5308\u5974, \u65f6, \u540f\uf9de, \u4e1e\u76f8, \u5c01, \u2f24\u5927\u592b, \u90e1, \u4ee4, \u5b5d, \u5355\u4e8e, \u83bd Topic 48 \u90d1, \u91ca, \u8bf8\u4faf, \u2fb3\u97f3, \u738b, \u5929\u2f26\u5b50, \u2f24\u5927\u592b, \u793c, \u638c, \u671d, \u2f20\u58eb, \u6849, \u53f8, \u6b63\u4e49, \u537f Topic 49 \u53bf, \u5dde, \u7f6e, \u57ce, \u521d, \u5fd7, \u90e1, \u5c5e, \u5e9c, \u5b8b, \u2ec4\u897f, \u5510, \u5e9f, \u2f2d\u5c71, \u6cbb Topic 50 \u94ed, \u5dde, \u592b\u2f08\u4eba, \u8bb3, \u5e9c, \u7f3a, \u2f63\u751f, \u5e74\uf98e, \u5fb7, \u5fd7, \u7891\ufa4b, \u2f42\u6587, \u5148, \u2f57\u7236, \u90e1 Topic 51 \u75c5, \u2f53\u6c14, \u8109, \u70ed, \u6cbb, \u5bd2, \u9634, \u2f8e\u8840, \u9633, \u865a, \u75db, \u2f55\u706b, \u90aa, \u8865, \u8bc1 Topic 52 \u5e1d, \u5c06\u519b, \u90e1, \u4faf, \u592a\u5b88, \u6c49, \u62dc, \u5c01, \u5c06, \u540f\uf9de, \u4ee4, \u4e66, \u65f6, \u521d, \u5f81 Topic 53 \u5377, \u9001, \u5f52, \u5bc4, \u2f69\u767d, \u2f2d\u5c71, \u5c45\u6613\uf9e0, \u8fdc, \u9898, \u522b, \u5e94, \u95f2, \u541f, \u5ba2, \u2f46\u65e0 Topic 54 \u738b, \u79e6, \u695a, \u2eec\u9f50, \u8d75, \u9b4f, \u2f82\u81e3, \u71d5, \u97e9, \u5929\u4e0b, \u4faf, \u5434, \u5175, \u653b, \u2f74\u7acb Topic 55 \u5b98, \u2f82\u81e3, \u5fa1\u53f2, \u5e1d, \u738b, \u90fd, \u65f6, \u5ef7, \u547d, \u5fe0, \u671d, \u674e\uf9e1, \u5f20, \u2f42\u6587, \u5c1a\u4e66 Topic 56 \u5c1a\u4e66, \u8fc1, \u65f6, \u90ce, \u4ee4, \u521d, \u8bae, \u62dc, \u537f, \u8f6c, \u5b98, \u5c11, \u9664, \u592a\u2f26\u5b50, \u2f24\u5927\u592b Topic 57 \u6cb3, \u2f54\u6c34, \u51b3, \u6e56, \u6dee, \u5824, \u5f00, \u2f1d\u53e3, \u7b51, \u95f8, \u6d5a, \u5830, \u8fd0, \u4e1c, \u901a Topic 58 \u2f3c\u5fc3, \u2f46\u65e0, \u5148\u2f63\u751f, \u6027, \u5b66, \u5584, \u7406\uf9e4, \u7269, \u6240\u8c13, \u6076, \u53d1, \u4f53, \u2f53\u6c14, \u81f4, \u52a8 Topic 59 \u672c, \u636e, \u8bcf, \u5b8b, \u6539, \u5dde, \u5377, \u9980, \u5b98, \u53f8, \u9601, \u89fd, \u4ee4, \u4e66, \u539f Topic 60 \u2f6f\u77f3, \u2ee2\u9a6c, \u2f69\u767d, \u2ef0\u9f99, \u2f2d\u5c71, \u5730, \u2f63\u751f, \u65f6, \u2fa6\u91d1\uf90a, \u5929, \u2f4a\u6728, \u2ee6\u9e1f, \u2f55\u706b, \u2ee5\u9c7c, \u8c61 Topic 61 \u796d, \u7940, \u5e99, \u793c, \u90ca, \u795e, \u914d, \u7956, \u7960, \u4eab, \u575b, \u5b97, \u732e, \u8bae, \u5e1d Topic 62 \u8d3c, \u8425, \u5175, \u603b\u5175, \u9547, \u519b, \u5bc7, \u527f, \u9980, \u514b, \u9677, \u7832, \u532a, \u64e2, \u56fd Topic 63 \u547d, \u2f24\u5927\u2f82\u81e3, \u5de1\u629a, \u603b\u7763, \u594f, \u8bae, \u5b66\u2f20\u58eb, \u8c15, \u4e7e\u9686\uf9dc, \u7f72, \u7531, \u6388, \u5dde\u53bf, \u76f4, \u8c03 Topic 64 \u5dde, \u8282\u5ea6, \u519b, \u90fd, \u5e1d, \u664b, \u5951\u4e39\uf95e, \u5f66, \u592a\u7956, \u9547, \u738b, \u523a\uf9ff\u53f2, \u5510, \u5ef6, \u6307\u6325 Topic 65 \u6539\u4f5c, \u2fa6\u91d1\uf90a\u2f08\u4eba, \u2fa6\u91d1\uf90a, \u5dde, \u519b, \u5e9c, \u864f, \u9063, \u5175, \u5ba3, \u7edf\u5236, \u654c, \u2f8f\u884c\ufa08, \u53f8, \u738b Topic 66 \u9053, \u7406\uf9e4, \u4e16, \u4fd7, \u4f53, \u53ef, \u5b9e, \u5fb7, \u8bba, \u83ab, \u5f62, \u60c5, \u96be, \u8fdc, \u6b8a Topic 67 \u738b, \u547d, \u2ea0\u6c11, \u5468, \u5fb7, \u4f20, \u821c, \u6bb7, \u5c27, \u5929, \u79b9, \u5e1d, \u6b63\u4e49, \u5468\u516c, \u2f42\u6587\u738b Topic 68 \u2f47\u65e5, \u5ea6, \u5206, \u4f59, \u5386, \u5dee, \u6cd5, \u6714, \u51cf, \u2f49\u6708, \u5b9a, \u52a0, \u2f8f\u884c\ufa08, \u534a, \u6c42 Topic 69 \u4e1c, \u2f2d\u5c71, \u5357, \u2ec4\u897f, \u5317\uf963, \u2f54\u6c34, \u53bf, \u5e9c, \u6eaa, \u4e1c\u5357, \u6c5f, \u2ec4\u897f\u5357, \u6cb3, \u4e1c\u5317\uf963, \u5408 Topic 70 \u670d, \u2fa6\u91d1\uf90a, \u2f90\u8863, \u2ecb\u8f66, \u51a0, \u65d7, \u5e26, \u5236, \u6b21, \u2ed8\u9752, \u8f82, \u9970, \u9a7e, \u2ee9\u9ec4, \u2f5f\u7389 Topic 71 \u70ba, \u65bc, \u7121, \u5247, \u570b, \u66f8, \u5f8c, \u79ae, \u8207, \u6642, \u6771, \u2ed1\u2fa7\u9577, \u5c07, \u8ecd, \u2f92\u898b\ufa0a Topic 72 \u5dde, \u5e1d, \u523a\uf9ff\u53f2, \u2eec\u9f50, \u6b66, \u4faf, \u4eea, \u5c01, \u6881\uf97a, \u666f, \u5468, \u9b4f, \u5b5d, \u5f00, \u5e9c Topic 73 \u8bd7, \u7eaa, \u7c7b\u805a, \u89c8, \u5fa1, \u5f15, \u2f42\u6587\u9009, \u2f42\u6587\u82d1, \u4e50\u5e9c, \u672c, \u96c6, \u5b66\u8bb0, \u521d, \u2f17\u5341\u4e5d, \u2f17\u5341\u2f0b\u516b Topic 74 \u65d7, \u603b, \u8425, \u961f, \u57ce, \u4ee4, \u5c3a, \u2ecb\u8f66, \u4e08, \u2299, \u6bcf, \u4ff1, \u67aa, The, \u6b65 Topic 75 \u5b98, \u5f15, \u6bbf, \u7687\u5e1d, \u4eea, \u4f4d, \u524d, \u518d\u62dc, \u2ec4\u897f, \u6b21, \u5949, \u8bbe, \u8be3, \u2f74\u7acb, \u8dea Topic 76 \u8bcf, \u6714, \u5c1a\u4e66, \u738b, \u590f, \u5dde, \u6625, \u79cb, \u7687, \u51ac, \u5e78, \u9063\u4f7f, \u7f62, \u7532\u2f26\u5b50, \u5bab Topic 77 \u6e38, \u6e05, \u2f46\u65e0, \u671b, \u2f47\u65e5, \u65f6, \u6000, \u2f2d\u5c71, \u601d, \u2edb\u98ce, \u8fdc, \u2fbc\u9ad8, \u60b2, \u6d41, \u2f3c\u5fc3 Topic 78 \u2f46\u65e0, \u2f7c\u8001\uf934, \u9152, \u5df2, \u5ba2, \u2f47\u65e5, \u5f52, \u79cb, \u65f6, \u2fac\u96e8, \u2f2d\u5c71, \u521d, \u95f2, \u591c, \u4e66 Topic 79 \u4ee5\u4e3a, \u2f46\u65e0, \u4e0d\uf967\u80fd, \u4e0d\uf967\u53ef, \u53ef, \u5929\u4e0b, \u4e0d\uf967\u77e5, \u2f84\u81f3\u4e8e, \u6240\u8c13, \u53e4, \u53d6, \u5c3d, \u5df2, \u56fa, \u4e66 Topic 80 \u2f42\u6587, \u2fa6\u91d1\uf90a, \u7f6e, \u7ecd\u5174, \u94b1, \u5b97, \u9662, \u7199, \u5d07, \u8bcf, \u517c, \u65e7, \u5143, \u5b81, \u5dde Topic 81 \u661f, \u72af, \u2f49\u6708, \u2f47\u65e5, \u5360, \u592a\u2f69\u767d, \u5929, \u5c81, \u8367\u60d1, \u2f55\u706b, \u2fa0\u8fb0\uf971, \u5b98, \u5175, \u2f53\u6c14, \u8d35 Topic 82 \u2f46\u65e0, \u7269, \u2fb3\u97f3, \u6210, \u5f62, \u4e0d\uf967\u77e5, \u9053, \u7bc7, \u5929\u4e0b, \u5ba3, \u53f8, \u540d, \u6e38, \u90aa, \u5fb7 Topic 83 \u94b1, \u76d0, \u5b98, \u6237, \u2f6f\u77f3, \u7ed9, \u5c81, \u53f8, \u7a0e, \u2f76\u7c73, \u5dde, \u8bcf, \u8d2f, \u7eb3, \u94f8 Topic 84 \u4e66, \u753b, \u56fe, \u7b14, \u7eb8, \u5e16, \u738b, \u5f1f, \u2f2f\u5de5, \u8349, \u4f59, \u5584, \u58a8\ufa3a, \u6cd5, \u5199 Topic 85 \u5b98, \u594f, \u4ee4, \u6555, \u5df2, \u51c6, \u53f8, \u4e3e, \u672c, \u4efb, \u5e94, \u5dde, \u987b, \u4e0d\uf967\u5f97, \u59d4 Topic 86 \u2f63\u751f, \u5929, \u2f46\u65e0, \u7269, \u5929\u5730, \u9053, \u2f53\u6c14, \u660e, \u5730, \u5f62, \u4e07\u7269, \u5723\u2f08\u4eba, \u795e, \u80fd, \u6210 Topic 87 \u4e66, \u2eec\u9f50, \u89e3\u4e91, \u4f20, \u4faf, \u79f0, \u4f10, \u662f\u4e5f, \u4f2f, \u5352, \u2f24\u5927\u592b, \u8bf8\u4faf, \u590f, \u2f42\u6587, \u846c Topic 88 \u5175, \u8d3c, \u9063, \u57ce, \u653b, \u5c06, \u519b, \u964d, \u7834, \u6218, \u8fdb, \u5b88, \u51fb, \u65a9, \u7387 Topic 89 \u9619, \u5143, \u20ac, \u9980, \u5c06, \u56e0, \u8d4b, \u2f06\u4e8c\u5b57, \u89c2, \u5b8f, \u2f46\u65e0, \u2fa6\u91d1\uf90a, \u2f5f\u7389, \u65f6, \u5c45\nTopic 90 \u5c06\u519b, \u738b, \u9063, \u5c06, \u523a\uf9ff\u53f2, \u5e1d, \u592a\u5b88, \u9b4f, \u9547, \u79e6, \u664b, \u6155\u5bb9, \u8ba8, \u575a, \u2f5e\u7384 Topic 91 \u2f82\u81e3, \u5929\u4e0b, \u965b\u4e0b, \u2f46\u65e0, \u56fd, \u660e, \u4ee5\u4e3a, \u5931, \u4e71, \u8d24, \u4e0d\uf967\u53ef, \u2f8f\u884c\ufa08, \u5b89, \u4e0d\uf967\u80fd, \u8c0f Topic 92 \u5dde, \u519b, \u8fb9, \u5175, \u590f, \u8def\uf937, \u90fd, \u6cb3, \u5be8, \u8543, \u2ec4\u897f, \u5e86, \u2ee2\u9a6c, \u4fdd, \u6307\u6325 Topic 93 \u5dde, \u5e1d, \u968b, \u592a\u5b97, \u603b\u7ba1, \u4ee4, \u5c06\u519b, \u7a81\u53a5, \u738b, \u5c01, \u523a\uf9ff\u53f2, \u2fbc\u9ad8\u7956, \u5de6, \u592a\u2f26\u5b50, \u2f5e\u7384 Topic 94 \u8c61, \u5366, \u5409, \u2f46\u65e0, \u4e7e, \u8d1e, \u5764, \u548e, \u9633, \u6613\uf9e0, \u6b63, \u4f4d, \u9634, \u521a, \u5229\uf9dd Topic 95 \u5b98, \u7f6e, \u638c, \u4ee4, \u4e1e, \u5e9c, \u53f8, \u90ce, \u6b63, \u76d1, \u53f3, \u5de6, \u5c06\u519b, \u536b, \u66f9 Topic 96 \u7ecf, \u4f5b, \u5bfa, \u50e7, \u6cd5, \u8bd1, \u83e9\u8428, \u672c, \u6c99\u2ed4\u95e8, \u738b, \u91ca, \u540d, \u5854, \u5f55, \u90e8 Topic 97 \u8bd7, \u4f59, \u4e88, \u2f42\u6587, \u65f6, \u8bed, \u53e5\uf906, \u5143, \u674e\uf9e1, \u4e66, \u5510, \u96c6, \u8bcd, \u540d, \u8bd7\u4e91 Topic 98 \u8ecd, \u7532, \u4e19, \u570b, \u90e8, \u6703, \u653f\u5e9c, \u7f8e, \u2f47\u65e5, \u5230, \u620a, \u5171, \u2f00\u4e00\u2f06\u4e8c, \u54e1, \u4e0a\u6d77\ufa45 Topic 99 \u53f8, \u5b98, \u6b63, \u5458, \u7701, \u7f6e, \u90fd, \u76d1, \u5185, \u638c, \u526f, \u90ce, \u9662, \u9886, \u804c\nAppendix 6: Distance measures\nOne of the strengths of LDA topic modeling lies in the fact that documents and topics are represented within the same probability space, making it possible to apply standard information theoretic measures to compute document-document, document-topic, and topic-topic similarities.\nJensen-Shannon Distance (JSD) (Endres & Schindelin 2003) is a widely used metric from information theory which measures the difference in bits between two probability distributions.\nFor this project, we computed the similarity between two documents as 1-JSD(P,Q), where JSD(P,Q) is the Jensen-Shannon Distance between the probability distributions corresponding to the topic mixtures P and Q of the documents.\nJensen-Shannon Distance is defined as the square root of the Jensen-Shannon divergence between the distributions P and Q.\nJensen-Shannon divergence in turn is based upon averaging the Kullback-Leibler (KL) divergences between the distributions (Kullback & Leibler 1951).\nInformally, KL measures the degree to which observations drawn from Q are unexpected when the expected values are given by P. KL is not symmetric: what is surprising about Q from the perspective of P needs not be the same as what is surprising about P from the perspective of Q.\nPrecise definitions of these measures can be found in the works cited and at https:// en.wikipedia.org/wiki/Kullback\u2013Leibler_divergence and https://en.wikipedia.org/wiki/Jensen\u2013 Shannon_divergence.\nFor topic-mediated queries we first compute word-topic similarity by creating a \u201cpseudodocument\u201d which distributes the available probability across the query terms. (In the case of the example in the main text of a query using the single term for yinyang, this means that the probability assigned to that term is 1, and all other terms are assigned 0.) This pseudo topic is then used to compute similarities to the model\u2019s topics using the JSD-based similarity function described above. A new topic distribution vector, weighted according to the pseudo-document\u2019s similarity to each of the topic-word distributions, is used to compute the nearest actual documents using the JSD-based similarity measure."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Ancient Chinese texts present an area of enormous challenge and opportunity for humanities scholars interested in exploiting computational methods to assist in the development of new insights and interpretations of culturally significant materials. In this paper we describe a collaborative effort between Indiana University and Xi\u2019an Jiaotong University to support exploration and interpretation of a digital corpus of over 18,000 ancient Chinese documents, which we refer to as the \u201cHandian\u201d ancient classics corpus (\u6c49\u5178\u53e4\u7c4d or H\u00e0n di\u0103n g\u016d j\u00ed, i.e, the \u201cHan canon\u201d or \u201cChinese classics\u201d). It contains classics of ancient Chinese philosophy, documents of historical and biographical significance, and literary works. We begin by describing the Digital Humanities context of this joint project, and the advances in humanities computing that made this project feasible. We describe the corpus and introduce our application of probabilistic topic modeling to this corpus, with attention to the particular challenges posed by modeling ancient Chinese documents. We give a specific example of how the software we have developed can be used to aid discovery and interpretation of themes in the corpus. We outline more advanced forms of computer-aided interpretation that are also made possible by the programming interface provided by our system, and the general implications of these methods for understanding the nature of meaning in these texts.", "creator": "Adobe Acrobat Pro DC 15.23.20056"}}}