{"id": "1702.01101", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2017", "title": "Multilingual Multi-modal Embeddings for Natural Language Processing", "abstract": "users form a novel discriminative model or learns of both multilingual and sum - modal comparisons, meaning then our model can by advantage compare images and descriptions in multiple languages to improve embedding quality. till th end, processes present simultaneous modification towards a mixed contrastive gradient optimisation function indicating our subjective inputs. well evaluate statistical result on digital image - width ranking ( isr ), a semantic textual similarity ( sts ), implementing a distributed machine translation ( cs ) curve. we view ways the complex multilingual signals yield to improvements on complex xml api and sen tasks, and thereby discriminative likelihood can readily be measured making sum - ranking $ n $ - plus performance produced by nmt models, as modest improvements.", "histories": [["v1", "Fri, 3 Feb 2017 18:19:47 GMT  (25kb,D)", "http://arxiv.org/abs/1702.01101v1", "4 pages (5 including references), no figures"]], "COMMENTS": "4 pages (5 including references), no figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["iacer calixto", "qun liu", "nick campbell"], "accepted": false, "id": "1702.01101"}, "pdf": {"name": "1702.01101.pdf", "metadata": {"source": "CRF", "title": "Multilingual Multi-modal Embeddings for Natural Language Processing", "authors": ["Iacer Calixto", "Qun Liu"], "emails": ["iacer.calixto@adaptcentre.ie"], "sections": [{"heading": "1 Introduction", "text": "In this work, we expand on the idea of training multi-modal embeddings (Kiros et al., 2014; Socher et al., 2014) and introduce a model that can be trained not only on images and their monolingual descriptions but also on additional multilingual image descriptions when these are available. We believe that having multiple descriptions of one image, regardless of its language, is likely to increase the coverage and variability of ideas described in the image, which may lead to a better generalisation of the depicted scene semantics. Moreover, a similar description expressed in different languages may differ in subtle but meaningful ways.\nTo that end, we introduce a novel training objective function that uses noise\u2013contrastive estimation adapted to the case of three or more input sources, i.e. an image and multilingual sen-\ntences (\u00a72). Our objective function links images and multiple sentences in an arbitrary number of languages, and we validate our idea in experiments where we use the Multi30k data set (\u00a73).\nWe evaluate our embeddings in three different tasks (\u00a74): an image\u2013sentence ranking (ISR) task, in both directions, where we find that it improves ISR to a large extent, i.e. the median ranks for English are improved from 8 to 5 and for German from 11 to 6, although the impact on ranking sentences given images is less conclusive; two sentence textual similarity (STS) tasks, finding consistent improvements over a comparable monolingual baseline; and also in neural machine translation (NMT), where we use our model to re-rank n-best lists generated by an NMT baseline and report strong and consistent improvements. Our main contributions are: \u2022 we introduce a novel ranking loss objective\nfunction to train a discriminative model that utilises not only multi-modal but also multilingual data; \u2022 we compare our proposed multilingual and\nmulti-modal embeddings (MLMME) to embeddings trained on only one language on three different tasks (ISR, STS and NMT), and find that it leads to improvements on both ISR and STS tasks, and that it can be efficiently used to re-rank NMT n-best lists."}, {"heading": "2 Multilingual and multi-modal embeddings", "text": "Our model has two main components: one textual and one visual. In the textual component, we have K different languages Lk, k \u2208 K, and for each language we use a recurrent neural network (RNN) with gated recurrent units (GRU) (Cho et al., 2014) as a sentence encoder. Let Sk = {wk1 , . . . , wkNK} denote sentences com-\nar X\niv :1\n70 2.\n01 10\n1v 1\n[ cs\n.C L\n] 3\nF eb\n2 01\n7\nposed of word indices in a language Lk, and Xk = (xk1,x k 2, \u00b7 \u00b7 \u00b7 ,xkNk) the corresponding word embeddings for these sentences, where Nk is the sentence length. An RNN \u03a6kenc reads X\nk word by word, from left to right, and generates a sequence of annotation vectors (hk1,h k 2, \u00b7 \u00b7 \u00b7 ,hkNk) for each embedding xki , i \u2208 [1, Nk]. For any given input sentence, we use the corresponding encoder RNN\u2019s last annotation vector hkNk for that language Lk as the sentence representation, henceforth vk.\nIn our visual component we use publicly available pre-trained models for image feature extraction. Simonyan and Zisserman (2014) trained deep convolutional neural network (CNN) models for classifying images into one out of 1000 ImageNet classes (Russakovsky et al., 2015). We use their 19-layer VGG network (VGG19) to extract image feature vectors for all images in our dataset: we feed an image to the pre-trained network and use the activations of the penultimate fully connected layer FC7, network configuration E, as our image feature vector (Simonyan and Zisserman, 2014).\nEach training example consists of a tuple (i) sentences Sk in Lk, \u2200k \u2208 K, and (ii) the associated image these sentences describe. Given a training instance, we retrieve the embeddings Xk = {xk1, . . . ,xkNK} for each sentence S\nk using one separate word embedding matrix for each language k. A sentence embedding representation vk is then obtained by applying the encoder \u03a6kenc onto each embedding xk1:NK and using the last annotation vector hkNK of each RNN, after it has consumed the last token in each sentence. Note that our encoder RNNs for different languages share no parameters. An image feature vector q \u2208 R4096 is extracted using a pre-trained CNN so that d = WI \u00b7 q is an image embedding and WI is an image transformation matrix trained with the model. Also, image embeddings d and sentence embeddings vk, \u2200k \u2208 K are all normalised to unit norm and have the same dimensionality. Finally, si(d,vk) = d> \u00b7 vk, \u2200k \u2208 K is a function that computes the similarity between images and sentences in all languages, and ss(v\nk,vl) = (vk)> \u00b7 vl, \u2200k \u2208 K,\u2200l \u2208 K, k 6= l, computes the similarity between sentences in two different languages.\nWe now describe a variation of a pairwise ranking loss function used to train our model. Our\nmodel takes into consideration not only the relation between sentences in a given language and images\u2014computed by the si(\u00b7, \u00b7) function\u2014, but also sentences in different languages in relation to each other, computed by ss(\u00b7, \u00b7). Our sentence\u2013 image, multi-modal similarity is given in (1):\nSI(v k,d) = \u2211 d \u2211 r\nmax {0, \u03b1\u2212 si(d,vk) + si(d,vkr )}+\u2211 vk \u2211 r max {0, \u03b1\u2212 si(vk,d) + si(vk,dr)}, \u2200k \u2208 K, (1)\nwhere vkr (subscript r for random) is a contrastive or non-descriptive sentence embedding in language Lk for image embedding d and vice-versa, and \u03b1 is a model parameter, i.e. the margin. SI(v\nk,d) scores sentences in every language Lk, \u2200k \u2208 K against all images d. Our sentence\u2013 sentence, multilingual similarity is given in (2):\nSS(v k) = \u2211 vk \u2211 r\nmax {0, \u03b1\u2212 ss(vk,vl) + ss(vk,vlr)}+\u2211 vl \u2211 r max {0, \u03b1\u2212 ss(vl,vk) + ss(vl,vkr )}, \u2200k \u2208 K, \u2200l \u2208 K, l 6= k, (2)\nwhere vkr is a contrastive or non-descriptive sentence embedding in language Lk for sentence vl in language Ll and vice-versa. SS(vk) scores sentences in one language against sentences in all other languages. In both SI and SS , contrastive terms are chosen randomly from the training set and resampled at every epoch.\nFinally, our optimisation function in Equation (3) minimises the linearly weighted combination of SI and SS :\nmin \u03b8k,WI\n\u03b2SI + (1\u2212 \u03b2)SS , \u2200k \u2208 K, (3)\n0 \u2265 \u03b2 \u2265 1,\nwhere \u03b8k includes all the encoder RNNs parameters for language Lk, and WI is an image transformation matrix. \u03b2 is a model hyperparameter that controls how much influence a particular similarity (multi-modal versus multilingual) has in the overall cost. The two extreme scenarios are \u03b2 = 0, in which case only the multilingual similarity is used, and \u03b2 = 1, in which case only the multimodal similarity is used. If the number of languages K = 1 and \u03b2 = 1, our model computes the Visual Semantic Embedding (VSE) of Kiros et al. (2014)."}, {"heading": "3 Datasets", "text": "The original Flickr30k data set contains 30k images and 5 English sentence descriptions for each image (Young et al., 2014). To train the NMT model in table 2 we use the translated Multi30k, henceforth M30kT, where for each of the 30k images in the original Flickr30k, one of its English descriptions is manually translated into German by a professional translator. Training, validation and test sets contain 29k, 1014 and 1k images respectively, each accompanied by one translated sentence pair in English and German. In all other experiments in \u00a75 we use the comparable Multi30k, henceforth M30kC, an expansion of the Flickr30k where 5 German descriptions were collected for each image in the original Flickr30k independently from the English descriptions (Elliott et al., 2016). Training, validation and test sets contain 29k, 1014 and 1k images respectively, each accompanied by 5 English and 5 German sentences. Since the M30kC\u2019s test set is not publicly available, we split its validation set in two as use the whole training set for training, the first 500 images and their corresponding bilingual sentences in the validation set for model selection and the remaining 514 images and bilingual sentences for evaluation. Source and target languages were estimated over the entire vocabulary, i.e. 22,965 English and 34,036 German tokens."}, {"heading": "4 Experiments", "text": "For each language we train a separate 1024D encoder RNN with GRU. Word embeddings are 620D and trained jointly with the model. All non-recurrent matrices are initialised by sampling from a Gaussian, N (0, 0.01), recurrent matrices\nare random orthogonal and bias vectors are all initialised to zero. We apply dropout (Srivastava et al., 2014) with a probability of 0.5 in both text and image representations, which are in turn mapped onto a 2048D multi-modal embedding space. We set the margin \u03b1 = 0.2. Our models are trained using stochastic gradient descent with Adam (Kingma and Ba, 2015) with minibatches of 128 instances."}, {"heading": "5 Results", "text": "As our main baseline, we retrain Kiros et al. (2014) monolingual models separately on English and German sentences (+images).\nImage\u2194Sentence Ranking In Table 1, we show results for the monolingual VSE English and German models of Kiros et al. (2014) and our MLMME models on the M30kC data set and evaluated on images and bilingual sentences. Recallat-k (r@k) measures the mean number of times the correct result appear in the top-k retrieved entries and mrank is the median rank.\nFirst, we note that multilingual models show consistent improvements in ranking images given sentences. All our models, regardless of the value of the hyperparameter \u03b2 (= .25, .5, .75, 1) show strong improvements in recall@k (up to +12.2) and median rank\u2014mrank in English reduced from 8 to 5 and in German from 11 to 6 in comparison to the best model by Kiros et al. (2014). Nevertheless, when ranking sentences given images, results are unclear. The best results achieved by our multilingual models, for both languages, are observed when \u03b2 = 1, with the recall@k deteriorating as we include more multilingual similarity, i.e. \u03b2 = .75, .5, .25, and the median rank also slightly\nincreasing for English (from 4 to 5) and German (from 4 to 6).\nSemantic Textual Similarity In the semantic textual similarity task, we use our model to compute the distance between a pair of sentences (distances are equivalent to cosine similarity and therefore lie in the [0, 1] interval). Gold standard scores for all tasks are given in the [0, 5] interval, where 0 means complete dissimilarity and 5 complete similarity. We simply use the cosine similarity distance and scale it by 5, directly comparing it to the gold standard scores. There is no SemEval data set including the German language, therefore we only use our English encoders to compute embedding vectors for both sentences in each test set. We report results for the two in-domain similarity tasks in SemEval, specifically the image description similarity tasks from years 2014 (Agirre et al., 2014) and 2015 (Agirre et al., 2015).\nIn Table 2, we note that our MLMME model consistently improves on the monolingual baseline of Kiros et al. (2014) in the two in-domain similarity tasks, remaining competitive even compared to the best comparable SemEval model (differences <10%). We note that we only use the English side of our models in these two evaluations. That means that the addition of the German encoder helps also the English encoder, specially for lower values of \u03b2 (\u03b2 \u2208 {.5, .25}).\nNeural Machine Translation (NMT) In this set of experiments, we train the attention-based model of Bahdanau et al. (2015)1 on the M30kT training set to translate from English into German. We use it to generate n-best lists (n = 20) for each entry in the M30kT validation and test sets. We use Kiros\u2019 monolingual VSE model trained on German sentences and images to compute the distance between them, and our MLMME models\n1https://github.com/rsennrich/nematus\ntrained with \u03b2 \u2208 {.25, .5, .75, 1} to compute the distance between German and English sentences with ss(\u00b7, \u00b7), and between a German sentence and an image using si(\u00b7, \u00b7), for all entries in the M30kT validation and test sets. We then train an n-best list re-ranker on the M30kT validation set\u2019s 20- best lists with k-best MIRA (Crammer and Singer, 2003; Cherry and Foster, 2012), and use the new distances as additional features to the original MT log-likelihood p(Y |X). We finally apply the optimised weights to re-rank the test set\u2019s 20-best lists.\nFrom Table 3, incorporating features from discriminative models to re-rank n-best lists leads to strong improvements according to traditional MT metrics, e.g. 4.5 BLEU, 2.6 METEOR or 6.1 TER points (Papineni et al., 2002; Denkowski and Lavie, 2014; Snover et al., 2006). We note that the highest improvements were found for our MLMME model when \u03b2 = 0.25, i.e. using much of the multilingual similarities, even though all models show consistent gains in all three metrics."}, {"heading": "6 Conclusions", "text": "We propose a new discriminative model that incorporates both multilingual and multi-modal similarities and introduce a modified noisecontrastive estimation function to optimise our model, which shows promising results in three different tasks. Results obtained with the recently released Multi30k data set demonstrate that our model can learn meaningful multimodal embeddings, effectively making use of multilingual signals and leading to consistently better results in comparison to a comparable monolingual model.\nIn the future we will train our model on a manylanguages setting, with images and descriptions in \u223c10 languages. Finally, we wish to study in more detail how these models can be exploited in NMT."}], "references": [{"title": "Semeval-2014 task 10: Multilingual semantic textual similarity", "author": ["Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guo", "Rada Mihalcea", "German Rigau", "Janyce Wiebe."], "venue": "Proceedings of the", "citeRegEx": "Agirre et al\\.,? 2014", "shortCiteRegEx": "Agirre et al\\.", "year": 2014}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations, ICLR 2015. San Diego, California.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Colin Cherry", "George Foster."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Cherry and Foster.,? 2012", "shortCiteRegEx": "Cherry and Foster.", "year": 2012}, {"title": "Learning Phrase Representations using RNN Encoder\u2013 Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "In", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["Koby Crammer", "Yoram Singer."], "venue": "J. Mach. Learn. Res. 3:951\u2013991. https://doi.org/10.1162/jmlr.2003.3.4-5.951.", "citeRegEx": "Crammer and Singer.,? 2003", "shortCiteRegEx": "Crammer and Singer.", "year": 2003}, {"title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["Michael Denkowski", "Alon Lavie."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation. Baltimore, Maryland, USA, pages 376\u2013380.", "citeRegEx": "Denkowski and Lavie.,? 2014", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Multi30K: Multilingual English-German Image Descriptions", "author": ["Desmond Elliott", "Stella Frank", "Khalil Sima\u2019an", "Lucia Specia"], "venue": "In Proceedings of the 5th Workshop on Vision", "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "International Conference on Learning Representations, ICLR 2015. San Diego, California.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel."], "venue": "CoRR abs/1411.2539. http://arxiv.org/abs/1411.2539.", "citeRegEx": "Kiros et al\\.,? 2014", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman."], "venue": "arXiv preprint arXiv:1409.1556 .", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "In Proceedings of Association for Machine Translation in the Americas. Cambridge, MA, pages", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Karpathy Andrej", "Q Le", "Chris Manning", "Andrew Ng."], "venue": "Transactions of the Association for Computational Linguistics 2014.", "citeRegEx": "Socher et al\\.,? 2014", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15:1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguis-", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "In this work, we expand on the idea of training multi-modal embeddings (Kiros et al., 2014; Socher et al., 2014) and introduce a model that can be trained not only on images and their monolingual descriptions but also on additional multilingual image descriptions when these are available.", "startOffset": 71, "endOffset": 112}, {"referenceID": 13, "context": "In this work, we expand on the idea of training multi-modal embeddings (Kiros et al., 2014; Socher et al., 2014) and introduce a model that can be trained not only on images and their monolingual descriptions but also on additional multilingual image descriptions when these are available.", "startOffset": 71, "endOffset": 112}, {"referenceID": 3, "context": "In the textual component, we have K different languages Lk, k \u2208 K, and for each language we use a recurrent neural network (RNN) with gated recurrent units (GRU) (Cho et al., 2014) as a sentence encoder.", "startOffset": 162, "endOffset": 180}, {"referenceID": 11, "context": "Simonyan and Zisserman (2014) trained deep convolutional neural network (CNN) models", "startOffset": 0, "endOffset": 30}, {"referenceID": 10, "context": "for classifying images into one out of 1000 ImageNet classes (Russakovsky et al., 2015).", "startOffset": 61, "endOffset": 87}, {"referenceID": 11, "context": "use the activations of the penultimate fully connected layer FC7, network configuration E, as our image feature vector (Simonyan and Zisserman, 2014).", "startOffset": 119, "endOffset": 149}, {"referenceID": 8, "context": "If the number of languages K = 1 and \u03b2 = 1, our model computes the Visual Semantic Embedding (VSE) of Kiros et al. (2014).", "startOffset": 102, "endOffset": 122}, {"referenceID": 8, "context": "Table 1: Monolingual baseline (VSE) of Kiros et al. (2014) and our MLMME model on the M30kC test set.", "startOffset": 39, "endOffset": 59}, {"referenceID": 15, "context": "The original Flickr30k data set contains 30k images and 5 English sentence descriptions for each image (Young et al., 2014).", "startOffset": 103, "endOffset": 123}, {"referenceID": 6, "context": "collected for each image in the original Flickr30k independently from the English descriptions (Elliott et al., 2016).", "startOffset": 95, "endOffset": 117}, {"referenceID": 14, "context": "We apply dropout (Srivastava et al., 2014) with a probability of 0.", "startOffset": 17, "endOffset": 42}, {"referenceID": 7, "context": "Our models are trained using stochastic gradient descent with Adam (Kingma and Ba, 2015) with minibatches of 128 instances.", "startOffset": 67, "endOffset": 88}, {"referenceID": 8, "context": "As our main baseline, we retrain Kiros et al. (2014) monolingual models separately on English and German sentences (+images).", "startOffset": 33, "endOffset": 53}, {"referenceID": 8, "context": "Image\u2194Sentence Ranking In Table 1, we show results for the monolingual VSE English and German models of Kiros et al. (2014) and our MLMME models on the M30kC data set and evaluated on images and bilingual sentences.", "startOffset": 104, "endOffset": 124}, {"referenceID": 8, "context": "2) and median rank\u2014mrank in English reduced from 8 to 5 and in German from 11 to 6 in comparison to the best model by Kiros et al. (2014). Nevertheless, when ranking sentences given images, results are unclear.", "startOffset": 118, "endOffset": 138}, {"referenceID": 0, "context": "2014) and 2015 (Agirre et al., 2015). In Table 2, we note that our MLMME model consistently improves on the monolingual baseline of Kiros et al. (2014) in the two in-domain similarity tasks, remaining competitive even compared to the best comparable SemEval model (differences <10%).", "startOffset": 16, "endOffset": 152}, {"referenceID": 1, "context": "of experiments, we train the attention-based model of Bahdanau et al. (2015)1 on the M30kT training set to translate from English into German.", "startOffset": 54, "endOffset": 77}, {"referenceID": 4, "context": "We then train an n-best list re-ranker on the M30kT validation set\u2019s 20best lists with k-best MIRA (Crammer and Singer, 2003; Cherry and Foster, 2012), and use the new distances as additional features to the original MT log-likelihood p(Y |X).", "startOffset": 99, "endOffset": 150}, {"referenceID": 2, "context": "We then train an n-best list re-ranker on the M30kT validation set\u2019s 20best lists with k-best MIRA (Crammer and Singer, 2003; Cherry and Foster, 2012), and use the new distances as additional features to the original MT log-likelihood p(Y |X).", "startOffset": 99, "endOffset": 150}, {"referenceID": 9, "context": "1 TER points (Papineni et al., 2002; Denkowski and Lavie, 2014; Snover et al., 2006).", "startOffset": 13, "endOffset": 84}, {"referenceID": 5, "context": "1 TER points (Papineni et al., 2002; Denkowski and Lavie, 2014; Snover et al., 2006).", "startOffset": 13, "endOffset": 84}, {"referenceID": 12, "context": "1 TER points (Papineni et al., 2002; Denkowski and Lavie, 2014; Snover et al., 2006).", "startOffset": 13, "endOffset": 84}], "year": 2017, "abstractText": "We propose a novel discriminative model that learns embeddings from multilingual and multi-modal data, meaning that our model can take advantage of images and descriptions in multiple languages to improve embedding quality. To that end, we introduce a modification of a pairwise contrastive estimation optimisation function as our training objective. We evaluate our embeddings on an image\u2013sentence ranking (ISR), a semantic textual similarity (STS), and a neural machine translation (NMT) task. We find that the additional multilingual signals lead to improvements on both the ISR and STS tasks, and the discriminative cost can also be used in re-ranking n-best lists produced by NMT models, yielding strong improvements.", "creator": "LaTeX with hyperref package"}}}