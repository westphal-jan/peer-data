{"id": "1702.03033", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2017", "title": "Local System Voting Feature for Machine Translation System Combination", "abstract": "supporting this iteration, we enhance the interactive knowledge network solution combination approach \u2014 an experimental loop activated by a neural network. this performance is shown by the fact traditionally the incorrectly used binary system voting models theoretically assign each contestant the actual global weight which is blamed for the global impact accompanying each input system on matching users. equivalence prevents individual users doing low mean weights from having restrictions on this system combination output, although in some situations this could be helpful. further, words likely have only partially seen matching one or zero subjects seldom hold a chance of being present in correctly combined output. we resolve a local system voting model by a genetic matrix whom is just like optimal preference mixed and the combinatorial occurrences of even different system outputs. correlation gives system combination the option to prefer other systems at different coordinate positions even for the same amount.", "histories": [["v1", "Fri, 10 Feb 2017 01:27:00 GMT  (60kb,D)", "http://arxiv.org/abs/1702.03033v1", "published WMT 2015"]], "COMMENTS": "published WMT 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["markus freitag", "jan-thorsten peter", "stephan peitz", "minwei feng", "hermann ney"], "accepted": false, "id": "1702.03033"}, "pdf": {"name": "1702.03033.pdf", "metadata": {"source": "CRF", "title": "Local System Voting Feature for Machine Translation System Combination", "authors": ["Markus Freitag", "Jan-Thorsten Peter", "Stephan Peitz", "Minwei Feng"], "emails": ["<surname>@cs.rwth-aachen.de"], "sections": [{"heading": "1 Introduction", "text": "Adding more linguistic informed models (e.g. language model or translation model) additionally to the standard models into system combination seems to yield no or only small improvements. The reason is that all these models should have already been applied during the decoding process of the individual systems (which serve as input hypotheses for system combination) and hence already fired before system combination. To improve system combination with additional models, we need to define a model which can not be applied by an individual system.\nIn state-of-the-art confusion network system combination the following models are usually applied:\nSystem voting (globalVote) models For each word the voting model for system i (1\u2264 i\u2264 I) is 1 iff the word is from system i, otherwise 0. Binary primary system model (primary) A model that marks the primary hypothesis. Language model 3-gram language model (LM) trained on the input hypotheses. Word penalty Counts the number of words.\nTo gain improvements with additional models, it is better to define models which are not used by an individual system. A simple model which can not be applied by any individual system is the binary system voting model (globalVote). This model is the most important one during system combination decoding as it determines the impact of each individual system. Each system i is assigned one globalVote model which fires if the word is generated by system i. Nevertheless, this simple model is independent of the actual words and the score is only based on the global preferences of the individual systems. This disadvantage prevents system combination from producing words which have only been seen by systems with low system weights (low globalVote model weights). To give systems and words with low weights a chance to affect the final output, we define a new local system voting model (localVote) which makes decisions based on the current word options and not only on a general weight. The local system voting model allows system combination to prefer different system outputs at different word positions even for the same sentence.\nMotivated by the success of neural networks in language modelling (Bengio et al., 2006, Schwenk and Gauvain, 2002) and translation modelling (Son et al., 2012), we choose feedforward neural networks to train the novel model. Instead of calculating the probabilities in a discrete space, the neural network projects the words into a continuous space. This projection gives us the option to assign probability also to input sequences which\nar X\niv :1\n70 2.\n03 03\n3v 1\n[ cs\n.C L\n] 1\n0 Fe\nb 20\n17\nwere not observed in the training data. In system combination each training sentence has to be translated by all individual system engines which is time consuming. Due to this we have a small amount of training data and thus it is very likely that many input sequences of a test set have not be seen during training.\nThe remainder of this paper is structured as follows: in Section 2, we discuss some related work. In Section 3, the novel local system voting model is described. In Section 4, experimental results are presented which are analyzed in Section 5. The paper is concluded in Section 6."}, {"heading": "2 Related Work", "text": "In confusion network decoding, pairwise alignments between all system outputs are generated. From the calculated alignment information, a confusion network is built from which the system combination output is determined using majority voting and additional models. The hypothesis alignment algorithm is a crucial part of building the confusion network and many alternatives have been proposed in the literature:\n(Bangalore et al., 2001) use a multiple string alignment (MSA) algorithm to identify the unit of consensus and applied a posterior language model to extract the consensus translations. In contrast to the following approaches, MSA is unable to capture word reorderings. (Matusov et al., 2006) produce pairwise word alignments with the statistical alignment algorithm toolkit GIZA++ that explicitly models word reordering. The context of a whole document of translations rather than a single sentence is taken into account to produce the alignments. (Sim et al., 2007) construct a consensus network by using TER (Snover et al., 2006) alignments. Minimum bayes risk decoding is applied to obtain a primary hypothesis to which all other hypotheses are aligned. (Rosti et al., 2007) extend the TER alignment approach and introduce an incremental TER alignment which aligns one system at a time to all previously aligned hypotheses. (Karakos et al., 2008) use the inversion transduction grammar (ITG) formalism (Wu, 1997) and treat the alignment problem as a\nproblem of bilingual parsing to generate the pairwise alignments.\n(He et al., 2008) propose an indirect hidden markov model (IHMM) alignment approach to address the synonym matching and word ordering issues in hypothesis alignment. (Heafield and Lavie, 2010) use the METEOR toolkit to calculate pairwise alignments between the hypotheses.\nAll confusion network system combination approaches only use the global system voting models. Regarding to this chapter, there has been similar effort in the area of speech recognition:\n(Hillard et al., 2007) Similar work has been presented for system combination of speech recognitions systems: the authors train a classifier to learn which system should be selected for each output word. The learning target for each slot is the set of systems which match the reference word, or the null class if no systems match the reference word. Their novel approach outperforms the ROVER baseline by up to 14.5% relatively on an evaluation set."}, {"heading": "3 Novel Local System Voting Model", "text": "In the following subsections we introduce a novel local system voting model (localVote) trained by a neural network. The purpose of this model is to prefer one particular path in the confusion network and therefore all local word decisions between two nodes leading to this particular path. More precisely, we want the neural network to learn an oracle path extracted from the confusion network graph which leads to the lowest error score. In Subsection 3.1, we describe a polynomial approximation algorithm to extract the best sentence level BLEU (SBLEU) path in a confusion network. Taking this path as reference path, we define the model in Subsection 3.2 followed by its integration in the linear model combination in Subsection 3.3."}, {"heading": "3.1 Finding SBLEU-optimal Hypotheses", "text": "In this section, we describe a polynomial approximation algorithm to extract the best SBLEU hypothesis from a confusion network. (Leusch et al., 2008) showed that this problem is generally NP-hard for the popular BLEU (Papineni et al., 2002) metric. Nevertheless, we need some paths which serve as \u201creference paths\u201c.\nUsing BLEU as metric to extract the best possible path is problematic as in the original BLEU definition there is no smoothing for the geometric mean. This has the disadvantage that the BLEU score becomes zero already if the four-gram precision is zero, which can happen obviously very often with short or difficult translations. To allow for sentence-wise evaluation, we use the SBLEU metric (Lin and Och, 2004), which is basically BLEU where all n-gram counts are initialized with 1 instead of 0. The brevity penalty is calculated only on the current hypothesis and reference sentence.\nWe use the advantage that confusion networks can be sorted topologically. We walk the confusion network from the start node to the end node, keeping track of all n-grams seen so far. At each node we keep a k-best list containing the partial hypotheses with the most n-gram matches leading to this node and recombine only partial hypotheses containing the same translation. As the search space can become exponentially large, we only keep k possible options at each node. This pruning can lead to search errors and hence yield nonoptimal results. If needed for hypotheses with the same n-gram counts, we prefer hypotheses with a higher translation score based on the original models. For the final node we add the brevity penalty to all possible translations.\nAs we are only interested in arc decisions which match a reference word, we simplify the confusion network before applying the algorithm. If all arcs between two adjacent nodes are not present in the reference, we remove all of them and add a single arc labeled with \u201dUNK\u201d. This reduces the vocabulary size and still gives us the same best SBLEU scores as before. In Figure 1, a confusion network of four input hypotheses is given. As the words black, red, orange, and green are all not present in the reference, all of them are mapped to one single \u201dUNK\u201d arc (cf. Figure 2). The best SBLEU path is the UNK car."}, {"heading": "3.2 localVote Model Training", "text": "The purpose of the new localVote model is to prefer the best SBLEU path and therefore to learn the word decisions between all adjacent nodes which lead to this particular path. During the extraction of the best SBLEU hypotheses from the confusion network, we keep track of all arc decisions. This gives us the possibility to generate local training examples based only on the I arcs between two nodes. For the confusion network illustrated in Figure 2, we generate two training examples for the neural network training. Based on the arcs the, an, a and a we learn the output the. Based on the arcs cab, train, car and car we learn the output car.\nIn all upcoming system setups, we use the open source toolkit NPLM (Vaswani et al., 2013) for training and testing the neural network models. We use the standard setup as described in the paper and use the neural network with one projection layer and one hidden layer. For more details we refer the reader to the original paper of the NPLM toolkit. The inputs to the neural network are the I words produced by the I different individual systems. The outputs are the posterior probabilities of all words of the vocabulary. The input uses the so-called 1-of-n coding, i.e. the i-th word of the vocabulary is coded by setting the i-th element of the vector to 1 and all the other elements to 0.\nFor a system combination of I individual systems, a training example consists of I + 1 words. The first I words (input of the neural network) are representing the words of the individual systems, the last position (output of the neural network) serves as slot for the decision we want to learn (extracted from the best SBLEU path). We do not add the \u201dUNK\u201d arcs to the neural network training as they do not help to increase the SBLEU score. Figure 3 shows the neural network training example for the last words of Figure 2. The output of each\ncar\ncar\ntrain\ncab P(w1| ) projection\nhiddenlayer layer P(w2| )\nP(w3| )\nP(wn| )\n0 0 1 0 .. . 1 0 0 0 .. . 0 1 0 0 .. . 0 1 0 0 .. .\n.\n.\n.\nFigure 3: Unigram neural network training example: System A produces cab, System B train, System C car, System D car, reference is car. 1-of-n encoding was applied to map words to a suitable neural network input.\nindividual system provides one input word. In Table 1 the two training examples for Figure 2 are illustrated.\nAs a neural network training example only consists of the I words between two adjacent nodes, we are able to produce several training examples for each sentences. For a system combination of I systems and a development set of S sentences with an average sentence length of L, we can generate up to I \u2217S\u2217L neural network training examples.\nFurther, we can expand the model to use arbitrary history size, if we take the predecessor words into account. Instead of just using the local word decision of a system, we add additionally the predecessors of the individual systems into the training data. In Figure 4, we e.g. use the bigram red train instead of the unigram train for system B into the training data. In Table 2 all bigram training examples of Figure 2 can be seen."}, {"heading": "3.3 localVote model Integration", "text": "Having a trained localVote model, we then add it as an additional model into the confusion network. We calculate for each arc the probability of the word in the trained neural network. E.g. for Figure 1, we extract the probabilities for all arcs by the strings illustrated in Table 3. Finally, we add the scores as a new model and assign it a weight which is trained additionally to the standard model\nweights with MERT."}, {"heading": "3.4 Word Classes", "text": "The neural network training sets are relatively small as all sentences have to be translated by all individual system engines. This results in many unseen words in the test sets. To overcome this problem, we use word classes (Och, 1999) instead of words which were trained (10 iterations) on the target part of the bilingual training corpus in some experiments. We use the trained word classes on both input layer and output layer."}, {"heading": "4 Experiments", "text": "All experiments have been conducted with the open source system combination toolkit Jane (Freitag et al., 2014). For training and scoring neural networks, we use the open source toolkit NPLM (Vaswani et al., 2013). NPLM is a toolkit for training and using feedforward neural language models. Variations in neural network architecture have been tested. We tried various hidden layer sizes as well as projection layer sizes. We achieved similar results for all setups and decided to stick to 1 hidden layer whose size is 200, a learning rate of 0.08 and let the training run 20 epochs in all experiments.\nTranslation quality is measured in lowercase with BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) whereas the performance of each setup is the best score on the tune set across five different MERT runs. The system combination weights of the linear model are optimized with MERT on 200-best lists with (TERBLEU)/2 as optimization criterion. For all language pairs we use three different test sets. In the following the test set for extracting the training examples for the neural network training is labeled as tune (NN). The test set tune (MERT) indicates the tune set for MERT and test indicates the blind test set.\nThe individual systems are different extensions of phrase-based or hierarchical phrase-based systems. The systems are built on the same amount of preprocessed training data and differ mostly in the models which are used to score the translation options. Further, some systems are syntactical augmented based on syntax trees on either source or target side."}, {"heading": "4.1 BOLT Chinese\u2192English", "text": "For Chinese\u2192English, we use the current BOLT data set (corpus statistics are given in Table 4). The test sets consist of text drawn from \u201ddiscussion forums\u201d in Mandarin Chinese. We use nine individual systems to perform the system combination experiments. The lambda weights are optimized on a tune set of 985 sentences (tune (MERT)). We train the proposed localVote model on 15,323,897 training examples extracted from the 1844 sentences tune (NN) set.\nAs a first step we have to determine the k-best pruning threshold for extracting the SBLEU optimal path from the current confusion networks (cf.\nSection 3.1). In Figure 5 the (TER-BLEU)/2 results of the SBLEU optimal hypotheses extracted with different k-best sizes are given. Although, the BLEU score improves by setting k to a higher value, the computational time increases. To find a tradeoff between running time and performance, we set the k-best size to 1200 in the following experiments.\nExperimental results are given in Table 5. The baseline is a system combination run without any localVote model of nine individual systems using the standard models as described in (Freitag et al., 2014). The oracle score is calculated on the hypothesis of the SBLEU best path extracted with k = 1200. We train the neural network on 15,323,897 training examples generated from the 1844 tune (NN) sentences. By training a neural network based on unigram decisions (unigram NN), we gain small improvements of -0.6 points in TER. As we have only few sentences of training data, many words have not been seen during neural network training. To overcome this problem, we train 1500 word classes on the target part of the bilingual data. Learning the localVote model on word classes (unigram wcNN) gain improvement of +0.7 points in BLEU and -0.6 points in\nTER. By taking a bigram history into the training of the neural network, we reach only small further improvement. Compared to the baseline, the system combination +bigram NN outperforms the baseline by +0.3 points in BLEU and -0.6 points in TER. By using word classes (+bigram wcNN) we gain improvement of +0.4 points in BLEU and -1.0 points in TER.\nAll results are reached with a word class size of 1500. In Figure 6 the (TER\u2212BLEU)/2 scores on tune(MERT) of system combinations including one unigram localVote model trained with different word class sizes are illustrated. Independent of the word class size, system combination including a localVote model always performs better compared to the baseline. The best performance is reached by a word class size of 1500. One reason for the loss of performance when using no word classes is the size of the neural network tune set. Within a size of 1844 sentences, many words of the test set have never been seen during neural network training. The test set has a vocabulary size of 6106 within 2487 words (40.73%) are not present in the training set (tune (NN)) of the neural network. For the MERT tune set 2556 words (40.91%) are not present in the neural network training set. Word classes tackle this problem and it is much more likely that each word class\nhas been seen during the training procedure of the neural network."}, {"heading": "4.2 BOLT Arabic\u2192English", "text": "For Arabic\u2192English, we use the current BOLT data set (corpus statistics are given in Table 6). The test sets consist of text drawn from \u201ddiscussion forums\u201d in Egyptian Arabic. We train the neural network on 6,591,158 training examples extracted from the 1510 sentences tune (NN) dev set. The model weights are optimized on a 1080 sentences tune set. All results are system combinations of five individual systems. The test set has a vocabulary size of 3491 within 1510 words (43.25%) are not present in the training set (tune (NN)) of the neural network. For the MERT tune set 1549 words (43.24%) are not part of the neural network training set.\nWe run the same experiment pipeline as for Chinese\u2192English and first determine the k-best threshold for getting the oracle paths in the confusion networks. As the Arabic\u2192English system combination is only based on 5 individual systems, the confusion networks are much smaller. We set the pruning threshold to 1000 (k = 1000) which is a good tradeoff between running time and performance. Figure 7 shows the (TER\u2212 BLEU)/2 scores for different k-best pruning thresholds. Increasing k to a higher value then 1000 improves the (TER\u2212BLEU)/2 only slightly.\nExperimental results are given in Table 7. The baseline is a system combination run without any localVote model of five individual systems using the standard models as described in (Freitag et al., 2014). The oracle score represents the score\nof the SBLEU best path extracted with k = 1000. Training a localVote model based on the best SBLEU path (+unigram NN) gives us improvement of +0.9 points in BLEU compared to the baseline. Adding bigram context to the neural network training (+bigram NN) yields improvement of +0.8 points in BLEU compared to the baseline system combination. By training word classes on the bilingual part of the training data, we gain additional improvements. When using word classes and a history size of two, +bigram wcNN yields the best performance with +1.1 points in BLEU compared to the baseline.\nAll results are conducted with a word class size of 1000. The tune set performance of different unigram localVote models trained on different word class sizes are illustrated in Figure 8. The results are fluctuating and we set the word class size to 1000 in all Arabic\u2192English experiments."}, {"heading": "5 Analysis", "text": "In this section we compare the final translations of the Chinese\u2192English system combination +bigram wcNN with the baseline. The word occurrence distributions for both setups are illustrated in Table 8. This table shows how many input systems produce a certain word and finally if it is part\nof the system combination output. As the original idea of system combination is based on majority voting, it should be more likely that a word which is produced by more input systems is in the final system combination output than a word which is only produced by few input systems. E.g. 11008 words have been produced by all 9 individual systems from which all of them are in both the system combination baseline and the advanced system +bigram wcNN. If a word is only produced by 8 individual systems, a ninth system does not produce this word. 98,9% of the words produced by only 8 different individual systems are in the final\nbaseline system combination output. The missing words result mostly from alignment errors produced by the pairwise alignment algorithm when aligning the single systems together.\nWe observe the problem that the globalVote models prevent words, which have only been produced by few systems, to be present in the system combination output. In Table 8, you can see that words which are only produced by 1-4 individual systems are more likely to be present in the final output when including the novel localVote model. As e.g. in the baseline 592 of the 6129 words which have only been produced by two individual system are in the output, the advanced +bigram wcNN setup contains additional 172 words. These statistics demonstrate the functionality of the novel localVote model which does not only improve the translation quality in terms of BLEU, but also tackles the problem of the dominating globalVote models.\nThe Arabic\u2192English word occurrence distribution is illustrated in Table 9. A similar scenario as for the Chinese\u2192English translation task can be observed. The words which only occur in few individual systems have a much higher chance to be in the final output when using the novel local voting system model. It is also visible that the neural network model prevents some words of being in the combined output even if the word have been produced by 4 of 5 systems. The novel local system voting model gives system combination the\noption to select words which have only be generated by few individual systems."}, {"heading": "6 Conclusion", "text": "In this work we proposed a novel local system voting model (localVote) which has been trained by a feedforward neural network. In contrast to the traditional globalVote model, the presented localVote model takes the word contents and their combinatorial occurrences into account and does not only promote global preferences for some individual systems. This advantage gives confusion network decoding the option to prefer other systems at different positions even in the same sentence. As all words are projected to a continuous space, the neural network gives also unseen word sequences a useful probability. Due to the relatively small neural network training set, we used word classes in some experiments to tackle the data sparsity problem.\nExperiments have been conducted with high quality input systems for the BOLT Chinese\u2192English and Arabic\u2192English translation tasks. Training an additional model by a neural network with word classes yields translation improvement from up to +0.9 points in BLEU and -0.5 points in TER. We also took word context into account and added the predecessors of the individual systems to the neural network training which yield additional small improvement. We analyzed the translation results and the functionality of the localVote model. The occurrence distribution shows that words which have been produced by only few input systems are\nmore likely to be part of the system combination output when using the proposed model."}, {"heading": "Acknowledgement", "text": "This material is partially based upon work supported by the DARPA BOLT project under Contract No. HR0011-12-C-0015. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA. Further, this paper has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement no 645452 (QT21)."}], "references": [{"title": "Computing consensus translation from multiple machine translation systems", "author": ["Srinivas Bangalore", "German Bordel", "Giuseppe Riccardi."], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 351\u2013354, Madonna", "citeRegEx": "Bangalore et al\\.,? 2001", "shortCiteRegEx": "Bangalore et al\\.", "year": 2001}, {"title": "Neural probabilistic language models", "author": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain."], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Jane: Open source machine translation system combination", "author": ["Markus Freitag", "Matthias Huck", "Hermann Ney."], "venue": "Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 29\u201332, Gothenburg, Sweden,", "citeRegEx": "Freitag et al\\.,? 2014", "shortCiteRegEx": "Freitag et al\\.", "year": 2014}, {"title": "Indirect-HMMbased Hypothesis Alignment for Combining Outputs from Machine Translation Systems", "author": ["Xiaodong He", "Mei Yang", "Jianfeng Gao", "Patrick Nguyen", "Robert Moore."], "venue": "Conference on Empirical Methods in Natural Language", "citeRegEx": "He et al\\.,? 2008", "shortCiteRegEx": "He et al\\.", "year": 2008}, {"title": "Combining Machine Translation Output with Open Source: The Carnegie Mellon Multi-Engine Machine Translation Scheme", "author": ["Kenneth Heafield", "Alon Lavie."], "venue": "The Prague Bulletin of Mathematical Linguistics, 93:27\u201336.", "citeRegEx": "Heafield and Lavie.,? 2010", "shortCiteRegEx": "Heafield and Lavie.", "year": 2010}, {"title": "i rover: improving system combination with classification", "author": ["Dustin Hillard", "Bj\u00f6rn Hoffmeister", "Mari Ostendorf", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Hillard et al\\.,? 2007", "shortCiteRegEx": "Hillard et al\\.", "year": 2007}, {"title": "Machine translation system combination using ITG-based alignments", "author": ["Damianos Karakos", "Jason Eisner", "Sanjeev Khudanpur", "Markus Dreyer."], "venue": "46th Annual Meeting of the Association for Computational Linguistics on Human Language Technolo-", "citeRegEx": "Karakos et al\\.,? 2008", "shortCiteRegEx": "Karakos et al\\.", "year": 2008}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388\u2013395, Barcelona, Spain, July.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Complexity of finding the bleu-optimal hypothesis in a confusion network", "author": ["Gregor Leusch", "Evgeny Matusov", "Hermann Ney."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 839\u2013847, Honolulu, HI, USA, Oc-", "citeRegEx": "Leusch et al\\.,? 2008", "shortCiteRegEx": "Leusch et al\\.", "year": 2008}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics", "author": ["Chin-Yew Lin", "Franz Josef Och."], "venue": "The 42nd Annual Meeting on Association for Computational Linguistics (ACL), page 605,", "citeRegEx": "Lin and Och.,? 2004", "shortCiteRegEx": "Lin and Och.", "year": 2004}, {"title": "Computing Consensus Translation from Multiple Machine Translation Systems Using Enhanced Hypotheses Alignment", "author": ["Evgeny Matusov", "Nicola Ueffing", "Hermann Ney."], "venue": "Conference of the European Chapter of the Association for Computa-", "citeRegEx": "Matusov et al\\.,? 2006", "shortCiteRegEx": "Matusov et al\\.", "year": 2006}, {"title": "An Efficient Method for Determining Bilingual Word Classes", "author": ["Franz Josef Och."], "venue": "Ninth Conference on European Chapter of the Association for Computational Linguistics (EACL), pages 71\u2013 76, Bergen, Norway, June.", "citeRegEx": "Och.,? 1999", "shortCiteRegEx": "Och.", "year": 1999}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "40th Annual Meeting on Association for Computational Linguistics (ACL), pages 311\u2013318, Philadelphia, PA, USA,", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Combining outputs from multiple machine translation systems", "author": ["Antti-Veikko Rosti", "Necip Fazil Ayan", "Bing Xiang", "Spyridon Matsoukas", "Richard Schwartz", "Bonnie Dorr."], "venue": "The Annual Conference of the North American Chapter of the Associ-", "citeRegEx": "Rosti et al\\.,? 2007", "shortCiteRegEx": "Rosti et al\\.", "year": 2007}, {"title": "Connectionist language modeling for large vocabulary continuous speech recognition", "author": ["Holger Schwenk", "Jean-Luc Gauvain."], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), volume 1, pages I\u2013765, Or-", "citeRegEx": "Schwenk and Gauvain.,? 2002", "shortCiteRegEx": "Schwenk and Gauvain.", "year": 2002}, {"title": "Consensus Network Decoding for Statistical Machine", "author": ["Khe Chai Sim", "William J. Byrne", "Mark J.F. Gales", "Hichem Sahbi", "Phil C. Woodland"], "venue": null, "citeRegEx": "Sim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sim et al\\.", "year": 2007}, {"title": "A Study of Translation Edit Rate with Targeted Human Annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciula", "John Makhoul."], "venue": "Association for Machine Translation in the Americas (AMTA), pages 223\u2013231, Cambridge,", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Continuous space translation models with neural networks", "author": ["Le Hai Son", "Alexandre Allauzen", "Fran\u00e7ois Yvon."], "venue": "The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Son et al\\.,? 2012", "shortCiteRegEx": "Son et al\\.", "year": 2012}, {"title": "Decoding with largescale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang."], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1387\u20131392,", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora", "author": ["Dekai Wu."], "venue": "Computational linguistics, 23(3):377\u2013403.", "citeRegEx": "Wu.,? 1997", "shortCiteRegEx": "Wu.", "year": 1997}], "referenceMentions": [{"referenceID": 17, "context": ", 2006, Schwenk and Gauvain, 2002) and translation modelling (Son et al., 2012), we choose feedforward neural networks to train the novel model.", "startOffset": 61, "endOffset": 79}, {"referenceID": 0, "context": "(Bangalore et al., 2001) use a multiple string alignment (MSA) algorithm to identify the unit of consensus and applied a posterior language model to extract the consensus translations.", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "(Matusov et al., 2006) produce pairwise word alignments with the statistical alignment algorithm toolkit GIZA++ that explicitly models word reordering.", "startOffset": 0, "endOffset": 22}, {"referenceID": 15, "context": "(Sim et al., 2007) construct a consensus network by using TER (Snover et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 16, "context": ", 2007) construct a consensus network by using TER (Snover et al., 2006) alignments.", "startOffset": 51, "endOffset": 72}, {"referenceID": 13, "context": "(Rosti et al., 2007) extend the TER alignment approach and introduce an incremental TER alignment which aligns one system at a time to all previously aligned hypotheses.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "(Karakos et al., 2008) use the inversion transduction grammar (ITG) formalism (Wu, 1997) and treat the alignment problem as a problem of bilingual parsing to generate the pairwise alignments.", "startOffset": 0, "endOffset": 22}, {"referenceID": 19, "context": ", 2008) use the inversion transduction grammar (ITG) formalism (Wu, 1997) and treat the alignment problem as a problem of bilingual parsing to generate the pairwise alignments.", "startOffset": 63, "endOffset": 73}, {"referenceID": 3, "context": "(He et al., 2008) propose an indirect hidden markov model (IHMM) alignment approach to address the synonym matching and word ordering issues in hypothesis alignment.", "startOffset": 0, "endOffset": 17}, {"referenceID": 4, "context": "(Heafield and Lavie, 2010) use the METEOR toolkit to calculate pairwise alignments between the hypotheses.", "startOffset": 0, "endOffset": 26}, {"referenceID": 5, "context": "(Hillard et al., 2007) Similar work has been presented for system combination of speech recognitions systems: the authors train a classifier to learn which system should be selected for each output word.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "(Leusch et al., 2008) showed that this problem is generally NP-hard for the popular BLEU (Papineni et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": ", 2008) showed that this problem is generally NP-hard for the popular BLEU (Papineni et al., 2002) metric.", "startOffset": 75, "endOffset": 98}, {"referenceID": 9, "context": "To allow for sentence-wise evaluation, we use the SBLEU metric (Lin and Och, 2004), which is basically BLEU where all n-gram counts are initialized with 1 instead of 0.", "startOffset": 63, "endOffset": 82}, {"referenceID": 18, "context": "In all upcoming system setups, we use the open source toolkit NPLM (Vaswani et al., 2013) for training and testing the neural network models.", "startOffset": 67, "endOffset": 89}, {"referenceID": 11, "context": "To overcome this problem, we use word classes (Och, 1999) instead of words which were trained (10 iterations) on the target part of the bilingual training corpus in some experiments.", "startOffset": 46, "endOffset": 57}, {"referenceID": 2, "context": "All experiments have been conducted with the open source system combination toolkit Jane (Freitag et al., 2014).", "startOffset": 89, "endOffset": 111}, {"referenceID": 18, "context": "For training and scoring neural networks, we use the open source toolkit NPLM (Vaswani et al., 2013).", "startOffset": 78, "endOffset": 100}, {"referenceID": 12, "context": "Translation quality is measured in lowercase with BLEU (Papineni et al., 2002) and TER (Snover et al.", "startOffset": 55, "endOffset": 78}, {"referenceID": 16, "context": ", 2002) and TER (Snover et al., 2006) whereas the performance of each setup is the best score on the tune set across five different MERT runs.", "startOffset": 16, "endOffset": 37}, {"referenceID": 2, "context": "The baseline is a system combination run without any localVote model of nine individual systems using the standard models as described in (Freitag et al., 2014).", "startOffset": 138, "endOffset": 160}, {"referenceID": 7, "context": "Significance is marked with \u2020 for 95% confidence and \u2021 for 99% confidence, and is measured with the bootstrap resampling method as described in (Koehn, 2004).", "startOffset": 144, "endOffset": 157}, {"referenceID": 2, "context": "The baseline is a system combination run without any localVote model of five individual systems using the standard models as described in (Freitag et al., 2014).", "startOffset": 138, "endOffset": 160}, {"referenceID": 7, "context": "Significance is marked with \u2021 for 99% confidence and is measured with the bootstrap resampling method as described in (Koehn, 2004).", "startOffset": 118, "endOffset": 131}], "year": 2017, "abstractText": "In this paper, we enhance the traditional confusion network system combination approach with an additional model trained by a neural network. This work is motivated by the fact that the commonly used binary system voting models only assign each input system a global weight which is responsible for the global impact of each input system on all translations. This prevents individual systems with low system weights from having influence on the system combination output, although in some situations this could be helpful. Further, words which have only been seen by one or few systems rarely have a chance of being present in the combined output. We train a local system voting model by a neural network which is based on the words themselves and the combinatorial occurrences of the different system outputs. This gives system combination the option to prefer other systems at different word positions even for the same sentence.", "creator": "TeX"}}}