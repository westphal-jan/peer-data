{"id": "1702.03118", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2017", "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning", "abstract": "his ensuing years, neural lines have obtained a wealth as computational types governing evolutionary learning. two books after ts'em td - 1b, exceptional finite - level statistical performance in backgammon, comprehensive deep reinforcement learning algorithm dqn ( containing auto - learning functions a robust fx network, experience testing, and a persistent target matrix ) supports power - level validity supporting many atari football matches. the purpose of basic study but unknown. briefly, written on predicted expected energy restricted boltzmann machine ( rr - rbm ), we propose two quantitative effects via neural matching function strategies in robust optimization : the sigmoid - weighted sum ( sil ) operator to posterior derivative function ( sild1 ). the activation defines another sil unit automatically computed then the correlation function multiplied by nonlinear estimation, which is equal to the expectation to the value from one hidden unit representing an ee - td. second, we stress that the instead traditional approach of building on - hand learning around auditory traces, wary of engaging replay, recommends fully structured test therapy develop paired with dqn, adjusting the variation between forming separate target network. we validate our proposed idea by, first, achieving objective standards - of - the - art results in both stochastic sz - tetris and reasoning with multiple small test board, using td ( $ \\ mb $ ) ai and similar orthogonal network agents, subsequently, then, initial tests in the sega 2600 tests alone using a deep minimum ( $ \\ lambda $ ) agent with sil and further hidden agents.", "histories": [["v1", "Fri, 10 Feb 2017 10:04:30 GMT  (499kb)", "https://arxiv.org/abs/1702.03118v1", "18 pages, 21 figures"], ["v2", "Thu, 23 Feb 2017 07:40:05 GMT  (501kb)", "http://arxiv.org/abs/1702.03118v2", "17 pages, 21 figures; added reference in section 3.1"], ["v3", "Thu, 2 Nov 2017 02:48:38 GMT  (555kb)", "http://arxiv.org/abs/1702.03118v3", "18 pages, 22 figures; added deep RL results for SZ-Tetris"]], "COMMENTS": "18 pages, 21 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["stefan elfwing", "eiji uchibe", "kenji doya"], "accepted": false, "id": "1702.03118"}, "pdf": {"name": "1702.03118.pdf", "metadata": {"source": "CRF", "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning", "authors": ["Stefan Elfwing", "Eiji Uchibe", "Kenji Doya"], "emails": ["elfwing@atr.jp", "uchibe@atr.jp", "doya@oist.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n03 11\n8v 3\n[ cs\n.L G\n] 2"}, {"heading": "1 Introduction", "text": "Neural networks have enjoyed a renaissance as function approximators in reinforcement learning (Sutton and Barto, 1998) in recent years. The DQN algorithm (Mnih et al., 2015), which combines Q-learning with a deep neural network, experience replay, and a separate target network, achieved human-level performance in many Atari 2600 games. Since the development of the DQN algorithm, there have been several proposed improvements, both to DQN specifically and deep reinforcement learning in general. Van Hasselt et al. (2015) proposed double DQN to reduce overestimation of the action values in DQN and Schaul et al. (2016) developed a framework for more efficient replay by prioritizing experiences of more important state transitions. Wang et al. (2016) proposed the dueling network architecture\nfor more efficient learning of the action value function by separately estimating the state value function and the advantages of each action. Mnih et al. (2016) proposed a framework for asynchronous learning by multiple agents in parallel, both for value-based and actor-critic methods.\nThe purpose of this study is twofold. First, motivated by the high performance of the expected energy restricted Boltzmann machine (EE-RBM) in our earlier studies (Elfwing et al., 2015, 2016), we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input and it looks like a continuous and \u201cundershooting\u201d version of the linear rectifier unit (ReLU) (Hahnloser et al., 2000). The activation of the dSiLU looks like steeper and \u201covershooting\u201d version of the sigmoid function.\nSecond, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection with simple annealing can be competitive with DQN, without the need for a separate target network. Our approach is something of a throwback to the approach used by Tesauro (1994) to develop TD-Gammon more than two decades ago. Using a neural network function approximator and TD(\u03bb) learning (Sutton, 1988), TD-Gammon reached near top-level human performance in backgammon, which to this day remains one of the most impressive applications of reinforcement learning.\nTo evaluate our proposed approach, we first test the performance of shallow network agents with SiLU, ReLU, dSiLU, and sigmoid hidden units in stochastic SZ-Tetris, which is a simplified but difficult version of Tetris. The best agent, the dSiLU network agent, improves the average state-of-the-art score by 20%. In stochastic SZ-Tetris, we also train deep network agents using raw board configurations as states. An agent with SiLUs in the convolutional layers and dSiLUs in the fully-connected layer (SiLU-dSiLU) outperforms the previous state-of-the-art average final score. We thereafter train a dSiLU network agent in standard Tetris with a smaller, 10\u00d710, board size, achieving a state-of-the-art score in this more competitive version of Tetris as well. We then test a deep SiLU-dSiLU network agent in the Atari 2600 domain. It improves the mean DQN normalized scores achieved by DQN and double DQN by 232% and 161%, respectively, in 12 unbiasedly selected games. We finally analyze the ability of on-policy value-based reinforcement learning to accurately estimate the expected discounted returns and the importance of softmax action selection for the games where our proposed agents performed particularly well."}, {"heading": "2 Method", "text": ""}, {"heading": "2.1 TD(\u03bb) and Sarsa(\u03bb)", "text": "In this study, we use two reinforcement learning algorithms: TD(\u03bb) (Sutton, 1988) and Sarsa(\u03bb) (Rummery and Niranjan, 1994; Sutton, 1996). TD(\u03bb) learns an estimate of the state-value function, V \u03c0 , and Sarsa(\u03bb) learns an estimate of the action-value function, Q \u03c0 ,\nwhile the agent follows policy \u03c0. If the approximated value functions, Vt \u2248 V \u03c0 and Qt \u2248 Q \u03c0\n, are parameterized by the parameter vector \u03b8t, then the gradient-descent learning update of the parameters is computed by\n\u03b8t+1 = \u03b8t + \u03b1\u03b4tet, (1)\nwhere the TD-error, \u03b4t, is \u03b4t = rt + \u03b3Vt(st+1)\u2212 Vt(st) (2)\nfor TD(\u03bb) and \u03b4t = rt + \u03b3Qt(st+1, at+1)\u2212Qt(st, at) (3)\nfor Sarsa(\u03bb). The eligibility trace vector, et, is\net = \u03b3\u03bbet\u22121 +\u2207\u03b8tVt(st), e0 = 0, (4)\nfor TD(\u03bb) and et = \u03b3\u03bbet\u22121 +\u2207\u03b8tQt(st, at), e0 = 0, (5)\nfor Sarsa(\u03bb). Here, st is the state at time t, at is the action selected at time t, rt is the reward for taking action at in state st, \u03b1 is the learning rate, \u03b3 is the discount factor of future rewards, \u03bb is the trace-decay rate, and \u2207\u03b8tVt and \u2207\u03b8tQt are the vectors of partial derivatives of the function approximators with respect to each component of \u03b8t."}, {"heading": "2.2 Sigmoid-weighted Linear Units", "text": "In our earlier work (Elfwing et al., 2016), we proposed the EE-RBM as a function approximator in reinforcement learning. In the case of state-value based learning, given a state vector s, an EE-RBM approximates the state-value function V by the negative expected energy of an RBM (Smolensky, 1986; Freund and Haussler, 1992; Hinton, 2002) network:\nV (s) = \u2211\nk\nzk\u03c3(zk) + \u2211\ni\nbisi, (6)\nzk = \u2211\ni\nwiksi + bk, (7)\n\u03c3(x) = 1\n1 + e\u2212x . (8)\nHere, zk is the input to hidden unit k, \u03c3(\u00b7) is the sigmoid function, bi is the bias weight for input unit si, wik is the weight connecting state si and hidden unit k, and bk is the bias weight for hidden unit k. Note that Equation 6 can be regarded as the output of a onehidden layer feedforward neural network with hidden unit activations computed by zk\u03c3(zk) and with uniform output weights of one.\nIn this study, motivated by the high performance of the EE-RBM in both the classification (Elfwing et al., 2015) and the reinforcement learning (Elfwing et al., 2016) domains, we propose the SiLU as an activation function for neural network function approximation\nin reinforcement learning. The activation ak of a SiLU k for an input vector s is computed by the sigmoid function multiplied by its input:\nak(s) = zk\u03c3(zk). (9)\nFor zk-values of large magnitude, the activation of the SiLU is approximately equal to the activation of the ReLU (see left panel in Figure 1), i.e., the activation is approximately equal to zero for large negative zk-values and approximately equal to zk for large positive zk-values. Unlike the ReLU (and other commonly used activation units such as sigmoid and tanh units), the activation of the SiLU is not monotonically increasing. Instead, it has a global minimum value of approximately \u22120.28 for zk \u2248 \u22121.28. An attractive feature of the SiLU is that it has a self-stabilizing property, which we demonstrated experimentally in Elfwing et al. (2015). The global minimum, where the derivative is zero, functions as a \u201csoft floor\u201d on the weights that serves as an implicit regularizer that inhibits the learning of weights of large magnitudes.\nWe propose an additional activation function for neural network function approximation: the dSiLU. The activation of the dSiLU is computed by the derivative of the SiLU:\nak(s) = \u03c3(zk) (1 + zk(1\u2212 \u03c3(zk))) . (10)\nThe activation function of the dSiLU looks like an steeper and \u201covershooting\u201d sigmoid function (see right panel in Figure 1). The dSiLU has a maximum value of approximately 1.1 and a minimum value of approximately \u22120.1 for zk \u2248 \u00b12.4, i.e., the solutions to the equation zk = \u2212 log ((zk \u2212 2)/(zk + 2)).\nThe derivative of the activation function of the SiLU, used for gradient-descent learning updates of the neural network weight parameters (see Equations 4 and 5), is given by\n\u2207wikak(s) = \u03c3(zk) (1 + zk(1\u2212 \u03c3(zk))) si, (11)\nand the derivative of the activation function of the dSiLU is given by\n\u2207wikak(s) = \u03c3(zk)(1\u2212 \u03c3(zk))(2 + zk(1\u2212 \u03c3(zk))\u2212 zk\u03c3(zk))si. (12)"}, {"heading": "2.3 Action selection", "text": "We use softmax action selection with a Boltzmann distribution in all experiments. For Sarsa(\u03bb), the probability to select action a in state s is defined as\n\u03c0(a|s) = exp(Q(s, a)/\u03c4 )\u2211 b exp(Q(s, b)/\u03c4 ) . (13)\nFor the model-based TD(\u03bb) algorithm, we select an action a in state s that leads to the next state s\u2032 with a probability defined as\n\u03c0(a|s) = exp(V (f(s, a))/\u03c4 )\u2211 b exp(V (f(s, b))/\u03c4 ) . (14)\nHere, f(s, a) returns the next state s\u2032 according to the state transition dynamics and \u03c4 is the temperature that controls the trade-off between exploration and exploitation. We used hyperbolic annealing of the temperature and the temperature was decreased after every episode i:\n\u03c4(i) = \u03c40\n1 + \u03c4ki . (15)\nHere, \u03c40 is the initial temperature and \u03c4k controls the rate of annealing."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 SZ-Tetris", "text": "Szita and Szepesv\u00e1ri (2010) proposed stochastic SZ-Tetris (Burgiel, 1997) as a benchmark for reinforcement learning that preserves the core challenges of standard Tetris but allows faster evaluation of different strategies due to shorter episodes by removing easier tetrominoes. Stochastic SZ-Tetris is played on a board of standard Tetris size with a width of 10 and a height of 20. In each time step, either an S-shaped tetromino or a Z-shaped tetromino appears with equal probability. The agent selects a rotation (lying or standing) and a horizontal position within the board. In total, there are 17 possible actions for each tetromino (9 standing and 8 lying horizontal positions). After the action selection, the tetromino drops down the board, stopping when it hits another tetromino or the bottom of the board. If a row is completed, then it disappears. The agent gets a score of +1 point for each completed row. An episode ends when a tetromino does not fit within the board.\nFor an alternating sequence of S-shaped and Z-shaped tetrominoes, the upper bound on the episode length in SZ-Tetris is 69 600 fallen pieces (Burgiel, 1997) (corresponding to a score of 27 840 points), but the maximum episode length is probably much shorter, maybe\na few thousands (Szita and Szepesv\u00e1ri, 2010). That means that to evaluate a good strategy SZ-Tetris requires at least five orders of magnitude less computation than standard Tetris.\nThe standard learning approach for Tetris has been to use a model-based setting and define the evaluation function or state-value function as the linear combination of hand-coded features. Value-based reinforcement learning algorithms have a lousy track record using this approach. In regular Tetris, their reported performance levels are many magnitudes lower than black-box methods such as the cross-entropy (CE) method and evolutionary approaches. In stochastic SZ-Tetris, the reported scores for a wide variety of reinforcement learning algorithms are either approximately zero (Szita and Szepesv\u00e1ri, 2010) or in the single digits 1.\nValue-based reinforcement learning has had better success in stochastic SZ-Tetris when using non-linear neural network based function approximators. Fau\u00dfer and Schwenker (2013) achieved a score of about 130 points using a shallow neural network function approximator with sigmoid hidden units. They improved the result to about 150 points by using an ensemble approach consisting of ten neural networks. We achieved an average score of about 200 points using three different neural network function approximators: an EE-RBM, a free energy RBM, and a standard neural network with sigmoid hidden units (Elfwing et al., 2016). Jaskowski et al. (2015) achieved the current state-of-the-art results using systematic n-tuple networks as function approximators: average scores of 220 and 218 points achieved by the evolutionary VD-CMA-ES method and TD-learning, respectively, and the best mean score in a single run of 295 points achieved by TD-learning.\nIn this study, we compare the performance of different hidden activation units in two learning settings: 1) shallow network agents with one hidden layer using hand-coded state features and 2) deep network agents using raw board configurations as states, i.e., a state node is set to one if the corresponding board cell is occupied by a tetromino and set to zero otherwise.\nIn the setting with state features, we trained shallow network agents with SiLU, ReLU, dSiLU, and sigmoid hidden units, using the TD(\u03bb) algorithm and softmax action selection. We used the same experimental setup as used in our earlier work (Elfwing et al., 2016). The networks consisted of one hidden layer with 50 hidden units and a linear output layer. The features were similar to the original 21 features proposed by Bertsekas and Ioffe (1996), except for not including the maximum column height and using the differences in column heights instead of the absolute differences. The length of the binary state vector was 460. The shallow network agents were trained for 200,000 episodes and the experiments were repeated for ten separate runs for each type of activation unit.\nIn the deep reinforcement learning setting, we used a deep network architecture consisting of two convolutional layers with 15 and 50 filters of size 5 \u00d7 5 using a stride of 1, a fullyconnected layer with 250 units, and a linear output layer. Both convolutional layers were followed by max-pooling layers with pooling windows of size 3\u00d73 using a stride of 2. The deep network agents were also trained using the TD(\u03bb) algorithm and softmax action selection. We trained three types of deep networks with: 1) SiLUs in both the convolutional and\n1http://barbados2011.rl-community.org/program/SzitaTalk.pdf\nfully-connected layers (SiLU-SiLU); 2) ReLUs in both the convolutional and fully-connected layers (ReLU-ReLU); and 3) SiLUs in the convolutional layers and dSiLUs in the fullyconnected layer (SiLU-dSiLU). The deep network agents were trained for 200,000 episodes and the experiments were repeated for five separate runs for each type of network.\nWe used the following reward function (proposed by Fau\u00dfer and Schwenker (2013)):\nr(s) = e\u2212(number of holes in s)/33. (16)\nWe set \u03b3 to 0.99, \u03bb to 0.55, \u03c40 to 0.5, and \u03c4k to 0.00025. We used a rough grid-like search to find appropriate values of the learning rate \u03b1 and it was determined to be 0.001 for the four shallow network agents and 0.0001 for the three deep network agents.\nFigure 2 shows the average learning curves as well as learning curves for the individual runs for the shallow networks, Figure 3 shows the average learning curves for the deep networks, and the final results are summarized in Table 1. The results show significant differences (p < 0.0001) in final average score between all four shallow agents. The networks with bounded hidden units (dSiLU and sigmoid) outperformed the networks with unbounded units (SiLU and ReLU), the SiLU network outperformed the ReLU network, and the dSiLU network outperformed the sigmoid network. The final average score (best score) of 263 (320) points achieved by the dSiLU network agent is a new state-of-the-art score, improving the previous best performance by 43 (25) points or 20% (8%). In the deep learning setting, the SiLU-dSiLU network significantly (p < 0.0001) outperformed the other two networks and the average final score of 229 points is better than the previous state-of-the-art of 220 points. There was no significant difference (p = 0.32) between the final performance of the SiLU-SiLU network and the ReLU-ReLU network."}, {"heading": "3.2 10\u00d710 Tetris", "text": "The result achieved by the dSiLU network agent in stochastic SZ-Tetris is impressive, but we cannot compare the result with the methods that have achieved the highest performance levels in standard Tetris because those methods have not been applied to stochastic SZTetris. Furthermore, it is not feasible to apply our method to Tetris with a standard board height of 20, because of the prohibitively long learning time. The current state-of-the-art for a single run of an algorithm, achieved by the CBMPI algorithm (Gabillon et al., 2013; Scherrer et al., 2015), is a mean score of 51 million cleared lines. However, for the best\nmethods applied to Tetris, there are reported results for a smaller, 10\u00d710, Tetris board, and in this case, the learning time for our method is long, but not prohibitively so.\n10\u00d710 Tetris is played with the standard seven tetrominoes and the numbers of actions are 9 for the block-shaped tetromino, 17 for the S-, Z-, and stick-shaped tetrominoes, and 34 for the J-, L- and T-shaped tetrominoes. In each time step, the agent gets a score equal to the number of completed rows, with a maximum of +4 points that can only be achieved by the stick-shaped tetromino.\nWe trained a shallow neural network agent with dSiLU units in the hidden layer. To handle the more complex learning task, we increased the number of hidden units to 250 and the number of episodes to 400,000. We repeated the experiment for five separate runs. We used the same 20 state features as in the SZ-Tetris experiment, but the length of the binary state vector was reduced to 260 due to the smaller board size. The reward function was changed as follows for the same reason:\nr(s) = e\u2212(number of holes in s)/(33/2). (17)\nWe used the same values of the meta-parameters as in the stochastic SZ-Tetris experiment. Figure 4 shows The average learning curve in 10\u00d710 Tetris, as well as learning curves for the five separate runs. The dSiLU network agent reached an average score of 4,900 points over the final 10,000 episodes and the five separate runs, which is a new state-of-the-art in 10\u00d710 Tetris. The previous best average scores are 4,200 points achieved by the CBMPI algorithm, 3,400 points achieved by the DPI algorithm, and 3,000 points achieved by the CE method (Gabillon et al., 2013). The best individual run achieved a final mean score of\n5,300 points, which is also a new state-of-the-art, improving on the score of 5,000 points achieved by the CBMPI algorithm.\nIt is particularly impressive that the dSiLU network agent achieved its result using features similar to the original Bertsekas features. Using only the Bertsekas features, the CBMPI algorithm, the DPI algorithm, and the CE method could only achieve average scores of about 500 points (Gabillon et al., 2013). The CE method has achieved its best score by combining the Bertsekas features, the Dellacherie features (Fahey, 2003), and three original features (Thiery and Scherrer, 2009). The CBMPI algorithm achieved its best score using the same features as the CE method, except for using five original RBF height features instead of the Bertsekas features."}, {"heading": "3.3 Atari 2600 games", "text": "To further evaluate the use of value-based on-policy reinforcement learning with eligibility traces and softmax action selection in high-dimensional state space domains, as well as the use of SiLU and dSiLU units, we applied Sarsa(\u03bb) with a deep convolution neural network function approximator in the Atari 2600 domain using the Arcade Learning Environment (Bellemare et al., 2013). Based on the results for the deep networks in SZ-Tetris, we used SiLU-dSiLU networks with SiLU units in the convolutional layers and dSiLU units in the fully-connected layer. To limit the number of games and prevent a biased selection of the games, we selected the 12 games played by DQN (Mnih et al., 2015) that started with the letters \u2019A\u2019 and \u2019B\u2019: Alien, Amidar, Assault, Asterix, Asteroids, Atlantis, Bank Heist, Battle Zone, Beam Rider, Bowling, Boxing, and Breakout.\nWe used a similar experimental setup as Mnih et al. (2015). We pre-processed the raw 210\u00d7160 Atari 2600 RGB frames by extracting the luminance channel, taking the maximum pixel values over consecutive frames to prevent flickering, and then downsampling the grayscale images to 105\u00d780. For computational reasons, we used a smaller network architecture. Instead of three convolutional layers, we used two with half the number of filters, each followed by a max-pooling layer. The input to the network was a 105\u00d780\u00d72 image consisting of the current and the fourth previous pre-processed frame. As we used frame skipping where actions were selected every fourth frame and repeated for the next four frames, we only needed to apply pre-processing to every fourth frame. The first convolutional layer had 16 filters of size 8\u00d78 with a stride of 4. The second convolutional layer had 32 filters of size 4\u00d74 with a stride of 2. The max-pooling layers had pooling windows of size 3\u00d73 with a stride of 2. The convolutional layers were followed by a fully-connected hidden layer with 512 dSiLU units and a fully-connected linear output layer with 4 to 18 output (or action-value) units, depending on the number of valid actions in the considered game. We selected meta-parameters by a preliminary search in the Alien, Amidar and Assault games and used the same values for all 12 games: \u03b1: 0.001, \u03b3: 0.99, \u03bb: 0.8, \u03c40: 0.5, and \u03c4k: 0.0005. As in Mnih et al. (2015), we clipped the rewards to be between \u22121 and +1, but we did not clip the values of the TD-errors.\nIn each of the 12 Atari games, we trained a SiLU-dSiLU agent for 200,000 episodes and the experiments were repeated for two separate runs. An episode started with up to 30\n\u2019do nothing\u2019 actions (no-op condition) and it was played until the end of the game or for a maximum of 18,000 frames (i.e., 5 minutes). Figure 5 shows the average learning curves, as well as the learning curves for the two separate runs, in the 12 Atari 2600 games. Table 2 summarizes the results as the final mean scores computed over the final 100 episodes for the two separate runs, and the best mean scores computed as average scores of the highest mean\nscores (over every 100 episodes) achieved in each of the two runs. The table also shows the reported best mean scores for single runs of DQN computed over 30 episodes, average scores over five separate runs of the Gorila implementation of DQN (Nair et al., 2015) computed over 30 episodes, and single runs of double DQN (van Hasselt et al., 2015) computed over 100 episodes. The last two rows of the table shows summary statistics over the 12 games, which were obtained by computing the mean and the median of the DQN normalized scores:\nScoreDQN_normalized = Scoreagent \u2212 Scorerandom ScoreDQN \u2212 Scorerandom\nHere, Scorerandom is the score achieved by a random agent in Mnih et al. (2015).\nThe results clearly show that our SiLU-dSiLU agent outperformed the other agents, improving the mean (median) DQN normalized best mean score from 127% (105%) achieved by double DQN to 332% (125%). The SiLU-dSiLU agents achieved the highest best mean score in 6 out of the 12 games and only performed much worse than the other 3 agents in one game, Breakout, where the learning never took off during the 200,000 episodes of training (see Figure 5). The performance was especially impressive in the Asterix (score of 100,322) and Asteroids (score of 10,614) games, which improved the best mean performance achieved by the second-best agent by 562% and 552%, respectively."}, {"heading": "4 Analysis", "text": ""}, {"heading": "4.1 Value estimation", "text": "First, we investigate the ability of TD(\u03bb) and Sarsa(\u03bb) to accurately estimate discounted returns:\nRt =\nT\u2212t\u2211\nk=0\n\u03b3krt+k.\nHere T is the length of an episode. The reason for doing this is that van Hasselt et al. (2015) showed that the double DQN algorithm improved the performance of DQN in Atari 2600 games by reducing the overestimation of the action values. It is known (Thrun and Schwartz, 1993; van Hasselt, 2010) that Q-learning based algorithms, such as DQN, can overestimate action values due to the max operator, which is used in the computation of the learning targets. TD(\u03bb) and Sarsa(\u03bb) do not use the max operator to compute the learning targets and they should therefore not suffer from this problem.\nFigure 6 shows that for episodes of average (or expected) length the best dSiLU network agent in SZ-Tetris learned good estimates of the discounted returns, both along the episodes (left panel) and as measured by the normalized sum of differences between V (st) and Rt (right panel):\n1\nT \u2211T t=1 (V (st)\u2212Rt) .\nThe linear fit of the normalized sum of differences data for 1,000 episodes gives a small underestimation (-0.43) for an episode of average length (866 time steps). The V (st)-values overestimated the discounted returns for short episodes and underestimated the discounted\nreturns for long episodes (especially in the middle part of the episodes), which is accurate since the episodes ended earlier and later, respectively, than were expected.\nFigure 7 shows typical examples of learned action values and discounted returns along episodes where the best SiLU-dSiLU agents in Asterix (score of 108,500) and Asteroids (score of 22,500) successfully played for the full 18,000 frames (i.e., 4,500 time steps since the agents acted every fourth frame). In both games, with the exception of a few smaller parts, the learned action values matched the discounted returns very well along the whole episodes. The normalized sums of differences (absolute differences) were 0.59 (1.05) in the Asterix episode and \u22120.23 (1.28) in the Asteroids episode. In both games, the agents overestimated action values at the end of the episodes. However, this is an artifact of that an episode ended after a maximum of 4,500 time steps, which the agents could not predict. Videos of the corresponding learned behaviors in Asterix and Asteroids can be found at http://www.cns.atr.jp/~elfwing/videos/asterix_deep_SiL.mov and http://www.cns.atr.jp/~elfwing/videos/asteroids_deep_SiL.mov."}, {"heading": "4.2 Action selection", "text": "Second, we investigate the importance of softmax action selection in the games where our proposed agents performed particularly well. Almost all deep reinforcement learning algo-\nrithms that have been used in the Atari 2600 domain have used \u03b5-greedy action selection (one exception is the asynchronous advantage actor-critic method, A3C, which used softmax output units for the actor (Mnih et al., 2016)). One drawback of \u03b5-greedy selection is that it selects all actions with equal probability when exploring, which can lead to poor learning outcomes in tasks where the worst actions have very bad consequences. This is clearly the case in both Tetris games and in the Asterix and Asteroids games. In each state in Tetris, many, and often most, actions will create holes, which are difficult (especially in SZ-Tetris) to remove. In the Asterix game, random exploratory actions can kill Asterix if executed when Cacofonix\u2019s deadly lyres are passing. In the Asteroids game, one of the actions sends the spaceship into hyperspace and makes it reappear in a random location, which has the risk of the spaceship self-destructing or of destroying it by appearing on top of an asteroid.\nWe compared softmax action selection (\u03c4 set to the final values) and \u03b5-greedy action selection with \u03b5 set to 0, 0.001, 0.01, and 0.05 for the best dSiLU network agent in SZTetris and the best SiLU-dSiLU agents in the Asterix and Asteroids games. The results (see Table 3) clearly show that \u03b5-greedy action selection with \u03b5 set to 0.05, as used for evaluation by DQN, is not suitable for these games. The scores were only 4% to 10% of the scores for softmax selection. The negative effects of random exploration were largest in Asteroid and SZ-Tetris. Even when \u03b5 was set as low as 0.001 and the agent performed only 2.1 exploratory actions per episode in Asteroids and 0.59 in SZ-Tetris, the mean scores were reduced by 30%\nand 20% (26% and 22%), respectively, compared with softmax selection (\u03b5 = 0)."}, {"heading": "5 Conclusions", "text": "In this study, we proposed SiLU and dSiLU as activation functions for neural network function approximation in reinforcement learning. We demonstrated in stochastic SZ-Tetris that SiLUs significantly outperformed ReLUs, and that dSiLUs significantly outperformed sigmoid units. The best agent, the dSiLU network agent, achieved new state-of-the-art results in both stochastic SZ-Tetris and 10\u00d710 Tetris. In the Atari 2600 domain, a deep Sarsa(\u03bb) agent with SiLUs in the convolutional layers and dSiLUs in the fully-connected hidden layer outperformed DQN and double DQN, as measured by mean and median DQN normalized scores.\nAn additional purpose of this study was to demonstrate that a more traditional approach of using on-policy learning with eligibility traces and softmax selection (i.e., basically a \u201ctextbook\u201d version of a reinforcement learning agent but with non-linear neural network function approximators) can be competitive with the approach used by DQN. This means that there is a lot of room for improvements, by, e.g., using, as DQN, a separate target network, but also by using more recent advances such as the dueling architecture (Wang et al., 2016) for more accurate estimates of the action values and asynchronous learning by multiple agents in parallel (Mnih et al., 2016)."}, {"heading": "Acknowledgments", "text": "This work was supported by the project commissioned by the New Energy and Industrial Technology Development Organization (NEDO), JSPS KAKENHI grant 16K12504, and Okinawa Institute of Science and Technology Graduate University research support to KD."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Temporal differences based policy iteration and applications in neuro-dynamic programming", "author": ["D.P. Bertsekas", "S. Ioffe"], "venue": "Technical Report LIDS-P-2349,", "citeRegEx": "Bertsekas and Ioffe,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas and Ioffe", "year": 1996}, {"title": "How to lose at Tetris", "author": ["H. Burgiel"], "venue": "Mathematical Gazette,", "citeRegEx": "Burgiel,? \\Q1997\\E", "shortCiteRegEx": "Burgiel", "year": 1997}, {"title": "Expected energy-based restricted boltzmann machine for classification", "author": ["S. Elfwing", "E. Uchibe", "K. Doya"], "venue": "Neural Networks,", "citeRegEx": "Elfwing et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Elfwing et al\\.", "year": 2015}, {"title": "From free energy to expected energy", "author": ["S. Elfwing", "E. Uchibe", "K. Doya"], "venue": null, "citeRegEx": "Elfwing et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elfwing et al\\.", "year": 2016}, {"title": "Neural network ensembles in reinforcement learning", "author": ["Online", "F. Schwenker"], "venue": null, "citeRegEx": "Online et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Online et al\\.", "year": 2013}, {"title": "Unsupervised learning of distributions on binary vectors", "author": ["Y. Freund", "D. Haussler"], "venue": "Neural Processing Letters,", "citeRegEx": "Freund and Haussler,? \\Q1992\\E", "shortCiteRegEx": "Freund and Haussler", "year": 1992}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["D. Silver"], "venue": null, "citeRegEx": "Silver,? \\Q2015\\E", "shortCiteRegEx": "Silver", "year": 2015}, {"title": "On-line Q-learning using connectionist systems", "author": ["G.A. Rummery", "M. Niranjan"], "venue": "Technical Report CUED/F-INFENG/TR 166,", "citeRegEx": "Rummery and Niranjan,? \\Q1994\\E", "shortCiteRegEx": "Rummery and Niranjan", "year": 1994}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Schaul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "Approximate modified policy iteration and its application to the game of tetris", "author": ["B. Scherrer", "M. Ghavamzadeh", "V. Gabillon", "B. Lesner", "M. Geist"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Scherrer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Scherrer et al\\.", "year": 2015}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume", "citeRegEx": "Smolensky,? \\Q1986\\E", "shortCiteRegEx": "Smolensky", "year": 1986}, {"title": "Learning to predict by the method of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton,? \\Q1988\\E", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["R.S. Sutton"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Sutton,? \\Q1996\\E", "shortCiteRegEx": "Sutton", "year": 1996}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "SZ-Tetris as a benchmark for studying key problems of reinforcement learning", "author": ["I. Szita", "C. Szepesv\u00e1ri"], "venue": "In ICML 2010 workshop on machine learning and games", "citeRegEx": "Szita and Szepesv\u00e1ri,? \\Q2010\\E", "shortCiteRegEx": "Szita and Szepesv\u00e1ri", "year": 2010}, {"title": "Td-gammon, a self-teaching backgammon program, achieves masterlevel play", "author": ["G. Tesauro"], "venue": "Neural Computation,", "citeRegEx": "Tesauro,? \\Q1994\\E", "shortCiteRegEx": "Tesauro", "year": 1994}, {"title": "Improvements on learning tetris with cross entropy", "author": ["C. Thiery", "B. Scherrer"], "venue": "International Computer Games Association Journal,", "citeRegEx": "Thiery and Scherrer,? \\Q2009\\E", "shortCiteRegEx": "Thiery and Scherrer", "year": 2009}, {"title": "Issues in using function approximation for reinforcement learning", "author": ["S. Thrun", "A. Schwartz"], "venue": "In Proceedings of the 1993 Connectionist Models Summer School,", "citeRegEx": "Thrun and Schwartz,? \\Q1993\\E", "shortCiteRegEx": "Thrun and Schwartz", "year": 1993}, {"title": "Double q-learning", "author": ["H. van Hasselt"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Hasselt,? \\Q2010\\E", "shortCiteRegEx": "Hasselt", "year": 2010}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": null, "citeRegEx": "Hasselt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "T. Schaul", "M. Hessel", "H. van Hasselt", "M. Lanctot", "N. de Freitas"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML2016),", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "Neural networks have enjoyed a renaissance as function approximators in reinforcement learning (Sutton and Barto, 1998) in recent years.", "startOffset": 95, "endOffset": 119}, {"referenceID": 11, "context": "Neural networks have enjoyed a renaissance as function approximators in reinforcement learning (Sutton and Barto, 1998) in recent years. The DQN algorithm (Mnih et al., 2015), which combines Q-learning with a deep neural network, experience replay, and a separate target network, achieved human-level performance in many Atari 2600 games. Since the development of the DQN algorithm, there have been several proposed improvements, both to DQN specifically and deep reinforcement learning in general. Van Hasselt et al. (2015) proposed double DQN to reduce overestimation of the action values in DQN and Schaul et al.", "startOffset": 96, "endOffset": 525}, {"referenceID": 9, "context": "(2015) proposed double DQN to reduce overestimation of the action values in DQN and Schaul et al. (2016) developed a framework for more efficient replay by prioritizing experiences of more important state transitions.", "startOffset": 84, "endOffset": 105}, {"referenceID": 9, "context": "(2015) proposed double DQN to reduce overestimation of the action values in DQN and Schaul et al. (2016) developed a framework for more efficient replay by prioritizing experiences of more important state transitions. Wang et al. (2016) proposed the dueling network architecture", "startOffset": 84, "endOffset": 237}, {"referenceID": 12, "context": "Using a neural network function approximator and TD(\u03bb) learning (Sutton, 1988), TD-Gammon reached near top-level human performance in backgammon, which to this day remains one of the most impressive applications of reinforcement learning.", "startOffset": 64, "endOffset": 78}, {"referenceID": 3, "context": "First, motivated by the high performance of the expected energy restricted Boltzmann machine (EE-RBM) in our earlier studies (Elfwing et al., 2015, 2016), we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input and it looks like a continuous and \u201cundershooting\u201d version of the linear rectifier unit (ReLU) (Hahnloser et al., 2000). The activation of the dSiLU looks like steeper and \u201covershooting\u201d version of the sigmoid function. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection with simple annealing can be competitive with DQN, without the need for a separate target network. Our approach is something of a throwback to the approach used by Tesauro (1994) to develop TD-Gammon more than two decades ago.", "startOffset": 126, "endOffset": 989}, {"referenceID": 12, "context": "In this study, we use two reinforcement learning algorithms: TD(\u03bb) (Sutton, 1988) and Sarsa(\u03bb) (Rummery and Niranjan, 1994; Sutton, 1996).", "startOffset": 67, "endOffset": 81}, {"referenceID": 8, "context": "In this study, we use two reinforcement learning algorithms: TD(\u03bb) (Sutton, 1988) and Sarsa(\u03bb) (Rummery and Niranjan, 1994; Sutton, 1996).", "startOffset": 95, "endOffset": 137}, {"referenceID": 13, "context": "In this study, we use two reinforcement learning algorithms: TD(\u03bb) (Sutton, 1988) and Sarsa(\u03bb) (Rummery and Niranjan, 1994; Sutton, 1996).", "startOffset": 95, "endOffset": 137}, {"referenceID": 4, "context": "In our earlier work (Elfwing et al., 2016), we proposed the EE-RBM as a function approximator in reinforcement learning.", "startOffset": 20, "endOffset": 42}, {"referenceID": 11, "context": "In the case of state-value based learning, given a state vector s, an EE-RBM approximates the state-value function V by the negative expected energy of an RBM (Smolensky, 1986; Freund and Haussler, 1992; Hinton, 2002) network:", "startOffset": 159, "endOffset": 217}, {"referenceID": 6, "context": "In the case of state-value based learning, given a state vector s, an EE-RBM approximates the state-value function V by the negative expected energy of an RBM (Smolensky, 1986; Freund and Haussler, 1992; Hinton, 2002) network:", "startOffset": 159, "endOffset": 217}, {"referenceID": 3, "context": "In this study, motivated by the high performance of the EE-RBM in both the classification (Elfwing et al., 2015) and the reinforcement learning (Elfwing et al.", "startOffset": 90, "endOffset": 112}, {"referenceID": 4, "context": ", 2015) and the reinforcement learning (Elfwing et al., 2016) domains, we propose the SiLU as an activation function for neural network function approximation", "startOffset": 39, "endOffset": 61}, {"referenceID": 3, "context": "An attractive feature of the SiLU is that it has a self-stabilizing property, which we demonstrated experimentally in Elfwing et al. (2015). The global minimum, where the derivative is zero, functions as a \u201csoft floor\u201d on the weights that serves as an implicit regularizer that inhibits the learning of weights of large magnitudes.", "startOffset": 118, "endOffset": 140}, {"referenceID": 2, "context": "Szita and Szepesv\u00e1ri (2010) proposed stochastic SZ-Tetris (Burgiel, 1997) as a benchmark for reinforcement learning that preserves the core challenges of standard Tetris but allows faster evaluation of different strategies due to shorter episodes by removing easier tetrominoes.", "startOffset": 58, "endOffset": 73}, {"referenceID": 2, "context": "For an alternating sequence of S-shaped and Z-shaped tetrominoes, the upper bound on the episode length in SZ-Tetris is 69 600 fallen pieces (Burgiel, 1997) (corresponding to a score of 27 840 points), but the maximum episode length is probably much shorter, maybe", "startOffset": 141, "endOffset": 156}, {"referenceID": 15, "context": "a few thousands (Szita and Szepesv\u00e1ri, 2010).", "startOffset": 16, "endOffset": 44}, {"referenceID": 15, "context": "In stochastic SZ-Tetris, the reported scores for a wide variety of reinforcement learning algorithms are either approximately zero (Szita and Szepesv\u00e1ri, 2010) or in the single digits .", "startOffset": 131, "endOffset": 159}, {"referenceID": 4, "context": "We achieved an average score of about 200 points using three different neural network function approximators: an EE-RBM, a free energy RBM, and a standard neural network with sigmoid hidden units (Elfwing et al., 2016).", "startOffset": 196, "endOffset": 218}, {"referenceID": 4, "context": "We used the same experimental setup as used in our earlier work (Elfwing et al., 2016).", "startOffset": 64, "endOffset": 86}, {"referenceID": 12, "context": "a few thousands (Szita and Szepesv\u00e1ri, 2010). That means that to evaluate a good strategy SZ-Tetris requires at least five orders of magnitude less computation than standard Tetris. The standard learning approach for Tetris has been to use a model-based setting and define the evaluation function or state-value function as the linear combination of hand-coded features. Value-based reinforcement learning algorithms have a lousy track record using this approach. In regular Tetris, their reported performance levels are many magnitudes lower than black-box methods such as the cross-entropy (CE) method and evolutionary approaches. In stochastic SZ-Tetris, the reported scores for a wide variety of reinforcement learning algorithms are either approximately zero (Szita and Szepesv\u00e1ri, 2010) or in the single digits . Value-based reinforcement learning has had better success in stochastic SZ-Tetris when using non-linear neural network based function approximators. Fau\u00dfer and Schwenker (2013) achieved a score of about 130 points using a shallow neural network function approximator with sigmoid hidden units.", "startOffset": 17, "endOffset": 996}, {"referenceID": 2, "context": "We achieved an average score of about 200 points using three different neural network function approximators: an EE-RBM, a free energy RBM, and a standard neural network with sigmoid hidden units (Elfwing et al., 2016). Jaskowski et al. (2015) achieved the current state-of-the-art results using systematic n-tuple networks as function approximators: average scores of 220 and 218 points achieved by the evolutionary VD-CMA-ES method and TD-learning, respectively, and the best mean score in a single run of 295 points achieved by TD-learning.", "startOffset": 197, "endOffset": 244}, {"referenceID": 1, "context": "The features were similar to the original 21 features proposed by Bertsekas and Ioffe (1996), except for not including the maximum column height and using the differences in column heights instead of the absolute differences.", "startOffset": 66, "endOffset": 93}, {"referenceID": 10, "context": "The current state-of-the-art for a single run of an algorithm, achieved by the CBMPI algorithm (Gabillon et al., 2013; Scherrer et al., 2015), is a mean score of 51 million cleared lines.", "startOffset": 95, "endOffset": 141}, {"referenceID": 17, "context": "The CE method has achieved its best score by combining the Bertsekas features, the Dellacherie features (Fahey, 2003), and three original features (Thiery and Scherrer, 2009).", "startOffset": 147, "endOffset": 174}, {"referenceID": 0, "context": "To further evaluate the use of value-based on-policy reinforcement learning with eligibility traces and softmax action selection in high-dimensional state space domains, as well as the use of SiLU and dSiLU units, we applied Sarsa(\u03bb) with a deep convolution neural network function approximator in the Atari 2600 domain using the Arcade Learning Environment (Bellemare et al., 2013).", "startOffset": 358, "endOffset": 382}, {"referenceID": 0, "context": "To further evaluate the use of value-based on-policy reinforcement learning with eligibility traces and softmax action selection in high-dimensional state space domains, as well as the use of SiLU and dSiLU units, we applied Sarsa(\u03bb) with a deep convolution neural network function approximator in the Atari 2600 domain using the Arcade Learning Environment (Bellemare et al., 2013). Based on the results for the deep networks in SZ-Tetris, we used SiLU-dSiLU networks with SiLU units in the convolutional layers and dSiLU units in the fully-connected layer. To limit the number of games and prevent a biased selection of the games, we selected the 12 games played by DQN (Mnih et al., 2015) that started with the letters \u2019A\u2019 and \u2019B\u2019: Alien, Amidar, Assault, Asterix, Asteroids, Atlantis, Bank Heist, Battle Zone, Beam Rider, Bowling, Boxing, and Breakout. We used a similar experimental setup as Mnih et al. (2015). We pre-processed the raw 210\u00d7160 Atari 2600 RGB frames by extracting the luminance channel, taking the maximum pixel values over consecutive frames to prevent flickering, and then downsampling the grayscale images to 105\u00d780.", "startOffset": 359, "endOffset": 916}, {"referenceID": 0, "context": "To further evaluate the use of value-based on-policy reinforcement learning with eligibility traces and softmax action selection in high-dimensional state space domains, as well as the use of SiLU and dSiLU units, we applied Sarsa(\u03bb) with a deep convolution neural network function approximator in the Atari 2600 domain using the Arcade Learning Environment (Bellemare et al., 2013). Based on the results for the deep networks in SZ-Tetris, we used SiLU-dSiLU networks with SiLU units in the convolutional layers and dSiLU units in the fully-connected layer. To limit the number of games and prevent a biased selection of the games, we selected the 12 games played by DQN (Mnih et al., 2015) that started with the letters \u2019A\u2019 and \u2019B\u2019: Alien, Amidar, Assault, Asterix, Asteroids, Atlantis, Bank Heist, Battle Zone, Beam Rider, Bowling, Boxing, and Breakout. We used a similar experimental setup as Mnih et al. (2015). We pre-processed the raw 210\u00d7160 Atari 2600 RGB frames by extracting the luminance channel, taking the maximum pixel values over consecutive frames to prevent flickering, and then downsampling the grayscale images to 105\u00d780. For computational reasons, we used a smaller network architecture. Instead of three convolutional layers, we used two with half the number of filters, each followed by a max-pooling layer. The input to the network was a 105\u00d780\u00d72 image consisting of the current and the fourth previous pre-processed frame. As we used frame skipping where actions were selected every fourth frame and repeated for the next four frames, we only needed to apply pre-processing to every fourth frame. The first convolutional layer had 16 filters of size 8\u00d78 with a stride of 4. The second convolutional layer had 32 filters of size 4\u00d74 with a stride of 2. The max-pooling layers had pooling windows of size 3\u00d73 with a stride of 2. The convolutional layers were followed by a fully-connected hidden layer with 512 dSiLU units and a fully-connected linear output layer with 4 to 18 output (or action-value) units, depending on the number of valid actions in the considered game. We selected meta-parameters by a preliminary search in the Alien, Amidar and Assault games and used the same values for all 12 games: \u03b1: 0.001, \u03b3: 0.99, \u03bb: 0.8, \u03c40: 0.5, and \u03c4k: 0.0005. As in Mnih et al. (2015), we clipped the rewards to be between \u22121 and +1, but we did not clip the values of the TD-errors.", "startOffset": 359, "endOffset": 2309}, {"referenceID": 18, "context": "It is known (Thrun and Schwartz, 1993; van Hasselt, 2010) that Q-learning based algorithms, such as DQN, can overestimate action values due to the max operator, which is used in the computation of the learning targets.", "startOffset": 12, "endOffset": 57}, {"referenceID": 18, "context": "The reason for doing this is that van Hasselt et al. (2015) showed that the double DQN algorithm improved the performance of DQN in Atari 2600 games by reducing the overestimation of the action values.", "startOffset": 38, "endOffset": 60}, {"referenceID": 21, "context": ", using, as DQN, a separate target network, but also by using more recent advances such as the dueling architecture (Wang et al., 2016) for more accurate estimates of the action values and asynchronous learning by multiple agents in parallel (Mnih et al.", "startOffset": 116, "endOffset": 135}], "year": 2017, "abstractText": "In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Tesauro\u2019s TD-Gammon achieved near toplevel human performance in backgammon, the deep reinforcement learning algorithm DQN achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear unit (SiLU) and its derivative function (dSiLU). The activation of the SiLU is computed by the sigmoid function multiplied by its input. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection with simple annealing can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, first, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10\u00d710 board, using TD(\u03bb) learning and shallow dSiLU network agents, and, then, by outperforming DQN in the Atari 2600 domain by using a deep Sarsa(\u03bb) agent with SiLU and dSiLU hidden units.", "creator": "LaTeX with hyperref package"}}}