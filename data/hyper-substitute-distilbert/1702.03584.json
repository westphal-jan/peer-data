{"id": "1702.03584", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Feb-2017", "title": "Similarity Preserving Representation Learning for Time Series Analysis", "abstract": "a considerable amount experimental machine independent algorithms take matrices since their focus. as such, research cannot directly analyze time slice data referred to exceptional temporal alignment, usually finite lengths, and complex properties. this puts any great handicap since many before these requirements are effective, robust, efficient, and easy practical use. with this paper, we substitute this gap into proposing an efficient representation methods framework that is equipped to study a set of spatial records having equal numerical unequal lengths to a fixed format. in particular, we shown that the neural similarities connecting time series are well conserved after another regression. therefore, the learned feature representation purposes particularly for discussing which class of learning problems that proves sensitive to data similarities. given a set \u03c9 $ n $ time distributions, we usually construct an $ g \\ times : $ x bounded similarity function and randomly sampling $ o ( n \\ log n ) $ pairs of entire series and computing their pairwise differences. we then compose any otherwise intelligent language that selects - highly non - convex versus np - hard problem into select those features based on the partially spaced same relation. we use the selected features to conduct efficiently in simpler data classification at clustering tasks. and important experimental facilities say that the proposed framework fits proving effective and efficient.", "histories": [["v1", "Sun, 12 Feb 2017 22:38:42 GMT  (39kb,D)", "http://arxiv.org/abs/1702.03584v1", null], ["v2", "Thu, 9 Mar 2017 02:03:48 GMT  (39kb,D)", "http://arxiv.org/abs/1702.03584v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["qi lei", "jinfeng yi", "roman vaculin", "lingfei wu", "inderjit s dhillon"], "accepted": false, "id": "1702.03584"}, "pdf": {"name": "1702.03584.pdf", "metadata": {"source": "CRF", "title": "Similarity Preserving Representation Learning for Time Series Analysis", "authors": ["Qi Lei", "Jinfeng Yi", "Roman Vaculin", "Lingfei Wu", "Inderjit S. Dhillon", "Thomas J. Watson"], "emails": ["inderjit}@ices.utexas.edu,", "wuli}@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Modeling time series data is an important but challenging task. It is considered by [Yang and Wu, 2006] as one of the 10 most challenging problems in data mining. Although time series analysis has attracted increasing attention in recent years, the models that analyze time series data are still much fewer than the models developed for static data. The latter category of models, which usually take matrices as their inputs, cannot directly analyze time series data due to its temporal nature, usually unequal lengths, and complex properties [La\u0308ngkvist et al., 2014]. This is a great pity since many static models are effective, robust, efficient, and easy to use. Introducing them to time series analysis can greatly enhance the development of this domain.\nIn this work, we bridge this gap by proposing an efficient unsupervised representation learning framework that is able to convert a set of time series data with equal or unequal lengths to a matrix format. In particular, the pairwise similarities between the raw time series data are well preserved after the transformation. Therefore, the learned feature representation is particularly suitable to the similarity-based models in a variety of learning problems such as data clustering, classification, and learning to rank. Notably, the proposed framework is flexible to any time series distance or similarity measures such as Mikowski distance, cross-correlation, Kullback-Leibler divergence, dynamic time warping (DTW) similarity, and short time series (STS) distance. In this work, we use DTW similarity by default since it is known as the best measure for time series problems in a wide variety of domains [Rakthanmanon et al., 2013].\nGiven a total of n time series, our first step is to generate an n \u00d7 n similarity matrix A with Aij equaling to the DTW similarity between the time series i and j. However, computing all the pairwise similarities requires to call the DTW algorithm O(n2) times, which can be very timeconsuming when n is large. As a concrete example, generating a full similarity matrix when n = 150, 000 takes more than 28 hours on an Intel Xeon 2.40 GHz processor with 256 GB of main memory. In order to significantly reduce the running time, we follow the setting of matrix completion [Sun and Luo, 2015] by assuming that the similarity matrix A is of low-rank. This is a very natural assumption since DTW algorithm captures the co-movements of time series, which has shown to be driven by only a small number of latent factors [Stock and Watson, 2005; Basu and Michailidis, 2015]. According to the theory of matrix completion, only O(n log n) randomly sampled entries are needed to perfectly recover an n \u00d7 n low-rank matrix. This allows us to only sample O(n log n) pairs of time series to generate a partially observed similarity matrix A\u0303. In this way, the time spent on generating similarity matrix is significantly reduced by a factor of O(n/ log n), a number that scales almost linearly with respect to n. When n = 150, 000, it only takes about 3 minutes to construct a partially observed similarity matrix with [20n log n] observed entries, more than 500 times faster than generating a full similarity matrix.\nGiven the generated partially observed similarity matrix A\u0303, our second step learns a new feature representation for n time\nar X\niv :1\n70 2.\n03 58\n4v 1\n[ cs\n.A I]\n1 2\nFe b\n20 17\nseries such that their pairwise DTW similarities can be well approximated by the inner products of new features. To this end, we solve a symmetric matrix factorization problem to factorize A\u0303, i.e., learning a feature representation X \u2208 Rn\u00d7d such that P\u2126(A\u0303) \u2248 P\u2126(XX>), where P\u2126 is a matrix projection operator defined on the observed entry set \u2126. Despite its relatively simple formulation, this optimization problem is hard to solve since it is highly non-convex and NP-hard. To address this challenge, we propose a very efficient exact cyclic coordinate descent algorithm. By wisely updating variables and taking the advantage of sparse observed entries in A\u0303, the proposed algorithm enjoys a very low computational cost, and thus can learn new feature representations in an extremely efficient way.\nTo evaluate the performance of the learned feature representation, we use more than 10 real-world data sets to conduct experiments on both data classification and clustering tasks. Our results show that classical static models that are fed with our learned features outperform the state-of-the-art time series classification and clustering algorithms in both accuracy and computational efficiency. In summary, our main contributions of this work are two-fold:\n1. We bridge the gap between time series data and a great amount of static models by learning a new feature representation of time series. The learned feature representation preserves the pairwise similarities of the raw time series data, and is general enough to be applied to a variety of learning problems.\n2. We propose the first, to the best of our knowledge, coordinate descent algorithm to solve symmetric matrix factorization problem on a partially observed matrix. The proposed algorithm enjoys a very low computational cost in each iteration and yields a fast convergence."}, {"heading": "2 Related Work", "text": "In this section, we review the existing work on learning feature representations for time series data. Among them, a family of methods use a set of derived features to represent time series. For instance, [Nanopoulos et al., 2001] proposed to use the mean, standard deviation, kurtosis, and skewness of time series to represent control chart patterns. [Wang et al., 2006] introduced a set of features such as trend, seasonality, serial correlation, chaos, nonlinearity, and self-similarity to partition different types of time series. [Deng et al., 2013] used some easy to compute features such as mean, standard deviation and slope temporal importance curves to guide time series classification. In order to automate the selection of features for time series classification, [Fulcher and Jones, 2014] proposed a greedy forward method that can automatically select features from thousands of choices. Besides, several techniques have been proposed to represent time series by a certain types of transformation, such as discrete Fourier transformation [Faloutsos et al., 1994], discrete cosine transformation [Korn et al., 1997], discrete wavelet transformation [Chan and Fu, 1999], piecewise aggregate approximation [Keogh et al., 2001], and symbolic aggregate approximation [Lin et al., 2007]. In addition, deep learning models\nsuch as Elman recurrent neural network [Elman, 1990] and long short-term memory [Schmidhuber, 2015] are capable of modeling complex structures of time series data and learn a layer of feature representations. Due to their outstanding performance on a number of applications, they have become increasingly popular in recent years.\nDespite the remarkable progress, the feature representations learned by these algorithms are usually problemspecific, and are not general enough for applications in multiple domains. Besides, the learned features cannot preserve similarities of the raw time series data, thus they are not suitable to the problems that are sensitive to the data similarity. These limitations inspire us to propose a problemindependent and similarity preserving representation learning framework for time series data."}, {"heading": "3 Similarity Preserving Representation Learning for Time Series Analysis", "text": "In this section, we first present the general idea of our similarity preserving time series representation learning framework. We then propose an extremely efficient algorithm that is significantly faster than the initial idea."}, {"heading": "3.1 Problem Definition and General Framework", "text": "Given a set of n time series T = {T1, \u00b7 \u00b7 \u00b7 , Tn} with equal or unequal lengths, our goal is to convert them to a matrix X \u2208 Rn\u00d7d such that the time series similarities are well preserved after the transformation. Specifically, we aim to learn a mapping function f : T \u2192 Rd that satisfies\nS(Ti, Tj) \u2248 \u3008f(Ti), f(Tj)\u3009 \u2200i, j \u2208 [n], (1)\nwhere \u3008\u00b7, \u00b7\u3009 stands for the inner product, one of the most commonly used similarity measure in analyzing static data. S(\u00b7, \u00b7) denotes the pairwise time series similarity that can be computed by a number of functions. In this work, we use dynamic time warping (DTW) algorithm to measure this similarity since it provides the best measure for a wide variety of time series problems [Rakthanmanon et al., 2013]. By warping sequences non-linearly in the time dimension, the DTW algorithm can calculate an optimal match between two given temporal sequences with equal or unequal lengths. Due to its superior performance, DTW has been successfully applied to many applications, including computer animation [Mu\u0308ller, 2007], surveillance [Sempena et al., 2011], gesture recognition [Celebi et al., 2013], signature matching [Efrat et al., 2007], protein sequence alignment [Vial et al., 2009], and speech recognition [Muda et al., 2010]. Normally, DTW algorithm outputs a pairwise distance between two temporal sequences, thus we need to convert it to a similarity score. Since the inner product space can be induced from the normed space using \u3008x, y\u3009 = (\u2016x\u20162 + \u2016y\u20162 \u2212 \u2016x \u2212 y\u20162)/2 [Adams, 2004], we generate the DTW similarity by\nS(Ti, Tj)= DTW(Ti, 0)2+DTW(Tj , 0)2\u2212DTW(Ti, Tj)2\n2 ,\nwhere 0 denotes the length one time series with entry 0. We note that the similarity computed via the above equation is a more robust choice than some other similarity measures such\nas the reciprocal of distance. This is because when two time series are almost identical, their DTW distance is close to 0 and thus its reciprocal tends to infinity.\nIn order to learn the matrix X, an intuitive idea is to factorize the similarity matrix A \u2208 Rn\u00d7n where Aij = S(Ti, Tj). In more detail, this idea consists of two steps, i.e., a similarity matrix construction step and a symmetric matrix factorization step. In the first step, it constructs a similarity matrix A as follows\nAij =  [b 2 i + b 2 j \u2212DTW 2(Ti, Tj)]/2, if i < j (b2i + b 2 j )/2, if i = j\nAji if i > j, (2)\nwhere bi = DTW(Ti, 0), i = 1, \u00b7 \u00b7 \u00b7, n. In the second step, it learns an optimal data-feature matrix X by solving the following optimization problem\nmin X=(x1,\u00b7\u00b7\u00b7 ,xn) n\u2211 i=1 n\u2211 j=i+1 \u2016Aij \u2212 \u3008xi,xj\u3009\u20162, (3)\nwhich can be further expressed in a matrix form\nmin X\u2208Rn\u00d7d\n\u2016A\u2212XX>\u20162F . (4)\nThe problems (3) and (4) can be solved by performing eigendecomposition on A, i.e.,\nX = Q1:n,1:d \u00d7 \u221a \u039b1:d,1:d ,\nwhere A = Q\u039bQ> and the notation Q1:k,1:r represents the sub-matrix of Q that includes its first k rows and the first r columns.\nAlthough the inner products of the learned data points x1, \u00b7 \u00b7 \u00b7 ,xn can well approximate the DTW similarities of the raw time series, the idea described above is not practical since both two steps are painfully slow when n is large. In order to generate a n\u00d7 n similarity matrix, we need to call the DTW algorithm O(n2) times.1 In addition, a naive implementation of eigen-decomposition takes O(n3) time. Although we can reduce this cost by only computing the d largest eigenvalues and the corresponding eigenvectors, it is still computational intensive with a large n. As a concrete example, when n = 150, 000 and the length of time series is 30, it takes more than 28 hours to generate the similarity matrix and more than 3 days to compute its 30 largest eigenvalues on an Intel Xeon 2.40 GHz processor with 256 GB of main memory."}, {"heading": "3.2 A Much More Efficient Algorithm", "text": "In this subsection, we propose an extremely efficient approach that significantly reduces the computational costs of both steps while learns the new feature representation with a high precision.\nTo significantly improve the efficiency of the first step, we make the key observation that the similarity matrix A should be of low-rank. This is due to the reason that the\n1Indeed, we need to call the DTW algorithm at least n(n+1)/2 times to generate full similarity matrix A. This number is achieved when we pre-compute all the bi, i = 1, \u00b7 \u00b7 \u00b7, n.\nDTW algorithm measures the level of co-movement between time series, which has shown to be dictated by only a small number of latent factors [Stock and Watson, 2005; Basu and Michailidis, 2015]. Indeed, we can verify the lowrankness in another way. Since the matrix A is a special case of Wigner random matrix, the gaps between its consecutive eigenvalues should not be small [Marc\u030cenko and Pastur, 1967], which implies the low-rank property since most of its energy is concentrated in its top eigenvalues [Erdo\u030bs et al., 2009].\nBased on the theory of matrix completion [Sun and Luo, 2015], only O(n log n) randomly sampled entries are needed to perfectly recover an n \u00d7 n low-rank matrix. Following this theory, we don\u2019t need to compute all the pairwise DTW similarities. Instead, it allows us to randomly sample only O(n log n) pairs of time series, and then compute the DTW similarities only within the selected pairs. In other words, we generate a partially observed similarity matrix A\u0303 with O(n log n) observed entries as\nA\u0303ij = { S(Ti, Tj) if \u2126ij = 1 unobserved if \u2126ij = 0,\n(5)\nwhere \u2126 \u2208 {0, 1}n\u00d7n is a binary matrix indicating the indices of sampled pairs. In this way, the running time of the first step is significantly reduced by a factor ofO(n/ log n). Since this factor scales almost linearly with n, we can greatly save the running time when n is large. For instance, when n = 150, 000, it only takes 194 seconds to construct a partially observed similarity matrix with [20n log n] observed entries, more than 500 times faster than generating a full similarity matrix.\nGiven the partially observed similarity matrix A\u0303, our second step aims to learn a new feature representation matrix X. Instead of first completing the full similarity matrix A and then factorize it, we propose an efficient symmetric factorization algorithm that is able to directly factorize the partially observed similarity matrix A\u0303, i.e., find a X \u2208 Rn\u00d7d that minimizes the following optimization problem\nmin X\u2208Rn\u00d7d\n\u2016P\u2126 (A\u0303 \u2212XXT )\u20162F , (6)\nwhere P\u2126 : Rn\u00d7n \u2192 Rn\u00d7n is a projection operator defined as\n[P\u2126 (B)]ij = { Bij if \u2126ij = 1 0 if \u2126ij = 0.\n(7)\nThe objective function (6) does not have a regularization term since it already bounds the Frobenius norm of X. Despite its relatively simple formulation, solving problem (6) is non-trivial since its objective function is highly non-convex and the problem is NP-hard. To address this issue, we propose a very efficient optimization algorithm that solves problem (6) based on exact cyclic coordinate descent (CD). Although [Ge et al., 2016] shows that the symmetric matrix factorization problem can be solved via (stochastic) gradient descent algorithm as well, our proposed coordinate descent algorithm has the following two advantages: (i) our CD algorithm directly updates each coordinate to the optimum in each iteration, thus does not need to tune parameters such as\nthe step sizes required by the gradient descent methods; and (ii) by directly updating coordinates to the optimums using the most up-to-date information, our CD algorithm converges at a very fast rate.\nAt each iteration of the exact cyclic CD method, all variables but one are fixed, and that variable is updated to its optimal value. One of the main strengths of our algorithm is its capacity to update variables in an extremely efficient way. Besides, the proposed algorithm takes the advantage of sparse observed entries in A\u0303 to further reduce the computational cost. In more detail, our algorithm consists of two loops that iterate over all the entries of X to update their values. The outer loop of the algorithm traverses through each column of X by assuming all the other columns known and fixed. At the i-th iteration, it optimizes the i-th column X1:n,i by minimizing the following subproblem\n\u2016R\u2212 P\u2126(X1:n,iXT1:n,i)\u20162F , (8)\nwhere R is the residual matrix defined as R = P\u2126(A\u0303 \u2212\u2211 j 6=i X1:n,jX T 1:n,j).\nIn the inner loop, the proposed algorithm iterates over each coordinate of the selected column and updates its value. Specifically, when updating the j-th entry Xji, we solve the following optimization problem\nmin Xji \u2016R\u2212 P\u2126(X1:n,iXT1:n,i)\u20162F\n\u21d0\u21d2 min Xji \u2016R\u20162F\u22122\u3008R, P\u2126(X1:n,iXT1:n,i)\u3009\n+ \u2016P\u2126(X1:n,iXT1:n,i)\u20162F \u21d0\u21d2 min\nXji X4ji + 2( \u2211 k\u2208\u2126j , k 6=j X2ki \u2212 Rjj)X2ji\n\u2212 4( \u2211\nk\u2208\u2126j , k 6=j\nXkiRjk)Xji + C\n\u21d0\u21d2 min Xji \u03c8(Xji) = X 4 ji + 2pX 2 ji + 4qXji + C,\n(9)\nwhere \u2126i, i = 1,\u00b7 \u00b7 \u00b7, n contains the indices of the observed entries in the i-th row of matrix A\u0303. C is a constant that is independent of Xji. Since the \u03c8(Xji) is a fourth-degree polynomial function, Xji, as will be shown later, can be updated in a very efficiently way. Algorithm 1 describes the detailed steps of the proposed exact cyclic CD algorithm.\nThe proposed algorithm enjoys a very low computational cost in each iteration. Lines 7-11 of the algorithm can be computed in O(n log n) operations. This is because the costs of computing p and q are only proportional to the cardinality of \u2126j . Besides, the derivative \u2207\u03c8(Xji) is a third-degree polynomial, thus its roots can be computed in closed form. By using Cardano\u2019s method [Cardano and Witmer, 1993], the optimal solution of Xji can be calculated in a constant time given the computed p and q. Likewise, lines 6 and 12 of the algorithm also take O(n log n) time since matrix R can be updated by only considering the observed entries. To sum up, the proposed algorithm has a per-iteration cost of O(dn log n), significantly faster than the recent matrix factorization algorithms that take at least O(dn2) time in each\nAlgorithm 1 Efficient Exact Cyclic Coordinate Descent Algorithm for Solving the Optimization Problem (6)\n1: Inputs:\n\u2022 A\u0303 \u2208 Rn\u00d7n: partially observed similarity matrix \u2022 \u2126i, i = 1,\u00b7 \u00b7 \u00b7, n: indices of the observed entries in\nthe i-th row of matrix A\u0303 \u2022 I: number of iterations \u2022 d: dimension of features\n2: Initializations: \u2022 X(0) \u2190 0n\u00d7d \u2022 R\u2190 P\u2126(A\u0303\u2212X(0)X(0)>) = P\u2126(A\u0303)\n3: for t = 1, \u00b7 \u00b7 \u00b7 , I do 4: X(t) \u2190 X(t\u22121) 5: for i = 1, \u00b7 \u00b7 \u00b7 , d do 6: R\u2190 R + P\u2126(X(t)1:n,iX (t)> 1:n,i) 7: for j = 1, \u00b7 \u00b7 \u00b7n do 8: p\u2190 \u2211 k\u2208\u2126j X (t)2 ki \u2212X (t)2 ji \u2212 Rjj\n9: q \u2190 \u2212 \u2211\nk\u2208\u2126j X (t) ki Rjk + X (t) ji Rjj\n10: X(t)ji \u2190 argmin{X4ji + 2pX2ji + 4qXji} 11: end for 12: R\u2190 R\u2212 P\u2126(X(t)1:n,iX (t)> 1:n,i) 13: end for 14: end for 15: Output: X(I)\niteration [Vandaele et al., 2015; Hsieh and Dhillon, 2011; Yu et al., 2012].\nIn addition to a low per-iteration cost, we also expect that the proposed algorithm yields a fast convergence. This is because our algorithm always uses the newest information to update variables and each variable is updated to the optimum in a single step. This hypothesis is verified by a convergence test conducted on the UCR Non-Invasive Fetal ECG Thorax1 testbed [Chen et al., 2015]. This testbed contains a total of 3, 765 time series with a length of 750. In this test, we generate a full similarity matrix A by computing the DTW similarities between all the time series pairs, and then randomly sample [20n log n] of its entries to generate a partially observed matrix A\u0303. We call the proposed algorithm to factorize matrix A\u0303 by setting d = 30. To measure the performance of the proposed method, we compute two error rates, i.e., the observed error \u2016P\u2126(A\u0303 \u2212XXT )\u2016F /\u2016P\u2126(A\u0303)\u2016F and the underlying true error \u2016A \u2212XXT \u2016F /\u2016A\u2016F , at each iteration. Figure 1 shows how they converge as a function of time. This figure clearly shows that the proposed exact cyclic CD algorithm converges very fast \u2013 it only takes 1 second and 8 iterations to converge. Besides, the construction accuracy is also very encouraging. The observed error and the underlying true error rates are close to each other and both of them are only about 0.1%, indicating that the inner products of the learned features can well approximate the pairwise DTW similarities of the raw time series. This result also validates the low-rank\nassumption. It shows that a 3, 765 \u00d7 3, 765 DTW similarity matrix can be accurately approximated by a rank 30 matrix."}, {"heading": "4 Experiments", "text": "In this section, we evaluate the proposed framework, i.e., Similarity PreservIng RepresentAtion Learning (SPIRAL for short), on both data classification and clustering tasks. For both tasks, we learn new feature representations by setting the number of features d = 30, the number of iterations I = 20, and the sample size |\u2126| = [20n log n]. We then fed the learned features into some static models and compare them with the state-of-the-art time series classification and clustering algorithms. Since the DTW function is also called by some baseline algorithms, we set a same DTW window size min(40, [average time series length/10]) for all the DTWrelated algorithms evaluated here. All the results were obtained on a Linux server with an Intel Xeon 2.40 GHz CPU and 256 GB of main memory. We have released the source code of the proposed algorithm at https://github.com/ cecilialeiqi/SPIRAL."}, {"heading": "4.1 Classification Task", "text": "Six real-world time series data sets are used in our classification analysis. As a research institute, we have partnered with one of the world\u2019s largest online brokers on a challenge problem of predicting its clients\u2019 propensity of trading options. We are provided with a historical records data of 64, 523 sampled clients with the lengths of the time series range from 1 month to 67 months. The data contains 76 dynamic attributes with none of them related to option trading. Given this data, our task is to predict whether the clients will trade options in the next 3 months. Besides, we also conduct experiments on 5 benchmark data sets, i.e., FordA, ECG5000, PhalangesOutlinesCorrect (POC), ItalyPowerDemand (IPD), and HandOutlines (HO), from the UCR Time Series Repository [Chen et al., 2015]. The data sets used in our experiments have widely varying sizes and encompass multiple domains such as finance, healthcare, and manufacture. Table 1 summarizes the statistics of these data sets.\nGiven the feature representations learned by the proposed SPIRAL framework, we fed them into the algorithms of XGBoost [Chen and Guestrin, 2016] and `2-regularized logistic regression that is implemented in LibLinear [Fan et\nal., 2008]. These approaches, denoted as SPIRAL-XGB and SPIRAL-LR, respectively, are compared with the state-of-theart time series classification algorithms NN-DTW [Wang et al., 2013] and Long Short-Term Memory (LSTM) [Schmidhuber, 2015]. For the XGBoost algorithm, we use the default parameters specified in its source code. The parameter of logistic regression is determined by a 5-fold cross validation.\nWe use AUC (area under the ROC Curve) to measure the classification performance. All the experiments in this study are repeated five times, and the AUCs averaged over the five trials are reported in Table 2. From this table, we first observe that XGBoost and logistic regression algorithms that are fed with our learned features outperform the two baseline algorithms on all the data sets. In particular, the method SPIRALXGB yields the best performance on five out of six data sets, and the method SPIRAL-LR performs the best on the other data set. More encouragingly, SPIRAL-LR can achieve better classification performance than NN-DTW and LSTM on almost all the data sets. This verifies that the feature representation learned by the proposed method is powerful \u2013 even classical models such as logistic regression can achieve satisfactory performance on it.\nOn the other hand, although LSTM has been successfully applied to a number of sequence prediction tasks, it fails to deliver strong performance in our empirical study. We conjecture that this is because the data sets are not large enough for LSTM to model complex structures. Besides, NN-DTW is usually a very strong baseline that is considered as hard-to-beat in the literature [Xi et al., 2006; Wang et al., 2013]. However, it also yields an overall worse performance than SPIRAL-LR and SPIRAL-XGB. One possible reason is that NN-DTW uses 1-nearest neighbor classifier, thus is sensitive to noises and outliers. This observation shows another advantage of the proposed method: by learning\na feature representation instead of directly developing a time series model, our method is more flexible and can exploit the strengths of different static algorithms.\nIn addition to the superior classification performance, SPIRAL-LR and SPIRAL-XGB are very efficient as well. They have a significantly lower running time than that of the baseline algorithms. For example, it takes NN-DTW more than 2 days to classify the FordA data set while the running time for SPIRAL-LR and SPIRAL-XGB are only about 5 minutes."}, {"heading": "4.2 Clustering Task", "text": "Similar to time series classification, time series clustering is also an important task that has found numerous applications. To further test our learned features on the clustering task, we conduct experiments on another 7 UCR time series data sets.Since data clustering is an unsupervised learning task, we merge their training and testing sets together. Statistics of these data sets are summarized in Table 3.\nWe fed the features learned by the proposed framework into the kMeans algorithm as our clustering method, and compare it to the state-of-the-art time series clustering algorithm k-Shape [Paparrizos and Gravano, 2015], which has been shown to outperform many state-of-the-art partitional, hierarchical, and spectral clustering approaches. Besides, we also compare our method with clustering algorithms kMeansDTW and CLDS [Li and Prakash, 2011] since our ideas are similar in some respects. kMeans-DTW is a popular time series clustering algorithm that uses DTW algorithm to measure pairwise distances between data points. Although it looks similar to the idea of our SPIRAL-kMeans that also utilizes the DTW and kMeans algorithms, it is less desirable than SPIRAL-kMeans mainly because: (i) kMeans-DTW suffers from a very high computational cost since it needs to compute the pairwise DTW distances between all the time series and all the cluster centers at each iteration; and (ii) the DTW distance does not satisfy the triangle inequality, thus can make the cluster centers computed by averaging multiple time series drift out of the cluster [Niennattrakul and Ratanamahatana, 2007]. By designing an efficient algorithm that only needs to call the DTW function O(n log n) times and by embedding time series data to the Euclidean space while preserving their original similarities, the proposed method SPIRAL successfully addresses both these issues. Similar to the idea of representation learning presented in this paper, CLDS also learns a new feature representation of the time series data, and then partition the learned representation via an EM-like algorithm.\nIn this study, we use the normalized mutual information (NMI for short) to measure the coherence between the inferred clustering and the ground truth categorization. NMI scales from 0 to 1, and a higher NMI value implies a better partition. Each experiment is repeated five times, and the performance averaged over the five trials is reported in Table 4. Compared to all the baseline algorithms, our clustering method SPIRAL-kMeans yields the best performance on all the seven data sets, indicating that it delivers the state-of-theart performance. In addition, SPIRAL-kMeans has a significantly lower running time than all the baseline clustering al-\ngorithms evaluated here. For instance, clustering the ED data set takes k-Shape, CLDS, and kMeans-DTW 20, 57, and 114 minutes, respectively. As a comparison, our clustering algorithm SPIRAL-kMeans only spends one minute and a half to partition the ED data set.\nWe finally note that kMeans is just one choice of clustering algorithms that can take our learned features as the input. By replacing it with some more powerful clustering algorithms, it is expected to achieve an even better clustering performance."}, {"heading": "5 Conclusions", "text": "In this paper, we propose a similarity preserving representation learning framework for time series analysis. Given a set of n time series, the key idea is to first generate a partially observed similarity matrix withO(n log n) observed DTW similarities, and then factorize this matrix to learn a new feature representation. To this end, we propose the first symmetric matrix factorization algorithm on a partially observed matrix. The proposed algorithm updates variables via exact cyclic coordinate descent, and enjoys a very low computational cost in each iteration. The feature representation learned by the proposed framework preserves the DTW similarities of the raw time series data, and is general enough to be applied to a variety of learning problems. Our empirical studies on both classification and clustering tasks verify the effectiveness and efficiency of the proposed method."}], "references": [{"title": "The knot book: an elementary introduction to the mathematical theory of knots", "author": ["Colin Conrad Adams"], "venue": "American Mathematical Soc.,", "citeRegEx": "Adams. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Low-rank and sparse modeling of highdimensional vector autoregressions", "author": ["Basu", "Michailidis", "2015] Sumanta Basu", "George Michailidis"], "venue": null, "citeRegEx": "Basu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Basu et al\\.", "year": 2015}, {"title": "Gesture recognition using skeleton data with weighted dynamic time warping", "author": ["S. Celebi", "A. Aydin", "T. Temiz", "T. Arici"], "venue": "VISAPP (1), pages 620\u2013625", "citeRegEx": "Celebi et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "pages 126\u2013133", "author": ["Kin-Pong Chan", "Ada Wai-Chee Fu. Efficient time series matching by wavelets. In ICDE"], "venue": "IEEE,", "citeRegEx": "Chan and Fu. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Xgboost: A scalable tree boosting system", "author": ["Tianqi Chen", "Carlos Guestrin"], "venue": "arXiv preprint arXiv:1603.02754,", "citeRegEx": "Chen and Guestrin. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "The ucr time series classification archive", "author": ["Yanping Chen", "Eamonn Keogh", "Bing Hu", "Nurjahan Begum", "Anthony Bagnall", "Abdullah Mueen", "Gustavo Batista"], "venue": "July", "citeRegEx": "Chen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Information Sciences", "author": ["Houtao Deng", "George Runger", "Eugene Tuv", "Martyanov Vladimir. A time series forest for classification", "feature extraction"], "venue": "239:142\u2013153,", "citeRegEx": "Deng et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "time warping", "author": ["Alon Efrat", "Quanfu Fan", "Suresh Venkatasubramanian. Curve matching"], "venue": "and light fields: New algorithms for computing similarity between curves. Journal of Mathematical Imaging and Vision, 27(3):203\u2013216,", "citeRegEx": "Efrat et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Cognitive science", "author": ["Jeffrey L Elman. Finding structure in time"], "venue": "14(2):179\u2013211,", "citeRegEx": "Elman. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Communications in Mathematical Physics", "author": ["L\u00e1szl\u00f3 Erd\u0151s", "Benjamin Schlein", "Horng-Tzer Yau. Local semicircle law", "complete delocalization for wigner random matrices"], "venue": "287(2):641\u2013655,", "citeRegEx": "Erd\u0151s et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "volume 23", "author": ["Christos Faloutsos", "Mudumbai Ranganathan", "Yannis Manolopoulos. Fast subsequence matching in time-series databases"], "venue": "ACM,", "citeRegEx": "Faloutsos et al.. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "JMLR, 9(Aug):1871\u2013 1874,", "citeRegEx": "Fan et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "IEEE Transactions on Knowledge and Data Engineering", "author": ["Ben D Fulcher", "Nick S Jones. Highly comparative feature-based time-series classification"], "venue": "26(12):3026\u20133037,", "citeRegEx": "Fulcher and Jones. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Matrix completion has no spurious local minimum", "author": ["R. Ge", "J. Lee", "T. Ma"], "venue": "NIPS, pages 2973\u2013 2981", "citeRegEx": "Ge et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "pages 1064\u20131072", "author": ["Cho-Jui Hsieh", "Inderjit S Dhillon. Fast coordinate descent methods with variable selection for non-negative matrix factorization. In KDD"], "venue": "ACM,", "citeRegEx": "Hsieh and Dhillon. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Knowledge and information Systems", "author": ["Eamonn Keogh", "Kaushik Chakrabarti", "Michael Pazzani", "Sharad Mehrotra. Dimensionality reduction for fast similarity search in large time series databases"], "venue": "3(3):263\u2013 286,", "citeRegEx": "Keogh et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "ACM SIGMOD Record", "author": ["Flip Korn", "Hosagrahar V Jagadish", "Christos Faloutsos. Efficiently supporting ad hoc queries in large datasets of time sequences"], "venue": "26(2):289\u2013300,", "citeRegEx": "Korn et al.. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "A review of unsupervised feature learning and deep learning for time-series modeling", "author": ["M. L\u00e4ngkvist", "L. Karlsson", "A. Loutfi"], "venue": "Pattern Recognition Letters, 42:11\u201324", "citeRegEx": "L\u00e4ngkvist et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Time series clustering: Complex is simpler! In ICML", "author": ["Lei Li", "B Aditya Prakash"], "venue": "pages 185\u2013192,", "citeRegEx": "Li and Prakash. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Experiencing sax: a novel symbolic representation of time series", "author": ["J. Lin", "E. Keogh", "L. Wei", "S. Lonardi"], "venue": "Data Mining and Knowledge Discovery, 15(2):107\u2013144", "citeRegEx": "Lin et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Distribution of eigenvalues for some sets of random matrices", "author": ["V. Mar\u010denko", "L. Pastur"], "venue": "Mathematics of the USSR-Sbornik, 1(4):457", "citeRegEx": "Mar\u010denko and Pastur. 1967", "shortCiteRegEx": null, "year": 1967}, {"title": "Voice recognition algorithms using mel frequency cepstral coefficient (mfcc) and dynamic time warping (dtw) techniques", "author": ["Lindasalwa Muda", "Mumtaj Begam", "I Elamvazuthi"], "venue": "arXiv preprint arXiv:1003.4083,", "citeRegEx": "Muda et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Information Retrieval for Music and Motion", "author": ["Meinard M\u00fcller. Dtw-based motion comparison", "retrieval"], "venue": "pages 211\u2013226,", "citeRegEx": "M\u00fcller. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Feature-based classification of timeseries data", "author": ["A. Nanopoulos", "R. Alcock", "Y. Manolopoulos"], "venue": "International Journal of Computer Research, 10(3):49\u201361", "citeRegEx": "Nanopoulos et al.. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Inaccuracies of shape averaging method using dynamic time warping for time series data", "author": ["V. Niennattrakul", "C. Ratanamahatana"], "venue": "International conference on computational science, pages 513\u2013520. Springer", "citeRegEx": "Niennattrakul and Ratanamahatana. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "k-shape: Efficient and accurate clustering of time series", "author": ["John Paparrizos", "Luis Gravano"], "venue": "ACM SIGMOD, pages 1855\u20131870. ACM,", "citeRegEx": "Paparrizos and Gravano. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks, 61:85\u2013 117,", "citeRegEx": "Schmidhuber. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 1\u20135", "author": ["Samsu Sempena", "Nur Ulfa Maulidevi", "Peb Ruswono Aryan. Human action recognition using dynamic time warping. In International Conference on Electrical Engineering", "Informatics"], "venue": "IEEE,", "citeRegEx": "Sempena et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Technical report", "author": ["James H Stock", "Mark W Watson. Implications of dynamic factor models for var analysis"], "venue": "National Bureau of Economic Research,", "citeRegEx": "Stock and Watson. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "pages 270\u2013289", "author": ["Ruoyu Sun", "Zhi-Quan Luo. Guaranteed matrix completion via nonconvex factorization. In FOCS"], "venue": "IEEE,", "citeRegEx": "Sun and Luo. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "CoRR", "author": ["Arnaud Vandaele", "Nicolas Gillis", "Qi Lei", "Kai Zhong", "Inderjit S. Dhillon. Coordinate descent methods for symmetric nonnegative matrix factorization"], "venue": "abs/1509.01404,", "citeRegEx": "Vandaele et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Combination of dynamic time warping and multivariate analysis for the comparison of comprehensive two-dimensional", "author": ["Vial et al", "2009] J\u00e9r\u00f4me Vial", "Hicham No\u00e7airi", "Patrick Sassiat", "Sreedhar Mallipatu", "Guillaume Cognon", "Didier Thi\u00e9baut", "B\u00e9atrice Teillet", "Douglas N Rutledge"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Data mining and knowledge Discovery", "author": ["Xiaozhe Wang", "Kate Smith", "Rob Hyndman. Characteristic-based clustering for time series data"], "venue": "13(3):335\u2013 364,", "citeRegEx": "Wang et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Experimental comparison of representation methods and distance measures for time series data", "author": ["X. Wang", "A. Mueen", "H. Ding", "G. Trajcevski", "P. Scheuermann", "E. Keogh"], "venue": "Data Mining and Knowledge Discovery, 26(2):275\u2013309", "citeRegEx": "Wang et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "pages 1033\u20131040", "author": ["Xiaopeng Xi", "Eamonn Keogh", "Christian Shelton", "Li Wei", "Chotirat Ann Ratanamahatana. Fast time series classification using numerosity reduction. In ICML"], "venue": "ACM,", "citeRegEx": "Xi et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "10 challenging problems in data mining research", "author": ["Qiang Yang", "Xindong Wu"], "venue": "International Journal of Information Technology & Decision Making, 5(04):597\u2013604,", "citeRegEx": "Yang and Wu. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "pages 765\u2013774", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Si Si", "Inderjit Dhillon. Scalable coordinate descent approaches to parallel matrix factorization for recommender systems. In ICDM"], "venue": "IEEE,", "citeRegEx": "Yu et al.. 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 35, "context": "It is considered by [Yang and Wu, 2006] as one of the 10 most challenging problems in data mining.", "startOffset": 20, "endOffset": 39}, {"referenceID": 17, "context": "The latter category of models, which usually take matrices as their inputs, cannot directly analyze time series data due to its temporal nature, usually unequal lengths, and complex properties [L\u00e4ngkvist et al., 2014].", "startOffset": 193, "endOffset": 217}, {"referenceID": 29, "context": "In order to significantly reduce the running time, we follow the setting of matrix completion [Sun and Luo, 2015] by assuming that the similarity matrix A is of low-rank.", "startOffset": 94, "endOffset": 113}, {"referenceID": 28, "context": "This is a very natural assumption since DTW algorithm captures the co-movements of time series, which has shown to be driven by only a small number of latent factors [Stock and Watson, 2005; Basu and Michailidis, 2015].", "startOffset": 166, "endOffset": 218}, {"referenceID": 23, "context": "For instance, [Nanopoulos et al., 2001] proposed to use the mean, standard deviation, kurtosis, and skewness of time series to represent control chart patterns.", "startOffset": 14, "endOffset": 39}, {"referenceID": 32, "context": "[Wang et al., 2006] introduced a set of features such as trend, seasonality, serial correlation, chaos, nonlinearity, and self-similarity to partition different types of time series.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "[Deng et al., 2013] used some easy to compute features such as mean, standard deviation and slope temporal importance curves to guide time series classification.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "In order to automate the selection of features for time series classification, [Fulcher and Jones, 2014] proposed a greedy forward method that can automatically select features from thousands of choices.", "startOffset": 79, "endOffset": 104}, {"referenceID": 10, "context": "Besides, several techniques have been proposed to represent time series by a certain types of transformation, such as discrete Fourier transformation [Faloutsos et al., 1994], discrete cosine transformation [Korn et al.", "startOffset": 150, "endOffset": 174}, {"referenceID": 16, "context": ", 1994], discrete cosine transformation [Korn et al., 1997], discrete wavelet transformation [Chan and Fu, 1999], piecewise aggregate approximation [Keogh et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 3, "context": ", 1997], discrete wavelet transformation [Chan and Fu, 1999], piecewise aggregate approximation [Keogh et al.", "startOffset": 41, "endOffset": 60}, {"referenceID": 15, "context": ", 1997], discrete wavelet transformation [Chan and Fu, 1999], piecewise aggregate approximation [Keogh et al., 2001], and symbolic aggregate approximation [Lin et al.", "startOffset": 96, "endOffset": 116}, {"referenceID": 19, "context": ", 2001], and symbolic aggregate approximation [Lin et al., 2007].", "startOffset": 46, "endOffset": 64}, {"referenceID": 8, "context": "In addition, deep learning models such as Elman recurrent neural network [Elman, 1990] and long short-term memory [Schmidhuber, 2015] are capable of modeling complex structures of time series data and learn a layer of feature representations.", "startOffset": 73, "endOffset": 86}, {"referenceID": 26, "context": "In addition, deep learning models such as Elman recurrent neural network [Elman, 1990] and long short-term memory [Schmidhuber, 2015] are capable of modeling complex structures of time series data and learn a layer of feature representations.", "startOffset": 114, "endOffset": 133}, {"referenceID": 22, "context": "Due to its superior performance, DTW has been successfully applied to many applications, including computer animation [M\u00fcller, 2007], surveillance [Sempena et al.", "startOffset": 118, "endOffset": 132}, {"referenceID": 27, "context": "Due to its superior performance, DTW has been successfully applied to many applications, including computer animation [M\u00fcller, 2007], surveillance [Sempena et al., 2011], gesture recognition [Celebi et al.", "startOffset": 147, "endOffset": 169}, {"referenceID": 2, "context": ", 2011], gesture recognition [Celebi et al., 2013], signature matching [Efrat et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 7, "context": ", 2013], signature matching [Efrat et al., 2007], protein sequence alignment [Vial et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 21, "context": ", 2009], and speech recognition [Muda et al., 2010].", "startOffset": 32, "endOffset": 51}, {"referenceID": 0, "context": "Since the inner product space can be induced from the normed space using \u3008x, y\u3009 = (\u2016x\u2016 + \u2016y\u2016 \u2212 \u2016x \u2212 y\u2016)/2 [Adams, 2004], we generate the DTW similarity by", "startOffset": 106, "endOffset": 119}, {"referenceID": 28, "context": "DTW algorithm measures the level of co-movement between time series, which has shown to be dictated by only a small number of latent factors [Stock and Watson, 2005; Basu and Michailidis, 2015].", "startOffset": 141, "endOffset": 193}, {"referenceID": 20, "context": "Since the matrix A is a special case of Wigner random matrix, the gaps between its consecutive eigenvalues should not be small [Mar\u010denko and Pastur, 1967], which implies the low-rank property since most of its energy is concentrated in its top eigenvalues [Erd\u0151s et al.", "startOffset": 127, "endOffset": 154}, {"referenceID": 9, "context": "Since the matrix A is a special case of Wigner random matrix, the gaps between its consecutive eigenvalues should not be small [Mar\u010denko and Pastur, 1967], which implies the low-rank property since most of its energy is concentrated in its top eigenvalues [Erd\u0151s et al., 2009].", "startOffset": 256, "endOffset": 276}, {"referenceID": 29, "context": "Based on the theory of matrix completion [Sun and Luo, 2015], only O(n log n) randomly sampled entries are needed to perfectly recover an n \u00d7 n low-rank matrix.", "startOffset": 41, "endOffset": 60}, {"referenceID": 13, "context": "Although [Ge et al., 2016] shows that the symmetric matrix factorization problem can be solved via (stochastic) gradient descent algorithm as well, our proposed coordinate descent algorithm has the following two advantages: (i) our CD algorithm directly updates each coordinate to the optimum in each iteration, thus does not need to tune parameters such as", "startOffset": 9, "endOffset": 26}, {"referenceID": 30, "context": "iteration [Vandaele et al., 2015; Hsieh and Dhillon, 2011; Yu et al., 2012].", "startOffset": 10, "endOffset": 75}, {"referenceID": 14, "context": "iteration [Vandaele et al., 2015; Hsieh and Dhillon, 2011; Yu et al., 2012].", "startOffset": 10, "endOffset": 75}, {"referenceID": 36, "context": "iteration [Vandaele et al., 2015; Hsieh and Dhillon, 2011; Yu et al., 2012].", "startOffset": 10, "endOffset": 75}, {"referenceID": 5, "context": "This hypothesis is verified by a convergence test conducted on the UCR Non-Invasive Fetal ECG Thorax1 testbed [Chen et al., 2015].", "startOffset": 110, "endOffset": 129}, {"referenceID": 5, "context": ", FordA, ECG5000, PhalangesOutlinesCorrect (POC), ItalyPowerDemand (IPD), and HandOutlines (HO), from the UCR Time Series Repository [Chen et al., 2015].", "startOffset": 133, "endOffset": 152}, {"referenceID": 4, "context": "Given the feature representations learned by the proposed SPIRAL framework, we fed them into the algorithms of XGBoost [Chen and Guestrin, 2016] and `2-regularized logistic regression that is implemented in LibLinear [Fan et Table 1: Description of Classification Data sets", "startOffset": 119, "endOffset": 144}, {"referenceID": 33, "context": "These approaches, denoted as SPIRAL-XGB and SPIRAL-LR, respectively, are compared with the state-of-theart time series classification algorithms NN-DTW [Wang et al., 2013] and Long Short-Term Memory (LSTM) [Schmidhuber, 2015].", "startOffset": 152, "endOffset": 171}, {"referenceID": 26, "context": ", 2013] and Long Short-Term Memory (LSTM) [Schmidhuber, 2015].", "startOffset": 42, "endOffset": 61}, {"referenceID": 34, "context": "Besides, NN-DTW is usually a very strong baseline that is considered as hard-to-beat in the literature [Xi et al., 2006; Wang et al., 2013].", "startOffset": 103, "endOffset": 139}, {"referenceID": 33, "context": "Besides, NN-DTW is usually a very strong baseline that is considered as hard-to-beat in the literature [Xi et al., 2006; Wang et al., 2013].", "startOffset": 103, "endOffset": 139}, {"referenceID": 25, "context": "We fed the features learned by the proposed framework into the kMeans algorithm as our clustering method, and compare it to the state-of-the-art time series clustering algorithm k-Shape [Paparrizos and Gravano, 2015], which has been shown to outperform many state-of-the-art partitional, hierarchical, and spectral clustering approaches.", "startOffset": 186, "endOffset": 216}, {"referenceID": 18, "context": "Besides, we also compare our method with clustering algorithms kMeansDTW and CLDS [Li and Prakash, 2011] since our ideas are similar in some respects.", "startOffset": 82, "endOffset": 104}, {"referenceID": 24, "context": "Although it looks similar to the idea of our SPIRAL-kMeans that also utilizes the DTW and kMeans algorithms, it is less desirable than SPIRAL-kMeans mainly because: (i) kMeans-DTW suffers from a very high computational cost since it needs to compute the pairwise DTW distances between all the time series and all the cluster centers at each iteration; and (ii) the DTW distance does not satisfy the triangle inequality, thus can make the cluster centers computed by averaging multiple time series drift out of the cluster [Niennattrakul and Ratanamahatana, 2007].", "startOffset": 522, "endOffset": 562}], "year": 2017, "abstractText": "A considerable amount of machine learning algorithms take matrices as their inputs. As such, they cannot directly analyze time series data due to its temporal nature, usually unequal lengths, and complex properties. This is a great pity since many of these algorithms are effective, robust, efficient, and easy to use. In this paper, we bridge this gap by proposing an efficient representation learning framework that is able to convert a set of time series with equal or unequal lengths to a matrix format. In particular, we guarantee that the pairwise similarities between time series are well preserved after the transformation. Therefore, the learned feature representation is particularly suitable to the class of learning problems that are sensitive to data similarities. Given a set of n time series, we first construct an n\u00d7n partially observed similarity matrix by randomly samplingO(n log n) pairs of time series and computing their pairwise similarities. We then propose an extremely efficient algorithm that solves a highly non-convex and NP-hard problem to learn new features based on the partially observed similarity matrix. We use the learned features to conduct experiments on both data classification and clustering tasks. Our extensive experimental results demonstrate that the proposed framework is both effective and efficient.", "creator": "LaTeX with hyperref package"}}}