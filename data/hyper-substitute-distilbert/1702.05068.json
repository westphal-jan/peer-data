{"id": "1702.05068", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2017", "title": "Discovering objects and their relations from entangled scene representations", "abstract": "our world we have complicated and compactly structured at fragmented scenes of objects like relations. yet typical room, for insects, contains salient objects identified as tables, cards and books, and these objects typically relate to each theme given their underlying causes and semantics. this gives insights to interactive features, such bounded memory, function and event. humans manipulate knowledge such static and entity relations for functions under wide scale of tasks, and more generally when learning functional relationship as observed functions. in this light, philosophers introduce communication semantics ( rns ) - type general purpose linear network design for object - driven reasoning. we show possible rns are capable of learning instance relations from homogeneous database data. recently, we show why relation can disappear as a bottleneck that induces the factorization extracted objects from related object description metadata, and conversely distributed deep representations of scene images provided by a variational autoencoder. semantic procedure can eventually be used applying conjunction / differentiable memory mechanisms for describing relation identification across batch - shot turing tests. our critics find that social networks uses a potentially powerful architecture for modifying a variety of mechanisms people require using relation reasoning.", "histories": [["v1", "Thu, 16 Feb 2017 18:08:27 GMT  (8935kb,D)", "http://arxiv.org/abs/1702.05068v1", "ICLR Workshop 2017"]], "COMMENTS": "ICLR Workshop 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["david raposo", "adam santoro", "david barrett", "razvan pascanu", "timothy lillicrap", "peter battaglia"], "accepted": false, "id": "1702.05068"}, "pdf": {"name": "1702.05068.pdf", "metadata": {"source": "CRF", "title": "DISCOVERING OBJECTS AND THEIR RELATIONS FROM ENTANGLED SCENE REPRESENTATIONS", "authors": ["D. Raposo", "A. Santoro", "D.G.T. Barrett", "R. Pascanu", "T. Lillicrap", "P. Battaglia"], "emails": ["peterbattaglia}@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "The ability to reason about objects and relations is important for solving a wide variety of tasks (Spelke et al., 1992; Lake et al., 2016). For example, object relations enable the transfer of learned knowledge across superficial dissimilarities (Tenenbaum et al., 2011): the predator-prey relationship between a lion and a zebra is knowledge that is similarly useful when applied to a bear and a salmon, even though many features of these animals are very different.\nIn this work, we introduce a neural network architecture for learning to reason about \u2013 or model \u2013 objects and their relations, which we call relation networks (RNs). RNs adhere to several key design principles. First, RNs are designed to be invariant to permutations of object descriptions in their input. For example, RN representations of the object set {table, chair, book} will be identical for arbitrary re-orderings of the elements of the set. Second, RNs learn relations across multiple objects rather than within a single object \u2013 a basic defining property of object relations. This results from the use of shared computations across groups of objects. In designing the RN architecture, we took inspiration from the recently developed Interaction Network (IN) (Battaglia et al., 2016) which was applied to modelling physical, spatiotemporal interactions. Our model is also related to various other approaches that apply neural networks directly to graphs (Scarselli et al., 2009; Bruna et al., 2013; Li et al., 2015; Duvenaud et al., 2015; Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2016; Edwards & Xie, 2016).\nIn principle, a deep network with a sufficiently large number of parameters and a large enough training set should be capable of matching the performance of a RN. In practice, however, such networks would have to learn both the permutation invariance of objects and the relational structure\n\u2217Denotes equal contribution.\nar X\niv :1\n70 2.\n05 06\n8v 1\n[ cs\n.L G\n] 1\n6 Fe\nb 20\n17\nof the objects in the execution of a desired computation. This quickly becomes unfeasible as the number of objects and relations increase.\nExploiting contextual relations among entities in scenes and other complex systems has been explored in various branches of computer science, biology, and other fields. In computer vision, Zhu et al. (2009) and Zhao & Zhu (2011) modelled relations among image features using stochastic grammars, Felzenszwalb et al. (2008) modelled relations among object parts using the \u201cdeformable parts\u201d model, and Choi et al. (2012) modelled relations among objects in scenes using tree structured context models. In graphics, a number of approaches have been used to capture contextual scene structure, such as energy models (Yu et al., 2011), graphical and mixture models (Fisher et al., 2011; 2012), stochastic grammars (Liu et al., 2014), and probabilistic programs (Talton et al., 2012; Yeh et al., 2012).\nWe used a scene classification task to evaluate RNs\u2019 ability to discover relations between objects. In this task, classification boundaries were defined by the relational structure of the objects in the scenes. There are various ways of encoding scenes as observable data; for example, scene description data can consist of sets of co-occurring objects and their respective features (location, size, color, shape, etc.). A typical room scene, then, might consist of the object set {table, chair, lamp} and their respective descriptions (e.g., the table is large and red). We note that, although we allude to visual objects, such as tables and chairs, and speak of modeling visual relations, such as location and size, the datasets on which RNs can operate are not necessarily grounded in visual scenes. RNs are meant to model entities and relations, whether they are embedded in visual scenes, molecular networks, voting patterns, etc. Here we consider synthetic scene description data generated by hierarchical probabilistic generative models of objects and object features.\nWe begin by motivating RNs as a general purpose architecture for reasoning about object relations. Critically, we describe how RNs implement a permutation invariant computation on implicit groups of factored \u201cobjects.\u201d We then demonstrate the utility of RNs for classification of static scenes, where classification boundaries are defined by the relations between objects in the scenes. Next, we exploit RNs\u2019 implicit use of factored object representations to demonstrate that RNs can induce the factorization of objects from entangled scene inputs. Finally, we combine RNs with memoryaugmented neural networks (MANNs) (Santoro et al., 2016) to solve a difficult one-shot learning task, demonstrating the ability of RNs\u2019 to act in conjunction with other neural network architectures to rapidly discover new object relations from entirely new scenes."}, {"heading": "2 MODEL", "text": ""}, {"heading": "2.1 DESCRIPTION AND IMPLEMENTATION", "text": "RNs are inspired by Interaction Networks (INs) (Battaglia et al., 2016), and therefore share similar functional insights. Both operate under the assumption that permutation invariance is a necessary requirement for solving relational reasoning problems in a data efficient manner. However, INs use relations between objects as input to determine object interactions, mainly for the purpose of reasoning about dynamics. RNs compute object relations, and hence aim to determine object-relational structure from static inputs.\nSuppose we have an object oi = (o1i , o 2 i , ..., o n i ), represented as a vector of n features encoding properties such as the object\u2019s type, color, size, position, etc. A collection of m objects can be gathered into a m\u00d7 n matrix D, called the scene description. Although the term scene description alludes to visual information, this need not be the case; scenes can be entirely abstract, as can the objects that constitute the scene, and the features that define the objects.\nWe can imagine tasks (see section 3) that depend on the relations, r, between objects. One such task is the discovery of the relations themselves. For example, returning to the predator and prey analogy, the predator-prey relation can be determined from relative features between two animals \u2013 such as their relative sizes, perhaps. Observation of the size of a single animal, then, does not inform whether this animal is a predator or prey to any other given animal, since its size necessarily needs to be compared to the sizes of other animals.\nThere are a number of possible functions that could support the discovery of object relations (figure 1). Consider a function g, with parameters \u03c8. The function g\u03c8 can be defined to operate on a\nparticular factorization of D; for example, g\u03c8(D) \u2261 g\u03c8(o21, ..., o j i , ..., o n m). We are interested in models generally defined by the composite form f \u25e6g, where f is a function that returns a prediction r\u0303.\nOne implementation of g\u03c8 would process the entire contents of D without exploiting knowledge that the features in a particular row are related through their description of a common object, and would instead have to learn the appropriate parsing of inputs and any necessary sub-functions: r\u0303 = f\u03c6(g\u03c8(o 1 1, o 2 1, ..., o n m)). An alternative approach would be to impose a prior on the parsing of the input space, such that g operates on objects directly: r\u0303 = f\u03c6(g\u03c8(o1), g\u03c8(o2), ..., g\u03c8(om)). A third middle-ground approach \u2013 which is the approach taken by RNs \u2013 recognizes that relations necessarily exist in the context of a set of objects that have some capacity to be related. Thus, the computation of relations should entail some common function across these sets. For example, g may operate on pairs of objects: r\u0303 = f\u03c6(g\u03c8(o1, o2), g\u03c8(o1, o3), ..., g\u03c8(om\u22121, om)). For RNs, g is implemented as a multi-layered perceptron (MLP) that operates on pairs of objects. The same MLP operates on all possible pairings of objects from D.\nThe second defining architectural property of RN\u2019s is object permutation invariance. To incorporate this property, we further constrain our architecture with an aggregation function a that is commutative and associative: r\u0303 = f\u03c6(a(g\u03c8(o1, o2), g\u03c8(o1, o3), ..., g\u03c8(om\u22121, om))). The order invariance of the aggregation function is a critical feature of our model, since without this invariance, the model would have to learn to operate on all possible permuted pairs of objects without explicit knowledge of the permutation invariance structure in the data. A natural choice for a is summation. Thus, the model that we explore is given by r\u0303 = f\u03c6( \u2211 i,j g\u03c8(oi, oj)) where f\u03c6 and g\u03c8 are MLP\u2019s that we optimise during training."}, {"heading": "3 EXPERIMENTAL TASKS AND DATA", "text": ""}, {"heading": "3.1 DATASETS", "text": "To probe a model\u2019s ability to both infer relations from scene descriptions and implicitly use relations to solve more difficult tasks \u2013 such as one-shot learning \u2013 we first developed datasets of scene descriptions and their associated images. To generate a scene description, we first defined a graph of object relations (see figure 2). For example, suppose there are four types of squares, with each type being identified by its color. A graph description of the relations between each colored square could identify the blue square as being a parent of the orange square. If the type of relation is \u201cposition,\u201d then this particular relation would manifest as blue squares being independently positioned in the scene, and orange squares being positioned in close proximity to blue squares. Similarly, suppose we have triangles and circles as the object types, with color as the relation. If triangles are parents to circles, then the color of triangles in the scene will be randomly sampled, while the color of a circle will be derived from the color of the parent triangle. Datasets generated from graphs, then, impose solutions that depend on relative object features. That is, no information can be used from within object features alone \u2013 such as a particular coordinate position, or RGB color value.\nGraphs define generative models that can be used to produce scenes. For scenes with position relations, root node coordinates (opx, o p y), were first randomly chosen in a bounded space. Children were then randomly assigned to a particular parent object, and their position was determined as a function of the parent\u2019s position: (ocx, o c y) = (o p x + d cos(\u03b8 c), opy + d sin(\u03b8 c)). Here, \u03b8c \u223c U(\u03b8p \u2212 \u03c0/3, \u03b8p + \u03c0/3), where \u03b8p is the angle computed for the parent. For root nodes, \u03b8p \u223c U(0, 2\u03c0). d is a computed distance: d = d0 + d1, where d0 is a minimum distance to prevent significant object overlap, and d1 is sampled from a half-normal distribution. This underlying generative structure \u2013 inherit features from parents and apply noise \u2013 is used to generate scenes from graphs that define other relations, such as color. For the case of color, the inherited features are RGB values. Ultimately, scene descriptions consisted of matrices with 16 rows, with each row describing the object type: position, color, size and shape (four rows for each of four types).\nCustom datasets were required to both explicitly test solutions to the task of inferring object relations, and to actively control for solutions that do not depend on object relations. For example, consider the common scenario of child objects positioned close to a parent object, analogous to chairs positioned around a table. In this scenario, the count information of objects (i.e., that there are more child objects than parent objects) is non-relational information that can nonetheless be used to constrain the solution space; the prediction of the relation between objects doesn\u2019t entirely depend on explicitly computed relations, such as the relative distance between the child objects and the parent objects. To return to the kitchen scene analogy, one wouldn\u2019t need to know the relative distance of the chairs to the table; instead, knowing the number of chairs and number of tables could inform the relation, if it is known that less frequently occurring objects tend to be parents to more frequently occurring objects. Although information like object count can be important for solving many tasks, here we explicitly sought to test models\u2019 abilities to compute and operate on object relations.\nThe datasets will be made freely available."}, {"heading": "3.2 TASK DESCRIPTIONS", "text": "The tasks with which we assessed the RN\u2019s performance fell into three categories. The first category involved the classification of scenes. In this task, the network was given a scene description as input, with the target being a binary matrix description of the edges between object types (this is known\nas the graph adjacency matrix) \u2013 see figure 2. Training data consisted of 5000 samples derived from 5, 10, or 20 unique classes (i.e., graphs), with testing data comprising of withheld withinclass samples. Although the target was a matrix description of the generating graph, this task was fundamentally one of classification. Nonetheless, since class membership can only be determined from the relational structure of the objects within a particular sample, the ability to classify in this way is dependent on the ability to infer relations.\nThe second category of tasks tested the ability of the RN to classify scenes from unstructured input domains (see figure 3). Since RNs operate on factored object representations, this task specifically probed the ability of RNs to induce the learning of object factorizations from entangled scene descriptions. In the first set of experiments we broke the highly structured scene description input by passing it through a fixed permutation matrix. The RN must now learn to reason about a randomly permuted object feature representation. To decode the entangled state we used a linear layer whose output provided the input to the RN. We then asked the network to classify scenes, as in the previous tasks. In the second set of experiments we pushed this idea further, and tested the ability of the RN to classify scenes from pixel-level representations. To enable this, images were passed through a variational autoencoder (VAE), and the latent variables were provided as input to a RN.\nThe final category of tasks tested the implicit use of discovered relations to solve a difficult overarching problem: one-shot relation learning (Vinyals et al., 2016; Santoro et al., 2016; Lake et al., 2015). In one-shot relation learning, a memory augmented RN must learn to classify a novel scene in oneshot (after a single presentation of a novel scene from our data-set). To train a memory augmented RN for one-shot relation learning, sequences of samples were fed to a memory-augmented neural network (MANN) with a relational network pre-processor. Sequences \u2013 or episodes \u2013 consisted of 50 random samples generated from five unique graphs, from a pool of 1900 total classes, presented jointly with time-offset label identifiers, as per Hochreiter et al. (2001) and Santoro et al. (2016). Critically, the labels associated with particular classes change from episode-to-episode. Therefore, the network must depend on within-episode knowledge to perform the task; it must learn the particular label assigned to a class within an episode, and learn to assign this label to future samples of the same class, within the same episode. Labels are presented as input in a time-offset manner (that is, the correct label for the sample presented at time t is given as input to the network at time t+ 1) to enable learning of an arbitrary binding procedure. Unique information from a sample \u2013 which is necessarily something pertaining to the relations of the contained objects \u2013 must be extracted, bound to its associated label, and stored in memory. Upon subsequent presentations of samples from this same class, the network must query its memory, and use stored information to infer class membership. There is a critical difference in this task compared to the first. In this task, identifying class labels change constantly from episode-to-episode. So, the network cannot simply encode mappings from certain learned relational structures to class labels. Instead, the only way the network can\nsolve the task is to develop an ability to compare and contrast extracted relational structures between samples as they occur within an episode. Please see the appendix for more details on the task setup."}, {"heading": "4 ADDITIONAL MODEL COMPONENTS AND TRAINING DETAILS", "text": ""}, {"heading": "4.1 VARIATIONAL AUTOENCODER", "text": "For inferring relations from latent representations of pixels we used a variational autoencoder (VAE) (Kingma & Welling, 2013) with a convolutional neural network (CNN) as the feature encoder and deconvolutional network as the decoder (see figure 3b). The CNN consisted of two processing blocks. The input to each block was sent through four dimension preserving parallel convolution streams using 8 kernels of size 1x1, 3x3, 5x5, and 7x7, respectively. The outputs from these convolutions were passed through a batch normalization layer, rectified linear layer and concatenated. This was then convolved again with a down-sampling kernel of size 3x3, halving the dimension size of the input, and, except for the final output layer, again passed through batch normalization and rectified linear layers. The entire CNN consisted of two of these blocks positioned serially. Therefore, input images of size 32x32 were convolved to feature-maps of size 8x8. The feature decoder consisted of these same blocks, except convolution operations were replaced with deconvolution operations.\nThe final feature maps provided by the CNN feature encoder were then passed to a linear layer, whose outputs constituted the observed variables x for the VAE. x, which was decomposed into \u00b5 and \u03c3, was then used with an auxiliary Gaussian noise variable to infer the latent variables z = \u00b5 + \u03c3. These latent variables were then decoded to generate the reconstruction x\u0302, as per the conventional implementation of VAEs. Along a separate pathway, the latent variable representation z of an image was fed as input to a linear layer, which projected it to a higher dimensional space \u2013 the scene description D (see figure 3b). Importantly, this connection \u2013 from z to D \u2013 did not permit the backward flow of gradients to the VAE. This prevented the VAE architecture from contributing to the RN\u2019s disentangling solution."}, {"heading": "4.2 MEMORY-AUGMENTED NEURAL NETWORK", "text": "For implicit discovery of relations from scene descriptions, the RN was used as a pre-processor for a memory-augmented neural network (MANN). The MANN was implemented as in Santoro et al. (2016), and the reader is directed here for full details on using networks augmented with external memories. Briefly, the core MANN module consists of a controller \u2013 a long-short term memory (LSTM) (Hochreiter & Schmidhuber, 1997) \u2013 that interacts with read and write heads, which in turn interact with an external memory store. The external memory store consists of a number of memory slots, each of which contains a vector \u201cmemory.\u201d During reading, the LSTM takes in an input and produces a query vector, which the read head uses to query the external memory store using a cosine distance across the vectors stored in the memory slots, and returns a weighted sum of these vectors based on the cosine distance. During writing, the LSTM outputs a vector that the write head uses to write into the memory store using a least recently used memory access mechanism (Santoro et al., 2016)."}, {"heading": "4.3 TRAINING DETAILS", "text": "The sizes of the RN \u2013 in terms of number of layers and number of units for both f\u03c6 and g\u03c8 \u2013 were {200, 200}, {500, 500}, {1000, 1000}, or {200, 200, 200}. We also trained a MLP baseline (for the sake of comparison) using equivalent network sizes. We experimented with different sizes for the output of g\u03c8 . Performance is generally robust to the choice of size, with similar results emerging for 100, 200, or 500. The MANN used a LSTM controller size of 200, 128 memory slots, 40 for the memory size, and 4 read and write heads.\nThe Adam optimizer was used for optimization (Kingma & Ba, 2014), with learning rate of 1e\u22124 for the scene description tasks, and a learning rate of 1e\u22125 for the one-shot learning task. The number of iterations varied for each experiment, and are indicated in the relevant figures. All figures show performance on a withheld test-set, constituting 2-5% of the size of the training set. The number of training samples was 5000 per class for scene description tasks, and 200 per class (for 100 classes)\nfor the pixel disentangling experiment. We used minibatch training, with batch-sizes of 100 for the scene description experiments, and 16 (with sequences of length 50) for the one-shot learning task."}, {"heading": "5 RESULTS", "text": ""}, {"heading": "5.1 SUPERVISED LEARNING OF RELATIONAL STRUCTURE FROM SCENE DESCRIPTIONS", "text": "We begin by exploring the ability of RNs to learn the relation structure of scenes, by training RNs to classify object relations (the adjacecy matrix) of a scene description, as descriped in section 3.2. As a baseline comparison, we contrasted their performance with that of MLPs of different sizes and depths. First, we compared the performance of these models on scenes where the relational structure was defined by position (figure 4a). After 200,000 iterations the RNs reached a cross entropy loss of 0.01 on a withheld test set, with MLPs of similar size failing to match this performance (figure 4a, top). In fact, the smallest RN performed significantly better than the largest MLP. The performance of the MLPs remained poor even after 1 million iterations (cross entropy loss above 0.2 \u2013 not shown). This result was consistent for datasets with 5, 10 and 20 scene classes (figure 4a, bottom).\nThese results were obtained using targets that explicitly described the relations between objects (the adjacency matrix). Using one-hot vectors as output targets for classification resulted in very similar results, with the RNs\u2019 accuracy reaching 97% (see figure 8 in appendix). We use adjacency matrix graph descriptions as a target for learning because this target explicitly probes that ability of the RN model to represent relations between individual pairs of object types independent of other relations that might be present in a scene. Explicitly targeting the individual relations could in principle allow the model to learn the particular components that form the overall scene structure. Indeed, when trained to classify hundreds of scene classes using the adjacency matrix for specifying the class, the RN was then able to generalize to unobserved classes (see figure 5). (Note: this generalization to unseen classes would be impossible with the use of one-hot labels for classification targets, since there is no training information for unseen classes in that case). Additionally, the ability to generalize to unobserved classes suggests that the RN is able to generalize in a combinatorially complex objectrelation space because of its ability to learn compositional object-relation structure. It is able to use pieces (i.e., specific relations) of learned information and combine them in unique, never-beforeseen ways, which is a hallmark feature of compositional learning.\nWe repeated this experiment using relational structure defined by the color of the objects, with position being randomly sampled for each object (figure 4b). Again, RNs reached a low classification error on withheld data (below 0.01), whereas the MLP models did not (error above 0.2; figure 4b, top). This was observed across datasets with 5, 10 and 20 classes (figure 4b, bottom)."}, {"heading": "5.2 INFERRING OBJECT RELATIONS FROM NON-SCENE DESCRIPTION INPUTS", "text": "Next, we explore the ability of RNs to infer object relations directly from input data that is not nicely organized into a scene description matrix of factored object representations. This is a difficult problem because RNs require a scene description matrix of objects as input. In most applications, a scene description matrix is not directly available, and so, we must augment our RN with a mechanism for transforming entangled representations (such as pixel images) directly into a representation that has the properties of a scene description matrix. Specifically, we require a transformation that produces an m\u00d7 n dimensional matrix D whose rows can be interpreted as objects, and whose columns can be interpreted as features.\nIn this section, we explore architectures that can support object relation reasoning from entangled representations. We also explore the extent to which RNs can act as an architectural bottleneck for inducing object-like representations in the RN input layer. We will consider two types of datasets: pixel image representations and entangled scene descriptions."}, {"heading": "5.2.1 INFERRING RELATIONS FROM ENTANGLED SCENE DESCRIPTIONS", "text": "In this task we probed the network\u2019s ability to classify scenes from entangled scene descriptions. Intuitively, the RN should behave as an architectural bottleneck that can aid the disentangling of objects by a downstream perceptual model to produce factored object representations on which it can operate. To create an entangled scene description data-set we started from the scene description matrix\nD, and reshaped it into a vector of sizemn (i.e., a concatenated vector of objects [o1; o2; o3; ..; om]). We then transformed this vector with a random permutation matrix B of size mn\u00d7mn. We chose a permutation matrix for entanglement because it preserves all of the input information without scaling between factors, and also, because it is invertable. Therefore, it should be possible to train a linear layer to disentangle the entangled scenes into a format suitable for RN\u2019s.\nFollowing this, we augment our RN with a downstream linear layer positioned before the RN and after the entangled scene data. The linear layer is represented by a learnable matrix U of size mn\u00d7mn (without biases) (see figure 3 (a)). This linear layer produces a mn dimensional vector as output. This output vector is reshaped into a matrix of size m\u00d7 n before being provided as input to the RN.\nThis augmented RN successfully learns to classify entangled scene descriptions (figure 6a). During training, the linear layer learns to provide a factorised representation of m objects to the RN (figure 6a, inset). To illustrate this we visualized the absolute value of UB (figure 6a, inset). We noticed a block structure emerging (where white pixels denote a value of 0, and black pixels denote a value of 1). The block structure of UB suggests that object k, as perceived by the network in the linear output layer, was a linear transformation (given by the block) of object l from the ground truth input inD. This gradual discovery of new objects over time is particularly interesting. It is also interesting to note that since the RN is order invariant, there is no pressure to recover the ground truth order of the objects. We can see that this is the case, because the matrix UB is not block-diagonal. Also, the exact order of features that define the object were not disentangled, since there is no pressure from the RN architecture for this to happen. In this way, RNs manage to successfully force the identification of different objects without imposing a particular object ordering or feature ordering within objects."}, {"heading": "5.2.2 INFERRING RELATIONS FROM PIXELS", "text": "Next, we explore the ability of the RN to classify scenes from pixel image representations of scenes. We use image depictions from 100 unique scene classes from our position-relation dataset (figure 11 in appendix).\nImage representations of scenes cannot be directly fed to a RN because the objects in images are not represented in a factorised form. We need to augment our RN with an image preprocessing mechanism that has the capacity to produce factorised object representations. For these purposes, we augment our RN with a variational autoencoder (VAE), whose latent variables provide input to a linear layer, which in turn provides input to the RN (similar to the linear layer in the previous section) (figure 3 (b)). Both the VAE and RN were trained concurrently \u2013 however, gradients from the RN were not propagated to the VAE portion of the model. The VAE learns to produce a entangled latent representation of the input images. There is no pressure for the VAE to produce a representation that\nhas factorised objects. Instead, as in the previous section, the linear layer must learn to disentagle the image representation into a object-factorised representation suitable for the RN.\nBy preventing gradients from the RN to propogate through to the VAE portion of the model, we prevent any of the VAE components from contributing to the solution of the RN\u2019s task. Thus, this experiment explicitly tested the ability of the RN to operate on distributed representations of scenes. It should be noted that this task is different from the previous entangling tasks, as the VAE, in principle, may be capable of providing both disentangled object representations, as well as relational information in its compressed latent code. Nonetheless, the results from this task demonstrate the capacity for RNs to operate on distributed representations, which opens the door to the possible conjunction of RNs with perceptual neural network modules (figure 6b)."}, {"heading": "5.3 IMPLICIT USE OF RELATIONS FOR ONE-SHOT LEARNING", "text": "Finally, we assess the potential to use RNs in conjunction with memory-augmented neural networks to quickly \u2013 and implicitly \u2013 discover object relations and use that knowledge for one-shot learning.\nWe trained a MANN with a RN pre-processor to do one-shot classification of scenes, as described in section 3.2. In order to solve this task, the network must store representations of the scenes (which, if class representations are to be unique, should necessarily contain relational information), and the episode-unique label associated with the scene. Once a new sample of the same class is observed, it must use inferred relational information from this sample to query its memory and retrieve the appropriate label.\nDuring training, a sequence of 50 random samples from 5 random classes (out of a pool of 1900) were picked to constitute an episode. The test phase consisted of episodes with scenes from 5 randomly selected and previously unobserved classes (out of a pool of 100). After 500,000 episodes the network showed high classification accuracy when presented with just the second instance of a class (76%) and performance reached 93% and 96% by the 5th and 10th instance, respectively (figure 7a). As expected, since class labels change from episode-to-episode, performance is at chance for the first instance of a particular class.\nAlthough samples from a class are visually very dissimilar, a RN coupled to an external memory was able to do one-shot classification. This suggests that the network has the capacity to quickly extract information about the relational structure of objects, which is what defines class membership, and does so without explicit supervision about the object-relation structure. Replacing the RN preprocessor with an MLP resulted in chance performance across all instances (figure 7b), suggesting that the RN is a critical component of this memory-augmented model."}, {"heading": "6 CONCLUSIONS", "text": "RNs are a powerful architecture for reasoning about object-relations. Architecturally, they operate on pairs of objects (or object-like entities) and they enforce permutation invariance of objects. This architecture naturally supports classification of object-relation structure in scenes. RNs can also be augmented with additional modules to support a wider range of tasks. For example, we have demonstrated that RNs can be augmented with MANNs to support one-shot relation learning.\nRNs can be used in conjunction with perceptual modules for reasoning about object-relations in images and other entangled representations. In doing so, they can induce factored object representations of entangled scenes. The form of these factored \u201cobjects\u201d can be quite flexible, and may permit representations of inputs that are not localised spatially, or that consist of multiple disjoint entities. In future work, it would be interesting to explore the range of \u201cobjects\u201d that RN\u2019s augmented with perceptual modules can discover across a variety of datasets.\nThe utility of the RN as a relation-reasoning module suggests that it has the potential to be useful for solving tasks that require reasoning not only about object-object relations, but also about verbobject relations, as in human-object interaction datasets (Chao et al., 2015) or question-answering tasks that involve reasoning between multiple objects (Krishna et al., 2016). Leveraging RNs for these problem domains represents an exciting direction for future investigation."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Scott Reed, Daan Wierstra, Nando de Freitas, James Kirkpatrick, and many others on the DeepMind team."}], "references": [{"title": "Interaction networks for learning about objects, relations and physics", "author": ["Peter Battaglia", "Razvan Pascanu", "Matthew Lai", "Danilo Jimenez Rezende", "Koray Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Battaglia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Battaglia et al\\.", "year": 2016}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["Joan Bruna", "Wojciech Zaremba", "Arthur Szlam", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.6203,", "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "Hico: A benchmark for recognizing human-object interactions in images", "author": ["Yu-Wei Chao", "Zhan Wang", "Yugeng He", "Jiaxuan Wang", "Jia Deng"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Chao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chao et al\\.", "year": 2015}, {"title": "A tree-based context model for object recognition", "author": ["Myung Jin Choi", "Antonio Torralba", "Alan S Willsky"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Choi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2012}, {"title": "Convolutional neural networks on graphs with fast localized spectral filtering", "author": ["Micha\u00ebl Defferrard", "Xavier Bresson", "Pierre Vandergheynst"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Defferrard et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Defferrard et al\\.", "year": 2016}, {"title": "Convolutional networks on graphs for learning molecular fingerprints", "author": ["David K Duvenaud", "Dougal Maclaurin", "Jorge Iparraguirre", "Rafael Bombarell", "Timothy Hirzel", "Al\u00e1n Aspuru-Guzik", "Ryan P Adams"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Duvenaud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2015}, {"title": "Graph based convolutional neural network", "author": ["Michael Edwards", "Xianghua Xie"], "venue": "arXiv preprint arXiv:1609.08965,", "citeRegEx": "Edwards and Xie.,? \\Q2016\\E", "shortCiteRegEx": "Edwards and Xie.", "year": 2016}, {"title": "A discriminatively trained, multiscale, deformable part model", "author": ["Pedro Felzenszwalb", "David McAllester", "Deva Ramanan"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2008}, {"title": "Characterizing structural relationships in scenes using graph kernels", "author": ["Matthew Fisher", "Manolis Savva", "Pat Hanrahan"], "venue": "In ACM Transactions on Graphics (TOG),", "citeRegEx": "Fisher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fisher et al\\.", "year": 2011}, {"title": "Examplebased synthesis of 3d object arrangements", "author": ["Matthew Fisher", "Daniel Ritchie", "Manolis Savva", "Thomas Funkhouser", "Pat Hanrahan"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "Fisher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fisher et al\\.", "year": 2012}, {"title": "Deep convolutional networks on graph-structured data", "author": ["Mikael Henaff", "Joan Bruna", "Yann LeCun"], "venue": "arXiv preprint arXiv:1506.05163,", "citeRegEx": "Henaff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Learning to learn using gradient descent", "author": ["Sepp Hochreiter", "A Steven Younger", "Peter R Conwell"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Semi-supervised classification with graph convolutional networks", "author": ["Thomas N Kipf", "Max Welling"], "venue": "arXiv preprint arXiv:1609.02907,", "citeRegEx": "Kipf and Welling.,? \\Q2016\\E", "shortCiteRegEx": "Kipf and Welling.", "year": 2016}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "Krishna et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishna et al\\.", "year": 2016}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Brenden M Lake", "Ruslan Salakhutdinov", "Joshua B Tenenbaum"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Building machines that learn and think like people", "author": ["Brenden M Lake", "Tomer D Ullman", "Joshua B Tenenbaum", "Samuel J Gershman"], "venue": "arXiv preprint arXiv:1604.00289,", "citeRegEx": "Lake et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "Gated graph sequence neural networks", "author": ["Yujia Li", "Daniel Tarlow", "Marc Brockschmidt", "Richard Zemel"], "venue": "arXiv preprint arXiv:1511.05493,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Creating consistent scene graphs using a probabilistic grammar", "author": ["Tianqiang Liu", "Siddhartha Chaudhuri", "Vladimir G Kim", "Qixing Huang", "Niloy J Mitra", "Thomas Funkhouser"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Metalearning with memory-augmented neural networks", "author": ["Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy Lillicrap"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Santoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "The graph neural network model", "author": ["Franco Scarselli", "Marco Gori", "Ah Chung Tsoi", "Markus Hagenbuchner", "Gabriele Monfardini"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Scarselli et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Scarselli et al\\.", "year": 2009}, {"title": "Origins of knowledge", "author": ["Elizabeth S Spelke", "Karen Breinlinger", "Janet Macomber", "Kristen Jacobson"], "venue": "Psychological review,", "citeRegEx": "Spelke et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Spelke et al\\.", "year": 1992}, {"title": "Learning design patterns with bayesian grammar induction", "author": ["Jerry Talton", "Lingfeng Yang", "Ranjitha Kumar", "Maxine Lim", "Noah Goodman", "Radom\u0131\u0301r M\u011bch"], "venue": "In Proceedings of the 25th annual ACM symposium on User interface software and technology,", "citeRegEx": "Talton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Talton et al\\.", "year": 2012}, {"title": "How to grow a mind: Statistics, structure, and abstraction", "author": ["Joshua B Tenenbaum", "Charles Kemp", "Thomas L Griffiths", "Noah D Goodman"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2011}, {"title": "Matching networks for one shot learning", "author": ["Oriol Vinyals", "Charles Blundell", "Timothy Lillicrap", "Koray Kavukcuoglu", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1606.04080,", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "Synthesizing open worlds with constraints using locally annealed reversible jump mcmc", "author": ["Yi-Ting Yeh", "Lingfeng Yang", "Matthew Watson", "Noah D Goodman", "Pat Hanrahan"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "Yeh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yeh et al\\.", "year": 2012}, {"title": "Make it home: automatic optimization of furniture", "author": ["Lap Fai Yu", "Sai Kit Yeung", "Chi Keung Tang", "Demetri Terzopoulos", "Tony F Chan", "Stanley J Osher"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2011}, {"title": "The one-shot learning task proceeds as in Hochreiter et al", "author": ["Santoro"], "venue": null, "citeRegEx": "Santoro,? \\Q2016\\E", "shortCiteRegEx": "Santoro", "year": 2016}], "referenceMentions": [{"referenceID": 23, "context": "The ability to reason about objects and relations is important for solving a wide variety of tasks (Spelke et al., 1992; Lake et al., 2016).", "startOffset": 99, "endOffset": 139}, {"referenceID": 18, "context": "The ability to reason about objects and relations is important for solving a wide variety of tasks (Spelke et al., 1992; Lake et al., 2016).", "startOffset": 99, "endOffset": 139}, {"referenceID": 25, "context": "For example, object relations enable the transfer of learned knowledge across superficial dissimilarities (Tenenbaum et al., 2011): the predator-prey relationship between a lion and a zebra is knowledge that is similarly useful when applied to a bear and a salmon, even though many features of these animals are very different.", "startOffset": 106, "endOffset": 130}, {"referenceID": 0, "context": "In designing the RN architecture, we took inspiration from the recently developed Interaction Network (IN) (Battaglia et al., 2016) which was applied to modelling physical, spatiotemporal interactions.", "startOffset": 107, "endOffset": 131}, {"referenceID": 22, "context": "Our model is also related to various other approaches that apply neural networks directly to graphs (Scarselli et al., 2009; Bruna et al., 2013; Li et al., 2015; Duvenaud et al., 2015; Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2016; Edwards & Xie, 2016).", "startOffset": 100, "endOffset": 273}, {"referenceID": 1, "context": "Our model is also related to various other approaches that apply neural networks directly to graphs (Scarselli et al., 2009; Bruna et al., 2013; Li et al., 2015; Duvenaud et al., 2015; Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2016; Edwards & Xie, 2016).", "startOffset": 100, "endOffset": 273}, {"referenceID": 19, "context": "Our model is also related to various other approaches that apply neural networks directly to graphs (Scarselli et al., 2009; Bruna et al., 2013; Li et al., 2015; Duvenaud et al., 2015; Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2016; Edwards & Xie, 2016).", "startOffset": 100, "endOffset": 273}, {"referenceID": 5, "context": "Our model is also related to various other approaches that apply neural networks directly to graphs (Scarselli et al., 2009; Bruna et al., 2013; Li et al., 2015; Duvenaud et al., 2015; Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2016; Edwards & Xie, 2016).", "startOffset": 100, "endOffset": 273}, {"referenceID": 10, "context": "Our model is also related to various other approaches that apply neural networks directly to graphs (Scarselli et al., 2009; Bruna et al., 2013; Li et al., 2015; Duvenaud et al., 2015; Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2016; Edwards & Xie, 2016).", "startOffset": 100, "endOffset": 273}, {"referenceID": 4, "context": "Our model is also related to various other approaches that apply neural networks directly to graphs (Scarselli et al., 2009; Bruna et al., 2013; Li et al., 2015; Duvenaud et al., 2015; Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2016; Edwards & Xie, 2016).", "startOffset": 100, "endOffset": 273}, {"referenceID": 28, "context": "In graphics, a number of approaches have been used to capture contextual scene structure, such as energy models (Yu et al., 2011), graphical and mixture models (Fisher et al.", "startOffset": 112, "endOffset": 129}, {"referenceID": 8, "context": ", 2011), graphical and mixture models (Fisher et al., 2011; 2012), stochastic grammars (Liu et al.", "startOffset": 38, "endOffset": 65}, {"referenceID": 20, "context": ", 2011; 2012), stochastic grammars (Liu et al., 2014), and probabilistic programs (Talton et al.", "startOffset": 35, "endOffset": 53}, {"referenceID": 24, "context": ", 2014), and probabilistic programs (Talton et al., 2012; Yeh et al., 2012).", "startOffset": 36, "endOffset": 75}, {"referenceID": 27, "context": ", 2014), and probabilistic programs (Talton et al., 2012; Yeh et al., 2012).", "startOffset": 36, "endOffset": 75}, {"referenceID": 21, "context": "Finally, we combine RNs with memoryaugmented neural networks (MANNs) (Santoro et al., 2016) to solve a difficult one-shot learning task, demonstrating the ability of RNs\u2019 to act in conjunction with other neural network architectures to rapidly discover new object relations from entirely new scenes.", "startOffset": 69, "endOffset": 91}, {"referenceID": 6, "context": "(2009) and Zhao & Zhu (2011) modelled relations among image features using stochastic grammars, Felzenszwalb et al. (2008) modelled relations among object parts using the \u201cdeformable parts\u201d model, and Choi et al.", "startOffset": 96, "endOffset": 123}, {"referenceID": 3, "context": "(2008) modelled relations among object parts using the \u201cdeformable parts\u201d model, and Choi et al. (2012) modelled relations among objects in scenes using tree structured context models.", "startOffset": 85, "endOffset": 104}, {"referenceID": 0, "context": "1 DESCRIPTION AND IMPLEMENTATION RNs are inspired by Interaction Networks (INs) (Battaglia et al., 2016), and therefore share similar functional insights.", "startOffset": 80, "endOffset": 104}, {"referenceID": 26, "context": "The final category of tasks tested the implicit use of discovered relations to solve a difficult overarching problem: one-shot relation learning (Vinyals et al., 2016; Santoro et al., 2016; Lake et al., 2015).", "startOffset": 145, "endOffset": 208}, {"referenceID": 21, "context": "The final category of tasks tested the implicit use of discovered relations to solve a difficult overarching problem: one-shot relation learning (Vinyals et al., 2016; Santoro et al., 2016; Lake et al., 2015).", "startOffset": 145, "endOffset": 208}, {"referenceID": 17, "context": "The final category of tasks tested the implicit use of discovered relations to solve a difficult overarching problem: one-shot relation learning (Vinyals et al., 2016; Santoro et al., 2016; Lake et al., 2015).", "startOffset": 145, "endOffset": 208}, {"referenceID": 12, "context": "Sequences \u2013 or episodes \u2013 consisted of 50 random samples generated from five unique graphs, from a pool of 1900 total classes, presented jointly with time-offset label identifiers, as per Hochreiter et al. (2001) and Santoro et al.", "startOffset": 188, "endOffset": 213}, {"referenceID": 12, "context": "Sequences \u2013 or episodes \u2013 consisted of 50 random samples generated from five unique graphs, from a pool of 1900 total classes, presented jointly with time-offset label identifiers, as per Hochreiter et al. (2001) and Santoro et al. (2016). Critically, the labels associated with particular classes change from episode-to-episode.", "startOffset": 188, "endOffset": 239}, {"referenceID": 21, "context": "During writing, the LSTM outputs a vector that the write head uses to write into the memory store using a least recently used memory access mechanism (Santoro et al., 2016).", "startOffset": 150, "endOffset": 172}, {"referenceID": 21, "context": "The MANN was implemented as in Santoro et al. (2016), and the reader is directed here for full details on using networks augmented with external memories.", "startOffset": 31, "endOffset": 53}, {"referenceID": 2, "context": "The utility of the RN as a relation-reasoning module suggests that it has the potential to be useful for solving tasks that require reasoning not only about object-object relations, but also about verbobject relations, as in human-object interaction datasets (Chao et al., 2015) or question-answering tasks that involve reasoning between multiple objects (Krishna et al.", "startOffset": 259, "endOffset": 278}, {"referenceID": 16, "context": ", 2015) or question-answering tasks that involve reasoning between multiple objects (Krishna et al., 2016).", "startOffset": 84, "endOffset": 106}], "year": 2017, "abstractText": "Our world can be succinctly and compactly described as structured scenes of objects and relations. A typical room, for example, contains salient objects such as tables, chairs and books, and these objects typically relate to each other by their underlying causes and semantics. This gives rise to correlated features, such as position, function and shape. Humans exploit knowledge of objects and their relations for learning a wide spectrum of tasks, and more generally when learning the structure underlying observed data. In this work, we introduce relation networks (RNs) a general purpose neural network architecture for object-relation reasoning. We show that RNs are capable of learning object relations from scene description data. Furthermore, we show that RNs can act as a bottleneck that induces the factorization of objects from entangled scene description inputs, and from distributed deep representations of scene images provided by a variational autoencoder. The model can also be used in conjunction with differentiable memory mechanisms for implicit relation discovery in one-shot learning tasks. Our results suggest that relation networks are a potentially powerful architecture for solving a variety of problems that require object relation reasoning.", "creator": "LaTeX with hyperref package"}}}