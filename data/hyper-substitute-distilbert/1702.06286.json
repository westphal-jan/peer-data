{"id": "1702.06286", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2017", "title": "Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection", "abstract": "sound events easily occur in acoustic environments whereas they exhibit wide difficulties in carrier frequency content and temporal interaction. enhanced communications networks ( cnn ) are able better extract higher noise features that are tuned to local or dominant temporal connectivity. adaptive telecommunication networks ( cn ) grow easier in learning the longer individual temporal relationships in the processing streams. cnns and rnns encoding classifiers demonstrate recently consolidated improved advantages over established methods in various sound recognition tasks. we improve on two approaches before selecting convolutional recurrent audio network ( sts ) and task it on diverse polyphonic signaling event detection tasks. we reconcile approximate performance of the proposed crnn candidate with one, rnn, considering other established methods, then observe comparatively considerable improvement for four different datasets implementing specific everyday traffic events.", "histories": [["v1", "Tue, 21 Feb 2017 07:37:59 GMT  (2893kb,D)", "http://arxiv.org/abs/1702.06286v1", "Accepted for IEEE Transactions on Audio, Speech and Language Processing, Special Issue on Sound Scene and Event Analysis"]], "COMMENTS": "Accepted for IEEE Transactions on Audio, Speech and Language Processing, Special Issue on Sound Scene and Event Analysis", "reviews": [], "SUBJECTS": "cs.LG cs.SD", "authors": ["emre \\c{c}ak{\\i}r", "giambattista parascandolo", "toni heittola", "heikki huttunen", "tuomas virtanen"], "accepted": false, "id": "1702.06286"}, "pdf": {"name": "1702.06286.pdf", "metadata": {"source": "CRF", "title": "Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection", "authors": ["Emre \u00c7ak\u0131r", "Giambattista Parascandolo", "Toni Heittola", "Heikki Huttunen", "Tuomas Virtanen"], "emails": ["emre.cakir@tut.fi."], "sections": [{"heading": null, "text": "Index Terms\u2014sound event detection, deep neural networks, convolutional neural networks, recurrent neural networks\nI. INTRODUCTION\nIN our daily lives, we encounter a rich variety of soundevents such as dog bark, footsteps, glass smash and thunder. Sound event detection (SED), or acoustic event detection, deals with the automatic identification of these sound events. The aim of SED is to detect the onset and offset times for each sound event in an audio recording and associate a textual descriptor, i.e., a label for each of these events. SED has been drawing a surging amount of interest in recent years with applications including audio surveillance [1], healthcare monitoring [2], urban sound analysis [3], multimedia event detection [4] and bird call detection [5].\nIn the literature the terminology varies between authors; common terms being sound event detection, recognition, tagging and classification. Sound events are defined with predetermined labels called sound event classes. In our work, sound event classification, sound event recognition, or sound event tagging, all refer to labeling an audio recording with the sound event classes present, regardless of the onset/offset times. On the other hand, an SED task includes both onset/offset detection for the classes present in the recording\nG. Parascandolo and E. Cakir contributed equally to this work. The authors are with the Department of Signal Processing, Tampere University of Technology (TUT), Finland e-mail: emre.cakir@tut.fi. The research leading to these results has received funding from the European Research Council under the European Unions H2020 Framework Programme through ERC Grant Agreement 637422 EVERYSOUND.\nG. Parascandolo has been funded by Google\u2019s Faculty Research award. The authors wish to acknowledge CSC IT Center for Science, Finland, for\ncomputational resources. The paper has a supporting website at http://www.cs.tut.fi/sgn/arg/taslp2017-crnn-sed/ Manuscript received July 12, 2016 (revised January 19, 2016).\nand classification within the estimated onset/offset, which is typically the requirement in a real-life scenario.\nSound events often occur in unstructured environments in real-life. Factors such as environmental noise and overlapping sources are present in the unstructured environments and they may introduce a high degree of variation among the sound events from the same sound event class [6]. Moreover, there can be multiple sound sources that produce sound events belonging to the same class, e.g., a dog bark sound event can be produced from several breeds of dogs with different acoustic characteristics. These factors mainly represent the challenges over SED in real-life situations.\nSED where at most one simultaneous sound event is detected at a given time instance is called monophonic SED. Monophonic SED systems can only detect at most one sound event for any time instance regardless of the number of sound events present. If the aim of the system is to detect all the events happening at a time, this is a drawback concerning the real-life applicability of such systems, because in such a scenario, multiple sound events are very likely to overlap in time. For instance, an audio recording from a busy street may contain footsteps, speech and car horn, all appearing as a mixture of events. An illustration of a similar situation is given in Figure 1, where as many as three different sound events appear at the same time in a mixture. A more suitable method for such a real-life scenario is polyphonic SED, where multiple overlapping sound events can be detected at any given time instance.\nSED can be approached either as scene-dependent or sceneindependent. In the former, the information about the acoustic scene is provided to the system both at training and test time, and a different model can therefore be trained for each scene. In the latter, there is no information about the acoustic scene given to the system.\nPrevious work on sound events has been mostly focused on sound event classification, where audio clips consisting of sound events are classified. Apart from established classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed. Initially, the interest on SED was more focused on monophonic SED. Gaussian mixture model (GMM) - Hidden Markov model (HMM) based modeling\u2014an established method that has been widely used in automatic speech recognition\u2014has been proposed to model individual sound events with Gaussian mixtures and detect each event through HMM states using Viterbi algorithm [12], [13]. With the emergence of more\nar X\niv :1\n70 2.\n06 28\n6v 1\n[ cs\n.L G\n] 2\n1 Fe\nb 20\n17\n2\nadvanced deep learning techniques and publicly available reallife databases that are suitable for the task, polyphonic SED has attracted more interest in recent years. Non-negative matrix factorization (NMF) based source separation [14] and deep learning based methods (such as feedforward neural networks (FNN) [15], CNN [16] and RNN [11]) have been shown to perform significantly better compared to established methods such as GMM-HMM for polyphonic SED.\nDeep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23]. In most of these domains, deep learning represents the state-of-the-art.\nFeedforward neural networks have been used in monophonic [7] and polyphonic SED in real-life environments [15] by processing concatenated input frames from a small time window of the spectrogram. This simple architecture\u2014while vastly improving over established approaches such as GMMHMMs [24] and NMF source separation based SED [25], [26]\u2014presents two major shortcomings: (1) it lacks both time and frequency invariance\u2014due to the fixed connections between the input and the hidden units\u2014which would allow to model small variations in the events; (2) temporal context is restricted to short time windows, preventing effective modeling of typically longer events (e.g., rain) and events correlations.\nCNNs [27] can address the former limitation by learning filters that are shifted in both time and frequency [8], lacking however longer temporal context information. Recurrent neural networks (RNNs), which have been successfully applied to automatic speech recognition (ASR) [20] and polyphonic SED [11], solve the latter shortcoming by integrating information from the earlier time windows, presenting a theoretically unlimited context information. However, RNNs do not easily capture the invariance in the frequency domain, rendering a high-level modeling of the data more difficult. In order to benefit from both approaches, the two architectures can be combined into a single network with convolutional layers followed by recurrent layers, often referred to as convolutional recurrent neural network (CRNN). Similar approaches combining CNNs and RNNs have been presented recently in ASR [21], [28], [29] and music classification [30].\nIn this paper we propose the use of multi-label convolutional\nrecurrent neural network for polyphonic, scene-independent sound event detection in real-life recordings. This approach integrates the strengths of both CNNs and RNNs, which have shown excellent performance in acoustic pattern recognition applications [4], [8], [9], [10], while overcoming their individual weaknesses. We evaluate the proposed method on three datasets of real-life recordings and compare its performance to FNN, CNN, RNN and GMM baselines. The proposed method is shown to outperform previous sound event detection approaches.\nThe rest of the paper is organized as follows. In Section II the problem of polyphonic SED in real-life environments is described formally and the CRNN architecture proposed for the task is presented. In Section III we present the evaluation framework used to measure the performance of the different neural networks architectures. In Section IV experimental results, discussions over the results and comparisons with baseline methods are reported. In Section V we summarize our conclusions from this work."}, {"heading": "II. METHOD", "text": ""}, {"heading": "A. Problem formulation", "text": "The aim of polyphonic SED is to temporally locate and label the sound event classes present in a polyphonic audio signal. Polyphonic SED can be formulated in two stages: sound representation and classification. In sound representation stage, frame-level sound features (such as mel band energies and mel frequency cepstral coefficients (MFCC)) are extracted for each time frame t in the audio signal to obtain a feature vector xt \u2208 RF , where F \u2208 N is the number of features per frame. In the classification stage, the task is to estimate the probabilities p(yt(k) | xt,\u03b8) for event classes k = 1, 2, . . . ,K in frame t, where \u03b8 denotes the parameters of the classifier. The event activity probabilities are then binarized by thresholding, e.g. over a constant, to obtain event activity predictions y\u0302t \u2208 RK .\nThe classifier parameters \u03b8 are trained by supervised learning, and the target outputs yt for each frame are obtained from the onset/offset annotations of the sound event classes. If class k is present during frame t, yt(k) will be set to 1, and 0 otherwise. The trained model will then be used to predict the activity of the sound event classes when the onset/offset annotations are unavailable, as in real-life situations.\nFor polyphonic SED, the target binary output vector yt can have multiple non-zero elements since several classes can be present in the same frame t. Therefore, polyphonic SED can be formulated as a multi-label classification problem in which the sound event classes are located by multi-label classification over consecutive time frames. By combining the classification results over consecutive time frames, the onset/offset times for each class can be determined.\nSound events possess temporal characteristics that can be beneficial for SED. Certain sound events can be easily distinguished by their impulsive characteristics (e.g., glass smash), while some sound events typically continue for a long time period (e.g. rain). Therefore, classification methods that can preserve the temporal context along the sequential feature vectors are very suitable for SED. For these methods, the input\n3 features are presented as a context window matrix Xt:t+T\u22121, where T \u2208 N is the number of frames that defines the sequence length of the temporal context, and the target output matrix Yt:t+T\u22121 is composed of the target outputs yt from frames t to t + T \u2212 1. For the sake of simplicity and ease of notation, X will be used to denote Xt:t+T\u22121\u2014and similarly Y for Yt:t+T\u22121\u2014 throughout the rest of the paper."}, {"heading": "B. Proposed Method", "text": "The CRNN proposed in this work, depicted in Fig. 2, consists of four parts: (1) at the top of the architecture, a timefrequency representation of the data (a context window of F log mel band energies over T frames) is fed to Lc \u2208 N convolutional layers with non-overlapping pooling over frequency axis; (2) the feature maps of the last convolutional layer are stacked over the frequency axis and fed to Lr \u2208 N recurrent layers; (3) a single feedforward layer with sigmoid activation reads the final recurrent layer outputs and estimates event activity probabilities for each frame and (4) event activity probabilities are binarized by thresholding over a constant to obtain event activity predictions.\nIn this structure the convolutional layers act as feature extractors, the recurrent layers integrate the extracted features over time thus providing the context information, and finally the feedforward layer produce the activity probabilities for each class. The stack of convolutional, recurrent and feedforward layers is trained jointly through backpropagation. Next, we present the general network architecture in detail for each of the four parts in the proposed method.\n1) Convolutional layers: Context window of log mel band energies X \u2208 RF\u00d7T is fed as input to the CNN layers with two-dimensional convolutional filters. For each CNN layer, after passing the feature map outputs through an activation function (rectified linear unit (ReLU) used in this work), nonoverlapping max pooling is used to reduce the dimensionality of the data and to provide more frequency invariance. As depicted in Fig. 2, the time dimension is maintained intact (i.e. does not shrink) by computing the max pooling operation in the frequency dimension only\u2014as done in [21], [31]\u2014and by zero-padding the inputs to the convolutional layers (also known as same convolution). This is done in order to preserve alignment between each target output vector yt and hidden activations ht.\nAfter Lc convolutional layers, the output of the CNN is a tensor H \u2208 RM\u00d7F \u2032\u00d7T , where M is the number of feature maps for the last CNN layer, and F \u2032 is the number of frequency bands remaining after several pooling operations through CNN layers.\n2) Recurrent layers: After stacking the feature map outputs over the frequency axis, the CNN output H \u2208 R(M \u00b7F \u2032)\u00d7T for layer Lc is fed to the RNN as a sequence of frames hLct . The RNN part consists of Lr stacked recurrent layers each computing and outputting a hidden vector ht for each frame as\nInput\nStacking\nRecurrent layer activations\n(2)\nEvent activity predictions\n(4)\n(3) Feed forward layer activations\nFrequency max pooling\n(1)\nFrequency max pooling\nConvolution\nConvolution\nT\nF\nF' M\nK\nK\nT\nThe function F , which can represent a long short term memory (LSTM) unit [32] or gated recurrent unit (GRU) [33], has two inputs: The output of the current frame of the previous layer (e.g., hLct ), and the output of the previous frame of the current layer (e.g., hLc+1t\u22121 ).\n4 3) Feedforward layer: recurrent layers are followed by a single feedforward layer which will be used as the output layer of the network. The feedforward layer outputs are obtained from the last recurrent layer activations hLc+Lrt as\nhLc+Lr+1t = G(h Lc+Lr t ), (2)\nwhere G represents a feedforward layer with sigmoid activation. Feedforward layer applies the same set of weights for the features extracted from each frame.\n4) Binarization: The outputs hLc+Lr+1t of the feedforward layer are used as the event activity probabilities for each class k = 1, 2, ...K as\np(yt(k) | x0:t,\u03b8) = hLc+Lr+1t (3)\nwhere K is the number of classes and \u03b8 represents the parameters of all the layers of the network combined. Finally, event activity predictions y\u0302t are obtained by thresholding the probabilities over a constant C \u2208 (0, 1) as\ny\u0302t(k) = { 1, p(yt(k) | x0:t,\u03b8) \u2265 C 0, otherwise\n(4)\nRegularization: In order to reduce overfitting, we experimented with dropout [34] regularization in the network, which has proven to be extremely effective in several deep learning applications [18]. The basic idea behind dropout is to temporarily remove at training time a certain portion of hidden units from the network, with the dropped units being randomly chosen at each iteration. This reduces units co-adaptation, approximates model averaging [34], and can be seen as a form of data augmentation without domain knowledge. For the recurrent layers we adopted the dropout proposed in [35], where the choice of dropped units is kept constant along a sequence.\nTo speed up the training phase we train our networks with batch normalization layers [36] after every convolutional or fully connected layer. Batch normalization reduces the internal covariate shift\u2014i.e., the distribution of network activations during training\u2014by normalizing a layer output to zero mean and unit variance, using approximate statistics computed on the training mini-batch.\nComparison to other CRNN architectures: The CRNN configuration used in this work has several points of similarity with the network presented in [21] for speech recognition. The main differences are the following: (i) We do not use any linear projection layer, neither at the end of the CNN part of the CRNN, nor after each recurrent layer. (ii) We use 5x5 kernels in all of our convolutional layers, compared to the 9x9 and 4x3 filters for the first and second layer respectively. (iii) Our architecture has also more convolutional layers (up to 4 instead of 2) and recurrent layers (up to 3 instead of 2). (iv) We use GRU instead of LSTM. (v) We use much longer sequences, up to thousands of steps, compared to 20 steps in [21]. While very long term context is not helpful in speech processing, since words and utterances are quite short in time, in SED there are several events that span over several seconds. (vi) For the experiments on CHiME-Home dataset we incorporate a new max pooling layer (only on time domain) before the output\nlayer. Therefore, if we have N mid-level features for T frames of a context window, we end up with N features for the whole context window to be fed to the output layer.\nCNNs and RNNs: It is possible to see CNNs and RNNs as specific instances of the CRNN architecture presented in this section: a CNN is a CRNN with zero recurrent layers, and an RNN is a CRNN with zero convolutional layers. In order to assess the benefits of using CRNNs compared to CNNs or RNNs alone, in Section III we directly compare the three architectures by removing the recurrent or convolutional layer, i.e., CNNs and RNNs respectively."}, {"heading": "III. EVALUATION", "text": "In order to test the proposed method, we run a series of experiments on four different datasets. We evaluate the results by comparing the system outputs to the annotated references. Since we are approaching the task as scene-independent, on each dataset we train a single model regardless of the presence of different acoustic scenes."}, {"heading": "A. Datasets and Settings", "text": "We evaluate the proposed method on four datasets, one of which is artificially generated as mixtures of isolated sound events, and three are recorded from real-life environments.\nWhile an evaluation performed on real audio data would be ideal, human annotations tend to be somewhat subjective, especially when precise onset and offset are required for overlapping events. For this reason we create our own synthetic dataset\u2014from here onwards referred to as TUT Sound Events Synthetic 2016 \u2014 where we use frame energy based automatic annotation of sound events.\nIn order to evaluate the proposed method in real-life conditions, we use TUT Sound Events 2009. This proprietary dataset contains real-life recordings from 10 different scenes and has been used in many previous works. We also compute and show results on the TUT Sound Events 2016 development and CHiME-Home dataset, which were used as part of DCASE2016 challenge 1.\na) TUT Sound Events Synthetic 2016 (TUT-SED Synthetic 2016): The primary evaluation dataset consists of synthetic mixtures created by mixing isolated sound events from 16 sound event classes. Polyphonic mixture were created by mixing 994 sound event samples. From the 100 mixtures created, 60% are used for training, 20% for testing and 20% for validation. The total length of the data is 566 minutes. Different instances of the sound events are used to synthesize the training, validation and test partitions. Mixtures were created by randomly selecting event instance and from it, randomly, a segment of length 3-15 seconds. Mixtures do not contain any additional background noise. Dataset creation procedure explanation and metadata can be found in the supporting website for the paper2.\n1http://www.cs.tut.fi/sgn/arg/dcase2016/ 2http://www.cs.tut.fi/sgn/arg/taslp2017-crnn-sed/tut-sed-synthetic-2016\n5 b) TUT Sound Events 2009 (TUT-SED 2009): This dataset, first presented in [37], consists of 8 to 14 binaural recordings from 10 real-life scenes. Each recording is 10 to 30 minutes long, for a total of 1133 minutes. The 10 scenes are: basketball game, beach, inside a bus, inside a car, hallway, office, restaurant, shop, street and stadium with track and field events. A total of 61 classes were defined, including (wind, yelling, car, shoe squeaks, etc.) and one extra class for unknown or rare events. The average number of events active at the same time is 2.53. Event activity annotations were done manually, which introduces a degree of subjectivity. The database has a five-fold cross-validation setup with training, validation and test set split, each consisting of about 60%, 20% and 20% of the data respectively from each scene. The dataset unfortunately can not be made public due to licensing issues, however three \u223c 10 minutes samples from the dataset are available at 3.\nc) TUT Sound Events 2016 development (TUT-SED 2016): This dataset consists of recordings from two real-life scenes: residential area and home [38]. The recordings are captured each in a different location (i.e., different streets, different homes) leading to a large variability on active sound event classes between recordings. For each location, a 3-5 minute long binaural audio recording is provided, adding up to 78 minutes of audio. The recordings have been manually annotated. In total, there are seven annotated sound event classes for residential area recordings and 11 annotated sound event classes for home recordings. The dataset and metadata is available through 4 and 5.\nThe four-fold cross-validation setup published along with the dataset [38] is used in the evaluations. Twenty percent of the training set recordings are assigned for validation in the training stage of the neural networks. Since in this work we investigate scene-independent SED, we discard the information about the scene, contrary to the DCASE2016 challenge setup. Therefore, instead of training a separate classifier for each scene, we train a single classifier to be used in all scenes. In TUT-SED 2009 all audio material for a scene was recorded in a single location, whereas TUT-SED 2016 contains multiple locations per scene.\nd) CHiME-Home: CHiME-Home dataset [39] consists of 4-second audio chunks from home environments. The annotations are based on seven sound classes, namely child speech, adult male speech, adult female speech, video game / TV, percussive sounds, broadband noise and other identifiable sounds. In this work, we use the same, refined setup of CHiMEHome as it is used in audio tagging task in DCASE2016 challenge [40], namely 1946 chunks for development (in four folds) and 846 chunks for evaluation.\nThe main difference between this dataset and the previous three is that the annotations are made per chunk instead of per frame. Each chunk is annotated with one or multiple labels. In order to adapt our architecture to the lack of frame-level annotations, we simply add a temporal max-pooling layer\u2014 that pools the predictions over time\u2014before the output layer\n3http://arg.cs.tut.fi/demo/CASAbrowser/ 4http://www.cs.tut.fi/sgn/arg/taslp2017-crnn-sed/#tut-sed-2016 5https://zenodo.org/record/45759#.WBoUGrPIbRY\nfor FNN, CNN, RNN and CRNN. CHiME-Home dataset is available at 6."}, {"heading": "B. Evaluation Metrics", "text": "In this work, segment-based evaluation metrics are used. The segment lengths used in this work are (1): a single time frame (40 ms in this work) and (2): a one-second segment. The segment length for each metric is annotated with the subscript (e.g., F1frm and F11sec).\nSegment-based F1 score calculated in a single time frame (F1frm) is used as the primary evaluation metric [41]. For each segment in the test set, intermediate statistics, i.e., the number of true positive (TP), false positive (FP) and false negative (FN) entries, are calculated as follows. If an event \u2022 is detected in one of the frames inside a segment and it is\nalso present in the same segment of the annotated data, that event is regarded as TP. \u2022 is not detected in any of the frames inside a segment but it is present in the same segment of the annotated data, that event is regarded as FN. \u2022 is detected in one of the frames inside a segment but it is not present in the same segment of the annotated data, that event is regarded as FP.\nThese intermediate statistics are accumulated over the test data and then over the folds. This way, each active instance per evaluated segment has equal influence on the evaluation score. This calculation method is referred to as micro-averaging, and is the recommended method for evaluation of classifier [42]. Precision (P ) and recall (R) are calculated from the accumulated intermediate statistics as\nP = TP\nTP + FP R = TP TP + FN\n(5)\nThese two metrics are finally combined as their harmonic mean, F1 score, which can be formulated as\nF1 = 2 \u00b7 P \u00b7R P + R\n(6)\nMore detailed and visualized explanation of segment-based F1 score in multi label setting can be found in [41].\nThe second evaluation metric is segment-based error rate as proposed in [41]. For error rate, intermediate statistics, i.e., the number of substitutions (s), insertions (i), deletions (d) and active classes from annotations (a) are calculated per segment as explained in detail in [41]. Then, the total error rate is calculated as\nER =\nN\u2211 t=1 st + N\u2211 t=1 it + N\u2211 t=1 dt\nR\u2211 t=1 at\n(7)\nwhere subscript t represents segment index and N is the total number of segments.\nBoth evaluation metrics are calculated from the accumulated sum for their corresponding intermediate statistics over the segments of the whole test set. If there are multiple scenes in\n6https://archive.org/details/chime-home\n6\nthe dataset, evaluation metrics are calculated for each scene separately and then the results are presented as the average across the scenes.\nThe main metric used in previous works [11], [14], [15] on TUT-SED 2009 dataset differs from the F1 score calculation used in this paper. In previous works, F1 score was computed in each segment, then averaged along segments for each scene, and finally averaged across scene scores, instead of accumulating intermediate statistics. This leads to measurement bias under high class imbalance between the classes and also between folds. However, in order to give a comprehensive comparison of our proposed method with previous works on this dataset, we also report the results with this legacy F1 score in Section IV-B.\nFor CHiME-Home dataset, equal error rate (EER) has been used as the evaluation metric in order to compare the results with DCASE2016 challenge submissions, where EER has been the main evaluation metric."}, {"heading": "C. Baselines", "text": "For this work, we compare the proposed method with two recent approaches: the Gaussian mixture model (GMM) of [38] and the feedforward neural network model (FNN) from [15]. GMM has been chosen as a baseline method since it is an established generative modeling method used in many sound recognition tasks [12], [13], [43]. In parallel with the recent surge of deep learning techniques in pattern recognition, FNNs have been shown to vastly outperform GMM based methods in SED [15]. Moreover, this FNN architecture represents a straightforward deep learning method that can be used as a baseline for more complex architectures such as CNN, RNN and the proposed CRNN.\nGMM: The first baseline system is based on a binary frame-classification approach, where for each sound event class a binary classifier is set up [38]. Each binary classifier consists of a positive class model and a negative class model. The positive class model is trained using the audio segments annotated as belonging to the modeled event class, and a negative class model is trained using the rest of the audio. The system uses MFCCs as features and a GMM-based classifier. MFCCs are calculated using 40 ms frames with Hamming window and 50% overlap and 40 mel bands. The first 20 static coefficients are kept, and delta and acceleration coefficients are calculated using a window length of 9 frames. The 0th order static coefficient is excluded, resulting in a frame-based\nfeature vector of dimension 59. For each sound event, a positive model and a negative model are trained. The models are trained using expectation-maximization algorithm, using kmeans algorithm to initialize the training process and diagonal covariance matrices. The number of parameters for GMM baseline is 3808\u2217K, where K is the number of classes. In the detection stage, the decision is based on the likelihood ratio between the positive and negative models for each individual sound class event, with a sliding window of one second. The system is used as a baseline in the DCASE2016 challenge [44], however, in this study the system is used as scene-independent to match the setting of the other methods presented.\nFNN: The second baseline system is a deep multi-label FNN with temporal context [15]. As the sound features, 40 log mel band energy features are extracted for each 40 ms time frame with 50% overlap. For the input, consecutive feature vectors are stacked in five vector blocks, resulting in a 100 ms context window. As the hidden layers, two feedforward layers of 1600 hidden units with maxout activation [45] with pool size of 2 units are used. For the output layer, a feedforward layer of K units with sigmoid activation is used to obtain event activity probabilities per context window, where K is the number of classes. The sliding window post-processing of the event activity probabilities in [15] has not been implemented for the baseline experiments in order to make a fair comparison based on classifier architecture for different deep learning methods. The number of parameters in the baseline FNN model is around 1.6 million."}, {"heading": "D. Experiments set-up", "text": "Preprocessing: For all neural networks (FNN, CNN, RNN and CRNN) we use log mel band energies as acoustic features. We first compute short-time Fourier transform (STFT) of the recordings in 40 ms frames with 50% overlap, then compute mel band energies through mel filterbank with 40 bands spanning 0 to 22050 Hz, which is the Nyquist rate. After computing the logarithm of the mel band energies, each energy band is normalized by subtracting its mean and dividing by its standard deviation computed over the training set. The normalized log mel band energies are finally split into sequences. During training we use overlapped sequences, i.e. we sample the sub-sequences with a different starting point at every epoch, by moving the starting index by a fixed amount that is not a factor of the sequence length (73 in our experiments). The stride is not equal to 1 in order to have\n7\nTABLE II: F1 score and error rate results for single frame segments (F1frm and ERfrm) and one second segments (F11sec and ER1sec). Bold face indicates the best performing method for the given metric.\nTUT-SED Synthetic 2016 TUT-SED 2009 TUT-SED 2016 Method F1frm ERfrm F11sec ER1sec F1frm ERfrm F11sec ER1sec F1frm ERfrm F11sec ER1sec\nGMM [38] 40.5 0.78 45.3 0.72 33.0 1.34 34.1 1.60 14.1 1.12 17.9 1.13 FNN [15] 49.2\u00b10.8 0.68\u00b10.02 50.2\u00b11.4 1.1\u00b10.1 60.9\u00b10.4 0.56\u00b10.01 57.1\u00b10.2 1.1\u00b10.01 26.7\u00b11.4 0.99\u00b10.03 32.5\u00b11.2 1.32\u00b10.06 CNN 59.8\u00b10.9 0.56\u00b10.01 59.9\u00b11.2 0.78\u00b10.08 64.8\u00b10.2 0.50\u00b10.0 63.2\u00b10.5 0.75\u00b10.02 23.0\u00b12.6 1.02\u00b10.06 26.4\u00b11.9 1.09\u00b10.06 RNN 52.8\u00b11.5 0.6\u00b10.02 57.1\u00b10.9 0.64\u00b10.01 62.4\u00b11.0 0.52\u00b10.01 61.8\u00b10.8 0.55\u00b10.01 27.6\u00b11.8 1.04\u00b10.02 29.7\u00b11.4 1.10\u00b10.04 CRNN 66.4\u00b10.6 0.48\u00b10.01 68.7\u00b10.7 0.47\u00b10.01 69.7\u00b10.4 0.45\u00b10.0 69.3\u00b10.2 0.48\u00b10.0 27.5\u00b12.6 0.98\u00b10.04 30.3\u00b11.7 0.95\u00b10.02\neffectively different sub-sequences from one training epoch to the next one. For validation and test data we do not use any overlap.\nWhile finer frequency resolution or different representations could improve the accuracy, our main goal is to compare the architectures. We opted for this setting as it was recently used with very good performance in several works on SED [11], [15].\nNeural network configurations: Since the size of the dataset usually affects the optimal network architecture, we do a hyperparameter search by running a series of experiments over predetermined ranges. We select for each network architecture the hyperparameter configuration that leads to the best results on the validation set, and use this architecture to compute the results on the test set.\nFor TUT-SED Synthetic 2016 and CHiME-Home datasets, we run a hyperparameter grid search on the number of CNN feature maps and RNN hidden units {96, 256} (set to the same value); the number of recurrent layers {1, 2, 3}; and the number of CNN layers {1, 2, 3 ,4} with the following frequency max pooling arrangements after each convolutional layer {(4), (2, 2), (4, 2), (8, 5), (2, 2, 2), (5, 4, 2), (2, 2, 2, 1), (5, 2, 2, 2)}. Here, the numbers denote the number of frequency bands at each max pooling step; e.g., the configuration (5, 4, 2) pools the original 40 bands to one band in three stages: 40 bands \u2192 8 bands \u2192 2 bands \u2192 1 band.\nAll networks have batch normalization layers after convolutional layers and dropout rate 0.25, which were found to be helpful in preliminary experiments. The output layer consists of a node for each class and has the sigmoid as activation\nfunction. In convolutional layers we use filters with shape (5, 5); in recurrent layers we opted for GRU, since preliminary experiments using LSTM yielded similar results and GRU units have a smaller number of parameters. The weights are initialized according to the scheme proposed in [46]. Binary cross-entropy is set as the loss function, and all networks are trained with Adam [47] as gradient descent optimizer, with the default parameters proposed in the original paper.\nTo evaluate the effect of having both convolutional and recurrent layers in the same architecture, we compare the CRNN with CNNs and RNNs alone. For both CNN and RNN we run the same hyperparameter optimization procedure described for CRNN, replacing recurrent layers with feedforward layers for CNNs, and removing convolutional layers for RNNs while adding feedforward layers before the output layer. This allows for a fair comparison, providing the possibility of having equally deep networks for all three architectures.\nAfter this first optimization process, we use the best CRNNs, CNNs and RNNs to separately test the effect of varying other hyperparameters. More specifically we investigate how performance is affected by variation of the CNN filter shapes and the sequence length. For the CRNN we test filter shapes in the set {(3,3), (5,5), (11,11), (1,5), (5,1), (3,11), (11,3)}, where (\u2217, \u2217) represents the filter lengths in frequency and time axes, respectively. For CRNN and RNN, we test shorter and longer sequences than the initial value of 128 frames, experimenting in the range {8, 32, 128, 256, 512, 1024, 2048} frames, which correspond to {0.16, 0.64, 2.56, 5.12, 10.24, 20.48, 40.96} seconds respectively. We finally use the hyperparameters that provide the highest validation scores as our final CRNN, CNN and RNN models.\nFor the other two datasets (TUT-SED 2009 and TUTSED 2016) we select a group of best performing model configurations on validation data from TUT-SED Synthetic 2016 experiments and to account for the different amount of data we run another smaller hyperparameter search, varying the amount of dropout and the sequence length. Again, we then select the best performing networks on the validation score to compute the test results. The hyperparameters used in the evaluation for all three datasets is presented in Table I.\nThe event activity probabilities are thresholded at C = 0.5, in order to obtain the binary activity matrix used to compute the reference metrics based on the ground truth. All networks are trained until overfitting starts to arise: as a criterion we use early stopping on the validation metric, halting the training if the score is not improving for more than 100 epochs and reverting the weights to the values that best performed on\n8\nvalidation. For feature extraction, the Python library Librosa [48] has been used in this work. For classifier implementations, deep learning package Keras (version 1.1.0) [49] is used with Theano (version 0.8.2) as backend [50]. The networks are trained on NVIDIA Tesla K40t and K80 GPUs."}, {"heading": "IV. RESULTS", "text": "In this section, we present results for all the datasets and experiments described in Section III. The evaluation of CNN, RNN and CRNN methods are conducted using the hyperparameters given in Table I. All the reported results are computed on the test sets. Unless otherwise stated, we run each neural network based experiment ten times with different random seeds (five times for TUT-SED 2009) to reflect the effect of random weight initialization. We provide the mean and the standard deviation of these experiments in this section. Best performing method is highlighted with bold face in the tables of this section. The methods whose best performance among the ten runs is within one standard deviation of the best performing method is also highlighted with bold face.\nThe main results with the best performing (based on the validation data) CRNN, CNN, RNN, and the GMM and FNN baselines are reported in Table II. Results are calculated according to the description in Section III-B where each event instance irrespective of the class is taken into account in equal manner. As shown in the table, the CRNNs consistently outperforms CNNs, RNNs and the two baseline methods on all three datasets for the main metric."}, {"heading": "A. TUT Sound Events Synthetic 2016", "text": "As presented in Table II, CRNN improved by absolute 6.6% and 13.6% on frame-based F1 compared to CNN and RNN respectively for TUT-SED synthetic 2016 dataset. Considering the number of parameters used for each method (see Table I), the performance of CRNN indicates an architectural advantage compared to CNN and RNN methods. All the four deep learning based methods outperform the baseline GMM method.\nTABLE IV: F1frm for accuracy vs. convolution filter shape for TUT-SED Synthetic 2016 dataset. (\u2217, \u2217) represents filter lengths in frequency and time axis, respectively.\nFilter shape (3,3) (5,5) (11,11) (1,5) (5,1) (3,11) (11,3)\nF1frm 67.2 68.3 62.6 28.5 60.6 67.4 61.2\nAs claimed in [51], this may be due to the capability of deep learning methods to use different subsets of hidden units to model different sound events simultaneously. An example mixture from TUT-SED Synthetic 2016 test set is presented in Figure 3 with annotations and event activity predictions from CNN, RNN and CRNN.\n1) Class-wise performance: The class-wise performance with F1frm metric for CNN, RNN and CRNN methods along with the average and total duration of the classes are presented in Table III. CRNN outperforms both CNN and RNN on almost all classes. It should be kept in mind that each class is likely to appear together with different classes rather than isolated. Therefore the results in Table III present the performance of the methods for each class in a polyphonic setting, as would be the case in a real-life environment. The worst performing class for all three networks is cat meowing, which consists of short, harmonic sounds. We observed that cat meowing samples are mostly confused by baby crying, which has similar acoustic characteristics. Besides, short, non-impulsive sound events are more likely to be masked by another overlapping sound event, which makes their detection more challenging. CRNN performance is considerably better compared to CNN and RNN for gun shot, thunder, bird singing, baby crying and mixer sound events. However, it is hard to make any generalizations on the acoustic characteristics of these events that can explain the superior performance.\n2) Effects of filter shape: The effect of the convolutional filter shape is presented in Table IV. Since these experiments were part of the hyperparameter grid search, each experiment is conducted only once. Small kernels, such as (5,5) and (3,3), were found to perform the best in the experiments run on this dataset. This is consistent with the results presented in [31] on a similar task. The very low performance given for the filter shape (1,5) highlights the importance of including multiple frequency bands in the convolution when spectrogram based\n9 \u22122 \u22121 0 1 2 0 \u22121 \u22122 \u22123 \u22124 \u22125 \u22126 \u22127 \u22128\nPitch shifted quartertones (24 quartertones per octave)\n\u2206 F1\nfr m\n(% )\nCNN RNN\nCRNN\nFig. 5: Absolute accuracy change vs. pitch-shifting over \u00b12 quartertones for CNN, RNN and CRNN.\nfeatures are used as input for the CRNN. 3) Number of parameters vs. accuracy: The effect of number of parameters on the accuracy is investigated in Figure 4. The points in the figure represent the test accuracy with F1frm metric for the hyperparameter grid search experiments. Each experiment is conducted one time only. Two observations can be made from the figure. For the same number of parameters, CRNN has a clear performance advantage over CNN and RNN. This indicates that the high performance of CRNN can be explained with the architectural advantage rather than the model size. In addition, there can be a significant performance shift for the same type of networks with the same number of parameters, which means that a careful grid search on hyperparameters (e.g. shallow with more hidden units per layer vs. deep with less hidden units per layer) is crucial in finding the optimal network structure.\n4) Frequency shift invariance: Sound events may exhibit small variations in their frequency content. In order to investigate the robustness of the networks to small frequency variations, pitch shift experiments are conducted and the absolute changes in frame-based F1 score are presented in Figure 5. For these experiments, each network is first trained with the original training data. Then, using Librosa\u2019s pitchshift function, the pitch for the mixtures in the test set is shifted by \u00b12 quartertones. The test results show a significant absolute drop in accuracy for RNNs when the frequency content is shifted slightly. As expected, CNN and CRNN are more robust to small changes in frequency content due to the convolution and max-pooling operations. However, accuracy decrease difference between the methods diminishes for negative pitch shift, for which the reasons should be further investigated. It should be also noted that RNN has the lowest base accuracy, so it is relatively more affected for the same amount of absolute accuracy decrease (see Table II).\n5) Closer look on network outputs: A comparative study on the neural network outputs, which are regarded as event activity probabilities, for a 13-second sequence of the test set is presented in Figure 6. For the parts of the sequence where dog barking and baby crying appear alone, all three networks successfully detect these events. However, when a gun shot appears overlapping with baby crying, only CRNN can detect\nbaby crying bird singing crowd applause crowd cheering\ndog barking gun shot\nmixer rain\n0 2 4 6 8 10 12\n30\n20\n10 0L og\nm el\ne ne\nrg y\nbi ns\nInput features\nGround truth\nEvent activity probabilities for CNN\nEvent activity probabilities for RNN\nEvent activity probabilities for CRNN\n0\n0.2\n0.4\n0.6\n0.8\n1\nbaby crying bird singing crowd applause crowd cheering\ndog barking gun shot\nmixer rain\nbaby crying bird singing crowd applause crowd cheering\ndog barking gun shot\nmixer rain\nbaby crying bird singing crowd applause crowd cheering\ndog barking gun shot\nmixer rain\nTime (secs)\nFig. 6: Input features, ground truth and event activity probabilities for CNN, RNN and CRNN from a sequence of test examples from TUT-SED synthetic 2016.\nthe gun shot although there is a significant change in the input feature content. This indicates the efficient modeling of the gun shot by CRNN which improves the detection accuracy even in polyphonic conditions. Moreover, when crowd applause begins to appear in the signal, it almost completely masks baby crying, as it is evident from the input features. CNN correctly detects crowd applause, but misses the masked baby crying in this case, and RNN ignores the significant change in features and keeps detecting baby crying. RNN\u2019s insensitivity to the input feature change can be explained with its input gate not passing through new inputs to recurrent layers. On the other hand, CRNN correctly detects both events and almost perfectly matches the ground truth along the whole sequence."}, {"heading": "B. TUT-SED 2009", "text": "For a comprehensive comparison, results with different methods applied to the same cross-validation setup and published over the years are shown in Table V. The main metric used in these previous works is averaged over folds, and may be influenced by distribution of events in the folds (see Section III-B). In order to allow a direct comparison, we have computed all metrics in the table the same way.\nFirst published systems were scene-dependent, where information about the scene is provided to the system and separate event models are trained for each scene [14], [24], [25]. More recent work [11], [15], as well as the current study,\n10\nconsist of scene-independent systems. Methods [24], [25] are HMM based, using either multiple Viterbi decoding stages or NMF pre-processing to do polyphonic SED. In contrast, the use of NMF in [14] does not build explicit class models, but performs coupled NMF of spectral representation and event activity annotations to build dictionaries. This method performs polyphonic SED through direct estimation of event activities using learned dictionaries.\nThe results on the dataset show significant improvement with the introduction of deep learning methods. CRNN has significantly higher performance than previous methods [14], [24], [25], [38], and it still shows considerable improvement over other neural network approaches."}, {"heading": "C. TUT-SED 2016", "text": "The CRNN and RNN architectures obtain the best results in terms of framewise F1. The CRNN outperforms all the other architectures for ER framewise and on 1-second blocks. While the FNN obtains better results on the 1-second block F1, this happens at the expense of a very large 1-second block ER.\nFor all the analyzed architectures, the overall results on this dataset are quite low compared to the other datasets. This is most likely due the fact that TUT-SED 2016 is very small and the sounds events occur sparsely (i.e. a large portion of the data is silent). In fact, when we look at class-wise results (unfortunately not available due to space restrictions), we noticed a significant performance difference between the classes that are represented the most in the dataset (e.g. bird singing and car passing by, F1frm around 50%) and the least represented classes (e.g. cupboard and object snapping, F1frm close to 0%). Some other techniques might be applied to improve the accuracy of systems trained on such small datasets, e.g. training a network on a larger dataset and then retraining the output layer on the smaller dataset (transfer learning), or incorporating unlabeled data to the learning process (semisupervised learning)."}, {"heading": "D. CHiME-Home", "text": "The results obtained on CHiME-Home are reported in Table VI. For all of our three architectures there is a significant\nimprovement over the previous results reported on the same dataset on the DCASE2016 challenge, setting new state-ofthe-art results.\nAfter the first series of experiments the CNN obtained slightly better results compared to the CRNN. The CRNN and CNN architecture used are almost identical, with the only exception of the last recurrent (GRU) layer in the CRNN being replaced by a fully connected layer followed by batch normalization. In order to test if the improvement in the results was due to the absence of recurrent connections or to the presence of batch normalization, we run again the same CNN experiments removing the normalization layer. As shown in the last row of VI, over 10 different random initializations the average EER increased to values above those obtained by the CRNN.\nE. Visualization of convolutional layers\nHere we take a peek at the representation learned by the networks. More specifically, we use the technique described in [55] to visualize what kind of patterns in the input data different neurons in the convolutional layers are looking for. We feed the network a random input whose entries are independently drawn from a Gaussian distribution with zero mean and unit variance. We choose one neuron in a convolutional layer, compute the gradient of its activation with respect to the input, and iteratively update the input through gradient ascent in order to increase the activation of the neuron. If the gradient ascent optimization does not get stuck into a weak local maximum, after several updates the resulting input will strongly activate the neuron. We run the experiment for several convolutional neurons in the CRNN networks trained on TUT-SED Synthetic 2016 and TUT-SED 2009, halting the optimization after 100 updates. In Figure 7 we present a few of these inputs for several neurons at different depth. The figure confirms that the convolutional filters have specialized into finding specific patterns in the input. In addition, the complexity of the patterns looked for by the filters seems to increase as the layers become deeper."}, {"heading": "V. CONCLUSIONS", "text": "In this work, we proposed to apply a CRNN\u2014a combination of CNN and RNN, two complementary classification methods\u2014on a polyphonic SED task. The proposed method\n11\nfirst extracts higher level features through multiple convolutional layers (with small filters spanning both time and frequency) and pooling in frequency domain; these features are then fed to recurrent layers, whose features in turn are used to obtain event activity probabilities through a feedforward fully connected layer. In CRNN, CNN\u2019s capability to learn local translation invariant filters and RNN\u2019s capability to model short and long term temporal dependencies are gathered in a single classifier. The evaluation results over four datasets show a clear performance improvement for the proposed CRNN method compared to CNN, RNN, and other established methods in polyphonic SED.\nDespite the improvement in performance, we identify a limitation to this method. As presented in TUT-SED 2016 results in Table II, the performance of the proposed CRNN (and of the other deep learning based methods) strongly depends on the amount of available annotated data. TUTSED 2016 dataset consists of 78 minutes of audio of which only about 49 minutes are annotated with at least one of the classes. When the performance of CRNN for TUT-SED 2016 is compared to the performance on TUT-SED 2009 (1133 minutes) and TUT-SED Synthetic 2016 (566 minutes), there is a clear performance drop both in the absolute performance and in the relative improvement with respect to other methods. Dependency on large amounts of data is a common limitation of current deep learning methods.\nThe results we observed in this work, and in many other classification tasks in various domains, prove that deep learning is definitely worth further investigation on polyphonic SED. As a future work, semi-supervised training methods can be investigated to overcome the limitation imposed by small datasets. Transfer learning [56], [57] could be potentially applied with success in this setting: by first training a CRNN on a large dataset (such as TUT-SED Synthetic 2016), the last feedforward layer can then be replaced with random weights and the network fine-tuned on the smaller dataset.\nAnother issue worth investigating would be a detailed study over the activations from different stages of the proposed\nCRNN method. For instance, a class-wise study over the higher level features extracted from the convolutional layers might give an insight on the common features of different sound events. Finally, recurrent layer activations may be informative on the degree of relevance of the temporal context information for various sound events."}], "references": [{"title": "Reliable detection of audio events in highly noisy environments", "author": ["P. Foggia", "N. Petkov", "A. Saggese", "N. Strisciuglio", "M. Vento"], "venue": "Pattern Recognition Letters, vol. 65, pp. 22\u201328, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic monitoring and localization for social care", "author": ["S. Goetze", "J. Schroder", "S. Gerlach", "D. Hollosi", "J.-E. Appell", "F. Wallhoff"], "venue": "Journal of Computing Science and Engineering, vol. 6, no. 1, pp. 40\u201350, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature learning with deep scattering for urban sound analysis", "author": ["J. Salamon", "J.P. Bello"], "venue": "2015 23rd European Signal Processing Conference (EUSIPCO). IEEE, 2015, pp. 724\u2013728.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Audio-based multimedia event detection using deep recurrent neural networks", "author": ["Y. Wang", "L. Neves", "F. Metze"], "venue": "2016 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 2742\u20132746.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Acoustic event detection for multiple overlapping similar sources", "author": ["D. Stowell", "D. Clayton"], "venue": "2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2015, pp. 1\u20135.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Sound event recognition in unstructured environments using spectrogram image processing", "author": ["J.W. Dennis"], "venue": "Nanyang Technological University, Singapore, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Recognition of acoustic events using deep neural networks", "author": ["O. Gencoglu", "T. Virtanen", "H. Huttunen"], "venue": "Proc. European Signal Processing Conference (EUSIPCO), 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust sound event recognition using convolutional neural networks", "author": ["H. Zhang", "I. McLoughlin", "Y. Song"], "venue": "2015 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 559\u2013563.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust audio event recognition with 1-max pooling convolutional neural networks", "author": ["H. Phan", "L. Hertel", "M. Maass", "A. Mertins"], "venue": "Interspeech, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Environmental sound classification with convolutional neural networks", "author": ["K.J. Piczak"], "venue": "Int. Workshop on Machine Learning for Signal Processing (MLSP), 2015, pp. 1\u20136.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural networks for polyphonic sound event detection in real life recordings", "author": ["G. Parascandolo", "H. Huttunen", "T. Virtanen"], "venue": "2016 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 6440\u20136444.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "A flexible framework for key audio effects detection and auditory context inference", "author": ["L.-H. Cai", "L. Lu", "A. Hanjalic", "H.-J. Zhang", "L.-H. Cai"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing, vol. 14, no. 3, pp. 1026\u20131039, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Acoustic event detection in real life recordings", "author": ["A. Mesaros", "T. Heittola", "A. Eronen", "T. Virtanen"], "venue": "Proc. European Signal Processing Conference (EUSIPCO), 2010, pp. 1267\u20131271.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Sound event detection in real life recordings using coupled matrix factorization of spectral representations and class activity annotations", "author": ["A. Mesaros", "O. Dikmen", "T. Heittola", "T. Virtanen"], "venue": "2015 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 151\u2013155.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Polyphonic sound event detection using multilabel deep neural networks", "author": ["E. Cakir", "T. Heittola", "H. Huttunen", "T. Virtanen"], "venue": "Int. Joint Conf. on Neural Networks (IJCNN), 2015, pp. 1\u20137.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Filterbank learning for deep neural network based polyphonic sound event detection", "author": ["E. Cakir", "E. Ozan", "T. Virtanen"], "venue": "Int. Joint Conf. on Neural Networks (IJCNN), 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "2013 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 6645\u20136649.  12", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "2015 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4580\u20134584.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3128\u20133137.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Context-dependent sound event detection", "author": ["T. Heittola", "A. Mesaros", "A. Eronen", "T. Virtanen"], "venue": "EURASIP Journal on Audio, Speech, and Music Processing, vol. 2013, no. 1, p. 1, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Supervised model training for overlapping sound events based on unsupervised source separation", "author": ["T. Heittola", "A. Mesaros", "T. Virtanen", "M. Gabbouj"], "venue": "Int. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), 2013, pp. 8677\u20138681.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Sound event detection using non-negative dictionaries learned from annotated overlapping events", "author": ["O. Dikmen", "A. Mesaros"], "venue": "2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2013, pp. 1\u20134.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1998}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "Proceedings of The 33rd International Conference on Machine Learning, 2016, pp. 173\u2013182.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning the speech front-end with raw waveform cldnns", "author": ["T.N. Sainath", "R.J. Weiss", "A. Senior", "K.W. Wilson", "O. Vinyals"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional recurrent neural networks for music classification", "author": ["K. Choi", "G. Fazekas", "M. Sandler", "K. Cho"], "venue": "arXiv preprint arXiv:1609.04243, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "An end-to-end neural network for polyphonic piano music transcription", "author": ["S. Sigtia", "E. Benetos", "S. Dixon"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 5, pp. 927\u2013939, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1997}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8), 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting.", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Y. Gal"], "venue": "Advances in neural information processing systems, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proceedings of The 32nd International Conference on Machine Learning, 2015, pp. 448\u2013456.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Audio context recognition using audio event histograms", "author": ["T. Heittola", "A. Mesaros", "A. Eronen", "T. Virtanen"], "venue": "Proc. of the 18th European Signal Processing Conference (EUSIPCO), 2010, pp. 1272\u20131276.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "TUT database for acoustic scene classification and sound event detection", "author": ["A. Mesaros", "T. Heittola", "T. Virtanen"], "venue": "24th European Signal Processing Conference (EUSIPCO), 2016.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Chime-home: A dataset for sound source recognition in a domestic environment", "author": ["P. Foster", "S. Sigtia", "S. Krstulovic", "J. Barker", "M.D. Plumbley"], "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). IEEE, 2015, pp. 1\u20135.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "DCASE2016 challenge - audio tagging", "author": ["T. Heittola"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Metrics for polyphonic sound event detection", "author": ["A. Mesaros", "T. Heittola", "T. Virtanen"], "venue": "Applied Sciences, vol. 6, no. 6, p. 162, 2016.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Apples-to-apples in cross-validation studies: pitfalls in classifier performance measurement", "author": ["G. Forman", "M. Scholz"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 12, no. 1, pp. 49\u201357, 2010.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic context detection based on hierarchical audio models", "author": ["W.-H. Cheng", "W.-T. Chu", "J.-L. Wu"], "venue": "Proceedings of the 5th ACM  SIGMM international workshop on Multimedia information retrieval, 2003, pp. 109\u2013115.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2003}, {"title": "2016) DCASE2016 baseline system", "author": ["T. Heittola", "A. Mesaros", "T. Virtanen"], "venue": "https://github.com/TUT-ARG/", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE Int. Conf. on Computer Vision, 2015, pp. 1026\u2013 1034.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "International conference on learning representations, 2015.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "librosa: Audio and music signal analysis in python", "author": ["B. McFee", "C. Raffel", "D. Liang", "D.P. Ellis", "M. McVicar", "E. Battenberg", "O. Nieto"], "venue": "Proceedings of the 14th Python in Science Conference, 2015.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: A python framework for fast computation of mathematical expressions", "author": ["T.T.D. Team", "R. Al-Rfou", "G. Alain", "A. Almahairi", "C. Angermueller", "D. Bahdanau", "N. Ballas", "F. Bastien", "J. Bayer", "A. Belikov"], "venue": "arXiv preprint arXiv:1605.02688, 2016.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}, {"title": "CQT-based convolutional neural networks for audio scene classification and domestic audio tagging", "author": ["T. Lidy", "A. Schindler"], "venue": "DCASE2016 Challenge, Tech. Rep., September 2016.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Domestic audio tagging with convolutional neural networks", "author": ["E. Cakir", "T. Heittola", "T. Virtanen"], "venue": "DCASE2016 Challenge, Tech. Rep., September 2016.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative training of GMM parameters for audio scene classification", "author": ["S. Yun", "S. Kim", "S. Moon", "J. Cho", "T. Kim"], "venue": "DCASE2016 Challenge, Tech. Rep., September 2016.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "ICLR Workshop, 2014.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning of representations for unsupervised and transfer learning.", "author": ["Y. Bengio"], "venue": "ICML Unsupervised and Transfer Learning,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "SED has been drawing a surging amount of interest in recent years with applications including audio surveillance [1], healthcare monitoring [2], urban sound analysis [3], multimedia event detection [4] and bird call detection [5].", "startOffset": 113, "endOffset": 116}, {"referenceID": 1, "context": "SED has been drawing a surging amount of interest in recent years with applications including audio surveillance [1], healthcare monitoring [2], urban sound analysis [3], multimedia event detection [4] and bird call detection [5].", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "SED has been drawing a surging amount of interest in recent years with applications including audio surveillance [1], healthcare monitoring [2], urban sound analysis [3], multimedia event detection [4] and bird call detection [5].", "startOffset": 166, "endOffset": 169}, {"referenceID": 3, "context": "SED has been drawing a surging amount of interest in recent years with applications including audio surveillance [1], healthcare monitoring [2], urban sound analysis [3], multimedia event detection [4] and bird call detection [5].", "startOffset": 198, "endOffset": 201}, {"referenceID": 4, "context": "SED has been drawing a surging amount of interest in recent years with applications including audio surveillance [1], healthcare monitoring [2], urban sound analysis [3], multimedia event detection [4] and bird call detection [5].", "startOffset": 226, "endOffset": 229}, {"referenceID": 5, "context": "Factors such as environmental noise and overlapping sources are present in the unstructured environments and they may introduce a high degree of variation among the sound events from the same sound event class [6].", "startOffset": 210, "endOffset": 213}, {"referenceID": 0, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 104, "endOffset": 107}, {"referenceID": 7, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 150, "endOffset": 153}, {"referenceID": 9, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 155, "endOffset": 159}, {"referenceID": 3, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 196, "endOffset": 199}, {"referenceID": 10, "context": "classifiers\u2014such as support vector machines [1], [3]\u2014deep learning methods such as deep belief networks [7], convolutional neural networks (CNN) [8], [9], [10] and recurrent neural networks (RNN) [4], [11] have been recently proposed.", "startOffset": 201, "endOffset": 205}, {"referenceID": 11, "context": "model (HMM) based modeling\u2014an established method that has been widely used in automatic speech recognition\u2014has been proposed to model individual sound events with Gaussian mixtures and detect each event through HMM states using Viterbi algorithm [12], [13].", "startOffset": 246, "endOffset": 250}, {"referenceID": 12, "context": "model (HMM) based modeling\u2014an established method that has been widely used in automatic speech recognition\u2014has been proposed to model individual sound events with Gaussian mixtures and detect each event through HMM states using Viterbi algorithm [12], [13].", "startOffset": 252, "endOffset": 256}, {"referenceID": 13, "context": "Non-negative matrix factorization (NMF) based source separation [14] and deep learning based methods (such as feedforward neural networks (FNN) [15], CNN [16] and RNN [11]) have been shown to perform significantly better compared to established methods such as GMM-HMM for polyphonic SED.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "Non-negative matrix factorization (NMF) based source separation [14] and deep learning based methods (such as feedforward neural networks (FNN) [15], CNN [16] and RNN [11]) have been shown to perform significantly better compared to established methods such as GMM-HMM for polyphonic SED.", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "Non-negative matrix factorization (NMF) based source separation [14] and deep learning based methods (such as feedforward neural networks (FNN) [15], CNN [16] and RNN [11]) have been shown to perform significantly better compared to established methods such as GMM-HMM for polyphonic SED.", "startOffset": 154, "endOffset": 158}, {"referenceID": 10, "context": "Non-negative matrix factorization (NMF) based source separation [14] and deep learning based methods (such as feedforward neural networks (FNN) [15], CNN [16] and RNN [11]) have been shown to perform significantly better compared to established methods such as GMM-HMM for polyphonic SED.", "startOffset": 167, "endOffset": 171}, {"referenceID": 16, "context": "Deep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23].", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "Deep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23].", "startOffset": 113, "endOffset": 117}, {"referenceID": 18, "context": "Deep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "Deep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23].", "startOffset": 144, "endOffset": 148}, {"referenceID": 20, "context": "Deep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23].", "startOffset": 150, "endOffset": 154}, {"referenceID": 21, "context": "Deep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23].", "startOffset": 176, "endOffset": 180}, {"referenceID": 22, "context": "Deep neural networks [17] have recently achieved remarkable success in several domains such as image recognition [18], [19], speech recognition [20], [21], machine translation [22], even integrating multiple data modalities such as image and text in image captioning [23].", "startOffset": 267, "endOffset": 271}, {"referenceID": 6, "context": "Feedforward neural networks have been used in monophonic [7] and polyphonic SED in real-life environments [15] by processing concatenated input frames from a small time window of the spectrogram.", "startOffset": 57, "endOffset": 60}, {"referenceID": 14, "context": "Feedforward neural networks have been used in monophonic [7] and polyphonic SED in real-life environments [15] by processing concatenated input frames from a small time window of the spectrogram.", "startOffset": 106, "endOffset": 110}, {"referenceID": 23, "context": "This simple architecture\u2014while vastly improving over established approaches such as GMMHMMs [24] and NMF source separation based SED [25], [26]\u2014presents two major shortcomings: (1) it lacks both time and frequency invariance\u2014due to the fixed connections between the input and the hidden units\u2014which would allow to model small variations in the events; (2) temporal context is restricted to short time windows, preventing effective modeling of typically longer events (e.", "startOffset": 92, "endOffset": 96}, {"referenceID": 24, "context": "This simple architecture\u2014while vastly improving over established approaches such as GMMHMMs [24] and NMF source separation based SED [25], [26]\u2014presents two major shortcomings: (1) it lacks both time and frequency invariance\u2014due to the fixed connections between the input and the hidden units\u2014which would allow to model small variations in the events; (2) temporal context is restricted to short time windows, preventing effective modeling of typically longer events (e.", "startOffset": 133, "endOffset": 137}, {"referenceID": 25, "context": "This simple architecture\u2014while vastly improving over established approaches such as GMMHMMs [24] and NMF source separation based SED [25], [26]\u2014presents two major shortcomings: (1) it lacks both time and frequency invariance\u2014due to the fixed connections between the input and the hidden units\u2014which would allow to model small variations in the events; (2) temporal context is restricted to short time windows, preventing effective modeling of typically longer events (e.", "startOffset": 139, "endOffset": 143}, {"referenceID": 26, "context": "CNNs [27] can address the former limitation by learning filters that are shifted in both time and frequency [8], lacking", "startOffset": 5, "endOffset": 9}, {"referenceID": 7, "context": "CNNs [27] can address the former limitation by learning filters that are shifted in both time and frequency [8], lacking", "startOffset": 108, "endOffset": 111}, {"referenceID": 19, "context": "Recurrent neural networks (RNNs), which have been successfully applied to automatic speech recognition (ASR) [20] and polyphonic SED [11], solve the latter shortcoming by integrating information from the earlier time windows, presenting a theoretically unlimited context information.", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": "Recurrent neural networks (RNNs), which have been successfully applied to automatic speech recognition (ASR) [20] and polyphonic SED [11], solve the latter shortcoming by integrating information from the earlier time windows, presenting a theoretically unlimited context information.", "startOffset": 133, "endOffset": 137}, {"referenceID": 20, "context": "Similar approaches combining CNNs and RNNs have been presented recently in ASR [21], [28], [29] and music classification [30].", "startOffset": 79, "endOffset": 83}, {"referenceID": 27, "context": "Similar approaches combining CNNs and RNNs have been presented recently in ASR [21], [28], [29] and music classification [30].", "startOffset": 85, "endOffset": 89}, {"referenceID": 28, "context": "Similar approaches combining CNNs and RNNs have been presented recently in ASR [21], [28], [29] and music classification [30].", "startOffset": 91, "endOffset": 95}, {"referenceID": 29, "context": "Similar approaches combining CNNs and RNNs have been presented recently in ASR [21], [28], [29] and music classification [30].", "startOffset": 121, "endOffset": 125}, {"referenceID": 3, "context": "This approach integrates the strengths of both CNNs and RNNs, which have shown excellent performance in acoustic pattern recognition applications [4], [8], [9], [10], while overcoming their individual weaknesses.", "startOffset": 146, "endOffset": 149}, {"referenceID": 7, "context": "This approach integrates the strengths of both CNNs and RNNs, which have shown excellent performance in acoustic pattern recognition applications [4], [8], [9], [10], while overcoming their individual weaknesses.", "startOffset": 151, "endOffset": 154}, {"referenceID": 8, "context": "This approach integrates the strengths of both CNNs and RNNs, which have shown excellent performance in acoustic pattern recognition applications [4], [8], [9], [10], while overcoming their individual weaknesses.", "startOffset": 156, "endOffset": 159}, {"referenceID": 9, "context": "This approach integrates the strengths of both CNNs and RNNs, which have shown excellent performance in acoustic pattern recognition applications [4], [8], [9], [10], while overcoming their individual weaknesses.", "startOffset": 161, "endOffset": 165}, {"referenceID": 20, "context": "does not shrink) by computing the max pooling operation in the frequency dimension only\u2014as done in [21], [31]\u2014and by zero-padding the inputs to the convolutional layers (also known as same convolution).", "startOffset": 99, "endOffset": 103}, {"referenceID": 30, "context": "does not shrink) by computing the max pooling operation in the frequency dimension only\u2014as done in [21], [31]\u2014and by zero-padding the inputs to the convolutional layers (also known as same convolution).", "startOffset": 105, "endOffset": 109}, {"referenceID": 31, "context": "The function F , which can represent a long short term memory (LSTM) unit [32] or gated recurrent unit (GRU) [33], has two", "startOffset": 74, "endOffset": 78}, {"referenceID": 32, "context": "The function F , which can represent a long short term memory (LSTM) unit [32] or gated recurrent unit (GRU) [33], has two", "startOffset": 109, "endOffset": 113}, {"referenceID": 33, "context": "Regularization: In order to reduce overfitting, we experimented with dropout [34] regularization in the network, which has proven to be extremely effective in several deep learning applications [18].", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "Regularization: In order to reduce overfitting, we experimented with dropout [34] regularization in the network, which has proven to be extremely effective in several deep learning applications [18].", "startOffset": 194, "endOffset": 198}, {"referenceID": 33, "context": "This reduces units co-adaptation, approximates model averaging [34], and can be seen as a form of data augmentation without domain knowledge.", "startOffset": 63, "endOffset": 67}, {"referenceID": 34, "context": "For the recurrent layers we adopted the dropout proposed in [35], where the choice of dropped units is kept constant along a sequence.", "startOffset": 60, "endOffset": 64}, {"referenceID": 35, "context": "To speed up the training phase we train our networks with batch normalization layers [36] after every convolutional or fully connected layer.", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "with the network presented in [21] for speech recognition.", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": "(v) We use much longer sequences, up to thousands of steps, compared to 20 steps in [21].", "startOffset": 84, "endOffset": 88}, {"referenceID": 36, "context": "b) TUT Sound Events 2009 (TUT-SED 2009): This dataset, first presented in [37], consists of 8 to 14 binaural recordings from 10 real-life scenes.", "startOffset": 74, "endOffset": 78}, {"referenceID": 37, "context": "2016): This dataset consists of recordings from two real-life scenes: residential area and home [38].", "startOffset": 96, "endOffset": 100}, {"referenceID": 37, "context": "The four-fold cross-validation setup published along with the dataset [38] is used in the evaluations.", "startOffset": 70, "endOffset": 74}, {"referenceID": 38, "context": "d) CHiME-Home: CHiME-Home dataset [39] consists of 4-second audio chunks from home environments.", "startOffset": 34, "endOffset": 38}, {"referenceID": 39, "context": "Home as it is used in audio tagging task in DCASE2016 challenge [40], namely 1946 chunks for development (in four folds) and 846 chunks for evaluation.", "startOffset": 64, "endOffset": 68}, {"referenceID": 40, "context": "Segment-based F1 score calculated in a single time frame (F1frm) is used as the primary evaluation metric [41].", "startOffset": 106, "endOffset": 110}, {"referenceID": 41, "context": "This calculation method is referred to as micro-averaging, and is the recommended method for evaluation of classifier [42].", "startOffset": 118, "endOffset": 122}, {"referenceID": 40, "context": "More detailed and visualized explanation of segment-based F1 score in multi label setting can be found in [41].", "startOffset": 106, "endOffset": 110}, {"referenceID": 40, "context": "The second evaluation metric is segment-based error rate as proposed in [41].", "startOffset": 72, "endOffset": 76}, {"referenceID": 40, "context": ", the number of substitutions (s), insertions (i), deletions (d) and active classes from annotations (a) are calculated per segment as explained in detail in [41].", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": "The main metric used in previous works [11], [14], [15] on TUT-SED 2009 dataset differs from the F1 score calculation used in this paper.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "The main metric used in previous works [11], [14], [15] on TUT-SED 2009 dataset differs from the F1 score calculation used in this paper.", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "The main metric used in previous works [11], [14], [15] on TUT-SED 2009 dataset differs from the F1 score calculation used in this paper.", "startOffset": 51, "endOffset": 55}, {"referenceID": 37, "context": "For this work, we compare the proposed method with two recent approaches: the Gaussian mixture model (GMM) of [38] and the feedforward neural network model (FNN) from [15].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "For this work, we compare the proposed method with two recent approaches: the Gaussian mixture model (GMM) of [38] and the feedforward neural network model (FNN) from [15].", "startOffset": 167, "endOffset": 171}, {"referenceID": 11, "context": "GMM has been chosen as a baseline method since it is an established generative modeling method used in many sound recognition tasks [12], [13], [43].", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "GMM has been chosen as a baseline method since it is an established generative modeling method used in many sound recognition tasks [12], [13], [43].", "startOffset": 138, "endOffset": 142}, {"referenceID": 42, "context": "GMM has been chosen as a baseline method since it is an established generative modeling method used in many sound recognition tasks [12], [13], [43].", "startOffset": 144, "endOffset": 148}, {"referenceID": 14, "context": "GMM based methods in SED [15].", "startOffset": 25, "endOffset": 29}, {"referenceID": 37, "context": "GMM: The first baseline system is based on a binary frame-classification approach, where for each sound event class a binary classifier is set up [38].", "startOffset": 146, "endOffset": 150}, {"referenceID": 43, "context": "The system is used as a baseline in the DCASE2016 challenge [44], however, in this study the system is used as scene-independent to match the setting of the other methods presented.", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "FNN: The second baseline system is a deep multi-label FNN with temporal context [15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "The sliding window post-processing of the event activity probabilities in [15] has not been implemented for the baseline experiments in order to make a fair comparison based on classifier architecture for different deep learning", "startOffset": 74, "endOffset": 78}, {"referenceID": 37, "context": "GMM [38] 40.", "startOffset": 4, "endOffset": 8}, {"referenceID": 14, "context": "FNN [15] 49.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "We opted for this setting as it was recently used with very good performance in several works on SED [11], [15].", "startOffset": 101, "endOffset": 105}, {"referenceID": 14, "context": "We opted for this setting as it was recently used with very good performance in several works on SED [11], [15].", "startOffset": 107, "endOffset": 111}, {"referenceID": 44, "context": "The weights are initialized according to the scheme proposed in [46].", "startOffset": 64, "endOffset": 68}, {"referenceID": 45, "context": "Binary cross-entropy is set as the loss function, and all networks are trained with Adam [47] as gradient descent optimizer, with the default parameters proposed in the original paper.", "startOffset": 89, "endOffset": 93}, {"referenceID": 46, "context": "For feature extraction, the Python library Librosa [48] has been used in this work.", "startOffset": 51, "endOffset": 55}, {"referenceID": 47, "context": "2) as backend [50].", "startOffset": 14, "endOffset": 18}, {"referenceID": 48, "context": "As claimed in [51], this may be due to the capability of deep learning methods to use different subsets of hidden units to model different sound events simultaneously.", "startOffset": 14, "endOffset": 18}, {"referenceID": 30, "context": "This is consistent with the results presented in [31] on a similar task.", "startOffset": 49, "endOffset": 53}, {"referenceID": 13, "context": "formation about the scene is provided to the system and separate event models are trained for each scene [14], [24], [25].", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "formation about the scene is provided to the system and separate event models are trained for each scene [14], [24], [25].", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "formation about the scene is provided to the system and separate event models are trained for each scene [14], [24], [25].", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "More recent work [11], [15], as well as the current study,", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "More recent work [11], [15], as well as the current study,", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "HMM multiple Viterbi decoding? [24] 20.", "startOffset": 31, "endOffset": 35}, {"referenceID": 24, "context": "4 NMF-HMM? [25] 36.", "startOffset": 11, "endOffset": 15}, {"referenceID": 24, "context": "7 NMF-HMM + stream elimination? [25] 44.", "startOffset": 32, "endOffset": 36}, {"referenceID": 37, "context": "9 GMM? [38] 34.", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "Coupled NMF? [14] 57.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "FNN [15] 63.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "BLSTM [11] 64.", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "Methods [24], [25] are HMM based, using either multiple Viterbi decoding stages or NMF pre-processing to do polyphonic SED.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "Methods [24], [25] are HMM based, using either multiple Viterbi decoding stages or NMF pre-processing to do polyphonic SED.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "In contrast, the use of NMF in [14] does not build explicit class models, but performs coupled NMF of spectral representation and event activity annotations to build dictionaries.", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "CRNN has significantly higher performance than previous methods [14], [24], [25], [38], and it still shows considerable improvement over other neural network approaches.", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "CRNN has significantly higher performance than previous methods [14], [24], [25], [38], and it still shows considerable improvement over other neural network approaches.", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "CRNN has significantly higher performance than previous methods [14], [24], [25], [38], and it still shows considerable improvement over other neural network approaches.", "startOffset": 76, "endOffset": 80}, {"referenceID": 37, "context": "CRNN has significantly higher performance than previous methods [14], [24], [25], [38], and it still shows considerable improvement over other neural network approaches.", "startOffset": 82, "endOffset": 86}, {"referenceID": 49, "context": "[52] 17.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[53] 17.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[54] 17.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "More specifically, we use the technique described in [55] to visualize what kind of patterns in the input data different neurons in the convolutional layers are looking for.", "startOffset": 53, "endOffset": 57}, {"referenceID": 53, "context": "Transfer learning [56], [57] could be potentially applied with success in this setting: by first training a CRNN on a large dataset (such as TUT-SED Synthetic 2016), the last", "startOffset": 18, "endOffset": 22}], "year": 2017, "abstractText": "Sound events often occur in unstructured environments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNN) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural networks (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a Convolutional Recurrent Neural Network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.", "creator": "LaTeX with hyperref package"}}}