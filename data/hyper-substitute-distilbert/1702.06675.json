{"id": "1702.06675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Context-Aware Prediction of Derivational Word-forms", "abstract": "adjective morphology signifies generally rigorous and practical analytic analytic language. in this paper can illustrate slightly new task of predicting ; ambiguous form of a given base - form lemma alone is explicit for a precise context. \u00b7 compare my encoder - - decoder style neural network to produce unique derived noun character - by - character, based on its corresponding semantics - linguistic connectivity across intended base case and the meaning. we demonstrate because our model is able cannot generate valid context - sensitive derivations from known latin forms, consequently is nevertheless accurate under a lexicon linguistic perspective.", "histories": [["v1", "Wed, 22 Feb 2017 04:50:23 GMT  (160kb,D)", "http://arxiv.org/abs/1702.06675v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ekaterina vylomova", "ryan cotterell", "timothy baldwin", "trevor cohn"], "accepted": false, "id": "1702.06675"}, "pdf": {"name": "1702.06675.pdf", "metadata": {"source": "CRF", "title": "Context-Aware Prediction of Derivational Word-forms", "authors": ["Ekaterina Vylomova", "Ryan Cotterell", "Timothy Baldwin", "Trevor Cohn"], "emails": ["evylomova@gmail.com", "ryan.cotterell@gmail.com", "tbaldwin@unimelb.edu.au", "tcohn@unimelb.edu.au"], "sections": [{"heading": "1 Introduction", "text": "Understanding how new words are formed is a fundamental task in linguistics and language modelling, with significant implications for tasks with a generation component, such as abstractive summarisation and machine translation. In this paper we focus on modelling derivational morphology, to learn, e.g., that the appropriate derivational form of the verb succeed is succession given the context As third in the line of . . . , but is success in The play was a great .\nEnglish is broadly considered to be a morphologically impoverished language, and there are certainly many regularities in morphological patterns, e.g., the common usage of -able to transform a verb into an adjective, or -ly to form an adverb from an adjective. However there is considerable subtlety in English derivational morphology, in the form of: (a) idiosyncratic derivations; e.g. picturesque vs. beautiful vs. splendid as adjectival forms of the nouns picture, beauty and splendour, respectively; (b) derivational generation in context, which requires the automatic determination of the part-\nof-speech (POS) of the stem and the likely POS of the word in context, and POS-specific derivational rules; and (c) multiple derivational forms often exist for a given stem, and these must be selected between based on the context (e.g. success and succession as nominal forms of success, as seen above). As such, there are many aspects that affect the choice of derivational transformation, including morphotactics, phonology, semantics or even etymological characteristics. Earlier works (Thorndike, 1941) analysed ambiguity of derivational suffixes themselves when the same suffix might present different semantics depending on the base form it is attached to (cf. beautiful vs. cupful). Furthermore, as Richardson (1977) previously noted, even words with quite similar semantics and orthography such as horror and terror might have non-overlapping patterns: although we observe regularity in some common forms, for example, horrify and terrify, and horrible and terrible, nothing tells us why we observe terrorize and no instances of horrorize, or horrid, but not terrid.\nIn this paper, we propose the new task of predicting a derived form from its context and a base form. Our motivation in this research is primarily linguistic, i.e. we measure the degree to which it is possible to predict particular derivation forms from context. A similar task has been proposed in the context of studying how children master derivations (Singson et al., 2000). In their work, children were asked to complete a sentence by choosing one of four possible derivations. Each derivation corresponded either to a noun, verb, adjective, or adverbial form. Singson et al. (2000) showed that childrens\u2019 ability to recognize the correct form correlates with their reading ability. This observation confirms an earlier idea that orthographical regularities provide a clearer clues to morphological transformations comparing to phonological rules (Templeton, 1980; Moskowitz, 1973), especially in lanar X iv :1\n70 2.\n06 67\n5v 1\n[ cs\n.C L\n] 2\n2 Fe\nb 20\n17\nguages such as English where grapheme-phoneme correspondences are opaque. For this reason we consider orthographic rather than phonological representations.\nIn our approach, we test how well models incorporating distributional semantics can capture derivational transformations. Deep learning models capable of learning real-valued word embeddings have been shown to perform well on a range of tasks, from language modelling (Mikolov et al., 2013a) to parsing (Dyer et al., 2015) and machine translation (Bahdanau et al., 2015). Recently, these models have also been successfully applied to morphological reinflection tasks (Kann and Schu\u0308tze, 2016; Cotterell et al., 2016a)."}, {"heading": "2 Derivational Morphology", "text": "Morphology, the linguistic study of the internal structure of words, has two main goals: (1) to describe the relation between different words in the lexicon; and (2) to decompose words into morphemes, the smallest linguistic units bearing meaning. Morphology can be divided into two types: inflectional and derivational. Inflectional morphology is the set of processes through which the word form outwardly displays syntactic information, e.g., verb tense. It follows that an inflectional affix typically neither changes the part-of-speech (POS) nor the semantics of the word. For example, the English verb to run takes various forms: run, runs and ran, all of which convey the concept \u201cmoving by foot quickly\u201d, but appear in complementary syntactic contexts.\nDerivation, on the other hand, deals with the formation of new words that have semantic shifts in meaning (often including POS) and is tightly intertwined with lexical semantics (Light, 1996). Consider the example of the English noun discontentedness, which is derived from the adjective discontented. It is true that both words share a close semantic relationship, but the transformation is clearly more than a simple inflectional marking of syntax. Indeed, we can go one step further and define a chain of words content 7\u2192 contented 7\u2192 discontented 7\u2192 discontentedness.\nIn this work, we deal with the formation of deverbal nouns, i.e., nouns that are formed from verbs. Common examples of this in English include agentives (e.g., explain 7\u2192 explainer), gerunds (e.g., explain 7\u2192 explaining), as well as other nominalisations (e.g., explain 7\u2192 explanation). Nominal-\nisations have varyingly different meanings from their base verbs, and a key focus of this study is the prediction of which form is most appropriate depending on the context, in terms of syntactic and semantic concordance. Our model is highly flexible and easily applicable to other related lexical problems."}, {"heading": "3 Related Work", "text": "Although in the last few years many neural morphological models have been proposed, most of them have focused on inflectional morphology (e.g., see Cotterell et al. (2016a)). Focusing on derivational processes, there are three main directions of research. The first deals with the evaluation of word embeddings either using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al., 2016). In this context, it has been shown that, unlike inflectional morphology, most derivational relations cannot be as easily captured using distributional methods. Researchers working on the second type of task attempt to predict derived forms using the embedding of its corresponding base form and a vector encoding a \u201cderivational\u201d shift. Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Schu\u0308tze (2017) represent derivational affixes as vectors and investigate various functions to combine them with base forms. Kisselew et al.\n(2015) and Pado\u0301 et al. (2016) extend this line of research to model derivational morphology in German. This work demonstrates that various factors such as part of speech, semantic regularity and argument structure (Grimshaw, 1990) influence the predictability of a derived word. The third area of research focuses on the analysis of derivationally complex forms, which differs from this study in that we focus on generation. The goal of this line of work is to produce a canonicalised segmentation of an input word into its constituent morphs, e.g., unhappiness7\u2192un+happy+ness (Cotterell et al., 2015; Cotterell et al., 2016b). Note that the orthographic change y7\u2192i has been reversed."}, {"heading": "4 Dataset", "text": "As the starting point for the construction of our dataset, we used the CELEX English dataset (Baayen et al., 1993). We extracted verb\u2013noun lemma pairs from CELEX, covering 24 different nominalisational suffixes and 1,456 base lemmas. Suffixes only occurring in 5 or fewer lemma pairs mainly corresponded to loan words and consequently were filtered out. We augmented this dataset with verb\u2013verb pairs, one for each verb present in the verb\u2013noun pairs, to capture the case of a verbal form being appropriate for the given context.1 For each noun and verb lemma, we generated all their inflections, and searched for sentential contexts of each inflected token in a pre-tokenised dump of English Wikipedia.2 To dampen the effect of high-frequency words, we applied a heuristic log function threshold which is basically a weighted logarithm of the number of the contexts. The final dataset contains 3,079 unique lemma pairs represented in 107,041 contextual instances.3"}, {"heading": "5 Experiments", "text": "In this paper we model derivational morphology as a prediction task, formulated as follows. We take sentences containing a derivational form of a given lemma, then obscure the derivational form by replacing it with its base form lemma. The system must then predict the original (derivational) form, which may make use of the sentential context. System predictions are judged correct if they exactly\n1We also experimented without verb\u2013verb pairs and didn\u2019t observe much difference in the results.\n2Based on a 2008/03/12 dump. Sentences shorter than 3 words or longer than 50 words were removed from the dataset.\n3The code and the dataset are available at https:// github.com/ivri/dmorph\nmatch the original derived form."}, {"heading": "5.1 Baseline", "text": "As a baseline we considered a trigram model with modified Kneser-Ney smoothing, trained on the training dataset. Each sentence in the testing data was augmented with a set of confabulated sentences, where we replaced a target word with other its derivations or a base form. Unlike the general task, where we generate word forms as character sequences, here we use a set of known inflected forms for each lemma (from the training data). We then use the language model to score the collections of test sentences, and selected the variant with the highest language model score, and evaluate accuracy of selecting the original word form."}, {"heading": "5.2 Encoder\u2013Decoder Model", "text": "We propose an encoder\u2013decoder model. The encoder combines the left and the right contexts as well as a character-level base form representation:\nt = max(0, H \u00b7 [h\u2192left;h\u2190left;h\u2192right;h\u2190right; h\u2192base;h \u2190 base] + bh),\nwhere h\u2192left, h \u2190 left, h \u2192 right, h \u2190 right, h \u2192 base,h \u2190 base correspond to the last hidden states of an LSTM (Hochreiter and Schmidhuber, 1997) over left and right contexts and the character-level representation of the base form (in each case, applied forwards and backwards), respectively; H \u2208 R[h\u00d7l\u00d71.5,h\u00d7l\u00d76] is a weight matrix, and bh \u2208 R[h\u00d7l\u00d71.5] is a bias term. [; ] denotes a vector concatenation operation, h is the hidden state dimensionality, and l is the number of layers.\nNext we add an extra affine transformation, o = T \u00b7 t + bo, where T \u2208 R[h\u00d7l\u00d71.5,h\u00d7l] and bo \u2208 R[h\u00d7l], then o is then fed into the decoder:\ng(cj+1|cj ,o, lj+1) = softmax(R \u00b7 cj +max (B \u00b7 o, S \u00b7 lj+1) + bd),\nwhere cj is an embedding of the j-th character of the derivation, lj+1 is an embedding of the corresponding base character, B,S,R are weight matrices, and bd is a bias term.\nWe now elaborate on the design choices behind the model architecture which have been tailored to our task. We supply the model with the lj+1 character prefix of the base word to enable a copying mechanism, to bias the model to generate a derived form that is morphologically-related to the base\nverb. In most cases, the derived form is longer than its stem, and accordingly, when we reach the end of the base form, we continue to input an end-of-word symbol. We provide the model with the context vector o at each decoding step. It has been previously shown (Hoang et al., 2016) that this yields better results than other means of incorporation.4 Finally, we use max pooling to enable the model to switch between copying of a stem or producing a new character."}, {"heading": "5.3 Settings", "text": "We used a 3-layer bidirectional LSTM network, with hidden dimensionality h for both context and base-form stem states of 100, and character embedding cj of 100.5 We used pre-trained 300-dimensional Google News word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b). During the training of the model, we keep the word embeddings fixed, for greater applicability to unseen test instances. All tokens that didn\u2019t appear in this set were replaced with UNK sentinel tokens. The network was trained using SGD with momentum until convergence."}, {"heading": "5.4 Results", "text": "With the encoder\u2013decoder model, we experimented with the encoder\u2013decoder as described in Section 5.2 ( \u201cbiLSTM+CTX+BS\u201d), as well as several variations, namely: excluding context information (\u201cbiLSTM+BS\u201d), and excluding the bidirectional stem (\u201cbiLSTM+CTX\u201d). We also investigated how much improvement we can get from knowing the POS tag of the derived form, by presenting it explicitly to the model as extra conditioning context (\u201cbiLSTM+CTX+BS+POS\u201d). The main motivation for this relates to gerunds, where without the\n4We tried to feed the context information at the initial step only, and this led to worse prediction in terms of context-aware suffixes.\n5We also experimented with 15 dimensions, but found this model to perform worse.\nPOS, the model often overgenerates nominalisations. We then tried a single-directional context representation, by using only the last hidden states, i.e., h\u2192left and h \u2190 right, corresponding to the words to the immediate left and right of the wordform to be predicted (\u201cLSTM+CTX+BS+POS\u201d).\nWe ran two experiments: first, a shared lexicon experiment, where every stem in the test data was present in the training data; and second, using a split lexicon, where every stem in the test data was unseen in the training data. The results are presented in Table 1, and show that: (1) context has a strong impact on results, particularly in the shared lexicon case; (2) there is strong complementarity between the context and character representations, particularly in the split lexicon case; and (3) POS information is particularly helpful in the split lexicon case. Note that most of the models significantly outperform our baseline under shared lexicon setting. The baseline model doesn\u2019t support the split lexicon setting (as the derivational forms of interest, by definition, don\u2019t occur in the training data), so we cannot generate results in this setting."}, {"heading": "5.5 Error Analysis", "text": "We carried out error analysis over the produced forms of the LSTM+CTX+BS+POS model. First, the model sometimes struggles to differentiate between nominal suffixes: in some cases it puts an agentive suffix (-er or -or) in contexts where a nonagentive nominalisation (e.g. -ation or -ment) is appropriate. As an illustration of this, Figure 2 is a t-SNE projection of the context representations for simulate vs. simulator vs. simulation, showing that the different nominal forms have strong overlap. Secondly, although the model learns whether to\ncopy or produce a new symbol well, some forms are spelled incorrectly. Examples of this are studint, studion or even studyant rather than student as the agentive nominalisation of study. Here, the issue is opaqueness in the etymology, with student being borrowed from the Old French estudiant. For transformations which are native to English, for example, -ate 7\u2192 -ation, the model is much more accurate. Table 2 shows recall values achieved for various suffix types. We do not present precision since it could not be reliably estimated without extensive manual analysis.\nIn the split lexicon setting, the model sometimes misses double consonants at the end of words, producing wraper and winer and is biased towards generating mostly productive suffixes. An example of the last case might be stoption in place of stoppage. We also studied how much the training size affects the model\u2019s accuracy by reducing the data from 1,000 to 60,000 instances (maintaining a balance over lemmas). Interestingly, we didn\u2019t observe a significant reduction in accuracy. Finally, note that under the split lexicon setting, the model is agnostic of existing derivations, sometimes overgenerating possible forms. A nice illustration of that is trailation, trailment and trailer all being produced in the contexts of trailer. In other cases, the model might miss some of the derivations, for instance, predicting only government in the contexts of governance and government. We hypothesize that it is either due to very subtle differences in their contexts, or the higher productivity of -ment.\nFinally, we experimented with some nonsense stems, overwriting sentential instances of transcribe to generate context-sensitive derivational forms. Table 3 presents the nonsense stems, the correct form of transcribe for a given context, and the predicted derivational form of the nonsense word. Note that the base form is used correctly (top row) for three of the four nonsense words, and that despite the wide variety of output forms, they resemble plausible words in English. By looking at a larger slice of the data, we observed some regularities. For instance, fapery was mainly produced in the contexts of transcript whereas fapication was more related to transcription. Table 3 also shows that some of the stems appear to be more productive than others."}, {"heading": "6 Conclusions and Future Work", "text": "We investigated the novel task of context-sensitive derivation prediction for English, and proposed an encoder\u2013decoder model to generate nominalisations. Our best model achieved an accuracy of 90% on a shared lexicon, and 66% on a split lexicon. This suggests that there is regularity in derivational processes and, indeed, in many cases the context is indicative. As we mentioned earlier, there are still many open questions which we leave for future work. Further, we plan to scale to other languages and augment our dataset with Wiktionary data, to realise much greater coverage and variety of derivational forms."}, {"heading": "7 Acknowledgments", "text": "We would like to thank all reviewers for their valuable comments and suggestions. The second author was supported by a DAAD Long-Term Research Grant and an NDSEG fellowship. This research was supported in part by the Australian Research Council."}], "references": [{"title": "The CELEX lexical data base on CDROM", "author": ["Harald R Baayen", "Richard Piepenbrock", "H van Rijn"], "venue": null, "citeRegEx": "Baayen et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Baayen et al\\.", "year": 1993}, {"title": "Neural machine translation by jointly", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Joint semantic synthesis and morphological analysis of the derived word", "author": ["Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "CoRR, abs/1701.00946.", "citeRegEx": "Cotterell and Sch\u00fctze.,? 2017", "shortCiteRegEx": "Cotterell and Sch\u00fctze.", "year": 2017}, {"title": "Labeled morphological segmentation with semi-markov models", "author": ["Ryan Cotterell", "Thomas M\u00fcller", "Alexander Fraser", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 19th Conference on Computational Natural Language Learning (CoNLL 2015), pages 164\u2013", "citeRegEx": "Cotterell et al\\.,? 2015", "shortCiteRegEx": "Cotterell et al\\.", "year": 2015}, {"title": "The SIGMORPHON 2016 shared task morphological reinflection", "author": ["Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden."], "venue": "Proceedings of the 14th SIGMORPHON Workshop on Computational Re-", "citeRegEx": "Cotterell et al\\.,? 2016a", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "A joint model of orthography and morphological segmentation", "author": ["Ryan Cotterell", "Tim Vieira", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Cotterell et al\\.,? 2016b", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn\u2019t", "author": ["Anna Gladkova", "Aleksandr Drozd", "Satoshi Matsuoka."], "venue": "Proceedings of the 15th Annual Conference of the North", "citeRegEx": "Gladkova et al\\.,? 2016", "shortCiteRegEx": "Gladkova et al\\.", "year": 2016}, {"title": "Argument structure", "author": ["Jane Grimshaw."], "venue": "The MIT Press, Cambridge, MA, US.", "citeRegEx": "Grimshaw.,? 1990", "shortCiteRegEx": "Grimshaw.", "year": 1990}, {"title": "Computing semantic compositionality in distributional semantics", "author": ["Emiliano Guevara."], "venue": "Proceedings of the 9th International Conference on Computational Semantics, pages 135\u2013144. Association for Computational Linguistics.", "citeRegEx": "Guevara.,? 2011", "shortCiteRegEx": "Guevara.", "year": 2011}, {"title": "Kenlm: Faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187\u2013197. Association for Computational Linguistics.", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Incorporating side information into recurrent neural network language models", "author": ["Cong Duy Vu Hoang", "Trevor Cohn", "Gholamreza Haffari."], "venue": "Proceedings of the 15th Annual Conference of the North", "citeRegEx": "Hoang et al\\.,? 2016", "shortCiteRegEx": "Hoang et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Singlemodel encoder-decoder with explicit morphological representation for reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 54st Annual Meeting of the Association for Computational Linguistics (ACL 2016).", "citeRegEx": "Kann and Sch\u00fctze.,? 2016", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "Obtaining a better understanding of distributional models of german derivational morphology", "author": ["Max Kisselew", "Sebastian Pad\u00f3", "Alexis Palmer", "Jan \u0160najder."], "venue": "Proceedings of the 11th International Conference on Computational Semantics", "citeRegEx": "Kisselew et al\\.,? 2015", "shortCiteRegEx": "Kisselew et al\\.", "year": 2015}, {"title": "Compositionally derived representations of morphologically complex words in distributional semantics", "author": ["Angeliki Lazaridou", "Marco Marelli", "Roberto Zamparelli", "Marco Baroni."], "venue": "Proceedings of the 51st Annual Meeting of the Association for", "citeRegEx": "Lazaridou et al\\.,? 2013", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "Morphological cues for lexical semantics", "author": ["Marc Light."], "venue": "Proceedings of the 34st Annual Meeting of the Association for Computational Linguistics (ACL 1996), pages 25\u201331.", "citeRegEx": "Light.,? 1996", "shortCiteRegEx": "Light.", "year": 1996}, {"title": "Visualizing data using t-SNE", "author": ["Laurens van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9(Nov):2579\u20132605.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the Workshop at the International Conference on Learning Representations, 2013.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of Neural Information Processing Systems Conference (NIPS 2013), pages", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "On the status of vowel shift in English", "author": ["Arlene Moskowitz."], "venue": "Timothy E. Moore, editor, Cognitive Development and the Acquisition of Language. Academic Press.", "citeRegEx": "Moskowitz.,? 1973", "shortCiteRegEx": "Moskowitz.", "year": 1973}, {"title": "Predictability of distributional semantics in derivational word formation", "author": ["Sebastian Pad\u00f3", "Aur\u00e9lie Herbelot", "Max Kisselew", "Jan \u0160najder."], "venue": "Proceedings of the 26th International Conference on Computational Linguistics (COLING 2016), pages", "citeRegEx": "Pad\u00f3 et al\\.,? 2016", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2016}, {"title": "Lexical derivation", "author": ["John TE Richardson."], "venue": "Journal of Psycholinguistic Research, 6(4):319\u2013336.", "citeRegEx": "Richardson.,? 1977", "shortCiteRegEx": "Richardson.", "year": 1977}, {"title": "The relation between reading ability and morphological skills: Evidence from derivational suffixes", "author": ["Maria Singson", "Diana Mahony", "Virginia Mann."], "venue": "Reading and writing, 12(3):219\u2013252.", "citeRegEx": "Singson et al\\.,? 2000", "shortCiteRegEx": "Singson et al\\.", "year": 2000}, {"title": "Spelling, phonology, and the older student", "author": ["Shane Templeton."], "venue": "Developmental and cognitive aspects of learning to spell: A reflection of word knowledge, pages 85\u201396.", "citeRegEx": "Templeton.,? 1980", "shortCiteRegEx": "Templeton.", "year": 1980}, {"title": "The teaching of English suffixes, volume 847", "author": ["Edward Lee Thorndike."], "venue": "Teachers College, Columbia University.", "citeRegEx": "Thorndike.,? 1941", "shortCiteRegEx": "Thorndike.", "year": 1941}, {"title": "Take and took, gaggle and goose, book and read: evaluating the utility of vector differences for lexical relation learning", "author": ["Ekaterina Vylomova", "Laura Rimmel", "Trevor Cohn", "Timothy Baldwin."], "venue": "Proceedings of the 54th Annual Meeting of the Asso-", "citeRegEx": "Vylomova et al\\.,? 2016", "shortCiteRegEx": "Vylomova et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 25, "context": "(Thorndike, 1941) analysed ambiguity of derivational suffixes themselves when the same suffix might present different semantics depending on the base form it is attached to (cf.", "startOffset": 0, "endOffset": 17}, {"referenceID": 22, "context": "Furthermore, as Richardson (1977) previously", "startOffset": 16, "endOffset": 34}, {"referenceID": 23, "context": "the context of studying how children master derivations (Singson et al., 2000).", "startOffset": 56, "endOffset": 78}, {"referenceID": 24, "context": "This observation confirms an earlier idea that orthographical regularities provide a clearer clues to morphological transformations comparing to phonological rules (Templeton, 1980; Moskowitz, 1973), especially in lanar X iv :1 70 2.", "startOffset": 164, "endOffset": 198}, {"referenceID": 20, "context": "This observation confirms an earlier idea that orthographical regularities provide a clearer clues to morphological transformations comparing to phonological rules (Templeton, 1980; Moskowitz, 1973), especially in lanar X iv :1 70 2.", "startOffset": 164, "endOffset": 198}, {"referenceID": 22, "context": "the context of studying how children master derivations (Singson et al., 2000). In their work, children were asked to complete a sentence by choosing one of four possible derivations. Each derivation corresponded either to a noun, verb, adjective, or adverbial form. Singson et al. (2000) showed that childrens\u2019 ability to recognize the correct form correlates with their reading ability.", "startOffset": 57, "endOffset": 289}, {"referenceID": 18, "context": "Deep learning models capable of learning real-valued word embeddings have been shown to perform well on a range of tasks, from language modelling (Mikolov et al., 2013a) to parsing (Dyer et al.", "startOffset": 146, "endOffset": 169}, {"referenceID": 6, "context": ", 2013a) to parsing (Dyer et al., 2015) and machine translation (Bahdanau et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 1, "context": ", 2015) and machine translation (Bahdanau et al., 2015).", "startOffset": 32, "endOffset": 55}, {"referenceID": 13, "context": "Recently, these models have also been successfully applied to morphological reinflection tasks (Kann and Sch\u00fctze, 2016; Cotterell et al., 2016a).", "startOffset": 95, "endOffset": 144}, {"referenceID": 4, "context": "Recently, these models have also been successfully applied to morphological reinflection tasks (Kann and Sch\u00fctze, 2016; Cotterell et al., 2016a).", "startOffset": 95, "endOffset": 144}, {"referenceID": 16, "context": "Derivation, on the other hand, deals with the formation of new words that have semantic shifts in meaning (often including POS) and is tightly intertwined with lexical semantics (Light, 1996).", "startOffset": 178, "endOffset": 191}, {"referenceID": 7, "context": "The first deals with the evaluation of word embeddings either using a word analogy task (Gladkova et al., 2016) or binary relation type classification (Vylomova et al.", "startOffset": 88, "endOffset": 111}, {"referenceID": 26, "context": ", 2016) or binary relation type classification (Vylomova et al., 2016).", "startOffset": 47, "endOffset": 70}, {"referenceID": 3, "context": ", see Cotterell et al. (2016a)).", "startOffset": 6, "endOffset": 31}, {"referenceID": 8, "context": "Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms.", "startOffset": 0, "endOffset": 15}, {"referenceID": 8, "context": "Guevara (2011) notes that derivational affixes can be modelled as a geometrical function over the vectors of the base forms. On the other hand, Lazaridou et al. (2013) and Cotterell and Sch\u00fctze (2017) represent derivational affixes as vectors and investigate various functions to combine them with base forms.", "startOffset": 0, "endOffset": 168}, {"referenceID": 2, "context": "(2013) and Cotterell and Sch\u00fctze (2017) represent derivational affixes as vectors and investigate various functions to combine them with base forms.", "startOffset": 11, "endOffset": 40}, {"referenceID": 21, "context": "(2015) and Pad\u00f3 et al. (2016) extend this line of", "startOffset": 11, "endOffset": 30}, {"referenceID": 8, "context": "This work demonstrates that various factors such as part of speech, semantic regularity and argument structure (Grimshaw, 1990) influence the predictability of a derived word.", "startOffset": 111, "endOffset": 127}, {"referenceID": 3, "context": ", unhappiness7\u2192un+happy+ness (Cotterell et al., 2015; Cotterell et al., 2016b).", "startOffset": 29, "endOffset": 78}, {"referenceID": 5, "context": ", unhappiness7\u2192un+happy+ness (Cotterell et al., 2015; Cotterell et al., 2016b).", "startOffset": 29, "endOffset": 78}, {"referenceID": 0, "context": "dataset, we used the CELEX English dataset (Baayen et al., 1993).", "startOffset": 43, "endOffset": 64}, {"referenceID": 12, "context": "where hleft, h \u2190 left, h \u2192 right, h \u2190 right, h \u2192 base,h \u2190 base correspond to the last hidden states of an LSTM (Hochreiter and Schmidhuber, 1997) over left and right", "startOffset": 111, "endOffset": 145}, {"referenceID": 11, "context": "It has been previously shown (Hoang et al., 2016) that this yields better results than other means of incorporation.", "startOffset": 29, "endOffset": 49}, {"referenceID": 18, "context": "5 We used pre-trained 300-dimensional Google News word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b).", "startOffset": 66, "endOffset": 112}, {"referenceID": 19, "context": "5 We used pre-trained 300-dimensional Google News word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b).", "startOffset": 66, "endOffset": 112}, {"referenceID": 17, "context": "Figure 2: An example of t-SNE projection (Maaten and Hinton, 2008) of context representations for simulate", "startOffset": 41, "endOffset": 66}], "year": 2017, "abstractText": "Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose the new task of predicting the derivational form of a given base-form lemma that is appropriate for a given context. We present an encoder\u2013 decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under a lexicon agnostic setting.", "creator": "TeX"}}}