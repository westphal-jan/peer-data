{"id": "1702.06915", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Solving DCOPs with Distributed Large Neighborhood Search", "abstract": "the philosophy covering distributed constraint removal modeling gained freedom in recent years, fundamental toward its ability. encompass various applications linked to multi - mechanism cooperation. nevertheless, solving incomplete constrained exclusion syndrome ( dcops ) problems is np - hard. importantly, in large - scale, complex applications, incomplete dcop algorithms seldom necessary. current algorithm resolve algorithms suffer of 10 or more of what following limitations : when ( is ) find their hazards without providing greater compromise ; ( b ) provide loose quality restraints ; them ( either ) are unable to benefit when the structure retaining the parameters, simply as node - defining knowledge and optimal knowledge. therefore, capitalizing on attention from local centralized risk reduction community, we propose a distributed optimal neighborhood dynamics ( d - lns ) function to solve dcops. the proposed framework ( albeit its novel primitive phase ) provides guarantees on solution types, refining upper levels lower bounds incorporating proper iterative construction, and can exploit bound - governing structures. our experimental values show that rd - lns outperforms other incomplete solve games containing robust fully und unstructured sparse instances.", "histories": [["v1", "Wed, 22 Feb 2017 17:54:23 GMT  (1654kb,D)", "https://arxiv.org/abs/1702.06915v1", null], ["v2", "Thu, 23 Feb 2017 01:21:38 GMT  (1654kb,D)", "http://arxiv.org/abs/1702.06915v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ferdinando fioretto", "agostino dovier", "enrico pontelli", "william yeoh", "roie zivan"], "accepted": false, "id": "1702.06915"}, "pdf": {"name": "1702.06915.pdf", "metadata": {"source": "CRF", "title": "Solving DCOPs with Distributed Large Neighborhood Search", "authors": ["Ferdinando Fioretto", "Agostino Dovier", "Roie Zivan"], "emails": ["fioretto@umich.edu", "agostino.dovier@uniud.it", "tell@cs.nmsu.edu", "wyeoh@cs.nmsu.edu", "zivanr@cs.bgu.ac.il"], "sections": [{"heading": "1 Introduction", "text": "In a Distributed Constraint Optimization Problem (DCOP), multiple agents coordinate their value assignments to maximize the sum of resulting constraint utilities [13, 28]. DCOPs represent a powerful approach to the description and solution of many practical problems in a variety of application domains, such as distributed scheduling, coordination of unmanned air vehicles, smart grid electrical networks, and sensor networks [19, 31, 10, 23].\nIn many cases, the coordination protocols required for the complete resolution of DCOPs demand a vast amount of resources and/or communication, making them infeasible to solve real-world complex problems. In particular complete DCOP algorithms find optimal solutions at the cost of a large runtime or network load, while incomplete approaches trade optimality for lower usage of resources. Since finding optimal DCOP solutions is NP-hard, incomplete algorithms are often necessary to solve large interesting problems. Unfortunately, several local search algorithms (e.g., DSA [29], MGM [12]) and local inference algorithms (e.g., Max-Sum [3]) do not provide guarantees on the quality of the solutions found. More recent developments, such as region-optimal algorithms [15, 26], Bounded\n1 University of Michigan, Ann Arbor, US, email: fioretto@umich.edu 2 University of Udine, Udine, IT, email: agostino.dovier@uniud.it 3 New Mexico State University, Las Cruces, US, email: epon-\ntell@cs.nmsu.edu 4 New Mexico State University, Las Cruces, US, email: wyeoh@cs.nmsu.edu 5 Ben Gurion University, Beersheba, IL, email: zivanr@cs.bgu.ac.il 6 An extended abstract of this work appeared in [4].\nMax-Sum [20], and DaC algorithms [25, 8] alleviate this limitation. Region-optimal algorithms allow us to specify regions with a maximum size of k agents or t hops from each agent, and they optimally solve the subproblem within each region. Solution quality bounds are provided as a function of k and/or t. Bounded Max-Sum is an extension of Max-Sum, which solves optimally an acyclic version of the DCOP graph, bounding its solution quality as a function of the edges removed from the cyclic graph. DaC-based algorithms use Lagrangian decomposition techniques to solve agent subproblems suboptimally. Good quality assessments are essential for sub-optimal solutions. However, many incomplete DCOP approaches can provide arbitrarily poor quality assessments (as confirmed in our experimental results). In addition, they are unable to exploit domain-dependent knowledge or the hard constraints present in problems.\nIn this paper, we address these limitations by introducing the Distributed Large Neighborhood Search (D-LNS) framework. D-LNS solves DCOPs by building on the strengths of centralized LNS [22], a centralized meta-heuristic that iteratively explores complex neighborhoods of the search space to find better candidate solutions. LNS has been shown to be very effective in solving a number of optimization problems [6, 21]. While typical LNS approaches focus on iteratively refining lower bounds of a solution, we propose a method that can iteratively refine both lower and upper bounds of a solution, imposing no restrictions (i.e., linearity or convexity) on the objective function and constraints.\nThis work advances the state of the art in DCOP resolution: (1) We provide a novel distributed local search framework for DCOPs, which provides quality guarantees by refining both upper and lower bounds of the solution found during the iterative process; (2) We introduce two novel distributed search algorithms, DPOPDBR and T-DBR, built within the D-LNS framework, and characterized by the ability to exploit problem structure and offer low network usage\u2014T-DBR provides also a low computational complexity per agent; and (3) Our evaluation against representatives of searchbased, inference-based, and region-optimal-based incomplete DCOP algorithms shows that T-DBR converges faster to better solutions, provides tighter solution quality bounds, and is more scalable.\nThe rest of the paper is organized as follows. In the next section, we introduce DCOPs and review centralized LNS. Section 3 presents our novel D-LNS schema. Section 4 presents a general algorithm framework, based on D-LNS, that iteratively refines lower and upper bounds of the DCOP solutions. We further describe two implementations of such framework offering different tradeoffs of agent complexity and solution quality. Prior concluding the Section, we report an example trace of the proposed repair algorithm, aimed at elucidate its behavior within the D-LNS framework. Section 5 discusses the theoretical properties of the algorithms presented, with particular ar X\niv :1\n70 2.\n06 91\n5v 2\n[ cs\n.A I]\n2 3\nFe b\n20 17\nemphasis on the correctness for the solution bounds returned during the iterative process. We present the related works in Section 6, and summarize our evaluation of the proposed framework against searchbased, inference-based, and region-optimal-based DCOP incomplete algorithms, in Section 7. Finally, Section 8 concludes the paper."}, {"heading": "2 Background", "text": "Distributed Constraint Optimization Problems. A Distributed Constraint Optimization Problem (DCOP) is a tuple \u3008X ,D,F ,A, \u03b1\u3009, where: X = {x1, . . . , xn} is a set of variables; D = {D1, . . . , Dn} is a set of finite domains (i.e., xi \u2208 Di); F={f1, . . . , fe} is a set of utility functions (also called constraints), where fi : \"xj\u2208xfi Di \u2192 R+\u222a{\u2212\u221e} and x\nfi\u2286X is the set of the variables (also called the scope) relevant to fi; A= {a1, . . . , ap} is a set of agents; and \u03b1 : X \u2192 A is a function that maps each variable to one agent. fi specifies the utility of each combination of values assigned to the variables in xfi . Following common conventions, we restrict our attention to binary utility functions and assume that each agent controls exactly one variable. Thus, we will use the terms \u201cvariable\u201d and \u201cagent\u201d interchangeably and assume that \u03b1(xi) = ai. We assume at most one constraint between each pair of variables, thus making the order of variables in the scope of a constraint irrelevant.\nA partial assignment \u03c3 is a value assignment to a set of variables X\u03c3 \u2286 X that is consistent with the variables\u2019 domains. The utility F(\u03c3) = \u2211 f\u2208F,xf\u2286X\u03c3 f(\u03c3) is the sum of the utilities of all the applicable utility functions in \u03c3. A solution is a partial assignment \u03c3 for all the variables of the problem, i.e., with X\u03c3 =X . We will denote with x a solution, while xi is the value of xi in x. The goal is to find an optimal solution x\u2217 = argmaxx F(x).\nGiven a DCOP P , G = (X , E ) is the constraint graph of P , where (x, y)\u2208E iff \u2203fi \u2208F s.t. {x, y}= xfi . A DFS pseudotree arrangement for G is a spanning tree T = \u3008X , ET \u3009 of G s.t. if fi \u2208 F and {x, y} \u2286 xfi , then x and y appear in the same branch of T . Edges ofG that are in (resp. out of)ET are called tree edges (resp. backedges). Tree edges connect a node with its parent and its children, while backedges connect a node with its pseudoparents and its pseudo-children. We useN(ai)={aj \u2208A|(xi, xj)\u2208 E } to denote the neighbors of the agent ai. We denote with Gk = \u3008Xk, Ek\u3009, the subgraph of G used in the execution of our iterative algorithms, where Xk \u2286 X and Ek \u2286 E .\nFig. 1(a) depicts the graph of a DCOP with agents a1, . . . , a4, each controlling a variable with domain {0,1}. Fig. 1(b) shows a possible pseudo-tree (solid lines identify tree edges, dotted lines refer to backedges). Fig. 1(c) shows the DCOP constraints.\nLarge Neighborhood Search. In (centralized) Large Neighborhood Search (LNS), an initial solution is iteratively improved by re-\nAlgorithm 1: D-LNS 1 k \u2190 0; 2 \u3008x0i , LB0i ,UB0i \u3009 \u2190 VALUE-INITIALIZATION(); 3 while termination condition is not met do 4 k \u2190 k + 1; 5 zki \u2190 DESTROY-ALGORITHM(); 6 if zki = \u25e6 then xki \u2190 NULL; else xki \u2190 xk\u22121i ; 7 \u3008xki , LBki ,UBki \u3009 \u2190 REPAIR-ALGORITHM(zki ); 8 if not Accept (xki ,xk\u22121i ) then x k i \u2190 xk\u22121i ;\npeatedly destroying it and repairing it. Destroying a solution means selecting a subset of variables whose current values will be discarded. The set of such variables is referred to as large neighborhood (LN). Repairing a solution means finding a new value assignment for the destroyed variables, given that the other non-destroyed variables maintain their values from the previous iteration.\nThe peculiarity of LNS, compared to other local search techniques, is the (larger) size of the neighborhood to explore at each step. It relies on the intuition that searching over a larger neighborhood allows the process to escape local optima and find better candidate solutions."}, {"heading": "3 The D-LNS Framework", "text": "In this section, we introduce D-LNS, a general distributed LNS framework to solve DCOPs. Our D-LNS solutions need to take into account factors that are critical for the performance of distributed systems, such as network load (i.e., number and size of messages exchanged by agents) and the restriction that each agent is only aware of its local subproblem (i.e., its neighbors and the constraints whose scope includes its variables). Such properties make typical centralized LNS techniques unsuitable and infeasible for DCOPs.\nAlgorithm 1 shows the general structure of D-LNS, as executed by each agent ai \u2208A. After initializing its iteration counter k (line 1), its current value assignment x0i (done by randomly assigning values to variables or by exploiting domain knowledge when available), and its current lower and upper bounds LB0i and UB 0 i of the optimal utility (line 2), the agent, like in LNS, iterates through the destroy and repair phases (lines 3-8) until a termination condition occurs (line 3). Possible termination conditions include reaching a maximum value of k, a timeout limit, or a confidence threshold on the error of the reported best solution.\nDestroy Phase. The result of this phase is the generation of a LN, which we refer to as LNk \u2286 X , for each iteration k. This step is executed in a distributed fashion, having each agent ai calling a DESTROY-ALGORITHM to determine if its local variable xi should be destroyed (\u25e6) or preserved (?), as indicated by the flag zki (line 5). We say that destroyed (resp. preserved) variables are (resp. are not) in LNk. In a typical destroy process, such decisions can be either random or made by exploiting domain knowledge. For example, in a scheduling problem, one may choose to preserve the start times of each activity and destroy the other variables. D-LNS allows the agents to use any destroy schema to achieve the desired outcome. Once the destroyed variables are determined, the agents reset their values and keep the values of the preserved variables from the previous iteration (line 6).\nRepair Phase. The agents start the repair phase, which seeks to find new value assignments for the destroyed variables, by calling a REPAIR-ALGORITHM (line 7). The goal of this phase is to find\nan improved solution by searching over a LN, which is carried exclusively by the destroyed agents. However, the step to compute the solution bounds requires the cooperation of all agents in the problem. D-LNS is general in that it does not impose any restriction on the way agents coordinate to solve this problem. We propose two distributed repair algorithms in the next section, that provide quality guarantees and online bound refinements. Once the agents find and evaluate a new solution, they either accept it or reject it (line 8). In our proposed distributed algorithms, the agents accept the solution if it does not violate any hard constraints, that is, its utility is not \u2212\u221e.\nWhile most of the current incomplete DCOP algorithms fail to guarantee the consistency of the solution returned w.r.t. the hard constraints of the problem [15], D-LNS can accommodate consistency checks during the repair phase."}, {"heading": "4 Distributed Bounded Repair", "text": "We now introduce the Distributed Bounded Repair (DBR), a general REPAIR algorithm framework that, within D-LNS, iteratively refines the lower and upper bounds of the DCOP solution. Its general structure is illustrated in the flow chart of Figure 2. At each iteration k, each DBR agent checks if its local variable was preserved or destroyed. In the former case, the agent waits for the Bounding phase to start, which is algorithm dependent. In the latter case the agent executes, in order, the following phases:\nRelaxation Phase. Given a DCOP P , this phase constructs two relaxations of P , P\u030c k and P\u0302 k, which are used to compute, respectively, a lower and an upper bound on the optimal utility for P . Let Gk = \u3008LNk, Ek\u3009 be the subgraph of G in iteration k, where Ek={(x, y) | (x, y)\u2208E ;x, y\u2208LNk} is the subset of edges ofE (defined in Section 2) whose elements involve exclusively nodes in LNk. Both problem relaxations P\u030c k and P\u0302 k are solved using a relaxation graph G\u0303k= \u3008LNk, E\u0303k\u3009, computed from Gk, where E\u0303k\u2286Ek depends on the algorithm adopted.\nIn the problem P\u030c k, we wish to find a partial assignment x\u030ck using\nx\u030ck = argmax x [ \u2211 f\u2208E\u0303k f(xi,xj) + \u2211\nf\u2208F, xf={xi,xj} xi\u2208LNk, xj 6\u2208LNk\nf(xi, x\u030c k\u22121 j ) ] (1)\nwhere x\u030ck\u22121j is the value assigned to the preserved variable xj for problem P\u030c k\u22121 in the previous iteration. The first summation is over all functions listed in E\u0303k, while the second is over all functions between destroyed and preserved variables. Thus, solving P\u030c k means optimizing over all the destroyed variables given that the preserved ones take on their previous value, and ignoring the (possibly empty) set of edges E \\ (E\u0303k \u222a {(x, y) | (x, y)\u2208E ;x\u2208LNk, y 6\u2208LNk}) that are not part of the relaxation graph. This partial assignment is used to compute lower bounds during the bounding phase.\nIn the problem P\u0302 k, we wish to find a partial assignment x\u0302k using\nx\u0302k = argmax x \u2211 f\u2208E\u0303k f(xi,xj) (2)\nThus, solving P\u0302 k means optimizing over all the destroyed variables considering exclusively the set of edges E\u0303k that are part of the relaxation graph. This partial assignment is used to compute upper bounds during the bounding phase.\nNotice that the partial assignments returned solving these two relaxed problems involve exclusively the variables in LNk.\nSolving Phase. Next, DBR solves the relaxed DCOPs P\u030c k and P\u0302 k using the equations above. At a high level, one can use any complete DCOP algorithm to solve P\u030c k and P\u0302 k. Below, we describe two inference-based DBR algorithms, defined over different relaxation graphs G\u0303k. Thus, the output of this phase are the values x\u030cki , x\u0302 k i for the agent\u2019s local variable, associated to eqs. (1) and (2).\nBounding Phase. Once the relaxed problems are solved, all agents start the bounding phase, which results in computing the lower and upper bounds based on the partial assignments x\u030ck and x\u0302k. To do so, both solutions to the problems P\u030c k and P\u0302 k are extended to a solution x\u030ck and x\u0302k, respectively, for P , where the preserved variables xj 6\u2208 LNk are assigned the values x\u030ck\u22121j from the previous iteration.\nThe lower bound is thus computed by evaluating F(x\u030ck). The upper bound is computed by evaluating F\u0302 k(x\u0302k) = \u2211 f\u2208F f\u0302 k(x\u0302ki , x\u0302 k j ), where\nf\u0302k(xi,xj)=  max di\u2208Di,dj\u2208Dj f(di, dj) if \u0393kf = \u2205 max { F\u0303k |E\u0303k| , max `\u2208\u0393k\u22121 f f\u0302 `(x\u0302`i , x\u0302 ` j) } if f \u2208 E\u0303k\nf\u0302k\u22121(x\u0302k\u22121i , x\u0302 k\u22121 j ) otherwise\n(3)\nwith F\u0303 k = maxx \u2211 f\u2208E\u0303k f(xi,xj) is the optimal utility on the relaxation graph G\u0303k, and \u0393kf is the set of past iteration indices for which the function f was an edge in the relaxation graph. Specifically, \u0393kf = { ` | f \u2208 E\u0303` \u2227 0 < ` \u2264 k } .\nTherefore, the utility of F\u0302 k(x\u0302k) is composed of three parts. The first part involves all functions that have never been part of E\u0303k up to the current iteration, the second part involves all the functions optimized in the current iteration, and the third part involves all the remaining functions. The utility of each function in the first part is the maximal utility over all possible pairs of value combinations of variables in the scope of that function. The utility of each function in the second part is the largest utility among the mean utility of the functions optimized in the current iteration (i.e., those in E\u0303k), and the utilities of such function optimized in a past iteration. The utility of each function in the third part is equal to the utility assigned to such function in the previous iteration. In particular, imposing that the edges optimized in the current iteration contribute at most equally (i.e., as the mean utility of F\u0303 k) to the final utility of P\u0302 k allows us to not\nunderestimate the solution upper bound within the iterative process (see Lemma 1). As we show in Theorems 1 and 2,F(x\u030ck)\u2264F(x\u2217)\u2264 F\u0302 k(x\u0302k). Therefore, \u03c1= mink F\u0302 k(x\u0302k)\nmaxk F(x\u030ck) is a guaranteed approximation\nratio for P .\nThe significance of this REPAIR framework is that it enables DLNS to iteratively refine both lower and upper bounds of the solution, without imposing any restrictions on the form of the objective function and of the constraints adopted.7 Below, we introduce two implementations of the DBR framework, summarized in the flowchart of Figure 2, whose solving phase is shown in the dotted area."}, {"heading": "4.1 DPOP-based DBR Algorithm", "text": "DPOP-based DBR (DPOP-DBR) solves the relaxed DCOPs P\u030c k and P\u0302 k over the relaxed graph G\u0303k = \u3008LNk, Ek\u3009. Thus, E\u0303k =Ek, and solving problem P\u030c k means optimizing over all the destroyed variables ignoring no edges in Ek.\nThe DPOP-DBR solving phase uses DPOP [17], a complete inference-based algorithm composed of two phases operating on a DFS pseudo-tree. In the utility propagation phase, each agent, starting from the leaves of the pseudo-tree, projects out its own variable and sends its projected utilities to its parent. These utilities are propagated up the pseudo-tree induced from G\u0303k until they reach the root. The hard constraints of the problem can be naturally handled in this phase, by pruning all inconsistent values before sending a message to its parent. Once the root receives utilities from all its children, it starts the value propagation phase, where it selects the value that maximizes its utility and sends it to its children, which repeat the same process. The problem is solved as soon as the values reach the leaves.\nNote that the relaxation process may create a forest, in which case one should execute the algorithm in each tree of the forest. As a technical note, DPOP-DBR solves the two relaxed DCOPs in parallel. In the utility propagation, each agent computes two sets of utilities, one for each relaxed problem, and sends them to its parent. In the value propagation phase, each agent selects two values, one for each relaxed problem, and sends them to its children.\nDPOP-DBR has the same worst case order complexity of DPOP, that is, exponential in the induced width of the relaxed graph G\u0303k. Thus, we introduce another algorithm characterized by a smaller complexity and low network load."}, {"heading": "4.2 Tree-based DBR Algorithm", "text": "Tree-based DBR (T-DBR) defines the relaxed DCOPs P\u030c k and P\u0302 k using a pseudo-tree structure T k=\u3008LNk, ETk \u3009 that is computed from the subgraph Gk. Thus, E\u0303k =ETk , and solving problem P\u030c\nk means optimizing over all the destroyed variables ignoring backedges. Its general solving schema is similar to that of DPOP, in that it uses Utility and Value propagation phases; however, the different underlying relaxation graph adopted imposes several important differences. Algorithm 2 shows the T-DBR pseudocode. We use the following notations: P ki , C k i , PP k i denote the parent, the set of children, and pseudo-parents of the agent ai, at iteration k. The set of these items is referred to as Tki , which is ai\u2019s local view of the pseudo-tree T\nk. We use \u201c \u201d to refer to the items associated with the pseudo-tree T . \u03c7\u030ci and \u03c7\u0302i denote ai\u2019s context (i.e., the values for each xj \u2208 N(ai)) w.r.t. problems P\u030c and P\u0302 , respectively. We assume that by the end of\n7 Note, however that this does not implies that the lower bound and the upper bound will converge to the same value.\nAlgorithm 2: T-DBR(zki ) 9 Tki \u2190 RELAXATION(zki )\n10 UTIL-PROPAGATION(Tki ) 11 \u3008\u03c7\u030cki , \u03c7\u0302ki \u3009 \u2190 VALUE-PROPAGATION(Tki ) 12 \u3008LBki ,UBki \u3009 \u2190 BOUND-PROPAGATION(\u03c7\u030cki , \u03c7\u0302ki ) 13 return \u3008x\u030cki , LBki ,UBki \u3009\nProcedure UTIL-Propagation(Tki )\n14 receive UTILac(U\u030cc, U\u0302c) from each ac \u2208 Cki 15 forall values xi,xPki do 16 U\u030ci(xi,xPki\n)\u2190 f(xi,xPki )+\u2211 ac\u2208Cki U\u030cc(xi) + \u2211 xj 6\u2208LNk f(xi, x\u030c k\u22121 j )\n17 U\u0302i(xi,xPki )\u2190 f(xi,xPki ) + \u2211 ac\u2208Cki U\u0302c(xi) 18 forall values xPki do 19 \u3008U\u030c \u2032i(xPki ), U\u0302 \u2032 i(xPki )\u3009\u2190\u3008max xi U\u030ci(xi,xPki ),max xi U\u0302i(xi,xPki )\u3009 20 send UTILai(U\u030c \u2032 i , U\u0302 \u2032 i) msg to P k i\nFunction VALUE-Propagation(Tki )\n21 if Pki = NULL then 22 \u3008x\u030cki , x\u0302ki \u3009 \u2190 \u3008argmaxxi U\u030ci(xi), argmaxxi U\u0302i(xi)\u3009 23 send VALUEai (x\u030c k i , x\u0302 k i ) msg to each aj \u2208 N(ai) 24 forall aj \u2208 N(ai) do 25 receive VALUEaj (x\u030c k j , x\u0302 k j ) msg from aj 26 Update xj in \u3008\u03c7\u030cki , \u03c7\u0302ki \u3009 with \u3008x\u030ckj , x\u0302kj \u3009 27 else 28 forall aj \u2208 N(ai) do 29 receive VALUEaj (x\u030c k j , x\u0302 k j ) msg from aj 30 Update xj in \u3008\u03c7\u030cki , \u03c7\u0302ki \u3009 with \u3008x\u030ckj , x\u0302kj \u3009 31 if aj = Pki then 32 \u3008x\u030cki , x\u0302ki \u3009\u2190\u3008argmaxxiU\u030ci(xi), argmaxxiU\u0302i(xi)\u3009 33 send VALUEai (x\u030c k i , x\u0302 k i ) msg to each aj \u2208N(ai)\n34 return \u3008\u03c7\u030cki , \u03c7\u0302ki \u3009\nProcedure BOUND-Propagation(\u03c7\u030cki , \u03c7\u0302ki )\n35 receive BOUNDSac (LBkc ,UBkc ) msg from each ac \u2208 C i 36 LBki\u2190f(x\u030cki , x\u030ckP i ) + \u2211 aj\u2208PP i f(x\u030cki , x\u030c k j ) + \u2211 ac\u2208C i\nLBkc 37 UBki\u2190f\u0302k(x\u0302i, x\u0302P i ) + \u2211 aj\u2208PP i f\u0302k(x\u0302i, x\u0302j) + \u2211 ac\u2208C i\nUBkc 38 send BOUNDSai (LB k i ,UB k i ) msg to P i\nthe destroy phase (line 6) each agent knows its current context as well as which of its neighboring agents has been destroyed or preserved. In each iteration k, T-DBR executes the following phases:\nRelaxation Phase. It constructs a pseudo-tree T k (line 9), which ignores, from G , the preserved variables as well as the functions involving these variables in their scopes. The construction prioritizes tree-edges that have not been chosen in previous pseudo-trees over the others.\nSolving Phase. Similarly to DPOP-DBR, T-DBR solving phase is composed of two phases operating on the relaxed pseudo-tree T k, and executed synchronously: \u2022 Utility Propagation Phase. After the pseudo-tree T k is constructed\n(line 10), each leaf agent computes the optimal sum of utilities in its subtree considering exclusively tree edges (i.e., edges in ETk ) and edges with destroyed variables. Each leaf agent computes the\nutilities U\u030ci(xi,xPki ) and U\u0302i(xi,xPki ) for each pair of values of its variable xi and its parent\u2019s variable xPki (lines 15-17), in preparation for retrieving the solutions of P\u030c and P\u0302 , used during the bounding phase. The agent projects itself out (lines 18-19) and sends the projected utilities to its parent in a UTIL message (line 20). Each agent, upon receiving the UTIL message from each child, performs the same operations. Thus, these utilities will propagate up the pseudo-tree until they reach the root agent. \u2022 Value Propagation Phase. This phase starts after the utility propagation (line 11) by having the root agent compute its optimal values x\u030cki and x\u0302 k i for the relaxed DCOPs P\u030c and P\u0302 , respectively\n(line 22). It then sends its values to all its neighbors in a VALUE message (line 23). When its child receives this message, it also compute its optimal values and sends them to all its neighbors (lines 31-33). Thus, these values propagate down the pseudo-tree until they reach the leaves, at which point every agent has chosen its respective values. In this phase, in preparation for the bounding phase, when each agent receives a VALUE message from its neighbor, it will also update the value of its neighbor in both its contexts \u03c7\u030cki and \u03c7\u0302 k i (lines 24-26 and 29-30).\nBounding Phase. Once the relaxed DCOPs P\u030c and P\u0302 have been solved, the algorithm starts the bound propagation phase (line 12). This phase starts by having each leaf agent of the pseudo-tree T compute the lower and upper bounds LBki and UB k i (lines 36-37). These bounds are sent to its parent in T (line 38). When its parent receives this message form all its children (line 35), it performs the same operations. The lower and upper bounds of the whole problem are determined when the bounds reach the root agent."}, {"heading": "4.3 T-DBR Example Trace", "text": "Figure 3 illustrates a running example of T-DBR during the first two D-LNS iterations, using the DCOP of Figure 1. The trees T 1 and T 2 are represented by bold solid lines (functions in ETk ); all other functions are represented by dotted lines. The preserved variables in each iteration are shaded gray. At each step, the resolution of the relaxed problems involves the functions represented by bold lines\u2014P\u0302 is solved optimizing over the blue colored functions, and P\u030c over the red ones. We recall that while solving P\u0302 focuses solely on the functions in ETk , solving P\u030c further accounts for the function involving a destroyed and a preserved variable. The nodes illustrating destroyed variables are labeled with red values representing x\u030cki ,\n8 and nodes representing preserved variables are labeled with black values representing x\u030ck\u22121i . Each edge is labeled with a pair of values representing the utilities f\u0302k(x\u030cki , x\u030c k j ) (top, in blue) and f(x\u030c k i , x\u030c k j ) (bottom, in red)\n8 In our example solving P\u030c and P\u0302 yields the same solution for k = 1, 2.\nof the corresponding functions. The lower and upper bounds of each iteration are shown below.\nWhen k = 0, each agent randomly assigns a value to its variable, which results in a solution with utility F(x\u030c0) = f(x\u030c01, x\u030c02)+ f(x\u030c01, x\u030c 0 3)+f(x\u030c 0 1, x\u030c 0 4)+f(x\u030c 0 2, x\u030c 0 4)+f(x\u030c 0 3, x\u030c 0 4)=0+10+0+0+0=10 to get the lower bound. Moreover, solving P\u0302 0 yields a solution x\u03020 with utility F\u0302 0(x\u03020)= f\u03020(x\u030201, x\u030202)+f\u03020(x\u030201, x\u030203)+f\u03020(x\u030201, x\u030204)+f\u03020(x\u030202, x\u030204)+ f\u03020(x\u030203, x\u0302 0 4)=10+10+10+10+10=50, which is the upper bound.\nIn the first iteration (k = 1), the destroy phase preserves x2, and thus x\u030c12 = x\u030c02 = 1. The algorithm then builds the spanning tree with the remaining variables choosing f13 and f34 as a tree edges. Thus the relaxation graph for P\u030c 1 involves the edges {f13, f34, f12, f24} (in red), and the relaxation graph for P\u0302 1 involves the edges {f13, f34} (in blue). Solving P\u030c 1 yields partial assignment x\u030c1 with utility F\u030c 1(x\u030c1)=f(x\u030c11, x\u030c13)+f(x\u030c13, x\u030c14)+f(x\u030c11, x\u030c12)+ f(x\u030c12, x\u030c 1 4) = 10 + 6 + 0 + 10 = 26, which results in a lower bound F(x\u030c1)= F\u030c 1(x\u030c1)+f(x\u030c11, x\u030c14)=26+6=32. Solving P\u0302 1 yields solution x\u03021 with utility F\u0302 1(x\u03021)= f\u03021(x\u030211, x\u030212)+f\u03021(x\u030211, x\u030213)+f\u03021(x\u030211, x\u030214)+ f\u03021(x\u030212, x\u0302 1 4)+ f\u0302 1(x\u030213, x\u0302 1 4) = 10+8+10+10+8 = 46, which is the current upper bound. Recall that the values for the functions in E\u0303k are computed as F\u0303 k(x)\n|E\u0303k| = 16 2 =8 (see eq. (3)). Finally, in the second iteration (k=2), the destroy phase retains x3 assigning it its value in the previous iteration x\u030c23 = x\u030c13 =0, and the repair phase builds the new spanning tree with the remaining variables choosing f12 and f24 as a tree edges. Thus the relaxation graph for P\u030c 2 involves the edges {f12, f24, f13, f34}, and the relaxation graph for P\u0302 2 involves the edges {f12, f24}. Solving P\u030c 2 and P\u0302 2 yields partial assignments x\u030c2 and x\u03022, respectively, with utilities F\u030c 2(x\u030c2) = 10+6+10+6=32, which results in a lower boundF(x\u030c2)=32+6=38, and an upper bound F\u0302 2(x\u03022)=8+8+10+8+8=42."}, {"heading": "5 Theoretical Properties", "text": "We report below the theoretical results on the bounds provided by our D-LNS framework with the DBR REPAIR algorithm, as well as the agents\u2019 complexity and network load of T-DBR. Due to space constraints, we report sketch proofs.\nTheorem 1 For each LNk, F(x\u030ck) \u2264 F(x\u2217).\nProof (Sketch). The result follows from that x\u030ck is an optimal solution of the relaxed problem P\u030c whose functions are a subset of F . 2\nLemma 1 For each k, \u2211 f\u2208E\u0303k f\u0302(x\u0302ki , x\u0302 k j ) \u2265 \u2211 f\u2208E\u0303k f(x\u2217i ,x \u2217 j ), where x\u0302ki is the value assignment to variable xi when solving the relaxed DCOP P\u0302 and x\u2217i is the value assignment to variable xi when solving the original DCOP P .\nProof (Sketch). For each iteration k, it follows:\n\u2211 f\u2208E\u0303k f\u0302(x\u0302ki , x\u0302 k j ) \u2265 \u2211 f\u2208E\u0303k max { F\u0303 k |E\u0303k| , max `\u2208\u0393k\u22121 f f\u0302 `(x\u0302`i , x\u0302 ` j) }\n(by def. of f\u0302 (case 2))\n\u2265 \u2211 f\u2208E\u0303k F\u0303 k |E\u0303k|\n\u2265 F\u0303 k = \u2211 f\u2208E\u0303k f(x\u0302ki , x\u0302 k j ) (by def. of F\u0303 k)\n\u2265 \u2211 f\u2208E\u0303k f(x\u0302\u2217i , x\u0302 \u2217 j ).\nThe last step follows from that, in each iteration k, the functions associated with the edges in E\u0303k are solved optimally. Since their cost is maximized it is also greater than the corresponding cost when evaluated on the optimal solution for the problem P . 2 Lemma 2 For each k, \u2211 f\u2208\u0398k f\u0302k(x\u0302ki , x\u0302 k j ) \u2265 \u2211 f\u2208\u0398k f(x\u2217i ,x \u2217 j ), where \u0398k = {f \u2208 F |\u0393kf 6= \u2205} is the set of functions that have been chosen as edges of the relaxation graph in a previous iteration.\nProof (Sketch). We prove it by induction on the iteration k. For ease of explanation we provide an illustration (below) of the set of relevant edges optimized in successive iterations.\nFor k=0, then \u03980 =\u2205, thus the statement vacuously holds. Assume the claim holds up to iteration k \u2212 1. For iteration k it follows that,\u2211 f\u2208\u0398k f\u0302k(x\u0302ki , x\u0302 k j )\n= \u2211\nf\u2208\u0398k-1 f\u0302k(x\u0302ki , x\u0302 k j ) + \u2211 f\u2208\u0398k\\\u0398k-1 f\u0302k(x\u0302ki , x\u0302 k j )\n= \u2211\nf\u2208\u0398k-1\\E\u0303k\nf\u0302k(x\u0302ki , x\u0302 k j ) + \u2211 f\u2208\u0398k-1\u2229E\u0303k f\u0302k(x\u0302ki , x\u0302 k j ) + \u2211 f\u2208\u0398k\\\u0398k-1 f\u0302k(x\u0302ki , x\u0302 k j )\n= \u2211\nf\u2208\u0398k-1\\E\u0303k\nf\u0302k-1(x\u0302k-1i , x\u0302 k-1 j ) + \u2211 f\u2208\u0398k\\\u0398k-1 f\u0302k(x\u0302ki , x\u0302 k j )\n+ \u2211\nf\u2208\u0398k-1\u2229E\u0303k\nmax { f\u0302k(x\u0302ki ,x\u0302 k j ), f\u0302 k-1(x\u0302k-1i ,x\u0302 k-1 j ) } (by def. of f\u0302k)\nThe last step follows from cases 2 and 3 of eq. (3). Additionally, the following inequalities hold:\u2211 f\u2208\u0398k-1\\E\u0303k f\u0302k-1(x\u0302k-1i , x\u0302 k-1 j ) + \u2211 f\u2208\u0398k-1\u2229E\u0303k max { f\u0302k(x\u0302ki ,x\u0302 k j ), f\u0302 k-1(x\u0302k-1i ,x\u0302 k-1 j ) }\n\u2265 \u2211\nf\u2208\u0398k-1 f\u0302k-1(x\u0302k-1i , x\u0302 k-1 j )\n\u2265 \u2211\nf\u2208\u0398k-1 f(x\u2217i ,x \u2217 j ). (by inductive hypothesis)\n\u2211 f\u2208\u0398k-1\u2229E\u0303k max { f\u0302k(x\u0302ki ,x\u0302 k j ), f\u0302 k-1(x\u0302k-1i ,x\u0302 k-1 j ) } + \u2211 f\u2208\u0398k\\\u0398k-1 f\u0302k(x\u0302ki , x\u0302 k j )\n\u2265 \u2211 f\u2208E\u0303k f\u0302k(x\u0302ki , x\u0302 k j )\n\u2265 \u2211 f\u2208E\u0303k f(x\u2217i ,x \u2217 j ). (by Lemma 1)\nThus, combining the above it follows:\u2211 f\u2208\u0398k-1\\E\u0303k f\u0302k-1(x\u0302k-1i , x\u0302 k-1 j ) + \u2211 f\u2208\u0398k\\\u0398k-1 f\u0302k(x\u0302ki , x\u0302 k j )\n+ \u2211\nf\u2208\u0398k-1\u2229E\u0303k\nmax { f\u0302k(x\u0302ki ,x\u0302 k j ), f\u0302 k-1(x\u0302k-1i ,x\u0302 k-1 j ) }\n\u2265 \u2211\nf\u2208\u0398k-1\\E\u0303k\nf(x\u2217i ,x \u2217 j ) + \u2211 f\u2208\u0398k\\\u0398k-1 f(x\u2217i ,x \u2217 j )\n+ \u2211\nf\u2208\u0398k-1\u2229E\u0303k\nmax { f(x\u2217i ,x \u2217 j ), f(x \u2217 i ,x \u2217 j )) }\n\u2265 \u2211 f\u2208\u0398k f(x\u2217i ,x \u2217 j ).\nWhich concludes the proof. 2. Lemma 3 ensures that the utility associated to the functions optimized in the relaxed problems P\u0302 , up to iteration k, is an upper bound for the evaluation of the same set of functions, evaluated under the optimal solution for P . The above proof relies on the observation that the functions in \u0398k include exclusively those ones associated with the optimization of problems P\u0302 `, with ` \u2264 k, and that the functions over which the optimization process operates multiple times (in \u0398k-1\u2229 E\u0303k), are evaluated with their maximal value observed so far.\nTheorem 2 For each LNk, F\u0302 k(x\u0302k) \u2265 F(x\u2217).\nProof (Sketch). By definition of F\u0302 k(x), it follows that,\nF\u0302 k(x) = \u2211 f\u2208F f\u0302k(x\u0302ki , x\u0302 k j )\n= \u2211 f\u2208\u0398k f\u0302k(x\u0302ki , x\u0302 k j ) + \u2211 f 6\u2208\u0398k f\u0302k(x\u0302ki , x\u0302 k j )\n= \u2211 f\u2208\u0398k f\u0302k(x\u0302ki , x\u0302 k j ) + \u2211 f 6\u2208\u0398k max di,dj f(di, dj) (by def. of f\u0302k)\n\u2265 \u2211 f\u2208\u0398k f(x\u2217i , x \u2217 j ) + \u2211 f 6\u2208\u0398k f(x\u2217i , x \u2217 j ) (by Lemma 2) = F(x\u2217)\nwhich concludes the proof. 2\nCorollary 1 An approximation ratio for the problem is\n\u03c1 = mink F\u0302\nk(x\u0302k) maxk F(x\u030ck) \u2265 F(x \u2217) maxk F(x\u030ck)\nProof (Sketch). This result follows from maxk F(x\u030ck) \u2264 F(x\u2217) (Theorem 1) and mink F\u0302 k(x\u0302k) \u2265 F(x\u2217) (Theorem 2). 2\nTheorem 3 In each iteration, T-DBR requires O(|F|) number of messages of size O(d), where d = max\nai\u2208A |Di|.\nProof (Sketch). The number of messages required at each iteration is bounded by the Value Propagation Phase of Algorithm 2, where each agent sends a message to each of its neighbors (lines 23 and 33). In contrast all other phases use up to |A| messages (which are reticulated from the leaves to the root of the pseudo-tree and viceversa). The size of the messages is bounded by the Utility Propagation Phase, where each agent (excluding the root agent) sends a message containing a value for each element of its domain (line 20). All other messages exchanged contain two values (lines 23, 33, and 38). Thus the maximum size of the messages exchanged at each iteration is at most d. 2\nTheorem 4 In each iteration, the number of constraint checks of each T-DBR agent is O(d2), where d= max\nai\u2208A |Di|.\nProof (Sketch). The number of constraint checks, performed by each agent in each iteration, is bounded by the operations performed during the Util-Propagation Phase. In this phase, each agent (except the root agent) computes the lower and upper bound utilities for each values of its variable xi and its parent\u2019s variable xPki (lines 16\u201317). 2"}, {"heading": "6 Related Work", "text": "Aside from the incomplete algorithms described in the introduction, researchers have also developed extensions to complete algorithms that trade solution quality for faster runtime. For example, complete search algorithms have mechanisms that allow users to specify absolute or relative error bounds [13, 27]. Researchers have also worked on non-iterative versions of inference-based incomplete DCOP algorithms, with and without quality guarantees [20, 14, 16]. Such methods are, however, unable to refine the initial solution returned. Finally, the algorithm that is the most similar to ours is LS-DPOP [18], which operates on a pseudo-tree performing a local search. However, unlike D-LNS, LS-DPOP operates only in a single iteration, does not change its neighborhood, and does not provide quality guarantees."}, {"heading": "7 Experimental Results", "text": "We evaluate the D-LNS framework against state-of-the-art incomplete DCOP algorithms, with and without quality guarantees, where we choose representative search-, inference-, and region optimalbased solution approaches. We select Distributed Stochastic Algorithm (DSA) as a representative of an incomplete search-based DCOP algorithm; Max-Sum (MS), and Bounded Max-Sum (BMS), as representative of inference-based DCOP algorithms, and k- and t-optimal algorithms (KOPT, and TOPT), as representative of region optimal-based DCOP methods. All algorithms are selected based on their performance and popularity. We run the algorithms using the following implementations: We use the FRODO framework [11] to run MS, and DSA,9 the authors\u2019 code of BMS [20], and the DALO framework [9] for KOPT and TOPT. We systematically evaluate the runtime, solution quality and network load of the algorithms on binary constraint networks with random, scale-free, and grid topologies, and we evaluate the ability of D-LNS to exploit domain knowledge over distributed meeting scheduling problems. The instances for each topology are generated as follows:\nRandom: We create an n-node network, whose density p1 produces bn (n\u2212 1) p1c edges in total. We do not bound the tree-width, which is based on the underlying graph.\nScale-free: We create an n-node network based on the BarabasiAlbert model [1]. Starting from a connected 2-node network, we repeatedly add a new node, randomly connecting it to two existing nodes. In turn, these two nodes are selected with probabilities that are proportional to the numbers of their connected edges. The total number of edges is 2 (n\u2212 2) + 1.\nGrid: We create an n-node network arranged in a rectangular grid, where internal nodes are connected to four neighboring nodes and nodes on the edges (resp. corners) are connected to two (resp. three) neighbors.\n9 We modified DSA-C in FRODO to DSA-B and set p = 0.6.\n|A| DPOP-DBR T-DBR BMS KOPT2 KOPT3 TOPT1 MaxSum DSA\nWe generate 50 instances for each topology, ensuring that the underlying graph is connected. The utility functions are generated using random integer costs in [0, 100]. We set as default parameters, |A| = 20, |Di| = 10 for all variables, and p1 = 0.5 for random networks. We use a random destroy strategy for the D-LNS algorithms. Algorithms\u2019 runtimes are measured using the simulated runtime metric [24], and we impose a timeout of 300s. Results are averaged over all instances and are statistically significant10 with p-values < 0.0001. The experiment are performed on an Intel i7 Quadcore 3.3GHz machine with 4GB of RAM.\nFigure 4 illustrates the convergence results (normalized upper and lower bounds) for grids (left), random (center), and scale-free (right) networks in increasing amounts of maximum time allowed to the algorithms to complete. A value of 0 (1), means worst (best) lower or upper bound w.r.t. the lower or upper bound reported within the pool of algorithms examined. All plots are in log-scale. These results show that the D-LNS-based algorithms converge to better solutions. In addition, they provide tighter upper bounds, and thus find better approximation ratios compared to the other algorithms. The figures reporting the upper bounds do not illustrate MS and DSA, as they do not provide bounded solutions. TOPT-1 timed-out for all instances on random and scale-free networks. D-LNS with DPOP-DBR is slower than D-LNS with T-DBR, and it reaches a timeout for the scale-free networks. This is due to the fact that the complexity of the former repair phase is exponential in the induced width of the relaxed constraint graph, and scale-free exhibit higher induced widths than grids and random network instances. In contrast, D-LNS with T-DBR does not encounter such limitations. The main reason behind fast convergence to good solutions of the D-LNS algorithms is that, on average, about half of the agents are destroyed at each iteration, thus reducing the search space significantly. Additionally, the destroy phase of D-LNS is likely to create pseudo-forests, thus agents operating in different pseudo-trees can perform their operations concurrently.\nNext, we validate our results at the varying of the number of agents in the problem, on random networks. Table 1 reports the approximation ratio \u03c1 and the ratio of the best quality found by all algorithms versus its quality, as well as the runtime t. Best approximation ratios, quality ratios, and runtimes are shown in bold. The results show that D-LNS with DPOP-DBR finds better approximation ratios \u03c1 than those of the competing algorithms. However, it fails to solve problems bigger than 20 agents. In contrast, D-LNS with T-DBR can scale to large problems better than other algorithms. Similarly to the trends observed in the previous experiment, DSA converges fastest to its solution for all problem sizes, however, D-LNS with T-DBR finds better solutions w.r.t. all the other algorithms (i.e., better quality ratios and better approximation ratios \u03c1 for |A| > 20).\nDistributed Meeting Scheduling. Many real-world problems model require the use of hard constraints, to avoid considering infeasible solutions (see, e.g., http://www.csplib.org). We also evaluate the ability of our D-LNS framework to exploit such structure, 10 t-test performed with null hypothesis: DLNS-based algorithms find solu-\ntion with better bounds than non-DLNS based ones.\nexhibited in presence of domain-dependent knowledge and hard constraints, and test its behavior on distributed meeting scheduling problems. In such problems, one wishes to schedule a set of events within a time range. We use the event as variable formulation [12], where events are modeled as decision variables. Meeting participants can\nattend different meetings, and have time preferences that are taken into account in the problem formulation. Each variable can take on a value from the time slot range in [0, 100], that is sufficiently early to schedule the required participant for the required amount of time. The problem requires that no meetings sharing some participants overlap. We generate the underlying constraint network using the random network model described earlier. The resulting meetings to schedule have 95, 613, and 2475 participants, in average respectively for the 20, 50, and 100 meetings experiments. We compare the repair phase T-DBR with both random (RN) destroy and domain-specific knowledge (DK) destroy methods. The latter destroys the set of variables in overlapping meetings. Table 2 reports the percentage of satisfied instances reported (% SAT) and the time needed to find the first satisfiable solution (TF), averaged over 50 runs.\nThe domain-specific destroy method has a clear advantage over the random one, being able to effectively exploit domain knowledge in presence of the hard constraints. All other local search algorithm failed to report satisfiable solutions for any of the problems\u2014only KOPT3 was able to find some satisfiable solutions for 20 meetings."}, {"heading": "8 Conclusions", "text": "In this paper, we proposed a Distributed Large Neighborhood Search (D-LNS) framework that can be used to find quality-bounded solutions in DCOPs. D-LNS is composed of a destroy phase, which selects a large neighborhood to search, and a repair phase, which performs the search over the selected neighborhood. We introduce two novel distributed repair phases, DPOP-DBR and T-DBR, built within the D-LNS framework, and characterized by low network usage; additionally, T-DBR provides a low computational complexity per agent. Experimental results show that the D-LNS algorithms quickly converge to better solutions compared to incomplete DCOP algorithms that are representative of search-, inference-, and regionoptimal-based approaches. The proposed results are significant\u2014the ability of refining online quality guarantees, its quick convergence to good solutions, and the ability to exploit domain-dependent structure, makes D-LNS-based algorithms good candidates to solve a wide class of DCOP problems. Additionally D-LNS can be extended to benefit of an anytime property, by using an anytime framework like that proposed in [30]. In the near future, we plan to investigate other schemes to incorporate into the repair phase of D-LNS, including constraints propagation techniques [2, 5, 7] to better prune the search\nspace, and techniques that actively exploit the bounds reported during the iterative procedure. We strongly believe that this framework has the potential to solve large distributed constraint optimization problems, with thousands of agents, variables, and constraints, and we plan to apply D-LNS based algorithms in the context of large distributed resource allocation problems in the near future."}], "references": [{"title": "Emergence of scaling in random networks", "author": ["Albert-L\u00e1szl\u00f3 Barab\u00e1si", "R\u00e9ka Albert"], "venue": "Science, 286(5439),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Including Soft Global Constraints in DCOPs", "author": ["Christian Bessiere", "Patricia Gutierrez", "Pedro Meseguer"], "venue": "Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Decentralised coordination of low-power embedded devices using the Max-Sum algorithm", "author": ["Alessandro Farinelli", "Alex Rogers", "Adrian Petcu", "Nicholas Jennings"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Large Neighborhood Search with Quality Guarantees for Distributed Constraint Optimization Problems", "author": ["Ferdinando Fioretto", "Federico Campeotto", "Agostino Dovier", "Enrico Pontelli", "William Yeoh"], "venue": "in AAMAS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Improving DPOP with branch consistency for solving distributed constraint optimization problems", "author": ["Ferdinando Fioretto", "Tiep Le", "William Yeoh", "Enrico Pontelli", "Tran Cao Son"], "venue": "Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Randomized large neighborhood search for cumulative scheduling.", "author": ["Daniel Godard", "Philippe Laborie", "Wim Nuijten"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Maintaining Soft Arc Consistencies in BnB-ADOPT + during Search", "author": ["Patricia Gutierrez", "Jimmy Ho-Man Lee", "Ka Man Lei", "Terrence W.K. Mak", "Pedro Meseguer"], "venue": "Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Deqed: An efficient divideand-coordinate algorithm for dcop", "author": ["Daisuke Hatano", "Katsutoshi Hirayama"], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Asynchronous algorithms for approximate distributed constraint optimization with quality bounds", "author": ["Christopher Kiekintveld", "Zhengyu Yin", "Atul Kumar", "Milind Tambe"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Distributed constraint optimization with structured resource constraints", "author": ["Akshat Kumar", "Boi Faltings", "Adrian P"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Frodo 2.0: An open-source framework for distributed constraint optimization", "author": ["Thomas L\u00e9aut\u00e9", "Brammert Ottens", "Radoslaw Szymanek"], "venue": "in International Workshop on Distributed Constraint Reasoning (DCR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Distributed algorithms for DCOP: A graphical game-based approach", "author": ["Rajiv Maheswaran", "Jonathan Pearce", "Milind Tambe"], "venue": "Proceedings of the Conference on Parallel and Distributed Computing Systems (PDCS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "ADOPT: Asynchronous distributed constraint optimization with quality guarantees", "author": ["Pragnesh Modi", "Wei-Min Shen", "Milind Tambe", "Makoto Yokoo"], "venue": "Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Pseudo-tree-based incomplete algorithm for distributed constraint optimization with quality bounds", "author": ["Tenda Okimoto", "Yongjoon Joe", "Atsushi Iwasaki", "Makoto Yokoo", "Boi Faltings"], "venue": "Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Quality guarantees on k-optimal solutions for distributed constraint optimization problems", "author": ["Jonathan Pearce", "Milind Tambe"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJ- CAI),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Approximations in distributed optimization", "author": ["Adrian Petcu", "Boi Faltings"], "venue": "Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "A scalable method for multiagent constraint optimization", "author": ["Adrian Petcu", "Boi Faltings"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "A hybrid of inference and local search for distributed combinatorial optimization", "author": ["Adrian Petcu", "Boi Faltings"], "venue": "Proceedings of the International Conference on Intelligent Agent Technology (IAT),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Agent-based control for decentralised demand side management in the smart grid", "author": ["Sarvapali D Ramchurn", "Perukrishnen Vytelingum", "Alex Rogers", "Nick Jennings"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Bounded approximate decentralised coordination via the maxsum algorithm", "author": ["Alex Rogers", "Alessandro Farinelli", "Ruben Stranders", "Nicholas Jennings"], "venue": "Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "An adaptive large neighborhood search heuristic for the pickup and delivery problem with time windows", "author": ["Stefan Ropke", "David Pisinger"], "venue": "Transportation science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Using constraint programming and local search methods to solve vehicle routing problems", "author": ["Paul Shaw"], "venue": "Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Decentralised coordination of mobile sensors using the Max- Sum algorithm", "author": ["Ruben Stranders", "Alessandro Farinelli", "Alex Rogers", "Nicholas Jennings"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "On modeling multiagent task scheduling as a distributed constraint optimization problem.", "author": ["Evan Sultanik", "Pragnesh Jay Modi", "William C Regli"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Divide-and-coordinate: Dcops by agreement", "author": ["Meritxell Vinyals", "Marc Pujol", "Juan A Rodriguez-Aguilar", "Jesus Cerquides"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Quality guarantees for region optimal DCOP algorithms", "author": ["Meritxell Vinyals", "Eric Shieh", "Jes\u00fas Cerquides", "Juan Rodriguez- Aguilar", "Zhengyu Yin", "Milind Tambe", "Emma Bowring"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Trading off solution quality for faster computation in DCOP search algorithms", "author": ["William Yeoh", "Xiaoxun Sun", "Sven Koenig"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJ- CAI),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Distributed problem solving", "author": ["William Yeoh", "Makoto Yokoo"], "venue": "AI Magazine, 33(3),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Distributed stochastic search and distributed breakout: Properties, comparison and applications to constraint optimization problems in sensor networks", "author": ["Weixiong Zhang", "Guandong Wang", "Zhao Xing", "Lars Wittenberg"], "venue": "Artificial Intelligence,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Anytime local search for distributed constraint optimization", "author": ["Roie Zivan"], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Explorative anytime local search for distributed constraint optimization", "author": ["Roie Zivan", "Steven Okamoto", "Hilla Peled"], "venue": "Artificial Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "In a Distributed Constraint Optimization Problem (DCOP), multiple agents coordinate their value assignments to maximize the sum of resulting constraint utilities [13, 28].", "startOffset": 162, "endOffset": 170}, {"referenceID": 27, "context": "In a Distributed Constraint Optimization Problem (DCOP), multiple agents coordinate their value assignments to maximize the sum of resulting constraint utilities [13, 28].", "startOffset": 162, "endOffset": 170}, {"referenceID": 18, "context": "DCOPs represent a powerful approach to the description and solution of many practical problems in a variety of application domains, such as distributed scheduling, coordination of unmanned air vehicles, smart grid electrical networks, and sensor networks [19, 31, 10, 23].", "startOffset": 255, "endOffset": 271}, {"referenceID": 30, "context": "DCOPs represent a powerful approach to the description and solution of many practical problems in a variety of application domains, such as distributed scheduling, coordination of unmanned air vehicles, smart grid electrical networks, and sensor networks [19, 31, 10, 23].", "startOffset": 255, "endOffset": 271}, {"referenceID": 9, "context": "DCOPs represent a powerful approach to the description and solution of many practical problems in a variety of application domains, such as distributed scheduling, coordination of unmanned air vehicles, smart grid electrical networks, and sensor networks [19, 31, 10, 23].", "startOffset": 255, "endOffset": 271}, {"referenceID": 22, "context": "DCOPs represent a powerful approach to the description and solution of many practical problems in a variety of application domains, such as distributed scheduling, coordination of unmanned air vehicles, smart grid electrical networks, and sensor networks [19, 31, 10, 23].", "startOffset": 255, "endOffset": 271}, {"referenceID": 28, "context": ", DSA [29], MGM [12]) and local inference algorithms (e.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": ", DSA [29], MGM [12]) and local inference algorithms (e.", "startOffset": 16, "endOffset": 20}, {"referenceID": 2, "context": ", Max-Sum [3]) do not provide guarantees on the quality of the solutions found.", "startOffset": 10, "endOffset": 13}, {"referenceID": 14, "context": "More recent developments, such as region-optimal algorithms [15, 26], Bounded", "startOffset": 60, "endOffset": 68}, {"referenceID": 25, "context": "More recent developments, such as region-optimal algorithms [15, 26], Bounded", "startOffset": 60, "endOffset": 68}, {"referenceID": 3, "context": "il 6 An extended abstract of this work appeared in [4].", "startOffset": 51, "endOffset": 54}, {"referenceID": 19, "context": "Max-Sum [20], and DaC algorithms [25, 8] alleviate this limitation.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "Max-Sum [20], and DaC algorithms [25, 8] alleviate this limitation.", "startOffset": 33, "endOffset": 40}, {"referenceID": 7, "context": "Max-Sum [20], and DaC algorithms [25, 8] alleviate this limitation.", "startOffset": 33, "endOffset": 40}, {"referenceID": 21, "context": "D-LNS solves DCOPs by building on the strengths of centralized LNS [22], a centralized meta-heuristic that iteratively explores complex neighborhoods of the search space to find better candidate solutions.", "startOffset": 67, "endOffset": 71}, {"referenceID": 5, "context": "LNS has been shown to be very effective in solving a number of optimization problems [6, 21].", "startOffset": 85, "endOffset": 92}, {"referenceID": 20, "context": "LNS has been shown to be very effective in solving a number of optimization problems [6, 21].", "startOffset": 85, "endOffset": 92}, {"referenceID": 14, "context": "the hard constraints of the problem [15], D-LNS can accommodate consistency checks during the repair phase.", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "The DPOP-DBR solving phase uses DPOP [17], a complete inference-based algorithm composed of two phases operating on a DFS pseudo-tree.", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "For example, complete search algorithms have mechanisms that allow users to specify absolute or relative error bounds [13, 27].", "startOffset": 118, "endOffset": 126}, {"referenceID": 26, "context": "For example, complete search algorithms have mechanisms that allow users to specify absolute or relative error bounds [13, 27].", "startOffset": 118, "endOffset": 126}, {"referenceID": 19, "context": "Researchers have also worked on non-iterative versions of inference-based incomplete DCOP algorithms, with and without quality guarantees [20, 14, 16].", "startOffset": 138, "endOffset": 150}, {"referenceID": 13, "context": "Researchers have also worked on non-iterative versions of inference-based incomplete DCOP algorithms, with and without quality guarantees [20, 14, 16].", "startOffset": 138, "endOffset": 150}, {"referenceID": 15, "context": "Researchers have also worked on non-iterative versions of inference-based incomplete DCOP algorithms, with and without quality guarantees [20, 14, 16].", "startOffset": 138, "endOffset": 150}, {"referenceID": 17, "context": "Finally, the algorithm that is the most similar to ours is LS-DPOP [18], which operates on a pseudo-tree performing a local search.", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "We run the algorithms using the following implementations: We use the FRODO framework [11] to run MS, and DSA, the authors\u2019 code of BMS [20], and the DALO framework [9] for KOPT and TOPT.", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "We run the algorithms using the following implementations: We use the FRODO framework [11] to run MS, and DSA, the authors\u2019 code of BMS [20], and the DALO framework [9] for KOPT and TOPT.", "startOffset": 136, "endOffset": 140}, {"referenceID": 8, "context": "We run the algorithms using the following implementations: We use the FRODO framework [11] to run MS, and DSA, the authors\u2019 code of BMS [20], and the DALO framework [9] for KOPT and TOPT.", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "Scale-free: We create an n-node network based on the BarabasiAlbert model [1].", "startOffset": 74, "endOffset": 77}, {"referenceID": 23, "context": "Algorithms\u2019 runtimes are measured using the simulated runtime metric [24], and we impose a timeout of 300s.", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "We use the event as variable formulation [12], where events are modeled as decision variables.", "startOffset": 41, "endOffset": 45}, {"referenceID": 29, "context": "Additionally D-LNS can be extended to benefit of an anytime property, by using an anytime framework like that proposed in [30].", "startOffset": 122, "endOffset": 126}, {"referenceID": 1, "context": "In the near future, we plan to investigate other schemes to incorporate into the repair phase of D-LNS, including constraints propagation techniques [2, 5, 7] to better prune the search", "startOffset": 149, "endOffset": 158}, {"referenceID": 4, "context": "In the near future, we plan to investigate other schemes to incorporate into the repair phase of D-LNS, including constraints propagation techniques [2, 5, 7] to better prune the search", "startOffset": 149, "endOffset": 158}, {"referenceID": 6, "context": "In the near future, we plan to investigate other schemes to incorporate into the repair phase of D-LNS, including constraints propagation techniques [2, 5, 7] to better prune the search", "startOffset": 149, "endOffset": 158}], "year": 2017, "abstractText": "The field of Distributed Constraint Optimization has gained momentum in recent years, thanks to its ability to address various applications related to multi-agent cooperation. Nevertheless, solving Distributed Constraint Optimization Problems (DCOPs) optimally is NP-hard. Therefore, in large-scale, complex applications, incomplete DCOP algorithms are necessary. Current incomplete DCOP algorithms suffer of one or more of the following limitations: they (a) find local minima without providing quality guarantees; (b) provide loose quality assessment; or (c) are unable to benefit from the structure of the problem, such as domain-dependent knowledge and hard constraints. Therefore, capitalizing on strategies from the centralized constraint solving community, we propose a Distributed Large Neighborhood Search (D-LNS) framework to solve DCOPs. The proposed framework (with its novel repair phase) provides guarantees on solution quality, refining upper and lower bounds during the iterative process, and can exploit domain-dependent structures. Our experimental results show that D-LNS outperforms other incomplete DCOP algorithms on both structured and unstructured problem instances.", "creator": "LaTeX with hyperref package"}}}