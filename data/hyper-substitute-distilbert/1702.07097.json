{"id": "1702.07097", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2017", "title": "Bidirectional Backpropagation: Towards Biologically Plausible Error Signal Transmission in Neural Networks", "abstract": "the back - antenna ( rf ) step becomes nowadays considered even de facto method for training modern neural computational. it back - propagates errors from the molecular level than the hidden elements in an exact case using feedforward antennas. using this work, we propose a radically biologically plausible paradigm of complexity architecture according to specified contexts. essentially, we propose two independent algorithms algorithms with mutual estimates of trainable weights. preliminary results show that our graphs predict approximately fitting the animal and dolphin cifar10 datasets among empirical asymmetric error signal passing methods, and instruction quality is exponential relative to that unlike bp.", "histories": [["v1", "Thu, 23 Feb 2017 05:00:54 GMT  (837kb,D)", "https://arxiv.org/abs/1702.07097v1", null], ["v2", "Sun, 26 Feb 2017 16:41:35 GMT  (971kb,D)", "http://arxiv.org/abs/1702.07097v2", "Extended the paper to the length of a long paper; added references in introduction; corrected the experiments of BFA"], ["v3", "Mon, 20 Mar 2017 01:16:07 GMT  (993kb,D)", "http://arxiv.org/abs/1702.07097v3", "[v2]Extended the paper to the length of a long paper; added references in introduction; corrected the experiments of BFA. [v3] Added link to source code"]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["hongyin luo", "jie fu", "james glass"], "accepted": false, "id": "1702.07097"}, "pdf": {"name": "1702.07097.pdf", "metadata": {"source": "META", "title": "Bidirectional Backpropagation: Towards Biologically Plausible Error Signal Transmission in Neural Networks", "authors": ["Hongyin Luo", "Jie Fu"], "emails": ["hyluo@mit.edu", "jie.fu@u.nus.edu", "glass@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Back-propagation (BP) algorithm is the combination of reverse-mode automatic differentiation [1] and steepest descent [10] which has been considered the de-facto method for training deep neural networks (DNNs). It back-propagates errors from output to input layer by layer in an exact manner. However, it has been argued that it is not biologically possible for learning in the brain to involve precise, symmetric backward channels [6, 15, 3, 12].\nIn the early days of deep learning, unsupervised pre-training with Boltzmann machines used to be applied before fine-tuning with BP [7], which does not involve symmetric weights and is biologically motivated. Recently, there has been a rising interest in developing both biologically feasible and practical alternatives for BP. In [21], target-propagation (TP)[12], whose objective is to let each layer to reproduce outputs the previous layer, is used to train a recurrent neural network for natural language processing tasks. The authors in [15] propose feedback-alignment (FA) model and showed that for BP-like methods, the weights used in the feedback pass do not have to be the transpose of feedforward weights. The direct feedback-alignment (DFA) model proposed in [17] suggest that error signals could be transmitted directly from output layer to any hidden layer with random and fixed matrices. One of the key requirements in FA and DFA model is that the feedback is random and fixed.\nOn the other hand, due to the literature in neuroscience, long-term potentiation (LTP) is considered an essential step in human memory and learning [16, 4]. As introduced in LTP, strong links between neurons are established starts from the neural adjustment step that one of the neurons moves more ion receptors onto the membrane of its dendrites. As a result, more ions can be captured, which amplifies the electrical impulses.\nBased on the principles of LTP and the hypothesis that the feedback weights are plastic [2], we propose a more biological plausible perceptron paradigm and two bidirectional learning models. In the bidirectional learning models, the feedforward weights are adjusted in forward phase, and feedback weights are learned in backward phase. Our proposed models dispel the assumption that the feedback weights have to be random and fixed. The feedback weights are trained to approximate the forward activations during training. Experiments on benchmark datasets show\n\u2217Equal contribution\nar X\niv :1\n70 2.\n07 09\n7v 3\n[ cs\n.N E\n] 2\n0 M\nar 2\n01 7\nthat our models outperform the FA and DFA counterparts, which use fixed feedback weights to transmit error signals. We also provide preliminary analysis on why transmitting error signals with adaptive weights outperforms using fixed weights. To the best of our knowledge, this is the first research attempt to show that adaptive asymmetric feedback channels are more effective than random and fixed feedback channels in DNNs."}, {"heading": "2 Background", "text": "Following the notation in [14], let (x, y) be a mini-batch of input-output of size 1. The DNN we consider here has 2 hidden layers. wi are the weights (absorbing biases) connecting the previous layer to a unit in the i-th hidden layer. The activations can be computed as\na1 = w1x,h1 = \u03c3(a1) (1)\na2 = w2h1,h2 = \u03c3(a2) (2)\nay = w3h2, y\u0302 = \u03c3y(ay) (3)\nwhere \u03c3y(\u00b7) the activation function used in the output layer and \u03c3(\u00b7) is activation function used in hidden layers.\nThe loss and the gradient at the output layer are:\nl = \u2212y \u00b7 logy\u0302 \u2212 (1\u2212 y) \u00b7 log(1\u2212 y\u0302) (4)\n\u2202l\n\u2202ay = y\u0302 \u2212 y = e (5)\nThe gradients for hidden layers for the back-propagation (BP) are:\n\u03b4a2 = \u2202l \u2202a2 = (wT3 \u03b4ay) \u03c3\u2032(a2) (6)\n\u03b4a1 = \u2202l \u2202a1 = (wT2 \u03b4a2) \u03c3\u2032(a1) (7)\nwhere \u03c3\u2032(\u00b7) is the derivative of the activation function and is an element-wise multiplication operator.\nFor feedback alignment (FA), as shown in Figure 1, the hidden layer update directions are:\n\u03b4a2 = (B2e) \u03c3\u2032(a2) (8)\n\u03b4a1 = (B1\u03b4a2) \u03c3\u2032(a1) (9) In direct feedback alignment (DFA) model, error signals are transmitted directly from the output layer to each hidden layer. Gradients for layer 1 are calculated differently from FA, which is\n\u03b4a1 = (B1e) \u03c3\u2032(a1) (10) where Bi is a fixed random weighted matrix with appropriate dimension.\nThe weight updates for BP, FA and DFA methods are calculated as\n\u03b4w1 = \u2212\u03b4a1xT , \u03b4w2 = \u2212\u03b4a2hT1 , \u03b4w3 = \u2212ehT2 (11) where we ignore the learning rates, and the random weights are only used to transfer gradients back to hidden neurons. In BP, updating the individual weights needs to store the weights used in the forward pass.\nAlthough FA and DFA with random and fixed feedback weights are more biologically plausible than BP, the feedback weights in the brain are plastic too [3]. It has been shown in [15] that the forward weights wi used in FA learns to resemble the pseudo-inverse of the feedback random weights Bi. Therefore, it would be desirable to prevent the forward weights wi from becoming too similar to a random matrix. We will demonstrate that the model can be optimized by bidirectional training later.\nThe conventional DNNs with BP, FA, and DFA are unidirectional in the sense that they only learn how to map inputs to target outputs. In this paper, based on related literature in neuroscience, we propose a paradigm of biologically plausible perceptron model. Then we propose bidirectional feedback alignment (BFA) and bidirectional direct feedback alignment (BDFA) model, which connect neurons by two sets of trainable weights for the forward and the backward processes, respectively. A DNN with either BFA or BDFA is trained to predict outputs and generate feature maps simultaneously."}, {"heading": "3 Biologically Plausible Perceptron", "text": "Classical perceptrons trained with gradient descent algorithms need to back-propagate error signals based on the exact feedforward synaptic weights, which is considered impossible in a biological neural system [19]. On the other hand, long-term potentiation (LTP) is considered an essential part of biological memory and learning in cognitive science [16, 4]. In this section, we first briefly describe the LTP mechanism and then propose a more biologically plausible perceptron paradigm."}, {"heading": "3.1 Long-term Potentiation", "text": "Biological neurons are connected by synapse, including axons and dendrites, where axons emit signals, and dendrites of the next neuron receive the electrical impulses released by axons [8]. However, axons and dendrites are separated by synaptic clefts, and the axons send electrical impulses by releasing ions into synaptic cleft [5]. The ions are captured by receptors on the cell membrane of dendrites [4, 13]. The architecture is shown in Figure 2.\nWhen a synapse transmits neural signals from neuron N1 to neuron N2 and is repeatedly simulated, neuron N2 will release more receptors on its dendrites and thus capture more ions [4]. This procedure reduces the ion concentration of the synaptic cleft between N1 and N2, which encourages N1 to release more ions [4]. Thus, a stronger connection between neuron N1 and N2 is established due to the LTP procedure [4]. LTP adjusts links among neurons and plays a significant role in forming memory and learning [4]."}, {"heading": "3.2 Biologically Plausible Perceptron Model", "text": "The first step of synaptic adjustment between neuron N1 and N2 is that N2 adjusts the quantity of receptors on its dendrites, which is an important observation from LTP procedure. Based on this principle, we propose a more biologically plausible perceptron (BioPP) model.\nThe components of BioPP model are described as follow,\n\u2022 Signals There are two sets of signals in BioPP architecture \u2013 Feedforward Signals: Signals propagated forward in network for inference tasks \u2013 Error Signals: Signals propagated backward for adjusting synaptic weights\n\u2022 Weights Weights stand for the quantity of signal a perceptron decides to capture from input or adjacent neurons. It should be noted that the quantity of the error signals taken by a perceptron is also decided by itself. A BioPP adjusts its own weights based on the incoming error signals, and then sends error signals to other neurons.\n\u2022 Activations and Biases The definition of activations and biases follows that of the standard DNNs trained with BP.\nThe architecture of BioPP is shown in Figure 3, where the green circles are neurons, the blue curves stand for forward synapses. The red curves stand for backward synapses. The blue squares and the red squares are receptors for the forward and the backward synapses, respectively. It is worth noting that according to the definition of BioPP, weights are adjusted by the receptors.\nThere are three restrictions on BioPP,\n\u2022 Error signals cannot be calculated using incoming weights because axons convey neural signals unidirectionally.\n\u2022 A Neuron learns its weights locally based on internal or external error signals. \u2022 All incoming weights should be adaptive. The neural models proposed in feedback-alignment (FA) and direct feedback-alignment (DFA) are not fully in accordance with the BioPP principles in that some of their incoming weights are fixed. In the following sections, we extend them by optimizing the feedback weights of FA and DFA based on BioPP principles and test the models on benchmark datasets."}, {"heading": "4 Training BioPP Networks Bidirectionally", "text": "Both FA and DFA train neural networks with fixed random weights to propagate error signals. In BioPP, we make those weights adaptive as general incoming weights. For a DNN with 2 hidden layers, the activations in the forward pass are calculated as\n\u2212\u2192a 1 = \u2212\u2192w1x, \u2212\u2192 h 1 = \u03c3( \u2212\u2192a 1) (12)\n\u2212\u2192a 2 = \u2212\u2192w2 \u2212\u2192 h 1, \u2212\u2192 h 2 = \u03c3( \u2212\u2192a 2) (13)\nay = \u2212\u2192w3 \u2212\u2192 h 2, y\u0302 = \u03c3y(ay) (14)\nIn this section, we propose bidirectional-FA (BFA) and bidirectional-DFA (BDFA) and describe their training pipeline. Then we provide preliminary analysis on why the adaptive feedback weights perform better than fixed feedback weights."}, {"heading": "4.1 Bidirectional-FA", "text": "DNNs with BFA or BDFA learn two mappings between the input and the target output in a twoway manner. To learn these two mappings, we define two loss functions: feedforward loss \u2212\u2192 l and feedback loss \u2190\u2212 l , which measure the error in predicting labels and features in the hidden layers or inputs, respectively. For BFA, the loss functions are:\n\u2212\u2192 l = \u2212y \u00b7 logy\u0302 \u2212 (1\u2212 y) \u00b7 log(1\u2212 y\u0302) (15)\n\u2190\u2212 l = 1\n2 \u2016x\u0302\u2212 x\u201622 (16)\nwhere y\u0302 and x\u0302 are predicted output and predicted inputs. y and x are target output and target inputs.\nWe define the forward weights as \u2212\u2192 W and the feedback weights as \u2190\u2212 W . The training pipeline includes forward learning phase and backward learning phase, and process them iteratively in each batch.\nThe gradient at the output layer is calculated as\n\u03b4ay = \u2202 \u2212\u2192 l\n\u2202ay = y\u0302 \u2212 y = \u2212\u2192e (17)\nFor BFA, the gradients for hidden layers in the forward pass are calculated as\n\u03b4\u2212\u2192a 2 = \u2202 \u2212\u2192 l\n\u2202\u2212\u2192a 2 = (\u2190\u2212w1\u2212\u2192e ) \u03c3\u2032(\u2212\u2192a 2) (18)\n\u03b4\u2212\u2192a 1 = \u2202 \u2212\u2192 l\n\u2202\u2212\u2192a 1 = (\u2190\u2212w2\u03b4\u2212\u2192a 2) \u03c3\u2032(\u2212\u2192a 1) (19)\nwhere \u2190\u2212w i is a trainable feedback weight matrix. Ignoring the learning rate, the updates for the forward weights are calculated as\n\u03b4\u2212\u2192w1 = \u2212\u03b4\u2212\u2192a 1xT (20)\n\u03b4\u2212\u2192w2 = \u2212\u03b4\u2212\u2192a 2 \u2212\u2192 h T1 (21)\n\u03b4\u2212\u2192w3 = \u2212\u2212\u2192e \u2212\u2192 h T2 (22)\nwhere the error signals are transmitted layer by layer through backward weights. The activations in the feedback pass are then calculated as\n\u2190\u2212a 1 =\u2190\u2212w1y, \u2190\u2212 h 1 = \u03c3( \u2190\u2212a 1) (23)\n\u2190\u2212a 2 =\u2190\u2212w2 \u2190\u2212 h 1, \u2190\u2212 h 2 = \u03c3( \u2190\u2212a 2) (24)\nax = \u2190\u2212w3 \u2190\u2212 h 2, x\u0302 = \u03c3x(ax) (25)\nThe gradients for hidden layers in the feedback pass are\n\u03b4ax = \u2190\u2212e (26)\n\u03b4\u2190\u2212a 2 = \u2202 \u2190\u2212 l\n\u2202\u2190\u2212a 2 = (\u2212\u2192w1\u2190\u2212e ) \u03c3\u2032(\u2190\u2212a 2) (27)\n\u03b4\u2190\u2212a 1 = \u2202 \u2190\u2212 l\n\u2202\u2190\u2212a 1 = (\u2212\u2192w2\u03b4\u2190\u2212a 2) \u03c3\u2032(\u2190\u2212a 1) (28)\nwhere the error signals of backward learning are transmitted through feedforward weights. Ignoring the learning rate, the updates for the feedback weights are calculated as\n\u03b4\u2190\u2212w1 = \u2212\u03b4\u2190\u2212a 1yT , \u03b4\u2190\u2212w2 = \u2212\u03b4\u2190\u2212a 2 \u2190\u2212 h T1 , \u03b4 \u2190\u2212w3 = \u2212\u2190\u2212e \u2190\u2212 h T2 (29)\nThe overall procedure for BFA is shown in Figure 4. The main idea of BFA is that when training one set of weights, the error signals are transmitted layer by layer through the other set of weights. BFA satisfies the principles and restrictions of BioPP. The difference between BFA and target propagation (TP) proposed in [12] is that BFA learns the input features and propagate error signal layer by layer, while each layer in TP learns the output of previous layer with an autoencoder."}, {"heading": "4.2 Bidirectional-DFA", "text": "For BDFA, the loss functions are:\n\u2212\u2192 l = \u2212y \u00b7 logy\u0302 \u2212 (1\u2212 y) \u00b7 log(1\u2212 y\u0302) (30)\n\u2190\u2212 l i = 1\u2212 \u03c3(\u2190\u2212a i \u00b7 \u2212\u2192 h i) (31)\nwhere y\u0302 and \u2190\u2212a are predicted labels and feature maps and \u03c3(x) = 11+e\u2212x . The feedforward and feedback weights are also defined as \u2212\u2192 W and \u2190\u2212 W . The training pipeline includes forward learning phase and backward learning phase, and process them iteratively on\neach training batch. For a DNN with 2 hidden layers, the activations in the forward pass are then calculated as\n\u2212\u2192a 1 = \u2212\u2192w1x, \u2212\u2192 h 1 = \u03c3( \u2212\u2192a 1) (32)\n\u2212\u2192a 2 = \u2212\u2192w2 \u2212\u2192 h 1, \u2212\u2192 h 2 = \u03c3( \u2212\u2192a 2) (33)\nay = \u2212\u2192w3 \u2212\u2192 h 2, y\u0302 = \u03c3y(ay) (34)\nThe gradient at the output layer is calculated as\n\u03b4ay = \u2202 \u2212\u2192 l\n\u2202ay = y\u0302 \u2212 y = \u2212\u2192e (35)\nFor BDFA, the gradients for hidden layers in the forward pass are calculated as\n\u03b4\u2212\u2192a 2 = \u2202 \u2212\u2192 l\n\u2202\u2212\u2192a 2 = (\u2190\u2212w1\u2212\u2192e ) \u03c3\u2032(\u2212\u2192a 2) (36)\n\u03b4\u2212\u2192a 1 = \u2202 \u2212\u2192 l\n\u2202\u2212\u2192a 1 = (\u2190\u2212w2\u2212\u2192e ) \u03c3\u2032(\u2212\u2192a 1) (37)\nwhere \u2190\u2212w i is a trainable feedback weight matrix. The updates for the forward weights are calculated as\n\u03b4\u2212\u2192w1 = \u2212\u03b4\u2212\u2192a 1xT , \u03b4\u2212\u2192w2 = \u2212\u03b4\u2212\u2192a 2 \u2212\u2192 h T1 , \u03b4 \u2212\u2192w3 = \u2212\u2212\u2192e \u2212\u2192 h T2 (38)\nIn the feedback pass, the activations in the feedback pass are then calculated as\n\u2190\u2212a 1 =\u2190\u2212w1y,\u2190\u2212a 2 =\u2190\u2212w2y (39) The losses on hidden layers are\n\u2190\u2212 l 1 = 1\u2212 \u03c3( \u2190\u2212\u0302 a 1 \u00b7 \u2212\u2192 h 1) (40)\n\u2190\u2212 l 2 = 1\u2212 \u03c3( \u2190\u2212\u0302 a 2 \u00b7 \u2212\u2192 h 2) (41)\nFor BDFA, ignoring the learning rate, the updates for the feedback weights are calculated as\n\u03b4\u2190\u2212w1 = \u2212 \u2190\u2212 l 1y T , \u03b4\u2190\u2212w2 = \u2212 \u2190\u2212 l 2y T (42)\nThe overall procedure for BDFA is shown in Figure 5. The main idea of BDFA model is that each hidden layer calculates a loss separately and updates corresponding feedback weight matrix connecting the hidden layer and output layer."}, {"heading": "4.3 Approximating BP Gradients with Adaptive Feedback Weights", "text": "In this section, we provide analysis on why the adaptive feedback weights applied in bidirectional training models are in principle better than fixed feedback weights. We prove that the overall training performance can be improved if the feedback weights can learn the mapping from output features to input features better.\nIn [14] the authors prove that random feedback weights act like the pseudoinverse of feedforward weights in the same layer. Following the proof in [14], we consider a linear network with one hidden layer,\nh = Ax (43)\ny\u0302 = Wh, (44)\nwhere x is input, y\u0302 is output of the network, A and W are feedforward weights. The feedback weight matrix transmitting error signal from output layer to hidden layer is B.\nTheorem 2 in [14] describes that in FA, the pseudogradients \u03b4FAh calculated by the random feedback weights satisfy\n\u03b4FAh = s\u03b4BPh (45)\nwhere s is a positive scalar, and \u03b4BPh is the exact gradients calculated by transpose of feedforward weights. As shown in Equation (75) in [14],\n\u03b4FAh = \u03b7(1\u2212 sy)By (46) and\n\u03b4BPh = \u03b7(1\u2212 sy)W+y (47) where \u03b7 and sy are scalars, B is the random feedback matrix, W+ is the pseudoinverse of feedforward matrix W and y is the target output. Feedback weights are trained by mapping output features to input features. In bidirectional training models, we approximate By to the hidden layer outputs h, that is\nBy \u2192 h. (48) As the model is converging, we have\nh = W+y\u0302 \u2192W+y. (49) Thus,\n\u03b4FAh\u2192 \u03b4BPh (50) We can see that as By converges to h, the gradient calculated with feedback weights will approximate the gradient calculated with transpose of feedforward weights. If backward weights learns the mapping from output features to input features better, then \u03b4FAh and \u03b4BPh will be more similar. This gives one explanation to why the adaptive feedback weights outperform the fixed feedback weights. It is worth noting that if backward weights fail to learn the mapping, it might disturb the training of feedforward weights and the convergence of the network."}, {"heading": "5 Experiments and Discussions", "text": "In this section, we investigate if BFA and BDFA can outperform FA and DFA on benchmark datasets with various hyperparameter settings.\nWe train MLPs on MNIST and CIFAR-10 dataset. The activation functions for hidden layers are Tanh. In order to make the training more stable, the learning rates in all experiments are fixed and set to 0.0001. All the models are trained for 300 epochs. All the results are based on 5 independent runs. The mini-batch size is set to 128. For both MNIST and CIFAR-10 dataset, we use 50,000 samples for training and 10,000 samples for testing.\nThe experimental results on MNIST dataset are summarized in Table 1. We can observe that BFA model performs best on MNIST, and BDFA model outperforms both FA and DFA. Our explanation on the fact that BFA performs better than BDFA is that BFA learns the mapping from output features to input features with a MLP, which has better fitting ability in the backward learning.\nTo demonstrate the ability of BFA to learn input features, we test if the network can generate input images given output features. For MNIST, the output features are 10-dimension one-hot vectors, according to the classification of inputs. For example, the output feature of digit \u201c4\" is [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]. Given output features, the network generates the input features by the backward generating procedure described in Equations (23) to (25).\nGiven the output features of digits \u201c0\" to \u201c9\", the input features of the digits generated by BFA are shown in Figure 6. The generated images indicate that the feedback weights successfully learn the mapping from the output features to the input features.\nThe experimental results on CIFAR-10 dataset are summarized in Table 2. Though BFA still outperforms FA and DFA, BDFA has the best performance among all the asymmetric methods on CIFAR-10. One possible reason is that the images of CIFAR-10 are more complicated than those of MNIST dataset, which makes it difficult for BFA-based network to learn the input features. According to our proof in section 4.3, the performance of feedforward weights will be compromised if the backward learning process fails to map the output features to the input features. The better performance of BDFA in this case mcan be attributed to the fact that it is only required to learn the features in the hidden layers, which is easier compared to the task to learn the raw inputs directly. In other words, those features captured in the hidden layers are more abstract and in a low-dimensional space. However, MLPs are not good at mapping the output features to the raw input images, as the features of which are not abstract enough. BFA model forces the network to fit input images with backward weights, and the numerical stability in training is seriously compromised. DFA faces the similar issue when learning convolutional weights on CIFAR-10 [17], which tried to learn transmitting error signal from output layer directly to convolutional layers. In future work, we plan on proposing more stable models that can convey the backward teaching signals to the layers which can produce more complicated outputs.\nTo better learn the backward features on CIFAR-10, we slightly modify the backward training in BDFA. The output features used to train the feedback weights on CIFAR-10 dataset are now calculated as:\ny\u2032 = y + \u03b1y\u0302 (51)\nwhere y\u2032 is the output features we actually used in training BDFA model on CIFAR-10 dataset, y is the target output features, y\u0302 is output of the network calculated with current parameters, and \u03b1 is a small positive scalar. In our experiments, \u03b1 is set to 0.25. We apply this modification because the current outputs contain certain amount of random features of input images. Given these random features, the negative effect of randomness of the input features is mitigated and thus it is easier for backward weights to learn the mapping from output features to hidden layer outputs and input features.\nBFA and BDFA demonstrate novel applications of the adaptive asymmetric gradient-based methods for optimizing DNNs. Especially in BDFA, the learning of the feedback weights and the learning of the feed forward weights are disconnected in the sense that the feedback weights are\nunaware of the existence of the feed forward weights. This learning process for feedback updating of BDFA is also consistent with the insight [11, 18, 9] that errors can result from mismatches between the actual and the expected perceptions, rather than coming from external teaching signals."}, {"heading": "6 Conclusion", "text": "In this work, we proposed biologically plausible perceptron paradigm based on related literature in neuroscience. We also designed and evaluated Bidirection-FA and Bidirectional-DFA models on benchmark datasets. To the best of our knowledge, this is the first research attempt to show that adaptive asymmetric feedback channels are more effective than random and fixed feedback channels in DNNs. Although it is not clear if the brain implements this particular form of adaptive feedback, it is a step towards better understanding how the brain supports learning from error signals."}], "references": [{"title": "Automatic differentiation in machine learning: a survey", "author": ["A.G. Baydin", "B.A. Pearlmutter", "A.A. Radul"], "venue": "arXiv preprint arXiv:1502.05767,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards biologically plausible deep learning", "author": ["Y. Bengio", "D.-H. Lee", "J. Bornschein", "Z. Lin"], "venue": "arXiv preprint arXiv:1502.04156,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards biologically plausible deep learning", "author": ["Y. Bengio", "D.-H. Lee", "J. Bornschein", "T. Mesnard", "Z. Lin"], "venue": "arXiv preprint arXiv:1502.04156,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "A synaptic model of memory: long-term potentiation in the hippocampus", "author": ["T.V. Bliss", "G.L. Collingridge"], "venue": "Nature, 361(6407):31,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1993}, {"title": "The time course of glutamate in the synaptic cleft", "author": ["J.D. Clements", "R.A. Lester", "G. Tong", "C.E. Jahr", "G.L. Westbrook"], "venue": "SCIENCE-NEW YORK THEN WASHINGTON-, 258:1498\u2013 1498,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "Theoretical neuroscience, volume 806", "author": ["P. Dayan", "L.F. Abbott"], "venue": "Cambridge, MA: MIT Press,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P.-A. Manzagol", "P. Vincent", "S. Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "A textbook of physiology", "author": ["M. Foster"], "venue": "part iii,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1897}, {"title": "The free-energy principle: a unified brain theory", "author": ["K. Friston"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "The ups and downs of hebb synapses", "author": ["G. Hinton"], "venue": "Canadian Psychology/Psychologie canadienne, 44(1):10,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Difference target propagation", "author": ["D.-H. Lee", "S. Zhang", "A. Fischer", "Y. Bengio"], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 498\u2013515. Springer,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "The neuron: cell and molecular biology", "author": ["I.B. Levitan", "L.K. Kaczmarek"], "venue": "Oxford University Press, USA,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Random feedback weights support learning in deep neural networks", "author": ["T.P. Lillicrap", "D. Cownden", "D.B. Tweed", "C.J. Akerman"], "venue": "arXiv preprint arXiv:1411.0247,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Random synaptic feedback weights support error backpropagation for deep learning", "author": ["T.P. Lillicrap", "D. Cownden", "D.B. Tweed", "C.J. Akerman"], "venue": "Nature Communications, 7,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Long-term potentiation and memory", "author": ["M. Lynch"], "venue": "Physiological reviews, 84(1):87\u2013136,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Direct feedback alignment provides learning in deep neural networks", "author": ["A. N\u00f8kland"], "venue": "arXiv preprint arXiv:1609.01596,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Biologically plausible error-driven learning using local activation differences: The generalized recirculation algorithm", "author": ["R.C. O\u2019Reilly"], "venue": "Neural computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1996}, {"title": "Learning representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling, 5(3):1,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1988}, {"title": "Training language models using target-propagation", "author": ["S. Wiseman", "S. Chopra", "M. Ranzato", "A. Szlam", "R. Sun", "S. Chintala", "N. Vasilache"], "venue": "arXiv preprint arXiv:1702.04770,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Back-propagation (BP) algorithm is the combination of reverse-mode automatic differentiation [1] and steepest descent [10] which has been considered the de-facto method for training deep neural networks (DNNs).", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "However, it has been argued that it is not biologically possible for learning in the brain to involve precise, symmetric backward channels [6, 15, 3, 12].", "startOffset": 139, "endOffset": 153}, {"referenceID": 13, "context": "However, it has been argued that it is not biologically possible for learning in the brain to involve precise, symmetric backward channels [6, 15, 3, 12].", "startOffset": 139, "endOffset": 153}, {"referenceID": 2, "context": "However, it has been argued that it is not biologically possible for learning in the brain to involve precise, symmetric backward channels [6, 15, 3, 12].", "startOffset": 139, "endOffset": 153}, {"referenceID": 10, "context": "However, it has been argued that it is not biologically possible for learning in the brain to involve precise, symmetric backward channels [6, 15, 3, 12].", "startOffset": 139, "endOffset": 153}, {"referenceID": 6, "context": "In the early days of deep learning, unsupervised pre-training with Boltzmann machines used to be applied before fine-tuning with BP [7], which does not involve symmetric weights and is biologically motivated.", "startOffset": 132, "endOffset": 135}, {"referenceID": 18, "context": "In [21], target-propagation (TP)[12], whose objective is to let each layer to reproduce outputs the previous layer, is used to train a recurrent neural network for natural language processing tasks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "In [21], target-propagation (TP)[12], whose objective is to let each layer to reproduce outputs the previous layer, is used to train a recurrent neural network for natural language processing tasks.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "The authors in [15] propose feedback-alignment (FA) model and showed that for BP-like methods, the weights used in the feedback pass do not have to be the transpose of feedforward weights.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "The direct feedback-alignment (DFA) model proposed in [17] suggest that error signals could be transmitted directly from output layer to any hidden layer with random and fixed matrices.", "startOffset": 54, "endOffset": 58}, {"referenceID": 14, "context": "On the other hand, due to the literature in neuroscience, long-term potentiation (LTP) is considered an essential step in human memory and learning [16, 4].", "startOffset": 148, "endOffset": 155}, {"referenceID": 3, "context": "On the other hand, due to the literature in neuroscience, long-term potentiation (LTP) is considered an essential step in human memory and learning [16, 4].", "startOffset": 148, "endOffset": 155}, {"referenceID": 1, "context": "Based on the principles of LTP and the hypothesis that the feedback weights are plastic [2], we propose a more biological plausible perceptron paradigm and two bidirectional learning models.", "startOffset": 88, "endOffset": 91}, {"referenceID": 15, "context": "(Modified from [17]).", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "2 Background Following the notation in [14], let (x, y) be a mini-batch of input-output of size 1.", "startOffset": 39, "endOffset": 43}, {"referenceID": 2, "context": "Although FA and DFA with random and fixed feedback weights are more biologically plausible than BP, the feedback weights in the brain are plastic too [3].", "startOffset": 150, "endOffset": 153}, {"referenceID": 13, "context": "It has been shown in [15] that the forward weights wi used in FA learns to resemble the pseudo-inverse of the feedback random weights Bi.", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "3 Biologically Plausible Perceptron Classical perceptrons trained with gradient descent algorithms need to back-propagate error signals based on the exact feedforward synaptic weights, which is considered impossible in a biological neural system [19].", "startOffset": 246, "endOffset": 250}, {"referenceID": 14, "context": "On the other hand, long-term potentiation (LTP) is considered an essential part of biological memory and learning in cognitive science [16, 4].", "startOffset": 135, "endOffset": 142}, {"referenceID": 3, "context": "On the other hand, long-term potentiation (LTP) is considered an essential part of biological memory and learning in cognitive science [16, 4].", "startOffset": 135, "endOffset": 142}, {"referenceID": 7, "context": "1 Long-term Potentiation Biological neurons are connected by synapse, including axons and dendrites, where axons emit signals, and dendrites of the next neuron receive the electrical impulses released by axons [8].", "startOffset": 210, "endOffset": 213}, {"referenceID": 4, "context": "However, axons and dendrites are separated by synaptic clefts, and the axons send electrical impulses by releasing ions into synaptic cleft [5].", "startOffset": 140, "endOffset": 143}, {"referenceID": 3, "context": "The ions are captured by receptors on the cell membrane of dendrites [4, 13].", "startOffset": 69, "endOffset": 76}, {"referenceID": 11, "context": "The ions are captured by receptors on the cell membrane of dendrites [4, 13].", "startOffset": 69, "endOffset": 76}, {"referenceID": 3, "context": "When a synapse transmits neural signals from neuron N1 to neuron N2 and is repeatedly simulated, neuron N2 will release more receptors on its dendrites and thus capture more ions [4].", "startOffset": 179, "endOffset": 182}, {"referenceID": 3, "context": "This procedure reduces the ion concentration of the synaptic cleft between N1 and N2, which encourages N1 to release more ions [4].", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "Thus, a stronger connection between neuron N1 and N2 is established due to the LTP procedure [4].", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "LTP adjusts links among neurons and plays a significant role in forming memory and learning [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 15, "context": "(Modified from [17]).", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "The difference between BFA and target propagation (TP) proposed in [12] is that BFA learns the input features and propagate error signal layer by layer, while each layer in TP learns the output of previous layer with an autoencoder.", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "(Modified from [17]).", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "In [14] the authors prove that random feedback weights act like the pseudoinverse of feedforward weights in the same layer.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "Following the proof in [14], we consider a linear network with one hidden layer,", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "Theorem 2 in [14] describes that in FA, the pseudogradients \u03b4FAh calculated by the random feedback weights satisfy", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "As shown in Equation (75) in [14],", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "For example, the output feature of digit \u201c4\" is [0, 0, 0, 0, 1, 0, 0, 0, 0, 0].", "startOffset": 48, "endOffset": 78}, {"referenceID": 15, "context": "DFA faces the similar issue when learning convolutional weights on CIFAR-10 [17], which tried to learn transmitting error signal from output layer directly to convolutional layers.", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "This learning process for feedback updating of BDFA is also consistent with the insight [11, 18, 9] that errors can result from mismatches between the actual and the expected perceptions, rather than coming from external teaching signals.", "startOffset": 88, "endOffset": 99}, {"referenceID": 16, "context": "This learning process for feedback updating of BDFA is also consistent with the insight [11, 18, 9] that errors can result from mismatches between the actual and the expected perceptions, rather than coming from external teaching signals.", "startOffset": 88, "endOffset": 99}, {"referenceID": 8, "context": "This learning process for feedback updating of BDFA is also consistent with the insight [11, 18, 9] that errors can result from mismatches between the actual and the expected perceptions, rather than coming from external teaching signals.", "startOffset": 88, "endOffset": 99}], "year": 2017, "abstractText": "The back-propagation (BP) algorithm has been considered the de-facto method for training deep neural networks. It back-propagates errors from the output layer to the hidden layers in an exact manner using the transpose of the feedforward weights. However, it has been argued that this is not biologically plausible because back-propagating error signals with the exact incoming weights is not considered possible in biological neural systems. In this work, we propose a biologically plausible paradigm of neural architecture based on related literature in neuroscience and asymmetric BP-like methods. Specifically, we propose two bidirectional learning algorithms with trainable feedforward and feedback weights. The feedforward weights are used to relay activations from the inputs to target outputs. The feedback weights pass the error signals from the output layer to the hidden layers. Different from other asymmetric BP-like methods, the feedback weights are also plastic in our framework and are trained to approximate the forward activations. Preliminary results show that our models outperform other asymmetric BP-like methods on the MNIST and the CIFAR-10 datasets. The source code of this paper can be obtained from https://github.com/SkTim/bdfa-torch.", "creator": "LaTeX with hyperref package"}}}