{"id": "1703.00472", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2017", "title": "Reinforcement Learning for Pivoting Task", "abstract": "in this modeling we propose an approach to learn as robust policy for solving the pivoting problems. increasingly, empirical model - free meta selection theories were applied to assess successful behaviour confirming prior knowledge with the dynamics prior a algorithm. however, some successful outputs generates thousands successive millions through training episodes, restricting the applicability of these approaches to real requirements. it developed a training procedure concept allows users of use a simple custom mapping to recognize policies robust to the complexity of victim vs robot. in our perspective, we predict that the policy formulation in the simulator is liable to transform the object to approximately desired target angle than the real robot. analysts also show simulations like an object yielding younger inertia, shape, mass and friction intensity than those attained during training. i effectively marked a step towards achieving strategy - free reinforcement hardware available at solving multiple tasks despite pre - training in simulators for offer only an imprecise rendering representing the real - world dynamics.", "histories": [["v1", "Wed, 1 Mar 2017 19:25:55 GMT  (6528kb,D)", "http://arxiv.org/abs/1703.00472v1", "(Rika Antonova and Silvia Cruciani contributed equally)"]], "COMMENTS": "(Rika Antonova and Silvia Cruciani contributed equally)", "reviews": [], "SUBJECTS": "cs.RO cs.LG", "authors": ["rika antonova", "silvia cruciani", "christian smith", "danica kragic"], "accepted": false, "id": "1703.00472"}, "pdf": {"name": "1703.00472.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning for Pivoting Task", "authors": ["Rika Antonova", "Silvia Cruciani", "Christian Smith", "Danica Kragic"], "emails": ["dani}@kth.se"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nIn this work we address the problem of pivoting an object held by a robotic gripper. Pivoting consists of rotating an object or a tool between two fingers to reorient it to a desired angle. This problem falls in the general class of extrinsic dexterity problems formulated by [1]. Since many tasks in robotics require interaction with objects and tool use, the ability of placing items in the correct pose with respect to the gripper is important.\nInstead of relying on releasing the object and picking it up again [2], we focus on in-hand manipulation. Successful strategies for this dexterous task often rely on multi-fingered robotic hands or customized grippers [3]\u2013[5]. However, currently many robots have only parallel grippers, e.g. Baxter, PR2, Yumi. Thus it is important to develop regrasping strategies that do not rely on the additional degrees of freedom provided by complex hands.\nWe formalize the pivoting task as a reinforcement learning problem. The goal is to discover policies that, given the current state of the gripper and the tool, produce a sequence of actions to pivot the tool to the desired target angle. The actions consist of commanding the acceleration of the gripper and the distance for gripper\u2019s fingers. It is important to note that learning directly on the robot could focus too narrowly on the specific tool and robotic manipulator, while attempts to generalize would likely make the learning infeasible in terms of time. Hence it is not straightforward to directly apply reinforcement learning to this problem, if one aims\nRika Antonova, Silvia Cruciani, Christian Smith and Danica Kragic are with the Robotics, Perception and Learning Lab, CSC at KTH Royal Institute of Technology, Stockholm, Sweden. {antonova, cruciani, ccs, dani}@kth.se\n1 Both of these authors contributed equally.\nto obtain policies that generalize beyond the exact hardware setup and tool properties.\nOur contribution is an approach to learn robust policies in a custom simulator, while taking into account the mismatch between the simulation and the real robot. We begin by constructing a simulator using dynamics equations for the setting. These equations describe well the general behavior of the system, but contain parameters that are infeasible to estimate precisely, like friction properties of the objects. Hence we generate a variety of training episodes with parameters randomly chosen from a range of values. We also simulate delays and errors in actuation. This results in learning policies robust to the mismatch between the outcome of control actions in simulation vs on a real robot.\nWe designed this approach of training with a custom simulator so that we are not limited to only sample-efficient learning algorithms. While sample-efficient learning in real time can be effective, it could limit the flexibility and applicability of the learned policies to slightly altered hardware setups. In contrast, we can apply recent model-free policy search algorithms, even those not widely used for real robots previously. This facilitates learning flexible non-linear control policies, including those represented by deep neural networks.\nWe demonstrate that our approach is able to learn policies to successfully solve the pivoting task when used on the robot. We present experiments with controlling the tool whose observable physical properties, like mass and inertia, are used when constructing the simulator. We also demonstrate that the same policy is able to successfully control an object whose properties differ from those used during training. Fig. 1 above shows one of our experiments with parallel gripper executing a pivoting task.\nar X\niv :1\n70 3.\n00 47\n2v 1\n[ cs\n.R O\n] 1\nM ar\n2 01\n7"}, {"heading": "II. RELATED WORK", "text": "Extrinsic dexterity has been widely studied in robotics and is still an open challenge. Pivoting is one type of extrinsic dexterity problem that recently attracted attention of the robotics research community. In the following section we provide and overview of the previous work. Since in contrast to prior approaches ours employs deep reinforcement learning, we also provide a brief overview of this learning approach in the context of robotics."}, {"heading": "A. Previous Work in Pivoting", "text": "Existing solutions for pivoting exploit environmental constraints, motions of the robot arm to generate inertial forces, and external forces, such as gravity. In [6] the authors exploit gravity to rotate an object between two stable poses by using a contact surface. The pivoting is performed open loop and there is no control of the gripping force. Conversely, several other works on pivoting strongly focus on controlling the torque applied by the gripper on the object: in [7] the authors focus on swing-up motions. They address the problem using an energy-based control and they consider the ability of the gripper to exert dissipative torques on the object thanks to the friction at the pivoting point. In this case the motion of the object appears to be limited to a vertical plane and the approach strongly depends on fast sensory feedback and rapid response time of the gripper. The adaptive control for pivoting presented in [8] exploits gravity and controlled slip with only visual feedback. This approach has then been extended to consider also tactile feedback in [9]. However, the gripper is assumed to be in a fixed position, therefore the motion of the tool is determined only by the gravitational torque and the torsional friction. This motion is limited to be in a vertical plane and the proposed strategy can successfully reorient the object only when the desired configuration has less potential energy than the initial one.\nAll these prior approaches rely strongly on having an accurate model of the object the gripper is holding, as well as precise measurement and modeling of the friction. Since it is difficult to obtain accurate estimates of these quantities, especially when they are related to friction modeling, in our work we do not rely strongly on highly accurate parameters for the successful outcome of a pivoting action."}, {"heading": "B. Deep Reinforcement Learning in Robotics", "text": "Reinforcement Learning (RL) algorithms can be classified into two broad categories: model-based and model-free. Model-based algorithms can make learning data-efficient by assuming the dynamics of the task can be captured by a particular model (frequently parametric), then estimating the parameters of the model from samples obtained when learning to solve a task. However, strict global assumptions on the model class can limit the flexibility of the model-based representation. While this can be partially resolved by learning local models (e.g. [10], [11]), such approaches might limit ability to incorporate prior knowledge \u2013 for example, forgoing the knowledge that can be easily incorporated into dynamics equations in a simulator.\nModel-free RL algorithms instead allow to learn flexible control policies by directly interacting with either simulated or real environment. Recently, several model-free continuous state continuous action reinforcement learning algorithms have been proposed. Among these, two algorithms were reported to perform well on several simulated robotic tasks: Trust Region Policy Optimization (TRPO) [12] and Deep Deterministic Policy Gradient (DDPG) [13]. Both use neural networks as policy and Q function approximators. While it has been shown that these approaches can handle problems similar in principle to those considered in robotics, significant practical problems arise when applying modelfree policy search algorithms to real-world robotics tasks.\nFirst of all, when using deep neural networks as function approximators, the question of data efficiency becomes key: the number of training episodes needed might be prohibitively large. For example, a recent benchmarking paper used up to 25 million steps when training [14]. The paper that introduced DDPG used up to 2 million steps, sometimes without achieving close to maximum reward (which in some cases means not being able to solve the task). Networks reported as successful had from 2 to 3 hidden layers with 25 to 400 nodes in each (15K to 200K parameters to learn, depending on the particular network structure). This was the case even for low-dimensional continuous problems, where state space is represented by joint angles, torques, velocities (as opposed to pixels) and the task of sensing the state is considered separate \u2013 e.g. assumed to be performed by an off-the-shelf tracking system or achieved via custom hardware sensors.\nSecondly, extensive hyper-parameter search might be needed for ensuring learning success on a given problem. In addition to the usual RL considerations, e.g. how aggressively to explore or how to shape the reward function, a new challenge is selecting the appropriate neural network architecture.\nOne additional challenge arises from the phenomenon known in deep learning literature as \u201cforgetting\u201d. It has been observed that when training a neural network on a new task, the capacity to perform old tasks well can diminish. For the pivoting problem considered in this work, \u201cforgetting\u201d is problematic for several reasons. Reaching different target angles could be seen as slightly different tasks, since the optimal policies might differ. Theoretically, this is resolved by adding the target angle to the representation of the state. Practically, the knowledge acquired when training to reach different target angles is embedded in the same network, and this could impede or stall the learning progress during training. This presents a challenge for real-time learning, hence giving the motivation for our approach to learn a robust policy in simulation, where training on a large number of episodes could allow to recover from slow or inconsistent learning progress. The problem of \u201cforgetting\u201d in the context of learning in robotics is discussed at length in [15] \u2013 the work that attempted to resolve the issue by pre-training in simulation, then using Progressive Neural Network architecture to continue learning on the real hardware. This or a\nsimilar approach could also be applicable to the pivoting task setting, however it might still require a significant amount of time for the second stage of training on the robot. Hence, we are motivated to develop an approach that can learn acceptable policies directly from the simulator, and would be applicable even when second stage training on the hardware is costly or infeasible."}, {"heading": "III. PROBLEM DESCRIPTION", "text": "The problem we address is pivoting the tool to a desired angle while holding it in the gripper. This can be accomplished by moving the arm of the robot to generate inertial forces sufficient to move the tool and, at the same time, opening or closing the fingers of the gripper to change the friction at the pivoting point, gaining more precise control of the motion. We assume that the robot is able to use one of the standard planning approaches to initially grasp the tool, but the position of the tool between the fingers is initially at some random angle \u03c6init. The goal is to pivot the tool to a desired target angle \u03c6tgt.\nThe first challenge is that the motion of the tool is influenced by the friction at the pivoting point. This is dictated by the materials of the tool and fingers, the deformation of the tool and fingers, and potentially the air flow. It is difficult to estimate precisely all the necessary coefficients to construct a high-fidelity friction model [16].\nThe second challenge is due to limitations of the robot hardware: delays in actuation, possible errors in execution, joint and velocity limits and constraints maintained to ensure safety. These cause the overall dynamics of the system to be uncertain. Combined with difficulties of estimating the object properties, this causes high uncertainty of the outcomes of the commanded actions overall.\nThe above challenges could be addressed by using a vision system with high frame rate to estimate the current state and high frequency control of the robot arm to ensure rapid response and re-adjustment. However, currently available commercial robots often have limited control frequency for adjusting gripper\u2019s fingers. Moreover, it is preferable to be able to solve the pivoting task with readily available vision systems, which have limited frame rate.\nWe take into consideration all of the challenges mentioned. Hence we propose a learning approach that lets us obtain control policies robust to some degree of uncertainty of both the tool properties and imprecisions of robot motion execution."}, {"heading": "IV. MODELING FOR SIMULATION", "text": "As mentioned in the previous section, simulators that can effectively incorporate global information about the task dynamics can be utilized for learning flexible control policies. In this section we describe the dynamic model of the system and the friction model that we used to simulate the pivoting task."}, {"heading": "A. Dynamic Model", "text": "Our system is composed of a 1 DOF parallel gripper attached to a link that can rotate around a single axis. This system is an under-actuated two-link planar arm, in which the under-actuated joint corresponds to the pivoting point. We assume that we can control the desired acceleration on the first joint. This system is shown in Fig. 2 above.\nThe dynamic model of the system is given by:\n(I +mr2 +mlr cos(\u03c6tl))\u03c6\u0308grp + (I +mr 2)\u03c6\u0308tl + ...\n...+mlr sin(\u03c6tl)\u03c6\u0307 2 grp +mgr cos(\u03c6grp + \u03c6tl) = \u03c4f ,\n(1)\nwhere the variables are as follows: \u03c6grp and \u03c6tl are the angles of the first and second link respectively; \u03c6\u0308grp and \u03c6\u0308tl are their angular acceleration and \u03c6\u0307grp is the angular velocity of the first link; l is the length of the first link; I is the tool\u2019s moment of inertia with respect to its center of mass; m is the tool\u2019s mass; r is the distance of its center of mass from the pivoting point; g is the gravity acceleration; \u03c4f is the torsional friction at the contact point between the gripper\u2019s fingers and the tool. In our case, the second link represents the tool and \u03c6tl is the variable we aim to control\nOn a real setup, the modeled two-link arm is the final part of a robotic manipulator. Therefore, the gravity component g varies according to the current orientation of the plane that contains the actuated link and the tool. We assume that the manipulator\u2019s configuration is such that this plane has only pitch angle and no roll. As a result, the acceleration due to gravity influences only one direction of motion and g varies between 0 and 9.8 according to the pitch angle, without the need of an additional term in Equation 1."}, {"heading": "B. Friction Model", "text": "Our proposed solution for pivoting exploits the friction at the contact point between the gripper and the tool to control the rotational motion. Such friction is controlled by enlarging or tightening the grasp.\nWhen the tool is not moving, that is \u03c6\u0307tl = 0, the static friction \u03c4s is modeled according to the Coulomb friction model:\n|\u03c4s| \u2264 \u03b3fn, (2)\nwhere \u03b3 is the coefficient of static friction and fn is the normal force applied by the gripper\u2019s fingers on the tool.\nWhen the tool moves with respect to the gripper, that is \u03c6\u0307tl 6= 0, we model the friction torque \u03c4f as viscous friction and Coulomb friction [17]:\n\u03c4f = \u2212\u00b5v\u03c6\u0307tl \u2212 \u00b5cfn sgn(\u03c6\u0307tl), (3)\nin which \u00b5v and \u00b5c are the viscous and Coulomb friction coefficients respectively and sgn(\u00b7) is the signum function.\nWhen the tool starts moving numerical singularities can occur due to switching between the two models in Equations 2 and 3. To alleviate this problem we follow the approach proposed in [18]. We define a neighborhood |\u03c6\u0307tl| \u2264 , for a small > 0, where the friction torque is equal to the net torque acting on the tool. Therefore, when the tool has zero velocity, the normal force will counterbalance the net torque.\nSince most of the robots are not equipped with tactile sensors to measure the normal force fn at the contact point, we follow the approach proposed in [8] and express this force as a function of the distance dfing between the two fingers, assuming a linear deformation model:\nfn = k(d0 \u2212 dfing), (4)\nwhere k is a stiffness parameter and d0 is the distance at which there is no fingertip deformation. In other words, d0 is the distance at which the fingers initiate the contact with the tool."}, {"heading": "V. LEARNING", "text": "As discussed in Section II-B, recent algorithmic advances in reinforcement learning suggest the possibility that modelfree algorithms could be applied to robotics tasks. However, since these algorithms frequently are not designed to be sample-efficient enough to learn in real time on the hardware, our approach is to construct a training procedure to learn robust policies in a simple custom simulated environment, then deploy on the robot.\nBelow we first describe the overall training approach, then present formalization of the pivoting task as a Markov Decision Process, then give a brief summary of the modelfree reinforcement learning algorithm we used for policy search."}, {"heading": "A. Learning Robust Policies using a Simulator", "text": "As described in Section III, our approach is to enable learning from simulated environment, while being robust to the discrepancies between the simulation and actual execution on the robot. For this purpose, we first built a simple custom simulator using the equations described in Section IV. Then, to facilitate learning policies robust to uncertainty, we injected up to 10% randomized delay for arm and finger actions in simulation. We also added 10% noise to friction values estimated for the tool modeled by the simulator. We then trained a model-free deep reinforcement learning policy search algorithm on our simulated setting. Lastly, we executed the resulting policy on the robot (Baxter) for evaluation. This approach allowed us to keep the simulator simple and fast, while still enabling learning policies robust to the mismatch of the simulated and real environments."}, {"heading": "B. Pivoting Task as a Markov Decision Process", "text": "We formulate the pivoting task as a Markov Decision Process (MDP) \u2013 a tuple {S, A, P (s\u2032|s, a), R(s, a, s\u2032), \u03b1}. The state space S is comprised of states st observed at each time step t: st= [\u03c6tl\u2212\u03c6tgt , \u03c6\u0307tl , \u03c6grp , \u03c6\u0307grp , dfing], with notation as in Section IV-A. Using the signed distance of the tool angle as the first component of the state vector allows us to facilitate generalization in learning, since in our setting the motions of the robot arm are symmetric, e.g. the optimal policy for reaching target angle 0\u25e6 starting from tool at \u221245\u25e6 would be symmetric to the case of starting from 45\u25e6. For the settings without the symmetry the state space representation could instead include \u03c6tl and \u03c6tgt separately.\nThe action space A is comprised of actions at at time t: at = {\u03c6\u0308grp, dfing}, where \u03c6\u0308grp is the rotational acceleration of the robot arm, and dfing is the distance between the fingers of the gripper. In cases where the hardware is limited in ability to achieve fingers\u2019 distances precisely, it is advantageous to learn to control the direction of the change in distance instead of precise target distance. We discuss this further in the experiments section.\nThe transition probabilities P (s\u2032|s, a) are not used during learning explicitly, since we employ model-free approaches for learning. The dynamics of the state transitions is implemented by the simulator (as described in Section IV-A), but the learner does not have an explicit access to the state transition dynamics.\nThe reward function R gives a reward rt \u2208 [\u22121, 1] at each time step t such that higher rewards are given when the angle of the tool is closer to the target angle: r(t) = \u2212|\u03c6tl\u2212\u03c6tgt|\u03c6RNG , where \u03c6RNG is a normalizing constant denoting the range of angles the tool can attain, e.g. close to 2\u03c0 if the motion is not further restricted by the shape of the tool or the gripper. A bonus of 1 is given when the goal region is reached \u2013 the tool is close to the target angle and the velocity of the tool with respect to the gripper is not changing, i.e. the tool is gripped firmly.\nFinally, to obtain infinite horizon discounted MDP, since we aim to achieve the goal within 100 steps on average, we use the discount factor of \u03b1 = 0.99 (from 11\u2212\u03b1 =100). Of course the horizon of 100 can be changed as needed depending on the desired duration of the pivoting task."}, {"heading": "C. Policy Search using Reinforcement Learning", "text": "As discussed in Section II-B, several continuous control model-free reinforcement learning algorithms would be suitable for learning optimal policies for our formulation of the pivoting task as MDP. In our experiments, we found that Trust Region Policy Optimization [12] was able to solve the problem without extensive parameter adjustment. Below we summarize the main ideas behind TRPO for a brief overview.\nTRPO is a method for optimizing large non-linear control policies, as those represented by neural networks. The algorithm aims to ensure monotonic improvement during training by computing a safe region for exploration.\nThe main optimization performed by the algorithm is to iteratively solve a set of optimization problems:\nmaximize\u03b8 Es\u223c\u03c1\u03b8old , a\u223cq [ \u03c0\u03b8(a|s) q(a|s) Q\u03b8old(s, a) ] subject to Es\u223c\u03c1\u03b8old [ DKL(\u03c0\u03b8old(\u00b7|s)||\u03c0\u03b8(\u00b7|s)) ] \u2264 \u03b4, (5)\nwhere \u03b8old denotes the initial (or previous) set of policy parameters, \u03b8 denotes the updated policy parameters, \u03c0\u03b8 is the stochastic policy parameterized by \u03b8, q is a sampling distribution for exploration, Q\u03b8old is the Q function approximator estimated from previous samples, and the expectation E is taken over samples obtained using the policy from previous iteration (see [12] for details). The constraint in the optimization aims to keep the new policy sufficiently close to the old to yield (in theory) monotonically improving policies by limiting KL divergence of \u03c0\u03b8(\u00b7|s) from \u03c0\u03b8old(\u00b7|s). Briefly, the overall structure of the algorithm is: 1) collect a set of state-action pairs along with Monte Carlo estimates of their Q-values; 2) by averaging over samples, construct the estimated objective and constraint from Equation 5; 3) approximately solve this constrained optimization problem to update policy parameter vector \u03b8 (using conjugate gradient followed by a line search).\nTRPO has been shown to be competitive with (and sometimes outperform) other recent continuous state and action RL algorithms [14]. However, to our knowledge it has not yet been widely applied to real-world robotics tasks. While the background for the algorithm is well-motivated theoretically, the approximations made for practicality, along with challenges in achieving reasonable training results with a smallto-medium number of samples, could impair the applicability of the algorithm to learning on the robot directly. Hence we explore the approach of using TRPO for policy search in a simulated environment."}, {"heading": "VI. EXPERIMENTS", "text": "In this section we first discuss implementation details of training in the simulator. We then show initial evaluation results indicating potential for robustness to changes in friction. Then we describe experiments run on Baxter robot. We discuss the performance on the tool with parameters similar to the ones used during training, as well as results for manipulating the tool with different shape, mass, inertia parameters and unknown friction properties. The results indicate that our training procedure is able to produce robust policies capable of solving the pivoting task on both known and unknown tool."}, {"heading": "A. Learning Robust Policies in Simulation", "text": "To facilitate experimenting with various RL algorithms, we implemented a custom environment in OpenAI Gym [19] for the pivoting task MDP (as defined in Section V-B). To experiment with TRPO [12] and DDPG [13] algorithms, we used rllab implementation [14] as a starting point, then adjusted the behavior and parameters as needed for various experiments for this project. Both TRPO and DDPG papers\ndocumented exploration and network training parameters used to obtain results for simulated control tasks. Starting with these reported values, we experimented with several options like rate of exploration, batch size for neural network training and network size. We were not able to obtain robust learning with DDPG (training progress would frequently stall), so for the rest of the experiments we decided to only use TRPO, which in our setting exibited a gradual but satisfactory learning progress.\nWe trained TRPO with a fully connected network with 2 hidden layers (with 32, 16 nodes) for policy and Q function approximators. We also experimented with larger networks of up to 3 hidden layers, though found that the smaller network was enough to solve the task. When generating training episodes initial and target angles were chosen at random from [\u2212\u03c0/2, \u03c0/2]; each training episode had 100 steps. The motion was constrained to be linear in a horizontal plane. However, since the learning was not at all informed of the physics of the task, any other plane could be chosen and implemented by the simulator if needed. Fig. 3 below visualizes evaluation of the policies as the number of training iterations increases.\nTo simulate the dynamics of the gripper arm and the tool we used the modeling approach described in Section IV. We used the parameters of tool 1 as specified in Table I with friction coefficients from Equation 3 set to \u00b5v = 0.066, k\u00b5c = 9.906.\nAs discussed in Section V-A, the noise injected when training in simulation aimed to help learning policies robust to mismatch between the simulator and the robot. Fig. 4 below visualizes using the policy obtained after 20K training iterations on simulated settings with a mismatch larger than noise used during training. While we allowed only up to 10% noise in the value k\u00b5c which modeled Coulomb friction, we observed that the policy was still able to retain 40% success\nrate of reaching the target angle when tested on settings with 250% to 500% change in k\u00b5c. This level of robustness suggests we could to avoid estimating friction coefficients precisely. This is crucial for the pivoting task, since, unlike estimating the dimensions of the tools, estimating the friction properties precisely would be an intractable challenge for most widely available robotic systems."}, {"heading": "B. Experiments on Hardware with Baxter", "text": "We implemented the proposed approach on a Baxter robot, using one of its 7 DOF arms. We selected slightly deformable fingertips for the gripper to be able to control the force applied on the tool by changing the distance between the fingers. As for many commercial robots, Baxter\u2019s gripper cannot be controlled at the same high frequency as the joints. In particular, we measured that the gripper\u2019s fingers were able to reach a desired distance only after allowing a delay of 80ms. This delay entails longer time needed for executing each control action on the hardware, hence longer times needed by the tool to reach the goal due to these limitations of the robot.\nTo estimate the angle \u03c6tl we used a color based segmentation to track a colored marker on the tool. The images were collected by a Kinect 2 RGB-D camera running at 30 fps.\nDuring the experiments, we kept the gripper and the actuated joint in a horizontal position, such that the gravitational acceleration would be g = 0 and the motion of the tool was only determined by the inertial forces generated by the commanded actions. The distance between the actuated joint and the pivoting point was l = 0.35 m.\nWe performed experiments with two different objects of different materials, hence different friction properties. The parameters of the tools are shown in Table I.\nThe robot is able to estimate the difference in the finger\u2019s distance when closing them to grasp the tool and therefore it can estimate the difference in the d0 value also for an unknown object. The friction coefficients used for modeling the tool in the simulator have been estimated only for the first tool. Hence the policy was not explicitly trained using the parameters matching those of the second tool.\nTable II summarizes the results of our experiments on the robot. To streamline the experiments, we cycled through the following target angles: starting from 0\u25e6, reach 45\u25e6, then 0\u25e6, then \u221260\u25e6, then 30\u25e6, then 5\u25e6 and finally 0\u25e6. This allowed us to test the policy on both wide and narrow ranges of motion. With this we obtained a \u224893% success rate with tool 1 and \u2248 83% with tool 2. As expected, the policy performs better with the tool whose parameters were used (in a noisy manner) during training in simulation. However, it is important to note that the performance using the tool not seen during training was robust as well. The experiments on the robot were performed without re-adjusting the tool. As a consequence, we observed that eventual sliding of the tool would cause drops after several rounds of experiments. However, these were still very infrequent.\nFig. 5 below illustrates two example episodes run on Baxter with the first and second tool. We observe that the target is reached faster when tool 1 is used (after \u2248 5s), and a bit slower when tool 2 is used (after \u2248 10s).\nFig. 6 below illustrates the performance averaged over all the trials. The deviation from the goal region after reaching the goal is likely due to inaccuracies in tracking. After reporting that the tool is in the goal region, the tracker might later report a corrected estimate, indicating further tool adjustment is needed. We observe that in such cases the policy still succeeds in further pivoting the tool to the target angle."}, {"heading": "VII. CONCLUSIONS AND FUTURE WORK", "text": "In this work we proposed solving the pivoting task via building a simple custom simulator of the task dynamics and using reinforcement learning to learn control policies. We presented the learning procedure that is able to manage the mismatch between the simulated and real settings, hence does not require precise estimates of all the tool parameters. We then demonstrated the ability of the learned policy to solve the pivoting task on the Baxter robot.\nTo extend our work to a more general set of dextrous manipulation problems, in the future we can combine pivoting with sliding. This would enable more possibilities for repositioning. In fact, with pivoting we can change the orientation of the object around a single axis, while with sliding we can translate the object, hence changing the contact point. The next step would be to incorporate existing approaches to modeling sliding into the simulator.\nWe can also explore augmenting the learning part of our approach. In situations where further training on the hardware is possible, we could develop a second stage to finetune the policies learned in simulation. This could be accomplished, for example, by quickly learning an adjustment to the discrepancy between the simulated and real environment without the need to re-learn control policies from scratch. Another direction is to experiment with recurrent networks to model aspects of the environment not observable directly.\nFurther into the future we can consider the very challenging problem of manipulating non-rigid objects. Building high fidelity simulators for deformable objects and objects with variable center of mass has been mostly intractable in the past. However, approximate simulators can be constructed. We have observed that for rigid objects only approximate\nsimulation of the dynamics is sufficient to learn effective control policies. So we can explore whether our approach with learning from approximate simulation can be adapted to the case of manipulating deformable objects."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the European Union framework program H2020-645403 RobDREAM."}], "references": [{"title": "Extrinsic dexterity: In-hand manipulation with external forces", "author": ["N.C. Dafle", "A. Rodriguez", "R. Paolini", "B. Tang", "S.S. Srinivasa", "M. Erdmann", "M.T. Mason", "I. Lundberg", "H. Staab", "T. Fuhlbrigge"], "venue": "2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2014, pp. 1578\u20131585.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Regrasping", "author": ["P. Tournassoud", "T. Lozano-Perez", "E. Mazer"], "venue": "Robotics and Automation. Proceedings. 1987 IEEE International Conference on, vol. 4, Mar 1987, pp. 1924\u20131928.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1987}, {"title": "A framework for planning dexterous manipulation", "author": ["J.C. Trinkle", "J.J. Hunter"], "venue": "Robotics and Automation, 1991. Proceedings., 1991 IEEE International Conference on, Apr 1991, pp. 1245\u20131251 vol.2.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1991}, {"title": "Dynamic regrasping using a high-speed multifingered hand and a high-speed vision system", "author": ["N. Furukawa", "A. Namiki", "S. Taku", "M. Ishikawa"], "venue": "Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006., May 2006, pp. 181\u2013 187.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "A two-phase gripper to reorient and grasp", "author": ["N. Chavan-Dafle", "M.T. Mason", "H. Staab", "G. Rossano", "A. Rodriguez"], "venue": "2015 IEEE International Conference on Automation Science and Engineering (CASE), Aug 2015, pp. 1249\u20131255.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "A general framework for open-loop pivoting", "author": ["A. Holladay", "R. Paolini", "M.T. Mason"], "venue": "2015 IEEE International Conference on Robotics and Automation (ICRA), May 2015, pp. 3675\u20133681.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Swing-up regrasping algorithm using energy control", "author": ["A. Sintov", "A. Shapiro"], "venue": "2016 IEEE International Conference on Robotics and Automation (ICRA), May 2016, pp. 4888\u20134893.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "In-hand manipulation using gravity and controlled slip", "author": ["F.E. Vi\u00f1a", "Y. Karayiannidis", "K. Pauwels", "C. Smith", "D. Kragic"], "venue": "Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, Sept 2015, pp. 5636\u20135641.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive control for pivoting with visual and tactile feedback", "author": ["F.E. Vi\u00f1a", "Y. Karayiannidis", "C. Smith", "D. Kragic"], "venue": "2016 IEEE International Conference on Robotics and Automation (ICRA), May 2016, pp. 399\u2013406.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Locally weighted learning for control", "author": ["C.G. Atkeson", "A.W. Moore", "S. Schaal"], "venue": "Lazy learning. Springer, 1997, pp. 75\u2013113.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S. Levine", "P. Abbeel"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1071\u20131079.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel"], "venue": "CoRR, abs/1502.05477, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "arXiv preprint arXiv:1604.06778, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["A.A. Rusu", "M. Vecerik", "T. Roth\u00f6rl", "N. Heess", "R. Pascanu", "R. Hadsell"], "venue": "arXiv preprint arXiv:1610.04286, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Predicting slippage and learning manipulation affordances through gaussian process regression", "author": ["F.E. Vi\u00f1a", "Y. Bekiroglu", "C. Smith", "Y. Karayiannidis", "D. Kragic"], "venue": "IEEE-RAS International Conference on Humanoid Robots, Oct 2013, pp. 462\u2013468.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Friction models and friction compensation", "author": ["H. Olsson", "K.J. strm", "M. Gfvert", "C.C.D. Wit", "P. Lischinsky"], "venue": "Eur. J. Control, p. 176, 1998.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Computer simulation of stick-slip friction in mechanical dynamic systems.", "author": ["D. Karnopp"], "venue": "J. Dyn. Syst. Meas. Control.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1985}, {"title": "Openai gym", "author": ["G. Brockman", "V. Cheung", "L. Pettersson", "J. Schneider", "J. Schulman", "J. Tang", "W. Zaremba"], "venue": "2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "This problem falls in the general class of extrinsic dexterity problems formulated by [1].", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "Instead of relying on releasing the object and picking it up again [2], we focus on in-hand manipulation.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "Successful strategies for this dexterous task often rely on multi-fingered robotic hands or customized grippers [3]\u2013[5].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "Successful strategies for this dexterous task often rely on multi-fingered robotic hands or customized grippers [3]\u2013[5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "In [6] the authors exploit gravity to rotate an object between two stable poses by using a contact surface.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "Conversely, several other works on pivoting strongly focus on controlling the torque applied by the gripper on the object: in [7] the authors focus on swing-up motions.", "startOffset": 126, "endOffset": 129}, {"referenceID": 7, "context": "The adaptive control for pivoting presented in [8] exploits gravity and controlled slip with only visual feedback.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "This approach has then been extended to consider also tactile feedback in [9].", "startOffset": 74, "endOffset": 77}, {"referenceID": 9, "context": "[10], [11]), such approaches might limit ability to incorporate prior knowledge \u2013 for example, forgoing the knowledge that can be easily incorporated into dynamics equations in a simulator.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[10], [11]), such approaches might limit ability to incorporate prior knowledge \u2013 for example, forgoing the knowledge that can be easily incorporated into dynamics equations in a simulator.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "reported to perform well on several simulated robotic tasks: Trust Region Policy Optimization (TRPO) [12] and Deep Deterministic Policy Gradient (DDPG) [13].", "startOffset": 101, "endOffset": 105}, {"referenceID": 12, "context": "reported to perform well on several simulated robotic tasks: Trust Region Policy Optimization (TRPO) [12] and Deep Deterministic Policy Gradient (DDPG) [13].", "startOffset": 152, "endOffset": 156}, {"referenceID": 13, "context": "For example, a recent benchmarking paper used up to 25 million steps when training [14].", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "The problem of \u201cforgetting\u201d in the context of learning in robotics is discussed at length in [15] \u2013 the work that attempted to resolve the issue by pre-training in simulation, then using Progressive Neural Network architecture to continue learning on the real hardware.", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "It is difficult to estimate precisely all the necessary coefficients to construct a high-fidelity friction model [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "When the tool moves with respect to the gripper, that is \u03c6\u0307tl 6= 0, we model the friction torque \u03c4f as viscous friction and Coulomb friction [17]:", "startOffset": 141, "endOffset": 145}, {"referenceID": 17, "context": "To alleviate this problem we follow the approach proposed in [18].", "startOffset": 61, "endOffset": 65}, {"referenceID": 7, "context": "Since most of the robots are not equipped with tactile sensors to measure the normal force fn at the contact point, we follow the approach proposed in [8] and express this force as a function of the distance dfing between the two fingers, assuming a linear deformation model:", "startOffset": 151, "endOffset": 154}, {"referenceID": 11, "context": "In our experiments, we found that Trust Region Policy Optimization [12] was able to solve the problem without extensive parameter adjustment.", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "where \u03b8old denotes the initial (or previous) set of policy parameters, \u03b8 denotes the updated policy parameters, \u03c0\u03b8 is the stochastic policy parameterized by \u03b8, q is a sampling distribution for exploration, Q\u03b8old is the Q function approximator estimated from previous samples, and the expectation E is taken over samples obtained using the policy from previous iteration (see [12] for details).", "startOffset": 375, "endOffset": 379}, {"referenceID": 13, "context": "TRPO has been shown to be competitive with (and sometimes outperform) other recent continuous state and action RL algorithms [14].", "startOffset": 125, "endOffset": 129}, {"referenceID": 18, "context": "To facilitate experimenting with various RL algorithms, we implemented a custom environment in OpenAI Gym [19] for the pivoting task MDP (as defined in Section V-B).", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "To experiment with TRPO [12] and DDPG [13] algorithms,", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "To experiment with TRPO [12] and DDPG [13] algorithms,", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "we used rllab implementation [14] as a starting point, then adjusted the behavior and parameters as needed for various experiments for this project.", "startOffset": 29, "endOffset": 33}], "year": 2017, "abstractText": "In this work we propose an approach to learn a robust policy for solving the pivoting task. Recently, several model-free continuous control algorithms were shown to learn successful policies without prior knowledge of the dynamics of the task. However, obtaining successful policies required thousands to millions of training episodes, limiting the applicability of these approaches to real hardware. We developed a training procedure that allows us to use a simple custom simulator to learn policies robust to the mismatch of simulation vs robot. In our experiments, we demonstrate that the policy learned in the simulator is able to pivot the object to the desired target angle on the real robot. We also show generalization to an object with different inertia, shape, mass and friction properties than those used during training. This result is a step towards making model-free reinforcement learning available for solving robotics tasks via pre-training in simulators that offer only an imprecise match to the real-world dynamics.", "creator": "LaTeX with hyperref package"}}}